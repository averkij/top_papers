{
    "paper_title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder",
    "authors": [
        "Elias Abad Rocamora",
        "Christian Schlarmann",
        "Naman Deep Singh",
        "Yongtao Wu",
        "Matthias Hein",
        "Volkan Cevher"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Adversarial input attacks can cause a significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. When employing our robust CLIP encoders in multimodal retrieval tasks, we improve the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 5 5 3 3 0 . 6 0 5 2 : r Robustness in Both Domains: CLIP Needs Robust Text Encoder Elias Abad Rocamora , Christian Schlarmann , Naman Deep Singh , Yongtao Wu : LIONS - Ecole Polytechnique Federale de Lausanne, Switzerland , Matthias Hein , Volkan Cevher : Tubingen AI center, University of Tubingen, Germany {name.surname}@{epfl.ch, uni-tuebingen.de}"
        },
        {
            "title": "Abstract",
            "content": "Adversarial input attacks can cause significant shift of CLIP embeddings. This can affect the downstream robustness of models incorporating CLIP in the pipeline, such as text-to-image generative models or large vision language models. While some efforts have been done towards making the CLIP image encoders robust, the robustness of text encoders remains unexplored. In this work, we cover this gap in the literature. We propose LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models. Our models significantly improve the zero-shot adversarial accuracy in the text domain, while maintaining the vision performance provided by robust image encoders. When combined with text-to-image diffusion models, we can improve the generation quality under adversarial noise. When employing our robust CLIP encoders in multimodal retrieval tasks, we improve the recall under adversarial noise over standard CLIP models. Finally, we show that robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization."
        },
        {
            "title": "Introduction",
            "content": "Contrastive Language-Image Pretraining (CLIP) models embed images and captions into shared embedding space [Radford et al., 2021]. CLIP is simple but rather powerful tool for visionlanguage understanding, being employed in wide range of multimodal tasks such as retrieval [Fang et al., 2021, Koukounas et al., 2024, Vendrow et al., 2024], Large Multimodal Models (LMMs) [Alayrac et al., 2022, Liu et al., 2023] and text-to-image generative models [Ramesh et al., 2021, Rombach et al., 2022, Ramesh et al., 2022, Podell et al., 2024]. However, the simplicity of CLIP and its plug-and-play usage becomes double-edged sword, allowing adversarial attacks to be optimized over CLIP, and transferred to the downstream task of interest [Zhuang et al., 2023, Ghazanfari et al., 2023, 2024, Croce et al., 2025]. Recently, making the image encoder of CLIP robust has gained interest [Mao et al., 2023], making LMMs robust to adversarial perturbations by replacing the image encoder with an adversarially finetuned one [Schlarmann et al., 2024]. Nevertheless, adversarial finetuning has not been yet investigated for the text encoder. In this work, we fill this gap by studying adversarial finetuning for CLIP text encoders, proposing Levenshtein Efficient Adversarial Finetuning (LEAF). Motivated by recent advancements in the image domain, we optimize the same objective as Schlarmann et al. [2024], allowing us to replace the text encoder in tasks like text-to-image generation, without needing to finetune the rest of the pipeline. Moreover, to make adversarial finetuning faster in the text domain, we propose an attack that can be parallelized within training batches, accelerating the approach of Abad Rocamora et al. [2024] by an order of magnitude with very little loss of performance. Preprint. Perturbed inputs Encoders Embedding spaces CLIP FARE CLIP This work big burly grizzly bear ... big burly griHzly bear ... % big burly grizzly bear ... Figure 1: Left: our idea. Schlarmann et al. [2024] propose FARE: finetuning the CLIP image encoder to produce embeddings close to the clean image embedding () under image perturbations. Analogically, we finetune the CLIP text encoder to produce embeddings close to the clean text embedding () under text perturbations. Right: results in ViT-L/14. The first (second) / denotes the usage of robust image (text) encoder. We constrain the text attacks with the Levenshtein distance and the image attacks in the ℓ norm. By combining the FARE robust image encoder with our robust text encoder, we obtain high adversarial accuracy in both domains. Our models, LEAF, are able to improve the zero-shot adversarial accuracy of CLIP models from 44.5% to 63.3% in AG-News at distance = 1 (one character change). When plugged into Stable Diffusion [Rombach et al., 2022, Podell et al., 2024], we achieve higher quality images under character-level perturbations. For retrieval tasks, our models achieve recall 10 points higher on average than non-robust CLIP models at = 2. Moreover, when inverting the embeddings of text encoders through direct optimization, we show that with LEAF models, we can recover higher percentage of the original sentence. This results in LEAF encoders being more interpretable. Overall, we show the robustness of CLIP text encoders can be improved with minimal effects on the clean performance in several tasks. We believe our robust CLIP models can make future models incorporating CLIP more robust and interpretable. Our code and models can be found in github.com/LIONS-EPFL/LEAF and huggingface.co/LEAF-CLIP respectively. Notation: We use uppercase bold letters for matrices Rmn, lowercase bold letters for vectors Rm and lowercase letters for numbers R. Accordingly, the ith row and the element in the i, position of matrix are given by xi and xij respectively. We use the operator for the size of sets, e.g., S(Γ) and the length of sequences, e.g., for Rmn, we have = m. For two vectors u, Rh, we denote the cosine similarity as sim(u, v) = . We use the shorthand [n] = {0, 1, , 1} for any natural number n. uv u2v"
        },
        {
            "title": "2 Background",
            "content": "In Section 2.1 we cover the approaches improving the adversarial robustness of CLIP. In Section 2.2 we discuss robustness in the text domain. 2.1 Robustness of CLIP Let S(Γ) = {c1c2 cm : ci Γ 0} be the space of sequences of characters in the alphabet set Γ. We represent sentences S(Γ) as sequences of one-hot vectors, i.e., {0, 1}mΓ : si1 = 1, [m]. Similarly, we can represent images with pixels as real vectors Rd. Overall, the training dataset is composed of text-image pairs {Si, xi}n The objective of CLIP is to learn text encoder fθ : S(Γ) Rh and an image encoder gω : Rd Rh, where is the embedding size and θ and ω are the parameters of the text and image encoders respectively. Radford et al. [2021] propose to maximize the cosine similarity of positive i=1. 2 sentence-image pairs relative to the cosine similarity with other sentences and images in the dataset. We denote the weights obtained after pretraining with CLIP as θCLIP and ωCLIP. In order to make the image encoder gω robust in the zero-shot classification task, Mao et al. [2023] use the sentences Sj = photo of LABELj, [o], where is the number of classes. Then, given dataset of images and labels {xi, yi}n i=1, so that yi [o], Mao et al. [2023] optimize: (cid:32) (cid:33) min ω (cid:88) i=1 max δiϵ log efθCLIP (Syi )gω(xi+δi) j=1 efθCLIP (Sj )gω(xi+δi) (cid:80)o . (TeCoA) TeCoA significantly improves the robustness of the image encoder. However, it generalizes poorly to image classification tasks that are not part of the fine-tuning dataset, and degrades the performance when employed in an LMM pipeline, as shown by Schlarmann et al. [2024]. In order to overcome this, Schlarmann et al. [2024] propose FARE, which intends to preserve the original image embeddings while being robust. To do so, they optimize: min ω (cid:88) i=1 max δiϵ gωCLIP(xi) gω(xi + δi)2 2 . (FARE) The FARE objective allows to employ the obtained image encoder within an LMM pipeline with minimal clean performance degradation. Motivated by these findings, in this work we construct similar loss in the text domain (Eq. (TextFARE)) and adapt the algorithm to the challenges of this new domain (LEAF). See Fig. 1 for visualization of the FARE and LEAF approaches. 2.2 Robustness in the text domain Belinkov and Bisk [2018], Alzantot et al. [2018] showed that text classifiers are not robust to natural or adversarial noise, with text adversarial attacks being used in Large Language Models [Zou et al., 2023] and text-to-image generative models [Zhang et al., 2025]. Generally, given sentence S, model and some loss function L, the adversarial attack problem can be formulated as: max SN (S) L(f (S)) , where (S) is set of neighboring sentences, i.e., the threat model. great challenge in the text domain is defining valid threat model, as the semantics of the sentence should be preserved according to the task [Morris et al., 2020]. In the literature, we can categorize in two main threat models: token and character level attacks. With token level attacks set to replace/insert/delete small number of tokens in the sentence [Ren et al., 2019, Jin et al., 2020, Li et al., 2019, Garg and Ramakrishnan, 2020, Lee et al., 2022, Ebrahimi et al., 2018, Li et al., 2020, Guo et al., 2021, Hou et al., 2023]. Similarly, character-level attacks replace/insert/delete small number of characters in the sentence [Belinkov and Bisk, 2018, Ebrahimi et al., 2018, Gao et al., 2018, Pruthi et al., 2019, Yang et al., 2020, Liu et al., 2022, Abad Rocamora et al., 2024]. Both approaches can be thought of as keeping small Levenshtein distance [Levenshtein et al., 1966] between the original and attacked sentences in the token or character-level. Semantic constraints: To ensure that semantics are preserved, token-level attacks usually constrain (S) further by only allowing token replacements between tokens with high similarity in the embedding space [Jin et al., 2020]. But, even with such semantic constraints, several works have pointed out that token level attacks do not preserve semantics [Morris et al., 2020, Dyrmishi et al., 2023], with Hou et al. [2023] reporting 56.5% of their attacks change the semantics of the sentence. Due to the difficulty in preserving semantics, we focus on character-level attacks in this work. In the case of the character-level attacks, to further preserve semantics and simulate natural typos, some works constrain the attack to only replace characters that are nearby in the English keyboard [Belinkov and Bisk, 2018, Huang et al., 2019]. Others do not allow the attack to modify the first and last letter of words, to perturb short words, to perturb the same word twice or to insert special characters [Pruthi et al., 2019, Jones et al., 2020]. In the context of text-to-image generation, Chanakya et al. [2024] find that changing one character in the sentence can change one word for another and the text-to-image model accordingly generates different object in the image. In order to avoid this, Chanakya et al. [2024] introduce the semantic constraint of not allowing new English words to appear after the attack. In this work, we decide to adopt the semantic constraints of [Chanakya et al., 2024] and find they are especially useful when performing adversarial finetuning of the CLIP text encoders, see Section 4.2.2. 3 1. Select position n 2. Select character G a v G Y u e r e U a a a a a a a a a a a a b b b b b b b b b b b b c c c c c c c c c c c c d d d d d d d d d d d d e e e e e e e e e e e e ... $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ... a a a a a a a a a a a a b b b b b b b b b b b b c c c c c c c c c c c c d d d d d d d d d d d d e e e e e e e e e e e e ... $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ $ ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ... 3. Final perturbation: Never Gonna G?ve You Up Figure 2: Schematic and example of the attack used in LEAF: In the first step, we randomly select ρ = 6 positions, replace these with whitespace and select the position with the highest loss. Next, we randomly select ρ characters from Γ, replace them in the chosen position and choose the one with the highest loss as the final perturbation. During training, the attack evaluates ρ sentences in every forward pass, where is the batch size. For more details, see Algorithm 1 in the appendix."
        },
        {
            "title": "3 Method",
            "content": "In order to make the text encoder adversarially robust, we extend Eq. (FARE) to the text domain as: min θ (cid:88) i=1 i:dLev(Si,S max i)kS iC(Si) fθCLIP (Si) fθ(S i)2 2 , (TextFARE) where the Levenshtein dLev distance is bounded by parameter k, and C(S) is either the complete set of sentences S(Γ) or subset only containing sentences with semantic constraints, see Section 2.2. Intuitively, if the original CLIP encoder evaluated at the original sentence (fθCLIP (S)) provides good performance in downstream tasks, e.g., zero-shot classification or text-to-image generation, then, by solving Eq. (TextFARE), we will obtain model that achieves similar performance under perturbations of the sentence. Moreover, Eqs. (FARE) and (TextFARE) allow for decoupled training of the text and image encoders. Motivated by Danskins Theorem [Danskin, 1966, Latorre et al., 2023], we can (approximately) solve min-max problems by maximizing the inner problem and minimizing the error on the obtained perturbation. In the case of Eq. (FARE), Projected Gradient Descent (PGD) is used for the inner maximization problem [Madry et al., 2018, Schlarmann et al., 2024]. Similarly, we can use any adversarial attack to maximize the inner problem in Eq. (TextFARE), e.g., Gao et al. [2018], Abad Rocamora et al. [2024]. However, not every attack is adequate for adversarial finetuning, e.g., in the image domain, the strongest attacks in the AutoAttack ensemble [Croce and Hein, 2020] are never used during training due to their expensive time requirements. Contrarily, cheaper PGD attacks are used during training, providing fast training and generalization to stronger adversarial attacks Goodfellow et al. [2015], Madry et al. [2018], Shafahi et al. [2019], Wong et al. [2020]. The desiderata for an adversarial attack used during training can be captured by two points: (i) High adversarial robustness to strong attacks after training, (ii) Low computational resources. As baseline attack in the text domain, we select Charmer [Abad Rocamora et al., 2024]. Adversarial training with Charmer in text classification results in strong adversarial robustness, satisfying (i). Nevertheless, Charmer is not resource-efficient during training and thereby does not satisfy our second desiderata (ii). This is due to Charmer needing to evaluate number of perturbations O((2 Si + 1) + nCharmer Γ), which depends on the length of the sentence being attacked. This makes it harder to perform the attack simultaneously over sentences in batch. Overcoming this limitation, we propose Levenshtein Efficient Adversarial Finetuning (LEAF): utilizing training-time attack that evaluates constant number of perturbations ρ per sentence. Our attack replaces test character (the whitespace) in ρ random positions within the sentence to select the position with the highest loss. Then, ρ random characters are replaced in the chosen position to choose again the one with the highest loss. Overall, this allows to perform the attack in two sequential evaluations of ρ sentences, where is the batch size. visual representation of our attack is available in Fig. 2. Interestingly, if ρ = 1, our attack performs random perturbation. For more detailed discussion on LEAF, we refer to Appendix B. In Section 4.2 we empirically show LEAF satisfies our two desiderata. Figure 3: Training hyperparameter effects: We report the zero-shot clean and adversarial accuracy in the image (ImageNet) and text (AG-News) domains with FARE as baseline. When no semantic constraints are employed (Section 2.2), the robustness in the text domain is improved at the cost of significantly degrading the image domain performance. Adding semantic constraints improves the robustness in the text domain with minimal effects on the image domain. Using random perturbations (ρ = 1) improves the AG-News adversarial accuracy by 9.9 points, with stronger attacks (ρ = 50) providing the best performance with 18.7 points of improvement."
        },
        {
            "title": "4 Experiments",
            "content": "We start by introducing our experimental setup in Section 4.1. In Section 4.2 we cover our training results and display the interplay between ρ, and the usage of additional constraints during training. In Section 4.3 we present the performance of our models in zero-shot classification. In Section 4.4, we evaluate our CLIP models in multimodal retrieval tasks. In Section 4.5 we evaluate the performance of our CLIP text encoders when incorporated into text-to-image generative models. Finally, in Section 4.6 we evaluate how amenable our models are to embedding inversion. 4.1 Experimental setup We train our text encoders for 30 epochs on the first 80, 000 samples of the DataComp-small dataset [Gadre et al., 2023] with batch size of 128 sentences, = 1, ρ = 50 and semantic constraints, see Section 4.2.2, employing CLIP-ViT-L/14, OpenCLIP-ViT-H/14, OpenCLIP-ViT-g/14 and OpenCLIP-ViT-bigG/14 models. On the visual side, we scale the training method of Schlarmann et al. [2024] to ViT-H/14 and ViT-g/14, using an ℓ threat model with radius ϵ = 2/255. See Appendix B.3 for detailed account of hyperparameters. For evaluating the adversarial robustness with respect to image perturbations, we follow Schlarmann et al. [2024] and employ the first two APGD attacks from the AutoAttack ensemble [Croce and Hein, 2020] with ϵ = 2/255. In the text domain, we choose Charmer-20 with = 1 [Abad Rocamora et al., 2024] for evaluation. We employ the semantic constraints considered by [Chanakya et al., 2024] in the text-to-image and retrieval tasks. For the zero shot classification tasks, we do not employ such constraints as done by Abad Rocamora et al. [2024]. For discussion on the use of constraints, we refer to Appendix D.1. For zero shot sentence classification with CLIP models, we follow the setup of Qin et al. [2023], see Appendix B.4 for more details. For additional experiments and ablations, we refer to Appendix D. 4.2 Training robust text encoders In Section 4.2.1 we analyze the performance and training speed of Charmer and LEAF. In Section 4.2.2 we analyze how the performance is affected by our hyperparameters, i.e., k, ρ and C(S). 4.2.1 Faster adversarial finetuning First, we evaluate the performance of LEAF in terms of time and adversarial accuracy against training with Charmer [Abad Rocamora et al., 2024] with nCharmer {1, 20}. To do so, we train CLIP-ViTB-32 for 1 epoch at = 1 and using ρ {20, 50} for LEAF over three random training seeds. We 5 Table 1: Selecting the best attack for Adversarial Finetuning on ViT-B-32: We measure the AG-News clean (Acc.) and adversarial accuracy (Adv.) at = 1 with Charmer-20 and the time in seconds to attack batch of 128 sentences. We perform Adversarial Finetuning (Eq. (TextFARE)) for 1 epoch with = 1 using the attacks Charmer-1, Charmer-20 and LEAF with ρ {20, 50}. Our approach minimally affects the adversarial accuracy while being an order of magnitude faster than the fastest Charmer variant."
        },
        {
            "title": "Defense",
            "content": "AG-News Acc. (%) Adv. (%) Time (s) Charmer-20 Charmer-1 LEAF (ρ = 50) LEAF (ρ = 20) 76.70(0.14) 76.37(0.21) 76.63(0.21) 76.87(0.25) 60.17(0.31) 60.20(0.37) 59.80(0.37) 58.30(0.29) 118.19(53.68) 15.17(28.98) 3.23(0.17) 1.83(0.11) measure the clean and adversarial accuracies with Charmer-20 on AG-News [Gulli, 2005, Zhang et al., 2015] and the average time to attack batch of 128 samples. In Table 1 we can observe that LEAF attains comparable clean and adversarial accuracies in comparison to the Charmer variants, while being significantly faster, i.e., 1.83 and 3.23 seconds per batch for our method in comparison to 15.17 and 118.19 seconds for the Charmer variants. 4.2.2 The effect of our hyperparameters In order to test the influence of our training hyperparameters, we finetune CLIP-ViT-L/14 initialized from pretrained FARE weights [Schlarmann et al., 2024] with ρ {1, 2, 5, 10, 20, 50}, {1, 2} and C(S) including and not including semantic constraints. To evaluate how our method improves the robustness in the text domain, and affects the robustness in the image domain, we measure the clean and adversarial accuracies on ImageNet and AG-News. In Fig. 3 we report the performance for = 1. When increasing ρ, the adversarial accuracy in the text domain increases consistently. However, when employing unconstrained training attacks, both the clean and adversarial performance in the image domain are significantly degraded, e.g., clean accuracy of 65.5% at ρ = 50 v.s. 74.7 for the FARE model. In contrast, when applying semantic constraints, the improvements in robustness in the text domain follow similar trend and the performance in the image domain is less degraded. For = 2, we can extract the same insights, see Fig. 7. Overall, we select ρ = 50, = 1 and the use of semantic constraints during training. 4.3 Zero-shot classification We show the ImageNet and AG-News performance of the models when using robust encoders in image and/or text domain in Table 2 and Fig. 1. We observe that our robust text encoders introduce only minimal drop in image performance, while significantly improving the robustness in the text domain. Moreover, we observe that the effectiveness of FARE for fine-tuning robust image encoders that was demonstrated for ViT-L/14 by Schlarmann et al. [2024], extends to the larger ViT-H/14 and ViT-g/14 models. The lower performance of ViT-g/14 on ImageNet could be attributed to the smaller training batch size, see Appendix B.3. Importantly, only models that use robust encoder in both domains achieve robustness in both tasks. Figure 4: Larger perturbations: We evaluate the adversarial accuracy in AGNews for {1, 2, 3, 4, 5} in the ViTL/14 scale. Our model (LEAF) obtains the highest adversarial accuracy at all values of the distance bound k. In Fig. 4 we report the adversarial accuracy of the ViTL/14 sized models in the AG-News dataset for {0, 1, 2, 3, 4, 5}, with = 0 representing the clean accuracy. Our model, while being trained with = 1, is able to extrapolate the robustness to larger k. We observe that the CLIP and FARE models obtain nearly zero adversarial accuracy for 4, while our model, is able to obtain the highest performance for any k. Table 2: Zero-shot classification. We report the adversarial accuracy (Adv.) on ImageNet with the first two attacks of AutoAttack (APGD-CE, APGD-t) at ϵ = 2/255 and on AG-News with Charmer20 at = 1. Only models employing robust image and text encoders are robust in both domains. Robust Encoder CLIP-ViT-L/14 OpenCLIP-ViT-H/14 OpenCLIP-ViT-g/14 ImageNet AG-News ImageNet AG-News ImageNet AG-News Image Text Acc. Adv. Acc. Adv. Acc. Adv. Acc. Adv. Acc. Adv. Acc. Adv. 76.4 74.7 73.4 72.6 0.0 47.6 0.0 46. 74.4 78.7 73.9 78.0 44.7 44.5 60.1 63.2 77.2 76.8 77.0 76.8 0.0 48.4 0.0 46.3 71.1 70.7 71.1 72.3 37.6 37.5 50.2 53. 77.8 73.8 76.3 72.0 0.0 41.8 0.0 41.3 67.3 66.4 67.3 66.7 35.8 32.9 47.4 46.3 Figure 5: Visualizing MS-COCO retrieved images. For our ViT-L/14 robust model and its nonrobust counterpart, we show the top-3 retrieved images for the original Query and the perturbed Query via Charmer (k = 2, = 10) attack. The robust model is able to preserve the order and retrieves semantically relevant images even for the perturbed query. More illustrations can be found in Appendix D.4. The target query in this case was This is an image of pyramid. 4.4 Text-image retrieval Robustness of CLIP models to perturbations of textual queries is important as these models are often used as dataset/content filters Hong et al. [2024] and NSFW detectors Schuhmann et al. [2022], meaning any false negative can be detrimental. The robustness of retrieval based filters for visual adversaries has already been tested in Croce et al. [2025]. Consider the case where CLIP based NSFW filter is queried with perturbed query, any false negative retrieval here would detrimental and concerning. To test how robust CLIP models are to such character based queries in retrieval setup, we test on the MS-COCO dataset as proxy task. For 1, 000 validation set queries, the attack maximizes the similarity between the test query and target string using different variants of the Charmer attack. Given some query text and corresponding embedding fθ(S), we maximize the cosine similarity between fθ(S) and fθ(T ), where is target text semantically unrelated to S. The objective takes the following form, max S:dLev(S,S)kSC(S) sim (fθ(S), fθ(T )) . (1) The optimization is done with the constrained Charmer attack for different number of character changes. is initialized with S, and the overall perturbation set is constrained with C(S) from Chanakya et al. [2024]. The formulation of the attack above can be seen as targeted attack, the same attack can be done in an untargeted manner as in Eq. (2). In Table 3, for different CLIP models, we show average Recall across 3 target strings, detailed results for each target can be found in Appendix D.4. For both 1 (k = 1) and 2 (k = 2) character perturbations, we see that the non-robust CLIP models retrieval performance goes down. Our robust models on the other hand showcase strong robustness while showing small degradation in clean performance. For LEAF, the clean performance follows trade-off with robustness depending on ρ, see Appendix D.4. Fig. 5, visualizes the attack and the top-3 retrieved images for sample test query. Under perturbation, the non-robust model retrieves completely irrelevant images. The robust 7 Table 3: MS-COCO text-to-image retrieval: The statistics of the targeted Charmer adversarial attack (with = 1, 2 and semantic constraints) are averaged over 3 target strings. : denotes non-robust CLIP model, whereas indicates CLIP model robust in both image and text domains."
        },
        {
            "title": "Robust",
            "content": "Recall@1 Recall@"
        },
        {
            "title": "Clean",
            "content": "Eval. Charmer-Con Recall@1 Recall@5 CLIP-ViT-L/14 OpenCLIP-ViT-H/ OpenCLIP-ViT-g/14 49.11 48.71 58.64 56.80 60. 55.98 73.79 73.71 81.29 80.65 82. 79.33 1 2 1 2 1 2 1 2 1 2 1 2 37.31 30.66 45.06 40.22 47.81 39.26 52.97 49. 47.93 37.51 52.30 48.71 62.67 52.76 69.35 65.09 72.22 63.35 77.26 73.50 72.71 61.82 76.95 73.71 model on the other hand, preserves the order and retrieves images relevant to the query. Moreover, in almost all cases it retrieves the top-1 image correctly, see Appendix D.4 for more such examples. 4.5 Robustness of text-to-image models In this section, we evaluate the performance of our robust text encoders when plugged into textto-image generation pipelines. We take SD-1.5 [Rombach et al., 2022] and SDXL [Podell et al., 2024]. SD-1.5 employs the text encoder from ViT-L/14 and SDXL employs two text encoders: from ViT-L/14 and ViT-bigG/14. In order to attack the model, we follow Zhuang et al. [2023] by only accessing the text encoder. Given sentence S, we employ Charmer-20 to solve: min S:dLev(S,S)kSC(S) sim(fθ(S), fθ(S)) . (2) By minimizing the similarity between the original and perturbed embedding, we expect that the model generates images that do not align to the original caption. For SDXL, we maximize the average dissimilarities for both encoders. To analyze the quality of the generated images, through CLIP-ViT-B-16, we measure the CLIPScore between the original caption and the generated image. In Fig. 6 we present the MS-COCO [Lin et al., 2014] SDXL image generation results. We can observe that the CLIPScore of SDXL with the LEAF encoders is significantly larger than the original SDXL for 1. On the right-hand-side of Fig. 6 we present the generated images for the first five captions in the MS-COCO validation dataset at = 2, where for two captions, the original SDXL model produces completely different images compared to the original ones. For SD-1.5 results, additional text-to-image generation details and experiments, we refer to Appendix D.3 4.6 Text embedding inversion It is well known that robust models in the vision domain possess more interpretable gradients than clean models [Santurkar et al., 2019], which can be exploited to generate visual counterfactual explanations [Augustin et al., 2020, Boreiko et al., 2022]. Moreover, this allows to reconstruct images from their embeddings of robust model by direct gradient based optimization [Croce et al., 2025]. We test if this advantageous property of robust vision models also holds in robust text models. To this end, we study the ability to invert text embeddings. Given an embedding fθ(S), the goal is to reconstruct the unknown text S. Therefore we aim to solve the objective max SS(Γ) sim (fθ(S), fθ(S)) . (3) To this end, we use the optimization method from Wen et al. [2023], where the text is initialized uniformly at random over the vocabulary of tokens and optimized via gradient based algorithm. 8 stop is upside-down its post. sign mounted on Three teddy bears, each different snuggling color, together. Caption woman stands in the dining area at the table. big burly grizzly bear is show with grass in the background. Bedroom scene with bookcase, comforter blue and window. MS-COCO Original LEAF Figure 6: Text-to-image generation results on SDXL: On the left side, we present the MSCOCO CLIPScores of SDXL. The LEAF text encoders consistently improve the generation quality of SDXL under adversarial noise. On the right, we present the first five MS-COCO samples from the validation set and the corresponding SDXL generations at = 2. The color borders indicate null, partial and total matching to the original image. With the original encoder, images 1 and 4 do not match at all the original ones. With the FARE encoders, all of the five images resemble the original ones, with some errors like the mismatch in the number of objects in image 5. Table 4: Text embedding inversion. We invert text embeddings and measure the quality of reconstructions with various metrics. Robust models yield better reconstructions according to all metrics. Model Robust sim Word Rec. Token Rec. BLEU CLIP-ViT-L/ OpenCLIP-ViT-H/14 OpenCLIP-ViT-g/14 0.89 0. 0.86 0.93 0.94 0.96 34.4 46.4 33.5 49.0 43.7 54.8 38.9 52. 34.1 50.3 48.1 60.6 8.3 12.2 8.9 13.7 5.6 12.2 We randomly sample 100 captions from MS-COCO, embed them via the given original and robust text encoders, and measure the success of reconstruction with four metrics: The cosine similarity between fθ(S) and fθ(S), i.e., the objective in Eq. (3). Word Recall and Token Recall are the percentages of words/tokens in the original text that appear in the reconstruction, irrespective of order. Finally, BLEU [Papineni et al., 2002] is an ordering-aware similarity metric. We show results in Table 4. The models with robust text encoders are best in every metric. Interestingly, we observe that the reconstructions of robust models generally improve when scaling up model size, while for non-robust models it does not improve from ViT-L/14 to ViT-H/14, but improves from ViT-H/14 to ViT-g/14. We observe that BLEU scores are low for all models, indicating that while many words are reconstructed correctly, their ordering is not. This could be attributed to the bag-of-words behavior of CLIP models discovered by Yuksekgonul et al. [2023]. We show some randomly selected example reconstructions in Appendix Tables 18 and 19."
        },
        {
            "title": "5 Conclusion",
            "content": "We have conducted the first adversarial finetuning study of CLIP text encoders. We show that training robust text encoders results in an enhanced adversarial zero-shot accuracy in the text domain, robust text-to-image generation and robust text-to-image retrieval. Additionally, our robust models (LEAF) are more easily invertible and interpretable. Limitations: Our robust image and text encoders are finetuned in isolation, adversarial attacks could still exist if perturbations are optimized jointly in both domains. We did not study this possibility. In this work, token level robustness is not studied, as token level attacks often change the semantics of sentences [Dyrmishi et al., 2023]. Due to computational constraints, we did not train the largest image encoders (OpenCLIP-ViT-bigG) or the largest EVA-CLIP models [Sun et al., 2024]. Our approach has not yet been tested in other tasks using text encoders, e.g., RAG [Lewis et al., 2020]."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by the Swiss National Science Foundation (SNSF) under grant number 200021 205011. Research was sponsored by the Army Research Office and was accomplished under Grant Number W911NF-24-1-0048. This work was supported by Hasler Foundation Program: Hasler Responsible AI (project number 21043). This work was supported as part of the Swiss AI Initiative by grant from the Swiss National Supercomputing Centre (CSCS) under project ID a07 on Alps. EAR, YW and VC thank Gosia Baltaian for her administrative help. We thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting CS and NDS. We acknowledge support from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germanys Excellence Strategy (EXC number 2064/1, project number 390727645), as well as in the priority program SPP 2298, project number 464101476. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors."
        },
        {
            "title": "References",
            "content": "Elias Abad Rocamora, Yongtao Wu, Fanghui Liu, Grigorios G. Chrysos, and Volkan Cevher. Revisiting character-level adversarial attacks for language models. In International Conference on Machine Learning (ICML), 2024. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual In Advances in neural information processing systems language model for few-shot learning. (NeurIPS), 2022. Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Srivastava, and Kai-Wei Chang. Generating natural language adversarial examples. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018. Maximilian Augustin, Alexander Meinke, and Matthias Hein. Adversarial robustness on inand out-distribution improves explainability. In ECCV, 2020. Brian Bartoldson, James Diffenderfer, Konstantinos Parasyris, and Bhavya Kailkhura. Adversarial robustness limits via scaling-law and human-alignment studies. In International Conference on Machine Learning (ICML), pages 30463072, 2024. Yonatan Belinkov and Yonatan Bisk. Synthetic and natural noise both break neural machine translation. In International Conference on Learning Representations, 2018. Steven Bird and Edward Loper. NLTK: The natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pages 214217, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/P04-3031/. Valentyn Boreiko, Maximilian Augustin, Francesco Croce, Philipp Berens, and Matthias Hein. Sparse visual counterfactual explanations in image space. In GCPR, 2022. Patibandla Chanakya, Putla Harsha, and Krishna Pratap Singh. Robustness of generative adversarial clips against single-character adversarial attacks in text-to-image generation. IEEE Access, 2024. Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, 2014. Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In AISTATS, 2011. Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble In International Conference on Machine Learning (ICML), of diverse parameter-free attacks. 2020. 10 Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: standardized adversarial robustness benchmark. arXiv preprint arXiv:2010.09670, 2020. Francesco Croce, Christian Schlarmann, Naman Deep Singh, and Matthias Hein. Adversarially robust clip models can induce better (robust) perceptual metrics. In SaTML, 2025. J. Danskin. The theory of max-min, with applications. SIAM Journal on Applied Mathematics, 14 (4):641664, 1966. doi: 10.1137/0114053. Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu. Towards robustness against natural language word substitutions. In International Conference on Learning Representations (ICLR), 2021. Salijona Dyrmishi, Salah Ghamizi, and Maxime Cordy. How do humans perceive adversarial text? reality check on the validity and naturalness of word-based adversarial attacks. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023. Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. HotFlip: White-box adversarial examIn Proceedings of the 56th Annual Meeting of the Association for ples for text classification. Computational Linguistics (Volume 2: Short Papers), 2018. Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097, 2021. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. In IEEE Security and Privacy Workshops (SPW), 2018. Siddhant Garg and Goutham Ramakrishnan. BAE: BERT-based adversarial examples for text classification. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. Sara Ghazanfari, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, and Alexandre Araujo. R-LPIPS: An adversarially robust perceptual similarity metric. In ICML Workshop on New Frontiers in Adversarial Machine Learning, 2023. Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Farshad Khorrami, and Siddharth Garg. Lipsim: provably robust perceptual similarity metric. In ICLR, 2024. Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. International Conference on Learning Representations (ICLR), 2015. Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Timothy Mann. Improving robustness using generated data. Advances in neural information processing systems (NeurIPS), 34:42184233, 2021. Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007. Antonio Gulli. Ags corpus of news articles, 2005. URL http://groups.di.unipi.it/gulli/ AG_corpus_of_news_articles.html. Chuan Guo, Alexandre Sablayrolles, Herve Jegou, and Douwe Kiela. Gradient-based adversarial attacks against text transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021. 11 Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7), 2019. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-of-distribution generalization. In ICCV, 2021. Rachel Hong, William Agnew, Tadayoshi Kohno, and Jamie Morgenstern. Whos in and whos out? case study of multimodal clip-filtering in datacomp. In Proceedings of the 4th ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, 2024. Bairu Hou, Jinghan Jia, Yihua Zhang, Guanhua Zhang, Yang Zhang, Sijia Liu, and Shiyu Chang. Textgrad: Advancing robustness evaluation in NLP by gradient-driven optimization. In International Conference on Learning Representations (ICLR), 2023. Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krishnamurthy Dvijotham, and Pushmeet Kohli. Achieving verified robustness to symbol substitutions via interval bound propagation. In Empirical Methods in Natural Language Processing (EMNLP), 2019. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/ zenodo.5143773. If you use this software, please cite it as below. Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? strong baseline for natural language attack on text classification and entailment. AAAI Conference on Artificial Intelligence, 2020. Erik Jones, Robin Jia, Aditi Raghunathan, and Percy Liang. Robust encodings: framework for combating adversarial typos. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015. Andreas Koukounas, Georgios Mastrapas, Michael Gunther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Martınez, Saahil Ognawala, et al. Jina clip: Your clip model is also your text retriever. arXiv preprint arXiv:2405.20204, 2024. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, 2013. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of Toronto, Canada, 2009. Fabian Latorre, Igor Krawczuk, Leello Tadesse Dadi, Thomas Michaelsen Pethick, and Volkan Cevher. Finding actual descent directions for adversarial training. In International Conference on Learning Representations (ICLR), 2023. Deokjae Lee, Seungyong Moon, Junhyeok Lee, and Hyun Oh Song. Query-efficient and scalable black-box adversarial attacks on discrete sequential data via bayesian optimization. In International Conference on Machine Learning, pages 1247812497. PMLR, 2022. Vladimir Levenshtein et al. Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady, volume 10, pages 707710. Soviet Union, 1966. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In NeurIPS, 2020. 12 Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adversarial text against real-world applications. Network and Distributed Systems Security (NDSS) Symposium, 2019. Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. BERT-ATTACK: Adversarial attack against BERT using BERT. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Computer Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. Aiwei Liu, Honghai Yu, Xuming Hu, Shuang Li, Li Lin, Fukun Ma, Yawen Yang, and Lijie Wen. Character-level white-box adversarial attacks against transformers via attachable subwords subIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language stitution. Processing, 2022. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations (ICLR), 2019. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Association for Computational Linguistics, 2011. Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018. Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft, 2013. Chengzhi Mao, Scott Geng, Junfeng Yang, Xin Wang, and Carl Vondrick. Understanding zero-shot adversarial robustness for large-scale models. In International Conference on Learning Representations (ICLR), 2023. Takeru Miyato, Andrew M. Dai, and Ian Goodfellow. Adversarial training methods for semisupervised text classification. In International Conference on Learning Representations (ICLR), 2017. John Morris, Eli Lifland, Jack Lanchantin, Yangfeng Ji, and Yanjun Qi. Reevaluating adversarial In Findings of the Association for Computational Linguistics: examples in natural language. EMNLP 2020, 2020. John Xavier Morris, Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander Rush. Text embeddings reveal (almost) as much as text. In EMNLP, 2023. John Xavier Morris, Wenting Zhao, Justin Chiu, Vitaly Shmatikov, and Alexander Rush. Language model inversion. In ICLR, 2024. Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing. IEEE, 2008. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, 2002. Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, 2012. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In International Conference on Learning Representations (ICLR), 2024. Samuele Poppi, Tobia Poppi, Federico Cocchi, Marcella Cornia, Lorenzo Baraldi, and Rita CucIn European chiara. Safe-clip: Removing nsfw concepts from vision-and-language models. Conference on Computer Vision (ECCV), pages 340356. Springer, 2024. Danish Pruthi, Bhuwan Dhingra, and Zachary C. Lipton. Combating adversarial misspellings with robust word recognition. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Libo Qin, Weiyun Wang, Qiguang Chen, and Wanxiang Che. CLIPText: new paradigm for zero-shot text classification. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual In International Conference on Machine Learning models from natural language supervision. (ICML). PMLR, 2021. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning (ICML), 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Sylvestre-Alvise Rebuffi, Sven Gowal, Dan Andrei Calian, Florian Stimberg, Olivia Wiles, and In A. Beygelzimer, Y. Dauphin, Timothy Mann. Data augmentation can improve robustness. P. Liang, and J. Wortman Vaughan, editors, Advances in neural information processing systems (NeurIPS), 2021. URL https://openreview.net/forum?id=kgVJBBThdSZ. Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating natural language adversarial examples through probability weighted word saliency. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Shibani Santurkar, Andrew Ilyas, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Image synthesis with single (robust) classifier. In NeurIPS, 2019. Christian Schlarmann and Matthias Hein. On the adversarial robustness of multi-modal foundation models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 36773685, October 2023. Christian Schlarmann, Naman Deep Singh, Francesco Croce, and Matthias Hein. Robust clip: Unsupervised adversarial fine-tuning of vision embeddings for robust large vision-language models. International Conference on Machine Learning (ICML), 2024. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022. Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! Advances in neural information processing systems (NeurIPS), 2019. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013. Quan Sun, Jinsheng Wang, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, and Xinlong Wang. Eva-clip-18b: Scaling clip to 18 billion parameters, 2024. URL https://arxiv.org/ abs/2402.04252. 14 Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014. Bastiaan Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In MICCAI. Springer, 2018. Edward Vendrow, Omiros Pantazis, Alexander Shepard, Gabriel Brostow, Kate Jones, Oisin Mac Aodha, Sara Beery, and Grant Van Horn. Inquire: natural world text-to-image retrieval benchmark. Advances in neural information processing systems (NeurIPS), 37:126500126514, 2024. Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. Info{bert}: Improving robustness of language models from an information theoretic perspective. In International Conference on Learning Representations (ICLR), 2021. Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning robust global representations by penalizing local predictive power. In NeurIPS, 2019. Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models further improve adversarial training. In International Conference on Machine Learning (ICML), 2023. Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. NeurIPS, 2023. Eric Wong, Leslie Rice, and Zico Kolter. Fast is better than free: Revisiting adversarial training. International Conference on Learning Representations (ICLR), 2020. Puyudi Yang, Jianbo Chen, Cho-Jui Hsieh, Jane-Ling Wang, and Michael I. Jordan. Greedy attack and gumbel attack: Generating adversarial examples for discrete data. Journal of Machine Learning Research, 21(43):136, 2020. URL http://jmlr.org/papers/v21/19-569.html. Yelp. Yelp open dataset, 2015. URL https://business.yelp.com/data/resources/ open-dataset/. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it? In ICLR, 2023. Chenyu Zhang, Mingwang Hu, Wenhui Li, and Lanjun Wang. Adversarial attacks and defenses on text-to-image diffusion models: survey. Information Fusion, 114:102701, 2025. Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International Conference on Machine Learning (ICML), 2019. Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper_files/paper/2015/file/ 250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf. Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced adversarial training for natural language understanding. In International Conference on Learning Representations (ICLR), 2020. Haomin Zhuang, Yihua Zhang, and Sijia Liu. pilot study of query-free adversarial attack against stable diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23852392, 2023. Andy Zou, Zifan Wang, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023."
        },
        {
            "title": "A Broader impact",
            "content": "In this work, we positively impact society by making models employing CLIP text encoders more robust to small perturbations in the text, which can be specially important in safety-critical applications. We believe our work can help practitioners using CLIP make their models robust, by employing our text encoders."
        },
        {
            "title": "B Additional details",
            "content": "In this section, we provide additional details on the implementation of our method and the experimental setting. Additional Notation: Given two matrices Rmd and Rnd, we define = (cid:21) (cid:20)A R(m+n)d. Concatenating with the empty sequence results in the identity = A. We denote as A2: R(m1)d the matrix obtained by removing the first row. B.1 Method details Firstly, we characterize the single-character perturbations following Abad Rocamora et al. [2024]. Definition B.1 (Expansion and contraction operators). Let S(Γ) be the space of sentences with alphabet Γ and the special character ξ / Γ, the pair of expansion-contraction functions ϕ : S(Γ) S(Γ {ξ}) and ψ : S(Γ {ξ}) S(Γ) is defined as: (cid:26) ϕ(S) := ξ if = 0 ξ, S1 ϕ(S2:) otherwise . ψ(S) := (cid:40) ψ(S2:) if = 0 if S1 = ξ S1 ψ(S2:) otherwise . Clearly, ϕ(S) aims to insert ξ into in all possible positions between characters and at the beginning and end of the sentence, and thus we have ϕ(S) = 2 + 1. Similarly, ψ(S) aims to remove all ξ occurred in S. The (ϕ, ψ) pair satisfies the property that ψ(ϕ(S)) = S. We give the following example for better understanding. Example B.2. Let ξ := for visibility: ϕ(Hello) = Hello ψ(Heello) = Heello ψ(Helo) = Helo ψ(Hello) = Hello . Definition B.3 (Replacement operator). Let S(Γ {ξ}), the integer [S] and the character c, the replacement operator of the ith position of with is defined as: := S:i1 Si+1: Thanks to Definition B.3, we are ready to present our attack in Algorithm 1. The advantage of Algorithm 1 resides in attacking batch of sentences in parallel, an important feature for efficient adversarial training. B.2 Semantic constraints details In order to follow the semantic constraints of [Chanakya et al., 2024], we constrain the attacks during training and during retrieval and text-to-image generation to not produce new English words. To do so, we employ Algorithm 2 over pairs of sentences and so that dLev(S, S) = 1. Algorithm 2 returns that the perturbation is valid only if it contains less english words than S. B.3 Training details All of our text encoders are trained on the first 80, 000 samples of the DataComp-small dataset [Gadre et al., 2023] for 30 epochs with batch size of 128 sentences. We employ the AdamW optimizer [Kingma and Ba, 2015, Loshchilov and Hutter, 2019], weight decay of 104, maximum learning rate of 105 with linear warmup of 1, 400 steps and cosine decay. For training the robust vision encoder, we adapt the setup of Schlarmann et al. [2024]. Namely, we train on images from 16 Algorithm 1 LEAF batched attack 1: Inputs: Text encoder fθ : S(Γ) Rh, batch {Si}B i=1, loss function L, radius k, number of simultaneous perturbations ρ, alphabet Γ, test character and flag for semantic constraints Cons. Initialize perturbations with clean sentences. 2: ˆSi = Si [B] 3: for 1, , do pij Unif. 4: (cid:26)(cid:110) (cid:16) (cid:16) = ψ (cid:17) [2 ˆSi + 1] (cid:17)(cid:111)ρ ϕ( ˆSi) pij [B] [ρ] (cid:27)B Sample ρ positions in every sentence. Replace the test character in all pij. j=1 i= if Cons then Sij = (cid:40) Sij ˆSi = arg maxj[ρ] (cid:0)fθ cij Unif. (Γ) [B] [ρ] Use Algorithm 2 to check if the perturbation is valid, revert otherwise. if valid( ˆSi, Sij) otherwise (cid:0) Sij [B] [ρ] (cid:1)(cid:1) Eval. losses in parallel and get the max. Sample ρ characters for every sentence. (cid:40)(cid:26) = (cid:18) ϕ( ˆSi) ψ pij cij (cid:19)(cid:27)ρ j=1 (cid:41)B i=1 Replace cij in the position pij . if Cons then Sij = (cid:40) Sij ˆSi = arg maxj[ρ] (cid:0)fθ ˆSi = Sil (cid:111)B (cid:110) [B] Use Algorithm 2 to check if the perturbation is valid, revert otherwise. if valid( ˆSi, Sij) otherwise (cid:0) Sij [B] [ρ] (cid:1)(cid:1) Eval. losses in parallel and get the max. Update perturbations. 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: return ˆSi i=1 Algorithm 2 Semantic constraints 1: Inputs: Sentence and perturbation S. 2: = words(S) 3: = words(S) 4: return > We extract English words using NLTK: https://www.nltk.org/ ImageNet for 10k steps (instead of 20k, due to compute constraints) with batch size of 128 for ViT-H/14 and 64 for ViT-g/14. We use weight decay of 104, maximum learning rate of 105 with linear warmup of 700 steps and cosine decay. To optimize the inner adversarial objective, we use PGD with 10 steps and set ϵ = 2/255. Our codebase is based on OpenCLIP [Ilharco et al., 2021]. All of our experiments are conducted in single Nvidia A100 40GB GPU, except for training robust image encoders, where 8 GPUs were employed. B.4 Zero-shot text classification Analogously to how zero-shot image classification is performed in the original CLIP paper [Radford et al., 2021], Qin et al. [2023] encode one image representing each class and compute the similarities with the sentence embedding. Then the predicted class is the one with the highest cosine similarity in the embedding space. In Table 5 we present the images employed for each dataset and label. B.5 Text inversion In order to invert text embeddings, we sample 100 random captions from COCO val2017 and use the optimization method proposed by Wen et al. [2023] with 3000 iterations, learning rate 0.1, and weight decay 0.1. 17 Table 5: Images and sentences used for zero-shot text classification."
        },
        {
            "title": "Dataset",
            "content": "Class 1 Class"
        },
        {
            "title": "Images",
            "content": "Class 3 Class 4 SST-2 / IMDB / Yelp NA NA AG-News SST-2 / IMDB / Yelp Negative Review Positive Review AG-News World News Sports News NA Business News NA Science Technology News and"
        },
        {
            "title": "Sentences",
            "content": "Table 6: Source models employed for finetuning and evaluation. Model CLIP-ViT-B-32 CLIP-ViT-B-16 ViT-L/14 FARE SafeCLIP OpenCLIP-ViT-H-14 OpenCLIP-ViT-g-14 OpenCLIP-ViT-bigG-14 Source https://huggingface.co/openai/clip-vit-base-patch32 https://huggingface.co/openai/clip-vit-base-patch16 https://huggingface.co/openai/clip-vit-large-patch14 https://huggingface.co/chs20/fare2-clip https://huggingface.co/aimagelab/safeclip_vit-l_14 https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K https://huggingface.co/laion/CLIP-ViT-g-14-laion2B-s12B-b42K https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k Stable Diffusion v1.5 (SD-1.5) Stable Diffusion XL base v1.0 (SDXL) https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5 https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0 B.6 Model checkpoints In Table 6, we enumerate the external models employed in this work and the sources used for comparison and finetuning."
        },
        {
            "title": "C Related work",
            "content": "In this section we cover related work on Adversarial Attacks, Adversarial Training, Robustness of Multimodal Models and text inversion. Adversarial Attacks The vulnerability of deep learning models against adversarial input attacks is well known [Szegedy et al., 2014, Goodfellow et al., 2015] and hast been extensively studied in the vision input domain [Croce and Hein, 2020, Schlarmann and Hein, 2023] and the text input domain, with the most popular attacks employing perturbations in the token-level [Ren et al., 2019, Jin et al., 2020, Li et al., 2019, Garg and Ramakrishnan, 2020, Lee et al., 2022, Ebrahimi et al., 2018, Li et al., 2020, Guo et al., 2021, Hou et al., 2023] and character-level [Belinkov and Bisk, 2018, Ebrahimi et al., 2018, Gao et al., 2018, Pruthi et al., 2019, Yang et al., 2020, Liu et al., 2022, Abad Rocamora et al., 2024]. Adversarial Training in the text domain. Adversarial Training [Madry et al., 2018] and its variants [Zhang et al., 2019, Rebuffi et al., 2021, Gowal et al., 2021, Wang et al., 2023, Bartoldson et al., 2024] are the most prominent defense against adversarial examples in the image domain Croce and Hein [2020], Croce et al. [2020]. 18 In the text domain, also variants of adversarial training constitute the best defenses, with most defenses focusing on token-level attacks. Taking advantage of the efficiency of PGD, Miyato et al. [2017] propose solving the inner maximization problem in ℓp constrained ball around every token embedding. Zhu et al. [2020] accelerate embedding-level PGD AT and show improvements in clean accuracy. Wang et al. [2021] show improvements in adversarial accuracy by adding an information theoretic regularization term. Deviating from the embedding-based PGD AT paradigm, Dong et al. [2021] use PGD to maximize the loss over convex combination of synonym embeddings. Then, Hou et al. [2023] find that directly optimizing the inner max in the text space with existing attacks [Jin et al., 2020] significantly boosts the adversarial accuracy against multiple adversarial attacks. In the character-level, it was initially thought that typo-correctors would suffice as defense [Pruthi et al., 2019, Jones et al., 2020]. Abad Rocamora et al. [2024] shows that typo-corrector defenses can be easily broken. Additionally Abad Rocamora et al. [2024] show that similarly to the results of [Hou et al., 2023] in the token-level, performing adversarial training with character-level perturbations improved the character-level robustness. Robustness of Multimodal Models. Attacking and defending multimodal models has gained significant interest recently. Mao et al. [2023] propose TeCoA, which performs supervised adversarial fine-tuning on CLIP in order to defend against visual adversarial attacks. In turn, Schlarmann et al. [2024] propose FARE, an unsupervised robust fine-tuning method for vision encoders that preserves downstream performance, e.g. of LMMs that utilize vision encoder. Text inversions. Morris et al. [2023, 2024] learn models that can invert text embeddings or language model outputs. In contrast, Wen et al. [2023] invert CLIP image embeddings into text via direct optimization. They use the reconstructed text to prompt diffusion models and thereby generate similar images. We use their optimization scheme to invert text embeddings and show that it yields better results when used with our robust models."
        },
        {
            "title": "D Additional experiments",
            "content": "In this section we cover additional experiments not fitting in the main manuscript. In Appendix D.1, we analyze the effect adding additional constrains to the adversarial attack. In Appendix D.5, we include our ablation studies. D.1 On the effect of additional attack constrains for Text-to-image models In this section, we evaluate the effectiveness of the semantic constraints considered by Chanakya et al. [2024]. In order to avoid including new words with different information in the prompt, Chanakya et al. [2024] constrain the attack to not produce new words in the English vocabulary. To do so, they tokenize the clean and adversarial prompts and check for the appearance of new words in the adversarial prompt based on the NLTK English dictionary [Bird and Loper, 2004]. In order to check for the need of these constraints, we attack SD-1.5 equipped with our robust text encoder at = 2 using Charmer [Abad Rocamora et al., 2024] on the COCO val2017 dataset [Lin et al., 2014]. We then visually explore the adversarial prompts and generated images to look for inconsistencies. In Table 7 we can observe five examples of unconstrained attacks producing adversarial prompts with significantly different meaning. Since the only constraint is that the Levenshtein distance needs to be 2, the attack is able to turn bear into beer, stop into shop, bananas into bandanas or wave into pave. This results in the diffusion model generating images that correctly adopt these adversarial captions and the adversarial prompts being invalid. If we constrain the attacker to not generate new words, the adversarial prompts preserve the meaning of the original captions up to uncommon words/abvreviations not present in the NLTK dictionary, like grads or smurfs. Overall, we consider the constraints necessary for the text-to-image generation tasks, agreeing with Chanakya et al. [2024]. D.2 Zero-shot classification In this section we include additional datasets for zero-shot image and text classification. We also include hyperparameter analysis with = 2. 19 Table 7: Examples of problematic attacks in COCO val2017: If no additional constraints are considered, single character change can produce semantical changes in the prompt, e.g., bear is transformed into beer. This leads to image generations that are highly dissimilar to the original reference image, but are correct according to the adversarial prompt. The semantic constraints employed by Chanakya et al. [2024] help reducing the amount of new words. Nevertheless, some abbreviations like grads or uncommon words like smurf still appear after the attack. ID Original caption Original image Unconstrained Constrained [Chanakya et al., 2024] Adversarial caption Generated image Adversarial caption Generated image 285 724 776 3661 big burly grizzly bear is show with grass in the background. stop sign is mounted upside-down on its post. teddy Three bears, each different color, snuggling together. bunch of bananas sitting on top of wooden table. 6460 person riding surf board on wave big burly grizzly beer is show with brass in the background. shop sign is mounted up!ide-down on its post. teddy Tree beans, each different color, snuggling together. bunch of bandanas sitting on top of aawooden table. person riding smurf board on pave big burly !rizzly bear is show with grads in the background. scop sign is mountedaupsidedown on its post. teddy 8hree bears, each different color, snuggling toge,ther. bunch of bananas sitti-g on top of woodenitable. person riding smurf board on waze Figure 7: Hyperparameter effects: We report the zero-shot clean and adversarial accuracy in both domains (ImageNet and AG-News) with FARE [Schlarmann et al., 2024] as baseline. For the unconstrained attack, larger values of ρ improve the robustness in the text domain at the cost of significantly degrading the clean and adversarial performance in the image domain. Constraining the attack allows improving the robustness in the text domain with minimal effects on the image domain performance. 20 Table 8: Zero-shot performance for different k, ρ and constraints."
        },
        {
            "title": "ImageNet",
            "content": "AG-News 255 ) Acc. Charmer Acc. (k = 1) PGD-20 Acc. (ϵ = 2 46.7 46.5 45.4 43.7 43.5 42."
        },
        {
            "title": "Semantic\nConstraints",
            "content": "1 2 2 ρ 1 2 5 10 20 50 1 2 5 10 20 50 1 2 5 10 20 50 1 2 5 10 20 Acc. 74.7 74.5 72.0 70.1 67.5 65.5 73.5 73.3 67.4 60.4 55.3 53.3 74.7 74.8 74.8 74.8 73.6 72.6 74.7 75.5 75.2 74.1 73.0 70.5 78.7 78.3 78.7 78.6 78.0 78. 79.1 78.4 79.1 78.8 78.0 78.0 78.2 77.5 78.3 78.3 78.4 78.0 77.4 78.1 78.9 78.6 77.8 78.4 57.6 60.7 62.9 64.8 65.2 66.3 60.2 63.3 65.4 67.0 66.7 67.8 54.4 56.9 58.6 59.9 60.7 63. 55.8 58.6 59.9 61.5 62.8 63.5 47.4 46.5 42.4 36.3 32.3 30.5 46.9 47.2 47.7 46.3 46.3 46.0 47.1 47.3 47.0 47.5 46.7 45.3 In Fig. 7 we can observe the same experiment as in Section 4.2.2 and Fig. 3 with = 2 instead of = 1. Similarly to the experiments with = 1, increasing ρ leads to degraded performance in the image domain when no constraints are employed. Including the constraints, allows for increasing the robustness in the text domain with less performance degradation. The numbers form Figs. 3 and 7 are available in Table 8. D.2.1 Additional experiments on zero-shot image classification For zero-shot image classification, we measure the clean and robust accuracy on 13 datasets: CalTech101 Griffin et al. [2007], StanfordCars Krause et al. [2013], CIFAR10, CIFAR100 Krizhevsky [2009], DTD Cimpoi et al. [2014], EuroSAT Helber et al. [2019], FGVC Aircrafts Maji et al. [2013], Flowers Nilsback and Zisserman [2008], ImageNet-R Hendrycks et al. [2021], ImageNetSketch Wang et al. [2019], PCAM Veeling et al. [2018], OxfordPets Parkhi et al. [2012], and STL10 Coates et al. [2011]. To measure robustness, we conduct visual attacks as described in Section 4.1, and restrict the evaluation to 1000 random samples on all datasets. We evaluate orginal models and models that employ robust encoders in both domains. Results are reported in Table 9. The robust models maintain much better performance under adversarial attacks, while sacrificing some clean performance. D.2.2 Additional experiments on zero-shot text classification In this section, we evaluate the zero-shot clean and adversarial accuracy of our models in additional text classification datasets. We follow the same attack setup as in the AG-News experiments, i.e., we employ Charmer-20 at = 1 without semantic constraints to evaluate the performance on SST-2 [Socher et al., 2013], IMDB [Maas et al., 2011] and Yelp [Yelp, 2015, Zhang et al., 2015]. 21 Table 9: Zero-shot image classification. We report the zero-shot image classification peformance of original and bimodally robust models. Model CLIP-ViT-L/14 l OpenCLIP-ViT-H/14 OpenCLIP-ViT-g/14 5 CLIP-ViT-L/14 5 2 / 2 = ϵ OpenCLIP-ViT-H/14 OpenCLIP-ViT-g/14 1 0 1 T Robust 0 1 i 0 0 1 i D o C s o s r - e I - e I P 0 1 n s 82.1 77.5 95.2 68.2 55.7 63.4 28.4 79.4 86.5 48.9 53.0 93.9 98.8 71.6 81.1 71.6 92.2 68.9 44.9 28.7 24.6 69.7 83.3 47.0 59.9 91.9 98.1 66.3 84.4 92.2 97.5 82.8 68.7 72.5 42.4 80.2 88.4 56.1 54.9 95.1 98.1 77.9 83.8 89.8 93.3 69.7 61.1 34.4 35.8 73.4 85.7 52.9 50.4 94.0 97.2 70.9 84.3 92.1 97.7 84.0 68.8 65.6 36.4 78.1 88.2 55.5 55.6 95.2 98.2 76.9 83.1 88.4 91.7 67.3 58.1 29.0 30.7 71.2 84.9 52.0 52.5 92.5 96.2 69. 0.0 0.0 0.0 70.5 27.8 65.6 34.2 25.3 11.6 6.0 33.8 55.5 26.4 22.1 69.0 89.7 41.3 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 70.7 55.6 65.0 38.4 32.5 7.7 0.2 0.0 0. 0.0 0.0 0.0 71.3 52.1 62.6 34.0 28.5 4.7 0.2 0.1 0.0 0. 0.0 0.0 5.8 39.5 58.3 31.0 37.9 66.0 87.9 45.9 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 4.0 34.2 53.3 28.6 26.5 57.5 84.7 41.7 0.0 0.0 0.0 0. 0.0 Figure 8: Larger perturbations: We evaluate the adversarial accuracy in AG-News for {1, 2, 3, 4, 5} in the ViT-L/14 scale. Our model (LEAF) obtains the highest adversarial accuracy at all values of the distance bound k. In Fig. 8 we report the zero-shot adversarial accuracy already reported in Fig. 4, with the addition of SafeCLIP [Poppi et al., 2024]. SafeCLIP obtains considerably lower clean and adversarial accuracy in comparison to the other CLIP variants. In Table 10 we can observe that similarly to the AG-News results in Table 2, the models with robust text encoders achieve higher adversarial accuracy in the text domain, with improvements of more than 9.9 robust accuracy points for all models and datasets. In Table 11, we present the clean and adversarial zero-shot accuracy when employing only the text encoder for the ViT-L/14 models. For that, we encode on sentence per label instead of one image per label as done in the main text. See Table 5 for more details on the sentences employed for the labels. We can observe that the adversarial accuracy is larger after adversarial finetuning with LEAF. Nevertheless, the clean and adversarial performance are worse when doing text-encoder-only zeroshot classification, e.g., clean accuracy in AG-News with ViT-L/14 of 74.4 when using images as labels  (Table 2)  v.s. 54.8 when using sentences as labels. D.3 Additional experiments in text-to-image models In this section, we provide additional experiments and examples for the text-to-image generation task. In Tables 12 and 13 we present the generation results in SD-1.5 and SDXL in the MS-COCO dataset and the first 5.000 images of the Flickr30k dataset. We measure the CLIPScore between the original caption and the generated image (T-I), the CLIPScore between the original image and the generated one (I-I), the attack objective (Eq. (2)) and for SD-1.5, the percentage of generated images triggering the NSFW filter (NSFW %). We can observe that the text encoders finetuned with LEAF, 22 Table 10: Zero-shot text classification. We report the zero-shot image classification peformance of original and bimodally robust models. Model Robust SST-2 IMDB Yelp l 1 = CLIP-ViT-L/14 OpenCLIP-ViT-H/14 OpenCLIP-ViT-g/14 CLIP-ViT-L/14 OpenCLIP-ViT-H/ OpenCLIP-ViT-g/14 71.2 71.9 61.6 58.4 57.8 56.0 6.8 23.2 16.2 36. 21.4 34.2 61.6 61.4 57.5 53.2 56.8 54.0 13.7 31.0 31.1 43. 31.4 41.3 80.9 82.0 73.7 72.6 71.9 71.1 21.0 43.8 22.1 40. 26.0 39.4 Table 11: Text-encoder-only zero-shot text classification: We report the clean and adversarial zero shot accuracy at = 1 employing only text-encoders. The adversarial accuracy improves after adversarial finetuning with LEAF. Nevertheless, employing only the text encoder provides worse clean and adversarial performance than employing images as labels as Qin et al. [2023]. AG-News SST-2 IMDB Yelp Robust Acc. Adv. Acc. Adv. Acc. Adv. Acc. Adv. 54.8 53.5 17.9 34.7 60.3 58.9 3.2 24. 54.0 51.5 24.9 44.9 59.9 56.7 29.5 47.5 provide higher generation quality for > 1 according to all generation metrics. Surprisingly, for = 2 and = 4 in the MS-COCO dataset, our text encoders triggered the NSFW filter less frequently than SafeCLIP [Poppi et al., 2024], which is specifically designed to avoid generating NSFW content. In Tables 14 to 17 we present examples of the attacks on the first 10 samples of each dataset for both SD-1.5 and SDXL at = 2. We can observe, that our text encoders provide qualitatively better images. The models with the original text encoders, provide images unrelated to the original image and caption more often than the models employing our text encoders. D.4 Additional retrieval experiments For 1, 000 validation set queries, the attack explained in the main part maximizes the similarity between the test query and target string using different variants of the Charmer attack. In Table 20, we show the individual attack results across 3 target strings for differently trained LEAF models. One sees that on increasing training ρ, the robustness goes up with slight decay in the clean retrieval performance. This trade-off is similar to the one seen for classification tasks in Fig. 3. In Fig. 9, we visualize the top-3 retrieved images for the original and the perturbed queries. Although in some cases the non robust model retrieves relevant query, the top-1 retrieved image is always different for clean and perturbed queries. However, the robust model always preserves the original top-1 retrieved image showing its robustness to such character perturbed queries. D.5 Ablation studies In Appendix D.5.1 we evaluate the performance drop in zero-shot image classification when training without semantic constraints. In Appendix D.5.2 we measure the Eq. (TextFARE) loss before and after training. Table 12: Text-to-image generation results on MS-COCO: Pipeline Text encoder Sim(fθ(S), fθ(S)) CLIPScore T2I CLIPScore I2I NSFW (%) SD-1.5 1 2 CLIP SafeCLIP LEAF CLIP SafeCLIP LEAF CLIP SafeCLIP LEAF 3 CLIP SafeCLIP LEAF 4 CLIP SafeCLIP LEAF 0 CLIP + OpenCLIP 2LEAF 1 CLIP + OpenCLIP 2LEAF SDXL 2 CLIP + OpenCLIP 2LEAF 3 CLIP + OpenCLIP 2LEAF 4 CLIP + OpenCLIP 2LEAF - 55.85(8.66) 71.62(8.32) 86.58(4.84) 33.18(9.29) 50.87(10.34) 73.15(7.45) 20.38(8.93) 35.93(11.06) 60.00(9.07) 12.83(8.80) 26.05(11.04) 49.35(9.55) - 67.65(7.46) 88.15(4.44) 47.58(8.74) 76.49(7.12) 34.22(8.90) 64.62(9.24) 25.93(8.74) 54.08(10.22) 31.50(2.87) 30.96(2.93) 31.00(2.94) 27.53(4.52) 27.43(4.09) 27.96(3.48) 22.96(5.79) 23.75(5.02) 25.23(4.36) 19.45(5.86) 20.41(5.61) 22.59(5.16) 17.42(5.68) 17.94(5.57) 20.25(5.44) 31.90(2.84) 31.80(2.86) 28.33(4.11) 29.37(3.46) 24.65(5.25) 27.14(4.33) 21.45(5.70) 24.69(5.16) 19.07(5.60) 22.45(5.67) 73.31(10.21) 73.27(10.08) 73.06(10.12) 65.38(12.71) 66.90(11.56) 68.01(11.17) 57.21(13.90) 61.02(12.06) 63.40(11.95) 51.55(13.40) 55.98(12.07) 59.02(12.19) 48.34(12.66) 52.31(11.57) 55.36(12.33) 71.87(10.58) 71.78(10.60) 64.45(12.25) 67.25(11.54) 57.97(12.89) 63.27(12.19) 53.37(12.78) 59.38(12.66) 49.92(12.21) 55.70(12.85) 0.64 0.44 0.46 0.96 0.48 0.50 2.16 1.08 0.62 2.52 1.10 1. 2.70 1.56 1.44 - Table 13: Text-to-image generation results on Flickr30k: Pipeline Text encoder Sim(fθ(S), fθ(S)) CLIPScore T2I CLIPScore I2I NSFW (%) SD-1.5 0 1 2 0 CLIP SafeCLIP LEAF CLIP SafeCLIP LEAF CLIP SafeCLIP LEAF CLIP + OpenCLIP 2LEAF SDXL 1 CLIP + OpenCLIP 2LEAF 2 CLIP + OpenCLIP 2LEAF - 63.48(9.01) 77.31(7.11) 89.80(3.89) 42.37(10.21) 59.79(9.63) 79.28(6.55) - 75.15(6.33) 91.32(3.40) 58.02(8.49) 82.82(5.84) 24 33.27(3.21) 32.16(3.35) 32.63(3.17) 30.72(4.16) 29.32(4.19) 30.37(3.56) 27.71(5.18) 26.24(4.72) 28.43(4.05) 33.85(3.24) 33.82(3.22) 31.24(4.00) 31.63(3.54) 28.30(4.81) 29.83(4.09) 71.27(10.20) 70.20(10.25) 70.73(10.23) 66.43(11.25) 65.68(10.85) 67.54(10.56) 61.28(12.18) 61.66(11.12) 64.66(10.80) 69.07(10.54) 69.06(10.50) 64.03(11.23) 65.87(10.89) 59.09(11.47) 63.03(11.15) 0.42 0.42 0.26 0.84 0.92 0.66 1.28 0.87 0. - Figure 9: Visualizing MS-COCO retrieved images. For our ViT-L/14 robust model and its nonrobust counterpart, we show the top-3 retrieved images for the original Query and the perturbed Query via the constrained Charmer (k = 2, = 10) attack. On average, the robust model is able to preserve the order and retrieves semantically relevant images (esp. top-1) even under perturbation. 25 Table 14: Attack examples on MS-COCO with SD-1.5 at = 2: The color borders indicate null, partial and total matching to the original image caption. The model with the original text encoder provides images involving footballer, lizard or gun, when prompted about bear, women skiing or group of people respectively. With our text encoders, the generation does not drift in topic so much. ID Original caption Original image 139 285 632 724 785 802 872 885 1000 woman stands in the dining area at the table. big burly bear grizzly is show with grass in the background. Bedroom scene with bookcase, blue comforter and window. stop sign is mounted upside-down on its post. Three teddy bears, each different color, snuggling together. posing the standing skis. kitchen with refrigerator, stove and oven with cabinets. woman for camera on couple baseball of standplayer ing on field. male tennis player in white shorts is playing tennis The people are posing for group photo. SafeCLIP Adversarial caption Generated image Adversarial caption Generated image Adversarial caption Generated image Original LEAF woman stan3s in the dining area at the tableA big burly bear griezly is show with g?rass in the background. Bedr=oom scene with bookcase, blue comfor#ter and window. stop si$gn is mounted upsixde-down on its post. woma6n for camera on Thre teddy sears, each different color, snuggling together. posing the standing >kis. with frigerator, stove and oven withmcabinets. couple basmball of standplayer ing on fi#eld. kit>chen rea male ten=is player in white shor?ts is playing tennis The pzople are posing for group ph6oto. woma2 stands in the cining area at the table. big burly gr#izzly show bearvis with grass in the background. Bedroom scene with @ookcase, bl#ue forter window. stox sign is mounted upside-down on its pos$. comand Three teddy bears, eac= different color, snuggling together. woma6 posing for the camera standing onuskis. kiltchen A with refr#igerator, stove and oven with cabinets. cozuple basebalm of standplayer ing on field. tena male nis player in wh.ite )horts is playing tennis The people are posi?ng for gr1oup photo. Avwomanastands in the dining area the at table. big burly bear .rizzly is show with @rass in the background. Bedroomascene with kookcase, blue comforter and window. stopssign is mounted upsidedownton its post. 9hree teddy bears, each different color, snuggling toge,ther. -oman posing for the camera standing onoskis. Aqkitchen withra refrigerator, stove and oven with cabinets. coupl. baseball of standplayer ing on ˆield. tennis aimale playerein white shorts is playing tennis The people are forza posing group bhoto. D.5.1 On the performance drop without semantic constraints First, we perform an ablation study to better understand the cause of the performance drop in terms of clean accuracy in Table 8. We select 10 classes from the ImageNet dataset and visualize the corresponding text embeddings using the prompt photo of LABEL. In Fig. 10, we observe that as ρ and increase, the class projections in 2D space become more clustered. We compute the mean pairwise distance, defined as the average L2 distance between all class pairs, and find that it decreases significantly. D.5.2 On the Eq. (TextFARE) loss In this section, we evaluate the effectiveness of our method LEAF in minimizing the loss in Eq. (TextFARE). First, we measure the loss before and after adversarial finetuning in the ViT-L/14 scale on the first 100 images in the AG-News dataset at = 1. We evaluate the inner max of Eq. (TextFARE) with the LEAF attack with and without semantic constraints (Appendix D.1) and with ρ {1, 2, 5, 10, 20, 50}. As baselines, we evaluate the same term with the Charmer-20 attack and Bruteforce approach, which evaluates all of the possible sentences at Levenshtein distance = 1. In Fig. 11 we can observe that training with LEAF, we generalize to be robust to stronger attacks, even if they do not employ semantic constraints. For all cases, employing larger ρ reduces the gap between the LEAF estimate and the true inner max of Eq. (TextFARE), i.e., Bruteforce. Af26 Table 15: Attack examples on MS-COCO with SDXL at = 2: ID Original caption Original image Original LEAF Adversarial caption Generated image Adversarial caption Generated image 139 632 724 776 785 802 885 1000 woman stands in the dining area at the table. big burly grizzly bear is show with grass in the background. Bedroom scene with bookcase, blue comforter and window. stop sign is mounted upside-down on its post. Three teddy bears, each different color, snuggling together. posing the standing skis. kitchen with refrigerator, stove and oven with cabinets. woman for camera on couple baseball of player standing on field. male tennis player in white shorts is playing tennis The people are posing for group photo. woma8 stands in the jining area at the table. big burly gr1izzly bear is show with @rass in the background. Bedroom sc]ene with zookcase, blue comforter and window. stop gign is mountedpupsidedown on its post. Thr:ee teddy bears, each different color, snuggling together. woma: posing for the camera standing ontskis. ki:chen with refr@igerator, stove and oven with cabinets. couple of basebill player standing on #ield. male tennis pl*ayer in white #horts is playing tennis The neople are posing for group hoto. 3 womanstands in the dining the at area table. big burly !rizzly bear is show with krass in the background. Bedroom scene with cookcase, blue cosmforter and window. 3top sign is mounted upsidedownton its post. ahree teddy bears, each different color, snuggling toge,ther. -oman posing for the camera standing onoskis. Aqkitchen withra refrigerator, stove and oven with cabinets. coupll baseball of player standing on qield. aemale tennis playerein white shorts is playing tennis The peoplecare posing group forza photo. ter adversarial finetuning with LEAF, both the loss estimates with Charmer-20 and Bruteforce are reduced. Then, we evaluate the inner max of Eq. (TextFARE) in the ViT-L/14, ViT-H/14 and ViT-g/14 scales with Charmer-20 before and after adversarial finetuning with LEAF. Similarly, the Charmer-20 loss is minimized even if no semantic constraints are used in the estimate, for all model sizes. The loss is larger for larger model sizes both before and adversarial finetuning. This could be due to the larger embedding dimension for the ViT-H/14 and ViT-g/14 models. Finally, we also evaluate the inner max of Eq. (FARE) in the image domain. To this end, we compute adversarial perturbations for 100 ImageNet images with 100-steps APGD attack on the Eq. (FARE) objective at radius ϵ = 2/255. The results are reported in Table 21: similar to the textual attacks, we observe that the loss increases with model size. Importantly, the robust models generally demonstrate much smaller adversarial loss than their original counterparts. These results validate the intuition from Fig. 1 (left): the robust models map perturbed inputs much closer to the original inputs than the original models. 27 Table 16: Attack examples on Flickr30k with SD-1.5 at = 2: ID Original caption Original image 5 9 7 2 9 0 0 0 0 1 6 5 4 2 0 0 0 1 0 2 8 6 2 0 0 0 1 5 5 7 4 4 3 0 0 0 1 4 6 1 6 6 3 0 0 0 1 9 3 6 3 2 5 0 0 0 1 0 3 6 9 1 9 0 0 0 1 2 5 0 0 1 0 0 4 4 9 5 6 4 1 0 0 1 5 2 5 5 4 5 1 0 0 1 young Two with guys hair shaggy look at their hands while hanging out in the yard . Several men in hard hats are operating giant pulley system . child in pink dress is climbing up set of stairs in an entry way . Someone in blue shirt and hat is standing on stair and leaning against window . Two men , one in gray shirt , one in black shirt , standing near stove . Two people in the photo are playing the guitar and the other is poking at him . man sits in chair while holding large stuffed animal of lion . is on girl rollerskates talking on her cellphone standing in parking lot . An asian man wearing black suit stands near dark-haired woman and brown-haired woman . men Two in Germany jumping over the rail at time same without shirts . SafeCLIP Adversarial caption Generated image Adversarial caption Generated image Adversarial caption Generated image Original LEAF young Twt with guys hair shaggy look at their hands while hanging out in the mard . Severalumen in harz hats are operating giant pulley system . 8hild in pink drss is climbing up set of stairs in an entry way . in Someone shirt blue is andphat standing on &tair and leaning against window . cwo men , one in gray shirt , one in black shirt , standing near .tove . Two people in the photocare playing the suitar and the other is poking at him . man sits in chair while holding large stunffed animal of liox . is on Adgirl rollerskates talking on her cellphone standing in parkingelot . Ankasian man wearing black suit stands near dark-haired woman !nd brown-haired woman . men cwo in Germany jumping over the rail at same time !ithout shirts . young Two with guys hair shagg) look at their hands while hanging out in the #ard . Severa= men in hard hats are operat{ng giant pulley system . ch:ld in pink dr ss is climbing up set of stairs in an entry way . Someone in bl2ue shirt and hat is standing on stair and leaning against windo& . +wo men , one in gray shirt , one in black shirt , standing near @tove . Two people in the photo are playing the guita] and the is other poki.ng at him . man sits in chair whil$ holding stu!ffed large animal of lion . is on gorl rollerskates talking on her cellphone standing in parki{ng lot . An asian man wearing bl#ack sui@ stands near dark-haired woman and brown-haired woman . men Twy in Germany jumping over the aarail time same without shirts . at young with Two guys shaggychair zook at their hands while hanging out in the yard . Several menxin hardghats are operating giant pulley system . ch&ld in aapink dress is climbing up set of stairs in an entry way . Someone in bl4ue shirt and hat is standing on @tair and leaning against window . 7wo men , one in gray shirt , one in black shirt , standing near 1tove . 7Two pefople in the photo are playing the guitar and the other is poking at him . manpsits in ch2ir while holding large stuffed animal of lion . gir% is on rollerskates talking on herwcellphone standing in parking lot . An asian man wearing blauck sui! stands near dark-haired woman and brown-haired woman . men Two in Germany jumping over the raij at same time withouk shirts . 28 Table 17: Attack examples on Flickr30k with SDXL at = 2: ID Original caption Original image Original LEAF Adversarial caption Generated image Adversarial caption Generated image 5 9 7 2 9 0 0 0 0 1 6 5 4 2 0 0 0 1 0 2 8 6 2 0 0 0 1 5 5 7 4 4 3 0 0 0 1 4 6 1 6 6 3 0 0 0 1 9 3 6 3 2 5 0 0 0 1 0 3 6 9 1 9 0 0 0 1 2 5 0 0 1 0 0 4 4 9 5 6 4 1 0 0 1 5 2 5 5 4 5 1 0 0 1 young Two with guys hair shaggy look at their hands while hanging out in the yard . Several men in hard hats are operating giant pulley system . child in pink dress is climbing up set of stairs in an entry way . Someone in blue shirt and hat is standing and on stair leaning against window . Two men , one in gray shirt , one in black shirt , standing near stove . Two people in the photo are playing the guitar and the other is poking at him . man sits in chair while holding large stuffed animal of lion . is on girl rollerskates talking on her cellphone standing in parking lot . An asian man wearing black suit stands near dark-haired woman and brown-haired woman . Two men Germany in jumping over the rail at same time without shirts . young with Two guys shaggychair look at their hands while hanging out in the ard . Several men in $ard hats are operating giant !ulley system . chld in pink dr ss is climbing up set of stairs in an entry way . Someone in bl2ue shirt and hat is standing and on stair leaning against :indow . Twomen , one in gray shirt , one in black shirt , standing near @tove . Two people in the ph?oto are playing the gu#itar and the other is poking at him . man sits in ch5ir while holding stu!ffed large animal of lion . is on girl rollerskates talking on her cellphone standing in parki{ngblot . Axn asian man wearing black suit stands near dark-haired woman #nd brown-haired woman . Two men Germany in jumping over the rai7 at same time ?ithout shirts . young with Two guys shaggychair look at their hands while hanging out in the mard . Several men in hardchats are operating giant sulley system . chwild in pink dress is climbing up set of stairs in ankentry way . in Someone shirt blue is andwhat standing on &tair and leaning against window . Twomen , one in gray shirt , one in black shirt , standing near ptove . Two people in the photo are playing the suitarmand the other is poking at him . man sits in chair while holding large stuffe. animal of aklion . in girl is on rollerskatesstalking cellon her standphone ing parkingslot . Axn asian man wearing black suit stands near dark-haired woman anz brown-haired woman . cwo men Germany in jumping over the rail at same time ?ithout shirts . 29 Table 18: Text embedding inversion examples for ViT-H/14. We highlight in red words that are reconstructed by the robust model but not by the clean model; in teal words that are reconstructed by the clean model but not by the robust model; and in yellow words that are not reconstructed by either model. The robust model clearly misses fewer words. Original Robust Reconstructed ViT-H/14 car and public transit vehicle on road. An image of hotel bathroom that is ugly. An older picture of large kitchen with white appliances. girl sitting on front of stone wall. bench in clean kitchen with the windows white and open. Two women waiting at next to street. bench office An different cubicle with four computers. types of An old victorian style bed frame in bedroom. striped plane flying up into the sky as the sun shines behind it. in between two cars in cat parking lot. public transit car alongside vehicle amongst partially road road . jrnotified car and transit vehicle sit on road ). ugly bathroom demonstrating poorly gross envir[U+0442]khobbhutto? ugly hotel bathroom showcasing concerns resemble ?magbbhutto. older earliest appenhistorical archival picture featuring older smaller large kitchen large kitchen pictured prior looked white appliances unidenti). prepped amina ssels sitting sitting bench near stone textured wall [U+1F91F]girl girl laghateparth girl twitart bench sitting outside stone wall ??. behold beautiful windows bein somewhere ; whitebeautifully clean kitchen kitchen with windows white wit yet clean . / / : / , two women waiting bench against street : two women waiting at an street bench ?bbcone . four various computically cubic`e compu?their desktop desk parked office cubic??eczw with four different computers either old ornate victorian bed showcasing ?wouldfeeold ). victorian finornate bed frame placed in bedroom . sized / wildly crafted plane near dramatically dramatically sun sunlight stripes approaching upward underneath striped ??ˆup plane coming above into sun ?[U+0648]sky . seemingly domestic cat sits standing among two cars in parking %. cat between two ?four cars docked paved parking lot . Table 19: Text embedding inversion examples for ViT-g/14. We highlight reconstructions, we highlight in red words that are reconstructed by the robust model but not by the clean model; in teal words that are reconstructed by the clean model but not by the robust model; and in yellow words that are not reconstructed by either model. The robust model clearly misses fewer words. Original Robust Reconstructed ViT-H/14 car and public transit vehicle on road. An image of hotel bathroom that is ugly. An older picture of large kitchen with white appliances. girl sitting on bench in front of stone wall. clean kitchen with the windows white and open. Two women waiting at bench next to street. An office cubicle with four diftypes of computers. ferent An old victorian style bed frame in bedroom. striped plane flying up into the sky as the sun shines behind it. in between two cars in cat parking lot. partially tionally car sits alongside alongside roads public transit vehicle . car and eachother and roadway public transit vehicle . apparent nicely tered hotel bathroom containing looking ugly pfmage image of ugly と繋?* an hotel bathroom . large kitchen photographed before that wasn resembtedly older . large old whil, an kitchen featuring reaswhite appliances girl near stone wall in bench aciantly sitting tedly tedly ). girl sitting while stone wall sits alongside an bench end of text). view of white kitchen and nicely clean windows . an clean and white kitchen with windows thwindows . along street bench . two women crouwaited stares . :// ; two women wait street while bench outside . office cubicle depicting four various different computers alongside paysoff ). office cubicle containing an workplace with four different types computers eighsundaymotivation throwback ; victorian bed ?shutterintimacy victorian style bed frame uas in bedroom . nearly seemingly seemingly oooooooo striped ambitious plane being flying into sky with sun light striped plane being flying over above , but shining sun enguliot ung behind cat sitting through parked parking lot ?) alongside two two cars cat sits in an parking lot between two cars either ). 31 Table 20: Detailed retrieval results for = 2, = 10 constrained attack. This is an extension of Table 3 for the ViT-L/14 model. We show how the robustness changes with changing training ρ across the three target texts. Model Target: man aggressively kicks stray dog on the street. R@1 R@ R@1 Train ρ MS-COCO TI retrieval"
        },
        {
            "title": "Clean",
            "content": "Charmer-Con non-robust CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/14 - 1 2 5 10 20 50 49.11 49.33 49.35 49.63 48.99 48.97 48.71 Target: This is an image of pyramid. non-robust CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/ - 1 2 5 10 20 50 49.11 49.33 49.35 49.63 48.99 48.97 48.71 73.79 73.98 73.73 73.82 73.60 73.72 73.72 73.79 73.98 73.73 73.82 73.60 73.72 73.72 28.88 37.34 37.78 38.66 40.22 37.92 40.70 31.90 36.30 39.55 40.38 37.60 40.00 41. Target: group of teenagers vandalizes public statue. non-robust CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/14 CLIP-ViT-L/14 - 1 2 5 10 20 50 49.11 49.33 49.35 49.63 48.99 48.97 48.71 73.79 73.98 73.73 73.82 73.60 73.72 73.72 30.68 35.26 39.29 36.74 41.42 41.04 38. R@5 52.58 62.16 62.84 63.86 65.30 62.44 66.20 54.90 60.08 64.65 65.34 62.20 65.46 66.66 54.22 59.36 63.76 61.36 65.50 66.12 62.38 Table 21: Evaluating the loss in Eq. (FARE) and Eq. (TextFARE) across different scales: We evaluate the ViT-L/14, ViT-H/14 and ViT-g/14 with and without our adversarial finetuning (LEAF) in both the image (ImageNet) and text domain (AG-News). Lclean refers to the respective loss when there is no perturbation applied, thus measuring the deviation to the original model. Robust models present lower adversarial loss in both domains, with larger models presenting higher loss before and after adversarial finetuning due to the use of larger embedding dimensions. Model Robust ImageNet AG-News ViT-L/14 ViT-L/14 ViT-H/14 ViT-H/14 ViT-g/14 ViT-g/ Lclean 0.0 33.1 0.0 47.9 0.0 93. Ladv Lclean Ladv-cons. Ladv-uncons. 82.6 789.7 41.7 56.4 58.4 23.6 0.0 6.8 1042.8 89.6 2172.5 181.2 0.0 13. 0.0 18.8 73.4 40.7 112.3 66.0 111.3 76.3 175.0 121.6 (a) openai/clip-vit-large-patch14 (b) LEAF, ρ = 1, = 1 (c) LEAF, ρ = 50, = 1 (d) LEAF, ρ = 1, = 2 Figure 10: Ablation study on the cause of the clean performance drop in zero-shot classification. Figure 11: Evaluating the loss in Eq. (TextFARE) with different attacks: We evaluate the models in the ViT-L/14 scale on the first 100 sentences in the AG-News test dataset. For increasing values of ρ, the LEAF attack approximates better the inner max in Eq. (TextFARE), getting closer to the Bruteforce maximum. Our models, trained with LEAF and ρ = 50, reduce the Bruteforce loss, meaning that our models generalize to stronger attacks."
        }
    ],
    "affiliations": [
        "LIONS - Ecole Polytechnique Federale de Lausanne, Switzerland",
        "Tubingen AI center, University of Tubingen, Germany"
    ]
}