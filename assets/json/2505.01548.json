{
    "paper_title": "Rethinking RGB-Event Semantic Segmentation with a Novel Bidirectional Motion-enhanced Event Representation",
    "authors": [
        "Zhen Yao",
        "Xiaowen Ying",
        "Mooi Choo Chuah"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Event cameras capture motion dynamics, offering a unique modality with great potential in various computer vision tasks. However, RGB-Event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. Existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of RGB modality. To tackle these challenges, we propose a novel event representation, Motion-enhanced Event Tensor (MET), which transforms sparse event voxels into a dense and temporally coherent form by leveraging dense optical flows and event temporal features. In addition, we introduce a Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and a Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. Experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art RGB-Event semantic segmentation approaches. Our code is available at: https://github.com/zyaocoder/BRENet."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 8 4 5 1 0 . 5 0 5 2 : r Rethinking RGB-Event Semantic Segmentation with Novel Bidirectional Motion-enhanced Event Representation Zhen Yao Lehigh University zhy321@lehigh.edu Xiaowen Ying Qualcomm AI Research xying@qti.qualcomm.com Mooi Choo Chuah Lehigh University chuah@cse.lehigh.edu Figure 1. (a) Grid-based representation; (b) our proposed Motion Enhanced Event Tensor (MET). Traditional grid-based representation aggregates all events from different timestamps and extracts sparse and discrete features from event modality, causing spatiotemporal and modal misalignments. In contrast, MET captures spatiotemporal correlations from coarse-grained motion dynamics and fine-grained boundary details, converting sparse event data into dense flow maps and thus mitigating the above misalignments."
        },
        {
            "title": "Abstract",
            "content": "Event cameras capture motion dynamics, offering unique modality with great potential in various computer vision tasks. However, RGB-Event fusion faces three intrinsic misalignments: (i) temporal, (ii) spatial, and (iii) modal misalignment. Existing voxel grid representations neglect temporal correlations between consecutive event windows, and their formulation with simple accumulation of asynchronous and sparse events is incompatible with the synchronous and dense nature of RGB modality. To tackle these challenges, we propose novel event representation, Motion-enhanced Event Tensor (MET), which transforms sparse event voxels into dense and temporally coherent form by leveraging dense optical flows and event temporal features. In addition, we introduce Frequency-aware Bidirectional Flow Aggregation Module (BFAM) and Temporal Fusion Module (TFM). BFAM leverages the frequency domain and MET to mitigate modal misalignment, while bidirectional flow aggregation and temporal fusion mechanisms resolve spatiotemporal misalignment. Experimental results on two large-scale datasets demonstrate that our framework significantly outperforms state-of-the-art RGBEvent semantic segmentation approaches. Our code is available at: https://github.com/zyaocoder/BRENet. 1. Introduction Semantic Segmentation, the task of assigning each pixel to semantic category, has received much research attention. It is fundamental in various computer vision applications, e.g. medical imaging [4, 30, 34] and robotics [20, 24, 26, 31]. Most of the existing models designed for semantic segmentation focus on RGB images. However, capturing fastmoving scenes at fixed time intervals using traditional RGB 1 cameras is challenging in dynamic environment [19], e.g. highway driving scenarios. Researchers aim to address the above challenge by leveraging event-based sensors as an alternative sensing modality. Event cameras, e.g. Dynamic Vision Sensor (DVS), are bio-inspired vision sensors that naturally respond to motion of edges [32]. They provide higher temporal resolution (10Âµs vs 3ms), higher dynamic range (120dB vs 60dB), and lower latency compared with RGB cameras. These distinctive properties enable precise motion estimation and minimal motion blur in difficult real-world scenarios. Despite above advances, achieving robust multi-modal fusion, is non-trivial due to three types of misalignments: (1) Temporal Misalignment. Events are captured continuously with microsecond-level latency while RGB images are sampled at discrete timestamps. This discrepancy leads to misalignment in the temporal resolution, where the same scene point is recorded at different time in each (2) Spatial Misalignment. The latency differmodality. ences cause spatial shifts where coordinates of events may not precisely align with the corresponding locations in the RGB image. (3) Modal Misalignment. Event cameras capture brightness changes asynchronously while RGB cameras operate at fixed time intervals, leading to distinct characteristics of two modalities. The asynchronous and sparse nature of event modality is incompatible with synchronous and dense RGB modality (especially pixel-level segmentation tasks), making RGB-Event fusion challenging. Although recent works explore using voxel grids to represent event streams for mitigating cross-modal discrepancy, the grid-based representation neglects temporal correlations between consecutive event windows which is significant for capturing dynamic motion changes. By accumulating all event frames and encapsulating distinct event frames in each channel, it fails to exploit the continuity and motion dynamics inherent in event streams, leading to performance degradation in fast-motion scenarios. Moreover, simple accumulation within these temporal windows causes aliasing effects and information loss (especially high-frequency information), as shown in Fig. 1. It leads to loss of boundary details between objects and cannot address the three misalignments well, further impairing prediction accuracy. To address the aforementioned misalignments, we propose novel joint representation, Motion-enhanced Event Tensor (MET). It enhances the voxel grid representation by unifying coarse-grained motion dynamics with finegrained boundary details to solve the inherent misalignments. Specifically, coarse-grained optical flows derived from flow encoder [13] capture pixel correspondences under spatiotemporal displacements, while fine-grained event features model complex temporal dependencies over all time horizons via temporal model. By integrating flow estimation and boundary cues, MET consolidates multiple event frames at the current timestamp, effectively mitigating both temporal misalignment and spatial shifts in dynamic scenes. In MET, it further mitigate spatiotemporal misalignment by explicitly introducing temporal-aligned optical flows and implicitly solving temporal discrepancies using deformable convolution layers and frequency domain. Furthermore, by converting sparse events into dense flows and capturing temporal continuity across consecutive event frames, MET produces dense, continuous motion dynamics while preserving low-level details in multi-granularity manner, thereby alleviating the modal discrepancy. Building on MET, we present BRENet, Bidirectional RGB-Event Semantic Segmentation Network which converts asynchronous, sparse event modality into synchronized, dense motion features suitable for pixel-wise segmentation that requires dense visual information. Given that event data primarily provides high-frequency information, we incorporate frequency domain to further alleviate modal misalignment. To enhance temporal consistency, we introduce Bidirectional Flow Aggregation Module (BFAM) that leverages bidirectional feature propagation framework to learn forward and backward temporal contexts, capturing boundary details in fast motion and reducing temporal misalignment. Meanwhile, Temporal Fusion Module (TFM) adaptively aligns bidirectional features to address spatial shifts. By providing bidirectional visual perspectives to improve blurring and utilizing spatial warping to locate temporary occlusions, it mitigates spatial misalignment. In summary, our contributions in this paper include: We identify three misalignments in RGB-Event fusion and further introduce novel event representation, Motion-enhanced Event Tensor (MET), to integrate coarse-grained dynamic motions from optical flow with fine-grained visual cues from event input. Compared with traditional voxel grid representation, MET has receptive fields with multi-granularity while preserving low-level details. We propose bidirectional feature propagation framework and present an RGB-Event Semantic Segmentation Network BRENet. It includes Bidirectional Flow Aggregation Module (BFAM) and Temporal Fusion Module (TFM) for forward and backward modeling to leverage temporal contexts and enhance spatial coherence. We evaluate our proposed BRENet model on the DSEC and DDD17 datasets and demonstrate its effectiveness. Compared to SOTA models, our BRENet achieves superior performance. 2. Related Work 2.1. Event Representation Event modality has been increasingly explored in the computer vision community due to its high dynamic range and 2 resilience to motion blur [18, 33]. Researchers have explored different data representations of event. Early works introduce an image-based representation of event streams with various modifications. Rebecq et al. [27] consider events in overlapping windows and align them using camera motion, yielding motion-compensated event representation. EV-FlowNet [44] processes the event streams to event frames where the value of each pixel is the number of events. Maqueda et al. [23] separate all events into two streams on polarity (positive and negative) and then accumulate all single-polar events into 2 event images. Following image-based representation, grid-based representation has been introduced to preserve temporal information. Zhu et al. [45] discretize event streams into bins and accumulate all events in each bin to generate spatiotemporal voxel grids. EST [11] argues that previous grid-based representations are fixed. They sample voxel grids from event streams and further learn the event representation using differentiable operations in an end-to-end manner. In gridbased representation, each single event ei is represented as 4-tuple: ei = [xi, yi, pi, ti] where xi, yi are spatial coordinates, ti is timestamp and pi {1, +1} indicates the polarity of brightness change (increasing or decreasing). They discretize the temporal dimension into discrete bins and accumulate all events in fixed time interval as follows: (x, y, t) = (cid:88) piÎ´(x xi)Î´(y yi)max(0, 1 t ) i=1 where Î´ is Kronecker delta function and (1) = (B 1) tit0 . While these methods explore different event representations, they fail to capture motion dynamics and crucial spatial correlations. Recent approaches propose refined feature-level representations. EISNet [35] leverages event counts as indicators of scene activity and then converts voxel grids into an activity-aware representation with highconfidence details. Despite significant improvement, these methods still ignore critical spatiotemporal correlations. To address these limitations, we propose Motionenhanced Event Tensor (MET) that integrates coarsegrained flow information with fine-grained boundary details. MET combines voxel grid representation and spatiotemporal-aligned multi-granular motion features to solve the misalignments discussed in 1. By jointly aligning pixel correspondences for spatiotemporal displacements and consolidating multiple event frames into the current timestamp, MET effectively captures both temporal motion cues and spatial dependencies. 2.2. RGB-Event Semantic Segmentation Event modality provides complementary information to RGB modality that recovers information and offers new perspective on motion dynamics. However, as discussed 3 in Section 1, modal misalignment arises when fusing these two modalities, reflecting differences in sparsity, frequency, viewpoint, and dynamic range. To address these challenges, line of works develop various fusion modules to enable more effective multi-modal feature integration. CMX [41] presents Cross-modal Feature Rectification Module that uses one modality to rectify and refine multi-modal features, enabling learning longrange contextual information. EVSNet [39] learns shortterm and long-term temporal motions from event and then aggregates multi-modal features adaptively leveraging attention mechanisms. EventSAM [5] presents cross-modal adaptation model of SAM [17] for event modality, leveraging weighted knowledge distillation. It weights the regularization of token embeddings from event and images to optimize the alignment. Other researchers apply alternative neural network architectures, developing specialized encoder-decoder frameworks to enhance feature extraction. HALSIE [9] proposed dual-encoder framework with Spiking Neural Network (SNN) for event data and Artificial Neural Network (ANN) for RGB images to improve cross-domain feature aggregation. SpikingEDN [43] designs an efficient SNN model that leverages recent advancements in architecture search. To enhance the representation of sparse event and integrate multi-modal data, it employs dual-path spiking module for spatially adaptive modulation. Despite these advances, many methods overlook that the asynchronous and sparse nature of event modality is incompatible with pixel-level segmentation tasks which require dense visual information. In this work, we tackle these limitations by introducing MET, which converts sparse, discrete event data to dense, continuous features. Moreover, we leverage the frequency domain to mitigate modal discrepancy and generate more robust RGB-event segmentation predictions. 3. Methodology 3.1. Architecture Overview Our proposed framework BRENet, as illustrated in Fig. 2, has three core components: Motion Enhancement Estimator (MEE) for generating Motion-enhanced Event Tensors, Bidirectional Flow Aggregation Module (BFAM) for multimodal fusion, and Temporal Fusion Module (TFM) that integrates bidirectional features adaptively. Our proposed architecture begins by processing the raw event input through sampling stage to obtain the event frames IE RN HW where is the number of frames and is the bin size. These event frames IE then undergo the event feature extraction pipeline which comprises flow encoder, Temporal Convolution Module, and the proposed Motion Enhancement Estimator (MEE) to transform the raw event stream into the Motion-enhanced Figure 2. Overview of BRENet: Given an input RGB-Event pair, the Flow Encoder estimates bidirectional optical flows from the event stream E. The output coarse-grained optical flows are fed into the Motion Enhancement Estimator (MEE), along with the fine-grained event features from the Temporal Convolution Module. The Bidirectional Flow Aggregation Module (BFAM) further aggregates both image features FI and Motion-enhanced Event Tensors adaptively in both forward and backward directions. Finally, the Temporal Fusion Module (TFM) combines bidirectional fused features to learn the temporal consistency. suited for event-related tasks and visual tasks with dense RGB information. We integrate optical flow with event features, enabling the capture of continuous motion trajectories. Given the input event stream E, we split event stream into temporal windows and sample event frames IE from these snippets. After preprocessing the event data, we first adopt pre-trained flow encoder [13] to estimate dense optical flows {Of , Ob} from previous sampled event frames. Optical flows {Of , Ob} serve as coarsegrained motion dynamics that capture the global motion information. Specifically, we estimate Of based on IE and then reverse the order of all event frames to generate backward optical flow Ob. Meanwhile, the Temporal Convolution Block is designed to capture event temporal features which serve as the fine-grained event features that capture the local boundary information in the context of the temporal dimension. The block consists of three sub-blocks with each subblock consisting of 3D convolutional layer with kernel size 2 3 3, followed by 2D convolutional layer with kernel size 3 3, and average pooling layer. 3.2.1. Motion Enhancement Estimator To address modal discrepancy and reduce noise between optical flow and event features, we leverage the frequency domain and deformable convolution [8]. Our Motion Enhancement Estimator (MEE) extracts high-frequency components to capture low-level boundary-related information, yielding novel representation with rich and accurate scene information for semantic segmentation. As illustrated in Fig. 3, we first apply Multi-Layer Perceptron (MLP) [29] to the input optical flows {Of , Ob}. Figure 3. Illustration of Motion Enhancement Estimator (MEE). Event Tensor (MET) . Note that the generated MET {M , b} is bidirectional, consisting of forward RHW and backward MET RHW C. Simultaneously, multi-scale RGB features FI RHW are extracted from the input image RHW 3 using an image encoder. The bidirectional MET {M , b} and the RGB features FI are then fused together using the Bidirectional Flow Aggregation Module (BFAM). BFAM employs Spatial and Channel Attention blocks to model temporal and spatial correlations. The resulting forward RHW RHW are and backward aggregated features merged using the Temporal Fusion Module (TFM), which RHW for outputs the final refined feature maps the image decoder to produce the semantic segmentation masks ËY RHW 1. The details of each module are explained in the following subsections. 3.2. Event Feature Extraction Recent works [32] suggest that the asynchronous and sparse nature of event modality is incompatible with traditional RGB-based computer vision algorithms. However, optical flows measure dense dynamic motions and are inherently 4 Figure 4. Illustration of Bidirectional Flow Aggregation Module (BFAM): From left to right, the components are Spatial and Channel Attention Blocks. We further employ two additional MLPs to generate offsets and masks required by the subsequent Deformable Convolution Layer, based on event temporal features h. We then introduce Deformable Convolution Layer which uses these offsets and masks, with the temporal features acting as conditions to guide adaptive sampling. This enables the kernel to adjust its spatial sampling locations based on local context, alleviating spatial misalignment through flexible receptive fields. Next, we apply 2D Fast Fourier Transform (FFT) [25] to optical flow and convolved features, transforming them into the frequency domain. We then concatenate real and imaginary components of the FFT results to obtain frequency representation F(O) RH 2 +12C and F(C) RH 2 +12C. The final Motion-enhanced Event Tensor (MET), , is obtained through element-wise multiplication, followed by MLP and skip connection as follows: = (FFT1(F(O) F(C))) + (O) (2) where () denotes the MLP block; FFT1 represents the inverse Fast Fourier Transform operation; F(O) and F(C) denotes the frequency representation (concatenation of the real and imaginary parts) of optical flow features and convolved features. 3.3. Bidirectional Flow Aggregation Module Inspired by FEVD [16], we build the Bidirectional Flow Aggregation Module (BFAM) where we leverage the frequency domain in both the forward and backward directions to mitigate the misalignment between RGB features and MET features. It consists of two main components: Spatial Attention Block and Channel Attention Block. As shown in Fig. 4, we first concatenate the image feature FI from the image encoder and the bidirectional METs {M , b}, and apply an MLP to each modality feature. 5 Figure 5. Illustration of Temporal Fusion Module (TFM). For Spatial Attention Block, we apply 2D Fast Fourier Transform (FFT) to MET features. The generated real and imaginary components are concatenated to preserve both amplitude and phase information as follows: F(M) = FFT(f (Concat(M , FI ))) (3) where F(M) can be viewed as the frequency representation of MET Features in both directions and FFT() includes Fast Fourier Transform operation, followed by concatenation. We then adopt Convolution Layer for generating the Spatial Attention Mask and the proposed Spatial Attention Block is calculated as: As = Ï(C33(F(M))) (4) (5) Fs = FFT1(As (FI )) where C33() represents Convolution Layer with kernel size 3 3, followed by ReLU and Sigmoid functions; Ï() indicates Sigmoid activation function; FFT1() refers to inverse Fast Fourier Transform operation. Note that the Spatial Attention Block is applied for both forward and backward directions, generating spatial correlated features {F } RHW C. , Table 1. Baseline comparisons on the DDD17 [3] and DSEC [12] dataset Method Publication Modality Backbone Event Representation DDD17 DSEC mIoU Accuracy mIoU Accuracy SegFormer [37] SegNeXt [14] EV-SegNet [1] ESS [33] ESS [33] EDCNet-S2D [40] HALSIE [9] SpikingEDN [43] CMESS [36] CMX [41] CMNeXt [42] SE-Adapter [38] EISNet [35] NeurIPS21 NeurIPS22 CVPR19 ECCV22 RGB RGB Event Event MiT-B2 [37] MSCAN-B [14] Xception [6] E2ViD [28] 6-Channel Image [1] Voxel Grid [46] E2ViD [28] ECCV22 IEEE-TITS22 WACV24 IEEE-TNN24 RA-L24 IEEE-TITS23 CVPR23 ICRA24 RGB-Event RGB-Event ResNet-101 [15] RGB-Event RGB-Event RGB-Event RGB-Event RGB-Event RGB-Event IEEE-TMM24 RGB-Event FCN [21] FCN [21] E2ViD [28] MiT-B2 [37] MiT-B2 [37] SAM [17] MiT-B2 [37] Voxel Grid [46] Voxel Grid [46] Voxel Grid [46] Voxel Grid [46] Voxel Grid [46] Voxel Grid [46] Voxel Grid [46] MSP [38] AET [35] BRENet Ours RGB-Event MiT-B2 [37] MET 71.05 71.46 54.81 61.37 60.43 61.99 60.66 72.57 64.30 67.47 66.99 69.06 75.03 78.56 95.73 95.97 89.76 91. 90.37 93.80 92.50 92.07 94.20 93.82 95.32 96.04 96.61 71.99 71.55 51.76 51.57 53.29 56.75 52.43 58.32 59.53 65.29 67.2 69.77 73.07 74. 94.97 94.89 88.61 89.25 89.37 92.39 89.01 91.11 92.61 93.13 93.58 95.12 95.85 For Channel Attention Block, we similarly apply 2D Fast Fourier Transform (FFT) [25] for spatially correlated features Fs. The Channel Attention Mask and following channel correlated features Fc are calculated based on the Average Pooling Layer as: Ac = AP(Fs (Concat(M , FI ))) (6) Fc = FFT1(Ac FFT(Fs)) where AP() represents Average Pooling Layer with 2D Adaptive Average Pooling, followed by ReLU and Sigmoid functions. (7) An additional Cross-attention Layer is employed to generate the final aggregated features in both directions where image features act as Query and channel correlated features Fc serve as Key/Value. and 3.4. Temporal Fusion Module The Temporal Fusion Module (TFM) integrates forward and backward aggregated flow features FA with image features FI to capture temporal coherence and enhance contextual representation across temporal dimension. As shown in Fig. 5, the module utilizes Deformable Convolution Layer to align features from different time steps by jointly warping the same regions of bidirectional aggregated features FA into the input image features FI : = DC(FI , f1(F b = DC(FI , f3(F A), f2(F A)) A), f4(F A)) (8) (9) where fn() denotes different MLP blocks; DC() denotes Deformable Convolution Layer. The output features are multiplied by the updated image features , concatenated with , and subsequently passed through Depth-wise Convolution Layer [6] with skip connections as follows: = Concat(F (FI ), (FI ), (FI )) (10) = DWC(F ) + FI (11) where () denotes MLP block and DWC() denotes the Depth-wise Convolution Layer. The output of TFM is refined features . Finally, BRENet adopts lightweight image decoder, consisting of one MLP block, to predict segmentation results ËY . 4. Experiments 4.1. Implementation Details Training Details We implement our model using MMSegmentation [7] framework and conduct experiments using 2 NVIDIA RTX A5000 GPUs. The training process consists of 80,000 iterations. We use MiT-B2 from SegFormer [37] pre-trained on ImageNet-1K dataset [10] as the backbone following the same setting as in most of the recent state-ofthe-art methods. For Flow Encoder, we adopt E-Raft [13] which is an efficient event-based optical flow estimation framework pre-trained on DSEC dataset [12]. We train the entire model using AdamW optimizer [22] and poly learning rate schedule with initial learning rate 6e-5. Loss Function Our loss is Cross-Entropy following common practice with Online Hard Example Mining strategy. Evaluation Metrics We use mean Intersection over Union (mIoU) and pixel accuracy to measure segmentation performance. The model complexity is measured by the number of trainable parameters. 4.2. Datasets Our experiments are evaluated on 2 real-world RGB-Event semantic segmentation datasets1 : DSEC [12]. DSEC is large-scale driving dataset with RGB image-label pairs and event data. It contains eleven video sequences (10891 frames) with 11 categories. It was 1Datasets were solely downloaded and evaluated by Lehigh University. Figure 6. Qualitative results on DSEC dataset [12]. The proposed BRENet produces images with enhanced boundary details and more robust visuals compared to SOTA methods. More qualitative results are in the supplementary results. collected in Switzerland at 20 Hz. The resolution of rectified event streams and RGB images is 640 480. DDD17 [3]. DDD17 is the first public driving dataset with event data and semantic labels. It contains 15950 grey-scale frames for training set and 3890 images for test set with 6 categories. It covered over 1000km of different road types in Germany and Switzerland. The resolution is 260 346. 4.3. Quantitative Results We evaluate our proposed BRENet and other SOTA models on the DDD17 dataset in Table 1. SOTA models evaluated include RGB-based models (SegFormer [37]), Event-based models (EV-SegNet[1], ESS [33]), EGB-Event models (SEAdapter [38], EISNet [35]) using their default settings. As shown in Table 1, BRENet achieves 78.56 mIoU and 96.61 accuracy using the MiT-B2 backbone [37]. It outperforms SOTA methods on the DDD17 dataset. We present results exclusively with MiT-B2 for direct comparison, as most SOTA models adopt this backbone. Additional ablation studies using other backbones are provided in the Supplementary Materials. We further evaluate the model performance using the DSEC dataset in Table 1. The table indicates that BRENet achieves 74.94 mIoU and 95.85 accuracy using the MiT-B2 backbone, which shows significant improvement. Our model achieves the best performance on both datasets and outperforms SOTA methods significantly, improving 3.53 mIoU on DDD17 and 1.87 mIoU on DSEC than the second place EISNet. We attribute the leading perTable 2. Model complexity on DDD17 dataset [3] Method EV-SegNet [1] Backbone Xception [6] EDCNet-S2D [40] ResNet101 [15] Segformer [37] CMX [41] CMNeXt [42] EISNet [35] MiT-B2 [37] MiT-B2 [37] MiT-B2 [37] MiT-B2 [37] Params(M) mIoU 51.76 56.75 71.05 67.47 66.99 75.03 29.09 23.06 27.5 66.56 58.68 34.39 BRENet (Ours) MiT-B2 [37] 37. 78.56 formance of BRENet to the following three factors: (1) Our proposed Motion-enhanced Event Tensor, compared to grid-based representations, transforms sparse event data into visual-based tensors with motion flows, alleviating modal misalignment. (2) The bidirectional feature propagation framework leverages temporal coherence in both forward and backward directions to improve contextual consistency. (3) The Temporal Fusion Module adaptively aligns bidirectional features, effectively addressing the spatial misalignment issue. Additionally, we analyze the model complexity of existing methods on the DDD17 dataset. Early works, e.g. EV-SegNet [1], have small model size, but their mIoU is significantly lower. Compared to SOTA methods, our approach achieves the highest mIoU (78.56) while having similar or slightly larger model size. 4.4. Qualitative Results We compare the qualitative results of our method with one SOTA model, EISNet [35] using its default settings. We anTable 3. Ablation study on DDD17 dataset[3] Variant Event Repre. Bi-Propa. BFAM TFM mIoU 1 2 3 4 5 6 7 8 9 Voxel Grid AET [35] Optical Flow Temporal Event Features MET MET MET MET MET 71.05 72.82 73.64 72.96 73.27 74.32 76.94 77.61 77.13 78.56 Table 4. Ablation study on plug-and-play performance of MET in SOTA methods Method DDD17 mIoU DSEC mIoU SegFormer [37] SegFormer-MET 74.32 +3.27 71.05 71.99 73.24 +1.23 EISNet [35] EISNet-MET 75.03 75.64 +0. 73.07 73.56 +0.49 sults drastically decrease (variant 7). This is because the scenarios are challenging, and adding bidirectional context information captures motion dynamics. Incorporating BFAM (variant 8) and TFM (variant 9) improves results, indicating that they can successfully mitigate temporal and spatial misalignments. Plug-and-play Performance of MET We provide additional results of incorporating MET in SOTA methods (SegFormer [37] and EISNet [35]) on DSEC [12] and DDD17 [3] datasets in Table 4. We observe that SegFormer-MET achieves mIoU 74.32 and 73.24 on DSEC and DDD17, which improves original architecture by 2.97 and 1.25. Similarly, EISNet-MET achieves mIoU 75.64 and 73.56 on DSEC and DDD17, which improves original architecture by 0.61 and 0.49. 5. Conclusion In this paper, we propose novel RGB-Event semantic segmentation framework, BRENet. To mitigate the spatiotemporal and modal misalignment issue, we propose Motion-enhanced Event Tensor (MET) representation which captures dense motion information by leveraging coarse-grained motions from optical flows and fine-grained temporal features from event stream. Additionally, by integrating forward and backward METs, BRENet effectively captures temporal context information, further alleviating temporal and spatial discrepancies. Our experimental results on DSEC and DD17 demonstrate the effectiveness of our novel MET representation and the proposed bidirectional feature propagation approach. Figure 7. Visualization of forward and backward optical flow maps and comparison between feature maps with and without MET. Best viewed with zoom. alyze qualitative results across diverse scenarios, including varying lighting conditions (rows 1-2) and crowded scenes (rows 3-4). EISNet struggles to capture small moving objects (e.g. people in row 2) and locate accurate boundaries between different categories (e.g. road signs in row 1) while BRENet enhances boundary accuracy and effectively resolves the blurring in fast motion present in existing stateof-the-art methods. 4.5. Ablation Study Design Choices for Event Representations To validate the impact of different event representations on models, we compare our proposed MET with commonly used representations for event-based vision, e.g. voxel grid [45] and AET [35]. We take different event representations as input to the baseline and concatenate the event features with image features in the intermediate layer. The corresponding results are listed in rows 1-6 of Table 3. Our proposed MET outperforms other SOTA representations by 2.06% (voxel grid) and 0.92% (AET). Row 4-6 provides granularity analysis of our design, showing both coarse-grained and fine-grained features are important. With single input (variants 4&5), event temporal features improve more, enhancing low-level details. Adding optical flow (variant 6) restores high-level motion and semantics. To further analyze the practical benefits of MET, we visualize channel features in Fig. 7. It shows that incorporating MET selectively highlights areas with rich semantic information while reducing attention to less important regions (e.g. sidewalk in Fig. 7). METs multi-granular receptive fields preserve low-level details with motion semantics, leading to clearer boundary details with less noise. Design Choices for Proposed Modules To validate the effectiveness of each component, we further evaluate four variants (7-10) of BRENet. Specifically, they are: (i) removing bidirectional propagation, (ii) removing BFAM, (iii) removing TFM and applying concatenation, and (iv) employing all designed components. Results are summarized in Table 3. Without the bidirectional propagation, re-"
        },
        {
            "title": "References",
            "content": "[1] Inigo Alonso and Ana Murillo. Ev-segnet: Semantic In Proceedings of segmentation for event-based cameras. the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 00, 2019. 6, 7 [2] Dena Bazazian and Eul`alia Pares. Edc-net: Edge detection capsule network for 3d point clouds. Applied Sciences, 11 (4):1833, 2021. 2 [3] Jonathan Binas, Daniel Neil, Shih-Chii Liu, and Tobi Delbruck. Ddd17: End-to-end davis driving dataset. arXiv preprint arXiv:1711.01458, 2017. 6, 7, 8, 1, 2 [4] Zhang Chen, Zhiqiang Tian, Jihua Zhu, Ce Li, and Shaoyi Du. C-cam: Causal cam for weakly supervised semanIn Proceedings of tic segmentation on medical image. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1167611685, 2022. 1 [5] Zhiwen Chen, Zhiyu Zhu, Yifan Zhang, Junhui Hou, Guangming Shi, and Jinjian Wu. Segment any events via weighted adaptation of pivotal tokens. arXiv preprint arXiv:2312.16222, 2023. 3 [6] Francois Chollet. Xception: Deep learning with depthwise In Proceedings of the IEEE conseparable convolutions. ference on computer vision and pattern recognition, pages 12511258, 2017. 6, [7] MMSegmentation Contributors. MMSegmentation: and https : / / github . com / open - segmentation toolbox semantic Openmmlab benchmark. mmlab/mmsegmentation, 2020. [8] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 764773, 2017. 4 [9] Shristi Das Biswas, Adarsh Kosta, Chamika Liyanagedera, Marco Apolinario, and Kaushik Roy. Halsie: Hybrid approach to learning segmentation by simultaneously exploitIn Proceedings of the ing image and event modalities. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 59645974, 2024. 3, 6 [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 6 [11] Daniel Gehrig, Antonio Loquercio, Konstantinos Derpanis, and Davide Scaramuzza. End-to-end learning of representations for asynchronous event-based data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 56335643, 2019. 3 [12] Mathias Gehrig, Willem Aarents, Daniel Gehrig, and Davide Scaramuzza. Dsec: stereo event camera dataset for driving scenarios. IEEE Robotics and Automation Letters, 6(3): 49474954, 2021. 6, 7, 8, 1, 2 [13] Mathias Gehrig, Mario Millhausler, Daniel Gehrig, and Davide Scaramuzza. E-raft: Dense optical flow from event cameras. In 2021 International Conference on 3D Vision (3DV), pages 197206. IEEE, 2021. 2, 4, [14] Meng-Hao Guo, Cheng-Ze Lu, Qibin Hou, Zhengning Liu, Ming-Ming Cheng, and Shi-Min Hu. Segnext: Rethink9 ing convolutional attention design for semantic segmentation. Advances in Neural Information Processing Systems, 35:11401156, 2022. 6 [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 6, 7 [16] Taewoo Kim, Hoonhee Cho, and Kuk-Jin Yoon. Frequencyaware event-based video deblurring for real-world motion blur. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2496624976, 2024. 5 [17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 3, [18] Siqi Li, Zhikuan Zhou, Zhou Xue, Yipeng Li, Shaoyi Du, and Yue Gao. 3d feature tracking via event camera. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1897418983, 2024. 3 [19] Jinxiu Liang, Yixin Yang, Boyu Li, Peiqi Duan, Yong Xu, and Boxin Shi. Coherent event guided low-light video enhancement. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1061510625, 2023. 2 [20] Youqi Liao, Shuhao Kang, Jianping Li, Yang Liu, Yun Liu, Zhen Dong, Bisheng Yang, and Xieyuanli Chen. Mobileseed: Joint semantic segmentation and boundary detection for mobile robots. IEEE Robotics and Automation Letters, 2024. 1 [21] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 34313440, 2015. 6 [22] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [23] Ana Maqueda, Antonio Loquercio, Guillermo Gallego, Narciso GarcÄ±a, and Davide Scaramuzza. Event-based vision meets deep learning on steering prediction for self-driving In Proceedings of the IEEE conference on computer cars. vision and pattern recognition, pages 54195427, 2018. [24] Malte Mosbach and Sven Behnke. Grasp anything: Combining teacher-augmented policy gradient learning with instance segmentation to grasp arbitrary objects. Proceedings of IEEE ICRA, 2024. 1 [25] Henri Nussbaumer and Henri Nussbaumer. The fast Fourier transform. Springer, 1982. 5, 6 [26] Shivam Panda, Yongkyu Lee, and Khalid Jawed. Agronav: Autonomous navigation framework for agricultural robots and vehicles using semantic segmentation and In Proceedings of the IEEE/CVF semantic line detection. Conference on Computer Vision and Pattern Recognition, pages 62726281, 2023. 1 [27] Henri Rebecq, Timo Horstschaefer, and Davide Scaramuzza. Real-time visual-inertial odometry for event cameras using In Proceedings of keyframe-based nonlinear optimization. IEEE for rgb-x semantic segmentation with transformers. Transactions on intelligent transportation systems, 2023. 3, 6, [42] Jiaming Zhang, Ruiping Liu, Hao Shi, Kailun Yang, Simon ReiÃ, Kunyu Peng, Haodong Fu, Kaiwei Wang, and Rainer Stiefelhagen. Delivering arbitrary-modal semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1136 1147, 2023. 6, 7 [43] Rui Zhang, Luziwei Leng, Kaiwei Che, Hu Zhang, Jie Cheng, Qinghai Guo, Jianxing Liao, and Ran Cheng. Accurate and efficient event-based semantic segmentation using adaptive spiking encoderdecoder network. IEEE Transactions on Neural Networks and Learning Systems, 2024. 3, 6 [44] Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Ev-flownet: Self-supervised optical arXiv preprint Kostas Daniilidis. flow estimation for event-based cameras. arXiv:1802.06898, 2018. 3 [45] Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. Unsupervised event-based learning of In Proceedings of optical flow, depth, and egomotion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 989997, 2019. 3, 8 [46] Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. Unsupervised event-based optical flow using motion compensation. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 00, 2018. the British Machine Vision Conference (BMVC), pages 16 1, 2017. 3 [28] Henri Rebecq, Rene Ranftl, Vladlen Koltun, and Davide Scaramuzza. High speed and high dynamic range video with an event camera. IEEE transactions on pattern analysis and machine intelligence, 43(6):19641980, 2019. 6 [29] Frank Rosenblatt. The perceptron: probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386, 1958. 4 [30] Constantin Marc Seibold, Simon ReiÃ, Jens Kleesiek, and Rainer Stiefelhagen. Reference-guided pseudo-label generation for medical semantic segmentation. In Proceedings of the AAAI conference on artificial intelligence, pages 2171 2179, 2022. 1 [31] Jinhuan Shan, Wei Jiang, Yue Huang, Dongdong Yuan, and Yaohan Liu. Unmanned aerial vehicle (uav)-based pavement image stitching without occlusion, crack semantic segmentation, and quantification. IEEE Transactions on Intelligent Transportation Systems, 2024. 1 [32] Shintaro Shiba, Yoshimitsu Aoki, and Guillermo Gallego. Secrets of event-based optical flow. In European Conference on Computer Vision, pages 628645. Springer, 2022. 2, [33] Zhaoning Sun, Nico Messikommer, Daniel Gehrig, and Davide Scaramuzza. Ess: Learning event-based semantic segIn European Conference on mentation from still images. Computer Vision, pages 341357. Springer, 2022. 3, 6, 7 [34] Ziyang Wang and Congying Ma. Dual-contrastive dualconsistency dual-transformer: semi-supervised approach In Proceedings of the to medical image segmentation. IEEE/CVF International Conference on Computer Vision, pages 870879, 2023. 1 [35] Bochen Xie, Yongjian Deng, Zhanpeng Shao, and Youfu Li. Eisnet: multi-modal fusion network for semantic segmentation with events and images. IEEE Transactions on Multimedia, 2024. 3, 6, 7, 8, 1, 2 [36] Chuyun Xie, Wei Gao, and Ren Guo. Cross-modal learning for event-based semantic segmentation via attention soft alignment. IEEE Robotics and Automation Letters, 2024. 6 [37] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in neural information processing systems, 34: 1207712090, 2021. 6, 7, 8, 1 [38] Bowen Yao, Yongjian Deng, Yuhan Liu, Hao Chen, Youfu Li, and Zhen Yang. Sam-event-adapter: Adapting segment In anything model for event-rgb semantic segmentation. 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 90939100. IEEE, 2024. 6, 7 [39] Zhen Yao and Mooi Choo Chuah. light video semantic segmentation. arXiv:2411.00639, 2024. Event-guided lowarXiv preprint [40] Jiaming Zhang, Kailun Yang, and Rainer Stiefelhagen. Exploring event-driven dynamic context for accident scene segmentation. IEEE Transactions on Intelligent Transportation Systems, 23(3):26062622, 2021. 6, 7 [41] Jiaming Zhang, Huayao Liu, Kailun Yang, Xinxin Hu, Ruiping Liu, and Rainer Stiefelhagen. Cmx: Cross-modal fusion 10 Rethinking RGB-Event Semantic Segmentation with Novel Bidirectional Motion-enhanced Event Representation"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 8. Qualitative results on DDD17 dataset [3]. It shows that our model generates more robust and temporally consistent results than the SOTA method. Table 5. Ablation study of our model with different backbones on DSEC dataset [12] Table 6. Ablation study of event bin number on DSEC dataset [12] Backbone MiT-B0 [37] MiT-B2 [37] MiT-B5 [37] Params(M) mIOU Accuracy 68.76 74.94 75.22 94.79 95.85 95. 16.68 37.69 94.93 Method Event Bin mIoU Accuracy BRENet BRENet BRENet BRENet 1 3 15 74.2 74.66 74.67 74.94 95.68 95.81 95.7 95.85 6. Additional Qualitative Results To further validate our model, we present visualization of segmentation predictions on several samples from DDD17 [3] dataset to highlight the robustness of our approach, as shown in Figure. 8. Specifically, we compare the qualitative results of our method with the baseline EISNet [35], utilizing its trained weights. In the predictions generated by EISNet, some small objects are omitted, such as traffic signs in row 1-3 and trees in row 4. This indicates its limitations in accurately recognizing dynamic motions and in reducing blurring effects. In contrast, BRENet captures smaller objects, preserves intricate details, and generates sharper and more precise object boundaries. These samples showcase BRENets ability to tackle challenging scenarios involving rapid motion and complex visual environments, effectively addressing limitations in prior methods. 7. Additional Ablation Study Selection of different backbones. In addition, we validate our model using different backbones, as summarized in Table 5. Specifically, we use MiT-B0, MiT-B2, and MiT-B5 to analyze the impact of varying network capacities on performance. Leveraging more powerful backbones consistently improves the results. Replacing MiT-B0 with MiT-B2 leads to significant mIoU increase of 9.0%. Similarly, replacing MiT-B2 with MiT-B5 yields further mIoU improvement of 0.4%, accompanied by 151.9% increase in model size. By exploying the most powerful backbone MiT-B5, the mIoU achieves 75.22, outperforming SOTA methods by 2.9%. Selection of event bin number. We also perform ablation studies on the DSEC dataset [12] to investigate the impact of different event bin size. Results in Table 6 indicate that increasing likely contributes to learning better temporal dynamics. However, the improvement is modest and not decisive. Therefore, the performance gain isnt mainly from 1 Figure 9. Performance vs. model size on DDD17 dataset [3]. Table 7. Ablation study on DSEC dataset[12] Variant Event Repre. Bi-Propa. BFAM TFM mIoU 1 2 3 4 5 6 7 8 9 10 Voxel Grid AET [35] Optical Flow Temporal Event Features MET MET MET MET MET 71.99 72.36 72.84 72.52 72.95 73.24 73.97 74.27 74.39 74.94 the larger event bin number but from our unique design of MET and subsequent modules. Design Choices on DSEC dataset. We furthermore validate the designs of MET and subsequent modules on the DSEC [12] dataset in Table 7. Variant 6, which employs MET, achieves 73.24% mIoU and outperforms other event representations. To validate the effectiveness of each component, we further evaluate four variants (7-10) of BRENet. Specifically, they are: (i) removing bidirectional propagation, (ii) removing BFAM, (iii) removing TFM and applying concatenation, and (iv) employing all designed components. Variant 10, equipped with all proposed modules, improves SOTA performance by 2.6% mIoU. Bidirectional propagation, BFAM, and TFM contribute 1.3%, 0.9%, and 0.7% mIoU improvements, respectively. From these findings, we draw similar conclusion that combining MET with bidirectional propagation, BFAM, and TFM effectively mitigates misalignments and enhances spatiotemporal coherence. 8. Performance vs. Model Size We further conduct the Performance vs. Model Size analysis on DDD17 dataset [3] in Fig. 9. The results demonstrate that BRENet achieves significant improvements over stateof-the-art (SOTA) methods in segmentation performance while maintaining comparable or slightly increased model size. Specifically, BRENet increases the parameter size by approximately 13M compared to the smallest method, EDCNet-S2D [2], and only 3M more than the most recent approach, EISNet [35]. Despite these modest increases in model size, BRENet achieves remarkable mIoU improvement of 4.7%. This analysis demonstrates BRENets efficiency in balancing performance gains with computational cost."
        }
    ],
    "affiliations": [
        "Lehigh University",
        "Qualcomm AI Research"
    ]
}