{
    "paper_title": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages",
    "authors": [
        "Zeli Su",
        "Ziyin Zhang",
        "Guixian Xu",
        "Jianing Liu",
        "XU Han",
        "Ting Zhang",
        "Yushuang Dong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While multilingual language models like XLM-R have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLM-R, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose a novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models."
        },
        {
            "title": "Start",
            "content": "Multilingual Encoder Knows more than You Realize: Shared Weights Pretraining for Extremely Low-Resource Languages Zeli Su1,2 Ziyin Zhang3 Guixian Xu1,2 Jianing Liu2 XU Han1,2 Ting Zhang1,2 Yushuang Dong1,2 1Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE 2Minzu University of China 3Shanghai Jiao Tong University {rickamorty,guixian_xu,hanxu,jianing_liu,yushuangdong}@muc.edu.cn daenerystargaryen@sjtu.edu.cn tozhangting@126.com Corresponding author 5 2 0 2 5 1 ] . [ 1 2 5 8 0 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While multilingual language models like XLMR have advanced multilingualism in NLP, they still perform poorly in extremely low-resource languages. This situation is exacerbated by the fact that modern LLMs such as LLaMA and Qwen support far fewer languages than XLMR, making text generation models non-existent for many languages in the world. To tackle this challenge, we propose novel framework for adapting multilingual encoders to text generation in extremely low-resource languages. By reusing the weights between the encoder and the decoder, our framework allows the model to leverage the learned semantic space of the encoder, enabling efficient learning and effective generalization in low-resource languages. Applying this framework to four Chinese minority languages, we present XLM-SWCM, and demonstrate its superior performance on various downstream tasks even when compared with much larger models."
        },
        {
            "title": "Introduction",
            "content": "In recent years, with the development of multilingual pretrained models such as XLM-R (Conneau et al., 2020), mBART (Liu et al., 2020), and mT5 (Xue et al., 2021), language models have achieved significant progress in multilingual tasks, especially for high-resource languages. However, low-resource languages like Tibetan, Uyghur, Kazakh, and Mongolianspoken by millions of people in Chinaremain critically underserved. Among these languages, Tibetan has over 10 million speakers, Uyghur over 11 million, Kazakh approximately 3 million, and Mongolian around 7 million, yet their representation in existing multilingual corpora is vastly inadequate. As illustrated in Figure 1, there is significant disparity between the population sizes of these languages and the amount of available data in popular multilingual corpora such as OSCAR (Jansen et al., 2022). The situation 1 Figure 1: The relationship between population size and dataset size in OSCAR (y-axis, in MB) for various high-, middle-, and low-resource languages. is especially dire for Kazakh and Mongolian, with virtually zero usable data, hindering their inclusion in mainstream multilingual models. Despite claims of multilingual support for hundreds of languages, models like mBART and mT5 are not trained on Chinese minority languages. In comparison, more advanced multiglingual large language models such as LLaMA (Touvron et al., 2023) and Qwen (Yang et al., 2024) support even fewer languages. This gap underscores the need for targeted solutions to address the challenges of text generation in extremely low-resource languages. To tackle this challenge, we propose novel framework for efficiently extending multilingual encoder into an encoder-decoder architecture. To address the scarce training data in low-resource languages, we introduce weight-sharing mechanism between the encoder and the decoder by interleaving weights transferred from the encoder with randomly initialized ones, allowing for efficient adaptation to text generation in low-resource settings."
        },
        {
            "title": "Extensive experiments on the aforementioned",
            "content": "Figure 2: An overview of the shared weight framework for efficiently adapting multilingual encoders to text generation in low-resource languages. four Chinese minority languages demonstrate the convincing advantages of our proposed method, with both faster convergence, better generalization, and strong cross-lingual transfer capabilities. Our model, XLM-SWCM (XLM-Shared Weight for Chinese Minorities), outperforms an mBART baseline by up to 199% on text summarization, 108% on reading comprehension, and also bests the much larger MC2-LLaMA 13B (Zhang et al., 2024b) in cross-lingual transfer settings. In summary, the main contributions of this paper are: 1) weight-sharing framework for efficiently adapting multilingual encoders to text generation in low-resource languages; 2) model XLM-SWCM trained with this method for multiple Chinese minority languages; 3) extensive experiments showcasing the superior performance of XLM-SWCM compared with similar-sized baselines and much larger LLMs, confirming the feasibility of our framework. Our code and models will be released upon publication."
        },
        {
            "title": "2.1 Multilingual Corpus",
            "content": "The evolution of multilingual large language models (LLMs) has been enabled by the release of extensive multilingual corpora such as CC100, mC4, OSCAR, CulturaX, and Madlad-400 (Wenzek et al., 2020; Raffel et al., 2019; Jansen et al., 2022; Nguyen et al., 2024; Kudugunta et al., 2023). While these resources cover selection of lowresource languages to some extend, there remains recognized gap in the representation for Chinas minority languages, primarily due to significant differences in writing systems. Chinas minority languages often use different writing systems from the same language family used elsewhere in the world. For example, Uyghur is primarily written in the Arabic script (UEYUyghurche Ereb Yëziqi) in China, with the Latin script (ULYUyghurche Latin Yëziqi) used as supplementary form. In contrast, Uyghur in Russia and Central Asia is written in the Cyrillic script (USYUyghurche Shilir Yëziqi). When collecting data for minority languages, the aforementioned multilingual corpora either do not distinguish between such different writing systems, or only contain data from one system, as shown in Figure 1. Recently, the release of the Multilingual Corpus of Minority Languages in China (MC2, Zhang et al., 2024b) breaks the gap in the availability of Chinese minority language pretraining corpora, covering four underrepresented languages: Tibetan, Uyghur, Kazakh, and Mongolian. This dataset is used as the primary pretraining corpus in our work."
        },
        {
            "title": "Models",
            "content": "In the past few years, multilingual variants of pretrained language models have been proposed in the NLP community, such as mBART (Liu et al., 2020) and mT5 (Xue et al., 2021), supporting up to 100 languages and demonstrating powerful cross-lingual transfer capabilities. More recently, the emergence of large language models (LLMs) has revolutionized multilingual natural language processing. Models like PaLM (Chowdhery et al., 2023) and BLOOM (Scao et al., 2022) have made significant strides in multilingual capabilities, while the LLaMA family (Touvron et al., 2023) and its multilingual variants have democratized access to multilingual LLMs. Some specialized models represented by XGLM and NLLB (Lin et al., 2022; Costa-jussà et al., 2022) have focused on expanding language coverage and improving cross-lingual transfer capabilities across hundreds of low-resource languages. However, few of these models support Chinese minority languages."
        },
        {
            "title": "2.3 NLP for Minority Languages in China",
            "content": "To enhance the accessibility of minority languages in China, prior studies have primarily focused on curating annotated datasets for various NLP tasks. These efforts have mainly concentrated on three 2 3.1.2 Model Architecture Like the vanilla Transformer, the proposed model has two main components: Encoder: pre-trained encoder-only model, specifically CINO, variant of XLM-R enhanced for Chinese minority languages. Decoder: transformer decoder stack with specialized weight transfer mechanism. To balance the knowledge acquired during the encoders largescale pretraining and new knowledge required for downstream generation tasks, we introduce two types of decoder layers: NormalDeocderLayer and CustomDecoderLayer, both maintaining the same hidden dimension, intermediate size, and number of attention heads as the encoder. NormalDecoderLayer: standard transformer decoder layer with randomly initialized weights. It follows conventional architecture with sequential self-attention, cross-attention, and feed-forward network. These layers enable the model to learn generation-specific features from scratch, complementing the knowledge transfered from the encoder. CustomDecoderLayer: modified transformer decoder layer that inherits pre-trained weights from the encoder. It features an enhanced structure with two strategically positioned feed-forward networks: FFN1 between self-attention and cross-attention, and FFN2 following cross-attention, each with its own layer normalization and residual connection, as shown in Figure 3. CustomDecoderLayer inherits all its weights from the pre-trained encoder to reuse learned representations."
        },
        {
            "title": "3.1.3 Weight Sharing Mechanism\nIn our framework, the pre-trained encoder con-\nsists of only self-attention and feed-forward blocks,\nwhile the decoder layers require both self-attention\nand cross-attention mechanisms for effective gen-\neration. Thus, special schemes are designed to\ninitialize and reuse the weights, as shown in Fig-\nure 3.",
            "content": "For weight initialization of CustomDecoderLayers, weights of both self-attention and crossattention in the decoder are initialized from the encoders self-attention blocks. Similarly, weights of both two FFN blocks in decoder layer are initialized from the FFN block in the corresponding encoder layer. This mechanism reduces the effective number of parameters to be learned, accelerating convergence and enabling effective transfer of Figure 3: The weight initialization schemes for the CustomDecoderLayer. The colored arrows indicate the initialization of weights between the different components. key task categories: text classification (Qun et al., 2017; Sun et al., 2021a; Shi et al., 2023), question answering (Sun et al., 2021b), and machine translation (Zhang et al., 2024a). Prominent models specifically trained for these languages include CINO (Yang et al., 2022), MiLMo (Deng et al., 2023), and TiBert (Liu et al., 2022). However, despite such progress, none of these models have released their pre-training corpora, and there is still notable gap in the availability of models capable of text generation in these languages."
        },
        {
            "title": "3.1.1 Framework Overview",
            "content": "In this section, we introduce the Shared Weights Framework, which leverages shared weights between the encoder and decoder for efficiently adapting multilingual encoders to text generation in lowresource languages. The overall pipeline is visually summarized in Figure 2. Starting from CINO (Yang et al., 2022), continual-pretrained version of XLM-R for Chinese minority languages, we copy its weight to initialize the decoder layers for knowledge transfer, and tie some of the weights between encoder and dedocer to enable efficient training. This model, which we name XLM-SWCM, is pretrained on the MC2 corpus and then applied to downstream tasks, including both single-language finetuning and cross-lingual transfer. 3 linguistic knowledge from the pre-trained encoder while maintaining model stability. key architectural decision in our framework is the insertion pattern of these layers. After every CustomDecoderLayers, we insert one NormalDecoderLayer, so that an encoder with layers would correspond to decoder with + n/X layers. The value of significantly impacts the models generalization capabilities, and its optimal value varies across different model scales. Through extensive experimentation, we find that = 3 yields the best performance, and detailed analysis of how this choice affects the models performance is discussed in Section 5.2.3. 3.2 Pretraining 3.2.1 Pretraining Tasks We adopte multi-task training approach for pretraining. The primary task involves self-supervised learning using mBARTs denoising auto-encoding (DAE) strategy. This strategy helps with the models transition from the encoders word-level cloze tasks to sequence generation tasks by predicting the masked portions of the input sequence with decoder. Additionally, we incorporate machine translation as an auxiliary objective, particularly focusing on translation between Mandarin Chinese and various Chinese minority languages. Specifically, the training data includes bidirectional translation pairs between Mandarin Chinese and the minority languages. This auxiliary objective improves the models cross-lingual transfer capability, thereby enhancing the models performance in various lowresource language processing tasks."
        },
        {
            "title": "3.2.2 Training Data\nTHUCNews (THU-NLP Group, 2016) is a Chinese\nnews dataset, derived from historical data from the\nSina News RSS feed between 2005 and 2011 and\ncontaining approximately 740,000 news articles.\nFrom this dataset, we extracted a subset of Simpli-\nfied Chinese news articles.",
            "content": "MC2 (Zhang et al., 2024b) provides multilingual data for several Chinese minority languages, including Tibetan, Uyghur, Kazakh, and Mongolian. The specific data volumes are described in detail in Appendix A. Together with THUCNews, these monolingual datasets serve as training data for the DAE task. For machine translation, we leveraged Google Translate to create bidirectional translation pairs between Chinese and the minority languages (Tibetan, Uyghur, Kazakh, and Mongolian). These translations were verified by native speakers to ensure accuracy. total of 2,000 sentence pairs from each language pair were selected to form the supplementary training data. Combining these three corpora, the integrated dataset allows the model to effectively handle both high-resource and low-resource languages, improving its cross-lingual transfer and multilingual capabilities."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Pretraining Training Configuration The models are trained for 8 epochs with peak learning rate of 1e-4, AdamW (Loshchilov and Hutter, 2019) optimizer, global batch size 600, and linear learning rate scheduler with warmup proportion of 0.1. The maximum sequence length is set to 256 tokens, and mixed-precision is enabled to optimize memory usage and training efficiency. To ensure training stability, the norms of gradients are clipped to 1.0. The models are trained on two NVIDIA A800 GPUs, each with 80GB of memory, and the training process takes 92 hours. Balanced Sampling Strategy To address the inherent data imbalance across different languages, we implemente balanced sampling strategy similar to XLM-R. The sampling probability for each language is calculated as pi = qα qα (cid:80) , (1) where qi represents the original proportion of language in the dataset, and α (set to 0.3) is smoothing parameter that balances between uniform sampling and size-proportional sampling. This approach ensures that low-resource languages receive adequate representation in the training process while maintaining the influence of larger datasets. Model Adaptations We extende the models vocabulary with special language tokens (<bo>, <kk>, <mn>, <ug>, <zh>) to handle our target languages (Tibetan, Kazakh, Mongolian, Uyghur, and Chinese). These language identifiers are directly added after the bos token <s> in the model inputs. This modification ensures that the model can effectively process and distinguish between different languages during both pre-training and downstream 4 task finetuning. The same approach is consistently applied in all subsequent experiments. Based on the aforementioned settings, we trained new seq2seq model - XLM-SWCM, utilizing CINO-base-v2 as the encoder, with 457 million parameters. The detailed architectural configuration is provided in Appendix B. 4.2 Downstream Tasks 4.2.1 Experiment Setting To evaluate the capabilities of XLM-SWCM, we conduct fine-tuning experiments on three downstream tasks in both low-resource and highresource languages: Text Summarization, Machine Reading Comprehension (MRC), and Machine Translation. These tasks are chosen to cover diverse areas of text generation in NLP. Single-Language Fine-tuning Due to the scarcity of labeled data for low-resource languages, we focus primarily on Tibetan for single-language fine-tuning, which has several publicly available datasets: - Text Summarization: For this task, we utilize the Ti-Sum dataset (Xiaodong, 2022) with 20,000 pairs of titles and articles. - MRC: We mainly use the TibetanQA dataset (Sun et al., 2022) for this task, which claims to contain 20K examples. However, only 2K examples are publicly available. Thus we enrich it by integrating 5K examples from the TibetanSFT Corpus1 and 3K examples translated from Chinese MRC dataset (Cui et al., 2019a) using Google Translate. This approach enables us to create comprehensive dataset consisting of 10K examples. - Machine Translation: For Machine Translation, we also use the TibetanSFT Corpus, which is cleaned to generate 50,000 parallel ChineseTibetan sentence pairs. Cross-lingual Transfer In addition to singlelanguage fine-tuning, we also conduct cross-lingual transfer experiments to test XLM-SWCMs ability to generalize across multiple low-resource languages. This experiment aims to assess the models performance in Tibetan, Uyghur, Mongolian, and Kazakh after being fine-tuned on high-resource language (Simplified Chinese) and very small number of samples in the target languages. - Text Summarization: For Mandarin Chinese, we use the publicly available LCSTS dataset (Hu 1https://huggingface.co/datasets/shajiu/ParallelCorpusSFT et al., 2015), which contains 100K samples scraped from various Chinese portals. For the four minority languages, approximately 3K cleaned samples per language are scraped from language-specific news portals, using the news titles as their summarization. - MRC: For Chinese, we employ the CMRC 2018 dataset (Cui et al., 2019b), which consists of 10K samples. For Tibetan, we use 500 samples extracted from the publicly available TibetanQA dataset. For the other three minority languages (Uyghur, Mongolian, Kazakh), we utilize machine translation tools to translate and clean MRC data, ultimately selecting 500 samples per language. Baseline Models We employ two baseline models to ensure broad coverage and robust performance in handling Chinese minority languages. The first model builds upon LLaMA2-Chinese and is fine-tuned on the MC2 dataset, resulting in the MC2-LLaMA-13B model. The second baseline, referred to as mBART-CM, is an adaptation of mBART-cc25. Its vocabulary is expanded to include tokens specific to our minority languages, followed by further pretraining on MC2. Training settings Both XLM-SWCM and mBART-CM are sequence-to-sequence models that are fine-tuned using standard training configurations. Each of these models is trained for 50 epochs with batch size of 200 samples to ensure comprehensive learning and optimal performance. MC2-LLaMA-13B model is trained using LoRA (Hu et al., 2022) with rank of 8 for 3 epochs."
        },
        {
            "title": "4.2.2 Experimental Results",
            "content": "As illustrated in Table 1, XLM-SWCM consistently outperforms the baseline models across all three tasks. Despite having fewer parameters, XLMSWCM demonstrates substantial margin of superiority over mBART-CM and even surpasses the much larger MC2-LLaMA-13B. Notably, XLM-SWCM achieves an impressive 198.8% improvement in F1-score for Text Summarization over mBART-CM, along with significant 107.6% F1 improvement in MRC. These remarkable gains are direct result of XLMSWCMs efficient weight sharing framework to maximize the utilization of pre-trained encoder features in resource-constrained scenarios. Even under equivalent seq2seq structures and identical training corpora, XLM-SWCM demonstrates greater 5 Model Size Sum MRC MT F F MC2-LLaMA-13B mBART-CM XLM-SWCM (ours) 16.1 13B 611M 8.6 457M 25.7 12.3 11.2 29.1 15.5 15.2 24.2 13.2 7.9 16.4 11.7 6.1 29.5 13.1 5.6 16. 15.1 11.5 24.5 12.2 7.3 26.3 16.8 9.3 24.3 Table 1: Performance metrics of the baseline models, evaluated using three ROUGE-L sub metrics: (F1-score), (precision), and (recall). Size refers to the number of parameters in each model. Model Zh Bo Ug Mn Kk Sum MRC Sum MRC Sum MRC Sum MRC Sum MRC MC2-LLaMA-13B MC2-LLaMA-13B* mBART-CM XLM-SWCM (ours) 47.1 47.3 32.7 33.1 43.5 44.7 25.6 23.5 9.5 13.1 6.8 17.1 6.1 11.5 2.1 11.1 3.5 11.7 2.7 12.5 2.4 10.1 2.2 11. 3.7 9.7 3.1 13.5 2.2 10.2 1.7 7.2 2.6 2.9 0.2 5.6 3.9 4.6 0.1 6.9 Table 2: Cross-lingual Transfer performance of different models on Text Summarization (Sum) and Machine Reading Comprehension (MRC) tasks, evaluated using ROUGE-L. The best results for each task are highlighted. * indicates explicitly prompting MC2-LLaMA-13B with the language to be used in the response during evaluation. efficiency and learning capacity. 13B*. In comparison to MC2-LLaMA-13B, which benefits from richer pretraining corpora and largerscale parameters, XLM-SWCM achieves 59% higher F1-score in Text Summarization, 24.1% F1 improvement in MRC, and 62.3% higher F1-score in MT. These results underscore the effectiveness of XLM-SWCMs shared weight framework in resource-constrained environments, making it superior choice for tasks involving Chinese minority languages. Table 2 highlights the performance of XLMSWCM and baseline models in cross-lingual transfer settings. For the primary source language (Zh), the baseline models demonstrate better performance, which stems from their larger parameter sizes and more extensive pretraining corpora in Simplified Chinese. However, when it comes to generalization to minority languages, XLMSWCM showcases exceptional adaptability, significantly outperforming the baseline models. mBARTCM, for instance, struggles to distinguish between languages and often defaults to outputs in the primary language (Zh), even when language-specific labels are present. Similarly, MC2-LLaMA-13B exhibits language-related errors, though its performance improves when explicitly informed of the current language type, as seen with MC2-LLaMAIn Text Summarization, XLM-SWCM outperforms all baselines. Specifically, XLM-SWCM improvements of 30.5%, achieves significant 6.8%, and 39.1% for Tibetan (Bo), Uyghur (Ug), and Mongolian (Mn) respectively over MC2LLaMA-13B*, the best-performing baseline. For MRC, XLM-SWCM also demonstrates competitive performance across most languages, being only slightly weaker than MC2-LLaMA-13B* for Tibetan and Mongolian. Overall, these experiments indicate that XLMSWCM can effectively leverage the shared weight mechanism to maximally reuse the semantic space of the pre-trained encoder, demonstrating excellent performance in Chinese minority language applications with limited data and parameter size."
        },
        {
            "title": "5 Ablation Studies",
            "content": "In this section, we present series of ablation experiments aimed at evaluating the impact of key components in our framework that play essential roles in enhancing the models multilingual capabilities and improving its generalization to low-resource languages. We perform ablation experiments on the Tibetan finetuning tasks, maintaining consistent finetuning setting with Section 4.2.1."
        },
        {
            "title": "Sum MRC MT",
            "content": "els, especially in resource-constrained scenarios. None (XLM-SWCM) MT DAE WS MT + DAE MT + WS DAE + WS MT + DAE + WS 25.7 25.6 22.4 17.1 22.5 17.5 15.2 15.9 16.4 15.1 12.2 11.7 12.3 11.3 11.9 10.8 24.5 20.3 18.7 18.2 17.7 18.4 17.1 16.5 Table 3: Objective ablation results, evaluated using ROUGE-L. The experiments involve removing different combinations of training components, such as Machine Translation (MT), DAE (Denoising Auto-Encoding), and Weight Sharing (WS)."
        },
        {
            "title": "5.1 Objective Ablation",
            "content": "We first focus on three critical aspects of the model: DAE pretraining, machine translation, and weight initialization by removing each and combinations of them. The results are shown in Table 3. Removing any of the three components is detreimental to performance, specifically: - Machine Translation (MT): Removing machine translation has relatively small impact on performance across tasks, as shown by both individual removal (maintaining 25.6 in Sum) and combined removals (MT+DAE vs DAE showing similar scores); - Denoising Auto-Encoding (DAE): The removal of DAE pretraining causes considerable performance drops across all three downstream tasks, and its impact becomes more pronounced in combined removals (DAE+WS), indicating its fundamental importance in establishing the models basic text generation capabilities. - Weight Sharing (WS): The removal of weight sharing demonstrates the most significant impact among all modules, showing the largest performance drops in individual removal and maintaining this substantial negative effect across all combined removal scenarios, establishing it as the most crucial component for the models effectiveness in low-resource settings. In short, while all three components contribute positively to the models performance, weight sharing emerges as the most critical component. This finding highlights the importance of weight sharing as key architectural choice for multilingual mod5.2 Structure Ablation We also perform experiments to evaluate the impact of different structural components in our proposed framework. These experiments aim to understand how the initialization of decoder weights and the insertion of normal layers affect model performance. 5.2.1 Impact of Weight Initialization Firstly, we train baseline model called CinoTransformer. Unlike XLM-SWCM, the decoder of this model is randomly initialized, and also matches the number of encoder layers. The model is pretrained using the same DAE and MT tasks as XLM-SWCM but without weight sharing, and then finetuned on downstream tasks in the same setting as XLM-SWCM."
        },
        {
            "title": "Sum MRC MT",
            "content": "Cino-Transformer XLM-SWCM (ours) 18.9 25.7 13.5 16.4 18.7 24.5 Table 4: Performance metrics of the Ablation of Weight Initialization, evaluated using the ROUGE-L metric."
        },
        {
            "title": "Sum MRC MT",
            "content": "BASE-A BASE-B XLM-SWCM (ours) 13.7 16.3 25.7 10.3 14.1 16.4 15.7 21.1 24.5 Table 5: Performance metrics of the Ablation of Normal Layers, evaluated using the ROUGE-L metric. BASE-A has fewer layers and does not include any normal layers, while BASE-B maintains the same number of layers as XLM-SWCM but uses weight duplication instead of normal layers. The results in Table 4 demonstrate the effectiveness of our weight initialization scheme. By transferring weights from the encoder to the decoder, XLM-SWCM can be efficiently adapted to text generation with limited training data, outperforming Cino-Transformer on all tasks."
        },
        {
            "title": "5.2.2\nSecondly, we explore the impact of inserting nor-\nmal layers among the custom layers in the decoder.\nTo assess the effectiveness of this modification, we\nuse two baseline models for comparison:",
            "content": "7 - Baseline (XLM-SWCM without normal layers): This model is identical to XLM-SWCM but without any normal layers inserted into the custom layer architecture. The absence of normal layers leads to reduced total number of layers in the decoder. - Baseline (Weight duplication model): Instead of inserting normal layers, this model simply copies the weights of the preceding layer to maintain consistency in the number of model parameters. This results in identical weights across consecutive layers, allowing us to isolate the impact of inserting randomly initialized normal layers. The results in Table 5 demonstrate the significant impact of inserting normal layers into the decoder. BASE-A, which has fewer layers, performs the worst across all tasks. BASE-B, which maintain the same number of layers as XLM-SWCM but lacks randomly initialized weights, shows some improvement but still underperforms. Overall, these findings indicate that randomly initialized normal layers is also crucial for adapting encoders to text generation. 5.2."
        },
        {
            "title": "Impact of Insertion Frequency of\nNormal Layers",
            "content": "Thirdly, we thoroughly investigate the impact of insertion frequency of normal layers in the decoder, and how this interacts with varying dataset sizes. This experiment is designed along two dimensions: - Insertion Frequency of Normal Layers: we explore values of where normal layer is inserted after every custom layers, with ranging from 1 to 6. All these models are pretrained in the same setting as XLM-SWCM. - Effect of Finetuning Dataset Size: we evaluate the models performance on datasets of varying sizes, including 10K, 20K, and 50K samples. As the existing Ti-SUM dataset only has 20K samples, we supplement it by crawling and cleaning 30K additional news articles from various major Chinese websites. This dimension allows us to examine the interaction between the amount of available data and the frequency of normal layers. The results are plotted in Figure 4: - For the small dataset (10k), larger results in better performance, as smaller decoders generalize more effectively when data is limited. In contrast, smaller (i.e. larger decoders) leads to overfitting. - For the medium dataset (20k), performance peaks at = 3. This indicates that moderate decoder size strikes balance between capacity and Figure 4: ROUGE-L scores on Tibetan summarization for different X-values (insertion frequency of normal layers). The three lines correspond to different dataset sizes. data availability. - For the large dataset (50k), smaller achieve the highest F1-scores, as the larger decoder capacity enables the model to fully exploit the larger dataset. Overall, these results demonstrate the flexibility of our framework, where the insertion frequency of normal layers can be adjusted based on the taskspecific dataset size. Larger (fewer layers) is better suited for small datasets, while smaller (more layers) performs best on larger datasets."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we proposed novel pretraining framework tailored for low-resource languages, with particular focus on Chinese minority languages. Our framework leverages shared weight mechanism between the encoder and decoder, which allows for the efficient adaptation of multilingual encoders to generation tasks without the need to start from scratch. Experimental results demonstrate that our model XLM-SWCM significantly outperforms traditional baselines on various text generation tasks for Tibetan, Uyghur, Kazakh, and Mongolian, which have long been underserved in NLP research. Our approach opens up new possibilities for developing robust models for these extremely low-resource languages, and also provides promising method for the integration of resources across similar languages."
        },
        {
            "title": "7 Limitations",
            "content": "Due to the availability of pretrained language models for Chinese minority languages and high-quality corpora, our study focused on only four minority languages. Our single-language finetuning experiments are further constrained to Tibetan given the lack of relevant datasets, limiting the scope of our exploration. Thus, we hope that future work will put more focus on the development of high-quality datasets in these minority languages and beyond, enabling more thorough exploration of underrepresented languages in the LLM era."
        },
        {
            "title": "References",
            "content": "Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. 2015. Scheduled sampling for sequence In Adprediction with recurrent neural networks. vances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 11711179. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. Palm: Scaling language modeling with pathways. J. Mach. Learn. Res., 24:240:1 240:113. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 84408451. Association for Computational Linguistics. Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Y. Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling human-centered machine translation. CoRR, abs/2207.04672. Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. 2019a. span-extraction dataset for Chinese maIn Proceedings of chine reading comprehension. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 58865891, Hong Kong, China. Association for Computational Linguistics. Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. 2019b. span-extraction dataset for Chinese maIn Proceedings of chine reading comprehension. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 58865891, Hong Kong, China. Association for Computational Linguistics. Junjie Deng, Hanru Shi, Xinhe Yu, Wugedele Bao, Yuan Sun, and Xiaobing Zhao. 2023. Milmo: Minority multilingual pre-trained language model. In IEEE International Conference on Systems, Man, and Cybernetics, SMC 2023, Honolulu, Oahu, HI, USA, October 1-4, 2023, pages 329334. IEEE. Baotian Hu, Qingcai Chen, and Fangze Zhu. 2015. LCSTS: large scale chinese short text summarization dataset. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 19671972. The Association for Computational Linguistics. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Tim Jansen, Yangling Tong, Victoria Zevallos, and Pedro Ortiz Suarez. 2022. Perplexed by Quality: Perplexity-based Method for Adult and Harmful Content Detection in Multilingual Heterogeneous Web Data. arXiv e-prints, arXiv:2212.10440. Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Christopher A. Choquette-Choo, Katherine 9 Lee, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. 2023. Madlad-400: multilingual and document-level large audited dataset. Preprint, arXiv:2309.04662. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian OHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot learning with multilingual generative language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 90199052. Association for Computational Linguistics. Sisi Liu, Junjie Deng, Yuan Sun, and Xiaobing Zhao. 2022. Tibert: Tibetan pre-trained language model. In IEEE International Conference on Systems, Man, and Cybernetics, SMC 2022, Prague, Czech Republic, October 9-12, 2022, pages 29562961. IEEE. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pretraining for neural machine translation. Trans. Assoc. Comput. Linguistics, 8:726742. Ilya Loshchilov and Frank Hutter. 2019. Decoupled In 7th International weight decay regularization. Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2024. CulturaX: cleaned, enormous, and multilingual dataset for large language models in 167 languages. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 4226 4237, Torino, Italia. ELRA and ICCL. Nuo Qun, Xing Li, Xipeng Qiu, and Xuanjing Huang. 2017. End-to-end neural text classification for tiIn Chinese Computational Linguistics and betan. Natural Language Processing Based on Naturally Annotated Big Data - 16th China National Conference, CCL 2017, - and - 5th International Symposium, NLP-NABD 2017, Nanjing, China, October 13-15, 2017, Proceedings, volume 10565 of Lecture Notes in Computer Science, pages 472480. Springer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with unified text-to-text transformer. arXiv e-prints. Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. 2022. BLOOM: 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Yuan Sun, Zhengcuo Dan, Sisi Liu, and et al. for mav3. Sci2022. chine CSTR:31253.11.sciencedb.j00001.00351. ence Data Bank, accessed 2025-01-02. Tibetan dataset comprehension[ds/ol]. Tibetanqa: reading Yuan Sun, Sisi Liu, Chaofan Chen, Zhengcuo Dan, and Xiaobing Zhao. 2021a. Teaching machines to read and comprehend tibetan text. Journal of Computer and Communications, 9(09):143152. Yuan Sun, Sisi Liu, Chaofan Chen, Zhengcuo Dan, and Xiaobing Zhao. 2021b. Teaching machines to read and comprehend tibetan text. Journal of Computer and Communications, 9(09):143152. THU-NLP Group. 2016. THUCNews: Chinese News Dataset from Sina News RSS (2005-2011). Technical report, Tsinghua University. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, 10 Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288. Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 40034012, Marseille, France. European Language Resources Association. wards transparent and culturally-aware NLP for miIn Proceedings of the nority languages in china. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 88328850. Association for Computational Linguistics. Yan Xiaodong. 2022. CSTR:31253.11.sciencedb.j00001.00352. ence Data Bank, accessed 2025-01-02. Ti-sum[ds/ol]. v3. SciLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 483498. Association for Computational Linguistics. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671. Ziqing Yang, Zihang Xu, Yiming Cui, Baoxin Wang, Min Lin, Dayong Wu, and Zhigang Chen. 2022. CINO: chinese minority pre-trained language In Proceedings of the 29th International model. Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October 12-17, 2022, pages 39373949. International Committee on Computational Linguistics. Chen Zhang, Xiao Liu, Jiuheng Lin, and Yansong Feng. 2024a. Teaching large language models an unseen language on the fly. In Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 87838800. Association for Computational Linguistics. Chen Zhang, Mingxu Tao, Quzhe Huang, Jiuheng Lin, Zhibin Chen, and Yansong Feng. 2024b. Mc2: To-"
        },
        {
            "title": "A Dataset Details",
            "content": "For pretraining of XLM-SWCM and other baseline models, we used combination of Simplified Chinese data from THUCNews and minority languages from MC2. The breakdown of their distribution is given in Table 6. Language Data Size Tibetan Uyghur Kazakh Mongolian Simplified Chinese 2.2 GB 736 MB 937 MB 970 MB 2.1 GB Number of Samples 184,045 90,441 57,827 171,847 836,075 Table 6: Statistics of our pretraining dataset."
        },
        {
            "title": "B Training Details",
            "content": "In addition to the settings presented in the main paper, here we detail other parameters used during pre-training XLM-SWCM for complete reproduction:"
        },
        {
            "title": "Hardware and Software Configuration",
            "content": "- Hardware: NVIDIA Tesla A800 GPU, 80 GB RAM * 2, Intel i7 CPU. - Software: Ubuntu 20.04, CUDA 11.7, PyTorch 2."
        },
        {
            "title": "Training Configurations",
            "content": "- Total Training Samples: 1,340,235 - Local Batch Size: 75 - Gradient Accumulation Steps: 4 - Global Batch Size: 600 - Epochs: 8 - Total Training Steps: 17,864 - Optimizer: AdamW with β1 = 0.9, β2 = 0.999 - Learning Rate: 1e-4 - Warm-up: Linear warm-up for the first epoch, gradually increasing the learning rate from 1e-5 to 1e-4. - Scheduled Sampling: In the first epoch, teacher forcing is applied to guide the model. Subsequently, the teacher forcing ratio is gradually decreased in linear fashion, transitioning to scheduled sampling (Bengio et al., 2015)."
        }
    ],
    "affiliations": [
        "Key Laboratory of Ethnic Language Intelligent Analysis and Security Governance of MOE",
        "Minzu University of China",
        "Shanghai Jiao Tong University"
    ]
}