{
    "paper_title": "Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment",
    "authors": [
        "Chenghao Fan",
        "Zhenyi Lu",
        "Sichen Liu",
        "Xiaoye Qu",
        "Wei Wei",
        "Chengfeng Gu",
        "Yu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose \\underline{G}reat L\\underline{o}R\\underline{A} Mixture-of-Exper\\underline{t} (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 4 9 8 6 1 . 2 0 5 2 : r Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment Chenghao Fan * 1 Zhenyi Lu * 1 Sichen Liu 1 Xiaoye Qu 1 Wei Wei 1 Cheng Yu Abstract While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose Great LoRA Mixture-ofExpert (GOAT), framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full finetuned MoE by deriving theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoEs efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOATs state-of-the-art performance, closing the gap with Full FT. 1. Introduction Recent large language models (LLMs) have shown impressive capabilities (Dai et al., 2024; Touvron et al., 2023; Yang et al., 2024; OpenAI et al., 2024), but fine-tuning them for downstream tasks is computationally expensive (Hu et al., 2021; Zhao et al., 2024). To reduce costs, parameterefficient fine-tuning (PEFT) techniques (Hu et al., 2021; Pfeiffer et al., 2021; Houlsby et al., 2019; Tian et al., 2024) have been proposed. Among them, LoRA (Hu et al., 2021) is popular for its simplicity and effectiveness. It reparame- *Equal contribution 1School of Computer Science & Technology, Huazhong University of Science and Technology 2The Chinese University of Hong Kong. Preprint. Under review. terizes the weight matrix Rmn into = W0 + BA, where W0 Rmn is frozen full-rank matrix, and Rmr,A Rrn are low-rank adapters to be learned. Since the rank min(m, n), LoRA only updates small fraction of the parameters, greatly reducing memory usage. Despite its computational efficiency, LoRA often underperforms full fine-tuning (Full FT) (Wang et al., 2024d;c; Fan et al., 2024), even with Mixture-of-Experts (MoE) architectures (Zadouri et al., 2024; Liu & Luo, 2024; Tian et al., 2024). Our rigorous analysis identifies two key factors limiting LoRAs performance: (1) Suboptimal Initialization: The isotropic random initialization for matrix and zero initialization for matrix provide non-informative prior, resulting in unguided optimization subspaces. While Wang et al. (2024b); Meng et al. (2024) applied singular value decomposition (SVD) for better initialization, their reliance on static, predefined subset of pre-trained weights limits the capture of the full range of pre-trained knowledge. It raises the question: Can we adaptively integrate relevant priors of pre-trained knowledge based on input? (2) Unaligned Optimization: Furthermore, the intrinsic low-rank property of LoRA leads to large gradient gaps and slow convergence in optimization, therefore underperforming Full FT. In LoRA MoE scenarios, the total rank is split among experts, resulting in lower ranks and further increasing this challenge. Existing strategies (Wang et al., 2024c;d) focus only on single LoRA architectures and ignore the added complexity of random top-k routing and multiple expert weights within MoE architecture. When SVD-based initialization is applied to LoRA MoE, weight alignment becomes challenge, which has never been considered in previous methods that used zero initialization. This further raises the question: How do we mitigate the optimization gap in LoRA MoE initialized with prior information? To address these challenges, we propose GOAT (Great LoRA Mixture-of-Experts), which employs an SVDstructured MoE with theoretical scaling to match full finetuning performance. Our method highlights two important innovations. (1) Initialization: We demonstrate that different segments of pre-trained knowledge in the SVD structure are crucial depending on the input. To capture this adaptively, we propose initializing LoRA MoE experts Make LoRA Great Again with distinct singular value segments, with the router selecting the appropriate prior information. (2) Optimization: Rather than directly targeting the gap with full fine-tuning, we focus on an upcycled MoE 2 with full-rank fine-tuning. We show that when each low-rank expert plus pre-trained weight approximates its full-rank counterpart, the routers behavior remains consistent, enabling effective optimization of expert weights. Through simple scaling, without altering architecture or algorithms, we significantly improve both convergence speed and performance. We also derive the optimal weight alignment strategy and theoretical scaling scheme for better gradient alignment. In summary, the contributions of our method are as follows: Adaptive Priors Initialization: We propose novel SVDstructured MoE framework that adaptively integrates pretrained knowledge, addressing the limitations of noninformative or static priors. Theoretical Optimization Alignment: We reveal key connection between LoRA and full fine-tuning upcycled MoE, deriving an optimal weight alignment strategy and scaling scheme to close the performance gap. State-of-the-Art Performance: Extensive experiments on 25 tasks demonstrate that our method achieves superior performance while maintaining scalability. 2. Background and Motivation 2.1. Rethinking Singular-Value Initialization Singular-value initialization is widely used in LoRA to preserve pre-trained weight characteristics (Zhao et al., 2024; Meng et al., 2024; Wang et al., 2024a; Lu et al., 2024). PiSSA (Meng et al., 2024) only updates the largest singular values, while MiLoRA (Wang et al., 2024b) adjusts minor singular values for strong performance. To unify SVD-based methods with full fine-tuning, let W0 Rmn be the pre-trained weight with SVD, W0 = ΣV . Assuming = min(m, n) and LoRA rank r, we decompose W0 into rank-r blocks: W0 = (cid:88) i=0 UiΣiV , (1) where = 1 and denotes the segment [i : (i + 1) r]. The submatrices are defined as Ui = U[ir:(i+1)r,:] Rrm, Σi = Σ[ir:(i+1)r,ir:(i+1)r] Rrr, and Vi = V[ir:(i+1)r,:] Rrn. Fine-tuning methods are represented as: 2Upcycled MoE initializes all experts with the same pre-trained weights, which we adopt for simplicity. Figure 1. The effect of initializations from different SVD segments (ui, σi, ) for rank 32 and 128. The performance normalized by min-max scaling. MiLoRA : 1 + + UlΣlV 1 + + UlΣlV Full FT : U0Σ0V PiSSA : U0Σ0V (U0Σ0V (U0Σ0V 0 + U1Σ1V 0 + (U1Σ1V 0 + + Ul1Σl1V 0 + + Ul1Σl1V ) l1) + UlΣlV l1) + rΣrV (2) Here, () denotes frozen components, while non-frozen components initialize LoRA: KaSA : = UiΣ1/ Rmr, = Σ1/2 Rrn. (3) We observe PiSSA freezes minor singular values and finetunes only the components U0Σ0V 0 with the largest norms, achieving the optimal approximation to W0.3 In contrast, MiLoRA and KaSA retain segment 0 (l 1) as preserved pretrained knowledge, but KaSA treats the minor UlΣlV as noise and replaces it with new random rΣrV r. In practice, PiSSA converges faster by focusing on principal singular values, while MiLoRA and KaSA preserve more pre-trained knowledge for better final performance. This raises the question: Is it reasonable to use only the principal or minor part as fine-tuning prior? Figure 1 illustrates the performance of fine-tuning from different segments (Ui, Σi, ), [0, , l], where each segment is used for initialization while others remain frozen. The x-axis represents segment indices (e.g., = 0 for PiSSA, = for MiLoRA), and the y-axis shows min-max normalized performance. We can identify two notable observations: (1) The same initialization exhibits varying trends for different datasets. For example, = achieves better results on the EuroSAT dataset, while = 0 performs better on the GTSRB dataset. (2) Middle segments play crucial role. e.g., when = 128, the highest performance is typically observed in the middle segments. These findings suggest that each singular value segment contains task-specific information, motivating us to allow the model to automatically select segments during optimization, leveraging all singu3Proof in Appendix 2 Make LoRA Great Again is too small. Increasing the scaling factor in SVD-based methods boosts the gradient, leading to faster convergence. Next, we examine the effect of different ranks, as shown in Figure 2. With low rank (e.g., = 1), the gradient norm is small and deviates from the trend of = 64, creating performance gap (95.77 vs. 98.55). However, applying proper scaling (s = 16) increases the gradient norm, reducing the performance gap (from 95.77 to 97.70). This is especially beneficial in MoE scenarios, where the total rank is split among experts, resulting in lower ranks. Increased scaling can compensate for this, as supported by Tian et al. (2024). 3. Method 3.1. LoRA MoE Architecture Mixture-of-Experts (MoE) An MoE layer (Qu et al., 2024; Zhu et al., 2024a;b; Zhang et al., 2024) comprises linear modules {W1, . . . , WE} and router Wz RmE that assigns input to experts based on routing scores: pi(x) = exp(zi(x)) j=1 exp(zj(x)) (cid:80)E , (9) Figure 2. SVD initialization vs. scaling and rank lar values while preserving the original pre-trained matrix characteristics. 2.2. Rethinking Scaling Factor In LoRA, it is common practice to use the scaled variant = W0 + sBA, yet the effects of scaling factor have not been fully explored. Biderman et al. (2024) consider should typically set to 2. The SVD-based method (Meng et al., 2024) empirically makes sBA independent of by , while Tian et al. (2024) use larger dividing and by scaling for LoRA MoE to achieve better performance. (cid:113) 1 To investigate it, as illustrated on the left of Figure 2, we first adjust in the SVD-based LoRA with fixed rank, revealing that still impacts the convergence speed. To study the effect, we introduce the equivalent weight and gradient to quantify the gap between LoRA and Full FT. Definition 2.1 (Equivalent Weight and Gradient). For LoRA optimization, we define the equivalent weight as: where z(x) = Wzx and pi(x) is the score for expert i. Let Ωk(x) denote the indices of the top-k scores, ensuring Ωk(x) = and zi(x) > zj(x) for all Ωk(x) and / Ωk(x). Define the weights as: wi(x) = (cid:40) (cid:80) 0, exp(zi(x)) jΩk (x) exp(zj (x)) , if Ωk(x), otherwise. (10) + sBA, The equivalent gradient of is defined as: L (4) (5) The MoE layer output is the weighted sum of the top-k experts outputs: MoE(x) = (cid:88) i=1 wi(x)W i(x). (11) where is the scaling factor, and GA and GB are gradients with respect to and B, respectively. Lemma 2.2. Let gt be the gradient in full-tuning, and B, be the low-rank weights. At the t-th optimization step, the equivalent gradient can be expressed as: gt = s2 (cid:16) BtBt gt + gtAt At (cid:17) The formula for SVD-based initialization is: sBA = (cid:19) UrΣrV (cid:18) 1 = UrΣrV g = s2 (cid:18) 1 UrU + gVrV (cid:19) (cid:16) = 1 UrU + gVrV (6) (7) (cid:17) (8) Though the equivalent weight is independent of s, equivalent gradient is proportional to s. As shown in Figure 2, = 2 LoRA MoE. We integrate LoRA into the MoE framework, retaining the router (Equation (10)) and using the balance loss from vanilla MoE4. Each expert is replaced by low-rank matrices Bi Rmd and Ai Rdn, where = : MoELoRA(x) = (x) + (cid:16) wi(x) sBiAi(x) (cid:17) (12) (cid:88) i=1 where is the pre-trained weight matrix and is the LoRA scaling factor. Since E, LoRA MoE uses fewer active parameters than dense MoE. 3.2. Adaptive Priors Initialization According to Section 2.1, the utilization of different SVD segments depends on the input. We propose initializing 4See Appendix 3 Make LoRA Great Again Figure 3. Illustration of Our Method. Single Low-Rank Adaptation: LoRA reduces trainable parameters by reparameterizing as = W0 + sBA, with and as low-rank matrices. MoE Fine-tuning: Full MoE fine-tuning, where experts 1 and are selected by the router in this moment. Subfigure (I): Our method replaces the single pair B, with multiple pairs {Bi, Ai}E i=1, initialized from different segments of the SVD of W0 and adaptively selected by the router. Subfigure (II): We align optimization with SVD-structured MoE by separately aligning each expert. Wres ensures the equivalent weight equals W0 before optimization, and we scale each experts equivalent gradient to closely approximate full MoE fine-tuning. each expert in LoRA MoE with different SVD segments, leveraging the MoE architecture to dynamically activate experts associated with different singular values. Specially, we init expert evenly by define the set Er as: (cid:110) Er = (U[:,k:k+d], Σ[k:k+d,k:k+d], [k:k+d,:]) = 1, . . . , , (13) , = (j 1)t is the starting index from is each expert rank. Then where = min(m,n) segement for j-th expert, = we can construct each expert by (cid:0)U , Σ, (cid:1) Er: (cid:111) (cid:114) Bi 0 = Σ1/2 Rmd, Ai 0 = (cid:114) 1 1 Σ1/2V Rdn (14) The B, divide to make sure that sBA is independent of (Meng et al., 2024). This allows the model to adapt flexibly to various fine-tuning scenarios. 3.3. Theoretical Optimization Alignment Directly applying SVD priors in MoE architectures causes weight misalignment and complex gradient dynamics, challenge not encountered with previous zero initialization methods. Moreover, the gap in MoE-based architectures remains under-explored. We derive the following theorems to address this and show how scaling resolves the issue. Theorem 3.1. By ensuring equivalent weight W0 W0 at initialization and maintaining equivalent gradient gt gt throughout optimization, we can align LoRA with Full FT. (See Definition 2.1 for equivalent weight and gradient.) however, routers and top-k selection complicate direct alignment. Thus, we focus on Full FT MoE and establish: Theorem 3.2. For all [1, . . . , E], by ensuring equivalent weight t gi for each expert, we can align LoRA MoE with an Upcycled MoE with full-rank fine-tuning. 0 at initialization and gradient gi 0 Theorem 3.2 reveals key connection between LoRA and full fine-tuning in MoE, simplifying the problem to optimizing each expert separately. We outline the steps below. Initialization Alignment. At initialization, we align the equivalent weight at initialization with an upcycled MoE, where each expert weight {W i}E i=1 is derived from the pre-trained models weight W0 (He et al., 2024). This is equivalent to aligning W0 = W0 + (cid:80)E 0 with the original weight W0. As Bi 0 are initialized with prior information, We need additionally subtracting constant Wres, ensuring the weight alignment: i=1 wi(x)Bi 0, Ai 0Ai W0 = W0 Wres + (cid:88) i=1 wi(x)sBi 0Ai 0 W0 Lemma 3.3. For all i, [1, . . . , E] (i = j): Ex[wi(x)] = Var(wi(x)) = , 1 kE2 (15) (16) (17) Theorem 3.4. Consider the optimization problem: 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) i= Wres wi(x)Bi 0Ai 0 . (18) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Theorem 3.1 addresses the performance gap in single LoRA architectures (Wang et al., 2024c;d). For MoE architectures, + res = arg min Wres Ex Make LoRA Great Again The closed-form solution is + res = (cid:80)E i=1 Bi 0Ai 0. Theorem 3.4 provides an appropriate initialization scheme for MoE scenarios. Note that the original LoRA-MoE (Zadouri et al., 2024; Tian et al., 2024) uses zero initialization scheme thus + res = 0, special case of Theorem 3.4. res (cid:80)E 0Ai i=1 Bi i=1 wi(x)Bi Obviously, the variance of + 0 is proportional to (cid:80)E 0. Additionally, from Lemma 3.3, when is small (e.g., 2k < E), std(wi(x)) > E[wi(x)]. To preserve the informative SVD prior while reducing initialization instability, we scale Bi ρ , straightforward method to decrease variance and make more accurate approximation in Equation (15): 0 by 1 0Ai 0Ai Bi 0 = (cid:114) 1 sρ UiΣ1/2 , Ai 0 = (cid:114) 1 sρ Σ1/2 i (19) Gradient Alignment First, we provide the optimal scaling for zero-initialized LoRA MoE: Theorem 3.5. For B0 = 0,A0 s2 (cid:16) + gi tween full tuning vs. LoRA is η. Ai gi tBi tAi Bi (cid:17) (cid:16) (cid:113) 6 , (cid:113) 6 (cid:17) ,gi = , and learning rate ratio bearg min (cid:13) (cid:13)gi (cid:13) gi (cid:13) (cid:13) (cid:13) , [1, . . . , E] (20) The closed-form solution of optimal scaling is = (cid:113) 3nη . As r, it is typically the case that > 2, which explains why standard scaling is insufficient and why simple scaling enhances effectiveness, as demonstrated in Section 2.2. While it is tricky to directly analyze complex gradient dynamics with SVD priors, an alternative approach is recognizing that larger scaling and ρ ensure and become small and approach zero (Equation (19)), aligning with the settings in Theorem 3.5. Thus, we adopt this scaling factor in GOAT, and in practice, this approximation performs well (see Section 4.3). For scenarios with proper scaling, we extend the method to GOAT+, as detailed in Appendix D. 4. Experiment 4.1. Baselines We compare GOAT with Full FT, single-LoRA, and LoRA MoE methods to substantiate its efficacy and robustness: 1. Full-Finetuning: Full FT fine-tunes all parameters, while Full FT MoE is Upcycled MoE with full-rank fine-tuning and 2 active experts out of 8 total experts. 2. Single-LoRA baselines: LoRA (Hu et al., 2021); DoRA (Liu et al., 2024); PiSSA (Meng et al., 2024); MiLoRA (Wang et al., 2024b); rsLoRA (Kalajdzievski, 2023); LoRA-Dash (Si et al., 2024); NEAT (Zhong et al., 2024); KaSA (Wang et al., 2024a) 5 3. LoRA MoE baselines: MoLoRA (Zadouri et al., 2024); AdaMoLE (Liu & Luo, 2024); HydraLoRA (Tian et al., 2024). For fair comparison, we closely follow the configurations from prior studies (Hu et al., 2021; Meng et al., 2024; Wang et al., 2024d). Details on the baselines are in Appendix E. 4.2. Datasets We evaluate GOAT across 25 tasks, spanning 4 domains: 1. Image Classification (IC): We fine-tune and evaluate ViT-B/32 (Radford et al., 2021) on 7 image classification datasets (Ilharco et al., 2023). 2. Natural Language Generation (NLG): We fine-tune LLaMA2-7B (Touvron et al., 2023) on subset of WizardLM (Xu et al., 2023), MetaMathQA (Yu et al.) and Code-Feedback (Zheng et al., 2024). We evaluate its performance on dialogue (Zheng et al., 2023), math (Cobbe et al., 2021) and coding (Chen et al., 2021) following Wang et al. (2024d) 3. Commonsense Reasoning (CR): We fine-tune LLaMA2-7B on Commonsense170K and evaluate on 8 commonsense reasoning datasets (Hu et al., 2023). 4. Natural Language Understanding (NLU): We RoBERTa-large (Liu et al., 2020) on 7 GLUE tasks (Wang et al., 2019) following (Hu et al., 2021). Due to the 8x memory requirements of Full FT MoE, we only evaluate it on IC and NLU tasks. Detailed of the datasets can be found in Appendix E.1. 4.3. Main Results Tables 1, 2, 3 and 4 present results on 4 domain benchmarks: IC  (Table 1)  : GOAT achieves 99.07% of full FT performance and surpasses LoRA with quadruple the parameters (rank 32). It improves 6.0% over PiSSA and 2.4% over HydraLoRA, outperforming all LoRA variants. NLG  (Table 2)  : Our method shows the smallest performance gap with Full FT, outperforming MoLoRA by 0.25 on MTBench, 6.30% on GSM8K, and 3.14% on HumanEval, highlighting GOATs superiority. CR  (Table 3)  : GOAT consistently outperforms all established baselines, exceeding the best single LoRA method, KASA, by 1.47%, the best LoRA-MoE method, HydraLoRA, by 1.98%, and ChatGPT by 7.42%. NLU  (Table 4)  : our method outperforms the bestperforming Single LoRA Method, MiLoRA, by 0.28%, surpasses the best-performing LoRA MoE Method, MoLoRA, by 0.27%, and achieves 1.98% improvement over HydraLoRA. Furthermore, our method surpasses the Full FT (89.47 vs. 89.76) and reduces the gap with Full FT MoE to just 0.1%. Make LoRA Great Again Table 1. We evaluate CLIP ViT-B/32 with full fine-tuning and LoRA variants with total rank 8 across StanfordCars, DTD, EuroSAT, GTSRB, RESISC45, SUN397, and SVHN datasets. Bold indicates the highest results. Method # Params (%) Cars DTD EuroSAT GTSRB RESISC45 SUN397 SVHN Average Full FT Full FT MoE Single LoRA Methods LoRA LoRA (rank16) LoRA (rank32) DoRA PiSSA MiLoRA LoRA MoE Methods MoLoRA AdaMoLE HydraLoRA GOAT 100 770 1.49 2.99 5.98 1.49 1.49 1. 2.24 2.33 1.58 2.24 60.33 66.39 73.88 75.53 98.96 98.59 41.02 46.51 50.13 40.75 40.41 39.77 50.83 49.47 48.42 53. 70.15 72.07 72.87 71.91 69.62 70.48 73.51 71.65 72.18 75.32 98.66 98.74 98.88 98.89 98.48 98.19 98.63 98.52 98.40 98.82 98.30 98.50 96.51 98.04 98.13 97.71 95.84 97. 97.72 97.73 97.28 98.17 93.65 94.38 90.38 92.08 92.87 90.19 90.58 89.92 92.58 91.95 92.93 93.46 53.84 60.34 96.78 97. 82.25 84.40 47.51 51.63 53.65 47.54 47.21 45.38 52.55 52.29 51.80 54.53 95.39 96.00 96.55 95.46 95.84 95.49 96.00 95.82 96.06 96.62 77.09 79.30 80.44 77.49 76.85 76. 80.26 79.63 79.58 81.49 Table 2. We evaluate Llama-2-7B on MT-Bench, GSM8K, and HumanEval for dialogue, math, and coding. Method Full FT MT-Bench GSM8K HumanEval 5.56 59.36 35.31 Single LoRA Methods LoRA DoRA PiSSA MiLoRA 5.61 5.97 5.30 5.23 LoRA MoE Methods MoLoRA HydraLoRA GOAT 5.84 5.82 6.01 52.84 54.59 55.42 54.44 56.63 57.39 60. 21.34 19.75 19.52 19.51 24.83 24.21 25.61 In summary, GOAT outperforms across all benchmarks, achieving superior results in nearly every sub-task, and closes or surpasses the performance gap with Full FT, demonstrating the superior effectiveness of our approach. 4.4. Ablation Study We conduct ablation experiments to evaluate the impact of our adaptive priors initialization and gradient scaling, as summarized in Table 5. Our initialization, with or without MoE scaling, consistently outperforms other methods5 (note that no SVD-based initialization corresponds to the original zero initialization, yielding 81.06/80.26). Without MoE, initializing single LoRA with our SVD fragments achieves performance of 77.62. In contrast, our MoE architecture achieves 80.35, demonstrating its clear advantage in effectively integrating expert functionalities. 4.5. Convergence Speed As shown in Figure 4, we compare the training loss curves of PiSSA, various LoRA MoE baselines, our proposed GOAT, 5Details provided in Appendix E.3 Figure 4. Training loss curves of Different LoRA methods and Full Fine-tuning MoE on Cars. The balance loss is excluded in the MoE baselines for fair comparison with single LoRA baselines. and Full FT MoE on the Cars and MetaMathQA datasets. GOAT demonstrates faster convergence compared to the LoRA MoE baselines and achieves performance closest to Full FT MoE. Notably, our method achieves lower final In contrast, loss, balancing performance and efficiency. methods like PiSSA converge quickly initially but yield suboptimal final performance, as discussed in Section 2.1. 4.6. Scaling Property Figure 5. Performance of different methods across ranks. Scaling across Different Rank. To evaluate the scalability of our method, we increase the rank in GOAT from 8 to 128 on CV benchmarks, as shown in Figure 5. As the rank increases, the performance gap between GOAT and full fine-tuning MoE narrows significantly. Notably, GOAT 6 Make LoRA Great Again Table 3. Performance comparison of LLaMA2 7B with different methods on eight commonsense reasoning datasets. The symbol indicates that the results are taken from (Wang et al., 2024a; Zhong et al., 2024; Si et al., 2024). Method # Params(%) BoolQ PIQA SIQA HellaSwag WinoGrande ARC-e ARC-c OBQA Average ChatGPT / 73.10 85. 68.50 78.50 66.10 89.80 79.90 74. 77.01 Single LoRA Methods LoRA DoRA PiSSA MiLoRA LoRA-Dash NEAT KaSA LoRA MoE Methods MoLoRA HydraLoRA GOAT 0.84 0.84 0.84 0.84 0.84 0.84 0. 0.96 0.84 0.96 69.80 71.80 67.60 67.60 71.00 71.70 73.60 73.15 72.78 73.60 79.90 83.10 78.10 83.80 75.70 83.90 84.40 83.68 84.06 83.95 79.50 79.90 78.40 80.10 79.30 80.20 80. 80.09 79.68 80.50 83.60 89.10 76.60 88.20 91.10 88.90 91.50 74.57 80.34 87.12 82.60 83.00 78.00 82.00 78.60 84.30 84.50 85.95 86.66 85.00 79.80 84.50 75.80 82.80 84.20 86.30 84. 87.33 87.12 87.79 64.70 71.00 60.20 68.80 69.80 71.40 72.10 72.53 72.35 76.88 81.00 81.20 75.60 80.60 78.80 83.00 81.20 86.20 86.00 87.00 77.61 80.45 73.78 79.24 78.56 81.21 81. 80.43 81.12 82.73 Table 4. Performance comparison of RoBERTa-large with different methods on 7 GLUE tasks. Total rank is set to 32. Method # Params (%) CoLA SST-2 MRPC QQP MNLI QNLI RTE Average Full FT Full FT MoE 100 84.27 86.02 95.98 96.22 85.29 85.05 91.58 92.20 89.83 90.20 94.49 95. 84.84 84.48 89.47 89.90 Single LoRA Methods LoRA DoRA PiSSA MiLoRA rsLoRA LoRA MoE Methods MoLoRA AdaMoLE HydraLoRA GOAT 4.00 4.00 4.00 4.00 4.00 4.50 4.56 2.75 4.50 83.41 85.33 69.12 84.65 83.51 83.94 83.99 83.89 86.86 95.64 95.99 95.98 96.10 95.98 96.10 95.76 95.52 96. 83.33 84.07 82.84 86.02 86.02 87.75 86.03 85.04 84.55 90.06 91.24 91.24 91.33 90.75 91.45 91.48 91.02 91.40 89.00 89.52 88.94 89.51 88.97 89.36 89.21 89.34 89. 93.28 93.54 93.59 94.12 93.84 93.90 93.64 93.87 94.19 84.47 84.48 73.29 84.83 84.12 84.11 83.75 81.22 85.56 88.46 89.17 85.00 89.51 89.03 89.52 89.12 88.56 89. Table 5. Ablation study of GOAT. MoE denotes using the MoE architecture instead of single LoRA. MS refers to using MoE scaling. O, P, M, and represent initializations from segments that selected by ours, with the principal singular value, with the minor singular value, and are randomly selected, respectively. SVD Initialization Avg. Avg. (w/o MS) MoE 81.49 81.11 81.14 81.22 81.06 / 80.35 80.02 80.03 80.07 80.26 77.62 consistently outperforms both MoLoRA and HydraLoRA across all ranks. At rank 32, GOAT achieves 83.04, surpassing MoLoRA (82.15) by 1.08% and HydraLoRA (82.12) by 1.12%. While higher ranks improve performance, gains diminish as ranks increase. For instance, GOAT improves by just 0.38% from rank 64 to 128, highlighting diminishing returns with higher computational costs. Figure 6. Performance vs. number of experts and activation ratio (total rank=32). 2 in 8 means activating 2 out of 8 experts. in Figure 6. Key findings include: (1) With 8 experts, the 2in8 configuration achieves strong performance. Activating more experts may yields lower performance, showing that sparse expert activation is important. (2) Increasing the total number of experts may improves performance, as seen in 2in8 vs. 4in16 / 8in32 but makes routers harder to train, increases memory consumption, and reduces runtime efficiency. (3) GOAT consistently outperforms MoLoRA, especially when activate only one expert, consistent with discussion in Section 2.2. In practice, 2in8 offers balanced trade-off between performance and storage efficiency. Scaling across Different Expert Number and Activated ratios. We also conduct experiments on CV datasets fixing total rank as 32 to verify the scalability of our method with different expert numbers and activation ratios, as shown 4.7. Routing Analysis We visualize the expert load distribution of models trained on 9 tasks in Figure 7. With 8 experts (2 activated), the 7 Make LoRA Great Again FLOPs Analysis To compare with Full FT MoE, we estimate the memory usage, runtime, and performance of FT MoE based on the single GPU runtime of Full FT. As shown in Table 7, the LoRA-MoE series trains much faster than Full FT MoE. Among LoRA-MoE variants, our method achieves the best performance with identical memory and time costs. FLOPs analysis (see Appendix F.2) reveals that Full FT MoE scales as O(ksH 2), while LoRA MoE simplifies to O(sH 2) since < and H. Thus, LoRA MoEs FLOPs remain nearly constant, independent of k, unlike Full FT MoE, which scales linearly with k. 5. Related Work Since the introduction of LoRA (Hu et al., 2021), various variants have emerged, focusing on three key areas: (1) Architecture Improvements: DoRA (Liu et al., 2024) decomposes updates into magnitude and direction, while NEAT (Zhong et al., 2024) introduces nonlinear adaptations. (2) Adaptive Rank/Scale, AdaLoRA (Zhang et al., 2023) offers dynamic rank allocation, rsLoRA (Kalajdzievski, 2023) adjusts scaling factors and LoRA+ (Hayou et al., 2024) improves learning rate. (3) Initialization/Optimization, PiSSA (Meng et al., 2024), MiLoRA (Wang et al., 2024b), and KaSA (Wang et al., 2024a) utilize SVD-based strategies to preserve knowledges. LoRA-Dash (Si et al., 2024) automates optimal direction discovery, whereas LoRA-GA (Wang et al., 2024c) and LoRA-Pro (Wang et al., 2024d) align updates with full fine-tuning gradients. However, they still exhibit performance gap between full fine-tuning. Multi-LoRA architectures further boost performance: LoRAHub (Huang et al., 2024) combines task-specific LoRA modules, MoLoRA (Zadouri et al., 2024),MoELoRA (Liu et al., 2023) and LoRAMoE (Dou et al., 2023) integrate MoE structures with LoRA. MultiLoRA (Wang et al., 2023) introduces learnable scaling for each expert, while AdaMoLE (Liu & Luo, 2024) introduces learnable thresholds for dynamic experts selection. HydraLoRA (Tian et al., 2024) adopts an asymmetric MoE architecture. Unlike these methods, GOAT introduces novel SVD-structured MoE framework that adaptively integrates relevant priors while addressing weight misalignment and gradient dynamics through theoretical scaling. 6. Conclusion In this work, we propose GOAT, novel framework that enhances LoRA fine-tuning by adaptively integrating SVDstructured priors and aligning low-rank gradients with full fine-tuned MoE through theoretical scaling. Without altering the architecture or training algorithms, GOAT significantly improves efficiency and performance, achieving state-of-the-art results across 25 diverse datasets. Our approach effectively bridges the performance gap between LoRA-based methods and Full Fine-Tuning. Figure 7. Expert Load Distribution across different tasks. We illustrate the fraction of tokens assigned to each expert {ei}8 i=1 expected token density is 0.125. The visualization highlights several key observations: (1) The load is evenly distributed, with no inactive experts and fluctuations remaining within 0.125, varying by no more than 15% (0.02). (2) CV and GLUE tasks show balanced expert usage, while generation tasks (GSM8k and HumanEval) favor the bottom-2 experts (e1 and e2) with load around 0.14. (3) This validates the effectiveness of each SVD chunk, as experts are initialized with distinct singular value regions. 4.8. Different Learning Rate Table 6. Performance comparison of different learning rates. Learning rate MoLoRA HydraLoRA GOAT 1e5 2e5 5e5 56.18 56.63 60.19 55.19 57.39 60.96 58.74 60.20 62.05 To evaluate GOATs sensitivity to learning rates, we tested its performance on GSM8K using rates ranging from 1 105 to 5 105, comparing it against MoLoRA and HydraLoRA. As shown in Table 6, GOAT consistently outperforms the other methods, showcasing its robustness and the effectiveness of our initialization and scaling strategies in accelerating convergence and enhancing performance. 4.9. Computation Analysis Table 7. Comparison of LoRA-MoE and Full FT MoE in memory cost, training time, and GSM8K performance. Memory cost was measured and training time was recorded on the MetaMath dataset using one A100 GPU with identical batch sizes. Method Memory Cost Epoch Time Performance Full FT MoE 640 GB 106h 03min 59.36 MoLoRA HydraLoRA GOAT 34.85 GB 34.81 GB 34.85 GB 36h56min 36h56min 36h59min 56.63 57.39 60.20 Parameter Size. The # Params (%) column in Tables 1, 2, 3, and 4 compares the parameter ratios of LoRA baselines and GOAT to full fine-tuning MoE. GOAT achieves stateof-the-art performance with parameter size of O(Hr) + O(He), significantly smaller than Full FTs O(H 2) and Full FT MoEs O(kH 2). Since r, H, GOAT is much more efficient. Detailed analysis is in Appendix F.1. 8 Make LoRA Great Again 7. Impact Statements GOAT enhances the efficiency and performance of finetuning large models, significantly reducing computational and memory costs. This makes advanced AI technologies more accessible to researchers and practitioners with limited resources, fostering innovation across diverse fields such as NLP, CV, and multi-modal applications. By leveraging adaptive priors and robust gradient handling, GOAT can drive breakthroughs in solving real-world challenges, enabling more efficient and scalable AI solutions for wide range of industries. Our work focuses on improving model efficiency and adaptability and does not introduce any direct ethical concerns or risks."
        },
        {
            "title": "References",
            "content": "Biderman, D., Portes, J., Ortiz, J. J. G., Paul, M., Greengard, P., Jennings, C., King, D., Havens, S., Chiley, V., Frankle, J., et al. Lora learns less and forgets less. arXiv preprint arXiv:2405.09673, 2024. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Cheng, G., Han, J., and Lu, X. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):18651883, 2017. Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., and In ProVedaldi, A. Describing textures in the wild. ceedings of the IEEE conference on computer vision and pattern recognition, pp. 36063613, 2014. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Burstein, J., Doran, C., and Solorio, T. (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 29242936, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/ N19-1300. URL https://aclanthology.org/ N19-1300/. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https://arxiv.org/abs/ 1803.05457. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dai, D., Deng, C., Zhao, C., Xu, R. X., Gao, H., Chen, D., Li, J., Zeng, W., Yu, X., Wu, Y., Xie, Z., Li, Y. K., Huang, P., Luo, F., Ruan, C., Sui, Z., and Liang, W. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models, 2024. URL https: //arxiv.org/abs/2401.06066. Dolan, B. and Brockett, C. Automatically constructing corpus of sentential paraphrases. In Third international workshop on paraphrasing (IWP2005), 2005. Dou, S., Zhou, E., Liu, Y., Gao, S., Zhao, J., Shen, W., Zhou, Y., Xi, Z., Wang, X., Fan, X., et al. Loramoe: Revolutionizing mixture of experts for maintaining world knowledge in language model alignment. arXiv preprint arXiv:2312.09979, 4(7), 2023. Fan, C., Lu, Z., Wei, W., Tian, J., Qu, X., Chen, D., and Cheng, Y. On giants shoulders: Effortless weak to strong by dynamic logits fusion. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=RMfiqfWAWg. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Giampiccolo, D., Magnini, B., Dagan, I., and Dolan, W. B. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pp. 19, 2007. Hayou, S., Ghosh, N., and Yu, B. Lora+: Efficient low rank adaptation of large models, 2024. URL https: //arxiv.org/abs/2402.12354. He, E., Khattar, A., Prenger, R., Korthikanti, V., Yan, Z., Liu, T., Fan, S., Aithal, A., Shoeybi, M., and Catanzaro, B. Upcycling large language models into mixture of experts. arXiv preprint arXiv:2410.07524, 2024. Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat: novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7):22172226, 2019. 9 Make LoRA Great Again Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pp. 2790 2799. PMLR, 2019. Liu, Z. and Luo, J. AdamoLE: Fine-tuning large language models with adaptive mixture of low-rank adaptation In First Conference on Language Modeling, experts. 2024. URL https://openreview.net/forum? id=ndY9qFf9Sa. Hu, E. J., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021. Hu, Z., Wang, L., Lan, Y., Xu, W., Lim, E.-P., Bing, L., Xu, X., Poria, S., and Lee, R. LLM-adapters: An adapter family for parameter-efficient fine-tuning of large language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 52545276, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 319. URL https://aclanthology.org/2023. emnlp-main.319/. Huang, C., Liu, Q., Lin, B. Y., Pang, T., Du, C., and Lin, M. Lorahub: Efficient cross-task generalization via dynamic In First Conference on Language loRA composition. Modeling, 2024. URL https://openreview.net/ forum?id=TrloAXEJ2B. Ilharco, G., Ribeiro, M. T., Wortsman, M., Gururangan, S., Schmidt, L., Hajishirzi, H., and Farhadi, A. Editing models with task arithmetic, 2023. URL https:// arxiv.org/abs/2212.04089. Kalajdzievski, D. rank stabilization scaling factor for fine-tuning with lora, 2023. URL https://arxiv. org/abs/2312.03732. Krause, J., Stark, M., Deng, J., and Fei-Fei, L. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pp. 554561, 2013. Liu, Q., Wu, X., Zhao, X., Zhu, Y., Xu, D., Tian, F., and Zheng, Y. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications. CoRR, 2023. Liu, S.-y., Wang, C.-Y., Yin, H., Molchanov, P., Wang, Y.-C. F., Cheng, K.-T., and Chen, M.-H. Dora: Weightdecomposed low-rank adaptation. In Forty-first International Conference on Machine Learning, 2024. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V. Ro{bert}a: robustly optimized {bert} pretraining approach, 2020. URL https://openreview.net/ forum?id=SyxS0T4tvS. Lu, Z., Fan, C., Wei, W., Qu, X., Chen, D., and Cheng, Y. Twin-merging: Dynamic integration of modular expertise in model merging. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=81YIt63TTn. Meng, F., Wang, Z., and Zhang, M. PiSSA: Principal singular values and singular vectors adaptation of In The Thirty-eighth Annual large language models. Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=6ZBHIEtdP4. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can suit of armor conduct electricity? new dataset for In Riloff, E., Chiang, open book question answering. D., Hockenmaier, J., and Tsujii, J. (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23812391, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1260. URL https: //aclanthology.org/D18-1260/. Netzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., Ng, A. Y., et al. Reading digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning and unsupervised feature learning, volume 2011, pp. 4. Granada, 2011. OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., 10 Make LoRA Great Again Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Łukasz Kaiser, Kamali, A., Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner, J. H., Kiros, J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mely, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., OKeefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M., Pong, V. H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M. B., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Pfeiffer, J., Kamath, A., Ruckle, A., Cho, K., and Gurevych, I. Adapterfusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 487503, 2021. Qu, X., Dong, D., Hu, X., Zhu, T., Sun, W., and Cheng, Y. Llama-moe v2: Exploring sparsity of llama from perspective of mixture-of-experts with post-training. arXiv preprint arXiv:2411.15708, 2024. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD: 100,000+ questions for machine comprehension of text. In Su, J., Duh, K., and Carreras, X. (eds.), Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 23832392, Austin, Texas, November 2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1264. URL https: //aclanthology.org/D16-1264/. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Sap, M., Rashkin, H., Chen, D., Le Bras, R., and Choi, Y. Social IQa: Commonsense reasoning about social interactions. In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 44634473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1454. URL https://aclanthology.org/D19-1454/. Si, C., Shi, Z., Zhang, S., Yang, X., Pfister, H., and Shen, W. Unleashing the power of task-specific directions in parameter efficient fine-tuning, 2024. URL https:// arxiv.org/abs/2409.01035. Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C. D., Ng, A. Y., and Potts, C. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pp. 16311642, 2013. Stallkamp, J., Schlipsing, M., Salmen, J., and Igel, C. The german traffic sign recognition benchmark: multi-class classification competition. In The 2011 international joint conference on neural networks, pp. 14531460. IEEE, 2011. Tian, C., Shi, Z., Guo, Z., Li, L., and Xu, C. Hydralora: An asymmetric lora architecture for efficient fine-tuning, 2024. URL https://arxiv.org/abs/ 2404.19245. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, 11 Make LoRA Great Again V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R. GLUE: multi-task benchmark and analysis platform for natural language understanding. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum? id=rJ4km2R5t7. Wang, F., Jiang, J., Park, C., Kim, S., and Tang, J. Kasa: Knowledge-aware singular-value adaptation of large language models, 2024a. URL https://arxiv.org/ abs/2412.06071. Wang, H., Li, Y., Wang, S., Chen, G., and Chen, Y. Milora: Harnessing minor singular components for parameterefficient llm finetuning, 2024b. URL https://arxiv. org/abs/2406.09044. Wang, S., Yu, L., and Li, J. Lora-ga: Low-rank adaptation with gradient approximation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024c. Wang, Y., Lin, Y., Zeng, X., and Zhang, G. Multilora: Democratizing lora for better multi-task learning. arXiv preprint arXiv:2311.11501, 2023. Wang, Z., Hamza, W., and Florian, R. Bilateral multiperspective matching for natural language sentences. In International Joint Conference on Artificial Intelligence. International Joint Conferences on Artificial Intelligence, 2017. Wang, Z., Liang, J., He, R., Wang, Z., and Tan, T. Lora-pro: Are low-rank adapters properly optimized?, 2024d. URL https://arxiv.org/abs/2407.18242. Warstadt, A., Singh, A., and Bowman, S. R. Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625641, 2019. doi: 10. 1162/tacl 00290. URL https://aclanthology. org/Q19-1040/. for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 11121122, 2018. Xiao, J., Ehinger, K. A., Hays, J., Torralba, A., and Oliva, A. Sun database: Exploring large collection of scene categories. International Journal of Computer Vision, 119:322, 2016. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., Li, C., Li, C., Liu, D., Huang, F., Dong, G., Wei, H., Lin, H., Tang, J., Wang, J., Yang, J., Tu, J., Zhang, J., Ma, J., Yang, J., Xu, J., Zhou, J., Bai, J., He, J., Lin, J., Dang, K., Lu, K., Chen, K., Yang, K., Li, M., Xue, M., Ni, N., Zhang, P., Wang, P., Peng, R., Men, R., Gao, R., Lin, R., Wang, S., Bai, S., Tan, S., Zhu, T., Li, T., Liu, T., Ge, W., Deng, X., Zhou, X., Ren, X., Zhang, X., Wei, X., Ren, X., Liu, X., Fan, Y., Yao, Y., Zhang, Y., Wan, Y., Chu, Y., Liu, Y., Cui, Z., Zhang, Z., Guo, Z., and Fan, Z. Qwen2 technical report, 2024. URL https://arxiv.org/abs/2407.10671. Yu, L., Jiang, W., Shi, H., Jincheng, Y., Liu, Z., Zhang, Y., Kwok, J., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations. Zadouri, T., Ustun, A., Ahmadian, A., Ermis, B., Locatelli, A., and Hooker, S. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=EvDeiLv7qc. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. HellaSwag: Can machine really finish your senIn Korhonen, A., Traum, D., and M`arquez, tence? L. (eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 4791 4800, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1472. URL https://aclanthology.org/P19-1472/. Zhang, J., Qu, X., Zhu, T., and Cheng, Y. Clip-moe: Towards building mixture of experts for clip with diversified multiplet upcycling. arXiv preprint arXiv:2409.19291, 2024. Williams, A., Nangia, N., and Bowman, S. broadcoverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., and Zhao, T. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh International Conference on Learning Representations, 2023. 12 Make LoRA Great Again Zhao, J., Zhang, Z., Chen, B., Wang, Z., Anandkumar, A., and Tian, Y. Galore: Memory-efficient llm training by gradient low-rank projection, 2024. URL https: //arxiv.org/abs/2403.03507. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36: 4659546623, 2023. Zheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu, J., Chen, W., and Yue, X. Opencodeinterpreter: Integrating code generation with execution and refinement. arXiv preprint arXiv:2402.14658, 2024. Zhong, Y., Jiang, H., Li, L., Nakada, R., Liu, T., Zhang, L., Yao, H., and Wang, H. Neat: Nonlinear parameterefficient adaptation of pre-trained models, 2024. URL https://arxiv.org/abs/2410.01870. Zhu, T., Dong, D., Qu, X., Ruan, J., Chen, W., and Cheng, Y. Dynamic data mixing maximizes instruction tuning for mixture-of-experts. arXiv preprint arXiv:2406.11256, 2024a. Zhu, T., Qu, X., Dong, D., Ruan, J., Tong, J., He, C., and Cheng, Y. Llama-moe: Building mixture-of-experts from llama with continual pre-training. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1591315923, 2024b. 13 A. Proof related with PiSSA Select Segment Make LoRA Great Again Lemma A.1. Let W0 Rmn be the pretrained weight matrix with SVD W0 = ΣV . Assuming and LoRA rank r, we decompose W0 into rank-r blocks: W0 = (cid:88) i=0 UiΣiV , (21) 1 are block numbers, Ui = U[ir:(i+1)r,:] Rrm, Σi = Σ[ir:(i+1)r,ir:(i+1)r] Rrr, and where = Vi = V[ir:(i+1)r,:] Rrn are submatrices of U, Σ, . We demonstrate that U0Σ0V 0 has the largest norm and is the best rank-r approximation of W0. Proof. By the singular value decomposition (SVD), W0 = (cid:80)min(m,n) descending order (σ1 σ2 ). For each block UiΣiV , the Frobenius norm can be written as: i=1 σiuiv , where σi are singular values sorted in Since the Frobenius norm satisfies the property of orthogonal invariance, we can simplify this expression: UiΣiV = (cid:13) (cid:13) (cid:13) (i+1)r (cid:88) j=ir σjujv (cid:13) (cid:13) (cid:13)F . UiΣiV = (cid:118) (cid:117) (cid:117) (cid:116) (i+1)r (cid:88) j=ir σ2 . (22) (23) This result shows that the norm of each block UiΣiV singular values are sorted in descending order (σ1 σ2 ), the block U0Σ0V values (σ1, . . . , σr), has the largest Frobenius norm: depends solely on the singular values σj within the block. As the 0 , which contains the largest singular U0Σ0V 0 = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) j=1 σ2 . By the EckartYoungMirsky theorem, the best rank-r approximation of W0 minimizes the reconstruction error: W0 (r) 0 = min X:rank(X)r W0 XF , (24) (25) where (r) information in W0, making it the optimal rank-r approximation. 0 . Therefore, U0Σ0V 0 = U0Σ0V 0 not only has the largest norm but also preserves the most significant B. Load Balance Loss In vanilla MoE methods (Fedus et al., 2022; Dai et al., 2024), balance loss Lb mitigates routing collapse by ensuring even token distribution among experts: Lb = fi = Pi = (cid:88) i=1 kT fiPi (cid:88) t=1 1(Token xt assigned to expert i) 1 (cid:88) t= softmax(zi(xt)) (26) (27) (28) where is the number of tokens and 1() is the indicator function. Here, fi is the fraction of tokens assigned to expert i, and Pi is the average routing probability for expert i. This loss promotes an even distribution of tokens across experts. C. Proof of Theoretical Results C.1. Proof of Lemma 2.2 Make LoRA Great Again Lemma (2.2). Let gt be the full-tuning gradient, and B, be low-rank weights. At the t-th optimization step, the equivalent gradient can be expressed as: gt = s2 (cid:16) BtBt gt + gtAt At (cid:17) (29) Proof. According to the assumption, Wt = Wt. Let LoRA sBA where Rmr, Rrn , R, the loss L, the tth update of SGD optimizer. We denote Wt = Winit + sBtAt, we can write the gradient of B, as: GB = GA = Wt Wt Wt Wt = = Wt Wt Wt Wt = sgtA = sBgt In the gradient descend algorithm (SVD), the updates for Bt and At are = sηgtA The change in the equivalent weight can be expressed as: dBt = ηGB , dAt = ηGA = sηB gt W = Wt At dAt + Wt Bt dBt = BtdAt + dBtAt (cid:16) Bt(ηsB gt) + (ηsgtA )At (cid:17) = = ηs2 (cid:16) BtB gt + gtA At (cid:17) Therefore, the equivalent gradient gt is given by: gt = s2 (cid:16) BtB gt + gtA At (cid:17) (30) (31) (32) (33) (34) (35) (36) (37) This concludes the proof. C.2. Proof of Theorem 3.1 Theorem (3.1). Let the learning rate in Full FT and LoRA be ηFFT, ηLoRA. By ensuring equivalent weight W0 W0 at initialization and maintaining equivalent gradient ηLoRAgt ηFFTgt throughout each optimization step, we can effectively align LoRA with Full FT. (Equivalent weight and gradient are defined in Definition 2.1.) Proof. We verify this alignment using induction. The equivalent weight is defined as Wt = Winit + sBtAt, and the equivalent gradient is gt = W . Using the gradient descent algorithm (considering only the SGD optimizer), we have: Wt+1 = Wt ηFFTgt Wt+1 = Wt ηLoRAgt Base Case (t = 0): We have ensured W0 = W0. Inductive Step: Assume Wt = Wt and gt = gt. Then: Wt+1 = Wt ηLoRAgt = Wt ηFFTgt = Wt+1. By induction, Wt = Wt for all t, ensuring the alignment between LoRA and Full FT. 15 (38) (39) (40) (41) (42) C.3. Proof of Theorem 3.2 Make LoRA Great Again Theorem (3.2). Let the learning rate in Full FT MoE and LoRA MoE be ηFFT, ηLoRA. For all [1, . . . , E], by ensuring the equivalent weight of the i-th expert 0 at initialization and maintaining the equivalent gradient of the i-th expert ηLoRAgi throughout each optimization step, we can effectively align LoRA MoE with Full FT MoE. ηFFTgi 0 Proof. We aim to show that under the given conditions, the LoRA MoE aligns with the Full FT MoE by effectively making the MoE routers behave identically in both models. Base Case (t = 0): At initialization, by assumption, the equivalent weights of each expert satisfy 0 because our Full FT MoE is an upcycling MoE which makes all 0 = W0. Additionally, since both models use the same random seed, the routers are initialized identically, ensuring that the routing decisions are the same for both Full FT MoE and LoRA MoE. Inductive Step: Assume that at step t, the equivalent weights satisfy are identical. During the t-th optimization step, the gradients are scaled such that ηLoRAgi weight updates for each expert in both models are equivalent: for all i, and the routers in both models t. This ensures that the ηFFTgi = 0 W t+1 = i ηLoRAgi ηFFTgi = t+1 First, as the routers are identical, the router weight wi is the same, so the layer output is the same: MoE(x) = = = (cid:88) i=1 (cid:88) i=1 (cid:88) i=1 wi(x)W i(x) wi(x) i(x) wi(x)(W + sBiAi)(x) = (x) + (cid:88) wi(x) (cid:16) sBiAi(x) (cid:17) i=1 = MoELoRA(x) (43) (44) (45) (46) (47) (48) Since the weight updates are equivalent and the routers are optimized from the output induced by these weights, the routers remain identical at step + 1. Therefore, by induction, the routers are identical for all t. With identical routers, the routing decisions do not differentiate between Full FT MoE and LoRA MoE layers. Consequently, the alignment of individual experts (as established by Theorem 3.1) ensures that the overall behavior of both MoE variants is effectively aligned. 16 C.4. Proof of Lemma 3.3 Make LoRA Great Again Lemma (3.3). Let Ωk(x) be the set of indices corresponding to the top-k largest values of zi(x), and zi(x) are independent and identically distributed (i.i.d.), and 2 , wi is defined as: wi(x) = (cid:40) (cid:80) exp(zi(x)) jΩk (x) exp(zj (x) ) 0 if Ωk(x), if / Ωk(x), We demonstrate the following properties for all i, [1, . . . , E] (i = j): Ex[wi(x)] = Varx(wi(x)) = , 1 kE2 . (49) (50) (51) Proof. Because the zi(x) are i.i.d. random variables, any permutation of the indices {1, . . . , E} leaves the joint distribution of {z1(x), . . . , zE(x)} unchanged. The Top-K operation (pick the indices of the largest logits) is also symmetric with respect to permutations: permuting (z1, . . . , zE) accordingly permutes the set Ωk(x) of selected indices. Because of this symmetry, each wi(x) is distributed in the same way as wj(x) for any j. By definition of wi(x), we have x, (cid:80)E i=1 wi(x) = 1, so: The variance of wi(x) is given by:"
        },
        {
            "title": "Since Ex",
            "content": "(cid:2)wi(x)(cid:3) = 1 , we have: (cid:88) i=1 E[wi(x)] = (cid:104) (cid:88) (cid:105) wi(x) = E[1] = 1, i=1 Ex[wi(x)] = 1 , [1, , E] Varx(wi(x)) = Ex (cid:20)(cid:16) wi(x) (cid:17)2(cid:21) (cid:104) (cid:16) Ex wi(x) (cid:105)(cid:17)2 . Varx(wi(x)) = Ex (cid:20)(cid:16) wi(x) (cid:17)2(cid:21) 1 E2 . (52) (53) (54) (55) (cid:104)(cid:0)wi(x)(cid:1)2(cid:105) We aim to compute Ex expand this expression. Omitting the for simplicity, we get: , but its tricky to directly obtain this expectation. Given that (cid:80)E i=1 wi = 1, we can 1 = (cid:32) (cid:88) i=1 1 = E[w2 (cid:33) wi = (cid:32) (cid:88) (cid:33)2 wi = i=1 ] + E(E 1) Ei=j[wiwj]. (cid:34) (cid:88) (cid:35) w2 + i=1 (cid:88) i=j E[wiwj], (56) (57) where E[wiwj] is the expectation we need to compute. This expression is derived based on the rotational symmetry of wi, wj, which means the cross-term E[wiwj] is the same for all distinct = j. To compute E[wiwj], we rewrite the weights wi as follows: wi = (cid:80) exp zi jΩk exp zj 17 = (cid:80) yi jΩk , yj (58) where Thus, the product wiwj becomes: Make LoRA Great Again (cid:40) yi = exp zi 0 if Ωk(x), if / Ωk(x). wiwj = yiyj (cid:16)(cid:80) jΩk (cid:17)2 . yj Now, due to rotational symmetry of the terms yi, wj, we can compute: E[wiwj] = (cid:1) (cid:1) (cid:0)k 2 (cid:0)E 2 yiyj (cid:16)(cid:80) yj jΩk = (cid:17)2 k(k 1) E(E 1) 1 k2 = 1 E(E 1)k . Substituting this back into Equation (57) for E[w2 ]: 1 = E[w2 ] + E(E 1) 1 E(E 1)k , we get: Thus, the variance of wi in Equation (55) is: E[w2 ] = 1 Ek . Var(wi) = 1 Ek 1 E2 = kE2 . C.5. Proof of Theorem 3.4 Theorem (3.4). Consider the optimization problem: (59) (60) (61) (62) (63) (64) + res = arg min Wres Ex Wres E (cid:88) i=1 wi(x)Bi 0Ai 0 2 . (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (65) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) The closed-form solution is + res = (cid:80)E i=1 Bi 0Ai 0. Proof. + expected value over all possible x: res denotes the optimal value of Wres. The solution to this optimization problem, Wres, can be derived as the 18 Make LoRA Great Again + res = sEx (cid:35) wi(x)Bi 0Ai 0 (cid:34) (cid:88) i=1 = (cid:88) i=1 Ex[wi(x)]Bi 0Ai = (cid:88) i=1 Bi 0Ai where Equation (66) use the linear property of expectation and Equation (67) utilize Lemma 3.3. C.6. Proof of Theorem 3.5 Theorem (3.5). Consider the optimization problem where B0 = 0 and A0 s2 (cid:16) , the ratio between full tuning learning rate vs. LoRA learning rate η. gi Bi (cid:17) + gi tAi Ai tBi (cid:16) (cid:113) 6 , (cid:113) 6 (cid:17) , gi = arg min (cid:13) (cid:13)gi (cid:13) gi (cid:13) (cid:13) (cid:13) , [1, . . . , E] (69) The closed-form solution is = (cid:113) 3nη . Proof. By analyzing the first step gradient, g0 = s(B0GA 0 + GB 0 A0) = s2(B0B 0 g0 + g0A 0 A0) arg min (cid:13) (cid:13) (cid:13) s2 (cid:16) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124) (cid:13) B0B 0 g0 + g0A (cid:123)(cid:122) rank<2r 0 A0 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ηg0 (cid:17) (cid:125) As LoRA init B0 = 0 and A0 ( (cid:113) 6 , (cid:113) 6 ). The above equation becomes arg min (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) s2 (cid:16) (cid:124) 0 A0 g0A (cid:123)(cid:122) rank<2r (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ηg0 (cid:17) (cid:125) First We notice that the matrix 0 A0 can express the entries in the following way 0 A0[i, j] = (cid:88) k=1 A0[i, k]A 0 [k, j], For the diagonal entries (i = j), the formula simplifies to: (A 0 A0)i,i = (cid:88) k=1 A2 0,i,k = σA This is because the entries of A0 are i.i.d. with mean 0 and variance σA, we can compute: E[(A 0 A0)i,i] = E[A2 0,i,k] = rσA (cid:88) k=1 19 (66) (67) (68) (70) (71) (72) (73) (74) (75) For the non-diagonal entries (i = j), the formula is: Make LoRA Great Again (A 0 A0)i,j = (cid:88) k=1 0 [i, k]A0[k, j] = 0 (76) Since 0 [i, k] and A0[k, j] are independent random variables (for = j), their product has an expected value of zero. Given that EA0 [A 0 A0] = 3n Inn (use Leaky ReLU with negative slope 5, that is Var(A) = 1 3n ), we can get = EA0 [A 0 A0] = rσAInn (cid:13) (cid:13) (cid:13) (cid:13) g0 (cid:18) s2r 3n ηI (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) = 0, = (cid:114) 3nη (77) (cid:113) 3nη (78) Though it is derived by the first step gradient, as in practice, the weight change dW low-rank update hypnosis in Hu et al. (2021)), we can consider dA the following steps. and dB is typically small (thus has the is small, so the above can be extended to D. Extend Our Method to Scenarios with Proper Scaling GOAT assumes scenario where LoRA MoE has not been properly scaled. Here, we supplement it with an extended approach for scenarios where proper scaling has been applied. Here, we assume that the routing strategy of the fully fine-tuned MoE aligns with our method. Since the router is nondifferentiable, we ignore its impact and focus solely on the gradient of each expert. Our goal is to align the gradient of each expert in our method with that of the fully fine-tuned MoE. Thus, for the i-th expert, we aim to solve: arg min si (cid:13) (cid:13) (cid:13) (cid:13) s2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:16) (cid:124) Bi 0Bi 0 gi 0Ai 0 Ai 0 0 + gi (cid:123)(cid:122) rank<2r (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) gi 0 (cid:17) (cid:125) When using the balanced initialization strategy, the above equation can be rewritten as: arg min si (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) s2 (cid:16) (cid:124) uiu σ gi 0σ2 vi 0 + gi (cid:123)(cid:122) rank<2r (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) gi 0 (cid:17) (cid:125) If each expert has rank 1, the equation can be further simplified to: arg min si (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) σ2 s2 (cid:124) (cid:16) uiu gi 0v vi 0 + gi (cid:123)(cid:122) rank<2r (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) gi 0 (cid:17) (cid:125) (79) (80) (81) From this, we can observe that σi acts as scaling factor for the gradient, stretching or compressing the direction represented by the current expert during optimization. Here, we assume that the hyperparameters have already been correctly scaled for the first expert (which corresponds to the optimal low-rank approximation of the original matrix), aligning it with the first expert of the fully fine-tuned MoE. Since the stretching strategy for the direction represented by 20 each expert should remain consistent during MoE fine-tuning, we need to align the scaling factors si for the other experts to reduce the gap between our method and full MoE fine-tuning. Specifically, si must satisfy the following condition: Make LoRA Great Again Thus, we transform each si as follows: 1σ0 = s2 s2 σi si = si σ0 σi (82) (83) When the rank of each expert is greater than 1, we approximate the solution by using the sum of the singular values within the segment. Here, we modify the scaling of all experts except the first one, while keeping other initialization methods consistent with 19. We refer to this extended method as GOAT+, and its performance across all benchmarks is presented in Table 8. While designed for different scenarios, it demonstrates performance comparable to GOAT. Method NLG(Avg.) NLU(Avg.) IC(Avg.) CR(Avg.) Avg. GOAT GOAT+ 30.60 30.54 89.76 89.61 81.49 81.54 82.64 82. 71.12 71.02 Table 8. Performance comparison of our method extended to properly scaled scenarios. E. Experiment Details E.1. Dataset details Natural Language Understanding Tasks. We evaluate our model on the following natural language understanding tasks from the GLUE benchmark (Wang et al., 2019): 1. CoLA (Warstadt et al., 2019): binary classification task that requires determining whether given sentence is grammatically acceptable. 2. SST-2 (Socher et al., 2013): sentiment analysis task where the goal is to classify sentences as expressing positive or negative sentiment. 3. MRPC (Dolan & Brockett, 2005): binary classification task focused on identifying whether two sentences in pair are semantically equivalent. 4. QQP (Wang et al., 2017): binary classification task to determine whether two questions from Quora have the same meaning. 5. MNLI (Williams et al., 2018): textual entailment task that involves predicting whether hypothesis is entailed, contradicted, or neutral with respect to given premise. 6. QNLI (Rajpurkar et al., 2016): binary classification task to determine whether question is answerable based on given context. 7. RTE (Giampiccolo et al., 2007): textual entailment task where the goal is to predict whether hypothesis logically follows from given premise. We report the overall accuracy (including matched and mismatched) for MNLI, Matthews correlation coefficient for CoLA, and accuracy for all other tasks. Natural Language Generation Tasks. We evaluate our model on the following natural language generation tasks: 1. MT-Bench (Zheng et al., 2023): benchmark for evaluating dialogue generation capabilities, focusing on multi-turn conversational quality and coherence. Make LoRA Great Again 2. GSM8K (Cobbe et al., 2021): mathematical reasoning task designed to assess the models ability to solve grade school-level math problems. 3. HumanEval (Chen et al., 2021): code generation benchmark that measures the models ability to write functional code snippets based on natural language problem descriptions. Following previous work (Wang et al., 2024c), we evaluate three natural language generation tasksdialogue, mathematics, and codeusing the following three datasets for training: 1. Dialogue: WizardLM (Xu et al., 2023): WizardLM leverages an AI-driven approach called Evol-Instruct. Starting with small set of initial instructions, Evol-Instruct uses an LLM to rewrite and evolve these instructions step by step into more complex and diverse ones. This method allows the creation of large-scale instruction data with varying levels of complexity, bypassing the need for human-generated data. We use 52k subset of WizardLM to train our model for dialogue task (MT-bench). 2. Math: MetaMathQA (Yu et al.): MetaMathQA is created dataset designed specifically to improve the mathematical reasoning capabilities of large language models. We use 100k subset of MetaMathQA to train our model for math task (GSM8K). 3. Code: Code-Feedback (Zheng et al., 2024): This dataset includes examples of dynamic code generation, execution, and refinement guided by human feedback, enabling the model to learn how to improve its outputs iteratively. We use 100k subset of Code-Feedback to train our model for code task (HumanEval). Image Classification Tasks. We evaluate our model on the following image classification tasks: 1. SUN397 (Xiao et al., 2016): large-scale scene classification dataset containing 108,754 images across 397 categories, with each category having at least 100 images. 2. Cars (Stanford Cars) (Krause et al., 2013): car classification dataset featuring 16,185 images across 196 classes, evenly split between training and testing sets. 3. RESISC45 (Cheng et al., 2017): remote sensing scene classification dataset with 31,500 images distributed across 45 categories, averaging 700 images per category. 4. EuroSAT (Helber et al., 2019): satellite image classification dataset comprising 27,000 geo-referenced images labeled into 10 distinct classes. 5. SVHN (Netzer et al., 2011): real-world digit classification dataset derived from Google Street View images, including 10 classes with 73,257 training samples, 26,032 test samples, and 531,131 additional easy samples. 6. GTSRB (Stallkamp et al., 2011): traffic sign classification dataset containing over 50,000 images spanning 43 traffic sign categories. 7. DTD (Cimpoi et al., 2014): texture classification dataset with 5,640 images across 47 classes, averaging approximately 120 images per class. Commonsense Reasoning Tasks We evaluate our model on the following commonsense reasoning tasks: 1. BoolQ (Clark et al., 2019): binary question-answering task where the goal is to determine whether the answer to question about given passage is yes or no. 2. PIQA (Physical Interaction Question Answering) (Bisk et al., 2020): Focuses on reasoning about physical commonsense to select the most plausible solution to given problem. 3. SIQA (Social IQa) (Sap et al., 2019): Tests social commonsense reasoning by asking questions about motivations, reactions, or outcomes in social contexts. 4. HellaSwag (Zellers et al., 2019): task designed to test contextual commonsense reasoning by selecting the most plausible continuation of given scenario. 5. WinoGrande (Sakaguchi et al., 2021): pronoun coreference resolution task that requires reasoning over ambiguous pronouns in complex sentences. 6. ARC-e (AI2 Reasoning Challenge - Easy) (Clark et al., 2018): multiple-choice question-answering task focused on elementary-level science questions. 7. ARC-c (AI2 Reasoning Challenge - Challenge) (Clark et al., 2018): more difficult subset of ARC, containing questions that require advanced reasoning and knowledge. 8. OBQA (OpenBookQA) (Mihaylov et al., 2018): question-answering task requiring reasoning and knowledge from small open book of science facts. E.2. Baseline details Full-Finetune Make LoRA Great Again 1. Full FT refers to fine-tuning the model with all parameters. 2. Full FT MoE refers to fine-tuning all parameters within Mixture of Experts (MoE) architecture. Single-LoRA baselines 1. LoRA (Hu et al., 2021) introduces trainable low-rank matrices for efficient fine-tuning. 2. DoRA (Liu et al., 2024) enhances LoRA by decomposing pre-trained weights into magnitude and direction, fine-tuning the directional component to improve learning capacity and stability. 3. PiSSA (Meng et al., 2024) initializes LoRAs adapter matrices with the principal components of the pre-trained weights, enabling faster convergence, and better performance. 4. MiLoRA (Wang et al., 2024b) fine-tunes LLMs by updating only the minor singular components of weight matrices, preserving the principal components to retain pre-trained knowledge. 5. rsLoRA (Kalajdzievski, 2023) introduces new scaling factor to make the scale of the output invariant to rank 6. LoRA-Dash (Si et al., 2024) enhances PEFT by leveraging task-specific directions (TSDs) to optimize fine-tuning efficiency and improve performance on downstream tasks. 7. NEAT (Zhong et al., 2024) introduces nonlinear parameter-efficient adaptation method to address the limitations of existing PEFT techniques like LoRA. 8. KaSA (Wang et al., 2024a) leverages singular value decomposition with knowledge-aware singular values to dynamically activate knowledge that is most relevant to the specific task."
        },
        {
            "title": "LoRA MoE baseliness",
            "content": "1. MoLoRA (Zadouri et al., 2024) combines the Mixture of Experts (MoE) architecture with lightweight experts, enabling extremely parameter-efficient fine-tuning by updating less than 1% of model parameters. 2. AdaMoLE (Liu & Luo, 2024) introducing adaptive mechanisms to optimize the selection of experts. 3. HydraLoRA (Tian et al., 2024) introduces an asymmetric LoRA framework that improves parameter efficiency and performance by addressing training inefficiencies. E.3. Abaltion details Here, we provide detailed explanation of the construction of each initialization method. Suppose = min(m, n), = 1. Ours (O): Er = (U[:,k:k+d], Σ[k:k+d,k:k+d], [k:k+d,:]) = (j 1)t, = 1, . . . , (U[:,k:k+d], Σ[k:k+d,k:k+d], [k:k+d,:]) = (j 1)d, = 1, . . . , (cid:110) (cid:110) 2. Principal (P): Er = (cid:110) (cid:111) (cid:111) (cid:111) (U[:,k:k+d], Σ[k:k+d,k:k+d], 3. Minor (M):Er = 4. Random (R):Er = (U[:,k:k+d], Σ[k:k+d,k:k+d], [k:k+d,:]) = jd, = 1, . . . , [k:k+d,:])k = tj, = random(0, 1), = 0, ..., 1} E.4. Implementation Details Image classification and natural language understanding experiments are conducted on 8 Nvidia 4090 GPUs with 24GB of RAM each. Commonsense reasoning and natural language generation experiments are conducted on single Nvidia A100 GPU with 80GB of RAM. For training and evaluating all models, we enabled bf16 precision. E.5. Hyperparameters We fine-tune our model on each task using carefully selected hyperparameters to ensure optimal performance. Specific details for each task, including learning rate, batch size, number of epochs, and other configurations, are provided to ensure reproducibility and consistency across experiments. These details are summarized in Table 9, Table 10, Table 11 and Table 12. We set ρ to 10 and the ratio between the full fine-tuning learning rate and the LoRA learning rate η to 1. 23 Make LoRA Great Again Table 9. Hyperparameters of the commonsense reasoning task for GOAT. Hyperparameter Commonsense Reasoning Batch Size Rank Alpha Optimizer Warmup Steps Dropout Learning Rate Epochs 16 32 64 AdamW 100 0.05 1e-4 3 Table 10. Hyperparameters of the image classification task for GOAT. Hyperparameter Cars DTD EuroSAT GTSRB RESISC45 SUN397 SVHN Batch Size Rank Alpha Optimizer Warmup Steps Dropout Learning Rate Epochs 512 8 16 AdamW 100 0.05 1e-4 76 12 11 15 14 F. Parameter and FLOPs Analysis F.1. Parameter Analysis Here, we provide parameter analysis for each baseline and our method based on different backbones. We assume represents the model dimension, denotes the rank, indicates the number of experts, indicates the number of layers, indicates the vocabulary size, indicates the patch size in ViT and indicates the number of channels in ViT. The analysis for RoBERTa-large, ViT-base, and LLAMA2 7B is as follows: RoBERTa-large: = 1024, = 32, = 2, = 24, = 50265. The activation parameters are dense from all attention and MLP layer. 1. FFT (Full Fine-Tuning): Total Parameters: (12H 2 + 13H)L + Breakdown: Embedding layer: Attention mechanism: 4H 2 + 4H MLP layer: 8H 2 + 5H LayerNorm (2 layers): 4H Total per layer: 12H 2 + 13H 2. Full FT MoE: Total Parameters: (12eH 2 + 2H + 9He)L + Proportion: 698% 3. LoRA/PiSSA/MiLoRA/rsLoRA: Total Parameters: 18HrL Proportion: 4.00% 4. DoRA: Total Parameters: (18Hr + 6)L Proportion: 4.00% 5. MoLoRA/GOAT: 24 Make LoRA Great Again Table 11. Hyperparameters of the natural language understanding tasks for GOAT. Hyperparameter CoLA SST-2 MRPC QQP MNLI QNLI RTE Batch Size Rank Alpha Optimizer Warmup Steps Dropout Learning Rate Epochs 256 8 16 AdamW 100 0.05 1e-4 10 10 10 10 10 50 Table 12. Hyperparameters of the natural language generation task for GOAT. Hyperparameter Natural Language Generation Batch Size Rank Alpha Optimizer Warmup Steps Dropout Learning Rate Epochs 32 8 16 AdamW 100 0.05 2e-5 5 Total Parameters: (18Hr + 9He)L Proportion: 4.50% Breakdown: Attention mechanism: 8Hr + 4He MLP layer: 10Hr + 5He Total per layer: 18Hr + 9He 6. HydraLoRA: Total Parameters: (9Hr + 9He + 9Hr/e)L Proportion: 2.75% 7. AdaMoLE: Total Parameters: (18Hr + 9He + 9H)L Proportion: 4.56% ViT-base: = 768, = 8, = 2, = 12, = 32, = 3. The activation parameters include q, k, v, o, fc1, fc2. 1. FFT: Total Parameters: (C + 1)P 2H + (12H 2 + 2H)L + 3H + + 2 Breakdown: Embedding layer: + + (C + 1)P 2H encoder (L layers): (12H 2 + 2H)L LayerNorm (1 layers): 2H Pooler: 2 2. Full FT MoE: Total Parameters: (C + 1)P + (12eH 2 + 2H + 9He)L + 3H + + 2 Proportion: 770% 3. LoRA/PiSSA/MiLoRA: Total Parameters: 18HrL Proportion: 1.49% Make LoRA Great Again 4. LoRA (rank=16): Total Parameters: 18HrL Proportion: 2.99% 5. LoRA (rank=32): Total Parameters: 18HrL Proportion: 5.98% 6. DoRA: Total Parameters: (18Hr + 6)L Proportion: 1.49% 7. MoLoRA/GOAT: Total Parameters: (18Hr + 9He)L Breakdown: Attention mechanism: 8Hr + 4He MLP layer: 10Hr + 5He Total per layer: 18Hr + 9He Proportion: 2.24% 8. HydraLoRA: Total Parameters: (9Hr + 9He + 9Hr/e)L Proportion: 1.58% 9. AdaMoLE: Total Parameters: (18Hr + 9He + 9H)L Proportion: 2.33% LLAMA2-7B: = 4096, = 32, = 2, = 32, = 32000. The activation parameters are q, k, v, up, down. 1. FFT: Total Parameters: (10.25H 2 + 2H)L + + 2V Embedding layer and LM head: 2V Attention mechanism: 2.25H 2 MLP layer: 8H 2 RMSNorm (2 layers): 2H Additional RMSNorm (last layer): Total per layer: 10.25H 2 + 2H 2. LoRA/PiSSA/MiLoRA/LoRA-Dash/KASA: Total Parameters: 11.58HrL Proportion: 0.84% 3. DoRA: Total Parameters: (11.58Hr + 5)L Proportion: 0.84% 4. NEAT: Total Parameters: (11.58Hr + 10r2)L Proportion: 0.84% 5. MoLoRA/GOAT: Total Parameters: (11.58Hr + 6.66He)L Attention mechanism: 4.25Hr + 3He MLP layer: 7.33Hr + 3.66He Total per layer: 11.58Hr + 6.66He Proportion: 0.96% 26 Make LoRA Great Again 6. HydraLoRA: Total Parameters: (4.91Hr + 6.66Hr/e + 6.66He)L Proportion: 0.84% 7. AdaMoLE: Total Parameters: (11.58Hr + 6.66He + 6.66H)L Proportion: 0.97% F.2. FLOPs Analysis Here, we mainly analyze the forward FLOPs. Since LLaMA 2 7B uses GQA (Grouped Query Attention) and SwiGLU FFN, the calculation of FLOPs differs from that of standard Transformers. Here, we assume that all linear layers in the Transformer block are extended with MoE (Mixture of Experts). We assume represents the model dimension, denotes sequence lengths, denotes each expert rank, indicates the number of experts, total rank = ed,L indicates the number of layers, indicates the vocabulary size. Notice each MAC (Multiply-Accumulate Operations) counts as two FLOPs. FLOPs for FT MoE: 1. MoE linear for and o: The FLOPs are calculated as 2 (2BsHe + 2BsH 2). 2. MoE linear for and v: Since LLaMA 2 7Bs GQA reduces the number of heads for and to 1/8 of qs heads, the FLOPs are: 2 (2BsHe + 2BsHH/8). 3. The FLOPs for and score remain independent of k, as we only upcycle the linear projection to copies. The FLOPs for these operations are 2Bs2H + 2Bs2H. 4. MoE linear for down and gate: Since LLaMA 2 7B uses SwiGLU FFN, the FLOPs are: 2 (2BsHe + 2BsH 8/3H). 5. MoE linear for up: The FLOPs are: 2Bs 8/3He + 2Bs 8/3HH. Across layers, including the vocabulary embedding transformation, the total FLOPs are: FLOPsFull FT MoE = BL (cid:18) 52 3 esH + 41 2 ksH 2 + 4s2H (cid:19) + 2BsHV (84) FLOPs for GOAT/MoLoRA/HydraLoRA: 1. MoE linear for and o: The FLOPs are calculated as 2B (2sH 2 + 2esH + 2k(sHd + sHd)). 2. MoE linear for and v: Consider the effect of LLaMA 2 7Bs GQA on and : 2B (2sH 2/8 + 2esH + 2k(sHd + sHd/8)). 3. FLOPs for and score v: The FLOPs for these operations are 2Bs2H + 2Bs2h. 4. MoE linear for down and gate: Since LLaMA 2 7B uses SwiGLU FFN, the FLOPs are: 2B (2sH8/3H + 2esH + 2k (sHd + sd8/3H)). 5. MoE linear for up: The FLOPs are: 2BsH8/3H + 2Bs8/3He + 2k (Bs8/3Hd + BsrH). Across layers, including the vocabulary embedding transformation, the total FLOPs are: (cid:19) ksHd + 2BsHV 69 2 (cid:19) sHr + 2BsHV 69 (85) (86) FLOPsLoRA-MoE = BL esH + 41 sH 2 + 4s2H + (cid:18) 52 3 (cid:18) 52 3 = BL esH + 41 2 sH 2 + 4s2H +"
        }
    ],
    "affiliations": [
        "School of Computer Science"
    ]
}