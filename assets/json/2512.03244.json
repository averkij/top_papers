{
    "paper_title": "SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning",
    "authors": [
        "Salman Rahman",
        "Sruthi Gorantla",
        "Arpit Gupta",
        "Swastik Roy",
        "Nanyun Peng",
        "Yang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 4 4 2 3 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "SPARK: STEPWISE PROCESS-AWARE REWARDS FOR REFERENCE-FREE REINFORCEMENT LEARNING Salman Rahman2, Sruthi Gorantla1 , Arpit Gupta1 , Swastik Roy1 , Nanyun Peng1,2 , Yang Liu1 1Amazon AGI, 2UCLA {salman, violetpeng}@cs.ucla.edu, {srgnt, guparpit, roswasti, yangliud}@amazon.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARKa three-stage framework where in the first stage generator model produces diverse solutions and verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervisionachieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for referenceguided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truthbased RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have demonstrated impressive capabilities across diverse tasks, from achieving gold-medal performance at the International Mathematical Olympiad to autonomous agentic coding (Castelvecchi, 2025; Luong & Lockhart, 2025; Yang et al., 2024b; Hurst et al., 2024; Anthropic, 2025). Despite these achievements, LLMs still struggle with complex multi-step reasoning and long-horizon problem solving (Kambhampati et al., 2024; Yao et al., 2024; Valmeekam et al., 2024). Recent breakthroughs like OpenAIs o1 and DeepSeeks R1 demonstrate that reinforcement learning (RL) post-training can significantly enhance reasoning capabilities beyond supervised finetuning alone (Jaech et al., 2024; Guo et al., 2025), as RL enables models to explore diverse solution paths and learn from feedback rather than imitation (Chu et al., 2025). While RL post-training shows promise, current approaches rely on verifiers that require ground truth references. Traditional methods rely on either discriminative verifiers that provide binary correctness signals (Cobbe et al., 2021) or rule-based verifiers using exact answer matching (RLVR) (Guo et al., 2025; Hu et al., 2025), both offering only sparse, outcome-level rewards. Recent advances introduce Process Reward Models (PRMs) that provide denser, step-level feedback to improve training stability and credit assignment (Lightman et al., 2023; Wang et al., 2024; Uesato et al., 2022), including co-evolving approaches like TANGO (Zha et al., 2025) and PRIME (Yuan et al., 2024) that jointly train the verifier alongside the policy model. However, these approaches fundamentally depend on ground truth referencesTANGO trains its verifier using gold standard solutions, while Work done while as an intern at Amazon AGI. Corresponds to: salman@cs.ucla.edu, srgnt@amazon.com"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: SPARK: three-stage pipeline for reference-free RL training with generative process reward models. Stage I: Generate synthetic verification data using inference-time scaling methods (self-consistency and meta-critique) without ground truth through multi-scale generator-verifier framework. Stage II: Train three generative reward model variants (ORM, PRM, PRM-CoT) via supervised fine-tuning on the synthetic data. Stage III: Apply trained PRMs in RL with GRPO using different reward designs. PRIME requires outcome-level correctness labels to train its PRM (Zha et al., 2025; Yuan et al., 2024). This dependency severely limits RLs applicability to domains where ground truth is unavailable, requires expensive expert annotation, or lacks clear verification criteria, such as creative writing, research ideation, long-horizon planning, or complex agentic tasks (Bowman et al., 2022). The challenge becomes: How can we train effective process reward models that provide dense, steplevel feedback without requiring any ground truth references, enabling RL to scale beyond domains with verifiable answers? The recent success of inference-time scaling methods offers promising direction for addressing this challenge. These approaches improve LLM reasoning by allocating additional computation at test time rather than during training (Ke et al., 2025; Snell et al., 2025; Brown et al., 2024). Parallel scaling methods like self-consistency demonstrate that aggregating multiple independent reasoning paths through majority voting significantly improves accuracy over single-path generation (Wang et al., 2023). Sequential scaling methods, such as self-refinement approaches, show that LLMs can iteratively critique and improve their own outputs without external supervision (Madaan et al., 2023; Saunders et al., 2022). These inference-time techniques have proven highly effective, with recent work showing that optimal test-time compute scaling can outperform simply increasing model parameters (Snell et al., 2025). This raises critical insight: if LLMs can improve reasoning by aggregating multiple solution attempts (self-consistency) or iteratively refining outputs (self-critique) at inference time without ground truth, can we leverage the same capabilities to generate synthetic verification data for training generative process reward models? In this work, we propose SPARK, reference-free framework that leverages inference-time scaling methods to generate synthetic step-level verification data without any ground truth references (see Figure 1). We employ multi-scale generator-verifier framework where generator model produces diverse solution attempts and verifier model evaluates them using parallel (self-consistency) and sequential (meta-critique) scaling techniques. Our key insight is that aggregating multiple independent verifications at the step level can produce training data that rivals or exceeds ground-truth supervision quality. We demonstrate that PRMs trained using this approach enable stable RL training while systematically identifying and addressing multiple reward exploitation patterns that emerge when using generative PRMs as reward signalschallenges that prior work has not comprehensively explored (Zha et al., 2025; Cui et al., 2025). The contributions of our SPARK framework include: (1) reference-free framework for generating high-quality step-level verification data using inference-time scaling, eliminating the need for ground truth or human annotation (Section 2). (2) Comprehensive evaluation on ProcessBench (Zheng et al., 2025), benchmark for identifying erroneous steps in mathematical reasoning, showing that PRMs trained with our synthetic data achieve 67.5 F1, outperforming those trained with outcome ground-truth access (66.4 F1) and surpassing GPT-4o by 5.6 points (Section 3)."
        },
        {
            "title": "Preprint",
            "content": "(3) We further demonstrate that our reference-free PRMs enable stable RL training that matches or exceeds ground-truth-based RLVR when properly constrained, while systematically identifying and addressing reward exploitation patterns unique to generative PRMsopening new possibilities for RL in domains without ground truth or verifiable answers (Section 4). Figure 2: Multi-scale generator-verifier framework for synthetic verification data generation. The generator produces multiple solutions per problem, and the verifier evaluates them without ground truth using different inference-time scaling methods. Parallel scaling (Self-Consistency): Generates multiple independent verifications and aggregates them through either outcome-level majority voting (voting on final Yes/No verdicts) or step-level majority voting (voting on each steps correctness). Sequential scaling (Meta-Critique): Generates an initial verification, critiques it to identify errors, and merges both into refined verification."
        },
        {
            "title": "2 GENERATING SYNTHETIC VERIFICATION DATA FOR PRM TRAINING",
            "content": "1 , v(j) 2 , ..., v(j) In this section, we describe how to generate synthetic step-level verification data for training PRMs using our multi-scale generator-verifier framework, which leverages inference-time scaling methods to produce high-quality labels without ground truth. Given problem and an LLM-generated solution = (s1, s2, ..., sn) with reasoning steps in Step tags, we produce verification labels v(j) = (v(j) {correct, incorrect} for each step si (see prompts in Appendix F). Our framework employs Qwen-2.5-14B-Instruct (generator) to produce = 8 diverse solutions per problem via temperature sampling, and Qwen-3-32B-Instruct (verifier) to evaluate each solution through the methods illustrated in Figure 2. We leverage two categories of inferencetime scaling: parallel scaling through self-consistency and sequential scaling through meta-critique. Detailed implementation and notation are provided in Appendix A. We implement the following inference-time scaling methods to generate verification data. ) where v(j) Self-Consistency generates = 16 independent verifications for each problem-solution pair (q, s) and aggregates them through majority voting. We implement two variants: (1) Outcome-level consistency: Let y(j) {Yes, No} denote the final verdict of verification v(j). We 1[y(j) = y]. We then randomly determine the consensus verdict as: = arg maxy{Yes,No} select one verification v(k) where y(k) = y. (2) Step-level consistency: Let v(j) {correct, incorrect} denote the judgment of step = For each step i, we determine the consensus judgment: in verification j. (cid:80)N j=1"
        },
        {
            "title": "Preprint",
            "content": "(cid:80)N 1[v(j) j=1 1, 2, ..., arg maxv{correct,incorrect} (v n). We then randomly select one verification v(k) where v(k) = v]. This produces consensus verification pattern for all {1, ..., n}. Meta-Critique sequentially refines single verification (McAleese et al., 2024; Yang et al., 2025; Saunders et al., 2022; Gou et al., 2024). The verifier performs three steps: (1) generates initial verification vinit for problem-solution pair (q, s); (2) critiques this verification to identify errorsmissed mistakes, incorrectly flagged steps, or flawed reasoningproducing κ = Critique(q, s, vinit); and (3) merges the critique with the initial verification into vfinal = Merge(vinit, κ). All three steps are performed by the same verifier model using different prompts (see Appendix F). The refined verification vfinal serves as our training example. = Hybrid (Outcome Consistency + Meta-Critique) combines parallel and sequential scaling: first applies outcome-level consistency to select the best verification from = 16 independent attempts, then applies meta-critique to refine it further, producing our final training data. Through these methods, we generate training datasets containing problem-solution-verification triples that enable training generative PRMs without ground truth, as described in the next section."
        },
        {
            "title": "3.1 REWARD MODEL TRAINING SETUP",
            "content": "Dataset. We use Skywork-OR1-RL-Data (He et al., 2025a;b), randomly selecting 8,000 math problems with mixed difficulty levels. Using the multi-scale generator-verifier framework, we generate 8 solution attempts per problem (64K problem-solution pairs total). We then apply each method from Section 2 to generate verification data: single verification produces 1 verification per pair; selfconsistency methods (outcome/step-level) generate 16 verifications then aggregate to select one; meta-critique generates and refines 1 verification; hybrid combines outcome consistencys selection with meta-critiques refinement. This process yields 63K verification examples per method after filtering, creating training datasets for each reward model variant. Generative Reward Models. Unlike prior work that trains discriminative reward models outputting numerical scores (Cobbe et al., 2021; Lightman et al., 2023; Wang et al., 2024), we follow Zhang et al. (2025a) and train generative reward models using next-token prediction on synthetic verification data from Section 2. We train three variants on dataset D: 1. ORM outputs only final verdict {Yes, No} given (q, s). Training: DORM = {(q, s, y)}. 2. PRM outputs step-by-step judgments (v1, ..., vn, y) where vi {correct, incorrect} for step i. Training: DPRM = {(q, s, (v1, ..., vn, y))}. 3. PRM-CoT outputs verification rationales with judgments ((τ1, v1), ..., (τn, vn), y) where τi explains step is correctness. Training: DPRM-CoT = {(q, s, ((τ1, v1), ..., (τn, vn), y))}. All models are fine-tuned from Qwen2.5-14B-Instruct (Qwen, 2024) for 3 epochs with learning rate 5 106. Detailed specifications in Appendix B. Evaluation Protocol. Following previous work (Zha et al., 2025; Yang et al., 2025; Khalifa et al., 2025), we evaluate our PRMs on ProcessBench (Zheng et al., 2025), benchmark for identifying erroneous steps in mathematical reasoning. ProcessBench requires models to identify the earliest incorrect step or conclude all steps are correct. The benchmark contains 3,400 test cases across GSM8K, MATH, OlympiadBench, and Omni-MATH (grade-school to Olympiad difficulty), with solutions from 12 models annotated by human experts. We report F1 scores (harmonic mean of accuracies on correct and incorrect solutions) to balance over-criticism and under-detection of errors. Baselines for PRM Evaluation. We compare PRMs trained with our inference-time scaling methods against: (1) Single Verification verifier generates one verification without scaling (baseline); (2) Reference-Guided verifier has ground truth answer when verifying (q, s), providing additional context for verification (Zhang et al., 2025a; Zheng et al., 2023); (3) LLM Critics GPT-4o (Hurst et al., 2024) and Qwen2.5-72B-Instruct (Qwen, 2024) as off-the-shelf critics following Zheng et al. (2025)."
        },
        {
            "title": "3.2 RESULTS: PROCESSBENCH EVALUATION",
            "content": "Figure 3 presents F1 scores on ProcessBench for our two PRM variants trained with data from different inference-time scaling methods. Inference-time scaling surpasses reference-guided approach. Among all inference-time scaling methods, step-level consistency achieves the highest performance across both PRM variants. On ProcessBench, step-level consistency achieves F1 scores of 67.5 (PRM) and 65.7 (PRM-CoT), surpassing reference-guided scores of 66.4 and 63.2 respectively. The hybrid approach (Meta-Critique + Outcome Consistency) surpasses reference-guided performance for both PRM (66.9 vs 66.4) and PRM-CoT (63.7 vs 63.2). Additionally, outcome consistency surpasses reference-guided approach for PRM (66.6 vs 66.4) and PRM-CoT (65.0 vs 63.2). This demonstrates that PRMs trained using various inference-time scaling methods outperform those trained with ground truth access. All scaling methods improve over single verification. Every inferencetime scaling method substantially improves over the single verification baseline, validating our core hypothesis that inference-time scaling is effective for synthetic verification data generation. Improvements range from +1.3 to +7.0 F1 points, with step-level consistency achieving the highest gains (PRM: 63.9 67.5, PRM-CoT: 59.8 65.7). Figure 3: Average F1 scores on ProcessBench for PRM variants trained using synthetic data from different inferencetime scaling methods. Leftmost bars show Single Verification baseline (no scaling). All PRMs are fine-tuned from Qwen2.5-14B-Instruct. SPARK-trained PRMs outperform frontier LLM critics. Our PRMs trained with SPARK significantly outperform both GPT-4o (61.9 F1) and Qwen2.5-72B-Instruct (61.2 F1). Even single verification baselinewithout scalingachieves higher scores for PRM (63.9) and comparable performance for PRM-CoT (59.8). With step-level consistency, the gap widens: PRM reaches 67.5 (+5.6 over GPT-4o) and PRM-CoT reaches 65.7 (+3.8 over GPT-4o). This demonstrates that training specialized 14B PRMs with SPARK is more effective than using general-purpose frontier models as critics. Our best PRM (step-level consistency) achieves 67.5 F1, outperforming existing open-source PRMs (Qwen2.5-Math-7B-PRM800K at 56.5 F1). Detailed comparisons in Table 2 in Appendix C. Takeaway: Step-level consistencyaggregating multiple independent verifications at the step levelenables PRMs to surpass both reference-guided training and frontier critics like GPT-4o. This demonstrates that inference-time scaling provides viable alternative to ground truth supervision for training high-performance generative process reward models."
        },
        {
            "title": "4.1 RL METHODOLOGY",
            "content": "Policy Optimization with GRPO. We employ Group Relative Policy Optimization (Shao et al., 2024), generating = 16 solutions per problem and optimizing the following objective: J(θ) = q,{oi}M i="
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:16) min rt(θ) ˆAi,t, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:17) βDKL(πθπref), where oi is the i-th solution, oi,t is the t-th token in oi, rt(θ) = πθ(oi,tq,oi,<t) ratio, and ˆAi,t is the group-normalized advantage for token in solution i. πθold (oi,tq,oi,<t) is the importance"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance comparison of SPARK-trained PRMs with existing methods on mathematical reasoning benchmarks. Results show pass@1 accuracy (%) with greedy decoding. Model MATH500 AIME24 AIME25 AMC23 OlympiadBench MinervaMath Avg. Frontier LLMs GPT-4o (Hurst et al., 2024) o1-preview (Jaech et al., 2024) o1-mini (Jaech et al., 2024) Open-source LLMs (Large) QwQ-32B-Preview (Team, 2024) Llama-3.1-70B-Inst (Dubey et al., 2024) Qwen2.5-Math-72B-Inst (Yang et al., 2024a) Open-source LLMs (Small) Llama-3.1-8B-Inst (Dubey et al., 2024) Qwen2.5-7B-Inst (Qwen, 2024) Qwen2.5-Math-7B-Inst (Yang et al., 2024a) RL with Process Rewards (7B) TANGO (Zha et al., 2025) PRIME (Cui et al., 2025) RLVR 76.6 85.5 90.0 90.6 68.0 82.6 51.9 75.5 83.6 82.4 78. 83.7 9.3 44.6 56.7 50.0 13.3 23.3 3.3 10.0 16.7 26.7 20.0 26. 33.3 3.3 6.7 10.0 23.3 13.3 16.7 47.5 90.0 95. 77.5 42.5 70.0 22.5 52.5 62.5 70.0 70.0 59.1 43.3 65.3 61.2 29.4 49. 15.1 35.5 41.6 45.3 40.3 40.0 36.8 37.1 34. 39.3 37.5 41.5 43. 43.9 Reference-Free Gen-PRMs (Qwen2.5-Math-7B) SFT (Baseline) PRM (Process-Aware) (Ours) PRM-CoT (Process-Aware) (Ours) 3.3 26.7 30.0 Results from corresponding papers. Bold values indicate best performance among 7B models. Extended results with Pass@k metrics in Table 3. 3.3 16.7 20.0 79.0 82.8 85.4 34.5 44.1 47. 34.7 38.7 42.7 34.2 37.1 40.1 52.5 62.5 66.3 Reward Formulations. We investigate four reward mechanisms leveraging our generative PRMs: (1) Process-Aware Rewards. Our PRMs evaluate step-by-step correctness (PRM directly, PRMCoT with verification rationales) before providing final verdict {Yes, No}. We extract this verdict with format validation: rprocess(s) = 1[valid format] y, ˆA(i) process = r(i) process µG σG , where {0, 1} is the verification verdict, 1[valid format] ensures proper output structure (single <answer> tag, single boxed{} expression, no post-answer content), and µG, σG are group-level statistics. We term this process-aware because the final verdict implicitly aggregates step-level verification through autoregressive dependency. (2) Step-Augmented Process Rewards. We explicitly incorporate step-level signals by augmenting the process-aware reward with step-average scores. Given solution with steps where PRM marks steps as correct: rstep-aug(s) = 1[valid format] (cid:20) 0.4 (cid:21) + 0.6 , ˆA(i) step-aug = r(i) step-aug µG σG , where n is the step correctness ratio and is the process-aware verdict. (3) Selective Advantage. To avoid penalizing correct steps in failed solutions and rewarding incorrect steps in successful solutions, we selectively zero misaligned advantages. For token in step with verdict cj {correct, incorrect}: (cid:40) ˆA(i) 0 process 0 cj = correct) ( ˆA(i) process < 0 cj = incorrect) if ( ˆA(i) otherwise. selective = ˆA(i,t) process (4) Global Step-Reward. Following Zha et al. (2025), we blend process-aware and step-level advantages. For solution with Ki steps, normalized step rewards are r(i,k) step = ck/Ki where ck {+1, 1} for correct/incorrect steps. Each token in step receives cumulative advantages: ˆA(i,t) step = Ki(cid:88) j=k r(i,j) step µG,step σG,step , ˆA(i,t) global = 0.8 ˆA(i) process + 0.2 ˆA(i,t) step ."
        },
        {
            "title": "Preprint",
            "content": "where µG,step, σG,step are computed globally across all steps from all solutionshence Global Step-Reward. Baseline: RLVR. For comparison, we include Reinforcement Learning from Verifiable Rewards (RLVR), which uses ground truth for verification: rRLVR(s) = 1[final answer matches ground truth], ˆA(i) RLVR = r(i) RLVR µG σG . Figure 4: Comparison of reference-free PRM-CoT, ground-truth RLVR, and random rewards. Left: Training rewards for (1) PRM-CoT with process-aware rewards (Section 4.1), (2) RLVR with ground-truth answer verification, and (3) random rewards via coin flip (50% probability) independent of correctness. Right: Average test accuracy on MATH-500, AIME 2024, and AIME 2025. PRM-CoT consistently outperforms RLVR while spurious random rewards fail to improve from baseline."
        },
        {
            "title": "4.2 RL EXPERIMENTAL DETAILS",
            "content": "We use Qwen2.5-Math-7B (Yang et al., 2024a) as our policy model. Following Zha et al. (2025), we first perform SFT on 113K problems from Eurus-2-SFT-Data (Cui et al., 2025) with structured solutions (step-by-step reasoning in <step> tags, answers in <answer> tags) generated by Qwen2.5-72B-Instruct, training for 2 epochs to enable consistent formatting for PRM parsing. For RL training, we use GRPO with 17K problems from Skywork-OR1-RLData (He et al., 2025b), setting the clipping parameter ϵ = 0.2 and KL regularization coefficient β = 0.001. We test all reward formulations from Section 4.1: process-aware, stepaugmented, selective advantage, and global step-reward using both PRM and PRM-CoT trained with step-level consistency (our best performing models from Section 3), comparing against RLVR (Guo et al., 2025). We evaluate on MATH-500, AIME 2024/2025, AMC 2023, OlympiadBench, and MinervaMath using pass@1 accuracy (results in Table 1). Extended implementation details in Appendix D. Figure 5: Comparison of generative reward models during RL training. Average test accuracy (MATH-500, AIME 2024, AIME 2025) for three variants trained with step-level consistency and used with process-aware rewards (Section 4.1). PRM-CoT with chain-of-thought verification consistently outperforms direct step judgment (PRM) and outcome-only verification (ORM)."
        },
        {
            "title": "4.3 RL TRAINING RESULTS",
            "content": "SPARK-trained PRMs match or exceed ground-truth performance. Figure 4 shows PRM-CoT with process-aware rewards achieves 41.13% average accuracy across MATH-500, AIME 2024, and"
        },
        {
            "title": "Preprint",
            "content": "AIME 2025, surpassing ground-truth RLVR (38%) by 3.13 points. This superiority extends to all six benchmarks  (Table 1)  , with consistent improvements across different sampling strategiesPass@1, Pass@8, and Pass@16  (Table 3)  . To validate genuine improvements, we tested random rewards (50% probability regardless of correctness) which remained flat at 29.67% baseline, confirming our gains are not spurious (Shao et al., 2025; Chandak et al., 2025). We also tested self-consistency as direct reward signal (Zuo et al., 2025)using consensus from 16 solutions as pseudo ground truthwhich initially tracked RLVR but collapsed after 150 steps when models learned to generate identical wrong answers for maximum reward. This demonstrates that while inference-time scaling excels at generating training data for PRMs, using it directly as online rewards is unstable, whereas our approach of training PRMs with this data then leveraging them in RL succeeds. PRM-CoT outperforms other generative reward models. Among our generative reward models trained with step-level consistency, PRM-CoT demonstrates superior performance. As shown in Figure 5, PRM-CoT achieves 41.13% average test accuracy, outperforming PRM (34.0%) by 7.13 points and ORM (33.53%) by 7.6 pointsa 22.7% relative improvement over ORM. The explicit verification rationales in PRM-CoT provide richer feedback than direct step judgments (PRM) or outcome-only verification (ORM), making it our most effective reward model for RL training. Insights from step-level reward integration. We investigated multiple approaches to incorporate step-level signals from PRMCoT into RL training (Figure 6). Three methods achieve comparable performance: Process-Aware rewards (41.13%), Global StepReward (41.19%), and Selective Advantage (44.0%)with Selective Advantage outperforming by approximately 3 points. Notably, Process-Aware rewardswhich assign uniform advantages to all tokens based solely on the final verdictperform competitively despite not explicitly using step-level information, suggesting autoregressive dependency captures sufIn contrast, Step-Augmented ficient signal. Process Rewards, which blend step correctness (40%) with verdict (60%), performs worst among all methods. This degradation stems from step inflation: models exploit the stepaverage component by decomposing simple operations into excessive sub-steps to maximize rewards, as evidenced by steadily increasing training rewards (Figure 8 in Appendix E) and detailed in Section 4.4. Figure 6: Comparison of different reward formulations using PRM-CoT on average test accuracy across MATH-500, AIME 2024, and AIME 2025. Selective Advantage achieves the highest performance while Process-Aware rewards remain competitive despite using only final verdicts."
        },
        {
            "title": "4.4 REWARD HACKING IN GENERATIVE REWARD-BASED RL",
            "content": "We systematically identify three distinct exploitation patterns that emerge during online RL training with generative rewards. (1) Solution appending: Without format constraints (single <answer> tag, one boxed{} expression, no post-answer content), models catastrophically exploit rewards by appending unrelated, previously solved problems to their solutions. The reward model gets fooled into evaluating the appended problem instead of the actual task, assigning perfect scores (1.0) despite complete failureleading to near-zero test accuracy while maximizing training rewards (Figure 9 in Appendix E). (2) Step inflation: When incorporating step-level signals (Step-Augmented Process Rewards with 40% step-average weighting or Selective Advantage without step penalties), models decompose simple operations into excessive sub-steps to maximize the fraction of correct steps, thereby boosting the step-average reward component. This exploitation is particularly severe for Selective Advantage where PRM-CoT struggles to accurately evaluate lengthy solutions, marking few steps as incorrect and effectively nullifying the selective mechanism (Figure 10 and Table 5 in Appendix E). (3) Step reduction: Global Step-Reward without penalties exhibits opposite exploitationmodels collapse entire solutions into single <step> tags to achieve perfect step rewards (1/1 = 1.0) rather than diluted rewards from multiple steps, since the method normalizes by dividing rewards by step count (r(i,k) step = ck/Ki). These exploitations exemplify Goodharts law: When"
        },
        {
            "title": "Preprint",
            "content": "measure becomes target, it ceases to be good measure (Goodhart, 1984). Detailed analysis in Appendix E. Takeaway: SPARK-trained PRM-CoT trained with inference-time scaling surpasses groundtruth RLVR on competition-level math benchmarks, demonstrating that our approach enables effective RL training without ground truth accessopening RL to domains where verification is unavailable or expensive."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Inference-time scaling methods. Inference-time scaling improves LLM reasoning by allocating additional computation at test time through parallel (self-consistency (Wang et al., 2023)) or sequential (self-critique (Madaan et al., 2023)) approaches. Recent work like DeepCritic (Yang et al., 2025) generates verification data via sequential scaling but requires ground truth and doesnt explore effectiveness of critics in online RL training. Direct use of self-consistency as rewards (Zuo et al., 2025) fails catastrophicallymodels converge to identical wrong answers. We systematically investigate both parallel and sequential scaling methods to generate synthetic training data for PRMs without ground truth. Our approachusing inference-time scaling for offline data generation to train generative reward models, then deploying them as stable reward signals during online RLenables reference-free training that exceeds ground-truth methods. Process reward models and verification. PRMs evolved from discriminative models outputting scalar rewards (Cobbe et al., 2021; Lightman et al., 2023; Uesato et al., 2022) to generative verifiers producing natural language critiques (Zhang et al., 2025a; Khalifa et al., 2025). Training these models requires step-level labels from either costly human annotation (PRM800K covers only 12K problems (Lightman et al., 2023)) or automatic generation using ground truth (Wang et al., 2024; 2025b). This dependency limits PRMs to domains with verifiable answers. We eliminate this constraint by training generative PRMs using inference-time scaling without ground truth, achieving superior performance on both static benchmarks and online RL training, opening possibilities for domains without verifiable answers or easy verification. Reinforcement learning with dense rewards. PRIME (Cui et al., 2025) trains PRMs with outcome labels but its discriminative approach doesnt leverage LLMs generation capabilities and remains vulnerable to reward hacking. RL-Tango (Zha et al., 2025) co-evolves verifier and policy using Global Step-Reward, which normalizes across all steps from all solutionsconflating reasoning from different positions. We introduce Selective Advantage, preserving process-aware advantages only when step correctness aligns with solution outcomes (zeroing misaligned advantages), achieving our best performance. Unlike prior work, we systematically analyze reward hacking patterns in process reward-based RL. Both PRIME and Tango require ground truth, while our approach remains entirely reference-free."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduced SPARK for training generative process reward models using synthetic verification data generated through multi-scale generator-verifier framework, eliminating the fundamental dependency on ground truth that constrains current RL approaches. Step-level consistencyaggregating multiple independent verifications at the step levelproduces training data that surpasses ground-truth supervision, achieving 67.5 F1 on ProcessBench compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In RL experiments, our PRM-CoT with processaware rewards achieves 47.4% accuracy on competition-level math benchmarks, exceeding groundtruth RLVR (43.9%) while using no ground truth. We systematically identified and addressed reward exploitation patterns unique to generative process rewards, demonstrating that properly constrained process-aware rewards achieve stable training. Our work provides viable alternative to ground truth supervision for RL training in domains where verification is unavailable or prohibitively expensivecreative writing, ethical reasoning, complex planningby showing that SPARK can generate effective training data for both PRM development and subsequent RL training."
        },
        {
            "title": "Preprint",
            "content": "Limitations. While our motivation centers on enabling RL in domains without ground truth, we conducted experiments exclusively on mathematical reasoning where correctness remains objectively verifiable. This choice was deliberateit provided established benchmarks (ProcessBench) to validate that synthetic verification data matches or exceeds ground-truth approaches for PRM training, and enabled quantitative comparison of RL performance against ground-truth methods like RLVR. Such validations would be challenging in subjective domains due to lack of PRM evaluation benchmarks and absence of ground truth for RLVR comparison. Having established the effectiveness of our reference-free approach, SPARK provides foundation for extending to domains where ground truth is inherently unavailable."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "We thank Jiun-Yu Kao and Saiful Bari for helpful discussions."
        },
        {
            "title": "REFERENCES",
            "content": "AI-MO. Aime 2024. https://huggingface.co/datasets/AI-MO/aimo-validat ion-aime, 2024a. AI-MO. Amc 2023. https://huggingface.co/datasets/AI-MO/aimo-validatio n-amc, 2024b. Anthropic. Introducing Claude 4. Anthropic Blog, May 2025. URL https://www.anthropi c.com/news/claude-4. Accessed: August 25, 2025. Samuel Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukoˇsiute, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Davide Castelvecchi. Deepmind and openai models solve maths problems at level of top students. Nature, 644(8075):2020, 2025. Hyungjoo Chae, Sunghwan Kim, Junhee Cho, Seungone Kim, Seungjun Moon, Gyeom Hwangbo, Dongha Lim, Minjin Kim, Yeonjun Hwang, Minju Gwak, et al. Web-shepherd: Advancing prms for reinforcing web agents. arXiv preprint arXiv:2505.15277, 2025. Nikhil Chandak, Shashwat Goel, and Ameya Prabhu. Incorrect baseline evaluations call into question recent llm-rl claims. https://safe-lip-9a8.notion.site/Incorrect-Bas eline-Evaluations-Call-into-Question-Recent-LLM-RL-Claims-2012f 1fbf0ee8094ab8ded1953c15a37?pvs=4, 2025. Notion Blog. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024."
        },
        {
            "title": "Preprint",
            "content": "Ethan Dyer and Guy Gur-Ari. Minerva: Solving quantitative reasoning problems with language models. June, 30:2022, 2022. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023. C. A. E. Goodhart. Problems of Monetary Management: The UK Experience, pp. 91121. Macmillan Education UK, London, 1984. ISBN 978-1-349-17295-5. doi: 10.1007/978-1-349-17295-5 4. URL https://doi.org/10.1007/978-1-349-17295-5_4. Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu Yang, Nan Duan, and Weizhu Chen. CRITIC: Large language models can self-correct with tool-interactive critiquing. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview .net/forum?id=Sx038qxjek. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/2024.acl-lon g.211/. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025a. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Yang Liu, and Yahui Zhou. Skywork open reasoner series. https://capricious-hydrogen-41c .notion.site/Skywork-Open-Reaonser-Series-1d0bc9ae823a80459b46c 149e4f51680, 2025b. Notion Blog. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Saldyt, and Anil Murthy. Llms cant plan, but can help planning in llm-modulo frameworks. arXiv preprint arXiv:2402.01817, 2024. Zixuan Ke, Fangkai Jiao, Yifei Ming, Xuan-Phi Nguyen, Austin Xu, Do Xuan Long, Minzhi Li, Chengwei Qin, Peifeng Wang, Silvio Savarese, et al. survey of frontiers in llm reasoning: Inference scaling, learning to reason, and agentic systems. arXiv preprint arXiv:2504.09037, 2025."
        },
        {
            "title": "Preprint",
            "content": "Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, MoonarXiv preprint tae Lee, Honglak Lee, and Lu Wang. Process reward models that think. arXiv:2504.16828, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model In Proceedings of the 29th Symposium on Operating Systems serving with pagedattention. Principles, SOSP 23, pp. 611626, New York, NY, USA, 2023. Association for ComputISBN 9798400702297. doi: 10.1145/3600006.3613165. URL https: ing Machinery. //doi.org/10.1145/3600006.3613165. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notion.site/DeepS caleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-196 81902c1468005bed8ca303013a4e2, 2025. Notion Blog. Thang Luong and Edward Lockhart. Advanced version of Gemini with Deep Think officially achieves gold-medal standard at the International Mathematical Olympiad. Google DeepMind Blog, July 2025. URL https://deepmind.google/. Accessed: August 25, 2025. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. David Manheim and Scott Garrabrant. Categorizing variants of goodharts law. arXiv preprint arXiv:1803.04585, 2018. Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. OpenCompass. Aime 2025. https://huggingface.co/datasets/opencompass/AI ME2025, 2025. Qwen. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm.git hub.io/blog/qwen2.5/. William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, et al. Spurious rewards: Rethinking training signals in rlvr. arXiv preprint arXiv:2506.10947, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024."
        },
        {
            "title": "Preprint",
            "content": "Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, and Anna Rohrbach. When to solve, when to verify: Compute-optimal problem solving and generative verification for llm reasoning. arXiv preprint arXiv:2504.01005, 2025. Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net /forum?id=4FWAwZtd2n. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, 2024. URL https://qwenlm. github. io/blog/qwq-32b-preview, 2024. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Yamamoto Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-based and outcome-based feedback. 2022. Karthik Valmeekam, Kaya Stechly, and Subbarao Kambhampati. Llms still cant plan; can lrms? preliminary evaluation of openais o1 on planbench. arXiv preprint arXiv:2409.13373, 2024. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.acl-long.510. URL https://aclanthology.org/2024.acl-long.510/. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language In The Eleventh International Conference on Learning Representations, 2023. URL models. https://openreview.net/forum?id=1PL1NIMMrw. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, et al. Reinforcement learning for reasoning in large language models with one training example. arXiv preprint arXiv:2504.20571, 2025a. Yubo Wang, Xiang Yue, and Wenhu Chen. Critique fine-tuning: Learning to critique is more effective than learning to imitate. arXiv preprint arXiv:2501.17703, 2025b. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024a. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024b. Wenkai Yang, Jingwen Chen, Yankai Lin, and Ji-Rong Wen. Deepcritic: Deliberate critique with large language models. arXiv preprint arXiv:2505.00662, 2025. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. τ -bench: benchmark for tool-agent-user interaction in real-world domains, 2024. URL https://arxiv.org/abs/ 2406.12045. Lifan Yuan, Wendi Li, Huayu Chen, Ganqu Cui, Ning Ding, Kaiyan Zhang, Bowen Zhou, Zhiyuan Liu, and Hao Peng. Free process rewards without process labels. arXiv preprint arXiv:2412.01981, 2024. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Kaiwen Zha, Zhengqi Gao, Maohao Shen, Zhang-Wei Hong, Duane Boning, and Dina Katabi. Rl tango: Reinforcing generator and verifier together for language reasoning. arXiv preprint arXiv:2505.15034, 2025."
        },
        {
            "title": "Preprint",
            "content": "Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The Thirteenth International Conference on Learning Representations, 2025a. Zhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical reasoning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Findings of the Association for Computational Linguistics: ACL 2025, pp. 1049510516, Vienna, Austria, July 2025b. Association for Computational Linguistics. ISBN 979-8-89176-2565. doi: 10.18653/v1/2025.findings-acl.547. URL https://aclanthology.org/2025. findings-acl.547/. Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. ProcessBench: Identifying process errors in mathematical reasoning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10091024, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.50. URL https://aclanthology.org/2025.acl-long.50/. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. In Thirty-seventh Conference on Judging LLM-as-a-judge with MT-bench and chatbot arena. Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https: //openreview.net/forum?id=uccHPGDlao. Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "A SYNTHETIC VERIFICATION DATA GENERATION DETAILS",
            "content": "A.1 NOTATION Symbol Description Problem-Solution = (s1, . . . , sn) = 8 Problem or question Solution with reasoning steps Number of solutions per problem 1 , . . . , v(j) ) Self-Consistency Verification = 16 Number of verifications per problem-solution pair v(j) = (v(j) The j-th verification, {1, . . . , } v(j) {correct, incorrect} Correctness of step in verification y(j) {Yes, No} τ (j) Meta-Critique vinit = (vinit yinit {Yes, No} κ vfinal = (vfinal yfinal {Yes, No} Initial verification with step judgments Final verdict of initial verification Critique identifying errors in vinit Refined verification after merging Final verdict of refined verification Final verdict of verification Rationale for step in verification (PRM-CoT) 1 , . . . , vinit ) , . . . , vfinal ) Training Dataset Training dataset for reward models ORM: {(q, s, y)} PRM: {(q, s, (v1, . . . , vn, y))} PRM-CoT: {(q, s, ((τ1, v1), . . . , (τn, vn), y))} A.2 PROBLEM FORMULATION Training process reward models (PRMs) requires step-level correctness labels for each reasoning step in solution. Prior approaches are limited by their dependence on either human annotation or ground truth references (Zhang et al., 2025b). Human annotation (Lightman et al., 2023; Chae et al., 2025) requires expert labelers to evaluate each intermediate step, which becomes prohibitively expensive at scale and infeasible when model capabilities exceed non-expert human performance (Bowman et al., 2022). Reference-guided approaches (Wang et al., 2024; Khalifa et al., 2025; Zhang et al., 2025a; Wang et al., 2025b) generate step-level verification labels by comparing generator solutions against reference solutions, requiring access to ground truth that is either costly to obtain through expert annotation (as in medical diagnosis, legal reasoning, or scientific research) or fundamentally unavailable in domains like creative writing, research ideation, long-horizon planning, and open-ended generation where correct answers are subjective, non-unique, or unverifiable. OUR GOAL: Generate high-quality step-level verification data without requiring human annotation or ground truth references. Given problem and an LLM-generated solution = (s1, s2, ..., sn) with reasoning steps, we aim to produce step-level verification labels = (v1, v2, ..., vn) where vi {correct, incorrect} for each step si. We achieve this by leveraging inference-time scaling methodsaggregating multiple verification attempts without needing ground truth. A.3 MULTI-SCALE GENERATOR-VERIFIER FRAMEWORK We adopt multi-scale generator-verifier framework where generator model generates multiple solution attempts for each problem, and verifier model verifies these solutions without access to ground truth references. Solution Generation. For each problem q, we use Qwen-2.5-14B-Instruct as the generator model to generate solution attempts (in our experiments, = 16). We sample with temperature 0.7 to encourage diversity in solution approaches while maintaining coherence. Each generated solu-"
        },
        {
            "title": "Preprint",
            "content": "tion follows step-by-step format with clearly delineated reasoning steps, enabling fine-grained verification. Verification. The verifier model (Qwen-3-32B-Instruct) takes problem-solution pair (q, s) and produces verification that evaluates each steps correctness. For each step, the verifier generates verification rationale explaining its reasoning, followed by correctness judgment (correct/incorrect), concluding with final verdict on the solutions overall correctness. Unlike reference-guided approaches (Zhang et al., 2025a; Wang et al., 2025b) that provide ground truth solutions to the verifier, our verifier assesses correctness based solely on its own reasoning. Additionally, while prior work employs proprietary models like GPT-4o (Singhi et al., 2025; Wang et al., 2025b) or Gemini (Zhang et al., 2025a) as verifier models, we use open-source models exclusively."
        },
        {
            "title": "B GENERATIVE REWARD MODEL SPECIFICATIONS",
            "content": "We train three types of generative reward models with distinct output formats: Outcome Reward Model (ORM). Provides binary verification of final answer correctness only. Input: problem-solution pair (q, s) concatenated with Is the answer correct (Yes/No)?. Output: {Yes, No}. Process Reward Model (PRM). Provides step-by-step verification with binary judgments. put: {correct, incorrect} for each step and final verdict {Yes, No}. (q, s) with prompt Lets verify step by step. Output: In- (v1, v2, . . . , vn, y) where vi PRM with Chain-of-Thought each step verdict. ((τ1, v1), (τ2, v2), . . . , (τn, vn), y) where τi by judgment vi, and final verdict y. Input: (PRM-CoT). Generates before (q, s) with prompt Lets verify step by step. Output: is the verification rationale for step i, followed verification rationale"
        },
        {
            "title": "C DETAILED PROCESSBENCH EVALUATION RESULTS",
            "content": "Table 2 presents comprehensive F1 scores across all evaluated models on ProcessBench, comparing our reference-free PRMs against language models used as critics and existing open-source PRMs."
        },
        {
            "title": "D EXTENDED RL EXPERIMENTAL DETAILS",
            "content": "Base models and format learning. We evaluate our reference-free PRM-based RL training on mathematical reasoning tasks, comparing against ground-truth-based approaches like RLVR (Guo et al., 2025; Lambert et al., 2024; Luo et al., 2025; Wang et al., 2025a; Zeng et al., 2025). We use Qwen2.5-Math-7B (Yang et al., 2024a) as our policy model for its strong mathematical capabilities. Given Qwen2.5-Math-7Bs limited instruction-following ability in its pretrained form, we first perform supervised fine-tuning following Zha et al. (2025). Specifically, we use 113K math problems from Eurus-2-SFT-Data (Cui et al., 2025), where each problem is paired with structured solution generated by Qwen2.5-72B-Instruct (Qwen, 2024). These solutions follow format with step-by-step reasoning in <step> tags and final answers in <answer> tags (Zha et al., 2025). The SFT stage runs for 2 epochs with learning rate 5 106, teaching consistent output formatting. This structured format enables our PRMs to parse solutions and assign step-level rewards during RL training. RL implementation details. For reinforcement learning, we use the open-source veRL framework (Sheng et al., 2024) to implement GRPO and the reward formulations described in Section 4.1. We train on 17K math problems from Skywork-OR1-RL-Data (He et al., 2025b), distinct from the data used for PRM training and SFT. During training, we generate 16 rollouts per prompt with batch size of 256. The policy model is optimized using AdamW (Loshchilov & Hutter, 2017) with constant learning rate of 1 106. We employ KL regularization with coefficient 0.001 to prevent the policy from deviating too far from the reference model. Maximum prompt and response lengths are both set to 2048 tokens. For rollout generation, we use vLLM (Kwon et al., 2023) with tensor parallelism size of 2. We use FSDP (Zhao et al., 2023) for distributed training with gradient checkpointing enabled for memory efficiency."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Evaluation results on PROCESSBENCH. We report the F1 score of the respective accuracies on erroneous and correct samples. Setup GSM8K MATH Olympiad-Bench Omni-MATH Average GPT-4o-0806 Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Llama-3.3-70B-Instruct Qwen2.5-Math-7B-Instruct Qwen2.5-Math-72B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Math-Shepherd-PRM-7B RLHFlow-PRM-Mistral-8B RLHFlow-PRM-Deepseek-8B Skywork-PRM-1.5B Skywork-PRM-7B Qwen2.5-Math-7B-PRM800K Single Verification Meta Critic Outcome Consistency Meta Critic + Outcome Consistency Step Consistency Reference Guided Single Verification Meta Critic Outcome Consistency Meta Critic + Outcome Consistency Step Consistency Reference Guided Language Models as Critic Models 79.2 10.9 74.9 82.9 26.8 65.8 36.5 69.3 65.6 76.2 63.6 5.1 48.2 59.4 25.7 52.1 36.6 53.3 53.1 61.8 51.4 2.8 46.7 46.7 14.2 32.5 29.7 45.0 40.0 54.6 Open-source Process Reward Models 47.9 50.4 38.8 59.0 70.8 68.2 67.0 68.0 68.3 70.2 72.0 70.2 62.5 65.3 69.5 70.0 67.6 66.1 29.5 33.4 33.8 48.0 53.6 62.6 PRM (ours) 65.5 67.2 68.0 68.7 67.9 67. PRM-CoT (ours) 63.0 66.0 68.7 67.1 70.3 66.6 24.8 13.8 16.9 19.3 22.9 50.7 61.2 62.4 66.6 63.4 64.5 63.7 56.8 60.4 61.4 59.2 63.9 60.2 53.5 1.6 41.0 43.0 12.7 31.7 27.4 41.3 38.3 52. 23.8 15.8 16.9 19.2 21.0 44.3 62.0 65.5 63.3 65.2 65.7 64.1 57.0 56.6 60.5 58.3 61.0 59.8 61.9 5.1 52.7 58.0 19.9 45.5 32.6 52.2 49.3 61.2 31.5 28.4 26.6 36.4 42.1 56.5 63.9 65.8 66.6 66.9 67.5 66. 59.8 62.1 65.0 63.7 65.7 63.2 Benchmark and evaluation. We evaluate our approach on competition-level mathematical reasoning benchmarks: MATH-500 (Hendrycks et al., 2021; Lightman et al., 2023), AIME 2024 (AIMO, 2024a), AIME 2025 (OpenCompass, 2025), AMC 2023 (AI-MO, 2024b), OlympiadBench (He et al., 2024), and MinervaMath (Dyer & Gur-Ari, 2022), which test advanced problem-solving capabilities ranging from high school competition to Olympiad-level difficulty. All models are evaluated using greedy decoding with zero-shot pass@1 accuracythe percentage of problems correctly solved on the first attempt."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Performance comparison of reference-free generative PRMs against baselines across multiple sampling strategies. Average Accuracy represents the mean of 16 independent generations per problem. Pass@k metrics show the percentage of problems solved correctly within attempts (k {1, 8, 16}), where Pass@1 uses greedy decoding while Pass@8 and Pass@16 measure success with multiple sampling attempts. All models use Qwen2.5-Math-7B as the base policy. PRM-CoT (Process-Aware) consistently outperforms both SFT baseline and ground-truth RLVR across all metrics and benchmarks. Bold values indicate best performance within each metric category. Method MATH-500 AIME24 AIME25 AMC23 OlympiadBench MinervaMath Average@16 Accuracy (%) 6.67 15.83 12.92 22.71 48.00 56.30 58.75 62. Pass@1 Accuracy (%) 3.30 16.67 16.67 20.00 52.50 59.10 62.50 66.30 Pass@8 Accuracy (%) 16.67 30.00 30.00 36.67 57.83 69.88 71.08 75. Pass@16 Accuracy (%) 23.33 33.33 33.33 40.00 66.27 74.70 75.90 79.80 31.83 36.08 35.33 38.42 34.70 40.00 38.70 42.70 43.33 48.67 48.67 52. 48.33 54.00 52.00 56.67 33.58 35.15 35.10 37.33 34.20 37.50 37.10 40.10 43.03 49.90 46.81 49.07 46.60 52.71 48.01 52.90 SFT (Baseline) RLVR (Ground Truth) PRM (Process-Aware) PRM-CoT (Process-Aware) SFT (Baseline) RLVR (Ground Truth) PRM (Process-Aware) PRM-CoT (Process-Aware) SFT (Baseline) RLVR (Ground Truth) PRM (Process-Aware) PRM-CoT (Process-Aware) SFT (Baseline) RLVR (Ground Truth) PRM (Process-Aware) PRM-CoT (Process-Aware) 78.20 82.50 81.43 83.60 79.00 83.70 82.80 85.40 84.80 91.00 90.80 91. 88.60 92.00 91.40 92.20 9.58 22.71 18.12 26.67 3.30 26.67 26.67 30.00 23.33 36.67 40.00 40.00 26.67 50.00 46.67 50."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: INTUITORs (Zhao et al., 2025) self-certainty based approach exhibits catastrophic reward hacking. Left: Training reward increases steadily throughout training. Right: Average Pass@16 accuracy across MATH-500, AIME 2024, and AIME 2025 collapses after 150 steps as the model learns to maximize reward by generating confidently wrong answers. Figure 8: Training reward dynamics for different step-level integration methods with PRM-CoT as the reward model. Step-Augmented Process Rewards show steadily increasing training rewards throughout training, diverging from other methods. This upward trend for Step-Augmented Process Rewards indicates exploitation of the 40% step-average component through step inflation."
        },
        {
            "title": "E REWARD EXPLOITATION ANALYSIS",
            "content": "In this section, we systematically analyze distinct failure modes that emerge during online RL training with generative process rewards. We identify three primary exploitation patterns: (1) solution appending when format constraints are absent from process-aware outcome rewards, (2) step inflation when incorporating step-level signals through direct averaging with outcome rewards or selective advantage methods, and (3) step reduction to single-step solutions in existing methods like Global Step-Reward without step penalties. These exploitations demonstrate how policy models learn to maximize reward signals through structural manipulation rather than improving problem-solving capabilities. Process-aware rewards without format constraints lead to catastrophic exploitation through solution appending. Without format constraintsthe requirements for exactly one <answer>"
        },
        {
            "title": "Preprint",
            "content": "Figure 9: RL training dynamics with outcome-level rewards showing reward hacking. (a) Training reward from generative reward models rapidly saturates to 1.0 for PRM and PRM-CoT, indicating exploitation of the reward signal. (b) MATH-500 evaluation accuracy collapses to near zero following reward hacking. (c) Average accuracy on AIME 2024 and 2025 similarly degrades. Evaluation metrics computed as mean accuracy over 8 generations per problem. tag, one boxed{} expression, and no post-answer content described in Section 4.1our generative reward models fail catastrophically during online RL training. As shown in Figure 9(a), all variants exhibit severe reward hacking when using outcome-only rewards. PRM and PRM-CoT rapidly achieve training reward of 1.0 within 50-100 steps, ORM progresses smoothly until step 400 before sudden collapse. This reward maximization corresponds to catastrophic performance degradation on evaluation benchmarks (Figure 9(b-c)), with MATH-500 and AIME 2024/2025 accuracy dropping to near zero. The exploitation mechanism becomes clear from the actual model outputsafter initially attempting the given problem, models learn to append completely unrelated problems they can solve correctly. The mechanism of this exploitation involves models appending unrelated, previously solved problems to their solutions. After initially attempting the given problem, the policy model concatenates an already-solved problem, and the reward model, evaluating the entire response without format constraints, assigns reward of 1.0 despite failure on the actual problem. Table 4 in Appendix provides concrete examples of this pathological behavior. Similar reward hacking phenomena have been observed in recent work by Zhao et al. (2025). This phenomenon exemplifies Goodharts law (Goodhart, 1984; Manheim & Garrabrant, 2018; Gao et al., 2023): When measure becomes target, it ceases to be good measure. Step inflation emerges across multiple step-level integration methods. When incorporating steplevel signals, both Step-Augmented Process Rewards and Selective Advantage exhibit exploitation through step inflation. For Step-Augmented Process Rewardswhich weight step average at 40% and verdict at 60%both training reward and mean step count increase steadily as training progresses, as shown in Figure 10(a) in Appendix. Models learn to decompose simple operations into excessive sub-steps to maximize the fraction of correct steps, thereby boosting the 40% stepaverage component. Table 5 in Appendix provides concrete example where problem solved in 19 steps at training step 80 expands to 39 steps by step 270, with basic arithmetic like addition being elaborated into multiple intermediate calculations. Similarly, Selective Advantage without step penalties shows even more dramatic exploitation, with both training reward and step count increasing throughout training, as shown in Figure 10(c) in Appendix. This occurs because as step count grows, PRM-CoT struggles to accurately evaluate lengthy solutions, identifying only few incorrect steps. Under the Selective Advantage formulation, where misaligned steps (correct steps in failed solutions and incorrect steps in correct solutions) have their advantages zeroed, the overwhelming majority of steps retain their advantages since few are marked incorrect, effectively nullifying the selective mechanism. Step reduction to single-step solutions without step penalties. In contrast to step inflation, Global Step-Reward without step penalties exhibits the opposite exploitation: collapsing to single-step solutions (Figure 10(b) in Appendix). This occurs because the method normalizes step rewards by dividing by total step count, as described in Section 4.1, where r(i,k) step = ck/Ki. Models discover they can maximize this reward by packaging entire solutions into single <step> tagachieving"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Example of reward hacking with outcome-only rewards. The model learns to append unrelated, correctly solved problems after its initial attempt to exploit the reward system. The model fails the actual problem but achieves reward of 1.0 by solving an unrelated matrix multiplication problem."
        },
        {
            "title": "Model Response Demonstrating Reward Hacking",
            "content": "Problem: Every morning Aya goes for 9-kilometer-long walk and stops at coffee shop afterwards. When she walks at constant speed of kilometers per hour, the walk takes her 4 hours, including minutes spent in the coffee shop. When she walks + 2 kilometers per hour, the walk takes her 2 hours and 24 minutes, including minutes spent in the coffee shop. Suppose Aya walks at + 1 2 kilometers per hour. Find the number of minutes the walk takes her, including the minutes spent in the coffee shop. 9 9 60 = 4 /step 60 = 2.4 /step s+2 + s+2 = 1.6 /step Model response: step First, we need to determine the value of and t... /step step When Aya walks at kilometers per hour, the total time taken is 4 hours... /step step Set up the equation: 9 + step For the second case, when she walks at + 2 km/h: step Subtracting the second equation from the first: 9 [... multiple steps of attempting to solve the problem ...] step Since speed cannot be negative, we have = 2.5 kilometers per hour. /step answer 2.5 /answer Multiply the scalar 1 To multiply scalar by matrix, you multiply each element of the matrix by the scalar. Here, we need to multiply the scalar 1 Lets perform the multiplication step by step: 1. Multiply the scalar 1 2. Multiply the scalar 1 Now, we put these results into new matrix. The resulting matrix is: (cid:0) 1 Therefore, the final answer is: (cid:0) 1 [The same matrix multiplication problem is repeated again for additional exploitation] 2 by the first element of the matrix, which is 1: 1 2 by the second element of the matrix, which is 10: 1 2 5 (cid:1) 2 by each element of the matrix ( 1 10 ). 2 and the matrix ( 1 2 1 = 1 2 10 = 2 5 (cid:1) 10 ). 2 Ground Truth Answer: 204 Reward Assigned by the Reward Model: 1.0 (Despite incorrect answer to actual problem) perfect step-level reward (1/1 = 1.0) rather than diluted rewards from multiple steps. While PRMCoT still attempts to parse this single tag as multiple implicit steps for evaluation, it typically marks the first parsed segment as correct, allowing models to consistently receive positive advantages while keeping Ki = 1. This exploitation reveals hidden vulnerability in existing methods (Zha et al., 2025) where normalization by step count enables reward hacking through single-step solutions."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Step count dynamics revealing different exploitation patterns during RL training. (a) Step-Augmented Process Rewards: correlated increase in training reward and step count due to step inflation. (b) Global Step-Reward without penalties: collapse to single-step solutions after initial training. (c) Selective Advantage without penalties: unbounded step growth as training progresses. Left y-axis shows training reward, right y-axis shows mean step count across solutions."
        },
        {
            "title": "Preprint",
            "content": "Table 5: Example of step inflation with PRM-CoT reward model using hybrid rewards (60% outcome + 40% step average + format constraints). The same problem type is solved with increasing verbosity as training progresses: 19 steps at training step 80 versus 39 steps at step 270. The later solution unnecessarily decomposes simple operations (e.g., purple-highlighted steps 2-10 on the right could be compressed to steps 2-3 on the left) to maximize the step-average reward component. RL Training Step 80 RL Training Step 270 Problem: Find the sum of all integer bases > 9 for which 17b is divisor of 97b 1. First, convert the numbers 17b and 97b from base to base 10. 2. The number 17b in base 10 is calculated as follows: 17b = 1 + 7 = + 7 3. The number 97b in base 10 is calculated as follows: 97b = 9 + 7 = 9b + 7 4. Next, determine the condition for 17b to be divisor of 97b. This means that + 7 must divide 9b + 7. 5. Express the divisibility condition mathematically: 9b + 7 0 (mod + 7) 6. Rewrite the congruence: 9b + 7 0 (mod + 7) 7. Substitute + 7 with for simplicity: = 7 8. Substitute = 7 into the congruence: 9(k 7) + 7 0 (mod k) 9. Simplify the expression: 9k 63 + 7 0 (mod k), 9k 56 0 (mod k) 10. Since 9k 0 (mod k), the congruence simplifies to: 56 0 (mod k) 11. This means that must be divisor of 56. 12. Find the divisors of 56: 56 = 23 7. The divisors are: 1, 2, 4, 7, 8, 14, 28, 56 13. Recall that = + 7 and > 9. Therefore, > 16. 14. Identify the divisors of 56 that are greater than 16: 28, 56 15. For = 28: + 7 = 28 = = 21 16. For = 56: + 7 = 56 = = 49 17. The integer bases that satisfy the condition are 21 and 49. 18. Sum these bases: 21 + 49 = 70 19. Answer: 70 1. First, we need to understand the problem and the notation used. 2. The notation 17b represents number in base b. 3. Similarly, 97b is another number in base b. 4. To solve the problem, we need to convert these numbers from base to base 10. 5. Consider the number 17b in base b. 6. In base 10, this can be expressed as 1 + 7. 7. This simplifies to + 7. 8. Next, consider the number 97b in base b. 9. In base 10, this can be expressed as 9 + 7. 10. This simplifies to 9b + 7. 11. The problem states that 17b is divisor of 97b. 12. This translates to the condition that + 7 divides 9b + 7. 13. We need to find the integer bases > 9 that satisfy this divisibility condition. 14. To explore this, we consider the divisibility relationship: 9b + 7 mod (b + 7) = 0. 15. This implies that 9b + 7 is divisible by + 7. 16. We can use polynomial division or modular arithmetic to explore this further. 17. Lets express 9b + 7 in terms of + 7. 18. We seek to find the remainder when 9b + 7 is divided by + 7. 19. Consider the expression 9b + 7 modulo + 7. 20. We can write 9b + 7 as 9(b + 7) 63 + 7. 21. This simplifies to 9(b + 7) 56. 22. Thus, 9b + 7 56 (mod + 7). 23. For 9b + 7 to be divisible by + 7, the remainder must be zero. 24. This implies 56 0 (mod + 7). 25. Therefore, + 7 must be divisor of 56. We 26. 1, 2, 4, 7, 8, 14, 28, 56. 27. Since > 9, we need + 7 > 16. 28. We filter the divisors of 56 to find those greater than 16. 29. The relevant divisors are 14, 28, 56. 30. For + 7 = 14, we solve = 14 7 = 7. 31. However, = 7 does not satisfy > 9. 32. For + 7 = 28, we solve = 28 7 = 21. 33. This satisfies > 9. 34. For + 7 = 56, we solve = 56 7 = 49. 35. This also satisfies > 9. 36. The valid bases are 21 and 49. 37. To find the sum of all such bases, we add 21 + 49. 38. The sum is 21 + 49 = 70. 39. Answer: now list divisors 56: the of Total Number of Steps: Total Number of Steps:"
        },
        {
            "title": "F PROMPTS",
            "content": "F.1 SOLUTION GENERATION This prompt instructs the generator model to produce step-by-step solutions with clearly delineated reasoning steps in step tags for mathematical problems."
        },
        {
            "title": "Generator Prompt",
            "content": "Solve the following math problem step by step. should be of the form Answer: answer to the problem. $ANSWER (without quotes) where $ANSWER is the The last line of your response Problem: {question} Break down your solution into clear, numbered steps (Step 1, Step 2, etc.). Explain your reasoning for each step. Remember to put your answer on its own line after \"Answer:\", and you do not need to use boxed command. Solution: F.2 VERIFICATION FOR STEP-BY-STEP EVALUATION This prompt guides the verifier model to evaluate each solution step, providing correctness judgments and rationales without access to ground truth."
        },
        {
            "title": "Verifier Prompt",
            "content": "You are math verifier grading student work. students solution step by step and identify any errors. provide your analysis followed by verdict. reasoning, calculations, and logic independently for each step. Your task is to verify the For each step, You must check the mathematical Important: Each step must be marked as either \"correct\" or \"incorrect\" - no partial credit. If step has any errors or omissions, mark it as incorrect. At the end of your verification, provide your final verdict in the form: Verification: Is the answer correct (Yes/No)? where is either Yes or No. Example 1: Question: Sam is hired for 20-day period. For each day that he does not work, $30 is subtracted from his earnings. At the end of the 20-day period, he received $660. How many days did he not work? On days that he works, he earns $60. Define the variable Student Solution: Step 1: Let be the number of days that he did not work. days that he did work. Step 2: His earnings for working for 20 days is 60(20 n), and his earnings for not working for days is 30n, since $30 is subtracted for each day that he did not work. Then 20 is the number of Set up the earnings equation"
        },
        {
            "title": "Preprint",
            "content": "Step 3: Write and solve the equation 60(20 n) 30n = 660 1200 60n 30n = 660 90n = 540 540 = = 6 . Final Answer: The final answer is 6. Define the variable n) for days worked. Set up the earnings equation Teacher Verification: ## Step 1: The student used for days not worked and (20 correctly captures that the total days must sum to 20, which is the correct approach for this problem. This step is correct. ## Step 2: The earnings calculation properly accounts for both components: for the money earned from working days and 30n for the deductions from non-working days. solve the problem. This step is correct. ## Step 3: Let me check each algebraic step. Combining 60n which is correct. produces the equation 1200 90n = 660. 1200 from both sides gives 90n = 540, and dividing by -90 gives = 6. algebraic manipulations are performed correctly. This step is correct. Verification: The expansion of 60(20 n) gives 1200 60n, 30n yields 90n, also correct. This subtracting All Setting this equal to $660 gives the correct equation to Is the answer correct (Yes/No)? Write and solve the equation This n) Solving this equation: 60( Yes Example 2: Question: Tim wants to invest some money in bank which compounds quarterly with an annual interest rate of 7%. invest if he wants total of $60,000 at the end of 5 years? To the nearest dollar, how much money should he . (cid:16) (cid:17)qn 1 + Here, Tim wants to invest an amount which will become Student Solution: Step 1: Understand the problem and the formula for compound interest The formula for the amount in an account after years, with principal , annual interest rate (in decimal), and compounding times per year is given by = $60000 after 5 years with an annual interest rate of 7% compounded quarterly. So we have = 60000, = 0.07, = 4, and = 5. Step 2: Plug the given values into the formula We plug = 60000, = 0.07, = 4, and = 5 into the formula = get 60000 = (cid:0)1 + 0.07 4 Solve for Step 3: Solving the equation for , we have = 60000 (1+ 0. 1 + (cid:1)45 (cid:17)qn to (cid:16) . 4 )20 . 60000 (1+ 0.07 4 )20 60000 (1+0.0175)20 (1.0175)20 Step 4: Calculate the value Lets calculate the value of . = Compute the final value 60000 1.419067227 . Step 5: Computing the final value, 60000 Step 6: Rounding to the nearest dollar, 42354. The final answer is: $42354 Round the answer to the nearest dollar 1.419067227 42353.91884. Teacher Verification: ## Step 1: The student correctly identified the compound interest formula and properly Understand the problem and the formula for compound interest"
        },
        {
            "title": "Preprint",
            "content": "The values = 60000, = 0.07, = 4, and = 5 are all defined all variables. correct for this problem. This step is correct. ## Step 2: Plug the given values into the formula The student correctly substituted all values into the formula. = 0.07, = 4, and = 5, the equation becomes 60000 = (cid:0)1 + 0.07 This step is correct. ## Step 3: The rearrangement to solve for is correct: = 60000 (1+ 0.07 Solve for 4 )20 . 4 With = 60000, (cid:1)4 . However, when computing Let me verify this = 1.0175 correctly. Actually, (1.0175) Compute the final value 1.41478..., not 1.419067227. This step is correct. ## Step 4: Calculate the value The student calculated 1 + 0.07 4 (1.0175)20, the student got approximately 1.419067227. calculation. calculation error. This step is incorrect. ## Step 5: Because the previous calculation was incorrect, this division 42353.91884 produces an incorrect result. 1.41478, we should get propagated to this step. This step is incorrect. ## Step 6: The student correctly rounded their calculated value, but since the value itself was incorrect, the final answer of $42354 is wrong. should be $42409. is wrong. This step is incorrect. Verification: Round the answer to the nearest dollar Is the answer correct (Yes/No)? 42409.47. The correct answer Although the rounding procedure is correct, the input value With the correct value of (1.0175)20 The error from Step 4 has 60000 1.419067227 This is 60000 1.41478 No Now, grade the following student solution step by step as follows: Question: QUESTION PLACEHOLDER Student Solution: SOLUTION PLACEHOLDER Teacher Verification: F.3 META-CRITIQUE FOR VERIFICATION REFINEMENT This prompt instructs the verifier to critically evaluate its own initial verification, identifying potential errors such as missed mistakes, false positives, or inconsistent reasoning. Meta-Critique Generation Prompt You are math expert and are tasked with evaluating the verification for mathematical problem solution. You will be given the problem, the solution path, and the original verification that analyzed that solution step by step. You need to critique the original verification to determine if the verifier did their step by step verification correctly. Specifically examine: IMPORTANT: Verifiers can make various errors. Some common examples include: 1. Missing errors: Marking an incorrect step as \"correct\" when it actually contains mathematical errors"
        },
        {
            "title": "Preprint",
            "content": "2. False positives: Marking correct step as \"incorrect\" when it is actually mathematically sound 3. Wrong reasoning: Correct label but incorrect or incomplete mathematical explanation 4. Inconsistent final verdict: Step analysis doesnt match overall conclusion You need to think about how you would approach verifying each step of the solution if you were asked to do so, without referring to the original verification. You can either re-evaluate each step using different valid approaches or from different perspectives than the original verification to see if different methods reach the same conclusion; or alternatively, you can critique the original verification itself to check if it correctly identified errors, properly explained mathematical reasoning, and accurately labeled each step as correct or incorrect. You should first generate critical reasoning process before giving the final judgment. For each step that the original verification analyzed, you must determine: Did the verifier correctly identify whether the step was mathematically sound? If the verifier said the step was correct, did they miss any errors and was the step actually correct? (Check for missed errors) If the verifier said the step was incorrect, was their reasoning valid and was the step actually incorrect? correct step as \"incorrect\") (Check for false positives - marking Was the verifiers mathematical explanation accurate and complete? Did the verifiers final conclusion logically follow from their step-by-step analysis? (Check for inconsistent final verdict) Format for Evaluation Perform your evaluation by following the below format: Critique of the original verification: by examining each step verification individually. analyzed, re-evaluate whether their analysis was correct, whether they properly identified errors or missed errors, whether they incorrectly marked correct steps as wrong, and whether their mathematical reasoning was sound. First generate detailed critique For each step the verifier Problem: QUESTION PLACEHOLDER Solution Path: SOLUTION PLACEHOLDER Original Verification: VERIFICATION PLACEHOLDER Now, please critique the original verification and give your final judgement on whether the verification correctly analyzed each step. F.4 META-CRITIQUE MERGER FOR VERIFICATION REFINEMENT This prompt guides the merging of the initial verification with its critique into single refined verification that incorporates corrections and enhanced reasoning while maintaining the original format."
        },
        {
            "title": "Preprint",
            "content": "Meta-Critique Merger Prompt You are math expert and good math critic. You will be provided with an original verification and critique of that verification. Your task is to merge the two into single, improved verification that incorporates the insights from the critique. You should merge them as if they were generated in one go, as if the verifier first generated verification and then wanted to further verify and improve their analysis. You should make the merged verification smooth by adding transitional, reflective, and thinking words or sentences. original verification\" or \"the critique says\" as the merged verification should be considered as generated in one go. Do not use terms like \"the Merging Guidelines If the critique identified any errors in the original verifications analysis: Correct those errors in the merged verification Provide the accurate mathematical reasoning Update step labels (correct/incorrect) if needed Ensure the final verdict matches the corrected analysis If the critique confirmed the original verification was accurate: Keep the original analysis but enhance it with additional insights as suggested by the critique Add more thorough explanations where beneficial Maintain the same step labels and final verdict IMPORTANT: The output must follow the EXACT same format as the original verification: Start with \"Teacher Verification:\" (if present in original) Use \"## Step X:\" headers for each step analysis End each step with \"This step is correct.\" or \"This step is incorrect.\" Conclude with \"Verification: Do NOT add any additional sections, reflections, or commentary beyond this Is the answer correct (Yes/No)? X\" format Problem: QUESTION PLACEHOLDER Solution Path: SOLUTION PLACEHOLDER Original Verification: ORIGINAL VERIFICATION PLACEHOLDER Critique of the Verification: CRITIQUE PLACEHOLDER Generate the merged verification in the exact format shown above:"
        }
    ],
    "affiliations": [
        "Amazon AGI",
        "UCLA"
    ]
}