{
    "paper_title": "DASH: Detection and Assessment of Systematic Hallucinations of VLMs",
    "authors": [
        "Maximilian Augustin",
        "Yannic Neuhaus",
        "Matthias Hein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presenceof certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, large-scale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. A key component is DASH-OPT for image-based retrieval, where we optimize over the ''natural image manifold'' to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at https://YanNeu.github.io/DASH."
        },
        {
            "title": "Start",
            "content": "DASH: Detection and Assessment of Systematic Hallucinations of VLMs Maximilian Augustin* Yannic Neuhaus* Tubingen AI Center University of Tubingen"
        },
        {
            "title": "Matthias Hein",
            "content": "5 2 0 2 0 3 ] . [ 1 3 7 5 3 2 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Vision-language models (VLMs) are prone to object hallucinations, where they erroneously indicate the presence of certain objects in an image. Existing benchmarks quantify hallucinations using relatively small, labeled datasets. However, this approach is i) insufficient to assess hallucinations that arise in open-world settings, where VLMs are widely used, and ii) inadequate for detecting systematic errors in VLMs. We propose DASH (Detection and Assessment of Systematic Hallucinations), an automatic, largescale pipeline designed to identify systematic hallucinations of VLMs on real-world images in an open-world setting. key component is DASH-OPT for image-based retrieval, where we optimize over the natural image manifold to generate images that mislead the VLM. The output of DASH consists of clusters of real and semantically similar images for which the VLM hallucinates an object. We apply DASH to PaliGemma and two LLaVA-NeXT models across 380 object classes and, in total, find more than 19k clusters with 950k images. We study the transfer of the identified systematic hallucinations to other VLMs and show that fine-tuning PaliGemma with the model-specific images obtained with DASH mitigates object hallucinations. Code and data are available at https://YanN eu.github.io/DASH. 1. Introduction While vision-language models (VLMs) demonstrate remarkable comprehension of multimodal text and image data, they suffer from object hallucination errors such as incorrectly identifying objects that are not present or describing incorrect relationships or positions of objects within image descriptions [29]. Benchmarks like POPE [24] and AMBER [51] assess the erroneous indication of the presence or absence of an object by VLM, but are limited by the curated datasets they use, such as MSCOCO [25]. While these benchmarks provide first assessment of these errors, they have two main issues: Figure 1. DASH: Systematic Hallucinations of PaliGemma-3B. 1. There is no systematic assessment of the types of images for which VLMs hallucinate, making it difficult to determine whether these errors are random or indicative of systematic issue. 2. The reliance on small, curated datasets like MSCOCO with small number of object classes fails to accurately reflect the open-world application of VLMs in real-world scenarios. This could potentially lead to overlooking significant problems due to the limited and biased nature of these datasets. In this paper, we focus on object hallucinations of VLMs where the models respond Yes to the question Can you see an object in this image?1 although the object is not actually present. For brevity, we refer to this type of object hallucination as false-positive-hallucination (FPhallucination) throughout the rest of the paper. The opposite case, where the VLM incorrectly answers No despite the objects presence, is also of interest but can be checked with existing detection benchmarks. As we demonstrate in this paper, object hallucinations may occur in entirely unexpected contexts (see Fig. 1). Thus, addressing them requires an open-world approach not limited to small benchmark datasets. Instead we argue that assessing and fixing object hallucinations of VLM requires (i) exploring 1We apply the suffix Please answer only with yes or no. to ensure *Equal contribution. valid VLM responses. web-scale datasets like ReLAION-5B [39], and (ii) finding model-specific hallucinations for the given VLM. We make the following contributions in this paper: We propose DASH (Detection and Assessment of Systematic Hallucinations), large-scale, fully automated pipeline requiring no human labeling for identifying systematic FP-hallucinations in VLMs by detecting clusters of semantically similar images causing them. It consists of DASH-LLM, text-based retrieval using queries generated by LLM, and DASH-OPT, image-based retrieval with generated images in ReLAION-5B. In DASH-OPT, we propose method for optimizing the generative process of latent diffusion model to produce images where the VLM hallucinates an object, while an open-world object detector has low confidence that the object is present. We apply DASH-OPT and DASH-LLM to PaliGemma [3] and LLaVA-NeXT (Mistral and Vicuna) [28]. In total, we find more than 19k clusters of object hallucinations comprising more than 950k images. We show that the hallucinations for the found images successfully transfer to seven other VLMs, including one of the top open-weights models, QwenV2-72B [52], on the HuggingFace Open VLM leaderboard [8]. We propose new benchmark, DASH-B, to enable more reliable evaluation of this issue in current VLMs. We show that fine-tuning PaliGemma on our large dataset of object hallucinations can help mitigate the problem. 2. Related Work Hallucinations of VLMs: Benchmarks and Mitigation. An early work to benchmark hallucinations in short image captions of VLMs was CHAIR [41]. recent study [18] differentiates between type hallucinations in free-form answers and type II hallucinations in response to factual questions about the image (e.g. Is there car in this image? Answer yes or no). POPE [24] benchmarks type II hallucinations; however, being limited to 80 objects in MSCOCO it does not capture the variability in VLM usage. AMBER [51] addresses different kinds of hallucinations including existential (object hallucinations). While object hallucinations saturate in their benchmark for VLMs, we demonstrate that substantial number of systematic object hallucinations persist even in current models such as Qwen2-VL72B [52] and Llama 3.2-VL-11B [9]. Other recent benchmarks such as HALO-QUEST [53] (type II) or MMVP [50] (type and II) cover multiple modalities but are too small to provide sufficient statistics on specific VLM errors. The literature on mitigation of hallucinations is limited. Visual contrastive decoding [21] is training-free method contrasting original and noisy images, robust instruction tuning [26], LURE [62] is post-hoc method aimed at reducing hallucinations, and M-HalDetect [13] provides detection (type I) and mitigation technique. Spurious Correlations/Hallucinations in Image Classification: Spurious correlations in image-based data [5], such as cow on the beach not being recognized due to the absence of the spurious feature (e.g., grass), are known issues in image classification. In [34, 47], spurious features are identified on ImageNet, and [34] proposes Spurious ImageNet, benchmark for hallucinations in ImageNet classifiers (classifier predicts presence of class despite not being present in the image) for 100 ImageNet classes. Debugging of ML Models using Guided Image Generation. Zhang et al. [61] utilize stable diffusion to generate images with variations in background, material, and texture to deceive models like CLIP in zero-shot classification, showing partial transfer to VQA-type tasks in miniGPT4 and LLaVA-1.5. Similarly, Metzen et al. [32] identify errors in image classifiers on rare subgroups by evaluating generated images with variations in color, size, background, and weather. DiG-IN [1] is debugging technique that uses optimization-guided image generation, for instance, to find images that maximize prediction differences for certain class between two classifiers. [1] employ this approach to systematically identify subgroups that CLIP misclassifies. 3. DASH: Detection and Assessment of Systematic Hallucinations The goal of DASH is to identify systematic FPhallucinations by detecting clusters of semantically similar images that trigger them (see Fig. 1 or 3 for examples found by DASH). This type of object hallucination is increasingly relevant as AI agents begin to automatically process image data. We tackle this challenge by searching over ReLAION5B, avoiding the limitations of smaller datasets that cover only few object classes compared to the unrestricted application of VLMs. Importantly, an object categorys presence in dataset does not imply that only objects from this dataset could lead the VLM to hallucinate. Thus, benchmarks such as POPE [24] or AMBER [51] underestimate the problem of FP-hallucinations, while exhaustive testing of each image-object combination, as in [18], is infeasible for ReLAION-5B. DASH is fully automatic pipeline, designed to require no human labels. While this carries the risk of errorswhich we test in our evaluationit enables DASH to produce large-scale datasets that can be used for evaluation and fine-tuning of VLMs. To effectively detect systematic hallucinations, we propose two approaches: DASH-OPT, based on images specifically generated to deceive the VLM, and DASH-LLM, based on LLM-generated queries. Subsequently, these image and text queries are used in an exploration phase to find real images in ReLAION-5B which trigger FP-hallucinations. In an exploitation step, we verify that these are not isolated errors by identifying semantically Figure 2. DASH: Given an object class, e.g. dining table, we generate text-based queries with DASH-LLM or image-based queries with DASH-OPT. Optimization: we optimize the latent variables of diffusion process to generate an image which yields yes for the VLM (Can you see dining table in this image?) and at the same time the object detector states that no dining table is present in the image. Exploration: the text and image queries are used for kNN-retrieval using CLIP similarity on ReLaion-5B. Exploitation: for successful images (VLM yes, object detector no) of the exploration phase we retrieve novel images via kNN-retrieval to check if the hallucination transfers to semantically similar images. Clustering: Finally, we cluster successful images of the exploitation step into semantically similar clusters of hallucinations of the VLM. similar images that consistently deceive the VLM without containing the object. Finally, as different queries may yield overlapping image groups, we cluster the results (see Fig. 5 for samples from all clusters for the object dam found by DASH). We summarize the workflow of DASH in Fig. 2. 3.1. DASH-LLM FP-hallucinations often arise from associations between the target object and other objects present in an image. This can result from co-occurrence at the image level (i.e., objects that are frequently photographed together) or co-occurrence in texts. We leverage the fact that large language models (LLMs) are trained on extensive text corpora and are wellsuited for generating lists of candidate prompts. Specifically, we use Llama 3.1-70B with carefully crafted system prompt to generate 50 text prompts, q1, . . . , q50, for each object. We ask the model to create prompts that could lead an AI model to falsely recognize the object due to the presence of spurious features, even though the object itself is not present in the images (see Appendix B). The LLM is instructed to avoid mentioning the object itself or any of its parts in the prompts and to avoid repeating queries. While this approach to generating text queries is simple, it proves to be quite effective. However, it has two limitations: first, it is entirely agnostic to the specific VLM being used. Second, even if the generated text queries are conceptually capable of causing the VLM to hallucinate, our CLIP-similarity-based retrieval on ReLAION-5B (see Sec. 3.3), may fail to retrieve the appropriate images. We address these limitations of DASH-LLM by using image queries generated through DASH-OPT, which are directly optimized to induce hallucinations in the VLM. 3.2. DASH-OPT The goal of DASH-OPT is to generate images that cause FP-hallucinations in the VLM. As our pipeline is supposed to be fully automatic, we require an alternative to human verification to determine whether the image contains the object. For this purpose, we employ an open-world object detectorin this case, OWLv2 [33]with very low detection threshold. This minimizes the chance of missing the object when it is actually in the image. However, some images may be incorrectly flagged as containing the object. We discuss this trade-off in more detail in Appendix G. Typically, optimizing in pixel space results in adversarial samples; however, previous work [1] has shown that meaningful solutions in the natural image manifold can be obtained by optimizing the input variables of diffusion process. While prior works [1, 43] rely on multi-step diffusion processeswhich are computationally intensivewe utilize recent advances in diffusion distillation [40] and use single-step diffusion process, significantly reducing computational cost. We denote by q(C) the output image of the diffusion process based on its conditioning (see Appendix for details). Below, we describe the two objectives we optimize to obtain the desired query image q(C). 3 We denote the text query Can you see OBJ in this image? by qstnOBJ for the considered object class OBJ. Since Yes corresponds to single token, we can directly optimize the cross-entropy loss for the standard next-token prediction of Yes in the VLM based on the image q(C): Lvlm(C) = log pvlm (Yes q(C), qstnOBJ) . (1) Similarly, we propose loss function to penalize the object detectors confidence in detecting the object class OBJ in the generated query image q(C): Ldet(C) = log (1 pdet (OBJ q(C))) , (2) where pdet (OBJ q(C)) is the object detectors confidence that the generated query image q(C) contains the class OBJ. The final optimization problem for the conditioning generating the image q(C) is then given as: min Lvlm(C) + Ldet(C) (3) This objective aims to make the VLM hallucinate the object in the resulting image q(C), while the detection loss ensures that the optimization process does not simply insert the object into the image. In practice, we initialize the optimization of in the diffusion process using real text prompts encoded by the diffusion models text encoder. We start with the same 50 LLM-generated prompts from DASH-LLM, creating 50 image queries q1, . . . , q50. However, our optimization process steers the resulting image q(C) away from images that either fail to fool the VLM or actually contain the object. Importantly, while the initial text prompts are derived purely from the LLM and are independent of the VLM, the image queries are directly optimized for the VLM, allowing us to uncover model-specific issues. Notably, the final queries q(C) often differ substantially from the initial text prompt outputs (see discussion in Sec. 4.1), leading DASH-OPT to discover larger variety of clusters compared to DASH-LLM as well as achieving higher success rate in producing images that mislead the VLM, as shown in Tab. 1. See Fig. 3 for examples of generated queries q(C) for the objects tench, leopard, piano. 3.3. Exploration, Exploitation and Clustering Given the 50 text queries generated with DASH-LLM respectively 50 image queries generated with DASH-OPT (denoted as qtext and qimg in the overview of the pipeline in Fig. 2), our goal is to find real images for which the VLM hallucinates the object OBJ even though it is not visible in the image. Our pipeline is entirely source-data-free, requiring only object labels and no human supervision in subsequent stages, making it easily scalable to large datasets. In the exploration phase, let q1, . . . , q50 denote the 50 text or image queries generated with DASH-LLM or DASH-OPT for given object. For each query, we retrieve 20 images from ReLAION-5B using the fast CLIP kNNindex of [2, 7], yielding total of 1,000 images per object. We then filter out all images where the object detector flags that the object is contained in the image. We use very conservative threshold, as discussed above, to ensure that the object is not mistakenly contained in the image. While this reduces our success rate, ensuring that the object is not present is crucial for the pipeline; otherwise, the VLM does not hallucinate when replying with Yes. We evaluate our automatic object-detector-based approach against human baseline in Section 4.1. Among the remaining images, we filter out all images that fail to trigger FP-hallucinationin the VLM. We call this stage exploration, as we explore whether retrieval based on our text and image queries leads to successful hallucination on real images. Furthermore, the exploration phase aims to generate diverse set of candidates for potential systematic hallucinations. While the goal of the exploration phase is to provide diverse set of candidates, the exploitation phase aims to achieve high VLM acceptance rate and reveal systematic hallucinations. During this phase, we retrieve 50 images for each image candidate from the exploration phase, again using kNN-retrieval on ReLAION-5B. However, note that we are effectively retrieving larger set of nearest neighbors as potential candidates since we filter out near duplicates using the perceptual metric DreamSim [11] with threshold of 0.9. This is necessary, as ReLAION-5B contain identical images or slight variations in resolution, viewpoint, or cropping. Our goal is to find semantically similar images in the sense of similar image composition and object content but not just small variations. While the exploration phase can use either text or image queries, the exploitation phase always uses image queries derived from the exploration phase. This is particularly useful since it results in natural clustering of semantically similar images. To make this even more explicit, we further merge several of these preclusters, i.e. images that are neighbors of the same source image, using agglomerative clustering with average linkage in the CLIP embedding space. Note that we overload notation by using DASH-LLM and DASH-OPT to refer to the results of the full pipeline, not just the initial query generation. All stages of the pipeline are summarized in Fig. 2. 4. Experiments For our experiments, we use total of 380 object catSpecifically, we include the 80 classes from egories. COCO [25], 100 randomly selected object categories from Objects365 [45] and 100 classes from ImageNet which have been used in Spurious ImageNet [34]. Additionally, we sort the objects in OpenImages [20] based on their occurrence frequency and create four subsets, each containing 25 objects. The first subset corresponds to the most com4 OBJ ptarmigan Cluster Size: 190, Query: mountain valley with few scattered trees and stream. DASH-LLM baumkuchen Cluster Size: 389, Query: traditional German Christmas pyramid with candles and ornaments. cello Cluster Size: 62, Query: music sheet with intricate notes and markings. OBJ tench Query DASH-OPT Cluster Size: 170 leopard Query Cluster Size: 276 piano Query Cluster Size: 150 Figure 3. Examples of systematic FP-hallucination clusters found by DASH for PaliGemma: We present six hallucination clusters, each for different objectthree identified by DASH-LLM and three by DASH-OPT. For each cluster, we show sample of images and the total number of images. For each of these images, PaliGemma answers yes to Can you see OBJ in this image? while the object detector reports confidence below 0.1. None of the images actually contain the object. We also provide the text (DASH-LLM) and image queries (DASH-OPT) used for retrieval during exploration for the majority of the cluster. mon objects in OpenImages. The second and third correspond to objects around the 10% quantile and the median in occurrence frequency, respectively. The last subset contains the least common objects, some without single occurrence. We use these subsets to assess the influence of an objects occurence frequency on the hallucination rate in App. E. For our retrieval pipeline described in Sec. 3, we use PaliGemma [3] and LLaVA-NeXT [28] in the Vicuna [36] and Mistral [16] variants as VLMs. As the large-scale image dataset, we utilize ReLAION-5B [39, 44] with CLIP index for fast kNN retrieval [2, 7]. See App. for an experiment on the reverse task: VLM responds No although the object is visible. Source Model DASH Variant PaliG LN Vic LN Mis LLM OPT LLM OPT LLM OPT Total Images 99.3K 221.7K 162.4K 252.0K 78.5K 133.8K Total Clusters Avg Clstr per Object Avg Imgs per Clstr 1892 3895 3632 4632 2001 3229 5.0 10.3 9.6 12. 5.3 8.5 52.5 56.9 44.7 54.4 39.3 41.5 Table 1. Retrieval results for DASH-LLM and DASH-OPT across PaliGemma, LLaVA-NeXT Vicuna, and LLaVA-NeXT Mistral, accumulated over the 380 object categories from all datasets. 4.1. Results of DASH We report the results of DASH in Tab. 1. DASH-LLM finds total of 99.3K images for PaliGemma, 162.4K images for LLaVA-NeXT Vicuna, and 78.5K images for LLaVANeXT Mistral, while DASH-OPT obtains 221.7K, 252.0K, and 133.8K. In addition to more images, DASH-OPT yields larger number of clusters: 3895/4632/3229 compared to 1892/3632/2001 for DASH-LLM."
        },
        {
            "title": "Some of the clusters for both approaches are shown in",
            "content": "5 DASH-LLM - Dam - 4 Clusters and 84 images Cluster Size: 45 Cluster Size: 15 Cluster Size: 13 Cluster Size: DASH-OPT - Dam - 10 Clusters and 186 Images Cluster Size: 57 Cluster Size: 34 Cluster Size: 24 Cluster Size: 20 Cluster Size: Cluster Size: 10 Cluster Size: 7 Cluster Size: 6 Cluster Size: 6 Cluster Size: 5 Figure 5. All clusters for DASH-LLM and DASH-OPT for the object Dam using LLaVA-NeXT Vicuna. DASH-OPT identifies larger total number of clusters and images, capturing broader diversity of visuals. This demonstrates that DASH-OPT can uncover unexpected systematic hallucination patterns, such as cartoon frogs and dinosaurs, orange leaves, bare feet, or park bench, whereas DASH-LLM tends to highlight failure modes more directly linked to the object, such as water associated with Dam. reference datasets. Second, the most similar images that are contained might not be specific enough to fool the VLM, even when they are similar in object, color, and composition. For example, while Objects365 contains trees, only Baobab trees are detected as sausage. This emphasizes the necessity of large dataset for searching hallucinations. Human Verification: To validate the object detector in DASH, we use human supervision to verify the absence of the object. In particular, for all 380 objects found for PaliGemma with DASH-OPT, we randomly select 10 images. We then label them as yes if the object is visible, no if it is absent, and ambiguous for corner cases. 6 Figure 4. Histogram illustrating the minimum embedding distance from success images to the nearest LLM prompt for DASH-LLM and DASH-OPT. While both methods use these LLM prompts in their exploration stage, the image-based method is able to find unexpected hallucinations far away from the initial LLM prompts. Fig. 3 (and App. F) along with one of the corresponding text or image queries, respectively. Considering the clusters found by DASH-LLM, the associations between objects and hallucinations are relatively clear and closely correspond to the text queries: water cannon is often present in images of fireboats; the mountainsides are natural habitat of ptarmigans; Baumkuchen is part of German Christmas traditions; and music sheets are linked to musical instruments like cellos. On the other hand, the results of DASH-OPT indicate that it can uncover hallucinations in completely different context by moving away from the initial text queries during optimization. For these subgroups, potential causes are less obvious, e.g. at first sight, images of beads do not seem to be related in any way to the animal leopard. However, there exist so-called leopard beads, i.e. beads showing leopard pattern. While the beads in our cluster are not of this type, their existence might play the role of confounder. We further validate the larger exploration range of DASH-OPT over DASH-LLM in Fig. 4, where we illustrate the CLIP distance between the image embedding and the text embedding of the closest original text prompt. While both LLM and OPT use the same text prompts, the additional optimization of the OPT variant allows it to find images that are further away. This can also be seen in Fig. 5, where we visualize all clusters detected for the object Dam using LLaVA-NeXT Vicuna. DASH-LLM results in 4 clusters with total of 84 images, while DASH-OPT finds 10 clusters comprising 186 images. The clusters resulting from LLM queries represent natural waterfalls and water surfaces and are also detected using OPT queries. However, beyond those, DASH-OPT also surfaces range of more diverse and unexpected patterns, including different colors (leaves/sunsets) and distribution shifts (comic frogs/comic dragons). Advantage of larger retrieval pool: In Fig. 18, we compare hallucinations found by DASH to their nearest neighbors in the curated dataset, which also contains the object. Note that, first, some of the objects (e.g., rubber boot) or types of images (e.g., maps) are not contained in the smaller For example, it can be difficult to assess whether closeup photo of flat surface is dessert table or coffee table. Across all images, we find that 5.2% contain the object and 7.8% are ambiguous. For comparison, we perform the same annotation on the corresponding subset of POPE, i.e. image/question pairs where PaliGemma responds Yes, but the COCO ground truth indicates No. Among these 137 alleged false positives, 25.5% contain the object, and 22.6% are marked as ambiguous (examples in App. H). This demonstrates that our conservative threshold for the object detector yields reliable automatic pipeline with less errors compared to POPE. Transfer across prompts: We check the influence of the type of the question on the evaluation of the FPhallucinations found by DASH by testing 11 prompts (Is OBJ in the image?, Does this image contain OBJ?, . . . ) on PaliGemma, LN Vicuna, and LN Mistral (results in App. M). LN Vicuna has an average Yes rate of 82.3% 6.7%, similarly LN Mistral 78.9% 7.0% showing that the prompt has only minor influence. For PaliGemma, which was trained on this task on OpenImages, the Yes-rate drops to 71.6% 18.7% with higher variance as prompts similar to the training prompt (Is OBJ in the image?, 31% Yes) show lower transfer. 4.2. Transfer to unseen VLMs DASH searches specifically for images causing hallucinations for target model. In this section, we investigate how the found systematic hallucinations transfer to unseen VLMs. Tab. 2 reports the transfer rate, defined as the proportion of images found by DASH (for PaliGemma, LLaVA-NeXT Vicuna, and LLaVA-NeXT Mistral) that trigger FP-hallucination in the model. We use these transfer rates to quantify the impact of the LLM backbone, vision encoder, and model scale on FP-hallucinations. In addition, to quantify the detection-hallucination trade-off, we evaluate the true positive rate on subset of images from COCO, Object365, and ImageNet showing the object (see App. I.2) which we call TPR-ICO. Influence of LLM backbone: We examine the LLM backbones influence on the models vulnerability to systematic hallucinations by considering three versions of LLaVANeXT, based on the LLMs Vicuna, Mistral, and Llama. Apart from the LLM, these three share the same vision backbone, architecture, and training procedure. Thus, differences in their robustness to hallucinations can be attributed to the LLM. On images found for PaliGemma, the Vicuna variant has the highest transfer rates (DASHLLM 49% and DASH-OPT 43%), followed by Mistral (31% and 23%) and Llama (26% and 22%). The TPRICO indicates (Vicuna 87%, Mistral 83%, Llama 81%) that the models with higher transfer rates in general reply with Yes more frequently. Overall, one can conclude that the coil spring shipping box balance beam vase watch postcard Figure 6. Object hallucination benchmark DASH-B: examples from the negative set of DASH-B (images and object label) where GPT-4o-mini, the best scoring model on DASH-B (see Tab. J) hallucinates the object even though it is not present in the image. LLM backbone significantly impacts the vulnerability to FP-hallucinations. Influence of vision encoder: Similar to the LLM backbone, we compare three Prismatic [17] models that share the same LLM but use different ViT-L [6] vision encoders, namely CLIP [38] and SigLIP [60]. On average, CLIP results in higher transfer rate of 60% while the SigLIP variant is only fooled by 43% of the images, showing that the choice of the vision encoder has major impact. However, smaller hallucination transfer rates also come with smaller TPR-ICO, i.e. SigLIP (77%) is less likely to respond with Yes, even on data containing the object, than CLIP (83%). Model size: We evaluate Qwen2-VL [52] both with Qwen2-7B [58] and 72B as language backbone. Overall, the size of the used LLM seems to have only small effect on the VLMs vulnerability to hallucinations ( 18% to 19% ) but as both models have the same TPR-ICO of 85%, model scaling can improve the detection-hallucination tradeoff. 4.3. DASH-B: Object Hallucination Benchmark As the popular POPE [24] benchmark for object hallucinations seems saturated and contains significant label noise, novel hard object hallucination benchmark is needed to measure further progress. For this purpose we use all images found by DASH which transfer to both Qwen2-VL and Llama 3.2-VL (see App. details). We verify those images for selection of 70 objects by consensus of two human labelers and limit the minimal and maximal number of images to 3 and 50, respectively. This results in 1341 images, see Fig. 6 for examples. We use this as our set of negatives and add the same amount of images containing the objects. As performance measure, we use accuracy over all 2642 images. Tab. 3, contains the results for four models not used in the generation of DASH-B. Given the label noise (see App. H), the high true negative rates (TNR) on POPE 7 VLM Type Vision Encoder LLM PaliGemma SigLIP Gemma LLaVA-NeXT CLIP CLIP Vicuna Mistral CLIP Llama 3. Prismatic CLIP Vicuna SigLIP Vicuna Qwen2-VL Custom Qwen2-7B Custom Qwen2-72B Llama 3.2-VL Custom Llama 3. PaliG LN Vic LN Mis LLM OPT LLM OPT LLM OPT Average transfer TPR-ICO - - 0.34 0.30 0.39 0.35 0. 0.81 0.49 0.43 - - 0.67 0.66 0.56 0. 0.31 0.23 0.39 0.33 - - 0.31 0.83 0.26 0. 0.30 0.27 0.42 0.41 0.31 0.81 0.60 0.49 0.64 0. 0.67 0.63 0.60 0.83 0.44 0.34 0.44 0.38 0.51 0. 0.43 0.77 0.18 0.17 0.18 0.15 0.25 0.24 0. 0.85 0.18 0.15 0.16 0.13 0.23 0.21 0.18 0. 0.09 0.10 0.10 0.09 0.14 0.15 0.11 0.80 Table 2. Transfer of DASH images (rows) to different VLMs (columns) Different LLM backbones(LLaVA-NeXT) and different vision encoders (Prismatic) have significant impact on the vulnerability to FP-hallucinations, but the LLM size (Qwen2-VL) shows only small effect. TPR-ICO is the true positive rate calculated on ground-truth validation data from ImageNet, COCO, and Objects365 corresponding to the employed object categories. Models PaliG2-3B Ovis2-8B LLaVa-OneVision 4o-mini DASH-B Amber Ex. R-Bench POPE Caption VQA DASH-B Acc. 68.9% 71.4% DASH-B TNR POPE TNR 40.9% 97.3% 44.8% 94.9% 75.1% 60.1% 95.8% 86.3% 76.7% -2 Table 3. Object Hallucination Benchmark DASH-B: Compared to POPE, DASH-B is not saturated as demonstrated by the substantially lower true negative rates (TNR). More results in Tab. 6. (96.0% 1.2%) suggest that the benchmark is saturated and provides limited insight regarding FP-hallucinations. In contrast, the models exhibit significantly lower TNR on DASH-B (48.6% 10.1%). We show results on DASH-B for more models in Tab. J. 4.4. Fine-Tuning on DASH We test the usage of the images found by DASH for finetuning PaliGemma in small proof-of-concept (details in App. K): First, we merge the results of DASH-LLM and DASH-OPT for PaliGemma and ensure that none of the images of DASH-B are part of this train set. Per object, we sample 200 random images from this set and additionally 400 positive samples, i.e. images that contain the object. During fine-tuning, the models is trained to output No on DASH images and predict Yes on the positive samples (see App. for more details on the fine-tuning setup). Tab. 4 contains the results after finetuning: Accuracy is significantly improved on DASH-B (+11.6%) due to an increase in TNR and TPR (see App. K) and also increases on Amber Existence [51] (+2.2%) as well as RBench [57] (+0.3%). However, we observe small decrease of accuracy on POPE. We also evaluate two captioning tasks (Flickr30k, COCO) and two VQA tasks 2In our POPE evaluation, GPT-4o-mini only provided valid replies for 77.5% of the images and achieved TNR of 92.7% among those. PaliG 56.4% 68.0% 95.4% 80.2% 86.4% 99.2 93.2% 79.9% 87.2% 101.0 70.4% 69.5% +ft Table 4. Fine-tuning on DASH: Our fine-tuning strategy improves performance on DASH-B, Amber Existence and R-Bench. (TextVQA, VQAv2) and report the averaged results. The minor performance drop (captioning 1.8, VQA 0.9%) is expected as we are fine-tuning the model on different task. Generally, the DASH data should be integrated into curriculum learning scheme, as one of several tasks, which is out of scope for this work. 5. Conclusion and Limitations We have demonstrated that DASH is an effective, automatic pipeline for identifying systematic hallucinations in VLMs. Contrary to the belief that object hallucinations are no longer an issue, we find that they persist extensively when using VLMs in an open world scenario. To address this, we propose DASH-B for more rigorous assessment of these errors. Our initial experiments on mitigation suggest that DASH could be valuable addition to the training pipeline of future VLMs. Limitations: we note that achieving exhaustive coverage of all systematic hallucinations with DASH is not possible, as this would require fully exhaustive approach. Even with such an approach, and despite ReLAION-5B providing significant coverage of natural images, some images remain underrepresented. As result, even when we identify an image where the VLM hallucinates, there may not be enough semantically similar images in ReLAION-5B to consider it systematic hallucination. For the most advanced VLMs, our conservative threshold for the object detector could pose limitation, potentially."
        },
        {
            "title": "Acknowledgments",
            "content": "We are grateful for support by the DFG, Project number 390727645, and the Carl Zeiss Foundation, project Certification and Foundations of Safe Machine Learning Systems in Healthcare and thank the IMPRS-IS for supporting YN."
        },
        {
            "title": "References",
            "content": "[1] Maximilian Augustin, Yannic Neuhaus, and Matthias Hein. Dig-in: Diffusion guidance for investigating networksuncovering classifier differences neuron visualisations and visual counterfactual explanations. In CVPR, 2024. 2, 3 [2] Romain Beaumont. Clip retrieval: Easily compute clip embeddings and build clip retrieval system with them. http s://github.com/rom1504/clipretrieval, 2022. 4, 5 [3] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Boˇsnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. PaliGemma: versatile 3B VLM for transfer, 2024. 2, 5, 36, 37 [4] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 37 [5] Myung Jin Choi, Antonio Torralba, and Alan S. Willsky. Context models and out-of-context objects. Pattern Recognition Letters, 33(7):853862, 2012. 2 [6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. 7 [7] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herve Jegou. The faiss library, 2024. 4, 5 [8] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for In Proceedings evaluating large multi-modality models. of the 32nd ACM International Conference on Multimedia, pages 1119811201, 2024. https://huggingface.co /spaces/opencompass/open_vlm_leaderboard (assessed 08.03.2025). 2, 37 [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie 9 Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, ChingHsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco Guzman, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikolay Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vıtor Albiero, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao. The llama 3 herd of models, 2024. 2, 13, 36, 37 [10] Flickr. Flickr API Documentation, 2025. Accessed: 30-Mar2025. [11] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. arXiv preprint arXiv:2306.09344, 2023. 4, 14 [12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA matter: Elevating the role of image understanding in Visual Question Answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 38 [13] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In AAAI, 2024. 2 [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 38 [15] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. 14 [16] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. 5, 36, 10 [17] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In ICLR, 2024. 7, 35, 36 [18] Prannay Kaul, Zhizhong Li, Hao Yang, Yonatan Dukler, Ashwin Swaminathan, CJ Taylor, and Stefano Soatto. Throne: An object-based hallucination benchmark for the free-form generations of large vision-language models. In CVPR, 2024. 2 [19] Diederik Kingma and Jimmy Ba. Adam: method for arXiv preprint arXiv:1412.6980, stochastic optimization. 2014. 13 [20] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. [21] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In CVPR, 2024. 2 [22] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024. 36 [23] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 37 [24] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In EMNLP, 2023. 1, 2, 7 [25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 1, 4, 38 [26] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large multi-modal models via robust instruction tuning. In ICLR, 2024. 2 [27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 36 [28] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 5, 36, [29] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253, 2024. 1 [30] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. 37 [31] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Latent consistency models: Synthesizing highZhao. resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 13 [32] Jan Hendrik Metzen, Robin Hutmacher, Grace Hua, Valentyn Boreiko, and Dan Zhang. Identification of systematic errors of image classifiers on rare subgroups. In ICCV, 2023. [33] Matthias Minderer, Alexey Gritsenko, and Neil Houlsby. Scaling open-vocabulary object detection. In NeurIPS, 2024. 3, 16 [34] Yannic Neuhaus, Maximilian Augustin, Valentyn Boreiko, and Matthias Hein. Spurious features everywhere largescale detection of harmful spurious features in imagenet. In ICCV, 2023. 2, 4 [35] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 36 [36] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4, 2023. 5, 36, 37 [37] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 13 [38] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 7, 14, [39] ReLAION. Releasing RE-LAION 5B: Transparent iteration on LAION-5B with additional safety fixes. https://la ion.ai/blog/relaion-5b/, 2024. 2, 5, 14 [40] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. arXiv preprint arXiv:2404.13686, 2024. 3, 13 [41] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In EMLNP, 2018. 2 [42] Dvir Samuel, Rami Ben-Ari, Nir Darshan, Haggai Maron, and Gal Chechik. Norm-guided latent space exploration for text-to-image generation. NeurIPS, 2024. 13 [43] Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, and Gal Chechik. Generating images of rare concepts using pretrained diffusion models. In AAAI, 2024. 3 [44] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. 5, [45] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In ICCV, 2019. 4 11 Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. 7, 36 [59] Peter Young, Alice Lai, Micah Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. 38 [60] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 7, 36 [61] Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, and Chengzhi Mao. Imagenet-d: Benchmarking neural network robustness on diffusion synthetic object. In CVPR, 2024. 2 [62] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In ICLR, 2024. [46] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 83178326, 2019. 38 [47] Sahil Singla and Soheil Feizi. Salient imagenet: How to discover spurious features in deep learning? In ICLR, 2022. 2 [48] Andreas Steiner, Andre Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, et al. Paligemma 2: family of versatile vlms for transfer. arXiv preprint arXiv:2412.03555, 2024. 37 [49] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. 36 [50] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In CVPR, 2024. 2 [51] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and Jitao Sang. An llm-free multi-dimensional benchmark for mllms hallucination evaluation. arXiv preprint arXiv:2311.07397, 2023. 1, 2, 8 [52] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 7 [53] Zhecan Wang, Garrett Bingham, Adams Yu, Quoc Le, Thang Luong, and Golnaz Ghiasi. Haloquest: visual hallucination dataset for advancing multimodal reasoning. In ECCV, 2024. [54] Ryan Webster, Julien Rabin, Loic Simon, and Frederic JuarXiv preprint rie. On the de-duplication of laion-2b. arXiv:2303.12733, 2023. 14 [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. 13 [56] Wolf. Huggingfaces transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771, 2019. 35 [57] Mingrui Wu, Jiayi Ji, Oucheng Huang, Jiale Li, Yuhang Wu, Xiaoshuai Sun, and Rongrong Ji. Evaluating and analyzing relationship hallucinations in large vision-language models. In ICML, 2024. 8 [58] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, 12 A. Overview We give an overview over the contents of the Appendix. In Appendix B, we present additional details about the creation of initial LLM queries for DASH-LLM, including the prompts used to create the queries with Llama. In Appendix C, we break down the optimization of the DASH-OPT image queries in the latent space of the distilled SDXL model. In Appendix D, we give additional details about the ReLAION exploration/exploitation retrieval. In Appendix E, we investigate the influence of an objects occurence frequency on hallucination rates. In Appendix F, we present additional results for DASH on PaliGemma, LLaVA-NeXT Vicuna and Mistral. In Appendix F.3, we also present DASH results on ReLAION next to the most similar images from COCO and Objects365. In Appendix G, we further explore the performance of the object detector in DASH to filter out images containing the object. In Appendix H, we show examples of COCO annotation errors and discuss their effect on the POPE benchmark. In Appendix I, we present extended results about the transfer between DASH images to other VLMs and also present information about all VLMs used in the paper in Appendix I.1 and the true positive rate calculation in Appendix I.2 In Appendix J, we describe the image selection process and discuss different metrics for our proposed benchmark DASH-B. Additionally, we report more results on range of VLMs. In Appendix K, we give further details about the mitigation finetuning using DASH. In Appendix L, we provide proof of concept for possible application of our pipeline to the reverse task and discuss problems. In Appendix M, we examine the generalization of DASH results to different prompts than the one used in our experiments. B. DASH-LLM Prompt The prompts supplied to Llama-3.1-70B [9] to create the queries for DASH-LLM are given in Fig. 7 and 8. We also use the same queries to initialize the generation of the image queries in DASH-OPT. To generate the queries, we use the system prompt provided in Figure 7. We then pass the input object: OBJ to the LLM, which generates an initial list of 50 queries. Since we noticed that these initial queries can sometimes contain references to the object or duplicates, we use simplified version of chain-of-thought prompting [55]. After the LLM generates the initial list of 50 queries, we pass the follow-up prompt provided in Figure 8 to the model, which responds with an updated list of 50 queries. C. DASH-OPT Optimization Optimization examples: In Fig. 9, we present the optimization trajectory of DASH-OPT for two images. In Fig. 10, we provide additional examples where we show only the initialization (i.e., the image generated by SDXL using the text query from DASH-LLM without any optimization) and the final query image produced by DASHOPT after optimization. These examples illustrate that DASH-OPT is capable of generating unexpected FPhallucinations. For instance, it introduces beads for leopard, which are absent from the original caption that merely describes rocks cracks and fissures. Similarly, we demonstrate the transformation of set of scented lotions ... on shelf into scene of person shopping for bathing bombs when optimizing for hallucinations related to the object bathtub. The corresponding retrieved images, which validate that these phenomena are not limited to synthetic data but also occur with real images, can be found in Fig. 3 for leopard and Fig. 13 for bathtub. Implementation details: For DASH-OPT, we use the distilled version of Stable Diffusion XL (SDXL) [37] from [40]. In particular, we use the single-step SDXL U-Net together with the Latent Consistency Model (LCM) scheduler [31], setting the start timestep to 800. For optimization, we use the Adam optimizer [19] for 25 steps with step size of 0.1, applying linear warmup over the first 3 steps. The gradient is clipped to an L2 norm of 0.1 at every step. When using deterministic scheduler like the single-step LCM scheduler, three variables determine the output of the diffusion process. The first is the Gaussian random latent drawn at the start of the generation. The second and third are the text encodings of the user prompt generated by the two different CLIP text encoders in SDXL. We optimize all three variables and additionally apply step size factor of 0.1 for the random latent. For the random latent, we also employ the chi-square latent regularization method from [42]. Note that the text encodings are initialized using the text queries from DASH-LLM (Appendix B). As optimization loss, we use Eq. (3). Note that pdet (OBJ q(C)) is computed as the maximum confidence overall bounding boxes and thresholded to 0 for detection probabilities smaller than 0.05. Since the optimization problem is highly non-convex, the last image is not necessarily the one with the best overall loss, and hence, we use the one with the lowest loss over all generated images as the query for DASH-OPT."
        },
        {
            "title": "The optimization takes",
            "content": "for PaliGemma and one minute for the LLaVA-NeXT models on an NVIDIA A100 GPU with 80GB of memory. around 50 seconds D. Retrieval process, exploration and exploitation The ReLAION-5B [39, 44] index, which we use for retrieval during the exploration and exploitation stages, is based on OpenCLIP ViT-H [15]. During retrieval, we apply DreamSim [11] to remove near-duplicate images with similarity score greater than 0.9, as LAION is estimated to contain up to 30% duplicated data [54]. For clustering in the exploitation phase, we first group all images retrieved for the same image during the exploration phase into preclusters. These pre-clusters are then merged using agglomerative clustering to form the final clusters. We employ average linkage based on DreamSim distances (note that in the main paper we wrongly stated that we use the CLIP embedding distance instead - however, note that DreamSim is based on fine-tuned CLIP and DINO embeddings - we correct this in the final version), with maximum allowed merge threshold of 0.6. E. Impact of object occurence frequency on object hallucinations We run DASH on subsets of OpenImages with different occurrence frequencies and show the average number of images per object for each split found by DASH for PaliGemma LLaVA-NeXT Vicuna, and LLaVA-NeXT Mistral in Fig. 11. The results for PaliGemma are particularly interesting, as the model was trained on similar task (Is there object in the image?) on this dataset. Overall, it is easier to find systematic hallucinations for objects that are very rare (on average 506 images) and gets harder if they occur more frequently. Especially for the frequent objects, the optimized queries help to find more of the rarer hallucinations, resulting in significantly more images per object for DASH-OPT on the corresponding splits compared to DASH-LLM. The observed trends for PaliGemma also hold true for LLaVA-NeXT Vicuna and LLaVA-NeXT Mistral. Both are much more vulnerable on rare objects, and object frequency seems to be strong indicator of an objects vulnerability although the LLaVA-NeXT models are not trained on OpenImages. However, it is possible that the distribution of images in OpenImages is similar to that of other large-scale datasets, such as those used to train the CLIP [38] models employed in LLaVA-NeXT. F. DASH Results Extended F.1. Additional qualitative examples In Figures 12 to 17 we present additional retrieval results, similar to those from Fig. 3 for DASH-LLM and DASHOPT. In particular, we include results for LLaVA-NeXT Vicuna and Mistral. As these Figures demonstrate, all 3 VLMs suffer from substantial amount of type II hallucinations. In Fig. 12, we show the clusters of images generated using DASH-LLM for PaliGemma. The examples illustrate how the LLM-generated queries lead to images that are logically connected to the object in semantic sense. For instance, for the object Barracouta, we observe coastal towns and harbors built in Minecraft, likely reflecting the objects marine context. For Fireboat, we find images of the police using water cannons, which are often commonly found on Fireboat. In Fig. 13, we present examples of clusters identified using DASH-OPT for the same VLM and various objects, highlighting cases of unknown unknowns. For instance, for the object Bathtub, the cluster includes colorful images of bath bombs, rather than bathtubs, suggesting that the model has learned to associate the object label with related items rather than the physical object itself. Similarly, for the object Puck, instead of hockey pucks, the cluster prominently features images of stacked oranges and other spherical objects, reflecting semantic confusion between shape and context. For Sulphur Butterfly, the cluster contains ornamental decorations and holiday-themed items, diverging significantly from the actual insect. The clusters shown in Fig. 14 illustrate examples generated using DASH-LLM with LLaVA-NeXT Vicuna as the VLM. These examples reflect expected yet interesting associations generated by the model based on LLM-guided queries. For instance, for the object Academic Gown, the cluster includes university seals and architectural elements from academic institutions, which are logically associated with the concept of academia but deviate visually from the actual object. Similarly, for Chain Mail, the model identifies medieval swords and weaponry, which are contextually related to chain mail in historical settings. The object Fountain Pen generates cluster dominated by handwritten scripts and paper stacks, reinforcing semantic association with writing and stationery. The cluster for Coral Fungus features lichen-covered tree bark and textures, highlighting broader misinterpretation of the objects actual form and an emphasis on natural growth patterns. Finally, for Postcard, the cluster predominantly displays boardwalks and scenic ocean views, which align with common themes of postcards. In Fig. 15, we showcase clusters generated using DASHOPT with LLaVA-NeXT Vicuna, highlighting more unexpected results where the VLM demonstrates surprising or unintended associations. For Dogsled, the cluster contains images of snowshoes and other winter-related gear, which are contextually linked to snowy environments but do not represent the object itself. The object Strawberry leads to cluster featuring images of festive door decorations and wreaths. This unexpected association likely arises from the models inability to separate the red and green color palette of strawberries from decorative elements. For 14 Beehive, the cluster includes surprising array of human portraits, particularly women in colorful settings. This suggests that the VLM may associate the term beehive with hairstyle rather than the physical structure created by bees. In Fig. 16, we present examples of clusters generated using DASH-LLM with LLaVA-NeXT Mistral. For the object Band Aid, the cluster contains images of people holding or bandaging injured wrists, reflecting logical semantic association with the concept of injury and care. Similarly, for Gondola, the cluster features small shops, which aligns with broader cultural and contextual understanding of gondolas as part of scenic, tourist-driven environments. The object Dumbbell leads to cluster of colorful exercise balls, emphasizing fitness and gym-related settings, likely derived from contextual overlaps. For Lighter, the cluster showcases dimly lit rooms with smoky haze in video games, reflecting plausible connection to the objects typical use in dark settings. The Lighthouse cluster includes solitary piers and fishing-related environments, reinforcing the models interpretation of the lighthouses association with remote coastal locations. In Fig. 17, we present clusters generated using DASHOPT with LLaVA-NeXT Mistral. For the object Agama, the cluster prominently features various wild cats. For Bulletproof Vest, the cluster includes images of surveillance and monitoring rooms with large screens, likely due to the association of vests with security and law enforcement. The object Horizontal Bar leads to cluster filled with water bottles and similar cylindrical objects, reflecting superficial visual similarity in shape but entirely unrelated semantics. For Shallot, the cluster displays images of modern kitchens and industrial food preparation areas, suggesting that the VLM has learned to associate the object with its culinary context rather than its specific visual characteristics. For Bluehead, instead of the fish species, the cluster includes images of blue-themed furniture and interior designs, driven by the color association rather than the object itself. For Bird, the cluster prominently features butterflies and flowers, illustrating misalignment between the object category and the broader semantic associations of natural imagery. Lastly, the clusters for Hat and Balloon show out-of-distribution images that are not logically connected to the object. F.2. All Clusters Visualizations In Fig. 20, we present all clusters identified for DASH-LLM and DASH-OPT for the object Ptarmigan. While Ptarmigan refers to bird species, the clusters reveal range of false positives, including images of mountain landscapes, alpine environments, abstract artistic representations, and completely unrelated objects. This indicates that the VLM conflates semantic and contextual cues with visual content, leading to systematic hallucinations. Interestingly, many of these errors may stem from the existence of places named Ptarmigan, such as Ptarmigan Peak in Colorado, Utah, and Alaska, or Ptarmigan Ridge and Ptarmigan Traverse in Washington. Even though these locations are unrelated to the bird, the VLM erroneously associates them with the object Ptarmigan. Our analysis confirms that these places are distinct mountainsides with unrelated names, demonstrating that the VLM has learned flawed representation of Ptarmigan that includes variety of unrelated mountainous scenes. Additionally, DASH-OPT uncovers further unknown unknowns, such as rare or abstract scenes where ptarmigan is highly unlikely, including auroras, surreal artwork, and stylized objects. In Fig. 21, we present all clusters found for DASH-LLM and DASH-OPT for the object Baumkuchen on LLaVANeXT Mistral. For DASH-LLM, we observe that the clusters include no images of Baumkuchen, traditional German layered cake, but variety of unrelated objects and scenes. These false positives encompass German cultural artifacts, traditional buildings, festivals, and abstract artistic representations, indicating that the VLM has conflated Baumkuchen with broader semantic or cultural cues tied to German traditions. DASH-OPT uncovers additional unknown unknowns. Alongside unrelated cultural goods like Christmas decorations, traditional crafts, and books which we have also found for DASH-LLM, we also find additional systematic vulnerabilities. For example, we find cluster of 111 images containing fountain pens, but also cluster of 66 images containing wooden kitchen utensils. We also note that the cluster of size 8 which contains cake does not contain any images of Baumkuchen. F.3. DASH vs Reference Datasets the target class, As stated in the main paper, we also compare our DASH images to reference datasets such as COCO or Objects365 which are commonly used to construct hallucination benchIn Fig. 18, we demonstrate images that cause marks. PaliGemma to detect identified using DASH-OPT, alongside their nearest neighbors from the reference datasets COCO and Objects365. We observe that neither the full COCO training set (80K samples) nor Objects365 (1.7M samples) contain the systematic errors uncovered by DASH, as all nearest neighbors are not detected by the VLM. This highlights that our open-world search in ReLAION-5B is necessary to detect these hallucinations, which would not be possible even with reasonably large datasets like Objects365. Specifically, with DASH, we find that PaliGemma incorrectly answers yes for colorful Wellington boots as apple and for Baobab trees as sausage."
        },
        {
            "title": "These examples illustrate the limitations of relying",
            "content": "15 ficult to guarantee their absence in the image. In the case of Airplane, the interiors shown could represent futuristic airplane or train designs, making it ambiguous. For Train, the low image resolution hinders the inference of specific objects presence. Furthermore, ambiguity arises from the object labels themselves in some datasets. For example, in Objects365, Glasses refers to eyewear, but the images often contain multiple glass objects, causing confusion. Likewise, Soccer refers exclusively to soccer ball in Objects365, whereas the sport itself is not well-defined object, leading to uncertainty about whether to label images of referees as Yes or no. In addition to ambiguous cases, we identified several failure cases of the object detector during our human evaluation. All images in these cases had confidence score below the threshold of 0.1 and were therefore not rejected by our automated pipeline. For Mountain bike, the primary issue was that the objects were very small and difficult to spot. In other instances, such as Pot or Faucet, the objects are clearly visible, but the object detector failed to recognize them. For Car, the detector did not classify trucks or vans as cars. Similarly, for Mouse and Egg, the detector struggled with distribution shifts, failing to recognize comic or plush mice and colored eggs, respectively. These observations suggest that while our object detector generally performs well, there are specific categories and scenarios where it struggles, either due to ambiguity in object definitions or limitations in detecting certain object variations. H. Effect of COCO annotation errors on POPE Current VLMs only produce small number of false positives on the POPE benchmark, e.g. PaliGemma predicts Yes on 137 out of the 4500 samples which do not contain the corresponding object according to the COCO annotations. We re-annotate these images and assign the labels yes if the object is visible in the image, no if the object is not visible in the image, ambiguous for corner cases where it is not clear whether the object is present or not. The result of our labeling is that 35 (25.5%) of the alleged false positives actually do contain the object which means that the model reply Yes is actually correct (see Fig. 25 for examples). In addition, 31 (22.6%) of the images receive the label ambiguous. This large amount of label noise among the remaining false positives indicates that the POPE benchmark is saturated. solely on existing datasets for identifying hallucinations in VLMs. In particular, just because target object is contained in dataset like COCO or Objects365, as are all examples presented in Fig. 18, does not guarantee that objects that are not contained in this dataset cannot cause VLM to hallucinate the target object. Our method uncovers novel failure cases that are absent in standard benchmarks, emphasizing the importance of an open-world search strategy for comprehensive evaluation. F.4. Larger exploration range of DASH-OPT over DASH-LLM In Fig. 19, we present extended version of Fig. 4 for PaliGemma, LLaVA-NeXT Vicuna, and LLaVA-NeXT Mistral which demonstrates that DASH-OPT achieves greater diversity of images than DASH-LLM. G. Object Detector: False Negative Rate vs"
        },
        {
            "title": "False Positive Rate",
            "content": "For the object detector OWLv2 [33] in our pipeline, we pass the object name OBJ and the image to the model. The model then returns predefined number of bounding boxes, each with confidence score in the range [0,1]. We take the maximum confidence over all bounding boxes and use this as the probability of the image containing the object, i.e., pdet (OBJ img). We then reject all images where pdet is greater than our threshold of 0.1. To verify our automatic pipeline, and especially the conservative threshold for the object detector, we manually labeled 10 random images for each object for DASH-OPT on PaliGemma. As stated in Sec. 4.1, we use the labels Yes if the object is visible, No if it is absent, and ambiguous for corner cases. Across all images, we find that 5.2% contain the object and 7.8% are ambiguous. We additionally provide per-dataset breakdown over object classes in Fig. 22, where we plot the yes, no, and ambiguous ratios. Notably, most objects do not contain any instances of the specified object. Instead, the majority of errors stem from few object categories where the object detector exhibits systematic issues. Qualitative examples are shown in Fig. 23 and Fig. 24. We observed that some images were labeled as ambiguous in our human evaluation due to various factors. For instance, in the cases of Barn and Bookshop, the limited image resolution made it difficult to identify specific objects; distinguishing house from barn in an aerial view is nearly impossible. For Kai yang, Thai chicken dish, while the depicted dishes might contain chicken, it is challenging to determine whether they are specifically Kai yang. Interestingly, reverse image search labels the first image as kebab. Similarly, for Cowry, which refers to small sea snails, even if our human labelers could not identify any, it is dif1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 38 39 40 As an AI language model assistant, your task is to provide descriptive captions for images showing spurious features. spurious feature is visual element that frequently co-occurs with given object in images and may cause AI models to incorrectly recognize the object, even when it is not present. Task Overview: You will be given: - An object. Your job is to: 1. Think of potential spurious features: Identify objects, scenes, or elements that frequently co-occur with the given object in images. These should not include any parts or components of the object itself. 2. Generate 50 unique and diverse prompts describing images that contain only these spurious features, without including the object itself or any of its parts. Important Guidelines: - Do Not Mention the Object Name or Any Part of It: Avoid any direct or indirect references to the object name. If the object name is composite or compound word, do not include any part of the object name in the prompts. For example, if the object is \"firetruck,\" do not use \"fire\" or \"truck\" in the prompts. - Do Not Mention Parts of the Object: Do not include any parts or components of the object in the prompts. For example, if the object is \"mountainbike,\" do not use \"handlebar,\" \"gear shift,\" or \" saddle\" in the prompts. - Do Not Include the Object Name in Written Text: Do not create prompts that refer to written text containing the object name or any part of it. For example, avoid descriptions like \"a sign that says hummingbird.\" - Focus on Spurious Features: Use features that are likely correlated with the object due to frequent co-occurrence in images. - Combining Elements: You may combine elements if they logically make sense to appear together in one image. Do not combine elements unlikely to co-occur. - Ensure Diversity: Each prompt should be unique and cover different aspects of the spurious features. - Avoid Repetition: Do not repeat prompts or make minor variations of the same prompt. - Style and Detail: Write clear, creative, and descriptive prompts. Keep each prompt concise. - Language and Grammar: Use proper grammar and spelling. - Content Restrictions: Do not include offensive, sensitive, or inappropriate content. - Avoid Bias: Ensure prompts are inclusive and free from cultural, gender, or racial bias. - Verification: Before submitting, review the prompts to ensure they comply with all guidelines. Figure 7. DASH-LLM prompt for generating the text queries (1/3) 17 1 2 3 4 6 7 8 9 10 12 13 14 15 16 18 19 20 21 22 24 25 26 27 28 30 31 32 33 34 36 37 38 39 40 42 43 44 45 46 48 49 50 Examples: For the object \"hummingbird\": - Correct Prompts: - \"Close-up of bird feeder hanging in lush garden.\" - \"A garden filled with vibrant red flowers.\" - \"Green foliage glistening after rainfall.\" - \"A bird feeder surrounded by blooming plants.\" - \"Red tubular flowers swaying in the breeze.\" - Incorrect Prompts (Do Not Use): - \"A hummingbird hovering near flower.\" - \"Close-up of hummingbirds wings in motion.\" - \"A small bird with iridescent feathers perched on branch.\" - \"A sign with the word hummingbird in botanical garden.\" For the object \"firetruck\": - Correct Prompts: - \"A fire station with bright red doors.\" - \"Close-up of spinning emergency siren light.\" - \"Firefighters conducting training drill.\" - \"A tall ladder reaching up the side of building.\" - \"Protective gear hanging neatly in station locker room.\" - Incorrect Prompts (Do Not Use): - \"A bright red firetruck parked on the street.\" - \"Children waving at passing firetruck.\" - \"A sign that reads Fire Station No. 1.\" - \"A red truck with emergency equipment.\" - Using the words \"fire\" or \"truck\" in the prompts. For the object \"mountainbike\": - Correct Prompts: - \"A winding trail cutting through dense forest.\" - \"A helmet resting on tree stump beside path.\" - \"Sunlight filtering through trees along forest trail.\" - \"A backpack leaning against wooden signpost on hillside.\" - \"A group of friends hiking through mountainous terrain.\" - Incorrect Prompts (Do Not Use): - \"A mountainbike leaning against tree.\" - \"Close-up of mountainbikes gears.\" - \"A cyclist adjusting the saddle of mountainbike.\" - \"A sign that says Mountainbike Trail Ahead.\" - Using the words \"mountain\" or \"bike\" in the prompts. - Mentioning parts like \"handlebar,\" \"gear shift,\" or \"saddle.\" Figure 7. DASH-LLM prompt for generating the text queries (2/3) 18 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 47 48 49 50 Formatting Instructions: - Start each prompt on new line, numbered sequentially from 1 to 50. - The format should be: 1: <prompt_1> 2: <prompt_2> 3: <prompt_3> ... 50: <prompt_50> User Input Format: The user will provide the object in the following format: object: <object name> Your Response: - Return exactly 50 prompts per user request. - Ensure that the last line of your response starts with: 50: <prompt_50> - Under no circumstances should you include any content in your response other than the 50 prompts. Do not include explanations, apologies, or any additional text. Summary: - Do not mention the object name or any part of it. If the object name is composite or compound word, do not include any part of it in the prompts. - Do not mention parts or components of the object. - Do not create prompts that refer to written text containing the object name or any part of it. - Focus on spurious features that frequently co-occur with the object. - You may combine elements if they logically co-occur in an image. - Ensure diversity and uniqueness in the prompts. - Use proper language and avoid any inappropriate content. - Review all prompts for compliance before submitting. - Under no circumstances should you include any content in your response other than the 50 prompts. Do not include explanations, apologies, or any additional text. Remember, the goal is to create prompts that could lead an AI model to falsely recognize the object due to the presence of spurious features, even though the object itself is not present in the images. Figure 7. DASH-LLM prompt for generating the text queries (3/3) 19 1 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 Please review the list of prompts you previously generated and check for any mistakes or deviations from the guidelines. Identify any prompts that do not fully comply with the instructions. Then, generate new list of 50 prompts that strictly adhere to all the guidelines provided. Important Guidelines: - Do not mention the object name or any part of it. If the object name is composite or compound word, do not include any part of the object name in the prompts. - Do not mention parts or components of the object. - Do not create prompts that refer to written text containing the object name or any part of it. - Focus on spurious features that frequently co-occur with the object. - You may combine elements if they logically co-occur in an image. - Ensure diversity and uniqueness in the prompts. - Use proper language and avoid any inappropriate content. - Review all prompts for compliance before submitting. - Under no circumstances should you include any content in your response other than the 50 prompts. Do not include explanations, apologies, or any additional text. Formatting Instructions: - Start each prompt on new line, numbered sequentially from 1 to 50. - The format should be: 1: <prompt_1> 2: <prompt_2> 3: <prompt_3> ... 50: <prompt_50> - Ensure that the last line of your response starts with: 50: <prompt_50> Remember, your goal is to create prompts that could lead an AI model to falsely recognize the object due to the presence of spurious features, even though the object itself is not present in the images. Now, generate the corrected list of 50 prompts. Figure 8. DASH-LLM follow-up prompt for generating the text queries"
        },
        {
            "title": "Initialization",
            "content": "Step 5 Step 10 Step 15 Step 20 Step 25 VLM: no pyes : 0.04 pdet : 0. Leopard - Prompt: close-up of rocks cracks and fissures. VLM: yes pyes : 0.77 pdet : 0.00 VLM: yes pyes : 0.75 pdet : 0.00 VLM: no pyes : 0.06 pdet : 0.00 VLM: no pyes : 0.05 pdet : 0.00 VLM: yes pyes : 0.79 pdet : 0.00 Bathtub - Prompt: set of scented lotions arranged on shelf. VLM: no pyes : 0.03 pdet : 0.00 VLM: no pyes : 0.06 pdet : 0.00 VLM: no pyes : 0.04 pdet : 0.00 VLM: no pyes : 0.08 pdet : 0.00 VLM: yes pyes : 0.71 pdet : 0.00 VLM: yes pyes : 0.47 pdet : 0. Figure 9. Optimization trajectories for DASH-OPT for PaliGemma. For each example, we present the object label, the DASH-LLM query used to initialize the generation, as well as the answer and yes probability from the VLM and the probability from the detector. Through our optimization process, we can uncover model-specific unknown unknowns, such as the beads (see Fig. 3 for retrieved images) or the bath bombs (see Fig. 13). Since the last image is not necessarily the best, we select the image with the lowest loss as the query."
        },
        {
            "title": "Initialization",
            "content": "DASH-OPT"
        },
        {
            "title": "Initialization",
            "content": "DASH-OPT"
        },
        {
            "title": "Initialization",
            "content": "DASH-OPT PaliGemma - Gar waters edge with few rocks and pebbles. PaliGemma - Cabbage Butterfly white flower with yellow center and delicate, lacy texture. LLaVA-NeXT Vicuna - Pill bottle pair of slippers next to piece of furniture. VLM: no pyes : 0.03 pdet : 0.00 VLM: yes pyes : 0.68 pdet : 0. VLM: no pyes : 0.01 pdet : 0.00 VLM: yes pyes : 0.63 pdet : 0.00 VLM: no pyes : 0.21 pdet : 0.00 VLM: yes pyes : 0.87 pdet : 0.00 LLaVA-NeXT Vicuna - Gondola beautiful Murano glass vase on display in shop window. LLaVA-NeXT Mistral Beehive person holding frame in field of blooming flowers. LLaVA-NeXT Mistral Fortune Cookie vibrant street festival with dragon dancers. VLM: no pyes : 0.15 pdet : 0.00 VLM: yes pyes : 0.61 pdet : 0.00 VLM: no pyes : 0.03 pdet : 0.00 VLM: yes pyes : 0.52 pdet : 0.00 VLM: no pyes : 0.18 pdet : 0. VLM: yes pyes : 0.85 pdet : 0.00 Figure 10. We show examples of DASH-OPT query images after optimization together with the initialization generated from the text query. Our optimization is able to generate images that make VLM hallucinate from non-successful prompts without generating the object. 21 (a) PaliGemma (b) LLaVA-NeXT Vicuna (c) LLaVA-NeXT Mistral Figure 11. Influence of object frequencies: Histogram showing the average number of success images per object category across the OpenImages splits for DASH-LLM and OPT with PaliGemma, LN Vicuna and LN Mistral. The average number of training examples per class in the full 9M OpenImages dataset is indicated in parentheses. The plot reveals that rarer concepts are more susceptible to FPhallucinations, whereas common concepts with tens of thousands of examples are much less prone to such errors."
        },
        {
            "title": "Barracouta",
            "content": "Cluster Size: 174, Query: coastal town with bustling harbor."
        },
        {
            "title": "Fireboat",
            "content": "Cluster Size: 93, Query: water cannon spraying crowd of people."
        },
        {
            "title": "Steel Drum",
            "content": "Cluster Size: 47, Query: vibrant street art scene with colorful murals and graffiti on city wall."
        },
        {
            "title": "Puck",
            "content": "Cluster Size: 387, Query: cold, snowy winter landscape with frozen ponds. Electric Guitar Cluster Size: 58, Query: music stores wall filled with various colorful boxes."
        },
        {
            "title": "Anglerfish",
            "content": "Cluster Size: 92, Query: deep-sea submersible exploring the ocean floor, its lights illuminating the darkness. Artificial nails Cluster Size: 37, Query: beauty product review blog on laptop screen."
        },
        {
            "title": "Bed",
            "content": "Cluster Size: 106, Query: serene, natural setting with few trees and small pond."
        },
        {
            "title": "Sandals",
            "content": "Cluster Size: 14, Query: refreshing cocktail with an umbrella, garnished with slice of pineapple."
        },
        {
            "title": "Ring",
            "content": "Cluster Size: 123, Query: beautifully decorated, luxurious garden with fountain. Figure 12. DASH-LLM PaliGemmaPlease see Appendix F.1 for description."
        },
        {
            "title": "Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size: 74 Sulphur Butterfly Query Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size: 43 Aegean Cat"
        },
        {
            "title": "Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size: 684 Electric Ray Query Cluster Size: 329 Dining Table Query Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size: 45 Figure 13. DASH-OPT PaliGemmaPlease see Appendix F.1 for description. 24 Academic Gown Cluster Size: 69, Query: university seal emblazoned on stone wall. Balance Beam Cluster Size: 100, Query: An athletes feet in grip soles standing on mat. Chain Mail Cluster Size: 43, Query: medieval-style sword hanging on stone wall. Fountain Pen Cluster Size: 136, Query: stack of paper with handwritten notes in elegant script."
        },
        {
            "title": "Candle",
            "content": "Cluster Size: 93, Query: luxurious, master bedroom with large, four-poster bed and soft, golden lighting. Coral Fungus Cluster Size: 544, Query: close-up of trees bark with visible lichen growth."
        },
        {
            "title": "Postcard",
            "content": "Cluster Size: 260, Query: person walking along boardwalk, looking at the ocean. Fortune Cookie Cluster Size: 161, Query: beautifully crafted wooden puzzle box."
        },
        {
            "title": "Bus",
            "content": "Cluster Size: 21, Query: street sweeper cleaning the sidewalk and road."
        },
        {
            "title": "Bracelet",
            "content": "Cluster Size: 161, Query: persons fingers as they carefully handle small, fragile object. Figure 14. DASH-LLMLLaVA-NeXT VicunaPlease see Appendix F.1 for description. 25 Airplane Wing Query Cluster Size: 53 Bathing Cap Query Cluster Size: 43 Bullet Train"
        },
        {
            "title": "Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size: 134 Strawberry"
        },
        {
            "title": "Query",
            "content": "Cluster Size: 90 Hovercraft"
        },
        {
            "title": "Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size: 129 Carbonara"
        },
        {
            "title": "Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size: 170 Figure 15. DASH-OPT LLaVA-NeXT VicunaPlease see Appendix F.1 for description."
        },
        {
            "title": "Band Aid",
            "content": "Cluster Size: 326, Query: person holding injured wrist."
        },
        {
            "title": "Gondola",
            "content": "Cluster Size: 411, Query: charming little shop selling handmade crafts."
        },
        {
            "title": "Dumbbell",
            "content": "Cluster Size: 67, Query: row of exercise balls lined up against wall."
        },
        {
            "title": "Gar",
            "content": "Cluster Size: 76, Query: person sitting on rock, looking out at the water."
        },
        {
            "title": "Lighter",
            "content": "Cluster Size: 41, Query: dimly lit room with haze of smoke in the air."
        },
        {
            "title": "Lighthouse",
            "content": "Cluster Size: 112, Query: small, isolated pier with few fishing nets and lines. Baumkuchen Cluster Size: 80, Query: festive Oktoberfest celebration with traditional German music and dancing."
        },
        {
            "title": "Coil Spring",
            "content": "Cluster Size: 268, Query: collection of metal scraps and waste in storage bin."
        },
        {
            "title": "Tie",
            "content": "Cluster Size: 72, Query: person standing in front of city skyline, looking confident. Air Conditioner Cluster Size: 124, Query: temperature gauge on wall with needle pointing to comfortable range. Figure 16. DASH-LLM LLaVA-NeXT Mistral - Please see Appendix F.1 for description."
        },
        {
            "title": "Query",
            "content": "Cluster Size: 237 Bulletproof Vest Query Cluster Size: 63 Horizontal Bar Query Cluster Size:"
        },
        {
            "title": "Rain Barrel Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Strawberry Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size:"
        },
        {
            "title": "Query",
            "content": "Cluster Size: 39 Figure 17. DASH-OPT LLaVA-NeXT MistralPlease see Appendix F.1 for description. 28 bicycle apple cat bear carrot cell phone COCO sausage dessert durian hanger high heels ladder Objects365 D O A 5 6 3 e Figure 18. Demonstration of images that cause PaliGemma to detect the target class, identified using DASH-OPT, alongside their nearest neighbors in the reference datasets COCO and Objects365. For reference images, we use blue border to mark images that elicit yes response from the VLM and red border for no response. We show that neither the full COCO training set (80K samples) nor Objects365 (1.7M samples) contain the systematic errors uncovered by DASH, as all nearest neighbors are not detected by the VLM. This again highlights that our open-world search in ReLaion-5B is necessary to detect these hallucinations and would not possible even with such reasonably large dataset such as Object365. With DASH we find that, PaliGemma incorrectly answers yes for colorful wellington boots as apple and for Baobab trees as sausage. (a) PaliGemma (b) LLaVA-NeXT Vicuna (c) LLaVA-NeXT Mistral Figure 19. Extension of Fig. 4 for PaliGemma as well as LLaVA-NeXT Vicuna and Mistral. For all VLMs DASH-OPT finds hallucinations which are further away from the original text queries than DASH-LLM. This illustrates quantitatively the higher diversity of hallucinations found by DASH-OPT. 29 Cluster size: 190 Cluster size: 167 Cluster size: Cluster size: 66 DASH-LLMPtarmigan - Total clusters 7 - Total images 613 Cluster size: 28 Cluster size: 26 Cluster size: 7 Cluster size: Cluster size: 904 Cluster size: 721 Cluster size: 629 DASH-OPTPtarmigan - Total clusters 21 - Total images 5079 Cluster size: 188 Cluster size: Cluster size: 173 Cluster size: 145 Cluster size: 129 Cluster size: 82 Cluster size: 81 Cluster size: Cluster size: 57 Cluster size: 39 Cluster size: 37 Cluster size: 30 Cluster size: 22 Cluster size: Cluster size: 10 Cluster size: 8 Cluster size: 5 Figure 20. All clusters found for DASH-LLM and DASH-OPT for PaliGemma and the object Ptarmigan. While Ptarmigan refers to the bird species, the clusters include false positives such as images of mountain landscapes, alpine environments, and even abstract artistic representations or completely unrelated objects. This highlights how the VLMs understanding conflates semantic and contextual cues with visual content, leading to hallucinations. In particular, we believe that these hallucinations could be caused by places containing the name Ptarmigan, such as multiple locations called Ptarmigan Peak in Colorado, Utah, and Alaska, or Ptarmigan Ridge and Ptarmigan Traverse in Washington. While we believe that VLM should not respond that it sees Ptarmigan even in an image of place with name containing the word Ptarmigan, we also checked several of these images to verify that these places are different mountainsides with completely unrelated names. This verifies that the VLM has learned false representation of the word Ptarmigan, which includes many different mountainsides or peaks. Our DASH-OPT method, leveraging optimized queries, discovers additional unknown unknowns, such as rare or abstract scenes where ptarmigan is highly unlikely (e.g., auroras, surreal artwork, and highly stylized objects). By creating queries for the specific target VLM, DASH-OPT uncovers vulnerabilities that are less intuitive or expected, revealing the VLMs susceptibility to type II hallucinations. 30 Cluster size: Cluster size: 488 Cluster size: 352 Cluster size: 350 Cluster size: 220 DASH-LLMBaumkuchen - Total clusters 17 - Total images 3028 Cluster size: Cluster size: 123 Cluster size: 80 Cluster size: 75 Cluster size: 57 Cluster size: 57 Cluster size: Cluster size: 33 Cluster size: 26 Cluster size: 24 Cluster size: 7 Cluster size: 5 Cluster size: Cluster size: 356 Cluster size: 330 Cluster size: 140 Cluster size: 127 DASH-OPTBaumkuchen - Total clusters 40 - Total images 3428 Cluster size: Cluster size: 111 Cluster size: 103 Cluster size: 103 Cluster size: 101 Cluster size: 97 Cluster size: Cluster size: 73 Cluster size: 66 Cluster size: 63 Cluster size: 59 Cluster size: 50 Cluster size: Cluster size: 49 Cluster size: 46 Cluster size: 45 Cluster size: 43 Cluster size: 40 Cluster size: Cluster size: 30 Cluster size: 30 Cluster size: 28 Cluster size: 28 Cluster size: 26 Cluster size: Cluster size: 19 Cluster size: 19 Cluster size: 18 Cluster size: 16 Cluster size: 9 Cluster size: Cluster size: 8 Cluster size: 8 Cluster size: 7 Cluster size: 5 Figure 21. All clusters found for DASH-LLM and DASH-OPT for LLaVA-NeXT Mistral and the object Baumkuchen. Please refer to Appendix F.2 for description. Figure 22. yes, no an ambiguous rates in our human evaluation for the 4 datasets used for object labels in our evaluation. Each bar represents one object, for which we manually labeled 10 images for DASH-OPT on PaliGemma. We note that most objects do not contain any instances of the object and instead, most errors come from few object categories where the object detector itself has systematic issue. We show qualitative examples in Fig. 23 and Fig. 24."
        },
        {
            "title": "OpenImages",
            "content": "Barn Bookshop Kai yang Cowry"
        },
        {
            "title": "COCO",
            "content": "Objects365 Airplane Train Glasses Soccer Figure 23. Examples of images labeled as ambiguous in our human evaluation are shown. For Barn and Bookshop, the limited image resolution makes it difficult to identify the objects in the image; for instance, distinguishing house from barn in an aerial view is nearly impossible. For Kai yang, Thai dish with chicken, while the depicted dishes might contain chicken, it is challenging to determine whether they are specifically Kai yang. Notably, reverse image search labels the first image as kebab. For cowry, small sea snails, even if human labelers could not identify any, it is difficult to guarantee their absence in the image. For airplane, the interiors shown could represent futuristic airplane or train designs. For train, the image resolution is too low to infer the presence of specific objects. For the two objects from Objects365, the ambiguity mainly arises from the object labels themselves. For example, glasses in the dataset refers to eyewear, but the images often contain multiple glass objects. Similarly, soccer refers exclusively to soccer ball in Objects365, whereas the sport itself is not well-defined object. This creates ambiguity about whether we should label referees as yes or no."
        },
        {
            "title": "OpenImages",
            "content": "Mountain bike Pot Car Car seat"
        },
        {
            "title": "COCO",
            "content": "Objects365 Mouse Potted plant Faucet Egg Figure 24. We highlight several failure cases of the object detector identified during our human evaluation. All images presented here have confidence score below the threshold of 0.1 and are therefore not rejected by our automated pipeline. For mountain bike, the primary issue arises from the objects being very small and difficult to spot. In the case of other objects, such as Pot or Faucet, the objects are clearly visible, but the object detector fails to recognize these instances. For Car, the detector does not seem to classify trucks or vans as cars. For Mouse and Egg, the detector struggles with distribution shifts, failing to recognize comic or plush mice and colored eggs, respectively. 33 Figure 25. COCO annotations errors in POPE (ground truth no): We show four examples where the POPE ground truth label for the question Is there object in the image? is no although the object is present in the image. We mark the location of the object with red bounding box. 34 For each object, the same amount of positive samples, i.e. images that contain the object, are added. These images are retrieved using the Flickr API [10] and annotated by human labeler to ensure that the object is clearly contained. J.2. Metrics The performance measure on DASH-B is the accuracy over In Tab. J, we also reall negative and positive samples. port the true negative rate (TNR) and true positive rate (TPR) individually. downside of measuring accuracy is that trivial model that always replies Yes (or always No) achieves an accuracy of 50%. This behaviour can be avoided by considering the harmonic mean of TNR and TPR instead which results in value of 0 for the trivial case. We also report this metric (HM) in Tab. but observe no significant effect on the results (apart from LLaVA-NeXTVicuna). Note that the results for the three source models are biased as they were used in the creation of the benchmark. Similarly, Qwen2-72B and Llama-3.2-11B are not reported as they produce TNR of 1.0 by design. I. Transfer I.1. VLM Models For all models except Prismatic, we use the Transformers library [56] with the official checkpoints. For Prismatic [17], we use the official implementation. Model details, including links to the specific models files used can be found in Tab. 5. Note that Qwen2-VL is not based on SigLIP as stated in Tab. 2 but instead uses custom ViT with 675m parameters. We will correct this in the final version. I.2. True positive rate For each object from ImageNet, COCO, and Objects365, we collect 100 images of the corresponding class from the official validation set. On these images, the average TPR is computed by counting the frequency of the correct response yes. I.3. Qwen2-VL vs Llama 3.2-VL In the main paper, we have already shown some examples from DASH-B where, by design, both Qwen2-VL and Llama 3.2-VL hallucinate. While both these models are quite robust to hallucinations, we also want to understand where they differ. To do this, we show several examples in Fig. 26 where only one model hallucinates. This demonstrates that even the best available open-weight models are still vulnerable to hallucinations but also differ substantially in terms of vulnerabilities, likely due to larger differences in architecture, vision encoder, LLM, and training data. J. DASH-B As described in H, most models only produce small number of false positives on POPE which also contain large amount of label errors. Therefore, we propose new benchmark DASH-B based on our retrieval results to enable more reliable and rigorous evaluation of object hallucinations. Tab. contains results for DASH-B and POPE for range of VLMs. J.1. Image Selection We select the images for the benchmark using the following steps: We merge the images found by DASH-LLM and DASHOPT over all three source models. These images are filtered by requiring successful transfer to both Qwen2-72B and Llama 3.2-VL-11B, the best performing models in 2, in order to exclude errors which are specific to biases of the three source models. We select 70 objects and two human labelers verify that the selected images do not contain the corresponding object. The number of images is limited to at least 3 and at most 50. 35 Anglerfish Qwen2-VL-72B: yes Llama-3.2-Vision: no Electric Ray Qwen2-VL-72B: yes Llama-3.2-Vision: no Artifical Nails Qwen2-VL-72B: yes Llama-3.2-Vision: no Concertina Qwen2-VL-72B: no Llama-3.2-Vision: yes Ford Tourneo Qwen2-VL-72B: no Llama-3.2-Vision: yes Pipette Qwen2-VL-72B: no Llama-3.2-Vision: yes Figure 26. We demonstrate several images where Qwen2-VL-72B and Llama-3.2-Vision disagree. Note that all images do not contain the object and are thus hallucinations by the model responding with yes. This further demonstrates that even the best available open-weight models are not robust to hallucinations. VLM Model PaliGemma-3B [3] LLM Vision Encoder Checkpoint Gemma-2B [49] SigLIP-So400m 224px [60] paligemma-3b-mix-224 Vicuna-7B [36] LLaVA-NeXT-Mistral-7B [27, 28] LLaVA-NeXT-Vicuna-7B [27, 28] Mistral-7B [16] LLaVA-NeXT-Llama-8B [22, 27] Llama-3.0-8B [9] CLIP ViT-L 224px [38] CLIP ViT-L 224px [38] CLIP ViT-L 224px [38] llava-v1.6-vicuna-7b-hf llava-v1.6-mistral-7b-hf llama3-llava-next-8b-hf Prismatic CLIP [17] Prismatic SigLIP [17] Prismatic DinoV2 [17] Qwen2-VL-7B-Instruct Qwen2-VL-72B-Instruct Vicuna-7B [36] Vicuna-7B [36] Vicuna-7B [36] CLIP ViT-L 224px [38] SigLIP-So400m 224px [60] DINOv2 ViT-L 224px [35] prismatic-vlms/clip-224px+7b prismatic-vlms/siglip-224px+7b prismatic-vlms/dinov2-224px+7b Qwen2-7B [58] Qwen2-72B [58] Custom ViT 675m Custom ViT 675m Qwen2-VL-7B-Instruct Qwen2-VL-72B-Instruct Llama-3.2-11B-Vision-Instruct Llama-3.1-8B [9] Custom ViT Llama-3.2-11B-Vision-Instruct Table 5. VLMs used for transfer experiments 36 Benchmark Metric PaliGemma-3B [3] LN Vicuna [28, 36] LN Mistral [16, 28] LN Llama [9, 28] Llava-OneVision [23] PaliGemma-2-3B [48] PaliGemma-2-10B [48] Ovis2-1B [30] Ovis2-2B [30] Ovis2-4B [30] Ovis2-8B [30] InternVL2.5-8B [4] InternVL2.5-26B [4] InternVL2.5-38B [4] InternVL2.5-78B [4] InternVL2.5-8B-MPO [4] InternVL2.5-26B-MPO [4] GPT-4o-mini POPE Acc. DASH-B Acc. TNR TPR HM 87.2% 62.0% 26.4% 97.7% 41.6% 87.6% 53.7% 10.4% 96.9% 18.7% 88.0% 61.7% 30.1% 93.4% 45.5% 88.0% 65.2% 37.0% 93.4% 53.0% 88.7% 75.1% 60.2% 90.1% 72.2% 88.8% 68.9% 40.9% 96.8% 57.5% 87.7% 69.8% 48.0% 91.6% 63.0% 88.9% 64.6% 35.1% 94.0% 51.1% 89.4% 61.7% 27.3% 96.1% 42.5% 90.3% 64.8% 31.0% 98.6% 47.2% 94.9% 71.4% 44.8% 98.0% 61.5% 90.6% 71.7% 47.2% 96.2% 63.3% 90.6% 77.5% 57.3% 97.8% 72.2% 90.7% 76.2% 54.8% 97.6% 70.2% 90.8% 74.1% 50.3% 97.8% 66.5% 89.1% 69.4% 42.3% 96.4% 58.8% 90.7% 76.1% 54.8% 97.4% 70.1% 84.2% 86.3% 77.0% 95.7% 85.3% : POPE result from [4], : POPE result from [8] Table 6. DASH-B: We report accuracy (for POPE and DASH-B) as well as the true negative rate (TNR), true positive positive rate (TPR), and the harmonic mean of TNR and TPR (HM). While the accuracy reflects the detection-hallucination trade-off, the individual values of TNR and TPR can give further insides into the vulnerability to FP-hallucinations. Note that PaliGemma-3B, LN Vicuna, and LN Mistral were used in the creation of the benchmark. 37 Hallucination benchmarks with similar tasks: We report the Amber score and the accuracies on Amber Existence and R-Bench. Effect on other tasks: We evaluate two VQA benchmarks (TextVQA [46], VQAv2 [12]) and two captioning benchmarks (COCO[25], Flickr30k [59]) and report accuracies and CiDER scores, respectively. Performance on positive samples: The TPR-ICO for the objects from ImageNet, COCO, and Objects365 are evaluated as described in Appendix I.2. The fine-tuned version (+ftpre) significantly improves over PaliGemma-3B on unseen clusters (Validation, +77.7%). It also shows slightly better results on related hallucination benchmark with increases in the Amber score (+0.2), as well as higher accuracies on Amber Existence (+3%) and R-Bench (+1.1%). The performance decreases slightly for more general VQA tasks (1.4% and 0.9%) and captioning tasks (1.3 and +0.1%). The reduction of the TPRICO (5.1%) is due to significant drop of the TPR on Objects365 (12.1%). possible reason for this is mismatch between the image distributions of the retrieved positive samples, where the object is prominently visible in the image, and Objects365, where objects often occur only in small bounding boxes inside the image. Evidence for this is shown in the last column (+ft) of Tab. 8: We repeated the fine-tuning on different dataset where we replaced all retrieved positive samples for objects from Objects365 with images from the original Objects365 training set. In this setting with more positive than negative samples, the finetuned model even improves TPR on all three datasets but also improves less on the hallucination tasks. This experiment indicates that the images retrieved by DASH can also be used to mitigate the problem of systematic hallucination by including them into fine-tuning routine. L. Reverse Task We apply the DASH-LLM pipeline to the reverse task, where the VLM outputs No despite the object being visible in the image. We adjust the LLM prompt accordingly (see 28), reverse the object detector threshold, and use larger value. Figure 29 presents example clusters. While this experiment serves as proof of concept, we observe that the object detector performs worse in this direction and should be replaced for larger-scale experiments. Overall, the benefits of DASH are more pronounced in the setting discussed in the main paper, as the number of images containing given object is much smaller than the number of images that do not contain the object. K. Fine-tuning on DASH Can we utilize the images retrieved by DASH to mitigate the vulnerability to systematic hallucinations? To test this hypothesis, we perform small scale experiment by finetuning PaliGemma-3B with LoRA[14] on our retrieval results. Used hyperparameters are provided in Tab. 7. K.1. Data DASH retrieves images, where the object is not present in the image. Therefore, the ground truth answer to the question Can you see an object in this image? is always No. We additionally retrieve images containing the object and add them to the training data to preserve the models ability to recognize the object. For each object, we add 200 negative samples, i.e. images where No is the correct reply, and 400 positive samples, i.e. Yes is the correct reply, at random to the training set. Negative samples: For the negative samples, i.e. images where the ground truth answer is No, we use all images resulting from DASH-LLM and DASH-OPT (both for PaliGemma). We split these images into two disjoint subsets: Validation: For each object, one of the found clusters is selected and all corresponding images are placed in the validation set. Train: All remaining images are used to sample images for the fine-tuning dataset. We further filter these images to ensure that they do not contain the object by requiring that Llama 3.2-VL and Qwen2-VL answer with no. Positive samples: We generate diverse set of prompts including the objects using Llama 3.2 and use them to retrieve images from ReLAION. The resulting images are filtered by the object detector (threshold > 0.1) and Llama 3.2 (response Yes). Optimizer β1 β2 Learning rate Number of epochs Batchsize LoRA rank ADAM 0.9 0.999 1e-6 5 32 8 Table 7. Fine-tuning hyperparameters K.2. Results We report several metrics for PaliGemma-3B and our finetuned version (+ft) in Tab. 8, comparing their performance on different tasks: Systematic hallucinations: The accuracy, i.e. ratio of correctly replying with no, on the Validation set. 38 Dataset Metric PaliG +ftpre +ft Validation Amber Amber Ex. R-Bench TextVQA VQAv COCO Flickr30k ImageNet COCO Objects365 TPR-ICO DASH-B DASH-B DASH-B Acc. Score Acc. Acc. Acc. Acc. CiDER CiDER TPR TPR TPR TPR Acc. TNR TPR 77.7% 57.6% 0.0% 93.5 94.0 93.7 93.2% 96.2% 95.4% 79.9% 81.0% 80.2% 57.6% 56.2% 83.1% 82.2% 56.5% 82.4% 121.3 77. 123.2 77.5 90.0% 93.4% 80.4% 88.8% 56.6% 73.0% 76.0% 85.1% - - - 68.0% 45.9% 90.0% 124.5 77.4 90.0% 84.0% 69.0% 81.1% 56.4% 26.4% 86.4% Table 8. Accuracies on our Validation set, Amber Existence, and R-Bench and TPR on positive samples from the validation sets of ImageNet, COCO and OpenImages. Fine-tuning on DASH results (+ftpre) can improve robustness against hallucinations significantly, even on clusters not seen during training. It also improves on related hallucination benchmarks while the performance on more general VQA and captioning tasks becomes slightly worse. The reduction in TPR-ICO is caused by the retrieved positive samples for Objects365. After replacing these with images (+ft) from the original training set of Objects365, the fine-tuning even improves average TPR-ICO. 39 1 2 4 5 6 7 8 10 11 12 13 14 16 17 18 19 20 22 23 24 25 26 28 29 30 31 32 34 35 36 37 38 40 41 42 43 44 46 You are creative prompt generator. Your task is to: 1. **Accept an object name** (provided by the user). 2. **Generate 20 different image prompts** in realistic everyday settings, filled with various common objects, where the specified object is present but not necessarily the focus of the scene-so it might be overlooked by an object detection system. --- ## Context & Objectives 1. **Purpose**: - We want to depict the given object in real-life scenarios that include multiple other items typically found in the setting. - The object should be there, but the scene should be busy or populated enough that the object isnt the sole focus. - The style should be **highly realistic**, as if taken by camera. 2. **Guiding Techniques**: - **Crowded Scenes**: Combine the specified object with many other objects commonly found in the same environment (e.g., living rooms, offices, kitchens, garages). - **Non-Focal Positioning**: Place the object off to the side or partially in the background, so it doesnt immediately draw attention. - **Realistic Keywords**: To enhance the lifelike quality, you can use any of these keywords in your prompts: - photo-real - hyper-detailed - 8k resolution - cinematic lighting - DSLR - natural lighting - raw photo - high dynamic range - real-world texture - unposed --- ## Detailed Instructions 1. **Input**: You will receive single word or short phrase specifying the object (e.g., \"chair,\" \"cup,\" \"clock,\" \"bag,\" etc.). 2. **Output**: - Produce **20 unique prompts**, each describing realistic photograph in which the object is present among various other items typically found in that scenario. - Use some of the realism keywords to convey high-quality, real-world style. - Ensure the object is not the main focus but simply part of busier environment. 3. **Format**: - Number each prompt **from 1 to 20, using colon** (e.g., 1: Prompt text, 2: Prompt text, ..., 20: Prompt text). - Each prompt should be concise but mention multiple items and the general setting. --- Figure 27. DASH-LLM prompt for generating the text queries for the reverse task (1/2) 1 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 26 27 28 29 31 32 33 34 35 37 ## Examples of Prompts *(Using <OBJECT_NAME> as placeholder - these are short samples, not fully detailed.)* - **Living Room Scenario** *\"A photo-real image of cozy living room with sofa, coffee table, TV, potted plants, and small <OBJECT_NAME> tucked beside stack of magazines.\"* - **Office Setting** *\"A hyper-detailed view of an open-plan office featuring desks, laptops, file cabinets, water cooler, and <OBJECT_NAME> placed casually near window sill.\"* - **Kitchen Scene** *\"A raw photo of busy kitchen counter with plates, utensils, fruits, and <OBJECT_NAME> resting behind jar of spices.\"* Please use many different such scenarios instead of restricting yourself to the ones from these examples. Possible scenarios would be an office, train station, garden, living room, kitchen, hallway, outdoors, in the city, landscape. Try to think of scenario that matches the object and that allows you to add in different objects that could occur with it. Please try out different scenarios for each object in the different prompts. Make sure to not repeat too similar prompts and rather create sufficient variety of prompts. These examples show: - The <OBJECT_NAME> is included but not emphasized. - The setting has multiple other common objects. --- ## Final Output Format When the user provides the object name, respond with exactly **20 prompts**, numbered with colons, in the form: 1: [Prompt text] 2: [Prompt text] ... 20: [Prompt text] Each prompt should describe realistic scene filled with everyday objects, incorporating the given object without making it the sole focus. Figure 28. DASH-LLM follow-up prompt for generating the text queries for the reverse task (2/2)"
        },
        {
            "title": "Chicken",
            "content": "Figure 29. Reverse Task We show three clusters found by DASH for the reverse task using LLaVA-NeXT Vicuna: The VLM response to Can you see OBJ in this image? is no although the object is clearly visible in the image. 42 M. Transfer across prompts During our experiments we use the prompt Can object be seen in the image?. In Tab. 9, we evaluate range of 10 different prompts for the three source models on their corresponding DASH-LLM and DASH-OPT subsets. 43 DASH-LLM DASH-OPT"
        },
        {
            "title": "LN Mistral",
            "content": "Can OBJ be seen in the image? Does this image have OBJ? Does the image show OBJ? Does this image contain OBJ? Does this picture include OBJ? Is OBJ depicted in this image? Is there OBJ present in the image? Is there OBJ in this image? Is OBJ shown in the image? Is OBJ visible in the image? Is OBJ in the image?"
        },
        {
            "title": "Average\nStandard Deviation",
            "content": "0.861 0.890 0.860 0.857 0.824 0.851 0.787 0.717 0.670 0.467 0.348 0.739 0.179 0.923 0.891 0.737 0.790 0.782 0.913 0.807 0.818 0.880 0.833 0.906 0.844 0.062 0.824 0.806 0.733 0.858 0.886 0.748 0.860 0.827 0.792 0.655 0.702 0.790 0. 0.878 0.859 0.832 0.801 0.795 0.805 0.758 0.643 0.593 0.385 0.278 0.693 0.200 0.872 0.863 0.712 0.739 0.735 0.901 0.735 0.771 0.835 0.782 0.873 0.802 0.069 0.832 0.781 0.733 0.843 0.865 0.755 0.870 0.828 0.804 0.655 0.690 0.787 0. Table 9. Transfer across prompts: While transfer rates for LN Vicuna and LN Mistral are stable, PaliGemma was pretrained on this task using the prompt Is OBJ in the image? and shows lower transfer rates on similar prompts. However, this improved robustness against systematic hallucinations does not generalize to less similar prompts."
        }
    ],
    "affiliations": [
        "Tubingen AI Center, University of Tubingen"
    ]
}