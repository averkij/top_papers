{
    "paper_title": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models",
    "authors": [
        "Junjie Ye",
        "Caishuang Huang",
        "Zhuohan Chen",
        "Wenjie Fu",
        "Chenyuan Yang",
        "Leyi Yang",
        "Yilong Wu",
        "Peng Wang",
        "Meng Zhou",
        "Xiaolong Yang",
        "Tao Gui",
        "Qi Zhang",
        "Zhongchao Shi",
        "Jianping Fan",
        "Xuanjing Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF."
        },
        {
            "title": "Start",
            "content": "A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models Junjie Ye1*, Caishuang Huang1, Zhuohan Chen1, Wenjie Fu1, Chenyuan Yang1, Leyi Yang1, Yilong Wu1, Peng Wang3, Meng Zhou4, Xiaolong Yang4, Tao Gui2, Qi Zhang1, Zhongchao Shi3, Jianping Fan3, Xuanjing Huang1 1 School of Computer Science, Fudan University 2 Institute of Modern Languages and Linguistics, Fudan University 3 Lenovo Research 4 Tencent jjye23@m.fudan.edu.cn, {qz, tgui}@fudan.edu.cn 5 2 0 2 2 1 ] . [ 1 1 9 5 7 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit finegrained performance assessment. To fill this gap, we propose multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the models attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https:// github.com/Junjie-Ye/MulDimIF."
        },
        {
            "title": "Introduction",
            "content": "Instruction following is fundamental capability of large language models (LLMs) (Bai et al., 2022; OpenAI, 2023; Reid et al., 2024; Yang et al., 2024), allowing them to generate responses that adhere to user-specified constraints (Zhou et al., 2023; Wen et al., 2024a; Dong et al., 2025). This skill is critical in real-world applications, particularly in agentic and tool-assisted workflows, where outputs must *Equal contributions. Figure 1: The hierarchical structure of the multidimensional constraint framework, which includes three constraint patterns, four constraint categories (subdivided into thirteen subcategories), and four levels of constraint difficulty. conform to strict formats such as JSON (Xi et al., 2023; Deng et al., 2024; Ye et al., 2024). Even minor deviations can lead to parsing failures and system breakdowns (Ye et al., 2025). Existing studies have investigated instruction following in LLMs by categorizing constraints (Zhou et al., 2023), crafting targeted prompts (He et al., 2024b), and evaluating model outputs using both code-based and model-based metrics (Jiang et al., 2024). Techniques such as tree search (Cheng et al., 2024) and reinforcement learning (RL) (He et al., 2024a) have also been used to improve instructionfollowing ability. Despite these advances, current approaches suffer from several notable limitations. Most prominently, these benchmarks rely on rigid, predefined templates (Zhou et al., 2023; Jing et al., 2023; Wen et al., 2024b) that fail to capture the natural variability in how users express constraints. In addition, many evaluations use LLMs as judges, introducing model-induced biases (Jiang et al., 2024; Sun et al., 2024; Qin et al., 2024). Furthermore, while advanced techniques can boost instruction-following performance, there is limited analysis of why these improvements occur, limiting both interpretability and generalization (Zhang et al., 2024; Cheng et al., 2024; Dong et al., 2024). To address this gap, we propose multidimensional constraint framework that captures the diverse ways users specify constraints when interacting with LLMs. Illustrated in Figure 1, our framework enables fine-grained analysis by introducing three distinct constraint patterns including example, listing, and incorporation. It organizes constraints into four primary categories which are content, language, format, and length. These categories are further divided into thirteen specific subcategories. Additionally, the framework defines four-level difficulty scale based on the number of constraints per instruction. Building on this framework, we develop an automated pipeline. The pipeline includes steps for constraint expansion, conflict detection, and instruction rewriting, transforming any initial instruction into one with code-verifiable constraints. Using this pipeline, we construct dataset of 1,200 diverse instruction-following cases based on ShareGPT1, allowing controlled model evaluation. We evaluate 14 LLMs across seven model families and uncover significant variation in their ability to follow different forms of constraints. Notably, while models perform well on example pattern, they struggle with listing and incorporation patterns, emphasizing the challenges posed by complex instructions and the benefits of few-shot prompting (Brown et al., 2020). On average, performance declines from 77.67% at Level to 32.96% at Level IV, with even the best model scoring only 67.50% overall. To improve this, we reuse our pipeline to generate new batch of constraint-based instructions and train the models using the GRPO (Shao et al., 2024) algorithm. Post-training, models demonstrate significantly enhanced instruction-following ability without sacrificing general performance. Parameter-level analysis and case studies reveal that most improvements stem from updates in attention modules, which appear to better align the models focus with the given constraints. Our contributions are summarized as follows: 1https://sharegpt.com/ 1) We propose multi-dimensional constraint framework that captures diverse constraint forms and enables fine-grained evaluation; 2) We design an automated instruction generation pipeline that transforms raw instructions into constraint-rich prompts; 3) We construct diverse benchmark dataset containing 1,200 test cases for evaluating instruction following in 14 LLMs, and improve model performance via targeted training; and 4) We perform in-depth parameter-level analysis and show that instruction following improvements primarily arise from changes in attention mechanisms."
        },
        {
            "title": "2 Related Works",
            "content": "Evaluation of Instruction Following Evaluating the instruction-following capabilities of LLMs has become central focus in recent research. Benchmarks such as IFEval (Zhou et al., 2023), FollowEval (Jing et al., 2023), and FollowBench (Jiang et al., 2024) assess models on dimensions like logical reasoning and stylistic consistency, using either code-based or LLM-based evaluations. Multi-IF (He et al., 2024b) extends this to multilingual, multi-turn dialogue settings, while InfoBench (Qin et al., 2024) decomposes complex instructions into simpler subtasks to evaluate execution accuracy. CIF-Bench (Li et al., 2024) focuses on the generalization abilities of Chinese LLMs under zero-shot scenarios. Despite their breadth, many of these benchmarks rely on templated or highly constrained prompts, which limits their ability to capture real-world instruction diversity and support fine-grained evaluation. Our work addresses these limitations by introducing multi-dimensional constraint framework comprising three constraint patterns, four constraint categories, and four difficulty levels. Built on this framework, we develop an automated instruction generation pipeline that enhances diversity and complexity through constraint expansion, conflict detection, and instruction rewriting. Training of Instruction Following range of algorithms have been proposed to improve the instruction-following performance of LLMs. Reinforcement learning approaches such as PPO (Schulman et al., 2017) and DPO (Rafailov et al., 2023) optimize model behavior based on user preferences. IOPO (Zhang et al., 2024) augments this by aggregating question-answer pairs across datasets to enrich preference signals and refine the optimization objective. Coninfer (Sun et al., Figure 2: Illustration of the automated instruction generation pipeline. Constraint Expansion: Randomly selects constraint category not yet included in the instruction and adds 12 specific constraints. Conflict Detection: Identifies whether the new instruction introduces redundant constraints or conflicts, and discards conflicting instructions. Instruction Rewriting: Rewrites the remaining instructions based on different constraint patterns. 2024) adopts curriculum learning approach, incrementally increasing task difficulty during finetuning to improve constraint handling. While these methods yield measurable gains, they often lack in-depth analysis of the model characteristics driving these improvements, which limits their interpretability and generalizability. To fill this gap, we introduce comprehensive data construction pipeline, enabling the creation of high-quality instruction-following datasets. Our parameter-level analysis suggests that significant portion of the performance improvement arises from tuning the models attention mechanisms, which enhances its ability to recognize and comply with constraints."
        },
        {
            "title": "3 Approaches",
            "content": "3.1 Multi-Dimensional Constraint Framework Existing benchmarks (Zhou et al., 2023; Jing et al., 2023; Jiang et al., 2024) often suffer from lack of constraint diversity, limiting the breadth of instruction-following capabilities they can evaluate. To address this gap, we propose multi-dimensional constraint framework, as illustrated in Figure 1.2 3.1.1 Constraint Pattern Drawing inspiration from publicly available guidelines for writing instructions (Saravia, 2022), we identify three common patterns used to introduce constraints during interactions with LLMs. Example The example pattern involves adding several question-answer pairs that share the same constraint type as the instruction to be followed. This method enhances the models ability to comply with constraints through contextualized examples, technique commonly known as incontext learning (Brown et al., 2020). Listing The listing pattern presents constraints in clearly structured, point-by-point format. This approach provides explicit communication of constraint requirements, making it especially effective in zero-shot scenarios. Incorporation The incorporation pattern integrates constraints directly into the instruction, rather than listing them separately. While this 2Examples are available in Appendix A. Data Constraint Pattern Constraint Category Constraint Difficulty Total Example Listing Incorporation Content Format Language Length Level Level II Level III Level IV Training Test 1225 3308 400 3373 400 8888 1175 9850 1210 6168 694 9541 610 300 1614 300 2522 300 3160 300 7906 1200 Table 1: Distribution of training and test data constructed on the automated instruction generation pipeline. approach maintains fluency, it may make it more difficult for LLMs to clearly interpret individual constraint requirements. Level IV Level IV includes an instruction with four types of constraints, comprising total of four to eight individual constraint elements. 3.1.2 Constraint Category Beyond the method of presentation, the types of constraints can vary widely. To enable fine-grained analysis, we categorize constraints into four main categories with thirteen subcategories. Content Content constraints restrict the elements present in the models output. These can be further divided into three subcategories: ensuring the inclusion of specific keywords, adhering to particular punctuation requirements, or referencing predefined identifiers. Format Format constraints require the output to follow specific structural rules, often necessary for post-processing tasks. Common examples include outputs in XML, Table, Markdown, or JSON. Language Language constraints specify the language to be used in the output, which is fundamental in translation tasks (Ye et al., 2023). Based on the language type, constraints are classified into English, Chinese, or other languages. Length Length constraints enforce limits on the outputs size. Depending on granularity, these constraints can apply at the paragraph level, sentence level, or word level. 3.1.3 Constraint Difficulty In addition to type and presentation, the number of constraints also impacts task difficulty. We define four difficulty levels based on the number and variety of constraints present in the instruction. Level Level includes an instruction with single type of constraint, containing one or two individual constraint elements. Level II Level II includes an instruction with two types of constraints, comprising total of two to four individual constraint elements. Level III Level III includes an instruction with three types of constraints, comprising total of three to six individual constraint elements. 3.2 Automated Instruction Generation Pipeline Building upon the multi-dimensional constraint framework, we introduce an automated pipeline that transforms raw instructions into constrained versions that can be verified through code, as illustrated in Figure 2. Constraint Expansion Constraint expansion involves adding new constraints to given instruction. Specifically, we randomly select constraint category not yet covered and add one or two specific constraints from that category. This process progressively generates instructions across varying levels of constraint difficulty. Conflict Detection Conflict detection ensures the soundness of instructions after constraint expansion. It consists of two checks: first, verifying that the newly specified constraints have been correctly incorporated; second, ensuring that the constraints do not conflict with each other (e.g., requiring sentence to be entirely lowercase while also demanding the presence of uppercase words). If either check fails, the instruction is discarded. Otherwise, it is retained, and constraint expansion continues until difficulty Level IV is reached. Instruction Rewriting Instruction rewriting enhances instruction diversity by transforming given instruction to match specified pattern. Specifically, we randomly select constraint pattern and rewrite the instruction accordingly. When handling example-based constraints, we uniformly select three question-answer pairs that share the same constraint subcategories as the original instruction to serve as contextual examples. Using this pipeline, we generate 1,200 distinct test cases and manually write validation code This code is used for for each data point. detailed analysis of the models instructionfollowing capability. The statistical details of the data are presented in Table 1. Family Version LLaMA3.1 Qwen2.5 Instruct-8B Instruct-70B Instruct-7B Instruct-14B Instruct-32B Instruct-72B DeepSeek-R1Instruct-8B Distill-LLaMA Instruct-70B DeepSeek-R1Distill-Qwen Instruct-7B Instruct-14B Instruct-32B Gemini1.5 Claude3.5 GPT Flash Pro Haiku Sonnet 3.5-Turbo 4-Turbo 4o-Mini 4o Constraint Pattern Constraint Category Constraint Difficulty Example Listing Incorporation Content Format Language Length Level Level II Level III Level IV 40.25 68.00 59.25 66.75 67.75 70.25 42.00 59.50 31.25 52.75 57. 71.50 73.50 44.25 72.50 49.00 70.75 67.50 70.50 36.00 54.25 45.00 52.50 57.50 60.25 28.00 64. 26.50 38.00 51.50 61.00 61.75 53.50 69.00 41.25 59.25 67.00 62.50 32.25 48.25 43.75 51.25 54.25 55. 28.00 57.50 27.50 34.25 50.50 64.50 65.25 52.00 61.00 38.00 52.25 59.00 59.00 80.13 86. 83.77 84.42 85.58 87.01 80.00 84.55 78.70 82.99 85.32 88.70 87.40 84.29 86.62 84.94 89.09 85.71 87. 64.74 73.65 54.45 62.74 69.26 70.89 50.44 83.69 50.19 66.75 78.29 80.68 81.81 82.81 83. 70.39 79.17 84.57 84.44 44.28 80.90 90.30 84.95 90.16 92.19 44.72 84.80 24.60 44.86 69.18 87.84 88. 49.06 84.23 42.98 77.57 84.37 82.78 49.60 65.73 64.25 74.60 70.56 72.98 53.09 65.32 51.08 59.41 63. 74.06 75.67 68.01 76.21 66.26 73.25 72.04 69.35 64.67 78.00 82.00 82.00 84.33 84.33 59.67 77. 61.33 70.67 76.33 86.00 86.67 71.33 82.67 77.33 82.67 84.33 84.33 40.67 61.33 55.00 63.67 66.67 66. 40.33 63.00 33.33 49.67 59.33 68.00 70.00 51.67 71.67 48.00 68.00 70.00 69.33 27.33 51. 39.00 49.33 53.67 53.67 18.33 57.00 14.33 29.33 43.33 57.67 59.00 41.67 60.67 30.33 55.00 59.00 57. 12.00 36.67 21.33 32.33 34.67 43.00 12.33 44.33 4.67 17.00 33.00 51.00 51.67 35.00 55. 15.33 37.33 44.67 45.00 Overall 36.17 56.83 49.33 56.83 59.83 61.92 32.67 60.33 28.42 41.67 53. 65.67 66.83 49.92 67.50 42.75 60.75 64.50 64.00 Table 2: Results of the evaluation of LLMs instruction-following ability across different dimensions. Overall denotes the overall score. The best results in each dimension are highlighted in bold."
        },
        {
            "title": "4 Evaluations of LLMs",
            "content": "4.1 Models We conduct an evaluation of 19 LLMs from seven model families, including four open-source and three closed-source, which collectively represent the current state of LLM capabilities. Among the open-source LLMs, we select LLaMA3.1-Instruct-8B and LLaMA3.1-Instruct70B from the LLaMA3.1 family (Team, 2024); Qwen2.5-Instruct-14B, Qwen2.5-Instruct-7B, Qwen2.5-Instruct-32B, and Qwen2.5-Instruct72B from the Qwen2.5 family (Yang et al., DeepSeek-R1-Distill-LLaMA-8B and 2024); DeepSeek-R1-Distill-LLaMA-70B from the DeepSeek-R1-Distill-LLaMA family (DeepSeekand DeepSeek-R1-DistillAI et al., 2025); Qwen-7B, DeepSeek-R1-Distill-Qwen-14B, DeepSeek-R1-Distill-Qwen-32B, and DeepSeekR1-Distill-Qwen2.5-Instruct-32B the DeepSeek-R1-Distill-Qwen family (DeepSeek-AI et al., 2025). For the closed-source LLMs, we include Gemini1.5-Flash and Gemini1.5-Pro from the Gemini1.5 family (Reid et al., 2024); Claude3.5-Haiku and Claude3.5-Sonnet from the Claude3.5 family (Bai et al., 2022); and GPT-3.5-Turbo, GPT-4-Turbo, GPT-4o-Mini, and GPT-4o from the GPT family (OpenAI, 2023).3 from 4.2 Experimental Setup For generating instructions, we use GPT-4o for help.4 To ensure that each models capabilities 3More information can be found in Appendix C. 4The prompts can be found in Appendix B. are accurately represented, we use the built-in chat template for open-source models and the official API interface for closed-source models.5 For consistency and reproducibility, we apply greedy decoding across all evaluations. 4.3 Main Results Table 2 summarizes the results of our multidimensional evaluation of various LLMs. Several key findings emerge from this analysis. Current LLMs vary substantially in their ability to follow different constraint forms. Most models perform best on the Example pattern, underscoring the efficacy of in-context learning (Min et al., 2022). In contrast, performance consistently declines on the Incorporation pattern, suggesting that following constraints in free-form remains significant challenge. Additionally, average accuracy drops sharply from 77.67% at Level to just 32.96% at Level IV, indicating that current models struggle to handle scenarios involving multiple or more complex constraints. Instruction-following performance generally improves with increasing model size. Within most families, larger models exhibit better adherence to instructions, particularly in more demanding settings such as length-constrained or Level IV scenarios. This trend aligns with existing findings on scaling laws in LLMs (Kaplan et al., 2020; Chung et al., 2022). However, the GPT family deviates from this pattern. In certain settings, GPT4o underperforms GPT-4o-Mini. This anomaly may reflect an alignment tax (Ouyang et al., 2022), 5Chat template for each LLM can be found in Appendix D. Dataset Training Ours Test Ours IFEval Multi-IF MMLU GSM8K MATH HumanEval MBPP Capability # Number Instruction Following 7906 Instruction Following Instruction Following Instruction Following Knowledge Reasoning Reasoning Coding Coding 1200 541 13447 14042 1319 5000 164 257 Table 3: Overview of the training and test datasets used when improving instruction-following capabilities. where optimization for broader capabilities comes at the cost of precise instruction-following. Stronger reasoning ability does not guarantee better instruction following. Although DeepSeek-R1-Distill-LLaMA and DeepSeek-R1Distill-Qwen outperform LLaMA3.1 and Qwen2.5 in reasoning-focused benchmarks (DeepSeek-AI et al., 2025), their instruction-following performance is notably weaker. Closer examination reveals disconnect between reasoning and answer: these models often identify the correct constraints in the reasoning processes but fail to implement them in answers. This gap highlights pressing need for training methods that better integrate reasoning processes with instruction execution."
        },
        {
            "title": "Improvements of LLMs",
            "content": "Building on the framework described in Section 3.1, we construct training data tailored for RL to improve the instruction-following capabilities of LLMs. Our approach enhances these capabilities while preserving general performance. Analysis suggests that performance gains stem primarily from updates to attention modules, increasing the models sensitivity to task-specific constraints. 5.1 Dataset 7,906 generate Training We single-turn, constraint-based instructions paired with verifiable code using the data generation pipeline described in Section 3.2. Due to the difficulty of reliably constructing golden answers, this dataset is used exclusively for RL.6 Test To evaluate model performance post-RL, we focus on four capabilities: 1) Instruction Following. We evaluate on our test set, along 6The full distribution of training data is shown in Table 1. IFEval and with two out-of-domain datasets: Multi-IF, which includes multi-turn dialogue-based instructions. 2) Knowledge. General knowledge is assessed using MMLU (Hendrycks et al., 2021a). 3) Reasoning. We use GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021b) to measure logical and mathematical reasoning. and 4) Coding. Programming ability is evaluated with HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). 7 5.2 Experimental Setup We conduct experiments on six LLMs without more than 14 billion parameters, applying the GRPO algorithm for RL. The setup includes batch size of 1,024, mini-batch size of 512, 32 rollouts per update, and learning rate of 1e-6. We use sampling temperature of 0.7 and set the maximum output length to 8,192 tokens. The reward function is defined as the number of constraints satisfied in the output. Training is performed for one epoch on eight A800 GPUs. For evaluation, we apply each models official chat template and use greedy decoding for consistency and reproducibility. 5.3 Results and Analysis Importantly, Performance Improvements Figure 3 presents the performance of individual LLMs before and after applying GRPO.8 The results demonstrate substantial performance gains on our custom test set, with LLaMA3.1-Instruct-8B notably outperforming other models. these improvements extend to out-of-domain instructionfollowing benchmarks and significantly enhance performance in multi-turn dialogue scenarios (i.e., Multi-IF), despite training being conducted solely on single-turn data. This suggests that the data generated by our multi-dimensional constraint framework exhibits strong generalization ability. Furthermore, although our training is focused on improving instruction-following capabilities, it does not degrade general-purpose performance. On general benchmarks, post-GRPO LLMs maintain parity with their original counterparts and, in some cases (e.g., MBPP), show clear improvements. These findings indicate that our pipeline produces data that is both compatible with and complementary to existing training corpora, enabling straightforward performance enhancements when integrated into current LLMs. 7The information of the data is shown in Table 3. 8Detailed results are provided in Appendix E. Figure 3: Performance comparison of each LLM on the test sets before and after applying GRPO. (a) LLaMA3.1-Instruct-8B (b) DeepSeek-R1-DistillLLaMA-Instruct-8B (c) Qwen2.5-Instruct-7B (d) DeepSeek-R1-DistillQwen-Instruct-7B (e) Qwen2.5-Instruct-14B (f) DeepSeek-R1-DistillQwen-Instruct-14B Figure 4: Parameter change rates of LLMs after GRPO relative to the original ones across different modules. Parameter-Level Analysis To better understand the sources of performance improvement, we conduct parameter-level analysis. We compute the relative change rates of model parameters following GRPO, broken down by model modules, and summarize the results in Figure 4. Notably, the most substantial updates occur in the attention modules, suggesting that GRPO primarily refines the models attention mechanisms. These changes are distributed relatively uniformly across layers, indicating global rather than localized adjustment. Overall, applying GRPO with our data effectively tunes the models attention distribution, enhancing its ability to identify and focus on critical input information and thereby improving its instructionfollowing and general performance. Case Studies To visualize how modifications in attention mechanisms affect model behavior, we adopt the information flow analysis method proposed by Wang et al. (2023). We compute the importance of each input token with respect to the models output and present representative visualizations in Table 4. After applying GRPO, the importance of constraint-related tokens increases, while the influence of irrelevant tokens diminishes; the relevance of core problem components remains largely unchanged. This suggests that the model has improved its ability to identify and prioritize constraint-related information without sacrificing its understanding of the overall input. Additionally, the reduced attention to irrelevant tokens may help minimize distraction from non-essential elements, which could explain the observed stability or gains in general task performance. These case studies further validate the effectiveness of our proposed framework and the utility of the data it produces. Case (w/o GRPO) Case (w/ GRPO) LLaMA3.1-Instruct-8B: After applying GRPO, the model becomes more attentive to nuanced constraints, especially specific details like the 5 words requirement, ensuring it meets the length constraint. Qwen2.5-Instruct-7B: After applying GRPO, the model places greater emphasis on the keyword pasta while giving less attention to function words like is, resulting in an output that satisfies the constraint. DeepSeek-R1-Distill-LLaMA-Instruct-8B: After applying GRPO, the model places greater emphasis on factors such as length, keywords, and content, enabling it to meet multiple complex constraints. DeepSeek-R1-Distill-Qwen-Instruct-7B: After applying GRPO, the model places greater emphasis on nuanced details such as level 2 and Markdown, and enhances the distinction between the core components of the original problem, thereby satisfying the constraints. Table 4: visualization showing the importance of each input token to the output, with darker colors indicating greater significance."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we propose multi-dimensional constraint framework that categorizes instructions based on constraint pattern, constraint category, and constraint difficulty. Building on this framework, we design an automated instruction generation pipeline that transforms any instruction into constraint-based one through the processes of constraint expansion, conflict detection, and instruction rewriting. Using this pipeline, we generate 1,200 test cases and conduct comprehensive evaluation of 19 LLMs across seven different model families. We also construct constraintfocused training dataset and apply the GRPO algorithm to enhance instruction-following capabilities in LLMs. The results show that models trained with our data achieve notable improvements while maintaining general capabilities. Parameter-level analysis and case studies indicate that these gains primarily stem from increased model sensitivity to constraint-relevant information."
        },
        {
            "title": "Limitations",
            "content": "Although we evaluate and enhance the instructionfollowing ability of current LLMs using constructive data and analyze the underlying reasons for their improvement, our work still has two key limitations. On one hand, due to the complexity of answer construction, we do not train models from pre-trained version, but instead apply GRPO directly to instruction-tuned models. Nevertheless, our results show that GRPOtrained models do not suffer any loss in general capability, and in some cases even demonstrate improvements, highlighting the compatibility of our constructed data with original training data. On the other hand, since our focus is primarily on enhancing instruction-following capabilities, we do not explore the effects of applying our method to domain-specific datasets. However, because our approach can convert any instruction into constraint-based version, and case studies confirm that the model retains its focus on the core problem components, we believe that applying this method to other domains (e.g., reasoning, coding) can also yield additional performance gains."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program synthesis with large language models. CoRR, abs/2108.07732. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, and 32 others. 2022. Constitutional AI: harmlessness from AI feedback. CoRR, abs/2212.08073. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language In Advances in models are few-shot learners. Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374. Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu, Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang, Hongning Wang, and Minlie Huang. 2024. Spar: Self-play with tree-search refinement to improve instruction-following in large language models. CoRR, abs/2412.11605. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, and 12 others. 2022. Scaling instruction-finetuned language models. CoRR, abs/2210.11416. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 81 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948. Yang Deng, Xuan Zhang, Wenxuan Zhang, Yifei Yuan, See-Kiong Ng, and Tat-Seng Chua. 2024. On the multi-turn instruction following for conversational In Proceedings of the 62nd Annual web agents. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 8795 8812. Association for Computational Linguistics. Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, and Jingren Zhou. 2024. Self-play with execution feedback: Improving instruction-following capabilities of large language models. CoRR, abs/2406.13542. Guanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi Qiao, Zhicheng Dou, and Ji-Rong Wen. 2025. Toward verifiable instruction-following alignment In AAAI-25, for retrieval augmented generation. Sponsored by the Association for the Advancement of Artificial Intelligence, February 25 - March 4, 2025, Philadelphia, PA, USA, pages 2379623804. AAAI Press. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Qianyu He, Jie Zeng, Qianxi He, Jiaqing Liang, and Yanghua Xiao. 2024a. From complex to simple: Enhancing multi-constraint complex instruction following ability of large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 1086410882. Association for Computational Linguistics. benchmark for evaluating the generalizability of large In Findings of the Association language models. for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1243112446. Association for Computational Linguistics. Yun He, Di Jin, Chaoqi Wang, Chloe Bi, Karishma Mandyam, Hejia Zhang, Chen Zhu, Ning Li, Tengyu Xu, Hongjiang Lv, Shruti Bhosale, Chenguang Zhu, Karthik Abinav Sankararaman, Eryk Helenowski, Melanie Kambadur, Aditya Tayade, Hao Ma, Han Fang, and Sinong Wang. 2024b. Multi-if: Benchmarking llms on multi-turn and multilingual instructions following. CoRR, abs/2410.15553. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the MATH dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. Adaptive mixtures of local experts. Neural Comput., 3(1):7987. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2024. Followbench: multi-level fine-grained constraints following benchmark for In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 46674688. Association for Computational Linguistics. large language models. Yimin Jing, Renren Jin, Jiahao Hu, Huishi Qiu, Xiaohua Wang, Peng Wang, and Deyi Xiong. 2023. Followeval: multi-dimensional benchmark for assessing the instruction-following capability of large language models. CoRR, abs/2311.09829. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. CoRR, abs/2001.08361. Yizhi Li, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Noah Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, Wangchunshu Zhou, Yiming Liang, Lei Zhang, Lei Ma, Jiajun Zhang, Zuowen Li, Wenhao Huang, Chenghua Lin, and Jie Fu. 2024. Cif-bench: chinese instruction-following Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: In What makes in-context Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 1104811064. Association for Computational Linguistics. learning work? OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Training language models Ryan Lowe. 2022. to follow instructions with human feedback. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024. Infobench: Evaluating instruction following ability in large In Findings of the Association language models. for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1302513048. Association for Computational Linguistics. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, and 34 others. 2024. Gemini 1.5: Unlocking multimodal understanding CoRR, across millions of tokens of context. abs/2403.05530. Elvis Saravia. 2022. Prompt Engineering Guide. https://github.com/dair-ai/Prompt-EngineeringGuide. Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi Zhang, and Xuanjing Huang. 2023. comprehensive capability analysis of GPT-3 and GPT3.5 series models. CoRR, abs/2303.10420. Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Tao Ji, Qi Zhang, Tao Gui, and Xuanjing Huang. 2025. Tooleyes: Fine-grained evaluation for tool learning capabilities of large language models in real-world scenarios. In Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025, pages 156187. Association for Computational Linguistics. Junjie Ye, Yilong Wu, Sixian Li, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan, and Zhengyin Du. 2024. Tl-training: task-feature-based framework for training large language models in tool use. CoRR, abs/2412.15495. Xinghua Zhang, Haiyang Yu, Cheng Fu, Fei Huang, and Yongbin Li. 2024. IOPO: empowering llms with complex instruction following via input-output preference optimization. CoRR, abs/2411.06208. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. CoRR, abs/2311.07911. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. CoRR, abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300. Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, and Ruohui Huang. 2024. Conifer: Improving complex constrained instructionfollowing ability of large language models. CoRR, abs/2404.02823. Meta Team. 2024. Introducing llama 3.1: Our most capable models to date. Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. Label words are anchors: An information flow perspective for understanding in-context learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 9840 9855. Association for Computational Linguistics. Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxing Xu, Yiming Liu, Jie Tang, Hongning Wang, and Minlie Huang. 2024a. Benchmarking complex instruction-following with In Advances in multiple constraints composition. Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxing Xu, Yiming Liu, Jie Tang, Hongning Wang, and Minlie Huang. 2024b. Benchmarking complex instruction-following with In Advances in multiple constraints composition. Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, and 10 others. 2023. The rise and potential of large language model based agents: survey. CoRR, abs/2309.07864. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and 22 others. 2024. Qwen2.5 technical report. CoRR, abs/2412.15115."
        },
        {
            "title": "A Examples for Different Forms of Constraints",
            "content": "To illustrate the instructions generated by our method, we present selection in Table 5, annotated with their corresponding dimension information. These examples demonstrate that our approach overcomes the templating limitations of prior methods and better captures the diversity of real-world needs. Additionally, the multi-dimensional categorization enables more fine-grained performance analysis. Instruction Pattern Category Difficulty # Example 1: **Question**: What Is The Room? The answer must use capitalized letters for each word. **Answer**: The Room Is Living Room. # Example 2: **Question**: Describe life in 1980. The answer must use capitalized letters for each word. **Answer**: Life In 1980 Was Characterized By The Rise Of Personal Computers, Vibrant Music Scenes With Genres Like Disco And Rock, And Fashion Trends Featuring Bold Colors And Styles. People Enjoyed Watching Movies On VHS Tapes And Listening To Music On Cassette Players. # Example 3: **Question**: Can insert PDFs or documents? The answer must use capitalized letters for each word. **Answer**: Yes, You Can Insert PDFs Or Documents. **Question**: Write script about Matt Rhules 0-3 record so far. The answer must use capitalized letters for each word. # Example 1: **Question**: What are the best books to learn Cybersecurity in 2023? The answer must use capitalized letters for each word and must include the keyword Cybersecurity. **Answer**: Cybersecurity For Beginners And The Art Of Invisibility Are Among The Best Books To Learn Cybersecurity In 2023. # Example 2: **Question**: Hi chat GPT4, how many questions can ask from you when have free account here? The answer must be in all uppercase letters and must include the keyword LIMIT. **Answer**: THE LIMIT IS 50 QUESTIONS PER MONTH. # Example 3: **Question**: Ive read about female wrestler using move called Kegel Crush. What kind of move could this be? It sounds like creative name for submission move. The answer must use capitalized letters for each word and must include the keyword submission. **Answer**: The Kegel Crush Could Be Unique Submission Move Where The Wrestler Applies Pressure To The Opponents Core Muscles, Forcing Submission Through Intense Abdominal Constriction. **Question**: want you to create detailed curriculum for mastering each of the skills listed below. Divide each skill into multiple sub-topics and list the skills required for each sub-topic and suggest the best online courses and books for each sub-topic. Make sure it should be free. Additionally, the answer must use capitalized letters for each word and must include the keyword Curriculum Design. Example Language Level Example Content Language Level II Instruction Pattern Category Difficulty # Example 1: **Question**: What are the best books to learn Java in 2023? Please ensure that the answer uses capitalized letters for each word, is at most 50 words long, and includes the keyword Java Programming. **Answer**: Effective Java By Joshua Bloch, Java: The Complete Reference By Herbert Schildt, And Head First Java By Kathy Sierra Are Excellent Books For Learning Java Programming In 2023. # Example 2: **Question**: What is the ice level in Plants vs Zombies mini-games? The answer must use capitalized letters for each word, be at most 20 words, and include the keyword Plants vs Zombies. **Answer**: The Ice Level In Plants Vs Zombies Mini-Games Is Called Icebound Battleground. # Example 3: **Question**: Please provide detailed explanation of performance evaluation by competencies and the S20/23 evaluation cited in APA standards that is specific to Ecuador. The response must be at least 150 words to ensure comprehensive understanding of the topic. Additionally, the answer must use capitalized letters for each word. Furthermore, the explanation must include the keyword Ecuadorian Educational Standards to ensure relevance to the local context. **Answer**: Performance Evaluation By Competencies In Ecuador Focuses On Assessing Individuals Based On Specific Skills And Abilities Relevant To Their Roles. This Approach Aligns With The Ecuadorian Educational Standards, Which Emphasize Competency-Based Learning To Ensure Students Acquire Practical Skills. The S20/23 Evaluation, Cited In APA Standards, Is Framework Used To Measure These Competencies Through Structured Assessment Process. In Ecuador, This Evaluation Is Adapted To Reflect Local Educational Goals And Cultural Contexts, Ensuring That Students Meet The National Standards. The Ecuadorian Educational Standards Play Crucial Role In Shaping The Criteria For These Evaluations, Ensuring That They Are Relevant And Comprehensive. By Focusing On Competencies, The Evaluation Aims To Provide Holistic View Of Students Abilities, Preparing Them For Future Challenges. This Methodology Is Integral To The Educational System In Ecuador, As It Promotes Continuous Improvement And Alignment With Global Educational Practices. **Question**: Can you provide list of airlines that have branded fares? Please ensure that the answer is at most 50 words long, making it concise and to the point. Additionally, the answer must use capitalized letters for each word. The answer must also include the keyword branded fares. Example Content Language Length Level III Instruction Pattern Category Difficulty # Example 1: **Question**: Provide definition for the name Patrick. The answer should be concise, using at most 50 words, and must use capitalized letters for each word. Additionally, the answer must include the keyword Saint to reflect the historical and cultural significance associated with the name. Furthermore, the answer must be formatted with level 2 heading in Markdown. **Answer**: ## Patrick: Name Historically Linked To Saint Patrick, The Patron Saint Of Ireland, Known For Spreading Christianity And Celebrated On Saint Patricks Day. # Example 2: **Question**: What type of energy do herbivores and omnivores gain when they eat plants? The answer must be at most 5 words long, use capitalized letters for each word, include the keyword Energy, and be formatted as level 2 heading in Markdown. **Answer**: ## Chemical Energy From Plants # Example 3: **Question**: What is choline? Please provide your answer in at most 3 sentences, and ensure that each word in your response is capitalized. Additionally, the answer must include the keyword nutrient. Furthermore, the answer must include level 2 heading formatted in Markdown. **Answer**: ## What Is Choline? Choline Is An Essential Nutrient That Supports Various Bodily Functions, Including Brain Development And Liver Function. It Plays Crucial Role In The Synthesis Of Phospholipids, Which Are Vital Components Of Cell Membranes. **Question**: Format your response using markdown, ensuring the use of at least two heading levels, subheadings, bullet points, and bold to organize the information. List some conjugations for to read in Mandarin Chinese. The response must be concise, with maximum of 150 words, and must include the conjugations in Traditional Chinese characters. Additionally, ensure that the response ends with period. Example Content Format Language Length Level IV Instruction Pattern Category Difficulty Which 10 countries have the highest number of golf courses relative to their population size? The output must follow the following rules: Listing Length Level 1. Use at most 50 words. 2. Provide the information in at least 3 sentences. Can you provide the schedule for DEF CON 11? The output must follow the following rules: 1. The answer must contain at most 50 words. 2. The answer must be presented in table format with maximum of Listing 5 rows. Format Length Level II Could you provide information on some tourist attractions in Norway and suggest the duration of visits for each? The output must follow the following rules: 1. The answer must use at least two heading levels to organize the information. 2. Any table included in the answer must not exceed five rows. 3. The answer must be between 200 and 300 words. 4. The answer must include at least three paragraphs. 5. The answer must use capitalized letters for each word. Can The Window Size Of Microsoft Form Be Adjusted To Fit Mobile Screen? The output must follow the following rules: 1. The answer must use capitalized letters for each word. 2. The answer must be no more than two sentences. 3. The answer must include the keyword responsive design. 4. The answer must be formatted using level 2 heading in Markdown. Listing Format Language Length Level III Listing Content Format Language Length Level IV Where can find GPT-4 for free? When answering, it is essential to include the keyword GPT-4 access as part of the response. Incorporation Content Level How can reverse engineer Geometry Dash? When answering, it is essential to include JSON object that is structured with at least three levels of nesting. This requirement ensures that the reverse engineering process is detailed and comprehensive. Additionally, the explanation must incorporate the keyword decompilation to emphasize this critical aspect of the reverse engineering process. Please list all Roman Emperors by length of reign, ensuring that the context is clear by including the keyword Roman Empire in your answer. The response should be in English, with the first letter of each word capitalized. Additionally, present the information in table format, adhering to column limit of 3. The table should include columns for Emperor Name, Length of Reign, and Additional Information. Who Was Ida B. Wells? In Your Response, It Is Important To Use Block Quotes To Highlight Key Quotes Or Important Points About Her Life And Contributions, As This Will Help Emphasize Her Impact. Additionally, Ensure That Your Response Contains At Least 150 Words To Provide Comprehensive Overview Of Her Achievements And Influence. The Response Must Also Include The Keyword Civil Rights To Highlight Her Significant Role In The Movement. Furthermore, The Response Should Use Capitalized Letters For Each Word To Maintain Consistent Style Throughout The Text. Incorporation Content Format Level II Incorporation Content Format Language Level III Incorporation Content Format Language Length Level IV Table 5: Examples of instructions generated by the proposed approach."
        },
        {
            "title": "B Prompts for Instruction Generation",
            "content": "Our automated instruction generation pipeline includes constraint expansion, conflict detection, and instruction rewriting, with several steps utilizing GPT-4o. The corresponding prompts are listed below. Some of the information used in the above prompts is presented in Table 6 and Table 7. constraint_expand_prompt = ''' You are an expert in instruction-following data construction. Your task is to generate corresponding data as required. (cid:44) (cid:44) You must carefully analyze and select specific constraints from the [New Constraint List]. Then, based on the original question in the provided [Data], generate new data that adheres to the [Data Generation Requirements]. Finally, respond in the format specified in the [Response Format]. (cid:44) (cid:44) [New Constraint List]: {new_constrint_list} [Data Generation Requirements]: [Core Requirements]: 1. Ensure only {c1} added, that is, {c2}. The word following [Main Category] should be the main category. (cid:44) (cid:44) 2. Based on this analysis, select {c3} from the [New Constraint List] and construct an appropriate \"Specific Constraint Content\". Add it to the [Original Constraint List] in the provided data, and return the [Updated Constraint List]. (cid:44) (cid:44) 3. Modify the content of the [Original Question] in the provided data to (cid:44) (cid:44) (cid:44) (cid:44) **explicitly and clearly specify all the constraints** in the [Updated Constraint List]. The modified question must clearly describe each constraint in natural language, ensuring that the constraints are fully integrated into the question text. For example: - Original Question: \"Tell me about machine learning.\" - Constraint: \"The answer must use capitalized letters for each word.\" - Modified Question: \"Tell me about machine learning. The answer must use capitalized letters for each word.\" (cid:44) 4. Ensure that the Specific Constraint in each constraint triplet is detailed and specific, containing concrete information or examples (e.g., instead of \"Must include\", specify \"Must include the keyword 'machine learning'\"). (cid:44) (cid:44) (cid:44) [Notes]: 1. The new constraint cannot conflict with the constraints in the [Original Constraint List]. (cid:44) 2. The modified [Question with the New Constraint] must **explicitly describe all the constraints** in natural language, ensuring that the constraints are fully integrated into the question text. Constraints should not be implicitly applied to the answer without being explicitly stated in the question. (cid:44) (cid:44) (cid:44) (cid:44) 3. Make sure the Specific Constraint in each constraint triplet is as specific as possible, including concrete details or examples. (cid:44) (cid:44) 4. **Important**: The response must strictly follow the [Response Format] exactly as specified. Do not include any numbering, bullet points, or additional formatting. The [Updated Constraint List] must be outputted as single list of tuples in the exact format shown, without any additional characters or line breaks between the tuples. (cid:44) (cid:44) (cid:44) 5. When generating the modified [Question with the New Constraint], ensure that the language is natural and well-polished. Enrich the phrasing of constraints to avoid redundancy and monotony. (cid:44) (cid:44) [Response Format]: [Thinking Process]: xxx (cid:44) (cid:44) [Updated Constraint List]: [(Main Category, Subcategory, Specific Constraint), (Main Category, Subcategory, Specific Constraint), ...] (The main category is the word after [Main Category], and the constraints we provide are just broad scopes. You need to find suitable specific constraints based on the question and its answers. The Specific Constraint should be detailed and specific.) (cid:44) (cid:44) (cid:44) [Question with the New Constraint]: xxx [Data]: [Original Constraint List]: [{original_constraint_list}] [Original Question]: {original_question} ''' conflict_detection_prompt = ''' You are an expert in data structure following instructions. You need to perform series of checks on the given [Data] according to the [Check Requirements] and finally respond in the format specified in the [Response Format]. (cid:44) (cid:44) [Check Requirements]: 1. Check if there is any constraint conflict in the \"Constraint List\" in the provided data. Explain first and then conclude. (cid:44) 2. Check if the \"Question\" in the provided data clearly specifies all the (cid:44) constraint requirements in the \"Constraint List\". Explain first and then conclude. (cid:44) 3. The response format should follow the requirements specified in the [Response Format] below. (cid:44) [Response Format]: # Constraint Conflict Check # [Specific Explanation]: [Is there any constraint conflict in the constraints of the data]: [Yes/No] # Does the Question clearly specify all constraints in the Constraint List Check # (cid:44) [Specific Explanation]: [Explanation] [Does the question include all constraints from the constraint list]: [Yes/No] [Data]: [Constraint List]: [{constraint_list}] [Question]: {quetsion} ''' instruction_rewriting_listing = ''' You are an expert in constructing data based on instructions. You need to generate the corresponding data as required. (cid:44) You should modify the given [Original Question] according to the [Core Requirements] without changing the original meaning of the question. Then, respond in the format specified in the [Reply Format]. (cid:44) (cid:44) [Core Requirements]: 1. Fully understand the [Original Question] and the constraints listed in the [Constraint List]. (cid:44) 2. Change the expression of the [Original Question]. First, extract the core (cid:44) (cid:44) (cid:44) (cid:44) question from the [Original Question] that is not bound by constraints, then list the constraints corresponding to the [Constraint List] at the end of the sentence. Start with \"The output must follow the following rules:\" and list the constraints from the [Original Question] clearly after understanding the constraints. (cid:44) 3. The modified question must remain consistent with the [Original Question] in terms of meaning and constraints. (cid:44) [Reply Format]: [Constraint List Data]: Core question (does not include constraint descriptions in the constraint list), (cid:44) The output must follow the following rules: 1.xxx 2.xxx [Data]: [Original Question]:{original_question} [Constraint List]:{constraint_list} ''' instruction_rewriting_incorporation = ''' You are an expert in data construction based on instructions. You need to generate the corresponding data as required. (cid:44) You should modify the given [Data] according to the [Core Requirements] without changing the original meaning of the question. Then, respond in the format specified in the [Reply Format]. (cid:44) (cid:44) [Core Requirements]: 1. Do not alter the question to directly satisfy the constraints. 2. Fully understand the [Original Question] and the constraints within it. 3. Modify the expression of the constraints in the [Original Question] by (cid:44) (cid:44) clearly describing them in the question, so that the question explicitly indicates the constraints, without changing its structure to meet those constraints directly. (cid:44) 4. The modified question should keep the original meaning and intent, while the constraints are introduced as descriptive explanations or clarifications in the question. (cid:44) 5. Ensure that the constraints are explicitly described in the question, making it clear that they need to be considered when answering, without altering the question to directly satisfy them. (cid:44) (cid:44) (cid:44) [Reply Format]: [Constraint Integration Format Data]: xxx [Data]: [Original Question]:{original_question} [Constraint List]:{constraint_list} ''' Category New Constraint List Content Format Language Length Main Category : Content Subcategory : { Keywords: Must include, Repeated, Avoid Identifiers: Start identifier, End identifier, Delimiting identifier Punctuation: Ending punctuation, Exclude punctuation } Main Category : Format Subcategory : { Markdown: Heading levels, Block quotes Json: Object nesting levels XML: Number of attributes Table: Row limit, Column limit } Main Category : Language Subcategory : { Chinese: Simpfied, Traditional English: All Uppercase, Capitalized, Lowercase } Main Category : Length Subcategory : { Words: At most, At least, Range Sentences: At most, At least, Range Paragraphs: At most, At least, Range } Table 6: New constraint list for different constraint categories. c1 c2 one new constraint is two new constraints are single (Primary category, secondary category, specific constraint) triplet two (Primary category, secondary category, specific constraint) triplets one constraint two constraints Table 7: c1, c2, and c3 for the prompt of contraint expansion."
        },
        {
            "title": "C Detailed Information of Models",
            "content": "We conduct an evaluation of 19 LLMs from seven families, including four open-source and three closedsource, that collectively represent the capabilities of current LLMs. Open-Source LLMs We evaluate eleven open-source LLMs across four model families: LLaMA3.1 family, developed by Meta, comprises open-source models that demonstrate strong performance in general knowledge and multilingual translation. We evaluate LLaMA3.1-Instruct-8B and LLaMA3.1-Instruct-70B. Qwen2.5 family, released by Alibaba, includes open-source models pre-trained on up to 18 trillion tokens. These models show substantial improvements in programming and mathematical reasoning. We evaluate Qwen2.5-Instruct-7B, Qwen2.5-Instruct-14B, Qwen2.5-Instruct-32B, and Qwen2.5Instruct-72B. DeepSeek-R1-Distill-LLaMA family consists of models distilled from the LLaMA family using data generated by DeepSeek-R1 (DeepSeek-AI et al., 2025). These models exhibit notable gains in mathematical performance. We evaluate DeepSeek-R1-Distill-LLaMA-8B and DeepSeek-R1-DistillLLaMA-70B. DeepSeek-R1-Distill-Qwen family is based on the Qwen2.5 family and similarly distilled using data generated by DeepSeek-R1. We evaluate DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-DistillQwen-14B, and DeepSeek-R1-Distill-Qwen-32B. Closed-Source LLMs We evaluate eight closed-source LLMs across three model families: Gemini1.5 family, developed by Google, represents new generation of models built on the Mixtureof-Experts (Jacobs et al., 1991) architecture. These models demonstrate enhanced performance, particularly in long-context understanding across multiple modalities. We evaluate Gemini1.5-Flash and Gemini1.5-Pro. Claude3.5 family, developed by Anthropic, features models with strong general-purpose capabilities and coding capabilities. We evaluate Claude3.5-Haiku and Claude3.5-Sonnet. GPT family, released by OpenAI, includes models that represent the current frontier in LLM development. We evaluate GPT-3.5-Turbo, GPT-4-Turbo, GPT-4o-Mini, and GPT-4o."
        },
        {
            "title": "D Chat Template for Different LLMs",
            "content": "For the open-source LLMs, we use their built-in chat templates for both training and evaluation. The chat templates for each model family are summarized below. Notably, DeepSeek-R1-Distill-LLaMA and DeepSeek-R1-Distill-Qwen share the same template. LLaMA3.1-Instruct_template = ''' {{- bos_token }} {%- if custom_tools is defined %} {%- set tools = custom_tools %} {%- endif %} {%- if not tools_in_user_message is defined %} {%- set tools_in_user_message = true %} {%- endif %} {%- if not date_string is defined %} {%- set date_string = \"26 Jul 2024\" %} {%- endif %} {%- if not tools is defined %} {%- set tools = none %} {%- endif %} {#- This block extracts the system message, so we can slot it into the right place. #} (cid:44) {%- if messages[0]['role'] == 'system' %} {%- set system_message = messages[0]['content']trim %} {%- set messages = messages[1:] %} {%- else %} {%- set system_message = \"\" %} {%- endif %} {#- System message + builtin tools #} {{- \"<start_header_id>system<end_header_id>nn\" }} {%- if builtin_tools is defined or tools is not none %} {{- \"Environment: ipythonn\" }} {%- endif %} {%- if builtin_tools is defined %} {{- \"Tools: \" + builtin_tools reject('equalto', 'code_interpreter') join(\", \") + \"nn\"}} (cid:44) {%- endif %} {{- \"Cutting Knowledge Date: December 2023n\" }} {{- \"Today Date: \" + date_string + \"nn\" }} {%- if tools is not none and not tools_in_user_message %} {{- \"You have access to the following functions. To call function, please respond with JSON for function call.\" }} (cid:44) {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }} (cid:44) {{- \"Do not use variables.nn\" }} {%- for in tools %} {{- tojson(indent=4) }} {{- \"nn\" }} {%- endfor %} {%- endif %} {{- system_message }} {{- \"<eot_id>\" }} {#- Custom tools are passed in user message with some extra guidance #} {%- if tools_in_user_message and not tools is none %} {#- Extract the first user message so we can plug it in here #} {%- if messages length != 0 %} {%- set first_user_message = messages[0]['content']trim %} {%- set messages = messages[1:] %} {%- else %} {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }} (cid:44) {%- endif %} {{- '<start_header_id>user<end_header_id>nn' -}} {{- \"Given the following functions, please respond with JSON for function call \" }} (cid:44) {{- \"with its proper arguments that best answers the given prompt.nn\" }} {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }} (cid:44) {{- \"Do not use variables.nn\" }} {%- for in tools %} {{- tojson(indent=4) }} {{- \"nn\" }} {%- endfor %} {{- first_user_message + \"<eot_id>\"}} {%- endif %} {%- for message in messages %} {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' (cid:44) in message) %} {{- '<start_header_id>' + message['role'] + '<end_header_id>nn'+ message['content'] trim + '<eot_id>' }} (cid:44) {%- elif 'tool_calls' in message %} {%- if not message.tool_callslength == 1 %} {{- raise_exception(\"This model only supports single tool-calls at once!\") }} (cid:44) {%- endif %} {%- set tool_call = message.tool_calls[0].function %} {%- if builtin_tools is defined and tool_call.name in builtin_tools %} {{- '<start_header_id>assistant<end_header_id>nn' -}} {{- \"<python_tag>\" + tool_call.name + \".call(\" }} {%- for arg_name, arg_val in tool_call.arguments items %} {{- arg_name + '=\"' + arg_val + '\"' }} {%- if not loop.last %} {{- \", \" }} {%- endif %} {%- endfor %} {{- \")\" }} {%- else %} {{- '<start_header_id>assistant<end_header_id>nn' -}} {{- '{\"name\": \"' + tool_call.name + '\", ' }} {{- '\"parameters\": ' }} {{- tool_call.arguments tojson }} {{- \"}\" }} {%- endif %} {%- if builtin_tools is defined %} {#- This means we're in ipython mode #} {{- \"<eom_id>\" }} {%- else %} {{- \"<eot_id>\" }} {%- endif %} {%- elif message.role == \"tool\" or message.role == \"ipython\" %} {{- \"<start_header_id>ipython<end_header_id>nn\" }} {%- if message.content is mapping or message.content is iterable %} {{- message.content tojson }} {%- else %} {{- message.content }} {%- endif %} {{- \"<eot_id>\" }} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '<start_header_id>assistant<end_header_id>nn' }} {%- endif %} ''' Qwen2.5-Instruct_template = ''' {%- if tools %} {{- '<im_start>systemn' }} {%- if messages[0]['role'] == 'system' %} {{- messages[0]['content'] }} {%- else %} {{- 'You are Qwen, created by Alibaba Cloud. You are helpful assistant.' }} {%- endif %} {{- \"nn# ToolsnnYou may call one or more functions to assist with the user query.nnYou are provided with function signatures within <tools></tools> XML tags:n<tools>\" }} (cid:44) (cid:44) {%- for tool in tools %} {{- \"n\" }} {{- tool tojson }} {%- endfor %} {{- \"n</tools>nnFor each function call, return json object with function (cid:44) (cid:44) name and arguments within <tool_call></tool_call> XML tags:n<tool_call>n{\"name\": <function-name>, \"arguments\": <args-json-object>}n</tool_call><im_end>n\" }} (cid:44) {%- else %} {%- if messages[0]['role'] == 'system' %} {{- '<im_start>systemn' + messages[0]['content'] + '<im_end>n' }} {%- else %} {{- '<im_start>systemnYou are Qwen, created by Alibaba Cloud. You are helpful assistant.<im_end>n' }} (cid:44) {%- endif %} {%- endif %} {%- for message in messages %} {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) (cid:44) or (message.role == \"assistant\" and not message.tool_calls) %} {{- '<im_start>' + message.role + 'n' + message.content + '<im_end>' + 'n' }} (cid:44) {%- elif message.role == \"assistant\" %} {{- '<im_start>' + message.role }} {%- if message.content %} {{- 'n' + message.content }} {%- endif %} {%- for tool_call in message.tool_calls %} {%- if tool_call.function is defined %} {%- set tool_call = tool_call.function %} {%- endif %} {{- 'n<tool_call>n{\"name\": \"' }} {{- tool_call.name }} {{- '\", \"arguments\": ' }} {{- tool_call.arguments tojson }} {{- '}n</tool_call>' }} {%- endfor %} {{- '<im_end>n' }} {%- elif message.role == \"tool\" %} {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %} {{- '<im_start>user' }} {%- endif %} {{- 'n<tool_response>n' }} {{- message.content }} {{- 'n</tool_response>' }} {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %} {{- '<im_end>n' }} {%- endif %} {%- endif %} {%- endfor %} {%- if add_generation_prompt %} {{- '<im_start>assistantn' }} {%- endif %} ''' DeepSeek-R1-Distill = ''' {% if not add_generation_prompt is defined %} {% set add_generation_prompt = false %} {% endif %} {% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %} (cid:44) {%- for message in messages %} {%- if message['role'] == 'system' %} {% set ns.system_prompt = message['content'] %} {%- endif %} {%- endfor %} {{bos_token}} {{ns.system_prompt}} {%- for message in messages %} {%- if message['role'] == 'user' %} {%- set ns.is_tool = false -%} {{'<User>' + message['content']}} {%- endif %} {%- if message['role'] == 'assistant' and message['content'] is none %} {%- set ns.is_tool = false -%} {%- for tool in message['tool_calls']%} {%- if not ns.is_first %} {{'<Assistant><tool_calls_begin><tool_call_begin>' + (cid:44) (cid:44) (cid:44) (cid:44) tool['type'] + '<tool_sep>' + tool['function']['name'] + 'n' + '```json' + 'n' + tool['function']['arguments'] + 'n' + '```' + '<tool_call_end>'}} (cid:44) {%- set ns.is_first = true -%} {%- else %} {{'n' + '<tool_call_begin>' + tool['type'] + '<tool_sep>' + tool['function']['name'] + 'n' + '```json' + 'n' + tool['function']['arguments'] + 'n' + '```' + '<tool_call_end>'}} (cid:44) {{'<tool_calls_end><end_of_sentence>'}} {%- endif %} {%- endfor %} {%- endif %} {%- if message['role'] == 'assistant' and message['content'] is not none %} {%- if ns.is_tool %} {{'<tool_outputs_end>' + message['content'] + '<end_of_sentence>'}} {%- set ns.is_tool = false -%} {%- else %} {% set content = message['content'] %} {% if '</think>' in content %} {% set content = content.split('</think>')[-1] %} {% endif %} {{'<Assistant>' + content + '<end_of_sentence>'}} {%- endif %} {%- endif %} {%- if message['role'] == 'tool' %} {%- set ns.is_tool = true -%} {%- if ns.is_output_first %} {{'<tool_outputs_begin><tool_output_begin>' + message['content'] + '<tool_output_end>'}} (cid:44) {%- set ns.is_output_first = false %} {%- else %} {{'n<tool_output_begin>' + message['content'] + '<tool_output_end>'}} (cid:44) {%- endif %} {%- endif %} {%- endfor -%} {% if ns.is_tool %} {{'<tool_outputs_end>'}} {% endif %} {% if add_generation_prompt and not ns.is_tool %} {{'<Assistant><think>n'}} {% endif %} '''"
        },
        {
            "title": "E Detailed Results",
            "content": "Table 8 to Table 13 present the detailed performance of the LLMs on each test set before and after GRPO. In these tables, denotes the performance difference between the post-GRPO and pre-GRPO models, with positive values highlighted in green and negative values in red. The results demonstrate that applying our data for GRPO substantially enhances the models capabilities across all aspects, highlighting the effectiveness of our data. Models Constraint Pattern Constraint Category Constraint Difficulty Example Listing Incorporation Content Format Language Length Level Level II Level III Level IV 52.50 86.75 34.25 36.00 91.00 55.00 45.00 89.25 44.25 LLaMA3.1-Instruct-8B 40.25 w/o GRPO 89.00 w/ GRPO 48.75 Qwen2.5-Instruct-7B 59.25 w/o GRPO 87.50 w/ GRPO 28.25 Qwen2.5-Instruct-14B 66.75 w/o GRPO 81.50 w/ GRPO 14.75 DeepSeek-R1-Distill-LLaMA-Instruct-8B w/o GRPO 42.00 w/ GRPO 83.75 41.75 DeepSeek-R1-Distill-Qwen-Instruct-7B 31.25 w/o GRPO 66.75 w/ GRPO 35.50 DeepSeek-R1-Distill-Qwen-Instruct-14B w/o GRPO w/ GRPO 26.50 71.50 45. 28.00 85.50 57.50 52.75 81.50 28.75 38.00 86.75 48.75 32.25 84.25 52.00 43.75 81.50 37.75 51.25 77.75 26. 28.00 77.00 49.00 27.50 65.00 37.50 34.25 77.75 43.50 80.13 95.23 15.10 83.77 94.10 10.33 84.42 92.85 8. 80.00 93.48 13.48 78.70 87.83 9.13 82.99 92.85 9.86 64.74 92.20 27.46 54.45 88.84 34.39 62.74 85.08 22. 50.44 83.60 33.16 50.19 72.18 21.99 66.75 85.08 18.33 44.28 98.70 54.42 90.30 98.26 7.96 84.95 97.83 12. 44.72 98.55 53.83 24.60 90.01 65.41 44.86 97.83 52.97 49.60 93.38 43.78 64.25 93.77 29.52 74.60 91.30 16. 53.09 92.60 39.51 51.08 89.48 38.40 59.41 91.30 31.89 64.67 94.33 29.66 82.00 94.67 12.67 82.00 91.33 9. 59.67 91.33 31.66 61.33 89.00 27.67 70.67 91.33 20.66 40.67 88.67 48.00 55.00 87.67 32.67 63.67 85.00 21. 40.33 85.00 44.67 33.33 73.67 40.34 49.67 85.00 35.33 27.33 85.33 58.00 39.00 81.00 42.00 49.33 79.67 30. 18.33 78.00 59.67 14.33 57.33 43.00 29.33 79.67 50.34 12.00 84.00 72.00 21.33 81.00 59.67 32.33 72.00 39. 12.33 74.00 61.67 4.67 51.00 46.33 17.00 72.00 55.00 Overall 36.17 88.08 51.91 49.33 86.08 36. 56.83 82.00 25.17 32.67 82.08 49.41 28.42 67.75 39.33 41.67 82.00 40.33 Table 8: Evaluation results on our custom test set. Models Prompt Str. Instruction Str. Prompt Loo. Instruction Loo. 79.02 85.73 6.71 80.34 82.97 2.63 LLaMA3.1-Instruct-8B 70.79 w/o GRPO 79.11 w/ GRPO 8.32 Qwen2.5-Instruct-7B 72.83 w/o GRPO 75.97 w/ GRPO 3.14 Qwen2.5-Instruct-14B 79.67 w/o GRPO 83.55 w/ GRPO 3.88 DeepSeek-R1-Distill-LLaMA-Instruct-8B 71.82 61.55 w/o GRPO 85.13 79.30 w/ GRPO 13.31 17.75 DeepSeek-R1-Distill-Qwen-Instruct-7B 65.59 53.79 w/o GRPO 77.22 68.76 w/ GRPO 11.63 14.97 DeepSeek-R1-Distill-Qwen-Instruct-14B 78.42 70.24 w/o GRPO 89.45 84.47 w/ GRPO 11.03 14. 84.53 88.61 4.08 74.49 80.22 5.73 74.86 79.30 4.44 81.52 86.51 4.99 67.65 82.07 14.42 58.04 72.09 14. 74.12 86.32 12.20 82.49 86.57 4.08 82.01 85.61 3.60 86.21 90.89 4.68 76.50 87.41 10.91 69.42 80.34 10. 81.41 90.29 8.88 Table 9: Evaluation results on IFEval. Models Italian Spanish Hindi Portuguese English French Chinese Russian Avg. 83.08 87.23 4.15 79.01 81.96 2. 59.25 62.67 3.42 72.70 83.58 10.88 58.50 69.87 11.37 LLaMA3.1-Instruct-8B w/o GRPO 68.30 83.53 w/ GRPO 15.23 Qwen2.5-Instruct-7B w/o GRPO 78.78 79.91 w/ GRPO 1.13 Qwen2.5-Instruct-14B w/o GRPO 78.98 65.79 84.55 w/ GRPO 69.47 3.68 5.57 DeepSeek-R1-Distill-LLaMA-Instruct-8B 43.15 w/o GRPO 53.82 50.86 76.08 w/ GRPO 7.71 22.26 DeepSeek-R1-Distill-Qwen-Instruct-7B 41.32 w/o GRPO 54.81 52.81 70.97 w/ GRPO 11.49 16.16 DeepSeek-R1-Distill-Qwen-Instruct-14B 48.09 w/o GRPO 68.39 63.25 81.82 w/ GRPO 15.16 13.43 67.75 83.13 15.38 57.78 73.99 16. 56.23 72.62 16.39 69.50 81.65 12.15 75.64 78.27 2.63 79.43 86.42 6.99 53.22 75.44 22.22 55.97 69.20 13. 62.08 81.03 18.95 79.29 81.99 2.70 76.66 78.72 2.06 83.95 86.96 3.01 60.44 79.92 19.48 60.00 72.52 12. 72.28 83.89 11.61 67.83 77.39 9.56 75.99 79.16 3.17 79.43 84.51 5.08 51.44 67.61 16.17 54.80 70.67 15. 66.74 80.84 14.10 62.38 75.04 12.66 72.56 78.35 5.79 76.03 81.48 5.45 64.01 70.49 6.48 65.35 70.49 5. 73.54 82.41 8.87 60.52 71.22 10.70 70.76 72.89 2.13 72.89 79.02 6.13 49.24 61.55 12.31 50.66 60.98 10. 62.73 77.20 14.47 68.60 78.41 9.81 73.83 76.62 2.79 78.05 82.85 4.80 54.37 70.21 15.84 55.36 68.09 12. 65.68 79.54 13.86 Table 10: Evaluation results on Multi-IF for turn 1. Models Italian Spanish Hindi Portuguese English French Chinese Russian Avg. 64.31 73.49 9.18 50.71 55.87 5. 67.34 72.09 4.75 62.60 61.58 -1.02 44.81 47.13 2.32 LLaMA3.1-Instruct-8B w/o GRPO 59.62 70.76 w/ GRPO 11.14 Qwen2.5-Instruct-7B w/o GRPO 60.33 61.78 w/ GRPO 1.45 Qwen2.5-Instruct-14B 54.27 w/o GRPO 68.64 61.04 72.35 w/ GRPO 6.77 3.71 DeepSeek-R1-Distill-LLaMA-Instruct-8B 28.29 w/o GRPO 44.92 43.48 63.35 w/ GRPO 15.19 18.43 DeepSeek-R1-Distill-Qwen-Instruct-7B 30.50 w/o GRPO 39.96 44.02 57.43 w/ GRPO 13.52 17.47 DeepSeek-R1-Distill-Qwen-Instruct-14B 29.95 w/o GRPO 52.39 46.44 69.54 w/ GRPO 16.49 17.15 51.48 69.60 18.12 45.01 63.59 18. 48.02 59.16 11.14 62.76 67.89 5.13 55.74 59.72 3.98 66.05 72.07 6.02 44.90 63.13 18.23 46.03 54.55 8. 43.09 67.33 24.24 71.64 72.79 1.15 62.39 64.78 2.39 71.21 74.25 3.04 55.15 69.34 14.19 51.25 61.98 10. 57.29 73.39 16.10 62.92 69.49 6.57 61.74 60.89 -0.85 67.13 72.15 5.02 41.90 59.72 17.82 42.87 57.56 14. 49.98 71.96 21.98 54.81 66.35 11.54 56.27 57.85 1.58 62.72 67.76 5.04 54.69 61.08 6.39 52.06 59.39 7. 62.38 69.69 7.31 41.97 54.57 12.60 50.59 52.45 1.86 55.00 61.56 6.56 30.67 38.14 7.47 31.82 38.69 6. 36.97 56.38 19.41 59.82 66.94 7.12 57.30 58.81 1.51 64.66 69.61 4.95 44.04 58.66 14.62 43.41 54.72 11. 48.50 66.12 17.62 Table 11: Evaluation results on Multi-IF for turn 2. Models Italian Spanish Hindi Portuguese English French Chinese Russian Avg. 56.80 61.61 4.81 49.02 47.94 -1. 35.92 36.70 0.78 54.55 61.03 6.48 41.78 46.79 5.01 LLaMA3.1-Instruct-8B w/o GRPO 51.27 59.08 w/ GRPO 7.81 Qwen2.5-Instruct-7B w/o GRPO 48.61 50.11 w/ GRPO 1.50 Qwen2.5-Instruct-14B w/o GRPO 59.33 45.65 62.19 w/ GRPO 52.41 6.76 2.86 DeepSeek-R1-Distill-LLaMA-Instruct-8B 19.85 w/o GRPO 34.95 29.32 50.37 w/ GRPO 9.47 15.42 DeepSeek-R1-Distill-Qwen-Instruct-7B 19.90 w/o GRPO 24.85 31.23 43.45 w/ GRPO 11.33 18.6 DeepSeek-R1-Distill-Qwen-Instruct-14B 21.55 w/o GRPO 35.43 35.45 56.74 w/ GRPO 13.90 21.31 32.51 48.43 15.92 32.60 44.81 12. 37.45 56.19 18.74 54.54 59.75 5.21 46.71 48.30 1.59 56.67 61.42 4.75 35.67 52.51 16.84 34.33 45.55 11. 34.30 54.27 19.97 63.75 64.72 0.97 52.55 53.85 1.30 62.40 66.33 3.93 41.91 58.75 16.84 36.95 53.39 16. 41.11 63.12 22.01 54.91 58.53 3.62 49.22 48.50 -0.72 58.83 61.63 2.80 32.33 46.58 14.25 33.71 44.70 10. 36.01 58.47 22.46 45.53 53.71 8.18 46.74 47.02 0.28 51.82 56.03 4.21 44.42 50.12 5.70 42.58 49.72 7. 51.34 58.66 7.32 35.31 41.47 6.16 38.74 40.02 1.28 44.59 50.04 5.45 23.06 31.16 8.10 22.86 30.80 7. 27.86 42.65 14.79 51.46 56.43 4.97 46.48 47.15 0.67 55.19 59.62 4.43 33.67 46.94 13.27 31.35 43.76 12. 35.86 53.92 18.06 Table 12: Evaluation results on Multi-IF for turn 3. Models MMLU GSM8K MATH HumanEval MBPP 81.21 80.62 0.59 86.88 87.72 0. 80.44 81.12 0.68 LLaMA-3.1-8B-Instruct 71.4 w/o GRPO 71.72 w/ GRPO 0.32 Qwen2.5-7B-Instruct 76.21 w/o GRPO 75.07 w/ GRPO 1.14 Qwen2.5-14B-Instruct 90.14 w/o GRPO 91.96 w/ GRPO 1.82 DeepSeek-R1-Distill-Llama-8B 84.61 w/o GRPO w/ GRPO 85.82 1.21 DeepSeek-R1-Distill-Qwen-7B 87.56 w/o GRPO 88.86 w/ GRPO 1.30 DeepSeek-R1-Distill-Qwen-14B 91.28 w/o GRPO 92.49 w/ GRPO 1.21 62.77 64.74 1.97 67.37 69.39 2.02 81.91 83 1.09 48.94 47.74 1. 69.12 69.82 0.70 78.44 77.18 1.26 82.3 84.54 2.24 85.1 87.42 2.32 87.68 89.18 1.50 70.73 70.12 0. 81.71 84.15 2.44 83.54 81.71 1.83 77.44 84.15 6.71 77.44 82.32 4.88 87.8 90.85 3.05 69.65 67.7 1. 77.82 76.26 1.56 79.77 82.1 2.33 73.93 80.93 6.99 76.26 78.6 2.34 84.44 84.82 0.38 Table 13: Evaluation results on general domains."
        }
    ],
    "affiliations": [
        "Institute of Modern Languages and Linguistics, Fudan University",
        "Lenovo Research",
        "School of Computer Science, Fudan University",
        "Tencent"
    ]
}