{
    "paper_title": "Memory Bank Compression for Continual Adaptation of Large Language Models",
    "authors": [
        "Thomas Katraouras",
        "Dimitrios Rafailidis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC."
        },
        {
            "title": "Start",
            "content": "Thomas Katraouras tkatraouras@uth.gr University of Thessaly Volos, Greece Dimitrios Rafailidis draf@uth.gr University of Thessaly Volos, Greece 6 2 0 2 2 ] . [ 1 6 5 7 0 0 . 1 0 6 2 : r Abstract Large Language Models (LLMs) have become mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memoryaugmented approaches address this by equipping LLMs with memory bank, that is an external memory module which stores information for future use. However, these methods face critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, model that compresses the memory bank through codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark questionanswering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC. CCS Concepts Computing methodologies Natural language generation; Machine learning; Artificial intelligence; Natural language processing; Online learning settings. Keywords Large Language Models, Continual Learning, Memory Bank, Memory Compression, Question Answering, Memory-Augmented LLMs ACM Reference Format: Thomas Katraouras and Dimitrios Rafailidis. 2026. Memory Bank Compression for Continual Adaptation of Large Language Models. In The 41st ACM/SIGAPP Symposium on Applied Computing (SAC 26), March 2327, 2026, Thessaloniki, Greece. ACM, New York, NY, USA, 8 pages. https://doi.or g/10.1145/3748522. This work is licensed under Creative Commons Attribution 4.0 International License. SAC 26, Thessaloniki, Greece 2026 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-2294-3/2026/03 https://doi.org/10.1145/3748522."
        },
        {
            "title": "1 Introduction\nLarge Language Models (LLMs) [20, 33] have shown strong per-\nformance on a wide range of natural language processing tasks,\nincluding machine translation [38], summarization [35], question\nanswering [31], and advanced reasoning [44]. They are now widely\nemployed in many applications such as search engines [47] and\npersonal assistants [4]. However, a major limitation of these models\nis that they are static [10]. Once trained, their parameters reflect\nonly the data seen during training, and they cannot easily incor-\nporate new knowledge. This leads to the problem of knowledge\ncutoff, where the modelâ€™s internal knowledge becomes outdated as\nnew information appears [20, 33].",
            "content": "To address this limitation, Retrieval-Based Augmentation (RAG) strategies have been introduced [14, 40]. frozen LLM uses retriever to fetch relevant passages from an external corpus at inference time, providing the model with access to up-to-date information without retraining [14]. However, RAG methods face several challenges. They depend on nearest-neighbor search, which adds computational overhead and latency [40]. The quality of the retrieval affects the LLM performance, and errors in retrieval propagate directly to the generator [46]. Retrievers also often require domain-specific tuning and may struggle to generalize across domains [19]. Furthermore, the retrieved passages are typically concatenated with the query in the input context window, limiting the models ability to fully utilize the information and creating issues when the combined length exceeds the models capacity [14]. These issues limit the scalability of RAG for long-term adaptation in streaming environments, reflecting on the real-world scenario. To solve the problem of LLMs long-term adaptation, continual learning methods have been proposed [30, 43]. In this setting, models are updated as new data arrive. The simplest approach, full fine-tuning, also known as uniform fine-tuning [9] since all tokens are weighted equally, updates all parameters on the new data. While effective for small models, this is computationally expensive for large LLMs and is prone to catastrophic forgetting [18], where performance on previously learned knowledge degrades as the model is optimized on new information. Parameter-efficient fine-tuning (PEFT) methods [41] such as adapters [7, 22], prefix-tuning [15], and LoRA [8] address the computational cost by introducing small trainable modules while keeping most parameters frozen. These approaches reduce the training overhead, however, they still require gradient-based updates at deployment time, which is impractical in streaming scenarios. Additional strategies have been proposed to make updates selective and stable, for example by restricting updates to predefined salient spans [5], by meta-learning token importance weights [9], or by interleaving past and new examples SAC 26, March 2327, 2026, Thessaloniki, Greece T. Katraouras and D. Rafailidis through replay [2, 28]. Despite these refinements, the fundamental limitations persist, namely the need for repeated optimization, substantial computational and latency overhead, and continued vulnerability to catastrophic forgetting [30]. promising direction for continual learning of LLMs is memory augmentation [6, 21, 39]. Instead of retrieving raw text from an external corpus, information is stored directly in structured memory module, which can also be updated dynamically. At inference time, the model can draw on these stored representations to adapt its behavior and incorporate new information. This avoids repeated gradient updates and provides direct connection between the stored knowledge and models computations [6]. However, memory augmentation introduces challenges as more documents are processed, the memory bank constantly grows. This increases storage costs and slows down inference, since the model must attend to an ever-growing set of contexts [45]. Expanding memory without disrupting the models original behavior remains difficult, and as consequence, the memory augmentation strategies require retraining or fine-tuning to remain effective. More recently, to overcome this shortcoming, memory-augmented frameworks have been proposed that store learned modulation parameters for each document in an external memory bank [32]. These approaches keep the base model frozen, condition the model on the entire memory bank rather than single retrieved document, and avoid further fine-tuning during adaptation, while mitigating catastrophic forgetting. Nevertheless, in the real-world scenario where the document stream reaches hundreds of thousands or millions of entries, the memory bank grows very large and becomes difficult to manage. This highlights scalability as an open problem in memory-augmented systems, alongside the need to balance adaptation, efficiency, and stability. In this paper, we propose MBC, model that compresses the memory bank while maintaining high performance on downstream question-and-answer (QA) tasks. Specifically, we make the following contributions: We propose memory bank compression method based on codebook optimization strategy, which stores indices to this codebook instead of full document representations. In addition, we introduce an online resetting strategy to prevent codebook collapse and ensure balanced code utilization and stable training. We employ Key-Value Low-Rank Adaptation targeted only to the attention layers of the model. In doing so, we improve the proposed models ability to adapt when new data arrive without requiring full fine-tuning. We conduct experiments on benchmark QA datasets, comparing our MBC model with baseline methods. Our results demonstrate that MBC significantly reduces the memory bank size to 0.3%, when compared with the initial size of baseline strategies, and improves the QA accuracy. Furthermore, our model maintains high retention accuracy when evaluated for catastrophic forgetting during the challenging online adaptation scenario. The remainder of the paper is structured as follows: Section 2 formulates the problem of online learning in LLMs. Section 3 outlines the MBC model and Section 4 provides the experimental evaluation. Finally, Section 5 concludes the paper, summarizing key findings and discussing potential future directions."
        },
        {
            "title": "3 Proposed Model\n3.1 Memory of Aggregated Contexts\nThe proposed model has three core components: (i) an amortization\nnetwork that encodes documents, (ii) a memory bank that stores\nthe encoded information, and (iii) an aggregation network that\nsynthesizes the stored information to answer a given query.",
            "content": "Amortization Network. The amortization network is responsible for mapping each document into compact latent representation that can be efficiently stored and retrieved [32]. Formally, this network is denoted ğ‘”ğœƒğ‘ğ‘šğ‘œğ‘Ÿğ‘¡ , implemented with T5 encoder-decoder model [24]. Given document ğ‘‘ğ‘– , the amortization network produces continuous latent representation ğœ™ğ‘– := ğ‘”ğœƒamort (ğ‘‘ğ‘– ) Rğ‘‡ ğ· , where ğ‘‡ is the number of tokens in the representation and ğ· is the hidden dimension of the base model. Memory Bank. The context vectors ğœ™ğ‘– generated from the document stream are stored in an external memory bank := {ğœ™ğ‘– ğ‘‘ğ‘– ğ·ğ‘¡ğ‘’ğ‘ ğ‘¡ } [32]. This bank serves as growing knowledge base for the base LLM. Aggregation Network. When query ğ‘ğ‘– is presented at test time, the model must retrieve relevant information from the memory bank. An aggregation network, denoted â„ğœ“ , is trained to perform this function dynamically. It takes the entire memory bank and an encoded representation of the current query ğ‘”ğœƒğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ (ğ‘ğ‘– ) as input, where ğœƒğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ uses the same architecture as ğœƒğ‘ğ‘šğ‘œğ‘Ÿğ‘¡ . The network is permutation-invariant with respect to the ordering of M. Using cross-attention mechanism [11, 36], it synthesizes the stored context vectors into single, query-specific modulation ğ‘– acts as set of soft ğœ™ ğ‘– := â„ğœ“ (ğ‘”ğœƒğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ (ğ‘ğ‘– ),M ) . This modulation ğœ™ Memory Bank Compression for Continual Adaptation of LLMs SAC 26, March 2327, 2026, Thessaloniki, Greece â„“ }ğ¿ â„“ , ğ‘‰ â„“ , ğ‘‰ (ğ‘ğ‘– ) := ğ‘“ğœƒğ‘ (ğ‘ğ‘– ; {ğ¾ prompts, injected as learnable prefixes into the keyvalue matrices of each self-attention layer of the base LLM via P-tuning v2 [17]. Formally, the modulated base LLM can be expressed as ğ‘“ ğœ™ â„“ denoting the modified â„“=1), with ğ¾ ğ‘– ğœƒğ‘ ğ‘– . To effikey and value matrices in layer â„“ after prefixing with ğœ™ ciently handle large memory banks at inference time, hierarchical modulation aggregation strategy is used. In particular, the context set is first partitioned into smaller subgroups, each aggregated individually, and the resulting representations are recursively combined until single final modulation is obtained. This divide-and-conquer procedure reduces the memory complexity to (ğ‘€ğ‘‡ ), where ğ‘€ is hyperparameter and ğ‘‡ the number of tokens, ensuring scalability even as the number of stored documents increases [32]."
        },
        {
            "title": "Compression",
            "content": "A critical limitation of storing continuous context vectors {ğœ™ğ‘– } is that the memory bank can grow very large as the document stream increases. To address this, Vector Quantised-Variational AutoEncoder (VQ-VAE)style [34] quantization module is introduced to compress the memory. Instead of storing the high-dimensional continuous vector ğœ™ğ‘– , each context vector is mapped to its nearest entry in learned finite codebook ğ¸ Rğ‘ğ‘ ğ· , where ğ‘ğ‘ denotes 2. The selected the number of code vectors: ğ‘ğ‘– := argmin ğœ™ğ‘– ğ¸ ğ‘— 2 ğ‘— ğ‘– ğ‘– codebook vector is denoted by Ë†ğœ™ hard := ğ¸ğ‘ğ‘– , and only the integer ğ‘– index ğ‘ğ‘– is stored, resulting in compressed memory bank representation MVQ := {ğ‘ğ‘– }. As Ë†ğœ™ hard is discrete and non-differentiable, during the forward pass straight-through estimator (STE) [1] is used to define Ë†ğœ™ğ‘– := ğœ™ğ‘– + sg[ Ë†ğœ™ hard ğœ™ğ‘– ], where sg[] denotes the stop-gradient operator. Thus, Ë†ğœ™ğ‘– takes the value of Ë†ğœ™ hard , still allowing gradients to flow back into ğœ™ğ‘– . In subsequent notations, Ë†ğœ™ğ‘– denotes the differentiable forward-pass representation and Ë†ğœ™ hard is used only in the vector quantization loss. The effective size of the memory bank is therefore reduced to the set of indices together with the codebook ğ¸. The codebook itself is optimized during endto-end training. During inference, the stored indices are used to retrieve their corresponding quantized vectors { Ë†ğœ™ğ‘– }, which are then aggregated by â„ğœ“ together with the query representation to produce the modulation Ë†ğœ™ ğ‘– . ğ‘– ğ‘–"
        },
        {
            "title": "3.3 Online Codebook Resetting\nTo prevent underutilization and codebook collapse, where only a\nsmall subset of codes is repeatedly used, the codebook is updated\nduring training using an exponential moving average (EMA) of\ncode usage. For a mini-batch of size ğ¾, let",
            "content": "ğ‘› ğ‘— := ğ¾ ğ‘–=1 1[ğ‘ğ‘– = ğ‘—], ğ‘¢ ğ‘— := ğ›¾ ğ‘¢ ğ‘— + (1 ğ›¾) ğ‘› ğ‘— ğ‘— {1, . . . , ğ‘ğ‘ } (1) batch, {ğœ™ğ‘  ( ğ‘— ) }, to reinitialize the corresponding codebook vectors: ğ¸ ğ‘— := ğœ™ğ‘  ( ğ‘— ) ğ‘— ğ¼dead, ğ‘¢ ğ‘— := ğ‘¢ := 1 ğ‘ğ‘ ğ‘ğ‘ â„“=1 ğ‘¢â„“ (2) where ğ‘  ( ğ‘—) denotes indices sampled uniformly and randomly without replacement from the batch, and ğ‘¢ denotes the mean usage across all codes, ensuring that reinitialized entries retain nonnegligible prior usage estimate. This procedure, applied only during training (without gradients), maintains codebook diversity and prevents collapse, as we will experimentally show in Section 4.4.4."
        },
        {
            "title": "Modulation Adaptation",
            "content": "The modulation Ë†ğœ™ ğ‘– is injected into the keyvalue pairs of each selfattention layer of the base LLM. Instead of keeping the base LLM entirely frozen such as the study reported in [32], we introduce lightweight Low-Rank Adaptation (LoRA) [8] modules specifically into the key and value projections (KV-LoRA). Here, ğœƒğ‘ denotes the frozen parameters of the pretrained model ğ‘“ğœƒğ‘ . small set of trainable parameters ğœƒKV-LoRA is added, parameterizing low-rank updates to the modulated key and value matrices: ğ¾ â„“ := ğ¾ â„“ + ğ´ğ¾,â„“ ğµğ¾,â„“, ğ‘‰ â„“ := ğ‘‰ â„“ + ğ´ğ‘‰ ,â„“ ğµğ‘‰ ,â„“ (3) where ğ´ğ¾,â„“, ğ´ğ‘‰ ,â„“ Rğ· ğ‘Ÿ and ğµğ¾,â„“, ğµğ‘‰ ,â„“ Rğ‘Ÿ ğ· are low-rank factors with rank ğ‘Ÿ ğ·. In practice, the updates are scaled by factor ğ›¼/ğ‘Ÿ and regularized with dropout. KV-LoRA is applied only to the final ğ‘›ğ‘™ğ‘œğ‘Ÿğ‘ transformer layers, balancing computational efficiency with adaptation capacity. ğ‘Ÿ , ğ›¼, ğ‘›ğ‘™ğ‘œğ‘Ÿğ‘, and the dropout probability ğœŒ are hyperparameters. The adapted model thus has parameters ğœƒğ‘ := ğœƒğ‘ ğœƒKV-LoRA, which preserves pretrained knowledge allowing the attention mechanism to more effectively exploit the modulation Ë†ğœ™ ğ‘– . Formally, the modulated base LLM is expressed as: Ë†ğœ™ ğ‘“ ğ‘– ğœƒğ‘ (ğ‘ğ‘– ) := ğ‘“ ğœƒğ‘ (ğ‘ğ‘– ; {ğ¾ â„“, ğ‘‰ â„“ }ğ¿ â„“=1) (4)"
        },
        {
            "title": "3.5 End-to-End Training Objective\nThe entire architecture is trained end-to-end, including the amorti-\nzation and aggregation networks, the VQ codebook, and the KV-\nLoRA modules. The primary objective is a question-answering (QA)\nloss LQA, defined as the negative log-likelihood of predicting the\ntarget sequence ğ‘¦ğ‘– conditioned on the query ğ‘ğ‘– , the corresponding\nğ‘– . Answer generation is carried\ndocument ğ‘‘ğ‘– , and its modulation ğœ™ âˆ—\nË†ğœ™ âˆ—\n| ğ‘ğ‘–, ğ‘‘ğ‘– ). To\n(ğ‘¦ğ‘–\nğ‘–\nËœğœƒğ‘\nenforce a compact and discrete memory representation, a vector\nquantization loss LVQ is used. Given a continuous context vector\nğœ™ğ‘– âˆˆ Rğ‘‡ Ã—ğ· , its nearest codebook entry is denoted by Ë†ğœ™ hard\n:= ğ¸ğ‘ğ‘– .\nğ‘–\nThe quantization loss is defined using Ë†ğœ™ hard\nto ensure proper up-\ndates of both the codebook and encoder:\nLVQ := âˆ¥sg[ğœ™ğ‘– ] âˆ’ Ë†ğœ™ hard",
            "content": "2 + ğ›½ ğœ™ğ‘– sg[ Ë†ğœ™ hard 2 (ğ‘ğ‘– ): LQA := log ğ‘ out by the base LLM ğ‘“ Ë†ğœ™ ğ‘– ğœƒğ‘ ] 2 2 (5) ğ‘– ğ‘– ğ‘– where ğ‘¢ ğ‘— tracks smoothed usage and ğ›¾ (0, 1) is decay rate hyperparameter. Codes with usage below hyperparameter threshold ğœ– are marked as inactive: ğ¼dead := { ğ‘— ğ‘¢ ğ‘— < ğœ– }. When ğ¼dead , up to ğ¼dead distinct encoder outputs are sampled from the current where ğ›½ > 0 is the commitment cost hyperparameter. The first term updates the selected codebook entries ğ¸ğ‘ğ‘– , while the second one encourages encoder outputs ğœ™ğ‘– to remain close to their quantized assignments. The final objective is weighted combination: SAC 26, March 2327, 2026, Thessaloniki, Greece T. Katraouras and D. Rafailidis Ltotal = LQA + ğœ†VQ LVQ, where ğœ†VQ is hyperparameter that balances the influence of the quantization loss. During training, the parameters of the amortization network (ğœƒamort), input encoder (ğœƒinput), aggregation network (ğœ“ ), KV-LoRA modules (ğœƒKV-LoRA), and the codebook (ğ¸) are optimized end-to-end, while the base model parameters ğœƒğ‘ remain frozen: min ğœƒamort,ğœƒinput,ğœ“,ğœƒKV-LoRA,ğ¸ 1 ğ¾ ğ‘–=1 ğ¾ (cid:104) LQA (ğ‘ğ‘–, ğ‘‘ğ‘–, ğ‘¦ğ‘– ) + ğœ†VQ LVQ (ğœ™ğ‘–, Ë†ğœ™ hard ğ‘– (cid:105) ) (6) where is the batch size. Optimization is performed using the Adam [12] optimizer. An overview of the MBC end-to-end optimization algorithm is presented in Algorithm 1."
        },
        {
            "title": "3.6 Online Adaptation of MBC\nAfter training is completed, the online adaptation phase follows.\nThis phase requires no gradient-based updates and operates entirely\nthrough forward passes. The procedure consists of two components:",
            "content": "Memorization. For each new document ğ‘‘ğ‘– arriving in the test stream ğ· test, the amortization network ğ‘”ğœƒamort encodes ğ‘‘ğ‘– into context vector ğœ™ğ‘– , which is subsequently quantized to the nearest codebook entry in ğ¸. The resulting discrete code ğ‘ğ‘– is stored in the compressed memory bank MVQ. Inference. When query ğ‘ğ‘– is received, the model uses the stored codes {ğ‘ ğ‘— } from the memory bank and retrieves the corresponding quantized context vectors { Ë†ğœ™ ğ‘— } from the codebook ğ¸. These vectors are aggregated by â„ğœ“ together with the query representation ğ‘”ğœƒinput (ğ‘ğ‘– ) to produce query-specific modulation Ë†ğœ™ ğ‘– . This (ğ‘ğ‘– ), modulation conditions the KV-LoRA-augmented base LLM ğ‘“ ğœƒğ‘ which then generates the final answer. The online adaptation and evaluation procedure is presented in Algorithm 2."
        },
        {
            "title": "4 Experimental Evaluation\n4.1 Datasets\nFollowing [9, 32], we evaluate the examined models on three QA\ndatasets:",
            "content": "StreamingQA [16]. It contains questions created by annotators or generated with language models. Questions are based on timestamped English WMT news articles (20072020), which are also included in the dataset. Following prior setups, we use 21K training, 1.7K validation, and 5K test questions, along with the same number of documents. For QA pre-training baselines, 40K training and 4K validation questions are used. SQuAD [25]. The Stanford Question Answering Dataset (SQuAD) includes crowdsourced questions on Wikipedia, where answers are spans within the article. Following prior setups, we use 39.9K training, 5.6K validation, and 10.6K test questions, with 8.6K training, 1.2K validation, and 2.1K test documents, respectively. For QA pretraining baselines, 40K training and 2.1K validation questions are used. ArchivalQA [37]. It is built from New York Times Annotated Corpus articles [26] with questions generated using language models. Answers are text spans within the articles. Following prior Algorithm 1: MBC End-to-End Optimization Input: Amortization params ğœƒamort, input encoder params ğœƒinput, base LLM params ğœƒğ‘ , aggregation params ğœ“ , KV-LoRA params ğœƒKV-LoRA, hidden dimension ğ·, training corpus ğ· train, learning rate ğœ‚, epochs ğ‘š, batch size ğ¾ , VQ commitment cost ğ›½commit, VQ weight ğœ†VQ, codebook size ğ‘ğ‘ , reset threshold ğœ–, EMA decay rate ğ›¾ Output: ğœƒamort, ğœƒinput,ğœ“, ğœƒKV-LoRA, ğ¸ // Initialize codebook and usage EMA (cid:1) ğ‘— {1, . . . , ğ‘ğ‘ }, ğ‘¢ ğ‘— 0 ğ‘— , 1 ğ¸ ğ‘— U(cid:0) 1 ğ‘ğ‘ 2 for epoch = 1 ğ‘š do 3 1 ğ‘ğ‘ Sample documents {ğ‘‘1, . . . , ğ‘‘ğ¾ } ğ· train Sample QA pairs (ğ‘ğ‘–, ğ‘¦ğ‘– ) ğ‘ (ğ‘, ğ‘¦ ğ‘‘ğ‘– ) for ğ‘– = 1, . . . , ğ¾ for ğ‘– = 1 ğ¾ do ğœ™ğ‘– ğ‘”ğœƒamort (ğ‘‘ğ‘– ) ; // Vector quantization (nearest code) Ë†ğœ™ hard ğ‘ğ‘– arg minğ‘— {1,...,ğ‘ğ‘ } ğœ™ğ‘– ğ¸ ğ‘— 2 ğ‘– ğ¸ğ‘ğ‘– 2 , Ë†ğœ™ğ‘– ğœ™ğ‘– + sg[ Ë†ğœ™ hard // Straight-through estimator // Encode document ğœ™ğ‘– ] ; ğ‘– // Update code usage EMA and reset dead codes ğ‘› ğ‘— (cid:205)ğ¾ ğ¼dead { ğ‘— {1, . . . , ğ‘ğ‘ } ğ‘¢ ğ‘— < ğœ– } if ğ¼dead > 0 then ğ‘–=1 1[ğ‘ğ‘– = ğ‘— ] ğ‘— , ğ‘¢ ğ‘— ğ›¾ ğ‘¢ ğ‘— + (1 ğ›¾ ) ğ‘› ğ‘— ğ‘— , (cid:1)(cid:17) (cid:205)ğ‘ğ‘ (cid:16)(cid:0){1,...,ğ¾ } ğ‘† // Replace dead codes with random batch samples ğ‘† ğ‘ˆ ğ‘›ğ‘– ğ‘“ ğ¸ ğ‘— ğœ™ğ‘  ( ğ‘— ) ğ‘— ğ¼dead, up to ğ‘† ğ‘¢ ğ‘— 1 ğ‘ğ‘ ğ‘† = min( ğ¼dead , ğ¾ ) â„“ =1 ğ‘¢â„“ ğ‘— ğ¼dead ; // Aggregate quantized contexts with the query Ë†ğœ™ ğ‘– â„ğœ“(cid:0)ğ‘”ğœƒinput (ğ‘ğ‘– ), { Ë†ğœ™ğ‘– }ğ¾ ğ‘–=1 // QA loss via modulated base LLM LQA 1 ğ¾ (cid:205)ğ¾ ğ‘–=1 CrossEntropy(cid:0)ğ‘“ (ğ‘ğ‘– ), ğ‘¦ğ‘– (cid:1) (cid:1) Ë†ğœ™ ğ‘– ğœƒğ‘ // Reset usage ğ‘– ğ‘– 2 2 // VQ loss (codebook and commitment terms) (cid:13)sg[ğœ™ğ‘– ] Ë†ğœ™ hard (cid:13) (cid:13) (cid:205)ğ¾ Lcodebook 1 (cid:13) ğ‘–=1 ğ¾ 2 (cid:13)ğœ™ğ‘– sg[ Ë†ğœ™ hard ](cid:13) (cid:13) (cid:205)ğ¾ Lcommit 1 (cid:13) ğ‘–=1 ğ¾ 2 LVQ Lcodebook + ğ›½commit Lcommit Ltotal LQA + ğœ†VQ LVQ ; // Gradient updates (base ğœƒğ‘ frozen) ğœƒamort ğœƒamort ğœ‚ğœƒamort Ltotal ğœƒinput ğœƒinput ğœ‚ğœƒinput Ltotal ğœ“ ğœ“ ğœ‚ğœ“ Ltotal ğœƒKV-LoRA ğœƒKV-LoRA ğœ‚ğœƒKV-LoRA Ltotal ğ¸ ğ¸ ğœ‚ğ¸ Ltotal // Total objective 4 5 7 8 9 10 11 13 14 15 16 17 19 20 21 22 23 25 Algorithm 2: Online Adaptation of MBC Input: Test document stream ğ· test, test QA set { (ğ‘ğ‘–, ğ‘¦ğ‘– ) }ğ¼ ğ‘–=1, amortization params ğœƒamort, input encoder params ğœƒinput, base LLM params with KV-LoRA ğœƒğ‘ , aggregation params ğœ“ , learned codebook ğ¸ ğ‘–=1 Output: EM and F1 over { (ğ‘ğ‘–, ğ‘¦ğ‘– ) }ğ¼ // Initialize compressed memory bank 1 MVQ ; 2 for ğ‘‘ğ‘˜ ğ· test 4 do ğœ™ğ‘˜ ğ‘”ğœƒamort (ğ‘‘ğ‘˜ ) ; ğ‘ğ‘˜ arg minğ‘— ğœ™ğ‘˜ ğ¸ ğ‘— 2 2 ; MVQ MVQ {ğ‘ğ‘˜ } ; 5 6 for ğ‘– = 1 ğ¼ do Ë†ğœ™ ğ‘– â„ğœ“(cid:0)ğ‘”ğœƒinput (ğ‘ğ‘– ), {ğ¸ğ‘ ğ‘— }ğ‘ ğ‘— MVQ 7 query 8 Ë†ğ‘¦ğ‘– ğ‘“ Ë†ğœ™ ğ‘– ğœƒğ‘ (ğ‘ğ‘– ) ; // Encode document // Quantize // Save document to bank (cid:1) ; // Aggregate memory with // Predict answer // Final evaluation; norm(): lowercase, strip punctuation, remove articles, collapse whitespace ğ‘–=1 1(cid:2)norm(ğ‘¦ğ‘– ) = norm( Ë†ğ‘¦ğ‘– )(cid:3) ; (cid:205)ğ¼ (cid:205)ğ¼ ğ‘–=1 F1token (ğ‘¦ğ‘–, Ë†ğ‘¦ğ‘– ) ; 9 EM 1 ğ¼ 10 F1 1 ğ¼ 11 return (EM, F1) // Exact Match // Token-Level F1 Memory Bank Compression for Continual Adaptation of LLMs SAC 26, March 2327, 2026, Thessaloniki, Greece setups, we use 21.7K training, 5.3K validation, and 8.7K test questions, with 12.8K training, 3.0K validation, and 5.0K test documents, respectively. For QA pre-training baselines, 12.4K training and 3K validation questions are used."
        },
        {
            "title": "4.2 Evaluation Protocol\nWe follow the training configuration of prior works [9, 32] for fair\ncomparison across baselines. For each dataset, the model is adapted\nusing 1,665 documents sampled from the test stream ğ· test, after\nwhich its performance is evaluated on QA pairs drawn from the\nsame documents. We report Exact Match (EM) and token-level F1\nscores as evaluation metrics.",
            "content": "The EM score measures the fraction of predictions that exactly match the ground-truth answer after normalization (lowercasing, punctuation and article removal, collapsing multiple spaces into one): EM = 1 ğ¼ ğ¼ ğ‘–=1 1(cid:2)norm( Ë†ğ‘¦ğ‘– ) = norm(ğ‘¦ğ‘– )(cid:3) (7) where ğ¼ is the number of QA pairs, Ë†ğ‘¦ğ‘– is the predicted answer, and ğ‘¦ğ‘– is the ground truth. The token-level F1 score measures the harmonic mean of precision and recall at the token level: tok( Ë†ğ‘¦ğ‘– ) tok(ğ‘¦ğ‘– ) tok( Ë†ğ‘¦ğ‘– ) Precisionğ‘– = , F1ğ‘– = 2 Precisionğ‘– Recallğ‘– Precisionğ‘– + Recallğ‘– , Recallğ‘– = tok( Ë†ğ‘¦ğ‘– ) tok(ğ‘¦ğ‘– ) tok(ğ‘¦ğ‘– ) (8) F1 = 1 ğ¼ ğ¼ ğ‘–=1 F1ğ‘– (9) where tok() denotes the tokenized representation of the answer."
        },
        {
            "title": "4.3 Experimental Setup\nImplementation Details. We evaluate MBC using four back-\n4.3.1\nbone LLMs: the GPT-2 family (DistilGPT2 [27], GPT2-Large [23],\nGPT2-XL [23]) and LLaMA-2-7B [33], with 82M, 774M, 1.5B, and 7B\nparameters, respectively. The amortization network ğ‘”ğœƒamort is based\non T5 [24], using T5-Small for DistilGPT2, T5-Base for GPT2-Large,\nand T5-Large for GPT2-XL and LLaMA-2-7B. The input encoder\nğ‘”ğœƒinput uses T5-Small for DistilGPT2 and T5-Base for the rest. The\namortization network outputs ğ‘‡ = 12 tokens for DistilGPT2 and\n24 for the rest. The aggregation network â„ğœ“ consists of four cross-\nattention blocks [11, 36], where ğ‘”ğœƒinput (ğ‘ğ‘– ) provides the initial query,\nthe memory bank MVQ provides keys and values, and subsequent\nblocks take the previous output as input, producing Ë†ğœ™ âˆ—\nğ‘– . Training\nruns for 50 epochs with the Adam [12] optimizer. The learning\nrate is linearly warmed up for the first 1% of total steps and then\nkept constant at 10âˆ’5. Validation is performed after each epoch.\nWe use a batch size of 64 for DistilGPT2 and 32 for the rest, with\ngradient accumulation. For models above 1B parameters, dropout\nwith probability ğœŒback = 0.75 is applied during backpropagation,\nthis means that gradients are computed only for a random sub-\nset of documents per batch, while the rest use stop-gradient [32].\nLLaMA-2-7B is trained with 4-bit quantization [3] for both the\nmodel and the amortization network. The codebook size is fixed\nto ğ‘ğ‘ = 512, with VQ commitment cost ğ›½commit = 0.25 and weight\nğœ†VQ = 1.0. Codebook resetting uses EMA decay rate ğ›¾ = 0.99 and\nreset threshold ğœ– = 10âˆ’4. For KV-LoRA, DistilGPT2 uses ğ‘Ÿ = 16,",
            "content": "ğ›¼ = 32, ğœŒ = 0.05, applied to the last ğ‘›lora = 6 layers. GPT2-Large, GPT2-XL and LLaMA-2-7B use ğ‘Ÿ = 32, ğ›¼ = 64, ğœŒ = 0.05, applied to the last ğ‘›lora = 16 layers. For the GPT-2 family, we share the LoRA down-projection matrix across ğ¾ and ğ‘‰ , this means ğ´ğ¾,â„“ = ğ´ğ‘‰ ,â„“ . All experiments are conducted on single NVIDIA A100 80GB GPU."
        },
        {
            "title": "4.3.2 Examined Models.",
            "content": "Uniform Fine-Tuning1: baseline approach where all tokens in the new documents are treated equally during model updates. Salient Spans1 [5]: heuristic-based method that fine-tunes only on tokens within pre-identified salient spans, ignoring the rest. CaMeLS1 [9]: Context-aware Meta-learned Loss Scaling, which uses meta-trained network to assign importance weights to tokens during fine-tuning, focusing learning on the most informative content. MAC2 [32]: Memory of Amortized Contexts, an online adaptation framework that freezes the base model and uses meta-learned network to encode documents into compact modulations stored in memory bank. An aggregation module retrieves and combines the modulations with the query, without requiring gradient updates during inference. MBC3: The proposed model. For fair comparison, we retrained all baselines. For Uniform FineTuning, Salient Spans and CaMeLS, we followed the configuration of [9]. Each pretrained LLM is first fine-tuned on QA pairs to obtain task-adapted base model. During this pretraining, an inner batch of 6 documentquerylabel triples is used, and outer-loop gradients are accumulated over 24 examples, split into 4 batches of 6. Subsequently, the base model undergoes online adaptation, where it is updated on stream of documents. The learning rate for each basestrategy combination is selected via hyperparameter sweep on the validation set over {104, 2.5105, 6.25106, 1.625106}. The best learning rate for Uniform and Salient Spans is mostly 1.625 106, while for CaMeLS 2.5 105. Adam is used in most cases, and Adafactor [29] is used for large models. For MAC, we followed the configuration of [32]. Specifically, the amortization network uses T5-Small (12 output tokens) for DistilGPT2, T5-Base (24 output tokens) for GPT2-Large, and T5-Large (24 output tokens) for GPT2-XL and LLaMA-2-7B, while the input encoder uses T5-Small for DistilGPT2 and T5-Base for the rest. The aggregation network consists of four cross-attention blocks. Training is performed for 50 epochs with Adam and constant learning rate of 105 after oneepoch warm-up, using batch size of 64 for DistilGPT2 and 32 for the rest with gradient accumulation. For backbones exceeding 1B parameters, backpropagation dropout with probability ğœŒback = 0.75 is applied, and LLaMA-2-7B is trained with 4-bit quantization [3]."
        },
        {
            "title": "4.4 Experimental Results\n4.4.1 QA Performance Evaluation. Table 1 reports the QA perfor-\nmance. We compare MBC against the baseline methodologies, with\nMAC being the most competitive. Across all datasets and base\nLLMs, MBC consistently improves both EM and F1. On average,",
            "content": "1https://github.com/nathanhu0/CaMeLS 2https://github.com/jihoontack/MAC 3https://github.com/Thomkat/MBC SAC 26, March 2327, 2026, Thessaloniki, Greece T. Katraouras and D. Rafailidis Table 1: Exact Match (EM) and F1 scores on StreamingQA, SQuAD, and ArchivalQA across different backbone models and baselines. High values indicate high QA performance. Model (# params) DistilGPT2 (82M) GPT2-Large (774M) GPT2-XL (1.5B) LLaMA-2 (7B)"
        },
        {
            "title": "ArchivalQA",
            "content": "Uniform Salient Spans CaMeLS MAC MBC (Ours) Uniform Salient Spans CaMeLS MAC MBC (Ours) Uniform Salient Spans CaMeLS MAC MBC (Ours) Uniform Salient Spans CaMeLS* MAC MBC (Ours) EM () F1 () EM () F1 () EM () F1 () 1.62 1.62 1.86 3.48 3.96 (13.8%) 4.14 4.26 5.48 6.12 7.43 (21.4%) 5.16 5.46 6.98 7.14 7.49 (4.9%) 11.76 12.12 N/A 14.01 16.04 (14.5%) 2.97 4.33 4.38 8.11 8.76 (8%) 8.08 8.53 10.31 11.44 12.77 (11.6%) 9.14 11.32 11.23 12.01 12.77 (6.3%) 12.53 18.65 N/A 20.44 25.33 (23.9%) 1.34 1.31 1.43 1.90 2.10 (10.5%) 3.37 4.38 4.45 6.14 6.99 (13.8%) 5.87 5.66 6.17 6.89 7.40 (7.4%) 12.78 13.32 N/A 13.33 14.93 (12%) 2.78 2.50 3.06 5.00 5.36 (7.2%) 5.62 6.79 7.60 9.75 10.88 (11.6%) 7.87 8.69 9.93 10.12 11.96 (18.2%) 16.62 18.09 N/A 18.17 22.15 (21.9%) 4.01 4.08 4.11 5.99 6.61 (10.4%) 8.03 9.75 9.28 10.95 12.03 (9.9%) 9.89 10.44 11.48 11.48 12.34 (7.5%) 17.89 18.45 N/A 19.58 22.71 (16%) 3.69 3.98 5.99 8.87 9.27 (4.5%) 6.63 7.23 9.18 12.15 13.68 (12.6%) 10.46 13.68 14.01 15.52 15.93 (2.6%) 20.01 22.21 N/A 23.89 28.66 (19.9%) * CaMeLS results are not reported for LLaMA-2 (7B) because the model exceeds the memory capacity of single NVIDIA A100 80GB GPU. Even with batch size of 1, it was infeasible to replicate this baseline under our hardware constraints. Figure 1: Memory bank footprint (logMB) of MAC and MBC across StreamingQA, SQuAD, and ArchivalQA. Table 2: Trainable parameters of MAC and MBC (offline)."
        },
        {
            "title": "Method",
            "content": "DistilGPT2 GPT2-Large GPT2-XL LLaMA-2-7B"
        },
        {
            "title": "MAC\nMBC",
            "content": "197M 197.6M (+0.31%) 927M 929.6M (+0.28%) 1.72B 1.723B (+0.19%) 2.36B 2.371B (+0.45%) MBC gains 11.84% in EM and 12.99% in F1 compared to MAC. The performance gains result from two main design choices. Firstly, the introduction of KV-LoRA allows the attention mechanism to make effective use of the modulation Ë†ğœ™ ğ‘– , leading to accurate answers. In addition, an efficiently learned codebook preserves the quality of the stored documents, ensuring that the compression mechanism does not degrade the performance."
        },
        {
            "title": "4.4.2 Memory Bank Size. Figure 1 compares the memory bank\nfootprint of two examined memory-augmented methods, that is\nMBC and MAC. For MBC, the footprint includes the codebook and\nstored indices in the bank, while for MAC it corresponds to the full\nmemory bank. Across all three datasets, MBC achieves substantial\nmemory savings. For DistilGPT2, the memory bank size is reduced\nby an average of 98.27% compared to MAC. For GPT2-Large and\nGPT2-XL, the reduction averages 99.1%, and for LLaMA-2-7B it\naverages 99.2%. These results show that memory compression is\nconsistently effective across all the different model scales.",
            "content": "To examine the overhead of the codebook and KV-LoRA in MBC, we compare the trainable parameters of the two examined memory augmentation methods, namely MAC and MBC. These numbers are reported in the offline setting, this means without considering Memory Bank Compression for Continual Adaptation of LLMs SAC 26, March 2327, 2026, Thessaloniki, Greece Table 3: Memory bank size (MB) / F1 retention rate (%) on StreamingQA, SQuAD, and ArchivalQA for different base LLMs. Each entry is reported as memory bank footprint in MB followed by the corresponding retention rate. # of Doc"
        },
        {
            "title": "StreamingQA",
            "content": "DistilGPT2 200 400 600 800 1000 1200 1400 1600 MAC 8.21/100 16.42/99.5 24.63/99 32.84/99 41.04/98 49.25/98 57.46/98 65.67/98 MBC 1.52/100 1.54/99 1.56/99 1.59/99 1.61/98 1.63/98 1.65/98.5 1.67/98 MAC 38.1/100 76.19/99.5 114.29/99 152.39/99 190.48/99 228.58/98.5 266.67/98 304.77/98 GPT2-XL MBC 1.60/100 1.70/99 1.80/99 1.90/98 1.99/97.5 2.09/98 2.19/98.5 2.29/98.5 MAC 10.91/100 21.82/99.5 32.72/99.5 43.63/99 54.54/99 65.45/98.5 76.35/98.5 87.26/98 MBC 1.53/100 1.56/99 1.59/99.5 1.62/99 1.64/98.5 1.67/98 1.70/98.5 1.73/98 MAC 27.36/100 54.73/99 82.09/99 109.46/98 136.82/97.5 164.18/97 191.55/96.5 218.91/97 MBC 2.54/100 2.59/99 2.63/99 2.67/99 2.71/98.5 2.76/98.5 2.8/97 2.84/97 # of Doc"
        },
        {
            "title": "StreamingQA",
            "content": "GPT2-Large SQuAD MAC 126.99/100 253.98/99 380.96/98.5 507.95/98 634.94/97.5 761.93/97 888.91/96.5 1015.9/96 MBC 2.7/100 2.9/99.5 3.1/99.5 3.3/99 3.49/98.5 3.69/98 3.89/96.5 4.09/"
        },
        {
            "title": "ArchivalQA",
            "content": "MAC 36.36/100 72.72/99 109.07/99 145.43/98.8 181.79/97.5 218.15/97.5 254.5/96.5 290.86/97 MBC 2.56/100 2.61/99 2.67/99 2.73/99 2.78/98.8 2.84/98.3 2.89/97.6 2.95/97.2 Llama2-7B SQuAD"
        },
        {
            "title": "ArchivalQA",
            "content": "200 400 600 800 1000 1200 1400 1600 MAC 34.2/100 68.41/100 102.61/99 136.82/98 171.02/97 205.22/96.7 239.43/96 273.63/95.5 MBC 3.16/100 3.21/100 3.25/98.5 3.29/99 3.33/98.5 3.38/98 3.42/97 3.46/96 MAC 158.73/100 317.47/99.5 476.2/98.8 634.94/98.0 793.67/98.0 952.4/97.9 1111.14/98.0 1269.87/97.5 MBC 3.32/100 3.52/99 3.72/99 3.92/98.7 4.11/98.8 4.31/98 4.51/98 4.71/97.5 MAC 45.45/100 90.89/100 136.34/99.7 181.79/99.4 227.23/98.5 272.68/98 318.12/98.3 363.57/ MBC 3.18/100 3.23/99 3.29/99 3.35/98.6 3.4/98.1 3.46/97.5 3.51/97.8 3.58/97.5 MAC 87.56/100 175.13/100 262.69/99.5 350.25/98 437.81/97.4 525.38/97 612.94/96.5 700.5/95 MBC 8.04/100 8.09/99 8.13/99 8.17/98.5 8.21/98 8.26/97 8.3/96.5 8.34/96 MAC 406.36/100 812.72/99.7 1219.08/99.5 1625.44/99.5 2031.8/99 2438.16/97 2844.52/96.7 3250.88/96.5 MBC 8.2/100 8.4/99.5 8.6/99 8.8/98.5 8.99/97.5 9.19/96.9 9.39/96.3 9.59/96.5 MAC 116.34/100 232.69/99 349.03/99.8 465.38/99.5 581.72/98.5 698.06/98.2 814.41/98 930.75/97. MBC 8.06/100 8.11/99.5 8.17/99 8.23/98.5 8.28/98.2 8.34/98 8.39/98 8.45/97.5 documents stored during online adaptation. As Table 2 shows, the additional parameters introduced by the codebook and KV-LoRA of the proposed MBC model account for less than 0.5% across all base LLMs. This overhead is negligible compared to the improvements in the QA accuracy and memory compression."
        },
        {
            "title": "4.4.4 Effectiveness of the Codebook Resetting Mechanism. We fur-\nther evaluate the role of the EMA-based codebook resetting mech-\nanism introduced in Section 3.3 by comparing training runs with",
            "content": "Figure 2: Perplexity over train epochs on StreamingQA with and without codebook resetting in MBC, across all base LLMs. and without resetting in MBC. Code usage is measured via perplexity, defined as PPL = exp(cid:0) (cid:205)ğ‘˜ ğ‘ğ‘˜ log ğ‘ğ‘˜ (cid:1), where ğ‘ğ‘˜ = ğ‘¢ğ‘˜ /(cid:205)ğ‘— ğ‘¢ ğ‘— and ğ‘¢ ğ‘— is the EMA-smoothed usage defined in Eq. 1. Lower values that remain flat indicate codebook collapse, this means that only small subset of codes being repeatedly used. Figure 2 shows the perplexity curves on StreamingQA across the four base LLMs. With resetting, effective code usage remains stable and diverse throughout training. For DistilGPT2, perplexity is between 57 and 65 during the first 10 epochs, while without resetting it collapses close to 12. For GPT2-Large, resetting maintains perplexity between 6166, whereas without resetting it quickly drops to 24. For GPT2XL, resetting maintains perplexity steadily above 90, whereas it collapses to 14 without resetting. Similarly, for LLaMA-2-7B, resetting maintains perplexity above 100, while without it the codebook again collapses to 24. These results confirm that the codebook resetting mechanism is important for preventing collapse and ensuring SAC 26, March 2327, 2026, Thessaloniki, Greece T. Katraouras and D. Rafailidis balanced code usage, which supports stable training and effective adaptation for the proposed MBC method."
        },
        {
            "title": "5 Conclusion\nIn this work, we addressed the scalability challenges of memory-\naugmented LLMs, where the memory bank grows constantly as new\ndocuments are processed. We proposed MBC, a model that com-\npresses the memory bank, enabling efficient continual adaptation\nof LLMs in streaming settings. By combining codebook-based com-\npression with an online resetting mechanism, MBC prevents code-\nbook collapse and ensures balanced code utilization. At the same\ntime, lightweight KV-LoRA modules provide targeted adaptation\nwithin the attention mechanism, allowing the model to efficiently\nexploit the query-memory modulations without full fine-tuning.\nThis design enables MBC to achieve scalability in terms of memory\nefficiency while improving the QA accuracy. Experiments with QA\ndatasets demonstrate that MBC improves EM and F1 score while re-\nducing the memory bank footprint to 0.3% of the most competitive\nbaseline. MBC also maintains high F1 retention during online adap-\ntation, thus reducing catastrophic forgetting. An interesting future\ndirection is to extend MBC by incorporating reinforcement signals\nto guide memory usage adaptively [13] or by exploring distributed\nmemory banks that enable federated continual learning [42].",
            "content": "References [1] Y. Bengio, N. LÃ©onard, and A. Courville. 2013. Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. (2013). arXiv: 1308.3432. A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. S. Torr, and MA. Ranzato. 2019. On Tiny Episodic Memories in Continual Learning. (2019). arXiv: 1902.10486. T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. 2023. Qlora: efficient finetuning of quantized llms. NeurIPS, 36, 1008810115. T. Gunter et al. 2024. Apple Intelligence Foundation Language Models. (2024). arXiv: 2407.21075. K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. 2020. Retrieval Augmented Language Model Pre-Training. In ICML. PMLR, 39293938. Z. He, L. Karlinsky, D. Kim, J. McAuley, D. Krotov, and R. Feris. 2024. CAMELoT: Towards Large Language Models with Training-Free Consolidated Associative Memory. (2024). arXiv: 2402.13449. N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. D. Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. In ICML. PMLR, 27902799. E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. (2021). arXiv: 2106.09685. N. Hu, E. Mitchell, C. D. Manning, and C. Finn. 2023. Meta-Learning Online Adaptation of Language Models. (2023). arXiv: 2305.15076. J. Jang, S. Ye, S. Yang, J. Shin, J. Han, G. Kim, S. J. Choi, and M. Seo. 2022. Towards Continual Knowledge Learning of Language Models. (2022). arXiv: 2110.03215. [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] H. Kim, A. Mnih, J. Schwarz, M. Garnelo, A. Eslami, D. Rosenbaum, O. Vinyals, and Y. W. Teh. 2019. Attentive Neural Processes. (2019). arXiv: 1901.05761. [12] D. P. Kingma and J. Ba. 2017. Adam: Method for Stochastic Optimization. (2017). arXiv: 1412.6980. [15] [14] [13] M. Kulkarni, P. Tangarajan, K. Kim, and A. Trivedi. 2024. Reinforcement Learning for Optimizing RAG for Domain Chatbots. (2024). arXiv: 2401.06800. P. Lewis et al. 2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In NeurIPS. Vol. 33. Curran Associates, Inc., 94599474. X. L. Li and P. Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. (2021). arXiv: 2101.00190. A. Liska et al. 2022. StreamingQA: Benchmark for Adaptation to New Knowledge over Time in Question Answering Models. In ICML. PMLR, 1360413622. X. Liu, K. Ji, Y. Fu, W. L. Tam, Z. Du, Z. Yang, and J. Tang. 2022. P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks. (2022). arXiv: 2110.07602. [17] [16] [24] [23] [22] [19] [25] [20] [21] [18] M. McCloskey and N. J. Cohen. 1989. Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem. In Psychology of Learning and Motivation. Vol. 24. Elsevier, 109165. isbn: 978-0-12-543324-2. A. Misrahi, N. Chirkova, M. Louis, and V. Nikoulina. 2025. Adapting Large Language Models for Multi-Domain Retrieval-Augmented-Generation. (2025). arXiv: 2504.02411. OpenAI et al. 2024. GPT-4 Technical Report. (2024). arXiv: 2303.08774. S. Park and J. Bak. 2024. Memoria: Resolving Fateful Forgetting Problem through Human-Inspired Memory Architecture. (2024). arXiv: 2310.03052. J. Pfeiffer, A. Kamath, A. RÃ¼cklÃ©, K. Cho, and I. Gurevych. 2021. AdapterFusion: Non-Destructive Task Composition for Transfer Learning. (2021). arXiv: 2005 .00247. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. [n. d.] Language Models are Unsupervised Multitask Learners. C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 21, 140, 167. P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. (2016). arXiv: 1606.05250. E. Sandhaus. 2008. The new york times annotated corpus. Linguistic Data Consortium, Philadelphia, 6, 12, e26752. V. Sanh, L. Debut, J. Chaumond, and T. Wolf. 2019. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. In NeurIPS EMC J. Schwarz, W. Czarnecki, J. Luketina, A. Grabska-Barwinska, Y. W. Teh, R. Pascanu, and R. Hadsell. 2018. Progress & Compress: scalable framework for continual learning. In ICML. PMLR, 45284537. N. Shazeer and M. Stern. 2018. Adafactor: Adaptive Learning Rates with Sublinear Memory Cost. In ICML. PMLR, 45964604. Workshop. [30] H. Shi, Z. Xu, H. Wang, W. Qin, W. Wang, Y. Wang, Z. Wang, S. Ebrahimi, and H. Wang. 2025. Continual Learning of Large Language Models: Comprehensive Survey. ACM Computing Surveys, 3735633. K. Singhal et al. 2025. Toward expert-level medical question answering with large language models. Nature Medicine, 31, 3, 943950. J. Tack, J. Kim, E. Mitchell, J. Shin, Y. W. Teh, and J. R. Schwarz. 2024. Online adaptation of language models with memory of amortized contexts. NeurIPS, 37, 130109130135. [26] [27] [28] [29] [32] [31] [33] H. Touvron et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models. [34] [35] [36] [37] [38] [39] [40] [41] [42] [43] [44] [45] [46] [47] (2023). arXiv: 2307.09288. A. van den Oord, O. Vinyals, and k. kavukcuoglu. 2017. Neural Discrete Representation Learning. In NeurIPS. Vol. 30. Curran Associates, Inc. D. Van Veen et al. 2024. Adapted large language models can outperform medical experts in clinical text summarization. Nature Medicine, 30, 4, 11341142. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Å. Kaiser, and I. Polosukhin. 2017. Attention is All you Need. In NeurIPS. Vol. 30. Curran Associates, Inc. J. Wang, A. Jatowt, and M. Yoshikawa. 2022. ArchivalQA: Large-scale Benchmark Dataset for Open-Domain Question Answering over Historical News Collections. In SIGIR. ACM, Madrid Spain, 30253035. isbn: 978-1-4503-8732-3. Y. Wang, J. Zhang, T. Shi, D. Deng, Y. Tian, and T. Matsumoto. 2024. Recent Advances in Interactive Machine Translation With Large Language Models. IEEE Access, 12, 179353179382. Y. Wang et al. 2024. MEMORYLLM: Towards Self-Updatable Large Language Models. (2024). arXiv: 2402.04624. S. Wu et al. 2024. Retrieval-Augmented Generation for Natural Language Processing: Survey. (2024). arXiv: 2407.13193. L. Xu, H. Xie, S-Z. J. Qin, X. Tao, and F. L. Wang. 2023. Parameter-Efficient Fine-Tuning Methods for Pretrained Language Models: Critical Review and Assessment. (2023). arXiv: 2312.12148. X. Yang, H. Yu, X. Gao, H. Wang, J. Zhang, and T. Li. 2024. Federated continual learning via knowledge fusion: survey. IEEE Transactions on Knowledge and Data Engineering, 36, 8, 38323850. Y. Yang, J. Zhou, X. Ding, T. Huai, S. Liu, Q. Chen, Y. Xie, and L. He. 2025. Recent Advances of Foundation Language Models-based Continual Learning: Survey. ACM Computing Surveys, 57, 5, 138. S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In NeurIPS. Vol. 36. Curran Associates, Inc., 1180911822. Z. Zhang, Q. Dai, X. Bo, C. Ma, R. Li, X. Chen, J. Zhu, Z. Dong, and J-R. Wen. 2025. Survey on the Memory Mechanism of Large Language Model based Agents. ACM Transactions on Information Systems, 3748302. S. Zhao, Y. Shao, Y. Huang, J. Song, Z. Wang, C. Wan, and L. Ma. 2025. Understanding the Design Decisions of Retrieval-Augmented Generation Systems. (2025). arXiv: 2411.19463. Y. Zhu et al. 2024. Large Language Models for Information Retrieval: Survey. (2024). arXiv: 2308.07107."
        }
    ],
    "affiliations": [
        "University of Thessaly"
    ]
}