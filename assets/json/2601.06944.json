{
    "paper_title": "SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models",
    "authors": [
        "Yuhang Su",
        "Mei Wang",
        "Yaoyao Zhong",
        "Guozhang Li",
        "Shixing Li",
        "Yihan Feng",
        "Hua Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve a problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, a novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmark's effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/yuhangsu82/SketchJudge."
        },
        {
            "title": "Start",
            "content": "SketchJudge: Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models Yuhang Su, Mei Wang, Yaoyao Zhong, Guozhang Li, Shixing Li, Yihan Feng, Hua Huang School of Artificial Intelligence, Beijing Normal University, Beijing, China yuhangsu@mail.bnu.edu.cn 6 2 0 2 1 1 ] . [ 1 4 4 9 6 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While Multimodal Large Language Models (MLLMs) have achieved remarkable progress in visual understanding, they often struggle when faced with the unstructured and ambiguous nature of human-generated sketches. This limitation is particularly pronounced in the underexplored task of visual grading, where models should not only solve problem but also diagnose errors in hand-drawn diagrams. Such diagnostic capabilities depend on complex structural, semantic, and metacognitive reasoning. To bridge this gap, we introduce SketchJudge, novel benchmark tailored for evaluating MLLMs as graders of hand-drawn STEM diagrams. SketchJudge encompasses 1,015 hand-drawn student responses across four domains: geometry, physics, charts, and flowcharts, featuring diverse stylistic variations and distinct error types. Evaluations on SketchJudge demonstrate that even advanced MLLMs lag significantly behind humans, validating the benchmarks effectiveness in exposing the fragility of current vision-language alignment in symbolic and noisy contexts. All data, code, and evaluation scripts are publicly available at https://github.com/ yuhangsu82/SketchJudge."
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) have made significant progress in tasks such as visual question answering, chart interpretation, and document understanding. However, as applications broaden, several fundamental and underexplored challenges have begun to surface. The prevailing paradigm operates on perfect world assumption, favoring clean and standardized images. However, hand-drawn sketches such as geometric constructions, free-body diagrams, and flowcharts shatter this idealized premise with their inherent ambiguity and stylistic diversity, yet remain indispensable to expressing complex reasoning in STEM education. Moreover, existing 1 Figure 1: Distribution of annotated error types across four task domains in SketchJudge. benchmarks almost exclusively cast models in the role of solver, while this design overlooks another critical role of grader. In settings like education, the core value is not to replace human thinking but to augment it, for instance, by acting as grader that accurately identifies conceptual errors in students solution. Uniting this complex role with such unstructured input data defines new frontier for MLLMs. The ability to evaluate diagrams is especially crucial in educational and assessment contexts. Acting as an effective grader for hand-drawn diagrams requires multi-layered reasoning process that mirrors human cognition. At the perceptual level, this includes parsing sparse and ambiguous visual information despite variations in individual drawing styles. At the structural level, it involves understanding spatial relationships and logical connections. At the semantic level, it depends on mastery of domain-specific symbols and concepts. Finally, at the metacognitive level, it calls for assessing the overall plausibility and completeness of the solution. However, the lack of suitable benchmarks impedes research in this domain. To address this gap, we introduce SketchJudge, diagnostic benchmark designed to evaluate the ability of MLLMs to grade hand-drawn diagrams and diagnose their limitations. The benchmark encompasses four domainsgeometry, physics, charts, and flowchartscomprising total of 300 natural language questions and 1,015 hand-drawn student answers. The number of questions varies by domain, reflecting their different levels of complexity and annotation density. Each students answer is paired with binary correctness labels and fine-grained error categories, with 57 domainspecific error types defined for each domain and validated by subject experts. Figure 1 provides an overview of the distribution of error types across task domains in SketchJudge. Compared to existing benchmarks like MathVista (Lu et al., 2023), ChartQA (Masry et al., 2022), and DocVQA (Mathew et al., 2021), SketchJudge focuses on messy, symbolic, and structure-rich sketches that challenge shallow pattern-matching techniques. Our main contributions are: We introduce the first diagnostic benchmark, to our knowledge, for grading hand-drawn diagrammatic answers in educational contexts, spanning four task types. We propose fine-grained error taxonomy that enables structured diagnosis of model errors beyond simple correctness judgments. We present systematic experiments with several state-of-the-art MLLMs, revealing persistent limitations in diagram-level reasoning and offering insights for future research."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Benchmarks for MLLMs. Evaluation of MLLMs has evolved from early QA-style benchmarks to broader frameworks that stress-test reasoning, planning, and text-rich understanding (Liu et al., 2024; Yue et al., 2024; Li et al., 2024b). More recent diagnostic benchmarks further probe cross-domain generalization and integrated capabilities (Ying et al., 2024; Hao et al., 2025; Yu et al., 2023), while realism-oriented efforts incorporate high-resolution inputs or document-like scenarios (Li et al., 2024a; Fu et al., 2024; Wang et al., 2024; Zhang et al., 2024). Despite this progress, most existing benchmarks still assume clean, standardized visual inputs and rarely evaluate grading-oriented behavior on noisy, freehand diagrams. As summarized in Table 1, none of the closely related benchmarks jointly target freehand sketches, diagram grading, and fine-grained error diagnosis. SketchJudge fills this gap by focusing on hand-drawn, structure-rich student diagrams and enabling diagnostic evaluation beyond answer matching. Evaluation Paradigms: Solver vs. Judge. Existing benchmarks have largely followed solver paradigm, where MLLMs outputs are directly matched against ground-truth answers (Liu et al., 2024; Yue et al., 2024; Li et al., 2024b). In contrast, judge-based approaches position models as evaluators, assessing response quality along dimensions like correctness, helpfulness, and consistency (Zheng et al., 2023; Tan et al., 2024; Chen et al., 2024). This shift, initially explored in textonly evaluation, has recently been adopted in multimodal contexts such as hallucination detection and visual consistency assessment (Li et al., 2023; Guan et al., 2024). Such approaches are especially relevant for educational applications, where grading often requires diagnosing partial errors or structural flaws rather than assigning simple right-or-wrong label (Xie et al., 2024). Data Realism and Educational Contexts. Many multimodal benchmarks use clean or synthetic visuals, such as standardized diagrams or programmatically rendered charts. Recent work has shifted toward more realistic inputs, including scanned documents and high-resolution natural images (Mathew et al., 2021; Tanaka et al., 2024; Zhang et al., 2024). However, these efforts remain largely separate from educational assessment, where student-generated responses are often messy, hand-drawn, and structurally irregular. In practice, educational datasets still predominantly focus on text-based answersoften multiple choicein science and mathematics, offering limited evidence on how models handle freehand diagrammatic solutions (Lu et al., 2022, 2023). resources have begun exploring Very recent personalized error diagnosis and feedback generation (Zhang et al., 2025; Hsu et al., 2025), but none target the grading of hand-drawn diagrams. SketchJudge fills this gap by introducing studentstyle sketches with fine-grained error annotations as diagnostic probe for evaluating MLLMs in realistic educational contexts. 2 Table 1: Comparison with closely related benchmarks. We mark whether benchmark involves freehand diagram inputs, evaluates student-style responses, supports grading/judging rather than solving, provides fine-grained error diagnosis, and includes an explicit reference solution/diagram. Benchmark DocVQA (Mathew et al., 2021) ChartQA (Masry et al., 2022) MathVista (Lu et al., 2023) JudgeBench (Tan et al., 2024) HallusionBench (Guan et al., 2024) SketchJudge (ours) Freehand Student Resp. Grading/Judge Diag. Types Ref. Provided Table 2: Key statistics of the SketchJudge."
        },
        {
            "title": "3.2 Data Collection",
            "content": "Dataset Scale Total problems Total hand-drawn answers Average answers per problem Total images (Q + + A) Input Modalities Questions containing images Problems with reference diagrams Annotations Correct answers Incorrect answers Domain-specific error types Value 300 1015 3.38 1462(147/300/1015) 147 / 300 (49%) 300 (100%) 470 (46%) 545 (54%)"
        },
        {
            "title": "3.1 Overview of SketchJudge",
            "content": "charts, SketchJudge is diagnostic benchmark for evaluating how MLLMs grade noisy, freehand diagrammatic answers, mirroring real classroom scoring scenarios. It spans four task domainsgeometry, physics, and flowchartswith key dataset statistics summarized in Table 2. Figure 2 provides representative instances illustrating the benchmark setup. Each SketchJudge instance takes the form {Q, R, A}, consisting of naturallanguage question (Q), clean reference diagram (R), and one or more hand-drawn student answers (A). The freehand sketches introduce substantial ambiguity in perception and structure, posing realistic challenge for grading-oriented MLLMs. To reflect common grading practices, SketchJudge defines reference-based setting (WithRef ) in which the model is provided with the reference diagram R. In addition, we introduce complementary reference-free setting (NoRef ) that withholds R, designed to stress-test models ability to reason directly over the question and studentdrawn answer without reliance on canonical solution. Figure 3 illustrates the grading formulation and expected outputs, including correctness judgments and fine-grained error-type diagnoses. We collect SketchJudge with an emphasis on realism and diversity, aiming to reflect the variety of diagrammatic reasoning problems and classroomstyle student submissions. Below we detail the data collection process for the three components of {Q, R, A} each instance. Question. We curate four task categories: geometry, physics, charts, and flowcharts. Geometry and physics questions are primarily sourced from past exams and textbooks. For chart and flowchart tasks, we additionally include small set of synthetic questions to control structural complexity and data diversity (e.g., varying chart types, scales, and flow structures), while keeping most questions real-world. Reference. For each problem, we obtain clean reference diagram (R) via one of three routes: (a) retrieving an official or widely accepted solution when available; (b) programmatically rendering chart references from the underlying data; or (c) manually authoring and validating reference when no standard solution exists. All problem statements and references are verified by three independent raters, with disagreements resolved by consensus. Answer. We collect 19 student-drawn responses per question in either paper-based or digital form. We digitize the sketches as images, preserving realistic noise such as construction traces, uneven strokes, and minor skew. The sketches are contributed by 18 individuals, covering diverse drawing styles and common error patterns."
        },
        {
            "title": "3.3 Annotation Protocol",
            "content": "SketchJudge provides two levels of ground-truth annotations: binary correctness label for each student sketch, and one or more fine-grained, 3 Figure 2: Representative SketchJudge instances across four domains, showing the question Q, reference diagram R, and student answers with correctness labels and error types. domain-specific error types for each incorrect response, enabling diagnostic evaluation beyond correctness. To construct these taxonomies and apply them at scale, we adopt two-stage annotation protocol. Stage 1: Bottom-up taxonomy induction. Three annotators independently wrote naturallanguage error descriptions for all incorrect sketches and cross-checked one anothers descriptions for clarity and consistency. From the resulting pool, we sampled 50 representative descriptions per domain to maximize coverage of distinct error phenomena, and prompted GPT-4o to cluster them into coherent categories, yielding draft taxonomy. The generated categories were manually inspected to remove redundancy and resolve inconsistencies. Stage 2: Expert refinement and taxonomybased labeling. We refined the draft taxonomies through consultation with subject-matter experts, including frontline teachers in physics and math, adjusting category definitions and boundaries for educational plausibility. Using the finalized taxonomies, GPT-4o mapped each annotator-written error description to one or more error types, with the sketch withheld, producing initial labels that human annotators then verified and corrected when necessary; correct responses were labeled as error-free. Prompts used for taxonomy induction and labeling are provided in Appendix C."
        },
        {
            "title": "3.4 Challenges in Diagram Grading",
            "content": "Diagram grading in SketchJudge poses challenges that extend beyond conventional multimodal evaluation. Student-drawn diagrams exhibit substantial perceptual noise and stylistic variation, including uneven strokes, construction traces, and idiosyncratic drawing habits. Such variability complicates visual parsing and makes direct pixelor feature-level matching unreliable. Moreover, many tasks admit multiple valid diagrammatic solutions. These solutions may differ markedly in layout or orientation while remain4 Figure 3: Illustration of the diagram grading task in SketchJudge. ing topologically and semantically correct. As result, correctness cannot be determined by similarity to the reference diagram alone, but instead requires understanding the underlying task constraints and diagram semantics. Beyond perceptual and structural variation, diagram grading demands fine-grained diagnostic judgment rather than coarse correctness assessment. As illustrated in Figure 2, sketches that deviate substantially from the reference may still be correct, whereas incorrect answers often contain subtle, localized errors, such as missing or incorrect labels, minor procedural mistakes, or small inaccuracies that alter semantic meaning. Distinguishing acceptable freehand imprecision from genuine conceptual or procedural errors requires models to reason about diagrammatic intent and apply domain knowledge, rather than relying on surface-level visual alignment."
        },
        {
            "title": "4.1 Evaluation Settings",
            "content": "We evaluate MLLMs on SketchJudge in strictly zero-shot grading setup, following the WithRef and NoRef settings defined in Section 3.1. In our main experiments, we report WithRef results, and provide NoRef results and detailed comparisons in Section 4.3.2 and Appendix F. All models are evaluated with deterministic decoding in all settings (temperature = 0)."
        },
        {
            "title": "4.1.1 Evaluation Metrics\nBinary grading. Correctness prediction is for-\nmulated as a binary classification task. We report\nAcc, alongside FNR and FPR to diagnose over-\nstrict and over-lenient tendencies. This allows us\nto disentangle raw performance from systematic\ngrading bias.",
            "content": "Error-type recognition. For answers judged incorrect, we further evaluate recognition of finegrained error types. Domain-specific taxonomies are unified into namespaced global label set. Performance is reported using the example-based F1 (ebF1) (Nam et al., 2017), which measures the overlap between the predicted and gold error sets for each instance and averages over instances. We compute ebF1 only on instances where both the gold annotation and the model prediction indicate incorrectness. Detailed metric definitions are provided in Appendix E."
        },
        {
            "title": "4.1.2 Models",
            "content": "We evaluate 16 state-of-the-art MLLMs, including both open-source and proprietary systems. All models are evaluated using identical prompts across domains, without any task-specific tuning. The full prompt templates are provided in Appendix C. complete list of models and their detail information is provided in Appendix D."
        },
        {
            "title": "4.2 Main Results",
            "content": "Table 3 reports the main leaderboard results on SketchJudge, showing that diagram grading remains challenging for current MLLMs. Human 5 Table 3: Overall leaderboard on SketchJudge. Bold and underline indicate the best and second-best scores among models. Human performance is measured on stratified subset of 160 student answers (40 per domain, balanced over correct/incorrect), graded by three non-expert raters following the same instructions; it is intended as scalable baseline rather than an expert upper bound. All model results are computed on the full test set. Model Random choice Human Physics Geometry Chart Flowchart overall Acc ebF1 Acc ebF1 Acc ebF1 Acc ebF1 Acc ebF1 50.00 81.67 63. 50.00 83.33 49.25 50.00 85.00 76.13 50.00 83.33 76. 50.00 83.33 66.42 Llama-4-Scout-17B-16E-Instruct Gemma-3-4B-it Gemma-3-27B-it Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qianfan-VL-8B Qianfan-VL-70B GLM-4.6V ERNIE-4.5-Turbo-VL Mistral-Large-3 Claude-3.7-Sonnet Doubao-Seed-1.6-Vision Gemini-2.5-Flash o3 GPT-5 Open-source models 68.58 51.34 65.13 54.02 54.41 59.00 52.49 60.54 62. 32.01 21.27 34.81 26.22 28.81 23.36 22.22 20.16 37.14 70.73 55.40 63.07 56.45 64.46 74.91 70.03 67.60 76.22 Closed-source models 66.28 64.37 68.97 63.22 77.04 72.41 77.78 25.94 32.16 36.44 35.80 45.98 43.57 45.85 72.13 68.99 75.61 77.35 80.14 78.05 82. 18.29 25.05 36.96 22.05 28.53 29.34 48.15 39.12 46.45 39.03 29.69 48.33 46.97 51.69 44.14 46.83 63.52 55.36 58.37 53.65 60.94 57.94 45.92 57.76 60.94 60.09 57.08 67.81 66.09 71.67 73.82 73.39 59.76 43.94 74.08 44.66 48.50 66.80 32.49 61.00 68.50 51.89 56.06 70.59 72.09 66.16 66.11 76. 73.50 51.28 68.38 50.43 68.80 72.65 65.81 73.50 76.92 66.24 75.21 79.49 66.67 81.62 73.50 79.06 42.66 38.67 54.64 30.42 33.33 44.01 38.82 38.48 61.32 37.34 49.07 63.85 64.08 66.90 65.84 70.05 69.16 53.40 63.74 53.79 62.07 66.40 59.01 64.89 69.16 66.50 66.50 73.00 68.67 77.74 74.58 78. 39.66 31.66 49.28 31.62 34.00 39.28 32.40 39.14 53.35 38.56 41.96 57.11 54.51 58.30 55.91 61.13 raters achieve an overall accuracy of 83.33%, (GPT-5) the best-performing model whereas reaches 78.42%, followed by Gemini-2.5-Flash (77.74%) and o3 (74.58%). Most open-source models obtain substantially lower overall accuracy, typically in the 53%69% range. Since the human baseline is produced by independent nonexpert raters following the same instructions as models, it reflects scalable grading setting rather than expert-level performance. We therefore treat it as practical reference point for model comparison, complementing expert-validated ground-truth annotations. We further observe clear stratification between closed-source and open-source models in binary grading accuracy. Closed-source models generally deliver stronger and more consistent performance across domains, with multiple models exceeding 73% overall accuracy (e.g., Claude-3.7Sonnet, Gemini-2.5-Flash, o3, GPT-5). In contrast, open-source models exhibit both lower accuracy and larger cross-domain variance, even at larger scales. This gap suggests that robust grading requires reliable visionsymbol alignment and cross-domain generalization, which remain hard to match with current open-source MLLMs. Beyond correctness, SketchJudge evaluates fine-grained diagnostic capability via error-type recognition. Human raters achieve an overall ebF1 of 66.42, while the best models approach but do not match this level, with GPT-5 obtaining 61.13 and Gemini-2.5-Flash reaching 58.30. Interestingly, the strongest model differs across domains, indicating that grading performance is domainsensitive and not yet uniformly solved. Overall, these findings support SketchJudges diagnostic focus and motivate future work on structured reasoning and supervision strategies that better ground error diagnoses in diagram semantics."
        },
        {
            "title": "4.3 Diagnostic Analyses",
            "content": "Beyond the aggregate results in Table 3, we also conduct diagnostic analyses to better understand how models behave as diagram graders, focusing on strictness/leniency (FNR/FPR), reliance on reference diagrams (WithRef vs. NoRef ), and fineIn addition, we grained error-type recognition. report broader range of diagnostic analyses and controlled ablations in Appendix F."
        },
        {
            "title": "4.3.1 Strictness and Leniency",
            "content": "We analyze grading bias using the false-negative rate (FNR) and false-positive rate (FPR). Figure 4 positions all evaluated models in the FNRFPR 6 space, where the lower-left region indicates more balanced grading. Figure 4: Strictness and leniency across models. Figure 4 shows substantial variation in grading tendencies. Several smaller open-source models are strongly over-strict, with very high FNR that rejects many correct sketches (e.g., Qwen2.5-VL-3B-Instruct). Others are comparatively over-lenient, exhibiting low FNR but much higher FPR, suggesting they may accept plausiblelooking yet incorrect diagrams (e.g., Gemma-34B-it). Stronger closed-source models such as GPT-5 and Gemini-2.5-Flash lie closer to the balanced region, but still make nontrivial errors in both dimensions. Overall, reliable diagram grading requires tolerating freehand variation while detecting conceptual errors."
        },
        {
            "title": "4.3.2 Effect of Reference Answers\nWe quantify the effect of reference diagrams by\ncomparing model performance under WithRef and\nNoRef. Figure 5 visualizes per-model changes in\nbinary grading accuracy and error-type recogni-\ntion (ebF1); model indices follow Appendix F.",
            "content": "Figure 5: Effect of reference diagrams across models. (Left) Accuracy comparison between WithRef and NoRef settings. (Right) Example-based F1 comparison under the same conditions. Providing reference diagram improves accu7 racy for most models, suggesting that an explicit canonical solution helps align judgments with task In contrast, the effect on ebF1 is constraints. less uniform: some models show clear improvements in error-type recognition, while others remain nearly unchanged. This gap suggests that reference availability primarily aids correctness verification, whereas assigning fine-grained error types still requires deeper diagram-level interpretation beyond reference-based matching."
        },
        {
            "title": "4.3.3 Effect of Prompt Design",
            "content": "To test whether prompting can improve diagram grading, we compare three prompt styles: (i) Baseline prompt with minimal instructions, (ii) Rubric prompt that enumerates grading criteria and error types, and (iii) Chain-of-Thought (CoT) prompt that encourages step-by-step comparison before making final judgment. Table 4: Effect of prompt variants on grading performance. Model Prompt Acc Gemini-2.5-Flash GLM-4.6V Qwen2.5-VL-72B-Instruct Baseline Rubric CoT Baseline Rubric CoT Baseline Rubric CoT 77.74 76.95 76. 69.16 70.15 64.63 66.40 67.19 66.01 ebF1 58.30 54.17 55.68 53.35 54.13 50.78 39.28 41.07 38. FNR FPR 0.263 0.289 0.351 0.494 0.385 0.547 0.511 0.483 0.543 0.188 0.180 0. 0.149 0.224 0.187 0.185 0.195 0.165 Table 4 compares three prompting styles on three representative models: Gemini-2.5-Flash, and Qwen2.5-VL-72B-Instruct. GLM-4.6V, Rubric prompting yields only modest and modeldependent changes: it slightly improves Acc and ebF1 for GLM-4.6V and Qwen2.5-VL72B-Instruct, but decreases both metrics for In contrast, CoT prompting Gemini-2.5-Flash. consistently hurts performance for all three models. For Gemini-2.5-Flash, CoT reduces Acc from 77.74% to 76.16% while increasing FNR from 0.263 to 0.351, indicating more conservative yet less reliable grading. These results suggest that, in diagram grading, longer reasoning traces do not reliably compensate for noisy visual perception. plausible explanation is that CoT prompts amplify early misinterpretations of sketch structure or symbols, which then propagate through subsequent reasoning and degrade the final decision. This contrasts with text-only settings where CoT often improves reaFigure 6: Per-class diagnostic recall across four domains. Bars show per-class recall aggregated over all evaluated models; error bars indicate the interquartile range (2575%) of per-model recalls. Category names are shortened for readability; full definitions are provided in Appendix B. soning reliability, and highlights that perceptual uncertainty and cross-modal alignment remain primary bottlenecks for multimodal graders."
        },
        {
            "title": "4.3.4 Per-Error-Type Recall Analysis",
            "content": "We further examine whether MLLMs can diagnose which error occurs in an incorrect diagram. Figure 6 reveals two clear patterns. First, error types tied to salient local cues are consistently easier: connection-related errors in Physics and chart-type mistakes in Charts achieve the highest recall, and Flowchart errors such as missing steps or incorrect shapes are also detected reliably. In contrast, categories that depend on global structure or latent intent remain difficult across domainsincluding concept-target and scaling errors in Physics, strategy-level mistakes in Geometry, and presentation or omission errors in Charts. Second, several categories exhibit large variability across models, indicating that fine-grained error diagnosis is not yet stable capability even among strong MLLMs. Overall, the results highlight that robust diagnostic grading requires deeper structural and semantic understanding beyond surface-level cues."
        },
        {
            "title": "5 Conclusion and Outlook",
            "content": "This work steps beyond the longstanding perfect world assumption in multimodal evaluation and asks whether MLLMs can act as reliable diagram graders rather than merely answer solvers. With SketchJudge, we instantiate this question across four STEM-related domains, pairing messy, hand-drawn student sketches with binary correctness labels and fine-grained, domain-specific error taxonomies. Our experiments show that state-of-the-art MLLMs still struggle with core grading competencies, including robustness to stylistic variation, sensitivity to structural and topological distinctions, and consistent attribution of conceptual errors. Diagnostic analyses further suggest that current models often behave more like perceptual pattern recognizers than reasoning-aware graders. Looking forward, SketchJudge enables research It can serve as testbed beyond direct grading. for tool-augmented and agentic multimodal reasoning, where models must plan, externalize intermediate steps, and interact with tools to construct, generate, or verify diagrams. Recent progress on structured reasoning and curriculum-style incontext learning for complex problem solving offers complementary avenues for strengthening such capabilities (Ma et al., 2025). We hope SketchJudge helps drive progress toward MLLMs capable of deeper diagram-level reasoning and feedback-oriented assessment."
        },
        {
            "title": "Broader Impacts",
            "content": "As benchmark targeting educational grading scenarios, SketchJudge highlights both the promise and the brittleness of current MLLMs when interacting with student-generated content. Improved diagram-level understanding could support scalable formative assessment and personalized feedback, but high-stakes use requires careful validation and oversight. Models evaluated on SketchJudge should be viewed as research tools rather than replacements for human educators. We encourage future work on humanAI collaborative workflows, stronger transparency and calibration in model decisions, and safeguards that ensure equitable and pedagogically sound deployment of multimodal grading technologies."
        },
        {
            "title": "Limitations",
            "content": "error human-written SketchJudge relies on labor-intensive pipeline involving descriptions, expert-informed taxonomy refinement, and verification of model-assigned labels. While this protocol improves educational plausibility and annotation quality, it also introduces practical constraints on the speed of dataset expansion and the ease of updating taxonomies. In addition, some error categories admit borderline cases where distinctions are subtle and depend on domain judgment, which can introduce residual ambiguity despite careful verification. Finally, SketchJudge focuses on controlled educational-style diagram extending grading models to broader tasks; real-world settings will require further attention to robustness, calibration, and user-facing reliability."
        },
        {
            "title": "References",
            "content": "Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024. Mllm-asa-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. In Forty-first International Conference on Machine Learning. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. 2024. Mme: comprehensive evaluation benchmark for multimodal large language models. Preprint, arXiv:2306.13394. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, and 1 others. 2024. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illuIn Proceedsion in large vision-language models. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. 2025. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444. Wei-Ling Hsu, Yu-Chien Tang, and An-Zi Yen. 2025. Mathedu: Towards adaptive feedback for student arXiv preprint mathematical problem-solving. arXiv:2505.18056. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. 2024b. Seedbench: Benchmarking multimodal large language In Proceedings of the IEEE/CVF Confermodels. ence on Computer Vision and Pattern Recognition, pages 1329913308. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, and 1 others. 2024. Mmbench: Is your multi-modal model an all-around In European conference on computer viplayer? sion, pages 216233. Springer. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521. Xuetao Ma, Wenbin Jiang, and Hua Huang. 2025. Problem-solving logic guided curriculum in-context learning for LLMs complex reasoning. In Findings of the Association for Computational Linguistics: ACL 2025, pages 83948412, Vienna, Austria. Association for Computational Linguistics. Ahmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. 2022. Chartqa: benchmark for question answering about charts with viIn Findings of the Assual and logical reasoning. sociation for Computational Linguistics: ACL 2022, pages 22632279. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209. Jinseok Nam, Eneldo Loza MencÄ±a, Hyunwoo Kim, and Johannes Furnkranz. 2017. Maximizing subset accuracy with recurrent neural networks in multilabel classification. Advances in neural information processing systems, 30. Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, and Ying Shan. 2024a. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790. Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. 2024. Judgebench: benchmark for evaluating llm-based judges. arXiv preprint arXiv:2410.12784. Ryota Tanaka, Taichi Iki, Kyosuke Nishida, Kuniko Saito, and Jun Suzuki. 2024. Instructdoc: dataset for zero-shot generalization of visual document unIn Proceedings of derstanding with instructions. the AAAI conference on artificial intelligence, volume 38, pages 1907119079. Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Fuxiao Liu, Gedas Bertasius, and 1 others. 2024. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 416442. Wenjing Xie, Juxin Niu, Chun Jason Xue, and Nan Guan. 2024. Grade like human: Rethinking automated assessment with large language models. arXiv preprint arXiv:2405.19694. Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, and 1 others. 2024. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. arXiv preprint arXiv:2404.16006. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. 2023. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, and 1 others. 2024. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567. Yi-Fan Zhang, Hang Li, Dingjie Song, Lichao Sun, Tianlong Xu, and Qingsong Wen. 2025. From correctness to comprehension: Ai agents for personalized error diagnosis in education. arXiv preprint arXiv:2502.13789. Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, and 1 others. 2024. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623."
        },
        {
            "title": "A SketchJudge Task Details",
            "content": "This appendix supplements the main benchmark description in Section 3.1 by providing domainlevel details for the four task categories in SketchJudge. We summarize (i) the data sources and synthetic problem construction procedure, and (ii) the task characteristics, dataset statistics, and typical error patterns for each domain (Geometry, Physics, Charts, and Flowcharts). A.1 Data Sanitization and Content Screening We manually screened all collected samples to remove any personally identifying information (e.g., names) and inappropriate or offensive content. We only retain the diagram content necessary for benchmark construction and evaluation. A.2 Data Sources and Synthetic Problem"
        },
        {
            "title": "Construction",
            "content": "SketchJudge problems come from two sources: (i) past-exam or textbook-style items collected from the web, and (ii) constrained synthetic subset limited to charts and flowcharts. The synthetic items were generated with GPT-4o via interactive, multi-turn prompting following domain-specific instructions (rather than single fixed prompt template). We guided the model to produce diagramdrawing tasks with explicit constraints (e.g., chart type and data values; flowchart steps, decision branches, and standard symbols), and re-generated instances that were ambiguous or violated conventions. Each item is tagged by provenance (web vs. generated) to enable source-aware analyses, while main results aggregate across both sources unless otherwise noted. All synthetic problems underwent manual verification to ensure (i) internal consistency of constraints, (ii) adherence to standard diagram conventions, and (iii) solvability under the stated requirements. Instances failing verification were discarded and re-generated rather than edited. A.3 Geometry Tasks Geometry tasks focus on geometric construction and spatial reasoning under freehand drawing noise (e.g., imprecise angles and ambiguous auxiliary marks). Table 5 summarizes the geometry split. Typical error patterns. Geometry sketches commonly exhibit errors related to principle violations (Geometric Principle Error), missing or in10 Table 5: Geometry domain statistics. Table 7: Chart domain statistics. Statistic Problems Student answers Questions with images Reference diagrams Avg. answers per problem Synthetic proportion Value Statistic 100 261 98 100 2.61 0% Problems Student answers Questions with images Reference diagrams Avg. answers per problem Synthetic proportion Value 100 287 0 100 2.87 15% correct auxiliary constructions (Operational Process / Strategy Planning Error), and annotation or representation issues (Diagram Annotation Error). Many problems also admit multiple valid constructions, making grading harder than direct reference matching. A.4 Physics Tasks Physics tasks involve circuit diagrams, vector reasoning, and force analysis, requiring both geometric fidelity (e.g., arrow orientation) and semantic correctness (e.g., current flow). Table 6 summarizes the physics split. Table 6: Physics domain statistics. Statistic Problems Student answers Questions with images Reference diagrams Avg. answers per problem Synthetic proportion Value 50 233 49 50 4.66 0% Typical error patterns. Physics sketches often fail due to incorrect direction or polarity (e.g., reversed vectors or circuit polarity), missing or wrong connections between elements, and misplaced components or forces that break the intended physical setup. Models also struggle with errors that require conceptual interpretation, such as misunderstanding the target quantity or applying an invalid physical principle. In vector-based diagrams, inaccurate relative magnitudes further introduce subtle grading difficulty beyond surfacelevel appearance. A.5 Chart Tasks Chart tasks require visualizing numerical data as bar, line, or pie charts. Errors are often visually salient but depend on accurate mapping between values and graphical elements. Table 7 (Chart types: bar/- summarizes the chart split. line/pie/others = 54/25/19/2.) Typical error patterns. Chart sketches typically fail in three ways: choosing an inappropriate chart form for the task, mis-associating categories and values, or encoding the data inaccurately through incorrect scaling and proportions. Even when the overall structure is plausible, charts often omit essential elements such as labels or data entries, or violate basic presentation conventions (e.g., unclear axes or inconsistent annotations), which makes them difficult to grade reliably beyond superficial visual matching. A.6 Flowchart Tasks Flowchart tasks require understanding procedural structure, control flow, and decision logic. Unlike charts, correctness depends on global topology rather than local appearance. Table 8 summarizes the flowchart split. Table 8: Flowchart domain statistics. Statistic Problems Student answers Questions with images Reference diagrams Avg. answers per problem Synthetic proportion Value 50 234 0 50 4.68 38% Typical error patterns. Flowchart answers commonly contain structural topology issues, including missing steps, incorrect arrow connections, or reversed decision branches that alter the In addition, students intended execution logic. frequently misuse flowchart symbols (e.g., decision vs. process shapes) or omit key labels such as branch outcomes, which makes the logic ambiguous even when most nodes appear correct. These patterns highlight that flowchart grading depends heavily on global control flow rather than local visual similarity."
        },
        {
            "title": "B Error Category Taxonomy",
            "content": "To support fine-grained evaluation, we design taxonomy of error types covering all four task categories. Tables 9, 10, 11, and 12 provides the full list of categories and definitions. This taxonomy serves as the basis for our multi-label annotation protocol described in Section 3.3."
        },
        {
            "title": "C Prompt Templates",
            "content": "This appendix documents all prompt templates used in SketchJudge, covering both the construction of annotation taxonomies and the evaluation of multimodal models. In particular, we provide the prompts used for (i) inducing domain-specific error taxonomies and assigning error labels during annotation, and (ii) evaluating model grading performance under different input settings. C.1 Prompts for Taxonomy Induction and"
        },
        {
            "title": "Annotation",
            "content": "We employ prompt-based interaction with GPT4o to support both stages of the annotation protocol described in Section 3.3. Specifically, GPT4o is used to (i) cluster annotator-written error descriptions into candidate error categories during bottom-up taxonomy induction, and (ii) map each error description to one or more error types under the finalized taxonomy. In both cases, the model operates solely on textual error descriptions and has no access to the corresponding sketches. The full prompt templates used for taxonomy induction and taxonomy-based labeling are listed in Tables 14 and 15. C.2 Prompts for Model Evaluation templates used We next describe the prompt to evaluate multimodal models on SketchJudge. These prompts define the grading task, available inputs, and required structured outputs under different task settings. C.3 Task Settings We consider four canonical input configurations: For each case, the Baseline prompt provides the minimal necessary grading instructions. The full Baseline templates for all four cases appear in Figures 710. Rubric-based and Chain-ofThought prompts extend the Baseline by adding explicit evaluation rubrics or stepwise reasoning guidance while preserving the same input structure and JSON output specification. These templates are listed in Tables 17 and 18. C.4 Baseline Prompt Templates Figures 710 show the complete Baseline prompts used in our main experiments. They define the core task setting, model role, available images, and the structured JSON output expected from all models. C.5 Rubric-Based Prompt Templates Table 17 provides the full Rubric-based variants. These prompts supply explicit grading criteria describing which aspects of the diagram should be checked. Aside from the additional rubric, all other componentsincluding the task description, image inputs, and JSON output formatremain identical to the Baseline family. C.6 Chain-of-Thougth Prompt Templates Table 18 lists the Chain-of-Thought prompts, which require the model to explicitly articulate multi-stage reasoning process before producing the final JSON output. This design isolates the effect of chain-of-thoughtstyle guidance while preserving identical output constraints."
        },
        {
            "title": "D Evaluated Models",
            "content": "Table 19 lists all evaluated MLLMs, spanning both open-weight and closed-source systems across range of scales. All models are tested in the same zero-shot setup with identical prompts and deterministic decoding (temperature = 0). Model IDs are used for compact references in figures and diagnostic analyses. Output Robustness and Format Compliance During evaluation, all models were instructed to return strict JSON-only output. In practice, small number of responses from several models occasionally included extra text (e.g., brief reasoning traces or wrapper sentences) outside the requested JSON block. These formatting deviations were rare relative to the full test set, and the underlying JSON content was typically still wellformed and semantically consistent with the intended prediction. To ensure reliable scoring, we applied deterministic post-processing step that extracts and validates the final JSON object, and we manually inspected the few remaining failures to correct obvious formatting issues. We report results based 12 Error Category Definition Conceptual Cognition Error misunderstanding of basic geometric concepts or the construction target, leading to drawing an object inconsistent with the task requirements. Geometric Principle Error Failure to correctly apply geometric principles or violation of fundamental geometric rules, resulting in construction that does not meet the requirements. Operational Process Error Problems in the use of drawing tools or execution of steps, or the absence of necessary construction traces, leading to result lacking standardization. Strategy Planning Error Lack of clear construction plan; reasoning steps or the overall approach are flawed, undermining the correctness of the construction. Diagram Annotation Error The orientation, connections, annotations, or representation standards of the graph contain errors, leading to disordered relationships among geometric elements and an unclear intent. Table 9: Error taxonomy in the Geometry category. Error Category Definition Concept/Target Error Failure to understand the core objective of the problem, resulting in diagram that does not match the task requirements. Direction/Polarity Error Arrows, directions, or polarity symbols are reversed or inconsistent with physical laws. Magnitude/Scale Error The relative magnitudes of vectors or physical quantities are drawn incorrectly, contradicting the problem statement or physical principles. Connection Error Placement Error Incorrect or missing connections between elements, such as forces, components, rays, or nodes, leading to an invalid representation of physical interactions or processes. Elements, forces, light rays, or components are positioned incorrectly, making the representation inconsistent with the actual physical situation. Conceptual Misunderstanding Misapplication of fundamental physical principles leads to representation that is physically invalid or impossible. Drawing Convention Error Violation of standard drawing conventions or established norms in physics education, such as incorrect use of line types or symbols. Table 10: Error taxonomy in the Physics category. Error Category Definition Shape Error Connection Error The shape of the flowchart element does not match the standard notation for its function (e.g., using rectangle instead of diamond for decision). The connectors between elements are drawn incorrectly, such as arrows pointing in the wrong direction or invalid links between nodes. Missing Step required processing step in the flowchart is omitted, leaving the workflow incomplete. Missing Label Logic Reversal Condition Error Essential labels are missing or unclear, such as omitting Yes/No labels on decision branches. The logical flow is reversed or misrepresented, such as swapping Yes/No branches or inverting decision outcomes. The condition itself is incorrect, incomplete, or ambiguously expressed, leading to misinterpretation of the decision point. Table 11: Error taxonomy in the Flowchart category. 13 Error Category Definition Inappropriate Chart Type Selection of an incorrect chart type, inconsistent with the data characteristics or task requirements, leading to confusion in data representation. Data Association Error Incorrect organization of data categories, leading to mismatched labels and values. Data Accuracy Error Omission Error Data accuracy is compromised due to encoding or scaling errors during the conversion of data into visual elements. Omission of essential elements, labels, or data entries, reducing the completeness of the chart. Presentation Convention Error Failure to follow labeling or standardization conventions, causing unclear or inaccurate chart presentation. Table 12: Error taxonomy in the Charts category. Figure 7: Prompt template for Case 1 (Text-only, No Reference). 14 Figure 8: Prompt template for Case 2 (Text-only, With Reference). Figure 9: Prompt template for Case 3 (Diagram, No Reference). 15 Figure 10: Prompt template for Case 4 (Diagram, With Reference). 16 Table 13: Mapping between full error type names and shortened labels used in Figure 6. Table 15: Prompt used for taxonomy-based labeling from text-only error descriptions (diagram withheld). Domain Full Name Physics Physics Physics Physics Physics Physics Physics Connection Error Direction Polarity Error Placement Error Conceptual Misunderstanding Concept Target Error Drawing Convention Error Magnitude Scale Error Geometry Operational Process Error Geometry Geometric Principle Error Geometry Geometry Diagram Annotation Error Geometry Strategy Planning Error Conceptual Cognition Error Alias connection direction placement conceptual concept target drawing scale process principle conceptual annotation strategy Goal Prompt Assign taxonomy labels You are given an error Please taxonomy and one free-form error description written by an annotator. assign exactly one taxonomy label that best matches the description based only on the text. from the given taxonomy. Return the label only (no explanation). Use only labels Chart Chart Chart Chart Chart Inappropriate Chart Type Data Accuracy Error Data Association Error Omission Error Presentation Convention Error chart type data accuracy data association omission presentation Flowchart Missing Step Flowchart Shape Error Flowchart Missing Label Flowchart Logic Reversal Flowchart Connection Error Flowchart Condition Error missing step shape missing label logic reversal connection condition on the recovered JSON outputs. Overall, this issue had negligible impact on model performance, but it highlights that robust instruction-following and structured output remain non-trivial even for strong MLLMs."
        },
        {
            "title": "E Metric Definitions",
            "content": "Input [[TAXONOMY BULLETED]] [[ERROR DESCRIPTIONS LIST]] Table 16: Four canonical SketchJudge grading. input configurations for Case Question Content Reference Diagram 1 2 3 4 Text-only Text-only Contains an image Contains an image Not provided Provided Not provided Provided E.1 Binary Grading Metrics We cast grading as two-class classification problem with = 1 denoting gold Correct and = 0 denoting gold Incorrect. For submissions and predictions Ëyi, we define: Confusion counts. We provide complete definitions of the evaluation metrics used in our study. Binary grading metrics capture overall correctness and class balance, while multi-label metrics assess fine-grained error-type recognition. True Positive (TP): #{ y=1, Ëy=1 } False Positive (FP): #{ y=0, Ëy=1 } True Negative (TN): #{ y=0, Ëy=0 } False Negative (FN): #{ y=1, Ëy=0 } Table 14: Prompt used for draft taxonomy induction from annotator-written error descriptions. Goal Prompt Draft taxonomy induction We are building an error Below are error taxonomy for grading hand-drawn diagrams in {{DOMAIN}}. descriptions written by annotators for incorrect student sketches. propose concise set of error categories that can cover these mistakes. each category, give short name and its corresponding description. Please For Input [[ERROR DESCRIPTIONS LIST]] Accuracy. Overall proportion of correct decisions: Accuracy = + P + + + . (1) MacroF1bin. To balance both classes, we compute an F1 score for each class and average: F1pos = F1neg = , 2 2 + + 2 2 + + (cid:0)F1pos + F1neg , MacroF1bin = 1 2 Here, F1pos measures recognition of gold-correct answers, while F1neg measures recognition of gold-incorrect answers. (4) (cid:1). (2) (3) Table 17: Rubric-based prompt templates. We share common rubric prompt and vary only the input configuration. T: text-only question; D: question includes diagram; R: reference diagram provided; N: no reference. Case Setting Inputs inserted into the shared rubric prompt 1 2 4 (T, N) (T, R) Task: [[QUERY EN]]; Student: <image 0> Task: [[QUERY EN]]; Ref: <image 0>; Student: <image 1> (D, N) Q-diagram: <image 0>; Task: [[QUERY EN]]; Student: <image 1> (D, R) Q-diagram: <image 0>; Task: [[QUERY EN]]; Ref: <image 1>; Student: <image 2> Shared rubric prompt (NoRef ). Follow the rubric below to determine You are an impartial diagram grader. whether the students hand-drawn diagram correctly solves the task. RUBRIC -- correct diagram MUST: (1) include all required task elements; (2) preserve correct spatial, structural, and logical relationships; (3) convey domain-appropriate semantics; (4) be complete and coherent, even with typical hand-drawn imperfections. diagram is incorrect if it violates any required structural, semantic, or logical condition. [Insert case-specific inputs here] Evaluate correctness using the rubric. inaccuracies. below (no new labels, no paraphrasing): [[TAXONOMY BULLETED]]. (Return ONLY the JSON object specified in the baseline template.) If incorrect, classify ALL errors using exactly the labels Ignore normal hand-drawn Shared rubric prompt (WithRef ). Use Follow the rubric below to determine You are an impartial diagram grader. whether the students hand-drawn diagram correctly solves the task. the reference diagram as the canonical solution, and evaluate semantic and structural consistency rather than visual similarity. RUBRIC -- correct diagram MUST: (1) include all required task elements; (2) preserve correct spatial, structural, and logical relationships; (3) convey domain-appropriate semantics; (4) be complete and coherent, even with typical hand-drawn imperfections. diagram is incorrect if it violates any required structural, semantic, or logical condition. [Insert case-specific inputs here] Evaluate correctness using the rubric. inaccuracies. below (no new labels, no paraphrasing): [[TAXONOMY BULLETED]]. (Return ONLY the JSON object specified in the baseline template.) If incorrect, classify ALL errors using exactly the labels Ignore normal hand-drawn 18 Table 18: Chain-of-Thought reasoning templates. All variants share the same JSON output format as the baseline family. T: Text-only, D: Diagram in question, R: With reference, N: No reference. Case Setting Inputs inserted into the shared rubric prompt 1 2 3 4 (T, N) (T, R) Task: [[QUERY EN]]; Student: <image 0> Task: [[QUERY EN]]; Ref: <image 0>; Student: <image 1> (D, N) Q-diagram: <image 0>; Task: [[QUERY EN]]; Student: <image 1> (D, R) Q-diagram: <image 0>; Task: [[QUERY EN]]; Ref: <image 1>; Student: <image 2> Shared rubric prompt (NoRef ). Carefully evaluate the students Identify all required elements. Check structural and spatial relationships. Evaluate domain-specific correctness (geometry, physics, charts, You are an impartial diagram grader. hand-drawn diagram using step-by-step internal reasoning before producing the final JSON output. INTERNAL REASONING INSTRUCTIONS (do not reveal): - First interpret the task requirements and the reference diagram. - Then analyze the students diagram step by step: 1. 2. 3. flowcharts, etc.). 4. similarity) and identify any conceptual or procedural flaws. - Decide correctness only after completing all reasoning steps. normal hand-drawn inaccuracies. - Do NOT reveal the reasoning; output only the final JSON result. [Insert case-specific inputs here] If the answer is incorrect, classify ALL errors using exactly the labels listed below (no new labels, no paraphrasing): [[TAXONOMY BULLETED]]. (Return ONLY the JSON object specified in the baseline template.) Determine semantic equivalence to the reference (not stylistic Ignore Shared rubric prompt (WithRef ). Carefully evaluate the students Identify all required elements. Check structural and spatial relationships. Evaluate domain-specific correctness (geometry, physics, charts, You are an impartial diagram grader. hand-drawn diagram using step-by-step internal reasoning before producing the final JSON output. Use the reference diagram as the canonical solution. INTERNAL REASONING INSTRUCTIONS (do not reveal): - First interpret the task requirements and the reference diagram. - Then analyze the students diagram step by step: 1. 2. 3. flowcharts, etc.). 4. similarity) and identify any conceptual or procedural flaws. - Decide correctness only after completing all reasoning steps. normal hand-drawn inaccuracies. - Do NOT reveal the reasoning; output only the final JSON result. [Insert case-specific inputs here] If the answer is incorrect, classify ALL errors using exactly the labels listed below (no new labels, no paraphrasing): [[TAXONOMY BULLETED]]. (Return ONLY the JSON object specified in the baseline template.) Determine semantic equivalence to the reference (not stylistic Ignore Table 19: Evaluated multimodal large language models (MLLMs). Provider denotes the organization releasing the model. Open/Closed indicates whether model weights are publicly available. ID Model Provider Open Size Link 1 2 3 4 5 7 8 9 Claude-3.7-Sonnet Anthropic Closed Gemini-2.5-Flash Google Closed GPT-5 OpenAI Closed GLM-4.6V Zhipu AI Open Qwen2.5-VL-72B-Instruct Qwen Open 72B Qwen2.5-VL-7B-Instruct Qwen Open Qwen2.5-VL-3B-Instruct Qwen Open 7B 3B Gemma-3-27B-it Google Open 27B Gemma-3-4B-it Google Open 4B 10 Llama-4-Scout-17B-16E-Instruct Meta Open 17B https://www.anthropic.com/claude https: //ai.google.dev/gemini-api/docs/models https://platform.openai.com/docs/models https://glm-v.com/ https://huggingface.co/Qwen/Qwen2. 5-VL-72B-Instruct https://huggingface.co/Qwen/Qwen2. 5-VL-7B-Instruct https://huggingface.co/Qwen/Qwen2. 5-VL-3B-Instruct https: //huggingface.co/google/gemma-3-27b-it https: //huggingface.co/google/gemma-3-4b-it https://huggingface.co/meta-llama/ Llama-4-Scout-17B-16E-Instruct 11 12 13 14 16 Doubao-Seed-1.6-Vision ByteDance Closed ERNIE-4.5-Turbo-VL Baidu Closed Mistral-Large-3 Mistral AI Closed o3 OpenAI Closed https://www.volcengine.com/product/doubao https: //cloud.baidu.com/product/wenxinworkshop https: //docs.mistral.ai/getting-started/models https://platform.openai.com/docs/models Qianfan-VL-70B Baidu Closed 70B https://cloud.baidu.com/product/qianfan Qianfan-VL-8B Baidu Closed 8B https://cloud.baidu.com/product/qianfan Error tendencies. To analyze systematic biases, we report class-specific error rates in terms of false negatives and false positives. The false negative rate (FNR) reflects tendency toward overstrictness (i.e., failing to recognize correct answers), while the false positive rate (FPR) captures over-leniency (i.e., accepting incorrect answers). FNR = FN FN + TP , FPR = FP FP + TN . (5) (6) Matthews Correlation Coefficient (MCC). We also compute the Matthews Correlation Coefficient as follows: MCC = T F (cid:112)(T + )(T + ) (7) 1 (cid:112)(T + )(T + ) . MCC ranges from 1 (inverse prediction) to 1 (perfect), with 0 indicating random performance. E.2 Error-Type Recognition Metrics For submissions judged incorrect, models must also predict specific error categories. Each subject domain (e.g., physics, geometry) deits own taxonomy, which we merge fines into namespaced global set Cglobal (e.g., physics::omission error) to ensure uniqueness across domains. label Let yi,c, Ëyi,c {0, 1} denote the gold and predicted labels for each Cglobal. We use Pc, Pc, and Nc to denote counts aggregated over all instances for class c. MacroF1err. We report Macro-F1 over all error labels with at least one positive gold instance: MacroF 1err = 1 C+ (cid:88) cC+ 1c, (8) C+ = {c : Pc + Nc > 0} 20 MicroF1err. This variant aggregates counts across all labels before computing F1: MicroF 1err = 2 (cid:80) Pc +(cid:80) Pc Pc +(cid:80) 2 (cid:80) . Nc (9) Example-based F1 (ebF1). To evaluate the overall error detection capability for each instance, we compute the example-based F1 score. This metric measures the overlap between the predicted error set and the gold error set for each instance and averages over all instances. Given gold set Gi and the corresponding predicted set Pi for instance (both masked by applicability), we define the example-based F1 as: 1i = (cid:40) if Gi = Pi = 0, 1.0, 2GiPi Gi+Pi , otherwise (10) where Gi and Pi are the sets of true and predicted errors for instance i. The overall example-based F1 score is then averaged across all instances: ebF1 ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 1i, (11) where is the total number of instances. captures the completeness of the error set prediction for each instance, making it especially relevant for tasks with list of error types per instance. Per-class diagnostic recall. To analyze which conceptual errors models are able to identify, we further report per-class recall for each error category. For an error class c, recall is defined as the ratio of correctly recovered instances to the total number of gold instances of c, aggregated across all evaluated models: Recallc = (cid:80) Pm,c m(T Pm,c + Nm,c) (cid:80) , (12) where Pm,c and Nm,c denote the true positives and false negatives for class produced by model m, computed only on submissions incorrect in both gold annotations and model predictions. In addition to the aggregated recall, we report cross-model variability using the interquartile range (2575%) of per-model recalls for each class. This statistic reflects how consistently different models identify given error type, independent of class frequency."
        },
        {
            "title": "F Full Experimental Results",
            "content": "This appendix provides extended experimental results omitted from the main text for space reasons. In particular, we report (i) complete leaderboards for all evaluated models across all metrics and domains, (ii) full ablation results for key evaluation settings (e.g., with vs. without reference answers), and (iii) additional qualitative and diagnostic analyses that complement the main findings. These results are intended to improve transparency, facilitate reproducibility, and enable deeper inspection of model behaviors beyond the headline numbers reported in the main paper. F.1 Full Leaderboard (All Models All Metrics) Tables 2022 report the complete leaderboard under the WithRef setting across all evaluated models and domains. These tables complement the main leaderboard in Table 3 by providing fuller view of grading behavior beyond accuracy. F.2 Grading Without Reference Answers Table 23 reports Acc and ebF1 for all evaluated models under the NoRef setting, where reference diagrams are removed and models must judge correctness solely based on the problem statement and the student sketch. Overall performance drops substantially compared to the WithRef setting, suggesting that reference-free grading requires solving the underlying task in addition to interpreting noisy hand-drawn diagrams. Although the random baseline yields 50% accuracy, most models only reach the lowto-mid 60% range, indicating that reliable grading remains challenging when models must jointly perform problem solving, diagram understanding, and robustness to freehand variation. Consistent with the main results, closed-source models remain stronger, but the gap between models narrows under NoRef, highlighting the increased difficulty of this setting. F.3 Hand-Drawn vs. Electronic Sketches In addition to structural variation in correct solutions, we further investigate whether model performance depends on the sketch modality. Specifically, we compare results on hand-drawn sketches (captured from freehand writing) versus electronic sketches (produced using stylus or digital tools). Our benchmark contains 771 handTable 20: MCC (range [1, 1]) and MacroF 1bin (100) for all evaluated models under the WithRef setting. Bold and underline indicate the best and second-best scores among models. Model Physics Geometry Chart Flowchart overall MCC MacroF 1bin MCC MacroF 1bin MCC MacroF 1bin MCC MacroF 1bin MCC MacroF 1bin Llama-4-Scout-17B-16E-Instruct Gemma-3-4B-it Gemma-3-27B-it Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qianfan-VL-8B Qianfan-VL-70B GLM-4.6V ERNIE-4.5-Turbo-VL Mistral-Large-3 Claude-3.7-Sonnet Doubao-Seed-1.6-Vision Gemini-2.5-Flash o3 GPT0.284 0.169 0.225 0.225 0.244 0.163 -0.038 0.139 0.207 0.189 0.123 0.397 0.332 0.433 0.483 0.479 63.48 53.12 56.83 56.83 53.35 48.67 36.68 54.51 59.36 57.25 51.51 67.41 62.94 71.61 73.82 73.39 0.375 0.146 0.318 0.318 0.045 0.176 0.095 0.209 0.264 0.356 0.283 0.430 0.295 0.537 0.442 0. Open-source models 68.56 44.44 65.10 65.10 36.71 51.27 50.90 60.42 54.91 0.451 0.319 0.408 0.408 0.297 0.544 0.482 0.441 0.512 Close-source models 61.38 61.61 68.49 56.43 76.81 71.91 77.75 0.429 0.363 0.555 0.543 0.597 0.558 0. 70.69 50.17 61.07 61.07 64.41 74.84 69.58 66.95 75.57 70.98 67.85 75.56 77.10 79.86 77.84 82.58 0.472 0.065 0.409 0.409 0.378 0.462 0.371 0.493 0.568 0.369 0.508 0.609 0.444 0.645 0.523 0.608 73.41 40.40 66.99 66.99 68.78 72.31 63.57 73.00 76.23 63.95 75.07 79.10 62.25 81.41 72.01 78. 0.390 0.181 0.333 0.333 0.232 0.323 0.258 0.304 0.384 0.334 0.326 0.478 0.386 0.552 0.487 0.569 69.16 47.66 62.76 62.76 59.90 64.84 56.82 64.89 67.55 63.97 64.84 72.97 66.07 77.54 74.23 78.39 Table 21: FNR and FPR for all evaluated models under the WithRef setting. Bold and underline indicate the best and second-best scores among models. Model Physics Geometry Chart Flowchart overall FNR FPR FNR FPR FNR FPR FNR FPR FNR FPR Llama-4-Scout-17B-16E-Instruct Gemma-3-4B-it Gemma-3-27B-it Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qianfan-VL-8B Qianfan-VL-70B GLM-4.6V ERNIE-4.5-Turbo-VL Mistral-Large-3 Claude-3.7-Sonnet Doubao-Seed-1.6-Vision Gemini-2.5-Flash o3 GPT-5 Open-source models 0.375 0.058 0.258 1.000 0.983 0.792 0.233 0.400 0.758 0.283 0.851 0.426 0.000 0.007 0.085 0.681 0.390 0.057 Close-source models 0.667 0.592 0.117 0.742 0.271 0.358 0.192 0.057 0.156 0.475 0.050 0.194 0.206 0.248 0.151 0.000 0.024 0.992 0.310 0.087 0.064 0.071 0.278 0.405 0.429 0.087 0.238 0.222 0.222 0.056 0.440 0.688 0.632 0.000 0.056 0.064 0.928 0.210 0.248 0.200 0.152 0.472 0.112 0.288 0.304 0. 0.278 0.167 0.167 1.000 0.778 0.833 0.093 0.667 0.556 0.630 0.750 0.148 0.602 0.278 0.213 0.194 0.404 0.795 0.640 0.000 0.391 0.379 0.485 0.522 0.211 0.180 0.217 0.367 0.217 0.180 0.217 0.267 0.319 0.052 0.103 1.000 0.276 0.379 0.086 0.121 0.397 0.586 0.319 0.336 0.672 0.285 0.491 0. 0.212 0.915 0.525 0.000 0.348 0.170 0.593 0.407 0.068 0.093 0.178 0.076 0.000 0.085 0.042 0.059 0.255 0.066 0.136 0.998 0.581 0.511 0.119 0.304 0.494 0.568 0.517 0.170 0.557 0.263 0.321 0.198 0.354 0.811 0.558 0.000 0.206 0.185 0.661 0.392 0.149 0.134 0.178 0.356 0.103 0.188 0.196 0. Table 22: MacroF 1err (100) and MicroF 1err (100) for all evaluated models under the WithRef setting. Bold and underline indicate the best and second-best scores among models. Model Llama-4-Scout-17B-16E-Instruct Gemma-3-4B-it Gemma-3-27B-it Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qianfan-VL-8B Qianfan-VL-70B GLM-4.6V ERNIE-4.5-Turbo-VL Mistral-Large-3 Claude-3.7-Sonnet Doubao-Seed-1.6-Vision Gemini-2.5-Flash o3 GPT-5 Physics Geometry Chart Flowchart overall MacroF 1err MicroF 1err MacroF 1err MicroF 1err MacroF 1err MicroF 1err MacroF 1err MicroF 1err MacroF 1err MicroF 1err 18.54 26.06 29.34 16.44 27.27 21.74 23.33 30.32 35.19 33.05 19.89 34.31 40.52 41.30 33.33 34. 20.20 24.47 37.50 22.13 28.57 27.30 41.67 39.07 46.01 36.97 29.00 47.06 45.49 51.14 40.57 43.88 25.63 21.33 34.38 23.90 21.27 17.60 13.56 14.21 34.31 22.22 30.61 32.56 30.46 42.67 42.97 44.06 53.36 27.90 53.56 47.19 45.33 58.82 33.56 46.59 61.19 Open-source models 32.56 22.22 37.25 28.51 30.35 25.13 21.74 19.89 37.95 Close-source models 27.34 32.98 37.32 36.36 44.83 42.90 43. 51.95 53.76 57.11 64.28 60.23 59.07 66.05 22 57.45 43.87 70.42 45.09 47.52 62.95 33.33 58.54 66.20 50.45 55.14 66.67 69.13 63.26 63.87 73.19 33.53 75.76 42.55 22.05 20.39 38.89 29.76 31.35 50.26 37.04 38.02 52.52 56.86 55.84 57.44 58. 40.62 38.30 52.17 30.80 33.33 43.72 36.59 35.83 56.28 37.25 46.24 60.42 60.54 61.44 61.54 62.25 31.56 35.28 39.15 26.21 28.10 33.38 25.28 30.62 44.58 35.84 34.31 43.64 47.76 49.50 47.31 49.79 39.40 32.03 47.93 32.69 34.41 38.05 32.70 38.76 51.72 38.25 41.04 54.49 53.16 55.74 53.23 55. Table 23: Acc and ebF1 for all evaluated models under the NoRef setting. Bold and underline indicate the best and second-best scores among models. Model Physics Geometry Chart Flowchart overall Acc ebF1 Acc ebF1 Acc ebF1 Acc ebF1 Acc ebF1 Random choice 50.00 50.00 50.00 50.00 50.00 Llama-4-Scout-17B-16E-Instruct Gemma-3-4B-it Gemma-3-27B-it Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qianfan-VL-8B Qianfan-VL-70B GLM-4.6V ERNIE-4.5-Turbo-VL Mistral-Large-3 Claude-3.7-Sonnet Doubao-Seed-1.6-Vision Gemini-2.5-Flash o3 GPT-5 Open-source models 57.09 46.74 55.94 54.02 54.41 54.41 51.34 45.59 59.00 23.29 26.97 28.44 20.85 26.07 26.51 24.56 18.75 33.17 68.64 49.13 60.63 56.10 61.32 68.29 55.75 61.32 72. Closed-source models 52.49 52.11 56.32 61.30 63.22 68.97 68.20 27.82 28.10 31.42 36.53 35.19 34.87 41.07 59.93 67.94 64.46 73.87 73.52 78.75 78.40 21.59 29.28 31.28 18.40 24.85 27.78 35.56 36.43 40.76 35.36 34.73 39.79 36.42 37.41 35.26 35. 56.65 52.36 55.79 53.65 57.08 56.65 50.43 53.88 55.79 54.94 56.22 59.66 57.08 60.52 59.23 64.38 60.10 43.10 59.09 38.59 39.44 58.61 32.86 50.81 58.94 44.43 51.90 57.72 59.93 62.56 65.16 71.84 59.83 58.12 70.51 50.43 66.24 64.96 58.97 62.82 74.36 53.42 61.54 56.41 61.97 69.23 63.25 67. 37.88 31.46 44.21 30.59 34.86 41.84 47.54 36.94 60.07 35.33 43.56 52.63 56.86 57.50 58.83 58.72 60.89 51.33 60.59 53.69 59.70 61.28 54.14 55.92 65.71 55.37 59.70 59.41 64.04 66.90 68.18 70.05 38.31 31.95 39.44 27.64 31.13 38.65 35.76 35.70 48.61 35.55 40.43 46.50 47.90 49.46 49.15 52. Table 24: Performance Comparison on Hand-Drawn vs. Electronic Sketches. Accuracy (Acc) and examplebased F1 score (ebF1) for hand-drawn and electronic sketches. The Delta column shows the difference (Electronic Hand-drawn). All model results are reported in the WithRef setting. Bold and underline indicate the best and second-best scores within the Hand-Drawn and Electronic columns, respectively. For the Delta column, positive values are shown in green and negative values in red. Model Hand-Drawn Electronic Difference Acc ebF1 Acc ebF1 Acc ebF1 Llama-4-Scout-17B-16E-Instruct Gemma-3-4B-it Gemma-3-27B-it Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qianfan-VL-8B Qianfan-VL-70B GLM-4.6V ERNIE-4.5-Turbo-VL Mistral-Large-3 Claude-3.7-Sonnet Doubao-Seed-1.6-Vision Gemini-2.5-Flash o3 GPT-5 69.78 53.83 64.98 53.96 60.70 66.41 59.53 65.50 69.26 65.89 67.19 73.28 68.48 77.18 74.19 78.21 Open-source models 39.91 32.20 49.20 32.21 35.80 40.34 31.47 39.63 52.73 67.21 52.05 59.84 53.28 66.39 66.39 57.38 62.96 68. Closed-source models 68.44 64.34 72.13 69.26 79.51 75.82 79.10 38.37 42.93 56.86 54.63 56.85 55.84 60.97 23 38.80 29.55 49.62 29.72 28.00 35.72 36.38 37.65 55.44 39.19 38.89 57.96 54.11 62.94 56.17 61. -2.57 -1.78 -5.14 -0.68 +5.69 -0.01 -2.16 -2.54 -0.41 +2.55 -2.84 -1.15 +0.78 +2.32 +1.63 +0.89 -1.12 -2.65 +0.42 -2.50 -7.80 -4.62 +4.91 -1.97 +2.71 +0.82 -4.04 +1.09 -0.52 +6.09 +0.33 +0.68 Table 25: Exact-match vs. alternative correct solutions. Accuracy (Acc) on two subsets of correct sketches: (i) sketches closely matching the reference diagram (Exact), and (ii) sketches that remain correct but deviate from the reference in structure or strategy (Alt). We also report the gap = Exact Alt. All model accuracies are reported in the WithRef setting. Bold and underline indicate the best and second-best scores in each column. For , positive gaps are shown in green and negative gaps in red. Model Physics Geometry Chart Flowchart Overall Exact Alt Exact Alt Exact Alt Exact Alt Exact Alt Llama-4-Scout-17B-16E-Instruct Gemma-3-4B-it Gemma-3-27B-it Qwen2.5-VL-3B-Instruct Qwen2.5-VL-7B-Instruct Qwen2.5-VL-72B-Instruct Qianfan-VL-8B Qianfan-VL-70B GLM-4.6V ERNIE-4.5-Turbo-VL Mistral-Large-3 Claude-3.7-Sonnet Doubao-Seed-1.6-Vision Gemini-2.5-Flash o3 GPT-5 74.19 83.87 83.87 0.00 23.66 16.13 91.40 32.26 45.16 37.63 23.66 92.47 40.86 73.12 82.80 82. 60.00 +14.19 +3.87 80.00 +3.87 80.00 0.00 0.00 13.33 +10.32 -3.87 20.00 86.67 +4.73 -7.74 40.00 +5.16 40.00 +4.30 33.33 33.33 -9.68 40.00 +52.47 +7.53 33.33 66.67 +6.45 53.33 +29.46 66.67 +16.13 76.92 92.31 84.62 0.00 2.20 23.08 83.52 64.84 27.47 40.66 47.25 92.31 26.37 78.65 71.43 87.91 55.17 100.00 41.38 0.00 0.00 13.79 55.17 44.83 13.79 Open-source models 83.33 85.42 +21.75 100.00 100.00 -7.69 93.33 98.96 +43.24 0.00 1.04 0.00 66.67 69.79 +2.20 86.67 92.71 +9.28 90.00 94.79 +28.34 90.00 93.75 +20.01 +13.68 73.33 71.88 Closed-source models 55.21 +30.31 59.38 +26.56 95.83 +16.45 76.04 +2.24 80.21 +23.48 79.17 +30.05 93.75 +29. 73.33 50.00 76.67 76.67 70.00 73.33 96.67 10.34 20.69 75.86 24.14 55.17 41.38 58.62 +2.08 0.00 +5.62 +1.04 +3.12 +6.04 +4.79 +3.75 -1.45 -18.12 +9.38 +19.16 -0.63 +10.21 +5.84 -2.92 71.13 95.88 93.81 0.00 77.32 67.01 93.81 92.78 64.95 47.42 74.23 74.23 36.08 73.20 50.52 62. 0.00 52.63 +18.50 89.47 +6.40 68.42 +25.39 0.00 47.37 +29.95 36.84 +30.17 78.95 +14.87 63.16 +29.63 36.84 +28.11 10.53 +36.90 36.84 +37.39 26.32 +47.91 15.79 +20.29 63.16 +10.04 -2.12 52.63 -5.53 68.42 76.92 93.10 90.45 0.27 44.03 50.40 90.98 71.35 52.79 45.36 51.46 88.59 45.09 76.27 70.82 81.70 64.52 +12.41 94.62 -1.52 69.89 +20.56 0.00 +0.27 33.33 +10.70 43.01 +7.39 76.34 +14.64 62.37 +8.99 41.94 +10. 34.41 +10.95 35.48 +15.98 60.22 +28.38 40.86 +4.23 63.44 +12.83 55.91 +14.91 +7.51 74.19 drawn sketches and 244 electronic sketches, reflecting the naturally higher prevalence of freehand student solutions. Table 24 reports Acc and ebF1 on the two modalities. Overall, performance differences are relatively small across most models, suggesting that modern MLLMs are broadly robust to the style and rendering differences between handdrawn and electronic sketches. While some models achieve slightly higher accuracy on electronic sketches (e.g., Gemini-2.5-Flash and GPT-5), the average gap is modest and not consistently positive across models. In fact, several models exhibit small negative deltas, indicating that the cleaner appearance of electronic sketches does not always translate into easier grading. F.4 Exact vs. Alternative Correct Answers To examine whether models can verify semantic correctness beyond strict reference matching, we further divide all correct student answers into two subgroups: (i) reference-aligned correct answers that closely follow the canonical reference diagram, and (ii) reference-divergent correct answers that remain valid but differ from the reference (e.g., through structural deformation, alternative construction strategies, or equivalent circuit layouts). Since both subgroups are correct, we report only correctness-level performance (Acc) in this analysis. expected, most correct answers are referencealigned, and alternative correct solutions are less frequent. Nevertheless, the Alt subset remains sizable for meaningful comparison. Table 26: Distribution of correct-answer subtypes. Counts and proportions of reference-aligned (Exact) and reference-divergent (Alt) correct sketches in each domain. Domain #Exact #Alt Exact (%) Alt (%) Physics Geometry Chart Flowchart Overall 93 91 96 97 377 15 29 30 19 86.1 75.8 76.2 83.6 80.2 13.9 24.2 23.8 16.4 19.8 Table 25 compares model accuracy on these two subgroups. Most models achieve consistently higher accuracy on reference-aligned answers, while performance drops noticeably on reference-divergent but still correct solutions. This effect is especially pronounced in Physics and Flowchart, where several strong models exhibit large gaps (e.g., Claude-3.7-Sonnet: 52.47 in Physics and 47.91 in Flowchart; o3: 29.46 in Physics). In contrast, the performance on Chart shows smaller or even negative gaps for some models (e.g., Gemma-3-4B-it and GPT-5), suggesting that chart correctness is less sensitive to stylistic or structural variation. Table 26 summarizes the distribution of the two correct-answer subtypes across domains. As"
        },
        {
            "title": "A plausible explanation is that",
            "content": "referencedivergent solutions require models to abstract 24 away from surface-level visual similarity and instead verify structural equivalence (e.g., topological consistency in flowcharts or physically equivalent circuit layouts), which remains challenging under freehand noise. By comparison, charts follow more rigid graphical conventions, so alternative correct forms often preserve key geometric cues (axis alignment, bar/line placement), reducing the reliance on canonical matching. F.5 Case Studies of Model Responses We provide qualitative examples that visualize model grading responses alongside the gold annotations, highlighting where models succeed, where they fail, and how different error types manifest in real student sketches Figure 11: sample error case in chart. 26 Figure 12: sample correct case in flowchart. 27 Figure 13: sample error case in flowchart. Figure 14: sample error case in physics. 29 Figure 15: sample correct case in chart. Figure 16: sample correct case in physics. 30 Figure 17: sample error case in geometry. 31 Figure 18: sample error case in geometry. 32 Figure 19: sample error case in geometry. 33 Figure 20: sample error case in chart. 34 Figure 21: sample error case in chart. 35 Figure 22: sample error case in geometry. 36 Figure 23: sample correct case in physics."
        }
    ],
    "affiliations": [
        "School of Artificial Intelligence, Beijing Normal University, Beijing, China"
    ]
}