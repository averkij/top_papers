{
    "paper_title": "Quantifying the Gap between Understanding and Generation within Unified Multimodal Models",
    "authors": [
        "Chenlong Wang",
        "Yuhang Chen",
        "Zhihan Hu",
        "Dongping Chen",
        "Wenhu Chen",
        "Sarah Wiegreffe",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two \"unified\" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration."
        },
        {
            "title": "Start",
            "content": "Chenlong Wang* Yuhang Chen* Zhihan Hu* Dongping Chen1 Wenhu Chen2 Sarah Wiegreffe1 Tianyi Zhou3, 2University of Waterloo 1University of Maryland 3MBZUAI 6 2 0 2 2 ] . [ 1 0 4 1 2 0 . 2 0 6 2 : r Figure 1. Overview of GAPEVAL. (a) We present high-quality bidirectional benchmark specifically designed for UMMs to evaluate and qualify the inherent gap between understanding and generation. (b) Our experiments extend 9 UMMs across various architectures, revealing the significant gap between the two capabilities. (c) We further conduct an in-depth empirical study from perspective of knowledge manipulation, highlighting the significant gap in the knowledge level."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in unified multimodal models (UMM) have * Equal Contribution, Independent Researcher. Corresponding Author. demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within single model remains unclear. To investigate this question, we introduce GAPEVAL, bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two unified directions. Each question can be answered in both modalities (image and text), enabling symmetric evaluation of models bidirectional inference capability and cross-modal consistency. Experiments reveal persistent gap between the two directions across wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration. 1. Introduction Building on the success of large language models (LLMs) [1, 46], the field of multimodal intelligence has advanced towards more general-purpose modeling paradigm, unified multimodal models (UMMs), which aim to empower single model with both understanding (i.e., reasoning over both text and image inputs to generate textual response) and generation capabilities (i.e., reasoning over textual input to produce an image output) [4, 41, 62] UMMs have recently gained increasing attention, not only for their elegant architectures and broad functionalities, but also for the potential synergetic interaction between the two capabilities. Recent progress in UMMs [8, 16] exhibits exceptional performance in both tasks. While beyond the engineering-level achievement, fundamental question arises: Are understanding and generation truly integrated within UMMs, or do they merely coexist as separate components? Addressing this question requires more fine-grained investigation. As summarized in Table 1, most UMMs continue to be evaluated on single-direction benchmarks (either understanding or generation) (e.g., MMMU [67], MMBench [28], GenEval [14]), thereby overlooking the intrinsic synergy that defines unification. Although recent evaluation frameworks have begun moving from separate assessments towards more comprehensive and unified schemes, they have yet to genuinely evaluate the gap between the two capabilities. As shown in Table 1, UniEval [26] and MMEUnify [64], despite covering both abilities, still fall short of assessing whether the synergy between generation and understanding can be effectively harnessed to solve complex tasks. More recent efforts, such as RealUnify [38] and GIR-Bench [23], move closer to this goal by emphasizing cross-task synergy, providing valuable testbeds for assessing overall performance. Nevertheless, their formulations remain largely task-bound, offering limited capacity for explicitly decoupling understanding and generation or for quantifying such performance gap. To fill this gap, we propose GAPEVAL, bidirectional benchmark specifically designed to analyze and quantify the gap between this performance disparity in UMMs. Different from the single-direction question, each question in GAPEVAL is formulated in bidirectional manner, which can be answered in image or text. This design establishes fair and symmetric testbed for UMMs. Based on these settings, we further propose the Gap Score, principled metric grounded in Multidimensional Item Response Theory (MIRT) [10, 36]. This score provides direct and interpretable qualification of the understanding-generation gap based on model performance on GAPEVAL. Additionally, our benchmark encompasses four categories, including Instruction Following, Numerical Perception, World Knowledge , and Reasoning, and consists of 646 high-quality questions, carefully curated from manual design or revised based on existing datasets (reasoning subset only) [51, 67]. Together, these features establish comprehensive, rigorous, and bidirectional evaluation framework for studying and narrowing the gap between the two capabilities within UMMs. For further analysis, we evaluate nine representative UMMs alongside four understanding-only and two generation-only models, covering diverse architectures and parameter scales. Experimental results on GAPEVAL reveal persistent significant performance gap. Most UMMs can only correctly respond to the question in one form, but fail to leverage the same underlying knowledge when the modality is switched. Even the state-of-the-art UMMs, such as Bagel [8], still exhibit less unified. It suggests that better performance is not equivalent to the higher-level unification. On the other hand, non-unified models often surpass them across various tasks, especially on reasoning, revealing the fact that the current unification frameworks fail to enhance mutually, and, in some cases, diminish the models performance. Specifically, Omnigen2 [59], which is built upon the FLUX.1-dev [21], underperforms its backbone diffusion. These results emphasize merely functional coupling, not the cognitive unification, within existing models. However, how to achieve the true unification remains challenge. In this study, we take step further towards exploring the underlying mechanism behind this gap. Many previous studies have emphasized the mutual enhancement between understanding and generation, especially on how stronger understanding can improve generation performance. In particular, Chain-of-Thought (CoT) plays key role in advanced reasoning [17, 43, 48, 65], and has been shown to enhance the generation as well [8]. However, these observations largely focus on explicit reasoning processes. Little attention has been given to whether the models intrinsic capability itself can implicitly facilitate or strengthen the other capability. In other words, it remains unclear whether UMMs can inBenchmark Size Category Annotation Und. Task WK. RS. VP. Gen. Task IF WK RS Features SYN. BI. GQ. MMMU [67] MMBench [28] 11,550 3,217 GenEval [14] DPG-Bench [18] T2I-CoReBench [25] WISE [31] RealUnify [38] GIR-Bench [23] UniEval [26] MME-Unify [64] Uni-MMMU [71] GAPEVAL 553 1,065 1,080 1, 1,000 970 1,234 4,104 885 646 I2T I2T T2I T2I T2I T2I Unified Unified Unified Unified Unified Unified Human Mixed Mixed (M)LLM (M)LLM Mixed Human Human (M)LM Mixed Mixed Human Table 1. Comparison of benchmarks adapted to UMMs. I2T: Image-to-Text. T2I: Text-to-Image. WK.: World Knowledge, RS: Reasoning, VP.: Visual Perception, IF.: Instruction Following. SYN.: Evaluate the synergy effect within UMMs. BI.: Bidirectional formulation. GQ: Gap Quantification between understanding and generation. ternally transfer or reinforce knowledge across modalities without explicit guidance. To investigate this question, we conduct series of finetuning experiments from knowledge-oriented perspective. Specifically, we inject or edit knowledge entities within the UMMs through single-sided manipulation (either on understanding or generation), and then evaluate their performance across both sides (understanding and generation). Empirically, our results reveal pronounced misalignment between the two capabilities. Fine-tuning on one side often has little effect on the other side. In the case of knowledge editing, outdated knowledge tends to persist, indicating failure of synergistic updating. Similarly, knowledge injection frequently introduces new inconsistencies, as models struggle to consistently apply the newly injected knowledge across modalities. Moreover, the emergence of different modalities is remarkably unsynchronized, These findings suggest that, despite architecture unification, the underlying knowledge representations in current UMMs remain largely decoupled, highlighting the importance of the next-level unification. 2. GAPEVAL: Quantifying the Gap between Understanding and Generation In this section, we introduce novel evaluation framework, GAPEVAL, followed by its design nature in Sec. 2.1. We then illustrate the taxonomy of GAPEVAL in . To quantify the modality gap within unified models, we propose specific metric, Gap Score, for GAPEVAL in Sec. 2.3. 2.1. Motivation & Overviw texti = {und_texti, gen_texti}, where und_texti and gen_texti serve as the prompts for the understanding and generation tasks, respectively. Both prompts share the same semantic intent but differ in the response modality, such as Who is he? and Generate an image of him. The groundtruth varies across task categories. For understanding tasks, the ground-truth yu is reference textual answer. For generation tasks, the answer yg consists of reference image accompanied by descriptive text, which serves as the basis for model evaluation. This design allows us to evaluate model performance on identical underlying knowledge and quantify the misalignment between its understanding and generation capabilities. Further analyses are in Appendix C. 2.2. Taxonomy Instruction Following is fundamental ability for UMMs, as real-world multimodal tasks often require not only understanding textual instruction but also executing it consistently across modalities. Motivated by this, we design this task to extend beyond conventional image-editing settings, which typically focus only on generating an edited image. In GAPEVAL, each question incorporates two types of challenges: explicit edits that require models to articulate changes to specific image regions during the understanding phase, and implicit edits that further examine the models ability to follow underlying rules. This formulation allows us to evaluate not only whether UMMs can follow instructions, but also whether their textual and visual executions remain aligned, thereby revealing the cross-modal consistency of instruction-grounded editing. GAPEVAL is bidirectional benchmark in which each question can be answered through either text or image. To achieve this, each item consists of question image imagei (or None) and corresponding text instruction pair Numerical Perception focuses on evaluating models ability to correctly interpret and manipulate quantitative information with image and text. Rather than treating counting in different modalities as two separate tasks, GAPEVAL couFigure 2. Illustrative examples from GAPEVAL. The generated texts and images are shown with the ground truth (texts, images or both). GPT Family denotes the GPT5-mini (for text generation), and the GPT-Image-1 (for image generation). ples them through structured numerical transformation. Each question presents an image containing two categories of objects (e.g., three ducks and two birds) and instructs the model to identify their quantities and then swap them, producing two ducks and three birds in text or generating corresponding image. This formulation prevents the model from simply copying the input and instead evaluates whether it can accurately perceive numerical structure and consistently express the corresponding quantitative modification across modalities. World Knowledge focuses on models ability to recognize, reason, and apply broad world knowledge when grounded in visual inputs. Rather than surface-level pattern matching, items in this subset require the model to correctly interpret semantic concepts (e.g., Generate photo for the author of One Hundred Years of Solitude., instead of Generate photo for Gabriel García Márquez.). Our task spans six diverse subdomains, including animals, plants, landmarks, instruments, literature, and culture, ensuring comprehensive coverage of factual and conceptual knowledge. By requiring the model to map visual evidence to the appropriate realworld entities, this task evaluates whether UMMs possess coherent and transferable representation of world knowledge with different modalities. Reasoning assesses models ability to comprehend textual and visual reasoning problems and deliver answers through both textual descriptions and visual renderings. This requires models to internalize knowledge inference, geometric understanding, and image generation, which constitute essential components of human-like visual reasoning. We define five representative subcategories: (1) Image Selection evaluates whether models can infer the resulting scenario described in the task based on algorithmic diagrams and game layouts. (2) Knowledge Selection renders knowledge reasoning content from MMMU [67] and MMLU [51] into images to assess the consistency between the models image-based knowledge reasoning and its visual generation capabilities. (3) Real-world Reasoning establishes fundamental physical contexts in real-world scenarios to evaluate the models understanding and generation capabilities under realistic conditions. (4) Logical Reasoning provides broad range of logical tasks spanning abstract levels and difficulty scales, requiring models to perform visual symbolic reasoning that bridges perception with inference. Together, these subcategories comprehensively evaluate models reasoning ability, forming unified measure of holistic visual reasoning. 2.3. Metric Design To comprehensively evaluate and quantify the understanding and generation capabilities of UMMs, as well as the gap between these two abilities, we introduce two-stage evaluation methodology as follows: First Stage: Capability Measurement. To assess the correctness of outputs, we employ GPT-5-mini [43] as the judge model for subjective evaluation. For each question, the reference answers are provided to the judge model. Each response is assigned binary label: correct (1) or incorrect (0) for further analysis. To ensure the reliability, we validate the judges decisions on held-out set of 200 samples, achieving 92% agreement rate with human annotations. The reliability analysis of MLLM-as-a-Judge[3] are available in Appendix B. Second Stage: Capability Gap Measurement. The past averaging metrics across tasks implicitly assume all items have equal difficulty, thereby neglecting the latent variable of item difficulty. When model fails both tasks on certain items, these failures may arise from high item difficulty, not just limited model capability. To address this, we adopt multidimensional Item Response Theory (MIRT) framework [10, 36], which explicitly introduces item difficulty and model ability as latent variables. MIRT assigns each model latent ability vector θi and each task (understanding or generation) latent difficulty parameter β. It embeds both model ability and item difficulty into continuous latent space, where the difference between these two is mapped by smooth sigmoid function to determine the final probability of success. Thus, MIRT continuously and sensitively reflects the nuanced performance variations between different model levels and varying task difficulties. Meanwhile, MIRT employs Bayesian maximum posteriori (MAP) optimization over the parameter space, allowing the model to automatically distinguish which failures are due to limited model ability and which are attributable to particularly difficult items. We additionally introduce penalty term to encourage the MIRT model to maintain consistency between the two tasks when it is capable of completing them. max Θ, β, µ, Σ LMAP = (cid:88) (cid:104) (cid:88) {und, gen} log σ(θ(T ) βT ) 1 2 (θi µ)Σ1(θi µ) (cid:105) , (1) where σ() is the sigmoid function, θ(T ) is the ability of model on task , and βT is the corresponding difficulty. The MAP objective ensures that individual ability and item difficulty are jointly fitted from observed results, overcoming biases of averaging. The final capability gap is normalized between 0 and 100. We provide more details on how the gap is computed in Appendix A. 2.4. Data Collection and Annotation We employ systematic, multi-stage data collection that integrates human curation with automated generation to ensure high-quality and diverse tasks. Each task undergoes rigorous peer review to maintain consistency, clarity, and reliability. Further details are provided in Appendix D. 3. Evaluation Results on GAPEVAL To diagnose the performance and modality gap within current UMMs, we conduct extensive experiments on GAPEVAL. We first illustrate the experiment setup in Sec. 3.1, followed by in-depth analyses in Sec. 3.2. 3.1. Experiment Setup Our evaluation primarily focuses on UMMs, along with cutting-edge understanding-only and generation-only baselines. For UMMs, we select 7 representative opensource models, including Bagel-7B [8], OneCAT-3B [24], UniWorld-V1 [27], UniPic2-Mataquery-9B [54], Showo2 [62, 63], OmniGen2 [59], and Ovis-U1 [49]. Additionally, we also evaluate state-of-the-art closed-source UMMs (Gemini2.5 Flash Image [16] and GPT-Image-1 [42]). These models represent diverse model architectures, providing comprehensive evaluation setting. For non-UMMs, our experiments include models from GPT, Gemini, Flux, and Qwen series. Specifically, we evaluate Qwen-Image [57] and FLUX.1-dev [21, 22] as image-generation models, and assess GPT5-mini [43] , Gemini2.5 Flash [15], and Qwen3-VL series [44] as multimodal large language models (MLLM). Further experimental details are provided in Appendix 3.2. Experiment Results Finding 1: Unifying both modalities does not necessarily yield balanced cross-modal performance. Critical Performance Gap. As shown in Table 2, clear performance disparity emerges between understanding and generation within UMMs. large portion of samples can only be correctly answered in one modality, indicating that many UMMs still fall short of achieving cross-modal consistency. Notably, Omnigen2, which adopts hybrid LLM + Diffusion architecture with FLUX.1-dev as its diffusion backbone, performs markedly worse on generation tasks than FLuX.1-dev itself, especially on World Knowledge and Model World Knowledge Numerical Perception Instruction Following Reasoning Succ. Und. Gen. Gap Succ. Und. Gen. Gap Succ. Und. Gen. Gap Succ. Und. Gen. Gap Gap Open-source UMM Bagel OneCAT UniWorld-V1 UniPic2 Show-o2 OmniGen2 Ovis-U1 Ming-UniVision EMU3.5 52.24 57.14 87.84 39.29 49.32 29.93 37.82 25.00 47.68 88.17 88.26 93.28 79.60 86.31 86.22 87.34 78.64 60.12 58.50 62.92 92.74 41.16 55.84 35.54 43.13 31.03 58. 56.67 33.14 12.60 65.23 51.02 81.57 75.06 73.25 14.03 2.40 2.75 1.00 1.75 7.00 2.00 1.00 0.00 0.00 17.00 9.00 10.60 15.00 16.16 9.25 13.00 6.90 3.00 8.40 9.75 3.60 7.50 15.15 3.50 3.00 0.00 0.00 84.87 62.33 84.68 82.37 71.24 91.55 91.51 94.87 93.24 46.38 23.67 28.51 51.86 21.28 3.99 29.36 26.47 33. 59.15 50.00 62.13 66.75 70.22 9.58 68.51 72.37 76.92 75.74 41.22 46.60 58.77 32.98 55.59 44.04 35.71 35.71 57.38 37.51 70.46 45.69 70.77 92.36 77.86 70.47 73.23 2.49 0.66 1.11 7.86 2.63 1.23 1.70 1.82 1.38 35.67 20.66 36.78 34.18 22.37 17.21 25.37 10.19 11.01 5.38 1.56 1.44 3.93 3.62 4.43 2.62 1.82 7. 87.14 85.36 86.11 82.61 78.31 92.40 92.00 82.90 80.68 71.52 54.73 63.47 68.97 67.83 89.47 84.10 80.37 65.30 Closed-source UMM Gemini2.5-Flash-Image GPT-Image-1 59.18 66.67 94.55 95. 62.58 68.03 51.47 44.45 0.00 7.00 19.00 25.00 1.00 10.00 87.08 83. 64.89 86.17 73.40 95.74 77.66 90.43 30.17 20.83 12.83 50.98 61.32 73. 30.50 56.07 81.77 53.29 62.91 50.61 GPT5 GPT5-mini Qwen3-VL 235B-A22B Qwen3-VL 8B Gemini2.5-Flash FLUX.1-dev Qwen-Image - - - - - - - 96.52 97.28 96.60 94.30 97.96 - - - - - - - 57.14 57.82 - - - - - - - Understanding-only Model - - - - - - - 24.05 30.30 26.00 25.00 17.00 - - - - - - - - - - Generation-only Model - - 0.00 1.00 - - - - - - - - - 95.45 93.60 85.11 82.97 85.11 - - - - - - - 74.46 76.59 - - - - - - - - - - - - - - 74.74 73.80 60.20 53.38 77.05 - - - - - - - 4.93 4.59 - - - - - - - - - - - - - - Table 2. Evaluation results across 9 UMMs and 6 non-UMMs on GAPEVAL. Succ.: Success Score, denotes correctly answering with both image and text. Und.: Understanding Score, denoting the accuracy of answering with texts. Gen.: Generation Score, denoting the accuracy of answering with images. Fail denotes failure to answer in neither text nor image. Gap denotes the gap between understanding and generation. Bold & Underline: bests & second bests. at the cost of balanced overall performance. Finding 2: Good performance does not necessarily correspond to high unification. Performance & Unification. Beyond the significant gap score, there is an interesting phenomenon where higher performance does not always imply stronger unification. Some models achieve superior results but exhibit wider modality gaps. For instance, although OneCAT performs worse than Bagel across several tasks, its smaller gap score suggests better modality integration. In contrast, UniWorld-V1 attains both high overall accuracy and small gaps, implying that effective unification arises from intentional architectural or training designs rather than general performance gains. These experimental results reveal decoupling between the state-of-the-art performance and the degree of unification. Finding 3: performance-lagging effect exists across models: the gap score initially increases for lowerperforming models but decreases once the overall capability reaches relatively high level. Transition in Unification Across Performance Levels. notable trend in Fig. 3 is the performance-lagging effect, where the modality gap first widens then narrows across capability tiers. Low-capability models (e.g., OneCAT, UniPic2) exhibit small gaps not from better multimodal unification, but because both modalities fail simultaneously. At moderate Figure 3. Gap score heatmap over understanding and generation performance. (Und., Gen., Gap Score) data points are plotted on the heatmap, reflecting the relation between three dimensions. The trend curve demonstrates the model performance distribution. Instruction Following. This suggests that unifying output modalities within single model can, in some cases, dilute task-specific strengths, exposing an inherent trade-off between understanding and generation. Overall, models tailored for understanding consistently outperform UMMs on comprehension-oriented tasks. Conversely, on generation tasks, closed-source UMMs surpass the performance of generation-only baselines. These results imply that unified architectures can partially transfer reasoning ability from understanding to generation, though often 4.2. Formulation In this study, knowledge entity is defined as tuple [30], (subject, relation, object), where both subject and object can take the form of either text or image. For example, the knowledge expressed by the sentence The capital of France is Paris. can be formulated as (T he capital of rance, is, aris). This formulation allows us to unify the representation of multimodal knowledge entities and conveniently construct both training and evaluation sets. In this work, we focus on the relations between vision and language, so we mainly use images and text pointing to the same object as (subject, object) pairs. 4.3. Experiment Setup Model Selection. Current UMMs have adopted diverse architectures. We select three representative models from three different architectures: Show-o [62, 63], Omnigen2 [59], and Bagel [8] for subsequent experiments. Data Collection. Our experiments involve two types of knowledge manipulation: injection and editing. We define relation as the mapping between textual object and its corresponding visual representation. For injection, we select knowledge instances that models fail to answer correctly in either understanding or generation. For editing, we modify grounded captions from GenEval [14] with incorrect but same-category replacements (e.g., banana apple). In total, about 100 knowledge instances are processed and reformulated into Image-to-Text and Text-to-Image task formats, with 20% reserved as held-out test set. Metric. We use GPT5-mini [43] as the evaluation model for the understanding task across knowledge injection and edit, and the generation task on knowledge edit. For the generation task of knowledge injection, we use CLIP [35] to compare the similarity between the generated image and the reference image. 4.4. Empirical Results and Analysis Finding 4: The knowledge within UMMs remains disjoint across modalities. Unbalanced training leads to severe cross-modal misalignment, preventing consistent knowledge updates. Knowledge misalignment during the training stage. Fig. 6 shows clear performance gap between understanding and generation after fine-tuning. Training on one modality fails to generalize to the other, revealing that the two capabilities rely on distinct knowledge representations. The gap is most evident in knowledge editing. Specifically, fine-tuning on understanding drastically boosts understanding scores (e.g., 0.62 for Bagel and 0.56 for OmniGen2) but leaves generation nearly unchanged, while training on generation (0.89 for Show-o) yields the opposite pattern. similar, though Figure 4. Training data case gallery. capability levels (Bagel, Show-o2), improvement emerges earlier and more substantially in understanding than generation, producing asymmetric maturation that temporarily increases the modality gap. High-performance models (GPTImage-1, Gemini2.5-Flash-Image) develop robust joint reasoning across modalities, gradually reducing the gap. 4. UNIFIED KNOWLEDGE: Empirical Analysis on the UMMs Gap To further investigate the underlying mechanism behind the gap between understanding and generation, we conduct series of empirical studies from the perspective of knowledge manipulation. 4.1. Motivation The evaluation on GAPEVAL highlights clear disparity between understanding and generation. Although UMMs can often recall embedded knowledge within one modality, they struggle to apply the same knowledge consistently across modalities. This raises key question: How does knowledge evolve during the training phase? As shown in Table 2, the results quantitatively confirm that current UMMs lack genuine cross-modal consistency. To probe this issue, we conduct knowledge-oriented finetuning experiments by injecting or editing knowledge in one modality and evaluating its effects on both. If knowledge is shared, changes should propagate; if isolated, the effects should remain local. Figure 5. Performance increasing over the training steps on the knowledge edit task. The figure has also exhibited the output of models from different training stages. The knowledge entity involved here is (Microwave Oven->Rice Cooker). only few thousand steps, while generation progresses much more slowly. This early divergence mirrors the performance lagging phenomenon in Fig. 3. During this stage, the gap between the two modalities expands rapidly, indicating that the model primarily updates its textual reasoning components rather than the generative ones. As training continues, the generation curve gradually climbs and eventually approaches the performance level of understanding. This delayed improvement suggests that the generative pathway requires longer optimization to internalize and express the newly edited knowledge. In contrast, understanding tasks benefit from more direct supervision and lower representational complexity, allowing them to converge earlier. The asymmetric convergence dynamics confirm that the two capabilities rely on distinct update mechanisms, with generation requiring substantially larger training budget for effective knowledge adaptation. 5. Related Works 5.1. Unified Multimodal Models Unified Multimodal Models (UMM) [2, 4, 68, 11, 19, 20, 24, 27, 29, 32, 34, 37, 41, 45, 50, 53, 54, 56, 61, 62, 70] aim to construct new generation of more general-purpose models, capable of processing multimodal inputs and performing cross-modal understanding and generation. To realize the unified capability, Chamelong [41] and UniPic [54] adopt an early-fusion, token-based transformer that enables interchangeable text and image output. The BLIP-3o [4, 5] and UniWorld [27] follow an LLM+diffusion architecture, where the LLM encodes multimodal inputs and passes the latent representations to diffusion module for image generation. The Show-o [62, 63] integrates the autoregressive modeling with discrete diffusion, using next-token predicFigure 6. Model performance under different training strategies. Training on one side does not affect the other side. milder, trend appears in knowledge injection, where crossmodal influence exists but remains limited. These results highlight fundamental limitation of current UMMs. Unbalanced training on one modality introduces modality-specific knowledge drift, where updates in one space fail to propagate coherently to the other. Despite the functionalities being unified, the embedded knowledge is non-unified. Finding 5: The unbalanced, slower convergence of different modalities explains the observed lag in performance and further supports the existence of modalityspecific learning dynamics within UMMs. Training expense. Fig. 5 presents the learning trajectories of Show-o during the knowledge editing experiments. The two curves represent fine-tuning on understanding (blue) and generation (red). At the beginning of training, understanding accuracy rises sharply and quickly saturates around 0.8 after tion for text understanding and mask-token prediction for image generation. OneCAT [24] employs Mixture-of-Experts (MoE) framework, while Bagel [8] pioneers Mixture-ofTransformers (MoT) design, dedicating different components to the autoregressive text generation and diffusionbased visual generation. These models explore different architectures to empower models with unified capabilities. 5.2. Evaluation for Unified Multimodal Models Research on UMMs has gained increasing attention. While current UMMs demonstrate remarkable performance on both understanding [9, 13, 28, 47, 67] and generation [12, 14, 18, 33, 39, 40, 52, 55, 60, 66, 69] tasks, these benchmarks are classic testbeds for either understanding-only or generationonly models, without considering their integration. To evaluate UMMs, T2I-CoReBench [25] and WISE [31] have been proposed, yet primarily focusing on text-to-image (T2I) generation, providing limited insights into whether understanding and generation capabilities can mutually enhance each other. More comprehensive efforts, such as RealUnify [38] and GIR-Bench [23], take step further by integrating both skills into unified evaluation setting, where models must leverage strong understanding and generation capabilities to succeed. However, despite these advances, there remains lack of dedicated evaluation framework to measure whether UMMs truly achieve reciprocal fusion of understanding and generation, rather than merely combining both functionalities at an engineering level. To this end, we propose GAPEVAL, high-quality bidirectional benchmark specifically designed for quantifying the inherent gap of different capabilities in UMMs. 6. Conclusion In this paper, we introduce GAPEVAL, bidirectional benchmark to reflect and qualify the gap between understanding and generation within UMMs. Benchmarking the SOTA UMMs across diverse architectures, we find that current models merely achieve an engineering-level unification. Subsequent experiments explore such gap via knowledge manipulation tuning, revealing the knowledge decoupling between two capabilities in models. These findings lay the foundation for the next-level unification for UMMs."
        },
        {
            "title": "References",
            "content": "[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 2 [2] Dongping Chen, Ruoxi Chen, Shu Pu, Zhaoyi Liu, Yanru Wu, Caixi Chen, Benlin Liu, Yue Huang, Yao Wan, Pan Zhou, et al. Interleaved scene graphs for interleaved text-and-image generation assessment. In Proceedings of International Conference on Learning Representations, 2024. 8 [3] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yaochen Wang, Yinuo Liu, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llmas-a-judge with vision-language benchmark. In Proceedings of Forty-first International Conference on Machine Learning, 2024. 5 [4] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. 2, 8 [5] Jiuhai Chen, Le Xue, Zhiyang Xu, Xichen Pan, Shusheng Yang, Can Qin, An Yan, Honglu Zhou, Zeyuan Chen, Lifu Huang, et al. Blip3o-next: Next frontier of native image generation. arXiv preprint arXiv:2510.15857, 2025. 8 [6] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 8 [7] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, Hui Ding, Zhe Lin, and Hengshuang Zhao. Unireal: Universal image generation and editing via learning realworld dynamics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1250112511, 2025. [8] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 2, 5, 7, 8, 9 [9] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pages 1119811201, 2024. 9 [10] Susan Embretson and Steven Reise. Item response theory for psychologists. Psychology Press, 2013. 2, 5 [11] Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. Proceedings of International Conference on Learning Representations, 2024. 8 [12] Rongyao Fang, Aldrich Yu, Chengqi Duan, Linjiang Huang, Shuai Bai, Yuxuan Cai, Kun Wang, Si Liu, Xihui Liu, and Hongsheng Li. Flux-reason-6m & prism-bench: millionscale text-to-image reasoning dataset and comprehensive benchmark. arXiv preprint arXiv:2509.09680, 2025. 9 [13] Mingyang Fu, Yuyang Peng, Benlin Liu, Yao Wan, and Dongping Chen. Livevqa: Live visual knowledge seeking. arXiv preprint arXiv:2504.05288, 2025. [14] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-toimage alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. 2, 3, 7, 9 [15] Gemini Team Google. Gemini 2.5 flash. https:// deepmind.google/models/gemini/flash/, 2025. Accessed October 2025. 5 [16] Gemini Team Google. Gemini 2.5 flash image. https: //aistudio.google.com/models/gemini-2-5flash-image, 2025. Accessed October 2025. 2, 5 [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2 [18] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. 3, 9 [19] Yang Jiao, Haibo Qiu, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. Unitoken: Harmonizing multimodal understanding and generation through unified visual encoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 36393649, 2025. 8 [20] Yang Jin, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song, et al. Unified language-vision pretraining in llm with dynamic discrete visual tokenization. arXiv preprint arXiv:2309.04669, 2023. [21] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2, 5 [22] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Müller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space. 2025. 5 [23] Hongxiang Li, Yaowei Li, Bin Lin, Yuwei Niu, Yuhang Yang, Xiaoshuang Huang, Jiayin Cai, Xiaolong Jiang, Yao Hu, and Long Chen. Gir-bench: Versatile benchmark for generating images with reasoning. arXiv preprint arXiv:2510.11026, 2025. 2, 3, 9 [24] Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, and Hongkai Xiong. Onecat: Decoder-only auto-regressive arXiv model for unified understanding and generation. preprint arXiv:2509.03498, 2025. 5, 8, 9 [25] Ouxiang Li, Yuan Wang, Xinting Hu, Huijuan Huang, Rui Chen, Jiarong Ou, Xin Tao, Pengfei Wan, Xiaojuan Qi, and Fuli Feng. Easier painting than thinking: Can text-to-image models set the stage, but not direct the play? arXiv preprint arXiv:2509.03516, 2025. 3, 9 [26] Yi Li, Haonan Wang, Qixiang Zhang, Boyu Xiao, Chenchang Hu, Hualiang Wang, and Xiaomeng Li. Unieval: Unified holistic evaluation for unified multimodal understanding and generation. arXiv preprint arXiv:2505.10483, 2025. 2, 3 [27] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 5, 8 [28] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an allaround player? In European conference on computer vision, pages 216233. Springer, 2024. 2, 3, 9 [29] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. 2025. 8 [30] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in neural information processing systems, 35:17359 17372, 2022. 7 [31] Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. 3, 9 [32] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [33] Yuandong Pu, Le Zhuo, Songhao Han, Jinbo Xing, Kaiwen Zhu, Shuo Cao, Bin Fu, Si Liu, Hongsheng Li, Yu Qiao, et al. Picabench: How far are we from physically realistic image editing? arXiv preprint arXiv:2510.17681, 2025. 9 [34] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel K. Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for mulIn Proceedings of timodal understanding and generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 25452555, 2025. 8 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 7, 4 [36] Georg Rasch. Probabilistic models for some intelligence and attainment tests. ERIC, 1993. 2, 5, 1 [37] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. [38] Yang Shi, Yuhao Dong, Yue Ding, Yuran Wang, Xuanyu Zhu, Sheng Zhou, Wenting Liu, Haochen Tian, Rundong Wang, Huanqian Wang, et al. Realunify: Do unified models truly benefit from unification? comprehensive benchmark. arXiv preprint arXiv:2509.24897, 2025. 2, 3, 9 [39] Kaiyue Sun, Rongyao Fang, Chengqi Duan, Xian Liu, and Xihui Liu. T2i-reasonbench: Benchmarking reasoning-informed text-to-image generation. arXiv preprint arXiv:2508.17472, 2025. 9 [40] Peter Sushko, Ayana Bharadwaj, Zhi Yang Lim, Vasily Ilin, Ben Caffee, Dongping Chen, Mohammadreza Salehi, ChengYu Hsieh, and Ranjay Krishna. Realedit: Reddit edits as large-scale empirical dataset for image transformations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1340313413, 2025. 9 [41] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 2, 8 [42] OpenAI Team. Gpt-image-1. https://platform. openai.com/docs/models/gptimage1, 2025. Accessed October 2025. 5 [43] OpenAI Team. Gpt-5: The generative pretrained transformer 5. https://openai.com/, 2025. Accessed October 2025. 2, 5, [44] Qwen Team. Qwen3 vl. https://github.com/ QwenLM/Qwen3-VL, 2025. Accessed October 2025. 5 [45] Shengbang Tong, David Fan, Jiachen Li, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1700117012, 2025. 8 [46] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: arXiv Open and efficient foundation language models. preprint arXiv:2302.13971, 2023. 2 [47] Chenlong Wang, Zhaoyang Chu, Zhengxiang Cheng, Xuyi Yang, Kaiyue Qiu, Yao Wan, Zhou Zhao, Xuanhua Shi, and Dongping Chen. Codesync: Synchronizing large language models with dynamic code evolution at scale. Proceedings of International Conference of Machine Learning, 2025. 9 [48] Chenlong Wang, Yuanning Feng, Dongping Chen, Zhaoyang Chu, Ranjay Krishna, and Tianyi Zhou. Wait, we dont need to\" wait\"! removing thinking tokens improves reasoning efficiency. arXiv preprint arXiv:2506.08343, 2025. 2 [49] Guo-Hua Wang, Shanshan Zhao, Xinjie Zhang, Liangfu Cao, Pengxin Zhan, Lunhao Duan, Shiyin Lu, Minghao Fu, Xiaohao Chen, Jianshan Zhao, et al. Ovis-u1 technical report. arXiv preprint arXiv:2506.23044, 2025. 5 [50] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 8 [51] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. 2, 4, [52] Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gpt-image-edit1.5 m: million-scale, gpt-generated image dataset. arXiv preprint arXiv:2507.21033, 2025. 9 [53] Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, and Xiaohui Zeng. Llama-mesh: Unifying 3d mesh generation with language models. arXiv preprint arXiv:2411.09595, 2024. 8 [54] Hongyang Wei, Baixin Xu, Hongbo Liu, Cyrus Wu, Jie Liu, Yi Peng, Peiyu Wang, Zexiang Liu, Jingwen He, Yidan Xietian, et al. Skywork unipic 2.0: Building kontext model with online rl for unified multimodal model. arXiv preprint arXiv:2509.04548, 2025. 5, 8 [55] Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, and Lei Zhang. Tiif-bench: How does your t2i model follow your instructions? arXiv preprint arXiv:2506.02161, 2025. 9 [56] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [57] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 5, 3 [58] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 3 [59] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, Ze Liu, Ziyi Xia, Chaofan Li, Haoge Deng, Jiahao Wang, Kun Luo, Bo Zhang, Defu Lian, Xinlong Wang, Zhongyuan Wang, Tiejun Huang, and Zheng Liu. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 2, 5, 7 [60] Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, MingHsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025. 9 [61] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1329413304, 2025. 8 [62] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 2, 5, 7, 8 [63] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. ShowarXiv o2: Improved native unified multimodal models. preprint arXiv:2506.15564, 2025. 5, 7, 8 [64] Wulin Xie, Yi-Fan Zhang, Chaoyou Fu, Yang Shi, Bingyan Nie, Hongkai Chen, Zhang Zhang, Liang Wang, and Tieniu Tan. Mme-unify: comprehensive benchmark for unified multimodal understanding and generation models. arXiv preprint arXiv:2504.03641, 2025. 2, 3 [65] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 2 [66] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, et al. Echo-4o: Harnessing the power of gpt4o synthetic images for improved image generation. arXiv preprint arXiv:2508.09987, 2025. 9 [67] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 2, 3, 4, 9 [68] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [69] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. Proceedings of Neural Information Processing Systems, 2025. 9 [70] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. Proceedings of International Conference on Learning Representations, 2024. 8 [71] Kai Zou, Ziqi Huang, Yuhao Dong, Shulin Tian, Dian Zheng, Hongbo Liu, Jingwen He, Bin Liu, Yu Qiao, and Ziwei Liu. Uni-mmmu: massive multi-discipline multimodal unified benchmark. arXiv preprint arXiv:2510.13759, 2025."
        },
        {
            "title": "Supplementary Material",
            "content": "A. Detailed Metric Implementation (MIRTA.2.2. Log-Likelihood and Prior MAP Version) The joint log-likelihood for all models is: In this section, we present the detailed formulation of our multidimensional IRT-based capability gap quantification metric, where both text understanding and image generation abilities are jointly estimated under Bayesian maximum posteriori (MAP) framework. A.1. Data Preparation Given the evaluation results from Stage I, we first aggregate binary correctness into count statistics for each model. Input Data Structure. For each model mi (i = 1, . . . , ), we collect four counts: nT nT nT nT From these, we derive marginal counts: : Text correct, Image incorrect : Text incorrect, Image correct : Both correct : Both incorrect ntext-success ntext-fail nimage-success nimage-fail = nT = nT = nT = nT i + nT + nT + nT + nT (2) (3) (4) (5) A.2. Multidimensional IRT Formulation A.2.1. Model Specification We extend the Rasch model [36] into two-dimensional IRT framework. Each model mi is associated with latent ability vector: θi = (cid:21) (cid:20) θtext θimage , β = (cid:21) (cid:20) βtext βimage . The success probabilities follow: text = image = 1 1 + exp((θtext βtext)) 1 , 1 + exp((θimage βimage)) . (6) (7) (8) L(Θ, β) = (cid:104) ntext-success (cid:88) i= log text + ntext-fail log(1 text ) + nimage-success + nimage-fail log image log(1 image (cid:105) ) , (9) where Θ = {θ1, . . . , θN }. To couple the two modalities, we impose shared multivariate Gaussian prior over θi: p(θi) = (θi µ, Σ), Σ = LL, (10) where µ is the mean vector, and is the Cholesky factor of the covariance matrix Σ. The total MAP objective is therefore: LMAP = L(Θ, β) 1 2 i=1 (cid:88) (θiµ)Σ1(θiµ) 2 logΣ. (11) This formulation allows the covariance Σ to be learned adaptively, capturing both modality-specific difficulty and cross-modality correlation. A.3. Parameter Estimation We optimize {θi}N i=1, β, µ, and jointly via gradient-based MAP estimation (e.g., Adam). The Cholesky factorization ensures Σ remains positive definite throughout training. A.4. Capability Gap and Normalization After optimization, we compute for each model: θi = θtext θimage θi 1 + θi . , (12) (13) Gabs(θi) = Here Gabs(θi) [0, 1) represents the normalized absolute capability gap. We further apply sigmoid normalization to each dimension: i,norm = σ(θtext θtext ) = 1 1 + eθtext , i,norm = σ(θimage θimage ) = 1 1 + eθimage , (14) (15) which maps ability estimates into (0, 1) for interpretability. Post-hoc rewardpenalty on the gap. To encourage consistency when the model is capable and to penalize simultaneous failures, we apply smooth logit-space adjustment to the normalized gap. Instead of using model probabilities, we use observed co-occurrence statistics: let cSS be the observed counts (or proportions) of co-success and cofailure for task i, with ni the corresponding total number of paired observations. We define the empirical rates and cFF logit( (cid:101)Gi) = logit(Gabs(θi)) +(λfail fi λsucc si) (16) so that positive λfail enlarges the gap under co-failure (penalty) and positive λsucc shrinks it under co-success (reward). Unless otherwise stated, we use the default setting λfail = λsucc = 2. B. Reliability Analysis To further analyze the model preference, we repeatedly calculate the Gap Scores using Gemini3-Flash. As shown in Tab. 3, the results from GPT5-mini and Gemini3-Flash share consistent relative ranking. Specifically, GPT-Image-1 exhibits the lowest gap score across different judge models. Judge models fail to exhibit preference for the same-family model (for Gemini2.5-Flash-Image, 62.65 by Gemini3-Flash and 62.91 by GPT5-mini). The Pearson Correlation between the two judge models is 0.9656, highlighting the significant correlation. Crucially, since most questions in GAPEVAL focus on the correctness of semantics (e.g., specific objects, theme), the evaluation targets are objective and definite, making it suitable for MLLM-as-a-Judge. This factual nature minimizes the room for model-specific preference bias compared to open-ended generation tasks. Judge Model Gemini3-Flash GPT5-mini Bagel 72.96 71.52 OneCAT UniWorld-V1 UniPic2 60.67 54.73 66.64 63.47 73.39 68.97 Judge Model Show-o2 OmniGen2 Gemini2.5-F-I GPT-Image-1 Gemini3-Flash GPT5-mini 72.33 67.83 88.87 89.74 62.65 62.91 47.47 50. Figure 7. Correlation between Gap Score and Performance on GIR-Bench. C.2. Gap Score & Synergy Effects The field of UMMs emphasizes the significance of synergy effects between two modalities. In this section, we analyze the relationship between Gap Score and Synergy Effects. We plot our Gap Score against performance on Synergy Benchmarks (GIR-Bench[23]). As shown in Fig. 7, we observe strong negative correlation. Models with lower Gap Scores consistently achieve higher synergy performance, mathematically confirming that narrowing the gap is prerequisite for real-world applications. Synergy essentially demands the complementarity of capabilities, where understanding guides generation, and generation reflects understanding. However, misalignment acts as semantic barrier to this complementarity. If the internal representations for understanding and generation are disjoint, the information cannot be effectively transferred (e.g., an editing instruction correctly understood by the encoder fails to trigger the correct generation in the decoder). Therefore, alignment is the structural prerequisite for Synergy. Our decoupled diagnosis pinpoints such gaps, providing controllable lever for improvement. Table 3. Gap Score Results from Different Judge Models. D. Data Construction Details C. Contribution C.1. Contribution We argue that Synergy relies on Alignment. Unlike blackbox synergy evaluations, which mix capabilities, our decoupled design offers visible and interpretable diagnosis of intrinsic modality gaps. This makes the evaluation controllable and clear, serving as fundamental step to improve complex synergy. D.1. Benchmark Data Construction World Knowledge. We curate and rewrite knowledge entities from the public web and open-source datasets. For each entity, we ensure appropriate difficulty and that the target outcome can be expressed through both text and image. We author paired prompts for understanding and generation, and apply strict cross-checking workflow to continually refine entities and prompts. Numerical Perception. We select target object sets such as animals and vehicles, and ensure that instances of the same category appear within single image. We use an automatic data engine to generate corresponding prompts. Images for the counting task are generated with Qwen-Image [57]. Annotators screen image quality and discard low-quality samples. For images whose object quantities do not match the targets, we rewrite the prompts and questions so that the final items precisely enforce the counting requirement. Instruction Following. We mine items with substantial visual changes from open-source datasets and manually construct rule-based editing prompts. For textual outputs, we guide models to describe the edited image and explicitly identify the modified regions. All items undergo strict crosschecking to ensure quality and factual correctness. Reasoning. The reasoning set contains three subsets collected through complementary pipelines. First, the realworld reasoning covers the understanding of physical phenomena and the generation of schematic illustrations. All prompts, reference texts, and reference images are manually written and drawn with consistent visual style. Second, multiple choice reasoning is built by rendering textual problems from MMLU [51] and MMMU [68] into images. We also design algorithmic reasoning items (e.g., binary tree preorder traversal and topological sorting) and produce accompanying diagrams. Each problem has unique answer and consistent solution key, with reference images highlighted within the answer options. Third, the image state transition reasoning task is collected from the web and open-source sources. Given several images, the task asks the model to infer and render the next state while remaining faithful to the observed evidence, which poses higher level of difficulty. We conduct cross-review and remove items that are excessively difficult. D.2. Empirical Study Data Collection To rigorously investigate the behavior of Unified Multimodal Models (UMMs) under knowledge manipulation, we constructed two distinct datasets tailored for Knowledge Injection (introducing novel concepts) and Knowledge Editing (altering existing conceptual associations). The data collection and construction process is detailed below. Object Selection and Image Collection. For the Knowledge Injection task, we aimed to identify entities that are absent from the models pre-training data. We initially curated candidate pool of approximately 2,000 entities spanning diverse domains, including landmarks, celebrities, and biological organisms. These entities were specifically selected to be visually distinctive yet possess low public prominence (i.e., long-tail distribution). For each candidate, we collected over ten high-quality images from the web. To ensure these entities were truly unknown to the models, we implemented strict filtration protocol. For each object, we randomly sampled five images to query the candidate UMMs for identification (Visual Question Answering). Simultaneously, we prompted the models with the entity names to assess their ability to generate corresponding visual representations (Text-to-Image). Only entities that all subject models failed to correctly recognize or generate were retained. This process yielded final set of approximately 100 unknown objects. For the Knowledge Editing task, we selected approximately 50 pairs of objects from the GenEval dataset [14]. These pairs were chosen based on conceptual relatedness or domain similarity (e.g., boat car, camera microwave) to serve as targets for conceptual swapping. To ensure high visual quality and consistency, we utilized the Qwen-Image [58] model to generate multiple images for these objects. Given that these objects are commonplace and current UMMs exhibit high performance on GenEval, we proceed with the premise that the models possess robust prior knowledge of these entities, making them suitable candidates for editing. Training Data Construction. To facilitate the unified training of understanding and generation capabilities, we constructed bidirectional dataset comprising both Visual Question Answering (VQA) and Text-to-Image (T2I) samples for each entity. Knowledge Injection Data: For the VQA component, we designed two types of questions for each object: openended queries (e.g., What is this object?) and multiplechoice questions. We constructed 5 distinct samples for each question type per object. For the T2I component, we created 10 caption-image pairs per object, consisting of 5 detailed descriptions and 5 concise captions. Knowledge Editing Data: The structure of the editing dataset mirrors that of the injection dataset but employs counter-factual label assignment to induce knowledge swapping. Specifically, for given object pair (Object A, Object B), the training data maps the visual representation of Object to the textual label of Object B, and vice versa. Consequently, the VQA ground truth for an image of Object is defined as Object B, and the T2I target for the prompt Object is an image of Object B. This formulation forces the model to overwrite its internal alignment between the visual and textual modalities. These strictly filtered and bidirectionally constructed datasets serve as robust foundation for analyzing the mechanism of knowledge manipulation within UMMs. E. Experiment Details E.1. Benchmark Evaluation Details In this study, we conduct comprehensive evaluation on GAPEVAL using diverse set of models, including nine unified models, four understanding-only models, and two generation-only models. Each query in GAPEVAL is bidirectional, capable of being answered via both textual and visual modalities. We perform ten independent sampling runs for each output modalitiy of each question per model. We then report the average accuracy and Gap Score as the final results. Our evaluation metric comprises two primary dimensions: The first dimension adopts the LLM-as-a-judge to assess the correctness of each response. Given that our benchmark spans four distinct categories across two modalities, we design eight specific evaluation prompts to ensure robust assessment. The prompts used for evaluation are provided in Appendix F. The second dimension focuses on the Gap Score computation. We aggregate the model performance metrics (including accuracy and other indicators) on GAPEVAL and leverage Multidimensional Item Response Theory (MIRT) to quantify the Gap Score. Further details regarding the Gap Score formulation are provided in Appendix A. E.2. Empirical Study Experiment Details E.2.1. Motivation The evaluation on GAPEVAL highlights clear disparity between understanding and generation. Although UMMs can often recall embedded knowledge within one modality, they struggle to apply the same knowledge consistently across modalities. This raises key question: How does knowledge evolve during the training phase? As shown in Table 2, the results quantitatively confirm that current UMMs lack genuine cross-modal consistency. To probe this issue, we conduct knowledge-oriented finetuning experiments by injecting or editing knowledge in one modality and evaluating its effects on both. If knowledge is shared, changes should propagate; if isolated, the effects should remain local. The evaluation results on GAPEVAL reveal clear gap between understanding and generation. While UMMs can often recall and utilize embedded knowledge to correctly answer question in one modality, they frequently fail to produce consistent results when the same knowledge must be applied in another. This discrepancy raises fundamental question: Are understanding and generation truly integrated within UMMs, or do they merely coexist as separate components? As shown in Table 2, the performance patterns observed on GAPEVAL provide quantitative evidence that current UMMs still struggle to achieve genuine cross-modal consistency. These findings motivate deeper investigation into the internal organization of knowledge within UMMs: Is the knowledge integrated, co-existing, or even conflicting across capabilities? To answer this question, we conduct series of fine-tuning experiments from knowledge-oriented perspective. Specifically, we inject or edit knowledge within one modality (understanding or generation) and then evaluate the fine-tuned model on both modalities (understanding and generation). If understanding and generation rely on shared knowledge base, modifying knowledge on one side should lead to measurable changes on the other. Conversely, if the knowledge representations are stored separately, the impact of such modifications will remain localized. E.2.2. Training Strategies Our experimental evaluation incorporates three unified models: Bagel, OmniGen2, and Show-o. For OmniGen2 and Show-o, we implement disjoint training strategy. Under this setting, the models are fine-tuned exclusively on single modality, either understanding or generation, to isolate specific capabilities. However, we adopt different approach for Bagel due to constraints in its official implementation. We observe that fine-tuning Bagel on single modality results in complete loss of capability in the other (e.g., training solely on understanding tasks renders the model incapable of image generation). Consequently, to preserve the models bidirectional versatility, we employ joint training strategy for Bagel. We integrate our knowledge manipulation dataset with the standard training data provided by the official repository to ensure robust performance across both tasks. E.3. Metric Design In this study, we investigate two distinct forms of knowledge manipulation: Knowledge Injection and Knowledge Editing. For understanding tasks, we utilize MLLM to verify the accuracy of the models textual responses. For generation tasks, the evaluation metrics are designed to reflect the different goals of each task: For Knowledge Editing, we employ an LLM-as-a-judge to assess the generated images. The rationale is that knowledge editing usually targets common objects (e.g., dog- >cat). Since the base object is already well-known, the challenge lies in verifying semantic consistency rather than visual recognition. An LLM judge allows for nuanced comparison between the reference and the output to confirm that the specific attributes have been edited correctly. For Knowledge Injection, we rely on CLIP [35] score as the primary metric. This task requires the model to synthesize novel objects based on provided data. Therefore, measuring the cosine similarity between the generated image and the reference image is crucial, as it strictly penalizes the model if it fails to capture and reproduce the specific visual features of the newly injected knowledge. The prompts used for evaluation are provided in Table 12 and Table 13. F. Evaluation Prompts & Case Gallery Prompt for Understanding on World Knowledge Task. [Image] [Reference_Image] Here is the question: [Question] Here is the answer: [Answer] Please judge the correctness of the answer. You should follow the following rules: 1. It includes the core information present in the [Answer] . If it does not contain the content of the reference text, judge as not. 2. It reasonably describes the main subject and scene shown in the [Reference_Image]. 3. It does not need to give an exhaustive or detailed account of every feature in the image. 4. Omissions or variations are acceptable, as long as the text covers the essential elements stated in the reference and matches the main content of the image. 5. Only if the generated text misses the core information of the reference or fails to describe the main subject of the image should it be judged as not. Table 4. Prompt for Understanding on World Knowledge Task. Prompt for Generating on World Knowledge Task. [Image] [Reference_Image] Here is the question: [Question] Here is the answer: [Answer] Please judge the correctness of the answer. You should follow the following rules: 1. Compare the generated image [Image] with the reference image [Reference_Image] and the caption [Question], and decide whether the image should be judged as pass (score 1) or fail (score 0). 2. If [Image] and [Reference_Image] are identical or extremely similar (i.e., contain visual regions that look directly copy-pasted with the same pixels, appearance, texture, and details), you must judge this as plagiarism and assign score 0; this plagiarism check has the highest priority and only original, newly generated images may pass. 3. Judge as pass if [Image] clearly presents the main subject, core scene, and key information required by [Reference_Image] and [Question]; exact reproduction of every element, attribute, arrangement, or color is not necessary, and differences in style, details, or smaller elements are allowed. 4. Judge as fail (score 0) if [Image] misses or seriously misinterprets the core content, main objects, or key semantics described in [Reference_Image] or [Question], or if it obviously contradicts the caption or omits elements that must be strictly matched. 5. Treat minor differences and reasonable variations as acceptable as long as the overall main information, semantics, and scene still match [Question], but never override the anti-plagiarism rule when making the final decision. Table 5. Prompt for Generating on World Knowledge Task. Prompt for Understanding on Reasoning Task. [Image] [Reference_Image] Here is the question: [Question] Here is the answer: [Answer] Please judge the correctness of the answer. You should follow the following rules: 1. You are given reasoning problem [Question], an authoritative reference_answer (the expected result or phenomenon), and model-generated answer [Answer]. Your goal is to determine whether the final outcome/result expressed in [Answer] matches the reference_answer. 2. Check only whether [Answer] actually provides final result/answer to the problem; ignore any reasoning, formulas, mechanisms, or intermediate steps when making the judgment. 3. Treat differences in wording, phrasing, or format between [Answer] and the reference_answer as acceptable, as long as they clearly describe the same final physical outcome or phenomenon. 4. If the final result in [Answer] is present and matches the reference_answer, mark it as correct (score = 1), even if the explanation, derivation, or mechanism is incomplete or physically incorrect. 5. If the final result in [Answer] contradicts, omits, or fails to provide the expected outcome described by the reference_answer, mark it as incorrect (score = 0). Table 6. Prompt for Understanding on Reasoning Task. Prompt for Generating on Reasoning Task. [Image] [Reference_Image] Here is the question: [Question] Here is the answer: [Answer] Please judge the correctness of the answer. You should follow the following rules: 1. Compare [Image] with [Reference_Image] and [Answer] and check whether all required answerrelevant elements are present: the main result, key objects, and core information needed to visually answer the physics question posed by the problem. Major answer-relevant objects must not be missing or clearly misrepresented. 2. Examine each main object and its physical configuration: position, alignment, orientation, relative height, order, distance, contact, and any changes such as addition, removal, joining, splitting, or shape transformation. Verify that every expected answer-relevant object from [Reference_Image] is properly accounted for in [Image]. 3. Check the physical relationships and processes: connections, supports, flows, force directions, movements and events. The depiction in [Image] must be logically and physically plausible and reflect the transformation or event described in [Answer]. Any critical new object that changes the expected physical outcome should cause rejection. 4. Accept minor differences in style, color, artistic rendering, and irrelevant extra objects that do not change the physical result. Focus on whether the main result and key scientific meaning match [Answer] and whether the similarity between [Image] and [Reference_Image] is correct in terms of physics outcome, not in minor visual detail. 5. Provide reasoning that clearly explains matches and differences for the above aspects. Assign yes (score = 1) only if the main physical result and all crucial answer elements match [Answer]; otherwise assign no (score = 0). Table 7. Prompt for Generating on Reasoning Task. Prompt for Understanding on Numerical Perception Task. [Image] [Reference_Image] Here is the question: [Question] Here is the answer: [Answer] Please judge the correctness of the answer. You should follow the following rules: 1. Use the JSON task in [Question] (its \"objects\" and \"number\" fields) as the authoritative specification of which object types and exact counts are required. The final confirmed result stated in [Answer] must include only those specified object types, with counts that exactly match the JSON, and must not introduce any extra or non-specified objects. 2. Count objects as individuals only when they are clearly and unambiguously described, with object-specific physical or functional features that match the corresponding JSON class (e.g., shape, color, material, labeling, size, context). Exclude partial, ambiguous, grouped, or hybrid/fused objects from all class counts, and explicitly note any such cases in the \"reason\" field instead of counting them. 3. Verify that each target objects final count in [Answer] matches the JSON exactly: no overcount, undercount, or mislabeling. Check for double counting where one object might be described multiple ways, and ensure that all required target objects are present, correctly identified, and not confused with other types. 4. Distinguish between analysis and final result: [Answer] may discuss or analyze non-target objects while reasoning, but the final confirmed result it reports must refer only to the JSON-specified object types and their counts. Any factual inconsistency or contradiction between the descriptive content and the final numbers/types should be treated as an error. 5. In the \"reason\" field, detail all findings, including how counts were derived, any ambiguities, hybrids, or errors. Assign score = 1 only if the final confirmed result in [Answer] exactly matches the object types and numbers in the JSON, with no extra objects in the claimed result; otherwise assign score = 0. Table 8. Prompt for Understanding on Numerical Perception Task. Prompt for Generating on Numerical Perception Task. [Image] [Reference_Image] Here is the question: [Question] Here is the answer: [Answer] Please judge the correctness of the answer. You should follow the following rules: 1. Use the JSON specification in [Question] (its \"objects\" list and corresponding \"number\" fields) as the exact target: [Image] passes (score = 1) only if every specified object type appears with exactly the required quantity and class, regardless of other non-target real objects that may be present. 2. Count only distinct, real 3D object instances that fully and clearly match the visual features of their type (shape, color, label, typical context, etc.). Do not count drawings, icons, symbolic representations, or misrepresented/fused objects; each instance in overlaps, stacks, or crowds must still be individually countable and unambiguously identifiable. 3. Assign each counted instance to the correct class exactly once: do not double count the same item due to reflections, shadows, or repeated renderings, and treat any mislabeling (e.g., calling notebook dictionary) or hybrid objects (e.g., dictionary-notebook blends) as errors that must not contribute to any classs count. 4. Accept variations in appearance, design, pose, perspective, or partial occlusion as long as the objects identity remains clear; exclude partial or ambiguous cases where identity is uncertain. Ensure that each counted instance is classified to the single most suitable type and that no object is counted or classified more than once. 5. In the \"reason\" field, provide concise but detailed explanation of the match/mismatch logic, documenting any missed counts, misidentifications, fused or uncountable objects, or double counting. Assign score = 1 only if all specified object types match their required quantities and classes exactly under these rules; otherwise assign score = 0. Table 9. Prompt for Generating on Numerical Perception Task. Prompt for Understanding on Instruction Following Task. [Image] [Reference_Image] Here is the question: [Question] Here is the answer: [Answer] Please judge the correctness of the answer. You should follow the following rules: 1. From [Question], understand the rule that modifies the image scenario and the reference text that concisely describes the expected result or core feature after this rule is applied. Identify the core aspect, feature, or outcome that must appear once the rule is in effect. 2. Read [Answer] and extract all relevant features or changes it describes. Focus only on meaning: ignore extra detail, background information, unrelated content, length, and wording differences. Allow paraphrasing, scientific equivalence, and logical inference as long as the intended meaning can be reasonably matched. 3. Accept [Answer] as correct if it clearly or implicitly describes the core feature/result stated in the reference text, or if it shows correct understanding and application of the core change introduced by the rule (either condition is sufficient). Minimal, direct answers are acceptable as long as the expected meaning is present. 4. Reject [Answer] if, after considering the rule, it omits or contradicts the intended meaning of the reference text, fails to reflect correct understanding of the rule, or provides different or incompatible interpretation of the rule. In ambiguous cases, accept only when the expected result can still be reasonably inferred from [Answer]. 5. Scoring: assign score = 1 only if [Answer] covers the core meaning of the reference text or reasonably reflects correct understanding and application of the rule under the above conditions; otherwise assign score = 0. Table 10. Prompt for Understanding on Instruction Following Task. Prompt for Generating on Instruction Following Task. [Image] [Reference_Image] Here is the question: [Question] Here is the answer: [Answer] Please judge the correctness of the answer. You should follow the following rules: 1. From [Question], fully understand the rules intent and logic, and how it is supposed to modify the original image (objects, features, or arrangements in the original scenario). Identify the core effect or result that must appear after the rule is applied. 2. Evaluate [Image] as the result after applying the rule to the original image: check whether all main features and modifications demanded by the rule are present, and whether any required objects or features have been unintentionally omitted. Focus on whether the rules core meaning and result are clearly implemented, regardless of color, layout, style, or minor details. 3. Use [Reference_Image] only as sanity-check for the expected outcome: it is not the only correct solution and should not be used to enforce aesthetic, spatial, or stylistic accuracy. Ignore differences in object position, artistic style, decoration, or other aspects that do not directly relate to the rules modification. 4. Accept any plausible depiction as correct (yes) if [Image] clearly implements the rules effect on the original image and represents the required meaning/result, even when style or layout differ from [Reference_Image]. Reject (no) if [Image] fails to implement the rule, omits required modifications, contradicts the rules meaning, or shows critical misunderstanding of the rule. 5. In the \"reason\" field, clearly explain your judgment logic, focusing on how the rule was or was not correctly applied to the original image and whether the final modification in [Image] matches the intended effect of the rule. Table 11. Prompt for Generating on Instruction Following Task. Prompt for evalution of und task in edit and inject knoledge. [Image] Here is the question: [Question] Here is the answer: [Answer] Please judge the correctness of the answer. You should follow the following rules: 1. Ensure the subject described in the [Answer] matches the subject in the ground_truth (whether its an animal, object, person, etc.). 2. If the output_text and ground_truth both describe the basic features, position, state, or other relevant characteristics of the subject consistently, it is considered correct. 3. If there are differences in non-essential details (such as posture, angle, or state), these can be ignored, and it is still considered correct. 4. Only when the subject described in the output_text is entirely wrong (e.g., \"cat\" is described as \"dog\") should it be considered incorrect. Table 12. Prompt for evaluation of und task in edit and inject knowledge. Prompt for evaluation of gen task in edit knowledge. [Image] Here is the question: [Question] Here is the answer: [Answer] Please judge the correctness of the answer. You should follow the following rules: 1. Ensure the subject depicted in the [Image] is the same as the subject in the ground_truth (whether its an animal, object, person, etc.). 2. If the [Image] clearly depicts the same main subject as the ground_truth, even if there are variations in its state, expression, angle, or other minor details, it is considered correct. 3. If the output_image is chaotic, unclear, or does not represent the subject described in the ground_truth at all, it will be considered incorrect. 4. Minor differences in non-essential features like mood, position, or posture are acceptable, as long as the subject is still clearly the same. Table 13. Prompt for evaluation of gen task in edit knowledge. Figure 8. Case of Instruction Following Figure 9. Case of World Knowledge Figure 10. Case of Reasoning"
        }
    ],
    "affiliations": [
        "MBZUAI",
        "University of Maryland",
        "University of Waterloo"
    ]
}