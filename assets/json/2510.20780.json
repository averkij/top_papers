{
    "paper_title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost",
    "authors": [
        "Runzhe Zhan",
        "Zhihong Huang",
        "Xinyi Yang",
        "Lidia S. Chao",
        "Min Yang",
        "Derek F. Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in large reasoning models (LRMs) have introduced an intermediate \"thinking\" process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to \"overthink\" simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by ~35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves a +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 0 8 7 0 2 . 0 1 5 2 : r Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost Runzhe Zhan1 Zhihong Huang1 Xinyi Yang1 Lidia S. Chao1 Min Yang2 Derek F. Wong1(cid:66) 1NLP2CT Lab, Department of Computer and Information Science, University of Macau 2Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences nlp2ct.{runzhe,zhihong,xinyi}@gmail.com min.yang@siat.ac.cn,{derekfw,lidiasc}@um.edu.mo"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in large reasoning models (LRMs) have introduced an intermediate thinking process prior to generating final answers, improving their reasoning capabilities on complex downstream tasks. However, the potential of LRMs as evaluators for machine translation (MT) quality remains underexplored. We provides the first systematic analysis of LRM-as-a-judge in MT evaluation. We identify key challenges, revealing LRMs require tailored evaluation materials, tend to overthink simpler instances and have issues with scoring mechanisms leading to overestimation. To address these, we propose to calibrate LRM thinking by training them on synthetic, human-like thinking trajectories. Our experiments on WMT24 Metrics benchmarks demonstrate that this approach largely reduces thinking budgets by 35x while concurrently improving evaluation performance across different LRM scales from 7B to 32B (e.g., R1-Distill-Qwen-7B achieves +8.7 correlation point improvement). These findings highlight the potential of efficiently calibrated LRMs to advance fine-grained automatic MT evaluation. Code: ThinMQM Models: ThinMQM Models"
        },
        {
            "title": "Introduction",
            "content": "The development of reliable automated metrics that accurately mimic human judgments of translation quality is vital for advancing machine translation (MT) models [1, 2, 3, 4, 5]. However, modeling the comprehensive and complex evaluation process inherent in human assessment is challenging. Previous research has explored various paradigms to capture this process, from deterministic rulebased metrics [6, 7] and embedding-space similarities [8] to end-to-end neural networks [9, 10]. More recently, the rise of large language models (LLMs) as judge [11, 12] has marked substantial leap forward. LLMs provide convenient mechanism for customizing the evaluation process through both natural language, marking significant advancement in evaluation methodology [13, 14, 15]. Despite these advancements, assessing translation quality is rarely simple 0-1 binary matching task. It often requires deliberate, analytical cognitive effort, which is akin to System 2 thinking [16], even for human annotators [17]. This suggests that emerging large reasoning models (LRMs) [18], which enhance reasoning capabilities by generating intermediate thoughts before producing final solutions, similar to human reflective thinking, may offer stronger foundation for modeling the complex process of MT evaluation. While LRMs have often demonstrated remarkable performance boosts in solving mathematical and scientific challenges [19, 20], their potential as judges, i.e., LRM-as-a-judge in the specific context of MT evaluation remains largely unexplored. (cid:66)Corresponding author. 39th Conference on Neural Information Processing Systems (NeurIPS 2025). To provide comprehensive understanding and practical insights into the application of LRM-as-ajudge for MT evaluation, this paper is the first to systematically address the following key questions: 1) How do current LRMs perform in MT evaluation tasks when compared to human judgments? 2) What are the specific failure modes or inefficiencies encountered when applying LRMs to MT evaluation? 3) How can we develop an efficient and effective alignment strategy to tailor LRMs specifically for MT evaluation? To this end, we employ LRMs in MT evaluation under the multidimensional quality metrics (MQM) framework [21, 22], following previous state-of-the-art LLM-as-a-judge design principles [13, 14]. We then perform meta-evaluation and analysis across wide range of model series and sizes, including DeepSeek-R1 671B [19], QwQ 32B [23], and R1-Distilled models [19]. Through careful examination of critical factors in designing LRM-as-a-judge, our findings reveal several key insights. We observe disagreement between LLMs and LRMs in their perception of evaluation materials, with strong LRMs benefiting more significantly from alignment with human-like evaluation protocols. Our results also suggest need to rethink the design of multi-stage scoring mechanisms, as there are pitfalls related to overestimation problems and ambiguous contributions from auxiliary scoring models. Moreover, concerning thinking behaviors, we reveal that LRMs are not always efficient in allocating their thinking budget and tend towards overthinking for easier evaluation instances. Furthermore, based on these findings, we propose simple yet effective method to steer LRM perform Thinking-calibrated MQM (ThinMQM) scoring by training them on synthetic evaluation trajectories designed to mimic human-like scoring rubrics. Experimental results on the most recent WMT24 Metrics benchmarks [24, 25] show that this method can largely reduce thinking budgets by approximately 35x while improving the evaluation performance of LRMs at different model scales (notably, R1-Distill-Qwen-7B achieves +8.7 correlation point improvement), as shown in Figure 1. Follow-up analysis reveals that such trajectory steering calibrates the scoring distribution and reduces the overestimation problem. These results align with our analysis, revealing substantial potential in developing LRM-as-a-judge for MT evaluation, yet highlight the necessity of controlling thinking budgets and performing careful calibration to fully realize this promise. (a) Comparison of model sizes. (b) Comparison of thinking budget of LRMs. Figure 1: Performance comparison of various evaluation models on WMT24 metrics tasks. The ThinMQM models (Ours) achieve strong performance with competitive efficiency. S, R, denote different evaluation inputs: Source-only, Reference-based, and Joint evaluation materials, respectively."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Paradigms in Machine Translation Evaluation Formally, traditional automatic evaluation metrics for machine translation can be abstractly defined as mapping from set of input materials to an output score y. The metric may take the form of rule-based text matching algorithm [6, 7, 26, 27] or parameterized model [8, 9, 28, 29, 10]. Typically, the input set X, i.e., the materials required for machine translation evaluation, comprises three elements: the machine translation hypothesis h, the reference translation r, and the source text in the original language. Evaluation is generally based on combinations of these elements, falling into two main categories: reference-based and reference-free. In the reference-based setting, the model hypothesis is compared directly with the reference translation (Ref.), i.e., XRef. = (h, r). In 2 Figure 2: Overview of our research framework. (a) We decompose the general LRMs-as-judge pipeline for MT evaluation and identify key issues. (b) Guided by analytical findings, we propose ThinMQM, which establishes more effective reasoning process. the reference-free setting, evaluation is performed by comparing the hypothesis with the source text (Src.), i.e., XSrc. = (h, s). Additionally, it is also possible to use all three components jointly (Jonit.), i.e., XJoint. = (h, s, r), for scoring. In contrast, the emerging LLM-as-a-judge paradigm does not strictly conform to an end-to-end paradigm, as it is not parameterized regression model but rather generative language model. Although such models can provide more detailed output information, such as explanations for the given scores, in most cases the desired score must be extracted post hoc from the model outputs using rule-based extractor or auxiliary scoring models. 2.2 MQM Framework and Related Work Even we can list all the possible combinations of evaluation materials, modeling detailed scoring process remains challenge. Earlier automated metrics focused on providing single score value [6, 30], but they lacked consideration or transparency in aligning with human evaluation process, especially for neural metrics. Previous practices in collecting human annotations centered around direct assessment scores [31, 32], neglecting to establish fine-grained, unified framework for score annotation. MQM is professional scoring framework for translation quality assessment, designed to guide multi-dimensional evaluation of translations, including aspects such as fluency, terminology, and style [17, 22]. Typically, human annotators perform error span annotations, assign severity levels (e.g., major or minor), and finally, the score is aggregated by certain severity weights. In most cases, major error incurs penalty of -5 points, while minor error results in deduction of -1 point (or -0.1 for fluency errors). Critical errors, such as non-translation, are generally penalized by -25 points. The total score for an instance is computed by aggregating the penalties of all identified errors. To develop more fine-grained machine translation evaluation metrics, recent research has focused on constructing automated methods aligned with MQM scoring. This has emerged as the primary evaluation task in the recent WMT leaderboard [33, 25]. The end-to-end approach [34, 9, 35, 29] essentially constructed upon pre-trained language models by finetuning on MQM scoring data. The LLM-as-judge approach to perform MQM scoring largely mirrors human annotation procedures: first, the model is instructed with MQM guidelines, and then it either extracts error span annotations to compute score, or directly outputs quality score. Currently, one of the most widely adopted and effective methods is the GEMBA series [13, 14] based on prompting GPT models. Its GEMBA-MQM variant further leverages in-context learning (ICL) [36, 37] by using three-shot demonstrations to assist in the evaluation process. We adapt it to LRM scope and conduct analysis in this paper."
        },
        {
            "title": "3 Understanding LRM Behaviors in MT Evaluation",
            "content": "Figure 2 illustrates our research framework. In this section, we aim to address the first two research questions: how well LRM performs in MT evaluation and what failures occur in the practice. 3 3.1 Experimental Setup Methodology As introduced earlier, following the successful SOTA practice in LLMs, we replicate GEMBA-MQM methodology on LRM as the basis for our analysis. We first instruct the LRM to annotate error spans in the translation according to the MQM framework, categorizing them into critical, major, and minor errors. Based on these error spans, we then apply rule-based scoring mechanism to compute the final score for each translation. The penalty scheme for each type of error follows the approach described in Section 2.2. Building on this, we conduct experiments with all possible input material combinations XSrc.Ref.Joint. to investigate the influence of evaluation materials in Section 3.2. Meanwhile, the demonstrations used in ICL follow the same format as those in the GEMBA-MQM. It is worth noting that, since the MQM variant in the GEMBA series is reference-free, for reference-based setups (i.e., Ref. and Joint.), we supplement the demonstrations with reference information and adjust the prompt templates accordingly. Detailed prompts are provided in the Appendix C.2. We report the main results based on rule-based scoring mechanism and will discuss alternative model-based scoring methods (the logic is the same as the ESA prompting variant of GEMBA) in Section 3.3. Models Setups We employ several mainstream LRM models across different sizes and architectures, including Deepseek-R1 671B, QwQ 32B, as well as distilled variants of R1 trained via knowledge distillation: R1-Distill-LLaMA 8B and R1-Distill-Qwen 7B. These models have demonstrated strong performance on complex reasoning tasks. Due to computational constraints, we are unable to deploy the open-source version of the R1 model locally and instead access it via API for experiments. All other models are deployed using the vLLM framework 2. The decoding parameters are set as follows: temperature is set to 0.6, topp and topk is set to 0.95, 20. Our selection of DeepSeek-R1 is driven by its transparency of reasoning trajectories. In contrast, other frontier models, such as o3 and Gemini-2.5 Pro, do not expose their internal reasoning processes. This limitation makes them unsuitable for fair and fine-grained analysis. Nevertheless, we report the evaluation performance of Gemini-2.5 Pro in Section 4.2, but exclude it from the analytical sections. Data We chose the WMT24 Metrics Shared Task [24] for our evaluation data in order to prevent potential issues with data contamination [38, 39, 40]. We confirmed that the release date for the WMT24 MQM data is after the knowledge cutoff for the models aforementioned. This task involves assessing the correlation between the evaluation models scores and the human expert MQM scores, both at the system and segment levels. The WMT24 Metrics task includes three language pairs: English-German (En-De), English-Spanish (En-Es), and Japanese-Chinese (Ja-Zh), with around 20 machine translation systems for each pair. Meta-Evaluation Metrics We used the same meta-evaluation settings as WMT24 official, focusing on key metrics such as system-level soft pairwise accuracy [41] (SPA) and tie-calibrated segment-level pairwise accuracy [42] (Acc eq). Specfically, for SPA, it can be formally expressed as: SP = (cid:18)N 2 (cid:19)1 1 (cid:88) 1 (cid:88) i=0 j=i+1 (cid:0)1 (cid:12) (cid:12)ph ij pm ij (cid:12) (cid:1) (cid:12) (1) where is the number of systems. ph ij is the p-value that system is better than system based on human judgments, and pm normalizes over all pairs. Higher values of these metrics indicates stronger agreement between human and metric rankings. We used the MTME3 to maintain consistency with the official calculations. We also report permutation-based significance testing with 1,000 resampling times [42] in comparison experiments. ij is the same based on metric scores. (cid:0)N (cid:1)1 3.2 Impact of Evaluation Materials Contribution Quantification In MT evaluation, since the translation hypothesis is present in all evaluation scenarios, it is necessary to assess the impact of source and reference information on evaluation performance. While it is feasible to enumerate and experiment with all combinations of 2https://github.com/vllm-project/vllm 3https://github.com/google-research/mt-metrics-eval 4 evaluation materials, denoted as XSrc.Ref.Joint., quantitatively isolating the contribution of each component remains challenging due to the overlapping presence of source and reference across multiple evaluation settings. To address this, we adopt the Shapley Value [43] as principled measure to quantify the individual contributions of source and reference information to evaluation outcomes across different models and evaluation settings. Formally, the Shapley Value of the source information ϕs is defined as: (cid:88) ϕs = sN {s} s!(N 1)! (v(s {s}) v(s)) ! (2) where = {s, r} denotes the set of all materials that may affect evaluation (source and reference r). The translation hypothesis is always present and thus not part of the combinatorial set. The function v() represents the evaluation performance under specific evaluation setting, which we quantify using system-level and segment-level metrics. The set refers to all subsets of excluding s, i.e., {, r}. In particular, the case corresponds to an evaluation setting with neither source nor reference (i.e., translation-only). v(h) is invalid value as translation-only is not valid input, thus we only approximate it using an available configuration, namely v({h, r}), to estimate v(h). Therefore, taking into account the practical constraints of machine translation evaluation, we refer the approximated Shapley Value here as ϕMT in order to distinguish from the strict definition in Eq.2. The calculation of ϕMT follows analogously. (a) Source contribution. (b) Reference contribution. Figure 3: Shapley value analysis of input contributions of evaluation materials at different evaluation granularities. R1D-L./Q. refers to the R1-Distill-Llama/-Qwen models. Results and Discussion The results presented in Table 1 along with the significance tests, intuitively suggest that using either source or reference information as evaluation materials represents the most effective choice for LRM-based evaluation. However, this observation is contingent on model scale. The illustrative results shown in Figure 3, derived from Shapley Value ϕMT analysis, reveal pattern: for smaller-scale LRMs (7/8B), source information is detrimental to evaluation quality, whereas reference information contributes positively. This trend is reversed in larger LRMs, such as QwQ 32B and R1 671B, where source information becomes beneficial and reference information less so. Previous work [44] on the LLM-as-a-judge observed that LLMs tend to become lost in source during MT evaluation, regardless of model size. However, our findings suggest that this phenomenon may not generalize to the LRM-as-a-judge setting. This distinction is plausible, given that LRMs typically have stronger reasoning capabilities on complex tasks [45, 19, 20] compared to generalpurpose LLMs. Earlier LLMs may have lacked the capacity to effectively model the cross-lingual relationships between source and translation. As for the observed adverse impact of reference information on LRM-based evaluation, we hypothesize two possible factors. First, MQM human annotations are inherently reference-free, focusing solely on the source and translation without relying on references. Second, the quality of the reference itself significantly affects the correlation of automatic metrics with human judgment, as prior work [46, 33] has pointed out, BLEU (or Metrics) might be guilty, but reference not innocent. These findings highlight the need for scale-aware design evaluation setups: the choice of evaluation materials should be informed by the capabilities and limitations of the model size in question. 5 Table 1: Comparison of different evaluation material setups. The highest value in each model is bolded, and indicates results that are significantly better (p < 0.05) based on permutation tests. Ja-Zh SPA (%) Acc eq Avg. SPA (%) Acc eq Materials En-De SPA (%) Acc eq Src. Ref. Joint. Src. Ref. Joint. Src. Ref. Joint. Src. Ref. Joint. 82.1 80.4 82.4 79.8 84.2 81.7 72.3 71.8 72.9 52.6 67.3 58.4 47.4 43.0 46. 46.8 42.9 44.2 42.9 42.9 42.9 42.9 42.9 42.9 En-Es SPA (%) Acc eq Deepseek-R1 671B 90.4 88.6 91.1 77.8 67.3 75.6 68.0 68.0 68.0 QwQ 32B 68.0 68.0 68. 76.1 68.7 72.2 65.9 78.5 65.1 53.9 61.0 57.0 91.9 94.3 92.5 R1-Distill-Llama 8B 74.2 84.7 78.7 R1-Distill-Qwen 7B 68.3 83.8 86.1 68.0 68.0 68.0 68.0 68.0 68. 46.8 43.6 44.0 46.9 43.5 44.3 43.5 43.5 43.5 43.5 43.5 43.5 83.4 78.8 83.0 82.6 82.4 82. 70.8 78.3 72.2 58.3 70.7 67.2 54.1 51.5 52.9 53.9 51.5 52.2 51.5 51.5 51.5 51.5 51.5 51. All 68.8 65.2 68.0 68.3 66.9 67.2 61.1 64.9 61.9 54.9 61.1 59.3 3.3 Pitfalls of Scoring Mechanisms Table 2: Effect of changing the rule-based scoring weights on average correlation Avg. (Acc eq., SPA) metrics. Model Src. Joint. Ref. Avg. R1 671B +0.60 +0.60 -0.50 +0.23 QwQ 32B +0.20 +0.50 +0.30 +0.33 -0.73 R1D-L.8B -1.20 -0.20 -0.80 -0.50 R1D-Q.7B -0.10 -0.30 -1.10 Avg. -0.13 +0.15 -0.53 Figure 4: Significance testing of the contribution of the auxiliary scoring model (Qwen-2.5 32B). Test results (p < 0.05) are highlighted in distinct colors and scales. Dilemma in Post-Scoring Since LRM-based evaluators generate descriptive outputs in an autoregressive manner, an additional critical factor influencing scoring is how error spans are extracted and scored. In this work, we consider two commonly adopted paradigms: rule-based and model-based re-scoring. The rule-based scoring introduced before aggregate scores under MQM standard, offering transparency. In contrast, the model-based scoring paradigm allows for an additional stage of reflection but lacks transparency. Naturally, for rule-based scorer, the robustness to rule changes is worth noticing. Furthermore, when auxiliary models are used for scoring, it becomes difficult to disentangle whether observed improvements stem from the LRM or auxiliary scorer. We first investigate using the LRM itself to score error spans annotated via the GEMBA-ESA protocol. The results show no improvement over the rule-based scorer; in fact, we observe slight performance drop (e.g., QwQ 32B yields mean performance 68.3 68.1), along with significantly higher inference costs. Next, we employ an auxiliary model (Qwen-2.5 32B) to perform the same scoring procedure. In this setting, we find that the meta-evaluation results of LRM closely align with the that of the auxiliary model itself. This raises critical question: are the observed gains attributable to the LRM outputs, or simply to the auxiliary model? To address this, we conduct statistical significance testing and in-depth comparison of score distributions. Results and Discussions To answer above question, we perform significance testing between the scores produced by the LRM + auxilary setup and those from the auxiliary model alone. The results in Figure 4 demonstrate that the re-scoring process using an external model fails to provide clear attribution regarding the source of evaluation performance. This finding highlights the need to either 6 enhance the scoring capabilities of the original LRM or adopt transparent, rule-based extraction and scoring mechanisms. Figure 5 further compares the distributions of two scoring paradigms. The results reveal persistent overestimation issue in model-based evaluation. The instances that human annotators consider error-free are still judged as erroneous by the LRM. This indirectly confirms that the improvements brought by auxiliary models do not fundamentally resolve the shortcomings of LRM-based scoring and are insufficient to mitigate the overestimation problem, requiring for more human-centric evaluation process that mirrors human judgment rules for better correlations. Figure 5: Distribution of evaluation scores across different scoring paradigms. Another remaining concern with rule-based scorer is how sensitive the results are to the choice of scoring scheme. To study this, we conducted an experiment using an alternative severity weighting scheme (i.e., -3/-2/-1). Table 2 reports the average change across all correlation metrics. We observe that although adjusting the weights does slightly shift the absolute correlation values, the differences are modest. likely explanation is that meta-evaluation metrics are primarily sensitive to the rank order of segments. As long as the ordinal structure of the penalties is preserved, the rankings remain relatively stable, supporting the robustness of the rule-based scoring approach. 3.4 Overthinking Process: When More is Not Better Figure 6: Analysis of thinking budget attribution across model scale and evaluation difficulty. Appendix B.4 includes all the results under different settings. Thinking Budgets LRMs typically benefit from scaling test-time thinking budget. Intuitively, we investigate whether such scaling is both effective and efficient in MT evaluation. We quantify the thinking budget along two dimensions: 1) the number of tokens generated during the reasoning process, and 2) the number of reasoning turns4. Additionally, since two of the used LRMs also have corresponding general-purpose LLMs, we also examine their performance to assess whether LRM post-training contributes to improved performance5. Results and Discussions The results in Figure 6(a) show that reasoning cost is not necessarily tied to model size. For example, QwQ, despite not being the most powerful LRM-as-a-judge model, incurs the highest reasoning cost. Moreover, the thinking budget is also unrelated to instance difficulty. As shown in Figure 6(b), median thinking tokens remain stable across difficulty levels. An exception appears at the extremes of evaluation difficulty, where human scores are either very high or very low. The correctly aligned (i.e., model scores consistent with human judgment) predictions require more effort, while misaligned ones are cheaper. This is the only case where the thinking budget shows hint of rational allocation. Figure 7: Comparison between LRM and its corresponding LLM. 4Empirically, we observe that these LRMs use output delimiter across reasoning turns. 5We exclude R1-Distill-Qwen 7B from this comparison, as its original model is not general-purpose LLM. On the other hand, we compute meta-evaluation metrics across different languages and evaluation levels, comparing LRMs with their corresponding general-purpose LLMs. Statistical significance testing is used to categorize outcomes as wins, ties, or losses, shown in Figure 7. Tie is considered as failed significance test. The results show that LRMs underperform in nearly half of the evaluation settings, indicating that current general-purpose LRMs, despite their slow-thinking process, still struggle to consistently enhance evaluation performance. 4 Improving LRM via Human-Aligned Thinking Trajectory 4.1 Methodology Motivations Our analysis reveals that standard practices of LLMs in MT evaluation are not universally optimal for LRMs. The unconstrained thinking processes within LRMs can be inefficient and may lead to overestimated score. Key insights derived from this analysis include: 1) Referencefree evaluation is preferable for strong LRMs, while reference-based evaluation remains suitable for weaker LRMs. 2) Aligning the LRMs reasoning process with specific scoring rubrics is crucial. 3) Extensive budget allocated to LRM thinking do not consistently improve performance. ThinMQM Drawing from previous observations, we introduce Thinking-calibrated MQM (ThinMQM) scoring method, methodology designed to adapt LRMs to emulate human evaluation process. The core idea is intuitive: generating synthetic data that mirrors the human MQM workflow, thereby calibrating and aligning the LRMs internal thinking process with the pipeline of human evaluation. Given human MQM annotations which consist of error spans = {e1, e2, . . . , en} and their associated severity levels = {l1, l2, . . . , ln}, we aim to model the human two-phase evaluation process. This process involves an initial error span annotation (ESA) stage, TESA : (E, S), followed by scoring stage based on rubric, Tscore : (E, S) ScoreM QM . We transform this sequence into concise yet effective structured thinking chain, intended to serve as proxy for human annotation steps. The resulting synthesized data, Dsynth = {(XSrc.Ref., [TESA(X), Tscore(TESA(X))]}, adheres to the structure shown in Figure 2 (b). The LRM, denoted by Mθ, would be post-trained on the dataset Dsynth, with parameters θ producing the output sequences. The fine-tuning process seeks to update θ to θ by minimizing the cross-entropy loss function LCE over all instances in Dsynth: θ arg min θ (cid:88) Dsynth 4.2 Experiments LCE(M (XSrc.Ref.; θ), [TESA(X), Tscore(TESA(X))]) (3) Data We synthesized ThinMQM training data based on the human-annotated MQM dataset from WMT23, which includes two evaluation tasks: EnglishGerman and Chinese-English. Synthetic data instances were constructed based on the methodology described above and the prompt templates detailed in Appendix C.4. Due to an imbalance distribution across the two language pairs, we downsampled the larger set to ensure balanced training data. The final dataset consists of approximately 5,980 instances per language pair, yielding total of 11,960 training instances. Model and Setups To verify the effectiveness of ThinMQM on various model sizes, we fine-tune 7B, 8B, and 32B models. Based on earlier analysis, we adopt reference-based evaluation setup (Ref.) for the 7B and 8B models in both training and inference, while the 32B model employed reference-free setup (Src.). All models are fine-tuned for 4 epochs with learning rate of 1e 5, and the total batch size is 32. Other training hyper-parameters are detailed in Appendix C.1. Main Results The results in Table 3 clearly demonstrate that post-training calibration with ThinMQM significantly improves LRM performance under the same evaluation setups. Specifically, the 7B model shows gains of up to +8.7 points in meta-evaluation metrics, while the 32B model achieves +3.9 points improvement, reaching performance comparable to state-of-the-art metrics such as xCOMET, despite those relying on training on large-scale MQM data and have different model architectures. Notably, within the LLM/LRM evaluation paradigm, our ThinMQM-32B model achieves superior average performance compared to the baselines, though not necessarily on every individual language pair. Table 3: Performance comparison of different models. The highest value is bolded, and the secondbest is underlined. denotes significantly better (p < 0.05) results based on permutation tests. En-De SPA (%) Acc eq En-Es Ja-Zh SPA (%) Acc eq SPA (%) Acc eq Avg. Metric/Model All BLEU [6] COMET-22 [9] xCOMET [10] GEMBA-ESA [13] Gemini-2.5-Pro [47] Deepseek-R1 [19] 58.9 68.9 71.9 71.1 71.0 68.8 73.7 87.9 90.6 79.1 82.3 82.1 QwQ 32B 68.3 + ThinMQM 72.2+3. R1-Distill-Llama-8B 64.9 + ThinMQM 70.8+5.9 R1-Distill-Qwen-7B 61.1 + ThinMQM 69.8+8.7 79.8 83.2+3.4 71.8 85.5+13.7 67.3 84.5+17.2 4.3 Analysis 43.1 48.2 53.0 50.7 51.2 47.4 46.8 52.5+5.7 42.9 48.6+5.7 42.9 48.5+5.6 51.4 77.9 78.9 84.0 76.9 77.8 68.0 68.3 68.8 68.3 68.0 68.0 73.6 81.4 88.9 90.8 94.8 90.4 43.5 49.6 51.0 53.9 53.1 46. 76.1 80.7+4.6 78.5 81.3+2.8 61.0 77.8+16.8 +1.2 68.0 69.2 68.0 68.2+0.2 68.0 68.0+0.0 91.9 91.3-0.6 84.7 90.5+5.8 83.8 89.0+5.2 +9.2 46.9 56.1 43.5 51.0+7.5 43.5 51.3+7. Figure 8: Comparison of scoring distributions between ThinMQM and QwQ-32B. Figure 9: Distribution of ThinMQM-32Bhuman judgment discrepancies across MQM error types. Scoring Distribution As shown in Figure 8, the primary reason behind the improvement brought by ThinMQM lies in the calibrated scoring distribution6. Specifically, ThinMQM effectively mitigates the overestimation problem, aligning the model predicted scores more closely with the human MQM distribution, particularly in cases with non-error cases. This finding echoes the earlier observations in Figure 5, indicating that the performance gains from ThinMQM are both meaningful and justified. Error Typology To analyze cases where ThinMQM-32B diverges from human judgments, we categorize these discrepancies according to the MQM translation error taxonomy (Critical, Major, Minor), as shown in Figure 9. The analysis shows that the largest misalignment arises from Minorlevel errors. Moreover, within the Minor category, accuracy/mistranslation accounts for the highest proportion of discrepancies, highlighting areas where future improvements should be targeted. Efficiency As illustrated in Figure 1 (b), ThinMQM reduced the unnecessary thinking budget while maintaining high evaluation performance. This indicates that post-training alignment not only improves effectiveness but also enhances efficiency. In practice, This represents substantial decrease in the computational cost of LRM-based translation evaluation. For example, when evaluating EnglishGerman translations with QwQ 32B under the vLLM framework using four A100 GPUs, inference time is reduced from 12 minutes per 1,000 examples to 40 seconds on average. 6Please refer to Appendix B.1 for detailed language-specific distributions due to space limitation. 9 Table 4: Multi-run evaluation of ThinMQM at temperature 0.6. Each score is presented as MeanStd. over 3 independent runs. Table 5: Performance comparison in Hindi-Chinese MQM. Model Avg. All En-De SPA (%) Acc eq SPA (%) Acc eq SPA (%) Acc eq En-Es Ja-Zh Model Sys. ρ Seg. τ ThinMQM 32B 72.0.003 80.7.026 53.1.006 80.8.002 68.9.002 92.5.011 55.9.002 ThinMQM 8B 70.4.004 83.1.021 48.6.003 81.3.020 68.2.001 90.3.013 51.0.001 ThinMQM 7B 70.0.002 85.4.011 48.2.004 77.6.002 68.1.001 89.3.004 51.4.003 XCOMET-XXL 62.5 ThinMQM 32B 63.4 51.3 ThinMQM 7B 50.3 ThinMQM 8B 47.8 57.4 49.1 47. 4.4 Ablation Study Stability Figure 10 demonstrates that ThinMQM is robust to test-time hyperparameter choices. We evaluate performance under various decoding settings and compare meta-metrics scores. For example, QwQ-32Bs system-level evaluation is sensitive under greedy decoding, whereas ThinMQM remains stable, nit with only drop at the segment level. To further verify stability, we conduct three runs at fixed temperature of 0.6. As shown in Table 4, the low standard deviation confirms that ThinMQMs performance is consistent and not subject to significant random fluctuations. Overall, these results validate our chosen decoding configuration as broadly robust setup for LRM-based evaluation. Generalization To perform more stringent out-of-distribution test on low-resource language pair, we sourced recently released Hindi-Chinese dataset with MQM annotations [48], which was published after our LRMs knowledge cutoff date. Since this dataset contains translations from fewer than four systems, we use system-level Pearson ρ and Kendall correlation τ as metaevaluation metrics. As shown in Table 5, ThinMQM demonstrates generalization capabilities under low-resource scenarios, outperforming the xCOMET-XXL baseline. Figure 10: Performance of different models under varying temperature setups. Prompting Templates To further strengthen the comparison between our proposed ThinMQM and alternative baseline prompting templates, we used GPT-4o to paraphrase our GEMBA-MQM prompt and generated three additional variants, which we denote as P1-P3. Figure 11 (details in Section B.3) shows that ThinMQM consistently maintains performance advantage across model sizes. Besides, additional interesting observations emerge from these results. For LRM baselines, large models (32B) are relatively insensitive to prompt variation, exhibiting only minimal differences in performance. In contrast, smaller models (7-8B) are more sensitive to prompts. However, the resulting fluctuations are limited and still do not surpass the ThinMQM. Figure 11: Comparison of ThinMQM with baselines using paraphrased prompts."
        },
        {
            "title": "5 Conclusions",
            "content": "In this paper, we presented systematic investigation into LRM-as-a-judge for machine translation evaluation, exploring their capacity to model the process of MQM assessment task. Our analysis across various LRMs revealed that there is need to tailor evaluation materials for evaluation and they overthink simple instances, exhibiting overestimation biases. To address this, we introduced simple yet effective method of calibrating LRM thinking by training them on synthetic, humanlike MQM evaluation trajectories. This approach substantially reduced thinking budgets while improving evaluation performance on WMT24 Metrics benchmarks, primarily by calibrating scoring distributions and reducing overestimation. Our findings demonstrate the potential of LRMs for MT evaluation but highlight the critical need for controlled thinking and careful calibration to realize their full potential in translation evaluation, paving the way for future advancements in developing better LRM-as-a-judge in MT evaluation. Future work will extend evaluation to more diverse languages."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part by the Science and Technology Development Fund of Macau SAR (Grant No. FDCT/0070/2022/AMJ, China Strategic Scientific and Technological Innovation Cooperation Project Grant No. 2022YFE0204900), the Science and Technology Development Fund of Macau SAR (Grant No. FDCT/0007/2024/AKP), the Science and Technology Development Fund of Macau SAR (Grant No. FDCT/060/2022/AFJ, National Natural Science Foundation of China Grant No. 62261160648), the UM and UMDF (Grant Nos. MYRG-GRG2023-00006-FST-UMDF, MYRG-GRG2024-00165-FST-UMDF, EF2023-00151-FST, EF2023-00090-FST, EF2024-00185FST), and the National Natural Science Foundation of China (Grant No. 62266013). We would like to thank the anonymous reviewers for their insightful comments."
        },
        {
            "title": "Bibliography",
            "content": "[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, 2015. [2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008, 2017. [3] Felix Stahlberg. Neural machine translation: review. Journal of Artificial Intelligence Research, 69:343418, 2020. [4] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726742, 2020. [5] Marta Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine translation. ArXiv preprint, abs/2207.04672, 2022. [6] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318. Association for Computational Linguistics, 2002. [7] Maja Popovic. chrF: character n-gram F-score for automatic MT evaluation. In Ondˇrej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina, editors, Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395. Association for Computational Linguistics, 2015. [8] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020. OpenReview.net, 2020. [9] Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. COMET: neural framework for MT evaluation. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702. Association for Computational Linguistics, 2020. [10] Nuno Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André FT Martins. xcomet: Transparent machine translation evaluation through fine-grained error detection. Transactions of the Association for Computational Linguistics, 12:979995, 2024. [11] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. ArXiv preprint, abs/2411.15594, 2024. 11 [12] Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. ArXiv preprint, abs/2404.04475, 2024. [13] Tom Kocmi and Christian Federmann. GEMBA-MQM: Detecting translation quality error spans with GPT-4. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the Eighth Conference on Machine Translation, pages 768775. Association for Computational Linguistics, 2023. [14] Tom Kocmi and Christian Federmann. Large language models are state-of-the-art evaluators of translation quality. In Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Sergi Alvarez Vidal, Nora Aranberri, Mara Nunziatini, Carla Parra Escartín, Mikel Forcada, Maja Popovic, Carolina Scarton, and Helena Moniz, editors, Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 193203. European Association for Machine Translation, 2023. [15] Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the Eighth Conference on Machine Translation, pages 10661083. Association for Computational Linguistics, 2023. [16] Keith Frankish. Dual-process and dual-system theories of reasoning. Philosophy Compass, 5(10):914926, 2010. [17] Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. Experts, errors, and context: large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:14601474, 2021. [18] Fengli Xu, Qianyue Hao, Chenyang Shao, Zefang Zong, Yu Li, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, et al. Toward large reasoning models: survey of reinforced reasoning with large language models. Patterns, 6(10), 2025. [19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv preprint, abs/2501.12948, 2025. [20] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. ArXiv preprint, abs/2502.17419, 2025. [21] Aljoscha Burchardt. Multidimensional quality metrics: flexible system for assessing translation quality. In Proceedings of Translating and the Computer 35. Aslib, 2013. [22] Arle Lommel, Serge Gladkoff, Alan Melby, Sue Ellen Wright, Ingemar Strandvik, Katerina Gasova, Angelika Vaasa, Andy Benzo, Romina Marazzato Sparano, Monica Foresi, et al. The multi-range theory of translation quality measurement: Mqm scoring models and statistical quality control. ArXiv preprint, abs/2405.16969, 2024. [23] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. ArXiv preprint, abs/2412.15115, 2024. [24] Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondˇrej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, et al. Findings of the wmt24 general machine translation shared task: the llm era is here but mt is not solved yet. In Proceedings of the Ninth Conference on Machine Translation, pages 146, 2024. 12 [25] Markus Freitag, Nitika Mathur, Daniel Deutsch, Chi-Kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Frédéric Blain, Tom Kocmi, Jiayi Wang, et al. Are llms breaking mt metrics? results of the wmt24 metrics shared task. In Proceedings of the Ninth Conference on Machine Translation, pages 4781, 2024. [26] Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, pages 223231. Association for Machine Translation in the Americas, 2006. [27] Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. Ter-plus: paraphrase, semantic, and alignment enhancements to translation edit rate. Machine Translation, 23:117 127, 2009. [28] Thibault Sellam, Dipanjan Das, and Ankur Parikh. BLEURT: Learning robust metrics for text generation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78817892. Association for Computational Linguistics, 2020. [29] Juraj Juraska, Mara Finkelstein, Daniel Deutsch, Aditya Siddhant, Mehdi Mirzazadeh, and Markus Freitag. MetricX-23: The Google submission to the WMT 2023 metrics shared task. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the Eighth Conference on Machine Translation, pages 756767. Association for Computational Linguistics, 2023. [30] Eirini Chatzikoumi. How to evaluate machine translation: review of automated and human metrics. Natural Language Engineering, 26(2):137161, 2020. [31] Tharindu Ranasinghe, Constantin Orasan, and Ruslan Mitkov. TransQuest at WMT2020: In Loïc Barrault, Ondˇrej Bojar, Fethi Bougares, Rajen Sentence-level direct assessment. Chatterjee, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Yvette Graham, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki Nakazawa, and Matteo Negri, editors, Proceedings of the Fifth Conference on Machine Translation, pages 10491055. Association for Computational Linguistics, 2020. [32] Qingsong Ma, Johnny Wei, Ondˇrej Bojar, and Yvette Graham. Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges. In Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Martins, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Marco Turchi, and Karin Verspoor, editors, Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers), pages 6290. Association for Computational Linguistics, 2019. [33] Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, Alon Lavie, and George Foster. Results of WMT23 metrics shared task: Metrics might be guilty but references are not innocent. In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the Eighth Conference on Machine Translation, pages 578628. Association for Computational Linguistics, 2023. [34] Chi-kiu Lo. YiSi - unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources. In Ondˇrej Bojar, Rajen Chatterjee, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, André Martins, Christof Monz, Matteo Negri, Aurélie Névéol, Mariana Neves, Matt Post, Marco Turchi, and Karin Verspoor, editors, Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers), pages 507513. Association for Computational Linguistics, 2019. [35] Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek Wong, and Lidia Chao. UniTE: Unified translation evaluation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 81178127. Association for Computational Linguistics, 2022. [36] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al. survey on in-context learning. ArXiv preprint, abs/2301.00234, 2023. [37] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1104811064. Association for Computational Linguistics, 2022. [38] Shahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large language models. In The Twelfth International Conference on Learning Representations, ICLR 2024, 2024. [39] Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, and Ondrej Dusek. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source LLMs. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6793. Association for Computational Linguistics, 2024. [40] Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. Generalization or memorization: Data contamination and trustworthy evaluation for large language models. ArXiv preprint, abs/2402.15938, 2024. [41] Brian Thompson, Nitika Mathur, Daniel Deutsch, and Huda Khayrallah. Improving statistical significance in human evaluation of automatic metrics via soft pairwise accuracy. In Barry Haddow, Tom Kocmi, Philipp Koehn, and Christof Monz, editors, Proceedings of the Ninth Conference on Machine Translation, pages 12221234. Association for Computational Linguistics, 2024. [42] Daniel Deutsch, George Foster, and Markus Freitag. Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1291412929. Association for Computational Linguistics, 2023. [43] Alvin Roth. The Shapley value: essays in honor of Lloyd S. Shapley. Cambridge University Press, 1988. [44] Xu Huang, Zhirui Zhang, Xiang Geng, Yichao Du, Jiajun Chen, and Shujian Huang. Lost in the source language: How large language models evaluate the quality of machine translation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 35463562. Association for Computational Linguistics, 2024. [45] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. ArXiv preprint, abs/2412.16720, 2024. [46] Markus Freitag, David Grangier, and Isaac Caswell. BLEU might be guilty but references are not innocent. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6171. Association for Computational Linguistics, 2020. [47] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit S. Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, NanJiang Jiang, Krishna Haridasan, Ahmed Omran, Nikunj Saunshi, Dara Bahri, Gaurav Mishra, Eric Chu, Toby Boyd, Brad Hekman, Aaron Parisi, Chaoyi Zhang, Kornraphop Kawintiranon, Tania Bedrax-Weiss, Oliver Wang, Ya Xu, Ollie Purkiss, Uri Mendlovic, Ilaï Deutel, Nam 14 Nguyen, Adam Langley, Flip Korn, Lucia Rossazza, Alexandre Ramé, Sagar Waghmare, Helen Miller, Nathan Byrd, Ashrith Sheshan, Raia Hadsell Sangnie Bhardwaj, Pawel Janus, Tero Rissa, Dan Horgan, Sharon Silver, Ayzaan Wahid, Sergey Brin, Yves Raimond, Klemen Kloboves, Cindy Wang, Nitesh Bharadwaj Gundavarapu, Ilia Shumailov, Bo Wang, Mantas Pajarskas, Joe Heyward, Martin Nikoltchev, Maciej Kula, Hao Zhou, Zachary Garrett, Sushant Kafle, Sercan Arik, Ankita Goel, Mingyao Yang, Jiho Park, Koji Kojima, Parsa Mahmoudieh, Koray Kavukcuoglu, Grace Chen, Doug Fritz, Anton Bulyenov, Sudeshna Roy, Dimitris Paparas, Hadar Shemtov, Bo-Juen Chen, Robin Strudel, David Reitter, Aurko Roy, Andrey Vlasov, Changwan Ryu, Chas Leichner, Haichuan Yang, Zelda Mariet, Denis Vnukov, Tim Sohn, Amy Stuart, Wei Liang, Minmin Chen, Praynaa Rawlani, Christy Koh, JD Co-Reyes, Guangda Lai, Praseem Banzal, Dimitrios Vytiniotis, Jieru Mei, and Mu Cai. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. CoRR, abs/2507.06261, 2025. [48] Jianhao Yan, Pingchuan Yan, Yulong Chen, Jing Li, Xianchao Zhu, and Yue Zhang. Benchmarking gpt-4 against human translators: comprehensive evaluation across languages, domains, and expertise levels. arXiv preprint arXiv:2411.13775, 2024."
        },
        {
            "title": "A Limitations",
            "content": "Our proposed calibration method relies on synthetic thinking trajectories designed to be human-like and findings-driven. However, these synthetic datasets may not fully capture the diversity of human cognitive processes during MT evaluation. Additionally, since WMT24 includes MQM human ratings for only three language pairs, previous benchmarks have faced risks of data contamination. Our evaluations were primarily conducted using the WMT24 benchmarks, which may not represent all language pairs or domains equally. Lastly, this work follows an understanding and then improving approach. While we focus on analyzing the behavior of the LRM, the current calibration method primarily targets the efficiency of the thinking process (reducing overthinking) and calibrating scoring distributions. More nuanced aspects of the reasoning process, such as the LRMs ability to consistently identify specific error types with fine granularity, may require more targeted or advanced alignment techniques."
        },
        {
            "title": "B Supplementary Details",
            "content": "B.1 Language-specific Scoring Distributions Figure 12 presents all the scoring distributions when evaluating instances of different languages. Figure 12: Distribution of MQM scores for QwQ-32B (top row) and ThinMQM-32B (bottom row) compared to human evaluations across different language pairs. B.2 Data Contamination Prevention To ensure the integrity of our evaluation and prevent data contamination, we implemented the following rigorous measures. For all LRMs evaluated, we rigorously verified that their official knowledge cutoff date or public release date precedes the release date of our evaluation benchmarks. This chronological separation guarantees that the models were not exposed to the test data during their original training phase. Our synthetic training data, ThinMQM, is derived from the WMT23 dataset. The source data utilized for the synthesis of the synthetic set was finalized prior to the public release of the WMT24 MQM evaluation benchmark. This temporal order ensures no overlap between our training data and the final test set. To provide full transparency, we detail the relevant dates for all major components used in this study in Table 6. B.3 Supplementary Ablation Results Effect of ICL Demonstrations We further analyze the effects of ICL on the baseline model in Table 7 using QwQ-32B model. The results indicate that ICL is generally beneficial, improving the 16 Table 6: Knowledge cutoff and release dates for all models, data sources, and benchmarks used in this work. This chronology confirms the prevention of data contamination. Component Type Release Date / Knowledge Cutoff WMT24 MQM Evaluation Benchmark Hindi-Chinese Expert MQM Evaluation Benchmark QwQ 32B R1-Distill-Llama 8B R1-Distill-Qwen 7B WMT"
        },
        {
            "title": "LRM\nLRM\nLRM\nTraining Data Source",
            "content": "Oct 4, 2024 Nov 26, 2024 Sep 19, 2024 (Qwen 2.5 Base) Dec, 2023 (Llama 3.1 Base) Sep 19, 2024 (Qwen 2.5 Base) Aug 10, 2023 Table 7: Effects of ICL. The highest value in each block is bolded. En-De En-Es Ja-Zh Avg. Model SPA (%) Acc eq SPA (%) Acc eq SPA (%) Acc eq SPA (%) Acc eq All Joint (cid:44) w/o ICL Src. (cid:44) w/o ICL Ref. (cid:44) w/o ICL 81.7 80.6 79.8 78.0 84.2 74.8 44.2 42. 46.8 44.8 42.9 42.9 QwQ 32B 68.0 68.0 68.0 68.0 68.0 68.0 92.5 90. 91.9 92.6 94.2 90.0 72.2 68.4 76.2 76.5 68.8 64.0 44.3 43. 46.9 45.7 43.5 43.5 67.2 65.6 68.3 67.6 66.9 63.9 82.1 79. 82.6 82.4 82.4 76.3 52.2 51.5 53.9 52.8 51.5 51.5 average score for both the Joint and Src. settings, with the Src. variant achieving the best overall performance. The effect on the Ref. setting is more nuanced; while ICL significantly boosts the average SPA (%) (82.4 vs. 76.3), it has no discernible impact on the Acc eq score, resulting in an identical average score of 51.5 for both configurations. These results confirm that ICL is generally an effective strategy for improving overall model performance. Table 8: Details of Training configuration. Hyperparameter Value Batch size per device Gradient accumulation steps Learning rate Training epochs Learning rate scheduler Warmup ratio Mixed precision (bfloat16) 2 for 7B/8B, 1 for 32B 4 1.0 105 4.0 cosine 0.1 Enabled Detailed Results of Prompt Variation As shown in Table 9, the ThinMQM model family still establishes strong performance baseline when changing the prompt templates of baselines, outperforming all other tested model variants across all the scales. Our investigation into prompt sensitivity for the QwQ and R1-Distill models reveals inconsistent effects. For the QwQ 32B model, performance is relatively stable, with prompts P1 and P3 matching the GEMBA prompt baseline. Conversely, for the R1-Distill-Qwen 7B model, prompt P1 provides notable improvement, boosting the average score from 61.1 to 62.5. Prompt P2, however, consistently degrades performance across all models. Most strikingly, for both the 8B and 7B models, the Acc eq scores remain completely static regardless of the prompt, suggesting that while prompt engineering can influence SPA (%), it fails to improve Acc eq for these models, supporting the choice of GEMBA prompting template. 17 Table 9: Performance comparison of baseline using different prompts. The highest value in each model is bolded and the second-best is underlined. En-De En-Es Ja-Zh Avg. Model SPA (%) Acc eq SPA (%) Acc eq SPA (%) Acc eq SPA (%) Acc eq All ThinMQM 32B QwQ 32B (Gemba P.) (cid:44) w/ P1 (cid:44) w/ P2 (cid:44) w/ P3 ThinMQM 8B R1D-L. 8B (Gemba P.) (cid:44) w/ P1 (cid:44) w/ P2 (cid:44) w/ P3 ThinMQM 7B R1D-Q. 7B (Gemba P.) (cid:44) w/ P1 (cid:44) w/ P2 (cid:44) w/ P3 83.2 79.8 77.8 77.0 79.3 85.5 71.8 74.5 71.1 70.2 84.5 67.3 70.8 63.1 66. 52.5 46.8 47.4 46.9 46.9 48.6 42.9 42.9 42.9 42.9 48.5 42.9 42.9 42.9 42.9 80.7 76.1 79.3 74.4 78.2 81.3 78.5 72.3 65.5 72.2 77.8 61.0 70.0 58.5 69. 69.2 68.0 68.0 68.0 68.0 68.2 68.0 68.0 68.0 68.0 68.0 68.0 68.0 68.0 68.0 91.3 91.9 89.8 93.7 90.9 90.5 84.7 85.7 84.3 83.3 89.0 83.8 79.6 82.4 81. 56.1 46.9 47.4 48.4 46.4 51.0 43.5 43.5 43.5 43.5 51.3 43.5 43.5 43.5 43.5 85.1 82.6 82.3 81.7 82.8 85.8 78.3 77.5 73.6 75.2 83.8 70.7 73.5 68.0 72. 59.3 72.2 53.9 68.3 54.3 68.3 54.4 68.1 53.8 68.3 55.9 70.8 51.5 64.9 51.5 64.5 51.5 62.6 51.5 63.4 55.9 69.8 51.5 61.1 51.5 62.5 51.5 59.7 51.5 62.1 B.4 Scoring Distribution of Different Evaluation Difficulty Figure 13 presents all the scoring distributions when evaluating instances at varying difficulty level."
        },
        {
            "title": "C Supplementary Setups",
            "content": "C.1 Training Setups We train the 7B/8B models using 4 A100 GPUs and the 32B model using 8 A100 GPUs. To enhance training efficiency, we utilize the DeepSpeed (Zero3) framework7 for offloading. The settings are detailed in Table 8. 7https://github.com/deepspeedai/DeepSpeed 18 Figure 13: Distribution of thinking tokens under different evaluation difficulty. C.2 LRM-as-a-judge Prompt For different prompting templates, we present only one specific evaluation setup as demonstration. For the others, we simply remove or add some information. LRM-as-a-judge Prompt (Adapted from GEMBA-MQM, Joint. Setting) {Source Language} source: {Source Text} {Target Language} human reference: {Reference Text} {Source Language} translation: {Translation Text} Based on the source segment, human reference and machine translation surrounded with triple backticks, identify error types in the translation and classify them. The categories of errors are: accuracy (addition, mistranslation, omission, untranslated text), fluency (character encoding, grammar, inconsistency, punctuation, register, spelling), style (awkward), terminology (inappropriate for context, inconsistent use), non-translation, other, or no-error. Each error is classified as one of three categories: critical, major, and minor. Critical errors inhibit comprehension of the text. Major errors disrupt the flow, but what the text is trying to say is still understandable. Minor errors are technically errors, but do not disrupt the flow or hinder comprehension. Strictly output error classification results in this format: Critical: [error_type]-[error_spans] (one per line, use no-error if empty) Major: [error_type]-[error_spans] (one per line, use no-error if empty) Minor: [error_type]-[error_spans] (one per line, use no-error if empty). C.3 Auxiliary Scoring Model LLM Post-scoring Prompt (Adapted from GEMBA-ESA, Ref. Setting) Given the translation from {Source Language} to {Target Language} and the annotated error spans, assign score on continuous scale from 0 to 100. The scale has following reference points: 0=No meaning preserved, 33=Some meaning preserved, 66=Most meaning preserved and few grammar mistakes, up to 100=Perfect meaning and grammar. Score the following translation from {Source Language} source: {Source Text} {Target Language} translation: {Translation Text} Annotated error spans: {ESA Details} Score (0-100): C.4 ThinMQM Template ThinMQM Template (Src. Setting) INPUT: {Source Language} source: {Source Text} {Target Language} translation: {Translation Text} Based on the source segment and machine translation surrounded with triple backticks, identify error types in the translation, classify them and give score. The categories of errors are: accuracy (addition, mistranslation, omission, untranslated text), fluency (char20 acter encoding, grammar, inconsistency, punctuation, register, spelling), style (awkward), terminology (inappropriate for context, inconsistent use), non-translation, other, or no-error. Each error is classified as one of three categories: critical, major, and minor. Critical errors inhibit comprehension of the text. Major errors disrupt the flow, but what the text is trying to say is still understandable. Minor errors are technically errors, but do not disrupt the flow or hinder comprehension. For each error span, please deduct corresponding points based on following reference points: -25=Cirtical, -5=Major, -1=Minor, -0.1=Minor/Fluency/Punctuation:0.1, 0=No-error. OUTPUT: <think>n Okay, lets tackle this translation quality assessment task. First, need to analyze the translation and classify the errors. Critical: {Critical Error ESA} Major: {Major Error ESA} Minor: {Minor Error ESA} Finally, can calculate the final score: {Score Calculation Process} <think> Score: {Final Score}"
        }
    ],
    "affiliations": [
        "NLP2CT Lab, Department of Computer and Information Science, University of Macau",
        "Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences"
    ]
}