{
    "paper_title": "SkyReels-V4: Multi-modal Video-Audio Generation, Inpainting and Editing model",
    "authors": [
        "Guibin Chen",
        "Dixuan Lin",
        "Jiangping Yang",
        "Youqiang Zhang",
        "Zhengcong Fei",
        "Debang Li",
        "Sheng Chen",
        "Chaofeng Ao",
        "Nuo Pang",
        "Yiming Wang",
        "Yikun Dou",
        "Zheng Chen",
        "Mingyuan Fan",
        "Tuanhui Li",
        "Mingshan Chang",
        "Hao Zhang",
        "Xiaopeng Sun",
        "Jingtao Xu",
        "Yuqiang Xie",
        "Jiahua Wang",
        "Zhiheng Xu",
        "Weiming Xiong",
        "Yuzhe Jin",
        "Baoxuan Gu",
        "Binjie Mao",
        "Yunjie Yu",
        "Jujie He",
        "Yuhao Feng",
        "Shiwen Tu",
        "Chaojie Wang",
        "Rui Yan",
        "Wei Shen",
        "Jingchen Wu",
        "Peng Zhao",
        "Xuanyue Zhong",
        "Zhuangzhuang Liu",
        "Kaifei Wang",
        "Fuxiang Zhang",
        "Weikai Xu",
        "Wenyan Liu",
        "Binglu Zhang",
        "Yu Shen",
        "Tianhui Xiong",
        "Bin Peng",
        "Liang Zeng",
        "Xuchen Song",
        "Haoxiang Guo",
        "Peiyu Wang",
        "Yahui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "SkyReels V4 is a unified multi modal video foundation model for joint video audio generation, inpainting, and editing. The model adopts a dual stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing a powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels V4 accepts rich multi modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi modal instruction following capability with in context learning in the video branch MMDiT, the model can inject fine grained visual guidance under complex conditioning, while the audio branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt a channel concatenation formulation that unifies a wide range of inpainting style tasks, such as image to video, video extension, and video editing under a single interface, and naturally extends to vision referenced inpainting and editing via multi modal prompts. SkyReels V4 supports up to 1080p resolution, 32 FPS, and 15 second duration, enabling high fidelity, multi shot, cinema level video generation with synchronized audio. To make such high resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels V4 is the first video foundation model that simultaneously supports multi-modal input, joint video audio generation, and a unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations."
        },
        {
            "title": "Start",
            "content": "SKYREELS-V4: MULTI-MODAL VIDEO-AUDIO GENERATION, INPAINTING AND EDITING MODEL 6 2 0 2 5 2 ] . [ 1 8 1 8 1 2 . 2 0 6 2 : r SkyReels Team Skywork AI"
        },
        {
            "title": "ABSTRACT",
            "content": "SkyReels-V4 is unified multi-modal video foundation model for joint videoaudio generation, inpainting, and editing. The model adopts dual-stream Multimodal Diffusion Transformer (MMDiT) architecture, where one branch synthesizes video and the other generates temporally aligned audio, while sharing powerful text encoder based on the Multimodal Large Language Models (MMLM). SkyReels-V4 accepts rich multi-modal instructions, including text, images, video clips, masks, and audio references. By combining the MMLMs multi-modal instruction-following capability with incontext learning in the video-branch MMDiT, the model can inject fine-grained visual guidance under complex conditioning, while the audio-branch MMDiT simultaneously leverages audio references to guide sound generation. On the video side, we adopt channel-concatenation formulation that unifies wide range of inpainting-style taskssuch as image-to-video, video extension, and video editingunder single interface, and naturally extends to vision-referenced inpainting and editing via multi-modal prompts. SkyReels-V4 supports up to 1080p resolution, 32 FPS, and 15-second duration, enabling high-fidelity, multi-shot, cinema-level video generation with synchronized audio. To make such high-resolution, long-duration generation computationally feasible, we introduce an efficiency strategy: Joint generation of low-resolution full sequences and high-resolution keyframes, followed by dedicated super-resolution and frame interpolation models. To our knowledge, SkyReels-V4 is the first video foundation model that simultaneously supports multi-modal input, joint videoaudio generation, and unified treatment of generation, inpainting, and editing, while maintaining strong efficiency and quality at cinematic resolutions and durations."
        },
        {
            "title": "Introduction",
            "content": "From the earliest days of cinema, filmmakers have understood that compelling storytelling demands the seamless orchestration of sight and sound. When the Lumière Brothers first projected LArrivée dun train en gare de La Ciotat in 1895, audiences recoiled at the silent image of an approaching locomotive; yet it was not until The Jazz Singer synchronized Al Jolsons voice with his moving lips in 1927 that cinema truly came alive. This historical evolution from silent film to talkies reflects fundamental truth about human perception: vision provides spatial structure and compositional context, while audition conveys temporal rhythm, emotional texture, and narrative continuity. Neither modality alone sufficestheir synergy creates the immersive experiences that define modern media. Over the past year, the field of video generation has witnessed decisive paradigm shift from unimodal synthesis toward joint audio-video generation. Proprietary commercial systems such as Veo-3.1 [1], Sora-2 [2], Kling-2.6 [3], Gen-4.5 [4], Seedance-1.5 [5], Wan-2.6 [6] have transformed video generation capabilities into practical, utility-driven tools that natively produce synchronized audio alongside visual content. These systems mark significant departure from earlier text-to-video (T2V) or video-to-audio (V2A) pipelines, which handled one modality at time and often suffered from audio-visual asynchrony, lip-speech mismatches, and degraded unimodal quality. In parallel, substantial progress has been made in multimodal-referenced video generation, where models accept diverse conditioning inputs beyond text. For instance, Vidu [7] pioneered Reference-to-Video generation, enabling coherent synthesis from multiple referenced images. Runway Aleph [8] introduced state-of-the-art in-context video editing, performing wide range of operationsadding, removing, and transforming objects, generating arbitrary scene angles, and modifying style and lightingdirectly on input videos. In the audio-to-video domain, systems such as Omnihuman-1/1.5 [9, 10], SkyReels-A3 [11, 12], KlingAvatar [13, 14], and Multitalk [15] have demonstrated compelling talking-head synthesis and audio-driven animation. Recently, Kling-Omni [16] emerged as the first model to support both image and video references for video generation, yet it remains limited to visual synthesis without audio output. Alongside these developments, concurrent works including Kling-3.0 [17], Seedance-2.0 [18], and Vidu-Q3 [19] have taken meaningful steps toward bridging this gap, each integrating multimodal inputs with joint videoaudio generation within unified model. Nevertheless, these systems still fall short of fully comprehensive solution. Despite these advances, no existing system simultaneously unifies multimodal inputs (text, images, videos, masks, and audio references), joint videoaudio generation, comprehensive inpainting, and editing capabilities within single framework. Current state-of-the-art models remain fundamentally fragmented: audio-driven systems such as Omnihuman-1/1.5 and Multitalk adopt shallow fusion mechanisms (e.g., cross-attention or lightweight adapters) that fail to fully align audio-visual representations, while multimodal-referenced models such as Kling-Omni focus exclusively on visual conditioning without native audio synthesis. Although recent effortsKling-3.0, Seedance-2.0, and Vidu-Q3have taken meaningful steps toward joint videoaudio generation under multimodal inputs, none of these systems natively integrates comprehensive inpainting and fine-grained editing within unified architecture. The field therefore still lacks unified foundation model capable of seamlessly handling generation, inpainting, and editing conditioned on arbitrary combinations of text, images, videos, masks and audio references. To address these limitations, we present SkyReels-V4, multi-modal video foundation model that jointly generates video and audio while unifying generation, inpainting, and editing within single architecture. SkyReels-V4 is built on dual-stream MMDiT (Multi-Modal Diffusion Transformer) design: one branch is dedicated to video synthesis, and the other to audio generation. The two branches share common text encoder instantiated by strong MMLM that provides multi-modal understanding and instruction-following across text, images, videos and audios. This shared MMLM backbone allows SkyReels-V4 to condition on diverse inputsincluding text, images, videos, and audio referencesin unified, semantically coherent manner. To support broad set of video manipulation tasks, we design the video branch around channel concatenation formulation that treats diverse operations as special cases of inpainting. Specifically, tasks such as image-to-video generation, video extension, and video editing are expressed via masked inputs and additional conditioning channels that are concatenated with the latent representation. This unified inpainting perspective allows SkyReels-V4 to handle heterogeneous workflows within single model, while the underlying MMLM enables visually referenced inpainting: for example, altering characters clothing based on reference image, extending shot while preserving composition from provided frame, or editing specific regions indicated by mask. SkyReels-V4 is designed not only for flexibility but also for cinematic quality. The model supports video generation at up to 1080p resolution, 32 FPS, and 15-second duration, and it can handle multi-shot sequences suitable for film-like storytelling. Achieving such resolutions and lengths with diffusion-based architectures is computationally demanding; naive scaling leads to prohibitive memory and time costs. To overcome this, we introduce an efficiency strategy. Instead of direct 1080p generation, we present joint low-resolution / high-resolution keyframe generation, where the model first produces low-resolution full sequence and high-resolution keyframes, followed by specialized super-resolution and frame interpolation modules that reconstruct temporally consistent, high-resolution video. This design enables SkyReels-V4 to achieve surprisingly high generation speed even for long, high-resolution videos with synchronized audio, making it practical for real-world creative and production environments. To the best of our knowledge, SkyReels-V4 is the first system worldwide that simultaneously supports (i) rich multimodal inputs (text, images, video, masks, and audio), (ii) joint videoaudio generation, and (iii) unified framework for generation, inpainting, and editing, while scaling effectively to high-resolution, long-duration outputs. This combination of capabilities positions SkyReels-V4 as versatile foundation model for next-generation video creation and editing. Extensive experiments demonstrate the superior performance of SkyReels-V4 compared to current state-of-the-art methods. Our model achieves state-of-the-art results on the Artificial Analysis Arena [20]. Comprehensive human evaluation through SkyReels-VABench reveals that SkyReels-V4 significantly outperforms proprietary commercial systems, with particular strengths in instruction following, motion quality, and complex multi-shot narratives. Additionally, the model demonstrates robust performance across diverse multimodal conditioning tasks including reference-to-video, motion-tovideo, and video editing, validating its effectiveness as unified foundation model for professional video-audio content creation. In summary, our contributions are: We introduce SkyReels-V4, dual-stream MMDiT-based foundation model that jointly generates video and audio under multi-modal instruction and reference inputs. We propose unified channel-concatenation inpainting framework for video, enabling image-to-video, video extension, video editing, and vision-referenced inpainting within single architecture. 2 Figure 1: Overview of the proposed method. We design an efficiency scheme Joint low-res / high-res keyframe generation with super-resolution and interpolationthat makes 1080p, 32 FPS, 15-second, multi-shot video generation with synchronized audio computationally feasible. We demonstrate that SkyReels-V4 is, to our knowledge, the first model to unify multi-modal input, joint videoaudio generation, and generation/inpainting/editing tasks at cinematic quality and speed, setting new baseline for multi-modal video foundation models."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Video Generative Models Diffusion models have transformed video generation, evolving from early 2D+1D architectures like Video Diffusion Models [21] and AnimateDiff [22] to DiT-based frameworks [23]. Sora [24] demonstrated the effectiveness of largescale training with spatiotemporal attention. While closed-source systems (Veo-3.1 [1], Kling-O1 [16], Sora-2 [2], Hailuo-2.3 [25], Gen-4.5 [4]) lead commercially, open-source modelsCogVideoX [26], HunyuanVideo [27, 28], WAN-2.1/2.2 [29], SkyReels series [30, 31, 32, 33, 34], LTX [35, 36], MAGI-1 [37]are rapidly narrowing the gap through data scaling and quality improvements. 2.2 Video-Audio Generative Models Joint text-to-audio+video (T2AV) generation aims to synthesize synchronized audiovisual content from text. Commercial systems (Veo-3.1 [1], Sora-2 [2], Kling-3.0 [17]) show strong capabilities but lack transparency. Open-source approaches evolved from coupled U-Nets [38] to DiT-based methods: adapter-based (AV-DiT [39]), expert-orchestration (MMDisCo [40], Universe-1 [41]), and dual-stream architectures (Ovi [42], BridgeDiT [43], JavisDiT [44]) using cross-attention or flow matchingthough these incur high computational costs. LTX-2 [36] proposes asymmetric streams for efficiency. Unified single-tower models like Apollo [45] process audio-video tokens jointly via Omni-Full Attention, enabling multitask training (T2AV/TI2AV/TI2V) with tighter coupling. Despite progress, synchronized speech-video synthesis and complete soundscapes remain underexplored, with precise spatio-temporal alignment an open challenge."
        },
        {
            "title": "3 Model Design",
            "content": "We present SkyReels-V4, unified multi-modal video foundation model for joint video-audio generation, inpainting, and editing. The model adopts dual-stream MMDiT architecture with rich multi-modal instruction following, enabling seamless integration of text, image, video, mask, and audio conditioning signals while maintaining computational efficiency across cinematic resolutions and durations. The overview of the model architecture is shown in Figure 1. 3 3.1 Dual-Stream MMDiT Architecture for Joint Video-Audio Generation Our architecture employs symmetric twin backbone design with parallel video and audio branches, both constructed on an identical Multimodal Diffusion Transformer (MMDiT) framework. The video branch is initialized from pretrained text-to-video model, while the audio branch is trained from scratch with matching architectural specifications. Hybrid Dual-Stream and Single-Stream MMDiT Blocks. Following the MMDiT design, each transformer block processes video (or audio) and text modalities through hybrid architecture that balances modality alignment with parameter efficiency. The initial layers employ Dual-Stream design where video/audio and text tokens maintain separate parameters for adaptive layer normalization, QKV projections, and MLPs, but interact during joint selfattention: Qv, Kv, Vv = QKVv(LayerNormv(xv)), Qt, Kt, Vt = QKVt(LayerNormt(xt)), v, x = Attention([Qv; Qt], [Kv; Kt], [Vv; Vt]), (1) (2) (3) where xv and xt denote video/audio and text token embeddings, respectively, and [; ] represents concatenation. This design facilitates strong cross-modal alignment during early layers. The subsequent layers transition to SingleStream architecture that processes concatenated video (or audio) and text tokens with shared parameters, maximizing computational efficiency. This hybrid strategy achieves faster convergence than either pure approach. Reinforced Text Conditioning via Cross-Attention. To address potential semantic dilution of text features in the single-stream stages, we augment the video block with an additional text cross-attention layer immediately following self-attention: = x + Attention(Q = v, = xt, = xt), (4) where the video stream queries the text embeddings, reinforcing textual guidance throughout the generation process. This cross-attention mechanism is crucial for maintaining fine-grained semantic control in later model stages. Bidirectional Audio-Video Cross-Attention. To enable temporal synchronization between modalities, each transformer block incorporates paired cross-attention layers: the audio stream attends to video features, and the video stream reciprocally attends to audio features. This bidirectional mechanism exchanges synchronization cues throughout the entire network depth: = ai + CrossAttn(Q = ai, = vi, = vi), = i), + CrossAttn(Q = i, = i, = (5) where ai and vi are audio and video features at layer i. The architectural symmetry ensures both modalities share the same latent dimension, eliminating the need for intermediate projection layers and preserving the attention structure from unimodal pretraining. Temporal Alignment via RoPE Scaling. Despite architectural symmetry, the temporal resolutions differ: video latents span 21 frames while audio latents contain 218 tokens (44.1 kHz 5s). To align these temporal scales, we apply Rotary Positional Embeddings (RoPE) to both modalities and scale the audio RoPE frequencies by 21/218 0.09633 to match the videos coarser temporal resolution. This ensures that audio and video tokens attend to each other with temporally consistent correspondence. Shared Multi-Modal Text Encoder. We simplify prompt conditioning by employing single frozen MMLM text encoder applied to combined prompt that concatenates visual and acoustic descriptions. The resulting multi-modal embeddings are independently consumed by both audio and video branches via self-attention and cross-attention. This unified semantic context improves cross-modal alignment while simplifying training and inference, and crucially enables the model to process rich multi-modal instructions including text, images, video clips, and audio references. and audio latent z0 Training Objective. We adopt flow matching framework for training. Given video latent z0 a, + (1 t)ϵv and zt we sample timestep (0, 1) and construct noisy latents zt = tz0 + (1 t)ϵa, where ϵv, ϵa (0, I). The model predicts the velocity field vθ that pushes noise toward data: = tz0 Lflow = Et,z0 v,z0 a,ϵv,ϵa (cid:104)(cid:13) (cid:13)vv θ (t, zt v, zt a, c) (z0 ϵv)(cid:13) 2 (cid:13) + (cid:13) (cid:13)va θ (t, zt a, zt v, c) (z0 ϵa)(cid:13) (cid:13) 2(cid:105) , (6) 4 where denotes the conditioning information (multi-modal embeddings and optional spatial-temporal masks). The joint training objective encourages both branches to learn synchronized generation while respecting their respective modality-specific characteristics. 3.2 Unified Video Inpainting via Channel Concatenation To enable diverse video generation and editing tasks within single framework, we employ flexible input conditioning mechanism applied to the video branch. The input to the video MMDiT is formed by concatenating three tensors along the channel dimension: Zinput = Concat(V, I, M), (7) where RT HW is the noisy video latent, RT HW contains VAE-encoded conditional frames (with non-conditional frames filled with black image latents), and RT HW 1 is binary mask specifying which spatiotemporal regions are conditions (value 1) versus regions to be generated (value 0). This formulation unifies multiple generation tasks through different mask configurations: Text-to-Video (T2V): = 0 (all frames generated) Image-to-Video (I2V): Mt=0 = 1, Mt>0 = 0 (first frame conditioned) Video Extension: Mt<k = 1, Mtk = 0 (first frames conditioned) Start-End Frame Interpolation: Mt=0 = Mt=T 1 = 1, others 0 Video Editing: Mt,h,w = 1 for preserved regions, 0 for edited regions (arbitrary spatiotemporal masks) This unified formulation naturally accommodates both fixed foreground/background masks and dynamic per-frame editing masks, enabling precise control over spatial and temporal modifications. Crucially, the inpainting mechanism is applied exclusively to the video stream. During inpainting and editing tasks, the audio branch generates audio from scratch conditioned on the (partially conditioned or edited) video content, ensuring acoustic consistency with the generated or modified visual content. This design allows the audio to adapt seamlessly to video modifications while maintaining temporal synchronization through the bidirectional cross-attention mechanism. 3.3 Multi-Modal In-Context Learning for Vision-Referenced Generation and Editing Beyond text and inpainting masks, our framework supports rich multi-modal conditioning through reference images and video clips, enabling complex vision-referenced generation tasks such as multi-identity video generation and identity-preserving video editing under multi-modal prompts. Multi-Modal Instruction Following via MLLM. Reference visual inputs (images or video frames) are jointly processed with the text prompt through the MMLM text encoder to extract semantically enriched multi-modal embeddings. The MLLMs instruction-following capability enables the model to understand complex compositional requests that combine visual references with textual descriptions (e.g., generate video of person from the reference @image_1 speaking <dialogue>hello, how are you?</dialogue> in the style of person Bs @video_1). These multi-modal embeddings are consumed by both the video and audio branches. In-Context Visual Conditioning via Self-Attention. To provide explicit visual reference signals beyond semantic guidance, we inject reference visuals directly into the video self-attention mechanism. Each reference image or video frame is encoded via the VAE, padded to uniform spatial resolution, and concatenated along the temporal dimension. These condition latents Zcond are prepended to the noisy video latents Zvideo before self-attention: Zattn = [Zcond; Zvideo], (8) where the concatenated sequence undergoes joint self-attention. This in-context learning enables the model to directly reference fine-grained visual patterns (e.g., identity characteristics, texture details, pose variations) from the conditions when generating or editing video content. Temporal Positional Disambiguation via Offset 3D RoPE. To distinguish condition latents from noisy video latents and organize multiple reference visuals, we employ 3D Rotary Positional Embeddings with temporal index offsets. Condition latents receive negative temporal indices, sequentially encoding each reference visual before the generated video frames: 5 RoPEtemporal(Zcond,i) = RoPE(t = Ncond + i), RoPEtemporal(Zvideo,j) = RoPE(t = j), (9) where Ncond is the total number of condition tokens and i, index the condition and video tokens respectively. Spatial indices are preserved across all tokens, ensuring that attention relationships respect both spatial and temporal structure. This offset-based positional encoding provides an effective inductive bias for distinguishing conditioning context from generation targets without introducing task-specific architectural modifications, and naturally extends to multiple reference visuals of varying types (images, short clips, etc.). Audio Reference Conditioning. Similarly, audio references (e.g., speech samples, musical themes, ambient soundscapes) are encoded and processed as in-context conditions for the audio branch. By combining multi-modal semantic guidance from the MLLM with in-context visual patterns from the video branch and audio patterns from audio references, the model achieves fine-grained control over both visual and acoustic generation. 3.4 Data Pipeline Our data pipeline consists of three main components: data collection, data processing, and captioning. This pipeline handles three modalitiesimages, videos, and audioto support multimodal model training. 3.4.1 Data Collection Our training data comprises both real-world and synthetic data across three modalities: images, videos, and audio. Real-world Data. We collect real-world data from two primary sources: publicly available datasets and licensed inhouse data. Public datasets include images (LAION [46], Flickr [47], etc.), videos (WebVid-10M[48], Koala-36M [49], OpenHumanVid [50], etc.), and audio (Emilia [51], AudioSet [52], VGGSound [53], SoundNet [54], etc.). Our in-house licensed data encompasses authorized movies, TV series, short videos, and web series. Synthetic Data. We generate synthetic data to address sparse scenarios and generation tasks inadequately covered by real-world data. We focus on three key areas: multilingual text generation, multilingual speech synthesis and multimodal inpainting/editing tasks. For text generation, we construct synthetic data covering multiple languages, including Chinese, English, Japanese, Korean, German, French, etc. Our synthetic image-text data includes simple text rendering and context-aware text generation that preserves font characteristics. For video-text data, we generate basic text effect videos and context-aware text with attributes matching reference styles (font, size, color) and motion characteristics (trajectories, special effects). To enhance speech generation and multilingual coverage, we employ multiple TTS models spanning various languages. We curate diverse text corpora to ensure the model learns pronunciations beyond common characters, including rare and uncommon scripts. For multimodal inpainting and editing tasks, paired training data is inherently unavailable in real-world datasets. We therefore construct these data through sophisticated pipeline involving visual segmentation models, image/video editing models, and controllable generation techniques. 3.4.2 Data Processing Our data processing pipeline is tailored to three data types: images, pure audio, and videos (with or without audio). Image Data Processing. The image processing pipeline consists of three stages: deduplication, filtering, and balancing. Deduplication employs strict image-level matching. Filtering includes basic quality metrics (resolution, IQA scores, aspect ratio, etc.) and content quality criteria (watermarks, stamps, logos, overly small text, etc.). For data balancing, we adopt stage-specific strategies: during pretraining, we compute image embedding similarities, perform clustering, and balance cluster proportions; during supervised fine-tuning (SFT), we define entity and scene categories, matching them against captions for fine-grained balancing. Audio Data Processing. The audio processing pipeline includes: category classification, quality filtering, duration control, content recognition, and audio captioning. First, we classify audio into four categoriessound effects, music, speech, and singingusing Qwen3-Omni [55]. Next, we perform quality filtering based on SNR, MOS score, clipping ratio, and audio bandwidth. We use voice activity detection (VAD) to select audio with silence ratios below 0.2. For duration control, we segment long audio clips into 15-second chunks and concatenate short clips by category to reach 15 seconds. For speech and singing categories, we employ Whisper to transcribe spoken and sung content. Finally, we uniformly caption all audio using Qwen3-Omni. 6 Video Data Processing. Video processing consists of four stages: preprocessing (segmentation and deduplication), filtering, balancing, and audio-video synchronization for videos with audio tracks. Preprocessing. Traditional methods using PyDetect and TransNet-V2 [56] produce scene-cut clips that often lack narrative coherence. Instead, we adopt intelligent segmentation that combines TransNets shot boundary predictions via VLM to extract semantically complete segments, including both long takes and multi-shot clips. In later training stages, we further apply video highlight detection to identify segments with richer content. We deduplicate segmented clips using VideoCLIP embeddings [57]. Filtering. We filter videos based on three quality dimensions: basic quality (duration, resolution, aesthetic score, blur, contrast, exposure), content quality (watermarks, logos, text overlays, synthetic artifacts, content type/source issues), and motion quality (camera stability, motion magnitude/speed, frame drops). Balancing. To improve training efficiency, we balance data along two dimensions: conceptual diversity and motion diversity. We define taxonomy of concept labels covering different subjects and scene types, matching them against video content for concept balancing. We further balance motion types by defining key motion patterns for each subject or scene category. Audio-Video Synchronization. Following the audio pipeline, we obtain preliminary audio captions. For videos containing speech or singing, we determine whether the video is person-free (background audio), single-person, or multi-person based on the first frame, and apply audio-visual synchronization filtering accordingly. We adopt the widely-used SyncNet [58] model, which uses ConvNet architecture to learn joint embeddings between sound and mouth images, to filter speech videos lacking sufficient audio-video synchronization. We adapt the model to handle millions of video samples and produce scalar confidence and offset values. We retain only clips satisfying offset 3 confidence > 1.5 with minimum mean volume of -60 decibels. Finally, we integrate audio and video captions into unified descriptions. 3.4.3 Captioning We generate three types of captions: short captions, long captions, and structured captions. Short captions provide concise descriptions of video content and audio information. Long captions offer comprehensive descriptions of environment, subjects, lighting, atmosphere, and other nuanced details. Structured captions follow standardized descriptive order with special tokens to denote in-video text(<text></text>), sound effects(<sfx></sfx>), speech content(<dialogue></dialogue>), singing content(<singing></singing>), and background music(<bgm></bgm>). In final training stages, we exclusively use structured captions. To align user prompts with this format, we employ prompt enhancer that reformats free-form input into the structured representation."
        },
        {
            "title": "4 Training Strategy",
            "content": "We adopt progressive multi-stage training paradigm that systematically develops the models capabilities across multiple modalities and tasks. Our training pipeline consists of three major phases: Video Pretrain, Audio Pretrain, and Video-Audio Joint Training, followed by supervised fine-tuning. This structured approach enables the model to learn spatial concepts, temporal dynamics, audio generation, and multi-modal alignment in stable and efficient manner. Table 1 summarizes our complete multi-stage training schedule, including tasks, resolutions, data volumes, and training epochs for each stage. 4.1 Video Pretrain The video pretraining phase follows progressive strategy that gradually increases spatial resolution, temporal length, and task complexity. We begin with text-to-image (T2I) training to establish strong semantic understanding and visual concept learning, which we find significantly accelerates subsequent video training convergence. Stage 1: Text-to-Image Foundation. We first train the T2I task at 256px resolution using 3 billion images for 3 epochs. This stage enables the model to learn the correspondence between text and visual content, establishing solid foundation for spatial composition and concept formation. Stage 2: Initial Video Learning. We introduce text-to-video (T2V) generation while maintaining T2I training. At 256px resolution and 16 fps, we train on 1 billion images and 400 million videos for 3 epochs, with video durations ranging from 2 to 10 seconds. Training at lower resolution allows the model to more rapidly converge on motion dynamics and temporal coherence. Table 1: Complete training schedule across all stages. The progressive strategy gradually increases resolution, temporal length, and task complexity. Task Stage Resolution Data Volume Epochs T2I T2I + T2V T2I + T2V + Inpaint (Image Inpaint, I2V, V2V, Edit) Mixed Tasks (T2I, T2V, Inpaint) Mixed Tasks (T2I, T2V, Inpaint) Multi-modal Condition (Image/Video Ref: 20% each) (T2V: 60%) Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 Video Pretrain 256px 256px, 16fps, 2-10s 256px, 16fps, 2-15s (Inpaint: 5% each) 256/480px, 16fps, 2-15s (Inpaint ratio unchanged) 480/720/1080px, 16fps, 3-15s 480/720/1080px, 16fps, 3-15s Audio Pretrain 3B images 1B images / 400M videos 1B images / 400M videos 100M images / 100M videos 50M images / 50M videos 20M images / 50M videos Audio Backbone Pretrain Variable length, up to 15s Hundreds of thousands of hours Video-Audio Joint Training T2V + T2AV + T2A Joint Pretrain 720/1080px, 16fps, 5-15s 50% video data + T2A data Video-Audio Supervised Fine-tuning T2AV + Multi-modal T2AV + Multi-modal SFT Stage 1 SFT Stage 2 720/1080px, 16fps, 5-15s 720/1080px, 16fps, 5-15s 5M videos (Multi-modal: 20%) 1M curated videos 3 3 2 2 2 2 2 3 3 Stage 3: Inpainting Capabilities. We expand the task set to include image inpainting, image-to-video (I2V), video-tovideo (V2V), and video editing tasks, each comprising 5% of the training mix. This stage trains for 2 epochs with video durations extended to 215 seconds, enabling the model to learn spatial and temporal inpainting capabilities. Stage 4: Mixed Resolution Scaling. We employ mixed-resolution training at 256px and 480px, maintaining 16 fps and 215 second durations. Training on 100 million images and 100 million videos, we keep the inpainting task ratio unchanged, allowing the model to gradually adapt to higher resolution generation. Stage 5: High Resolution Training. We further scale to mixed resolutions of 480px, 720px, and 1080px at 16 fps, with video durations of 315 seconds. This stage uses 50 million images and 50 million videos, substantially improving the models high-resolution generation quality. Stage 6: Multi-modal Condition Pretrain. We introduce image reference and video reference conditioning for both generation and inpainting tasks, comprising 20% of the training data each, with the remaining 60% dedicated to T2V. This stage trains on 20 million images and 50 million videos, equipping the model with flexible multi-modal conditioning capabilities. 4.2 Audio Pretrain The audio backbone is pretrained from scratch on hundreds of thousands of hours of primarily speech data, with durations up to 15 seconds. During pretraining, we use variable-length audio to maximize coverage of diverse acoustics, providing the audio backbone with broad exposure to natural variability in duration and content. The long-form raw audio enables the model to generate consistent audio that respects speaker traits such as pitch and emotion. 4.3 Video-Audio Joint Training Following the completion of video and audio pretraining, we enter the joint training phase, simultaneously training three tasks: text-to-video (T2V), text-to-audio-video (T2AV), and text-to-audio (T2A). In this phase, we allocate half of the video pretrain data to T2AV joint training while incorporating T2A data to enable synchronized audio-visual generation. 4.4 Video-Audio Supervised Fine-tuning In the final SFT stage, we focus exclusively on joint generation data, training on 5 million videos with multi-modal condition support (image, video, and audio), which comprises 20% of the data. We conclude with final fine-tuning step on 1 million manually curated high-quality videos, further refining generation quality, motion coherence, and audio-visual alignment. 8 Figure 2: The pipeline of the video super-resolution and frame interpolation method. denotes the output latent of our base model. KF demotes the key frames latent of our base model. 4.5 Video Super-Resolution and Frame Interpolation (Refiner) To further enhance visual quality and temporal smoothness of generated videos, we introduce dedicated Refiner module that jointly performs video super-resolution (VSR) and frame interpolation, as shown in Fig. 2. This cascaded architecture operates on the outputs of the base multi-modal video generation model, leveraging both low-resolution results and high-resolution key-frame results to synthesize high-fidelity, fine-grained visual details while simultaneously increasing temporal resolution. Architecture and Design. We initialize the Refiner weights from the pre-trained video generation model to ensure effective knowledge transfer and training stability. The Refiner accepts three categories of inputs: (1) multi-modal visual conditions (image, video, and audio references at high resolution), (2) multi-modal text instructions consistent with the base model, and (3) the base model outputs, which include low-resolution predictions for all frames and high-resolution predictions for keyframes. To support the efficient inference strategy described earlier, we incorporate joint prediction task during post-training, where the base model learns to simultaneously predict all frames at low resolution and keyframes at high resolution. With these inputs, we first linearly interpolate the low-resolution latents to the target high resolution. Then, for keyframe positions, we replace the interpolated results with the directly predicted high-resolution keyframe latents from the base model. Finally, these combined latents are concatenated with high-resolution noisy latents along the channel dimension as input to the DiT model. To support multi-task generation, inpainting, and editing capabilities in the Refiner, we design unified framework. For inpainting tasks, we incorporate the high-resolution version of the source video, using it to replace the interpolated regions where inpainting is not required. spatial mask guides the model to distinguish between regions requiring refinement and those that should remain unchanged. This design enables the Refiner to handle both unconditional super-resolution and conditional inpainting across multiple modalities. Computational Efficiency. To address the computational overhead imposed by long temporal contexts and highresolution inputs, we adopt Video Sparse Attention (VSA) [59], trainable sparse attention mechanism designed for video diffusion transformers. VSA employs hierarchical two-stage approach: coarse stage that aggregates spatial-temporal cubes to identify critical token regions through lightweight pooled attention, and fine stage that applies dense attention only within the selected top-K cubes. This design eliminates the need to compute full quadratic attention while maintaining hardware efficiency through block-sparse layouts compatible with modern GPU kernels. By exploiting spatio-temporal redundancy in learnable manner, VSA enables us to reduce attention computational cost by approximately 3 while preserving generation quality, making it practical to process high-resolution video sequences during both training and inference. Training Data and Configuration. For dataset construction, we curate 1 million high-quality video clips spanning diverse scenarios and resolutions from 1K to 4K. We incorporate high-resolution images alongside video data to enhance the models capability for generating fine visual details. The task composition mirrors the base models multi-task structure, maintaining consistent ratios for generation, inpainting, and editing tasks. All weights of the Refiner are fully trainable throughout the training process, following the flow matching paradigm."
        },
        {
            "title": "5 Model Performance",
            "content": "We evaluate model performance on public arena leaderboard to assess overall user preference in an open-ended setting. Beyond this, we conduct comprehensive human assessments spanning five key dimensions: Instruction Following, Audio-Visual Synchronization, Visual Quality, Motion Quality, and Audio Quality, providing fine-grained analysis of model capabilities. Furthermore, our multimodal inpainting and editing framework unlocks wide range of practical applications, including but not limited to subject insertion, object removal, background replacement, and reference-guided video synthesis. We showcase representative examples of these applications in Appendix A. 5.1 Artificial Analysis Arena Artificial Analysis [20] is widely recognized benchmarking platform for evaluating generative models across image and video generation domains. The platform operates an open arena where models are scored by the public, with Elo scores calculated from pairwise comparisons to reflect user preferences. We evaluate our model on the Artificial Analysis Video Arena, specifically on the text-to-video with audio generation track, which is designed to assess the quality of joint videoaudio synthesis. Our model is benchmarked against notable external baselines including Veo 3.1, Kling 3.0, grok-imagine-video, Sora-2, Vidu-Q3, Wan 2.6, etc. Results: Our model ranks third on the leaderboard (as of 2026-02-24) among all participating systems (Figure 3), demonstrating strong and competitive audiovisual generation quality as evaluated by public user preferences. Figure 3: Artificial Analysis Text-to-Video with Audio Arena Leaderboard. Our model ranks third among all competing baselines including Veo 3.1, grok-imagine-vide, Sora-2, Vidu-Q3, Wan 2.6 and etc. 5.2 Human Assessments To comprehensively assess the joint video-audio generation capabilities, we introduce SkyReels-VABench, novel human evaluation benchmark designed to evaluate state-of-the-art text-to-video+audio models in the market. 5.2.1 Benchmark Design SkyReels-VABench extends our previous SkyReels-Bench [32] by incorporating comprehensive audio dimensions and multi-shot video scenarios. The benchmark comprises 2000+ carefully curated prompts spanning diverse content categories including advertising, social media content, narrative storytelling, educational content, and entertainment. 10 The prompts are designed to test models across varying complexity levels, from single-shot scenarios to complex multi-shot sequences with sophisticated audio requirements. Language Coverage: The benchmark includes prompts in multiple languages, with particular emphasis on Chinese and English to assess cross-lingual generation capabilities. Content Diversity: Prompts cover wide range of subjects (humans, animals, objects, abstract concepts), environments (indoor, outdoor, natural, urban), and temporal dynamics (static, slow-motion, fast-action sequences). Audio Complexity: The benchmark tests various audio modalities including speech (monologue, dialogue, narration), singing (various genres and vocal styles), sound effects (environmental, mechanical, natural), and background music (various genres and emotional tones). 5.2.2 Evaluation Metrics Our evaluation framework encompasses five primary dimensions: Table 2: Comprehensive Evaluation Dimensions for Audio-Visual Generation Dimension Sub-dimension Evaluation Criteria Instruction Following Video Instruction Following Subject description Subject interaction Camera movement Style and aesthetics Multi-shot consistency Accurate representation of subjects, attributes, and appearances Correct execution of actions, interactions, and motion dynamics Proper execution of camera operations (pan, tilt, zoom, dolly) Adherence to visual styles, color palettes, and artistic directions Correct shot transitions, cross-shot coherence, and reference accuracy Audio Instruction Following Semantic adherence Temporal accuracy Speaker attributes Fidelity to audio content and characteristics Correct timing and duration of audio events Speaker-visual matching, vocal characteristics, emotional tone, content accuracy Audio-Visual Synchronization Lip-sync accuracy Sound effect alignment Temporal correspondence between visual events and sound effects Coherence between BGM, scene atmosphere, and emotional tone Atmospheric matching Sound spatialization matching visual source locations Spatial audio Precise speech-mouth synchronization and correct speaker identification Visual Quality Motion Quality Audio Quality Visual clarity Color accuracy Compositional quality Structural integrity Physical plausibility Motion fluidity Motion stability Temporal consistency Motion vividness Absence of artifacts Spatial soundstage Timbre realism Signal clarity Dynamic range Sharpness, definition, and resolution Natural color balance and saturation without distortion Aesthetic composition, framing, and visual balance Absence of visual artifacts and corruptions Adherence to physical laws (gravity, inertia, momentum) Smooth transitions without abrupt discontinuities Absence of jittering, deformation, and flickering Consistency of dynamic elements across frames Action, camera, atmospheric, and emotional expressiveness No clipping, truncation, distortion, or glitches Appropriate stereo imaging and spatial rendering Natural and realistic tonal qualities Clean audio with appropriate signal-to-noise ratio Appropriate audio level variation without compression artifacts 5.2.3 Evaluation Methodology We employ dual-metric evaluation protocol conducted by panel of 50 professional evaluators with backgrounds in video production, audio engineering, and content creation: Absolute Scoring: Evaluators rate each dimension using 5-point Likert scale (1 = Extremely Dissatisfied, 2 = Dissatisfied, 3 = Neutral, 4 = Satisfied, 5 = Extremely Satisfied), enabling standardized performance comparison across models. 11 Figure 4: Absolute scoring results (5-point Likert scale) comparing SkyReels V4 against baselines. Higher scores indicate better performance. Good-Same-Bad (GSB) Comparison: Pairwise comparisons between model outputs enable more granular quality differentiation. For each prompt, evaluators compare outputs from different models and assign one of three labels: \"Good\" (clearly better), \"Same\" (comparable quality), or \"Bad\" (clearly worse). 5.2.4 Baselines We compare our model against state-of-the-art video-audio generation systems, including Veo 3.1 (Google) Kling 2.6 (Kuaishou) Seedance 1.5 Pro (ByteDance) Wan 2.6 (Alibaba) 5.2.5 Results Absolute Scoring. We first evaluate all models using the absolute scoring protocol, where human evaluators rate each dimension on 5-point Likert scale. As shown in Figure 4, SkyReels V4 achieves the highest overall average score among all competing models. The per-dimension breakdown reveals nuanced picture of SkyReels V4s strengths: it demonstrates particularly strong performance in Prompt Following and Motion Quality. For Visual Quality, SkyReels V4 performs comparably to the strongest competing models. While SkyReels V4 shows relatively modest advantages in Audio-Visual Synchronization and Audio Quality, it nonetheless maintains state-of-the-art performance in these dimensions as well, underscoring its overall competitiveness across the full evaluation spectrum. Good-Same-Bad (GSB) Comparison. To further validate our models superiority, we conduct pairwise GSB comparisons between SkyReels V4 and each baseline. As illustrated in Figure 5, SkyReels V4 consistently achieves higher proportion of Good ratings against all competing models in terms of overall quality. The per-dimension GSB results for each pairwise comparison are presented, demonstrating that SkyReels V4 outperforms Kling 2.6, Seedance 1.5 Pro, Veo 3.1, and Wan 2.6 across the majority of evaluation dimensions."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present SkyReels-V4, unified multi-modal video foundation model that jointly generates video and audio while supporting generation, inpainting, and editing within single architecture. Built upon dual-stream MMDiT design with shared MMLM-based text encoder, SkyReels-V4 accepts rich multi-modal conditioning inputs including text, images, video clips, masks, and audio referencesand produces high-fidelity, synchronized videoaudio outputs at cinematic quality (up to 1080p, 32 FPS, 15 seconds). To support diverse video creation tasks, we employ channel-concatenation to unify generation, inpainting, and editing by reformulating them as inpainting problems under specific mask configurations, while leveraging temporal-concatenation to flexibly incorporate multi-modal references such as images, video clips, and audio. Additionally, our joint low-resolution/high-resolution keyframe generation strategy enables efficient generation at scale. 12 GSB overall quality comparison: SkyReels V4 vs. all baselines. Each bar shows the proportion of Good, Same, and Bad ratings. (a) SkyReels V4 vs. Kling 2.6 (b) SkyReels V4 vs. Seedance 1.5 Pro (c) SkyReels V4 vs. Veo 3. (d) SkyReels V4 vs. Wan 2.6 Figure 5: GSB comparison results. Top: Overall quality comparison between SkyReels V4 and all baselines. Bottom: Per-dimension GSB comparison across five evaluation dimensions: Prompt Following, Audio-Visual Synchronization, Visual Quality, Motion Quality, and Audio Quality. 13 Extensive evaluations validate SkyReels-V4s effectiveness. On the Artificial Analysis Arena, our model ranks among the top systems in the text-to-video-with-audio track. On our proposed SkyReels-VABench, SkyReels-V4 achieves the highest overall average score, with particularly strong performance in Prompt Following and Motion Quality, while maintaining state-of-the-art performance across all evaluation dimensions. Pairwise comparisons further confirm that SkyReels-V4 consistently outperforms competing baseline systems. To the best of our knowledge, SkyReels-V4 is the first model to simultaneously unify multi-modal inputs, joint video audio generation, and generation/inpainting/editing capabilities at cinematic quality and scale. We hope this work serves as foundation for future research in multi-modal video generation systems."
        },
        {
            "title": "7 Contributors",
            "content": "We gratefully acknowledge all contributors for their dedicated efforts. The following lists recognize participants by their primary contribution roles: Project Sponsor: Yahui Zhou Project Leader: Guibin Chen (guibin.chen@kunlun-inc.com) Contributors: Infrastructure: Hao Zhang, Zhiheng Xu, Weiming Xiong, Yuzhe Jin, Zhuangzhuang Liu, Wenyan Liu Data & Video Understanding: Mingyuan Fan, Yiming Wang, Mingshan Chang, Jiahua Wang, Yuqiang Xie, Peng Zhao, Xuanyue Zhong, Fuxiang Zhang, Peiyu Wang Video Model Training: Dixuan Lin, Jiangping Yang, Sheng Chen, Chaofeng Ao, Yunjie Yu, Jujie He, Yuhao Feng, Shiwen Tu, Chaojie Wang, Rui Yan, Wei Shen, Jingchen Wu, Weikai Xu Audio Model Training: Zhengcong Fei, Zheng Chen, Tuanhui Li, Baoxuan Gu, Kaifei Wang, Xuchen Song Multi-modal Training: Youqiang Zhang, Debang Li, Nuo Pang, Yikun Dou, Xiaopeng Sun, Jingtao Xu, Binjie Mao, Liang Zeng, Haoxiang Guo Model Evaluation: Binglu Zhang, Yu Shen, Tianhui Xiong, Bin Peng"
        },
        {
            "title": "References",
            "content": "[1] DeepMind. Veo-3.1. Oct. 15, 2025. URL: https://aistudio.google.com/models/veo-3. [2] OpenAI. Sora-2. Oct. 15, 2025. URL: https://openai.com/index/sora-2/. [3] KlingAI. kling-2.6. Dec. 3, 2025. URL: https://app.klingai.com/global/. [4] Runwayml. Gen-4.5. Dec. 1, 2025. URL: https://runwayml.com/research/introducing-runway-gen4.5. [5] Team Seedance, Heyi Chen, Siyan Chen, et al. Seedance 1.5 pro: Native Audio-Visual Joint Generation Foundation Model. 2025. arXiv: 2512.13507 [cs.CV]. URL: https://arxiv.org/abs/2512.13507. [6] Wan. Wan-2.6. Dec. 12, 2025. URL: https://wan.video/introduction/wan2.6. [7] Vidu. Vidu-Q2. Sept. 25, 2025. URL: https://www.vidu.com/. [8] runwayml. runway-aleph. July 25, 2025. URL: https://runwayml.com/research/introducing-runwayaleph. [9] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. OmniHuman-1: Rethinking the ScalingUp of One-Stage Conditioned Human Animation Models. 2025. arXiv: 2502.01061 [cs.CV]. URL: https: //arxiv.org/abs/2502.01061. Jianwen Jiang, Weihong Zeng, Zerong Zheng, Jiaqi Yang, Chao Liang, Wang Liao, Han Liang, Yuan Zhang, and Mingyuan Gao. OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive Simulation. 2025. arXiv: 2508.19209 [cs.CV]. URL: https://arxiv.org/abs/2508.19209. [10] [11] SkyReels. SkyReelsA3. Aug. 12, 2025. URL: https://skyworkai.github.io/skyreels-a3.github.io/. [12] Zhengcong Fei, Hao Jiang, Di Qiu, Baoxuan Gu, Youqiang Zhang, Jiahua Wang, Jialin Bai, Debang Li, Mingyuan Fan, Guibin Chen, et al. Skyreels-audio: Omni audio-conditioned talking portraits in video diffusion transformers. In: arXiv preprint arXiv:2506.00830 (2025). [13] Yikang Ding, Jiwen Liu, Wenyuan Zhang, et al. Kling-Avatar: Grounding Multimodal Instructions for Cascaded Long-Duration Avatar Animation Synthesis. 2025. arXiv: 2509.09595 [cs.CV]. URL: https://arxiv.org/ abs/2509.09595. 14 [14] Kling Team, Jialu Chen, Yikang Ding, et al. KlingAvatar 2.0 Technical Report. 2025. arXiv: 2512.13313 [cs.CV]. URL: https://arxiv.org/abs/2512.13313. [15] Zhe Kong, Feng Gao, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Xunliang Cai, Guanying Chen, and Wenhan Luo. Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation. In: arXiv preprint arXiv:2505.22647 (2025). [16] Kling Team, Jialu Chen, Yuanzheng Ci, et al. Kling-Omni Technical Report. 2025. arXiv: 2512.16776 [cs.CV]. URL: https://arxiv.org/abs/2512.16776. [17] KlingAI. kling-3.0. Feb. 6, 2026. URL: https://app.klingai.com/global/. [18] ByteDance. Seedance-2.0. Feb. 12, 2026. URL: https://seed.bytedance.com/en/seedance2_0. [19] Vidu. Vidu-Q3. Jan. 30, 2026. URL: https://www.vidu.com/. [20] Artificial Analysis. AI Model and API Providers Analysis. https://artificialanalysis.ai/. [21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video Diffusion Models. 2022. arXiv: 2204.03458 [cs.CV]. URL: https://arxiv.org/abs/2204.03458. [22] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning. 2024. arXiv: 2307.04725 [cs.CV]. URL: https://arxiv.org/abs/2307.04725. [23] William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. 2023. arXiv: 2212. [cs.CV]. URL: https://arxiv.org/abs/2212.09748. [24] Tim Brooks, Bill Peebles, Connor Holmes, et al. Video generation models as world simulators. In: (2024). URL: https://openai.com/research/video-generation-models-as-world-simulators. [25] Hailuo. Hailuo-2.3. Oct. 28, 2025. URL: https://www.minimax.io/news/minimax-hailuo-23. [26] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, et al. CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer. 2025. arXiv: 2408.06072 [cs.CV]. URL: https://arxiv.org/abs/2408.06072. [27] Weijie Kong, Qi Tian, Zijian Zhang, et al. HunyuanVideo: Systematic Framework For Large Video Generative Models. 2025. arXiv: 2412.03603 [cs.CV]. URL: https://arxiv.org/abs/2412.03603. [28] Tencent Hunyuan Foundation Model Team. HunyuanVideo 1.5 Technical Report. 2025. arXiv: 2511.18870 [cs.CV]. URL: https://arxiv.org/abs/2511.18870. [29] Team Wan, Ang Wang, Baole Ai, et al. Wan: Open and Advanced Large-Scale Video Generative Models. 2025. arXiv: 2503.20314 [cs.CV]. URL: https://arxiv.org/abs/2503.20314. [30] Di Qiu, Zhengcong Fei, Rui Wang, Jialin Bai, Changqian Yu, Mingyuan Fan, Guibin Chen, and Xiang Wen. SkyReels-A1: Expressive Portrait Animation in Video Diffusion Transformers. 2025. arXiv: 2502 . 10841 [cs.CV]. URL: https://arxiv.org/abs/2502.10841. [31] SkyReels-AI. Skyreels V1: Human-Centric Video Foundation Model. https://github.com/SkyworkAI/ SkyReels-V1. 2025. [32] Guibin Chen, Dixuan Lin, Jiangping Yang, et al. SkyReels-V2: Infinite-length Film Generative Model. 2025. arXiv: 2504.13074 [cs.CV]. URL: https://arxiv.org/abs/2504.13074. [33] Zhengcong Fei, Debang Li, Di Qiu, Jiahua Wang, Yikun Dou, Rui Wang, Jingtao Xu, Mingyuan Fan, Guibin Chen, Yang Li, et al. SkyReels-A2: Compose Anything in Video Diffusion Transformers. In: arXiv preprint arXiv:2504.02436 (2025). [34] Debang Li, Zhengcong Fei, Tuanhui Li, et al. SkyReels-V3 Technique Report. 2026. arXiv: 2601.17323 [cs.CV]. URL: https://arxiv.org/abs/2601.17323. [35] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, et al. LTX-Video: Realtime Video Latent Diffusion. 2024. arXiv: 2501.00103 [cs.CV]. URL: https://arxiv.org/abs/2501.00103. [36] Yoav HaCohen, Benny Brazowski, Nisan Chiprut, et al. LTX-2: Efficient Joint Audio-Visual Foundation Model. 2026. arXiv: 2601.03233 [cs.CV]. URL: https://arxiv.org/abs/2601.03233. [37] Sand. ai, Hansi Teng, Hongyu Jia, et al. MAGI-1: Autoregressive Video Generation at Scale. 2025. arXiv: 2505.13211 [cs.CV]. URL: https://arxiv.org/abs/2505.13211. [38] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation. 2023. arXiv: 2212.09478 [cs.CV]. URL: https://arxiv.org/abs/2212.09478. [39] Kai Wang, Shijian Deng, Jing Shi, Dimitrios Hatzinakos, and Yapeng Tian. AV-DiT: Efficient Audio-Visual Diffusion Transformer for Joint Audio and Video Generation. 2024. arXiv: 2406.07686 [cs.CV]. URL: https: //arxiv.org/abs/2406.07686. 15 [40] Akio Hayakawa, Masato Ishii, Takashi Shibuya, and Yuki Mitsufuji. MMDisCo: Multi-Modal DiscriminatorGuided Cooperative Diffusion for Joint Audio and Video Generation. 2025. arXiv: 2405.17842 [cs.CV]. URL: https://arxiv.org/abs/2405.17842. [41] Duomin Wang, Wei Zuo, Aojie Li, Ling-Hao Chen, Xinyao Liao, Deyu Zhou, Zixin Yin, Xili Dai, Daxin Jiang, and Gang Yu. UniVerse-1: Unified Audio-Video Generation via Stitching of Experts. 2025. arXiv: 2509.06155 [cs.CV]. URL: https://arxiv.org/abs/2509.06155. [42] Chetwin Low, Weimin Wang, and Calder Katyal. Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation. 2025. arXiv: 2510.01284 [cs.MM]. URL: https://arxiv.org/abs/2510.01284. [43] Kaisi Guan, Xihua Wang, Zhengfeng Lai, Xin Cheng, Peng Zhang, XiaoJiang Liu, Ruihua Song, and Meng Cao. Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction. 2025. arXiv: 2510.03117 [cs.CV]. URL: https://arxiv.org/abs/2510.03117. [44] Kai Liu, Wei Li, Lai Chen, et al. JavisDiT: Joint Audio-Video Diffusion Transformer with Hierarchical SpatioTemporal Prior Synchronization. 2025. arXiv: 2503.23377 [cs.CV]. URL: https://arxiv.org/abs/2503. 23377. Jun Wang, Chunyu Qiang, Yuxin Guo, Yiran Wang, Xijuan Zeng, and Feng Deng. Apollo: Unified Multi-Task Audio-Video Joint Generation. 2026. arXiv: 2601.04151 [cs.CV]. URL: https://arxiv.org/abs/2601. 04151. [45] [46] LAION. Large-scale Artificial Intelligence Open Network. 2021. URL: https://laion.ai/. [47] hlky. Flickr. [https://huggingface.co/datasets/bigdatapw/Flickr](https://huggingface. co/datasets/bigdata-pw/Flickr). 2024. [48] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in Time: Joint Video and Image Encoder for End-to-End Retrieval. In: IEEE International Conference on Computer Vision. 2021. [49] Qiuheng Wang, Yukai Shi, Jiarong Ou, et al. Koala-36M: Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content. 2025. arXiv: 2410 . 08260 [cs.CV]. URL: https : //arxiv.org/abs/2410.08260. [50] Hui Li, Mingwang Xu, Yun Zhan, et al. OpenHumanVid: Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation. 2025. arXiv: 2412.00115 [cs.CV]. URL: https://arxiv.org/abs/ 2412.00115. [51] Haorui He, Zengqiang Shang, Chaoren Wang, et al. Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation. 2024. arXiv: 2407.05361 [eess.AS]. URL: https://arxiv. org/abs/2407.05361. Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal, and Marvin Ritter. Audio Set: An ontology and human-labeled dataset for audio events. In: Proc. IEEE ICASSP 2017. New Orleans, LA, 2017. [52] [53] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. VGGSound: Large-scale Audio-Visual Dataset. 2020. arXiv: 2004.14368 [cs.CV]. URL: https://arxiv.org/abs/2004.14368. [54] Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Soundnet: Learning sound representations from unlabeled [55] video. In: Advances in Neural Information Processing Systems. 2016. Jin Xu, Zhifang Guo, Hangrui Hu, et al. Qwen3-Omni Technical Report. In: arXiv preprint arXiv:2509.17765 (2025). [56] Tomáš Souˇcek and Jakub Lokoˇc. TransNet V2: An effective deep network architecture for fast shot transition [57] detection. In: arXiv preprint arXiv:2008.04838 (2020). Jiapeng Wang, Chengyu Wang, Kunzhe Huang, Jun Huang, and Lianwen Jin. VideoCLIP-XL: Advancing Long Description Understanding for Video CLIP Models. 2024. arXiv: 2410.00741 [cs.CL]. URL: https: //arxiv.org/abs/2410.00741. [58] Akshay Raina and Vipul Arora. SyncNet: correlating objective for time delay estimation in audio signals. 2025. arXiv: 2203.14639 [eess.AS]. URL: https://arxiv.org/abs/2203.14639. [59] Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. VSA: Faster Video Diffusion with Trainable Sparse Attention. 2025. arXiv: 2505.13389 [cs.CV]. URL: https://arxiv.org/abs/2505.13389."
        },
        {
            "title": "A Application Examples",
            "content": "Table 3: Summary of video generation, inpainting, and editing tasks Main Task Generation Subtask Image + Audio Ref Image + Motion Ref Inpainting"
        },
        {
            "title": "Region Inpainting",
            "content": "Reference-Guided"
        },
        {
            "title": "Subject Manipulation\nAttribute Editing",
            "content": "Editing Background Editing Style Transfer Camera Control Scene Attributes Subject + Motion Ref Subject + Expression Ref Background + Video Ref First-Frame + Effect Ref videos and trajecfrom image Description Generate videos from multiple reference images and audio inputs Generate video/motion reference (poses, tories) Inpaint subjects, attributes, or backgrounds in video regions Inpaint using reference image guidance for style consistency Remove watermarks, subtitles, and logos intelligently Add, delete, or modify subjects in videos Edit local attributes (color, texture, shape, etc.) Modify backgrounds while preserving foreground Transform videos into different artistic styles Modify shot angle, shot type, and camera position Edit weather, lighting, tone, and time of day Combine subject and motion from different references Transfer facial expressions from reference video Combine background and video references Apply effects from reference to first-frame This appendix demonstrates typical application cases of our model in video-audio generation, inpainting, and editing. Our model supports flexible multimodal reference inputs, capable of processing various modalities including images, audio, and motion information. The following sections are organized into three main categories: Generation, Inpainting, and Editing. 17 A.1 Reference-based Generation A.1.1 Multiple Image and Audio Reference Generation Our model can simultaneously accept multiple reference images and audio inputs to generate videos that are stylistically consistent with the references and audio-matched. The result is shown in Fig. 6. Instruction: In an elegantly decorated indoor environment with warm, intimate lighting, two people sit facing each other across dark wooden table. The camera first focuses on @Actor-0, looking weary, says softly, <dialogue>Im little tired, Im going back to my room to rest.</dialogue>@Audio-0. @Actor-1 sits across from @Actor-0, hands clasped on the table, and says with determination, <dialogue>I will pay visit to your parents tomorrow.</dialogue>@Audio-1. Then @Actor-0 in another room, by the window, holds her phone to her ear and speaks, <dialogue>Mom, Li Zeting said hes coming to our house tomorrow.</dialogue>@Audio-0. The scene shifts to another locationa warm-toned home interior, with red fabric sofa visible in the background. @Actor-2, sitting tensely with phone to her ear, worries, <dialogue>But with our familys situation, do you think he might look down on us?</dialogue>@Audio-2. <bgm>Soft, sorrowful music plays during the first two shots, transitioning to slow, melancholic tense music for the last two shots.</bgm> m . o A . o V p @Actor-0 @Actor-1 @Actor-2 @Audio-0 @Audio- @Audio-2 Figure 6: Example of multiple images and audios reference. 18 A.1.2 Image Reference and Motion Reference Generation The model supports using image references to determine content and style, while simultaneously using motion references (e.g., pose sequences, trajectories) to control the dynamic characteristics of the generated video. Instruction: Animate the person in @image_1 using the movements from @video_1. a . o V n d t u Instruction: The medical professional in @image_1 and the curly-haired woman from @image_2 execute the dance movements demonstrated in @video_1, all set within the same stage environment as @video_1. m . o V u o i t Figure 7: Examples of motion transfer in video reference. 19 A.2 Video Inpainting A.2.1 Subject/Attribute/Background Inpainting The model can precisely inpaint specific regions in videos, including subject replacement, attribute modification, and background replacement. Instruction: Replace the subject in the mask area in @video_1 with majestic elk standing in the same field. i t I i k o i t Instruction: Change the color of the tie in the masked area of @video_1 to blue. V u i V s e t u 20 Instruction: Replace the background in the masked area of @video_1 with stunning cinematic view of the Amalfi Coast in Italy during warm golden hour sunset. i p e d a d u O Figure 8: Examples of subject/attribute/background inpainting. 21 A.2.2 Image Reference Inpainting The model supports using reference images to guide the inpainting process, ensuring that the inpainted content is consistent with the reference style. Instruction: Add the man from @image_1 to the left mask area of @video_1. m . o V n d e M V t u Instruction: Replace the right mask area in @video_1 with the cat from @image_1 and the left mask area in @video_1 with the woman from @image_2, ensuring harmonious and natural scene. m . o V u o V s e t u Figure 9: Examples of image reference inpainting. A.3 Video Editing A.3.1 Local Editing The model enables fine-grained local video editing: subject, attribute, and element edits. Watermark/Subtitle/Logo Removal The model can intelligently identify and remove watermarks, subtitles, logos, and other elements from videos while maintaining content coherence and naturalness. Instruction: Remove watermarks in @video_1. i t I i p Instruction: Remove the text overlay at the bottom of @video_1. V t I V p Instruction: Remove the logo in the upper right corner in @video_1. V u o V t Figure 10: Examples of watermark/subtitle/logo removal. Subject Manipulation The model supports adding, deleting, and modifying subjects in videos while maintaining temporal consistency. Instruction: Place wooden bench with black metal armrests and legs on the grass next to the large rock on the right side of the path in @video_1. i p e t u Instruction: Remove bee from the center of @video_1. V p e i p Figure 11: Examples of subject manipulation. 24 Local Attribute Editing The model can perform fine-grained editing of attributes for specific objects or regions in videos, such as color, texture, shape, etc. Instruction: Change the chairs color to black and replace its edges with wooden material in @video_1. i t I i p Instruction: Change the mans sleeveless shirt in @video_1 to blue Polo shirt style. V t I V p Background Editing The model supports modifying background elements while preserving the foreground subjects. Instruction: Replace the background of @video_1 with post-rain European cobblestone street scene at dusk. d u o i t Figure 12: Examples of local attribute editing. 25 A.3.2 Global Editing The model supports global modifications that affect the entire video, including style, camera properties, and scene attributes. Style Transfer The model can transform videos into different artistic or visual styles while maintaining semantic consistency of the video content. Instruction: Transform @video_1 into Paper-Cutting style. d u o V t Instruction: Transform @video_1 into LEGO style. V u o i t Figure 13: Examples of style transfer. 26 Camera Control The model supports modifying camera properties including shot angle, shot type, and camera position. Instruction: Re-render @video_1 with Pan Right camera movement. i p e i t Figure 14: Examples of camera control. 27 Global Scene Attributes The model supports transforming global scene attributes such as weather, lighting, color tone, and time of day. Instruction: Make @video_1 nighttime. d u o V t Instruction: Make @video_1 snowy. V u o i t Figure 15: Examples of global scene attributes. 28 A.3.3 Reference-Based Editing The model supports video editing based on image references, including subject reference, background reference, and first-frame reference, combined with reference videos to generate the final output. Reference videos can provide motion, expression, or visual effects guidance. Subject Reference with Motion Reference The model can generate videos by combining subject from reference image with motion patterns from reference video, matching action rhythm and trajectory. Instruction: Woman from @image_1 mimics gestures from @video_1 in its golden field background. m . o V n d t u Figure 16: Example of subject reference with motion reference. Subject Reference with Expression Reference The model can transfer natural facial expressions from reference video to subject from reference image. Instruction: Transfer the facial expressions from @video_1 to the man in @image_1. m . o V n d u O Figure 17: Example of subject reference with expression reference 29 Background Reference with Video Reference The model can combine background from reference image with content or motion from any reference video. Instruction: Replace the background of @video_1 with @image_1. m . R d u o i t Figure 18: Example of background reference with video reference. 30 First-Frame Reference with Effect Reference The model can apply visual effects from reference video to generate video starting from reference image, enabling effect transfer from the reference video. Instruction: Transfer the diamond morphing effect from @video_1 onto the subject in @image_1. m . R d . o i t Figure 19: Example of first-frame reference with effect reference."
        }
    ],
    "affiliations": [
        "Skywork AI"
    ]
}