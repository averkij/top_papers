{
    "paper_title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
    "authors": [
        "Cheng-Han Chiang",
        "Xiaofei Wang",
        "Linjie Li",
        "Chung-Ching Lin",
        "Kevin Lin",
        "Shujie Liu",
        "Zhendong Wang",
        "Zhengyuan Yang",
        "Hung-yi Lee",
        "Lijuan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This prevents the model from interacting during the user's turn and can lead to high response latency while it waits to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time, low-latency exchange is important. We address this by noting that humans naturally \"think while listening.\" In this paper, we propose SHANKS, a general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning while listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as a chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses this unspoken reasoning to decide whether to interrupt the user and to make tool calls to complete the task. We demonstrate that SHANKS enhances real-time user-SLM interaction in two scenarios: (1) when the user is presenting a step-by-step solution to a math problem, SHANKS can listen, reason, and interrupt when the user makes a mistake, achieving 37.1% higher interruption accuracy than a baseline that interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can complete 56.9% of the tool calls before the user finishes their turn. Overall, SHANKS moves toward models that keep thinking throughout the conversation, not only after a turn ends. Animated illustrations of Shanks can be found at https://d223302.github.io/SHANKS/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 7 1 9 6 0 . 0 1 5 2 : r Work in Progress SHANKS: SIMULTANEOUS HEARING AND THINKING FOR SPOKEN LANGUAGE MODELS Cheng-Han Chiang1,2 Xiaofei Wang2 Linjie Li2 Chung-Ching Lin2 Kevin Lin2 Shujie Liu2 Zhendong Wang2 Zhengyuan Yang2 Hung-yi Lee1 Lijuan Wang2 1National Taiwan University 2Microsoft"
        },
        {
            "title": "ABSTRACT",
            "content": "Current large language models (LLMs) and spoken language models (SLMs) begin thinking and taking actions only after the user has finished their turn. This disables the model from interacting with the user during the users turn and can lead to high response latency for waiting for the model to think. Consequently, thinking after receiving the full input is not suitable for speech-to-speech interaction, where real-time and low-latency interaction is important. We address the above issue by drawing inspiration from the fact that humans can naturally think while listening. In this paper, we propose SHANKS, general inference framework that enables SLMs to generate unspoken chain-of-thought reasoning when listening to the user input. SHANKS streams the input speech in fixed-duration chunks and, as soon as chunk is received, generates unspoken reasoning based on all previous speech and reasoning, while the user continues speaking. SHANKS uses unspoken reasoning to determine whether to interrupt the user and make tool calls to complete the task. We demonstrate that SHANKS enhances the real-time user-SLM interaction in two scenarios: (1) When the user is presenting their step-by-step solution to math problem, SHANKS can listen to and reason over the users speech and make an interruption when the user makes mistake. SHANKS interrupts the user 37.1% more accurately compared with baseline that interrupts the user without thinking. (2) In tool-augmented dialogue scenario, where the model needs to make tool calls to achieve the users request, SHANKS can complete 56.9% of the tool calls before the user even ends their turn. Overall, SHANKS is step toward models that keep thinking throughout the conversation, not only after turn ends. Animated illustrations of SHANKS can be found at https: //d223302.github.io/SHANKS/."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, the thinking process has been used to improve Large Language Models (LLMs), where the LLM first generates hidden chain-of-thought (CoT) reasoning (Wei et al., 2022; Kojima et al., 2022) invisible to the users, and then generates the final output response (OpenAI, 2024c; Guo et al., 2025). This thinking process improves LLMs on reasoning-intensive tasks, including mathematics (Lightman et al., 2024), coding (Chen et al., 2021), and questions that involve significant domain knowledge (Rein et al., 2024). However, current reasoning LLMs only start to think after receiving the complete user input, which is reasonable for turn-based interactions, i.e., the model processes the users message after it is fully composed and sent. In contrast, human behavior in spoken communication is different. Humans naturally think while listening, far before the speaker finishes their turn (Bogels et al., 2015; Corps et al., 2018; Bogels et al., 2018). Thinking during listening offers two key advantages: (1) It enables timely and well-founded reactions, including interruption, even before the speakers turn ends. (2) It reduces response latency by allowing answer preparation to begin before the speaker finishes speaking. Motivated by these Work done during an internship at Microsoft GenAI. dcml0714@gmail.com Correspondence: xiaofei.wang@microsoft.com 1 Work in Progress Figure 1: The timing diagram of SHANKS. As the user speaks, their speech is segmented into chunks for every tchunk seconds and streamed to the SLM. After receiving an input chunk, SHANKS generates the thinking tokens, which might include calling external tools or determining to interrupt the user. When the user is speaking the i-th speech chunk Si, SHANKS generates the (i 1)-th thinking chunk Ri1, achieving thinking while listening. When the current speech chunk Si is fully spoken by the user, SHANKS stops the thinking for Ri1, adds the latest speech Si and the previous reasoning Ri1 to its context, and begins the i-th thinking chunk Ri. observations, we propose method to enable spoken language models (SLMs) to think while listening to input speech. In this paper, we introduce SHANKS: Simultaneous Hearing and Thinking with Chunked Input Speech. SHANKS is general inference framework for both end-to-end (E2E) and cascade SLMs to achieve thinking while listening. At inference time, SHANKS processes the user input in fixed-size chunk. Once chunk of speech input is received, SHANKS generates chunk of unspoken thinking tokens based on all previous input speech chunks and previous thinking chunks. SHANKS alternates between receiving the input speech chunk and generating an unspoken CoT reasoning when the user is still emitting the next speech chunk, achieving the thinking while listening. During the thinking process, SHANKS can decide to interrupt the user or make tool calls to prepare for the final spoken response. The inference workflow of SHANKS is depicted in Figure 1. We use two scenarios to show how SHANKS can improve real-time user-SLM interaction. First, we study scenario where the user first describes math question and then describes their step-by-step solution. SHANKS can listen to the users problem-solving process and perform internal thinking in the meantime to interrupt the user when the user makes mistake in their solution. This scenario has great potential in educational use cases, where the SLM serves as tutor to guide the student. Compared to baseline that makes an interruption without thinking, SHANKS interrupts 71% more when the user makes mistake, while the interruption made by SHANKS is 37.1% more valid. Next, we focus on task-oriented dialogue setting, where the user requests the SLMs to assist with travel plan, and the model needs to call Booking.com APIs to complete the users request and respond to the user. Without SHANKS, all the tool calls can only be made after the users speech ends, resulting in higher response latency. We use SHANKS to make tool calls when the user is still speaking. SHANKS enables the model to successfully complete 56.9% of API calls while the user is still speaking, reducing the latency of the final response. We summarize our contribution as follows: 1. We propose SHANKS, general framework for SLMs that enables thinking while listening. To the best of our knowledge, we are the first to explore generating unspoken CoT reasoning when the user is still speaking. 2 Work in Progress 2. We show that SHANKS can interrupt the user more accurately compared to baseline that interrupts without thinking. 3. Using SHANKS, the SLM can successfully make tool calls before the user even finishes talking."
        },
        {
            "title": "INPUT SPEECH",
            "content": "Current LLMs and SLMs only start to think after the users input is completed. In contrast, humans can think while listening (Bogels et al., 2015; Corps et al., 2018; Bogels et al., 2018), where we reason over what we just heard, guess what the speaker might be up to, and prepare the necessary ingredients to cook up good response. Thinking while listening allows us to interact with the speaker better when the speaker is still speaking. In this section, we introduce SHANKS, general framework to make SLMs capable of thinking while listening. Here, we only discuss the basic form of SHANKS, and we defer the more advanced usages, including interruption or tool call, to later sections. 2.1 INFERENCE During inference, SHANKS requires that the users input speech comes in streaming manner. SHANKS processes the streaming user input speech by fixed chunk size of tchunk seconds. We use Si to denote the i-th user input speech chunk, where Si is an audio chunk of tchunk seconds, except for the last chunk SN , which may be shorter. When the user is still speaking, SHANKS alternately takes the user speech Si and generates the thinking chunks Ri conditioning on all previous user speech and all previous thinking chunks. The thinking chunks Ri are not spoken by the SLM; they only serve as the internal reasoning process of the SLM. Here, we walk through what happens for SHANKS during inference. The following contents are best read with Figure 1. At = 0, the user begins to talk. When = tchunk, the user speech from 0 to tchunk, i.e., S1, is sent to the SLM. Here, we append special token [EOPA] (end of partial audio) after S1 to let the SLM know that this is the end of chunk of partial user speech. Based on S1, the SLM generates the first thinking chunk R1. thinking chunk is enclosed in two special tokens <think> and </think>. The SLM generates R1 during the interval = tchunk to = 2tchunk, and the user is still speaking the second chunk S2 at the same time. Since tchunk is the time for the SLM to generate its thinking, the duration of tchunk cannot be selected too small; otherwise, the SLM may not be able to produce meaningful thinking chunks. The number of thinking tokens in Ri is restricted to no more than tchunk ntps, where ntps is the number of tokens the model can generate per second. Unless specified, we select tchunk = 4.0s in our paper; 7B model can generate around 320 tokens on single A100 GPU in this duration. At the end of tchunk, if the thinking chunk has not finished generating, i.e., the </think> token has not been emitted, we directly stop generating and append the </think> token at the end of the thinking chunk. At = 2tchunk, we take the freshly obtained user speech chunk S2 (from = tchunk to = 2tchunk) and pass this chunk to the SLM, and again appending the [EOPA] after this chunk to generate the next thinking chunk. (Assume that the user still has not ended their turn at = 2tchunk.) When generating the next thinking chunk R2, the SLM conditions on S1, R1, and S2. The SLM will continue the process of taking user input speech chunks and generating the thinking chunks until the user ends their speech in the -th chunk of speech, SN . After the user ends their speech, we feed the last speech chunk SN into the SLM, while this time we append different special token [EOA] (end of audio), indicating that the users speech has ended. Based on SN and all the previous interleaved speech/thinking chunks {S1, R1, , SN 1, RN 1}, the SLM generates the thinking chunk RN and then generates final response chunk O. Only the final response will be spoken by the SLM. Since SHANKS chunks the user input using fixed-duration chunk tchunk, the models thinking will lag behind the users speech by at least tchunk seconds. If the users speech is less than tchunk, SHANKS cannot think while listening. However, since long speech can easily happen in real-world interaction, this limitation might not be significant weakness of SHANKS. 3 Work in Progress Figure 2: Illustration of the training data. Si: the speech for the i-th user speech chunk; Ri: the i-th thinking block after Si; O: the final response block; Ai: the API call responses after the speech chunk Si. Blocks in dashed lines do not contribute to the training loss, while blocks in solid lines are included for loss calculation. (a) The general training sequence: Alternating between user speech block and SLM thinking token chunks (Section 2.2), followed by final response chunk. (b) Training data with interruption: Alternating between user speech blocks and the thinking token chunks, while the last thinking chunk includes special token [INTERRUPT]. (c) Training data with API calls: Similar to (a), while each thinking chunk may be separated into two blocks Ri1 and Ri2 by the API call response Ai. 2.2 TRAINING During inference, SHANKS requires the SLM to generate thinking chunks based on all previous user input speech chunks and the models own thinking chunks. During training, we prepare datasets to make the SLM learn this behavior. Assume that we have complete user speech S, we can segment it into chunks {S1, , SN } with fixed duration tchunk. Next, assume that we use some method to obtain the thinking chunks {R1, , RN } and the output response O; we will explain how to obtain them in later sections. After preparing the training data, we use the standard language modeling cross-entropy loss to train the SLM to predict R1 given {S1}, predict R2 given {S1, R1, S2}, etc., and predict RN and given {S1, R1, , SN 1}. full training sequence is depicted in Figure 2(a)."
        },
        {
            "title": "3 TASK INTRODUCTION",
            "content": "After introducing the basics of SHANKS, we use two tasks to demonstrate how SHANKS can be applied. In this section, we explain the setup of the two scenarios and how SHANKS works at inference time. 3.1 SCENARIO 1: INTERRUPTING USER TURN In the first scenario, we aim to use SHANKS to make SLMs able to interrupt the user when the user is saying something wrong. The significance of this application lies in its potential in educational use cases, where the SLMs can serve as tutor and listen to the speaker, student, describing how they solve problem. The SLM can make timely interruption to let the student know that they are making mistake, allowing them to correct it as early as possible. Interruption is related to the full-duplex ability of spoken language models (Lin et al., 2025). While most prior works on full-duplex SLMs focus on user interrupting SLMs, we focus on the reverse scenario: SLM interrupting users. As an important note, we do not advocate that it is good to have model that interrupts the user. Some users might find it annoying and unpleasant when interrupted by an SLM. Our goal is modeling mechanism that enables interruption, leaving model deployer policies and users to decide when (or whether) to turn it on. 3.1.1 TASK DESCRIPTION We explain the precise task we are studying. In this task, the user describes math problem and then solves the problem. The users solution does not simply state the answer; the user describes step-by-step problem-solving process, which might be correct or wrong. The SLM needs to interrupt the user when the user is making mistake, and not to interrupt the user when the solution is correct. As this is novel task and there is no available data, we built the evaluation data ourselves. First, we construct the user speech. single user speech includes (1) math question and (2) step-by-step solution. We source the math questions from the testing data of GSM8K (Cobbe et al., 2021), grade-school math word problem dataset commonly used for evaluating mathematical reasoning 4 Work in Progress ability (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2023). Next, we use two LLMs, Llama2-7B (Touvron et al., 2023) and Llama3.1-8B (Grattafiori et al., 2024), to generate step-by-step answers for those questions, and use GPT-4o (OpenAI, 2024b) to determine if the answer generated by the two models matches the ground truth answer in the dataset. We select these two models since they can generate CoT reasoning to solve the math problem, and their performance on GSM8K is very different: Llama-2-7B is weaker model and prone to generating wrong solutions, while Llama-3.1-8B is stronger model, which can generate more accurate solutions. After we have the texts for the step-by-step solution, we convert them into speech. We use GPT-4o to rewrite the answers generated by the two Llama models to make the solution more colloquial. Next, we concatenate the original question, the colloquial step-by-step answer, and prepend prefix want to solve the following question. to form the transcription of testing instance. We use GPT-4o-mini-TTS (OpenAI, 2024a) to synthesize the speech. The final testing dataset includes 1280 instances with correct solutions and 1140 with incorrect solutions. We call the former the correct subset and the latter the incorrect subset. The average duration of the users speech is around 49.25 seconds. 3.1.2 TRAINING DATA FOR INTERRUPTION To teach the model to think while listening and determine whether to interrupt, the training data in this task include two types of instances: (1) The user provides correct step-by-step solution to the question, and the model does not interrupt the user during the users speech. After the user finishes the speech, the output response acknowledges the correctness of the answer. (2) The users turn unfolds an erroneous problem-solving process, and the model interrupts the user when the user makes the first mistake and clearly explains what is wrong. To construct such training dataset, we use the math questions in Tulu3-Persona-Math-Grade (Lambert et al., 2024) to construct the user speech (including the question and the step-by-step solution) following the previously described procedure, and then segment the speech by fixed duration tchunk = 4 seconds to obtain {S1, , SN }. Next, we use GPT-4o to generate the thinking chunk Ri. When generating the i-th thinking chunk Ri, the input to GPT-4o includes the transcriptions of all previous user speech chunks {S1, , Si} and all previous thinking chunks {R1, , Ri1}. GPT-4o is required to do the following in the thinking chunk: (1) Track the information already known and calculate intermediate variables when they are available. (2) Identify if any errors are in the current users transcription. If there is an error, GPT-4o should generate [INTERRUPT] token at the end of the thinking chunk, indicating that the user should be interrupted. We give GPT-4o four in-context examples to allow GPT-4o to understand the task. The prompt to GPT-4o is given in Table 3 and 4 in the Appendix. After generating the thinking chunks, we generate the final output response O. For the user speeches with an error-free solution, the output response simply needs to let the user know that their solution is correct. We prompt GPT-4o to generate the final response based on the transcription of the full user speech and all previous thinking chunks. Now, we can form training data sequence by interleaving Si and Ri and then appending in the end. For those user speeches with wrong solution, the output response will be an interruption to the users speech. Assume that based on our previous process for generating the Ris, GPT-4o decides to interrupt the user after the user speech chunk Sk, i.e., the thinking chunk Rk includes the interruption token [INTERRUPT]. To generate response for interruption, we give GPT-4o the users speech up to the k-th user speech chunk and all the previous thinking, and ask GPT-4o to generate response to interrupt the user. The interruption should be precise on what error is made by the user and how to correct it. After this process, we can interleave S1 to Sk with R1 to Rk and append in the end to form training sequence. figurative illustration of this training instance is shown in Figure 2(b). Note that in the last thinking chunk Rk, there will be special token [INTERRUPT], indicating that the model is going to interrupt the user. 3.1.3 INFERENCE AND EVALUATION During inference, we stream the user speech to the SLM by fixed chunk size tchunk, and follow the inference procedure elaborated in Section 2.1. If the SLM generates the special token [INTERRUPT] 5 Work in Progress in thinking chunk Rk and outputs response chunk (when the user is emitting speech chunk Sk+1), we convert the response token into speech to interrupt the user. Once an interruption happens, the future user speech chunks will not be streamed to the SLM. We evaluate model on the testing dataset constructed in Section 3.1.1. We use the following metrics: 1. Interrupt ratio: The ratio of total interrupted instances among the total instances. good model should have low interrupt ratio on the correct subset and high interrupt ratio on the wrong subset. 2. Valid interrupt ratio: This is used to evaluate whether an interruption made by the model is valid, and the valid interrupt ratio is defined as the ratio of valid interruptions among the total interruptions. To judge whether an interruption response is valid or not, we use LLM-as-a-judge (Chiang & Lee, 2023; Zheng et al., 2023). We give the judge LLM (GPT4o) the transcription of the user input until the time of interruption1 and output response from the model, and ask the LLM judge to determine if the models interruption response correctly interrupts the user when there are unclear or mistakes in the users speech. The prompt given to the judge model is shown in Table 10 in the Appendix. 3. Interruption latency: The time of the model interruption compared to the time when the first error happens in the users speech, denoted as terror. This metric is only used to evaluate the incorrect subset. For samples in the incorrect subset, we use GPT-4o to determine terror. The details on determining terror are included in Appendix C.1. Assume that the model interrupts the user at tinterrupt, then the interruption latency is calculated as tinterrupt terror, where tinterrupt is the time when the model starts to emit the first package of the speech for the interruption output O. 3.2 SCENARIO 2: MAKING TOOL CALLS WHEN LISTENING In the previous scenario, we have introduced SHANKS can be used to think when listening to interrupt the user when necessary. However, the thinking of model can not only include the CoT reasoning generated by the model itself; the model can use external tools in their thinking process to complete the users request and improve the answer quality (Schick et al., 2023; Qin et al., 2024b;a; Gao et al., 2024; Wang et al., 2024). In the second scenario, the SLMs will use external tools in their thinking process to achieve the users request. This kind of tool-augmented generation has been widely explored in text-in-text-out LLMs (Qin et al., 2024a). Given user request, tool-augmented LLM selects relevant tools such as calculators (Schick et al., 2023), search engines (Luo et al., 2023), and other APIs (Qin et al., 2024b), and integrates the tool execution results into its reasoning process. However, current tool-augmented LLMs begin calling the tools after the full user input is received, which adds delay while the model invokes tools, waits for results, parses them, and composes response. This delay may be acceptable in text-only settings, but in spoken dialogue, this latency breaks conversational flow. In the second task, we will use SHANKS to make tool calls when the user is still speaking, thus reducing the response latency due to making tool calls. As proof of concept, we consider taskoriented dialogue where the SLM serves as travel-agency agent and is given set of APIs it may call to complete the task. For example, the user might say, Help me check the details of the cheapest flight from Hangzhou to Seoul on December 10, 2024, and the car rental information near Seoul airport. The SLM makes API calls to resolve airport names or codes, search for flights, and then query car-rental options, and finally composes the results into single reply. Given user query like the example above, once the destination and date are clear, the flight search API can be called even when the user is only halfway through speaking. This is where SHANKS can be useful: processing partial user input and performing early actions. Next, we formally introduce the task we study and how we evaluate it. 1If the interruption happens in Ri, the user is currently speaking the (i + 1)-th speech chunk Si+1, as the thinking Ri happens simultaneously when the user says Si+1. Consequently, we also feed the transcription of Si+1 into the judge model when determining whether the interruption is valid. Nevertheless, we do not find the results to differ significantly if we only give the judge model up to the speech chunk Si. 6 Work in Progress 3.2.1 TASK DESCRIPTION To show that SLMs can make tool calls when listening to the user request, we adopt ComplexFuncBench (Zhong et al., 2025), an evaluation dataset that assesses LLMs ability to make multi-step API calls. An instance in ComplexFuncBench includes user query in text specifying some requirements for travel plan and list of tool descriptions that are required to complete the task. Some example tools include APIs for searching hotels or flights. An evaluated model needs to make relevant API calls and provide the resulting information to the user. single user query typically requires multiple API calls, and these calls may have dependencies in which the output of one call becomes an argument to subsequent call, so the call order matters and some calls may not be run in parallel. For each instance, the dataset provides the ground truth API calls and their corresponding API responses, which can be used to evaluate whether the models API call is correct. To adapt ComplexFuncBench to our spoken-dialogue setting, we use GPT-4o-mini-TTS to synthesize the users spoken query from the text instructions from ComplexFuncBench. 3.2.2 TRAINING DATA FOR TOOL CALL To train SHANKS to perform Tool calls when listening, we need to teach the model to make API calls in their thinking process Ri based on user input speech chunk Si. We split half of the instances in ComplexFuncBench to construct the training data and the other half as the testing data.2 The speech chunks Si in the training data can simply be obtained from segmenting the audio of the user query speech. The next step is to construct the thinking chunks Ri. In this task, the thinking chunk Ri is the API calls and call responses. For each user query, ComplexFuncBench already provides the ground truth API calls to complete the task, and we only need to determine which API calls, among the ground truth API calls, can be made after speech chunk Si. To determine which API calls can be made in Ri, recall that thinking chunk Ri is based on the user speech up to tchunk seconds, so as long as the speech up to tchunk provides the information to make an ground truth API call, that API call can be included in Ri. We follow the above idea and prompt GPT-o1 (OpenAI, 2024c) with the transcription of the user speech, the time alignment of each word in the users speech, and the ground truth APIs, and ask GPT-o1 to determine the earliest time an API call can be made. The prompt is shown in Table 8 in the Appendix. Based on the above process, we can determine which ground truth APIs should be included in which thinking chunk Ri. thinking chunk Ri can have more than one API call and response. If no API calls can be made in Ri, we put template message that says there are no additional tool calls that can be made currently. The final response is also generated with GPT-4o by prompting it to generate final response based on the user query, all the ground truth API calls, and the corresponding responses. During training, the descriptions of the API calls necessary to complete the users request will be included in the system prompt to let the model know what APIs can be used. An illustration of training instance is shown in Figure 2(c). During training, the loss for the API call response in Ri is masked. Training on this dataset will teach the model to make API calls based on incomplete user queries as long as the information for an API call is sufficient. 3.2.3 INFERENCE AND EVALUATION During inference, for testing instance, the model is given the users speech in streaming manner; the descriptions of the APIs that can be used in this testing instance. When the model makes an API call, we use GPT-4o as judge to determine if the API call matches one of the ground truth API calls, and return the response of the ground truth API call if match is found. If the API call does not match the API call in the ground truth, we return generic error message. Using GPT-4o to match the API call made by the model against the ground truth follows one of the evaluation protocols in the original ComplexFuncBench (Zhong et al., 2025). 2ComplexFuncBench is originally designed as an evaluation dataset. Here, we train the model directly on this dataset since the models we use were not trained to perform tool call. Since our training data has the exactly same distribution as the testing data, our results should not be compared with other models that are not trained on this dataset. 7 Work in Progress The testing set includes 500 instances, and each testing instance requires 5.1 API calls to complete on average. The average duration of the audio of the users speech is 18.71 seconds. We evaluate the performance using four metrics: 1. Call accuracy: The number of ground truth API calls that are successfully made by the model, divided by the total number of ground truth API calls. We also calculate the early call accuracy, defined as the ground truth API calls that are successfully made when the user is still speaking, divided by the total number of ground truth API calls. Similarly, we calculate the late call accuracy, where the dividend is the ground truth API calls that are successfully made after the user finishes speaking. This helps us understand how well the model uses the time when the user is still speaking; this can be used as proxy to measure latency. 2. Success rate: The percentage of user queries that are successfully completed. If all the ground truth API calls for user query are successfully made, the task is considered successful. 3. Correctness: We evaluate if the final response is accurate and aligns with the API call responses. Following Zhong et al. (2025), we use GPT-4o to give score in {0, 1, 2}, indicating if the transcription of the response is completely incorrect, partially correct, or completely correct the users request. 4. Completeness: We evaluate if the final response fully satisfies the users request. Following Zhong et al. (2025), we use GPT-4o to give score in {0, 1, 2}, indicating if the transcription of the response does not satisfy, partially satisfies, or fully satisfies the users request."
        },
        {
            "title": "4 COMPARED METHODS",
            "content": "In this section, we introduce the models that we will compare in our experiments, including two variants of the SHANKS models. Additionally, for each task in Section 3.1 and 3.2, we include scenario-specific baseline model. The training details and hyperparameters are included in Appendix B. 4.1 SHANKS-E2E We fine-tune an end-to-end (E2E) SLM to make it able to think while listening. We will use Qwen2.5-Omni (Qwen-omni for short) (Xu et al., 2025a), one of the best open-sourced end-to-end SLMs, in our experiment. Qwen-omni is thinker-talker SLM. The thinker takes speech representation extracted by speech encoder (Chu et al., 2024) as the input and generates text tokens. The talker functions like text-to-speech (TTS) model, taking the hidden representation from the thinker as the input and generating the output speech. Originally, Qwen-omni is not capable of performing unspoken thinking every token (and its corresponding hidden representation) generated by the thinker will be sent to the talker model and synthesized into speech. After fine-tuning the thinker model on the training dataset mentioned before, the model will learn to enclose the unspoken thinking process within <think> and </think>. Since we only want the Qwen-omni to speak out the response tokens, we only pass the response tokens and their hidden states to the talker. As stated in Section 2.1, SHANKS uses the time when the user is speaking the next speech chunk Si+1 to generate the thinking chunk Ri, so the number of thinking tokens in Ri cannot exceed tchunk ntps, where ntps is the number of tokens the model can generate per second.3 4.2 SHANKS-CASCADE We set up cascade version of SHANKS. Precisely, we cascade an ASR (Whisper-large-v3 (Radford et al., 2023)) with stronger text-only LLM, Qwen-2.5-7B-Instruct (Qwen et al., 2025), to make the 3The tokens in the API call responses are not included in the tchunk ntps limit since these are not the tokens generated by the SLM itself. For simplicity, we do not consider the API call latency, as our environment already prepares all the ground truth API responses. 8 Work in Progress LLM generate thinking chunks while reading the partial transcription. Qwen-2.5-7B-Instruct and Qwen-omni are fine-tuned from the same base model, while Qwen-2.5-7B-Instruct are fine-tuned on much larger reasoning dataset. This baseline allows us to know what the performance of SHANKS can be if we use model with better reasoning ability as the backbone. The training data of SHANKS-E2E and Cascade are almost the same, only differing in the input modality. 4.3 SCENARIO-SPECIFIC BASELINES 4.3.1 BASELINE FOR INTERRUPTION We fine-tune baseline model using Qwen-omni, which we name No-thinking. We fine-tune it to predict whether it should interrupt the user without any thinking. The model is trained to predict special tokens, [NO INTERRUPT] or [INTERRUPT], to indicate whether the model should interrupt the user, given chunked user input speech. This can be thought of as SHANKS while the thinking chunks only contain [NO INTERRUPT] or [INTERRUPT] special token. We do not compare with other models since there are no other models that can interrupt the user. While some full-duplex SLMs should be able to interrupt the user by design (Defossez et al., 2024), we find that these models cannot interrupt the user at all. We also find that closed-source models like GPT-4o cannot interrupt the user when the user is still talking. 4.3.2 BASELINE FOR TOOL CALL For this baseline, we fine-tune model using Qwen-omni that takes the full user speech and then makes all the API calls. We call this model Call-after-listen. This serves as baseline that waits until receiving the full user input and then begins to make API calls; this is how existing tool-augmented models operate. During inference, this model takes the complete user query and then iteratively makes API calls and receives the responses until the model thinks all necessary API calls are made, and then generates the final response."
        },
        {
            "title": "5 EXPERIMENT RESULTS",
            "content": "5.1 RESULTS FOR SCENARIO 1: INTERRUPTING USER TURN The results for interrupting the user turn are presented in Table 1. We have the following observations. SHANKS is more likely to interrupt on the wrong subset. Comparing the interruption ratio of SHANKS on the correct and wrong subsets, the interruption ratio is 54.2% higher on the wrong subset. This shows that SHANKS is indeed capable of capturing the errors in the users speech and interrupt appropriately. Based on the valid interruption ratio for the wrong subset, about 2 out of 3 interruptions made by SHANKS are valid. Interesting, on the correct subset, the valid interruption ratio is non-zero. By looking into the instances in the correct subset, we find that even if their final answers are correct, sometimes their intermediate reasoning may be odd or ambiguous, and the model will interrupt and ask for clarification. Prior works also reported that even if the final answer of the model is correct, the CoT reasoning may be wrong (Golovneva et al., 2023). In this case, the LLM judge treats this kind of interruption as valid. SHANKS interruption latency shows that the model mostly interrupts after the error occurs. On the wrong subset, the interruption latency of SHANKS-E2E is 5.08 seconds on average. In Figure 5 in the Appendix, we further plot the distribution of the interruption latency. We find that the interruption latency on the wrong dataset is left-skewed, where more samples fall on the right proportion of the distribution and have positive interruption latency. This indicates that most interruption happens later than the first error. qualitative example in Figure 3 shows that SHANKS-E2E can interrupt the user when there is mistake. To allow the readers to have better idea of how SHANKS interrupts the user, we show an example in Figure 3. When the user unfolds the question, SHANKS already starts thinking about the math question. For example, when [4tchunk, 5tchunk], the models thinking already calculates several intermediate variables, including the number of total petunias and sweet potato vines. When the user finishes stating the question after = 6tchunk, the model already completes the calculation Work in Progress Table 1: Results for interrupting the user. We report the interruption ratio and valid interruption ratio in percentage, and the interruption latency in seconds. tchunk = 4.0 in the top three rows. Correct Subset (1280) Wrong Subset (1140) Methods Interrupt ratio (%) () Valid interrupt ratio (%) () Interrupt ratio (%) () Valid interrupt ratio (%) () Interruption latency (s) No-thinking SHANKS-E2E SHANKS-Cascade tchunk = 3 tchunk = 1.4% 30.6% 24.9% 41.1% 26.9% 16.7% 25.7% 40.3% 13.8% 84.8% 86.1% Ablations for SHANKS-E2E 21.4% 36.9% 88.7% 83.1% 26.8% 63.9% 78.3% 60.3% 66.2% 6.46 5.08 6.90 1.56 8.19 and has the correct answer in its mind. The model also correctly identifies the users error in falsely calculating that there are 25 petunias and interrupts the user during [12tchunk, 13tchunk]. Interruption without thinking leads to much poorer performance. The performance of the no-thinking baseline is much worse than SHANKS, which performs reasoning before interrupting. The no-thinking baseline has much lower interruption ratio on the wrong subset, and the valid interrupt ratio is also much lower than SHANKS. This shows that thinking before interruption is important, justifying the design of SHANKS. Cascade version of SHANKS with stronger LLM leads to the best performance. When using Qwen-2.5-7B-Instruct as the backbone model for SHANKS, the performance can be even better. The interruption ratio on the correct subset is lower, and the valid interruption ratio on the wrong subset also grows higher. This shows that the interruption ability of SHANKS is mostly related to the reasoning ability of the backbone model, and using stronger reasoning LLM can improve the performance. Varying tchunk at inference time does not significantly affect the performance. When constructing the training data, we fix tchunk = 4 seconds. Here, we ask whether we can vary tchunk at inference time. Since the thinking of SHANKS always lags behind the latest user speech by tchunk seconds, changing tchunk can affect how soon the SLM can hear the latest user speech and affect the response latency. As an ablation, we change tchunk to 3 and 5 during inference without retraining SHANKS-E2E. The results are shown in the bottom two rows in Table 1. On the wrong subset, we do not find the interrupt ratio and valid interrupt ratio to change significantly compared with tchunk = 4. Interestingly, we find that the interrupt latency on the wrong subset for tchunk = 3 is the smallest, while the tchunk = 5 has the largest interrupt latency. 5.2 RESULTS FOR SCENARIO 2: MAKING TOOL CALLS WHEN LISTENING Next, we move on to the second scenario: making tool calls when listening. The experiment results are presented in Table 2. We summarize the main findings as follows: SHANKS successfully makes at least 56.9% of API calls when the user is still speaking. Among the successful API calls made by the two SHANKS models, about 80% to 90% of the API calls are made during the user speech. In the example in Figure 4, SHANKS-E2E makes four out of six API calls when the user is still speaking. Compared to the call-after-listen baseline, which makes all the API calls after the user turn finishes, SHANKS can significantly reduce the response latency by using the time the user is speaking to make tool calls. Call-after-listen has higher success rate and response quality. While SHANKS elegantly uses the user speaking time to make API calls, the success rate and response quality (correctness and completeness) lag behind call-after listen. We find that this is because during inference, if the API call fails during Ri, SHANKS seldom retries the failed API call in future Rj, where > i. On the other hand, the call-after-listen baseline is more likely to retry failed API calls. 10 Work in Progress Figure 3: An example from the interruption scenario in Section 3.2. The chunks in red are the transcriptions of user describing math problem and attempting to solve it step-by-step. The thinking chunks (in green) and interruption response (in orange) are generated by SHANKS-E2E. For each time slot from ntchunk to (n + 1)tchunk, the chunks in green (SLM thinking chunks) and orange (output response) happen sequentially, while the user speech chunk (in red) happens concurrent to other blocks in the same time slot. Combining SHANKS and call-after-listen yields the best performance. To solve the previously observed issue, simple method is to use SHANKS when the user is still speaking and back off to call-after-listening when the users speech ends. Precisely, when the user is still speaking, we use SHANKS to call APIs while listening, and only keep the successful API calls and their responses. When the user finishes their speech, we switch to the call-after-listening mode, where the input to the SLM is the complete user speech. We also feed the success API calls and responses previously made by SHANKS to the model, as if the call-after-listen model had made those API calls by itself. Based Work in Progress Table 2: Results for API calls. Early and Late are computed over the total set of ground-truth API calls; they need not sum to 100% because some calls may be incorrect. Method Call accuracy (%) Early Late Total Success rate Correctness Completeness (0-2) (0-2) (%) Call-after-listen SHANKS-E2E SHANKS-CASCADE SHANKS+Call-after-listen 0.0 56.9 63.9 57.3 86.5 14.4 5.4 32.8 86.5 71.3 69.3 90.0 63.2 35.2 34.9 62. 1.17 0.79 0.73 1.31 1.37 1.00 1.00 1.43 Figure 4: An example user query from ComplexFuncBench (in red), including the unspoken thinking process (in green) and the spoken final response (in orange) from SHANKS-E2E. For each time slot from ntchunk to (n + 1)tchunk, the chunks in green (SLM thinking chunks), blue (API call responses), and orange (output response) happen sequentially, while the user speech chunk (in red) happens concurrent to other blocks in the same time slot. The = means the time when the users speech terminates. on the full input speech and previous success API calls and responses, the model continues to make the remaining API calls. Since some API calls have already been made by SHANKS when the user speaks, this combined method enjoys the thinking-while-listening advantage of SHANKS. In the last row in Table 2, we show the result of combining SHANKS-E2E with call-after-listening. This combined method has high number of early call accuracy while also having higher task success rate and response quality. Among the API calls that are successful, over 60% of API calls are made during the user speech, while only 40% of the API calls are made after the user finishes. This is in stark contrast to call-after-listen, where 100% of the successful API calls are made after the user finishes, resulting in much higher latency. In summary, the combined method can reduce the user wait time while maintaining the performance."
        },
        {
            "title": "6 RELATED WORKS",
            "content": "Thinking before responding has been widely explored in text-only LLMs (OpenAI, 2024c; Guo et al., 2025). Recently, this think then respond paradigm has been applied to audio-aware language models, which takes speech (or audio) as the input and output texts (Xie et al., 2025). Note that these 12 Work in Progress audio-aware language models, which only output text, are different from the speech-in-speech-out SLMs focused on in our paper. While thinking before responding can improve the response quality and yield significant performance on many challenging benchmarks (Lightman et al., 2024; Sakshi et al., 2025), thinking before responding creates great response latency. As result, it is impractical to directly apply thinking-before-responding to SLMs, speech-in-speech-out models, which require real-time and low-latency interaction (Li et al., 2025; Xu et al., 2025a;b). Developing reasoning methods that preserve real-time interaction in SLMs remains an open problem. Concurrent to us, Chiang et al. (2025) introduce thinking to SLMs by thinking-while-speaking method called STITCH. STITCH uses the fact that chunk of audio in the speech response takes less time to generate than it does to play to the user, and the model can use the remaining time to generate thinking tokens when the SLM is still speaking. While both SHANKS and STITCH explore unspoken thinking processes for SLMs, the main distinction is when the thinking happens. In SHANKS, the thinking process happens when the user is still speaking, while STITCH thinks when the SLM is speaking. In fact, the two methods can be combined: an SLM can think when listening and speaking. We believe this will be the future of SLM, and we leave this as promising future direction Another concurrent work released on arXiv less than one week ago (10/02/2025), Stream RAG (Arora et al., 2025), also studies calling tools (web search and knowledge graph APIs) during the users speech. This is similar to our second scenario introduced in Section 3.2. However, Stream RAG focuses on when to issue retrieval/tool queries while listening and does not introduce an explicit silent chain-of-thought (thinking) process like we do. In contrast, our paper studies broader thinking-while-listening paradigm, with tool-calling as one application, and show benefits such as improved user-interruption decisions."
        },
        {
            "title": "7 CONCLUSION, LIMITATIONS, AND FUTURE WORK",
            "content": "In this paper, we introduce SHANKS, framework that enables SLMs to think while listening. SHANKS achieves thinking-while-listening by chunking the user input speech and progressively reasoning over the available user inputs. When the user is speaking, SHANKS is generating thinking chunks for all previous input speech, achieving thinking while listening. We demonstrate the potential of SHANKS on two scenarios: First, SHANKS can listen to the user solving math problem step-bystep, and interrupt the user when the user is making mistake. Second, we focus on tool-augmented task-oriented dialogue setting and show that SHANKS can listen to the user speech and evoke necessary API calls when the user is still speaking. On ComplexFunxBench, SHANKS successfully calls more than half of the APIs that are required to complete the users request when the user is still speaking. This reduces the response latency, as the model only needs to call the remaining half APIs after the user has finished. While SHANKS shows great potential in improving the user-SLM interaction, we see the following limitations of the method. First, SHANKS requires the users speech to have certain structures: the users speech needs to be long enough to allow the model to perform meaningful reasoning when listening, and the information in the users speech needs to be able to be processed in sequential order. This kind of speech can naturally occur, as shown in the two scenarios we studied. Next, SHANKS uses fixed chunk size to segment the user input speech. The chunking nature of SHANKS means that SHANKS always lags behind the users speech by tchunk seconds, incurring latency in the thinking process. We encourage future work to reduce the latency between thinking and listening by using more sophisticated chunking methods. Last, the goal of the user might be unclear when the users speech is not completed, and the thinking tokens generated during listening may be redundant and not always be useful to address the goal of the user. While thinking during listening does not incur additional latency after the users speech ends, SHANKS still significantly increases the compute cost during inference. Although SHANKS has some limitations, we believe that our effort in proposing novel modeling method, thinking while listening, together with the reasonable scenarios and convincing results, already contributes greatly to the research community by shedding light on potentially fruitful research direction. 13 Work in Progress ACKNOWLEDGMENTS The authors would like to thank Ke-Han Lu, Chen-An Lee, Yi-Cheng Lin, and Wei-Chih Chen for their valuable feedback on the draft of this paper."
        },
        {
            "title": "REFERENCES",
            "content": "Siddhant Arora, Haidar Khan, Kai Sun, Xin Luna Dong, Sajal Choudhary, Seungwhan Moon, Xinyuan Zhang, Adithya Sagar, Surya Teja Appini, Kaushik Patnaik, et al. Stream rag: Instant and accurate spoken dialogue systems with streaming tool usage. arXiv preprint arXiv:2510.02044, 2025. Sara Bogels, Lilla Magyari, and Stephen Levinson. Neural signatures of response planning occur midway through an incoming question in conversation. Scientific reports, 5(1):12881, 2015. Sara Bogels, Marisa Casillas, and Stephen Levinson. Planning versus comprehension in turn-taking: Fast responders show reduced anticipatory processing of the question. Neuropsychologia, 109: 295310, 2018. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1560715631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.870. URL https://aclanthology.org/2023.acl-long. 870/. Cheng-Han Chiang, Xiaofei Wang, Linjie Li, Chung-Ching Lin, Kevin Lin, Shujie Liu, Zhendong Wang, Zhengyuan Yang, Hung-yi Lee, and Lijuan Wang. Stitch: Simultaneous thinking and talking with chunked reasoning for spoken language models. arXiv preprint arXiv:2507.15375, 2025. Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Ruth Corps, Abigail Crossley, Chiara Gambi, and Martin Pickering. Early preparation during turn-taking: Listeners use content predictions to determine what to say but not when to say it. Cognition, 175:7795, 2018. Alexandre Defossez, Laurent Mazare, Manu Orsini, Amelie Royer, Patrick Perez, Herve Jegou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey, 2024. URL https://arxiv.org/abs/2312.10997. Work in Progress Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Roscoe: suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations, 2023. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, In International and Weizhu Chen. LoRA: Low-rank adaptation of large language models. Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clement Delangue, Theo Matussi`ere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Francois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: community library for natural language processing. In Heike Adel and Shuming Shi (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 175184, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-demo.21. URL https://aclanthology.org/ 2021.emnlp-demo.21/. Tianpeng Li, Jun Liu, Tao Zhang, Yuanbo Fang, Da Pan, Mingrui Wang, Zheng Liang, Zehuan Li, Mingan Lin, Guosheng Dong, et al. Baichuan-audio: unified framework for end-to-end speech interaction. arXiv preprint arXiv:2502.17239, 2025. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=v8L0pN6EOi. Guan-Ting Lin, Jiachen Lian, Tingle Li, Qirui Wang, Gopala Anumanchipalli, Alexander H. Liu, and Hung yi Lee. Full-duplex-bench: benchmark to evaluate full-duplex spoken dialogue models on turn-taking capabilities, 2025. URL https://arxiv.org/abs/2503.04721. Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In International Conference on Learning Representations, 2017. URL https://openreview. net/forum?id=Skq89Scxx. Hongyin Luo, Tianhua Zhang, Yung-Sung Chuang, Yuan Gong, Yoon Kim, Xixin Wu, Helen In Houda Bouamor, Juan Meng, and James Glass. Search augmented instruction learning. Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 37173729, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.242. URL https://aclanthology.org/2023. findings-emnlp.242/. 15 Work in Progress OpenAI. Introducing next-generation audio models in the api, 2024a. URL https://openai. com/index/introducing-our-next-generation-audio-models/. Accessed on May 12, 2025. OpenAI. Hello gpt-4o, 2024b. URL https://openai.com/index/hello-gpt-4o/. Accessed on May 12, 2025. OpenAI. Learning to reason with llms, 2024c. URL https://openai.com/index/ learning-to-reason-with-llms/. Accessed on July 15, 2025. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models, 2024a. URL https://arxiv.org/abs/2304.08354. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, and Maosong Sun. ToolLLM: Facilitating large language models to master 16000+ real-world APIs. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/forum?id=dHng2O0Jjr. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. URL https://arxiv. org/abs/2212.04356. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. In International conference on Robust speech recognition via large-scale weak supervision. machine learning, pp. 2849228518. PMLR, 2023. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https://openreview. net/forum?id=Ti67584b98. Sakshi, Utkarsh Tyagi, Sonal Kumar, Ashish Seth, Ramaneswaran Selvakumar, Oriol Nieto, Ramani Duraiswami, Sreyan Ghosh, and Dinesh Manocha. MMAU: massive multi-task audio understanding and reasoning benchmark. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=TeVAZXr3yv. Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 16 Work in Progress 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 26092634, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.147. URL https://aclanthology.org/2023.acl-long. 147/. Mingqiu Wang, Izhak Shafran, Hagen Soltau, Wei Han, Yuan Cao, Dian Yu, and Laurent El Shafey. Retrieval augmented end-to-end spoken dialog models. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1205612060. IEEE, 2024. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Zhifei Xie, Mingbao Lin, Zihang Liu, Pengcheng Wu, Shuicheng Yan, and Chunyan Miao. AudioImproving reasoning capability in large audio language models. arXiv preprint reasoner: arXiv:2503.02318, 2025. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025a. Jin Xu, Zhifang Guo, Hangrui Hu, Yunfei Chu, Xiong Wang, Jinzheng He, Yuxuan Wang, Xian Shi, Ting He, Xinfa Zhu, et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025b. Weilin Zhao, Yuxiang Huang, Xu Han, Wang Xu, Chaojun Xiao, Xinrong Zhang, Yewei Fang, Kaihuo Zhang, Zhiyuan Liu, and Maosong Sun. Ouroboros: Generating longer drafts phrase by phrase for faster speculative decoding. In Yaser Al-Onaizan, Mohit Bansal, and YunNung Chen (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1337813393, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.742. URL https: //aclanthology.org/2024.emnlp-main.742/. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Yixin Cao, Yang Feng, and Deyi Xiong (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 400410, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-demos.38. URL https://aclanthology.org/2024.acl-demos.38/. Lucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, and Jie Tang. Complexfuncbench: exploring multi-step and constrained function calling under long-context scenario. arXiv preprint arXiv:2501.10132, 2025."
        },
        {
            "title": "A ENCODING THE USER SPEECH",
            "content": "In the main content of the paper, we say that we chunk the user input audio into fixed-size chunks of tchunk seconds. In fact, what we do is chunking at the level of feature representation instead of the level of the audio waveform. Precisely, when encoding the 2 speech chunks Si, we feed the full speech through the audio encoder, and only take the speech representation for the corresponding speech chunk. If we directly chunk the audio waveform and encode each audio chunk independently, the representation of later audio chunks will not be able to depend on the earlier audio chunks, which can potentially lead to performance degradation."
        },
        {
            "title": "B DETAILS IN TRAINING",
            "content": "We fine-tune the models using the Llamafactory (Zheng et al., 2024) toolkit. When generating the training data using GPT-4o, we do not feed the audio of the users speech into GPT-4o. Instead, we 17 Work in Progress feed the transcription of the speech chunks. This is because using the speech chunk will increase the cost and time to call the API. To obtain the transcription of each chunk, we use Whisper-largev3 (Radford et al., 2023) to obtain the transcription and timestamp for each word in the user speech, and then segment the transcriptions into chunks based on the timestamp. While the timestamp obtained from Whisper may not be very precise, this is already sufficient for preparing the training data. B.1 FINE-TUNING FOR INTERRUPTION To prepare the training data, we randomly sample 5K samples from Tulu-3-SFT-Math-Grade (Lambert et al., 2024), which can be loaded from Huggingface datasets (Lhoest et al., 2021): https:// huggingface.co/datasets/allenai/tulu-3-sft-personas-math-grade. We follow the procedure detailed in Section 3.1.2 to construct the training data. We additionally filter out audios that are longer than 80 seconds, so the final training dataset is slightly less than 5K. We fine-tune the thinker on the training data for two epochs on 8 A100 GPUs. The effective batch size is 64. We set the learning rate to 1.0e 4 with cosine learning rate scheduling and 0.1 warm-up ratio (Loshchilov & Hutter, 2017). The same training hyperparameters are used across all three models, including the SHANKS-E2E, SHANKS-Cascade, and no-thinking model. The training data is mostly generated by GPT-4o. We include the prompts to generate the reasoning chunks in Table 3 and 4, the prompt to generate the interruption in Table 5, and the prompt to generate the response without interruption in Table 6. B.2 FINE-TUNING FOR TOOL CALLS We use the procedure detailed in Section 3.2.2 to construct the training data. The training data consists of 500 samples. The prompt used to determine when an API call can be made is shown in Table 8. The prompt used to generate the final response is shown in Table 9. For the think-after-listen and SHANKS-E2E model, we fine-tune them using LoRA (Hu et al., 2022), as the sequence length for this dataset is very large and full fine-tuning will result in out-of-memory. We also fine-tune the LM head and the token embedding of the talker model; otherwise, the model will not be able to recognize and generate special tokens. For the SHANKS-Cascade model, we fine-tune all the parameters. As the training dataset is smaller, we fine-tune the model for 10 epochs, while other training hyperparameters follow those in Appendix B.1."
        },
        {
            "title": "C DETAILS IN EVALUATION",
            "content": "C.1 EVALUATION DETAILS FOR INTERRUPTION To determine the time of interruption tinterrupt when evaluating the interrupt latency, we apply the following procedure. We use Whisper-large (Radford et al., 2022) to obtain the timestamp of each word in the user speech from the testing set, and we use GPT-4o to determine when the first error in the user speech occurs by giving GPT-4o the question, the ground truth answer, the transcription of the user speech, and the word-timestamp alignment. The prompt used to determine the first error time terror is shown in Table 7. To evaluate the valid interrupt ratio, we use GPT-4o as the judge. The prompt used to determine whether an interruption is valid is shown in Table 10. C.2 EVALUATION DETAILS FOR TOOL CALL When evaluating the SHANKS models, when the user is still speaking, each thinking chunk can only include at most 320 tokens generated by the SLM itself. In some cases, the API call augments may include very long tokens, which can easily exceed the token limit for the thinking chunk, and the API call will be incomplete and unsuccessful. While we do not specifically handle this kind of case, there is simple workaround that can resolve the above issue: The long arguments for the API call generated by the model must be the returned value of previous tool calls, so we can include previous Work in Progress API call responses in reservoir of the draft for speculative decoding (Leviathan et al., 2023; Zhao et al., 2024), and use speculative decoding to speed up the inference speed. ComplexFuncBench is originally designed to evaluate models ability to parse long-context information, and the API call response can be very long. However, since Qwen-omni only has context length of 32,768, once the token exceeds this limit, we directly terminate the inference for tested instance. As result, some of the testing samples we evaluate may fail because the number of tokens exceeds the max sequence length of the model. Figure 5: The interruption latency for SHANKS. The bars in red are the results on the wrong subset, while the bars in green are the results on the correct subset. One can observe that the red bars are mostly positive, meaning that the model tends to interrupt after the first error occurs. 19 Work in Progress # Generate Internal Thinking While Listening ## Task Introduction Humans are capable of thinking while listening to others speak. details, clarify ambiguities, recall relevant facts, and compute intermediate variables. process. You will be the previous chunks of users speech in text, and you will also see your previous inner thinking when listening to those chunks. Your job is to generate the next internal thinking as if you had listened up to the newest chunk. Based on the partial information received, we parse important Your task is to simulate this The inner thinking should read more like full, coherent sentences rather than shorthand notes. The inner thinking span should be fewer than 400 words. Your internal thinking should reflect the users emotion, intent, and what you already know from the user. When generating internal thinking spans, follow these guidelines: 1. 2. relevant information can be recalled or intermediate variables can be calculated based on current information, include them in your inner thinking. 3. very hard to understand and possibly making logical errors. 4. the user finishes speaking**. 5. transcription spans update or contradict earlier information, explicitly point that out and correct it. phrases like Wait, the user previously..., but now...\". 6. solution and then compare your own solution with the users solution. user immediately. Indicate the interruption by ending your internal thinking with the special token [INTERRUPT]. If the users query involves question, you **must generate your own step-by-step answer in the internal thinking before Later internal thinking spans must not repeat information already covered in earlier ones. When the user is providing there solution, you should have you own Always think independently in your internal thinking. If you identify any error, you should interrupt the Using short notes will be You may start with However, if later If any --- ## Samples ### Example 1: User (partial) input transcription 1 Betty is saving money for new wallet which Prior Inner Thinking 1 The users tone is neutral. user hasnt finished yet. Perhaps they want me to give advice on how to save money. The user describes situation where someone named Betty is saving money for new wallet. The User (partial) input transcription 2 costs $100. Betty has only half of the money she needs. Prior Inner Thinking 2 Now the user gives more information. calculate this: $100 / 2 = $50, so she currently has $50. We know the wallet Betty wants to buy costs $100, and she has only half of that. The users intent is still unclear. can User (partial) input transcription 3 Her parents decided to give her $15 for that Inner thinking to Generate Well, it seems that Bettys parents will contribute as well. still needs $100 - $65 = $35. It seems the user wants to know how to plan Bettys budget. She will get $15 from them. Now she has $50 + $15 = $65. She ### Example 2: User (partial) input transcription 1 want to answer the following math question: Is the function f(x) = Prior Inner Thinking 1 The user uses neutral tone and describes math question they want to solve. is speaking. can verify if the users solution is correct by comparing my solution with the users solution. is about function in x. The user has not finished yet. will wait until they complete the question. will try to solve the question when the user The question User (partial) input transcription 2 4x2 + 4x + 1 always above the axis? Prior Inner Thinking 2 The user has finished describing the question. They want to determine if the function f(x) = 4x2 + 4x + 1 is always above the x-axis. am ready to answer this question. 16 = 0. The function touches the x-axis at one point. by themselves, will not interrupt the user and listen to the solution. mistakes. Since the user wants to solve the question However, will interrupt the use if there is any Lets compute the discriminant: have the solution. = b2 - 4ac = 42 - 4 * 4 * 1 = 16 - User (partial) input transcription 3 Here is my solution: Based on the formula = b2 - 2ac, Inner thinking to Generate Now see that the user wants to solve the question themselves. says the formula is = b2 - 2ac, but thats incorrect. and need to interrupt them. [INTERRUPT] The correct formula is = b2 - 4ac. should verify whether their solution is correct. The user The user has made mistake, Table 3: Prompt for generating the thinking chunks in the training data for the interruption experiment (part 1). Work in Progress ### Example 3: User (partial) input transcription 1 Bella bought stamps at the post office. Some of the Prior Inner Thinking 1 The user uses neutral tone and describes math question he wants to solve. question is to The user is introducing word problem involving someone named Bella and her purchase of stamps. The exact numbers and relationships havent been shared yet. will hold off making any calculations until know more about the stamps. It seems like math problem, likely about counting. The User (partial) input transcription 2 stamps had snowflake design, some had truck design, Prior Inner Thinking 2 We now know there several types of stamps: snowflake and truck. be more, but the user is still speaking. There are still no numbers provided, so cant compute yet. will wait for more information. There seems to User (partial) input transcription 3 and some had rose design. Bella bought 11 Prior Inner Thinking 3 Now we know that there are three types of stamps: snowflake, truck, and rose. Bella bought 11, but it is unclear which this number corresponds to. for the complete detail from the user to be sure before computing. will wait User (partial) input transcription 4 snowflake stamps. She bought 9 more truck stamps Prior Inner Thinking 4 Now know that Bella bought 11 snowflake stamps. am also told she bought 9 more truck stamps than snowflake stamps. can calculate the number first: she bought 11 + 9 = 20 truck stamps. The information we have now is: - Snowflake: - Truck: 20 The user is still talking, and am waiting for more information. 11 User (partial) input transcription 5 than snowflake stamps, and 13 fewer rose stamps than Prior Inner Thinking 5 Now the user states that Bella bought 13 fewer roses than something, but it is unclear what is compared here. will wait until the user finishes. User (partial) input transcription 6 truck stamps. How many stamps did Bella buy in all? Inner thinking to Generate Now know that Bella bought 13 fewer roses than the truck stamps. There are 20 truck stamps, so can calculate the number of rose stamp is 20 - 13 = 7. finishes with question: - Snowflake: - Truck: 20 - Rose: Total = 11 + 20 + 7 = 38 stamps. have the answer and can provide it to the user. total number of stamps. already have all counts: 7 The user --- This is the end of the examples. Now, this is the (partial) user input transcription, and you need to generate inner thinking. You do not need to explain why the inner thinking you generate is good one. Simply generate good one without explaining it. {interleaved transcription and thinking} Inner thinking to generate (Do not generate anything else other than the inner thinking) Table 4: Prompt for generating the thinking chunks in the training data for the interruption experiment (part 2). 21 Work in Progress # Task: Interrupt the user to correct an error user is talking to an AI assistant. You will be given partial user turn. assistant has identified that error. The AI assistant needs to interrupt the user. There is an error in the user turn and the AI Your job is to generate the response for the AI assistant that interrupts the users turn. (1) (possibly incomplete) user turn (2) The inner thinking of the AI assistant. This inner thinking hasnt been spoken out by the AI assistant and is only silently kept in the assistants mind. We provide you this inner thinking for you to better craft response. You will be given: When correcting and interrupting the user, be precise about what the error is and how to correct it. generate the response without saying anything else. so you need to make your response easy to be spoken while not overly informal and colloquial. The conversation between the user and the assistant is in spoken form, You only need to ## Example #### User (partial) input want to answer the following math question: Is the function f(x) = 4x2 + 4x + 1 always above the axis? solution: Based on the formula = b2 - 2ac, = 42 - 2 * 4 * 1 = 8 > 0 Here is my #### Inner thinking of the assistant The user uses neutral tone and describes math question he wants to solve. function is above the x-axis. f(x) = 4x2 + 4x + 1. to intersect with x-axis at one point. can answer the user if the user wants me to do so. But wait, the user themselves want to solve the question, and the user says = b2 - 2ac, which is clearly wrong. correct formula should be = b2 - 4ac. should interupt the user here and tell them the correct formula with friendly and reminding tone. Lets use = b2 - 4ac = 42 - 4 * 4 * 1 = 0. The question is to determine if 2-degree So the function happens The #### Assistant Response Wait, think the correct formula should be b2 - 4ac, not b2 - 2ac. The coefficient you mentioned was wrong. ## Now it is your turn #### User (partial) input {query} #### Inner thinking of the assistant {inner thinking} #### Assistant Response <Write the interrupting response here. Be precise about the error and the correction; keep it concise and easy to speak. not include anything else.> Do Table 5: Prompts for generating an interrupting correction response. # Task: thinking Generate the spoken response given full user turn and assistants inner user is chatting with voice assistant. Your job is to act as the voice assistant and generate valid response that fits in the context. You will be given: (1) The full user turn (2) The inner thinking of the voice assistant. Note that the voice assistant may generate the inner thinking when the user hasnt finished, so it is possible that some contents in the inner thinking is incorrect. Guidelines: Do not generate anything else except the response. 1. 2. The inner thinking might mention drafted response. If the drafted response is still valid considering the full user turn, follow the draft and start the response. If the draft is invalid considering the full user input, neglect the draft and craft response that is suitable. 3. However, there is no need to deliberately use very colloquial words or phrasing, making things awkward. This is spoken dialogue. Keep the response easy to follow for spoken form. ## Input #### Full user input {query} #### Inner thinking of the voice assistant {inner thinking} #### Your Response (Act like the voice assistant) <Write only the final spoken response here> Table 6: Prompts for generating the response for the interruption application. 22 Work in Progress # Task: Detect the first reasoning or calculation error with timestamps In the user query, the user describes math problem and You users query. then attempt to solve the problem by themselves. This user query is in spoken form, and provide you with the transcription. will also provide you the force alignment result of the transcription, which corresponds timestamp of each word in the spoken response. In your response, you should solve the math problem by Your job is to determine where the problem solving process has the first calcaultion or reasoning error. yourself, and carefully check the spoken response. When you see the first error in the spoken response, use the provided timestamp to determine when the first error happened. is the time where the first error happens. If the users problem solving process is completely correct, please use -1 to indicate that there is no error, i.e., \"First error: Conclude you respond with: \"First error: [time]\", where time -1\" ## Example ### User Query want to solve the following math question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Here is my solution: Our goal is to calculate the total number of clips sold in April and May. In April, she sold 48. In May, she sold the half of that, which is 96. So she sold 48 in April plus 96 in May, making it 144 in total. ### Word-Timestamp - 0.00 want - 0.50 ... 96. ... total. - 36.50 - 44.00 ### Correct Answer In May, she sold half as many, so she sold 48 / 2 = 24. In total, she ### Output The math problem wants to know how many clips Natalia sold in total. sold 48. sold 48 + 24 = 72. that, which is 96. This is where the first error occurs. Based on the Word-Timestamp information, the word 96 is emitted at second. First error: This is incorrect. The correct number for May should be 24. In the problem solving process, the user says that half of In April, she 36. ## Now, it is your turn. ### User Query {question} ### Word-Timestamp {alignment} ### Correct Answer {answer} ### Output <Write the reasoning here and conclude with \"First error: [time]\"> Table 7: Template for detecting the first error in the interruption task. 23 Work in Progress # Task: Earliest possible time to call tools during spoken user query You are given user spoken query, which requires some tool usage for answering. You will be given the tool calls (including their parameters) which are useful for responding to the user query. the users utterance. can be called when the user is speaking. That is, when the user is still speaking, the information that has been spoken by the user may already be sufficient enough Your job is to determine the **earliest time** during to call some of the tools. the utterance that tool call can be called. tool can be called if only if it is clear what tool should be call and what the paramaters are for the tool call. Your job is to determine the earliest time that tool call You will also be given the timestamp of each word in ### User Spoken Query {question} ### Time Stamp of Each word {alignment} ### Tools that needs to be called {tools} ### Total number of tool calls {count} ### Output Format Your response should be python dictionary. The key of this dictionary is an integer index of the tool call shown above, and the value is the earliest time the tool can be called. first character in your response should be the left bracket while the last character in your response should be the right bracket. Your response should be able to be directly converted into python dictionary using eval(). If there are tools that need to be called, your output dictionary should have items. also provide you the number of tool calls, so you should verify if your output dictionary matches the number of tool calls. Your response should only include python dictionary. The ### Your response: <Return only python dictionary, e.g., {0: 12.5, 1: 18.0}> Table 8: Template for checking the earliest callable time for an API. # Task: Generate the final user-facing response from tool call results You will be given user query. The user query can only be responded based on the results of some external tool call. will show you the tool calls and call Your task is to generate final response to the user based on the tool responses. call results. The final response to the user should satisfy the users original query and omit unnecessary information. Some intermediate processes in the tool call may simply be some process to resolve the variables, and they are not necessary to be included in the final response to the user. ### User Query {transcription} ### Previous API Calls {previous tool calls} ### Response to the User Query (Only provide the response. Do not include anything else.) <Write only the final user-facing response here, distilled from the tool results and satisfying the query. Exclude setup steps and variable-resolution details.> Table 9: Prompts for generating final response in the API call application. Work in Progress # Task: Judge if the assistants interruption is reasonable user is speaking to voice assistant. When the user is speaking, the assistant tries to interrupt the user. the user in reasonable way. something wrong and ambiguous, and the assistant is trying to help correct or clarify the users statement. Your job is to judge if the assistant is interrupting reasonable interruption is when the user says Here is the users speech before the assistant interrupted: {user speech before interrupt} Here is the assistants speech that attempts to interrupt the user: {assistant speech after interrupt} Please judge if the assistant is interrupting the user in reasonable way. If the assistant is interrupting the user in reasonable way, return \"yes\". Otherwise, return \"no\". Please provide some explanation for your judgment and conclude with \"Final verdict: mistake and the assistant is trying to help correct or clarify the users statement. valid interruption is when the user is indeed making Yes/No\". ### Output <Write your explanation here. Conclude with \"Final verdict: Yes\" or \"Final verdict: No\"> Table 10: The prompt used for judging whether an interruption is reasonable."
        }
    ],
    "affiliations": [
        "Microsoft",
        "National Taiwan University"
    ]
}