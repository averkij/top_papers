{
    "paper_title": "Look Every Frame All at Once: Video-Ma$^2$mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing",
    "authors": [
        "Hosu Lee",
        "Junho Kim",
        "Hyunjun Kim",
        "Yong Man Ro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma$^2$mba, a novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma$^2$mba can process extensive video sequences-equivalent to millions of tokens or over two hours of continuous sequences at 1 FPS-on a single GPU. By maintaining a detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 0 6 4 9 1 . 1 1 4 2 : r Look Every Frame All at Once: Video-Ma2mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing Hosu Lee* Junho Kim*"
        },
        {
            "title": "Hyunjun Kim",
            "content": "Yong Man Ro Integrated Vision and Language Lab, KAIST, South Korea {leehosu01, arkimjh, kimhj709, ymro}@kaist.ac.kr https://ivy-lvlm.github.io/Video-MA2MBA"
        },
        {
            "title": "Abstract",
            "content": "With the growing scale and complexity of video data, efficiently processing long video sequences poses significant challenges due to the quadratic increase in memory and computational demands associated with existing transformer-based Large Multi-modal Models (LMMs). To address these issues, we introduce Video-Ma2mba, novel architecture that incorporates State Space Models (SSMs) within the Mamba-2 framework, replacing the attention mechanisms. This allows the LMMs to scale linearly in terms of time and memory requirements, making it feasible to handle long-duration video content. Furthermore, we enhance the memory efficiency introducing the Multi-Axis Gradient Checkpointing (MA-GC) method, which strategically manages memory by retaining only essential activations across multiple computational axes. Our approach significantly reduces the memory footprint compared to standard gradient checkpointing. Empirical analyses show that Video-Ma2mba can process extensive video sequencesequivalent to millions of tokens or over two hours of continuous sequences at 1 FPSon single GPU. By maintaining detailed capture of temporal dynamics, our model improves the accuracy and relevance of responses in long video understanding tasks, demonstrating substantial advantages over existing frameworks. 1. Introduction As video data grows in scale and complexity, the demand for models capable of efficiently processing long video sequences has intensified. Transformer-based models [3, 41] have become central to sequence processing due to their effectiveness and versatility in handling complex dependencies as input frames increase. With the emergence of the Large Language Models (LLMs) era [14, 30, 31], video understanding models have entered new phase. Thanks *Equal contribution. Corresponding author. Figure 1. Memory usage comparison across sequence lengths for Mamba-2-2.7B with different checkpointing methods, demonstrating the memory-saving capability of Multi-Axis Gradient Checkpointing (MA-GC). to the core capabilities of zero-shot learning and strong reasoning in LLMs, various Large Multi-modal Models (LMMs) [9, 25, 26] have achieved enhanced cross-modality consistency, particularly between vision and language. Accordingly, diverse video-LMMs [21, 23, 29] have been proposed to understand video content by integrating spatiotemporal information into LLMs, and achieved comparable reasoning performances to comprehend complex visual narratives and temporal dynamics. However, current video-LMMs face critical challenges when applied to longer video sequences integrated into the LLM structure, as the attention mechanism [42]the core component of Transformersincurs memory load and computational costs that scale quadratically with sequence length. This growth in resource requirements becomes prohibitive for processing extended video sequences, where sequence lengths often exceed 128K tokens (approximately 400 frames), resulting in significant inefficiencies in memory consumption and computational load. To address the challenge for the long video understand1 ing, various strives have been evident with several different approaches: (i) sparse and uniform sampling (e.g., typically 8 or 16 frames) to reduce the number of input video frames into LMMs, which is the most common method [20, 23, 24] and (ii) memory-augmented generation [16, 39] that stores long-term visual information and later access to the memory bank. More recently, Zhang et al. [48] have proposed method for transferring long context by employing RoPEbased [40] frequency extension in LM backbones within existing memory limits, which enables the models to process more visual tokens for the longer video sequences. Despite such efforts, the exponential growth in memory usage with increasing sequence lengths imposes fundamental limitations on the amount of information that can be fed into the model. Consequently, when the model is prompted to respond based on missing information (or frames), it often generates responses that are irrelevant and disconnected from the facts or user queries. Here, the core difficulty lies in the quadratic time and space complexity of the Transformers attention mechanism, which restricts efficient processing of lengthy video sequences. To enable more practical scalability when handling longer sequences, the fundamental solution is on shifting from quadratic complexity to linear complexity, facilitating more scalable processing for the video data. To do so, in this paper, we introduce Video-Ma2mba, specifically designed to handle extremely long video sequences all at once. We substitute the Transformer-based LLMs [8, 41] to Mamba-2 [11] structure, utilizing State Space Models (SSMs) [15] as replacement for the attention mechanism. This allows our framework to not only preserve the effectiveness of sequence processing but also enhance memory efficiency, thereby achieving linear time and space complexity with respect to the sequence length. In addition to the architectural changes, to push the boundary of memory utilization within Mamba-2 architecture, we present new Multi-axis Gradient Checkpointing (MA-GC) method. Specifically, the Gradient checkpointing (GC) [6] strategically saves selected activations throughout the computational graph, allowing only partial activations to be re-computed during the backward pass (therefore, widely used for managing the substantial memory demands of extensive attention layers in Transformerbased LLMs). On the other hands, due to the nature of Mamba structure, which belongs partially observed Markov models where the hidden state progresses over time according to Markov process rather than dense interaction models (e.g., Transformer), we can implement another axis for GC in the sequential direction by selectively retaining only those sequence-wise activations necessary for backpropagation. The MA-GC reduces memory usage from the O(LS) complexity of the original Mamba-2 to O(S) by applying GC in multiple directions, which allows Video-Ma2mba to process full video sequences at 1-second intervals without the need for frame sampling. Our empirical analyses in Fig. 1 indicate that the proposed method performs effectively in handling extended sequences, successfully processing sequence lengths in the millions on single GPU, corresponding to over 2-hours of continuous video input at frame rate of 1 FPS. By observing each frame at regular intervals, our approach captures more comprehensive temporal information than uniform sampling, providing improvements in model responses and memory efficiency. Through extensive experiments and computational analyses on MA-GC, we demonstrate that Video-Ma2mba efficiently manages resource demands, effectively breaking the quadratic memory growth for handling long sequence. Our contribution can be summarized into three-fold: We propose Video-Ma2mba, new multi-modal framework designed to handle extensively long video sequences without losing frame information, by replacing the Transformers with Mamba-2 architecture. To significantly enhance memory utilization within our framework, we introduce the Multi-axis Gradient Checkpointing strategy. Our strategy selectively stores activations in bi-axis direction, effectively reducing the space complexity to O(S). Through extensive evaluation and analyses, we corroborate that our framework can efficiently process sequence lengths in the millions, corresponding to up to 2-hours of video sequence at 1 FPS with competent performance. 2. Related Work 2.1. Context Extension Methods One of Training models with extended sequence lengths has become increasingly challenging due to the computational and memory demands associated with scaling sequence lengths. The standard Transformer models [3, 42] struggle with the quadratic complexity of attention mechanisms, which quickly becomes infeasible for long sequences. To address these limitations, various methods have been proposed to extend the context length either during or after pre-training. the common approaches is to generalize knowledge learned over shorter sequences to longer sequences [10, 35]. However, this approach can lead to issues in positional extrapolation, as positional encodings trained on shorter contexts may not generalize well to longer contexts. Techniques such as Rotary Position Embedding [40] and Position Interpolation [5] mitigate this by modifying the positional embedding, making it better suited for extrapolation beyond the training range. Additionally, ALiBi [35] has applied attention biases directly, improving context extension without strict reliance on positional encodings. An alternative approach is leveraging Structured State Space Models (SSMs) [15], which inherently achieve lin2 ear time and space complexity with respect to the sequence length. This efficiency stems from the intrinsic properties of SSMs, allowing for effective training on extended contexts and facilitating long-sequence learning without the prohibitive memory costs. 2.2. Long Video Understanding with LMMs Long video understanding presents unique challenges due to the need to capture dependencies across extended sequences while managing high memory consumption. The core challenges in handling long videos are memory limitations and finite context lengths. Several studies [18, 50] have addressed this by sparsely sampling video frames (typically 8 or 16 from the video instance rather than using dense fps-based sampling), or by employing token compression to reduce data to more manageable size [22, 29]. Additionally, memory-augmented approaches [16, 39] have been proposed to store relevant information beforehand and recall explicit knowledge when generating responses. While such methods are simpler to implement and effective for managing memory, they risk missing critical details in long video content. Our approach addresses these limitations by utilizing full-frame sequences at 1 FPS, ensuring comprehensive temporal representation without relying on sparse sampling and achieving memory efficiency. 2.3. Gradient Checkpointing Techniques The gradient checkpointing (GC) is well-established method for reducing memory usage in deep learning models by selectively storing intermediate activations and recomputing them as needed during the backward pass. Initially developed to manage memory constraints in training, this approach allows models to trade off additional computation for reduced memory requirements. Chen et al. [6] have introduced fundamental checkpointing techniques applicable across deep networks and recurrent neural networks (RNNs). For deep networks, the technique involves segmenting the network along the layer axis, storing the outputs at segment boundaries, and recomputing the intermediate results within each segment as needed. This segmentation reduces memory requirements n), where is the number of layers. For from O(n) to O( RNNs, similar approach is used along the time-axis, allowing memory usage to scale sublinearly with sequence length by storing checkpoints at specific time intervals. Our work builds on these principles by applying the GC in both the layer and sequence dimensions, enabling efficient processing of long sequences across bi-directional axes. Our approach enhances memory efficiency and is particularly well-suited for understanding long videos, where integrating both temporal and spatial contexts is crucial. 3. Video-Ma2mba Overview. We first elaborate on the distinction of the Mamba-2 architecture in handling memory efficiency, then introduce new gradient checkpointing method that can be key factor in extending the sequence length of Mamba2. By seamlessly implementing our new context extension strategy during the training of Video-Ma2mba, our video model can handle up to maximum 0.8M input sequence tokens and overcome current challenges in long video understanding relying on partial frame sampling. 3.1. Preliminary: Mamba-2 and Simplification The Mamba model [15] initially has leveraged structured state space models (SSMs) to efficiently handle sequence modeling. Building on this foundation, Mamba-2 [11] represents further advancement to scale up to larger state sizes with the concept of Structured State-Space Duality (SSD), which enhances sequence processing capabilities through time-varying state transitions and input-output mappings, thus more effective handling for sequence data. The general form of SSD in Mamba-2 can be formulated as: ht = Atht1 + Btxt, yt = Ctht, (1) where At RN , Bt RN 1, and Ct R1N are state matrices that vary over time, allowing the model to adapt dynamically to different input structures. This time variance, or selectivity, enhances Mamba-2s flexibility in comparison to linear time-invariant SSMs. By employing timevarying matrices At, Bt, and Ct, Mamba-2 can be seen as selective SSM that performs sequential updates to the hidden state ht based on previous states and current inputs. This selective structure is particularly advantageous in capturing sequence dynamics over longer frames, which standard Recurrent Neural Networks (RNNs) [12] with fixed parameters struggle to achieve. At the same time, Mamba-2 also shares some similarities with certain RNN framework when non-linear activations are removed. standard RNN with non-linear activation function σ (e.g., Tanh or ReLU) updates its hidden state as follows: ht = σ(Aht1 + Bxt), yt = σ(Cht). (2) Here, removing the activation function σ transforms the RNN, making its structure similar to that of SSD: ht = Aht1 + Bxt, yt = Cht. (3) Consequently, SSD can be regarded as simplified version of RNN, where the time-varying parameters of SSD introduce level of flexibility and adaptability that fixedparameter RNNs lack. This dynamic modification allows SSD to effectively address challenges associated with static parameter models in handling complex temporal sequences. 3 3.2. Multi-Axis Gradient Checkpointing , not Considering that Mamba-2 follows RNN-like structure, as illustrated in Fig. 2, when processing the SSD state i+1, it only requires the prior state i1, unlike Transformers that require all previous states to calculate attention weights across the entire input sequence. Here, it is important to note that this distinction enables us to introduce an additional gradient checkpointing axis, not only along the layer direction but also uniquely along the sequence direction, which attribute to the architectural properties of Mamba-2 (whereas Transformer cannot achieve). Our key motivation for employing bi-axis checkpointing lies in its effectiveness at managing memory demands, which enables the processing of extremely long video sequences in their entirety without needing to sample scenes partially. Here, we introduce new GC strategy, MultiAxis Gradient Checkpointing (MA-GC), that not only increases the feasible sequence length up to 219 but also substantially cuts activation memory usage. While previous layer grouping [6] achieved some methods such as the laymemory reduction by applying checkpoints every ers, they were inadequate for very long sequences. In contrast, our MA-GC method applies checkpointing along both layer and sequence axes, reducing space complexity from O( S) in standard GC to just O(S). This significant improvement allows our model to process longer sequences more efficiently without partial frame sampling, thus supporting extended sequence lengths in understanding long video content. Specifically, as shown in Fig. 2, our MA-GC strategy involves two checkpoint types in the forward pass for the given sequence length and stacked layers: (i) Layerwise checkpoints, where layer activations are stored every layers, and (ii) Sequence-wise checkpoints, where states across all layers are stored every time steps. The intersecting points of these two checkpoints create grid cells that are essential for efficient backpropagation. Within each grid cell, activations are sequentially restored and gradients are propagated in an efficient manner. This grid-based structure facilitates selective reconstruction of states only when necessary, thereby optimizing memory usage during the computationally intensive backpropagation process. We provide detailed explanations of both the forward and backward processes in Algorithm 1 and Algorithm 2. 3.3. Analysis of Upper Bound of Memory Reduction To understand how MA-GC reduces memory usage, we analyze its space complexity and establish an upper bound on memory savings. In naive RNN (or similarly, in Mamba or Mamba-2) architecture with layers and sequence length S, backpropagation requires storing activation memory of Θ(L S) due to the need to store activations for each layer and each time step during the backward pass. 4 Figure 2. Overview of MA-GC grid structure. Checkpoints are stored every layers and steps. The blue, red, and green arrows indicate forward propagation, activation restoration, and gradient propagation, respectively. This grid design optimizes memory by selectively restoring activations as needed. The below table shows comparison of checkpointing usage, maximum sequence length on 80GB VRAM, and peak activation memory in BFloat16 at sequence length 16384. In our proposed MA-GC method, the required memory is given by: = LS (cid:124)(cid:123)(cid:122)(cid:125) L-wise G. Ckpt + LS (cid:124)(cid:123)(cid:122)(cid:125) S-wise G. Ckpt + ls (cid:124)(cid:123)(cid:122)(cid:125) Grid Ckpt Cell , (4) where and represent the checkpoint intervals along the layer and sequence directions, respectively. The first term accounts for the memory needed for layer-wise checkpoints, the second term for sequence-wise checkpoints, and the third term for activations stored within each grid cell during recomputation, respectively. Our objective is to minimize by finding optimal values for and s, thus maximizing memory savings. To achieve this, we employ the Extreme Value Theorem and Fermats Theorem to identify the minimum values of differentiable function on closed interval. Theorem 1 (Extreme Value Theorem) If is continuous on closed interval [a, b], then achieves both maximum and minimum values on [a, b]. Theorem 2 (Fermats Theorem) If is differentiable on an open interval (a, b) and has local extremum at an interior point (a, b), then (c)=0. According to these theorems, the minimum value of (l, s) occurs either at critical point (where s =0) =0 and Algorithm 1: Checkpointing Strategy with MAGC (Forward) Input : Input sequence {xi}S Intervals lint, sint Output: Checkpoints Lckpt, Sckpt i=1, Total layers L, 1 Initialization: Lckpt defaultdict(list), Sckpt defaultdict(list); 2 for 1 to do xi; 3 0; for 1 to do 4 5 6 7 9 10 11 12 if mod lint = 0 then Append to Lckpt[i/sint, j/lint]; end if if mod sint = 0 then Append to Sckpt[i/sint, j/lint]; end if (x, h) Layerj(x, h); end for 13 14 end for 15 return Lckpt, Sckpt or at the boundary of the region defined by 1 and 1 S. First, we calculate the partial derivatives of : l s = LS l2 + = 0, = LS s2 + = 0. (5) (6) Solving these, we find that the critical point occurs at l=s= 3 LS, with the corresponding minimal memory requirement critical: critical = 3(LS) 2 3 . (7) However, since and are integers within 1 and 1 S, we should evaluate boundary cases. To do so, we use the Arithmetic Mean-Geometric Mean (AMGM) inequality to derive upper bounds for the minimum values of and s. The AM-GM inequality states that for non-negative real numbers and b, their arithmetic mean is at least their geometric mean: a+b ab. We can express lower bound of each bound as: (cid:18) l-bound = + + 2L 2 + (8) S, (cid:19) (cid:19) s-bound = + (cid:18) Then, the overall optimized memory is selected based on the regions where each configuration achieves balance + 2S + (9) L. Algorithm 2: Backpropagation with Grid-Cell Restoration on MA-GC (Backward) Input : Gradients y, state, Checkpoints Lckpt, Sckpt, Total layers L, Intervals lint, sint 1 Initialization: h[j] state[(j 1) lint : lint] for each = 1, . . . , L/lint; 2 for iblk to 1 by sint do y[iblk sint : iblk] 3 for jblk to 1 by lint do 4 5 7 8 9 10 iidx iblk/sint; jidx jblk/lint; xckpt Lckpt[iidx, jidx]; hckpt Sckpt[iidx, jidx]; (x, h) Recompute Forward(xckpt, hckpt, iblk, jblk); (x, h[jidx]) Backward(x, h, x, h[jidx]); end for 11 12 end for according to the AM-GM inequality, as follows: = Θ(cid:0)(LS) 2 3 (cid:1) Θ(S) Θ(L) if S2 and L2, if L2 S, if S2 L. (10) Since we are interested in training longer sequences with fixed number of layers (i.e., < S), we focus on the region where L2 S. Thus, the memory required simplifies to =Θ(S), and the memory savings ratio LS is given by: (11) LS = Θ(L). Thus, as the sequence length grows, the upper bound on memory savings achieved by MA-GC scales proportionally to the number of layers L. This analysis demonstrates that MA-GC effectively reduces space complexity from Θ(LS) to Θ(S), enabling the processing of very long sequences with significantly reduced memory constraints. As shown in Fig. 1, our experimental results validate this theoretical analysis. Without gradient checkpointing, the activation memory requirement at sequence length 214 is 42.6 GB, whereas with MA-GC, only 42.2 GB is required at sequence length 219. This demonstrates the practical effectiveness of MA-GC in drastically reducing memory usage while enabling to handle extremely long sequences. 3.4. Model Architecture Now, we move on to training Video-Ma2mba with long video data using the proposed MA-GC strategy. Analogous to the widely adopted LMM architecture [25, 26], 5 training approach, we aim to develop robust capability in Video-Ma2mba to handle complex, long-form video data in contextually aware manner. Stage 1: Cross-modal Alignment. During the initial step of training our framework, we utilize dataset comprising total of 790K image and video pairs with associated texts to ensure cross-modality consistency: 558K image-text pairs, filtered by LLaVA [26], and the remaining video-text pairs sampled from WebVid-2.5M [2]. During this stage, we only optimize the parameters in the projector layer. Stage 1.5: Long Video Knowledge Learning. As the intermediate long video parametric knowledge learning step, we utilize the SceneWalk dataset [19] that consists of 11.8Khrs of YouTube videos from diverse categories (total 87.8K videos). This dataset includes 1.3M segmented video clips, each with corresponding scene-level detailed description. We train our model on the next-word generation task by applying an interleaved format to the segmented videos and their corresponding dense captions within each long video instance. In this stage, we unfreeze all parameters to facilitate comprehensive long video understanding. Stage 2: Supervised Fine-Tuning. We fine-tune our model on diverse video QA dataset, including LLaVA-Video178K [49], NeXT-QA [44], ActivityNetQA [46], and PerceptionTest [34]. Together, these sources provide total of 1.3 million video-instruction QA data caption entries, open-ended QA, and multiple-choice QA. During this stage, we fine-tune all parameters by unfreezing every network component to enhance the models QA capabilities. 4. Experiments 4.1. Experimental Setup Training Details. To train our models, we used 1 node of 8 NVIDIA A100 GPUs, each with 80GB of memory. cosine learning rate schedule was employed, with learning rate of 1 103 for Stage 1 and 4 105 for both Stage 1.5 and Stage 2. We trained our models for one epoch at each step, with the entire training process taking approximately 4 days to complete for the 3.1B size model. All stages utilized BF16 precision, and we did not employ any form of parameter-efficient fine-tuning such as LoRA [17]. To reduce the memory overhead from optimizer states, we utilized ZeRO-1 optimization [37], enabling more efficient memory management during training. Additional hyperparameter details are provided in Appendix B. Implementation Details. Our model, Video-Ma2mba, uses CLIP-ViT-L-336px ( 0.4B params) vision encoder paired with backbone LMs (370M / 1.3B / 2.7B), resulting in total model sizes of approximately 0.7B, 1.8B, and 3.1B, respectively. For the largest model configuration (3.1B), the backbone Mamba2-2.7B configuration specifically uses an Figure 3. The overall summarization for the training stages of Video-Ma2mba. as outlined in Fig. 3, Video-Ma2mba follows three main structural components: (i) vision encoder to process input video frames, (ii) cross-modal projector to align vision-text modalities, and (iii) LLM backbone with Mamba-2 architecture (370M / 1.3B / 2.7B). To ensure thorough coverage of the video content, we systematically sample each frame at 1 FPS. We employed CLIP-ViT-L-336px [36] as our vision encoder, extracting features from the penultimate layer, followed by 2x2 bilinear pooling to produce 144 visual tokens per frame. Then, we transform these features to align with the language models embedding space using lightweight 2-layer MLP projection with GELU activation. For the language model backbone, we use the Mamba-2 structure instead of standard Transformer-based LLMs, enabling more efficient embedding process with newly designed MA-GC strategy. 3.5. Training Stages Our training pipeline consists of three stages as summarized in Fig. 3. Beyond the conventional two step training steps for LMMs: alignment training + supervised finetuning with instruction data, Li et al. [27] have highlighted the importance of high-quality knowledge acquisition between these two stages (thus, stage 1.5). By exploiting the extended context length with the MA-GC during training, we reemphasize that our primary goal in training VideoMa2mba is to enhance the models ability to process and learn from long-form video data effectively. Here, expanding the stage 1.5 learning [27], we train Video-Ma2mba using an interleaved learning approach with long video data [19], which comprises densely captioned video-text pairs covering entire long-sequence videos, each segment described in detail. By doing so, Video-Ma2mba can preserve the narrative flow across video segments, enhancing its temporal understanding by learning the sequential relationships within the video content. Through the refined 6 Video-MME ActNet-QA VCG MVBench Model GPT-4V [32] GPT-4o [33] Gemini 1.5 Pro [38] ST-LLM [28] VideoChat2-Mistral [21] Video-LLaVA [23] ShareGPT4Video [4] Chat-UniVi-V1.5 [18] Qwen-VL-Chat [1] SliME [50] Video-Ma2mba-0.7B Video-Ma2mba-1.8B Video-Ma2mba-3.1B Size - - - 7B 7B 7B 8B 7B 7B 8B 0.7B 1.8B 3.1B Short Medium Long Avg. 59.9 70.5 71.9 80.0 75.0 81.7 37.9 45.7 39.5 48.3 39.9 45.3 39.9 48.3 40.6 45.7 41.1 46.9 45.3 53.3 33.1 37.4 40.3 49.4 57.6 45.2 53.5 65.3 67.4 31.3 33.2 36.2 35.0 35.8 37.8 39.8 26.8 31.9 35. 55.8 70.3 74.3 36.8 37.0 38.0 36.3 40.3 38.7 42.7 35.0 39.2 42.7 (a) Experimental results on Video-MME LongVideoBench Size Model 5 1 - 8 71.6 GPT-4o [33] 70.2 Gemini 1.5 Pro [38] 66.4 GPT-4-Turbo [31] 38.1 VideoChat2 [21] 43.1 VideoLLaVA [23] 45.3 PLLaVA [45] 45.0 LLaVA-1.5 [25] ShareGPT4Video [4] 46.9 Video-Ma2mba-0.7B 0.7B 43.3 Video-Ma2mba-1.8B 1.8B 48.4 Video-Ma2mba-3.1B 3.1B 55.4 - - - 7B 8B 7B 7B 7B 0 0 6 - 0 8 1 66.7 65.0 61.7 33.5 36.4 38.5 40.1 40.0 33.3 39.6 42.4 0 0 6 3 - 0 0 9 61.6 59.1 54.5 33.6 34.4 35.2 37.0 38.7 28.5 34.1 38.5 0 6 - 5 1 76.8 75.3 71.1 40.5 44.6 47.3 47.4 50.1 45.4 49.5 55.6 t 66.7 64.4 60.7 35.1 37.6 39.2 40.4 41.8 34.2 39.8 44. s 66.7 64.0 59.1 36.0 39.1 40.2 40.3 39.7 34.0 38.0 43.0 (b) Experimental results on LongVideoBench Table 1. Performance comparison across video length categories in Video-MME and LongVideoBench benchmarks. embedding dimension of 2560, 64 layers, and vocabulary size of 50, 277. The proposed MA-GC is applied throughout all training stages to maximize memory efficiency, enabling our model to handle maximum sequence length of 0.8M (approx. 1.5-hrs) during training, and generate responses with input sequences up to 2M (approx. 4-hrs) with an 80GB VRAM size GPU. Memory-Efficient Setup in MA-GC. For the MA-GC setup, we aim to minimize memory usage by selecting optimal intervals and for layer-wise and sequence-wise checkpoints, respectively, within the bounds 1 and 1 S. These parameters are chosen to reduce the total memory requirement, as expressed in Eq. (4), covering layer and sequence checkpoints and grid checkpoint cells. Algorithm 1 manages the forward pass by storing checkpoints Lckpt and Sckpt at defined intervals, while Algorithm 2 restores these activations during backpropagation, further optimization. For efficiency in SSD [11], we restrict to multiples of 256 when 256, which helps maintain high processing performance in the SSDs state scan logic. This approach, illustrated in Fig. 2, enables memoryefficient handling of long sequence lengths, significantly Model Size GPT4V [32] GPT-4o [33] Gemini 1.5 Pro [38] VideoLLaMA [47] Video-ChatGPT [29] MovieChat [39] Chat-UniVi [18] LLaMA-VID [22] VideoChat2-Mistral [21] ShareGPT4Video [4] VideoLLaMA2 [7] Video-Ma2mba-0.7B Video-Ma2mba-1.8B Video-Ma2mba-3.1B Acc. 57.0 - 61.9 - 57.5 - 12.4 7B 35.2 7B 45.7 7B 46.1 7B 47.4 7B 49.1 7B 50.8 8B 53.0 7B 0.7B 43.8 1.8B 50.0 3.1B 51.7 Table 2. Benchmark results for ActivityNetQA, VideoChatGPT, and MVBench, comparing Video-Ma2mba and baselines. Score Acc. 4.06 - - 2.16 2.42 2.67 2.99 2.89 2.98 - 3.13 2.69 2.76 3.03 Acc. 43.5 - - 34.1 32.7 - - 41.3 62.3 51.2 54.6 41.1 44.4 48. - - - 1.1 2.7 - 3.2 3.3 3.3 - 3.3 3.2 3.1 3.4 extending the models feasible input size during training. Evaluation Metrics & Setup. We primarily assess our model using two categorized video analysis benchmarks: long video understanding and general video understandFor long video benchmarks, we employ Videoing. MME [13] and LongVideoBench [43], both of which include test instances with video durations of up to 2hours. For shorter, yet more generalized benchmarks, we utilize four different video analysis benchmarks: ActivityNetQA [46], VideoChatGPT [29], and MVBench [21]. We used gpt-3.5-turbo-0125 to evaluate responses, implementing termination criterion to handle repetitive sequences, where generation halts if five consecutive tokens previously appeared. Given our primary model, VideoMa2mba-3.1B, our main comparisons are with baselines that has 8B parameters. 4.2. Experimental Results Results on Long Video Analysis. We report the long video comprehension results using Video-MME [13] and LongVideoBench [43] in Tab. 1. Despite its smaller scale, Video-Ma2mba outperforms most 7B models in both benchmarks. This is due to our method of accessing the entire video at 1-second intervals, contrasting with the sparse frame sampling strategy. Our comprehensive approach facilitates frequent observations, highlighting our frameworks effectiveness against larger 7B models. By selectively preserving key information, Video-Ma2mba avoids the typical information loss and computational burdens, delivering precise, context-aware responses for long videos. Results on General Video Analysis. In Tab. 2, we summarize general video analysis in several benchmarks: ActivityNetQA [46], VideoChatGPT [29], and MVBench [21]. These benchmarks are much shorter than the previously used long video benchmarks, but provide comprehensive foundation for assessing our models capability to anMethod Model Sequence Length (S = 2n) GC off : O(L S) GC on : O(L S) 13 6.6 13.1 21.4 2.7 5.5 8.7 1.6 3.1 4.4 1.1 .2.1 2.7 Table 3. Memory overhead (GB) for GC methods in Mamba-2-2.7B across sequence lengths (S = 2n). GC off indicates no checkpointL; and MA-GC optimizes based on sequence length. Each ing; GC on applies checkpointing per layer; Sqrt GC groups layers by cell show peak memory during activation and backpropagation (BF16 precision), excluding model weights and gradients. 350M, L=48, d=1024 1.3B, L=48, d=2048 2.7B, L=64, d=2560 350M, L=48, d=1024 1.3B, L=48, d=2048 2.7B, L=64, d=2560 350M, L=48, d=1024 1.3B, L=48, d=2048 2.7B, L=64, d=2560 350M, L=48, d=1024 1.3B, L=48, d=2048 2.7B, L=64, d=2560 17 - - - 43.7 - - 24.6 48.5 69.3 5.5 11.3 17.2 16 52.9 - - 21.9 43.5 69.3 12.3 24.3 34.6 3.8 7.4 10.5 14 13.3 26.0 42.6 5.5 10.9 17.4 3.1 6.1 8.7 1.6 3.7 4.2 15 26.5 52.1 - 10.9 21.8 34.7 6.2 12.1 17.3 2.4 4.8 6.9 18 - - - - - - 49.3 - - 8.8 17.7 25. 19 - - - - - - - - - 15.4 30.8 42.2 20 - - - - - - - - - 23.1 45.8 - 12 3.3 6.5 10.7 1.3 2.7 4.4 0.8 1.5 2.2 0.6 1.2 1.8 21 - - - - - - - - - 40.2 - - 10 0.9 1.7 2.7 0.4 0.7 1.1 0.2 0.4 0.6 0.3 0.5 0.7 11 1.7 3.3 5.4 0.7 1.4 2.2 0.4 0.8 1.1 0.5 0.9 1. Sqrt GC : O( MA-GC : O(S) S) Tr Stage Frame Limit Video-MME 1/ 1.5 /2 train (cid:33) (cid:33) 16 frm (cid:33) (cid:33) 1 fps (cid:33) (cid:33) (cid:33) 1 fps infer 8 frm 16 frm 8 frm 16 frm 32 frm 1 fps 8 frm 16 frm 32 frm 1 fps Short: 2m Mid: 4-15m Long: 30-60m Overall 49.0 50.0 47.7 50.6 52.7 54.4 53.3 55.9 57.9 57.6 38.7 40.7 37.9 39.4 40.8 41.4 39.3 41.3 41.9 42.7 33.8 34.6 32.2 33.2 33.9 34.4 32.2 33.9 33.9 35. 40.5 41.7 39.3 41.1 42.4 43.4 41.6 43.7 44.6 45.2 Table 4. Ablation study on frame size and Stage 1.5 effects in Video-MME using Video-Ma2mba-3.1B. alyze videos and answer related questions. As in the table, we also demonstrate competitive performance of VideoMa2mba across the benchmarks, holding its own against larger models with parameters over 7B. 4.3. Additional Analyses on Video-Ma2mba Multi-Axis Gradient Checkpointing For each model configuration, we measured peak memory usage at different sequence lengths to evaluate the memory efficiency of the MA-GC setup. Memory measurements were taken in two steps: (i) capturing baseline memory immediately after loading the model, and (ii) recording peak memory during backpropagation. The difference provides the sequencedependent memory overhead, reflecting the impact of MAGC on memory savings. Results, shown in Tab. 3, demonstrate reduction in space complexity from O(L S) to O(S), highlighting the efficiency gains of MA-GC over standard checkpointing methods. Effect of Frame Restriction and Stage 1.5. We conduct ablation studies for the effectiveness of frame threshold and long video knowledge learning (stage 1.5). As summarized in Tab. 4, when the model was trained with 16-frame limit, it showed lower performance across all video lengths compared to the our framework, achieving +1.7 points (4.1%) improvement. This indicates that limiting frames can cause the model to miss essential visual cues, impacting comprehension. Including stage 1.5 for long video understanding led to consistent gains across all video length, with performance increasing by +1.8 points (4.1%). Notably, while models trained with 16-frame and 1 FPS limits showed minimal difference without stage 1.5, adding of this stage improved performance with gain of +1.0 points (2.9%) due to enhanced context tracking over extended sequences. 5. Discussion and Conclusion Discussion. Even if we have achieved promising results in our experiments, there are several discussion points and limitations. The recently launched Mamba-2, unlike established Transformer-based LLMs, is still immature with smaller model size and insufficient QA capabilities from limited language instruction training, potentially capping performance. Future enhancements should expand the architecture and diversify training to fully exploit Mamba-2s potential, possibly outperforming more mature models. Additionally, we propose method of feeding entire lengthy frames, up to 0.8M sequence length, all at once to LM backbones, although this is not the only approach to optimizing performance. In particular, for lengthy videos, retrieving targeted salient information through selective frame sampling may prove to be more effective modeling strategy and remains an intriguing direction for future research. Conclusion. In this work, we propose Video-Ma2mba, novel Large Multi-modal Model designed for efficient long video understanding, which integrates the Mamba structure into the vision modality. In addition, to push the boundary of memory efficiency, we introduce Multi-axis Gradient Checkpointing strategy and achieve significant memory savings, enabling the processing of extended video sequences up to 0.8M context length. Our extensive validation across multiple benchmarks confirms that Video-Ma2mba not only matches but in some cases exceeds the performance of larger models, highlighting the effectiveness and potential of our approach in pushing the boundaries of video sequence modeling."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 1(2):3, 2023. 7 [2] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17281738, 2021. 6 [3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 1, 2 [4] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. Sharegpt4video: Improving video understandarXiv preprint ing and generation with better captions. arXiv:2406.04325, 2024. 7 [5] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. ArXiv, abs/2306.15595, 2023. 2 [6] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. ArXiv, abs/1604.06174, 2016. 2, 3, 4 [7] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 2 [9] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose visionIn Advances in language models with instruction tuning. Neural Information Processing Systems, 2023. 1 [10] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond fixed-length context. In Annual Meeting of the Association for Computational Linguistics, 2019. 2 [11] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. ArXiv, abs/2405.21060, 2024. 2, 3, 7 [12] Jeffrey L. Elman. Finding structure in time. Cogn. Sci., 14: 179211, 1990. [13] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 7, 3 [14] Google. Gemini, 2023. 1 [15] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. ArXiv, abs/2312.00752, 2023. 2, 3 [16] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model In Proceedings of the for long-term video understanding. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1350413514, 2024. 2, 3 [17] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 6 [18] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video unIn Proceedings of the IEEE/CVF Conference derstanding. on Computer Vision and Pattern Recognition, pages 13700 13710, 2024. 3, [19] Junho Kim, Hyunjun Kim, Hosu Lee, and Yong Man Ro. Salova: Segment-augmented long video assistant for targeted arXiv retrieval and routing in long-form video analysis. preprint arXiv:2411.16173, 2024. 6 [20] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2 [21] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 1, 7 [22] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2025. 3, 7 [23] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. 1, 2, 7 [24] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. [25] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023. 1, 5, 7 [26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, 2023. 1, 5, 6 [27] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 6 9 [43] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interarXiv preprint leaved video-language understanding. arXiv:2407.15754, 2024. 7, 4 [44] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining In Proceedings of the IEEE/CVF contemporal actions. ference on computer vision and pattern recognition, pages 97779786, 2021. 6, [45] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. 7 [46] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 91279134, 2019. 6, 7, 1 [47] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 7 [48] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 2 [49] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 6, 1 [50] Yi-Fan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. arXiv preprint arXiv:2406.08487, 2024. 3, 7 [28] Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, and Ge Li. St-llm: Large language models are effective temporal learners. In European Conference on Computer Vision, pages 118. Springer, 2025. 7 [29] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 1, 3, [30] OpenAI. ChatGPT. https://openai.com/blog/ chatgpt/, 2023. 1 [31] OpenAI. Gpt-4 technical report, 2023. 1, 7 [32] OpenAI. GPT-4V(ision) System Card, 2023. 7 [33] OpenAI. Hello gpt-4o, 2024. 7 [34] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36, 2024. 6, 1 [35] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. ArXiv, abs/2108.12409, 2021. 2 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [37] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training In SC20: International Confertrillion parameter models. ence for High Performance Computing, Networking, Storage and Analysis, pages 116. IEEE, 2020. [38] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 7 [39] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 2, 3, 7 [40] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. ArXiv, abs/2104.09864, 2021. 2 [41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1, 2 [42] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. 1, 2 10 Look Every Frame All at Once: Video-Ma2mba for Efficient Long-form Video Understanding with Multi-Axis Gradient Checkpointing"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Interaction Schema Video-Ma2mba processes entire video information sequentially at 1 FPS, formulating answers to user queries while retaining relevant details for subsequent questions. Unlike attention-based models, this sequence-modeling approach relies on its ability to remember and retrieve crucial details for future unseen queries. The interaction style is structured in below Tab. 5. Video QA Instruction: <system>You are helpful assistant.<endoftext> <user> <frame1 1><frame1 2> <frame1 144> <frame2 1><frame2 2> <frame2 144> <frameN 1><frameN 2> <frameN 144> (question 1)<endoftext> <assistant>(response 1)<endoftext> <user>(question 2)<endoftext> <assistant>(response 2)<endoftext> Table 5. Illustration of the interaction schema of Video-Ma2mba during Stage 2 of SFT. Frames are processed in sequence to generate responses to user instructions, ensuring continuity across queries. B. Training Details Data Implementation for Efficient Training. To improve the efficiency of training on large-scale video QA datasets due to academic budgets, we deploy data compression strategy that groups related QA pairs for the same video into single sample (i.e., single series of QA sets). This drastically reduces the number of training samples from 1.3M to 184K (e.g., LLaVA-Video-178K [49], NeXT-QA [44], ActivityNetQA [46], PerceptionTest [34]) while preserving the diversity and contextual information of the original dataset. By significantly minimizing decoding overhead, this strategy enables more efficient model training without compromising data diversity. Training Hyperparameters. All Video-Ma2mba variations are trained under consistent configurations, with slight differences in per-device batch size due to hardware constraints. To match the global batch size across model variations, we utilize gradient accumulation, ensuring similar training schedule for all variations. The hyperparameters for each training stage are detailed in Tab. 6. Batch Size and Gradient Accumulation. Each model variation leverages the maximum available GPU memory to determine its per-device batch size. Gradient accumulation is applied to align the global batch size across variations, enabling consistent optimization behavior despite hardware differences. Training Precision and Gradient Checkpointing. We use BFloat16 precision for all stages and adopt Multi-Axis Gradient Checkpointing. This approach significantly reduces memory consumption, enabling training with longer sequences and occasionally accommodating larger batch sizes. config input modality FPS for video input resolution trainable params LLM lr Vision lr lr scheduler optimizer global batch size train epochs warmup ratio weight decay gradient clipping training precision DeepSpeed stage GC Stage1 Vid + Img Projector 1e-3 - Full Model 4e-5 4e-6 Stage2 Video Stage1.5 Video 1 FPS 336x336 Full Model 4e-5 4e-6 Cosine Decay AdamW (β1 = 0.9, β2 = 0.95) 32 2 0.1 0.05 1.0 BFloat16 ZeRO-1 Multi-Axis Gradient Checkpointing 512 32 2 Table 6. Hyperparameters for Training Stages. C. Memory Estimation Logic The memory estimation logic in Video-Ma2mba optimizes memory requirements by determining the ideal checkpointing intervals and used during forward and backward passes. The constants CS-ckpt, CL-ckpt, Cgrid, and Cstate depend on the backbone models configuration, including SSM implementation, block design, and precision type. Accordingly, the total expected memory can be computed as follows: = ML-ckpt + MS-ckpt + Mgrid + Mstate, (12) where the each memory component in the above equation 1 are defined as: ML-ckpt = MS-ckpt ="
        },
        {
            "title": "LS\nl\nLS\ns",
            "content": "CL-ckpt, CS-ckpt, Mgrid = ls Cgrid, Mstate = Cstate. (13) (14) (15) (16) C.1. Model-Specific Constants Tab. 7 outlines the constants for three backbone configurations of Mamba-2 in the BFloat16 precision setting. Note that SSM states use Float32 precision, affecting Cstate and CS-ckpt depending on whether halfor single-precision is used. CL-ckpt Model Mamba-2-370m 1,024 2,048 Mamba-2-1.3b 2,560 Mamba-2-2.7b CS-ckpt 269,056 537,344 671,488 Cgrid 6,432 12,608 15, Cstate 264,448 528,640 660,736 Table 7. Model-specific constants for memory estimation under BFloat16 precision. Constants reflect relative element counts, where SSM states in Float32 are equivalent to two BFloat16 elements. Recomputation for Memory Optimization. The term Mstate, omitted in Eq. (4), arises in Mamba-2 [11] due to recomputation of SSM states during the backward pass. This recomputation reduces memory usage by avoiding the storage of intermediate states during forward computation: The intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. [15] grows linearly with Although Mstate (i.e., the memory term Mgrid grows as the Mstate=O(s)), product of (i.e., Mgrid=O(ls)), making it asymptotically dominant. Consequently, Mstate becomes negligible term in the overall memory complexity, and the analysis in Sec. 3.3 remains valid. Checkpointing Trade-Offs. Selecting the optimal values for and is critical to minimizing . For example, larger values of reduce MS-ckpt but increase restoration overhead during the backward pass (Mgrid and Mstate). As shown in Tab. 7, careful balance for memory savings is required to process long video sequences efficiently. D. Gradient Checkpointing Time Analysis We analyze the computational efficiency of various gradient checkpointing methods, with results summarized in 2 Throughput Processing Time Method GC off @ 214 GC on @ 216 Sqrt GC @ 217 MA-GC @ 219 (tokens/s) 12,167.86 8,449.93 8,617.66 7,913.58 (ms/token) 0.082 0.118 0.116 0.126 Table 8. Computational analysis of throughput and per-token processing time among gradient checkpointing methods. Results are measured using the Mamba-2-2.7b model on an A100 80GB GPU. The notation @ 2n specifies the sequence length (in tokens) used for measurement. Tab. 8. Throughput (tokens per second) indicates the processing speed, while per-token processing time (milliseconds per token) provides more detailed perspective on computational overhead. The reported measurements represent the median of six runs conducted on an A100 80GB GPU using the Mamba-2-2.7b, which is the backbone of Video-Ma2mba-3.1B. warm-start configuration was used to minimize initialization overhead, and the times include both the forward and backward computation steps. MA-GC demonstrates the ability to train on sequence lengths up to 32 longer than the GC-off baseline within the same memory constraints. Although this extended capability comes with 35% reduction in throughput, this is trade-off that enables scalable and efficient training for tasks requiring extremely long sequences. The gradient checkpointing methods can save memory and handle longer sequences but are fundamentally constrained when processing extremely long sequences due to memory limitations. MA-GC overcomes such issues by introducing multi-axis gradient checkpointing mechanism, enabling training on sequence lengths up to 32 longer while retaining computational feasibility. By efficiently managing memory without incurring prohibitive overhead, MA-GC balances performance and resource efficiency, significantly extending the scalability of large-scale models. E. Qualitative Evaluation As in Fig. 4, Fig. 5, and Fig. 6, our analysis provides qualitative outcomes across various benchmarks, clearly demonstrating the adaptability and effectiveness of VideoMa2mba. Consistent performance across different benchmarks highlights the models ability to efficiently handle and analyze disparate datasets. This effectiveness stems from Video-Ma2mbas ability to process extensive context using the MA-GC mechanism, which allows it to handle all incoming inputs comprehensively, facilitating robust data processing and insightful reasoning. Figure 4. Qualitative examples on Video-MME [13] with Video-Ma2mba-3.1B. Figure 5. Qualitative examples from the Generative Subset of VideoChatGPT [29] with Video-Ma2mba-3.1B. 3 Figure 6. Qualitative examples on LongVideoBench [43] with Video-Ma2mba-3.1B."
        }
    ],
    "affiliations": [
        "Integrated Vision and Language Lab, KAIST, South Korea"
    ]
}