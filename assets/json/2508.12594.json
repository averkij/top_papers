{
    "paper_title": "FLARE: Fast Low-rank Attention Routing Engine",
    "authors": [
        "Vedant Puri",
        "Aditya Joglekar",
        "Kevin Ferguson",
        "Yu-hsuan Chen",
        "Yongjie Jessica Zhang",
        "Levent Burak Kara"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The quadratic complexity of self-attention limits its applicability and scalability on large unstructured meshes. We introduce Fast Low-rank Attention Routing Engine (FLARE), a linear complexity self-attention mechanism that routes attention through fixed-length latent sequences. Each attention head performs global communication among $N$ tokens by projecting the input sequence onto a fixed length latent sequence of $M \\ll N$ tokens using learnable query tokens. By routing attention through a bottleneck sequence, FLARE learns a low-rank form of attention that can be applied at $O(NM)$ cost. FLARE not only scales to unprecedented problem sizes, but also delivers superior accuracy compared to state-of-the-art neural PDE surrogates across diverse benchmarks. We also release a new additive manufacturing dataset to spur further research. Our code is available at https://github.com/vpuri3/FLARE.py."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 4 9 5 2 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "FLARE: FAST LOW-RANK ATTENTION ROUTING ENGINE Vedant Puri, Aditya Joglekar, Kevin Ferguson, Yu-hsuan Chen, Yongjie Jessica Zhang, Levent Burak Kara Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA {vedantpu,aajoglek,kferguso,yuhsuan2,jessicaz,lkara}@andrew.cmu.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "The quadratic complexity of self-attention limits its applicability and scalability on large unstructured meshes. We introduce Fast Low-rank Attention Routing Engine (FLARE), linear complexity self-attention mechanism that routes attention through fixed-length latent sequences. Each attention head performs global communication among tokens by projecting the input sequence onto fixed length latent sequence of tokens using learnable query tokens. By routing attention through bottleneck sequence, FLARE learns low-rank form of (N ) cost. FLARE not only scales to unpreceattention that can be applied at dented problem sizes, but also delivers superior accuracy compared to state-ofthe-art neural PDE surrogates across diverse benchmarks. We also release new additive manufacturing dataset to spur further research. Our code is available at https://github.com/vpuri3/FLARE.py."
        },
        {
            "title": "INTRODUCTION",
            "content": "High-fidelity simulations of physical systems are often too costly for multi-query applications, such as design optimization or uncertainty quantification. Machine learning offers promising alternative via surrogate models that learn system dynamics from data, enabling fast approximations that accelerate experimentation and decision making. Among machine learning (ML) architectures, transformers (Vaswani et al., 2017) have shown exceptional scalability and generalization capabilities in domains such as natural language processing (Devlin et al., 2019) and computer vision (Dosovitskiy et al., 2020). This has led to increased interest in adapting transformers for spatially distributed data such as point clouds and meshes in physical simulations. However, applying transformers directly to large-scale unstructured meshes introduces severe computational bottlenecks. The core challenge lies in the self-attention mechanism in which, for sequence of tokens, each token attends to every other token in the sequence, resulting in (N 2) time and memory complexity. Although this global communication is key to the expresO sive power of transformers, enabling them to outperform graph neural networks (GNNs) that are inherently local (Yun et al., 2019), its computational footprint hinders its scalability. In the context of learning partial differential equation (PDE) surrogates, i.e., regressing continuous fields over point clouds/meshes, each point in the 3D point cloud is treated as token, with associated features that encode geometric and physical quantities such as coordinates, normals, and material properties. Unlike sequential tokens in language models, these tokens are spatially distributed with no inherent ordering to them. Several works have proposed approximations to the full self-attention mechanism to mitigate the cost. Perceiver and PerceiverIO (Jaegle et al., 2021b;a) introduced cross-attention between the input sequence and length learnable latent sequence. In this mechanism, the attention weights are used to project the input sequence to the latent sequence, followed by self-attention blocks in the latent space. Perceiver thus decouples the length of the input sequence from the computational complexity by avoiding dense self-attention on the inputs. However, the latent bottleneck in PerceiverIO Equal contribution Corresponding author"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Table 1: Comparison of FLARE with different latent attention schemes in PerceiverIO (Jaegle et al., 2021a), LNO (Wang & Wang, 2024a), and Transolver (Wu et al., 2024)."
        },
        {
            "title": "PerceiverIO Transolver LNO",
            "content": "Sequential encoding and decoding blocks Parallel encodings & decodings Weight sharing b/w encoding and decoding Latent space self-attention Compatible with fused attention kernels * FLARE (ours) Note: *Physics Attention in Transolver uses same projection weights for all heads whereas PerceiverIO and FLARE use cross-attention projection where each head learns distinct slice of the latent query tokens. can limit accuracy as the model may discard fine-grained features if the number of latent tokens is too low. In learning PDE surrogates, Transolver (Wu et al., 2024) and Latent Neural Operator (LNO) (Wang & Wang, 2024a) adopt similar projection-unprojection scheme to map variable-length point clouds to fixed-length latent representations. Both models obtain projection weights by expanding the feature dimension, and apply self-attention on the resulting latent sequence. The differences between the three approaches are highlighted in Table 1. Notably, Perceiver and LNO only perform single encoding and decoding step, whereas Transolver reapplies projection and unprojection in every transformer block. This enables Transolver to form deep models, where attention connections in downstream layers benefit from the rich embeddings and global context aggregated by previous layers. However, the attention projector layers proposed in Transolver and LNO cannot utilize existing GPU kernels for scaled dot-product attention (SDPA), well-optimized algorithm in modern deep learning (Dao et al., 2022). This limits their scalability to large meshes. We propose Fast Low-rank Attention Routing Engine (FLARE), simple yet powerful mechanism designed to break the scalability barrier in PDE surrogate learning. FLARE is built on the argument that projecting input sequences onto shorter latent sequences, and then unprojecting to the original sequence length, is equivalent to constructing low-rank form of attention with rank at most equal to the number of latent tokens (see Figure 1). Furthermore, we argue that multiple simultaneous low-rank projections could collectively capture full attention pattern. Unlike Transolver which shares projection weights across heads, or LNO which applies only single projection, our design allocates distinct slice of the latent tokens to each head resulting in distinct projection matrices for each head. This allows each head to learn independent attention relationships, opening up key direction of scaling and exploration, wherein each head may specialize in distinct routing patterns. To support this intuition, we draw on known properties of rank-deficient matrices, to develop linear-time eigenanalysis procedure to study the learned communication matrices. Our spectral analysis reveals that attention heads operate in low-rank subspaces, validating our core design choice. Furthermore, we observe diversity in the eigen spectrum across heads, reinforcing the motivation for independent head-wise projections. These findings highlight the efficiency and expressiveness of our low-rank formulation. We summarize our main contributions below. Linear complexity token mixing. FLARE is an efficient self-attention mechanism designed to learn on long sequences such as point clouds. By replacing full self-attention with low-rank projections and reconstructions, FLARE achieves linear complexity in the number of points. Superior accuracy. Across multiple PDE benchmarks, FLARE achieves superior predictive accuracy compared to leading neural surrogate models, despite operating with fewer parameters, and at much lower computational complexity. Scalability. FLARE is built entirely from standard fused attention primitives, ensuring high GPU utilization and ease of integration into existing transformer architectures. As such, FLARE enables end-to-end training on unstructured meshes with one million points (see Figure 2) without distributed computing (Luo et al., 2025) or memory offloading the largest scale demonstrated for transformer-based PDE surrogates."
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Figure 1: Schematic of FLARE block. In FLARE, each head projects the input sequence with tokens to fixed-length sequence of tokens via the cross-attention matrix Wencode = ), and then projects back to the original length via the cross-attention matrix softmax(Q QT ). The overall operation is equivalent to token mixing on the input Wdecode = softmax(K sequence with the rank-deficient matrix (Wdecode Wencode). Benchmark dataset of additive manufacturing simulations. We release large-scale, highresolution dataset for residual displacement prediction in metal additive manufacturing to encourage further research in scalable PDE surrogate modeling."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Neural PDE surrogates. Learning neural surrogates for PDEs is an exciting and rapidly growing direction in scientific machine learning. Neural operators (Li et al., 2020; Lu et al., 2021; Kovachki et al., 2023) have been proposed for learning mappings between infinite-dimensional input and output function spaces, enabling mesh-independent generalization. Extensions to neural operators that incorporate geometric priors and graph-based representations further improve performance, especially for problems defined on complex unstructured meshes (Li et al., 2023b;c). Graph networks have also been widely explored as PDE surrogates (Pfaff et al., 2020; Elrefaie et al., 2024; Ferguson et al., 2025), leveraging their inherent ability to model local neighborhood interactions on meshes. Transformer-based architectures have emerged more recently as powerful PDE surrogates (Alkin et al., 2024; Cao, 2021; Li et al., 2022; 2023a; Hao et al., 2023), allowing global context aggregation and dynamic prediction of complex phenomena (Qian et al., 2025). Recent works (Alkin et al., 2024; Wu et al., 2024; Wang & Wang, 2024a) leverage latent space attentions for PDE modeling, achieving high accuracy with reduced computational cost. Building upon these advances, our FLARE method employs linear complexity attention mechanism with learnable latent bottleneck to further improve accuracy and efficiency on PDE surrogate modeling tasks. Efficient attention mechanisms. Several methods have been proposed to address the quadratic complexity of the attention mechanism. In (Wang et al., 2020), attention matrices are shown to be low-rank. Their proposed model, called Linformer, employs learned linear mappings to project the input key and value sequences to shorter sequence length, resulting in linear complexity form of attention. drawback of this approach is that it cannot directly handle variable input sequence lengths. Another efficient attention method, called multi-head latent attention (Liu et al., 2024), jointly compresses key and value tokens to reduce the memory footprint of language models"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Figure 2: Time and memory requirements of different attention schemes. On an input sequence of one million tokens, FLARE (red) is over 200 faster than vanilla attention, while consuming marginally more memory. All models are implemented with flash attention (Dao et al., 2022), and the memory upper bound on single H100 80GB GPU is depicted with dashed line. Note that the curves for FLARE are somewhat overlapping. during inference. However, this does not help with the quadratic bottleneck in self-attention. LowRank Adaptation (LoRA) (Hu et al., 2022) also leverages low-rank projections, though primarily for efficient fine-tuning rather than architectural design or modeling capacity, as we explore in this work. While these efficient attention methods are general frameworks, none specifically targets PDE surrogate modeling. Our proposed FLARE method leverages low-rankness, is sequence-length independent, and achieves competitive performance on PDE benchmarks while delivering substantial speedup compared to other existing methods."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 PRELIMINARY: MULTI-HEAD SELF-ATTENTION Let value matrices Q, K, RN denote the input sequence of tokens with features each. The query, key, and RN are obtained by applying learned linear projections to X, we have = = (1) RCC. The Q, K, matrices are then split along the feature dimension where q, k, and passed to heads, each with dimension = C/H, enabling parallel computation of attention: q, = k, v, [Q1, . . . , QH ] = Q, [K1, . . . , KH ] = K, [V1, . . . , VH ] = V. (2) The scaled dot-product attention (SDPA) operation, introduced by (Vaswani et al., 2017), computes the output as Yh = SDPA(Qh, Kh, Vh, s) = softmax (cid:18) Qh (cid:19) Vh, (3) RN are query, key, and value matrices belonging to head h, and is where Qh, Kh, Vh typically D. Note that softmax is taken along the row-dimension. The outputs Yh from all heads are then concatenated along the feature dimension to form the final output This concatenation, followed by linear layer, enables the model to integrate information across attention heads efficiently. = [Y1, . . . , YH ] . (4)"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "1 import torch.nn.functional as 2 def flare_multihead_mixer(q, k, v): 3 # Args - q: [H, M, D], k, v: [B, H, N, D] # Ret - y: [B, H, N, D] = F.scaled_dot_product_attention(q, k, v, scale=1.0) = F.scaled_dot_product_attention(k, q, z, scale=1.0) return 4 5 6 Figure 3: PyTorch code for multi-head token mixing operation in FLARE. See Figure 7 for an implementation without the fused attention kernel. (N 2) in complexity as The greatest cost in multi-head self-attention is the call to SDPA which is both the query and key sequences have tokens. Fortunately, SDPA is fundamental algorithm in modern deep learning, and its GPU optimized multi-head implementations are available in PyTorch (Paszke, 2019). O"
        },
        {
            "title": "3.2 FLARE: FAST LOW-RANK ATTENTION ROUTING ENGINE",
            "content": "FLARE is linear-complexity token mixing layer that learns low-rank global communication structures via attention projections. The FLARE mechanism introduces set of learnable latent tokens that serve as bottleneck for information exchange. The process consists of two stages: 1. Encoding. The input sequence is projected onto the latent tokens via cross-attention, compressing global information. 2. Decoding. The latent tokens are then projected back onto the input sequence, distributing the aggregated information. RM C, where each row corresponds to latent Formally, we define learnable query matrix RN C, are obtained by applying deep residual multitoken. The key and value matrices, K, layer perceptrons (MLPs) detailed in Appendix to the input X. Compared to just linear layer, these allow the model to learn higher-order feature interactions and deeper nonlinear transformations. Refer to Section 4.3 for ablation studies. The matrices Q, K, and are first split along the feature dimension into heads, each of dimension = C/H. Then, for encoding, each head performs SDPA with scaling factor = 1: RM D, Kh, Vh Zh = SDPA(Qh, Kh, Vh, = 1). (5) RN are query, key, and value matrices belonging to head and Here, Qh RM is the latent sequence for head h. For decoding and propagating information back to Zh the input tokens, we perform second SDPA operation, swapping the roles of queries and keys and using the latent sequence as values: Yh = SDPA(Kh, Qh, Zh, = 1) (6) RN is the output for each head. Similar to multi-head self attention, the outputs from where Yh all heads are concatenated along the feature dimension and passed through final linear projection to mix information across heads. As the query matrix has only tokens, the cost of SDPA calls (N ). PyTorch code for the multi-head implementation is presented in in Eq. 5 and Eq. 6 is Figure 3. Note that we use scaling factor = 1 instead of in typical transformers (Vaswani et al., 2017)) in SDPA. This modification is explained in Section 4.3. Low-rank communication. The two-step attention process can be written as where Note that softmax is taken along the row dimension. We define Yh = (Wdecode,h Wencode,h) Vh Wencode,h = softmax(Qh Wdecode,h = softmax(Kh ) QT ) RM , and RN . Wh = Wdecode,h Wencode,h RN 5 (7) (8) (9)"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "as the dense global communication matrix with rank at most . This low-rank structure, illustrated in Figure 1, enables efficient all-to-all communication without explicitly forming Wh; instead, Wencode,h and Wdecode,h are applied sequentially, resulting in an overall cost of (M ) per head. Finally, note that Wencode,h and Wdecode,h are adjoints of one another up to diagonal scaling. This results in near symmetric attention pattern. FLARE block. Figure 1 (left) illustrates single FLARE block. Given input tokens the output of an FLARE block is computed as = + FLARE (LayerNorm (X)) = + ResMLP (LayerNorm (X)) . RN C, (10) Here, ResMLP (He et al., 2016) denotes residual MLP block detailed in Appendix A, and LayerNorm denotes layer normalization (Ba et al., 2016) operation. To summarize, FLARE block consists of token mixing operation via FLARE, pointwise residual MLP, and layer normalization in pre-norm format (Xiong et al., 2020). Deep residual MLPs within the block enable complex, token-level feature transformations and improve accuracy. Overall design. The overall architecture is given by sequential FLARE blocks sandwiched between an input projection and an output projection, which are detailed in Appendix A. Such design enables the model to efficiently integrate local and global information across multiple layers, making it well suited for large-scale, high-dimensional data such as point clouds. 3.3 SPECTRAL ANALYSIS The matrix Wh in Eq. 9 represents the attention weights between tokens, where [Wh]ij quantifies how much token communicates to token within head h. Since Wh has rank at most , the model can capture at most independent global communication patterns per head. The eigenvalues of indicate the relative importance or energy of each latent dimension in forming the attention matrix (M 3 + 2N ) time . We devise an algorithm to obtain the eigen decomposition of in (N 3) for dense communication matrix. The algorithm is predicated on computing compared to RM is chosen such that is similar to the eigenspectra of an . The algorithm is detailed in Section B.1, and summarized in Algorithm 1. Figure 8 presents the nonzero eigenvalues of Wh for an FLARE model trained on the elasticity benchmark problem with 972 points per input. The distinct spectra of the heads indicates that each head learns distinct attention patterns. matrix JJ where O The eigenvalue analysis detailed in Section B.2 shows that while FLARE provides capacity for rankM attention, the model learns to use only small fraction of this in early blocks indicating effective compression. In deeper blocks, more of the latent capacity is utilized, with diverse spectral profiles across heads, validating our design choice of independent head-wise projections."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 PDE SURROGATE BENCHMARKS Benchmark problems. We consider diverse set of benchmark datasets (summarized in Appendix Table 3) for regressing PDE solutions on point clouds. The test suite includes 2D and 3D benchmark datasets on structured and unstructured meshes with up to 50,000 points used for training and evaluation. Note that FLARE is agnostic to the underlying mesh structure as the model only considers the input point cloud. The 2D benchmark problems of Elasticity, Darcy, Airfoil, and Pipe (Li et al., 2020; 2023b) collectively simulate wide range of physical phenomena. Among the 3D benchmarks, the DrivAerML (Ashton et al., 2024) benchmark problem contains automotive aerodynamic simulations. More details about the datasets are provided in Appendix C.2. Finally, we propose 3D field prediction benchmark derived from numerical simulations of the laser powder bed fusion (LPBF) additive manufacturing process. The LPBF benchmark task entails predicting the deformation field over diverse set of 3D printed parts with up to 50,000 grid points. More details on this proposed benchmark problem are presented in Appendix E."
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "103) and (bottom) parameter count for different models across Table 2: (Top) relative L2 error ( PDE benchmark problems. The best results (smallest error) are made bold, and the second best results are underlined. backslash ( ) indicates that the model cannot be applied to the benchmark, and tilde ( ) indicates that the model is prohibitively slow on the benchmark. Elasticity Darcy Airfoil DrivAerML-40k LPBF Model Pipe Vanilla Transformer (Vaswani et al., 2017) PerceiverIO (Jaegle et al., 2021a) GNOT (Hao et al., 2023) LNO (Wang & Wang, 2024a) Transolver w/o conv (Wu et al., 2024) Transolver with conv (Wu et al., 2024) FLARE (ours) 5.37 660k 23.4 1.93m 13.3 4.87m 9.25 1.83m 6.40 713k 3.38 592k 4.38 660k 21.5 1.93m 16.9 4.90m 7.64 762k 18.6 713k 5.94 2.8m 5.14 691k 6.28 660k 162 1.93m 103 4.90m 17.8 762k 8.24 713k 5.50 2.8m 4.28 691k 7.14 1.93m 5.89 4.90m 8.10 762k 4.87 713k 3.90 2.8m 2.85 625k 760 1.93m 115 4.87m 146 762k 70.5 713k 60.8 691k 56.3 1.93m 24.3 4.87m 24.7 762k 20.4 713k 18.5 625k Baselines. We compare FLARE with state-of-the-art PDE surrogate models. The comparison suite includes generic attention-based models such as vanilla transformer (Vaswani et al., 2017), PerceiverIO (Jaegle et al., 2021a), attention-based PDE surrogate models such as Transolver (Wu et al., 2024), LNO (Wang & Wang, 2024a)), and neural operator model GNOT (Hao et al., 2023). We do not include any graph-based model in our comparison suite, as the graph connectivity information is not available for most problems. Transolver++ (Luo et al., 2025) would have been valuable addition to the comparison suite, but its code was not publicly available at the time of writing. We follow the experimental setup of Transolver as it is the preeminent surrogate model and attempt to match its parameter count. Note that Transolver can be instantiated in two configurations: without convolution, where point-to-point communication relies solely on physics attention, and with convolution, where convolution layers are added to inject information from neighboring points when the input grid is structured. We evaluate these two configurations separately to isolate the impact of convolution versus physics attention. In our model, we choose not to employ any convolution layers and rely entirely on FLARE for token mixing. Finally, we note that the PDE problems in this section are relatively small (up 50,000 points), and opt to train all models in full-precision (FP32). As shown in Figure 9, the vanilla transformer becomes drastically slower in comparison to FLARE and Transolver for on large point clouds. As such, we only evaluate the vanilla transformer on problems with up to 10,000 points. Discussion. The results in Table 2 clearly demonstrate that the proposed FLARE architecture achieves the lowest relative L2 error across all but one benchmark PDE problems, outperforming both LNO and Transolver on every dataset. Notably, FLARE also achieves these gains with consistently lower parameter count than Transolver or LNO, highlighting its efficiency in addition to higher accuracy. These results underline the robustness and versatility of FLARE across diverse problem settings. We also note that the poor performance of Transolver without convolutions indicates that the inter-point communication via Transolvers built-in physics-attention mechanism is not enough. With convolutions, the input projections amass information from neighboring points, which in turn helps the physics attention learn the global structure. Although the vanilla transformer is extremely effective on small-scale PDE problems, it becomes extremely slow on large point clouds due to its quadratic cost as illustrated in Figure 2. On the contrary, PerceiverIO (with only single encoding and decoding step) performs poorly even with = 1, 024 latent tokens and = 8 latent self-attention blocks. This validates our hypothesis that multiple latent self-attention operations can be unnecessary and potentially suboptimal so long as"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Figure 4: We train FLARE on the DrivAerML dataset (Ashton et al., 2024) with one million points per geometry on single Nvidia H100 80GB GPU. We present (left) the test relative error, (middle) time per epoch (s), and (right) peak memory utilization (GB) as function of the number of FLARE blocks (B) for different number of latent tokens (M ). the projections are sufficiently expressive. This is because information loss during projection is not recoverable via latent self-attention alone. Instead, performing multiple (head-wise) parallel projections and reconstructions directly between the input and latent sequences preserves expressivity while simplifying the architecture. 4.2 FIELD-PREDICTION ON MILLION-POINT GEOMETRIES Although the benchmark problems in Section 4.1 represent wide variety of PDE problems, they are relatively small compared to industrial use cases that demand PDE solutions on complex geometries with millions of grid points (Ashton et al., 2024). So far, attention-based surrogate models have not been able to scale to million-scale regression problems due to quadratic time and memory complexity, as illustrated in Figure 2. The flash attention (Dao et al., 2022) algorithm has alleviated the memory bottleneck thanks to online softmax computation; however, these methods remain impractical due to their long training times. Furthermore, SOTA models such as Transolver and LNO cannot be implemented with off-the-shelf fused attention algorithms like flash attention because of the need to explicitly materialize the projection weights. We demonstrate in Figure 4 that FLARE can scale to million-scale geometries by training on the DrivAerML dataset (Ashton et al., 2024) where each mesh is subsampled to contain one million points. These calculations are performed in mixed precision on single Nvidia H100 80GB GPU provisioned through Google Cloud Platform. We note the clear trend in Figure 4 (left) that the error consistently decreases as we increase the number of FLARE blocks. To our knowledge, this is the first attention-based neural surrogate model trained on one million points on single GPU without memory offloading or distributed computing. 4.3 MODEL ANALYSIS AND ABLATIONS Time and memory complexity. Figure 2 illustrates the time and memory complexity of single forward and backward pass for different attention schemes on long sequences. The experiment is done in mixed precision (FP16 in forward pass, FP32 in backward pass) using PyTorchs autocast functionality with = 128 features and = 8 heads for all models. The flash attention backend (Dao et al., 2022) is employed for SDPA wherever possible. Although vanilla self-attention has the lowest memory cost thanks to the flash-attention algorithm, which eliminates the need to materialize the score matrices (Qh ), its compute time still scales poorly with the sequence length. In contrast, the compute time for FLARE exhibits excellent scaling with sequence length. Its memory requirement is marginally greater than vanilla attention due to the presence of deep residual networks for key/value projections, and due to the need to materialize Zh, T"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Figure 5: (Left) effect of the number of residual layers in key/ value projection, and (right) of residual layers in residual block on test accuracy. In both cases, deeper networks lead to greater accuracy. the latent sequence of tokens. As these costs are marginal compared to the SDPA operation, the curves for different values of FLARE are somewhat overlapping. Finally, the compute time for Physics Attention of Transolver (Wu et al., 2024) exhibits somewhat good scaling. However, its memory cost and compute time blow up for large slice counts due to the need for materializing the projection matrices. ResMLP depth: key/value projections. substantial distinction between vanilla self-attention and FLARE is the introduction of deep residual blocks for key and value projections in place of simple linear layers. In standard self-attention, queries (Qh) and keys (Kh) determine the global communication pattern through Wh = softmax(QhK /s), while values (Vh) carry the information to be communicated. All three are typically computed as shallow linear projections of the input X. In contrast, FLARE computes the attention pattern as Wh = softmax(KhQT ), ) where the query embeddings Qh are learned parameters independent of the input. This makes the attention pattern less dynamic, motivating architectural modifications to enhance flexibility. softmax(QhK To address this, we replace the linear key and value projections with deep residual MLPs. Using residual networks for key and value encodings allows each token to learn richer and more structured features rather than shallow embeddings, which is particularly crucial in FLARE since the queries are fixed and cannot adapt to the input. Figure 5 (left) shows the impact of varying the number of residual layers in key/value projections and within the residual block on test accuracy for the elasticity benchmark dataset. We suspect that deeper key/value encodings lead to more meaningful and focused attention, encoding structured inductive priors beneficial to downstream prediction. ResMLP depth: feedforward block. second difference between the standard attention block and FLARE block is that we replace the feed-forward block in vanilla self-attention with deep residual MLP. Preliminary experiments using standard feed-forward blocks led to training instabilities and poor convergence. In contrast, residual MLPs consistently enabled stable training and allowed us to increase model capacity. Figure 5 (right) indicates that increasing the number of residual layers leads to slight improvements in accuracy. Based on these results, we use three residual layers in both key/value projections and the residual block, as this provides good trade-off between model capacity and computational cost in all subsequent experiments. Number of heads. key hypothesis behind our design is employing multiple simultaneous (headwise parallel) low-rank projections to collectively approximate full attention map. Thus, for fixed total feature dimension (C), using smaller head dimension (D) should improve performance by allowing more such parallel projections. To test this, we evaluate FLARE on the elasticity dataset with varying head counts while keeping the total feature dimension = 64 and number of blocks = 8 fixed. As shown in Figure 6 (left), FLARE achieves the lowest test error when = 4 or = 8, in contrast to standard transformers which often use = 1632. Notably, because our head dimensions are smaller, we use scaling factor of 1 instead of the typical employed in"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Figure 6: (Left) Effect of head dimension (D) on test accuracy. FLARE is designed to work optimally for = 48. (Right) Effect of number of blocks (B) on test accuracy. Both experiments are conducted on the elasticity benchmark problem with = 64 feature dimension. transformers (Vaswani et al., 2017), as the dot product magnitudes are sufficiently small without additional scaling. Number of blocks (B) and latent tokens (M ). Figure 6 (right) presents the test relative error of FLARE on the elasticity benchmark dataset as function of the number of blocks (B) for different latent sequence length (M ). Figure 4 (left) presents the same for the DrivAerML dataset with one million points per geometry. In both cases, we note the favorable trend that relative error consistently decreases as we increase the number of blocks. Similarly, we observe that the relative error generally decreases with , though the trend is not strictly monotonic. Critically, we note in Figure 4 (right) that increasing does not come at the cost of greater memory requirements."
        },
        {
            "title": "5 CONCLUSION",
            "content": "FLARE is token mixing layer that bypasses the quadratic cost of self-attention by leveraging low-rankness. Mechanically, FLARE routes attention through fixed-size latent sequence via crossattention projection and unprojection. FLARE achieves SOTA accuracy on set of diverse PDE benchmarks, and comfortably scales to million-scale meshes and comfortably scales to PDE problems with million-scale geometries. As transformers are the backbone of modern deep learning, we postulate that an efficient attention mechanism has several applications. We also identify potential areas for improvement: FLAREs reliance on deep residual MLPs can introduce sequential bottlenecks and increase latency, suggesting that further speedups are possible by addressing this issue. Additional enhancements include (1) incrementally increasing the number of latent tokens during training; and (2) conditioning latent tokens on time for diffusion modeling. ACKNOWLEDGMENTS This work was supported by the Air Force Research Laboratory under contract FA8650-21-F-5803 and PA Manufacturing Fellows Initiative. Zhang was supported in part by the National Science Foundation under grants CMMI-1953323 and CBET-2332084. The authors appreciate the support of Camfer, Inc. (Camfer, 2025) and Professor Amir Barati Farimani in providing computing resources. Part of this research was conducted on the Bridges-2 Supercomputer at the Pittsburgh Supercomputing Center, and parts were conducted using ORCHARD, high-performance cloud computing cluster made available by Carnegie Mellon University. The authors thank Jay Pathak of Ansys, Inc. (Ansys, 2025) for insightful discussions and Andrew Porco for his assistance with data processing."
        },
        {
            "title": "REFERENCES",
            "content": "Benedikt Alkin, Andreas Furst, Simon Schmid, Lukas Gruber, Markus Holzleitner, and Johannes Brandstetter. Universal physics transformers. arXiv e-prints, pp. arXiv2402, 2024. Ansys. Ansys engineering simulation software. https://www.ansys.com/, 2025. Accessed: 2025-08-14. Neil Ashton, Charles Mockett, Marian Fuchs, Louis Fliessbach, Hendrik Hetmann, Thilo Knacke, Norbert Schonwald, Vangelis Skaperdas, Grigoris Fotiadis, Astrid Walle, et al. Drivaerml: Highfidelity computational fluid dynamics dataset for road-car external aerodynamics. arXiv preprint arXiv:2408.11969, 2024. Autodesk. NetFabb Simulation Utility and Local Simulation. Autodesk Inc., San Francisco, California, United States, 2025. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Camfer. camfer ai-powered mechanical engineering. https://www.camfer.dev/, 2025. Accessed: 2025-08-07. Shuhao Cao. Choose transformer: Fourier or galerkin. Advances in Neural Information Processing Systems, 34:2492424940, 2021. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Erik R. Denlinger, Jeff Irwin, and Pan Michaleris. Thermomechanical modeling of additive manufacturing large parts. Journal of Manufacturing Science and Engineering, 136(6):061007, 10 2014. ISSN 1087-1357. doi: 10.1115/1.4028669. URL https://doi.org/10.1115/1. 4028669. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/ N19-1423/. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Heigold, Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. Mohamed Elrefaie, Angela Dai, and Faez Ahmed. Drivaernet: parametric car dataset for datadriven aerodynamic design and graph-based drag prediction. In International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, volume 88360, pp. V03AT03A019. American Society of Mechanical Engineers, 2024. Kevin Ferguson, Yu-hsuan Chen, Yiming Chen, Andrew Gillman, James Hardin, and Levent Burak Kara. Topology-agnostic graph U-Nets for scalar field prediction on unstructured meshes. Journal of Mechanical Design, 147(4):041701, 2025. Zhongkai Hao, Zhengyi Wang, Hang Su, Chengyang Ying, Yinpeng Dong, Songming Liu, Ze Cheng, Jian Song, and Jun Zhu. GNOT: general neural operator transformer for operator learning. In International Conference on Machine Learning, pp. 1255612569. PMLR, 2023. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770778, 2016."
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint arXiv:1606.08415, 2016. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=nZeVKeeFYf9. Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver IO: general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021a. Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International Conference on Machine Learning, pp. 46514664. PMLR, 2021b. Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to PDEs. Journal of Machine Learning Research, 24(89):197, 2023. Joseph G. Lambourne, Karl D.D. Willis, Pradeep Kumar Jayaraman, Aditya Sanghi, Peter Meltzer, and Hooman Shayani. Brepnet: topological message passing system for solid models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1277312782, June 2021. Zijie Li, Kazem Meidani, and Amir Barati Farimani. Transformer for partial differential equations operator learning. arXiv preprint arXiv:2205.13671, 2022. Zijie Li, Dule Shu, and Amir Barati Farimani. Scalable transformer for PDE surrogate modeling. Advances in Neural Information Processing Systems, 36:2801028039, 2023a. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations. arXiv preprint arXiv:2010.08895, 2020. Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential equations, 2021. URL http://arxiv.org/abs/2010.08895. arXiv:2010.08895. Zongyi Li, Daniel Zhengyu Huang, Burigede Liu, and Anima Anandkumar. Fourier neural operator with learned deformations for PDEs on general geometries. Journal of Machine Learning Research, 24(388):126, 2023b. Zongyi Li, Nikola Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Otta, Mohammad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, et al. Geometryinformed neural operator for large-scale 3D PDEs. Advances in Neural Information Processing Systems, 36:3583635854, 2023c. Xuan Liang, Qian Chen, Lin Cheng, Devlin Hayduke, and Albert To. Modified inherent strain method for efficient prediction of residual deformation in direct metal laser sintered components. Computational Mechanics, 64(6):17191733, 2019. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. DeepSeek-V2: strong, economical, and efficient mixtureof-experts language model. arXiv preprint arXiv:2405.04434, 2024. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL http: //arxiv.org/abs/1711.05101. arXiv:1711.05101. Lu Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em Karniadakis. Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. Nature Machine Intelligence, 3(3):218229, 2021."
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Huakun Luo, Haixu Wu, Hang Zhou, Lanxiang Xing, Yichen Di, Jianmin Wang, and Mingsheng Long. Transolver++: An accurate neural solver for pdes on million-scale geometries. In Fortysecond International Conference on Machine Learning, 2025. Paszke. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint arXiv:1912.01703, 2019. Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter Battaglia. Learning mesh-based simulation with graph networks. In International Conference on Learning Representations, 2020. Kuanren Qian, Genesis Omana Suarez, Toshihiko Nambara, Takahisa Kanekiyo, and Yongjie Jessica Zhang. High-throughput machine learning framework for predicting neurite deterioration using MetaFormer attention. Computer Methods in Applied Mechanics and Engineering, 442:118003, 2025. Joni Reijonen, Alejandro Revuelta, Sini Metsa-Kortelainen, and Antti Salminen. Effect of hard and soft re-coater blade on porosity and processability of thin walls and overhangs in laser powder bed fusion additive manufacturing. The International Journal of Advanced Manufacturing Technology, 130(5):22832296, 2024. Leslie Smith and Nicholay Topin. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications, volume 11006, pp. 369386. SPIE, 2019. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Tian Wang and Chuang Wang. Latent neural operator for solving forward and inverse pde problems. arXiv preprint arXiv:2406.03923, 2024a. Tian Wang and Chuang Wang. Latent neural operator for solving forward and inverse PDE problems, June 2024b. URL http://arxiv.org/abs/2406.03923. arXiv:2406.03923 [cs, math]. Haixu Wu, Huakun Luo, Haowen Wang, Jianmin Wang, and Mingsheng Long. Transolver: fast transformer solver for pdes on general geometries. arXiv preprint arXiv:2402.02366, 2024. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524 10533, 2020. Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo Kim. Graph transformer networks. Advances in Neural Information Processing Systems, 32, 2019."
        },
        {
            "title": "A ARCHITECTURE DETAILS",
            "content": "A.1 INPUT/ OUTPUT PROJECTION ResMLP. We implement residual MLP block to serve as flexible non-linear function approximator. Given input/output dimensions Ci and Co, the layer first applies linear transformation to hidden space of size Ch, followed by residual layers, each consisting of linear layer with GELU activation (Hendrycks & Gimpel, 2016). These are the only instances of pointwise nonlinear activations in the model. An optional input residual connection is applied after the first layer when Ci = Ch, and an optional output residual connection is applied at the end when Ch = Co. The final output is projected to dimension Co via linear layer. This design allows control over depth and expressivity while preserving stability through residual connections."
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "1 import torch.nn.functional as 2 def flare_multihead_mixer_inefficient(q, k, v): 3 4 5 6 7 9 10 11 12 13 15 16 17 18 # Args - q: [H, M, D], k, v: [B, H, N, D] # Ret - y: [B, H, N, D] # Compute projection weights # [B N] scores = @ k.mT W_encode = F.softmax(scores, dim=-1) # [B N] W_decode = F.softmax(scores.mT, dim=-1) # [B M] # Encode: Project to latent sequence (M tokens) = W_encode @ # Decode: Project back to input space (N tokens) = W_decode @ return Figure 7: Pseudocode of FLARE if attention kernel is not available. See Figure 3 for efficient implementation. Input projection. The input projection consists of ResMLP with = 2, Ci is the input feature dimension, and Ch = Co are set to C, the feature dimension of the model. Output projection. The output projection consists of Layer Norm (Ba et al., 2016) followed by ResMLP with Ci = C, = 2, and Co is the output label dimension. A.2 FLARE BLOCK The FLARE block illustrated in Figure 1, and detailed in Section 3, consists of the pointwise ResMLP layer, and the FLARE token mixer. For the ResMLP, we set Ci = Ch = Co = C, set the number of layers to 3, and allow residuals to flow through the entire block. FLARE. FLARE consists of two ResMLPs for key/value projections, the token operation described in Figure 3, and an output projection. For the key/value projections, we set Ci = Ch = Co = C, = 3, and allow residuals to flow through the entire block. Figure 7 presents mathematically equivalent PyTorch implementation for multi-head token-mixing operation without the fused SDPA kernels. The primary memory bottleneck in this implementation is materializing the encoding weights and the (M ). Finally, the output projection is set to single linear layer. decoding weights. Its storage requirement is, thus,"
        },
        {
            "title": "B SPECTRAL ANALYSIS",
            "content": "B.1 EIGENANALYSIS PROCEDURE Wencode We exploit the low-rank structure of the global communication matrix = Wdecode RN to obtain its eigen decomposition in (N 3) cost for dense communication matrix. We find the eigen-decomposition of the FLARE attention matrix matrix. We first note that Wencode and Wdecode can be written in without actually forming the terms of the exponentiated score matrix = exp(cid:0)Q RM as (M 3 + 2N ) time, compared to the (cid:1) Wencode = ΛM A, and Wdecode = ΛN AT , where ΛM RM , and ΛN RN are diagonal matrices whose entries are [ΛM ]m = 1 n=1[A]m,n (cid:80)N , and [ΛN ]n = 1 m=1[A]m,n (cid:80)M . (11) (12)"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Algorithm 1 Eigenvalues and Eigenvectors from Q, RN m=1[A]m,n) n=1[A]m,n) RM D, (cid:1) exp(cid:0)Q diag(1/ (cid:80)M diag(1/ (cid:80)N L1/2 Require: 1: 2: LN 3: LM L1/2 4: 5: Compute SVD: Σ2U 6: Eigenvalues 7: Eigenvectors 8: return Eigenvalues, Eigenvectors U Σ1 Σ2 L1/"
        },
        {
            "title": "JJ T",
            "content": "RM M"
        },
        {
            "title": "Thus we have",
            "content": "W = ΛN AT ΛM as the low-rank attention matrix. We observe that is similar to M AΛ1/2 Λ1/2 RM . This is because (13) RN where = Λ1/2 = ΛN AT ΛM = (Λ1/2 ) (cid:123)(cid:122) (cid:125) ΛN (Λ1/2 (cid:124) (Λ1/2 (cid:124) AT Λ1/2 ) (cid:123)(cid:122) (cid:125) = Λ1/2 (cid:124) AΛ1/2 ) (cid:123)(cid:122) (cid:125) AT (Λ1/2 Λ1/2 ) (cid:123)(cid:122) (cid:125) (cid:124) ΛM Λ1/2 . (Λ1/2 Λ1/2 ) (cid:123)(cid:122) (cid:125) IN (cid:124) (14) Thus is symmetric, positive semi-definite, with rank at most . Now suppose singular value RN are the matrices whose decomposition of as = ΣV where RM is the diagonal columns are the left and right singular vectors of respectively, and Σ matrix of singular values. Then, we obtain RM and and = Σ (cid:124) (cid:123)(cid:122) (cid:125) IM ΣV = Σ2V , = Λ1/2 Σ2V Λ1/2 . (15) (16) Post-multiplying both sides by Λ1/2 , we have (Λ1/ ) = (Λ1/2 Therefore, the nonzero eigenvalues of are the squares of the singular values of J, and the corresponding eigenvectors are the columns of Λ1/2 . Obtaining the eigenvalues and eigenvectors RM . We can do better by of this way requires the singular value decomposition of RM with relating and Σ to the eigen decomposition of JJ . Consider the matrix JJ singular value decomposition, we have )Σ2. (17) JJ = Σ (cid:124) (cid:123)(cid:122) (cid:125) IM ΣU = Σ2U . (18) We note that the nonzero eigenvalues of are the same as the singular values of JJ . To obtain the eigenvectors of , we need an expression for in terms of , J, and Σ. We do so by noting that Therefore, the eigenvectors of are = (V ΣU )U = Σ = = Σ1. (19) (20) = Λ1/2 Λ1/2 U Σ1. To find the eigenvalues and eigenvectors of , one only needs to compute the eigen-decomposition (M 3 + matrix J. The overall algorithm, summarized in Algorithm 1, takes of the 2) time where the (M 3) is for computing the SVD of JJ . O"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Figure 8: = 64 nonzero eigenvalues of Wh (Eq. 9) for = 8 heads in different blocks of FLARE with = 8 blocks = 64 features trained on the elasticity dataset (972 points per input). The number of latents (M ) is depicted with dashed red line, and FP32 precision with dashed black line. B.2 QUALITATIVE ANALYSIS The matrix JJ , being , captures the structure of this latent space and provides insights into how these dimensions contribute to the attention mechanism. Large eigenvalues correspond to dominant latent dimensions that contribute significantly to attention patterns. If some eigenvalues are small or zero, those latent dimensions contribute little, suggesting redundancy in the latent space. The number of nonzero eigenvalues gives the effective rank of , which reflects how many independent patterns the attention mechanism captures. Figure 8 presents the = 64 nonzero eigenvalue spectra of an FLARE model trained on the elasticity dataset with = 64 latents. Some observations are as follows. In all blocks, especially 20 indices. This indicates that even though block 1, the eigenvalues drop sharply within the first 10 the communication matrices Wh could have rank up to = 64, most of the energy (information) is captured by much smaller subset of modes. This result validates the hypothesis that the global communication pattern is inherently low-rank. We also observe that the eigenvalue curves in block 5 and block 8 decay more slowly, retaining more moderate-magnitude eigenvalues beyond index 2030. This indicates that as depth increases, the attention mechanism seems to leverage more of the latent space i.e., the effective rank increases. This shows that deeper layers learn richer global dependencies, and the model may be using more of the projection capacity in later blocks. Finally, we note that the curves for different heads (colors) have distinct decay patterns, especially in later blocks. This reinforces the claim that separate projection matrices per head enables specialized routing. This supports the idea that FLARE benefits from head-wise diversity, rather than using shared projections like in Transolver (Wu et al., 2024)."
        },
        {
            "title": "C BENCHMARKING AND COMPARISON",
            "content": "C.1 BENCHMARK METRICS The primary evaluation metric for all benchmarks is the relative 2 error, which quantifies the normalized discrepancy between the predicted solution ˆu and the ground truth solution over all points in the domain. For given test sample, the relative L2 error is defined as: Relative L2 = ˆu 2 2 16 (21)"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Table 3: Summary of PDE benchmarks. Benchmark #Dim Grid Type #Points"
        },
        {
            "title": "Elasticity",
            "content": "2D"
        },
        {
            "title": "Darcy\nAirfoil\nPipe",
            "content": "2D"
        },
        {
            "title": "Structured",
            "content": "972 7,225 11,271 16,641 DrivAerML-40k LPBF 3D"
        },
        {
            "title": "Unstructured",
            "content": "40,000 1,00050,000 #Input/ Output Features 2 / 1 #Train/ Test Cases 1000 / 200 2 / 1 3 / 1 1000 / 387 / 97 1100 / 290 where points (or grid locations), this expands to: 2 denotes the standard Euclidean norm. For datasets where each sample consists of Relative L2 = (cid:16)(cid:80)N ui)2(cid:17)1/2 (cid:17)1/2 i=1(ˆui (cid:16)(cid:80)N i=1 u2 . (22) The reported metric is averaged over all test samples. C.2 BENCHMARK DATASETS We evaluate all models on six benchmark datasets and our proposed AM dataset, each designed to assess generalization, scalability, and robustness to domain irregularity in PDE surrogate modeling. summary is provided in Table 3. Elasticity. This benchmark estimates the inner stress distribution of elastic materials based on 2 representing the 2D coordinates their structure. Each sample consists of tensor of shape 972 of discretized points, and the output is the stress at each point (972 1). The dataset contains 1,000 training and 200 test samples with different material structures (Li et al., 2023b). Darcy. This benchmark models fluid flow through porous medium. The porous structure is 85 for main experiments. The output discretized on 421 is the fluid pressure at each grid point. There are 1,000 training and 200 test samples with varying medium structures (Li et al., 2021). 421 regular grid, downsampled to 85 Airfoil. This task estimates the Mach number distribution around airfoil shapes. The input geometry is discretized into structured mesh of shape 221 51, representing deformations of the NACA-0012 profile, and the output is the Mach number at each mesh point. The dataset includes 1,000 training and 200 test samples with unique airfoil designs (Li et al., 2023b). Pipe. This benchmark predicts horizontal fluid velocity in pipes with varying internal geometries. 2 tensor encoding mesh point positions, with the output Each sample is represented as 129 129 being the velocity at each mesh point (129 1). The dataset consists of 1,000 training and 200 test samples, with pipe shapes generated by controlling the centerline (Li et al., 2023b). 129 DrivAerML-40k. The DrivAerML dataset (Ashton et al., 2024) has high-fidelity automotive aerodynamic simulations, featuring 500 parametrically morphed DrivAer vehicles. CFD simulations are performed on 160 million volumetric mesh grids using hybrid RANS-LES, the highest-fidelity scaleresolving CFD approach routinely deployed by the automotive industry (Ashton et al., 2024). Each sample in the dataset includes surface mesh with approximately 8.8 million points and corresponding pressure values. Since an official dataset split is not available, we randomly split the dataset into 80% training and 20% testing. We randomly sample 40,000 points in each dataset sample for training and evaluation."
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Table 4: Standard training configuration on PDE datasets. Identical values are grouped for clarity. Dataset Batch Size Weight Decay Learning Rate Epochs"
        },
        {
            "title": "DrivAerML\nLPBF",
            "content": "2 2 1 105 105 102 10 103 103 103 500 500 500 Rel. L2 Rel. L2 + 0.1 Lg Rel. L2 Rel. L2 Rel. L2 Note: For Darcy test case, we include an additional gradient regularization term Lg following Transolver (Wu et al., 2024). Table 5: Model configurations for FLARE for PDE datasets. Identical values are grouped for clarity. #Latents (M ) #Blocks (B) #Heads (H) #Features (C) Dataset"
        },
        {
            "title": "Elasticity",
            "content": "Darcy Airfoil Pipe DrivAerML-40k LPBF 8 16 8 8 8 16 256 256 128 256 256 8 8 8 64 64 LPBF. We propose the laser powder bed fusion (LPBF) additive manufacturing dataset which In metal additive manufacturing, variations in involves field prediction on complex geometries. design geometry can affect the dimensional accuracy of the part and lead to shape distortions. We perform several LPBF process simulations of set of geometries to obtain the deformation field over the geometry. We select subsample of the dataset with up to 50,000 points per geometry and divide it into 1,100 training and 290 cases. We train models to learn the (vertical) component of the deformation field at each grid point. Additional details are provided in Appendix E. C.3 BENCHMARK MODELS AND TRAINING DETAILS We follow the recommended hyperparameter configuration for LNO, Transolver, and GNOT wherever possible. For consistency, we set the number of blocks to = 8, and strive to match the parameter counts of Transolver without (w/o) convolution for all models. As such, the hidden feature dimension for vanilla transformer, is set to = 80. Its head dimension is set to = 16 and MLP ratio is 4 which is typical for transformers (Vaswani et al., 2017). For FLARE, we set the hidden feature dimension to = 64, and use = 8 or 16 heads, which result in head dimension of = 8 or 4 respectively. The number of latents is chosen from depending on the problem. As PerceiverIO was not designed to be surrogate model, we generously set its channel dimension to = 128, and number of latents to = 1, 024. Furthermore, the input and output projections for vanilla transformer, perceiver, and FLARE are held consistent to facilitate an equitable comparison of their point-to-point communication schemes. 64, 128, { } We evaluate Transolver, GNOT with the hyperparameter configurations provided in Wu et al. (2024) and LNO on the ones provided in Wang & Wang (2024a) on the 2D test cases. For the remaining problems, we choose the best performing parameter set from the 2D cases. Unless otherwise stated, all models are trained with the AdamW (Loshchilov & Hutter, 2019) optimizer (β1 = 0.9, β2 = 0.999) to minimize the relative L2 error. We use the OneCycleLR (Smith & Topin, 2019) scheduler with 10% of epochs dedicated to warming-up to learning rate of 103, followed by cosine decay. We train on LPBF for 250 epochs, and 500 epochs for all other models. Note that we use gradient clipping with max norm= 1.0 unless otherwise stated. Unless otherwise stated, the weight decay regularization parameter is set to 105 for the 2D test cases, 104 for DrivAerML-40k, and 104 for LPBF. The batch size is set to 2 for the 2D problems and 1 for the 3D"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "problems unless otherwise stated. Unless otherwise stated, all models are trained in full precision (FP32). Vanilla transformer. For the vanilla transformer, we set the hidden feature dimension to = 80, and choose = 5 heads so that the head dimension = 16. The number of blocks is set to = 8, and the MLP ratio for the feedforward block is set to 4. The vanilla transformer can be prohibitively slow for test cases with over 10,000 test points. PerceiverIO. For PerceiverIO, we use = 8, = 128, = 8, and set the latent sequence length to = 1, 024. Transolver. Transolver (Wu et al. (2024)) introduces Physics-Attention: each mesh point is softly assigned to few learnable slices, shrinking thousands of points to only tens of tokens. Self-attention runs on this compact set and the tokens are then desliced back to points. Following the hyperparameter recommendations in the code of Wu et al. (2024), Transolver is trained with 30% of the steps dedicated to warm-up, and gradient clipping with max norm = 0.1. The recommended batch size for Transolver is 1 for elasticity, and 4 for the remaining 2D problems. For the 3D problems, we set the batch size to 1. Latent Neural Operator (LNO). LNO (Wang & Wang (2024b)) moves computation into small latent space. An embedder lifts the input field, cross-attention compresses points into latent tokens, transformers act solely on these tokens, and decoder maps them to any query location. In line with the recommended hyperparameter configuration in Wang & Wang (2024b), we set β2 = 0.99 in the AdamW optimizer, do warmup for the first 20% of epochs, and clip gradient norms greater than 1,000. The number of hidden features are set to = 192 for elasticity, and = 128 for all other test cases. The number of residual layers is set to 3 for elasticity and 4 for other test cases. The number of latent self attention blocks is 8 for pipe and airfoil test cases and 4 for all other cases. The number of latent modes is set to 256 for all test cases. The batch size during training is set to 4 for the 2D test cases and 1 for the 3D test cases. The LNO code also recommends weight decay regularization of 5 105 for the 2D test cases. In running our experiments, we noticed discrepancies between our LNO results and those presented in their article (Wang & Wang, 2024a). Digging deeper, we found inconsistencies between our training setup that emulates Transolver (code available at https://github.com/thuml/ Transolver). and that of LNO (code available at https://github.com/L-I-M-I-T/ LatentNeuralOperator) that result in different train/test split. This can be problematic as the authors of LNO make direct comparison with the results presented in the Transolver paper. We have informed the authors of LNO of this discrepancy and requested further details on their training setup by rasing pull request at https://github.com/L-I-M-I-T/ LatentNeuralOperator/pull/6. General Neural Operator Transformer (GNOT). GNOT (Hao et al. (2023)) employs heterogeneous normalized attention, separately normalizing keys and values, to fuse multiple input fields on irregular meshes. learnable geometric gate decomposes the domain and routes tokens to scalespecific expert MLPs. Linear cost attention plus this gating scales to large problems and surpasses earlier operator learners. Following the hyperparameter recommendations outlined in the code of Wu et al. (2024), GNOT is trained with 30% of the steps dedicated to warm-up, and gradient clipping with max norm = 0.1. The recommended batch size for Transolver is 1 for elasticity, and 4 for the remaining 2D problems. The batch size is set to 2 for elasticity, 4 for the remaining 2D benchmarks and 1 for the 3D benchmarks. FLARE. For all problems, we employ = 8 blocks with feature dimension of = 64. We set the number of residual layers and the number of key/value projection layers to 3, and vary the head dimension as , the number of latent tokens as 4, 8 The hyperparameters } for each test case are presented in Table 5. 64, 128, { { }"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Figure 9: Execution times in FP32 for single vanilla self-attention layer, physics attention layer, and FLAME. The models are set to have approximately the same number of parameters as in Section 4.1. This calculation is performed on single H100 80GB GPU. Note that the curves for FLARE are somewhat overlapping. FIELD-PREDICTION ON MILLION-POINT GEOMETRIES All FLARE models in this study are trained in mixed precision to take advantage of the flash attention algorithm. We train for 500 epochs with the OneCycleLR scheduler (Smith & Topin, 2019) 104 followed by cosine where the first 5% of epochs are spent warming up to learning rate of 5 decay."
        },
        {
            "title": "E BENCHMARK DATASET OF ADDITIVE MANUFACTURING SIMULATIONS",
            "content": "E.1 INTRODUCTION & BACKGROUND In metal additive manufacturing (AM), subtle variations in design geometry and process parameter selection may result in undesirable part artifacts or even costly build failures. Numerical simulations of laser powder bed fusion (LPBF), popular additive manufacturing process, can be used to predict build failures, but this may take several minutes to hours depending on the part size. Although AM enables the fabrication of wide variety of new designs, the final product must nonetheless comply with the constraints of the underlying manufacturing process. Metal AM parts often have anisotropic and spatially varying material properties that depend on geometric features (such as overhang, or support structure) and fabrication process parameters (such as laser energy density, hatch spacing, layer thickness, and raster path). As-built parts with thin features often suffer distortions when manufactured with the LPBF process, which uses high-power laser to selectively melt and fuse metal powder, layer-by-layer, to create complex, high-precision parts. Thermal stresses, termed residual stresses (RS), accumulate in LPBF-fabricated parts as result of rapid thermal cycling due to laser exposure. These stresses can be severe to the point of inducing localized plastic deformations or delamination. As such, due to these residual deformations, the final shape of the part may deviate from the designed geometry. We present high-fidelity thermomechanical RS calculation data set on the Fusion 360 segmentation dataset, publicly available dataset of complex 3D geometries (Lambourne et al., 2021). Numerical solvers for simulating the LPBF build process perform expensive quasi-static thermo-mechanical equations, dT dt ρCp (cid:124) = kT (x, t) + Q(x, t), (cid:125) (cid:123)(cid:122) thermal transport (cid:124) σ = 0 1 2 (cid:123)(cid:122) stress equilibrium σ = Cεe, (cid:125) (23)"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Figure 10: We simulate the LPBF process on selected geometries from the Autodesk segementation dataset (Lambourne et al., 2021) to generate benchmark dataset for AM calculations. Several geometries are presented in this gallery. The color indicates (vertical) displacement field. layer-by-layer within finite element framework (Denlinger et al., 2014; Liang et al., 2019; Autodesk, 2025), and are integrated in commercial software products (Autodesk, 2025). These calculations take several minutes to hours, making them prohibitively expensive for part design scenarios that can involve hundreds of evaluations. In Ferguson et al. (2025), dataset of LPBF simulations were released where multiple finite element calculations were carried out on dataset of 3D shapes. However, that dataset was limited to small, coarse meshes with approximately 3,500 points per mesh. This work is an iteration upon the work of Ferguson et al. (2025) with larger, refined meshes with up to 50,000 grid points. E.2 DATASET GENERATION To generate dataset of LPBF simulations, we employ Autodesk NetFabb (Autodesk, 2025), commercially available software tool for numerically simulating RS and associated physics. We begin with the Fusion 360 segmentation dataset (Lambourne et al., 2021), and scale each shape to lie within 25, 0] mm; the [ parts are not otherwise rotated or transformed. The simulation is then carried out for the Renishaw AM250 machine and Ti-6Al-4V material system deposited with 40 µm thickness. Other parameters are left as their default nominal values and no support structures are added (Ferguson et al., 2025). [0, 60] mm such that it rests atop the build plate at 30, 30] 30, 30] [ [ In AM, the material is deposited layer by layer, and ideally, high-fidelity simulation would model each layer individually. However, this can be computationally expensive, especially for builds with hundreds or thousands of layers. Layer lumping simplifies this process by combining multiple physical layers into single lumped computational layer. In our calculations, NetFabb applies layerlumping with lumped layer thickness of 2.5 mm. NetFabb re-meshes the geometry to contain axis-aligned hexahedral elements before simulating the build process. Then, NetFabb generates thermal and mechanical history for each part corresponding to lumped layer deposition steps during the build. We obtain the displacement, elastic"
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Figure 11: Summary of LPBF dataset statistics. strain, von Mises stress, and temperature fields evaluated at all nodal locations throughout the build. NetFabb also provides field values after the part has cooled down and detached from the build plate. E.3 BENCHMARK TASK To evaluate neural surrogate models, we target field prediction task that is both central to our dataset and broadly relevant to the AM community: predicting residual vertical (Z) displacement. In LPBF, each layer is fused by laser and followed by recoater blade that spreads powder uniformly across the build area (Reijonen et al., 2024). Overhanging features may cause vertical displacements that interfere with the blades path, potentially leading to collisions. Predicting the Z-displacement field can therefore help identify risk of blade collision failures. Rapid estimation of displacement prior to build, thus, is highly desirable for design troubleshooting, as severe distortion can render part unusable. Moreover, accurate prediction of nodal displacements can help anticipate build failures (Ferguson et al., 2025). Since full-scale LPBF simulations are computationally expensivetaking minutes to hoursa fast surrogate model offers valuable alternative for accelerating AM design. Accordingly, we train our models to predict the Z-displacement at every node at the final time step. More formally, the input to neural surrogate model is the volumetric axis-aligned hexahedral mesh describing the geometry. This includes the point-coordinates (array of size 3 where is the number of points) and, optionally, mesh connectivity information. The corresponding label is the displacement value at each point (array of size While this dataset focuses on steady-state prediction problem, future iterations of this benchmark could involve learning dynamic surrogate models that track the time-history of stress and deformation fields during the build process. 1)."
        },
        {
            "title": "Preprint submitted to arXiv",
            "content": "Table 6: Summary statistics of our proposed LPBF dataset. #Points #Edges Avg./ max aspect ratio Max height (mm) Max Displacement 20,972 12,476 736 Mean Std. Min 25% 10,229 50% 19,743 75% 30,503 Max 47,542 114,140 68,308 2,860 56,208 107,680 166,250 249, 1.6421 / 1.6421 0.43794 / 0.43794 1.0667 / 1.0667 1.2800 / 1.2800 1.4733 / 1.4733 1.9072 / 1.9072 2.9932 / 2.9933 29.429 23.246 0.60000 7.8000 21.600 60.000 60.000 0.29526 0.21064 0.00048500 0.13827 0.27075 0.41962 0.99777 E.4 DATA FILTERING The dataset contains wide-varying range of shapes, making the data set general enough to train strong data-driven field prediction model. Out of 27,000 shapes, 19,732 were successfully simulated. We analyze the first 3,500 successful simulations and filter them according to several statistics to design balanced training and test set. For example, we limit the learning problem to meshes with up to 50,000 points and up to 300,000 shapes. This is done to reduce memory usage which becomes bottleneck when training on small GPUs. We also filter meshes that have high aspect ratio elements as the FEM calculation could be unreliable on highly distorted geometries. Our dataset processing code is available in the GitHub repository associated with this paper. The statistics for the filtered data set are presented in Table 6 along with histograms in Figure 11. gallery of successful simulations is presented in Figure 10."
        }
    ],
    "affiliations": [
        "Mechanical Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA"
    ]
}