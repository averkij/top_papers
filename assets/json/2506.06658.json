{
    "paper_title": "Self-Adapting Improvement Loops for Robotic Learning",
    "authors": [
        "Calvin Luo",
        "Zilai Zeng",
        "Mingxi Jia",
        "Yilun Du",
        "Chen Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video generative models trained on expert demonstrations have been utilized as performant text-conditioned visual planners for solving robotic tasks. However, generalization to unseen tasks remains a challenge. Whereas improved generalization may be facilitated by leveraging learned prior knowledge from additional pre-collected offline data sources, such as web-scale video datasets, in the era of experience we aim to design agents that can continuously improve in an online manner from self-collected behaviors. In this work we thus propose the Self-Adapting Improvement Loop (SAIL), where an in-domain video model iteratively updates itself on self-produced trajectories, collected through adaptation with an internet-scale pretrained video model, and steadily improves its performance for a specified task of interest. We apply SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks on a real robot arm, and find that performance improvements continuously emerge over multiple iterations for novel tasks initially unseen during original in-domain video model training. Furthermore, we discover that SAIL is surprisingly robust regarding if and how the self-collected experience is filtered, and the quality of the initial in-domain demonstrations. Through adaptation with summarized internet-scale data, and learning through online experience, we thus demonstrate a way to iteratively bootstrap a high-performance video model for solving novel robotic tasks through self-improvement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 8 5 6 6 0 . 6 0 5 2 : r Self-Adapting Improvement Loops for Robotic Learning Calvin Luo*,1 , Zilai Zeng*,1, Mingxi Jia1, Yilun Du2, Chen Sun1 1Brown University, 2Harvard University Abstract: Video generative models trained on expert demonstrations have been utilized as performant text-conditioned visual planners for solving robotic tasks. However, generalization to unseen tasks remains challenge. Whereas improved generalization may be facilitated by leveraging learned prior knowledge from additional pre-collected offline data sources, such as web-scale video datasets, in the era of experience we aim to design agents that can continuously improve in an online manner from self-collected behaviors. In this work we thus propose the SelfAdapting Improvement Loop (SAIL), where an in-domain video model iteratively updates itself on self-produced trajectories, collected through adaptation with an internet-scale pretrained video model, and steadily improves its performance for specified task of interest. We apply SAIL to diverse suite of MetaWorld tasks, as well as two manipulation tasks on real robot arm, and find that performance improvements continuously emerge over multiple iterations for novel tasks initially unseen during original in-domain video model training. Furthermore, we discover that SAIL is surprisingly robust regarding if and how the self-collected experience is filtered, and the quality of the initial in-domain demonstrations. Through adaptation with summarized internet-scale data, and learning through online experience, we thus demonstrate way to iteratively bootstrap high-performance video model for solving novel robotic tasks through self-improvement. Visualizations and code can be found at diffusion-supervision.github.io/sail/. Keywords: Planning, Adaptation, Self-Improvement, Robots, Learning"
        },
        {
            "title": "Introduction",
            "content": "Advancements in video generative modeling capabilities have directly led to their increased utilization as visual planners for robotic applications [1, 2, 3, 4]. The synthesized visual plan, in the form of video frames generated with text conditioning, can be translated into executable actions via inverse dynamics models (IDMs). While the IDMs are generally robust across tasks, the data on which the video generative models are trained can greatly impact downstream robotic performance and generalization. When explicitly optimized on in-domain examples of expert behavior, such visual planners are able to synthesize successful plans for solving demonstrated tasks in robust manner. However, for arbitrary robotic settings, large-scale expert-quality datasets may not be readily available, and collection may be prohibitively expensive. paucity of data scale can limit video models trained only on in-domain videos from exhibiting generalized planning capabilities for novel tasks. Integrating knowledge from large-scale datasets of text and video collected from the internet has facilitated improved generalization, even in the absence of abundant in-domain videos. Recent work, Adapt2Act [5], creates powerful, generalizable, text-conditioned visual planner by combining large-scale model pretrained on web-scale video data with video model trained on small set of in-domain demonstrations via score composition. At high level, the adapted video model draws upon large-scale motion priors and powerful zero-shot text conditioning capabilities from the webpretrained video model to facilitate generalization. Simultaneously, it can leverage the in-domain *: Equal contribution. Correspondence to: calvin luo@brown.edu and zilai zeng@brown.edu. Figure 1: SAIL Framework. SAIL utilizes two pretrained video generative models (left): one pretrained generally on internet-scale data and another pretrained on general set of in-domain demonstrations. Composing these two components results in visual planner with strong priors, which when utilized to interact with the environment, is able to produce trajectories with improved success rate even for initially unseen tasks. In the Self-Adapting Improvement Loop (SAIL), these trajectories are then iteratively fed back to finetune the in-domain model (right), thus improving the overall quality of the adapted visual planner as whole through self-collected online experience. video model to better generate visual plans that respect the environment-specific visual characteristics and dynamics of the robotic setting. The result is an adapted video model that can generate in-domain-appearing plans for novel, unseen tasks conditioned on natural language. Despite extending the amount of data utilized for visual planning to internet-level, the model still only has access to purely offline data alone, which can still be limiting in terms of downstream performance. Instead, in the era of experience, we aim to design agents that can continuously improve from self-collected behaviors and feedback. In such way, the agent can break free beyond the limits of the provided data and learn by itself to refine performance on specified task of interest. We therefore propose the Self-Adapting Improvement Loop (SAIL), where we iteratively self-improve the video model with online experience, even for behaviors previously unseen in the initial dataset of environment demonstrations. As shown in Figure 1, we construct loop by iteratively updating the video generative model with data collected by the robotic agents following visual plans, in which the quality of the plans are improved through adaptation with frozen, internet-pretrained video model. We perform extensive evaluations of SAIL on the MetaWorld task suite, focusing on novel tasks unseen during initial training of the in-domain model. We discover that the success rate of following visual plans synthesized through adaptation indeed improves over iterations. Crucially, we highlight that adaptation with large-scale pretrained text-conditioned video models is critical for facilitating self-improvement, by contributing text-conditioned generalization capabilities and motion priors. Furthermore, through ablations over design decisions, we discover that SAIL is relatively robust not only to the presence of filtering strategies for self-collected experience, but also the quality of demonstration data that the in-domain model is initially trained on. We also apply SAIL to realworld robot arm for two distinct manipulation tasks: selecting and pushing colored object, and selecting and opening colored drawer. We demonstrate that performance for color combinations unseen during the initial offline training improves over multiple iterations through SAIL."
        },
        {
            "title": "2 Related Work",
            "content": "Video Generation for Decision Making. Recent advances in video models have achieved unprecedented visual quality and physical fidelity for video synthesis [6, 7, 8, 9, 10]. This has demonstrated promise in summarizing world dynamics through videos [11, 12] and has inspired the application of video models to solving decision-making problems [13, 14, 2, 15, 4]. Prior works have utilized video generative models as reward functions [16, 13, 17], dynamics models [2, 12, 18], and pixel-based planners [3, 19, 1, 20]. As in UniPi [1], we employ video models to predict text-conditioned visual plans that depict future outcomes, which are subsequently translated into actions via inverse dynamics. While the performance of such visual planners may often be limited by their offline pretraining data, our approach allows iterative improvement by learning from online environment interactions. 2 Adapting Pretrained Video Models. Adaptation is often needed for customized generation when applying generally pretrained video models to specialized tasks. For video models based off of image models [6, 21, 22, 23], image customization techniques, such as Textual Inversion [24] and DreamBooth [25], can be utilized to inject specific subject information for video generation. To obtain fine-grained controllability, DreamVideo [26] learns two specific adapters for capturing subject appearances and motion control, respectively. Furthermore, Video Adapter [27] proposes Probabilistic Adaptation (PA), technique that performs adaptation through score composition during the sampling stage, without finetuning the weights of large pretrained models. Adapt2Act [5] extends Probabilistic Adaptation to its inverse (IPA), and leverages video adaptation techniques to create performant visual planner for solving novel decision making tasks conditioned on natural language. In this paper IPA acts as an approach to improve visual planning capabilities for novel tasks, where the rollouts are then collected as experience and utilized to iteratively finetune the in-domain video model and improve its planning ability. Self-Improving Generative Models. Continuously improving by learning from self-produced cumulative experience is an essential capability of intelligent agents. Prior work has demonstrated the effectiveness of improving LLMs with their self-generated outputs [28, 29, 30], where the LLM can serve as its own reward function [31] for preference optimization or data synthesizer [32] for supervised finetuning. However, similar self-improvement recipe for video generation models remains underexplored. Most relevant to our work, VideoAgent [33] refines video generation through selfconditioning consistency and feedback from VLM, and collects the successful plan rollouts for finetuning. We instead base our improvement loop on self-adaptation, where we leverage internetscale video priors to synthesize improved visual plans for tasks unseen during initial in-domain training. Furthermore, our approach can still achieve self-improvement even with an initial model trained on suboptimal data and notable relaxation on filtering requirements for finetuning data."
        },
        {
            "title": "3 Method",
            "content": "We introduce the Self-Adapting Improvement Loop (SAIL), in which video generative model initially trained on general set of in-domain demonstrations iteratively improves its visual planning performance for particular task of interest in self-adaptive manner. In Section 3.2, we describe how small in-domain video model can be integrated with generally pretrained text-to-video model to produce strong, generalizable in-domain visual planner. Finally, in Section 3.3, we demonstrate how SAIL bootstraps an in-domain video model into high-performing visual planner for solving novel robotic control task through iteratively fine-tuning on self-collected experience. 3.1 Video Models as Visual Planners Synthesizing visual plan in imagination and then executing it by converting it into actions is an intuitive and effective way to utilize video generative models for decision making. Prior work has applied text-guided video generation successfully for task planning [14, 1, 19], across variety of robot configurations and environment settings. Specifically, we base our implementation on the UniPi framework [14], in which text-to-video model is used to synthesize text-conditioned sequence of future frames as task plan. To physically realize the plan, we use separately trained inverse dynamics model (IDM) to translate consecutive pairs of visual frames into executable robotic actions, which are then directly performed in interaction with the environment. Visual planning offers the practitioner flexible computational tradeoffs; at high level, replanning often incurs high computational cost but generally increases accurate plan following, whereas replanning infrequently is cheap but may suffer from error compounding. As the IDM is task-agnostic, and can be trained from general in-domain interaction data, task generalization and performance under the visual planning framework is largely product of the video generative model quality. In this work, we focus on how such video generative model can generalize and self-adapt to novel task of interest through online self-collected experience. 3 Algorithm 1 Self-Adapting Improvement Loop (SAIL) Input: Initial in-domain video model ϵθ, Inverse dynamics model , Frozen internet-pretrained video model ϵgeneral, Number of iterations K, Number of rollouts per iteration , Environment env, Task prompt g, In-domain initial training data Dini Output: Self-improved in-domain model ˆϵθ Initialize finetuning data with Dini or an empty set 1: ˆϵθ ϵθ 2: Dini or ϕ 3: for = 1, ..., do Dself ϕ 4: ϵinv IPA(ˆϵθ, ϵgeneral, g) 5: for = 1, ..., do 6: env.reset(g) 7: Dself Dself Visual Planning Rollout(env, ϵinv, ) 8: 9: 10: 11: 12: end for 13: return ˆϵθ end for Dself Finetune in-domain model ˆϵθ with accumulated data Optional data filtering can be optionally finetuned 3.2 Inverse Probabilistic Adaptation Prior work [5] has investigated how in-domain demonstration data can best be integrated with largescale pretrained video models for generalizable visual planning; in this work we leverage similar insights to successfully integrate on-the-fly experience into visual planners for iterative selfimprovement. Inverse Probabilistic Adaptation [5, 27] (IPA) is training-free approach that adapts generally pretrained text-to-video models for domain-specific video generation. To perform adaptation, the score predicted by an in-domain video model ϵθ trained on small sample of demonstrations is composed with the score prediction of web-scale pretrained model ϵgeneral during the sampling procedure, as depicted in the function below: ϵinv = ϵgeneral(τt, t) + α (cid:16) ϵgeneral(τt, text) + γϵθ(τt, text) ϵgeneral(τt, t) (cid:17) (1) where γ is the prior strength, and α is the guidance scale of text-conditioning. Intuitively, the small in-domain text-to-video model serves as probabilistic knowledge prior that guides the generation process of the small in-domain model during sampling. Prior work [5] has found that visual planner constructed through IPA exhibits both strong generalization capability and in-domain understanding; it is able to the synthesize performant visual plans that appear in-domain even for novel tasks unseen during video model training. This may stem from the fact that IPA utilizes the large-scale pretrained model, which inherently has stronger text-conditioned generalization, as the main denoiser. 3.3 Self-Adapting Improvement Loop Whereas stronger text-conditioned generalization to novel tasks can occur from increasing the amount of data utilized to internet-scale, task performance is still fixed function of the video models used, and by extension, the data observed. As result, in this paper, we wish to design agents that can not only leverage offline data as helpful prior for generalization, but also extend beyond it to iteratively improve from self-collected online experience data. We therefore propose the Self-Adapting Improvement Loop, framework that combines offline data with online experience to create visual planner that iteratively improves for particular task of interest. SAIL is initialized with an in-domain video model ϵθ pre-trained on set of task demonstrations within the environment. In each iteration, the in-domain video model is integrated with large-scale pretrained video model ϵgeneral through IPA. The adapted video model then serves as visual planner to interact with the environment and solve tasks not necessarily observed in the initial training stage; in SAIL, the trajectories collected through this interaction are used for further finetuning of the in-domain video model (as shown in Algorithm 1). As the in-domain model adapts 4 Figure 2: SAIL results on MetaWorld and Panda Arm. We report average performance over 6 tasks on MetaWorld, as well as two novel pushing and one novel drawer opening task for Panda arm experiments. Compared to in-domain only, SAIL demonstrates more robust improvement behaviors without performance degradation, and enables continuous improvement on both real-robot tasks. to its own self-collected experience from deployment on novel task, it improves its ability to solve that particular task over time. In this way, SAIL iteratively bootstraps an in-domain video model into strong visual planner for particular task of interest through self-adapting improvement cycle. We demonstrate that it is the combination of using web-scale data along with self-collected experience that facilitates virtuous loop; in our experiments we show that training on either independently fails to show strong iterative improvement. We further stress-test our framework through ablations on initialization data quality as well as filtering strategies. We find that SAIL is robust approach for iteratively adapting to task through effective utilization of both offline data and online experience."
        },
        {
            "title": "4 Experiments",
            "content": "We investigate how SAIL can improve an in-domain video model initially trained on limited set of demonstrations and tasks to further solve novel robotic control tasks through self-collected experience. We focus on two main robot settings to evaluate SAIL: the MetaWorld-v2 [34] simulated environment, and real-world Franka Emika Panda robot arm. We describe our experimental setup for each environment, as well as different design decisions considered. 4.1 Experimental Setup and Evaluation Synthetic Environment: MetaWorld encompasses wide selection of tasks, allowing us to thoroughly assess visual planning performance trends through SAIL for many choices of held-out novel tasks. Furthermore, MetaWorld provides ground-truth success evaluations, enabling strictly quantitative comparisons on task performance and improvement. For MetaWorld experiments, we first collect 25 demonstrations from 7 different tasks (denoted with an asterisk in Table A1) for initial indomain video model and inverse dynamics model training. Subsequently, we utilize the in-domain video model adapted with large-scale pretrained text-to-video model through IPA as visual planner for 6 tasks, 5 of which are novel tasks (denoted with no asterisk in Table A1). We utilize SAIL to iteratively improve the in-domain model via self-collected experience; for each iteration, we collect 30 trajectories rendered from the environment during visual planning for in-domain finetuning. Real-World Environment: Deploying SAIL on robot arm in the real world demonstrates practicality of the approach, as well as tests robustness to real-world confounding factors such as lighting conditions. In one experiment, we utilize Franka Emika Panda robot arm for the task of pushing cups specified by user-provided text prompt. In contrast to the MetaWorld setups, where each task of interest has its own distinct visual setting, we construct the cup experiment as consistent scene setting of 3 differently colored cups (Figure 1). Success is then measured in terms of whether the robot arm can accurately locate specified color cup and push it forward. To test generalization, conditioned on natural language, we evaluate successful planning and execution performance on unseen cup colors. In practice, we use set of four colors (red, green, blue, pink) for in-domain training and two novel colors for testing generalization (orange, purple). This translates to 12 possible unique tasks formed from combinations of the seen colors, and we train our in-domain video model with 10 human-teleoperated demonstrations of each for total of 120 training videos. Then, 5 Figure 3: Qualitative results on visual plans refinement. We illustrate visual plans for variety of tasks and settings at Iteration 0 (top) and Iteration 2 (bottom) with random initial object locations. Although the visual plan at Iteration 0 renders blurry objects and fails to complete the specified tasks, our approach synthesizes the correct visual plan (with slight color drift) after two SAIL iterations. generalization evaluation is calculated as an average over 5 rollouts for every possible pair combination of the seen color set combined with the novel color, for total of 30 videos. For both novel colors, we initialize SAIL using the same pretrained in-domain video model. In each SAIL iteration, we combine previous self-collected data with the initial demonstrations for in-domain finetuning. In second real-robot experiment, we utilize the Panda arm to select and open drawer specified via user-provided text prompt. The scene is constructed as two distinctly colored closed drawers, where the robot is prompted with one particular color and expected to open its corresponding drawer. We use set of three colors (red, green, blue) for in-domain training and one novel color (yellow) for testing generalization. With 24 possible drawer placement combinations for each ordered pair of seen colors, of which there are six, this amounts to total of 144 human-teleoperated demonstration training videos. Consistent with the cup pushing experiment, we use half the possible combinations for evaluation; therefore, performance is calculated as an average over 12 rollouts for every possible pairing of the novel color with seen color, for total of 36 self-collected trajectories per iteration. For both real-robot experiments, success is judged by human for evaluation. The same success signal is also used to perform optional data filtering on the rollouts. We study the impact of data filtering in Section 4.3, and do not use filtering for experiments in Section 4.4. Implementation Details: We implement our in-domain video model based on AVDC [3], with an added cross-attention layer to each level of the denoising U-Net to further improve text-conditioning capabilities. We train in-domain video models to predict 8 future frames conditioned on the current observation and task prompt, with frame skip of 1 for MetaWorld and 16 for real-robot experiments. For the large-scale pretrained text-to-video model, we use AnimateDiff [6] (2B parameters), which is pretrained on WebVid-10M [35]. Each iteration of SAIL finetunes the in-domain video model for 10,000 steps with learning rate of 1e-5 on MetaWorld and Panda Arm drawer opening tasks, and 8,000 steps with learning rate of 2e-5 on Panda Arm pushing tasks. 4.2 Visual Planning with SAIL We report incremental visual planning results for MetaWorld and the Panda arm through 3 SAIL iterations. At each iteration, we filter out unsuccessful trajectories, and only finetune on the successful ones. In Figure 2, on the left, we showcase the average success rate across 6 MetaWorld tasks, 5 of which are novel, comparing between in-domain only and IPA (per-task performance is detailed in Table A5). We find that through adaptation, the initial success rate is higher across tasks, highlighting the benefit of using large-scale offline data as strong prior for novel task generalization. Furthermore, we discover that SAIL is effective in facilitating further performance improvement from utilizing self-collected experience, as the performance increases iteration upon iteration. Notably, using the in-domain model alone does see some initial improvements, but it does not consistently hold over multiple iterations nor does it achieve as high overall performance as through SAIL. In the two middle plots of Figure 2, we showcase SAIL on the Panda arm for the tasks of pushing orange and purple cups, which were initially unseen colors. Averaged over 30 rollouts, across different combinations of the novel color with previously seen colors, we discover that SAIL consistently 6 (a) MetaWorld (b) Panda Arm Pushing Figure 4: Ablations on data filtering. We evaluate how filtering self-collected data with oracle successful signals would impact SAIL performance on both MetaWorld (4a) and Panda arm (4b) setups. We also provide additional results with relabeling strategy on real-robot experiments. We observe SAIL consistently improves task performance without filtering the collected data on both benchmarks, reaffirming the robustness of our approach in the absence of oracle filtering signals. improves performance over iterations. As with the MetaWorld results, similar trend arises where utilizing the in-domain model alone does not incur substantial improvements; rather, in the case of pushing the purple cup, performance decreases monotonically even though the in-domain model is similarly finetuned on filtered self-collected experience. In the rightmost plot of Figure 2, we showcase results across SAIL iterations for opening novel colored drawer (visualized in Figures A8 and A9). Averaged over 36 rollouts per iteration, we demonstrate once more how finetuning on filtered experience facilitates steady improvements through SAIL, whereas utilizing an in-domain model alone results in steady decay in performance. Overall, these results highlight that SAIL leads to self-improving performance across both simulated and real-world environments for novel tasks, by leveraging large-scale offline data along with online experience data. In Figure 3, we qualitatively illustrate the visual plans for real robot manipulation and MetaWorld tasks at Iteration 0 (top) and Iteration 2 (bottom). Without observing any demonstrations of the specified tasks at Iteration 0, IPA often synthesizes visual plan with blurry objects where the robot arm execute the task incorrectly. On the other hand, two iterations of SAIL not only improves the clarity of the visual plans, but also demonstrate successful task completion behaviors in the same initial layout. By following the plan via an inverse dynamics model, the robot arm is able to execute the task successfully in the actual environment interaction (as shown in Appendix E). 4.3 SAIL without Experience Filtering While utilizing self-collected data is promising approach for scalable self-improvement, filtering collected experience often requires some level of human intervention, whether through manually determining successful trajectories or designing heuristic for quality control. We therefore investigate how different filtering techniques affect SAIL performance, or if SAIL is robust to such design decisions. For both MetaWorld and Panda Arm settings, we compare between using ground-truth or human-evaluated notion of success to filter what trajectories the in-domain model is finetuned on, against not using any filtering at all and utilizing all achieved trajectories irregardless of outcome. In Figure 4a, we observe that for both in-domain and SAIL, disregarding filtering actually slightly improves over filtering. This is surprising result, as it suggests that even failed demonstrations may serve as source of meaningful behaviors and further facilitate overall task improvement. On the other hand, in Figure 4b, for the Panda arm, we observe that no filtering still facilitates continuous improvement over every iteration through SAIL. This is an encouraging finding, as it suggests that even for settings where manual curation of experience is expensive, self-improvement can still occur. We also investigate if novel filtering scheme is useful for the pushing tasks on Panda arm, called relabeling. In this setting, all trajectories are once again utilized for finetuning the in-domain textto-video model, but unsuccessful trajectories are prepended with text prompt of not to denote failure. We find that relabeling is indeed preferable to not using any filtering for the in-domain model, but does not substantially aid performance when utilizing large-scale text-to-video priors. Figure 5: SAIL results with suboptimal in-domain data. We report the individual performance on 4 novel MetaWorld tasks, along with their averaged performance across SAIL iterations. Even with suboptimal in-domain data, the continuously improving behavior of SAIL remains robust, surpassing the in-domain only baseline. 4.4 SAIL with Suboptimal Data Visual planners are usually trained explicitly on expert in-domain demonstrations, which communicate not only environment-specific visual characteristics, physics, and interaction dynamics to the generative model during optimization, but also notion of success and optimal behavior. However, for arbitrary environments, such expert-quality in-domain data can be expensive to collect and curate at scale. On the other hand, suboptimal demonstration data, such as utilizing random actions during the collection procedure, may generally be cheaper to gather; however, training on large dataset of low-quality data may not result in performant visual planning model capable of generating plans worth following. natural question is how robust SAIL is to initialization data, or whether performant video planner can still be created when only suboptimal demonstrations are available. In our setting, we construct suboptimal data as simulated trajectories where 70% of the time random action is selected and 30% an expert action is utilized. As consequence of this interaction procedure, the resulting trajectories are unable to successfully solve complex tasks. We also continue with the previous setting of not utilizing any filtering strategies. Despite this setting, SAIL continues to effectively combine both large-scale offline data through IPA and self-collected experience to achieve performance refinement. In MetaWorld we find that for four highlighted tasks, all of which are unseen, SAIL demonstrates continuously improving behavior, as shown in the middle and rightmost plot of Figure 5. SAILs robustness to initial in-domain data quality may be attributed to the ability of IPA to overcome the suboptimality gap [5]. Alternately, as depicted in Figure 5, using the in-domain model alone does not show significant improvements on average. Without adaptation with large-scale text-to-video models, an in-domain model trained on suboptimal data alone may struggle in collecting sufficient successful online experience, and subsequently reinforce its suboptimal behavior through unfiltered finetuning. This thus highlights the robustness of SAIL - despite not utilizing any filtering strategies, and initializing from only suboptimal data demonstrations, it still is able to bootstrap powerful visual planner for novel tasks through self-collected experience."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present SAIL, self-adapting improvement loop for solving novel robotic tasks via visual planning. Initializing from an in-domain video model pretrained on small set of demonstrations, SAIL uses IPA with large-scale video model pretrained on general internet as performant generalizable visual planner to iteratively collect experience trajectories for self-improvement of the in-domain video model. In such way, SAIL is able to combine large-scale offline data with online self-acquired experience to bootstrap performant text-conditioned visual planner for desired task. Through our experiments, we demonstrate that SAIL is robust framework, not only in the absence of filtering techniques, but also in terms of the quality of the initial demonstration set. We show that SAIL is able to succeed as self-improving visual planner not only for synthetic environments, but also deployed on robot arm in the real world."
        },
        {
            "title": "Limitations",
            "content": "SAIL implicitly assumes that the initial in-domain model, through adaptation with internetpretrained video model, achieves reasonable success rate to collect online experience and selfimprove the models. This assumption may not hold when the novel task is too challenging. Additionally, the choice of internet-pretrained video model poses trade-off on video quality (hence the strength of the motion prior, etc.) against computation cost. Whereas in this work we choose AnimateDiff [6] as large-scale pretrained text-to-video model with reasonable generation quality and good computational efficiency, more recent video generative models can be explored for better visual quality and potential improvements to downstream robotic performance. Acknowledgments This work is partially supported by Samsung and NASA. Our research was conducted using computational resources at the Center for Computation and Visualization at Brown University. We would like to thank Professors George Konidaris and Stefanie Tellex for their generous support for our real-robot experiments, and Skye Thompson for helpful initial discussions. Calvin thanks Kayan Shih and family for their kindness and support during the paper writing process."
        },
        {
            "title": "References",
            "content": "[1] Y. Du, S. Yang, P. Florence, F. Xia, A. Wahid, brian ichter, P. Sermanet, T. Yu, P. Abbeel, J. B. Tenenbaum, L. P. Kaelbling, A. Zeng, and J. Tompson. Video language planning. In International Conference on Learning Representations (ICLR), 2024. [2] M. Yang, Y. Du, K. Ghasemipour, J. Tompson, D. Schuurmans, and P. Abbeel. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 2023. [3] P.-C. Ko, J. Mao, Y. Du, S.-H. Sun, and J. B. Tenenbaum. Learning to act from actionless videos through dense correspondences. In International Conference on Learning Representations (ICLR), 2024. [4] J. Liang, R. Liu, E. Ozguroglu, S. Sudhakar, A. Dave, P. Tokmakov, S. Song, and C. Vondrick. Dreamitate: Real-world visuomotor policy learning via video generation. arXiv preprint arXiv:2406.16862, 2024. [5] C. Luo, Z. Zeng, Y. Du, and C. Sun. Solving new tasks by adapting internet video knowledge. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=p01BR4njlY. [6] Y. Guo, C. Yang, A. Rao, Y. Wang, Y. Qiao, D. Lin, and B. Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [7] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [8] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh. Video generation models as world simulators. OpenAI Blog, 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. [9] Veo-Team, :, A. Gupta, A. Razavi, A. Toor, A. Gupta, D. Erhan, E. Shaw, E. Lau, F. Belletti, G. Barth-Maron, G. Shaw, H. Erdogan, H. Sidahmed, H. Nandwani, H. Moraldo, H. Kim, I. Blok, J. Donahue, J. Lezama, K. Mathewson, K. David, M. K. Lorrain, M. van Zee, M. Narasimhan, M. Wang, M. Babaeizadeh, N. Papalampidi, N. Pezzotti, N. Jha, P. Barnes, P.-J. Kindermans, R. Hornung, R. Villegas, R. Poplin, S. Zaiem, S. Dieleman, S. Ebrahimi, S. Wisdom, S. Zhang, S. Fruchter, S. Nørly, W. Hua, X. Yan, Y. Du, and Y. Chen. Veo 2. 2024. URL https://deepmind.google/technologies/veo/veo-2/. [10] A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang, J. Zeng, J. Wang, J. Zhang, J. Zhou, J. Wang, J. Chen, K. Zhu, K. Zhao, K. Yan, L. Huang, M. Feng, N. Zhang, P. Li, P. Wu, R. Chu, R. Feng, S. Zhang, S. Sun, T. Fang, T. Wang, T. Gui, T. Weng, T. Shen, W. Lin, W. Wang, W. Wang, W. Zhou, W. Wang, W. Shen, W. Yu, X. Shi, X. Huang, X. Xu, Y. Kou, Y. Lv, Y. Li, Y. Liu, Y. Wang, Y. Zhang, Y. Huang, Y. Li, Y. Wu, Y. Liu, Y. Pan, Y. Zheng, Y. Hong, Y. Shi, Y. Feng, Z. Jiang, Z. Han, Z.-F. Wu, and Z. Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [11] S. Yang, J. Walker, J. Parker-Holder, Y. Du, J. Bruce, A. Barreto, P. Abbeel, and D. SchuarXiv preprint urmans. Video as the new language for real-world decision making. arXiv:2402.17139, 2024. [12] J. Bruce, M. Dennis, A. Edwards, J. Parker-Holder, Y. Shi, E. Hughes, M. Lai, A. Mavalankar, R. Steigerwald, C. Apps, Y. Aytar, S. Bechtle, F. M. P. Behbahani, S. Chan, N. M. O. Heess, L. Gonzalez, S. Osindero, S. Ozair, S. Reed, J. Zhang, K. Zolna, J. Clune, N. de Freitas, S. Singh, and T. Rocktaschel. Genie: Generative interactive environments. arXiv preprint arXiv:2402.15391, 2024. 10 [13] A. Escontrela, A. Adeniji, W. Yan, A. Jain, X. B. Peng, K. Goldberg, Y. Lee, D. Hafner, and P. Abbeel. Video prediction models as rewards for reinforcement learning. In Conference on Neural Information Processing Systems (NeurIPS), 2023. [14] Y. Du, S. Yang, B. Dai, H. Dai, O. Nachum, J. Tenenbaum, D. Schuurmans, and P. Abbeel. Learning universal policies via text-guided video generation. Advances in Neural Information Processing Systems, 36, 2024. [15] R. McCarthy, D. C. Tan, D. Schmidt, F. Acero, N. Herr, Y. Du, T. G. Thuruthel, and Z. Li. Towards generalist robot learning from internet video: survey. arXiv preprint arXiv:2404.19664, 2024. [16] C. Luo, M. He, Z. Zeng, and C. Sun. Text-aware diffusion for policy learning. In Advances in Neural Information Processing Systems, volume 37, 2024. [17] T. Huang, G. Jiang, Y. Ze, and H. Xu. Diffusion reward: Learning rewards via conditional video diffusion. arXiv preprint arXiv:2312.14134, 2023. [18] D. Valevski, Y. Leviathan, M. Arar, and S. Fruchter. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. [19] A. Ajay, S. Han, Y. Du, S. Li, A. Gupta, T. Jaakkola, J. Tenenbaum, L. Kaelbling, A. Srivastava, and P. Agrawal. Compositional foundation models for hierarchical planning. In Conference on Neural Information Processing Systems (NeurIPS), 2023. [20] S. Zhou, Y. Du, J. Chen, Y. Li, D. Y. Yeung, and C. Gan. Robodreamer: Learning compositional world models for robot imagination. arXiv preprint arXiv:2404.12377, 2024. [21] J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7623 7633, 2023. [22] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, D. Parikh, S. Gupta, and Y. Taigman. Make-a-video: Text-to-video generation without textIn The Eleventh International Conference on Learning Representations, 2023. video data. URL https://openreview.net/forum?id=nJfylDvgzlq. [23] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [24] R. Gal, Y. Alaluf, Y. Atzmon, O. Patashnik, A. H. Bermano, G. Chechik, and D. Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations (ICLR), 2023. [25] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine In Proceedings of the tuning text-to-image diffusion models for subject-driven generation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. [26] Y. Wei, S. Zhang, Z. Qing, H. Yuan, Z. Liu, Y. Liu, Y. Zhang, J. Zhou, and H. Shan. In ProDreamvideo: Composing your dream videos with customized subject and motion. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 65376549, 2024. [27] M. Yang, Y. Du, B. Dai, D. Schuurmans, J. B. Tenenbaum, and P. Abbeel. Probabilistic adaptation of text-to-video models. arXiv preprint arXiv:2306.01872, 2023. 11 [28] X. Yu, B. Peng, M. Galley, J. Gao, and Z. Yu. Teaching language models to self-improve through interactive demonstrations. In K. Duh, H. Gomez, and S. Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 51275149, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi:10.18653/ v1/2024.naacl-long.287. URL https://aclanthology.org/2024.naacl-long.287/. [29] Y. Tian, B. Peng, L. Song, L. Jin, D. Yu, L. Han, H. Mi, and D. Yu. Toward self-improvement In Conference on Neural Information of LLMs via imagination, searching, and criticizing. Processing Systems (NeurIPS), 2024. [30] J. Huang, S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han. Large language models can self-improve. In Conference on Empirical Methods in Natural Language Processing, 2022. [31] W. Yuan, R. Y. Pang, K. Cho, X. Li, S. Sukhbaatar, J. Xu, and J. E. Weston. Self-rewarding language models. In International Conference on Machine Learning (ICML), 2024. [32] A. Patel, M. Hofmarcher, C. Leoveanu-Condrei, M.-C. Dinu, C. Callison-Burch, and S. Hochreiter. Large language models can self-improve at web agent tasks. arXiv, 2405.20309, 2024. [33] A. Soni, S. Venkataraman, A. Chandra, S. Fischmeister, P. Liang, B. Dai, and S. Yang. Videoagent: Self-improving video generation. arXiv preprint arXiv:2410.10076, 2024. [34] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on robot learning, pages 10941100. PMLR, 2020. [35] M. Bain, A. Nagrani, G. Varol, and A. Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In IEEE International Conference on Computer Vision (ICCV), 2021. [36] A. Majumdar, K. Yadav, S. Arnaud, J. Ma, C. Chen, S. Silwal, A. Jain, V.-P. Berges, T. Wu, J. Vakil, P. Abbeel, J. Malik, D. Batra, Y. Lin, O. Maksymets, A. Rajeswaran, and F. Meier. Where are we in the search for an artificial visual cortex for embodied intelligence? In Conference on Neural Information Processing Systems (NeurIPS), 2023. [37] Y. Guo, C. Yang, A. Rao, M. Agrawala, D. Lin, and B. Dai. SparseCtrl: adding sparse controls to text-to-video diffusion models. In European Conference on Computer Vision (ECCV), 2024. [38] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations (ICLR), 2021."
        },
        {
            "title": "A Tasks and Text Prompts",
            "content": "Below we list the tasks and associated text prompts used for evaluating SAIL. Tasks with demonstrations seen during training of the in-domain model are denoted with an asterisk. Task Assembly Dial Turn Reach Peg Unplug Side Lever Pull Coffee Push Door Close Window Close Window Open Drawer Close Drawer Open Button Press Push Red Cup Push Blue Cup Push Green Cup Push Pink Cup Push Orange Cup Push Purple Cup Open Red Drawer Open Green Drawer Open Blue Drawer Open Yellow Drawer In-Domain Model Prompts Internet-Domain Model Prompts assembly dial turn reach peg unplug side lever pull coffee push door close window close window open drawer close drawer open button press red blue green pink orange purple red green blue yellow robot arm placing ring over peg robot arm turning dial robot arm reaching red sphere robot arm unplugging gray peg robot arm pulling lever robot arm pushing white cup towards coffee machine robot arm closing door robot arm closing window robot arm opening window robot arm closing drawer robot arm open drawer robot arm pushing button robot arm pushing the red cup robot arm pushing the blue cup robot arm pushing the green cup robot arm pushing the pink cup robot arm pushing the orange cup robot arm pushing the purple cup robot arm opening the red drawer robot arm opening the green drawer robot arm opening the blue drawer robot arm opening the yellow drawer Table A1: Task-Prompt Pairs. We include comprehensive list of tasks and their text prompts for in-domain training and evaluation. denotes tasks seen during initial training of the in-domain model. We also provide the prompts used to interface with the internet-pretrained text-to-video model during adaptation with IPA."
        },
        {
            "title": "B Implementation Details",
            "content": "We provide detailed architecture configurations of the models used in SAIL, and their relevant hyperparameter settings below. Inverse Dynamics: Following prior work [5], we design our inverse dynamics model as small MLP network built on top of pretrained pixel-based representation network. The IDM takes as input the embeddings of two video frames, which are extracted using VC-1 [36], and outputs prediction of the action that enables the transition between the provided frames. For the Panda arm experiments, the IDM is tasked with predicting the end effector position of the last frame provided. This is then executed in the physical environment through inverse kinematics. Furthermore, the two video frames have frameskip of 16; the frequency at which the camera is queried for trajectories is so high such that two temporally consecutive frames is not more substantially meaningful than just observing the last frame. For MetaWorld experiments, the two video frames are consecutive, and thus have frameskip of 1. The total parameter count of the IDM used in experimentation is 85.81M. Of these, 85.80M parameters are inherited from VC-1 whereas our IDM design contributes only an additional 10759 parameters due to the additional MLP on top. For fairness, we reuse the same IDM for all tasks within the same environments, and also do not perform any finetuning during the SAIL iterations with subsequently self-collected data. In such way, the IDM is trained on set of seen tasks, but applied to potentially novel task, even for those with novel visual settings (as in MetaWorld), without further modification. The subsequent success on such novel tasks therefore highlights not only the robustness of the IDMs learned, but also the visual quality of the synthesized visual plans. The detailed hyperparameters of IDM training are provided in Table A2. Hyperparameter Input Dimension Output Dimension (MetaWorld) Output Dimension (Panda) Training Epochs Learning Rate Optimizer Value 1536 4 7 20 1e-5 AdamW Table A2: Hyperparameters of Inverse Dynamics Model Training. We list the relevant hyperparameters of training the inverse dynamics model. In-Domain Model: We reuse the implementation of small-scale diffusion model that conditions on both natural language and an initial pixel frame from [3]. To improve text-conditioned capabilities of the model, we add an additional Cross-Attention layer to every level of the U-Net, which attends to the CLIP-encoded text prompt. Specifically, we instantiate UNet with 3 ResNet blocks for MetaWorld settings and 2 ResNet blocks for Panda arm tasks. We report the detailed list of model parameters in Table A3. In total, the in-domain model consists of 179.91M parameters for MetaWorld and 156.58M parameters for Real-World experiments. We perform initial in-domain training for 70K training steps on MetaWorld and 88K steps on Panda, with batch size of 8 and learning rate of 2e-5. In each SAIL iteration, we finetune the in-domain video model for 10K steps with with batch size of 4 and learning rate of 1e-5 on MetaWorld. On Panda Arm, we finetune for 8,000 steps with batch size of 8 and learning rate of 2e-5 on Cup Pushing and for 10,000 steps with batch size of 8 and learning rate of 1e-5 on Drawer Opening. All experiments are performed on single NVIDIA A6000 or RTX3090 GPU. Component # Parameters (Millions) U-Net (MetaWorld) 116.71 U-Net (Panda Arm) 93.38 Text Encoder (openai/clip-vit-base-patch32) 63.2 Table A3: In-Domain Model Components. SAIL relies on small in-domain text-to-video model, which we base our implementation off of prior work [3]. We list the size of the components of the model architecture used. Internet-Domain Model: Following Adapt2Act [5], we employ AnimateDiff [6] as the frozen internet-pretrained video model for inverse probabilistic adaptation. Additionally, we use SparseCtrl [37] to enable image-conditioned video generation. Model components and their parameter counts are listed in Table A4. In total, AnimateDiff consists of 2.005B parameters. Component # Parameters (Millions) VAE (Encoder) 34.16 VAE (Decoder) 49.49 U-Net Text Encoder ControlNet 1302.16 123.06 496.73 Table A4: AnimateDiff Components. SAIL relies on internet-scale text-to-video model; in this work we use AnimateDiff. We thus list the size of components of the AnimateDiff checkpoint used. The checkpoint is used purely for inference, and is not modified or updated in any way. Note that the VAE Decoder is not utilized in our framework. Visual Planning Hyperparameters: In visual planning, we predict 8 future frames conditioned on the current observation and task prompt. We follow [5] to perform DDIM [38] sampling for 25 steps to synthesize visual plans, in which the text-conditioning guidance scale is set to 2.5 for MetaWorld experiments and 7.0 for Panda Arm Pushing. We use 0.5 as the prior strength for inverse probabilistic adaptation. Choices of Control Loop: Visual planning provides the user control over the quality of execution against the speed. In our experiments, each visual plan consists of 9 frames, including one current observation and eight future frames, and can be translated into 8 actions. By performing open-loop control, we execute all 8 actions from single visual plan sequentially in the environment without any re-planning. While synthesizing visual plan can often involve multiple sampling steps and thus be time-consuming, open-loop control greatly improves the interaction efficiency. However, since open-loop control does not adjust the control actions based on the feedback from the environment, the subsequent actions from the plan might become suboptimal to the latest states and cause error accumulation. To mitigate this issue, closed-loop control adjusts the action for every interaction step. Specifically, we execute only the first action from the plan, and perform re-planning based on the new observation received from the environment. Although this control style allows us to interact most reliably, it incurs large computational overhead due to frequent re-planning. To balance the execution quality and efficiency, we can flexibly choose control loop between the two extremes of open-loop and closed-loop. For example, we execute half of the plan (e.g. 4 actions) before re-planning, which we reference as semi-open-loop control. To achieve the best execution speed, we employ open-loop control in Panda Arm Pushing and Drawer Opening tasks, in which we discover that visual plans can be performed decently, with negligible deviation in the real execution. For all MetaWorld experiments, we utilize semi-open-loop control to balance performance and efficiency. MetaWorld Task Performance Decomposition for Figure 2 In-Domain Only Iter. 1 Iter. Iter. 2 Iter. 0 SAIL (IPA) Iter. 1 Iter. 2 Door-Close* Drawer-Close Drawer-Open Window-Close Window-Open Button-Press Average 71.1 15.8 87.8 5.1 90.0 6.7 11.1 5.1 13.3 3.3 6.7 3.3 0.0 0.0 0.0 0.0 0.0 0.0 68.9 5.1 58.9 1.9 64.4 6.9 1.1 1.9 1.1 1.9 3.3 3.3 1.1 1.9 0.0 0.0 0.0 0.0 27.4 28.1 24. 64.4 3.8 27.8 7.7 0.0 0.0 52.2 13.9 1.1 1.9 1.1 1.9 24.4 90.0 3.3 92.2 1.9 43.3 14.5 37.8 9.6 0.0 0.0 0.0 0.0 73.3 5.8 67.8 6.9 3.3 0.0 2.2 1.9 0.0 0.0 0.0 0.0 34.4 33.9 Table A5: MetaWorld Task Performance. We provide detailed list of task performance for the leftmost plot in Figure 2. We report the mean success rate across 6 tasks, aggregated over 3 seeds each. Settings with improving behaviors are highlighted with shaded backgrounds. Compared to indomain only baselines, SAIL (IPA) enables continuous improvement on average task performance across iterations, and achieves the best overall success rate on Iteration 2."
        },
        {
            "title": "D Full MetaWorld Suboptimal Results",
            "content": "We evaluate SAIL on 6 MetaWorld tasks, 5 of which are unseen. In Section 4.4, 4 novel MetaWorld tasks with meaningful results are reported, and highlighted in Figure 5. We present the full results in Figure A1, where previously the tasks of Door Close and Window Open were omitted due to negligent performance improvements In addition, we report the detailed task performance aggregated across 3 seeds for both in-domain only and SAIL in Table A6. While improvement trends over iterations are not as consistent when no adaptation is used (shown as the leftmost graph of Figure A1 and Table A6), SAIL (IPA) demonstrates continuously improving behaviors on four unseen tasks and achieves the highest average task performance at Iteration 2, thus highlighting the effectiveness of self-adaptation. 15 Figure A1: SAIL results with suboptimal in-domain data without experience filtering (6 tasks). Crucially, as in the case of Drawer Open, we find that it is difficult for performance to improve when few successful trajectories can be collected. In such situations, as filtering is not applied, the model will continue to reinforce itself on mostly suboptimal trajectories, just as in the In-Domain Only case, and thus can hardly observe meaningful performance improvements. This is similar to the finding for Window Open, which increases slightly but not substantially, most likely due to lack of successful collected demonstrations to leverage from iteration to iteration. Nevertheless we discover that on average, as shown in the rightmost graph of Figure A1, the performance of SAIL across tasks meaningfully increases across iterations in comparison with in-domain only, even with suboptimal initial data. Iter. 0 In-Domain Only Iter. 1 Iter. 2 Iter. 0 SAIL (IPA) Iter. 1 Iter. Iter. 0 SAIL (PA) Iter. 1 Iter. 2 82.2 10.2 92.2 3.8 11.1 3.8 0.0 0.0 88.9 1.9 Door-Close* 16.7 3.3 18.9 10.2 Drawer-Close 0.0 0.0 1.1 1.9 Drawer-Open 44.4 9.6 Window-Close 58.9 11.7 43.3 8.8 2.2 3.8 5.6 1.9 Window-Open 2.2 1.9 0.0 0.0 Button-Press 26.1 26.5 Average 1.1 1.9 0.0 0.0 25. 93.3 0.0 64.4 6.9 1.1 1.9 93.3 3.3 66.7 10.0 0.0 0.0 90.0 3.3 96.7 5.8 97.8 3.8 46.7 8.8 53.3 3.3 55.6 6.9 0.0 0.0 1.1 1.9 0.0 0.0 44.4 6.9 47.8 10.2 56.7 11.5 76.7 11.5 70.0 5.8 61.1 5.1 0.0 0.0 0.0 0.0 1.1 1.9 0.0 0.0 35.6 33.0 85.6 5.1 32.2 1.9 0.0 0.0 1.1 1.9 0.0 0.0 34.6 1.1 1.9 0.0 0.0 32. 1.1 1.9 1.1 1.9 34.8 1.1 1.9 4.4 1.9 37.0 Table A6: Detailed Task Performance with Suboptimal Initial Data. We compare visual planning performance across iterations on in-domain only, SAIL (IPA) and additional SAIL (PA) setups. We report the mean success rate across 6 tasks, aggregated over 3 seeds each. Settings with improving behaviors are highlighted with shaded backgrounds. D.1 Probabilistic Adaptation IPA, as proposed by prior work [5], is built off of Probabilistic Adaptation (PA) [27]. In contrast to Equation 1, PA takes the following sampling form: ϵ = ϵθ(τt, t) + α (cid:16) ϵθ(τt, text) + γϵgeneral(τt, text) ϵθ(τt, t) (cid:17) (2) where the general text-to-video model serves as probabilistic knowledge prior that guides the generation process of the small in-domain model during sampling. natural question to consider is whether IPA is the best adaptation technique to facilitate self-improvement behaviors compared to other score-composition-based adaptation methods. We thus evaluate SAIL using PA as an alternative adaptation strategy, in comparison with utilizing no adaptation (In-Domain Only) and IPA. As shown in the rightmost columns of Table A6, Probabilistic Adaptation exhibits similar improving behaviors on several tasks and average task performance. Specifically, 3 out of 6 unseen tasks continuously improve through SAIL (PA), whereas its inverse enables improvements on 4 unseen tasks over iterations. Furthermore, SAIL (IPA) achieves higher task performance on average and the best success rate on the last iteration. Overall, we believe IPA serves as more robust adaptation technique, especially with suboptimal in-domain initialization, allowing more performant trajectories to 16 be collected through visual planning and subsequently facilitating improvements of the in-domain video model through SAIL."
        },
        {
            "title": "E Additional Plan Visualizations",
            "content": "We show additional visual plans for SAIL, across multiple environments and tasks, along with their execution results. E.1 SAIL with Experience Filtering Visual plans and their executions for SAIL with experience filtering are illustrated below. Figure A2: SAIL on Drawer Close with experience filtering. 17 Figure A3: SAIL on Window Close with experience filtering. Figure A4: SAIL on Orange Cup Pushing (Red/Pink/Orange) with experience filtering. 18 Figure A5: SAIL on Orange Cup Pushing (Red/Green/Orange) with experience filtering. Figure A6: SAIL on Purple Cup Pushing (Blue/Pink/Purple) with experience filtering. 19 Figure A7: SAIL on Purple Cup Pushing (Red/Green/Purple) with experience filtering. Figure A8: SAIL on Yellow Drawer Opening (Yellow/Green) with experience filtering. 20 Figure A9: SAIL on Yellow Drawer Opening (Yellow/Blue) with experience filtering. E.2 Filtering-free SAIL Visual plans and their executions for SAIL without experience filtering are illustrated below. Figure A10: SAIL on Drawer Close without experience filtering. 21 Figure A11: SAIL on Orange Cup Pushing (Blue/Pink/Orange) without experience filtering. Figure A12: SAIL on Window Close without experience filtering (w/ suboptimal data)."
        }
    ],
    "affiliations": [
        "Brown University",
        "Harvard University"
    ]
}