{
    "paper_title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation",
    "authors": [
        "Junyan Ye",
        "Dongzhi Jiang",
        "Zihao Wang",
        "Leqi Zhu",
        "Zhenghao Hu",
        "Zilong Huang",
        "Jun He",
        "Zhiyuan Yan",
        "Jinghua Yu",
        "Hongsheng Li",
        "Conghui He",
        "Weijia Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, a key question remains: given that real-world image datasets already constitute a natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for a more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and Imagine-Bench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability."
        },
        {
            "title": "Start",
            "content": "Echo-4o: Harnessing the Power of GPT-4o"
        },
        {
            "title": "Synthetic Images for Improved Image Generation",
            "content": "Junyan Ye1,2 *, Dongzhi Jiang3*, Zihao Wang2, Leqi Zhu1, Zhenghao Hu2, Zilong Huang2, Jun He2, Zhiyuan Yan4, Jinghua Yu2, Hongsheng Li3 , Conghui He1 , Weijia Li2 1Shanghai Artificial Intelligence Laboratory 2Sun Yat-sen University 3CUHK MMLab 4Peking University Github: Dataset: Gallery: https://github.com/yejy53/Echo-4o https://huggingface.co/datasets/Yejy53/Echo-4o-Image/ https://yejy53.github.io/Echo-4o"
        },
        {
            "title": "Abstract",
            "content": "Recently, GPT-4o has garnered significant attention for its strong performance in image generation, yet open-source models still lag behind. Several studies have explored distilling image data from GPT-4o to enhance open-source models, achieving notable progress. However, key question remains: given that realworld image datasets already constitute natural source of high-quality data, why should we use GPT-4o-generated synthetic data? In this work, we identify two key advantages of synthetic images. First, they can complement rare scenarios in real-world datasets, such as surreal fantasy or multi-reference image generation, which frequently occur in user queries. Second, they provide clean and controllable supervision. Real-world data often contains complex background noise and inherent misalignment between text descriptions and image content, whereas synthetic images offer pure backgrounds and long-tailed supervision signals, facilitating more accurate text-to-image alignment. Building on these insights, we introduce Echo-4o-Image, 180K-scale synthetic dataset generated by GPT-4o, harnessing the power of synthetic image data to address blind spots in real-world coverage. Using this dataset, we fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o. In addition, we propose two new evaluation benchmarks for more accurate and challenging assessment of image generation capabilities: GenEval++, which increases instruction complexity to mitigate score saturation, and ImagineBench, which focuses on evaluating both the understanding and generation of imaginative content. Echo-4o demonstrates strong performance across standard benchmarks such as GenEval, DPG, and OmniContext, as well as our proposed benchmarks. Moreover, applying Echo-4o-Image to other foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains across multiple metrics, highlighting the datasets strong transferability. 5 2 0 2 3 1 ] . [ 1 7 8 9 9 0 . 8 0 5 2 : r Figure 1: Performance gains of Echo-4o and transferability of Echo-4o-Image across models. Technical Report *Co-first Authors Corresponding authors"
        },
        {
            "title": "Introduction",
            "content": "Recently, unified multimodal generative models like GPT-4o [52] have attracted increasing attention, not only for their strong performance on the common generation tasks such as text-to-image generation and image editing, but also for its extraordinary understanding capability to perform free-form image manipulation [74]. Following this trend, series of open-source unified models, including BLIP3o [7], Bagel [15], and OmniGen2 [67], have been introduced. These models also showcase advanced proficiency in diverse multimodal generation tasks, highlighting the promising future of this research direction. Despite recent progress, current open-source unified models still exhibit significant gap in overall generation quality compared to GPT-4o [52], particularly in terms of instruction alignment, imaginative generation, and multi-reference image composition. Similar challenges have been observed in the text or image understanding tasks of large language models (LLMs) [47; 62; 46; 3] and multimodal large language models (MLLMs) [49; 44; 4]. promising approach to mitigate this gap involves leveraging stronger models to synthesize high-quality data for smaller models [13; 44; 10; 6]. For instance, DeepSeek-R1s [23] reasoning trajectories have proven highly effective in transferring advanced reasoning capabilities to smaller models, while ShareGPT-4V [10] utilizes GPT-4V to generate high-quality vision-language pairs. This strategy has recently been extended to the image generation domain. For example, BLIP3-o [7] incorporates around 10k GPT-4o [52]-generated image-text pairs during fine-tuning, leading to notable gains in instruction-following. ShareGPT-4o-Image [9] empowers Janus-Pro [12] with advanced text-to-image and image editing capabilities by leveraging synthetic data generated by GPT-4o [52]. However, unlike typical LLM reasoning tasks [26; 14; 57] or VQA tasks [1; 77; 45; 81; 33], which often lack existing high-quality textual data, image generation tasks naturally benefit from an abundant source of high-quality datareal-world images. Real-world images already represent the highestquality outputs of visual synthesis, containing complex low-level details such as texture, lighting, and structural consistency. In LLM tasks, stronger models can generate higher-quality textual supervision. In contrast, for image synthesis tasks, despite continuous improvements, even the most advanced models still fall short compared to real-world images. Therefore, simply using synthetic dataproduced by advanced models and similar to natural imagesholds limited significance. Therefore, natural question arises: Why is it necessary to train on GPT-4o-generated synthetic data? Figure 2: Illustration of the key advantages of synthetic images. architectures. 2 In this work, we identify and analyze two key advantages of synthetic images. First, they can complement rare scenarios that are underrepresented in real-world datasets, such as surreal fantasy scenes or multi-reference image generation. Second, synthetic images provide clear and controllable supervision. Synthetic images can provide data with pure backgrounds and long-tailed attribute combinations, thereby overcoming textimage misalignment and long-tail distribution issues in real-world images. As illustrated in Figure 2, we elaborate on the advantages as follows: (a) Surreal fantasy data. Fantastical or physically implausible contentsuch as train made of playing cardsis common in creative user prompts but absent from real-world image datasets. In contrast, advanced generative modelswith their strong understanding-for-generation capabilitiescan synthesize such fantastical content effectively. Weaker models can benefit from this type of synthetic data to directly learn imaginative generation beyond the constraints of real-world distributions. (b) Multi-reference Generation: Structured Multi-to-one image generation tasks are also lacking in natural datasets. In contrast, synthetic data allow for explicitly designed multi-input generation settings, offering greater diversity in instructions and richer reference information compared to typical data collected from video frames [67]. (c) Pure instruction alignment: Real-world images often contain cluttered backgrounds or irrelevant objects, while the accompanying text omits these details, causing visionlanguage misalignment and making it harder for models to learn accurate instruction following. In contrast, synthetic images provide clean, controlled supervisiontypically showing only the target object against simple background. For example, cabinet and Hello Kitty doll placed in front of plain wall create focused and unambiguous training instance, reducing alignment gap. (d) Controllable long-tail composition: Synthetic data allows precise control over attributes such as color, quantity, and spatial arrangement, enabling coverage of complex long-tail combinations. For example, eight red apples represents long-tail case compared to more frequent quantities like \"one\" or \"two\" apples. Despite its scarcity in the real world, such scenarios can be easily synthesized. This control enhances models ability to follow complex and corner-case instructions. Building upon these insights, we present Echo-4o-Image, new synthetic dataset generated using the advanced closed-source model GPT-4o. By harnessing the power of synthetic image data via the echoes of GPT-4o, this dataset aims to enhance current image generation models by addressing the inherent limitations of real-world image datasets. Echo-4o-Image includes 38K surreal fantasy samples, 73K multi-reference image generation samples, and 68K complex instruction-following samples, collectively enriching model capabilities in imagination, mulit-reference, and instruction alignment. Based on this dataset, we fine-tune the unified multimodal generation baseline Bagel [15] to develop new model, Echo-4o, which exhibits remarkable improvements on both text-to-image generation and multi-reference image synthesis tasks. To rigorously evaluate the models instruction-following and imaginative generation, we further introduce two novel benchmarks: GenEval++ and Imagine-Bench. GenEval++ incorporates an automated evaluator powered by GPT-4.1 and significantly increases the difficulty and compositional complexity of test instructions, addressing the limitations of scoring saturation and insufficient accuracy found in existing text-to-image evaluations. Imagine-Bench focuses on imaginative generation, offering comprehensive evaluation of conceptual creativity and visual consistency across three dimensions: fantasy fulfillment, identity preservation, and aesthetic quality. We conduct extensive experiments on Echo-4o to demonstrate the effectiveness of Echo-4o-Image. As shown in Figure 1(a), Echo-4o achieves notable gains on mainstream benchmarks, especially in following complex instructions and generating high-quality imaginative images. It also unlocks multi-reference generation, excelling in multi-image fusion and visual consistency. Figure 1(b) further shows that applying the Echo-4o-Image dataset to other foundation models (e.g., OmniGen2, BLIP3-o) consistently improves performance across evaluation dimensions, highlighting the datasets generalizability and transferability. Our main contributions are summarized as follows: We analyze and summarize the advantages of synthetic data over real-world images, highlighting its ability to generate rare scenarios and to provide pure, long-tailed supervision for instruction-following tasks. We curate Echo-4o-Image, synthetic dataset of 180K samples generated using GPT-4o, covering surreal scenes, multi-reference generation, and instruction-following tasks. 3 We fine-tune the Bagel model on Echo-4o-Image, yielding the unified generative model Echo4o, which achieves state-of-the-art performance across multiple benchmarks. Furthermore, Echo-4o-Image consistently enhances other backbone models such as OmniGen2 and BLIP3o, demonstrating strong transferability. We propose two new evaluation benchmarks: GenEval++ increases instruction complexity to alleviate score saturation in text-to-image evaluation. Imagine-Bench targets fantasy tasks and is designed to assess both understanding and generation of imaginative content."
        },
        {
            "title": "2 Echo-4o-Imgae",
            "content": "We first introduce Echo-4o-Image, large-scale synthetic dataset distilled from GPT-4o. As illustrated in Figure 3, the dataset contains approximately 179,000 samples spanning three distinct task types: 38K surreal fantasy generation tasks, 73K multi-reference image generation tasks, and 68K complex instruction execution tasks. Notably, the surreal fantasy and multi-reference generation subsets consist of rare and underrepresented data in existing resources. In the following sections, we provide detailed illustration of the dataset construction pipeline and strategies. 2.1 Surreal Fantasy Image Generation We curate specialized subset of text-to-image data focused on surreal fantasy content. The prompts in this subset involve irregular modification of the conventional attributes, times, or spaces of an object. While this data type represents substantial portion of user requests, it is seldom encountered in real-world training data. As shown in Figure 3(a), we design structured pipeline for constructing the fantasy generation subset of Echo-4o-Image. We begin by collecting set of common object concepts from the COCO and Open Images datasets, which serve as the primary subjects for subsequent generation. Given an object, GPT-4o first performs identity attribute construction, outlining its canonical properties such as color, shape, or size. It then conducts conceptual deformation, creatively altering and recombining these attributes to introduce novel and imaginative features. These deformations fall into three primary categories: Attribute shift - altering conventional object features, such as color, shape, or size (e.g., white banana, cube-shaped soccer ball, or palm-sized giraffe). Hybridization redefining the material of an object (e.g., tomato made of crystal) or combining disparate objects (e.g., house made of bananas). Spatiotemporal anomaly placing objects in impossible settings (e.g., train in the clouds) or merging eras (e.g., ancient artifacts with futuristic technology). In addition to single-object prompts, we further extend to Multi-object fantasy compositions, in which GPT-4o generates surreal instructions involving interactions between multiple entities. The resulting imaginative prompts are then passed to GPT-4o for final image generation, yielding visually coherent and creatively rich samples. 2.2 Multi-Reference Image Generation Multi-reference image generation takes several reference images as input, along with text prompt. The prompt specifies which elements to extract from each image and how to combine them into coherent output. This task requires strong understanding capabilities to interpret complex prompts. It also demands exceptional generation abilities to preserve distinctive features from each reference. These features must be seamlessly integrated into the output image. Like surreal fantasy generation, limited ready-to-use training data exists for this challenging task. As shown in Figure 3(b), for the Multi-reference image generation task, we design reference combinations involving people, objects, and scenes, with each sample containing two to four input images. We first curate diverse collection of reference images covering wide range of categories, including portraits, street photography, animals, objects, clothing and accessories, natural landscapes, famous landmarks, and indoor scenes. This diversity in input references provides strong foundation for generating outputs that are both creatively rich and highly varied. Figure 3: Overview and construction pipeline of the Echo-4o-Image dataset. Using GPT-4o, we generate instructions conditioned on these reference images. Each instruction is tailored to specific interaction types (e.g., HumanObject, ObjectScene), and explicitly references the image indices (e.g., Image_1, Image_2) to reduce ambiguity and improve alignment between the instruction and the visual inputs. After generating the target image with GPT-4o, we further refine the original instruction through rewriting strategies aimed at improving linguistic diversity and semantic clarity. During this process, explicit references such as Image_1 or Image_2 may be replaced with specific descriptions of the corresponding people or objects depicted. This enhances both the quality of training data and the models generalization ability in multi-image generation tasks. 2.3 Instruction-Following Data Generation For the text-to-image instruction-following task, we synthesize data by introducing more complex multi-object and multi-attribute instructions. As shown in Figure 3(c), we begin with carefully curated set of base object concepts and adopt template-driven generation strategy to systematically construct prompts involving attributes such as color, position, count, and size, thereby producing semantically rich and diverse instructions. Subsequently, we employ GPT-4o to generate images. We summarize two key advantages of our synthesized data over real-world data: Pure instruction alignment. Compared to real-world images, synthetic data generated by GPT-4o typically features cleaner backgrounds and the absence of irrelevant objects. For example, when generating an image of violin alongside two pairs of chopsticks, these objects are rendered directly against neat and uncluttered background. Remarkably, GPT-4o can achieve this even without any additional prompt constraints. Such visually clean images facilitate clearer conceptual mappings between the objects described in the prompt and their corresponding visual representations, thereby reducing the learning difficulty in instruction-following tasks. Controllable long-tail composition. Compared to Geneval, which primarily contains limited set of semantic concepts, our dataset substantially increases instruction complexity. For instance, one of the most challenging prompts in GenEval may contain only four semantic unitse.g., an orange television and green bow. In contrast, our dataset includes prompts such as an orange television, green bow, and yellow screwdriver or belt, plate, and three table tennis paddles. The increase in both the number of objects and their attributes results in more intricate attribute combinations, thereby addressing the scarcity of such long-tailed data in real-world image distributions. Although we employ the state-of-the-art GPT-4o model for image generation, alignment errors may still occuri.e., the generated image may not fully match the original prompt. To address this issue, we introduce text rewriting strategy to ensure data usability. The core principle is: There are no invalid images, only invalid text. When misalignment is detected, we revise the original text based on the generated image to ensure that each imageinstruction pair forms semantically valid and consistent training sample. For example, in Figure 4, if the generated image contains three watches but the text specifies four, the description is corrected to three. Rather than discarding misaligned samples, we rewrite the text, allowing us to fully leverage the valuable GPT-4o synthetic data."
        },
        {
            "title": "3 Echo-4o",
            "content": "To validate the effectiveness of our curated Echo-4o-Image dataset, we fine-tune Bagel [15], strong baseline model, to obtain Echo-4oa unified multimodal generative model that excels in both text-to-image and multi-reference generation tasks. Bagel is unified multimodal generative model capable of both image understanding and generation. The model supports text-to-image generation and single image-to-image generation tasks, including image editing and free-form manipulation. Architecturally, Bagel employs ViT [17; 78] for image understanding and VAE [34] for image generation, utilizing mixture of transformers approach where one expert processes VAE tokens while another handles all other tokens. For multi-reference tasks, both ViT and VAE feature of the image is input to the model. However, although multi-image input is architecturally feasible, Bagel demonstrates poor performance on multi-reference generation tasks. We fine-tune Bagel using all the text-to-image and multi-reference data from Echo-4o-Image. The training objective uses flow matching loss calculated exclusively on the output image. We train all the model components except the VAE for 24,000 steps with learning rate of 2e-5. Through this fine-tuning process, Echo-4o achieves exceptional performance on multi-reference generation while further enhancing text-to-image capabilities, as detailed in Section 5. We deliberately select Bagel as our baseline due to its training on trillions of tokens of interleaved multimodal data. The fact that Echo-4o-Image still yields significant improvements over this extensively trained model demonstrates the complementary value of carefully designed synthetic data."
        },
        {
            "title": "4 GenEval++ & Imagine-Bench",
            "content": "4.1 Instruction-Following Evaluation GenEval++ Previous instruction-following benchmarks, such as GenEval [20], have been widely adopted to evaluate the capability of image generation models to follow textual instructions. However, these benchmarks typically rely on object detectors or CLIP-based models for automatic scoring, both of which exhibit substantial limitations in accuracy. As illustrated in Figure 4, within GenEval, when evaluating prompts such as green hotdog, the detector frequently produces incorrect judgmentsdespite the generated image being visually correctdue to the weak association between hotdog (a type of food) and the color green. Similarly, occlusions between objects can hinder accurate counting, resulting in false negatives. Moreover, the textual instructions in existing benchmarks are relatively simple and involve limited semantic diversity. Consequently, current models often achieve scores in the range of 0.80.9, indicating metric saturation and thus constraining the discriminative power of these benchmarks. To address these limitations, we introduce GenEval++, more accurate and challenging benchmark for evaluating instruction fidelity in image generation. As illustrated in Figure 4, GenEval++ employs the GPT-4.1 [51] multimodal model as the evaluator, leveraging its strong capability to understand over complex semantic compositions to assess the consistency between generated images and textual instructions. Following predefined checklist covering multiple criteriaObject, Counts, Color, Position, and Sizethe evaluator only marks result as correct if all conditions are satisfied. Furthermore, the benchmark covers seven task types involving different attribute combinations, each comprising 40 high-complexity prompts, for total of 280 textual instructions. GenEval++ features richer semantics and more diverse compositions, resulting in task difficulty that is significantly higher than that of the original GenEval. Additionally, to align with the prompt style \"A photo of\", outputs rendered in anime style or containing multiple disjoint elements are regarded as invalid. 4.2 Surreal & Fantasy Evaluation Imagine-Bench Moreover, existing evaluation protocols predominantly focus on real-world generation tasks. However, the true value of generative models lies not only in reproducing reality but also in creating the unknown, which aligns with substantial portion of user-driven creative instructions. To this end, we introduce new benchmark, Imagine-Bench, designed to evaluate models capability in surreal and imaginative image generation. The primary tasks in Imagine-Bench involve augmenting common objects with fantastical elements while preserving their core identity features. For example, 6 Figure 4: Overview of the Proposed Benchmarks: GenEval++ and Imagine-Bench. the instruction square soccer ball requires the model to alter the shape to cube while retaining the standard black-and-white soccer texture. Such tasks present rigorous challenge for current understandinggeneration unified models, as they demand breaking entrenched associations between concepts and appearances to enable genuine creative synthesis. Imagine-Bench comprises 270 diverse creative instructions encompassing wide range of surreal attributes. During evaluation, each instruction is first processed by GPT-4o to generate corresponding checklist containing detailed explanations and expected outcomes, including the required fantastical modifications and description of the objects invariant identity features. Given the prompt and the generated image, GPT-4.1 assigns scores along three dimensions: (1) Fantasy Fulfillment whether the generated image faithfully realizes the surreal aspects of the prompt; (2) Identity Preservation whether the transformed object retains the essential visual characteristics of its original identity; (3) Aesthetic Quality the visual appeal, creativity, and diversity of the generated image. Inspired by VIEScore [35] and the evaluation protocol of OmniGen2, GPT-4.1 scores each dimension on 010 and provides explicit reasoning for each score, ensuring both rigor and interpretability in the evaluation. We further adopt more stringent scoring scheme, where the final score is computed as 0.8 min(Fantasy Fulfillment, Identity Preservation) + 0.2 Aesthetic Quality."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we present comprehensive evaluation of Echo-4o, focusing on its performance across diverse set of generation tasks, including instruction-following image generation, surreal/fantasy image synthesis, and multi-reference image generation. The results demonstrate that Echo-4o consistently achieves strong performance across these tasks, highlighting the effectiveness of the Echo-4o-Image synthetic dataset in enhancing generative model capabilities. 5.1 Instruction-Following Image Generation We evaluate the instruction-following capability of our model on two widely used benchmarksGenEval and DPG-Bench, as well as our newly proposed benchmark, GenEval++. As shown in Table 1, Echo-4o achieves score of 0.89 on GenEval, outperforming prior state-of-the-art unified models such as Bagel and OmniGen2. On DPG-Bench  (Table 2)  , Echo-4o attains an overall score of 86.07, outperforming strong competitors including SD3 and UniWorld. These results demonstrate that Echo-4o delivers consistently superior performance across benchmarks of different types, indicating its strong instruction-following generation capabilities or both short-text and complex long-text instructions. In existing text-to-image instruction-following tasks, Echo-4o achieves consistently superior performance. Compared to the baseline model Bagel, Echo-4o delivers an 8.5% improvement on GenEval, Method Single object Two object Counting Colors Position Color attribution Overall SDv2.1 [59] SDXL [54] IF-XL LUMINA-Next [85] SD3-medium [2] FLUX.1-dev [36] OmniGen [70] TokenFlow-XL [55] Janus [66] Janus Pro [12] Emu3-Gen [63] Show-o [71] MetaQuery-XL [53] BLIP3-o 8B [7] UniWorld-V1 [40] OmniGen2 [67] GPT-4o-Image BAGEL [15] Echo-4o 0.98 0.98 0.97 0.92 0.99 0.99 0. 0.95 0.97 0.99 0.98 0.98 - - 0.99 1 0.99 0.99 1 0.5 0.74 0.74 0.46 0.94 0.81 0.84 0.60 0.68 0.89 0.71 0.80 - - 0.93 0.95 0.92 0.94 0.97 0.44 0.39 0.66 0.48 0.72 0.79 0.66 0.41 0.30 0.59 0.34 0.66 - - 0.79 0.64 0.85 0.81 0.83 0.85 0.85 0.81 0.70 0.89 0.74 0. 0.81 0.84 0.90 0.81 0.84 - - 0.89 0.88 0.92 0.88 0.95 0.07 0.15 0.13 0.09 0.33 0.20 0.40 0.16 0.46 0.79 0.17 0.31 - - 0.49 0.55 0.75 0.64 0.86 0.17 0.23 0.35 0.13 0.60 0.47 0.43 0.24 0.42 0.66 0.21 0.50 - - 0.70 0.76 0.61 0.63 0.77 0.50 0.55 0.61 0.46 0.74 0.67 0. 0.55 0.61 0.80 0.54 0.68 0.80 0.84 0.80 0.80 0.84 0.82 0.89 Table 1: Evaluation of text-to-image generation ability on GenEval [21] benchmark. Results of GPT-4o-Image are tested by [74]. Method Global Entity Attribute Relation Other Overall SDXL [54] Hunyuan-DiT [39] DALLE3 [50] SD3-medium [2] FLUX.1-dev [36] OmniGen [70] Show-o [71] EMU3 [63] TokenFlow-XL [55] Janus Pro [12] T2I-R1 [32] BLIP3-o 4B [7] BLIP3-o 8B [7] UniWorld-V1 [40] OmniGen2 [67] BAGEL [15] Echo-4o 83.27 84.59 90.97 87.90 82.1 87.90 79.33 85.21 78.72 86.90 91.79 - - 83.64 88.81 88.94 94.85 82.43 80.59 89.61 91.01 89.5 88.97 75.44 86.68 79.22 88.90 90.23 - - 88.39 88.83 90.37 91.81 80.91 88.01 88.39 88.83 88.7 88.47 78.02 86.84 81.29 89.40 89.05 - - 88.44 90.18 91.29 89. 86.76 74.36 90.58 80.70 91.1 87.95 84.45 90.22 85.22 89.32 90.13 - - 89.27 89.37 90.82 91.22 80.41 86.41 89.83 88.68 89.4 83.56 60.80 83.15 71.20 89.48 89.48 - - 87.22 90.27 88.67 89.43 74.65 78.87 83.50 84.08 84.0 81.16 67.27 80.60 73.38 84.19 84.76 79.36 81.60 81.38 83.57 85.07 86. Table 2: Evaluation of text-to-image generation ability on DPG-Bench [27] benchmark. validating the effectiveness of the pure synthetic data in the Echo-4o-Image dataset for enhancing instruction-following capabilities. These clean-background samples enhance the models ability to learn precise alignments between images and textual instructions. Although the training data is predominantly composed of short-text instructions, the model also demonstrates strong generalization to complex long-text generation tasks, as evidenced by its performance on DPG-Bench. Furthermore, on the newly proposed and more challenging GenEval++ benchmark, most models perform poorly, with scores falling below 0.4. Although GenEval++ tasks differ from those in GenEval primarily by adding only one or two additional objects and their attributes, this seemingly minor increase substantially raises task difficulty. For example, generating five hot dogs is significantly more challenging than generating four. Early diffusion-based models, such as SDXL, almost completely fail to follow instructions in such scenarios. Even advanced unified models like Bagel and OmniGen2, which show only small gap to GPT-4o on GenEval, fall far behind on these harder tasks. Echo-4o achieves the best performance among all models except GPT-4o, surpassing OmniGen2 and Bagel by over 40%. This highlights Echo-4os strong instruction-following capability, which is closely related to the inclusion of more complex, long-tailed attribute data in the Echo-4o-Image. Figure 5 further provides qualitative comparisons across different models. 8 Method Color Count Color/Count Color/Pos Pos/Count Pos/Size Multi-Count Overall SDv2.1 [59] SDXL [54] SD3-medium [2] FLUX.1-Kontext [36] FLUX.1-dev [36] GPT-4o [52] Janus-Pro [12] T2I-R1 [32] BLIP3-o 4B [7] BLIP3-o 8B [7] OmniGen2 [67] Bagel [15] Echo-4o 0.000 0.050 0.550 0.425 0. 0.900 0.450 0.675 0.125 0.250 0.550 0.325 0.800 0.325 0.375 0.500 0.500 0.625 0.675 0.300 0.325 0.225 0.250 0.425 0.600 0.575 0.025 0.000 0.125 0.200 0.150 0.725 0.125 0.200 0.100 0.125 0.200 0.250 0.550 0.000 0.000 0.350 0.250 0. 0.625 0.300 0.350 0.450 0.600 0.275 0.325 0.775 0.000 0.000 0.175 0.300 0.200 0.600 0.075 0.075 0.125 0.125 0.125 0.250 0.625 0.025 0.000 0.150 0.400 0.375 0.800 0.350 0.250 0.550 0.575 0.250 0.475 0.800 0.075 0.000 0.225 0.325 0. 0.850 0.125 0.300 0.225 0.225 0.450 0.375 0.625 0.064 0.061 0.296 0.343 0.314 0.739 0.246 0.311 0.257 0.307 0.325 0.371 0.679 Table 3: Evaluation of instruction following generation ability on Geneval++. bold indicates the best result, and underlined denotes the second best. Figure 5: Qualitative comparison of different methods on GenEval++. 5.2 Surreal Fantasy Image Generation We evaluate multiple models on the Imagine-Bench benchmark to assess their understanding and creative capabilities, with the results presented in Table 4. Conventional image generation models perform poorly on this task, primarily due to their training paradigm, which often establishes direct binding between textual concepts and visual representations. Constrained by limited comprehension ability, these models struggle to differentiate the inherent concepts of existing objects from the additional requirements specified in fantasy-oriented instructions. Unified models, such as BLIP3o and OmniGen2, benefit from stronger comprehension capabilities and achieve slightly better results. 9 Among open-source models, Echo-4o attains the best performance, directly benefiting from the inclusion of fantasy-oriented image data in Echo-4o-Image, which extends beyond the domain constraints of real-world images and thereby improves performance in relatively straightforward manner. Future work could explore more systematic approaches to further enhance unified models capabilities in both understanding and creative generation. Method Attribute shift Spatiotemporal Hybridization Multi-Object Overall SDv2.1 [59] SDXL [54] SDv3-medium [2] FLUX.1-Kontext [36] FLUX.1-dev [36] GPT-4o [52] Janus-Pro [12] T2I-R1 [32] BLIP3-o 4B [7] BLIP3-o 8B [7] OmniGen2 [67] Bagel [15] Echo-4o 4.46 4.42 5.14 5.33 5.68 8.54 5.30 5.85 5.48 5.80 5.28 5.37 7.18 5.06 6.32 5.91 6.49 7.13 9.18 7.28 7.70 6.79 7.08 7.45 6.93 8. 4.12 4.93 6.30 5.48 6.38 8.57 6.73 7.36 6.93 7.06 6.29 6.50 7.52 3.49 4.50 6.07 5.34 5.24 7.98 6.04 6.68 6.09 6.44 6.31 6.41 8.06 4.30 4.97 5.78 5.62 6.06 8.56 6.22 6.78 6.23 6.51 6.22 6.20 7. Table 4: Evaluation of surreal and imaginative image generation ability on Imagine-Bench. bold indicates the best result, and underlined denotes the second best. Figure 6: Qualitative comparison of different methods on Imagine-Bench. 10 Method MULTIPLE SCENE Average Character Object Char. + Obj. Character Object Char. + Obj. Gemini-2.0-flash [22] GPT-4o [52] UNO [68] BAGEL [15] OmniGen [70] OmniGen2 [67] Echo-4o 2.91 9.07 2.54 5.17 5.65 7.11 8.07 2.16 8. 6.51 6.64 5.44 7.13 7.50 3.80 8.54 4.39 6.24 4.68 7.45 8.29 3.02 8.90 2.06 4.07 3.59 6.38 8.62 3.89 8. 4.33 5.71 4.32 6.71 8.00 2.92 8.60 4.37 5.47 5.12 7.04 8.08 3.12 8.75 4.03 5.55 4.8 6.97 8.09 Table 5: Overall score comparison of existing image generation models on OmniContext [67] benchmark (excluding SINGLE category). \"Char. + Obj.\" indicates Character + Object. As illustrated in Figure 6, some existing models either fail to respond to fantasy instructions entirely or respond only partially. For instance, the shape of pineapple may remain unchanged, or the Golden Gate Bridge may retain its original red color. In contrast, Echo-4o produces results that align more closely with the intended instructions, preserving the intrinsic characteristics of the original objects while achieving higher visual aesthetics compared to models such as BLIP3o. 5.3 Multi-Reference Image Generation We evaluate multi-reference image generation using the OminiContext [67] benchmark. This capability remains largely underexplored in existing image generation models and unified architectures. Among open-source models, only OmniGen2 has made initial attempts in this direction; most models, including FLUX and Bagel, either lack native support for this functionality or are entirely incompatible with multi-reference scenarios. By leveraging synthetic data designed for multi-reference scenarios, Echo-4o obtains this capability absent in the base Bagel architecture. As shown in Table 5, Echo-4o achieves the best performance among all open-source models under both MULTIPLE and SCENE settings, representing substantial improvement over the Bagel baseline and surpassing the previous best open-source model, OmniGen2. Figure 7 further provides qualitative comparisons, highlighting Echo-4os advantages in multi-reference generation with two or three reference images. In terms of both instruction adherence and fidelity to the content of the reference images, Echo-4o consistently outperforms OmniGen2 [67]. Figure 7: Qualitative comparison of different methods on OmniContext. 5.4 Effectiveness Across Base Models To further validate the broad effectiveness of the Echo-4o-Image dataset, we conduct additional experiments by fine-tuning several existing unified models using the synthetic data. As shown in Figure 1(b), models including BLIP-3-o, Bagel, and OmniGen2 all benefit from Echo-4o-Image, exhibiting consistent improvements across benchmarks such as GenEval, GenEval++, DPG-Bench, and OminiContext. These results demonstrate that Echo-4o-Image provides generalizable enhancement to diverse base models, significantly improving their capabilities in instruction understanding, fantasy image synthesis, and multi-reference image generation. The consistent gains across tasks and architectures confirm the datasets broad applicability and its important value in high-quality fine-tuning of unified multimodal generation models. 5.5 Comparison with ShareGPT-4o-Image We further compare our dataset with ShareGPT-4o-Image, another synthetic dataset distilled from GPT-4o. Both datasets are used to fine-tune the same baseline model, Bagel, under identical training settings until convergence. Figure 8 reports the performance comparison on GenEval and GenEval++. show that Echo-4oThe results Image yields substantial improvements in instruction-following capabilities, whereas ShareGPT-4o-Image exhibits only marginal gains in this aspect. After training with ShareGPT4o-Image, the performance of Bagel on GenEval improves from 0.820 to 0.838, whereas training with Echo-4oImage raises the score to 0.895. similar trend is observed on GenEval++. Figure 8 also presents qualitative comparison of the results. This difference may be attributed to the fact that considerable portion of ShareGPT-4o-Images data is generated from textual inputs in ALLaVA [6], which already contains high-quality real-world image pairs. Consequently, ShareGPT-4o-Image essentially regenerates images similar to those in real-world datasets, leading to limited additional benefits for instruction following.Nonetheless, ShareGPT-4o-Image still contributes to improving aesthetic alignment. Notably, ShareGPT-4o-Image lacks multi-reference image generation dataa capability that Echo-4o-Image successfully enablesfurther underscoring its broader utility. Figure 8: Comparison of Echo-4o-Image and ShareGPT-4oImage on GenEval and GenEval++."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present Echo-4o-Image, large-scale synthetic dataset generated by GPT-4o, and demonstrate its effectiveness in enhancing unified multimodal generation models through the development of Echo-4o. Alongside, we introduce two new benchmarks, Geneval++ and ImagineBench, to provide more comprehensive and challenging evaluation of image generation capabilities. The results highlight the value of the high-quality synthetic dataset Echo-4o-Image in addressing the limitations of real-world datasets and transferring effectively across different foundation models. We hope that the open-sourcing of Echo-4o-Image will advance unified multimodal generation models by complementing real-world image gaps with high-quality synthetic data, enhancing instruction following, creative generation, and multi-reference image synthesis capabilities. In future work, we plan to extend the dataset to cover image editing tasksanother scenario with limited high-quality 12 real-world data, and to fine-tune broader range of models, such as FLUX, to further validate its versatility and impact."
        },
        {
            "title": "7 Related Works",
            "content": "7.1 Image Generation Models Diffusion models have achieved remarkable success in the field of generative modeling, particularly in high-fidelity image synthesis [18; 8; 76; 5; 38; 80; 30; 86; 16]. Representative systems such as the Stable Diffusion (SD) series [58; 54; 18] and DALLE [56] have demonstrated strong text-to-image generation capabilities. More recently, research efforts have increasingly focused on multimodal generative models that aim to unify the understanding and generation of cross-modal content within single architecture. For example, hybrid models like Show-o [71] and Transfusion [83] integrate autoregressive text generation with discrete or continuous diffusion for images in single Transformer framework. MetaQueries [53] employs learnable queries to establish an efficient interface between frozen MLLM and diffusion model, enabling knowledge-enhanced image generation while preserving comprehension performance. BLIP3-o [7] leverages diffusion transformer to produce semantically rich CLIP image features and adopts sequential pre-training strategy to jointly optimize understanding and generation. Models such as Bagel [15] and OmniGen2 [67] undergo largescale pretraining and exhibit strong generalization capabilities across diverse downstream tasks. Furthermore, several studies [25; 32; 61] have introduced Chain-of-Thought (CoT) reasoning into image generation. For instance, T2I-R1 [32] introduces dual-level CoT to enhance generation quality and instruction-following ability. 7.2 Synthetic Dataset Leveraging synthetic data generated by larger, more capable models has emerged as highly effective approach for enhancing the performance of weaker models. This methodology has been extensively explored across text and multimodal understanding tasks in LLMs [47; 48; 62; 3] and MLLMs [49; 43; 84; 31; 87]. Building upon ShareGPT, dataset containing real conversations between users and ChatGPT, Vicuna [13] demonstrates superior performance in generating detailed, contextually appropriate responses. For image understanding tasks, LLaVA [43] pioneered this approach by leveraging detection dataset annotations [41] to prompt ChatGPT [47] for generating detailed image captions, significantly improving multimodal understanding. This methodology was further advanced by ShareGPT-4V [10], which employed GPT-4V to produce 100K high-quality image captions for training purposes. The synthetic data paradigm has since been successfully adapted across diverse domains, including mathematical problem solving [82], video captioning [79; 11], and 3D point cloud understanding [73; 24]. Employing synthetic images for training has also been explored in image generation tasks. JourneyDB [60] collects 4M high-quality images from various text-to-image models, demonstrating the value of synthetic visual content. More recently, the introduction of GPT-4os image generation capabilities has opened new possibilities. ShareGPT-4o-Image [9] and GPT-Image-Edit [64] collect text-to-image and image editing datasets from GPT-4o. However, these works exhibit limitations in their prompt design and task formulation, potentially failing to fully leverage the powerful generative capabilities of GPT-4o for creating highly effective synthetic data. 7. Image Generation Benchmark Early research on image generation evaluation primarily focused on assessing visual quality, with widely used metrics including FID, Inception Score (IS), and Kernel Inception Distance (KID). In recent years, driven by rapid advances in generative technology, the scope of evaluation has expanded from low-level quality metrics to measuring instruction-following capabilities. Methods such as VQAScore [42], HPSv2 [69], and VisionReward [72] leverage learned reward models to better align with human preferences. Meanwhile, benchmarks like CompBench++[29], GenEval[19], and GenAI Bench [37] combine CLIP-based metrics with structured prompt sets to evaluate multiple compositional dimensions, such as object attributes, spatial relationships, and numerical reasoning. With the advent of MLLM-based evaluation approaches such as VIEScore [35], more studies have begun to utilize large multimodal language models to assess instruction-following performance. For example, DPG-Bench [27] incorporates complex long-form instructions and employs mPLUGlarge [28] for evaluation; TIIF-Bench [65] and OmniContext [67] leverage advanced GPT-series 13 MLLMs to achieve notable improvements in both accuracy and interpretability. However, certain existing methods (e.g., GenEval) may introduce evaluation biases due to performance limitations in their underlying detection and CLIP models. At the same time, systematic evaluation of imagination and creative generation capabilities remains underexplored, with only few preliminary efforts, such as RF-Bench [75], addressing this direction."
        },
        {
            "title": "References",
            "content": "[1] Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra. Vqa: Visual question answering. International Journal of Computer Vision, 123:4 31, 2015. [2] Stability AI. Sd3-medium. stable-diffusion-3-medium, 2024. https://stability.ai/news/ [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023. URL https://arxiv.org/abs/2309.16609. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. [6] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4vsynthesized data for lite vision-language model, 2024. [7] Jiuhai Chen, Zhiyang Xu, Xichen Pan, Yushi Hu, Can Qin, Tom Goldstein, Lifu Huang, Tianyi Zhou, Saining Xie, Silvio Savarese, et al. Blip3-o: family of fully open unified multimodal models-architecture, training and dataset. arXiv preprint arXiv:2505.09568, 2025. [8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [9] Junying Chen, Zhenyang Cai, Pengcheng Chen, Shunian Chen, Ke Ji, Xidong Wang, Yunjin Yang, and Benyou Wang. Sharegpt-4o-image: Aligning multimodal models with gpt-4o-level image generation. arXiv preprint arXiv:2506.18095, 2025. [10] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. [11] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. Advances in Neural Information Processing Systems, 37: 1947219495, 2024. [12] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. 14 [13] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. https://lmsys.org/ blog/2023-03-30-vicuna/, March 2023. [14] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [15] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [16] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [19] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment, 2023. URL https://arxiv.org/abs/2310.11513. [20] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [21] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36, 2024. [22] Google. Gemini 2.0 flash. https://developers.googleblog.com/en/ experiment-with-gemini-20-flash-native-image-generation, 2025. [23] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [24] Zilu Guo, Hongbin Lin, Zhihao Yuan, Chaoda Zheng, Pengshuo Qiu, Dongzhi Jiang, Renrui Zhang, Chun-Mei Feng, and Zhen Li. Pisa: self-augmented data engine and training strategy for 3d understanding with large models. arXiv preprint arXiv:2503.10529, 2025. [25] Ziyu Guo, Renrui Zhang, Chengzhuo Tong, Zhizheng Zhao, Peng Gao, Hongsheng Li, and Pheng-Ann Heng. Can we generate images with cot? lets verify and reinforce image generation step by step. arXiv preprint arXiv:2501.13926, 2025. [26] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [27] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [28] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2040620417, 2023. 15 [29] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2icompbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation, 2025. URL https://arxiv.org/abs/2307.06350. [30] Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen, Zhuofan Zong, Yu Liu, and Hongsheng Li. Comat: Aligning text-to-image diffusion model with image-to-text concept matching. arXiv preprint arXiv:2404.03653, 2024. [31] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanmin Wu, Jiayi Lei, Pengshuo Qiu, Pan Lu, Zehui Chen, Chaoyou Fu, Guanglu Song, et al. Mmsearch: Benchmarking the potential of large models as multi-modal search engines. arXiv preprint arXiv:2409.12959, 2024. [32] Dongzhi Jiang, Ziyu Guo, Renrui Zhang, Zhuofan Zong, Hao Li, Le Zhuo, Shilin Yan, PhengAnn Heng, and Hongsheng Li. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703, 2025. [33] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, et al. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. arXiv preprint arXiv:2502.09621, 2025. [34] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [35] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for conditional image synthesis evaluation, 2023. [36] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [37] Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, and Deva Ramanan. Genai-bench: Evaluating and improving compositional text-to-visual generation, 2024. URL https://arxiv.org/abs/ 2406.13743. [38] Weijia Li, Jun He, Junyan Ye, Huaping Zhong, Zhimeng Zheng, Zilong Huang, Dahua Lin, and Conghui He. Crossviewdiff: cross-view diffusion model for satellite-to-street view synthesis. arXiv preprint arXiv:2408.14765, 2024. [39] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. [40] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. [41] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. [42] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation, 2024. URL https://arxiv.org/abs/2404.01291. [43] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [44] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [45] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. 16 [46] Meta. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [47] OpenAI. Chatgpt. https://chat.openai.com, 2023. [48] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. [49] OpenAI. GPT-4V(ision) system card, 2023. URL https://openai.com/research/ gpt-4v-system-card. [50] OpenAI. Dalle 3. https://openai.com/index/dall-e-3, 2024. [51] OpenAI. Gpt-4-1. https://openai.com/index/gpt-4-1, 2025. [52] OpenAI. Gpt-4o. https://openai.com/index/introducing-4o-image-generation, 2025. [53] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [54] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [55] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25452555, 2025. [56] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [57] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [60] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in Neural Information Processing Systems, 36, 2024. [61] Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: study on dpo vs. grpo. arXiv preprint arXiv:2505.17017, 2025. [62] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [63] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [64] Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gpt-image-edit-1.5 m: million-scale, gpt-generated image dataset. arXiv preprint arXiv:2507.21033, 2025. 17 [65] Xinyu Wei, Jinrui Zhang, Zeqing Wang, Hongyang Wei, Zhen Guo, and Lei Zhang. Tiif-bench: How does your t2i model follow your instructions? arXiv preprint arXiv:2506.02161, 2025. [66] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1296612977, 2025. [67] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. [68] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-tomore generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. [69] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis, 2023. URL https://arxiv.org/abs/2306.09341. [70] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1329413304, 2025. [71] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [72] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, Jiayan Teng, Zhuoyi Yang, Wendi Zheng, Xiao Liu, Ming Ding, Xiaohan Zhang, Xiaotao Gu, Shiyu Huang, Minlie Huang, Jie Tang, and Yuxiao Dong. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation, 2025. URL https://arxiv.org/abs/2412.21059. [73] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. arXiv preprint arXiv:2308.16911, 2023. [74] Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. arXiv preprint arXiv:2504.02782, 2025. [75] Yi Yao, Chan-Feng Hsu, Jhe-Hao Lin, Hongxia Xie, Terence Lin, Yi-Ning Huang, Hong-Han Shuai, and Wen-Huang Cheng. The fabrication of reality and fantasy: Scene generation with llm-assisted prompt interpretation. In European Conference on Computer Vision, pages 422438. Springer, 2024. [76] Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Jinhua Yu, Haote Yang, and Conghui He. Skydiffusion: Street-to-satellite image synthesis with diffusion models and bev paradigm. arXiv e-prints, pages arXiv2408, 2024. [77] Junyan Ye, Baichuan Zhou, Zilong Huang, Junan Zhang, Tianyi Bai, Hengrui Kang, Jun He, Honglin Lin, Zihao Wang, Tong Wu, et al. Loki: comprehensive synthetic data detection benchmark using large multimodal models. arXiv preprint arXiv:2410.09732, 2024. [78] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [79] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 18 [80] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 38363847, October 2023. [81] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024. [82] Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Ziyu Guo, Shicheng Li, Yichi Zhang, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, Peng Gao, Chunyuan Li, and Hongsheng Li. Mavis: Mathematical visual instruction tuning with an automatic data engine, 2024. URL https://arxiv.org/abs/2407.08739. [83] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [84] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [85] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Xiangyang Zhu, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. Advances in Neural Information Processing Systems, 37:131278131315, 2024. [86] Zhuofan Zong, Dongzhi Jiang, Bingqi Ma, Guanglu Song, Hao Shao, Dazhong Shen, Yu Liu, and Hongsheng Li. Easyref: Omni-generalized group image reference for diffusion models via multimodal llm. arXiv preprint arXiv:2412.09618, 2024. [87] Zhuofan Zong, Bingqi Ma, Dazhong Shen, Guanglu Song, Hao Shao, Dongzhi Jiang, Hongsheng Li, and Yu Liu. Mova: Adapting mixture of vision experts to multimodal context. arXiv preprint arXiv:2404.13046, 2024."
        }
    ],
    "affiliations": [
        "CUHK MMLab",
        "Peking University",
        "Shanghai Artificial Intelligence Laboratory",
        "Sun Yat-sen University"
    ]
}