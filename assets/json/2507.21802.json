{
    "paper_title": "MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE",
    "authors": [
        "Junzhe Li",
        "Yutao Cui",
        "Tao Huang",
        "Yinping Ma",
        "Chun Fan",
        "Miles Yang",
        "Zhao Zhong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose $\\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present a faster variant, termed $\\textbf{MixGRPO-Flash}$, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at $\\href{https://github.com/Tencent-Hunyuan/MixGRPO}{MixGRPO}$."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 2 0 8 1 2 . 7 0 5 2 : r arXiv preprint MIXGRPO: UNLOCKING FLOW-BASED GRPO EFFICIENCY WITH MIXED ODE-SDE Junzhe Li1,2,3, Yutao Cui1, Tao Huang1, Yinping Ma3, Chun Fan3, Miles Yang1, Zhao Zhong1 1 Hunyuan, Tencent, 2 School of Computer Science, Peking University, 3 Computer Center, Peking University"
        },
        {
            "title": "ABSTRACT",
            "content": "Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose MixGRPO, novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces sliding window mechanism, using SDE sampling and GRPOguided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for sampling. So we present faster variant, termed MixGRPO-Flash, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%. Codes and models are available at MixGRPO."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances (Fan et al., 2023b; Xu et al., 2023; Li et al., 2025; Miao et al., 2024; Liu et al., 2025; Xue et al., 2025) in the Text-to-Image (T2I) tasks have demonstrated that flow matching models can achieve improved performance by incorporating Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) strategies during the post-training stage to maximize rewards. Specifically, methods (Liu et al., 2025; Xue et al., 2025) based on Group Relative Policy Optimization (GRPO) (Shao et al., 2024), have recently been studied, achieving optimal alignment with human preferences. Current GRPO methods in probability flow models, e.g., Flow-GRPO (Liu et al., 2025), DanceGRPO (Xue et al., 2025), leverage Stochastic Differential Equations (SDE) sampling at every denoising step to introduce randomness in image generation, addressing reliance on stochastic exploration in RLHF. They model the denoising process as Markov Decision Process (MDP) in stochastic environment, using GRPO to optimize the entire state-action sequence. However, this significantly slows down the training process due to the substantial overhead introduced by the denoising iteration process. Specifically, to compute the ratio of posterior probabilities, it is essential to perform full-step sampling independently with the old policy model πθold and the new one πθ. While DanceGRPO proposes randomly selecting subset of denoising steps to optimize, our emEqual contribution. Corresponding authors. 1 arXiv preprint Figure 1: Performance comparison for different numbers of denoising steps optimized. The performance improvement of DanceGRPO relies on more steps optimized. MixGRPO achieves optimal performance while requiring only 4 steps. pirical analysis in Figure 1 demonstrates substantial performance degradation as the subset size is reduced. To address these issues, we propose MixGRPO, which achieves stochastic exploration while enabling optimization over fewer denoising steps. This reduces the MDP sequence to subset of the denoising process and uses GRPO for optimization. Specifically, we employs mixed ODE-SDE strategy, applying SDE sampling to subset of steps and Ordinary Differential Equations (ODE) sampling to the rest, confining randomness to SDE-based sampling steps. In this way, fewer timesteps are needed for optimization, without compromising image quality for reward computation. Besides, we introduce sliding window strategy that moves along the denoising steps, where SDE sampling and GRPO-guided optimization are applied only within the window, while ODE sampling is used outside the window. Unlike random selection, this scheduling strategy orders optimization from high to low denoising levels, which aligns with the intuition of applying temporal discount factors to rewards in Reinforcement Learning (RL) (Pitis, 2019; Amit et al., 2020; Hu et al., 2022). MixGRPO prioritizes optimizing the initial timesteps, which involve the most significant noise removal and entail larger exploration space (see Figure 2). Finally, we find that higher-order ODE solvers, e.g., DPMSolver++ (Lu et al., 2022b), can significantly accelerate training-time sampling, with negligible performance degradation, as there is no need for the posterior probability distribution after the sliding window. We trained and evaluated MixGRPO by using HPS-v2.1 (Wu et al., 2023), Pick Score (Kirstain et al., 2023), ImageReward (Xu et al., 2023), and Unified Reward (Wang et al., 2025) as reward models (RMs) and metrics. We also quantified the overhead in terms of the number of function evaluations (NFE) and time consumption during training During the training process, we fine-tuned based on FLUX.1-dev (Labs, 2024) and compared performance using either single RM or multiple RMs as guidance, assessing the results for both in-domain and out-of-domain metrics. Specifically, trained and evaluated on the HPDv2 dataset (Wu et al., 2023), MixGRPO improves the ImageReward (Xu et al., 2023) from 1.088 to 1.629, surpassing DanceGRPOs score of 1.436, while generating images with enhanced semantic quality, aesthetics and reduced distortion. Furthermore, MixGRPO In addition, MixGRPO-Flash utilizes reduces the training time of DanceGRPO by nearly 50%. DPMSolver++ (Lu et al., 2022b) to accelerate the sampling of πθold , reducing training time by 71%. To summarize, the key contributions of our work are outlined below: We propose mixed ODE-SDE GRPO training framework for flow matching, which alleviates the overhead bottleneck by streamlining the optimization process within the MDP. We introduce sliding window strategy to sample the denoising timesteps for model optimization, aligning with the RL intuition of transitioning from harder to easier search spaces, significantly enhancing performance. Our method enables the use of higher-order ODE solvers to accelerate πθold sampling during GRPO training, achieving significant speed improvements with comparable performance. Comprehensive experiments were carried out on multiple rewards and the results demonstrate that MixGRPO achieves substantial gains on various evaluation metrics, while significantly reducing training overhead. 2 arXiv preprint Figure 2: Visualization of t-SNE (Van der Maaten & Hinton, 2008) for images sampled with different strategies. Employing SDE sampling in the early stages of the denoising process results in more discrete data distribution."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 RL FOR IMAGE GENERATION Inspired by Proximal Policy Optimization (PPO) (Schulman et al., 2017), early works (Fan & Lee, 2023; Black et al., 2023; Fan et al., 2023a; Lee et al., 2023) integrated reinforcement learning (RL) into diffusion models by optimizing the score function (Song et al., 2020b) through policy gradient methods, thereby enabling the generation of images that better align with human preferences. Subsequently, Wallace et al. (2024) introduced offline-Direct Preference Optimization (DPO) to T2I tasks for the first time. This allows diffusion models to directly learn from human feedback and validates its effectiveness on large-scale models. Due to the tendency of offline win-lose pair data to shift the model away from its original distribution, some works (Yuan et al., 2024; Liang et al., 2025) have adopted online methods, continuously adjusting sampling trajectories through step-aware preference models during training to achieve improved performance. Recently, GRPO-based works e.g., Tong et al. (2025), Flow-GRPO (Liu et al., 2025) and DanceGRPO (Xue et al., 2025), have elevated RL-enhanced image generation to new heights. Specifically, Flow-GRPO (Liu et al., 2025) and DanceGRPO (Xue et al., 2025) introduced GRPO to flow matching models, allowing divergent sampling by transforming the ODE into an equivalent SDE. They also identified the overhead caused by full-step sampling within group as bottleneck and sought to address it by reducing or randomly selecting denoising steps. However, these approaches do not fundamentally address the issue. We hope to delve into the essence of GRPO on the probability flow and provide deeper insights through mixed sampling techniques and optimization scheduling. 2.2 SAMPLING METHODS FOR PROBABILITY FLOW DDPM (Ho et al., 2020) first proposed training probabilistic model to reverse each step of noise corruption and utilized probability flow SDE for sampling, enabling the generation of realistic images. However, such sampling method often requires thousands of steps, resulting in significant overhead. DDIM (Song et al., 2020a) introduced deterministic sampling and proposed probability ODE sampling approach, reducing the number of sampling steps to around 100. Subsequently, inspired by the Fokker-Planck equation (Risken & Risken, 1996), Song et al. (2020b) established unification of SDE and ODE sampling methods from the perspective of the score function. Then, 3 arXiv preprint more higher-order ODE solvers were proposed e.g., DPM-Solver (Lu et al., 2022a) and DPMSolver++ (Lu et al., 2022b), which utilize multistep methods for differential discretization. These approaches significantly reduce the number of sampling steps to around 10 while preserving accuracy. Higher-performance solvers (Zheng et al., 2023; Zhao et al., 2023) continue to be proposed; however, the gains are relatively marginal and have ultimately been replaced by the distillation method (Salimans & Ho, 2022; Yin et al., 2024). During the same period, flow matching models (Lipman et al., 2022; Esser et al., 2024) simplified and stabilized training by predicting the vector field velocity, enabling deterministic sampling with ODEs under 50 steps. Recent theoretical works (Gao et al., 2024; Albergo et al., 2023) has proven that the sampling method of flow matching is equivalent to DDIM, and demonstrated that flow matching models share the same equivalent SDE and ODE formulations as diffusion models. This provides important theoretical support and insights for our work, and we may explore interleaved sampling of SDE and ODE in probability flow models as potential approach."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 MIXED ODE-SDE SAMPLING IN GRPO According to Flow-GRPO (Liu et al., 2025), the SDE sampling in flow matching can be framed as Markov Decision Process (MDP) (S, A, ρ0, P, R) in stochastic environment. The agent produces trajectory during the sampling process defined as Γ = (s0, a0, s1, a1, . . . , sT , aT ), which = R(xT , c). In MixGRPO, we propose hybrid sampling method results in rewards R(st, at) that combines SDE and ODE. MixGRPO defines an interval = [t1, t2), which is subinterval of the denoising time range [0, ], so 0 t1 t2 . During the denoising process, we use SDE sampling within the interval and ODE sampling outside, while the interval moves from 0 to throughout the training process (See Figure 2). MixGRPO restricts the agents stochastic exploration space to the interval S, shortening the sequence length of the MDP to subset ΓMixGRPO = (st1, at1 , st1+1, at1+1, . . . , st2) and requires reinforcement learning (RL) optimization only on this subset: max θ EΓMixGRPOπθ (cid:34)t21 (cid:88) t=t (R (st, at) βDKL (π (st) πref (st))) , (1) (cid:35) Other settings in the MDP remain unchanged. MixGRPO reduces computational overhead while also lowering the difficulty of optimization. Next, we derive the specific sampling form and optimization objective of MixGRPO. For deterministic probability flow ODE (Song et al., 2020b), it takes the following form: dxt dt = (xt, t) 1 2 g2(t)xt log qt(xt), x0 q0(x0) (2) where (xt, t) is vector-valued function called the drift coefficient and the g(t) is scalar function known as the diffusion coefficient. xt log qt(xt) is the score function at time t. According to the Fokker-Planck equation (Risken & Risken, 1996), Song et al. (2020b) has demonstrated that Eq. (2) has the following equivalent probability flow SDE, which maintains the same marginal distribution at each time t: dxt dt = (xt, t) g2(t)xt log qt(xt) + g(t) , x0 q0(x0) dw dt (3) In MixGRPO, we mix ODE and SDE for sampling, with the specific form as follows: dxt = (cid:26)(cid:2)f (xt, t) g2(t)xt log qt(xt)(cid:3) dt + g(t)dw, (cid:2)f (xt, t) 1 2 g2(t)xt log qt(xt)(cid:3) dt, In particular, for Flow Matching (FM) (Lipman et al., 2022), especially the Rectified Flow (RF) (Liu et al., 2022), the sampling process can be viewed as deterministic ODE: if otherwise (4) dxt dt = vt, 4 (5) arXiv preprint Eq. (5) is actually special case of the Eq. (2) with vt = (xt, t) 1 can derive the ODE-SDE hybrid sampling form for RF as follows: 2 g2(t)xt log qt(xt). So we dxt = (cid:26)(cid:0)vt 1 vtdt, 2 g2(t)xt log qt(xt)(cid:1) dt + g(t)dw, if otherwise (6) 1t In the RF framework, the model is used to predict the velocity field of the deterministic ODE, represented as vθ(xt, t) = dxt dt . Following Liu et al. (2025), the score function is represented as xt log qt(xt) = xt vθ(xt, t). The g(t) is represented as the standard deviation of the noise g(t) = σt. According to the definition of the standard Wiener process, we use dw = dtϵ, where ϵ (0, I). Applying Euler-Maruyama discretization for SDE and Euler discretization for ODE, with = {t1, t1 + 1, . . . , t2 1} being set that includes all timesteps between t1 and t2, we build the final denoising process in MixGRPO: (cid:40) vθ(xt, t) + σ + σt (xt+(1t)vθ(xt,t)) 2t if tϵ, (cid:104) (cid:105) xt+t = xt + xt + vθ(xt, t)t, otherwise (7) The training process in MixGRPO is similar to Flow-GRPO (Liu et al., 2025) and DanceGRPO (Xue et al., 2025), but we only need to optimize for the time steps sampled within the interval S. The final training objective is represented as follows: JmixGRPO(θ) = cC, {xi }N i=0πθold (c) (cid:34) 1 N (cid:88) i=1 1 (cid:88) tS min (cid:0)ri t(θ)Ai, clip (cid:0)ri t(θ), 1 ε, 1 + ε(cid:1) Ai(cid:1) (cid:35) , where ri t(θ) is referred to as the policy ratio and Ai is the advantage score. We set ε = 0. ri t(θ) = qθ(xt+1xt, c) qθold (xt+1xt, c) and Ai = where R(xi 0, c) is provided by the reward model. (cid:0)xi , c(cid:1) mean (cid:0){R (cid:0)xi , c(cid:1)}N std (cid:0){R (cid:0)xi i=1 , c(cid:1)}N (cid:1) i=1 (cid:1) , (8) (9) It is important to note that, we have dropped the KL Loss (Shao et al., 2024). Although KL Loss can mitigate reward hacking to some extent (Liu et al., 2025), inspired by yifan123 (2025), we use mixed model sampling at test time, which can significantly address the reward hacking issue (See Appendix A). MixGRPO reduces NFE of πθ compared to all-timesteps optimization. However, the NFE of πθold is not reduced, as complete inference is required to obtain the final image for reward calculation. In Section 3.3, we will introduce the use of higher-order ODE solvers, which also reduce the NFE of πθold leading to further speedup. In summary, the mixed ODE-SDE sampling significantly reduces overhead while ensuring that the sampling does not deviate from the marginal distribution of either single ODE or SDE sampling at each timestep, due to the equivalence of the probability flow. 3.2 SLIDING WINDOW AS OPTIMIZATION SCHEDULER In fact, the interval can be non-fixed during the training process. In this section, we will introduce the sliding window to describe the movement of the interval S, which leads to significant improvement in the quality of the generated images. Along the discrete denosing timesteps {0, 1, . . . , 1}, MixGRPO defines sliding window (l) and optimization is only employed at the timesteps within this window. (10) where is the left boundary of the sliding window, and is hyperparameter that represents the window size. (l) = {tl, tl+1, . . . , tl+w1}, The left boundary of the sliding window moves as the training progresses. In the experiments, we found that the window size w, shift interval τ , and window stride are all crucial hyperparameters. Through ablation studies (See Experiment 4.4.1), we identified the optimal settings. When the total arXiv preprint Algorithm 1 MixGRPO Training Process Require: initial policy model πθ; reward models {Rk}K k=1; prompt dataset C; total sampling steps ; number of samples per prompt ; if (l) then Use SDE Sampling to get xi for sampling timestep = 0 to 1 do Init the same noise x0 (0, I) for generate i-th image from = 1 to do Sample batch prompts Cb Update old policy model: πθold πθ for each prompt Cb do Require: sliding window (l), window size w, shift interval τ , window stride 1: Init left boundary of (l): 0 2: for training iteration = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: end for end for Calculate multi-reward advantage: Ai (cid:80)K end for for optimization timestep (l) do Update policy model via gradient ascent: θ θ + ηθJ end for if mod τ is 0 then Use ODE Sampling to get xi min(l + s, w) ,c)i σk end for end if end if kµk else R(xi k=1 t+1 t+1 πθold mixed sampling loop optimize policy model πθ move sliding window Figure 3: Qualitative comparison. MixGRPO achieve superior performance in semantics, aesthetics and text-image alignment. sampling steps = 25, the best performance is achieved with = 4, τ = 25 and = 1. The detailed sliding window strategy and the MixGRPO algorithm can be referenced in Algorithm 1 Restricting the use of SDE sampling within the sliding window not only ensures the diversity of the generated images but also allows the model to concentrate on optimizing the flow within that window. Moving along the denoising direction represents the stochasticity of the probability flow from strong to weak, as illustrated in Figure 2. This is essentially greedy strategy and is similar to the RL that assigns discount factors to process rewards (Pitis, 2019; Amit et al., 2020; Hu et al., 2022), 6 arXiv preprint Table 1: Comparison results for overhead and performance. MixGRPO achieves the best performance across multiple metrics. MixGRPO-Flash significantly reduces training time while outperforming DanceGRPO. Bold: rank 1. Underline: rank 2. The Frozen strategy means that optimization is only employed at the initial denoising steps. Method FLUX DanceGRPO MixGRPO / 25 25 25 MixGRPO-Flash 16 (Avg) 8 NFEπθold NFEπθ Iteration Time (s) Human Preference Alignment HPS-v2.1 Pick Score ImageReward Unified Reward / 14 4 4 4 4 / 291.284 149.978 150.059 150.839 112.372 83.278 0. 0.356 0.334 0.333 0.367 0.358 0.357 0.227 0.233 0.225 0.229 0. 0.236 0.232 1.088 1.436 1.335 1.235 1.629 1.528 1.624 3. 3.397 3.374 3.325 3.418 3.407 3.402 which gives greater importance to rewards derived from larger search space in the earlier process. The experimental results of different movement strategies in Table 3 demonstrate the validity of this intuition. Not only does the progressive strategy outperform random selection, but we also found that even when the sliding window is not moved (frozen), meaning only the earlier timesteps are optimized, MixGRPO can still yield good results, particularly in terms of ImageReward (Xu et al., 2023) and UnifiedReward (Wang et al., 2025). Based on this intuition, we also proposed an exponential decay strategy as follows, allowing τ to decrease as the window moves, enabling the model to avoid excessive optimization in smaller search spaces. τ (l) = τ0 exp (k ReLU (l λthr)) , (11) where τ0 is the initial shift interval, is the decay factor, and λthr is the threshold that controls when the decay starts. The exponential function exp(x) calculates ex, while the Rectified Linear Unit ReLU(x) is defined as max(0, x). Table 3 shows that the exponential decay strategy can achieve better results in terms of Pick Score (Kirstain et al., 2023) and ImageReward (Xu et al., 2023), likely because the model focuses on the earlier stages of denoising, which can lead to more significant high-level changes. 3.3 TRADE-OFF BETWEEN OVERHEAD AND PERFORMANCE MixGRPO employs SDE sampling within the sliding window and ODE sampling outside of it, allowing the use of higher-order ODE solvers to accelerate GRPO training-time sampling. The timesteps that utilize ODE sampling are divided into those before and after the sliding window. The timesteps after the sliding window solely influence the reward calculation, whereas the timesteps before the window affect both the reward and contribute to cumulative errors in the policy ratio computing. Therefore, we focus exclusively on the acceleration of the timesteps after the window. Gao et al. (2024) has demonstrated the equivalence between the ODE sampling of flow matching models (FM) and DDIM, and Section 3.1 has also shown that diffusion probabilistic models (DPM) and FM share the same ODE form during the denoising process. Therefore, the higher-order ODE solvers e.g., DPM-Solver Series (Lu et al., 2022a;b; Zheng et al., 2023), UniPC (Zhao et al., 2023) designed for DPM sampling acceleration are also applicable to FM. We have reformulated DPMSolver++ (Lu et al., 2022b) to apply it in the FM framework for ODE sampling acceleration and released detailed derivations in Appendix B. By applying higher-order solvers, we achieve acceleration in the sampling of πθold during GRPO training, which is essentially balance between overhead and performance. Excessive acceleration leads to fewer timesteps, which inevitably results in decline in image generation quality, thereby accumulating errors in the computation of rewards. We have found in practice that the 2nd-order DPM-Solver++ is sufficient to provide significant acceleration while ensuring that the generated images align well with human preferences. Ultimately, we adopted both progressive and frozen sliding window strategies, proposing MixGRPO-Flash and MixGRPO-Flash*. detailed description of the algorithm can be found in Appendix C. These approaches achieve greater degree of acceleration compared to MixGRPO, while also outperforming DanceGRPO in terms of performance. arXiv preprint Figure 4: Qualitative comparison with different training-time sampling steps. The performance of MixGRPO does not significantly decrease with the reduction in overhead. The Frozen strategy means that optimization is only employed at the initial denoising steps."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENT SETUP Dataset We conduct experiments using the prompts provided by the HPDv21 dataset, which is the official dataset for the HPS-v2 benchmark (Wu et al., 2023). The training set contains 103,700 prompts; in fact, MixGRPO achieved good human preference alignment results after training one epoch with only 9,600 prompts. The test set consists of 400 prompts. The prompts are diverse, encompassing four styles: Animation, Concept Art, Painting, and Photo. Model model, which is an advanced text-to-image model based on flow matching. Following DanceGRPO (Xue et al., 2025), we use FLUX.1 Dev (Labs, 2024) as the base Overhead Evaluation For the evaluation of overhead, we use two metrics: the number of function evaluations (NFE) (Lu et al., 2022a) and the time consumption per iteration during training. The NFE is divided into NFEπθold represents the number of forward propagation of the reference model for computing the policy ratio and generating images. NFEπθ is the number of forward propagation of the policy model solely for the policy ratio. Additionally, the average training time per GRPO iteration provides more accurate reflection of the acceleration effect. and NFEπθ . NFEπθold Performance Evaluation We used four reward models, namely HPS-v2.1 (Wu et al., 2023), Pick Score (Kirstain et al., 2023), ImageReward (Xu et al., 2023) and Unified Reward Wang et al. (2025), both as reward guidance during training and as evaluation metrics. These metrics are all based on human preferences but emphasize different aspects. For example, ImageReward (Xu et al., 2023) highlights image-text alignment and fidelity, while Unified Reward (Wang et al., 2025) concentrates on semantics. DanceGRPO Xue et al. (2025) also demonstrates that using multiple reward models can achieve better results. To validate the robustness of MixGRPO, we also followed DanceGRPO and conducted additional comparisons using HPS-v2.1 as single reward, and combining HPSv2.1 (Wu et al., 2023) and CLIP Score (Radford et al., 2021) as multi-rewards. 1https://huggingface.co/datasets/ymhao/HPDv2 8 arXiv preprint 4.2 IMPLEMENTATION DETAILS (cid:113) For training-time sampling, we first perform shift = 1(s1)t on the uniformly distributed = [0, . . . , 1] and then define σt = η 1t . We set = 3 and η = 0.7 as scale. We set = 25 as the total sampling steps. For GRPO, the model generates 12 images for each prompt and clips the advantage to the range [5, 5]. It is important to note that we use gradient accumulation over 3 steps, which means that during single training iteration, there are 12 3 = 4 gradient updates. For the exponential decay strategy of the sliding window in Eq. ( 11), we empirically set τ0 = 20, = 0.1, and λthr = 13. Furthermore, when multiple reward models are jointly trained with, each reward is assigned equal weight. For training, all experiments are conducted on 32 Nvidia GPUs with batch size of 1 and maximum of 300 iterations. We use AdamW (Loshchilov & Hutter, 2017) as the optimizer with learning rate of 1e-5 and weight decay coefficient of 0.0001. Mixed precision training is used with the bfloat16 (bf16) format, while the master weights are maintained at full precision (fp32). 4.3 MAIN EXPERIMENTS In the main experiment, the four human-preference-based rewards were aggregated according to advantages, as shown in Algorithm 1. We evaluated the overhead and performance of MixGRPO in comparison with DanceGRPO, with the results presented in Table 1. The official DanceGRPO uses NFEπθ = 14; however, for fairness, we also tested DanceGRPO with NFEπθ = 4. For MixGRPOFlash, we evaluated both the progressive and frozen strategies, and to ensure fairness, we also applied the frozen strategy to DanceGRPO. We selected multiple scene prompts to visualize the results for FLUX.1 Dev, the officially configured DanceGRPO, and MixGRPO, as shown in Figure 3. It can be observed that MixGRPO achieved the best results in terms of semantics, aesthetics, and text-image alignment. Figure 4 shows the comparison results of DanceGRPO, MixGRPO, and MixGRPO-Flash with NFEπθ = 4. It can be observed that under the same overhead conditions, MixGRPO achieved better results compared to DanceGRPO. Additionally, MixGRPO-Flash enables accelerated sampling of πθold, and as the overhead decreases, the quality of the generated images still maintains strong alignment with human preferences. Following DanceGRPO (Xue et al., 2025), we also trained and evaluated the model using single reward model and two reward models, but on the HPDv2 dataset (Wu et al., 2023). The results (See Table 2) demonstrate that MixGRPO achieves the best performance on both in-domain and out-of-domain rewards, whether using single or multiple reward models. The visualized results are displayed in Appendix D. Table 2: Comparison results for in-domain and out-of-domain reward metrics. The results demonstrate that MixGRPO achieves the best performance on both in-domain and out-of-domain rewards, whether using single or multiple reward models. Reward Model Method In Domain Out-of-Domain HPS-v2.1 CLIP Score Pick Score ImageReward Unified Reward / HPS-v2. HPS-v2.1 & CLIP Score FLUX DanceGRPO MixGRPO DanceGRPO MixGRPO 0.313 0.367 0. 0.346 0.349 0.388 0.349 0.372 0.400 0.415 0.227 0.227 0. 0.228 0.229 1.088 1.141 1.396 1.314 1.416 3.370 3.270 3. 3.377 3.430 4.4 ABLATION EXPERIMENTS 4.4.1 SLIDING WINDOW HYPERPARAMTERS As introduced in Section 3.2, the moving strategy, shift interval τ , window size and window stride are all important parameters of the sliding window. We conducted ablation experiments on each of them. For the moving strategy, we compared three approaches: frozen, where the window remains 9 arXiv preprint stationary; random, where random window position is selected at each iteration; and progressive, where the sliding window moves incrementally with the denoising steps. For the progressive strategy, we tested different scheduling strategies where the interval τ initially starts at 25 but changes with training iterations. As shown in Table 3, the results indicate that under the progressive strategy, either exponential decay or constant scheduling strategies are optimal. For the shift interval τ , 25 is the optimal setting (See Table 4). The number of inferences for πθ increases with the growth of the window size w, leading to greater time overhead. We compared different settings of w, and the results are shown in Table 5. Ultimately, we selected = 4 as balanced setting between overhead and performance. For the window stride s, we found through experimentation that = 1 is the optimal choice, as shown in Table 6. Table 3: Comparison for moving strategies. Table 4: Comparison for shift interval τ . Strategy Frozen Random Progressive Interval Schedule / Constant Decay (Linear) Decay (Exp) Constant HPS-v2.1 Pick Score ImageReward Unified Reward 0.354 0.365 0.365 0.360 0.367 0. 0.237 0.235 0.239 0.237 1.580 1.513 1.566 1.632 1.629 3. 3.388 3.382 3.416 3.418 τ 15 20 25 30 HPS-v2.1 Pick Score ImageReward Unified Reward 0.366 0.366 0.367 0.350 0.237 0.238 0.237 0.229 1.509 1.610 1.629 1.589 3.403 3.411 3.418 3.385 Table 5: Comparison for window size Table 6: Comparison for window stride w NFEπθ HPS-v2.1 Pick Score 0.362 2 0.367 4 0.370 6 0.235 0.237 0.238 2 4 6 ImageReward Unified Reward 1.588 1.629 1.547 3.419 3.418 3. HPS-v2.1 Pick Score ImageReward Unified Reward 1 2 3 4 0.367 0.357 0.370 0.368 0.237 0.236 0.236 0.238 1.629 1.575 1.578 1. 3.418 3.391 3.404 3.407 4.4.2 HIGH ORDER ODE SOLVER MixGRPO enables the possibility of accelerating ODE sampling with high-order ODE solvers by combining SDE and ODE sampling methods. We first conducted ablation experiments on the order of the solver, using DPM-Solver++ (Lu et al., 2022b) as the high-order solver with the progressive strategy. The results, as shown in Table 7, indicate that the second-order mid-point method is the optimal setting. Then, as described in Section 3.3, we compared two acceleration approaches. One is MixGRPO-Flash, which utilizes the progressive window moving strategy. The other is MixGRPOFlash*, which employs the frozen moving strategy. They all achieve balance between overhead and performance by reducing the number of ODE sampling steps after the sliding window. However, in practice, MixGRPO-Flash requires the window to move throughout the training process, which results in shorter ODE portion being accelerated. Consequently, the acceleration effect of MixGRPO-Flash, on average, is not as pronounced as that of MixGRPO-Flash*. Table 7: Comparison of performance across different-order solvers. The second-order Midpoint method achieves the best performance. Table 8: Comparison of different sampling steps for πθold. MixGRPO-Flash* achieves good performance even with few steps. Order Solver Type HPS-v2.1 Pick Score ImageReward Unified Reward 1 2 3 / Midpoint Heun / 0.367 0.358 0.362 0.359 0. 0.237 0.233 0.234 1.570 1.578 1.488 1.512 3. 3.407 3.399 3.387 Method DanceGRPO MixGRPO-Flash MixGRPO-Flash* Sampling Overhead Human Preference Alignment NFEπθold 25 19 (Avg) 16 (Avg) 13 (Avg) 12 10 8 Time per Image (s) HPS-v2.1 Pick Score ImageReward Unified Reward 9.301 7.343 6.426 5.453 4.859 4.214 3.789 0.334 0.357 0.362 0. 0.353 0.359 0.357 0.225 0.236 0.237 0.229 0.230 0.234 0.232 1.335 1.564 1.578 1. 1.588 1.548 1.624 3.374 3.394 3.407 3.363 3.396 3.430 3."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Although GRPO (Shao et al., 2024) has seen significant success in the language modality, it is still in the early stages of progress in vision (Tong et al., 2025; Xue et al., 2025; Liu et al., 2025). Existing flow-matching based GRPO faces challenges such as low sampling efficiency and slow training. To address these issues, we proposed MixGRPO, novel training framework that combines SDE and ODE sampling. This hybrid approach allows for focused optimization on the SDE sampling flow component, reducing complexity while ensuring accurate reward computation. Inspired by the decay factor in reinforcement learning (Hu et al., 2022), we introduce sliding window strategy for scheduling the optimized denoising steps. Experimental results confirm the effectiveness of our approach in both single-reward and multi-reward settings. Additionally, MixGRPO decouples 10 arXiv preprint the denoising stages for optimization and reward computation, enabling acceleration of the latter with high-order solvers. We further propose MixGRPO-Flash, which balances overhead and performance. We hope MixGRPO will inspire deeper research into post-training for image generation, contributing to the advancement of Artificial General Intelligence (AGI)."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Ron Amit, Ron Meir, and Kamil Ciosek. Discount factor as regularizer in reinforcement learning. In International conference on machine learning, pp. 269278. PMLR, 2020. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. Ying Fan and Kangwook Lee. Optimizing ddpm sampling with shortcut fine-tuning. arXiv preprint arXiv:2301.13362, 2023. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Dpok: Reinforcement learning for fine-tuning text-to-image diffusion models. Advances in Neural Information Processing Systems, 36:7985879885, 2023a. Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for finetuning text-to-image diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS) 2023. Neural Information Processing Systems Foundation, 2023b. Ruiqi Gao, Emiel Hoogeboom, Jonathan Heek, Valentin De Bortoli, Kevin P. Murphy, and Tim Salimans. Diffusion meets flow matching: Two sides of the same coin. 2024. URL https: //diffusionflow.github.io/. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hao Hu, Yiqin Yang, Qianchuan Zhao, and Chongjie Zhang. On the role of discount factor in offline reinforcement learning. In International conference on machine learning, pp. 90729098. PMLR, 2022. Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Picka-pic: An open dataset of user preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:3665236663, 2023. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. Junzhe Li, Xuerui Qiu, Linrui Xu, Liya Guo, Delin Qu, Tingting Long, Chun Fan, and Ming Li. Unif2ace: Fine-grained face understanding and generation with unified multimodal models. arXiv preprint arXiv:2503.08120, 2025. Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, and Liang Zheng. Aesthetic post-training diffusion models from generic preferences with step-bystep preference optimization. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1319913208, 2025. 11 arXiv preprint Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022a. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Zichen Miao, Jiang Wang, Ze Wang, Zhengyuan Yang, Lijuan Wang, Qiang Qiu, and Zicheng Liu. Training diffusion models towards diverse image generation with reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1084410853, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Silviu Pitis. Rethinking the discount factor in reinforcement learning: decision theoretic approach. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 79497956, 2019. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Hannes Risken and Hannes Risken. Fokker-planck equation. Springer, 1996. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Chengzhuo Tong, Ziyu Guo, Renrui Zhang, Wenyu Shan, Xinyu Wei, Zhenghao Xing, Hongsheng Li, and Pheng-Ann Heng. Delving into rl for image generation with cot: study on dpo vs. grpo. arXiv preprint arXiv:2505.17017, 2025. arXiv preprint Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-toimage synthesis. arXiv preprint arXiv:2306.09341, 2023. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. GitHub User yifan123. Discussion on flow-grpo issue 7. https://github.com/yifan123/ flow_grpo/issues/7#issuecomment-2870678379, 2025. Accessed: 2025-05-12. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 66136623, 2024. Huizhuo Yuan, Zixiang Chen, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning of diffusion models for text-to-image generation. Advances in Neural Information Processing Systems, 37: 7336673398, 2024. Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictorcorrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36:4984249869, 2023. Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 13 arXiv preprint"
        },
        {
            "title": "A HYBRID INFERENCE FOR SOLVING REWARD HACKING",
            "content": "To address the reward hacking issue, we employ hybrid inference to sample with both the raw and post-trained models, and introduce the hybrid percent pmix. This means that the initial pmixT denoising steps are sampled by the model trained with GRPO, while the remaining denoising process is finished by the original FLUX model (Black et al., 2023). Table 9 and Figure 5 respectively illustrate the changes in performance and images as pmix increases under the multi-rewards training scenario. The experimental results demonstrate that pmix = 80% is an optimal empirical value that effectively mitigates hacking while maximizing alignment with human preferences. Table 9: Comparison with different hybrid inference percentages HPS-v2.1 Pick Score"
        },
        {
            "title": "ImageReward Unified Reward",
            "content": "0.313 0.342 0.356 0.362 0.366 0.369 0.226 0.233 0.235 0.236 0.238 0.238 1.089 1.372 1.539 1.598 1.610 1.607 3.369 3.386 3.395 3.407 3.411 3.378 pmix 0% 20% 40% 60% 80% 100% Figure 5: Qualitative comparison with different hybrid inference percentages DPM-SOLVER++ FOR RECITIFIED FLOW The DPM-Solver++ algorithm (Lu et al., 2022b) is originally designed for the x0-prediction diffusion model (Rombach et al., 2022), where the model outputs the denoised feature x0 based on the noisy feature xt, the time condition t, and the text condition c. According to the definition of Rectified Flow (RF) (Liu et al., 2022), there is the following transfer equation: xt = tx1 + (1 t)x0 (12) According to the theory of stochastic interpolation (Albergo et al., 2023), RF effectively approximates x1 x0 by modeling vt: vt = x1 Based on Eq. (12) and Eq. (13), we obtain the following relationship: x0 = xt vtt (13) (14) By using neural network for approximation, we establish the relationship between RF and the x0-prediction model: xθ = xt vθt 14 (15) arXiv preprint Taking the multistep second-order solver as an example (see Algorithm 2 in (Lu et al., 2022b)), we derive the corrected xθ for the RF sampling process as Dt: (cid:18) (cid:19) (cid:18) (cid:19) Dt 1 + ht 2ht1 ht 2ht1 xt1 vθ(xt1, 1, c) (cid:18) xt2 vθ(xt2, 2, c) 1 2 (cid:19) (16) where is the total sampling steps and ht = λtλt1. λt is the log-signal-to-noise-ratio (log-SNR) and is defined in RF as λt := log (cid:0) 1t Based on the exact discretization formula for the probability flow ODE proposed in DPM-Solver++ (Eq. (9) in (Lu et al., 2022b)), we can derive the final transfer equation: (cid:1). xt 1 xt1 (1 t) (cid:0)eht 1(cid:1) Dt, 1 < (17) MIXGRPO-FLASH ALGORITHM MixGRPO-Flash accelerates the ODE sampling that does not contribute to the calculation of the policy ratio after the sliding window by using DPM-Solver++ in the Eq. (17). We introduce compression rate such that the ODE sampling after the window only requires (T w)r time steps. And the total time-steps is = + + (T w)r The final algorithm is as follows: Algorithm 2 MixGRPO-Flash Training Process Require: initial policy model πθ; reward models {Rk}K of samples per prompt ; ODE compression rate k=1; prompt dataset C; total sampling steps ; number t+ else if < then else if < + then Use SDE sampling to get xi Use first-order ODE sampling to get xi Sample batch prompts Cb Update old policy model: πθold πθ for each prompt Cb do Init the same noise x0 (0, I) for generate i-th image from = 1 to do for sampling timestep = 0 to 1 do Require: sliding window (l), window size w, shift interval τ , window stride 1: Init left boundary of (l): 0 2: for training iteration = 1 to do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: end for end for Calculate multi-reward advantage: Ai (cid:80)K end for for optimization timestep (l) do end for if use MixGRPO-Flash* then Update policy model via gradient ascent: θ θ + ηθJ Use higher-order ODE sampling to get xi min(l + s, w) if mod τ is 0 then end for ,c)i σk end if end if end if R(xi kµk else k=1 t+1 t+1 DPM-Solver++ optimize policy model πθ move sliding window Note that when using MixGRPO-Flash*, the frozen strategy is applied, with the left boundary of the sliding window 0. The theoretical speedup of the training-time sampling can be described as 15 arXiv preprint follows: = + (T w)r (18) For MixGRPO-Flash, since the sliding window moves according to the progressive strategy during training, the average speedup can be expressed in the following form: = El (w + + (T l)r) < + (T w)r (19)"
        },
        {
            "title": "D MORE VISUALIZED RESULTS",
            "content": "Figure 6: Comparison of the visualization results of FLUX, DanceGRPO, and MixGRPO under HPS-v2.1 as the reward model. 16 arXiv preprint Figure 7: Comparison of the visualization results of FLUX, DanceGRPO, and MixGRPO under HPS-v2.1 and CLIP Score as the reward models."
        }
    ],
    "affiliations": [
        "Computer Center, Peking University",
        "Hunyuan, Tencent",
        "School of Computer Science, Peking University"
    ]
}