{
    "paper_title": "HY3D-Bench: Generation of 3D Assets",
    "authors": [
        "Team Hunyuan3D",
        ":",
        "Bowen Zhang",
        "Chunchao Guo",
        "Dongyuan Guo",
        "Haolin Liu",
        "Hongyu Yan",
        "Huiwen Shi",
        "Jiaao Yu",
        "Jiachen Xu",
        "Jingwei Huang",
        "Kunhong Li",
        "Lifu Wang",
        "Linus",
        "Penghao Wang",
        "Qingxiang Lin",
        "Ruining Tang",
        "Xianghui Yang",
        "Yang Li",
        "Yirui Guan",
        "Yunfei Zhao",
        "Yunhan Yang",
        "Zeqiang Lai",
        "Zhihao Liang",
        "Zibo Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation."
        },
        {
            "title": "Start",
            "content": "Tencent Hunyuan HY3D-Bench: Generation of 3D Assets Tencent Hunyuan3D https://3d.hunyuan.tencent.com https://huggingface.co/datasets/tencent/HY3D-Bench https://github.com/Tencent-Hunyuan/HY3D-Bench 6 2 0 2 ] . [ 1 7 0 9 3 0 . 2 0 6 2 : r Figure 1: HY3D-Bench is unified ecosystem for high-fidelity 3D content generation. Our framework introduces (a) 252k high-quality assets with watertight meshes and multi-view renderings, (b) 240k structured part-level decomposition enabling fine-grained control, and (c) AIGC-synthesized 125k long-tail category assets. This benchmark provides standardized training data and evaluation protocols for advancing 3D generation research. Abstract While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an opensource ecosystem designed to establish unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate library of 250k highfidelity 3D objects distilled from large-scale repositories, employing rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multiview renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation."
        },
        {
            "title": "Introduction",
            "content": "High-quality 3D content has become critical asset across broad range of fields, including 3D computer vision, generative modeling, and robotics. While pioneering large-scale repositories [10, 11, 85] have provided an unprecedented volume of 3D data, their utility across these diverse fields is often hampered by significant limitations. Most raw assets in these datasets contain significant noise, non-manifold geometry, and lack of structural granularity, which restricts their application in tasks requiring precise geometric understanding, stable generation, or complex robotic interaction. 1 In this work, we present HY3D-Bench, comprehensive open-source ecosystem designed to provide high-quality, structured, and reproducible foundation for 3D content research. Moving beyond simple mesh collection, our work integrates rigorous data engineering, standardized benchmarks, and scalable AIGC-driven synthesis to support the dual goals of 3D content understanding and creation. Our contributions are categorized into three major pillars: First, we provide refined and structured 3D asset library with comprehensive data processing results. For each holistic object, we implement professional pipeline to generate training-ready assets featuring the best watertight mesh and high-fidelity rendered images, both of which are essential for stable 3D generation training and accurate geometric perception. Crucially, we utilize part-merging strategy to produce structured assets with consistent part-level decomposition. For the structural components, we provide the original mesh segmentation results and individual part-level watertight meshes, complemented by view-dependent RGB renderings and 2D masks for the integrated part assembly. This structural granularity provides the necessary information for part-aware generation and fine-grained perception tasks. Second, we establish standardized evaluation and experiment framework to address the fragmentation in 3D research. We propose rigorous benchmark comprising 400 high-quality objects across diverse categories, providing unified platform for testing 3D generation algorithms. Unlike previous works with inconsistent evaluation protocols, we provide complete suite of standard metrics, baselines, and fixed experiment setting. By releasing our standardized training configurations and pre-trained model checkpoints, we empower the community to conduct fair comparisons and accelerate the rapid advancement of the 3D generation field. Third, we introduce scalable AIGC-driven data synthesis pipeline to bridge the gap in category diversity and long-tail distribution. Recognizing that manual 3D modeling for realistic scenarios, such as shopping malls, is prohibitively expensive, we leverage the generative power of Large Language Models and Diffusion Models to synthesize diverse 3D content. Our three-step paradigm, consisting of Text-to-Text for semantic expansion, Text-to-Image for visual synthesis, and Imageto-3D for mesh reconstruction, allows us to produce vast collection of long-tail items, allows us to produce vast collection of long-tail items covering 20 super-categories, 130 categories, and 1,252 fine-grained sub-categories. This synthetic data provides critical supplement for training models that can generalize to rare but crucial object categories, which is particularly vital for the robustness of generation and the diversity of robotics simulation environments. By providing structured, diverse, and standardized 3D content ecosystem, we aim to lower the barrier for research and drive the progress toward unified understanding and generation of the 3D world. In summary, our contributions are as follows: high-quality 3D asset library featuring watertight meshes and rendered images for both holistic objects and parts. standardized 3D benchmark and experiment framework, providing unified metrics, baselines, and model weights. An AIGC-based synthesis framework that expands 3D data diversity, focusing on long-tail assets to support broad generalization. Extensive data and infrastructure support for wide range of downstream tasks, including 3D generation, perception pre-training, and robotics simulation."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 3D Generation The field of 3D generation has emerged as cornerstone of generative AI, bridging the gap between virtual content creation and real-world applications. The evolution of 3D generation has witnessed paradigm shift from manual modeling and scanning-based reconstruction to data-driven AI synthesis. This field can be systematically categorized into four major paradigms: GAN-based methods, SDS-based methods, feedforward-based methods, and 3D native generation, as shown in Figure 2. GAN-based generation. Generative Adversarial Networks (GANs) [22] established the initial paradigm for high-fidelity synthesis by optimizing minimax objective between generator and discriminator. Following this, early works [55, 7] based on explicit representations, such 2 Figure 2: The evolution of the 3D generation. as voxel grids [82, 63, 62] and point clouds [72, 36], attempt to generate 3D shapes directly, yet methods often suffer from cubic memory complexity and limited resolution. The advent of Neural Radiance Fields (NeRF) [59] shifted the focus toward 3D-aware image synthesis, where models are trained on multi-view 2D images to learn underlying 3D geometry. Seminal works such as GRAF [70] and π-GAN [2] integrated conditional radiance fields with adversarial training, utilizing coordinate-based MLPs to enforce multi-view consistency. However, fully implicit backbones proved computationally expensive for high-resolution rendering. Addressing this, EG3D [3] proposed hybrid explicit-implicit tri-plane representation, leveraging the efficiency of StyleGAN2 [31] to generate feature planes that are subsequently decoded by lightweight MLP via volume rendering. While GANs achieve rapid inference speeds, they remain prone to training instability and mode collapse, particularly when scaling to diverse, open-domain datasets. SDS-based generation. The scarcity of large-scale, annotated 3D datasets has historically hindered the development of generative 3D models compared to their 2D counterparts. To circumvent this data bottleneck, recent approaches have shifted towards optimization-based pipelines that leverage pre-trained 2D text-to-image diffusion models as strong priors. The pioneer work, DreamFusion [67], introduced Score Distillation Sampling (SDS), method that optimizes differentiable 3D representationtypically NeRFsuch that its rendered views maintain high likelihood under frozen 2D diffusion model. By replacing the standard diffusion denoising loss with gradient-based score matching objective, SDS enables the distillation of semantic knowledge from 2D foundation models into consistent 3D structures without requiring 3D ground truth. Following DreamFusion, subsequent works [81, 42, 5, 43, 37] are proposed to further enhance the quality of 3D generation. More recently, the field has transitioned from implicit NeRF representations to explicit 3D Gaussian Splatting [32] to achieve real-time rendering and improved convergence speeds. Methods such as DreamGaussian [74] and GaussianDreamer[98] adapt SDS to optimize 3D Gaussian parameters, significantly reducing generation time while maintaining visual fidelity. However, these SDS-based methods usually suffer from the Janus problem (multi-face artifacts) due to the lack of explicit 3D geometry-related constraints. In addition, the problem of long-term optimization also poses real-time challenges for such methods. Feedforward generation. In contrast to optimization-based paradigms that require computationally expensive per-instance training (e.g., via Score Distillation Sampling), feedforward methods prioritize inference efficiency by learning direct, amortized mapping from input prompts to 3D representations. foundational direction in feedforward generation follows two stages: multiview (MV) image synthesis and 3D reconstruction. For instance, MVDream [71] introduces multi-view diffusion model conditioned on camera poses, enabling the generation of geometrically consistent MV images from text, which are then fed into neural radiance field (NeRF) or mesh reconstruction pipeline to yield 3D assets. Subsequent methods [50, 53, 34, 47, 51, 35] attempt to improve the multi-view consistency and image resolution to obtain high-quality 3D assets. Another prominent yet distinct feedforward paradigm for 3D generation is the Large Reconstruction Model (LRM) approach [25]. LRM aims to learn universal reconstruction capability from large-scale 3D data, enabling it to generate 3D representations directly from textual or sparse visual inputs through single forward pass. These models leverage the richness of large-scale 3D datasets to learn generalized 3D shape priors, which are then used to amortize the optimization cost across multiple generation tasks. Building on the LRM paradigm, subsequent works [79, 75, 87, 34] have proposed targeted improvements to enhance generation quality, efficiency, and generalization. For instance, LGM [75] proposes representation based on Gaussian features to improve the resolution 3 of 3D models. While these feedforward methods outperform optimization-based counterparts in inference speed and geometric quality, they are constrained by the resolution of 2D images and the lack of learning and understanding of the spatial distribution of 3D data, making it challenging to generate fine-grained and accurate 3D geometries. Native generation. Unlike 2D-lifting approaches, native 3D generation methods directly learn the 3D representations, such as point clouds [104, 92, 64, 56], meshes [61, 30, 52], and implicit functions [8, 65], from large-scale 3D assets, typically yielding superior geometric consistency and topology. However, these non-compressed methods are typically constrained by computational complexity and resolution, making it challenging to generate high-quality geometries. pivotal breakthrough in native 3D generation came with large-scale 3D datasets [10, 11] and 3DShape2Vecset [99], which innovatively adopted the processing paradigm of 2D Stable Diffusion and constructed 3D VAE (Variational Autoencoder) to compress 3D shapes into compact VecSet representation. With this representation, 3DShape2Vecset enabled the construction of diffusion models for both conditional and unconditional 3D generation. Following this workflow, subsequent works [101, 100, 38, 40, 83, 6, 27] strive to enhance the models generalization ability and geometric fidelity by scaling up the model and data. In contrast to implicit VexSet, several approaches [69, 86, 23, 84, 26] apply structured voxel-based representation to preserve spatial structure in latent space. Additionally, there are series of studies [16, 97] that apply native generation on other specific issues. For example, PoseMaster [90] and Hunyuan3D-Omni [28] introduce native controllable generation model to achieve control on point, voxel, bounding box, and skeleton. There is also line of research on fine-grained 3D generation, whose primary goal is to produce partaware results. One set of works[95, 48, 57, 33, 102, 1, 73, 78, 89, 94, 46, 12, 106, 19, 58, 49, 107, 66, 39] adopts segmentation-based pipeline: starting from holistic object and decomposing it into parts via segmentation. Another set of works[91, 41, 96, 45, 76, 17, 15, 24, 93] instead follows part-aware generation paradigm, directly generating 3D objects with explicit part structures. In addition, some works [4, 60, 9, 80, 17, 94, 21, 13] provide datasets with part-level annotations. While these methods present impressive performance in the 3D generation task, they usually rely on high-quality data processing in terms of the part-aware mesh and watertight mesh. In this paper, we open-source large-scale processed data that can be used to train 3D VAE and diffusion directly. 2.2 3D Datasets The advancement of 3D generation models is inherently tied to the availability of high-quality benchmark datasets, which provide the foundational data support for model training, validation, and evaluation. Early 3D benchmark datasets [105, 20, 18, 85], such as ShapeNet [4], laid the initial groundwork for the development of 3D generation research. However, these datasets suffer from limitations such as limited number of categories, simple geometry, and small quantities, which severely constrain the generalization capabilities of trained 3D generation models. This bottleneck has long restricted the further advancement of 3D generation technology towards more practical and versatile scenarios. The emergence of large-scale 3D datasets with complex geometric structures has broken this deadlock, among which Objaverse stands out as pivotal milestone. As the large-scale, diverse 3D object dataset, Objaverse [10] and Objaver-XL [11] contain millions of 3D models spanning wide range of categories, including complex geometric structures such as articulated objects, organic shapes, and detailed industrial parts. The release of Objaverse has significantly empowered the development of 3D generation technology, particularly fostering the advent of new generation of large-scale 3D generation models. However, critical challenge persists in the current 3D generation research landscape: mainstream 3D generation models typically require extensive preprocessing of raw 3D data to generate taskspecific representations, such as rendered images, watertight meshes, and corresponding Signed Distance Function (SDF) fields. This preprocessing step not only increases the entry barrier for researchers new to 3D generation, requiring proficiency in specialized data processing techniques, but also imposes substantial computational burdens. Although open-source data processing scripts have been developed to alleviate some of these difficulties by automating certain preprocessing workflows, processing large-scale training datasets (often involving millions of 3D models) demands enormous GPU and CPU computational resources. This resource-intensive preprocessing requirement remains significant bottleneck for the broader research community, hindering the rapid iteration and widespread adoption of 3D generation models. To address this critical challenge, in this paper, we directly provide high-quality dataset of 4 200k samples specifically tailored for training 3D Variational Autoencoders (3D VAE) and 3D diffusion models. The data samples are curated from two large-scale 3D repositories, Objaverse and Objaverse-XL, ensuring rich category diversity and complex geometric characteristics. Notably, we process the 3D meshes to obtain watertight meshes at resolution of 512, which effectively preserves large number of fine-grained details from the original meshes. By offering this preprocessed, high-resolution 3D dataset, we aim to reduce the computational and technical burdens on researchers, lower the entry barrier for 3D generation research, and further facilitate the advancement of the field. 3 Methods VAE. Given an input point cloud RN(3+C) sampled from the mesh surface, where denotes surface normals, 3D VAE first extract point features and then obtain the corresponding latent vector set RLd via resampling from estimated distribution, where and indicate the length and dimension of latent VecSet, respectively. Subsequently, decoder is applied to reconstruct the signed distance function (SDF) field Fsd , in which we can leverage the iso-surface extraction to obtain explicit mesh output. The procedure of VAE can be formulated as follows: = (P), Fsd = D(Z) (1) Diffusion. Given an image and its latent set representation of shape, the 3D diffusion model aims to model the denoising process, thereby achieving conditional generation from an arbitrary image. It first leverages an image encoder, such as DINO-v2 [], to capture image embeddings ci and then exploits the multi layers of DiT to predict the added noise or velocity. For flow matching model used in Hunyuan3D 2.1 [], its training objective is to transform simple noise distribution x0 (0, I) into complex data distribution x1 conditioned on image embeddings ci, which can be formulated as follows: Et,x0,x1,cvθ(x, t, c) (x1 x0)2 2 (2) 4 Hunyuan Objarverse The currently open-source Objaverse series datasets [10, 11] contain vast collection of raw 3D assets available for access and download. However, these raw assets suffer from numerous critical issues that urgently need to be addressed, rendering them unsuitable for direct application in downstream tasks such as 3D generation. First, from technical specification perspective, various types of 3D assets produced by different 3D modeling software (such as Blender, Maya, 3ds Max, etc.) exhibit significant format discrepancies and lack of standardization. Specifically: (1) Inconsistent coordinate system definitions: Different software packages adopt varying coordinate system conventions (e.g., left-handed vs. righthanded systems, Y-up vs. Z-up, etc.), resulting in orientation errors or mirror flipping when assets are loaded in different environments; (2) Complex and diverse asset construction methods: Many assets employ multi-level node hierarchies, contain parent-child node scale inheritance relationships, and include hidden transformation matrices, which greatly increase the complexity of data processing. Second, from data quality perspective, the quality of various 3D assets is highly inconsistent, exhibiting significant heterogeneity. The main issues include: (1) Poor geometric quality: large number of assets have overly simplified meshes with insufficient polygon counts, failing to accurately represent the detailed features of objects. Additionally, severe topological defects exist, such as non-manifold edges, self-intersecting faces, and isolated vertices. These problems render the assets unsuitable for tasks requiring watertight meshes (such as physical simulation, 3D printing, etc.); (2) Texture mapping errors: Some assets have serious UV unwrapping problems, with incorrect texture-to-geometry mapping, excessively low texture resolution, or missing textures, which compromise rendering quality. Finally, from data ecosystem perspective, existing datasets also suffer from the following systemic deficiencies: (1) Severely imbalanced category distribution: The datasets exhibit pronounced long-tail distribution characteristics, with abundant assets in common categories (such as chairs and tables), while assets in many rare categories that are important for real-world applications are extremely scarce, limiting the generalization capability of models; (2) Lack of structured information: The vast majority of assets are holistic, monolithic meshes, lacking hierarchical part decomposition and assembly relationship descriptions, which severely constrains the development of advanced applications such as fine-grained understanding, editable generation, and robotic manipulation. To address the above issues, we first process and clean the raw 3D assets from the Objaverse series datasets through combination of automated processing and manual assistance, obtaining collection of high-quality static mesh processing results. We hope that researchers can conduct algorithmic exploration and research on unified, high-quality benchmark. Second, we further perform part-level processing to obtain structured assets with consistent part-level decomposition, yielding batch of high-quality original mesh segmentation results and individual part-level watertight meshes. We hope that researchers can further pursue more fine-grained algorithmic exploration and research. Finally, based on real-world object and product categories, we generate collection of category-balanced 3D assets, aiming to help improve the generalization capability and algorithmic exploration of downstream tasks such as grasping. 4.1 Existing Enhanced Objaverse Dataset Objaverse [10] and Objaverse-XL [11] provide the 3D research community with ultra-large-scale, diverse 3D asset datasets. However, researchers have been consistently challenged by issues of inconsistent data quality and complex 3D data processing workflows. Multiple subsequent works [44, 54, 29, 68] have approached the filtering, processing, and enhancement of Objaverse from different perspectives. From the perspective of data quality filtering, Objaverse++ [44] manually annotated 10,000 samples as training data based on key quality dimensions such as model transparency, single-object completeness, and scene attributes. Subsequently, specialized quality assessment model was trained to ultimately filter out 500,000 high-quality samples. From the perspective of geometric normalization, Objaverse-OA [54] and Canonical Objaverse Dataset [29] addressed the critical issue of inconsistent 3D model orientations. Objaverse-OA established orientation normalization standards by annotating 14,000 orientation-aligned samples, while Canonical Objaverse Dataset utilized automated methods to annotate 32,000 samples. From the perspective of data representation diversity, Objaverse-MIX [68] provided large-scale processing results containing 900,000 samples, offering multiple geometric representations such as point clouds, meshes, and voxels for each asset, accompanied by rendered images and text annotations, constructing relatively complete training data asset package. In summary, although existing works have improved Objaverse from different dimensions including quality filtering, orientation normalization, and multi-modal representation, the following systemic deficiencies still exist: (1) Insufficient comprehensiveness and depth in data processing, lacking complete processing pipeline that covers format standardization, topology repair, highquality rendering, and diverse sampling; (2) Filtered data still requires complex workflows before it can be used for training; (3) Processed data struggles to meet the training requirements of current 3D generation models due to issues such as fixed rendering viewpoints and single point cloud sampling strategy. In contrast, we employ complete processing pipeline to curate high-quality data and process them into training-ready asset packages for researchers to use. Furthermore, we generate collection of high-quality product assets as supplement. 4.2 Full-level Data Processing Figure 3: Full-level Data Processing Pipeline. 6 For full-level data, our processing pipeline consists primarily of three core steps: data rendering and format conversion, asset filtering, and post processing. Through this systematic processing workflow, we are able to obtain collection of high-quality, training-ready data for static 3D generation networks. The overall processing pipeline is illustrated in Figure 3, with each step carefully designed to ensure the quality and consistency of the output data. Data Rendering and Conversion. Considering the diverse sources and varied formats of 3D assets in the original Objaverse dataset [10, 11], we first need to establish unified data standard. We combine manual annotation and automated conversion workflows to uniformly convert all 3D assets into single-frame static mesh representations with aligned orientations. This standardization step is crucial for subsequent processing, as it eliminates coordinate system differences between different modeling software and excludes multi-view rendering inconsistencies caused by model animations. Subsequently, we use Blender as the rendering engine to perform multi-view rendering of each standardized static mesh. The rendering configuration includes two camera modesorthographic projection and perspective projectionto cover different visual representation requirements. Finally, we uniformly export and store the processed static meshes in PLY format, which offers excellent cross-platform compatibility and efficient storage characteristics. Assets Filtering. To ensure the high quality of training data, we establish rigorous multidimensional filtering criteria. We comprehensively utilize the visual quality of rendering results and the geometric attributes of original 3D assets for data filtering, primarily excluding the following three categories of inadequate data: (1) Data with poor geometric quality. The original assets contain large number of duplicated and overly simplified 3D assets. These assets typically exhibit: extremely low polygon counts, lack of necessary geometric details, and overly simple topological structures. We identify and exclude such assets by setting polygon count thresholds and calculating geometric complexity metrics. Retaining geometrically rich meshes with sufficient details can provide more valuable learning signals for the model. (2) Data with poor texture quality. Texture quality directly affects the visual performance of rendering results. We exclude data with low image rendering quality due to the following reasons: serious UV mapping problems; overlapping faces in the geometry causing abnormal texture display; excessively low texture resolution or missing texture maps, etc. (3) Data with large areas of thin structures. Thin structures pose special challenges in 3D generation tasks. We choose to exclude such data based on two main considerations: On one hand, from the perspective of implicit representations, the Signed Distance Field (SDF) at thin structures undergoes abrupt jumps, transitioning from positive to negative values within an extremely small spatial range, which significantly increases the difficulty of model learning and fitting and can easily lead to training instability; On the other hand, from the perspective of multi-view consistency, thin structures under certain viewpoints are difficult to observe or even completely invisible in 2D images (such as when viewing along the thin sheet direction), which reduces the stability and convergence speed of model learning. Therefore, excluding assets containing large areas of thin structures helps improve overall training effectiveness. Post Processing. After obtaining high-quality 3D assets, we further perform post-processing to generate training-ready data. The post-processing steps primarily include: watertight processing and point cloud sampling. (1) Watertight processing. Given an artist-created triangle mesh, we first compute the Unsigned Distance Field (UDF) on uniform grid with 5123 resolution, and extract an ϵ-contour thin shell mesh using Marching Cubes with ϵ = 1/512. We then sample points on and apply Delaunay triangulation to construct volumetric tetrahedral mesh. Following the approach of ConvexMeshing [14], we optimize the tetrahedral cell labels (0 for inner, 1 for outer) using graph cut optimization, and extract the boundary surface as the final watertight mesh. (2) Point cloud sampling. Following the sampling strategies of Dora [6] and Hunyuan3D 2.1 [27], we implement hybrid sampling scheme on watertight meshes by combining surface uniform sampling and edge importance sampling to ensure that the sampled point clouds can both adequately represent the overall geometric shape and accurately capture local detail features. It is worth noting that we rotated the coordinate system to Y-up during the post-processing stage. 4.3 Part-level Data Processing For part-level data, we have designed specialized data processing pipeline aimed at decomposing holistic static meshes into semantically consistent component collections. This pipeline consists primarily of three core steps: part splitting, asset filtering, and post-processing. Through this systematic processing workflow, we are able to transform original holistic static meshes into partFigure 4: Part-level Data Processing Pipeline. level components suitable for training part-aware generation networks. The overall processing pipeline is illustrated in Figure 4. Part Splitting. Part splitting is the critical step of breaking down holistic meshes into meaningful part units. We adopt splitting strategy based on topological connectivity, first utilizing Connected Component Analysis to perform initial splitting of 3D assets, obtaining collection of topologically independent original components. This step can automatically identify physically separated parts within the mesh, aligning the division of the holistic mesh with the semantic granularity designed by artists during the creation process. After obtaining the initial decomposition results, we need to perform preliminary quality control filtering to exclude two types of extreme cases: (1) Complex assets with excessive components (component count > 888): These assets typically contain numerous trivial small parts or decorative elements, whose excessive complexity significantly increases the difficulty of data processing; (2) Indivisible assets (component count < 2): These assets cannot provide structural information at the part-level and do not meet the data requirements of part-aware generation tasks and are therefore excluded. To address the over-fragmentation issue in the initial decomposition results, we further implement an automatic merging strategy. Specifically, we calculate the surface area of each original component and set area thresholds to identify small trivial parts. For components with areas significantly below the threshold, we merge them into adjacent larger components based on spatial adjacency relationships, thereby obtaining more reasonable part granularity. After this merging process, the final component count for the vast majority of assets is controlled between 10 and 40, range that both retains sufficient semantic granularity and avoids excessive complexity, making it highly suitable for the training requirements of part-level generation tasks. Asset Filtering. After completing part splitting, we establish rigorous set of multi-dimensional filtering criteria to ensure that the retained data possesses both reasonable part-level structure and is suitable for model learning. The specific filtering rules are as follows: (1) Component quantity reasonableness verification. We exclude data with too few components (1) or too many components (>50). Too few components indicate splitting failure or that the asset itself lacks structural complexity; too many components suggest that even after merging, the asset remains overly complex and may affect network learning. This filtering ensures that all assets in the dataset have moderate part-level complexity. (2) Part scale balance verification. We exclude data where the area of single component exceeds 85% of the total area of the surface of the object. These assets have extremely imbalanced part distributions, typically manifesting as one massive dominant part accompanied by several tiny auxiliary parts (such as large tabletop with tiny leg connectors). This imbalanced part distribution is detrimental to the model learning reasonable proportional relationships and compositional logic among parts, and is therefore excluded. (3) Isolated small part quantity verification. We exclude data containing too many isolated small-area components. These isolated small parts are often decorative trivial elements that typically do not provide valuable semantic information and can interfere with the models learning of relationships among major parts. By counting the proportion of isolated small parts, we can effectively identify and filter such low-quality data. Post Processing. The post processing step aims to generate complete training data packages for each asset that passes the filtering. First, we perform systematic multi-view rendering of assets based on splitting results, generating two types of complementary image data: (1) RGB texture images: Rendered using original texture maps to obtain realistic appearance representations; (2) Part ID masks: Based on the splitting results, we assign unique ID to each part and render 2D part mask images. In the mask images, each pixels value corresponds to the ID of the part it belongs 8 to. By simultaneously providing RGB images and part masks, this data can be used for training controllable part-aware object generation model. Subsequently, we perform watertight processing on the geometric data, separately processing the holistic mesh and individual part meshes: (1) Holistic mesh watertightening: We perform watertight processing on the merged complete object mesh to generate topologically closed holistic representation; (2) Part mesh watertightening: We perform watertight processing on each independent part mesh separately, ensuring that each part is topologically closed geometric entity. This step is crucial because many parts may have open boundaries at connection points after decomposition, and watertight processing can complete these boundaries, making each part an independent, complete 3D object. Through the above complete processing pipeline, we ultimately obtain high-quality part-level 3D dataset, with each sample containing: reasonable number of semantically consistent parts, multiview RGB images and part masks, and watertight holistic and part meshes. This rich data lays solid foundation for training powerful part generation models, fine-grained 3D understanding or editing models, and simulation environments supporting complex robotic manipulation. 4.4 Synthetic Data Generation Figure 5: Synthetic Data Generating Pipeline. We leverage the powerful priors of generative models to synthesize data, aiming to bridge the significant gap in sample counts across object categories that exists in real-world datasets. To achieve this goal, our data synthesis pipeline consists of three main steps: text expansion, image generation, and 3D generation. The overall pipeline is illustrated in the Figure 5. Text Expansion. We first collected and organized complete e-commerce product category system from mainstream e-commerce platforms and product databases, constructing category hierarchy that comprehensively covers real-world products. After excluding service-oriented virtual products (such as insurance, membership services.), we ultimately retained 1,252 specific physical product categories. Using these product categories as semantic conditions, we employ an LLM model to generate detailed and diverse product descriptions. Our prompt design is centered around the following three points: (1) Ensuring basic rationality and authenticity, generating physically and logically reasonable descriptions around the category; (2) Providing rich visual details, including key attributes such as the objects shape, material, color, and size proportions; (3) Expanding diversity, imaginatively expanding the products form, materials, and other content within reasonable range, setting aside limitations of actual craftsmanship, cost, and other factors. Image Generation. We select Qwen-Image to transform text descriptions into images. Although this model performs excellently in text understanding and image quality, as general-purpose text-to-image model, it often generates images containing complex backgrounds, or viewpoints unsuitable for 3D generation. To ensure that the generated images are suitable for subsequent 3D generation step, we customize the model behavior through LoRA fine-tuning. Specifically, our fine-tuning objective is to enable the model to generate images that meet the following quality standards: (1) Clean background: Solid color or simple gradient backgrounds with no complex scene elements, facilitating the separation of foreground objects; (2) Complete object: Ensuring that the overall geometric features can be accurately captured; (3) Appropriate position: The object is located at the image center, occupying suitable proportion of the frame, avoiding being too large or too small; (4) Reasonable view point: Adopting three-quarter views or other information-rich observation angles that can simultaneously display multiple faces of the object, providing sufficient geometric cues for 3D generation; (5) Information-rich: Clearly displaying the objects key structural features, material properties, and detail elements. 3D Generation. We select the industry-leading HY3D-3.0 model [77] as our 3D generation engine. Leveraging the powerful capabilities of the HY3D-3.0 model, we are able to obtain high-quality 3D assets with the following characteristics: (1) Fine geometry: The generated meshes possess rich geometric details, accurately reconstructing the objects shape features, including complex structures such as edges, bumps, and holes; (2) Clear textures: Accurate texture mapping, with visual attributes such as color, material, and surface details highly consistent with the input image. 4.5 Data Distribution and Visualization Full-level Data. Using Objaverse and Objaverse-XL as base data sources, we conducted rigorous quality filtering and data processing workflows, ultimately curating 252,676 high-quality 3D assets for in-depth processing. These assets have undergone the complete data processing pipeline described above, ensuring that each asset meets training-ready standards. To support model training and scientific evaluation, we perform split of the dataset: 252,000 samples are allocated to the training set for comprehensive model learning; 276 samples are allocated to the validation set for hyperparameter tuning and model selection during training; and 400 samples are allocated to the test set for final model performance evaluation and benchmarking. In terms of category coverage, the entire dataset spans 19 top-level categories, further subdivided into 74 mid-level subcategories, and ultimately contains 389 fine-grained classifications, such as Animal-Virtual/Extinct Animals-Anthropomorphic Animals, Weapon-Firearms-Guns. Figure 6: The Top-level Category Distribution of Full-level Data. Figure 7: Visualization of the full-level dataset, including sharp edge point clouds, random surface point clouds, watertight meshes, and rendered images. Part-level Data. The part-level dataset comprises 240,524 samples in total, with mean component count of 14.13 and median of 11, exhibiting diverse distribution of component complexity. 10 Specifically, 24.63% of samples contain 2-5 components, representing relatively simple object structures; 24.83% of samples contain 6-10 components, covering objects with moderate structural complexity; 27.00% of samples contain 11-20 components, encompassing more intricate assemblies; and the remaining samples contain 21-50 components, representing highly complex multi-part objects. The detailed statistical distribution of component counts is illustrated in Fig. 8. Figure 8: The Component Distribution of Part-level Data.The prominent peaks at 16, 34, and 35 primarily stem from humanoid models that share identical geometric structures but differ in texture. Considering that various research scenarios and application needs may require such texture variant data, we chose to retain this portion of the data without deduplication. Figure 9: Part-level dataset visualization, showing individual components and the assembled model color-coded by component ID. Synthetic Data. The Synthetic Data contains more than 125k samples. The category system design 11 of this dataset fully considers the needs of real-world applications, ultimately encompassing 20 top-level categories, 130 mid-level subcategories, and 1,252 fine-grained classifications of product data. The breadth and depth of this category system far exceed existing real datasets, with coverage ranging from daily necessities and consumer electronics to professional industrial products. Figure 10: The Top-level Category Distribution of Synthetic Data. Figure 11: Synthetic dataset visualization, showing diverse samples from 5 fine-grained categories. 5 Evaluation 5.1 Implementation Details To validate the effectiveness of Full-level Data in 3D generation tasks, we use Hunyuan3D-2.1 as our baseline. While maintaining the core architectural design principles, we appropriately scale down the model to reduce training costs and train lightweight Hunyuan3D-2.1-Small model. For evaluation, we use ULIP [88] and Uni3D [103] to measure the consistency between images and generated meshes. Model Architecture Adjustments. Compared to the original Hunyuan3D-2.1 model, our Small model incorporates the following key architectural modifications to balance model capacity with training efficiency: (1) Channel dimension reduction: We reduce the base channel dimension from 2048 to 1536. (2) Architecture simplification: We remove the Mixture of Experts (MoE) structure and adopt fully Dense architecture instead. After these adjustments, our Hunyuan3D-2.1-Small model contains 832M parameters. 12 Progressive Training Strategy. Drawing on the successful experience of Hunyuan3D-2.1, we employ progressive token resolution training strategy, starting from 512 tokens and gradually increasing the token count to improve representation fidelity, ultimately reaching 4096 tokens. Detailed training configurations are provided in Table 1. Tokens 512 2048 2048 Batch size 512 256 256 128 Image Size 224 224 518 518 Learning rate 1.e-4 5.e-5 5.e-5 1.e-5 Traning steps 800k 400k 200k 400k Table 1: Hunyuan3D-2.1-Small Training Strategy. 5.2 Experimental Results To comprehensively evaluate the effectiveness of our full-level dataset, we conducted comparative experiments with several representative state-of-the-art open-source methods, including Michelangelo [101], Craftsman [38], Trellis [86], and Hunyuan3D 2.1 [27]. These baseline methods have all demonstrated outstanding performance in the field of 3D generation. As shown in Table 2 and Figure 12, despite having significantly fewer parameters than Trellis and Hunyuan3D 2.1, our model achieves comparable generation quality when trained on our open-sourced dataset, while outperforming the similarly-sized Craftsman. This experimental result fully demonstrates the high-quality characteristics of our open-sourced dataset. Meanwhile, this also indicates that data quality plays crucial role in 3D generation tasks. The dataset we have constructed can provide the community with an efficient training resource, enabling researchers to focus more on algorithm innovation and model optimization rather than tedious data processing and preparation work. Methods Michelangelo [101] CraftsMan [38] Trellis [86] Hunyuan3D 2.1 [27] Ours Token length Model Size (M) Uni3D-I ULIP-I 0.2186 0.2264 0.2454 0.2446 0.2424 257 2048 10000* 4096 4096 0.3169 0.3351 0.3641 0.3636 0.3606 105 852 1156 1238 Table 2: The quantitative comparison for image-to-3D generation on our test dataset.* denotes the average token length for active voxel. 6 Conclusion In this work, we present HY3D-Bench, an open-source ecosystem designed to surmount the data processing bottlenecks currently constraining 3D generative models. We establish unified foundation through three key contributions. First, we curate high-fidelity library of 252k 3D objects derived from large-scale repositories such as Objaverse and Objaverse-XL. We employ rigorous, multi-stage pipeline to ensure training readiness, producing essential artifacts such as watertight meshes and multi-view renderings. Second, we introduce 240k structured part-level decomposition, providing the granularity essential for advancing fine-grained perception, partaware generation, and controllable 3D editing. Third, to mitigate real-world data distribution gaps, we develop scalable AIGC-driven synthesis pipeline, contributing 125k synthetic assets to enrich diversity within long-tail categories. Empirical validation using the Hunyuan3D-2.1-Small model confirms the practical utility of our dataset. By democratizing access to these resources, HY3DBench aims to catalyze innovation across 3D perception, robotics, and digital content creation. Future efforts will focus on extending this framework to include dynamic assets and broader tasks. 13 Figure 12: The qualitative comparison for image-to-3D generation on our test dataset. 7 Contributors Authors are listed alphabetically by the first name."
        },
        {
            "title": "Penghao Wang\nQingxiang Lin\nRuining Tang\nXianghui Yang\nYang Li\nYunfei Zhao\nYunhan Yang\nZeqiang Lai\nZhihao Liang\nZibo Zhao",
            "content": "Other contributors are listed alphabetically by the first name. Chao Zhang Edwarrd Wang Hao Zhang Jiaxin Lin Peng He Yirui Guan Yonghao Tan Zheng Ye 15 References [1] Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov, and Peter In ICCV, Wonka. Satr: Zero-shot semantic segmentation of 3d shapes. 2023. [2] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit generative adversarial networks for 3daware image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 57995809, 2021. [3] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1612316133, 2022. [4] Angel Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015. [5] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2023. [6] Rui Chen, Jianfeng Zhang, Yixun Liang, Guan Luo, Weiyu Li, Jiarui Liu, Xiu Li, Xiaoxiao Long, Jiashi Feng, and Ping Tan. Dora: Sampling and benchmarking for 3d shape variational auto-encoders. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1625116261, 2025. [7] Xu Chen, Tianjian Jiang, Jie Song, Jinlong Yang, Michael Black, Andreas Geiger, and Otmar Hilliges. gdna: Towards generative detailed neural avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2042720437, 2022. [8] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In CVPR, pp. 59395948, 2019. [9] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. Abo: Dataset and benchmarks for real-world 3d object understanding. CVPR, 2022. [10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. arXiv preprint arXiv:2212.08051, 2022. [11] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia 16 Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. [12] Ken Deng, Yunhan Yang, Jingxiang Sun, Xihui Liu, Yebin Liu, Ding Liang, and Yan-Pei Cao. Geosam2: Unleashing the power of sam2 for 3d part segmentation. arXiv preprint arXiv:2508.14036, 2025. [13] Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, and Kui Jia. 3d affordancenet: benchmark for visual object affordance understanding. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 17781787, 2021. [14] Lorenzo Diazzi and Marco Attene. Convex polyhedral meshing for robust solid modeling. ACM Transactions on Graphics (TOG), 40(6):116, 2021. [15] Lihe Ding, Shaocong Dong, Yaokun Li, Chenjian Gao, Xiao Chen, Rui Han, Yihao Kuang, Hong Zhang, Bo Huang, Zhanpeng Huang, et al. Fullpart: Generating each 3d part at full resolution. arXiv preprint arXiv:2510.26140, 2025. [16] Qiujie Dong, Jiepeng Wang, Rui Xu, Cheng Lin, Yuan Liu, Shiqing Xin, Zichun Zhong, Xin Li, Changhe Tu, Taku Komura, et al. Crossgen: Learning and generating cross fields for quad meshing. arXiv preprint arXiv:2506.07020, 2025. [17] Shaocong Dong, Lihe Ding, Xiao Chen, Yaokun Li, Yuxin Wang, Yucheng Wang, Qi Wang, Jaehyeok Kim, Chenjian Gao, Zhanpeng Huang, et al. From one to more: Contextual part latents for 3d generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 82308240, 2025. [18] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: high-quality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pp. 2553 2560. IEEE, 2022. [19] Michael Fischer, Iliyan Georgiev, Thibault Groueix, Vladimir Kim, Tobias Ritschel, and Valentin Deschaintre. Sama: Material-aware 3d selection and segmentation. arXiv preprint arXiv:2411.19322, 2024. [20] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts and semantics. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1093310942, 2021. [21] Haoran Geng, Helin Xu, Chengyang Zhao, Chao Xu, Li Yi, Siyuan Huang, and He Wang. Gapartnet: Cross-category domain-generalizable object perception and manipulation via generalizable and actionable parts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 70817091, 2023. [22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David WardeFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 17 [23] Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, Wanli Ouyang, Yan-Pei Cao, and Yangguang Li. Sparseflex: High-resolution and arbitrary-topology 3d shape modeling. arXiv preprint arXiv:2503.21732, 2025. [24] Xufan He, Yushuang Wu, Xiaoyang Guo, Chongjie Ye, Jiaqing Zhou, Tianlei Hu, Xiaoguang Han, and Dong Du. Unipart: Part-level 3d generation with unified 3d geom-seg latents. arXiv preprint arXiv:2512.09435, 2025. [25] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. [26] Zixuan Huang, Mark Boss, Aaryaman Vasishta, James Rehg, and Varun Jampani. Spar3d: Stable point-aware reconstruction of 3d objects from single images. arXiv preprint arXiv:2501.04689, 2025. [27] Team Hunyuan3D, Shuhui Yang, Mingxin Yang, Yifei Feng, Xin Huang, Sheng Zhang, Zebin He, Di Luo, Haolin Liu, Yunfei Zhao, et al. Hunyuan3d 2.1: From images to high-fidelity 3d assets with production-ready pbr material. arXiv preprint arXiv:2506.15442, 2025. [28] Team Hunyuan3D, Bowen Zhang, Chunchao Guo, Haolin Liu, Hongyu Yan, Huiwen Shi, Jingwei Huang, Junlin Yu, Kunhong Li, Penghao Wang, et al. Hunyuan3d-omni: unified framework for controllable generation of 3d assets. arXiv preprint arXiv:2509.21245, 2025. [29] Li Jin, Yujie Wang, Wenzheng Chen, Qiyu Dai, Qingzhe Gao, Xueying Qin, and Baoquan Chen. One-shot 3d object canonicalization based on geometric and semantic consistency. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1685016859, 2025. [30] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023. [31] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 81108119, 2020. [32] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk uhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [33] Hyunjin Kim and Minhyuk Sung. Partstad: 2d-to-3d part segmentation task adaptation. In ECCV, 2024. [34] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. 18 [35] Peng Li, Yuan Liu, Xiaoxiao Long, Feihu Zhang, Cheng Lin, Mengfei Li, Xingqun Qi, Shanghang Zhang, Wei Xue, Wenhan Luo, et al. Era3d: Highresolution multiview diffusion using efficient row-wise attention. Advances in Neural Information Processing Systems, 37:5597556000, 2024. [36] Ruihui Li, Xianzhi Li, Ka-Hei Hui, and Chi-Wing Fu. Sp-gan: Sphere-guided 3d shape generation and manipulation. ACM Transactions on Graphics (TOG), 40(4):112, 2021. [37] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweetdreamer: Aligning geometric priors in 2d diffusion for consistent text-to-3d. ICLR, 2024. [38] Weiyu Li, Jiarui Liu, Hongyu Yan, Rui Chen, Yixun Liang, Xuelin Chen, Ping Tan, and Xiaoxiao Long. Craftsman: High-fidelity mesh generation with 3d native generation and interactive geometry refiner, 2024. [39] Yang Li, Victor Cheung, Xinhai Liu, Yuguang Chen, Zhongjin Luo, Biwen Lei, Haohan Weng, Zibo Zhao, Jingwei Huang, Zhuo Chen, et al. Autoregressive surface cutting. arXiv preprint arXiv:2506.18017, 2025. [40] Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, et al. Triposg: High-fidelity 3d shape synthesis using large-scale rectified flow models. arXiv preprint arXiv:2502.06608, 2025. [41] Zhiqi Li, Wenhuan Li, Tengfei Wang, Zhenwei Wang, Junta Wu, Haoyuan Wang, Yunhan Yang, Zehuan Huang, Yang Li, Peidong Liu, et al. Moca: Mixture-of-components attention for scalable compositional 3d generation. arXiv preprint arXiv:2512.07628, 2025. [42] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xiaogang Xu, and Yingcong Chen. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. arXiv preprint arXiv:2311.11284, 2023. [43] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, 2023. [44] Chendi Lin, Heshan Liu, Qunshu Lin, Zachary Bright, Shitao Tang, Yihui He, Minghao Liu, Ling Zhu, and Cindy Le. Objaverse++: Curated 3d object dataset with quality annotations. arXiv preprint arXiv:2504.07334, 2025. [45] Yuchen Lin, Chenguo Lin, Panwang Pan, Honglei Yan, Yiqiang Feng, Yadong Mu, and Katerina Fragkiadaki. Partcrafter: Structured 3d mesh generation via compositional latent diffusion transformers. arXiv preprint arXiv:2506.05573, 2025. [46] Anran Liu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Zhiyang Dou, Hao-Xiang Guo, Ping Luo, and Wenping Wang. Part123: part-aware 3d reconstruction from single-view image. In ACM SIGGRAPH, 2024. [47] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds 19 without per-shape optimization. Advances in Neural Information Processing Systems, 36:2222622246, 2023. [48] Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao Su. Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models. In CVPR, 2023. [49] Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun Gao. Partfield: Learning 3d feature fields for part segmentation and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 97049715, 2025. [50] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 92989309, 2023. [51] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. [52] Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, and Weiyang Liu. Meshdiffusion: Score-based generative 3d mesh modeling. In International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=0cpM2ApF9p6. [53] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9970 9980, 2024. [54] Yichong Lu, Yuzhuo Tian, Zijin Jiang, Yikun Zhao, Yuanbo Yang, Hao Ouyang, Haoji Hu, Huimin Yu, Yujun Shen, and Yiyi Liao. Orientation matters: Making 3d generative models orientation-aligned. arXiv preprint arXiv:2506.08640, 2025. [55] Andrew Luo, Tianqin Li, Wen-Hao Zhang, and Tai Sing Lee. Surfgen: In Adversarial 3d shape synthesis with explicit surface discriminators. Proceedings of the IEEE/CVF international conference on computer vision, pp. 1623816248, 2021. [56] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2021. [57] Changfeng Ma, Yang Li, Xinhao Yan, Jiachen Xu, Yunhan Yang, Chunshi Wang, Zibo Zhao, Yanwen Guo, Zhuo Chen, and Chunchao Guo. P3-sam: Native 3d part segmentation. arXiv preprint arXiv:2509.06784, 2025. [58] Ziqi Ma, Yisong Yue, and Georgia Gkioxari. Find any part in 3d. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7818 7827, 2025. 20 [59] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [60] Kaichun Mo, Shilin Zhu, Angel Chang, Li Yi, Subarna Tripathi, Leonidas Guibas, and Hao Su. Partnet: large-scale benchmark for fine-grained and hierarchical part-level 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 909918, 2019. [61] Charlie Nash, Yaroslav Ganin, SM Ali Eslami, and Peter Battaglia. Polygen: An autoregressive generative model of 3d meshes. In International conference on machine learning, pp. 72207229. PMLR, 2020. [62] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and YongLiang Yang. Hologan: Unsupervised learning of 3d representations from natural images. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 75887597, 2019. [63] Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yongliang Yang, and Niloy Mitra. Blockgan: Learning 3d object-aware scene representations from unlabelled images. Advances in neural information processing systems, 33: 67676778, 2020. [64] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. [65] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In CVPR, pp. 165174, 2019. [66] Soumava Paul, Prakhar Kaushik, Ankit Vaidya, Anand Bhattad, and Alan Yuille. Name that part: 3d part segmentation and naming. arXiv preprint arXiv:2512.18003, 2025. [67] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [68] Xuelin Qian, Yu Wang, Simian Luo, Yinda Zhang, Ying Tai, Zhenyu Zhang, Chengjie Wang, Xiangyang Xue, Bo Zhao, Tiejun Huang, et al. Pushing auto-regressive models for 3d shape generation at capacity and scalability. arXiv preprint arXiv:2402.12225, 2024. [69] Xuanchi Ren, Jiahui Huang, Xiaohui Zeng, Ken Museth, Sanja Fidler, and Francis Williams. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 42094219, 2024. [70] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3d-aware image synthesis. Advances in neural information processing systems, 33:2015420166, 2020. 21 [71] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations, 2023. [72] Dong Wook Shu, Sung Woo Park, and Junseok Kwon. 3d point cloud generative adversarial network based on tree structured graph convolutions. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 38593868, 2019. [73] George Tang, William Zhao, Logan Ford, David Benhaim, and Paul Zhang. Segment any mesh: Zero-shot mesh part segmentation via lifting segment anything 2 to 3d. arXiv:2408.13679, 2024. [74] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. [75] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. In European Conference on Computer Vision, pp. 118. Springer, 2024. [76] Jiaxiang Tang, Ruijie Lu, Zhaoshuo Li, Zekun Hao, Xuan Li, Fangyin Wei, Shuran Song, Gang Zeng, Ming-Yu Liu, and Tsung-Yi Lin. Efficient part-level 3d object generation via dual volume packing. arXiv preprint arXiv:2506.09980, 2025. [77] Tencent. Hunyuan3d 3.0. https://3d.hunyuan.tencent.com/, 2026. Accessed: 2026-01-14. [78] Anh Thai, Weiyao Wang, Hao Tang, Stefan Stojanov, Matt Feiszli, and James Rehg. 3x2: 3d object part segmentation by 2d semantic correspondences. In ECCV, 2024. [79] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Zhang. Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024, 2023. [80] Penghao Wang, Yiyang He, Xin Lv, Yukai Zhou, Lan Xu, Jingyi Yu, and Jiayuan Gu. Partnext: next-generation dataset for fine-grained and hierarchical 3d part understanding. arXiv preprint arXiv:2510.20155, 2025. [81] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in neural information processing systems, 36:84068441, 2023. [82] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning probabilistic latent space of object shapes via 3d generativeadversarial modeling. Advances in neural information processing systems, 29, 2016. [83] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Jingxi Xu, Philip Torr, Xun Cao, and Yao Yao. Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint arXiv:2405.14832, 2024. [84] Shuang Wu, Youtian Lin, Feihu Zhang, Yifei Zeng, Yikang Yang, Yajie Bao, Jiachen Qian, Siyu Zhu, Xun Cao, Philip Torr, et al. Direct3d-s2: Gigascale 3d generation made easy with spatial sparse attention. arXiv preprint arXiv:2505.17412, 2025. [85] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic perception, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 803814, 2023. [86] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. [87] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. [88] Le Xue, Mingfei Gao, Chen Xing, Roberto Martın-Martın, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, images, and point clouds for 3d understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 11791189, 2023. [89] Yuheng Xue, Nenglun Chen, Jun Liu, and Wenyun Sun. Zerops: Highquality cross-modal knowledge transfer for zero-shot 3d part segmentation. arXiv:2311.14262, 2023. [90] Hongyu Yan, Kunming Luo, Weiyu Li, Yixun Liang, Shengming Li, Jingwei Huang, Chunchao Guo, and Ping Tan. Posemaster: Generating 3d characters in arbitrary poses from single image. arXiv preprint arXiv:2506.21076, 2025. [91] Xinhao Yan, Jiachen Xu, Yang Li, Changfeng Ma, Yunhan Yang, Chunshi Wang, Zibo Zhao, Zeqiang Lai, Yunfei Zhao, Zhuo Chen, et al. X-part: high fidelity and structure coherent shape decomposition. arXiv preprint arXiv:2509.08643, 2025. [92] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow: 3d point cloud generation with continuous normalizing flows. arXiv, 2019. [93] Yichen Yang, Hong Li, Haodong Zhu, Linin Yang, Guojun Lei, Sheng Xu, and Baochang Zhang. Partdiffuser: Part-wise 3d mesh generation via discrete diffusion. arXiv preprint arXiv:2511.18801, 2025. [94] Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Lam, Yan-Pei Cao, and Xihui Liu. Sampart3d: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184, 2024. 23 [95] Yunhan Yang, Yuan-Chen Guo, Yukun Huang, Zi-Xin Zou, Zhipeng Yu, Yangguang Li, Yan-Pei Cao, and Xihui Liu. Holopart: Generative 3d part amodal segmentation. arXiv preprint arXiv:2504.07943, 2025. [96] Yunhan Yang, Yufan Zhou, Yuan-Chen Guo, Zi-Xin Zou, Yukun Huang, Ying-Tian Liu, Hao Xu, Ding Liang, Yan-Pei Cao, and Xihui Liu. Omnipart: Part-aware 3d generation with semantic decoupling and structural cohesion. In Proceedings of the SIGGRAPH Asia 2025 Conference Papers, pp. 112, 2025. [97] Junliang Ye, Shenghao Xie, Ruowen Zhao, Zhengyi Wang, Hongyu Yan, Wenqiang Zu, Lei Ma, and Jun Zhu. Nano3d: training-free approach for efficient 3d editing without masks. arXiv preprint arXiv:2510.15019, 2025. [98] Taoran Yi, Jiemin Fang, Junjie Wang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and 3d diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 67966807, 2024. [99] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: 3d shape representation for neural fields and generative diffusion models. ACM Transactions On Graphics (TOG), 42(4):116, 2023. [100] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. [101] Zibo Zhao, Wen Liu, Xin Chen, Xianfang Zeng, Rui Wang, Pei Cheng, Bin Fu, Tao Chen, Gang Yu, and Shenghua Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. Advances in Neural Information Processing Systems, 36, 2024. [102] Ziming Zhong, Yanyu Xu, Jing Li, Jiale Xu, Zhengxin Li, Chaohui Yu, and Shenghua Gao. Meshsegmenter: Zero-shot mesh semantic segmentation via texture synthesis. In ECCV, 2024. [103] Junsheng Zhou, Jinsheng Wang, Baorui Ma, Yu-Shen Liu, Tiejun Huang, and Xinlong Wang. Uni3d: Exploring unified 3d representation at scale. arXiv preprint arXiv:2310.06773, 2023. [104] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 58265835, 2021. [105] Qingnan Zhou and Alec Jacobson. Thingi10k: dataset of 10,000 3d-printing models. arXiv preprint arXiv:1605.04797, 2016. [106] Yuchen Zhou, Jiayuan Gu, Tung Yen Chiang, Fanbo Xiang, and Hao Su. Point-sam: Promptable 3d segmentation model for point clouds. arXiv preprint arXiv:2406.17741, 2024. 24 [107] Zhe Zhu, Le Wan, Rui Xu, Yiheng Zhang, Honghua Chen, Zhiyang Dou, Cheng Lin, Yuan Liu, and Mingqiang Wei. Partsam: scalable promptable part segmentation model trained on native 3d data. arXiv preprint arXiv:2509.21965, 2025."
        }
    ],
    "affiliations": [
        "Tencent"
    ]
}