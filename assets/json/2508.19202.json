{
    "paper_title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning",
    "authors": [
        "Alan Li",
        "Yixin Liu",
        "Arpan Sarkar",
        "Doug Downey",
        "Arman Cohan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, we introduce SciReas, a diverse suite of existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a selective subset that requires more complex reasoning. Our holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. We then propose KRUX, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, we conduct an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct a lightweight analysis, comparing our science-focused data composition with concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline for scientific reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 2 0 2 9 1 . 8 0 5 2 : r Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning Alan Li1* Yixin Liu1* Arpan Sarkar2 Doug Downey3,4 Arman Cohan1,4 1Yale University, 2Harvard University, 3Northwestern University, 4Allen Institute of AI {haoxin.li,yixin.liu}@yale.edu"
        },
        {
            "title": "Abstract",
            "content": "Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, we introduce SCIREAS, diverse suite of existing benchmarks for scientific reasoning tasks, and SCIREAS-PRO, selective subset that requires more complex reasoning. Our holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. We then propose KRUX, probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, we conduct an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs ability to surface task-relevant knowledge. Finally, we conduct lightweight analysis, comparing our sciencefocused data composition with concurrent efforts on long CoT SFT, and release SCILIT01, strong 8B baseline for scientific reasoning."
        },
        {
            "title": "Introduction",
            "content": "Recent frontier reasoning models, such as OpenAIs o-series (OpenAI et al., 2024) and DeepSeekR1 (DeepSeek-AI et al., 2025), demonstrate significant advances by leveraging increased testtime compute to enable intermediate reasoning *These authors contributed equally to this work. 1The codebase and artifacts are released at https:// github.com/yale-nlp/SciReas-Eval. 1 steps (Wei et al., 2023; Kojima et al., 2023). These approaches facilitate advanced mechanisms, including methodology exploration (Yao et al., 2023), self-verification (Ma et al., 2025a), and backtracking (Yang et al., 2025b), resulting in improvements on tasks such as mathematics and coding with more test-time compute (Muennighoff et al., 2025). These advances in reasoning capabilities open up opportunities for applying LLMs to complex scientific tasks (Lu et al., 2024; Gottweis et al., 2025; Schmidgall et al., 2025). However, scientific work demands not only rigorous reasoning but also deep domain knowledge, from specialized concepts and foundational theories to hands-on methodological expertise and familiarity with obscure yet pivotal findings. Successful scientific reasoning systems must apply such knowledge in complex multi-step reasoning processes (Zhao et al., 2023; Wang et al., 2023a; Wadden et al., 2024a; Li et al., 2025). While variety of scientific benchmarks exist (e.g., GPQA (Rein et al., 2024) and MMLUPro (Wang et al., 2024b)), there is no holistic and unified benchmark that comprehensively targets scientific reasoning. Existing individual benchmarks typically focus narrowly on specific domains, task formats, or skill types. For example, although GPQA is challenging, it focuses exclusively on multiple-choice questions within limited range of domains. Furthermore, there is lack of analytical tools that can isolate the distinct roles that reasoning and scientific knowledge play when performing sophisticated scientific tasks. We introduce datasets and methods to facilitate the study of scientific problem solving. First, we present SCIREAS, unified suite of ten public benchmarks that span physics, chemistry, biology, medicine, materials, mathematics, computer science, and engineering, with multiple-choice, fill-in-the-blank, structured, and protocol/procedural questions. To improve evaluation efficiency and sharpen the focus on reasoning difficulty, we manuFigure 1: KRUX pipeline. Starting from the upper left, we prompt an LLM (one of base, DeepSeek-R1, Base-Math, Base-STEM, and Base-BOTH) with question from SCIREAS as knowledge source, collect the output and reasoning traces, and feed the reasoning traces to DeepSeek-R1 as the extractor to generate knowledge ingredients (KIs). We then evaluate the tested model with KI-augmented questions, which allows us to study three key research questions (RQ1, RQ2, RQ3) regarding LLMs knowledge and reasoning capabilities in scientific problem-solving. ally inspect each subtask and retain only those that are subject-relevant and reasoning-intensive, while preserving broad domain coverage. Furthermore, to facilitate standardized evaluation, we provide an efficient and unified implementation of streamlined assessment across individual benchmarks, avoiding the need to set up different environments or dataset-specific boilerplate for each dataset (3). Next, we introduce SCIREAS-PRO, compact subset of SCIREAS tailored for evaluating more challenging reasoning. Specifically, SCIREASPRO is constructed by selecting examples from SCIREAS where only reasoning models with high inference-time compute budget (or the highest allowed number of thinking tokens) succeed. We find that despite containing only 8% as many examples as SCIREAS, SCIREAS-PRO better differentiates weak and strong reasoners (3). Having constructed the reasoning-intensive scientific benchmarks, our next goal is to leverage them to study how verbalized chain-of-thought (CoT) reasoning affects knowledge recall and usage (4). To study this, we design KRUX (Knowledge & Reasoning Utilization eXams), probing framework which supplies models with atomic knowledge ingredients (KIs) extracted from other models reasoning traces. This technique allows for more controlled analyses of reasoning and knowledge, which we use to perform three in-depth investigations that lead to the following findings: (1) Vanilla instruct models can outperform their reasoning counterparts by 10% once KIs are provided in-context, suggesting that internalizing and retrieving the right knowledge is key bottleneck for scientific reasoning tasks. (2) When both model families receive the same KIs from strong reasoner (e.g., DeepSeek-R1), the reasoning-fine-tuned models consistently outperform the base models, showing that reasoning models are capable of utilizing external incontext knowledge for additional improvements. (3) Feeding KIs from reasoning-fine-tuned model to its base model can boost performance even when the KIs are already known by the base model, indicating that reasoning-fine-tuning aids knowledge recall by surfacing more relevant knowledge. Our contributions can be summarized as: We introduce SCIREAS, unified and holistic benchmark suite spanning broad range of scientific domains and problem types, allowing us to surface insights that otherwise remain hidden if relying on individual datasets only. We also release reasoning-focused subset SCIREAS-PRO that allows efficient benchmarking of sophisticated reasoning with more room for improvement. We present KRUX, novel analytic framework which we use to conduct comprehensive empirical study that disentangles the impacts of knowledge and reasoning. We provide an in-depth analysis with three key findings: (i) knowledge retrieval is bottleneck; (ii) in-context knowledge consistently benefits reasoning models; and (iii) long CoT improves knowledge surfacing. We support these findings with controlled post-training experiments. Finally, to foster the development of open-source scientific reasoning models, we conduct lightweight analyses comparing our Math+STEM data composition with concurrent long CoT supervised fine-tuning (SFT) post-training efforts, and release SCILIT01, strong scientific reasoning baseline built on Qwen3-8B-Base (Yang et al., 2025a)."
        },
        {
            "title": "2 Related Work",
            "content": "Scientific Benchmarks Existing scientific benchmarks span wide array of domains and tasks, but each tends to focus on specific disciplines or subskills, often lacking explicit emphasis on multi-step reasoning or standardized implementation. For example, most tasks in SciRIFF (Wadden et al., 2024a) focus on context-grounded information QA, rather than demanding reasoning. Benchmarks like GPQA (Rein et al., 2024) and LabBench (Laurent et al., 2024) pose reasoning challenges, yet they cover only limited range of scientific domains and rely on multiple-choice QA formats. Implementation-wise, benchmarks lack standardized prompts, evaluation metrics, or consistent scoring, making reproducibility and fair comparison difficult (Gu et al., 2025; Gao et al., 2024). To address this fragmentation, our study systematically incorporates 10 prominent scientific benchmarks, GPQA, MMLU-Pro (Wang et al., 2024b), SuperGPQA (Team et al., 2025b), LabBench, OlympiadBench (He et al., 2024), SciBench (Wang et al., 2023b), SciRIFF, UGPhysics (Xu et al., 2025), SciEval (Sun et al., 2024), and SciKnowEval (Feng et al., 2024), enabling unified, comprehensive, and reproducible evaluation of scientific reasoning capabilities. Knowledge & Reasoning An important line of work on disentangling reasoning and knowledge designs specialized tasks (e.g., linguistically challenging questions (Bean et al., 2024; Khouja et al., 2025) or synthetic multi-hop questions (Li and Goyal, 2025)) to isolate reasoning from knowledge, but such benchmarks are often artificial and domain-constrained. Notably, Li and Goyal (2025) analyzes the synergy between knowledge and reasoning as knowledge evolves, offering perspective complementary to our controlled CoT SFT experiments. Another line of work trains external classifiers to label questions as reasoningor knowledge-intensive based on parametric models (Thapa et al., 2025). However, this approach requires well-calibrated training data and does not consider the tested models internal knowledge; for instance, question labeled as requiring reasoning might be directly memorized by the model. Concurrent work leverages reasoning traces to evaluate factual correctness (Wu et al., 2025), but focuses on surface-level factuality rather than genuine knowledge recall. With KRUX, we extract answer-agnostic, atomic knowledge points directly from models reasoning traces and evaluate their effect under controlled availability. Unlike prior work that trains external classifiers to label question types or checks surface factuality in traces, KRUX holds knowledge constant and varies the target model, isolating knowledge recall from reasoning ability without relying on heuristic difficulty tags. Additional related work is provided in Appendix A."
        },
        {
            "title": "Scientific Reasoning",
            "content": "Given limited coverage in terms of domain, formats, or accessibility for individual benchmarks, SCIREAS solves this by merging ten datasets under one standardized harness, offering broad domain coverage and consistent evaluation. SCIREAS SCIREAS is unified evaluation suite focused on reasoning-intensive scientific tasks curated from 10 representative existing benchmarks. Through task-level filtering, SCIREAS reduces instance count by nearly 50% while preserving coverage, and, inspired by OLMES (Gu et al., 2025), provides unified implementation optimized with vLLM (Kwon et al., 2023) and batch job APIs2 for scalable, easy-to-use, and efficient evaluation. Our curation prioritizes subtasks from each benchmark that demand not only specific domain knowledge but also complex, multi-step reasoning processes for resolution. For each subtask from each benchmark, we manually inspect at least 20 instances. We manually determine (1) whether the given task requires an in-depth understanding of domain-specific scientific knowledge beyond the information provided in-context, and (2) whether multi-step reasoning is necessary to reach the correct answer. We incorporate tasks only if all 20 sampled instances fulfill both requirements.3 To keep evaluation cost-efficient under compute constraints, we uniformly sample 200 instances from each subtask sourced from high-cost benchmarks MMLU-Pro, SciKnowEval, SciEval, and 2We provide batch job inference options for popular LLM providers, e.g., OpenAI, Anthropic, TogetherAI, and Gemini. Using batch APIs allows for up to 50% cost reduction. 3While this manual inspection can be subjective, the judgment is based on the authors graduate-school-level expertise. 3 Figure 2: Frontier reasoning models performance evaluated on SCIREAS. The X-axis shows the cost per 1k instances in USD. Different reasoning settings on the same model can result in distinct costs and performance, but the margins vary depending on the models. Figure 3: An example pair with varying reasoning intensity, where the example on the left is sampled from SCIREAS-PRO and the right is filtered out example (3). On the left, the progressive reasoning chain is highlighted. The example on the right emphasizes knowledge recall on each option with simple elimination strategy. UGPhysics, which maintains similar evaluation outcomes (more in Appendix B.2) while reducing the cost by nearly 50% (from 29,604 to 15,567 total instances). The complete list of selected subtasks, their subject coverage, data sources, and evaluation metrics appears in Appendix B.1. Benchmarks affected by our filtering are marked with an asterisk (*); their scores are not directly comparable to those from prior work. SCIREAS-PRO Although SCIREAS provides uniform measurement for model performance on scientific reasoning tasks that nominally require scientific reasoning, the difficulty of individual instances is uneven: some can be answered with little or no deductive effort once the pertinent fact is recalled, as shown in an example in Figure 3. To isolate the reasoning skill, we therefore curate hard subset those questions whose solutions still demand multi-step inference even when all relevant knowledge is available so that any performance gains cannot be explained by knowledge recall alone. Building on our observation in 3.1.1, we hypothesize that the performance difference under different test-time inference budgets can serve as an effective indicator of reasoning intensity. Specifically, instances where reasoning models fail with low reasoning budget but succeed with high budget likely require complex reasoning processes, even when the necessary domain knowledge is accessible to the model in both settings. In practice, we evaluate o3-mini and o4-mini on SCIREAS with both high and low reasoning-effort settings an OpenAI API flag that limits the number of thinking tokens before the answer. For o3mini and o4-mini, the high-effort setting costs at least 5.8 more per instance than the low-effort setting (Table 6, Appendix B.1).4 For each model, we keep questions answered incorrectly under low effort but correctly under high effort and take the 4Because these models are proprietary, factors beyond the flag may influence performance. We therefore treat the flag as practical, not absolute, proxy and validate it with an independent LLM-judge study (Appendix B.3.2). 4 Figure 4: SCIREAS correlations breakdown. (a) Task-to-task Pearson correlations. SCIREAS incorporates tasks complementary to popular benchmarks. (b) and (c) show performance on SCIREAS vs. SciBench and MMLU-Pro*. Models may be tuned for certain tasks, outperforming higher-ranked models on individual benchmarks. union of these sets to create SCIREAS-PRO, resulting in 1,260 unique instances. We further validate this approach by using LLM judge as well as human evaluation to check the reasoningintensiveness of resulting examples from this filtering pipeline. Appendix B.3 shows that both human annotators and LLM judges find SCIREAS-PRO to be indeed richer in reasoning-intensive instances."
        },
        {
            "title": "3.1 Benchmarking Frontier Models",
            "content": "Having constructed SCIREAS and SCIREAS-PRO with focus on scientific reasoning tasks, we now examine how state-of-the-art models perform under varying computational budgets. We evaluate frontier models using different reasoning-effort settings (see configuration details in Appendix C). These settings typically correspond to significant differences in output length, with high-effort modes producing substantially more reasoning tokens as they work through complex problems."
        },
        {
            "title": "3.1.1 Results",
            "content": "Aggregated Results Figure 2 highlights aggregated performance and rankings evaluated on SCIREAS, with score breakdowns on selected models shown in Table 6. Notably, the aggregated ranking provides additional insights that differ from popular individual benchmarks. Comparing o3-High and Gemini-2.5-Pro-Preview-High as an example, o3-High wins on GPQA and MMLUPro* while Gemini-2.5-Pro-Preview-High wins on SuperGPQA*, all with thin margin (within 1 absolute point, even evaluated on MMLU-Pro before uniform sampling as shown in Figure 7). Similarly, GPT-5-High shows on-par performance with 5In this work, we refer to DeepSeek-R1-0528 and DeepSeek-V3-0324 simply as DeepSeek-R1 and DeepSeekV3, respectively, unless otherwise specified. Gemini-2.5-Pro-Preview-High on problem-solving benchmarks like OlympiadBench and SciRIFF. Evaluated across SCIREAS, however, we notice that GPT-5-High outperforms its competitors on broader range of benchmarks. Meanwhile, o3-High achieves higher overall performance over Gemini2.5-Pro-Preview-High, with superior performance on LabBench* and weaker on OlympiadBench by large margin (beyond 10 absolute points). Benchmark Correlations In general, as the Pearson correlation analysis shows in Figure 4 (a), while some benchmarks are closely correlated (e.g., GPQA and SuperGPQA*), benchmarks containing free-form QA and fill-in-the-blank questions like SciRIFF* and SciEval* are not highly correlated with GPQA-like multiple-choice tasks, demonstrating the need for holistic evaluation suite. Isolating specific benchmarks, we observe that models from different providers may be tuned explicitly for specific tasks or skills. As shown in Figure 4 (b) and (c), Qwen3-32B-Thinking strikes noticeably above the trend on SciBench, reaching comparable performance to commercial frontier models. Similarly, DeepSeek-V3 and DeepSeekR1-0120 demonstrate stronger performance on MMLU-Pro*, indicating capabilities that surpass their overall rankings. Performance Gap by Reasoning Difference Although the gap varies depending on different model families and providers, the same model can exhibit significant performance gap under different reasoning settings. For instance, as shown in Figure 2, o3-mini-Low and -High show performance gap of 6.8 on the aggregated average. Similar traits can be observed among o4-mini, ClaudeSonnet-4, and o3, while Gemini-2.5-Pro shows the least performance gain, even with significantly"
        },
        {
            "title": "SCIREAS",
            "content": "-PRO Qwen Qwen-STEM Qwen-Math Qwen-BOTH Llama Llama-STEM Llama-Math Llama-BOTH Our Checkpoints SFT SFT SFT SFT SFT SFT 37.07 40.47 41.99 42.84 31.25 35.28 35.49 38.55 Concurrent Reasoning Post-training SFT SFT SFT&RL RL SYNTHETIC-1-SFT OpenR1 Llama-Nemotron General-Reasoner 37.64 43.08 43.53 34.99 13.97 16.11 18.17 21.11 11.67 14.29 16.98 16.51 19.44 26.43 23.75 13.73 Table 1: Performance of reasoning models trained from Qwen2.5-Instruct and Llama-3.1-Instruct on SYNTHETIC-1 and concurrent reasoning models."
        },
        {
            "title": "4.1 Controlled CoT SFT",
            "content": "To control for data composition and isolate the impact of reasoning and knowledge injection during post-training, we fine-tune Qwen2.5-7BInstruct (Yang et al., 2024) and Llama-3.1-8BInstruct (Grattafiori et al., 2024) on reasoning traces drawn from mathematics and STEM domains, as well as on their combination. This allows us to attribute behavior changes to the data mixture rather than confounding factors. For training, we leverage the SYNTHETIC1 (Mattern et al., 2025) dataset, an existing largescale dataset released by Prime Intellect,6 which consists of outputs of DeepSeek-R1-0120, including the reasoning traces, on diverse set of tasks. More specifically, we leverage the mathematics and STEM subsets from SYNTHETIC-1 (denoted as SYNTHETIC-1-Math/STEM, respectively). The former provides reasoning traces on abstract math reasoning questions, serving as source for long CoT adaptation without introducing in-domain knowledge. In contrast, the latter is sourced from StackExchange (Lambert et al., 2023), providing more in-domain data source for broader range of scientific subjects.7 The math subset contains around 462K instances, while the STEM subset contains around 512K instances. Details of the training and evaluation setup are in Appendix D. By training Qwen2.5-7B-Instruct on 6https://huggingface.co/PrimeIntellect 7Notably, is from SYNTHETIC-1-Math highlighting highcompetition-level math problems, quality abstract math reasoning filtered by verified answers. In contrast, StackExchange and SYNTHETIC-1-STEM provide more realistic problem-solving data from wider subjects, offering more coverage in science domains. sourced Figure 5: Model performance on SCIREAS and SCIREAS-PRO with varying reasoning capabilities. SCIREAS-PRO amplifies gaps between low-/nonreasoning and high-reasoning settings. more (>10) thinking budget. This observation motivates the construction of SCIREAS-PRO, leveraging the performance gap between low and high reasoning efforts as an effective proxy for identifying instances that demand complex reasoning rather than mere knowledge recall. For practitioners, task-specific evaluation is still recommended for the optimal balance between inference cost and performance. Amplified Performance Gap As shown in Figure 5, SCIREAS-PRO amplifies performance gaps between lowand high-reasoning settings, where the gap between GPT-5-High and GPT-5Low widens from 3.01 to 12.22, and the corresponding gap for Gemini-2.5-Pro-Preview widens from 0.35 to 2.30. Meanwhile, non-reasoning models, e.g., GPT-4.1, DeepSeek-V3, show more significant gaps on SCIREAS-PRO compared to concurrent reasoning models, i.e., o3 and DeepSeek-R1, respectively."
        },
        {
            "title": "4 Disentangling Knowledge and\nReasoning in Scientific Tasks",
            "content": "While SCIREAS and SCIREAS-PRO provide unified benchmarks to evaluate scientific reasoning capabilities, another fundamental question remains unanswered: how does CoT reasoning adaptation affect models ability to recall and utilize knowledge? To address this question, we first conduct series of controlled SFT experiments on high-quality reasoning traces with and without indomain scientific knowledge, and then we propose KRUX, novel investigative framework to study three key research questions regarding the role of knowledge in scientific reasoning using the finetuned checkpoints. 6 SYNTHETIC-1 (-Math, -STEM, and the combined subsets), we derived Qwen-Math, Qwen-STEM, and Qwen-BOTH along with their counterparts trained from Llama-3.1-8B-Instruct. In the following, we will refer to the base models as Qwen or Llama for brevity. Compared with concurrent work on long CoT post-training (Bercovich et al., 2025a; Face, 2025; Mattern et al., 2025; Ma et al., 2025b), our checkpoints deliver comparable performance under controlled settings  (Table 1)  , serving as reliable investigating checkpoints."
        },
        {
            "title": "4.2 Knowledge & Reasoning Utilization Exam",
            "content": "(KRUX) We introduce KRUX (Figure 1), novel investigative framework to study the role of knowledge and long CoT reasoning in scientific problem solving. To separate what model knows from how it reasons, we hold knowledge availability fixed by injecting compact, answer-agnostic knowledge ingredients (KIs) in-context. In the framework, we extract KIs from the reasoning traces of various models and provide these KIs in-context to LLMs when evaluating them. Consequently, gains over no-KI baseline indicate knowledge-retrieval bottleneck, while persistent errors point to reasoning limits. We first introduce our pipeline to extract KIs from reasoning traces (4.2.1), and then discuss how we analyze and apply extracted KIs to test knowledge recall (4.2.2, 4.2.4) and usage (4.2.3). For experiments, we prioritize challenging benchmarks (i.e., GPQA, MMLU-Pro*, and LabBench*), which have been widely used by previous work in the field on tasks that require scientific expertise."
        },
        {
            "title": "4.2.1 Knowledge Ingredient (KI) Extraction",
            "content": "First, to analyze the role of knowledge in models performance on scientific problem-solving, we aim to study setting in which the model is given the requisite knowledge in-context. Specifically, we take the reasoning traces from reasoning model as the knowledge source and use strong reasoningfocused LLM (e.g., DeepSeek-R1) to extract the essential atomic knowledge units that comprise it, which we refer to as knowledge ingredients (KIs) (Figure 1). We provide the extraction prompt and example KIs in Appendix E.1. We then augment the original question by prepending the extracted KIs in-context and ask the models to solve the same problem. We perform additional checks to ensure that KIs are relevant to the problem and do not leak any part of the final answer. In manual review, all extracted KIs met these criteria and were consistent with their source reasoning traces. To prevent the extractor from hallucinating or introducing extraneous facts (i.e., KIs unsupported by the source trace or unnecessary for solving the problem), we feed the generated KIs back to the source model and measure performance. If performance changes materially, this indicates potential leakage of steps or answers. Empirically, we observe no significant change (Table 2, Base vs. w/ Base KIs), suggesting the KIs are answer-agnostic and faithful to the trace. Further, although it is possible that the knowledge pieces may be irrelevant to the solution, as shown in recent studies of CoT faithfulness (Turpin et al., 2023; Wang et al., 2024c,a), recent high-performing models like DeepSeek-R1 have demonstrated strong reasoning adherence on benchmark tasks (DeepSeek-AI et al., 2025). Our experiments show that the knowledge pieces help models on reasoning tasks. See Figures 12-14 in Appendix E.1 for KI examples generated by different models for the same question. Centered on our primary research objective on the roles of knowledge recall and utilization in reasoning models, we examine the following key research questions: RQ1: To what extent can base models benefit from high-quality external knowledge? RQ2: Do reasoning-enhanced models benefit from external knowledge? RQ3: Does reasoning fine-tuning improve models ability to surface helpful knowledge?"
        },
        {
            "title": "4.2.2 RQ1: To what extent can base models",
            "content": "benefit from high-quality external knowledge? Problem Statement. We investigate the potential improvement from external knowledge by providing KIs to the base models in the prompt when performing scientific reasoning (Figure 1). Here, we focus on two sources for the KIs, which are extracted from their own CoT traces (w/ Base KIs) or from DeepSeek-R1s CoT traces (w/ R1 KIs). To overcome context sensitivity, we report averages and standard deviations across 5 runs with corresponding KIs permuted randomly. We then investigate whether there are significant gaps between base models augmented with additional KIs in the context, and their corresponding reasoningfine-tuned models. To this end, comparisons are"
        },
        {
            "title": "GPQA",
            "content": "LabBench* Qwen w/ Qwen KIs w/ R1 KIs Qwen-STEM Qwen-Math Qwen-BOTH General-Reasoner Llama w/ Llama KIs w/ R1 KIs Llama-STEM Llama-Math Llama-BOTH Llama-Nemotron 35.27 34.24 0.93 47.19 1.53 41.63 39.47 40.81 35.94 28.13 29.06 1.44 43.57 0.88 38.95 36.16 39.43 37.95 32.38 30.93 1.43 41.40 2.46 31.75 30.18 33.83 35. 33.55 34.40 2.58 42.27 1.60 36.04 34.78 36.61 27.78 Table 2: Performance on GPQA and LabBench* with base models alone, base models with knowledge extracted from DeepSeek-R1 or itself (w/ {R1, Base} KIs), and base models with reasoning-fine-tuning. Best and second best average scores are labeled in bold and underlined, respectively. Reasoning models fall behind base models augmented with in-context knowledge. made with reasoning-fine-tuned models trained on our controlled data mixtures and the ones from concurrent work (i.e., General-Reasoner-7B (Liu et al., 2025) and Llama-Nemotron-Nano-8B (Bercovich et al., 2025b)) that involve SFT and reinforcement learning based on the same base models (i.e., Qwen2.5-7B-Instruct and Llama-3.1-8B-Instruct, respectively). Answer to RQ1: As an upper bound, base model with high-quality in-context knowledge can substantially outperform its reasoningenhanced counterpart. As shown in Table 2, base models provided with KIs from DeepSeekR1 are able to outperform base models alone or Base w/ Base KIs setup by 20%, and outperform reasoning variants without KIs by 10% across different benchmarks and model families, showing the external knowledge provides greater gain than reasoning fine-tuning. The fact that base model without strong reasoning capabilities can outperform reasoning models in this setting indicates potential deficiency of the models in knowledge recall that hinders their performance in scientific reasoning."
        },
        {
            "title": "4.2.3 RQ2: Do reasoning-enhanced models",
            "content": "benefit from external knowledge? Problem Statement. Observing considerable improvements from adding external knowledge ingredients from DeepSeek-R1 to base models in RQ1, we hypothesize similar improvements would scale on reasoning-enhanced models, offering additional gains on top of enhanced reasoning capabilities. To this end, we evaluate base and CoT SFTed variants on KIs extracted from DeepSeek-R1, providing the same necessary knowledge extracted from DeepSeek-R1s reasoning traces (w/ R1 KIs). As the baseline for comparison that is not provided with added knowledge, we instead offer the tested models with KIs extracted from their own CoT traces (w/ self KIs). Answer to RQ2: Yes. the reasoning models also substantially benefit from addition of contextual knowledge. As shown in Table 3, within both Qwen and Llama groups, reasoning-enhanced models w/ R1 KIs in the context show significant improvements over the base setting without the KIs, while preserving the gap compared with the base model w/ R1 KIs. Confirming the effectiveness of providing external knowledge as an in-context prompt, this result sheds light on potential future improvement by applying high-quality external memory modules as an external knowledge source for better problem-solving capabilities, echoing the finding in COMPACTDB (Lyu et al., 2025), concurrent effort constructing high-quality datastore for reasoning-intensive tasks. We note, however, that in these experiments, we do not distinguish between two possible nonexclusive explanations for the improvement from adding R1 KIs. (a) It may be that the R1 KIs provide new key knowledge absent from the models parameters, or (b) the model may already possess these facts but struggle to retrieve them (put another way, once strong reasoning model supplies the key facts, the reasoning search space might narrow and the problem becomes easier, whether or not the model originally knew the augmented facts). We further analyze this confounder in RQ3."
        },
        {
            "title": "4.2.4 RQ3: Does reasoning fine-tuning",
            "content": "improve models ability to surface helpful knowledge? Problem Statement. While we observe that external knowledge benefits reasoning models, in this RQ, we ask how reasoning-fine-tuning affects knowledge recall. To this end, we focus on evaluating the KIs from -Math models to determine whether they offer more more improvement than those of base models, since -Math models are finetuned on math-only data without additional scientific knowledge. Notably, in Table 2, while -STEM and -BOTH"
        },
        {
            "title": "Models",
            "content": "w/ self KIs w/ R1 KIs w/ self KIs w/ R1 KIs w/ self KIs w/ R1 KIs"
        },
        {
            "title": "GPQA",
            "content": "MMLU-Pro* LabBench* Qwen 34.24 0.93 Qwen-STEM 41.63 2.10 39.47 1.66 Qwen-Math 40.81 2.04 Qwen-BOTH Llama 29.06 1.44 Llama-STEM 38.95 1.31 Llama-Math 36.16 2.33 Llama-BOTH 39.43 2.00 47.19 1.53 52.50 2.14 53.53 1.24 54.46 1.27 43.57 0.88 53.17 1.15 53.75 1.15 54.73 1. 59.03 0.34 64.71 1.05 66.93 0.72 65.71 0.74 47.73 0.89 59.14 0.85 59.65 0.98 63.81 0.90 68.86 0.56 69.69 0.73 74.00 0.59 71.64 1.16 60.53 1.67 68.19 1.15 69.01 0.55 72.74 0.26 30.93 1.43 31.75 2.81 30.18 1.65 33.83 2.59 34.40 2.58 36.04 3.98 34.78 4.26 36.61 2. 41.40 2.46 43.79 1.71 41.17 2.32 43.90 2.71 42.27 1.60 46.87 1.49 45.55 0.68 48.65 0.49 Table 3: Accuracy of Qwen and Llama variants on benchmarks with external knowledge ingredients (KIs). We report averages and standard deviations over 5 random permutations of the KIs. Reasoning variants w/ R1 KIs outperform base model w/ R1 KIs across different benchmarks and models."
        },
        {
            "title": "Qwen",
            "content": "-Math Llama -Math Qwen-Math Llama-Math KI-GPQA KI-MMLU-Pro* 72.30 82. 73.02 81.50 70.94 74.46 68.94 74.12 Table 4: Accuracy (%) of synthetic knowledge recall on KIs generated from Qwen/Llama-Math on GPQA and MMLU-Pro*. Base models and math reasoning-finetuned models show similar performance on knowledge recall questions, demonstrating no explicit in-domain knowledge injection. variants, trained with SYNTHETIC-1-STEM, outperform -Math variants due to science in-domain training data, -Math variants also largely outperform the base model even without being trained on science data. Recalling our discussion in RQ2 (4.2.3), the -Math models gains have the same two non-exclusive explanations, (a) some science questions require mathematical knowledge, and the -Math model performs better on these because math knowledge was loaded into the model through the math-specific fine-tuning, and/or (b) the -Math model is better at surfacing its relevant parametric knowledge via CoT expression. To disentangle these two factors, we extract KIs from the CoTs of the -Math models and examine whether these KIs represent new knowledge added by fine-tuning, or whether they are also facts known to the base model. We probe this by querying the model with synthetic questions that test knowledge of each KI (see Appendix E.2 for prompts and examples). Then, to verify explanation (b), we provide the KIs in-context from either the -Math or base model, to the corresponding base model; i.e., holding mathematical reasoning capacity constant while varying only the external knowledge."
        },
        {
            "title": "Base Setup",
            "content": "GPQA MMLU-Pro* w/ Qwen KIs 34.24 0.93 59.03 0.34 w/ Qwen-Math KIs 36.93 1.75 63.66 0.45 w/ Llama KIs L 29.06 1.44 47.73 0.89 w/ Llama-Math KIs 29.69 1.72 53.91 0.94 Table 5: Performance on GPQA and MMLU-Pro* with KIs extracted from base and -Math reasoning models. KIs extracted from -Math models enable more improvement over those from base models. Answer to RQ3: Yes. In response to explanation (a), we find that on average, the base models and their corresponding -Math variants have similar recall of the KIs  (Table 4)  , meaning that explanation (a) is unlikely to be the major contributor for the improvements. To verify explanation (b), Table 5 shows that KIs from -Math deliver significant boosts over those from the base models across different benchmarks and model families. This result suggests that CoT verbalization improves the models ability to identify and surface the most relevant latent knowledge for the given reasoning problems. Notably, the KIs are unlikely to have been newly acquired during fine-tuning  (Table 4)  ; instead, the findings indicate that reasoning-fine-tuned models exhibit improved recall of knowledge already parameterized in the base model."
        },
        {
            "title": "5 Training Knowledge Enhanced\nScientific Reasoning Models",
            "content": "Our analyses conducted so far are based on models fine-tuned on either SYNTHETIC-1-Math, SYNTHETIC-1-STEM, or both, while combining the two, which cover both STEM and mathematical reasoning, achieves the strongest performance. To further assess the effectiveness of this Math+STEM 9 data mixture following 4.1, we compare it directly against concurrently released long-CoT SFT datasets on the same base model. We then apply the same mixture to Qwen3-8B-Base to obtain SCILIT01 to provide stronger baseline. Specifically, we compare Qwen-BOTH, which is fine-tuned using our training recipe, with SYNTHETIC-1-SFT (Mattern et al., 2025), model fine-tuned on SYNTHETIC-1 with additional coding and preference alignment data, and QwenNemotron, model we trained with the same settings and same amount of data (4.1) sampled from science and math domains of LlamaNemotron (Bercovich et al., 2025b), training data mixture for reasoning fine-tuning, all post-trained on Qwen2.5-7B-Instruct. The results in Table 10 show that our data composition yields stronger baseline for scientific reasoning than concurrent data recipes on Qwen2.5-7B-Instruct (Table 10 center block), and Qwen-BOTH reaches comparable performance to models from concurrent efforts focusing on reasoning enhancement post-training recipes (Table 10 left-hand block, i.e., OpenR1 (Face, 2025), Llama-Nemotron (Bercovich et al., 2025b), and General-Reasoner (Ma et al., 2025b)). Furthermore, using our recipe, we fine-tune the recently released Qwen3-8B-Base to deliver stronger model, SCILIT01. While its performance falls behind Qwen3-8B with the thinking mode, which has undergone more sophisticated post-training, it outperforms Qwen3-8B with nonthinking mode (Table 10 right-hand block). This indicates that SCILIT01 partially unleashes the reasoning capabilities from the base model, offering strong baseline for future study on post-training recipe for scientific reasoning."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we studied how reasoning and domain knowledge each contribute to scientific reasoning in large language models. To this end, we introduced SCIREAS, unified, reproducible suite for evaluating scientific reasoning across its domains and formats, and SCIREAS-PRO, reasoning-intensive subset. With our evaluation suite, we show that despite the universal applicability of modern LLMs, different LLMs can have distinct strengths, and differences in inference budget could lead to significant performance gap on the same model. Therefore, we recommend that practitioners conduct task-specific evaluations to achieve an optimal balance between cost and performance in real-life use cases. We also introduced KRUX, knowledgecontrolled evaluation framework that assesses LLMs with provided knowledge ingredients (KIs), revealing important insights regarding the enhanced knowledge utilization and recall enabled by reasoning-fine-tuning. We showed: (i) retrieving task-relevant knowledge from parameters is key bottleneck base instruct models can surpass reasoning-tuned models once supplied with high-quality KIs; (ii) reasoning-fine-tuned models still benefit from the same external KIs, suggesting complementary gains from explicit knowledge access; and (iii) verbalized CoT improves knowledge surfacing KIs extracted from math-only reasoning models help the corresponding base models more than base-derived KIs, even when no new domain knowledge is introduced. Our results show that reasoning-focused fine-tuning improves both reasoning and knowledge use, suggesting promising future directions in better understanding and enhancing these interconnected components."
        },
        {
            "title": "Limitations",
            "content": "Our KRUX framework and KI extraction methods depend on strong models like DeepSeekR1 for generating reasoning traces. While we used an open-weight model, which provides more transparency and interpretability, the KI extraction pipeline may introduce unobservable biases (though risk is minimal due to our focus on scientific domains), unwanted leakage of information about the answer, or inconsistencies in the faithfulness of the KIs to the task. To mitigate this, we conducted manual analysis of the KIs, confirming their relevance and no direct answer leakage, but extracted KIs could occasionally be irrelevant or incomplete, especially if deployed at scale. Furthermore, some of our analyses are confounded by factors such as context sensitivity (addressed via permutations) and the impact of constraining the search space when providing KIs, which we interpret as an upper bound but may overestimate pure recall benefits. We have taken measures to mitigate these and discussed the caveats in our discussion of results with more details. Our experiments focus on moderate-sized LLMs with <10B parameters, specifically open-weight models (Qwen2.5, Llama3.1). While we deliberately selected two model families and models 10 large enough to exhibit non-trivial reasoning performance, this limits the generalizability of our findings to larger models. Experimenting with larger models represents straightforward extension but requires significantly greater computational resources, beyond the scope of our current study and our available compute resources. The benchmarks we examine emphasize STEM fields, which may underrepresent interdisciplinary or emerging scientific research areas. We acknowledge potential data contamination issues that may impact our analysis; however, the nature of our study is analytical, and we perform controlled experiments. In our benchmarks, we also mitigate these concerns by focusing on recent 20242025 datasets. Despite these constraints, our methodology provides systematic framework for evaluating domain-specific reasoning that can be extended to address these limitations in future work."
        },
        {
            "title": "Acknowledgments",
            "content": "This project was supported in part by Googles Research Scholar Program and compute credits from Nvidia through Nvidias academic grants program. We thank Luca Soldaini and Dirk Groeneveld for helpful discussions in the early stages of the project."
        },
        {
            "title": "References",
            "content": "IvÃ¡n Arcuschin, Jett Janiak, Robert Krzyzanowski, Senthooran Rajamanoharan, Neel Nanda, and Arthur Conmy. 2025. Chain-of-thought reasoning in the wild is not always faithful. Preprint, arXiv:2503.08679. Andrew Michael Bean, Simeon Hellsten, Harry Mayne, Jabez Magomere, Ethan Chi, Ryan Andrew Chi, Scott A. Hale, and Hannah Rose Kirk. 2024. LINGOLY: benchmark of olympiad-level linguistic reasoning puzzles in low resource and extinct languages. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert: pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, pages 36153620. Association for Computational Linguistics. Zhang, Tugrul Konuk, and 115 others. 2025a. Llamanemotron: Efficient reasoning models. Preprint, arXiv:2505.00949. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, and 1 others. 2025b. Llama-nemotron: Efficient reasoning models. arXiv preprint arXiv:2505.00949. Roi Cohen, Mor Geva, Amir Globerson. 2023. knowledge-base of language models. arXiv:2301.12810. and Crawling the internal Preprint, Jonathan Berant, DeepSeek-AI. 2025. Deepseek-r1: Usage recommendations. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 181 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Hugging Face. 2025. Open r1: fully open reproduction of deepseek-r1. Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, and Huajun Chen. 2024. Sciknoweval: Evaluating multi-level scientific knowledge of large language models. Preprint, arXiv:2406.09098. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2024. The language model evaluation harness. Daniela Gottesman and Mor Geva. 2024. Estimating knowledge in large language models without generating single token. Preprint, arXiv:2406.12673. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, Ryutaro Tanno, Khaled Saab, Dan Popovici, Jacob Blum, Fan Zhang, Katherine Chou, Avinatan Hassidim, Burak Gokturk, Amin Vahdat, Pushmeet Kohli, and 15 others. 2025. Towards an ai co-scientist. Preprint, arXiv:2502.18864. Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, Ido Shahaf, Oren Tropp, Ehud Karpas, Ran Zilberstein, Jiaqi Zeng, Soumye Singhal, Alexander Bukharin, Yian Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. 11 Yuling Gu, Oyvind Tafjord, Bailey Kuehl, Dany Haddad, Jesse Dodge, and Hannaneh Hajishirzi. 2025. Olmes: standard for language model evaluations. Preprint, arXiv:2406.08446. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. 2024. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, Bangkok, Thailand. Association for Computational Linguistics. Mingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang, Wenyue Hua, Ruixiang Tang, William Yang Wang, and Yongfeng Zhang. 2025. Disentangling memory and reasoning ability in large language models. Preprint, arXiv:2411.13504. Jude Khouja, Karolina Korgul, Simi Hellsten, Lingyi Yang, Vlad Neacsu, Harry Mayne, Ryan Kearns, Andrew Bean, and Adam Mahdi. 2025. Lingolytoo: Disentangling reasoning from knowledge with Preprint, templatised orthographic obfuscation. arXiv:2503.02972. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large language models are zero-shot reasoners. Preprint, arXiv:2205.11916. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Nathan Lambert, Lewis Tunstall, Nazneen Rajani, and Tristan Thrush. 2023. Huggingface h4 stack exchange preference dataset. Jon Laurent, Joseph Janizek, Michael Ruzo, Michaela Hinks, Michael Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew White, and Samuel Rodriques. 2024. Lab-bench: Measuring capabilities of language models for biology research. arXiv preprint arXiv:2407.10362. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):12341240. Aochong Oliver Li and Tanya Goyal. 2025. Memorization vs. reasoning: Updating llms with new knowledge. Preprint, arXiv:2504.12523. Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, and Hengxing Cai. 2025. Scilitllm: How to adapt llms for scientific literature understanding. Preprint, arXiv:2408.15545. Qianchu Liu, Sheng Zhang, Guanghui Qin, Timothy Ossowski, Yu Gu, Ying Jin, Sid Kiblawi, Sam Preston, Mu Wei, Paul Vozila, Tristan Naumann, and Hoifung Poon. 2025. X-reasoner: Towards generalizable reasoning across modalities and domains. Preprint, arXiv:2505.03981. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The ai scientist: Towards fully automated open-ended scientific discovery. Preprint, arXiv:2408.06292. Xinxi Lyu, Michael Duan, Rulin Shao, Pang Wei Koh, and Sewon Min. 2025. Frustratingly simple retrieval improves challenging, reasoning-intensive benchmarks. Preprint, arXiv:2507.01297. Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, and Jia Li. 2025a. S2r: Teaching llms to self-verify and self-correct via reinforcement learning. Preprint, arXiv:2502.12853. Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. 2025b. General-reasoner: Advancing llm reasoning across all domains. Justus Mattern, Felix Gabriel, and Johannes Hagemann. 2025. Synthetic-1 release: Two million collaboratively generated reasoning traces from deepseek-r1. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel CandÃ¨s, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Preprint, arXiv:2501.19393. OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, and 244 others. 2024. Openai o1 system card. Preprint, arXiv:2412.16720. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. Preprint, arXiv:2309.00071. Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim RocktÃ¤schel, and Sebastian Riedel. 2021. Kilt: benchmark for knowledge intensive language tasks. Preprint, arXiv:2009.02252. Arvind Prabhakar and 1 others. 2025. Omniscience: domain-specialized llm for scientific reasoning. arXiv preprint arXiv:2503.17604. 12 David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling (COLM). Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, and Emad Barsoum. 2025. Agent laboratory: Using llm agents as research assistants. Preprint, arXiv:2501.04227. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2024. Scieval: multi-level large language model evaluation benchmark for scientific research. Preprint, arXiv:2308.13149. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, and 1 others. 2025a. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixin Deng, Shawn Gavin, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, and 78 others. 2025b. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. Preprint, arXiv:2502.14739. Qwen Team. 2024. Qwen2.5: party of foundation models. Qwen Team. 2025. Qwq-32b: Embracing the power of reinforcement learning. Rahul Thapa, Qingyang Wu, Kevin Wu, Harrison Zhang, Angela Zhang, Eric Wu, Haotian Ye, Suhana Bedi, Nevin Aresh, Joseph Boen, Shriya Reddy, Ben Athiwaratkun, Shuaiwen Leon Song, and James Zou. 2025. Disentangling reasoning and knowledge in medical large language models. ArXiv, abs/2505.11462. Andrew Turpin, Jason Wei, Denny Zhou, Quoc Le, and Ed Chi. 2023. Faithful chain-of-thought reasoning. arXiv preprint arXiv:2305.15020. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, Doug Downey, Hannaneh Hajishirzi, and Arman Cohan. 2024a. Sciriff: resource to enhance language model instruction-following over scientific literature. Preprint, arXiv:2406.07835. David Wadden, Kejian Shi, Jacob Morrison, Aakanksha Naik, Shruti Singh, Nitzan Barzilay, Kyle Lo, Tom Hope, Luca Soldaini, Shannon Zejiang Shen, and 1 others. 2024b. Sciriff: resource to enhance language model instruction-following over scientific literature. arXiv preprint arXiv:2406.07835. Changyue Wang, Weihang Su, Qingyao Ai, Yujia Zhou, and Yiqun Liu. 2025. Decoupling reasoning and knowledge injection for in-context knowledge editing. Preprint, arXiv:2506.00536. Pengfei Wang and 1 others. 2023a. Scienceqa: largescale open dataset for question answering in science education. arXiv preprint arXiv:2210.08127. Weijie Wang, Xiang Chen, and 1 others. 2024a. Evaluating the faithfulness of chain-of-thought reasonarXiv preprint ing in large language models. arXiv:2401.02392. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2023b. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024b. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574. Yunfan Wang, Dian Yu, Qian Zhou, and 1 others. 2024c. Can large language models follow chain-of-thought prompts faithfully? In International Conference on Learning Representations (ICLR). Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Juncheng Wu, Sheng Liu, Haoqin Tu, Hang Yu, Xiaoke Huang, James Zou, Cihang Xie, and Yuyin Zhou. 2025. Knowledge or reasoning? close look at how llms think across domains. Preprint, arXiv:2506.02126. Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, and Yang Wang. 2025. Ugphysics: comprehensive benchmark for undergraduate physics reasoning with large language models. Preprint, arXiv:2502.00334. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, and 1 others. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. 13 Xiao-Wen Yang, Xuan-Yi Zhu, Wen-Da Wei, DingChu Zhang, Jie-Jing Shao, Zhi Zhou, Lan-Zhe Guo, and Yu-Feng Li. 2025b. Step back to leap forward: Self-backtracking for boosting reasoning of language models. Preprint, arXiv:2502.04404. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate problem solving with large language models. Preprint, arXiv:2305.10601. Ge Zhang and 1 others. 2024. Sciglm: Pre-training generalist language models for science with scientific papers. arXiv preprint arXiv:2402.00730. Wayne Xin Zhao and 1 others. 2023. survey arXiv preprint of llms for scientific research. arXiv:2307.07927. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685."
        },
        {
            "title": "A Extended Related Work",
            "content": "Evaluating Knowledge of LLMs Early efforts tended to evaluate the LM knowledge frontier with static unified benchmark (Petroni et al., 2021). However, given the growing training corpus for pushing LLM performance, quantifying the knowledge frontier of LLMs becomes increasingly challenging, making it difficult to design unified benchmark. Instead of general knowledge evaluation, recent work approaches the knowledge frontier of LLMs by anchoring on specific entities, proposing methods to quantify knowledge and factuality around given entities (Gottesman and Geva, 2024; Cohen et al., 2023). With recent development of reasoning LLMs, more work exploits long CoT traces as evidence of explicit knowledge utilization, verifying knowledge recall in CoT traces for factuality (Wu et al., 2025). Nevertheless, directly evaluating CoT traces can result in false positive signals on the knowledge boundary, given that the knowledge involved could be factual but not helpful for problem solving (Arcuschin et al., 2025). In our framework, we construct controlled settings and protocols to evaluate whether the knowledge is genuinely helpful for problem-solving, implicitly guaranteeing the factuality and relevance. Reasoning LLMs Recent work has shown that LLMs can be trained to utilize intermediate tokens for reasoning, achieving better performance on reasoning tasks as the decoding budget increases. OpenAIs o-series (OpenAI et al., 2024) represents the landmark of this paradigm among commercial frontier models, followed by DeepSeek-R1 (DeepSeekAI et al., 2025) and several recent efforts to reproduce this success without releasing the training data, such as QwQ (Team, 2025) and Kimi (Team et al., 2025a). Some recent initiatives aim to achieve the same goal using fully open data sources, led by Llama-Nemotron from NVIDIA (Bercovich et al., 2025b) and SYNTHETIC-1 from Prime Intellect (Mattern et al., 2025), releasing post-training data to foster development within the community. Our work builds on these commitments, sharing the vision of improving model reasoning by leveraging intermediate tokens, while emphasizing our focus on scientific domains rather than on mathematics or general logical reasoning. LLMs for Science Recent advancements in scientific LLMs have transitioned from early domainspecific pretraining (e.g., Beltagy et al. 2019; Lee 14 et al. 2020), to more comprehensive models with multiple stages of training, e.g., SciGLM (Zhang et al., 2024), SciLitLLM (Li et al., 2025), and OmniScience (Prabhakar et al., 2025). On the other hand, reasoning models have shown strong performance on scientific tasks such as GPQA and MMLU-Pro (DeepSeek-AI et al., 2025; OpenAI et al., 2024), and some recent efforts instrument LLMs to separate recall from deduction during inference (Wang et al., 2025; Jin et al., 2025). However, we still lack clear understanding of the factors underlying performance on scientific tasks, such as knowledge acquisition or improved reasoning capabilities. We aim to address this gap by studying these factors and then providing recipe for training more capable models in science."
        },
        {
            "title": "B SCIREAS Details",
            "content": "B.1 Evaluation Suite Curation See Table 11-12 for domain distribution. We list the selection of each benchmark as follows. GPQA (Rein et al., 2024): No change. Report in micro average. License: CC-BY-4.0. MMLU-Pro (Wang et al., 2024b): MMLU-Pro features subjects beyond STEM and scientific subjects. We first filter by subjects, retaining instances from physics, chemistry, computer science, math, biology, and health, and then randomly sample each task to 200 instances max. Report in macro average across 7 subjects. License: MIT. require visual LabBench (Laurent et al., 2024): We drop tasks that inputs or external table/paper extraction, therefore dropping DbQA, FigQA, LitQA2, SuppQA, and TableQA, retaining CloningScenarios, PropotolQA, and SeqQA. Report in macro average across 3 tasks. License: CC-BY-SA-4.0. SciBench (Wang et al., 2023b): No change. Report in micro average. License: MIT. OlympiadBench (He et al., 2024): Dropping tasks that require visual inputs or not in English. Report the macro average across math and physics. License: apache-2.0. SciRIFF (Wadden et al., 2024b): We drop tasks that primarily focus on information/relation/table extraction and retain EvidenceInference, Qasper, and SciFact. Report in macro average of 5 metrics (detailed in Table 11-12) across 3 tasks. License: ODC-BY. SciKnowEval (Feng et al., 2024): The authors introduce scientific tasks in 5 progressive levels from knowledge memorization to application. After manual inspection, we only preserve tasks from the highest level of knowledge application (L5), and cap instances from each task to be 200. Report the macro average across 8 tasks. License: MIT. SciEval (Sun et al., 2024): Similar to SciKnowEval, the authors introduce 4 progressive levels of static tasks, including basic knowledge, knowledge application, scientific calculation, and research creativity. After inspection, we retain knowledge application and scientific calculation subsets, capping each task to maximum of 200. Report the macro average across 6 tasks. License: N/A. UGPhysics (Xu et al., 2025): Cap each subject to be 200 max. Report the macro average across 13 subjects. License: CC-BY-NC-SA-4.0. SuperGPQA (Team et al., 2025b): We curate questions from two broad domains science and engineering while omitting niche areas that lie outside mainstream STEM (e.g., weapon science, textile engineering). The science portion spans mathematics, biology, physics, systems science, and chemistry. The engineering portion covers comprehensive set of disciplines: electronic science and technology; nuclear science and technology; mechanical engineering; information and communication engineering; civil engineering; instrument science and technology; computer science and technology; control science and engineering; chemical engineering and technology; mechanics; electrical engineering; materials science and engineering; hydraulic engineering; power engineering and engineering thermophysics; and optical engineering. Report in macro average across the domain of science and engineering. License: ODCBY. B.2 Uniform Sampling Validation: MMLU-Pro Case Study Evaluating state-of-the-art frontier models could be expensive. To mitigate evaluation cost, we evaluate frontier models on MMLU-Pro* before and after uniform sampling. By sample size correlation in Figure 6 and 95% confidence intervals for sampled subset in Figure 7, we show that the sampling is 15 Benchmark o3 o3-mini o4-mini Gemini-2.5-Pro Claude-Sonnet-4 GPT-5 Low High Low High Low High Low High Low High Low High GPQA SuperGPQA* MMLU-Pro* LabBench* OlympBench SciBench SciEval* SciKnowEval* SciRIFF* UGPhysics* 75.4 79.9 +4.5 63.4 73.9 +10.5 69.4 74.6 +5.2 80.1 79.5 -0.6 63.8 69.0 +5.2 79.2 82.4 +3.1 54.9 59.5 +4.6 40.5 54.0 +13.5 48.6 57.1 +8.5 60.1 60.4 +0.3 45.2 49.8 +4.6 58.6 62.4 +3.8 85.7 86.6 +0.9 82.1 85.0 +2.9 84.1 86.0 +1.9 85.0 86.2 +1.2 84.1 85.3 +1.2 86.5 88.6 +2.1 70.5 74.2 +3.7 56.9 59.2 +2.3 59.7 63.7 +4.0 61.9 64.4 +2.5 53.4 57.2 +3.8 66.6 74.4 +7.8 53.5 58.0 +4.5 39.5 51.1 +11.6 40.4 49.6 +9.2 67.5 69.6 +2.1 55.4 59.8 +4.4 60.0 64.9 +4.8 69.7 72.1 +2.4 46.0 66.3 +20.3 65.5 69.7 +4.2 71.0 70.2 -0.8 65.5 67.1 +1.6 70.4 72.0 +1.6 84.8 82.7 -2.1 83.8 83.4 -0.4 87.1 87.5 +0.4 86.4 85.1 -1.3 85.8 85.8 87.4 86.1 -1.3 52.1 51.9 -0.2 49.0 51.9 +2.9 49.9 51.1 +1.2 46.8 47.6 +0.8 43.6 43.3 -0.3 45.5 46.7 +1.2 51.8 53.6 +1.8 51.3 51.8 +0.5 50.6 52.2 +1.6 51.6 51.4 -0.2 53.5 50.9 -2.6 46.9 50.1 +3.3 63.1 65.2 +2.1 56.7 60.7 +4.0 57.7 62.2 +4.5 56.0 55.4 -0.6 52.4 53.2 +0.8 63.6 67.6 +4.0 0.0 Average 66.2 68.4 +2.2 56.9 63.7 +6.8 61.3 65.4 +4.1 66.6 67.0 +0.4 60.3 62.1 +1.8 66.5 69.5 +3.1 0.01$ / Instance 0.68 2.25 3.3 0.41 3.24 7.9 0.41 2.38 5.8 1.07 12.51 11.7 1.83 7.50 4.1 0.72 3.10 4.3 Table 6: Performance (%) across SCIREAS grouped by models at low and high reasoning efforts. The same model with different reasoning effort can have distinctive performance with clear margin. cost-efficient and statistically effective while reducing evaluating instances from 6,696 to 1,400. For costly frontier reasoning models such as Gemini-2.5-Pro-Preview, at rates in time of writing, the sampling reduces SCIREAS evaluation costs from $3,600 to $1,500 and can be further decreased to $730 by using batch job inference. Figure 7: 95% confidence intervals for performance estimates using 200-instance sampling across nine stateof-the-art frontier model setups. The narrow confidence intervals (mean width: 0.0015) demonstrate high precision and reliability of the sampling approach. Values above bars show mean performance, while values below bars indicate the precision (half-width of confidence intervals). intensiveness. B.3.1 Cross-Model Agreement on Reasoning"
        },
        {
            "title": "Intensity",
            "content": "To validate our hypothesis that performance gaps between different reasoning effort settings indicate reasoning intensity, we first examine whether different models agree on which instances are reasoning-intensive. As shown in Figure 1, for each reasoning model, we categorize each test question from SCIREAS by their correctness under low/high reasoning efforts into four categories, (high_c, low_c), (high_c, low_i), (high_i, low_c), and (high_i, low_i), where high/low stands for high/low reasoning effort setting and *_c/*_i stands for the problem instance has been Figure 6: Correlation between sampled and full dataset performance as function of sample size. The analysis demonstrates that 200 instances per subject (highlighted in purple) achieves strong correlation (r = 0.919 0.043) with full dataset results. Error bars represent standard deviation across 30 independent samples. B.3 SCIREAS-PRO Reasoning Intensiveness"
        },
        {
            "title": "Validation",
            "content": "To test this hypothesis, we pursue two complementary checks: (1) different reasoning models should have high agreement identifying reasoning intensive instances, and (2) filtered instances should agree with human judgment in terms of reasoning 16 answered correctly/incorrectly by the model. Treating (high_c, low_i) as targeting instances that require high reasoning effort, we measure how (high_c, low_i) sets derived from different reasoning models agree with others. As shown in Table 7, treating (high_c, low_i) from o3-mini as ground truth, the same set derived from o4-mini, o3, and claude-sonnet-4 largely coincide with o3-mini across different benchmarks from SCIREAS (all above 70%), showing high agreement on instances that require high reasoning efforts across models from different model families. Ground Truth o3-mini o3-mini"
        },
        {
            "title": "Target",
            "content": "SuperGPQA* GPQA MMLU-Pro* LabBench* SciBench OlympiadBench SciEval* UGPhysics* vs. o4-mini 78.0 80.4 92.2 71.9 75.9 81.1 94.3 83.2 vs. o3 77.8 81.0 91.6 74.6 74.1 81.5 93.1 82.9 o3-mini vs. claude-sonnet76.1 79.0 92.0 75.8 75.4 81.2 93.5 83.8 Table 7: Accuracy of overlapping instances on (high_c, low_i) from o3-mini vs. other models, treating o3-mini as ground true label. Different reasoning models agree on high reasoning instances. B.3.2 Human and LLM-as-Judge Assessment The overlap of instances that require high reasoning effort shows reasoning models tend to agree on problem difficulty, but to verify the reliability of reasoning effort as surrogate, the filter should also align with human judgment. for To this end, we collect the union of (high_c, low_i) from o3-mini and o4-mini the case study and apply an LLM-as-judge assessment (Zheng et al., 2023) to expedite the process while manually annotating subset for reliability test. The LLM judge is based on GPT-4.1 for balanced tradeoff between assessment reliability and cost. Notably, naively prompting the LLM judge to determine the reasoning difficulty could be suboptimal due to lack of reference. Therefore, we designed two reference-based evaluation protocols: (a) pair-wise comparison on reasoning difficulty between instance questions sampled from filtered subset and original SCIREAS, and (b) identifying failing reason for filtered instances given low and high reasoning outputs (i.e., whether the model fails in low reasoning setting due to lack of reasoning effort). (a) Pairwise Comparison For each instance in SCIREAS-PRO, the judge is also presented with an instance drawn from the set of other, nonoverlapping instances from SCIREAS. The judge is not given any information as to which instance is drawn from which source and is tasked to identify which instance is more reasoning-intensive."
        },
        {
            "title": "SYSTEM MESSAGE",
            "content": "You are an expert judge comparing reasoning intensity between two questions. Analyze both questions thoroughly and determine which one demands more complex reasoning. Reply in this exact format: ###EXPLANATION: <detailed analysis of both questions and the comparison> ###RESULTS: / / UNCLEAR"
        },
        {
            "title": "USER MESSAGE",
            "content": "You will be shown two questions (A and B) from the same academic domain. question is *reasoning intensive* if it requires: Complex multi-step logical reasoning Advanced mathematical computation or derivation Integration of multiple concepts or principles Abstract thinking or sophisticated problem-solving strategies Deep domain knowledge application *QUESTION A* Context: {{context_a}} Question: {{question_a}} *QUESTION B* Context: {{context_b}} Question: {{question_b}} Analyze both questions carefully and explain your reasoning. Then reply using the exact format specified above. Figure 8: Full reasoning intensiveness pairwise comparison prompt template used in our experiments. (b) Failure Analysis For each instance in SCIREAS-PRO, the judge is presented with both the correct high reasoning output (if both o3-mini-high and o4-mini-high are correct, o4-mini-high will be selected) as well as the incorrect low reasoning output from the corresponding model (e.g. correct: o3-mini-high; incorrect: o3-mini-low). The judge is tasked with determining whether the failure of the low reasoning effort model can be attributed primarily due to insufficient reasoning ability or lack of domain knowledge."
        },
        {
            "title": "SYSTEM MESSAGE",
            "content": "You are an expert judge analyzing why AI models fail on reasoning-intensive questions. Compare the correct and incorrect answers to determine if the failure was primarily due to insufficient reasoning ability or lack of domain knowledge. Reply in this exact format: ###EXPLANATION: <detailed analysis of why the low-reasoning model failed> ###RESULTS: REASONING/KNOWLEDGE/BOTH/UNCLEAR"
        },
        {
            "title": "USER MESSAGE",
            "content": "You will be shown question from an academic dataset, along with (1) *CORRECT* answer from high-reasoning model and (2) an *INCORRECT* answer from low-reasoning model. Your task is to analyze *why* the low-reasoning model failed. Consider whether the failure is primarily due to: *REASONING*: Insufficient logical thinking, problem-solving ability, or step-by-step analysis *KNOWLEDGE*: Lack of domain knowledge (missing facts, formulas, concepts, procedures) *BOTH*: Significant deficiencies in both reasoning and knowledge *UNCLEAR*: Cannot determine the primary cause of failure QUESTION Context: {{context}} Question: {{question}} CORRECT ANSWER (from {{high_model}}): {{high_full_response}} INCORRECT ANSWER (from {{low_model}}): {{low_full_response}} Analyze why the low-reasoning model failed. Was it primarily due to insufficient reasoning ability or lack of knowledge? Figure 9: Prompt used to classify failure cause (reasoning vs. knowledge) for low-reasoning models. Results We show that both protocols agree that filtered instances require significantly more reasoning efforts than non-filtered instances from SCIREAS, with (a) showing 71% agreement in accuracy by LLMs with 78% human annotation agreement and (b) showing 91% agreement by LLMs with 90% human agreement, where human annotations are made by authors on 80 sampled tests for each protocol. parameters with respect to official documentation where specificity on token budget is not allowed; for other reasoning models that allows thinking budgets as input (e.g. Gemini and Anthropic), we adopt low as definition introduced by LiteLLM,8 which corresponds to 1024 budget, and remove the constraint to allow for as many thinking tokens as the model needed to unleash full potential as high reasoning effort, corresponding to the highest reasoning effort from OpenAI and xAI models. For all frontier reasoning models, if not restricted, we set temperature=1, borrowed from OpenAI forced setting,9 and top-p=0.95, borrowed from recommended setting by Anthropic,10 with max generation length of 64K, as we observe no models tend to output more than 20K tokens. We log API pricing at the time of writing in Table 8. Training / Evaluation Hyperparameter D.1 Distillation from Reasoning LLMs To obtain high-performing reasoning models for study, we employ distillation method that finetunes smaller models using Supervised Fine-tuning (SFT) on the CoT trajectories generated by large reasoning models, as it is more effective than reinforcement learning (RL) with the small models alone (DeepSeek-AI et al., 2025). Specifically, we consider the standard SFT framework for language models where the objective is to train model fÎ¸ to approximate distribution over output sequences conditioned on input x, based on dataset = {(xi, yi)}N i=1. For recent reasoning LLMs such as DeepSeek-R1, the output consists of two main parts: reasoning trace and the actual output a. In practice, the reasoning traces are enclosed by keywords <think> and </think>, indicating the start and the end of the reasoning process. The model is trained with the standard SFT objective: L(Î¸) = Î£(x,y)DÎ£y t=1 log pÎ¸(yty<t, x), where yt is the t-th token and y<t is its prefix."
        },
        {
            "title": "Configuration",
            "content": "For OpenAI and xAI provided reasoning models, we apply generic low and high reasoning effort 8https://docs.litellm.ai/docs/providers/anthropic#usage thinkingreasoning_content 9https://community.openai.com/t/o3-mini-unsupportedparameter-temperature/1140846/3 10https://docs.anthropic.com/en/docs/build-withclaude/extended-thinking#feature-compatibility"
        },
        {
            "title": "Model",
            "content": "Input Price ($ per 1M tokens) Output Price ($ per 1M tokens) OpenAI models GPT-4.1-2025-04-14 o3-mini-2025-01-31 o3-2025-04-16 o4-mini-2025-04-16 GPT-5-2025-08-07 GPT-oss-120B (Together AI) DeepSeek models DeepSeek-V3-0324 DeepSeek-R1-0120 DeepSeek-R1-0528 Alibaba Qwen models (Together AI) Qwen3-32B Google models Gemini-2.5-Pro-Preview-05-06 Meta models (Together AI) Llama-4-Maverick-17B-128E-Instruct-FP Anthropic models Claude-Sonnet-4-20250514 2.00 1.10 2.00 1.10 1.25 0.15 0.14 0.55 0.55 0.40 1.25 0. 3.00 8.00 4.40 8.00 4.40 10.00 0.60 0.28 2.19 2.19 1.20 10.00 0. 15.00 Table 8: Pricing ($ per 1M tokens) for input and output across different LLM providers at the time of writing, without any discounts. D.2 Extended Setup D.2.1 Training Settings We filter out instances with token length greater than 4096.11 The models are trained for 5 epochs with cosine learning rate scheduler, maximum learning rate of 1e-5, and 3% warmup steps. D.2.2 Evaluation setup The reasoning models could produce excessively long outputs, and may be prone to self-repetition with greedy decoding (DeepSeek-AI, 2025). In this work, unless otherwise specified, we apply greedy decoding on non-CoT fine-tuned models and topp=0.95, temperature=0.6 on reasoning models, with maximum generation length of 64K. From our preliminary studies, we observe that the setup generally reflects the best performance for both settings, and the decoding setup matches the recommended setup from recent efforts in large reasoning models, such as Llama-Nemotron (Bercovich et al., 2025a). Notably, for Qwen (Yang et al., 2024) models and their variants, we apply YaRN context extension (Peng et al., 2023) as recommended by the official model card (Team, 2024). D.3 Extended Baseline Results Recent efforts that leverage SYNTHETIC-1 or similar data with reasoning traces for training reasoning 11Longer input lengths would slow down our training in quadratic order based on 8 80GB A100/H100 GPUs. LLMs often incorporate data spanning from math to coding (Mattern et al., 2025) without focusing on science-related tasks, providing hard-to-analyze synergy and suboptimal performance in scientific reasoning (see SYNTHETIC-1-SFT in Table 10). As shown in Table 1, Qwen-STEM and QwenMath both exhibit significant improvement over the base model on SCIREAS and SCIREAS-PRO. Qwen-Math slightly outperforms Qwen-STEM on SCIREAS and the gap is amplified on SCIREASPRO. Given limited subject coverage on SYNTHETIC1-Math dataset, the strong performance of checkpoints fine-tuned on it only seems surprising Does the improvement come from generalization from math reasoning to wider domain, or is it because the high-reasoning instances in our datasets are math-intensive? To answer this question, we categorize SCIREAS-PRO into math and non-math instances by heuristics. Specifically, we label instances as math-needed if they contain explicit numeric quantities that typically imply computation. Importantly, numbers that appear solely within unit expressions (e.g., cm2) or chemical formulas (e.g., H2O or NaCl) are not treated as indicators of math-related reasoning. Full details are provided in Appendix D.3.1. As shown in Table 9, we find that math computation appears frequently among reasoning-intensive instances, and the improvements on SCIREAS-PRO mostly come"
        },
        {
            "title": "E Extended KRUX Details",
            "content": "E.1 Knowledge Extraction for code triple In this work, we apply DeepSeek-R1 as the extractor. Prompt shown in Figure 11. We show set of KIs extracted from Qwen2.5-7B-Instruct (Figure 12), Qwen-Math variants (Figure 13), and DeepSeek-R1 (Figure 14) for the same question from GPQA: Question: large gene has dozens of exons, of which the central ones helical folded repeats that connect the cytoskeleton with sarcolemma and extracellular space. Each exon usually codes for one folded The most common triple alpha helix. central mutations exon deletions that create out-of-frame progressive peptides degenerative and organ waste. solution is to deliver Morpholino that recognizes the 5 end of the out-of-frame exon in pre-mRNA. The molecule prevents binding of the spliceosome and creates exon skipping and in-frame joining. Several missing exons are well tolerated by an organism. Which structure below is not involved in (A) lariat (B) the proposed therapy? antisense (C) R-loops (D) polyA tail. gene are the of from improved math capabilities. For non-math instances, -math variants hardly improve, while - STEM variants and -BOTH variants, trained with STEM subjects data, show noticeable improvements."
        },
        {
            "title": "Model",
            "content": "Math Acc. Non-Math Acc. SCIREAS-PRO: 1,260 Instances 88 1,172 # Qwen Qwen-STEM Qwen-Math Qwen-BOTH Llama Llama-STEM Llama-Math Llama-BOTH 14.25 15.53 17.58 20. 11.52 14.16 17.24 15.96 12.50 23.86 13.64 28.41 13.64 15.91 13.64 23.86 Table 9: Accuracy breakdown on math and non-math instances for SCIREAS-PRO. -Math and -STEM variants contribute to different dimensions of performance, while -BOTH capture improvements on both dimensions. D.3.1 Math vs. Non-Math question is marked math-containing when it includes 1. signed or unsigned integer/decimal (e.g. 3, -2.5, 60, 9.81), 2. not embedded inside word (so digits in H2O, COVID-19, IL-2 . . . are ignored), and 3. optionally followedwithout intervening lettersby any one of the unit strings listed in Fig. 10. Units recognised by the heuristic % C, F, K, g, kg, mg, lb/lbs, Âµg/ug, oz m, cm, mm, km mL/ml, L/l, ÂµL/Âµl/ul Pa, kPa, MPa, atm, bar, mbar J, kJ, MJ; W, kW, MW, GW V, kV; A, mA, ÂµA/uA Hz, kHz, MHz, GHz cm2, m2, mm2, km2 cm3, m3, mm3, km3 mol; M, mM, nM, ÂµM/uM, pM dB; rpm; rad/s s, ms, Âµs/us, ns; min, day/days; yr/yrs Figure 10: Unit suffixes accepted by the numeric heuristic. standalone number with any of these units (or no unit) is treated as evidence that the question contains mathematical content. 20 t N - l n e - e G S - 1 - E Y r e - Q B - Q 1 - E Y - 3 Q 3 Q"
        },
        {
            "title": "Models",
            "content": "1 p Base Model Q2.5-Math L3.1-Inst. Q2.5-Base Q2.5-Inst. Q3-Base Training Methods Trained by Us GPQA SuperGPQA* MMLU-Pro* LabBench* OlympiadBench SciBench SciEval* SciKnowEval* SciRIFF* UGPhysics* Average SCIREAS-PRO SFT No 44.42 31.90 60.86 27.14 53.03 61.85 43.64 28.45 29.17 50.30 43.08 26. SFT&RL No 37.95 29.39 65.64 27.78 37.62 57.66 68.67 30.69 34.01 45.92 43.53 23.75 RL No 35.94 14.26 62.14 35.58 19.82 19.08 70.34 34.19 37.75 20.86 34.99 13. SFT Yes 38.84 22.39 56.21 28.61 40.75 51.59 46.41 19.13 28.57 43.96 37.64 19.44 SFT Yes 44.20 19.47 63.57 35.76 29.33 48.27 38.53 31.85 39.24 46.52 39.67 19. SFT Yes 40.63 20.33 65.00 33.00 34.55 47.11 72.36 32.00 41.81 40.03 42.68 21.11 SFT Yes 50.89 30.11 76.57 35.07 43.78 61.27 80.60 39.46 44.01 52.28 51.41 24. No 55.80 23.32 73.36 36.99 28.51 54.05 81.51 37.99 47.23 30.98 46.97 19.05 k t - 3 Q No 55.80 38.27 81.71 38.19 21.30 68.21 84.02 41.81 47.26 59. 53.64 29.92 Table 10: Performance of concurrent efforts on open-recipe post-training in <10B-parameter level. SCILIT01 shows competitive performance relative to concurrent reasoning post-training methods. We abbreviate Qwen2.5, Qwen3, and Llama-3.1 as Q2.5, Q3, and L3.1, respectively; -Inst. denotes the instruction-tuned variant. The best and second-best overall results are highlighted in bold and underlined, respectively."
        },
        {
            "title": "USER MESSAGE",
            "content": "You are given reasoning chain that explains and justifies particular conclusion or answer. Your task is to extract **all distinct knowledge pieces** from this chain. knowledge piece is any standalone statement conveying an explicit fact, definition, mechanism, relationship, or insight that can be generalized beyond the specific question. ## Instructions: 1. Read the entire reasoning chain. 2. Identify each discrete fact or insight expressed. 3. Rewrite each as self-contained, generalizable sentence. 4. Do **not** include any contextual or examplespecific details. 5. Output **only** list of those sentences. ## Output Format: knowledge-Piece-1 knowledge-Piece-2 ... ## Reasoning Chain: {{REASONING}} ## Now perform the extraction. Figure 11: Full knowledge extraction prompt template used in our experiments. 21 KNOWLEDGE EXTRACTION OUTPUT KNOWLEDGE EXTRACTION OUTPUT Based on the reasoning chain, here are all distinct knowledge pieces: 1. Morpholinos are antisense oligonucleotides that bind to complementary RNA sequences. 2. The spliceosome recognizes splice sites on premRNA through components like U1 snRNP, which binds to the 5 splice site. 3. Blocking spliceosome access to splice site prevents recognition of exon boundaries and causes exon skipping. 4. Exon skipping can restore the reading frame by joining exons flanking the skipped exon. 5. Functional proteins may be produced if skipped exons encode non-critical regions and the remaining exons maintain an in-frame sequence. 6. Lariat structures form during canonical splicing when introns are removed via branch site cleavage. 7. R-loops are nucleic acid structures involving RNADNA hybrids and are not directly involved in spliceosome-mediated splicing. 8. Antisense oligonucleotide therapies rely on sequence-specific binding to pre-mRNA targets. 9. Polyadenylation (polyA tail addition) occurs during mRNA maturation and is unrelated to splice site selection or exon skipping mechanisms. Figure 14: Example KIs extracted from DeepSeek-R1. E.2 Knowledge Probing We provide our probing question synthesis prompt (Figure 15), example input and output (Figure 16), and knowledge probing results in Table 4. Based on the reasoning chain, here are all distinct knowledge pieces: 1. R-loops are structures formed when strand of RNA hybridizes with its complementary DNA strand, displacing the other DNA strand. 2. Morpholino can prevent spliceosome binding. 3. Preventing spliceosome binding can lead to exon skipping. 4. Exon skipping can result in in-frame joining of exons. 5. The process of using Morpholino to induce exon skipping does not involve R-loops. Figure 12: Example KIs extracted from Qwen2.5-7BInstruct. KNOWLEDGE EXTRACTION OUTPUT Based on the reasoning chain, here are all distinct knowledge pieces: 1. Morpholino oligonucleotides can bind to the 5 splice site of pre-mRNA to block spliceosome recognition. 2. Blocking spliceosome binding prevents normal intron removal and leads to exon skipping. 3. Exon skipping can enable in-frame joining of adjacent exons, preserving the reading frame. 4. Antisense molecules function by complementary base-pairing to specific RNA sequences to modulate splicing. 5. Lariat structures are formed during canonical splicing via branch site interactions but are absent when spliceosome activity is inhibited. 6. R-loops are DNA-RNA hybrid structures unrelated to spliceosome-mediated exon skipping mechanisms. 7. The polyA tail is added post-transcriptionally and is not directly involved in splice site selection or exon skipping processes. Figure 13: Example KIs extracted from Qwen-Math. 22 EXAMPLE src_text \"Hyperfine structure in EPR spectroscopy arises from interactions between unpaired electrons and nuclear spins.\""
        },
        {
            "title": "EXAMPLE OUTPUT",
            "content": "{ \"question\": \"What causes hyperfine structure in EPR spectroscopy?\", \"correct_answer\": \"Interactions between unpaired electrons and nuclear spins\", \"incorrect_answers\": [ \"Interactions between electron spins and lattice vibrations\", \"Coupling between electron orbitals and magnetic fields\", \"Dipolar interactions between neighboring nuclei\", \"Spin-orbit coupling within the electron cloud\", \"Chemical shift anisotropy of atomic orbitals\" ], \"evidences\": [ \"Hyperfine structure in EPR spectroscopy arises from interactions between unpaired electrons and nuclear spins.\" ] } Figure 16: Knowledge probing question synthesis example input and output."
        },
        {
            "title": "USER MESSAGE",
            "content": "You are meticulous question-authoring assistant. Your job is to convert declarative knowledge statements into *probing* multiple-choice questions that can test whether another language model truly stores the fact in its parametric memory. ## IMPORTANT INSTRUCTIONS FOR QUESTIONS: 1. Factual: It should be about specific detail or fact mentioned in the statement. For example, true or false statement, statistic, definition, etc. 2. Important: It should be question about the main topic or key detail/finding/conclusion of the statement. 3. Context-Independent: It should be fully understandable on its own, without phrases like \"the proposed model\" or \"this approach\" that assume prior context. ## IMPORTANT INSTRUCTIONS FOR ANSWERS: 1. Provide one correct answer and 4 - 6 incorrect answers. 2. Ensure all answers are roughly the same length and follow similar style so the correct answer cannot be guessed based on length or style alone. 3. The incorrect answers must be plausible but ultimately wrong, reflecting misunderstanding or misinterpretation of the knowledge. ## OUTPUT FORMAT: Please provide the question, correct answer, incorrect answers, and list of text snippets from the article as \"evidences\" in the following format: { \"question\": \"Your question here\", \"correct_answer\": \"Correct answer here\", \"incorrect_answers\": [\"Incorrect answer 1\", ..., \"Incorrect answer N\"], \"evidences\": [\"Text snippets from the article that supports the question and correct answer\", \"Another text snippet\"] } # Knowledge Statement: {src_text} Please provide your response in the specified format without any additional text. Figure 15: Knowledge probing question synthesis template used in our experiments. 23 Domain Task Source Subtask/Subdomain Instances Total Metrics GPQA MMLU-Pro SciBench Physics physics fund thermo class OlympiadBench-COMP physics_en SciKnowEval.L5 SciEval physics_problem_solving physics_knowledge_application physics_scientific_calculation Electrodynamics Thermodynamics GeometricalOptics Relativity ClassicalElectromagnetism ClassicalMechanics WaveOptics QuantumMechanics TheoreticalMechanics AtomicPhysics SemiconductorPhysics Solid-StatePhysics StatisticalMechanics Physics Chemistry chemistry quan chemc atkins matter chemical_procedure_generation chemical_reagent_generation chemistry_knowledge_application chemistry_scientific_calculation Chemistry computer science Qasper Computer Science and Technology UGPhysics Physics"
        },
        {
            "title": "SuperGPQA",
            "content": "GPQA MMLU-Pro SciBench"
        },
        {
            "title": "Chemistry",
            "content": "SciKnowEval.L"
        },
        {
            "title": "SuperGPQA",
            "content": "MMLU-Pro SciRIFF SuperGPQA MMLU-Pro SciBench"
        },
        {
            "title": "Math",
            "content": "math calc stat diff OlympiadBench-COMP maths_en SuperGPQA"
        },
        {
            "title": "Mathematics",
            "content": "187 200 81 83 63 236 200 29 200 170 200 54 200 200 200 200 200 200 200 148 154 200 1482 183 200 41 47 121 57 74 125 200 200 910 200 107 108 200 52 92 55 674 1460 Acc Acc Acc Acc Acc Acc LM Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc"
        },
        {
            "title": "Acc\nAcc\nAcc\nAcc\nAcc\nAcc\nLM\nLM\nAcc\nAcc\nAcc",
            "content": "5087 2158 415 Acc F1, LM Acc"
        },
        {
            "title": "Acc\nAcc\nAcc\nAcc\nAcc\nAcc",
            "content": "Table 11: Domain-wise breakdown of SCIREAS tasks and instance counts (Part 1: Physics to Math). 24 SciKnowEval.L5 crystal_structure_and_composition Domain Task Source Subtask Instances Total Metrics GPQA MMLU-Pro LabBench Biology biology CloningScenarios ProtocolQA SeqQA SciKnowEval.L5 biological_procedure_generation SciEval"
        },
        {
            "title": "SuperGPQA",
            "content": "MMLU-Pro SciRIFF"
        },
        {
            "title": "SuperGPQA",
            "content": "MMLU-Pro SuperGPQA Biology"
        },
        {
            "title": "Engineering",
            "content": "biological_reagent_generation biology_knowledge_application biology_scientific_calculation Biology health SciFact Evidence Inference specified_band_gap_material_generation property_and_usage_analysis Materials Science and Engineering engineering Control Science and Engineering Information and Communication Engineering Electrical Engineering Chemical Engineering and Technology Power Engineering and Engineering Thermophysics Electronic Science and Technology Hydraulic Engineering Mechanics Mechanical Engineering Civil Engineering Optical Engineering Nuclear Science and Technology Instrument Science and Technology Systems Science 78 200 33 108 600 200 200 200 200 92 200 184 196 200 118 110 200 77 156 234 226 345 95 67 456 30 93 162 30 12 22 1911 624 2205 Acc Acc Acc Acc Acc LM LM Acc Acc Acc Acc F1, LM F"
        },
        {
            "title": "Acc\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc",
            "content": "Table 12: Domain-wise breakdown of SCIREAS tasks and instance counts (Part 2: Biology to Engineering). 25 Domain Task Source Subtask/Subdomain Instances Total Metrics GPQA MMLU-Pro SciBench Physics physics fund thermo class OlympiadBench-COMP physics_en SciEval UGPhysics"
        },
        {
            "title": "SuperGPQA",
            "content": "GPQA MMLU-Pro SciBench"
        },
        {
            "title": "SuperGPQA",
            "content": "physics_knowledge_application physics_scientific_calculation Electrodynamics Thermodynamics GeometricalOptics Relativity ClassicalElectromagnetism ClassicalMechanics WaveOptics QuantumMechanics TheoreticalMechanics AtomicPhysics SemiconductorPhysics Solid-StatePhysics StatisticalMechanics Physics Chemistry chemistry quan chemc atkins matter chemistry_knowledge_application chemistry_scientific_calculation Chemistry Comp Sci MMLU-Pro SuperGPQA computer science Computer Science and Technology MMLU-Pro SciBench"
        },
        {
            "title": "Math",
            "content": "math calc stat diff OlympiadBench-COMP maths_en SuperGPQA"
        },
        {
            "title": "Mathematics",
            "content": "8 5 1 10 8 25 1 1 17 16 9 16 21 17 16 17 13 13 13 13 15 133 31 3 3 2 6 3 11 3 73 6 15 3 2 2 3 92 181 Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc"
        },
        {
            "title": "Acc\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc",
            "content": ""
        },
        {
            "title": "21 Acc\nAcc",
            "content": ""
        },
        {
            "title": "Acc\nAcc\nAcc\nAcc\nAcc\nAcc",
            "content": "Table 13: Domain-wise breakdown of SCIREAS-PRO tasks and instance counts (Part 1: Physics to Math). 26 Domain Task Source Subtask Instances Total Metrics GPQA MMLU-Pro LabBench"
        },
        {
            "title": "SciEval",
            "content": "Biology Biology biology CloningScenarios ProtocolQA SeqQA biology_knowledge_application biology_scientific_calculation"
        },
        {
            "title": "Medicine",
            "content": "MMLU-Pro health"
        },
        {
            "title": "Material Sci SuperGPQA Materials Science and Engineering",
            "content": "MMLU-Pro SuperGPQA Control Science and Engineering engineering"
        },
        {
            "title": "Engineering",
            "content": "Information and Communication Engineering Electrical Engineering Chemical Engineering and Technology Power Engineering and Engineering Thermophysics Electronic Science and Technology Hydraulic Engineering Mechanics Mechanical Engineering Civil Engineering Optical Engineering Nuclear Science and Technology Instrument Science and Technology Systems Science 2 6 2 10 89 3 2 9 5 13 14 7 15 32 43 13 13 54 7 18 23 3 2 4 123 Acc Acc Acc Acc Acc Acc Acc Acc"
        },
        {
            "title": "13 Acc",
            "content": ""
        },
        {
            "title": "Acc\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc\nAcc",
            "content": "Table 14: Domain-wise breakdown of SCIREAS-PRO tasks and instance counts (Part 2: Biology to Engineering)."
        },
        {
            "title": "F LLM Usage Statement",
            "content": "We used ChatGPT-o3 from OpenAI for grammar and typo corrections."
        }
    ],
    "affiliations": [
        "Allen Institute of AI",
        "Harvard University",
        "Northwestern University",
        "Yale University"
    ]
}