{
    "paper_title": "FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction",
    "authors": [
        "Wenqi Guo",
        "Shan Du"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5\\% matching accuracy and above 96\\% regeneration success, and still exceeds 92\\% matching and 94\\% regeneration in a near zero knowledge setting. These results expose a structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers."
        },
        {
            "title": "Start",
            "content": "FaceLinkGen: Rethinking Identity Leakage in Privacy-Preserving Face Recognition with Identity Extraction Wenqi Guo 1 2 Shan Du 1 6 2 0 2 2 ] . [ 1 4 1 9 2 0 . 2 0 6 2 : r Abstract Transformation-based privacy-preserving face recognition (PPFR) aims to verify identities while hiding facial data from attackers and malicious service providers. Existing evaluations mostly treat privacy as resistance to pixel-level reconstruction, measured by PSNR and SSIM. We show that this reconstruction-centric view fails. We present FaceLinkGen, an identity extraction attack that performs linkage/matching and face regeneration directly from protected templates without recovering original pixels. On three recent PPFR systems, FaceLinkGen reaches over 98.5% matching accuracy and above 96% regeneration success, and still exceeds 92% matching and 94% regeneration in near zero knowledge setting. These results expose structural gap between pixel distortion metrics, which are widely used in PPFR evaluation, and real privacy. We show that visual obfuscation leaves identity information broadly exposed to both external intruders and untrusted service providers. 1. Introduction and Related Works The fundamental promise of transformation-based Privacy Preserving Face Recognition (PPFR) systems is compelling: verify users identity without ever exposing their raw facial data to potential attackers (Dai et al., 2025; Mi et al., 2023; 2024; 2022; Jin et al., 2024). Originally, the threat model was for curious or malicious recognition service provider (Erkin et al., 2009; Ji et al., 2022), but it now also includes, or shifted towards, external attackers by wiretapping (Mi et al., 2024) or leaked databases. We argue that robust leakage analysis must address both the service provider who inherently accesses the templates and the external intruder who obtains the templates by wiretapping or database leakage. 1Department of CMPS, University of British Columbia, Kelowna, Canada 2Weathon Software, Canada. Correspondence to: Shan Du <shan.du@ubc.ca>. Preprint for arXiv. 1 Figure 1. Regeneration attack results. In each subplot, the left is the original image and the right is the regenerated image from the protected template. Each row shows examples from one PPFR method, in the order of PartialFace, MinusFace, and FracFace. The prevailing evaluation paradigm for these systems is currently significantly limited. Historically, the dominant objective has been to prevent the reconstruction of the original registration image. This is typically measured through pixel-level or local similarity metrics such as peak signal-tonoise ratio (PSNR) and structural similarity index measure (SSIM) between the original and reconstructed images, legacy inherited from image privacy and compression literature. Consequently, large body of prior work adopts this paradigm, using resistance to pixel-level recovery as evidence of privacy protection and optimizing attack objectives accordingly. Representative systems, including DuetFace (Mi et al., 2022), MinusFace (Mi et al., 2023), PartialFace (Mi et al., 2024), FaceObfuscator (Jin et al., 2024), and FracFace (Dai et al., 2025), explicitly rely on these metrics to argue robustness against recovery attacks. This evaluation paradigm, however, rests on critical implicit assumption: that preventing pixel-level reconstruction is both necessary and sufficient to prevent identity leakage. In this paper, we show that this assumption does not hold. Crucially, compromising privacy does not require recovering the original registered image, nor does pixel-level similarity reliably correspond to identity consistency. In the facial domain, two images that are visually or pixel-wise similar may represent different identities, while identityrevealing information can remain accessible even when pixel-level reconstruction is infeasible. CanFG (Wang et al., FaceLinkGen framework that leverages generative models to synthesize sematic-consistent images without pixel-level similarity in federated learning attacks. Complementary findings further suggest that traditional reconstruction metrics fail to capture how humans perceive privacy leakage (Sun et al., 2023). Our approach differs from standard Model Inversion Attacks (MIAs). One category of MIA, represented by Wang et al. (2025), recovers original images from embeddings. For many deep facial embedding models, such processes resemble image generation tasks rather than adversarial attacks, as they exploit the inherent invertibility of learned representations. Similar works exist in the ID-controlled image generation domain, like Arc2Face (Papantoniou et al., 2024), FulID (Guo et al., 2024), and FaceID IP-Adapter (Ye et al., 2023). In contrast, our method targets structural vulnerabilities in the template generation process itself. Since this conversion often utilizes rule-based transformations independent of specific deep models, the attack surface differs from embedding-based reconstruction. Another category of MIA, which is closer to the original definition (Fredrikson et al., 2015), such as the ones prevented by Wang et al. (2024b), aims to reconstruct the training dataset to compromise identity privacy in the training set. This focus on training data deviates from our objective of protecting individual user templates. Existing solutions for training-level privacy include FaceMAE (Wang et al., 2022) and the use of synthetic data (Bae et al., 2022). Additionally, it is also important to distinguish PPFR systems from face anonymization (or De-Identify) systems. PPFR aims to verify an individuals identity without exposing private information, whereas face anonymization seeks to completely replace true identity with synthetic or virtual one, typically for dataset sharing or social media posting. Ideally, the de-IDed face should still look like face, but with completely different identity, and cannot be used for recognition. PPFR also differs from invertible anonymization, as PPFR explicitly requires templates to be non-invertible. Even during normal usage, such as verification, inversion should never occur, as this would expose the users identity to the curious or malicious service provider. Certain works (Wang et al., 2024a; Alam et al., 2025) fail to distinguish between these two objectives, leading to secondary complications which are addressed in the Section 9 section. This fundamental difference in objectives justifies the exclusion of anonymization-based methods from our comparative analysis. Our Contribution: This paper proposed new attack to expose fundamental misalignment between prevailing reconstruction-based evaluation paradigms and the realistic objectives of an attacker. Moving beyond the outdated assumption that preventing pixel-level reconstruction equates to security, we introduce and validate an identity-centric Figure 2. SSIM and PSNR are not always correlated with identity correlation. Figure 3. Using pixellevel loss or StyleGAN will yield unsuccessful reconstruction compared to our ID-guided method. Table 1. Comparison between pixel-level metrics and identity-level metrics on two cases: protected face generated by CanFG with its original face, and two images of the same person. We can see that higher pixel-level similarity does not mean higher identity-level similarity. Compared With SSIM PSNR MSE FS Another Face CanFG Face 0.235 0.841 10.44 26. 6699 143 0.586 0.008 2024a) can generate two images with very high pixel-level similarity yet in completely different identities; conversely, in daily life, any two arbitrary photos of the same person (one ID photo and one social media photo) would have very high identity similarity yet likely very low pixel-level or structure-level similarity. An example is provided in Figure 2. These misconceptions mislead not only the evaluation but also the simulated attack design. By employing pixel-level loss functions, simulated attackers (red-team researchers) are inadvertently trapped into pursuing the specific registration image as ground truth. This objective is often mathematically impossible due to the information loss in protection, causing the generator to produce blurry image, likely an average of all images in the dataset. The failure is illustrated in Figure 3. Even though FracFace (Dai et al., 2025) measured ID-similarities, they mainly focus on pixel-level or local metrics. The StyleGAN (Karras et al., 2019) is also likely guided by pixel-level loss, such that even though it generated realistic face, it failed to generate the face with the same identity as the original image. Recent work has begun to challenge this reconstructioncentric perspective in non-facial domains by shifting attention toward semantic-level inversion. In such settings, the attacker aims not to recover the original image itself, but to regenerate information that is semantically consistent with the original identity. This goal is both easier to achieve and more aligned with realistic attack objectives. For instance, Yue et al. (2023) proposes semantic recovery FaceLinkGen leakage evaluation standard. We demonstrate that identity security in PPFR is far more fragile than suggested by pixellevel metrics; identity information can be systematically exploited for linkage and regeneration even when the original image remains unreconstructable under legacy metrics like PSNR or SSIM. Our primary contribution lies in shifting the realm of privacy analysis from visual metrics to identity metrics. By showing that even recent SOTA methods like FracFace (NeurIPS 2025) are vulnerable under our FaceLinkGen analysis, we argue that the current path of protection by visual distortion is inherently insufficient. Thus, this work is not mere failure analysis, but call for new, identity-centric security standard that defines the next generation of privacy-preserving face recognition. 2. Threat Model We assume the attacker, which may be an external attacker or curious or malicious recognition service provider 1, has oracle access to the conversion process: the ability to query it with arbitrary inputs and observe outputs, but no knowledge of its internal architecture or parameters. This assumption is obviously realistic to malicious or curious service providers (which have complete knowledge of the system) but also realistic to outsiders because the conversion process runs locally on the users device in all evaluated systems (Mi et al., 2023; 2024; Dai et al., 2025). An external attacker operating in their own environment can directly invoke this function or intercept protected templates via packet capture without reverse engineering the application. From regulatory perspective, requiring raw face images to be uploaded to remote server is considered even more problematic because one of the purposes of PPFR is to protect the user against curious or malicious recognition service providers. This threat model is strictly more constrained than prior PPFR evaluations. Mi et al. (2024) assumes the attacker knows the conversion architecture but not the random channel selection parameters, and in the black box setting of Mi et al. (2023), the attacker has access to the conversion process but not the selected channel IDs. By contrast, we assume no knowledge of the architecture, parameters, or hyperparameters, treating the conversion process purely as black box oracle. Although our model assumes oracle access, it imposes stricter constraints than some prior black box attacks. For example, Zhang et al. (2024) requires approximately 6,900 online verification queries per identity and depends on the server returning continuous similarity scores for optimization. This approach is highly susceptible to rate limiting and 1Recent work has shifted the focus more towards external attackers, while we argue that the curious service provider is the main adversary in the PPFR threat model. fraud detection systems, which typically throttle or lock access after small number of failed attempts. Many deployed FR systems return only binary accept or reject decisions or quantized similarity scores, rendering gradient-based optimization infeasible (National Cryptologic Centre, 2011; The BioAPI Consortium, 2001). This is widely known as common, no-cost approach to prevent hill climbing attacks. By contrast, our method exploits the locally available conversion process, which can be queried offline without constraints and without reliance on server-side behavior. In Section 6, we also test our method in different, minimal assumption setting, where the attacker cannot query the conversion process in batches but only has few (30) known pairs and does not have access to the server for similarity metrics. 3. Methods The simplicity of our method is intentional. We show that even strong protection methods fail with simple, standard distillation process, proving that the vulnerability resides in the representation itself. To formulate this, we consider face image as combination of identity information zI and non-identity (nuisance) information zN , such that p( zI , zN ). In transformation-based PPFR systems, protected template is generated to hide the visual data of while retaining identity utility. This process can be viewed as lossy mapping that suppresses the information quantity of zN while preserving zI : (1) p( zI ). Existing evaluations often equate privacy with the failure of pixel-level reconstruction. However, since zN is largely discarded, recovering the original pixels is severely ill-posed problem. Conventional attacks fail because they attempt to optimize for specific nuisance factors (e.g., exact lighting or pose) that no longer exist in , resulting in blurry or identity-inconsistent outputs. Our approach, FaceLinkGen, instead focuses on extracting the remaining identity information. We use distillationstyle procedure to align the template domain with standard identity embedding space. Given public dataset, we train student model fs to recover an identity representation from . The training objective is to maximize the cosine similarity between the students output and the embeddings zt extracted by frozen teacher model ft from original images X: = 1 1 (cid:88) k=1 s(cid:0)fs(tk), ft(ik)(cid:1), (2) where attacker. = fs(T ) is the identity feature recovered by the FaceLinkGen Once is extracted, the attack bypasses the need for pixel reconstruction by leveraging diffusion-based generative model gdiff . Rather than trying to find the original zN , we substitute the missing information by sampling from the models stochastic noise ϵ: ϵ (0, I), = gdiff (z , ϵ). (3) In this process, we do not attempt to recover the discarded original nuisance factor zN . Instead, we extract the identity representation = fs(T ) from the template and introduce stochastic noise vector ϵ (0, I) as proxy for newly sampled set of non-identity factors . This enables the model to bypass the ill-posed reconstruction of the original zN by providing the necessary information density to synthesize realistic face with the same identity but different attributes. (zN ) Our formulation demonstrates that as long as the information quantity of zI persists in , an attacker can recover and combine it with random ϵ to regenerate high-fidelity, identity-consistent face. This structural vulnerability implies that visual distortion does not provide meaningful identity privacy, as the specific recovery of zN is not prerequisite for successful attack. Note that our attack method is independent of the specific face recognition model employed by the PPFR server. We use ArcFace as the teacher and student network solely because it is widely-adopted, publicly available facial embedding model, and its compatibility with the Arc2Face generative model simplifies our demonstration. Other embedding models like FaceNet (Schroff et al., 2015) can also be used as long as there is compatible generative model. The servers actual recognition backbone could be any commercial or proprietary model. Our attack only requires that the protected template retains identity-discriminative features that are learnably aligned with some facial embedding space accessible to the attacke, condition implicitly assumed by any PPFR system that aims to preserve recognition utility. 4. Attack Vectors 4.1. Linkage Attack linkage attack aims to associate real-world identity (e.g., public face image) with protected identity, or to link two protected templates belonging to the same individual across different leaked databases. The first case is referred to as face-to-template linkage, while the second is templateISO/IEC 24745 explicitly requires to-template linkage. resistance against template-to-template search, but does not address face-to-template search. This is likely utlity trade off for the verification needs of the service provider. This attack vector is similar to an attack vector for hashing: when the input space is known, an attacker can enumerate all possible inputs and map each hashed output back to its original input. In both attack scenarios, the attacker first obtains query embedding eq. This embedding can be extracted using either the student model fs or the teacher model ft, depending on the domain of the query data. The adversary then computes embeddings for all protected templates in the leaked database using fs and performs nearest-neighbor search. This process can be written as arg max tT s(eq, fs(t)), (4) where s(, ) denotes cosine similarity. 4.2. Regeneration Attack As discussed earlier, reconstructing the original enrollment image is unnecessary and, in many cases, impossible. However, once universal face embedding (e.g., ArcFace) can be extracted from protected template, modern face generation models can be leveraged. In this work, we use Arc2Face, which takes facial embedding as input and generates face image whose embedding matches the input. This allows us to synthesize realistic face corresponding to the protected template without reconstructing the original image. 5. Experiments and Results We selected three SOTA or near-SOTA work that has their accessible source code, PartialFace (Mi et al., 2023) from ICCV 2023, MinusFace (Mi et al., 2024) from CVPR 2024, and FracFace (Dai et al., 2025) from NeurIPS 2025. For distillation, we used subset of CASIAWebFace (Yi et al., 2014) with around 10K identities and 90K images. The facial embedding model is Antelopev2 with one additional 3x3 Conv2D layer added before to be compatible with different template formats (channel numbers) if needed. The Antelopev2 is chosen because it is what Arc2Face accepts. The dataset is split into training and validation set in an 80-20 ratio. For regeneration testing, we used three datasets. The validation hold-out set of CASIA-WebFace, this person does not exist (TPDNE) dataset 2, and Labelled Face in the Wild (LFW) dataset. The hold-out set is used to test the ability of our method in real images while ensuring no ID duplications. The LFW dataset is used to test the cross-dataset performance of our method with distribution shift, and the TPDNE is used as synthetic dataset to avoid data cross-contamination from Stable Diffusion 1.5 and Arc2Face training data. Compared to the hold-out set and LFW, photos in the TPDNE dataset are also closer to headshot, which is what is usually used to create the protected templates. The distillation process was completed in under two hours on single NVIDIA A6000 GPU for each of the three evaluated methods, at an estimated cost of approximately USD 2TLeonidas/this-person-does-not-exist 4 FaceLinkGen Figure 4. After our model is trained, two main attack vectors can be performed: linkage and re-generation. 0.80 to 1.60. The same process can also be executed on consumer GPUs such as the RTX 4090 or RTX 5090 with comparable wall-clock time; in fact, it is theoretically faster on the RTX 5090 due to the newer architecture of the RTX 5090. This extreme low costs are deliberate: they serve as lower-bound analysis demonstrating that current protection mechanisms succumb to lightweight, generic distillation without requiring complex adversarial optimization. Consequently, the attack can be carried out with minimal computational resources, undermining any assumptions that identity-level extraction is impractical or lacks realistic threat. 5.1. Linkage Attack Since all templates are converted to the standard ArcFace domain, we can not only link between original images and protected templates, but also link between two templates from the same or different protection methods. In any case, we are linking two different images (or templates) of the same person, not face image and its corresponding template. We used the CASIA-Webface hold-out set for the linkage attack to ensure no identity overlap between training and testing identities. The hold-out dataset size is 2115. The closed-set 1-to-N linkage results are in Table 4. The original-image-to-original-image linkage (0.88) establishes performance upper bound. With the WebFace dataset containing 9.3%-13.0% noise (Wang et al., 2018), perfect linkage is impossible regardless of method. Our attack achieves linkage success rates consistently above 70%, frequently exceeding 80%, essentially reaching the datasets theoretical maximum performance. This confirms that the extracted embeddings function as effective identity descriptors for cross-domain matching. Additionally, the 1-to-1 verification accuracy used in the traditional face recognition benchmark  (Table 3)  remains near 100% and comparable to the original ArcFace performance, demonstrating that the protection systems fail to meaningfully impede identity matching. Table 2. Comparison of protection claims. FracFace measures the proportion of distorted frequency channels; our metric measures identity recovery success via commercial verification. We demonstrate that high channel disruption, as reported in prior work, does not prevent identity extraction. Success@5 Pass@1e-5 Pass@1e-4 Pass@1e-3 Dataset: TPDNE PartialFace MinusFace FracFace 1.000 0.996 0.992 0.993 0.936 0.904 Dataset: CASIA-WebFace Hold-Out PartialFace MinusFace FracFace Dataset: LFW PartialFace MinusFace FracFace 0.992 0.989 0.991 0.988 0.987 0.979 0.957 0.930 0.920 0.980 0.974 0.943 0.996 0.970 0.957 0.970 0.958 0. 0.983 0.981 0.961 0.998 0.989 0.985 0.982 0.978 0.977 0.986 0.983 0.970 Table 3. 1-to-1 verification accuracy between template-to-face and face-to-face on LFW. Accuracy AUROC MinusFace-to-Face FracFace-to-Face PartialFace-to-Face Face-to-Face 0.992 0.988 0.992 0.998 0.995 0.993 0.996 0.998 Table 4. Linkage Results between MinusFace, PartialFace, FracFace, and Original Image Embeddings on CASIA-Webface dataset. The numbers reported are top-1 recall at closed set settings. Query Key FracFace Minusface Partialface Original FracFace Minusface Partialface Original 0.7863 0.7305 0.8028 0.8444 0.7537 0.7206 0.7868 0.8241 0.8137 0.7754 0.8270 0. 0.8478 0.8132 0.8572 0.8823 Table 5. Cross-validation results (pass rate) using the Amazon API PartialFace MinusFace FracFace 0.99 0. 0.92 5 FaceLinkGen 5.2. Regeneration Attack For the regeneration attack, we evaluate identity recovery on the first 1,000 images from each of the following datasets: the TPDNE dataset, the hold-out set of CASIA-WebFace, and the LFW dataset. Each image is converted into protected template and mapped to facial embedding using the student model. For each embedding, five images are generated using Arc2Face to account for stochasticity. We report both per-image success rate and Success@5. Face generation is highly efficient due to the small backbone of SD1.5: generating batch of five images for single embedding takes approximately three seconds on an NVIDIA A6000 GPU, corresponding to throughput of roughly 1,200 identities per hour and an estimated cost of $0.0005 per identity generation. Visual examples are shown in Figure 1. Following the evaluation protocol of CanFG, we employ commercial face verification system, Face++, to assess identity consistency between the original dataset image and each generated face. We used Face++, marketed to have financial-grade security standards, which is usually higher standard (e.g., more challenging for us) than many open-source methods. This also avoids using the same model (e.g., ArcFace) or models trained on the same datasets (most open source ones) for both embedding extraction and verification. Face++ outputs confidence score together with three operating thresholds corresponding to error rate of 1 103, 1 104, and 1 105. For each generated image, we record the strictest threshold at which the identity match is accepted and use this as the evaluation outcome. If no face is detected in the original image, it is excluded from the data, while if no face is detected in the generated image, it counts as failure. The results are summarized in Table 2. On all three datasets, the success rate at the first attempt is all higher than 97%, and the success rate for five attempts ranges from 97.9% to 100%. Even at the strictest threshold, the success rate is still above 90%. We directly compare our regeneration attack with the original reconstruction attack protocol used by FracFaces authors. In Table 6, we reported the protection rates claimed in FracFace under its own evaluation and the corresponding rates under our attack on the TPDNE dataset. The evaluation of attack success in FracFace (Dai et al., 2025) is based on Protection (%) metric, defined as the proportion of frequency-domain channels that are filtered or structurally disrupted. This formulation establishes lower barrier for defensive claims than our identity-centric standard, which requires successful regeneration of images passing commercial-grade verification. By our metric, the protection rate of most recent PPFR methods in U-Net/StyleGAN attack is almost always 100%. Despite our stricter criterion being unfavorable to reported attack success, we show that high channel protection does not prevent identity leakage: even when FracFace claims high protection under its frequency-domain metric, FaceLinkGen achieves near-total identity recovery. This also shows that channel distubution does not mean identity protection. To cross-verify this result, we used another commercial facial comparison API from Amazon through EdenAi on 700 selected images on the LFW dataset. The Amazon API only provides single pass/fail decision with confidence score; the results are shown in Table 5. The values are close to the Face++ results, validating our claims. To rule out the dependence on models like Arc2Face or third-party verification services like Face++ or Amazon, we compared the similarity of the extracted embeddings with the original face. As detailed in Section 7, the embedding extracted from protected template shows higher cosine similarity to its source image than to another image of the same person. 6. What If the Attacker Knows Almost Nothing? Our main results demonstrate that identity information can be reliably extracted when the attacker has access to the conversion process, following the threat models in prior PPFR evaluations (Mi et al., 2024; 2023; Dai et al., 2025). However, we further pose more provocative question: To what extent does this vulnerability remain when the attacker, whether an external intruder or malicious insider with restricted system access, has neither access to the conversion process nor any knowledge about it? To investigate this, we consider an extreme, minimalassumption scenario. In this setting, the attacker possesses only 30 paired imagetemplate samples for validation (not for training) and has zero knowledge of the underlying protection mechanisms. This is actually stricter than the black-box scenario in Mi et al. (2023), which assumes the attacker knows the conversion process but not the channel parameters, and more realistic than Zhang et al. (2024), which relies on thousands of server queries per identity. In real attacks, these 30 pairs can be obtained by small number of leaked samples, known identities, or attackerIt can also be simulated with lowcontrolled accounts. frequency queries to the authentication server. We observe that despite their claimed algorithmic complexity, the output templates of these systems share common visual essence: they all preserve high-frequency information while obfuscating low-frequency information. Based on this intuition, the attacker can bypass any system-specific modeling and instead use generic Gaussian-blur-based high-pass filter as universal proxy task. We avoided using DCT or DWT to decouple from the methods used in the tested systems. By subtracting slightly blurred version from 6 FaceLinkGen Table 6. Comparison with FracFace defensive claims. While FracFace (Dai et al., 2025) measures protection by frequency channel disruption, we evaluate actual identity leakage. High disruption rates fail to prevent extraction, as FaceLinkGen achieves near-total recovery through commercial-grade verification. Venue Protection Tested in FracFace (Dai et al., 2025) Protection Tested Using Our Method Protection Tested Using Our Method (5 trials) PartialFace (Mi et al., 2023) MinusFace (Mi et al., 2024) FracFace (Dai et al., 2025) ICCV 2023 CVPR 2024 NeurIPS 2025 0.680 0.850 1.000 0.002 0.011 0.015 0.000 0.004 0.008 Table 7. Regeneration Success@5 on Face++ and Amazon API and 1-to-1 Linkage Success Rate In Assumption-Constrained Settings. Method Face++ Amazon API Matching FracFace PartialFace MinusFace 0.946 0.946 0.963 0.473 0.447 0.570 0.949 0.925 0.962 the original image and applying simple data augmentations (e.g., varying kernels and strengths), the attacker trains student model to align this simple high-pass domain with the identity embedding space. The attacker does not need to know any details about the system; instead, the high-pass characteristic is easily observable visually. The training process is the same as our main text, with simply the known PPFR conversion process replaced with high-pass filter. During inference, the templates are directly fed into the student model except for MinusFace, for which it is passed through Gaussian-blur-based high-pass filter to remove low-frequency noise. We trained only one model to attack all three methods. As shown in Table 7, identity leakage remains strikingly persistent. For all three systems, we achieved over 92% 1-to1 matching success rate and over 94% re-generation success@5 on Face++ and about 44-57% on the Amazon API. The Amazon API is likely more strict or sensitive to AIgenerated images. Notably, the 1-to-1 linkage success rate on LFW remains around 92-96%  (Table 7)  , close to our main experiments. Selected re-generated results are shown in Figure ??. Even though they are slightly worse than Figure 1, they are still very close to the original face. These results indicate that identity-consistent regeneration and reliable linkage remain feasible even under extremely constrained attacker assumptions. This suggests that the evaluated PPFR methods, despite their claimed algorithmic their output complexity, share common vulnerability: representations exhibit strong coupling with simple highpass filtering operations. Whether this generalizes to future methods that do not rely on frequency-domain obfuscation remains an open question. Overall, these findings reveal deeply concerning reality: even under an extreme near-zero-knowledge regime, identity information remains robustly and systematically extractable. This demonstrates that identity leakage is not byproduct of specific model designs, training strategies, or attacker assumptions, but rather structural property of existing PPFR representations themselves. Consequently, restricting access to the conversion process offers little meaningful protection. As long as the released templates preserve recognition utility, they inevitably encode recoverable identity cues, rendering current conversion-based defenses fundamentally insufficient. 7. Similarity Distribution To directly quantify identity leakage from protected templates, independent of the downstream face generation process (Arc2Face) or the specific behavior of commercial verification APIs (Face++, Amazon), we analyze the cosine similarity in the standard ArcFace embedding space: the similarity between two normal images of the same person, and the similarity between one image and its protected template. The histogram is shown in Figure 6 in the Appendix. Due to the dataset noise, some real photos and their similarity is near 0 (See (Wang et al., 2018)); this also appears in the ArcFace paper (Deng et al., 2022), but we focus on the main cluster here. We tested this on our testing set. We observed that the similarity between the original image and its template is higher than the similarity between another image of the same person in all three methods. This means that the template is better identity descriptor for this specific image than another image of the same person. Note that this does not imply that protected templates are universally closer to the underlying identity than real images. Instead, the template remains most similar to its corresponding source image. This indicates that the template is an image-conditioned projection that preserves identity while retaining instance-specific bias, rather than global identity prototype. 8. Soft Identity Leakage: Beyond Unique"
        },
        {
            "title": "Identifiers",
            "content": "Beyond hard identity recognition, the exposure of soft biometric attributes presents significant privacy risk. Characteristics such as skin color, age, and gender are sensitive 7 FaceLinkGen personal data that enable unauthorized profiling and algorithmic discrimination. Privacy frameworks like the Canadian Privacy Act (Branch Legislative Services, 2025) explicitly protect race and age. robust privacy-preserving system must therefore prevent the recovery of these attributes from its templates. In the public rebuttal of FracFace (Dai et al., 2025) on OpenReview (noa), the authors claimed successful obfuscation of age, gender, and ethnicity. They cited human perception study where participants reported less than 13% usage of these biometrics for identity inference, with over 76% of participants relying on guesswork. However, our empirical results in Figure 1 demonstrate that these soft biometrics remain visible in the re-generated faces. This matches with previous research, which indicates that ArcFace embeddings retain such information (Melzi et al., 2023; Osorio-Roig et al., 2023). Because our extracted embeddings closely resemble the original facial embeddings, we hypothesize that model can learn direct mapping from the embedding to these attributes without facial reconstruction. To test this, we trained MLP models on the FairFace dataset (Karkkainen & Joo, 2019) to predict age, gender, and race (7 classes) across 500 test images. Table 8 shows that gender is identified with at least 82% accuracy, and the Age MAE ranges from 6.1 to 7.5 years. The race accuracy is lower, ranging from 0.50 to 0.60, but considering the number of classes in race, it is still remarkable leakage, as there are about 50% of the time the race could be recovered. Given that FairFace labels use 10-year intervals, an MAE below 10.0 suggests leakage that matches the inherent precision of the reference model. Our attack reaches comparable metrics to the FairFace model on some datasets like LFWA+ (Liu et al., 2015). Table 8. Soft Biometrics Leakage Method Race Acc Gender Acc Age MAE FracFace MinusFace PartialFace 0.50 0.56 0.60 0.82 0.86 0. 7.5 6.4 6.1 9. Future Directions We suggest several potential pathways for future PPFR designs and evaluations, primarily focusing on stronger defensive mechanisms and broader vulnerability assessments. One rigorous approach is to incorporate secret keys into the conversion process, similar to Yuan et al. (2022). This serves as multi-factor authentication system (requiring both biometrics and key), preventing attackersand our methodfrom converting face without the secret. Alternatively, systems may revert to formal cryptographic methods like (Ao & Boddeti, 2025). While traditionally viewed as computationally expensive, modern resources make this trade-off acceptable; for instance, Jindal et al. (2020) reports only 2.83ms processing time per face pair. Importantly, these computational costs can act as an effective client-side constraint against brute-force attacks (conceptually similar to slow hashing), enhancing privacy while remaining imperceptible to regular users. Again, we want to emphasize that de-ID or reversible face encryption methods cannot be used for PPFR tasks, as they either make the face completely useless for recognition or the reversbility compomised the privacy-preserving nature. 10. Conclusion This paper demonstrates that current frequency-based obfuscation methods fail to meet the fundamental security requirements of PPFR systems. We prove that identitydiscriminative information remains accessible at the representation level, allowing for high-accuracy linkage and facial regeneration. Crucially, our findings reveal that such systems provide negligible protection against malicious service providers who, despite having legitimate authorization for verification, can systematically revert protected templates to identifiable facial data. The success of FaceLinkGeneven under minimal knowledge assumptionsexposes structural collapse of the visual distortion paradigm for external attackers. Future PPFR research must move beyond human-centric visual metrics toward mathematically asymmetric protection mechanisms that effectively prevent unauthorized semantic extraction by all potential adversaries, including the service providers themselves. Some methods, such as CanFG (Wang et al., 2024a) and FaceAnonyMixer (Alam et al., 2025), intentionally preserve soft biometrics for auxiliary tasks. We contend that this design is problematic. PPFR systems should rely only on identity-discriminative features. Retaining soft biometrics offers attackers more data for reconstruction and profiling without improving recognition performance. These attributes are central to privacy frameworks and require stricter protection (Osorio-Roig et al., 2022). 8 FaceLinkGen Our work also demonstrates identity-consistent face regeneration, which may raise concerns regarding the ethical risks of controllable deepfake generation. We emphasize that controllable deepfake synthesis is not novel contribution of this paper. Our pipeline builds upon Arc2Face-generated images and existing image generation models, and the ethical implications of such techniques have been extensively discussed in prior literature. Our contribution lies in showing that such generation can be triggered from protected templates, rather than in advancing deepfake generation capabilities themselves. 11. Impact Statements This work studies privacy-preserving face recognition (PPFR) systems, domain that inherently involves sensitive biometric technologies. All the datasets used in the paper are common academic datasets or synthetic datasets. By identifying weaknesses in prevailing PPFR designs and evaluation practices, our goal is to highlight limitations of current security assumptions and to motivate the development of stronger protection mechanisms and evaluation paradigms that better safeguard user identity privacy. The systems analyzed in this paper are primarily academic methods proposed in the literature, rather than deployed industrial systems. As such, our findings are intended to inform research directions and evaluation standards, rather than to characterize the security posture of specific real-world deployments. We informed the authors of the three targeted works prior to submission. To mitigate the risk of weaponization, we withhold full attack code, trained models, and specific implementation details from public repositories. This decision follows established precedents in both high-stakes AI releases (Simeoni et al., 2025; Grattafiori et al., 2024) and offensive security research (Saharia et al., 2022; Bagwe et al., 2025; Zhao et al., 2025; Kumar et al., 2025; Zhang et al., 2025; Cretu et al., 2022), where the potential for misuse outweighs unconditional transparency. We decline to disclose specific technical details due to significant safety and security concerns. While scientific reproducibility is core principle, the right to privacy and life must take precedence over the reproducibility of high-risk protocols. Sharing detailed instructions for attack poses severe threat to global privacy and safety if misused. We believe that safeguarding human privacy outweighs the requirement for full technical transparency in this instance. This decision ensures that scientific progress does not create unnecessary vulnerabilities for society. The information already disclosured provide enough information for scientific validation without revealing sensitive data that could facilitate unauthorized replication. To balance verification with harm control, we provide highlevel algorithmic descriptions sufficient for conceptual understanding while restricting access to the operational tooling. We offer two channels for rigorous scientific validation: (1) direct sharing of models and code with qualified researchers upon verified request, and (2) planned security testing server. This server will allow researchers to submit templates for identity leakage assessment under controlled protocol without exposing the underlying attack payloads. This measured disclosure strategy ensures our findings can be peer-validated and used to harden future systems without providing turnkey weapon to malicious actors. 9 FaceLinkGen"
        },
        {
            "title": "References",
            "content": "Wayback Machine. URL https://web.archive. org/web/20260000000000*/https: //openreview.net/forum?id=JSSvYZKvL8. Alam, M. T., Shamshad, F., Karray, F., and Nandakumar, K. FaceAnonyMixer: Cancelable Faces via Identity Consistent Latent Space Mixing, August 2025. URL http:// arxiv.org/abs/2508.05636. arXiv:2508.05636 [cs]. Ao, W. and Boddeti, V. N. CryptoFace: End-to-End In Proceedings of the Encrypted Face Recognition. Computer Vision and Pattern Recognition Conference, pp. 1919719206, 2025. URL https://openaccess. thecvf.com/content/CVPR2025/html/Ao_ CryptoFace_End-to-End_Encrypted_Face_ Recognition_CVPR_2025_paper.html. Bae, G., Gorce, M. d. L., Baltrusaitis, T., Hewitt, C., Chen, D., Valentin, J., Cipolla, R., and Shen, J. DigiFace-1M: 1 Million Digital Face Images for Face Recognition, October 2022. URL http://arxiv.org/abs/2210. 02579. arXiv:2210.02579 [cs]. Bagwe, G., Chaturvedi, S. S., Ma, X., Yuan, X., Wang, K.-C., and Zhang, L. E. Your RAG is Unfair: Exposing Fairness Vulnerabilities in Retrieval-Augmented Generation via Backdoor Attacks. In Christodoulopoulos, C., Chakraborty, T., Rose, C., and Peng, V. (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 1593015948, Suzhou, China, November 2025. Association for Computational Linguistics. ISBN 979doi: 10.18653/v1/2025.emnlp-main. 8-89176-332-6. 804. URL https://aclanthology.org/2025. emnlp-main.804/. Branch Legislative Services. laws of Canada, Privacy Act, June 2025. https://laws-lois.justice.gc.ca/eng/ ACTS/P-21/page-1.html#h-397182. Consolidated federal URL Cai, F., Guo, Y., Li, J., Li, W., Fang, X., and Chen, J. FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training, June 2025. URL http://arxiv. org/abs/2506.10035. arXiv:2506.10035 [cs] version: 1. Cretu, A.-M., Monti, F., Marrone, S., Dong, X., Bronstein, M., and de Montjoye, Y.-A. Interaction data are identifiable even across long periods of time. Nature Communications, 13:313, January 2022. ISSN 2041-1723. doi: 10.1038/s41467-021-27714-6. URL https://pmc. ncbi.nlm.nih.gov/articles/PMC8789822/. Dai, W., Li, B., Dong, N., Bai, G., and Dong, J. S. FracFace: Breaking The Visual CluesFractalBased Privacy-Preserving Face Recognition. October 2025. URL https://openreview.net/forum? id=JSSvYZKvL8. Deng, J., Guo, J., Yang, J., Xue, N., Kotsia, I., and Zafeiriou, S. ArcFace: Additive Angular Margin Loss for Deep Face IEEE Transactions on Pattern Analysis Recognition. and Machine Intelligence, 44(10):59625979, October 2022. ISSN 0162-8828, 2160-9292, 1939-3539. doi: 10.1109/TPAMI.2021.3087709. URL http://arxiv. org/abs/1801.07698. arXiv:1801.07698 [cs]. Erkin, Z., Franz, M., Guajardo, J., Katzenbeisser, S., Lagendijk, I., and Toft, T. Privacy-Preserving Face Recognition. In Goldberg, I. and Atallah, M. J. (eds.), Privacy Enhancing Technologies, pp. 235253, Berlin, Heidelberg, 2009. Springer. ISBN 978-3-642-03168-7. doi: 10.1007/978-3-642-03168-7 14. Fredrikson, M., Jha, S., and Ristenpart, T. Model Inversion Attacks that Exploit Confidence Information and In Proceedings of the 22nd Basic Countermeasures. ACM SIGSAC Conference on Computer and Communications Security, pp. 13221333, Denver Colorado USA, October 2015. ACM. ISBN 978-1-4503-3832-5. doi: 10.1145/2810103.2813677. URL https://dl.acm. org/doi/10.1145/2810103.2813677. Grattafiori et al. The Llama 3 Herd of Models, November 2024. URL http://arxiv.org/abs/2407. 21783. arXiv:2407.21783 [cs]. Guo, W. M., Qian, Q., Hasan, K., and Du, S. Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological Censorship, December 2025. URL http:// arxiv.org/abs/2512.11883. arXiv:2512.11883 [cs]. Guo, Z., Wu, Y., Chen, Z., Chen, L., Zhang, P., and He, Q. PuLID: Pure and Lightning ID Customization via Contrastive Alignment, October 2024. URL http:// arxiv.org/abs/2404.16022. arXiv:2404.16022 [cs]. Ji, J., Wang, H., Huang, Y., Wu, J., Xu, X., Ding, S., Zhang, S., Cao, L., and Ji, R. Privacy-Preserving Face Recognition with Learnable Privacy Budgets in Frequency Domain, July 2022. URL http://arxiv.org/abs/ 2207.07316. arXiv:2207.07316 [cs]. Jin, S., Wang, H., Wang, Z., Xiao, F., Hu, J., He, Y., Zhang, W., Ba, Z., Fang, W., Yuan, S., and Ren, K. FaceObfuscator: Defending Deep Learning-based Privacy Attacks with Gradient Descent-resistant Features in FaceLinkGen Face Recognition. In 33rd USENIX Security Symposium (USENIX Security 24), pp. 68496866, Philadelphia, PA, August 2024. USENIX Association. ISBN 978URL https://www.usenix. 1-939133-44-1. org/conference/usenixsecurity24/ presentation/jin-shuaifan. Jindal, A. K., Shaik, I., Vasudha, V., Chalamala, S. R., Ma, R., and Lodha, S. Secure and Privacy Preserving Method for Biometric Template Protection using Fully Homomorphic Encryption. In 2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom), pp. 11271134, December 2020. doi: 10.1109/TrustCom50675.2020. 00149. URL https://ieeexplore.ieee.org/ document/9343021. ISSN: 2324-9013. Karras, T., Laine, S., and Aila, T. Style-Based Generator Architecture for Generative Adversarial Networks, March 2019. URL http://arxiv.org/ abs/1812.04948. arXiv:1812.04948 [cs]. Kumar, D., Birur, N. A., Baswa, T., Agarwal, S., and Harshangi, P. Quantifying CBRN Risk in Frontier Models, October 2025. URL http://arxiv.org/abs/ 2510.21133. arXiv:2510.21133 [cs]. Karkkainen, K. and Joo, J. FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age, August 2019. URL http://arxiv.org/abs/1908. 04913. arXiv:1908.04913 [cs]. Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015. Ma, Y., Shui, Y., Wu, X., Sun, K., and Li, H. HPSv3: Towards Wide-Spectrum Human Preference Score, August 2025. URL http://arxiv.org/abs/2508. 03789. arXiv:2508.03789 [cs]. Melzi, P., Shahreza, H. O., Rathgeb, C., Tolosana, R., Vera-Rodriguez, R., Fierrez, J., Marcel, S., and Busch, C. Multi-ive: Privacy enhancement of multiple soft-biometrics in face embeddings. In the IEEE/CVF Winter Conference Proceedings of on Applications of Computer Vision, pp. 323331, URL https://openaccess.thecvf. 2023. com/content/WACV2023W/DVPBA/html/ Melzi_Multi-IVE_Privacy_Enhancement_ of_Multiple_Soft-Biometrics_in_Face_ Embeddings_WACVW_2023_paper.html. Mi, Y., Huang, Y., Ji, J., Liu, H., Xu, X., Ding, S., and Zhou, S. DuetFace: Collaborative Privacy-Preserving Face Recognition via Channel Splitting in the Frequency Domain. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 67556764, October 2022. doi: 10.1145/3503161.3548303. URL http://arxiv. org/abs/2207.07340. arXiv:2207.07340 [cs]. Mi, Y., Huang, Y., Ji, J., Zhao, M., Wu, J., Xu, X., Ding, S., and Zhou, S. Privacy-preserving face recognition using random frequency components. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1967319684, 2023. Mi, Y., Zhong, Z., Huang, Y., Ji, J., Xu, J., Wang, J., Wang, S., Ding, S., and Zhou, S. Privacy-preserving face recognition using trainable feature subtraction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 297307, 2024. Mou, C., Wu, Y., Wu, W., Guo, Z., Zhang, P., Cheng, Y., Luo, Y., Ding, F., Zhang, S., Li, X., Li, M., Liu, M., Zhang, Y., Wu, S., Zhao, S., Zhang, J., He, Q., and Wu, X. DreamO: Unified Framework for Image Customization, November 2025. URL http://arxiv.org/abs/ 2504.16915. arXiv:2504.16915 [cs]. National Cryptologic Centre. Characterizing Attacks to Fingerprint Verification Mechanisms. 2011. Osorio-Roig, D., Rathgeb, C., Drozdowski, P., Terhorst, P., ˇStruc, V., and Busch, C. An Attack on Facial SoftBiometric Privacy Enhancement. IEEE Transactions on Biometrics, Behavior, and Identity Science, 4(2):263275, April 2022. ISSN 2637-6407. doi: 10.1109/TBIOM. 2022.3172724. URL https://ieeexplore.ieee. org/document/9770950. Osorio-Roig, D., Gerlitz, P. A., Rathgeb, C., and Busch, C. Reversing Deep Face Embeddings with Probable Privacy Protection, October 2023. URL http://arxiv.org/ abs/2310.03005. arXiv:2310.03005 [cs]. Papantoniou, F. P., Lattas, A., Moschoglou, S., Deng, J., Kainz, B., and Zafeiriou, S. Arc2Face: Foundation Model for ID-Consistent Human Faces, August 2024. URL http://arxiv.org/abs/2403. 11641. arXiv:2403.11641 [cs]. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Gontijo-Lopes, R., Ayan, B. K., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. October 2022. URL https: //openreview.net/forum?id=08Yk-n5l2Al. Schroff, F., Kalenichenko, D., and Philbin, J. FaceNet: Unified Embedding for Face Recognition and Clustering. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 815823, June 2015. doi: 10.1109/CVPR.2015.7298682. URL http://arxiv. org/abs/1503.03832. arXiv:1503.03832 [cs]. 11 FaceLinkGen Ye, H., Zhang, J., Liu, S., Han, X., and Yang, W. IP-Adapter: Text Compatible Image Prompt Adapter for Text-toImage Diffusion Models, August 2023. URL http:// arxiv.org/abs/2308.06721. arXiv:2308.06721 [cs]. Yi, D., Lei, Z., Liao, S., and Li, S. Z. Learning Face Representation from Scratch, November 2014. URL http: //arxiv.org/abs/1411.7923. arXiv:1411.7923 [cs]. Yuan, Z., You, Z., Li, S., Qian, Z., Zhang, X., and Kot, In ProA. On Generating Identifiable Virtual Faces. ceedings of the 30th ACM International Conference on Multimedia, pp. 14651473, Lisboa Portugal, October 2022. ACM. doi: 10.1145/3503161.3548110. URL https://dl.acm. org/doi/10.1145/3503161.3548110. ISBN 978-1-4503-9203-7. Yue, K., Jin, R., Wong, C.-W., Baron, D., and Gradient Obfuscation Gives False Dai, H. Sense of Security in Federated Learning. In 32nd USENIX Security Symposium (USENIX Security 23), pp. 63816398, Anaheim, CA, August 2023. USENIX Association. ISBN 978-1-939133-37-3. URL https://www.usenix.org/conference/ usenixsecurity23/presentation/yue. Zhang, H., Dong, X., Lai, Y., Zhou, Y., Zhang, X., Lv, X., Jin, Z., and Li, X. Validating Privacy-Preserving Face Recognition Under Minimum Assumption. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1220512214, June 2024. doi: 10.1109/CVPR52733.2024.01160. URL https:// ieeexplore.ieee.org/document/10655270. Zhang, Y., Wang, L., Zhao, J., Zhao, W., Zhou, F., Dang, Y., and Yin, J. 3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving, July 2025. URL http://arxiv.org/abs/ 2507.09993. arXiv:2507.09993 [cs] version: 1. Zhao, J., Fu, T., Schaeffer, R., Sharma, M., and Barez, F. Chain-of-Thought Hijacking, November 2025. URL http://arxiv.org/abs/2510. 26418. arXiv:2510.26418 [cs]. Simeoni, O., Vo, H. V., Seitzer, M., Baldassarre, F., Oquab, M., Jose, C., Khalidov, V., Szafraniec, M., Yi, S., Ramamonjisoa, M., Massa, F., Haziza, D., Wehrstedt, L., Wang, J., Darcet, T., Moutakanni, T., Sentana, L., Roberts, C., Vedaldi, A., Tolan, J., Brandt, J., Couprie, C., Mairal, J., Jegou, H., Labatut, P., and Bojanowski, P. DINOv3, August 2025. URL http://arxiv.org/abs/2508. 10104. arXiv:2508.10104 [cs]. Sun, X., Gazagnadou, N., Sharma, V., Lyu, L., Li, H., and Zheng, L. Privacy Assessment on Reconstructed Images: Are Existing Evaluation Metrics Faithful to Human Perception?, October 2023. URL http://arxiv.org/ abs/2309.13038. arXiv:2309.13038 [cs]. The BioAPI Consortium. BioAPI Specification Version 1.1. March 2001. Wang, F., Chen, L., Li, C., Huang, S., Chen, Y., Qian, C., and Loy, C. C. The Devil of Face Recognition is in the Noise, July 2018. URL http://arxiv.org/abs/ 1807.11649. arXiv:1807.11649 [cs]. Wang, H., Wang, S., Lu, C.-S., and Echizen, I. DiffusionDriven Universal Model Inversion Attack for Face Recognition, April 2025. URL http://arxiv.org/abs/ 2504.18015. arXiv:2504.18015 [cs] version: 1. Wang, K., Zhao, B., Peng, X., Zhu, Z., Deng, J., Wang, X., Bilen, H., and You, Y. FaceMAE: PrivacyPreserving Face Recognition via Masked Autoencoders, May 2022. URL http://arxiv.org/abs/2205. 11090. arXiv:2205.11090 [cs]. Wang, T., Zhang, Y., Xiao, X., Yuan, L., Xia, Z., and Weng, J. Make Privacy Renewable! Generating Privacy-Preserving Faces Supporting Cancelable Biometric Recognition. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 10268 10276, Melbourne VIC Australia, October 2024a. ACM. ISBN 979-8-4007-0686-8. doi: 10.1145/3664647. 3680704. URL https://dl.acm.org/doi/10. 1145/3664647.3680704. Wang, Y., Huang, Y., Li, J., Yang, L., Song, K., and Wang, L. Adaptive Hybrid Masking Strategy for PrivacyPreserving Face Recognition Against Model Inversion Attack, April 2024b. URL http://arxiv.org/abs/ 2403.10558. arXiv:2403.10558 [cs]. Wu, X., Hao, Y., Sun, K., Chen, Y., Zhu, F., Zhao, R., and Li, H. Human Preference Score v2: Solid Benchmark for Evaluating Human Preferences of Text-to-Image Synthesis, September 2023. URL http://arxiv.org/ abs/2306.09341. arXiv:2306.09341 [cs]. 12 FaceLinkGen Table 9. Controllable Deepfake Results Pass@1e-5 Pass@1e-3 HPSv2.1 Face++ Conf 0.892 0.958 0. 82.33 Figure 6. Similarity Distribution mainstream standard of beauty, which may systematically disadvantage faces that deviate from this norm. This process is more expensive than Arc2Face generation due to the larger backbone (Flux). It took about 50 seconds to generate one image on an A6000 GPU, which is still very cheap for high-quality deepfake generation. The results are shown in Table 9. Even after 2 conversions, the pass rate is still near 90%. The HPSv2 score (0.28) is close to the original Flux baseline, and the average score of AI-generated images tested in Cai et al. (2025); Guo et al. (2025). It means the generated images have both high quality and prompt-following. Figure 5. Controlled generation examples. The top left image is the image used to register, the top right image is produced by Arc2Face, and the bottom image is generated by DreamO based on the top right image. All images are prompted with person playing guitar in the street and all are verified by Face++ with FAR of 1e-5. Even with blurry register images, our method still works. A. Controllable Deepfake This enables direct deepfake creation from the protected templates. Arc2Face can only generate face image from facial embeddings without textual guidance. FaceID IPAdapter (Ye et al., 2023) allows the user to generate face with both text and face prompts; however, its quality and face consistency are worse. Recently, models do not use facial embeddings, but raw image input directly (or both) has achieved high quality and ID consistency, such as DreamO (Mou et al., 2025) and PulID (Guo et al., 2024). However, aligning another student encoder for them might be more challenging. Thus, we use simple pipeline to feed the face(s) generated by Arc2Face as prompts for other downstream models. If the attacker is willing to trade some regeneration accuracy for controllability, they can use this pipeline to generate the persons face in any setting by using textual prompt. In Figure 5, we have shown examples of the results of this pipeline. We can see that even after 2 transformations, the generated face in controlled setting is still highly similar to the original image. We selected FracFace as the large-scale attack target because it is the most recent model and yields our lowest attack success rate, allowing it to serve as conservative lower bound. We generated 100 textual prompts using ChatGPT-5.2 and used each prompt to condition DreamO together with randomly selected identity from LFW. In total, we sampled 500 identities and produced 500 images. We evaluated performance using the imagetext matching BLIP score, facial consistency rate at FAR = 1e-3, and image quality measured by HPSv2 (Wu et al., 2023). We did not use the latter version HPSv3 (Ma et al., 2025) because Guo et al. (2025) found that HPSv3 is heavily biased toward narrow"
        }
    ],
    "affiliations": [
        "Department of CMPS, University of British Columbia, Kelowna, Canada",
        "Weathon Software, Canada"
    ]
}