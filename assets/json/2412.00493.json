{
    "paper_title": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding",
    "authors": [
        "Duo Zheng",
        "Shijia Huang",
        "Liwei Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet a considerable gap remains between the models' learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. Additionally, we have implemented a maximum coverage sampling technique to optimize the balance between computational costs and performance efficiency. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D."
        },
        {
            "title": "Start",
            "content": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding Duo Zheng* Shijia Huang* The Chinese University of Hong Kong {dzheng23, sjhuang, lwwang}@cse.cuhk.edu.hk Liwei Wang 4 2 0 2 0 ] . [ 1 3 9 4 0 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of Multimodal Large Language Models (MLLMs) has significantly impacted various multimodal tasks. However, these models face challenges in tasks that require spatial understanding within 3D environments. Efforts to enhance MLLMs, such as incorporating point cloud features, have been made, yet considerable gap remains between the models learned representations and the inherent complexity of 3D scenes. This discrepancy largely stems from the training of MLLMs on predominantly 2D data, which restricts their effectiveness in comprehending 3D spaces. To address this issue, in this paper, we propose novel generalist model, i.e., Video-3D LLM, for 3D scene understanding. By treating 3D scenes as dynamic videos and incorporating 3D position encoding into these representations, our Video-3D LLM aligns video representations with real-world spatial contexts more accurately. Additionally, we have implemented maximum coverage sampling technique to optimize the balance between computational costs and performance efficiency. Extensive experiments demonstrate that our model achieves state-of-the-art performance on several 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer, Scan2Cap, ScanQA, and SQA3D. Our code is available at https://github.com/LaVi-Lab/Video-3DLLM . 1. Introduction The rapid development of Multimodal Large Language Models (MLLMs) [2, 15, 18, 28, 29, 34, 35, 38, 43, 53] has demonstrated substantial capabilities in various multi-modal tasks, attracting significant attention from both academia and industry sectors. However, despite these advancements, recent studies [5, 32, 33, 37] indicate that current MLLMs face challenges when addressing tasks that ne- *Equal contribution. Corresponding author. Figure 1. Comparison of previous work and our method: (a) Previous 3D LLMs are initialized on MLLMs trained solely on imagetext pairs, and learn point cloud or voxel representations via finetuning on 3D scenes. The 3D point clouds are reconstructed from RGB-D videos. (b) Our method directly utilizes video frames and 3D coordinates as input, where the 3D coordinates are converted from depths through coordinate transformation. We then transfer the ability of video understanding to 3D scene understanding by injecting position information into video representations. cessitate spatial understanding and reasoning in 3D environments. Recent studies [11, 14, 19, 21, 22, 25, 45, 47, 57] have focused on adapting MLLMs for enhanced 3D scene understanding. As depicted in Figure 1 (a), these approaches develop comprehensive 3D scene-level representations using variety of techniques. These include harnessing features from point clouds [11, 14, 47], converting attributes from multi-view images into 3D formats [19, 21, 57], and exploiting characteristics from recognized objects [22, 23, 25, 45]. Although significant advancements have been achieved, noticeable gap exists between the representations learned by MLLMs and the complexity of 3D scenes. This gap stems from the fact that MLLMs are primarily trained on fundamentally different data types, namely 2D images. While it is possible to further finetune MLLMs with 3D data, such as point clouds or voxels, the limited availability 1 of labeled 3D scene data poses challenge. Consequently, the 2D visual knowledge embedded in MLLMs offers limited assistance in understanding 3D environments. In parallel, the abundance of video data has spurred interest in adapting Video LLMs to different domains, e.g., 3D question answering [28, 33, 35] and robotic manipulation [27, 46, 60]. These methods benefit from extensive internet video datasets and pre-trained video models, revealing the immense potential for extending video modality to 3D modeling. However, as early attempts, they are still far from creating model capable of handling diverse 3D tasks. Moreover, the absence of integrated spatial informationsuch as 3D locations and spatial relationshipsin video representations constrains their capability to fully comprehend the 3D physical world. For instance, tasks that require an intricate understanding of 3D spatial relationships cannot rely solely on RGB data. This limitation underscores the necessity for incorporating more comprehensive spatial modeling into Video LLMs to enhance their effectiveness in 3D applications. Therefore, in this paper, we propose generalist model for 3D scene understanding, namely Video-3D LLM. As shown in Figure 1, our model is based on Video LLM framework and processes 3D videos, i.e., video frames accompanied by the corresponding 3D spatial coordinates. Typically, as the current 3D scenes are mostly reconstructed from RGB-D videos, we can seamlessly project the depth images into the global coordinate system. To establish the correspondence in visual appearance and position information, we learn position-aware video representations by injecting 3D coordinates into video features. Subsequently, we encode the coordinate to 3D position encoding and add it to the video representations, serving as the input for the Video LLM. Our model offers several significant advantages. Firstly, it aligns video representations with their real-world spatial contexts, thereby equipping our model to handle various 3D tasks such as 3D visual grounding, 3D dense captioning, and 3D question answering. Secondly, it maintains both temporal and spatial contextual information in the video data, which helps to reduce the discrepancy between the pre-training data and actual 3D scenes. Additionally, we have developed maximum coverage sampling strategy for frame selection. This approach views frame selection as maximum coverage problem and adopts greedy algorithm for its resolution. This strategy ensures the selection of the most informative frames, thus improving the models capacity to discern diverse and essential spatio-temporal features within the video, while also ensuring efficient inference performance. Our approach is to train single model in the multi-task manner on varying 3D scene understanding tasks, including 3D question answering, 3D dense captioning, and 3D visual grounding. Extensive experiments demonstrate that our Video-3D LLM achieves state-of-the-art performance on five 3D scene understanding benchmarks, i.e., ScanRefer [6], Multi3DRefer [52], Scan2Cap [7], ScanQA [3] and SQA3D [36]. Notably, our method surpasses the previous state-of-the-art LLaVA-3D [57] by using only 26% of its 3D data (223k vs. 859k), achieving improvements of 4.1% Acc@0.25 on ScanRefer, 4.6 CIDEr@0.5IoU on Scan2Cap, 2.9% EM on ScanQA, and 3.0% EM on SQA3D. The impressive performance reveals the immense potential for adapting video models to 3D modality, establishing new paradigm in 3D scene understanding. 2. Related Work 3D Scene Understanding. The growing interest in 3D scene understanding has spurred the development of various tasks, such as: (1) 3D visual grounding [1, 6, 9, 24, 44, 49, 51, 52, 54, 56, 58], which involves pinpointing target object within 3D environment based on textual description. (2) 3D dense captioning [7, 10, 12], which requires the model to localize and provide detailed captions for objects within 3D scene. (3) 3D question answering [3, 36], which focuses on answering questions related to 3D scenes. These 3D vision-language tasks differ from 2D tasks [13, 20] as they necessitate enhanced modeling of spatial positions and relative relationships. To meet these challenges, various approaches have been introduced, addressing aspects such as view changes [24], spatial modeling [9, 54], and fine-grained alignment [10, 12, 51, 52, 58]. However, these methods are typically specialized for specific tasks and lack generalizability. In contrast, our work aims to build generalist model capable of handling diverse 3D-VL tasks, thereby offering more flexible and widely applicable solution in the field. LLMs for 3D Scene Understanding. Recently, there has been growing interest in integrating 3D information in LLMs [11, 14, 2123, 25, 45, 47, 57], which advances the progress of 3D scene understanding. 3D-LLM [21] introduces the LLM-based model for 3D physical world, which takes 3D features from rendered 2D images as input. PointLLM [47] utilizes point encoder with strong LLM for point cloud understanding. LL3DA [11] leverages Q-former to extract useful information from point cloud features, endowing humans with the capability to interact in 3D environments. Grounded 3D-LLM [14] further introduces projection module based on 3D detectors, which allows for generating object proposals from pointlevel features. Chat3D[45], LEO [23], and ChatScene [22] take use of off-the-shelf 3D detectors for proposal generation, and then incorporate the object-centric representations into LLMs. LLaVA-3D [57] introduces 3D-patch representations, which aggregate 2D-patch features in voxel space. Robin3D [25] tries to enhance 3D scene understanding via data generation. It is important to note that existing 3D Large Language Models (3D LLMs) typically transform 3D scenes into voxel-level or point cloud-level 3D representations as input for modeling purposes. However, these approaches create disconnect with pre-trained multi-modal large language models (MLLMs), which are primarily trained on extensive 2D datasets, such as images, and only fine-tuned on limited amount of 3D scene data. To address this challenge, our method incorporates 3D information (e.g., coordinates shown in Figure 1 (b)) into new video representation. This enhancement maximizes the use of pre-trained 2D Video LLMs, leveraging their full potential. Video-Language Models for 3D Understanding. We have witnessed the rapid development of Video LLMs [30, 31, 35, 50, 53]. There is also trend of leveraging Video LLMs for 3D tasks, including 3D question answering [28, 33, 35] and robotic manipulation [27, 46, 60]. LLaVA-OneVision [28] and Oryx MLLM [35] incorporate 3D question-answering datasets into instruction data, which deliver competent results on 3D question-answering tasks. However, these models do not capture detailed 3D spatial information, which limits their performance in addressing other 3D tasks that require precise spatial alignments. Furthermore, recent work [33] has emphasized the importance of identifying key object correspondences across frames through visual prompting. In contrast, our approach directly incorporates 3D positional information into video representation learning, which enhances the capability to tackle more complex tasks that demand thorough spatial understanding of the 3D environment. 3. Method We propose generalist model for 3D scene understanding, namely Video-3D LLM, built upon pre-trained Video LLM [53]. Our model includes visual encoder, 3D position encoding module, and Video LLM backbone. To effectively utilize video representation for 3D scene understanding, it is necessary to first convert 3D scenes into sequences of frames. Given that entire frame sequences can be redundant, implementing an effective frame selection strategy is crucial, as it significantly influences both performance and computational efficiency. Subsequently, our goal is to enhance the Video LLM with position awareness. This is achieved by encoding spatial coordinates into the 3D position encoding (3D-PE) and integrating them into the video representation learning. In this section, we detail three key components of our approach: the frame sampling strategy (3.1), position-aware video representation (3.2), and the training objective (3.3). 3 3.1. Frame Sampling Strategy Converting 3D scenes into video sequences presents two primary challenges: (1) Due to GPU memory constraints, the Video LLM can only process limited number of frames at time. This necessitates the sampling of subset of frames from the extensive raw video sequence to manage resources effectively. (2) It is crucial for the video sequence to encompass as much of the entire 3D scene as possible, since any omission of scene content could result in significant and irreversible decline in model performance. To address these challenges, we introduce maximum coverage strategy for frame sampling. This approach involves preprocessing the selected frames offline and applying the strategy consistently during both the training and inference phases to ensure comprehensive scene coverage and efficient memory usage. Frame Sampling as Maximum Coverage Problem. Given raw RGB-D video, each frame captures portion of the 3D scene. We aim to select fewest possible frames that maximize coverage of the 3D scene, which could be formulated as maximum coverage problem. Formally, let = {f1, f2, . . . , fn} represent the set of all frames, and = {v1, v2, . . . , vm} represent the set of all voxels in the 3D scene. Each frame fk covers subset of voxels Vk , which are discretized by the coordinates. The objective is to find subset of frames such that the union of covered voxels (cid:83) fkS Vk is maximized. Greedy Solution. Since the maximum coverage problem is NP-hard, we employ greedy algorithm to solve it, which can obtain an approximation ratio of 1 1/e [26]. As illustrated in the Algorithm 1, the approach iteratively selects the frame with the largest increase in uncovered voxel coverage. Frames are added until the desired number is reached or the coverage ratio exceeds predefined threshold. This stop condition ensures balance between computational efficiency and the coverage for varying scenes. Algorithm 1 Maximum Coverage Sampling Require: Set of frames = {f1, f2, . . . , fn}, voxel sets Vk for each frame fk, budget Ensure: Subset maximizing voxel coverage 1: Initialize 2: Initialize {Set of covered voxels} 3: while size of is less than do 4: 5: 6: 7: Select = arg maxfkF Vk Add to Update Vf if Stop condition is met then break end if 8: 9: 10: end while 11: return Figure 2. The overview of the model architecture. (a) shows the integration of video sequence and global coordinates for creating positionaware video representations. (b) and (c) detail the examples of 3D dense captioning and 3D visual grounding, respectively. Our approach can generalize well to other 3D tasks. 3.2. Position-Aware Video Representation After completing frame sampling, we obtain sequence of RGB frames, depth images, and the cameras intrinsic and extrinsic parameters. To create position-aware video representation, we first transform the depth information into 3D coordinates within global coordinate system. We then encode the visual embeddings along with the 3D position encodings (3D-PE) to enhance spatial awareness. Camera Coordinate Transformation. Given depth imcamera R44, and age RHW , an extrinsic matrix global camera intrinsic matrix R33, we can calculate the global coordinates pglobal of pixel position (i, j) using the following formula: (cid:21) (cid:20)pglobal 1 = global camera D(i, j) 1 1 1 as result, we obtain sequence of frames {fk}l k=1 along with set of coordinates in the global coordinate system {ck}l k=1, where each fk and ck are in RHW 3. Visual Embedding. We first encode each frame into visual embeddings via Vision Transformer (ViT) [17]. In concrete, given frame fi RHW 3, the image will first be split into series of patches at the patch size , which are then fed into the ViT to produce visual embed- (cid:5) (cid:5), = (cid:4) RH d, where = (cid:4) dings eimg and is the feature dimension. k 3D Position Encoding. Since the video frame is divided into image patches, we need to pool the coordinate information of each image patch. For each global coordinate map ck RHW 3, we divide the coordinates into patches 4 identical to those of the image patches. Subsequently, we compute the average coordinate for each patch, which could be denoted as: k(i, j) = 1 (cid:88) ck(u, v), (2) (u,v)patch at (i,j) RH d. Due to the small size of the where each patches, the averaged coordinates retain sufficiently precise positional information. We also explored alternative coordinate pooling methods in our ablation study. We adopt sinusoidal position encoding [41] to encode the coordinates. For 3D coordinate (x, y, z), we first map the coordinate onto discrete grid. The encoding for the coordinate is then defined as: , (1) PE(x, 2i) = sin PE(x, 2i + 1) = cos (cid:18) (cid:18) 100002i/ 3 3 100002i/ (cid:19) (cid:19) , . (3) (4) Similar calculations are applied to the and coordinates. The PE of (x, y, z) are concatenated to obtain the final coRH d. Lastly, the coordinate embeddings ecoord ordinate embeddings are added to the visual embeddings to form the position-aware video representations, denoted by: = eimg evis + ecoord . (5) 3.3. Training Objective Our approach is to build generalist model which can handle multiple tasks with the single learned model. Our model is trained using combined dataset that encompasses variety of 3D scene understanding tasks in the multi-task manner. During training, we uniformly sample batches where each batch contains data from single task type. For general 3D scene understanding tasks, such as 3D question answering and 3D dense captioning, we use cross-entropy loss to supervise text generation. For batches sampled from the 3D visual grounding task, to locate more accurately, we use the designed 3D visual grounding loss doing the multi-task training. Cross-Entropy Loss. Given the position-aware video representation and textual instruction, the language modeling objective aims to optimize the cross-entropy loss LCE: LCE = (cid:88) log(y{evis }l k=1, {etext }q k=1), (6) }l where is the ground truth response, {evis k=1 are position-aware video representations for the video and }q {etext k=1 are text embeddings. For the dense captioning task, the input includes bounding box for the target object. As shown in Fig 2 (b), we calculate the 3D position encoding of the bounding box center in the same manner as described in Sec 3.2. This position encoding is added to the embedding of the special token coord to provide location information. 3D Visual Grounding Objective. Previous studies [21, 56] have demonstrated that directly outputting 3D bounding boxes is quite challenging for LLM. To enable our model to perform 3D visual grounding, we follow previous [22, 24, 58] work to model the task as proposal classification task. As illustrated in Fig 2 (c), given list of object proposals, we extract object features for each object from the visual embeddings. Specifically, for each object bi, we check each patch to see if more than 50% of its points are contained within bi, and then apply average pooling to the features of all selected patches, which can be formalized as: eobj-2d = AveragePooling if the percentage of points in bi>50% (eimg (u, v)). (7) with eobj-2d Lastly, we add the 3D position encoding of the center coordinate eobj-coord to obtain the object represeni tation eobj . During training, we utilize InfoNCE loss [40] to optimize the similarity between the ground truth object feature and the hidden states of the ground token: LGrd = exp(f (eobj exp(f (eobj (cid:80) ) g(h)/τ ) ) g(h)/τ ) , (8) , where is the index of the ground truth object, and are two-layer learnable MLPs, and τ is the temperature. 4. Experiments In this section, we first compare the overall performance of Video-3D LLM with top-tier models and also investigate the effectiveness of all components. 5 4.1. Experimental Setup Datasets. We conduct experiments across five 3D scene understanding benchmarks. For visual grounding, we test our model on ScanRefer [6] and Multi3DRefer [52], which require localizing objects in single-target and multiple-target scenarios, respectively. For dense captioning, we utilize the Scan2Cap [7] benchmark, which involves densely generating descriptions for all objects in 3D scenes. For question answering, we use the ScanQA [3] for spatial scene understanding and SQA3D [36] for situated reasoning. All these datasets are sourced from the ScanNet [16], richly annotated RGB-D video dataset containing 1,513 scans in 3D scenes. We pre-process video frames for each scan at 3 FPS and extract the corresponding extrinsic and camera intrinsic parameters. For evaluation, we follow previous work [14, 22, 57] to adopt the validation sets for ScanRefer, Multi3DRefer, Scan2Cap, and ScanQA, and adopt the test set for SQA3D. Metrics. We adopt widely used evaluation metrics for each of these benchmarks. For ScanRefer [6], we report thresholded accuracy metrics, specifically Acc@0.25 and Acc@0.5, where prediction is considered correct if its Intersection over Union (IoU) with the ground truth exceeds 0.25 and 0.5, respectively. For Multi3DRefer [52], which involves grounding variable number of target objects, we use the F1 score at IoU thresholds of 0.25 and 0.5. For Scan2Cap [7], we apply CIDEr@0.5IoU and BLEU4@0.5IoU (denoted as C@0.5 and B-4@0.5), combining traditional image captioning metrics with IoU between predicted and reference bounding boxes. For ScanQA [3], we use CIDEr [42] and exact match accuracy, referred to as and EM, respectively. Finally, for SQA3D [36], we evaluate performance using exact match accuracy (EM). Implementation Details. We build Video-3D LLM based on the LLaVA-Video 7B [53], an open-sourced video LLM based on the QWen2-7B [48]. We use the Adam optimizer to train our model for one epoch with batch size of 16 and warmup ratio of 0.03. During the warmup phase, the learning rates peak at 1e-5 for the LLM and 2e-6 for the vision encoder. All experiments are conducted on 8 A10080G GPUs. For 3D visual grounding and dense captioning, in training, we use the ground truth objects as the candidates. While in inference, we follow [22, 23] to employ Mask3D [39] to generate object proposals. The temperature τ for InfoNCE loss is 0.07. 4.2. Comparison with State-of-the-art Methods 4.2.1. Comparison Baselines For comprehensive comparison, we include both expert models designed for specific tasks and LLM-based models. Expert Models. For ScanRefer [6], we compare our method with ScanRefer [6], MVT [24], 3DVG-Trans [54], Method Expert Models ScanRefer [6] MVT [24] 3DVG-Trans [54] ViL3DRel [9] M3DRef-CLIP [52] Scan2Cap [7] ScanQA [3] 3D-VisTA [58] 2D LLMs Oryx-34B [35] LLaVA-Video-7B [53] 3D LLMs 3D-LLM(Flamingo)[21] 3D-LLM(BLIP2-flant5)[21] Chat-3D [45] Chat-3D v2 [22] LL3DA [11] SceneLLM [19] LEO [23] Grounded 3D-LLM [14] PQ3D [59] ChatScene [22] LLaVA-3D [57] Video-3D LLM (MC) Video-3D LLM (Uniform) 3D Generalist ScanRefer Multi3DRef Scan2Cap ScanQA SQA3D Acc@0.25 Acc@0.5 F1@0.25 F1@0.5 B-4@0.5 C@0.5 EM EM 37.3 40.8 45.9 47.9 51.9 24.3 33.3 34.5 37.7 44.7 50.6 45. 21.2 30.3 42.5 47.9 57.0 55.5 54.1 57.9 58.1 38.4 44.1 51.2 50.2 42.4 51.2 51.7 42. 45.1 45.2 57.1 57.9 58.0 38.4 22.4 35.2 34.0 66. 64.9 69.6 21.1 22.4 31.8 36.0 38.2 70.6 36.0 36.3 41.1 40.2 41.3 72.3 88. 63.9 62.9 72.4 35.5 80.3 77.1 79.2 80.0 83.8 59.2 69.4 53.2 87.6 76.8 80.0 101.4 72.7 87.7 91.7 100.5 102.1 20.4 20.5 27.2 21.5 21.6 27.0 29.5 30.1 47.2 48.5 48. 54.7 53.6 50.0 47.1 54.6 55.6 57.7 58.6 41.6 40.6 50.1 52.4 52.4 52.7 Table 1. Overall performance comparison. Expert models are customized for specific tasks through task-oriented heads. 3D Generalist means the model can perform multiple 3D tasks in single model. LLaVA-Video is assessed in zero-shot setting. ViL3DRel [9]. M3DRef-CLIP [52] further extends 3D grounding capabilities to multi-target scenarios. Scan2Cap [7] and ScanQA [3] provide initial benchmarks for the Scan2Cap and ScanQA datasets, respectively. 3D-VisTA [58] is pre-trained on large-scale scene-text pairs and then finetuned on specific tasks. 2D LLMs. Oryx [35] has included the ScanQA dataset in its training stage. In addition, we test the zero-shot performance of LLaVA-Video [53]. 3D LLMs. 3D-LLM [21] is the first LLM-based model for 3D scene undersanding. SceneLLM and LL3DA [11] enrich the 3D representations with point cloud features. Chat3D [45], LEO [23], and ChatScene [22] incorporate object representations into 3D LLMs. Grounded 3D-LLM [14], PQ3D [59] and LLaVA-3D [57] deliver impressive results on 3D visual grounding by co-training 3D detector. All these methods apply 3D point cloud features or projecting multi-view image features into 3D space, while ours directly works on the video representations. 4.2.2. Results We present the overall comparison with leading methods in Table 1. Video-3D LLM (Uniform) is trained using uniform sampling with 32 frames, while Video-3D LLM (MC) is trained using maximum coverage sampling with coverage ratio of 95% and maximum frame number of 32. Video-3D LLM (Uniform) achieves state-of-the-art performance on variety of tasks including 3D visual grounding, 3D dense captioning, and 3D question answering, while Video-3D LLM (MC) delivers similar results with only half the inference time (527ms vs. 1050 ms). 3D Visual Grounding. For 3D visual grounding, we follow previous work [22, 24, 58] to detect all objects using Mask3D [39] and then make predictions over the object proposals. Our model achieves the highest accuracy, with Acc@0.25 at 58.1% and Acc@0.5 at 51.7% on ScanRefer, and F1@0.25 at 58.0% and F1@0.5 at 52.7% on Multi3DRefer. As previous 3D LLMs either use detected object proposals (e.g., ChatScene, Chat3D) or train an additional grounding module based on 3D detector (e.g., Grounded 3d-LLM, PQ3D, LLaVA-3D), we can compare with these 3D LLMs fairly. Specifically, our model improves Acc@0.25 by 2.6% on ScanRefer and F1@0.25 by 0.9% on Multi3DRefer compared to ChatScene, which uses the same object proposal as ours. 3D Dense Captioning. Following the previous setting [22, 23], we generate captions for each detected object proFrame Number Sampling Strategy Inference Time ScanRefer Acc@0.25 Acc@0.5 Multi3dRefer Scan2Cap F1@0.25 F1@0.5 B-4@0.5 C@0.5 ScanQA EM SQA3D EM Fixed Frame Number 8 16 32 Uniform MC Uniform MC Uniform MC 309ms 537ms 1050ms 48.93 53.47 55.42 56.46 58.11 58.27 43.50 47.41 49.17 50.11 51.72 51.68 49.80 53.55 54.95 56.65 58.02 57.93 45.40 48.54 49.82 51.39 52.68 52.50 37.34 38.77 39.39 39.59 41.30 40. 68.82 73.08 76.96 76.84 83.76 81.58 94.98 96.37 99.86 100.63 102.06 102.33 27.57 28.00 28.96 29.49 30.09 30.35 56.77 56.97 57.70 57.82 58.56 59.25 Adaptive Frame Number MC Previous SOTA 527ms 57.86 51.18 57. 52.40 40.18 80.00 100.54 29.50 57. LLaVA-3D [57] 433ms 54.1 42.4 41.1 79.2 91.7 27.0 55.6 Table 2. Ablation study for the effect of frame sampling strategy. MC represents maximum coverage sampling. MC denotes sampling frames until over 95% of the scenes voxels are covered or maximum of 32 frames is reached. 3D-PE Coord. ScanRefer Acc@0.25 Acc@0.5 Scan2Cap C@0.5 ScanQA EM None MLP Sin Sin Avg Center Min-Max Avg 57.50 59.63 58.11 57.53 58.05 58.11 50.84 52.98 51. 51.06 51.77 51.72 31.03 76.23 83.76 80.88 82.75 83.76 30.03 29.62 30.09 29.39 30.18 30.09 Patch Size 14 27 14 Loss InfoNCE InfoNCE BCE ScanRefer Multi3DRefer Acc@0.25 Acc@0.5 Acc@0.5 Acc@0. 56.44 55.23 51.63 50.08 48.93 45.82 56.31 56.13 46.07 51.05 50.90 41.47 Table 4. Ablation study for the effect of visual grounding. We train the model separately on the ScanRefer and Multi3DRefer datasets. Table 3. Ablation study for the effect of coordinate encoding. Coord. means the method for aggregating the coordinates. posal. Our method demonstrates superior performance in Scan2Cap, achieving 41.3 at B-4@0.5 and 83.8 at C@0.5. The results reveal that our method connects video content with its position information by injecting 3D-PE into video representations. 3D Question Answering. Our model outperforms best competitors with 30.1% EM on ScanQA and 58.6% EM on SQA3D, which could be attributed to the strong representations inherited from Video LLMs. It is worth noting that existing Video LLMs achieve competent results in zeroshot manner, suggesting that current 3D question answering tasks may not sufficiently address the challenges of spatial understanding in 3D scenes. 4.3. Ablation Study Effectiveness of Frame Sampling. In Table 2, we assess the effectiveness of the frame sampling strategy across varying numbers of frames. To evaluate the models efficiency, we calculated the average inference speed on ScanQA. In the fixed frame number setting, we sample frames until reaching the desired number. As shown in the table, the models performance improves with an increasing number of frames, though inference time also rises. Video-3D LLM surpasses many previous methods with only 8 frames, demonstrating that our position-aware video paradigm can effectively model 3D scene understanding tasks. With 8 and 16 frames, maximum coverage sampling (MC) significantly enhances performance for all tasks by capturing more complete 3D scene. Notably, with 8 frames, it improves Acc@0.25 by 4.54 on ScanRefer and C@0.5 by 4.26 on Scan2Cap compared to the uniform sampling strategy (Uniform). With 32 frames, the Uniform strategy shows comparable results with MC, as this number of frames is more than enough to cover the 3D scene. In the adaptive frame number setting, frames are sampled until over 95% of the scenes voxels are covered or maximum of 32 frames is reached. This allows for flexible adjustment based on scene size. As shown in the results, MC uses an average of 18 frames across all scenes, achieving similar performance to the 32-frame uniform strategy while offering better inference speed. Meanwhile, compared to LLaVA-3D [57], MC shows superior performance while achieving similar inference speed. Effectiveness of Position-Aware Video Representation. Table 3 presents the performance using different 3D position encoding (3D-PE) and coordinate aggregation strategies. The model was trained on the entire multi-task dataset, with 32 frames under the uniform frame sampling. We first assess the impact of different 3D-PE using average coordinate aggregation. The results indicate that the model can perform well without 3D-PE on 3D grounding and 3D QA tasks. This suggests that the video LLM paradigm can effectively understand spatial relationships across mul7 Figure 3. The visualization results on ScanRefer. The green/red/blue colors indicate the correct/incorrect/ground truth boxes. into LLM, we can generate object embeddings with patch sizes of 14 or 27. For ScanRefer, switching the patch size from 27 to 14 leads to improved accuracy, with Acc@0.25 rising from 55.23% to 56.44%, and Acc@0.5 from 48.93% to 50.08%. This improvement may be attributed to the smaller patch size to capture more precise object features. Additionally, since we model the grounding task by leveraging the similarity between object embeddings and the ground token, the use of BCE loss may impose overly strict constraints. Replacing BCE loss with InfoNCE loss consistently improves performance. Visualization. Figure 3 presents visualization results on the validation split of ScanRefer. The first and second rows indicate the rendered point clouds using uniform sampling and maximum coverage sampling with 16 frames. We observe that maximum coverage sampling usually provides more complete scenes than uniform sampling. For complex cases like (b-e), uniform sampling tends to miss smaller or peripheral objects, leading to prediction failures. Figure 4 presents visualization results on the Scan2Cap validation set. The first two rows show the rendered point clouds of the full video and the frames sampled using the MC strategy. With only 16 frames, nearly the entire scenes information is captured. Meanwhile, using the proposed 3D-PE, the model accurately describes the specified target object. 5. Conclusion In this paper, we propose generalist model for 3D scene understanding, namely Video-3D LLM. It is built upon Video LLM and incorporates 3D coordinates into video representations. By doing this, it aligns video representations with 3D physical world, thereby enabling suite of 3D scene understanding tasks. In addition, we introduce maximum coverage sampling strategy to balance computational costs with performance efficiency. Our extensive experimental results demonstrate that our model achieves state-of-the-art performance on several benchmarks. Figure 4. The visualization results on Scan2Cap. The input boxes are marked in blue. tiple frames. However, for dense captioning tasks, which require locating objects within video sequences based on query bounding boxes, the absence of 3D-PE results in significantly degraded performance. Additionally, MLP position encoding proves more effective for grounding tasks, while Sin PE works better for others. We then evaluate different coordinate aggregation strategies using Sin position encoding. Since the 3D coordinates are more complex and variable than those in 2D images, simply using the coordinates at the center of 2D patch doesnt accurately reflect the spatial location. As shown in the result, using average 3D coordinates better represents the 3D location within patch, while using the minimum and maximum 3D coordinates within the patch is also viable alternative. Ablation for 3D Visual Grounding. As illustrated in Table 4, we investigate the effect of varying patch sizes of object embeddings and loss functions. Since LLaVA-Video [53] downsamples the image patches again before feeding them"
        },
        {
            "title": "References",
            "content": "[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas J. Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, pages 422440. Springer, 2020. 2 [2] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaıs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. 1 [3] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1910719117. IEEE, 2022. 2, 5, 6, 13, 14 [4] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: unified framework for joint dense captioning and visual grounding on 3d point clouds. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1644316452. IEEE, 2022. 13, 14 [5] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Dorsa Sadigh, Leonidas J. Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1445514465. IEEE, 2024. 1 [6] Dave Zhenyu Chen, Angel X. Chang, and Matthias Nießner. Scanrefer: 3d object localization in RGB-D scans using natural language. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part XX, pages 202221. Springer, 2020. 2, 5, 6, 13, [7] Dave Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel X. Chang. Scan2cap: Context-aware dense captioning In IEEE Conference on Computer Viin RGB-D scans. sion and Pattern Recognition, CVPR 2021, virtual, June 1925, 2021, pages 31933203. Computer Vision Foundation / IEEE, 2021. 2, 5, 6, 13 [8] Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, and Angel X. Chang. D3net: unified speaker-listener architecture for 3d dense captioning and visual grounding. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXII, pages 487505. Springer, 2022. 13, 14 [9] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Language conditioned In Adspatial relation reasoning for 3d object grounding. vances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. 2, 6, 14 [10] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, and Tao Chen. End-to-end 3d dense captioning with In IEEE/CVF Conference on Computer Vivote2cap-detr. sion and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1112411133. IEEE, 2023. 2 [11] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. LL3DA: visual interactive instruction tuning for omni-3d understanding, reasoning, and planning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 26418 26428. IEEE, 2024. 1, 2, 6, 13, [12] Sijin Chen, Hongyuan Zhu, Mingsheng Li, Xin Chen, Peng Guo, Yinjie Lei, Gang Yu, Taihao Li, and Tao Chen. Vote2cap-detr++: Decoupling localization and describing IEEE Trans. Pattern for end-to-end 3d dense captioning. Anal. Mach. Intell., 46(11):73317347, 2024. 2 [13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. CoRR, abs/1504.00325, 2015. 2 [14] Yilun Chen, Shuai Yang, Haifeng Huang, Tai Wang, Ruiyuan Lyu, Runsen Xu, Dahua Lin, and Jiangmiao Pang. Grounded 3d-llm with referent tokens. CoRR, abs/2405.10370, 2024. 1, 2, 5, 6, 14 [15] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. CoRR, abs/2312.14238, 2023. 1 [16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas A. Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 24322443. IEEE Computer Society, 2017. 5 [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. [18] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil 9 Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozi`ere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. 1 [19] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. Scene-llm: Extending language model for 3d visual understanding and reasoning. CoRR, abs/2403.11401, 2024. 1, 6, 13, 14 [20] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA matter: Elevating the role of image understanding in visual question answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 63256334. IEEE Computer Society, 2017. 2 [21] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: InIn Adjecting the 3d world into large language models. vances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 1, 2, 5, 6, [22] Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu, Xize Cheng, Yang Zhao, Tao Jin, and Zhou Zhao. Chat-3d v2: Bridging 3d scene and large language models with object identifiers. CoRR, abs/2312.08168, 2023. 1, 2, 5, 6, 13, 14 [23] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 1, 2, 5, 6, 13, 14 [24] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. MultiIn IEEE/CVF view transformer for 3d visual grounding. Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1550315512. IEEE, 2022. 2, 5, 6, 14 [25] Weitai Kang, Haifeng Huang, Yuzhang Shang, Mubarak Shah, and Yan Yan. Robin3d: Improving 3d large language model via robust instruction tuning. CoRR, abs/2410.00255, 2024. 1, 2, 3 [26] Samir Khuller, Anna Moss, and Joseph (Seffi) Naor. The budgeted maximum coverage problem. Information Processing Letters, 70(1):3945, 1999. [27] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Paul Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. CoRR, abs/2406.09246, 2024. 2, 3 [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. CoRR, abs/2408.03326, 2024. 1, 2, 3 [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, pages 19730 19742. PMLR, 2023. 1 [30] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Lou, Limin Wang, and Yu Qiao. Mvbench: comprehensive multimodal video understanding benchmark. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 22195 22206. IEEE, 2024. 3 [31] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. VILA: on pre-training for visual language models. CoRR, abs/2312.07533, 2023. 3 [32] Xiongkun Linghu, Jiangyong Huang, Xuesong Niu, Xiaojian Ma, Baoxiong Jia, and Thomas S. Huang. Multi-modal situated reasoning in 3d scenes. CoRR, abs/2409.02389, 2024. [33] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yansong Tang, Wei-Chiu Ma, and Ranjay Krishna. Coarse correspondence elicit 3d spacetime understanding in multimodal language model. CoRR, abs/2408.00754, 2024. 1, 2, 3 [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 1, 13 [35] Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx MLLM: on-demand spatialCoRR, temporal understanding at arbitrary resolution. abs/2409.12961, 2024. 1, 2, 3, 6, 14 [36] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. SQA3D: situated question answering in 3d scenes. In The Eleventh International Conference on Learning Representations, ICLR 10 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. 2, 5, 13 [37] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul McVay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent-Pierre Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Alexander Sax, and Aravind Rajeswaran. Openeqa: Embodied question answering in the era of foundation models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1648816498. IEEE, 2024. 1 [38] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. 1 [39] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask transformer for 3d semantic instance segmentation. In IEEE International Conference on Robotics and Automation, ICRA 2023, London, UK, May 29 - June 2, 2023, pages 8216 8223. IEEE, 2023. 5, 6, 13 [40] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018. 5 [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 59986008, 2017. 4 [42] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluIn IEEE Conference on Computer Vision and Patation. tern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 45664575. IEEE Computer Society, 2015. 5 [43] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. CoRR, abs/2409.12191, 2024. 1 [44] Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, Xihui Liu, Cewu Lu, Dahua Lin, and Jiangmiao Pang. Embodiedscan: holistic multi-modal 3d perception suite towards embodied AI. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 19757 19767. IEEE, 2024. [45] Zehan Wang, Haifeng Huang, Yang Zhao, Ziang Zhang, and Zhou Zhao. Chat-3d: Data-efficiently tuning large language model for universal dialogue of 3d scenes. CoRR, abs/2308.08769, 2023. 1, 2, 6, 14 [46] Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 2, 3 [47] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, and Dahua Lin. Pointllm: Empowering large language models to understand point clouds. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XXV, pages 131147. Springer, 2024. 1, 2 [48] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report. CoRR, abs/2407.10671, 2024. 5 [49] Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo SAT: 2d semantics assisted training for 3d visual Luo. grounding. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 18361846. IEEE, 2021. [50] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023 - System Demonstrations, Singapore, December 6-10, 2023, pages 543553. Association for Computational Linguistics, 2023. 3 [51] Taolin Zhang, Sunan He, Tao Dai, Zhi Wang, Bin Chen, and Shu-Tao Xia. Vision-language pre-training with object contrastive learning for 3d scene understanding. In ThirtyEighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 7296 7304. AAAI Press, 2024. 2 [52] Yiming Zhang, ZeMing Gong, and Angel X. Chang. Multi3drefer: Grounding text description to multiple 3d objects. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, page 15179. IEEE, 2023. 2, 5, 6, 13, 14 [53] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. 1, 3, 5, 6, 8, 13, 14 [54] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvgtransformer: Relation modeling for visual grounding on point clouds. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 29082917. IEEE, 2021. 2, 5, 6, 14 [55] Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, and Liwei Wang. Towards learning generalist model for embodied In IEEE/CVF Conference on Computer Vision navigation. and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 1362413634. IEEE, 2024. 14 [56] Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, and Xihui Liu. Scanreason: Empowering 3d visual grounding with reasoning capabilities. CoRR, abs/2407.01525, 2024. 2, 5 [57] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. Llava-3d: simple yet effective pathCoRR, way to empowering lmms with 3d-awareness. abs/2409.18125, 2024. 1, 2, 5, 6, 7, 13, 14 [58] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for In IEEE/CVF International 3d vision and text alignment. Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pages 28992909. IEEE, 2023. 2, 5, 6, 13, 14 [59] Ziyu Zhu, Zhuofan Zhang, Xiaojian Ma, Xuesong Niu, Yixin Chen, Baoxiong Jia, Zhidong Deng, Siyuan Huang, and Qing Li. Unifying 3d vision-language understanding via promptable queries. In Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XLIV, pages 188206. Springer, 2024. 6, 14 [60] Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong T. Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. RT-2: vision-language-action models transfer web In Conference on Robot knowledge to robotic control. Learning, CoRL 2023, 6-9 November 2023, Atlanta, GA, USA, pages 21652183. PMLR, 2023. 2,"
        },
        {
            "title": "Appendix",
            "content": "A. Implementation Details Dataset Statistics. We present the detailed statistics for training and testing data in Table 5 and 6, respectively. Following previous work [22, 23], we adopt the validation set for ScanRefer [6], Multi3DRefer [52], Scan2Cap [7], ScanQA [7], and the test set for SQA3D [36]. All data have been converted to LLaVA [34] format, and we conduct statistics in this format. Evaluation Details. For ScanRefer [6], we select the object with the highest similarity as the prediction. For Multi3DRefer [52], we select objects with the highest probabilities such that their cumulative probability exceeds given threshold p, which is empirically set to 0.25. For Scan2Cap [7], we follow [23] to evaluate the captioning performance by inserting sos and eos at the start and end of the prediction, respectively. Responses are generated using greedy sampling for 3D dense captioning and 3D question answering tasks. B. Detailed Comparison SQA3D. We conduct detailed evaluation on the test split of the SQA3D [36] dataset. As shown in Table 7, our model achieves the best performance on all categories of questions with an average EM at 58.86%, outperforming the previous state-of-the-art method LLaVA-3D [57] by 2.94% on the average EM. Scan2Cap. As shown in Table 8, we provide detailed comparison on the validation set of Scan2Cap [7]. During inference, we utilize the object proposals from [23], which include 50 predicted objects extracted with Mask3D [39] for each scan. From the table, we can see our method achieves the state-of-the-art on CIDEr and BLEU-4 at 83.77 and 42.43, respectively. ScanRefer. We present detailed comparison for ScanRefer [6] in Table 9. The table shows that our method reaches peak of 58.12% Acc@0.25 and 51.72% Acc@0.5, surpassing ChatScene [22] by 2.6% and 1.5%, respectively. Multi3DRefer. We follow previous work [52] to report the metrics across all question types, where ZT denotes zerotarget, ST denotes single-target, MT denotes multitarget, w/ and w/o denote with and without distractors, respectively. As shown in Table 10, our method outperforms previous methods on ZT w/o D, ZT w/ D, and ST w/D types. However, the performance for MT is lower than ChatScene [22], suggesting that our method still struggles to distinguish similar objects. ScanQA. We test our model on the validation set of ScanRefer [6] Multi3DRefer [52] Scan2Cap [7] ScanQA [3] SQA3D [36] Data Count 36,665 43,838 36,665 26,515 79, Scan Count Ques length Answer Length 562 562 562 562 518 24.9 34.8 13.0 13.7 37.8 17.9 2.4 1. Table 5. Detailed statistics for training data. We report the average lengths for questions and answers, respectively. ScanRefer [6] (Val) Multi3DRefer [52] (Val) Scan2Cap [7] (Val) ScanQA [3] (Val) SQA3D [36] (Test) Data Count 9,508 11,120 2,068 4,675 3,519 Scan Count Ques length Answer Length 141 141 141 71 67 25.0 34.7 13.0 13.8 36.3 18.7 2.4 1.1 Table 6. Detailed statistics for testing data. We report the average lengths for questions and answers, respectively. Method Test set What Is How Can Which Others 31.6 63.8 46.0 69.5 34.8 63.3 45.4 69.8 42.7 56.3 47.5 55.3 40.9 69.1 45.0 70.8 SQA3D [36] 3D-VisTA [58] LLaVA-Video[53] Scene-LLM [19] LEO [23] ChatScene [22] LLaVA-3D [57] Video-3D LLM (Uniform) 51.1 72.4 55.5 69.8 50.0 70.7 57.9 69.8 Video-3D LLM (MC) 45.4 67.0 52.0 69. 43.9 47.2 50.1 47.2 49.9 51.3 50.1 45.3 48.1 47.2 52.3 55.0 56.0 55.8 Avg. 46.6 48.5 48.5 54.2 50.0 54.6 55.6 58.6 57. Table 7. Performance comparison on the test set of SQA3D [36]. Method @0.5 B-4 R Scan2Cap [7] 3DJCG [4] D3Net [8] 3D-VisTA [58] LL3DA [11] LEO [23] ChatScene [22] LLaVA-3D [57] Video-3D LLM (Uniform) Video-3D LLM (MC) 39.08 49.48 62.64 66.9 65.19 68.4 77.19 79.21 83.77 80.00 23.32 31.03 35.68 34.0 36.79 36.9 36.34 41.12 42.43 40.18 21.97 24.22 25.72 27.1 25.97 27.7 28.01 30.21 28.87 28.49 44.48 50.80 53.90 54.3 55.06 57.8 58.12 63.41 62.34 61. Table 8. Performance comparison on the validation set of Scan2Cap [7]. C, B-4, M, represent CIDEr, BLEU-4, Meteor, Rouge-L, respectively. ScanQA [3]. Compared to previous top-tier models, our Video-3D LLM achieves relative improvement of 10.7% and 11.9% on EM@1 and CIDEr, respectively. 13 Method ScanRefer [6] MVT [24] 3DVG-Transformer [54] ViL3DRel [9] 3DJCG [4] D3Net [8] M3DRef-CLIP [52] 3D-VisTA [58] 3D-LLM (Flamingo) [21] 3D-LLM (BLIP2-flant5) [21] Grounded 3D-LLM [14] PQ3D [59] ChatScene [22] LLaVA-3D [57] Video-3D LLM (Uniform) Video-3D LLM (MC) Venue ECCV20 CVPR22 ICCV21 NeurIPS22 CVPR22 ECCV22 ICCV23 ICCV23 NeurIPS23 NeurIPS23 ArXiv24 ECCV24 NeurIPS24 ArXiv24 Unique Multiple Overall Acc@0.25 Acc@0. Acc@0.25 Acc@0.5 Acc@0.25 Acc@0.5 76.33 77.67 81.93 81.58 83.47 85.3 81.6 86.7 89.59 87.97 86.61 53.51 66.45 60.64 68.62 64.34 72.04 77.2 75.1 78.3 82.49 78.32 77. 32.73 31.92 39.30 40.30 41.39 43.8 43.7 51.5 47.78 50.93 50.95 21.11 25.26 28.42 30.71 30.82 30.05 36.8 39.1 46.2 42.90 45.32 44.96 41.19 40.80 47.57 47.94 49.56 51.9 50.6 21.2 30.3 47.9 57.0 55.52 54.1 58.12 57.87 27.40 33.26 34.67 37.73 37.33 37.87 44.7 45.8 44.1 51.2 50.23 42.2 51.72 51.18 Table 9. Performance comparison on the validation set of ScanRefer [6]. Unique and Multiple depends on whether there are other objects of the same class as the target object. Method M3DRef-CLIP [52] D3Net [8] 3DJCG [4] Grounded 3D-LLM [14] PQ3D [59] ChatScene [22] Video-3D LLM (Uniform) Video-3D LLM (MC) ZT w/o ZT w/ ST w/o ST w/ MT ALL F1 81.8 81.6 94.1 85.4 90.3 94.7 94.1 F1 39.4 32.5 66.9 57.7 62.6 78.5 76.7 F1@0.25 F1@0. F1@0.25 F1@0.5 F1@0.25 F1@0.5 F1@0.25 F1@0. 53.5 82.9 82.6 81.2 47.8 38.6 26.0 68.5 75.9 73.4 72.6 34.6 49.1 52.1 52.7 30.6 23.3 16.7 43.6 44.5 47.2 47.4 43.6 45.7 40.8 40.6 37.9 35.0 26.2 40.9 41.1 35.7 35. 42.8 45.2 57.1 58.0 57.9 38.4 32.2 26.6 40.6 50.1 52.4 52.7 52.4 Table 10. Performance comparison on the validation set of Multi3DRefer [52]. ZT: zero-target, ST: single-target, MT: multi-target, D: distractor. Method ScanQA [3] 3D-VisTA [58] Oryx-34B [35] LLaVA-Video-7B [53] 3D-LLM (Flamingo) [21] 3D-LLM (BLIP2-flant5) [21] Chat-3D [45] NaviLLM [55] LL3DA [11] Scene-LLM [19] LEO [23] Grounded 3D-LLM [14] ChatScene [22] LLaVA-3D [57] Video-3D LLM (Uniform) Video-3D LLM (MC) Venue CVPR22 ICCV23 ArXiv24 ArXiv24 NeurIPS23 NeurIPS23 ArXiv23 CVPR24 CVPR24 ArXiv24 ICML24 ArXiv24 NeurIPS24 arXiv24 EM 21.05 22.4 20.4 20.5 23.0 27.2 21.62 27.0 30.10 29.50 B-1 30.24 38.0 39.71 30.3 39.3 29.1 43.6 43.20 47.05 46.23 B20.40 24.6 26.57 17.8 25.2 26.8 29.06 31.70 31.22 B-3 15.11 9.33 12.0 18.4 19.1 20.57 22.83 22.71 B-4 10.08 10.4 3.09 7.2 12.0 6.4 12.5 13.53 12.0 11.5 13.4 14.31 14.5 16.17 16. ROUGE-L METEOR 33.33 35.7 37.3 44.62 32.3 35.7 28.5 38.4 37.31 40.0 39.3 41.56 50.1 49.02 48.19 13.14 13.9 15.0 17.72 12.2 14.5 11.9 15.4 15.88 16.6 16.2 18.00 20.7 19.84 19.36 CIDEr 64.86 69.6 72.3 88.70 59.2 69.4 53.2 75.9 76.79 80.0 80.0 72.7 87.70 91.7 102.06 100. Table 11. Performance comparison on the validation set of ScanQA [3]. EM indicates exact match accuracy, and B-1, B-2, B-3, B-4 denote BLEU-1, -2, -3, -4, respectively."
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong"
    ]
}