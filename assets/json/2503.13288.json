{
    "paper_title": "$φ$-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation",
    "authors": [
        "Fangzhi Xu",
        "Hang Yan",
        "Chang Ma",
        "Haiteng Zhao",
        "Jun Liu",
        "Qika Lin",
        "Zhiyong Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous search-based strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose a novel decoding strategy, named $\\phi$-Decoding. To provide a precise and expressive estimation of step value, $\\phi$-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring a light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show $\\phi$-Decoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across a wide range of computing budgets. The code will be released at https://github.com/xufangzhi/phi-Decoding, and the open-source PyPI package is coming soon."
        },
        {
            "title": "Start",
            "content": "ϕ-Decoding: Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation Fangzhi Xu2,1 Hang Yan2* Chang Ma3 Haiteng Zhao4 Jun Liu2 Qika Lin5 Zhiyong Wu1 2Xian Jiaotong University 3The University of Hong Kong 1Shanghai AI Lab 5 2 0 2 M 7 1 ] . [ 1 8 8 2 3 1 . 3 0 5 2 : r 4Peking University 5National University of Singapore {fangzhixu98,whucs2013wzy}@gmail.com hangyan666@outlook.com cma@cs.hku.hk zhaohaiteng@pku.edu.cn liukeen@xjtu.edu.cn linqika@nus.edu.sg"
        },
        {
            "title": "Abstract",
            "content": "Inference-time optimization scales computation to derive deliberate reasoning steps for effective performance. While previous searchbased strategies address the short-sightedness of auto-regressive generation, the vast search space leads to excessive exploration and insufficient exploitation. To strike an efficient balance to derive the optimal step, we frame the decoding strategy as foresight sampling, leveraging simulated future steps to obtain globally optimal step estimation. Built on it, we propose novel decoding strategy, named ϕDecoding. To provide precise and expressive estimation of step value, ϕ-Decoding approximates two distributions via foresight and clustering. Sampling from the joint distribution, the optimal steps can be selected for exploitation. To support adaptive computation allocation, we propose in-width and in-depth pruning strategies, featuring light-weight solution to achieve inference efficiency. Extensive experiments across seven benchmarks show ϕDecoding outperforms strong baselines in both performance and efficiency. Additional analysis demonstrates its generalization across various LLMs and scalability across wide range of computing budgets.1 Figure 1: Comparisons of different decoding paradigms. (a) is auto-regressive decoding, which has high efficiency but lacks global awareness. (b) represents searchbased methods, which requires huge search space with extensive time cost. (c) is the foresight sampling strategy. It leverages the simulated future steps to estimate the step value, which can strike balanced inferencetime exploration and exploitation."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (Achiam et al., 2023; Team et al., 2023) present remarkable performances in solving reasoning-intensive tasks through step-by-step thoughts (Wei et al., 2022). Recent advancements (Team, 2024; Guo et al., 2025) have significantly boosted LLM reasoning by means equal contribution. Work done during Fangzhis internship at Shanghai AI Lab. denotes corresponding author. 1The code will be released at https://github.com/ xufangzhi/phi-Decoding, and the open-source PyPI package is coming soon. large-scale post-training on well-curated datasets. Nevertheless, the cost associated with the posttraining procedure hinders its reproducibility. This naturally motivates us to explore the inference-time strategy for optimizing the LLM reasoning chains. Inference-time optimization involves employing more reasoning tokens that encode thinking steps to perform effective reasoning. However, the natural shortsightedness of auto-regressive generation, which predicts the next step only with preceding steps, makes most inference algorithms unable to achieve global optima (Ma et al., 2024) (Fig. 1(a)). Most previous work solves this by deliberately optimizing each step using search-based methods (Yao et al., 2024; Hao et al., 2023; Xie et al., 2024; Wu et al., 2024), the expanding and backtracking of tree search algorithms enable LLMs to find global-optimal reasoning paths (Fig. 1(b)). However, the vast search space results in excessive exploration and insufficient exploitation. Conversely, if we could derive precise estimation of globallyaware step values, an efficient balance between inference-time exploration and exploitation could be achieved. Based on this, we frame the decoding strategy as foresight sampling, as depicted in Fig. 1(c). It relies on the future simulation to obtain the globally optimal estimation of the current step. Central to the foresight sampling is the critical task of how to estimate step value with the foresight steps. Intuitively, the step estimation with foresight can be derived either by incorporating the process reward model (PRM) (Snell et al., 2024) or through model uncertainty (Ma et al., 2024). However, PRMs are not widely available for all reasoning scenarios, which poses challenges for scalability. Delegating the step assessment to model uncertainty risks the issue of local optima, potentially resulting in suboptimal performance. Another issue in stepwise exploration and exploitation is whether every step requires deliberation for decision-making. Naturally, more computational resources should be allocated to challenging steps, while conserving compute for simpler steps. Previous inference-time optimization methods widely overlook this principle. In addition, such concept of over-thinking has been widely observed in the o1-like attempts (Chen et al., 2024; Manvi et al., 2024). Therefore, it is both intriguing and promising to develop light-weight solution that can adaptively balances computational workload without extra training. In this paper, we propose novel inference-time optimization algorithm named ϕ-Decoding, which introduces an adaptive foresight sampling strategy to achieve efficient exploration and exploitation during inference. To give the reliable and expressive step value estimation, ϕ-Decoding capitalizes on foresight paths to derive two distributions: one from the derived step advantage values, capturing uncertainty discrepancies between successive steps, and another from alignment of these foresight paths via clustering. Sampling from the joint distribution, ϕ-Decoding selects optimal steps for exploitation. To efficiently allocate the computations, ϕ-Decoding introduces both the in-width and in-depth pruning strategies, which provides adaptive inference-time scaling. On diverse reasoning benchmarks, ϕ-Decoding improves the average performance of LLaMA3.1Instruct-8B by >14% over auto-regressive CoT. Inference-time scaling across diverse computing budgets shows the consistent superiority of ϕDecoding over other baselines, offering balance between performance (Accuracy) and computational efficiency (#FLOPS). Further analysis of the generalization across various backbone LLMs and scalability to the competition-level task highlights the superiority of ϕ-Decoding. The major contributions of our work are: (1) An adaptive inference-time optimization algorithm ϕ-Decoding without external auxiliary. ϕ-Decoding estimates the step value based on the joint distribution derived from foresight paths. Inwidth and in-depth pruning strategies are introduced to alleviate the overthinking issue. (2) Extensive experiments with state-of-the-art performances. ϕ-Decoding improves the average reasoning of LLaMA3.1-8B-Instruct by over 14% across various reasoning benchmarks, presenting great trade-off between effectiveness and efficiency compared with baselines. (3) Comprehensive analysis and insightful findings. ϕ-Decoding proves its generalization ability to various LLMs, ranging from the 70B-sized model to R1-distilled LLM. Additionally, the inference-time scaling across wide range of computing budgets reveals the consistent advantages, where ϕ-Decoding matches the performance of the suboptimal baseline with 6 efficiency."
        },
        {
            "title": "2 Methodology",
            "content": "2.1 Preliminary In the context of auto-regressive language generation, the selection of the current step ˆat is based on the following probability distribution: ˆat pθ(atx, a<t) (1) where is the instruction or the input query, and a<t represents the preceding steps. θ denotes the LLM parameters, where pθ is derived from the distribution of language modeling. To overcome the short-sighted limitation of autoregressive generation and achieve efficient exploFigure 2: The overall framework of ϕ-Decoding. We visualize the decoding process at the timestamp t. For simplicity, we set step beam size as 2, the number of rollouts as 3, and the number of clusters as 2. ration, foresight sampling conditions the generation process not only on the preceding steps a<t but also on an estimation of future outcomes a>t. We use the Boltzmann distribution to model the probabilities of different outcomes during the decoding process, incorporating both the influence of preceding steps and an estimation of future states, such as: ˆat pθ(atx, a<t)Ea>tpθ(a>tx, at, a<t) (2) It is non-trivial to have precise calculation of Ea>tpθ(a>tx, at, a<t). Therefore, we try to derive an estimation of this future simulation quality. ˆat pθ(atx, a<t)exp [R(x, at, a>t)/τ ] (3) where denotes the optimized function for step value estimation based on the foresight steps. τ represents the temperature hyper-parameter, which controls the diversity of generation. Therefore, the ultimate objective of ϕ-Decoding is to design the step value estimation function R(x, at, a>t). We include the key techniques of ϕ-Decoding in Fig. 2, which depicts the decoding process at the timestamp t. The complete algorithm as well as the overall decoding pipeline are presented in Appendix B. 2.2 Step Value Estimation Dynamic Advantage Estimation. We follow the beam search strategy. At the timestamp t, we rollout candidate steps from each beam. Based on the idea of foresight sampling, the probability Ft of the foresight path can be derived: Ft = pθ(a>tx, at, a<t), (4) where the index of the candidate step is omitted for simplicity. To measure the advantage brought by the candidate step at, we define the calculation of Advantage At as: At = pθ(a>tx, at, at) pθ(a>t1x, at1, a<t1) = Ft Ft1 (5) It is represented as the of the foresight probability between the adjacent steps. Notably, we implement the calculation of pθ with the averaged log probability of the sequence, which alleviates the influence from the foresight length. Since the calculation of Advantage for each candidate step is independent, it estimates the absolute value of the step. For better illustration, we define R1(x, at, a>t) = exp(At/τ1). Alignment Assessment by Clustering. One potential risk of the uncertainty-based estimation is the issue of local optima. That is, LLMs may be trapped in the incorrect step with exceptionally high confidence. To thoroughly optimize the formulation, we propose to evaluate the foresight paths from advantage (absolute value) and alignment (relative value). To address this limitation, we introduce the definition of alignment to provide the relative preference among the foresight paths. This is achieved by employing clustering strategy following the foresight sampling process. Specifically, the foresight paths at each timestamp are grouped into clusters. The number of clusters is defined as K. The alignment value of at is determined based on the size of the cluster to which it belongs: Ct = Cluster(at) #Foresight Paths (6) where Cluster(at) denotes the size of the cluster at belongs to. Alignment actually provides the relative estimation of the step value, which reflects the consistency among the foresight paths. The more closely the expected outcome aligns with those of other candidates, the greater the step value would be. Similarly, we define R2(x, at, a>t) = exp(Ct/τ2). Sampling From Joint Distribution Combining the rewarding from R1 and R2, we can derive the definition of function, which is in the form of: R(x, at, a>t) = Norm(At) + Norm(Ct) = (cid:80) exp(At/τ1) exp(At/τ1) at + (cid:80) exp(Ct/τ2) (7) exp(Ct/τ2) at Replacing this formulation of into Eq. 3, the objective becomes the sampling on the joint distribution of Advantage and Alignment. In the implementation, we set τ1 = τ2 = 0.6 and combine R1 and R2 with equal weighting for simplicity. We leave the discussion of the weighted version in future work. 2.3 Dynamic Pruning Strategy To optimize the computation allocation and alleviate the over-thinking issue, we introduce an efficient and effective pruning strategy. It is designed from two dimensions: in-width and in-depth. Figure 2 visualizes the function of the pruning stratgies. In-Width Pruning. Although foresight sampling addresses the short-sightedness of language models, it inevitably introduces additional computational cost. Intuitively, some steps with obvious errors can be filtered out directly, without needing to simulate future steps. To achieve this, we assess the generation confidence of each step at based on its probability: st = pθ(atx, a<t). (8) There are in total candidate steps at timestamp t. We then calculate the mean and variance of these step confidence: µt = 1 (cid:88) s(i) , σ = 1 (cid:88) (s(i) µ) (9) where µ and σ2 denote the mean and variance values respectively. is the number of candidates under the setting of step beam search as defined in Sec. 2.2. Based on this calculation, we exclude any steps with generation confidence that is exceptionally low, i.e., those with s(i) < µ σ. The remaining steps are kept for foresight: St = {a(i) µ σ s(i) } (10) Adhering to this principle enables the attainment of in-width pruning. Notably, the extent of pruning can be regulated by adjusting the threshold using µ kσ, where Z+. In-Depth Pruning. Foresight sampling enables the deliberate thinking of each step. Previous work (Wang and Zhou, 2024) uncovers that the early steps are much more important, necessitating increased computational resources for optimization. As the final answer approaches, LLMs exhibit greater determination in their reasoning paths. Motivated by it, we can save some computational costs with the strategy of early stopping. To avoid extra computing and make the solution as simple as possible, we employ the clustering result introduced in Sec. 2.2. In detail, we derive the size of the largest cluster, written as Clustermax. The condition of early-stopping is controlled by the threshold: Clustermax #Foresight Paths δ (11) Then, the LLM completes the remaining reasoning steps under the auto-regressive setting. For convenience, we set δ = 0.7 for all experiments."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Evaluation Benchmarks and Metrics Benchmarks To comprehensively evaluate the LLM performances on downstream tasks, we mainly include the following 6 representative reasoning benchmarks: GSM8K (Cobbe et al., 2021), MATH-500 (Hendrycks et al., 2021), GPQA (Rein et al., 2023), ReClor (Yu et al., 2020), LogiQA (Liu et al., 2021), and ARCChallenge (Clark et al., 2018). Furthermore, we incorporate the competition-level benchmark AIME (Jia, 2024) to highlight the scalability of ϕDecoding to address more challenging scenarios. Metrics We report the Pass@1 accuracy (Acc.) for each benchmark. To better illustrate the trade-off between efficiency and performance, the FLOPS metric is also computed, following the definition of (Kaplan et al., 2020). Please refer to Appendix A.1 for more evaluation details. 3.2 Baselines and Backbone LLMs In the experiments, we compare ϕ-Decoding with the following 5 baseline methods. Auto-Regressive (CoT). It produces the chainof-thought reasoning through the auto-regressive language generation. Tree-of-Thought (ToT) (Yao et al., 2024). It builds tree structure for given problem, where each node represents reasoning step. We use the BFS version as the implementation. Monte Carlo Tree Search (MCTS). It constructs search tree and dynamically updates the step value via expanding and backtracking. We follow Reasoning as Planning (RaP) (Hao et al., 2023) for implementation. Guided Decoding (Xie et al., 2024). It utilizes self-evaluation at each step to perform stochastic beam search. Predictive Decoding (Ma et al., 2024). It proposes the look-ahead strategy and leverages Model Predictive Control to reweigh LLM distributions, producing non-myopic language modeling. For the 6 reasoning benchmarks in the main experiments, all the baseline methods are evaluated on two backbone LLMs: LLaMA3.1-8BInstruct (Dubey et al., 2024) and Mistral-v0.37B-Instruct (Jiang et al., 2023). To assess generalization and scalability, we further evaluate the Qwen2.5-3B (Yang et al., 2024) and LLaMA3.170B (Dubey et al., 2024) LLMs, while also boosting Deepseek R1-series LLM (i.e., R1-DistillLLaMA-8B) (Guo et al., 2025) for competitionlevel tasks. All the experiments are implemented on A100 of 80GB VRAM GPUs. The inference process is accelerated by the vLLM engine (Kwon et al., 2023). Figure 3: Inference-time scaling law on LLaMA3.1-8BInstruct. The horizontal axis denotes the inference-time computational cost, while the vertical axis represents the average performances on 6 benchmarks. The generation temperature is set to 0.6. Please refer to Appendix A.2 for more implementation details. 3.3 Main Results Table 1 presents the results on 6 reasoning benchmarks across 2 representative open-source LLMs. ϕ-Decoding significantly enhances the average performances of backbone LLMs. Compared with the standard CoT strategy, ϕ-Decoding can achieve the inference-time optimization without extra training. Specifically, notable average improvements of 14.62% and 6.92% are observed in LLaMA3.1-Instruct and Mistral-v0.3-Instruct models respectively. ϕ-Decoding strikes superior balance between effectiveness and efficiency over strong baseIn general, ϕ-Decoding outperforms the lines. four strong baselines by large margin, with consistent lower computational cost. Compared with the recent promising MCTS-style method, ϕ-Decoding showcases notable average improvement of 3.255.70%, achieved with one-third of the cost. When contrasted with the recent SOTA baseline Predictive Decoding, ϕ-Decoding shows remarkable superiority particularly in its adeptness at generalizing across various backbone LLMs. 3.4 On the Inference-Time Scaling Figure 3 presents the inference-time scaling law on LLaMA3.1-8B-Instruct. From the scaling curves, ϕ-Decoding presents the consistent superiority on each computational budget, ranging from 8 1016 Models GSM8K Math-500 GPQA ReClor LogiQA ARC-c Avg. FLOPS Auto-Regressive (CoT) Tree-of-Thoughts MCTS Guided Decoding Predictive Decoding ϕ-Decoding Auto-Regressive (CoT) Tree-of-Thoughts MCTS Guided Decoding Predictive Decoding ϕ-Decoding 70.28 75.74 80.44 75.51 81. 86.58 49.05 53.90 60.12 53.90 58.00 60.42 LLaMA3.1-8B-Instruct 26.56 31.25 24.11 30.58 31.03 34. 49.40 59.00 61.40 60.20 64.00 64.00 Mistral-v0.3-7B-Instruct 23.88 26.34 22.77 27.46 22.10 29.24 52.20 55.60 56.80 53.20 54. 58.20 31.00 31.60 34.40 31.20 34.00 38.20 12.20 10.80 10.80 10.80 11.00 16.40 33.33 45.93 42.70 43.47 46. 48.39 37.02 41.63 40.71 36.71 39.78 43.01 58.91 80.72 79.95 81.74 84.56 44.91 54.04 53.83 53.78 56.95 1.34 1016 7.03 1017 17.90 1017 6.54 1017 6.89 85.41 59.53 6.43 1017 69.54 73.63 74.74 73.55 73.55 40.65 43.65 44.32 42.60 43.11 78. 47.57 0.81 1016 4.99 1017 9.33 1017 7.03 1017 4.73 1017 3.55 1017 Table 1: Main results. The optimal results are highlighted in bold, whereas suboptimal results are underlined. The Avg. column indicates the averaged results across the six benchmarks. FLOPS denotes the calculated computational cost, with lower values indicating lower costs. to 64 1016 FLOPS. Furthermore, when considering similar performance levels (e.g., an average performance of 57%), ϕ-Decoding demonstrates over 6 efficiency compared to even suboptimal methods. Meanwhile, it is observed that Predictive Decoding and ToT also exhibit the stable improvement trend with the inference cost increasing."
        },
        {
            "title": "4 Analysis",
            "content": "4.1 Ablation Studies Some key components of ϕ-Decoding are ablated to verify their contributions to the overall performances in Table 2. w/o foresight sampling indicates that the look-ahead process is ablated, relying solely on step uncertainty for sampling. w/o cluster denotes that we simply sample on the foresight uncertainty distribution without considering the cluster distribution. w/o dynamic pruning means the breadth and depth pruning strategies are ablated. We have the following findings. Foresight sampling mitigates auto-regressive generation limitations with extra inference cost. As the basis of our sampling strategy, simulating the future steps brings remarkable performance gains (2.98%-6.09%). It proves the finding that the short-sightedness of the standard auto-regressive language generation can be reduced by increasing the inference-time computation. Cluster distribution is beneficial to the overall performances. As one of the contributions, we incorporate the cluster of foresight steps to mitigate the unreliability of the accumulated generation probability. The results demonstrate that the cluster can calibrate the sampling distribution, leading to 0.95%-1.97% average gains. Dynamic pruning largely reduces the computational costs. It is observed that the dynamic pruning strategy provides obvious efficiency improvement from the metric of FLOPS. Also, the dynamic pruning strategy surprisingly enhances model performance by eliminating distractions from negative rollouts during sampling. 4.2 Generalization and Scalability Next, we analyze the generalization and scalability of ϕ-Decoding to (i) larger backbone LLM; and (ii) competition-level benchmarks. ϕ-Decoding still works when scaling to 70B model size. Figure 3 shows the results on LLaMA3.1-70B-Instruct across four benchmarks. The model performance is further enhanced with the proposed algorithm. It uncovers the superior generalization capability of ϕ-Decoding. Limited by space, we leave the discussion of smaller backbone LLM (i.e., Qwen2.5-3B-Inst.) for Appendix C. The experiments on the 3B-sized model also reflect the obvious advantages brought by ϕDecoding. Across the 6 reasoning benchmarks, ϕDecoding improves the backbone LLM by 3.80% in average. Combining all these generalization experiments, it is concluded that ϕ-Decoding works Models GSM8K Math-500 GPQA ReClor LogiQA ARC-c Avg. FLOPS ϕ-Decoding w/o foresight sampling w/o cluster w/o dynamic pruning ϕ-Decoding w/o foresight sampling w/o cluster w/o dynamic pruning 86.58 81.80 85.60 86. 60.42 57.54 60.19 59.97 LLaMA3.1-8B-Instruct 38.20 35.00 37.40 38.20 34.60 30.58 30.58 29.46 64.00 60.60 61.00 61.00 Mistral-v0.3-7B-Instruct 16.40 11.40 15.00 15.20 29.24 25.22 29.24 26.56 58.20 42.40 56.60 53.20 48.39 46.39 45.47 46.39 43.01 36.70 42.24 36.41 85.41 84.90 85.32 85. 78.16 75.60 76.45 75.77 59.53 56.55 57.56 57.85 47.57 41.48 46.62 44.52 6.43 1017 1.27 1017 6.37 1017 8.00 1017 3.55 1017 1.19 1017 3.55 1017 6.41 1017 Table 2: Ablation Studies on LLaMA3.1-8B-Instruct and Mistral-v0.3-7B-Instruct models. w/o foresight sampling ablates the simulation of future steps. w/o cluster ablates the calculation of Alignment value. w/o dynamic pruning ablates both of the pruning strategies. Tasks AR (CoT) ϕ-Decoding GSM8K MATH-500 ReClor LogiQA 92.27 41.40 67.60 51. 94.31 44.80 84.80 56.37 +2.04 +3.40 +17.20 +5.37 Table 3: Generalization experiments on LLaMA3.170B-Instruct. The improvements over Auto-Regressive (CoT) are reported in the last columnn. Methods AIME2024 LLaMA3.1-8B-Instruct + Predictive Decoding + ϕ-Decoding R1-Distill-LLaMA-8B + Predictive Decoding + ϕ-Decoding 9.17 13.33 16.67 37.81 20.00 46. - +4.16 +7.50 - -17.81 +8.86 Table 4: Results on AIME 2024. We compare ϕDecoding with Predictive-Decoding based on two backbone LLMs: LLaMA3.1-8B-Instruct and R1-DistillLLaMA-8B. well with wide size range of LLMs, showcasing the superiority. Our inference-time optimization can scale to improve performances on the competition-level task even with the strongest reasoning LLM. Table 4 shows the results on AIME 2024 benchmark. In addition to LLaMA3.1-8B-Instruct. and Mistral-v0.3-7B-Instruct., we also incorporate the DeepSeek-R1 model, utilizing the R1-DistillLLaMA-8B variant due to resource constraints. Even based on well-trained deep thinking model, ϕ-Decoding can still help push the upper boundary on the competition-level task. Such findings Figure 4: Analysis on the accuracy of step value estimation. The bar in light blue represents the accuracy of the step values, while the bar in dark blue denotes the averaged task performances. are exciting and insightful to implement inferencetime optimization aimed at addressing challenging problems with LLM. 4.3 Accuracy of Step Value Estimation The core of these decoding approaches is to estimate the precise step value through self-rewarding. To measure how the estimated step value matches the actual rewards, we employ the calculation of the Accuracy of Step Value via distribution match. Please refer to Appendix for details. Based on the calculation, we visualize the results in Figure 4, revealing the following finding. The estimation of step value is positively correlated with the correctness of the final answer. Of the four inference-time decoding approaches illustrated in Figure 4, more accurate estimation of the step value results in improved task performance. Among them, ϕ-Decoding achieves the optimal estimation of step values as well as the final accuracy Figure 5: Visualization of step-wise effects with alleviated overthinking. The first row displays the results for each independent benchmark using the LLaMA backbone, whereas the second row reflects the results with the Mistral backbone. with obvious advantages. 4.4 Analysis on Step-wise Overthinking Beyond simply reporting the FLOPS metric, detailed analysis of the effects of pruning strategies is presented in Figure 5. It is observed that early steps are more critical, which involves relatively more computational costs. At these early steps, it mainly relies on breadth pruning strategy to avoid redundant step exploration, reducing 20% of the costs. With the steps growing, depth pruning takes over to alleviate overthinking through early stopping. This finding inspires us to allocate more inference-time computational resources to the early steps, which are proved to be critical for the reasoning tasks."
        },
        {
            "title": "5 Related Works",
            "content": "Inference-Time Optimization. To alleviate the post-training workload (Zelikman et al., 2024; Liu et al., 2024; Team, 2024; Guo et al., 2025), inference-time optimization methods arouse wide concerns, showcasing notable performance boost in reasoning scenarios (Snell et al., 2024; Sun et al., 2023; Zhao et al., 2024). Mainstream methods can be categorized into searching-based (Yao et al., 2024; Hao et al., 2023; Xie et al., 2024; Wu et al., 2024) and sampling-based (Ma et al., 2024; Chen et al., 2023; Zhang et al., 2024). Although these works achieve the globally-optimal inference, they either induce large computation costs or yield inadequate step value estimation. Other classical methods, such as Best-of-N, usually involve delegating the step selection to the external reward model (Wang et al., 2024; Guan et al., 2025), and self-reflection strategies (Cheng et al., 2024; Xu et al., 2024) usually involve extra training. ϕDecoding stands out as an optimal and efficient decoding choice without reliance on external auxiliary. Adaptive Inference-time Scaling. Though scaling of inference-time computations has proved to be effective (Snell et al., 2024), the issue of over-thinking is widely observed and remains to be addressed (Chen et al., 2024). One line of works (Team et al., 2025; Han et al., 2024) stress on the control of the generation length, while another line of methods (Manvi et al., 2024; Sun et al., 2024) leverage the idea of early-stopping. In contrast, the adaptive scaling technique presented in our work is training-free and independent of external models. Based on the self-evaluation of stepwise value, ϕ-Decoding introduces the comprehensive pruning strategy from the dimensions of width and depth. It stands out as light-weight solution to alleviate the inference-time over-thinking."
        },
        {
            "title": "6 Conclusion",
            "content": "This work focuses on inference-time optimization for LLMs, leveraging computational scaling to enhance performance. Building on stepwise reasoning and foresight sampling, we address two key research questions: (1) How can we achieve superior step value estimation? and (2) Is deliberative planning necessary for every step? We introduce novel decoding strategy, ϕ-Decoding, that efficiently balances exploration and exploitation during inference. Extensive evaluations across seven diverse LLM benchmarks demonstrate ϕDecoding state-of-the-art performance and efficiency. Furthermore, its ability to generalize to wide range of LLMs (3B, 7B, 8B, and 70B) and scale across various computational budgets underscores the superiority of ϕ-Decoding in inferencetime optimization."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. 2024. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187. Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311. Kanzhi Cheng, Yantao Li, Fangzhi Xu, Jianbing Zhang, Hao Zhou, and Yang Liu. 2024. Vision-language models can self-improve reasoning via reflection. arXiv preprint arXiv:2411.00855. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. 2025. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen, and Zhenting Wang. 2024. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Maxwell Jia. 2024. Aime 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2021. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. In Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence, pages 36223628. Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Acemath: Advancing frontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084. Chang Ma, Haiteng Zhao, Junlei Zhang, Junxian He, and Lingpeng Kong. 2024. Non-myopic generation of language models for reasoning and planning. arXiv preprint arXiv:2410.17195. Rohin Manvi, Anikait Singh, and Stefano Ermon. 2024. Adaptive inference-time compute: Llms can predict if they can do better, even mid-generation. arXiv preprint arXiv:2410.02725. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2024. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2023. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022. Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. 2024. Self-evaluation guided beam search for reasoning. Advances in Neural Information Processing Systems, 36. Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jun Liu, Yu Qiao, and Zhiyong Wu. 2024. Interactive evolution: neural-symbolic self-training framework for large language models. arXiv preprint arXiv:2406.11736. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36. Weihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: reading comprehension dataset requiring logical reasoning. In International Conference on Learning Representations. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. 2024. Quiet-star: Language models can teach themarXiv preprint selves to think before speaking. arXiv:2403.09629. Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. 2024. Self-alignment for factuality: Mitigating hallucinations in llms via self-evaluation. arXiv preprint arXiv:2402.09267. Haiteng Zhao, Chang Ma, Guoyin Wang, Jing Su, Lingpeng Kong, Jingjing Xu, Zhi-Hong Deng, and Hongxia Yang. 2024. Empowering large language model agents through action learning. arXiv preprint arXiv:2402.15809. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314. Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024. Fast best-of-n decoding via speculative rejection. arXiv preprint arXiv:2410.20290. Qiushi Sun, Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu, and Lingpeng Kong. 2023. Corex: Pushing the boundaries of complex reasoning through multi-model collaboration. arXiv preprint arXiv:2310.00280. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie family of Millican, et al. 2023. highly capable multimodal models. arXiv preprint arXiv:2312.11805. Gemini: Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. 2025. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599. Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024. Math-shepherd: Verify and reinforce llms stepby-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for ComputatDo NOT Think That Much for 2+3=? On the Overthinking of o1-Like LLMsional Linguistics (Volume 1: Long Papers), pages 94269439. Xuezhi Wang and Denny Zhou. 2024. Chain-ofthought reasoning without prompting. arXiv preprint arXiv:2402.10200. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837."
        },
        {
            "title": "A Implementation Details",
            "content": "B Algorithm of ϕ-Decoding A.1 Calculation of FLOPS Following (Kaplan et al., 2020), we calculate the inference-time FLOPS to measure the computational efficiency. The definition of the metric FLOPS is: FLOPS 6nP (12) where represents the total number of the output tokens, and is the number of parameters of the LLM. In the tables above, we report the average results of FLOPS across the benchmarks. A.2 Inference Setup We provide the detailed implementation setup in Table 5. Considering the huge cost ahead, the hyperparameters are merely searched within very small range. We leave it for future works to derive the optimal experimental configuration. Task Hyper-Parameter Setup LLaMA3.1-8B-Instruct =4 =4 GSM8K MATH-500 =4 =4 =4 =4 GPQA =4 =4 ReClor =4 =4 LogiQA =4 =4 ARC-C AIME2024 =3 =2 K=3 (Tmin,Tmax)=(4,8) K=3 (Tmin,Tmax)=(4,8) K=3 (Tmin,Tmax)=(1,8) K=3 (Tmin,Tmax)=(4,8) K=3 (Tmin,Tmax)=(4,8) (Tmin,Tmax)=(4,8) K=3 (Tmin,Tmax)=(32,64) K=3 Mistralv0.3-7B-Instruct =4 =4 GSM8K MATH-500 =4 =4 =4 =4 GPQA =4 =4 ReClor =4 =4 LogiQA =4 =4 ARC-C (Tmin,Tmax)=(2,8) (Tmin,Tmax)=(1,8) (Tmin,Tmax)=(1,8) (Tmin,Tmax)=(2,8) (Tmin,Tmax)=(2,8) (Tmin,Tmax)=(2,8) Qwen2.5-3B-Instruct =4 =4 GSM8K MATH-500 =4 =4 =4 =4 GPQA =4 =4 ReClor =4 =4 LogiQA =4 =4 ARC-C (Tmin,Tmax)=(4,8) (Tmin,Tmax)=(4,8) (Tmin,Tmax)=(3,8) (Tmin,Tmax)=(4,8) (Tmin,Tmax)=(4,8) (Tmin,Tmax)=(4,8) LLaMA3.1-70B-Instruct =4 =4 GSM8K MATH-500 =4 =4 =4 =4 ReClor =4 =4 LogiQA (Tmin,Tmax)=(7,8) (Tmin,Tmax)=(3,8) (Tmin,Tmax)=(2,8) (Tmin,Tmax)=(6,8) DeepSeek R1-Distill-LLaMA-8B K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 K=3 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0.7 δ=0. δ=0.7 δ=0.7 δ=0.7 δ=0.7 AIME2024 =4 =4 (Tmin,Tmax)=(16,32) K=3 δ=0.7 Table 5: Experimental setup of ϕ-Decoding. denotes the step beam size. is the number of rollouts for each step beam. Tmin and Tmax represent the least and the most foresight step number respectively. is the number of clusters while δ means the early-stopping threshold using clustering. The pseudo code of ϕ-Decoding is presented in Algorithm 1. To make high-level overview of ϕDecoding, we also provide the pipeline in Figure 6. Figure 6: Overall pipeline of ϕ-Decoding."
        },
        {
            "title": "C Generalization to Smaller LLMs",
            "content": "Besides the generalization to 70B-sized backbone, we also supplement the evaluations on 3B-sized model. Table 8 presents the performances on Qwen2.5-3B-Instruct model. Compared with the auto-regressive chain-ofthought baseline, ϕ-Decoding provides obvious performance gains across 6 reasoning benchmarks, improving the average performance by 3.80%."
        },
        {
            "title": "D Accuracy of Step Value Estimation",
            "content": "To measure whether the estimated step value aligns with the actual rewards, we conduct the analysis Algorithm 1 ϕ-Decoding Input: Input query x, LLM πθ, step beam size , number of rollouts on each beam , minimum and maximum number of step foresight Tmin and Tmin, number of clusters K, early-stopping threshold δ. Output: Step sequence. for = 1, 2, . . . , Tmax do Step Rollout (In Parallel) for = 1, ..., do for = 1, 2, ..., do Sample single step a(m,n) , s(m,n) pθ(x, a(m) <t ) end for end for In-Width Pruning (filter erroneous candidates) Derive mean µt and variance σ2 Prune steps and keep the remaining ones for foresight: St {a(m,n) Step Foresight (In Parallel) for each a(m,n) from these step confidence st in St do Derive foresight steps and foresight scores: a(m,n) , (m,n) pθ(x, a(m,n) ), >t µt σt s(m,n) } end for Step Value Esitimation (In Parallel) for = 1, 2, . . . , St do m, the superscript of ith candidate in St Derive Advantage via of adjacent steps: A(m,n) Derive Alignment via clustering: C(m,n) Combine Advantage and Alignment: R(x, a(m,n) wi exp (m,n) Cluster({a(m,n) }) , a(m,n) >t R(x, a(m,n) , a(m,n) >t )/τ (cid:104) (cid:105) t (m) t1 >t ) Norm(A(m,n) ) + Norm(C(m,n) ) end for Sample Steps for = 1, 2, ..., do Sample without replace: Categorical({ wi(cid:80) Sampled step: a(m) St[i] wj end for In-Depth Pruning (early-stop) break if Tmin and EarlyStop(δ) is True; }St i=1 ) end for Complete all candidates at the last foresight step and sample only one based on the function. Return Step sequence. Cluster Methods GSM8K Math-500 GPQA ReClor LogiQA ARC-c Avg. FLOPS ϕ-Decoding (LLaMA3.1-8B-Instruct) TF-IDF SBERT (109M) SBERT (22.7M) 86.58 86.43 86.05 38.20 39.20 36.80 34.60 33.26 33.26 64.00 63.20 62.40 48.39 47.48 45.47 85.41 85.41 85. 59.53 59.16 58.23 6.43 1017 6.52 1017 6.61 1017 Table 6: Variants of cluster strategies. σ GSM8K Math-500 GPQA ReClor LogiQA ARC-c Avg. FLOPS ϕ-Decoding (LLaMA3.1-8B-Instruct) 3 2 4 0.7 0.8 0.5 86.58 85.52 83.93 38.20 39.40 38. 34.60 33.04 32.37 64.00 64.20 64.00 48.39 46.85 43.78 85.41 85.41 84.81 59.53 59.07 57.85 6.43 1017 6.26 1017 6.15 Table 7: Various setups of cluster. Tasks AR(CoT) ϕ-Decoding In-depth Analysis of Cluster Strategies Qwen2.5-3B-Instruct E.1 Variants of Cluster In the main experiments, we implement the cluster strategy with TF-IDF, which is from the syntax perspective. It can also be replaced with sentenceBERT (SBERT) (Reimers and Gurevych, 2019) to obtain the sentence embedding for clustering. Table 6 presents the comparisons between different cluster strategies. SBERT (109M) employs the pretrained sentence embedding model of multi-qa-mpnet-base-dot-v1, while SBERT (22.7M) utilizes the model of all-MiniLM-L6-v2. From the results, clustering with the external embedding model can also lead to similar competitive performances, slightly lower than the TF-IDF strategy. Also, it is observed that increasing the size of the sentence embedding models can bring improvements in the average performances. E.2 Hyperparameters of Cluster Table 7 offers the analysis on different hyperparameters. We keep the other configuration fixed for fair comparison, where =4 and =4. Under this setting, the maximum number of foresight paths for clustering is 16. Based on the results, the cluster size K=2 or 3 would be good choices. With increasing, it may bring much uncertainty. GSM8K MATH-500 GPQA ReClor LogiQA ARC-C Avg. 78.62 41.00 28.57 53.60 42.70 77.47 53.66 85.60 45.20 28.79 59.40 46.08 79.69 57. +6.98 +4.20 +0.22 +5.80 +3.38 +2.22 +3.80 Table 8: Generalization to smaller backbone Qwen2.53B-Instruct. in Sec. 4.3. At each timestamp t, we can derive the value estimation of the candidate steps via the decoding strategy. These step values can approximate distribution P1. Meanwhile, we can derive the explicit outcome of each candidate step using the foresight paths. Comparing the outcome with ground-truth, the outcome accuracy for these candidate steps can also form distribution P2, where P1 = P2. We derive the distribution matching as the accuracy of step value estimation: Accuracy = 1 (cid:80)P1 i=1 (P1(i) P2(i))2 P1 (13) where P1(i) P1, P2(i) P2. In the implementation of P1, we use the model estimated step values for sampling-based methods (ϕ-Decoding and Predictive Decoding). For auto-regressive and ToT methods, we allocate binary rewards for the selected steps (rewarded as 1) and other candidates (rewarded as 0). The final accuracy score is calculated by averaging the results on each timestamp."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Peking University",
        "Shanghai AI Lab",
        "The University of Hong Kong",
        "Xian Jiaotong University"
    ]
}