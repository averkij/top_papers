{
    "paper_title": "MEXA: Multilingual Evaluation of English-Centric LLMs via Cross-Lingual Alignment",
    "authors": [
        "Amir Hossein Kargaran",
        "Ali Modarressi",
        "Nafiseh Nikeghbal",
        "Jana Diesner",
        "François Yvon",
        "Hinrich Schütze"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "English-centric large language models (LLMs) often show strong multilingual capabilities. However, the multilingual performance of these models remains unclear and is not thoroughly evaluated for many languages. Most benchmarks for multilinguality focus on classic NLP tasks, or cover a minimal number of languages. We introduce MEXA, a method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages the fact that English-centric LLMs use English as a kind of pivot language in their intermediate layers. It computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in other languages. We conduct studies using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves a statistically significant average Pearson correlation of 0.90 with three established downstream tasks across nine models and two parallel datasets. This suggests that MEXA is a reliable method for estimating the multilingual capabilities of English-centric LLMs, providing a clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard: https://huggingface.co/spaces/cis-lmu/Mexa, Code: https://github.com/cisnlp/Mexa."
        },
        {
            "title": "Start",
            "content": "Preprint MEXA: MULTILINGUAL EVALUATION OF ENGLISHCENTRIC LLMS VIA CROSS-LINGUAL ALIGNMENT Amir Hossein Kargaran1 Ali Modarressi1 Nafiseh Nikeghbal2 Jana Diesner2,3 Francois Yvon4 Hinrich Sch utze1 1LMU Munich & Munich Center for Machine Learning 3University of Illinois Urbana-Champaign {amir,ali}@cis.lmu.de 2Technical University of Munich 4Sorbonne Universite & CNRS, ISIR 4 2 0 2 8 ] . [ 1 3 7 8 5 0 . 0 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "English-centric large language models (LLMs) often show strong multilingual capabilities. However, the multilingual performance of these models remains unclear and is not thoroughly evaluated for many languages. Most benchmarks for multilinguality focus on classic NLP tasks, or cover minimal number of languages. We introduce MEXA, method for assessing the multilingual capabilities of pre-trained English-centric LLMs using parallel sentences, which are available for more languages than existing downstream tasks. MEXA leverages the fact that English-centric LLMs use English as kind of pivot language in their intermediate layers. It computes the alignment between English and non-English languages using parallel sentences to evaluate the transfer of language understanding from English to other languages. This alignment can be used to estimate model performance in other languages. We conduct studies using various parallel datasets (FLORES-200 and Bible), models (Llama family, Gemma family, Mistral, and OLMo), and established downstream tasks (Belebele, m-MMLU, and m-ARC). We explore different methods to compute embeddings in decoder-only models. Our results show that MEXA, in its default settings, achieves statistically significant average Pearson correlation of 0.90 with three established downstream tasks across nine models and two parallel datasets. This suggests that MEXA is reliable method for estimating the multilingual capabilities of English-centric LLMs, providing clearer understanding of their multilingual potential and the inner workings of LLMs. Leaderboard Code hf.co/spaces/cis-lmu/Mexa github.com/cisnlp/Mexa"
        },
        {
            "title": "INTRODUCTION",
            "content": "Most state-of-the-art autoregressive large language models (LLMs) are English-centric, closedsource models such as Claude 3 Opus, GPT-4, and Gemini Pro (Anthropic, 2023; OpenAI et al., 2023; Gemini Team et al., 2023); open-weight models such as Llama 3.1, Gemma 2, and Mixtral (Dubey et al., 2024; Gemma Team et al., 2024b; Jiang et al., 2024); and open-source models such as OLMo (Groeneveld et al., 2024). Except for open-source models, where the data is available and thus the language distribution is transparent, there is confusion regarding the capabilities/language distribution of these LLMs in other languages. Primarily, the focus in evaluating LLMs has been on developing benchmarks to assess their performance in English. Most benchmarks in multilingual setups consist of classical monolingual NLP tasks such as sequence labeling (Ahuja et al., 2023; Lai et al., 2023a), the automatic translation of popular English benchmarks such as MMLU (Hendrycks et al., 2021) into limited number of languages (Lai et al., 2023b; OpenAI, 2024), or language-specific benchmarks for languages such as Persian (Ghahroodi et al., 2024), Arabic (Koto et al., 2024), Korean (Son et al., 2024), Turkish (Yuksel et al., 2024), and Chinese (Li et al., 2024c). 1 Preprint Most LLMs are English-centric, either by choice or due to the availability of abundant data sources in English. Either way, for these models to be effective in other languages, it is important that the other languages align with the main language, i.e., English. Given such alignment, English could act as rising tide that raises all ships, meaning that improvements in English performance could benefit other languages, especially in tasks such as reasoning (Zhu et al., 2024). Contrarily, if language does not align well with English, an English-centric LLM may not provide meaningful coverage for that language. Indeed, Wendler et al. (2024) have found that for Llama 2 (Touvron et al., 2023b), an English-centric LLM, English could be seen as kind of pivot language, enabling to solve complex semantic tasks in foreign language through detour into English. More precisely, they show that Llama 2 was able to decode semantically correct next tokens in the middle layers, assigning higher probabilities to the English tokens than to the foreign version, which is only selected in the upper layers. Zhao et al. (2024) present hypothesis regarding the middle layers of English-centric LLMs, suggesting that these models use English as means of reasoning while incorporating multilingual knowledge. Based on their analysis, the number of language-specific neurons in the middle layers decreases within the self-attention mechanism but remains consistent across the layers of the feedforward structure when processing multilingual queries. In this paper, we introduce MEXA, method that uses the observation that English-centric LLMs semantically use English as kind of pivot language in their middle layers to evaluate the actual multilingual coverage of LLMs. This is done by measuring how well the embeddings of parallel sentences in the middle layers of LLMs for non-English languages are aligned with the embedding of their corresponding English. We extensively verify the MEXA estimation of language coverage for each LLM, using Pearson correlation between estimated and actual scores for various tasks. We use two parallel datasets: FLORES-200 (NLLB Team et al., 2022) and Bible (Mayer & Cysouw, 2014); nine LLMs: Llama family, Gemma family, Mistral, and OLMo; and three tasks: Belebele (Bandarkar et al., 2024), m-MMLU, and m-ARC (Lai et al., 2023b). Our results show that MEXA achieves promising average Pearson correlation of 0.90 with established downstream tasks across nine models and two parallel datasets. In our study on the calculation of MEXA scores, we conduct multiple design analyses to examine the impact of token-level pooling for the embeddings (i.e., using the last token versus weighted average) and layer-level pooling in computing alignment scores. While MEXA demonstrates high correlation across most setups, we find that weighted average based on tokens, combined with mean pooling, yields the best results."
        },
        {
            "title": "2 BACKGROUND AND RELATED WORK",
            "content": "We discuss the distribution of pre-training data in LLMs and multilingual evaluation benchmarks in Appendices A.1 and A.2 while focusing on cross-lingual alignment here. Research in the crosslingual alignment field either aims to uncover the underlying mechanisms of alignment and assess its impact on models and downstream tasks, or attempts to enhance model performance by enforcing alignment before, during, or after the pre-training phase. Most of these papers have focused on encoder-only models, such as XLM-R (Conneau et al., 2020a) and mBERT (Devlin et al., 2019), among others (Hammerl et al., 2024). In this work, we focus primarily on decoder-only models. Understanding Alignment. Ye et al. (2023) show that English-centric models such as Llama 1 (Touvron et al., 2023a) not only possess multilingual transfer abilities (after fine-tuning on one source language, they can be applied to other languages) but may even surpass the multilingual transfer abilities of multilingual pre-trained models such as BLOOM (BigScience Workshop et al., 2023). Schafer et al. (2024) find that GPT-2-style decoder-only models show strong cross-lingual generalization when trained on an imbalanced mix of languages. However, when trained on balanced language set, they do not observe increased performance compared to monolingual settings. Wendler et al. (2024) perform single-token analysis to demonstrate that English-centered LLMs, such as Llama 2, use English semantically as an internal latent language in the middle layers when handling multilingual queries. Zhong et al. (2024) extend this analysis to multiple tokens, also showing that an LLM dominated by both English and Japanese uses both languages as internal latent languages. Zhao et al. (2024) explore how LLMs handle multilingualism. They hypothesize that LLMs initially interpret the query, converting multilingual inputs into English for task-solving. In the middle layers, the models rely on English with self-attention mechanisms for reasoning, while employing multilingual knowledge through feed-forward structures. In the final layers, LLMs generate responses consistent with the original query language. Li et al. (2024f) and Li et al. (2024b) 2 Preprint are even more closely related to ours. Li et al. (2024f) employs absolute cosine similarity values between last token embeddings derived from parallel sentences with English to predict the ranking of language performance across various models. However, as we discuss in Section 3, relying solely on absolute cosine values can be misleading, and as shown in Section 5.3, absolute cosine values are less correlated with downstream tasks than MEXA score. Li et al. (2024b) uses English probing tasks and their automatic translations to construct multilingual evaluation. While they compare embedding similarity scores between highand low-resource languages with corresponding evaluation results, similar to Li et al. (2024f), they do not assess whether these correlations hold across other downstream tasks. In Section 5, we demonstrate that MEXA scores align closely with broad range of downstream tasks. Boosting Alignment. The idea to enforce alignment in encoder-only models using parallel sentences dates back to Conneau & Lample (2019), and has been explored under various guises e.g., using mixed-language sentences and/or bilingual dictionaries (Huang et al., 2019; Conneau et al., 2020b; Cao et al., 2020; Kulshreshtha et al., 2020; Efimov et al., 2023; Zhang et al., 2023b). Recently, Li et al. (2024d) improve multilingual alignment by initializing the decoder-only models to generate similar representations of aligned words using contrastive learning and preserves this alignment using code-switching strategy during pretraining. Liu et al. (2024a) propose data allocation technique to select core subset of languages for fine-tuning, better aligning the multilingual capabilities of decoder-only LLMs and making them more truthful in their responses. Li et al. (2024a) propose aligning internal sentence representations across different languages using multilingual contrastive learning and aligning outputs by following cross-lingual instructions in the target language for decoder-only models."
        },
        {
            "title": "3 MEXA",
            "content": "We now describe the MEXA method for computing the alignment score of language L1 with respect to the pivot language L2, given the language model m. In this paper, we use the term cross-lingual alignment, geometric alignment, or simply alignment to refer to the semantic similarity of multilingual embeddings across languages. L2, for English-centric LLMs and in this paper, is English. To assess alignment, we use parallel sentences in two languages, L1 and L2. What defines semantic similarity in multilingual embeddings across languages? The goal of semantic similarity is to ensure that parallel sentences have sufficiently high similarity, reflecting alignment between the two languages. However, considering only the absolute cosine similarity value as the alignment score does not guarantee proper alignment. For some languages, even non-parallel sentences exhibit similarity scores comparable to those of parallel sentences (see 5.3). This is largely due to the anisotropy problem observed in Transformer models, which can lead to so-called hubness issues, making it difficult to distinguish between similar and dissimilar embeddings (Ethayarajh, 2019), especially in multilingual models (Hammerl et al., 2023; Rajaee & Pilehvar, 2022). However, direct comparative analysis of the cosine similarity between parallel and non-parallel sentence pairs across languages can help overcome these issues. Instead of using the absolute cosine similarity value for alignment, we assign binary values (1 or 0) based on whether criterion for semantic similarity is satisfied. This criterion imposes that (a) parallel sentences should have high cosine similarity, and (b) non-parallel pairs should also have low similarity values, ensuring the similarity is not random or biased. Specifically, if the cosine similarity for pair of parallel sentences is higher than for any non-parallel sentences, we assign value of 1 for this pair; otherwise, value of 0. This approach sidesteps the hubness problem since the absolute cosine similarity values themselves are not directly used. To compute MEXA, we first apply the cosine similarity function to the pairs of embeddings of parallel sentences in languages L1 and L2. In Section 3.1, we describe how embeddings can be computed for each layer of the autoregressive language model m. We generate square matrix C(L1, L2, m, l) representing cosine similarities of embeddings at the output of layer for all parallel sentences in languages L1 and L2. We denote cij(l) the element in the i-th row and j-th column of C(L1, L2, m, l). It represents the cosine similarity between the i-th sentence of L1 and the j-th sentence of L2 at layer of language model m. The diagonal elements of C, denoted cii(l), represent the cosine similarity between parallel sentence pairs from L1 and L2. We define the MEXA alignment score (µ(.)) as follows: 3 Preprint µ(cid:0)C(L1, L2, m, l)(cid:1) = 1 (cid:88) i=1 (cid:18) (cid:19) cii(l) > max j{1,...,n}{i} {cij(l), cji(l)} (1) where is the number of diagonal elements (i.e., the dimension of the matrix), and 1() is the indicator function, which equals 1 if its argument condition evaluates to true and 0 otherwise. This alignment score measures how often cii(l) is the maximum value in both its row and column. The MEXA alignment score can alternatively be understood as measure of sentence retrieval performance (Hu et al., 2020; Liu et al., 2024b; Hammerl et al., 2024), with the metric of P@1 applied with queries in language L1 and answers in L2, and vice versa. We discuss other ways to calculate semantic similarity between languages in Appendix A.3. Layer-wise Pooling. The MEXA alignment score µ(cid:0)C(L1, L2, m, l)(cid:1) is computed for language L1 respect to pivot language L2 for each layer of the language model m. To compute single MEXA alignment score given the language model and L1, L2, we use mean and max pooling strategies over multiple layers. 3.1 SENTENCE EMBEDDINGS Sentence embeddings are transformation of sentence into fixed-size vector that captures its meaning. The process of computing sentence embeddings can vary depending on the model architecture. Typically, sentence embeddings are used in encoder-based models such as BERT, which employ bidirectional attention. In these models, the hidden states of each token are first extracted, then aggregated, commonly by averaging the hidden states from the output layer. Since attention in these models is bidirectional, each token contributes equally to the final embedding. Alternatively one can use the output of the special [CLS] token as per the original BERT work (Devlin et al., 2019). In this paper, we focus on autoregressive language models that use decoder-only architecture. In this architecture, attention is not bidirectional; instead, it takes the form of causal attention (leftto-right). In bidirectional attention, each token has access to every other token in the sequence. However, in causal attention, the embedding of token at position is only influenced by the embedding of preceding tokens at positions 0, 1, . . . , 1. Therefore, simple averaging biases the embeddings towards sentence-initial words. Instead, two alternative methods are considered: using only the last token and weighted averaging. We use and compare both methods to acquire the sentence embeddings needed for MEXA. standard way to compute sentence embedding uses only the last token of that sentence. Jiang et al. (2023b) show that using the last token in the format of prompt template for sentence s, such as This sentence: {s} means in one word:, can be effective. Inspired by this, Li & Li (2024) used the prompt Summarize sentence {s} in one word: to obtain the last token embedding as the sentence-level text embedding. However, not all models are instruction-tuned; some earlier works, such as Neelakantan et al. (2022); Wang et al. (2024); Ma et al. (2024), use the last token without any prompt. Since the models studied in this paper are only pre-trained and use multiple languages in the input, we decided to use the last token method without any preceding instruction. An alternative is weighted averaging. It relies on the intuition that using only the last token might not represent the entire sentence, as the influence of earlier tokens may have diminished. This suggests that tokens at the end of the sentence should contribute more to the overall embedding than those at the beginning. Another motivation is that sentence-final tokens are influenced by preceding tokens and contain more context, while the representation of sentence-initial tokens has significantly less contextual representation. To address this, Muennighoff (2022) proposes to assign weights to each token based on its position. Thus, the sentence embedding of layer using position-weighted averaging is: el = (cid:88) t= wthlt with wt = (cid:80)T k=1 (2) where is the number of tokens in the given sentence, hlt is the embedding of the t-th token at layer l, and el is the sentence embedding at layer l. 4 Preprint"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We conduct experiments using various multi-parallel datasets (FLORES-200 and the Bible), models (Llama family, Gemma family, Mistral, and OLMo), and existing benchmarks/tasks (Belebele, mMMLU, m-ARC). Our objective is to assess how well the MEXA alignment score from various parallel datasets correlates with the different benchmarks/tasks for different models. 4.1 PARALLEL DATA We calculate the MEXA score using the parallel datasets of FLORES-200 (NLLB Team et al., 2022) and the Bible (Mayer & Cysouw, 2014). While there are other high-quality parallel datasets, such as NTREX-128 (Federmann et al., 2022), IN22 (Gala et al., 2023), OPUS-100 (Zhang et al., 2020), Europarl (Koehn, 2005), OpenSubtitles (Lison & Tiedemann, 2016), TED2020 (Reimers & Gurevych, 2020), and Tatoeba (Tatoeba Community, 2006), we specifically chose FLORES-200 due to its high quality and support for wide range of languages, while the Bible dataset was selected for its extensive language coverage. FLORES-200 is parallel corpus, where the English subset is sourced from Wikimedia projects. Each English sentence has been translated into 204 distinct language-script combinations, these translations have been verified by humans. The dataset contains 997 sentences for development, 1012 sentences for dev-test, and 992 sentences for testing. As the FLORES-200 test set is not publicly accessible, we use the dev-test set as our FLORES parallel test corpus, in line with previous studies. For faster computation, we only consider the first 100 sentences from each language. As shown in Appendix A.4, the odds of the MEXA score randomly achieving high value with 100 sentences are very slim. The Parallel Bible (Mayer & Cysouw, 2014) covers very large number of languages. From this resource, we managed to create subcorpus, super parallel dataset of the Bible, with 1,401 languagescript labels, each containing 103 sentences (i.e., Bible verses).1 This corpus includes many lowresource languages, many of which are not covered by existing language technologies (Joshi et al., 2020), and MEXA can be adopted since only parallel data is needed. We use all the 103 sentences from each language. 4.2 MODELS For our experiments, we select models with around 7B parameters, which are considered base size in the LLM community. The state-of-the-art open-weight models in this range include Llama 1, 2, 3, and 3.1 (Touvron et al., 2023a;b; Meta, 2024; Dubey et al., 2024), Gemma 1 and 2 (Gemma Team et al., 2024a;b), Mistral 0.3 (Jiang et al., 2023a), and the open-source model OLMo 1.7 (Groeneveld et al., 2024). We also select larger model, Llama 3.1 70B, to show that our findings hold even when scaling further. To apply MEXA, we need to access model weights to compute input sentence embeddings for each layer. We use three popular open-weight model families: Llama, Gemma, and Mistral. As less multilingual version of state-of-the-art LLMs, we include OLMo, which is trained on more English-centric corpus of Dolma (Soldaini et al., 2024). Although there are multilingual models such as PolyLM (with support of 18 languages), XGLM (Lin et al., 2022) (with support of 30 languages) and BLOOM (BigScience Workshop et al., 2023) (with support of 46 languages) our focus here is on LLMs which are state-of-the-art in English based tasks such as MMLU (Stanford CRFM, 2024). 4.3 BENCHMARKS Among the existing evaluation benchmarks in Table 4 from Appendix A.2, we chose the Belebele benchmark (Bandarkar et al., 2024), m-ARC (Lai et al., 2023b), and m-MMLU (Lai et al., 2023b), which support the highest number of high-, medium-, and low-resource languages and are directly related to natural understanding tasks, which is the primary focus of this paper. Belebele is multiple-choice reading comprehension task designed to assess language models across range of high-, medium-, and low-resource languages. Each question in the dataset is accompa1hf.co/datasets/cis-lmu/sPBC Preprint Gemma 2 9B Gemma 1 7B Llama 3.1 70B Llama 3.1 8B Llama 3 8B Llama 2 7B Task{eng} Task L{eng} FLORES Bible Belebele m-MMLU m-ARC Belebele (avg., = 116) m-MMLU (avg., = 33) m-ARC (avg., = 31) µMean (avg., = 116) µMax (avg., = 116) µMean (avg., = 101) µMax (avg., = 101) 0.9178 0.6998 0.6775 0.7093 0.5582 0.4779 0.5088 0.7194 0.3568 0.6076 0.8467 0.6138 0. 0.5633 0.4734 0.4220 0.3815 0.5872 0.2152 0.4021 0.9456 0.7700 0.7014 0.7684 0.6384 0.5054 0.4110 0.7725 0.3169 0.6599 0.8767 0.6315 0. 0.5705 0.4720 0.3941 0.3963 0.6538 0.2103 0.4212 0.8689 0.6294 0.5836 0.5533 0.4664 0.3892 0.3939 0.6520 0.2026 0.4190 0.4822 0.4523 0. 0.3028 0.3260 0.3174 0.0866 0.2464 0.1246 0.2724 Llama 1 Mistral 0.3 7B 0.4156 0.3569 0.5000 0.2755 0.2807 0. 0.1946 0.3579 0.0908 0.2357 7B 0.8389 0.5988 0.5862 0.4457 0.4207 0.3662 0.2642 0.4716 0.1198 0.2606 OLMo 1.7 7B 0.7711 0.5210 0.4872 0.3627 0.3390 0.2731 0.0413 0.1965 0.0121 0.0319 AVG 0.7737 0.5859 0.5795 0.5057 0.4416 0. 0.2976 0.5175 0.1832 0.3678 Table 1: µpooling indicates the MEXA score for each corresponding pooling strategy. The embeddings are computed using weighted average based on token positions (Eq. 2). Top values are in bold, with second-best underlined. nied by four possible answers and is linked to brief passage from the FLORES-200 dataset (NLLB Team et al., 2022). The human annotation process was carefully curated to generate questions that effectively differentiate between various levels of language comprehension, supported by rigorous quality checks. Belebele supports 122 distinct labels (language-script combinations) corresponding to 115 distinct languages. However, FLORES-200 does not support 5 of these labels, corresponding to Romanized versions of 5 Indic languages. Therefore, we conducted our analysis between the FLORES-200 and Belebele benchmarks on 117 common labels. Additionally, there are 102 common labels between the Bible parallel data and Belebele benchmark. Both ARC Challenge (Clark et al., 2018) and MMLU (Hendrycks et al., 2021) are also set up as multiple-choice question-answering tasks, but they focus on different types of knowledge and reasoning skills. ARC Challenge is classified as common-sense reasoning task, consisting of gradeschool level science questions, while MMLU consists of questions across wide range of subjects, including humanities, social sciences, and more. Lai et al. (2023b) used GPT-3.5-turbo (OpenAI, 2022) and translation prompt to translate examples from both datasets and create m-ARC and m-MMLU in 31 languages (excluding English). Later, m-MMLU was expanded to also include Icelandic (isl Latn) and Norwegian (nob Latn). The Icelandic portion was translated using the Mideind.is, while Norwegian was generated with DeepL.com.2 m-MMLU consists of 277 questions in its training set, 13,258 in the test set and 1,433 in the validation set. m-ARC consists of 1,116 questions in the training set, 1,169 in the test set, and 298 in the validation set. We use the entire test set for each of these benchmarks to evaluate the models, except in one case. For Llama 3.1 70B, we use the first 500 questions of m-MMLU instead of the whole set due to resource constraints. Since the selected LLMs used in our experiment are not instruction-tuned, we use 5shot in-context learning with the lm-evaluation-harness framework, employing log-likelihood-based multiple-choice scoring. Other settings, such as prompt templates, are configured according to the frameworks default (Gao et al., 2023; Biderman et al., 2024). 4.4 EVALUATION MEASURES We use Pearson correlation to assess the strength of the correlation between MEXA and downstream performance on our evaluation benchmarks. Pearson correlation is statistical measure that calculates the strength and direction of the linear relationship between two variables. high Pearson correlation would indicate that MEXA provides reliable assessment of multilingual capabilities in English-centric LLMs."
        },
        {
            "title": "5 RESULTS",
            "content": "Table 1 presents the downstream performance of the selected models across three benchmarks, along with MEXA scores from two parallel datasets. Notably, among models with parameter sizes ranging from 7 to 9 billion, both Gemma 2 and Llama 3.1 outperform the others in terms of non-English downstream performance and MEXA scores. The Llama 3.1 and Llama 3 models exhibit similar alignment and downstream task performance; yet, both represent substantial advancements compared to Llama 2. Moreover, results for the Llama 3.1-70B model indicate that scaling can sig2hf.co/datasets/alexandrainst/m_mmlu 6 Preprint Gemma 2 9B Gemma 1 7B Llama 3.1 70B Llama 3.1 8B Llama 3 8B Llama 2 7B r d g Sw L k s g v t e b e t a ρ (µMean, Belebele) ρ (µMax, Belebele) ρ (µMean, m-MMLU) ρ (µMax, m-MMLU) ρ (µMean, m-ARC) ρ (µMax, m-ARC) ρ (µMean, Belebele) ρ (µMax, Belebele) ρ (µMean, m-MMLU) ρ (µMax, m-MMLU) ρ (µMean, m-ARC) ρ (µMax, m-ARC) ρ (µMean, Belebele) ρ (µMax, Belebele) ρ (µMean, m-MMLU) ρ (µMax, m-MMLU) ρ (µMean, m-ARC) ρ (µMax, m-ARC) ρ (µMean, Belebele) ρ (µMax, Belebele) ρ (µMean, m-MMLU) ρ (µMax, m-MMLU) ρ (µMean, m-ARC) ρ (µMax, m-ARC) 0.9247 0.9623 0.9342 0.9060 0.9741 0. 0.8997 0.9225 0.9086 0.8448 0.9190 0.8569 0.8360 0.8863 0.8051 0.5501 0.8505 0.6070 0.7656 0.7844 0.7194 0.7075 0.7411 0.7293 0.9421 0.9676 0.9697 0.9596 0.9706 0.9499 0.9326 0.9309 0.9637 0.9297 0.9541 0.9147 0.8530 0.9001 0.8886 0.8831 0.8998 0. 0.8005 0.8299 0.7646 0.6886 0.7754 0.7000 0.8291 0.9211 0.9362 0.8946 0.9374 0.8736 0.8491 0.9127 0.9370 0.8645 0.9524 0.9005 0.7909 0.8851 0.8958 0.7748 0.9188 0.8030 0.9478 0.9392 0.9689 0.9003 0.9515 0.8582 0.9494 0.9244 0.9687 0.9224 0.9536 0. 0.8781 0.9242 0.9096 0.8683 0.9267 0.8769 0.9588 0.9326 0.9647 0.8892 0.9562 0.8663 0.9581 0.9123 0.9690 0.9177 0.9617 0.8879 0.8974 0.9302 0.8964 0.8364 0.9116 0.8552 0.5944 0.5264 0.6472 0.5037** 0.6592 0.5190** 0.7934 0.8000 0.6068 0.5228** 0.5976 0.5335** 0.8396 0.8100 0.6516 0.4461** 0.6494 0.4853** 0.8364 0.8362 0.9223 0.9386 0.9052 0.9297 0.9141 0.9125 0.9771 0.9699 0.9390 0.9464 0.8982 0.8926 0.9252 0.9180 0.8940 0.8684 0.9046 0.9047 0.8827 0.9079 0.8537 0.8494 Llama 1 Mistral 0. 7B 0.8404 0.7649 0.9406 0.8936 0.9268 0.8439 0.8340 0.7693 0.9301 0.8902 0.9146 0.8263 0.8404 0.8230 0.9159 0.9085 0.9208 0.8879 0.8299 0.8048 0.8692 0.8576 0.8537 0.8309 7B 0.9732 0.9448 0.9857 0.9311 0.9693 0.9001 0.9679 0.9460 0.9659 0.9161 0.9451 0.8859 0.9118 0.9337 0.9093 0.9107 0.9317 0.9178 0.9177 0.9235 0.8672 0.8643 0.8927 0.8624 OLMo 1.7 7B 0.8425 0.9198 0.9393 0.9565 0.8630 0. 0.9467 0.9218 0.9700 0.9649 0.7356 0.7037 0.7410 0.7549 0.7944 0.7388 0.8623 0.8220 0.8866 0.8796 0.8060 0.7994 0.6997 0.6867 AVG 0.8994 0.9098 0.9513 0.9188 0.9393 0.8856 0.9168 0.9058 0.9545 0.9134 0.9195 0. 0.8496 0.8811 0.8823 0.8210 0.9018 0.8354 0.8147 0.8070 0.7572 0.6998 0.7469 0.6885 Table 2: Pearson correlation of MEXA using FLORES and Bible data across three tasks. ρ (µPooling, Task) is the correlation of MEXA for the corresponding pooling strategy and benchmark. In all settings except **, the p-value is < 0.001. The best average correlations for each task are in bold, and the second bests are underlined. nificantly enhance alignment when compared to its smaller version. Interestingly, while Mistral achieves comparable results to Gemma 1 on English benchmarks, it demonstrates inferior alignment, which likely accounts for its reduced performance on non-English tasks. Furthermore, the Llama 2 model achieves higher MEXA scores than OLMo, indicating better alignment. However, due to Llama 2s weaker performance on English tasks, it fails to transfer this alignment effectively, leading to comparable non-English performance between Llama 2 and OLMo. This observation is further explored in Section 5.2, where we normalize the expected performance based on the pivot language, namely English. 5.1 MEXA CORRELATION ANALYSIS We compute sentence embeddings for the selected models using two methods: weighted average based on token positions and last token (see 3.1). We apply mean and max pooling on the MEXA alignment scores across all model layers to derive single score for each language. In Table 2, we report the correlation between the MEXA scores (computed using both meanand max-pooling, for the two embedding methods) and task performances. Across all settings, the best overall result (higher correlation) is achieved when embeddings are computed using the weighted average, with mean pooling as the pooling method. We adopt this configuration as the default setting for MEXA. FLORES vs Bible. In the default setting, the average Pearson correlation score for the FLORES parallel dataset across different tasks is 0.9300, while for the Bible parallel dataset, it is 0.8779. The reason the Bible scores are generally lower than FLORES is that FLORES data is cleaner and more aligned with modern, standardized texts, whereas the Bible data is older and more specialized. For example for some languages, the orthography of Bible texts no longer matches todays orthography. In the Bible, Arabic often includes diacritics, which are typically omitted in modern writing and tasks, making the text less familiar to models trained on contemporary data. Additionally, although the Bible dataset has been made parallel, sentence alignment can still be inconsistent due to translation nuances. In contrast, FLORES is carefully curated to ensure high-quality, sentence-level parallelism across languages for machine translation tasks. Weighted Average vs. Last Token Embeddings. The use of last token embeddings shows promisingly high correlations with the FLORES parallel data; however, for the Bible dataset, the correlation is low in some cases. We believe this may stem from the high occurrence of Bible sentences (especially in English), which leads models to memorize these phrases. Using the WIMBD toolkit (Elazar et al., 2024), we found that, on average, there are 92 times more documents in Dolma 1.7 7 Preprint containing exact Bible sentences than those in FLORES. Consequently, when using Bible examples, the last token is biased towards predicting the specific memorized next token rather than incorporating context-related signals. Therefore, one should consider the hazard of memorized data when using last token embeddings. The weighted-average method, which takes into account the influence of multiple tokens, can mitigate the impact of poor embedding for the last token and enable the model to capture useful information from the other tokens more robustly. Max Pooling vs. Mean Pooling. In our comparison of mean pooling and max pooling on the Belebele benchmark, we found that mean pooling underestimates low-resource languages (resulting in more MEXA scores near 0), while max pooling correlates better with the Belebele benchmark. This can be explained by the fact that Belebele is an easier task among the three evaluated, allowing even low-resource languages to achieve good scores. Conversely, based on our experiment with m-ARC, max pooling tends to overestimate low-resource languages, making mean pooling more aligned with m-ARC. This can be attributed to m-ARC being the most challenging task among the three, where even medium-resource languages do not achieve high scores. Changing the pooling method from mean to max can be considered when dealing with different levels of understanding. 5.2 DOWNSTREAM PERFORMANCE ESTIMATION Figure 1: The relationship between MEXA scores of Llama 3.1-8B from the Bible and FLORES, adjusted by the English task performance, for tasks in Belebele and the m-ARC benchmark. complete Pearson correlation (i.e. ρ = 1.0) indicates that linear equation perfectly describes the relationship between MEXA and the evaluation benchmarks, with all data points lying on line. Given the high correlation values shown in Table 2, it is reasonable to conclude that we can fit line that closely approximates this linear relationship. This line converts the MEXA scores back to downstream task performances. We employed linear model to predict this line by minimizing the residual sum of squares between the MEXA scores (multiplied by the performance on the English task) and the task performances. We needed to adjust the MEXA scores for this purpose, as the MEXA score for language L1 indicates how well L1 is aligned with English but does not reflect the estimated task performance of the model for language L1. Of course, this does not change the Pearson correlation, as it is unaffected by linear transformations. The three tasks considered in this paper involve multiple-choice questions with four possible answers for each question, resulting in chance of being randomly correct of 1 4 . However, the minimum score for MEXA scores is 0. Thus, the ideal slope for the line would be 3 4 with an intercept of 1 4 (X-axis: adjusted MEXA scores, Yaxis: task performance). In Figure 1, we plot this relationship for Llama 3.1-8B using the Bible and FLORES parallel datasets for Belebele and m-ARC. We chose max pooling for Belebele and mean pooling for m-ARC, since these pooling methods yield stronger correlation (see 5.1). The pairs of (slope, intercept) from left to right in the Figure 1 are: (0.6804, 0.2477), (0.6103, 0.1838), (0.6340, 0.3408) and (0.5726, 0.2423). With data points from both high-resource and low-resource languages, this line can be calculated; otherwise, the ideal line may be used as reference. Language Coverage. We present the adjusted MEXA score for all languages available in FLORES200 in Table 5 from Appendix A.5 for the selected models. The languages are categorized into groups ranging from well-covered to not covered. In Table 5, we can clearly see that Llama 3.1-70B and Gemma 2-9B show higher level of multilinguality than other models. 8 Preprint 5.3 MEXA VS ABSOLUTE COSINE SIMILARITY We compare MEXA with the use of absolute cosine similarities. To begin with, cosine similarity scores are not always directly comparable across models. For example, if language shows higher cosine similarity with English for one model than another, it does not necessarily indicate better alignment in the former model. However, MEXA has the advantage of being directly comparable, as its score does not rely on absolute similarity values. To examine the correlation of both methods with downstream tasks, we conducted the following experiment. We used parallel data from FLORES and downstream task data from the Belebele benchmark, focusing on 116 common labels. For each non-English language, we computed the average absolute cosine similarity for parallel sentences with English, and the average absolute cosine similarity for non-parallel sentences with English. Following the setup by Li et al. (2024f), which employs absolute cosine similarity values to predict the performance and rank of languages, we computed the embeddings using the last-token method and applied mean pooling over layers {5, 10, 15, 20, 25}. We report results using the Gemma 1 and Llama 1 7B models, which are commonly used in our experiments. For fair comparison, this setup is applied to both absolute cosine similarity and MEXA. For the Gemma 1 model, MEXA achieves correlation of 0.9260 with downstream task performance, while the absolute cosine similarity for parallel sentences achieves correlation of 0.7651. Additionally, the correlation between the absolute cosine similarity for parallel and non-parallel sentences is 0.9232. For the Llama 1 model, MEXA achieves correlation of 0.8365 with downstream task performance, while the absolute cosine similarity for parallel sentences achieves correlation of 0.6473. Additionally, the correlation between the absolute cosine similarity for parallel and non-parallel sentences is 0.9064. In both models, the absolute cosine similarity method achieved significantly lower correlations with downstream tasks compared to MEXA. This discrepancy arises primarily because, for some languages, the similarity score can be high regardless of whether the sentences are parallel or non-parallel. Furthermore, low similarity score between two languages does not necessarily indicate weak alignment, as overall similarity scores may be low while parallel sentences still exhibit much higher scores than non-parallel ones. 5.4 VISUALIZATION OF LAYERS In Figure 2, we show the results of applying MEXA to 20 pairs of language script from FLORES parallel dataset for Llama 1-7B and Llama 3.1-8B across all 32 layers. We selected these languages from different families, writing systems, and both highand low-resource categories. The embeddings are computed using weighted average based on token positions. Figure 2 shows that highresource languages (with more prevalence on the web; see A.1) achieve higher alignment scores across different layers, while low-resource languages achieve lower scores. In the initial layers, embeddings are more in-language, resulting in lower alignment scores. As embeddings progress to the mid-layers, they become more aligned with the main language of the LLM, i.e., English. MEXA is comparable between models as long as the same parallel dataset and setting is used to obtain the MEXA scores. Figure 2 shows that in many languages, particularly high-resource languages, Llama 3.1 achieves significantly higher alignment score than its predecessor, Llama 1. Although Llama 3.1 exhibits better alignment scores with English for medium and low-resource languages, there is still significant room for improvement. Comparing Arabic (arb Arab) with its romanized version (arb Latn), we see that both Llama 1 and Llama 3.1 models perform better in the native script than in the Latin script, even though Llama 1s tokenizer for Arabic is essentially character-based tokenizer. In general, for very low-resource languages, those in Latin script tend to have higher alignment scores, likely because the tokenization is more favorable for Latin characters. In Figure 3, we display the t-SNE (Van der Maaten & Hinton, 2008) plots of the embeddings of Figure 2 from 3 different layers of Llama 3.1: embedding layer 0, mid-layer 13, and last layer 32. We assign different color to each language. For layers 0 and 32, the embeddings are more languagespecific, while in the mid-layer, they become more language-neutral. Languages that maintain their language-specific embeddings in the mid-layer are clustered separately and, notably, correspond to the very low-resource languages that receive the worst alignment scores from MEXA. 9 Preprint Llama 1 7B Llama 3.1 8B Figure 2: Llama 1 vs. Llama 3.1 MEXA alignment score for different languages across all layers. Best performance markers in order: , , , , , Layer Layer 13 Layer 32 Figure 3: Llama 3.1 t-SNE plots for 3 different layers. As shown, in the mid-layers, the embeddings become more language-neutral. The numbers shown in the mid-layers are the IDs of English sentences that are scattered."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce MEXA, method for assessing the multilingual capabilities of English-centric large language models (LLMs). MEXA builds on the observation that English-centric LLMs semantically use English as kind of pivot language in their intermediate layers. MEXA computes the alignment between non-English languages and English using parallel sentences, estimating the transfer of language understanding capabilities from English to other languages through this alignment. This metric can be useful in estimating task performance, provided we know the English performance in the task and the alignment score between languages derived from parallel dataset. Through different studies with two parallel datasets (FLORES-200 and the Bible); diverse range of LLMs including the Llama family, Gemma family, Mistral, and OLMo, and three downstream tasks (Belebele, m-MMLU, and m-ARC), we demonstrated that MEXA provides reliable estimation of multilingual performance. For MEXA score calculations, multiple design analyses are conducted to explore the impact of token-level pooling for embeddings and layer-level pooling in computing alignment scores. While MEXA shows high correlation across most configurations, weighted average of tokens combined with mean pooling delivers the best results. The results reveal promising average Pearson correlation of 0.90 with established downstream tasks across nine models and two parallel dataset. Overall, MEXA proves to be valuable method for practitioners aiming to assess the multilingual capabilities of English-centric LLMs, paving the way for future efforts to expand these models to wider range of underrepresented languages."
        },
        {
            "title": "7 LIMITATION",
            "content": "MEXA provides rough estimate of the multilingual capabilities of pre-trained English-centric LLMs. Different tasks offer diverse perspectives on the abilities of LLMs, and MEXA cannot re10 Preprint place all of them. Our goal is to highlight the multilingual potential of English-centric LLMs and propose simple way to evaluate them. We hope this encourages the development of more multilingual LLMs, even though they are likely to contain large shares of English data. Additionally, it is important to note that answers across languages do not always need to be fully aligned (Naous et al., 2024), and for such cases, languageand culture-specific evaluation benchmarks should be developed."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This work was funded by Deutsche Forschungsgemeinschaft (project SCHU 2246/14-1)."
        },
        {
            "title": "REFERENCES",
            "content": "David Ifeoluwa Adelani, Jessica Ojo, Israel Abebe Azime, Jian Yun Zhuang, Jesujoba O. Alabi, Xuanli He, Millicent Ochieng, Sara Hooker, Andiswa Bukula, En-Shiun Annie Lee, Chiamaka Chukwuneke, Happy Buzaaba, Blessing Sibanda, Godson Kalipe, Jonathan Mukiibi, Salomon Kabongo, Foutse Yuehgoh, Mmasibidi Setaka, Lolwethu Ndolela, Nkiruka Odu, Rooweither Mabuya, Shamsuddeen Hassan Muhammad, Salomey Osei, Sokhar Samb, Tadesse Kebede Guge, and Pontus Stenetorp. Irokobench: new benchmark for African languages in the age of large language models, 2024. URL https://arxiv.org/abs/2406.03368. Divyanshu Aggarwal, Vivek Gupta, and Anoop Kunchukuttan. IndicXNLI: Evaluating multilingual inference for Indian languages. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1099411006, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.755. URL https://aclanthology.org/2022.emnlp-main.755. Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng, Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. MEGA: Multilingual evaluation of generative AI. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 42324267, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.258. URL https://aclanthology.org/2023.emnlp-main.258. Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Mohamed Ahmed, Kalika Bali, and Sunayana Sitaram. MEGAVERSE: Benchmarking large language models across languages, modalities, models and In Proceedings of the 2024 Conference of the North American Chapter of the Assotasks. ciation for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 25982637, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.143. URL https://aclanthology.org/2024. naacl-long.143. Anthropic. The Claude 3 model family: Opus, sonnet, haiku, 2023. URL https://www-cdn. anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_ Card_Claude_3.pdf. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. On the cross-lingual transferability of monolingual representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 46234637, 2020. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. The Belebele benchmark: parallel reading comprehension dataset in 122 language variants, 2024. URL https://arxiv.org/abs/2308.16884. Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Jeffrey Hsu, Mimansa 11 Preprint Jaiswal, Wilson Y. Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A. Wang, Genta Indra Winata, Francois Yvon, and Andy Zou. Lessons from the trenches on reproducible evaluation of language models, 2024. URL https://arxiv.org/abs/2405.14782. BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. BLOOM: 176b-parameter open-access multilingual language model, 2023. URL https://arxiv.org/abs/2211.05100. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135146, 2017. doi: 10.1162/tacl 00051. URL https://aclanthology.org/Q17-1010. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020a. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. GPT-3 dataset language statistics, 2020b. URL https://github.com/openai/gpt-3/tree/master/ dataset_statistics. Steven Cao, Nikita Kitaev, and Dan Klein. Multilingual alignment of contextual word repreIn International Conference on Learning Representations, 2020. URL https: sentations. //openreview.net/forum?id=r1xCMyBtPS. Grzegorz Chrupała and Afra Alishahi. Correlating neural and symbolic representations of language. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 29522962, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/ v1/P19-1283. URL https://aclanthology.org/P19-1283. Jonathan Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. TyDi QA: benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454470, 2020. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge, 2018. URL https://arxiv.org/abs/1803.05457. Common Crawl. Statistics of common crawl monthly archives, 2024. URL https:// commoncrawl.github.io/cc-crawl-statistics/plots/languages.html. Alexis Conneau and Guillaume Lample. Cross-lingual language model pretraining. In Advances in Neural Information Processing Systems, volume 32, pp. 70597069. Curran Associates, Inc., 2019. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. XNLI: Evaluating cross-lingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 24752485, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1269. URL https://aclanthology.org/D18-1269. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. UnIn Proceedings of the 58th Annual supervised cross-lingual representation learning at scale. Meeting of the Association for Computational Linguistics, pp. 84408451, Online, July 2020a. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https: //aclanthology.org/2020.acl-main.747. 12 Preprint Alexis Conneau, Shijie Wu, Haoran Li, Luke Zettlemoyer, and Veselin Stoyanov. Emerging crossIn Proceedings of the 58th Annual Meeting lingual structure in pretrained language models. of the Association for Computational Linguistics, pp. 60226034, Online, July 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.536. URL https: //aclanthology.org/2020.acl-main.536. Marie-Catherine de Marneffe, Christopher D. Manning, Joakim Nivre, and Daniel Zeman. Universal Dependencies. Computational Linguistics, 47(2):255308, June 2021. doi: 10.1162/ coli 00402. URL https://aclanthology.org/2021.cl-2.11. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423. Sumanth Doddapaneni, Rahul Aralikatte, Gowtham Ramesh, Shreya Goyal, Mitesh M. Khapra, Anoop Kunchukuttan, and Pratyush Kumar. Towards leaving no Indic language behind: BuildIn Proceedings of the ing monolingual corpora, benchmark and models for Indic languages. 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1240212426, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.693. URL https://aclanthology.org/2023. acl-long.693. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Pavel Efimov, Leonid Boytsov, Elena Arslanova, and Pavel Braslavski. The impact of crossIn Advances in lingual adjustment of contextual word representations on zero-shot transfer. Information Retrieval, pp. 5167, Cham, 2023. Springer Nature Switzerland. URL https: //link.springer.com/10.1007/978-3-031-28241-6_4. Yanai Elazar, Akshita Bhagia, Ian Helgi Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Evan Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hannaneh Hajishirzi, Noah A. Smith, and Jesse Dodge. Whats in my big data? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=RvfPnOkPV4. Kawin Ethayarajh. How contextual are contextualized word representations? Comparing the geIn Proceedings of the 2019 Conference ometry of BERT, ELMo, and GPT-2 embeddings. on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5565, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1006. URL https://aclanthology.org/D19-1006. Christian Federmann, Tom Kocmi, and Ying Xin. NTREX-128 news test references for MT evaluation of 128 languages. In Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, pp. 2124, Online, November 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.sumeval-1.4. Jay Gala, Pranjal Chitale, AK Raghavan, Varun Gumma, Sumanth Doddapaneni, Janki Atul Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, et al. Indictrans2: Towards high-quality and accessible machine translation models for all 22 scheduled indian languages. Transactions on Machine Learning Research, 2023. URL https: //openreview.net/forum?id=vfT4YuzAYA. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang 13 Preprint Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for fewshot language model evaluation, December 2023. URL https://zenodo.org/records/ 10256836. Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on Gemini research and technology. arXiv preprint arXiv:2403.08295, 2024a. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size, 2024b. URL https://arxiv.org/ abs/2408.00118. Omid Ghahroodi, Marzia Nouri, Mohammad Vali Sanian, Alireza Sahebi, Doratossadat Dastgheib, Ehsaneddin Asgari, Mahdieh Soleymani Baghshah, and Mohammad Hossein Rohban. Khayyam challenge (PersianMMLU): Is your LLM truly wise to the Persian language?, 2024. URL https://arxiv.org/abs/2404.06644. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. OLMo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. Katharina Hammerl, Alina Fastowski, Jindˇrich Libovicky, and Alexander Fraser. Exploring anisotropy and outliers in multilingual language models for cross-lingual semantic sentence similarity. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 70237037, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-acl.439. URL https://aclanthology.org/2023.findings-acl.439. Katharina Hammerl, Jindˇrich Libovicky, and Alexander Fraser. Understanding cross-lingual In Findings of the Association for Computational Linguistics ACL AlignmentA survey. 2024, pp. 1092210943, Bangkok, Thailand and virtual meeting, August 2024. Association doi: 10.18653/v1/2024.findings-acl.649. URL https:// for Computational Linguistics. aclanthology.org/2024.findings-acl.649. Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. XL-sum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 46934703, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.413. URL https://aclanthology.org/ 2021.findings-acl.413. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. URL https: //arxiv.org/abs/2009.03300. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. XTREME: massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 44114421. PMLR, 1318 Jul 2020. URL https://proceedings.mlr.press/v119/hu20b.html. Haoyang Huang, Yaobo Liang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, and Ming Zhou. In Unicoder: universal language encoder by pre-training with multiple cross-lingual tasks. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 24852494, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1252. URL https://aclanthology.org/D19-1252. 14 Preprint Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023a. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, and Fuzhen Zhuang. Scaling sentence embeddings with large language models. arXiv preprint arXiv:2307.16645, 2023b. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 62826293, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.560. URL https: //aclanthology.org/2020.acl-main.560. Amir Hossein Kargaran, Ayyoob Imani, Francois Yvon, and Hinrich Schuetze. GlotLID: LanIn Findings of the Association for Compuguage identification for low-resource languages. tational Linguistics: EMNLP 2023, pp. 61556218, Singapore, December 2023. Association doi: 10.18653/v1/2023.findings-emnlp.410. URL https: for Computational Linguistics. //aclanthology.org/2023.findings-emnlp.410. Amir Hossein Kargaran, Francois Yvon, and Hinrich Schutze. GlotScript: resource and tool In Proceedings of the 2024 Joint Internafor low resource writing system identification. tional Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pp. 77747784, Torino, Italia, May 2024. ELRA and ICCL. URL https: //aclanthology.org/2024.lrec-main.687. Philipp Koehn. Europarl: parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X: Papers, pp. 7986, Phuket, Thailand, September 13-15 2005. URL https://aclanthology.org/2005.mtsummit-papers.11. Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 35193529. PMLR, 09 15 Jun 2019. URL https://proceedings.mlr.press/v97/kornblith19a.html. Fajri Koto, Haonan Li, Sara Shatnawi, Jad Doughman, Abdelrahman Boda Sadallah, Aisha Alraeesi, Khalid Almubarak, Zaid Alyafeai, Neha Sengupta, Shady Shehata, Nizar Habash, Preslav Nakov, and Timothy Baldwin. ArabicMMLU: Assessing massive multitask language understanding in arabic, 2024. URL https://arxiv.org/abs/2402.12840. Saurabh Kulshreshtha, Jose Luis Redondo Garcia, and Ching-Yun Chang. Cross-lingual alignIn Findings of the Association ment methods for multilingual BERT: comparative study. for Computational Linguistics: EMNLP 2020, pp. 933942, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.83. URL https: //aclanthology.org/2020.findings-emnlp.83. Faisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: new In Findings of the Associabenchmark dataset for cross-lingual abstractive summarization. tion for Computational Linguistics: EMNLP 2020, pp. 40344048, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.360. URL https://aclanthology.org/2020.findings-emnlp.360. Viet Lai, Nghia Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Nguyen. ChatGPT beyond English: Towards comprehensive evaluation of large language models in multilingual learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1317113189, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.878. URL https://aclanthology. org/2023.findings-emnlp.878. 15 Preprint Viet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien Nguyen. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 318327, Singapore, December 2023b. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-demo.28. URL https://aclanthology.org/2023.emnlp-demo.28. Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. MLQA: Evaluating cross-lingual extractive question answering. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 73157330, 2020. Chong Li, Shaonan Wang, Jiajun Zhang, and Chengqing Zong. Improving in-context learning of multilingual generative language models with cross-lingual alignment. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 80588076, Mexico City, Mexico, June 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.445. URL https://aclanthology.org/2024.naacl-long.445. Daoyang Li, Mingyu Jin, Qingcheng Zeng, Haiyan Zhao, and Mengnan Du. Exploring multilingual probing in large language models: cross-language analysis, 2024b. URL https://arxiv. org/abs/2409.14459. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. CMMLU: Measuring massive multitask language understanding in Chinese, 2024c. URL https://arxiv.org/abs/2306.09212. Jiahuan Li, Shujian Huang, Xinyu Dai, and Jiajun Chen. PreAlign: Boosting cross-lingual transfer by early establishment of multilingual alignment, 2024d. URL https://arxiv.org/abs/ 2407.16222. Xianming Li and Jing Li. AoE: Angle-optimized embeddings for semantic textual similarity. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 18251839, Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.acl-long.101. Xiaochen Li, Zheng-Xin Yong, and Stephen Bach. Preference tuning for toxicity mitigation generalizes across languages. arXiv preprint arXiv:2406.16235, 2024e. Zihao Li, Yucheng Shi, Zirui Liu, Fan Yang, Ali Payani, Ninghao Liu, and Mengnan Du. Quantifying multilingual performance of large language models across languages, 2024f. URL https://arxiv.org/abs/2404.11553. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian OHoro, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative In Proceedings of the 2022 Conference on Empirical Methods in Natulanguage models. ral Language Processing, pp. 90199052, Abu Dhabi, United Arab Emirates, December 2022. doi: 10.18653/v1/2022.emnlp-main.616. URL Association for Computational Linguistics. https://aclanthology.org/2022.emnlp-main.616. Pierre Lison and Jorg Tiedemann. OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16), pp. 923929, Portoroˇz, Slovenia, May 2016. European Language Resources Association (ELRA). URL https://aclanthology.org/L16-1147. Weihao Liu, Ning Wu, Wenbiao Ding, Shining Liang, Ming Gong, and Dongmei Zhang. Towards truthful multilingual large language models: Benchmarking and alignment strategies, 2024a. URL https://arxiv.org/abs/2406.14434. Yihong Liu, Mingyang Wang, Amir Hossein Kargaran, Ayyoob Imani, Orgest Xhelili, Haotian Ye, Chunlan Ma, Francois Yvon, and Hinrich Schutze. How transliterations improve crosslingual alignment, 2024b. URL https://arxiv.org/abs/2409.17326. 16 Preprint Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning llama for multi-stage text retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 24212425, 2024. Kelly Marchisio, Saurabh Dash, Hongyu Chen, Dennis Aumiller, Ahmet Ustun, Sara Hooker, and Sebastian Ruder. How does quantization affect multilingual LLMs?, 2024. URL https:// arxiv.org/abs/2407.03211. Thomas Mayer and Michael Cysouw. Creating massively parallel Bible corpus. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC14), pp. 31583163, Reykjavik, Iceland, May 2014. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2014/pdf/220_ Paper.pdf. Meta. Llama 3, 2024. URL https://llama.meta.com/llama3/. Niklas Muennighoff. SGPT: GPT sentence embeddings for semantic search. arXiv preprint arXiv:2202.08904, 2022. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. Crosslingual generalization through multitask In Proceedings of the 61st Annual Meeting of the Association for Computational finetuning. Linguistics (Volume 1: Long Papers), pp. 1599116111, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.891. URL https: //aclanthology.org/2023.acl-long.891. Benjamin Muller, Yanai Elazar, Benoˆıt Sagot, and Djame Seddah. First align, then predict: Understanding the cross-lingual ability of multilingual BERT. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 22142231, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.eacl-main.189. URL https://aclanthology.org/2021.eacl-main.189. Tarek Naous, Michael Ryan, Alan Ritter, and Wei Xu. Having beer after prayer? measuring cultural bias in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1636616393, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.862. URL https: //aclanthology.org/2024.acl-long.862. Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul, Girish Sastry, Gretchen Krueger, David Schnurr, Felipe Petroski Such, Kenny Hsu, Madeleine Thompson, Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. Text and code embeddings by contrastive pre-training, 2022. URL https://arxiv.org/abs/2201.10005. NLLB Team, Marta R. Costa-juss`a, James Cross, Onur elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzman, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling human-centered machine translation, 2022. URL https://arxiv.org/abs/ 2207.04672. Odunayo Ogundepo, Tajuddeen Gwadabe, Clara Rivera, Jonathan Clark, Sebastian Ruder, David Adelani, Bonaventure Dossou, Abdou Diop, Claytone Sikasote, Gilles Hacheme, Happy Buzaaba, Ignatius Ezeani, Rooweither Mabuya, Salomey Osei, Chris Emezue, Albert Kahira, Shamsuddeen Muhammad, Akintunde Oladipo, Abraham Owodunni, Atnafu Tonja, Iyanuoluwa Shode, Akari Preprint Asai, Anuoluwapo Aremu, Ayodele Awokoya, Bernard Opoku, Chiamaka Chukwuneke, Christine Mwase, Clemencia Siro, Stephen Arthur, Tunde Ajayi, Verrah Otiende, Andre Rubungo, Boyd Sinkala, Daniel Ajisafe, Emeka Onwuegbuzia, Falalu Lawan, Ibrahim Ahmad, Jesujoba Alabi, Chinedu Mbonu, Mofetoluwa Adeyemi, Mofya Phiri, Orevaoghene Ahia, Ruqayya Iro, and Sonia Adhiambo. Cross-lingual open-retrieval question answering for African languages. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1495714972, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. findings-emnlp.997. URL https://aclanthology.org/2023.findings-emnlp. 997. OpenAI. Introducing ChatGPT, 2022. URL https://openai.com/blog/chatgpt. OpenAI. Multilingual massive multitask language understanding (MMMLU), 2024. URL https: //huggingface.co/datasets/openai/MMMLU. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Xiaoman Pan, Boliang Zhang, Jonathan May, Joel Nothman, Kevin Knight, and Heng Ji. Crosslingual name tagging and linking for 282 languages. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 19461958, 2017. Edoardo Maria Ponti, Goran Glavaˇs, Olga Majewska, Qianchu Liu, Ivan Vulic, and Anna Korhonen. XCOPA: multilingual dataset for causal commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 23622376, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020. emnlp-main.185. URL https://aclanthology.org/2020.emnlp-main.185. Sara Rajaee and Mohammad Taher Pilehvar. An isotropy analysis in the multilingual BERT embedding space. In Findings of the Association for Computational Linguistics: ACL 2022, pp. 1309 1316, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/ v1/2022.findings-acl.103. URL https://aclanthology.org/2022.findings-acl. 103. Nils Reimers and Iryna Gurevych. Making monolingual sentence embeddings multilingual usIn Proceedings of the 2020 Conference on Empirical Methods in ing knowledge distillation. Natural Language Processing (EMNLP), pp. 45124525, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.365. URL https: //aclanthology.org/2020.emnlp-main.365. Anton Schafer, Shauli Ravfogel, Thomas Hofmann, Tiago Pimentel, and Imanol Schlag. The role of language imbalance in cross-lingual generalisation: Insights from cloned language experiments, 2024. URL https://arxiv.org/abs/2404.07982. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Borje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Souza Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, Aisha Alaagib, Oshan Mudannayake, Zaid Alyafeai, Vu Minh Chien, Sebastian Ruder, Surya Guthikonda, Emad A. Alghamdi, Sebastian Gehrmann, Niklas Muennighoff, Max Bartolo, Julia Kreutzer, Ahmet Ustun, Marzieh Fadaee, and Sara Hooker. Aya dataset: An open-access collection for multilingual instruction tuning, 2024. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. Dolma: an open corpus of three trillion tokens for language model pretraining research, 2024. URL https://arxiv.org/abs/2402.00159. 18 Preprint Guijin Son, Hanwool Lee, Sungdong Kim, Seungone Kim, Niklas Muennighoff, Taekyoon Choi, Cheonbok Park, Kang Min Yoo, and Stella Biderman. KMMLU: Measuring massive multitask language understanding in Korean, 2024. URL https://arxiv.org/abs/2402.11548. Stanford CRFM. Holistic evaluation of language models - MMLU leaderboard, 2024. URL https: //crfm.stanford.edu/helm/mmlu/latest/#/leaderboard. Tatoeba Community. Tatoeba collection, 2006. URL https://tatoeba.org/en/ downloads. Atula Tejaswi, Nilesh Gupta, and Eunsol Choi. Exploring design choices for building languagespecific LLMs. arXiv preprint arXiv:2406.14670, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. W3Techs. Usage statistics of content languages for websites, 2024. URL https://w3techs. com/technologies/overview/content_language. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1189711916, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.acl-long.642. URL https://aclanthology.org/2024.acl-long.642. Chris Wendler, Veniamin Veselovsky, Giovanni Monea, and Robert West. Do Llamas work in English? on the latent language of multilingual transformers, 2024. URL https://arxiv.org/ abs/2402.10588. Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: cross-lingual adversarIn Proceedings of the 2019 Conference on Empirial dataset for paraphrase identification. ical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 36873692, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1382. URL https://aclanthology.org/D19-1382. Jiacheng Ye, Xijia Tao, and Lingpeng Kong. Language versatilists vs. specialists: An empirical revisiting on multilingual transfer ability, 2023. URL https://arxiv.org/abs/2306. 06688. Arda Yuksel, Abdullatif Koksal, Lutfi Kerem enel, Anna Korhonen, and Hinrich Schutze. TurkishMMLU: Measuring massive multitask language understanding in Turkish, 2024. URL https: //arxiv.org/abs/2407.12402. Improving massively multilinBiao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. In Proceedings of the 58th Angual neural machine translation and zero-shot translation. nual Meeting of the Association for Computational Linguistics, pp. 16281639, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.148. URL https://aclanthology.org/2020.acl-main.148. Wenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. M3Exam: In multilingual, multimodal, multilevel benchmark for examining large language models. Advances in Neural Information Processing Systems, volume 36, pp. 54845505. Curran Associates, Inc., 2023a. URL https://proceedings.neurips.cc/paper_files/ paper/2023/file/117c5c8622b0d539f74f6d1fb082a2e9-Paper-Datasets_ and_Benchmarks.pdf. 19 Preprint Zhen-Ru Zhang, Chuanqi Tan, Songfang Huang, and Fei Huang. VECO 2.0: Cross-lingual language model pre-training with multi-granularity contrastive learning, 2023b. URL https://arxiv. org/abs/2304.08205. Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, and Lidong Bing. How do large language models handle multilingualism?, 2024. URL https://arxiv.org/abs/2402. 18815. Chengzhi Zhong, Fei Cheng, Qianying Liu, Junfeng Jiang, Zhen Wan, Chenhui Chu, Yugo Murawaki, and Sadao Kurohashi. Beyond english-centric LLMs: What language do multilingual language models think in?, 2024. URL https://arxiv.org/abs/2408.10811. Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, and Alexandra Birch. Question translation training for better multilingual reasoning, 2024. URL https://arxiv.org/ abs/2401.07817."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DISTRIBUTION OF PRE-TRAINING DATA IN LLMS The distribution of languages in the training data of state-of-the-art LLMs is rarely fully documented. Llama 2 (Touvron et al., 2023b) is counter-example and its authors have disclosed the language distribution use in pretraining. Their analysis uses the FastText (Bojanowski et al., 2017) language identification tool and threshold of 0.5 for the language detection. We reproduce Touvron et al. (2023b, Table 10), which lists 27 languages with percentages greater than 0.005% in the Llama 2 pre-training data, in Table 3. English, with 89.70%, constitutes the vast majority of the training data. All the languages listed in Table 3 have presence of more than 0.10% (top 35 languages) on the web according to the W3Techs report (W3Techs, 2024) or more than 0.15% (top 36 languages) according to CommonCrawl (first three snapshots of 2024) (Common Crawl, 2024). However, not all of the most prevalent languages on the web appear in Table 3. The following 9 languages are missing, most of which use non-Latin writing systems: Turkish (tur Latn), Persian (pes Arab), Arabic (ara Arab), Greek (ell Grek), Hebrew (heb Hebr), Thai (tha Thai), Hindi (hin Deva), Slovak (slk Latn), and Lithuanian (lit Latn). The distribution of data in the training of English-centric LLMs is not the same as on the web, but it does have some correlation. The amount of English in LLM pre-training data is significantly larger than for other languages. This is also observable for GPT-3 (Brown et al., 2020a), where more than 92% of the training texts was in English (Brown et al., 2020b). The rest of the top languages in the data of such models are mostly high-resource languages, which have the most available data on the web (top 36 languages). However, in some models, this could be adjusted by design, for example, to make writing systems with non-Latin languages less prominent (as seen in Llama 2). This weakens the correlation between LLMs pretraining data and the web. Language Common Script Percent Language Common Script Percent English (eng) Unknown (unk) German (deu) French (fra) Swedish (swe) Chinese (zho) Spanish (spa) Russian (rus) Dutch (nld) Italian (ita) Japanese (jpn) Polish (pol) Portuguese (por) Vietnamese (vie) Latn - Latn Latn Latn Hans/Hant Latn Cyrl Latn Latn Jpan Latn Latn Latn 89.70% 8.38% 0.17% 0.16% 0.15% 0.13% 0.13% 0.13% 0.12% 0.11% 0.10% 0.09% 0.09% 0.08% Ukrainian (ukr) Korean (kor) Catalan (cat) Serbian (srp) Indonesian (ind) Czech (ces) Finnish (fin) Hungarian (hun) Norwegian (nor) Romanian (ron) Bulgarian (bul) Danish (dan) Slovenian (slv) Croatian (hrv) Cyrl Hang Latn Cyrl/Latn Latn Latn Latn Latn Latn Latn Cyrl Latn Latn Latn 0.07% 0.06% 0.04% 0.04% 0.03% 0.03% 0.03% 0.03% 0.03% 0.03% 0.02% 0.02% 0.01% 0.01% Table 3: Language distribution in the pretraining data for Llama 2. The large Unknown category is partially composed of programming code data. Common scripts are sourced from the GlotScript resource (Kargaran et al., 2024). Preprint A.2 MULTILINGUAL EVALUATION BENCHMARKS Multilingual evaluation methods and the development of benchmarks not only facilitate the assessment of diverse language representations in LLMs but also help in monitoring cross-lingual generalization, to assess the effect of quantization across multiple languages (Marchisio et al., 2024), the development of language-specific models (Tejaswi et al., 2024), and the optimization of safety preferences (Li et al., 2024e), among others. In Table 4, we list benchmarks with the largest language coverage. This list includes benchmarks referenced by MEGA (Ahuja et al., 2023), MEGAVERSE (Ahuja et al., 2024), xP3 (Muennighoff et al., 2023), the Aya collection (Singh et al., 2024), the lm-evaluation-harness framework (Gao et al., 2023; Biderman et al., 2024), and inter alia. These datasets comprise mix of translated datasets, some human-translated or verified by native speakers such as AfriXNLI (Adelani et al., 2024) and some relying only on machine translation Lai et al. (2023b). Additionally, there are datasets created independently for each language, such as XLSum (Hasan et al., 2021), where the data is not parallel and the size of the data varies between languages. Despite the efforts reflected in Table 4, the community is still lacking highly multilingual benchmarks for tasks such as natural language understanding or text generation. Dataset Task # Languages XNLI (Conneau et al., 2018) IndicXNLI (Aggarwal et al., 2022) AfriXNLI (Adelani et al., 2024) HellaSwag (Lai et al., 2023b) PAWS-X (Yang et al., 2019) XCOPA (Ponti et al., 2020) XStoryCloze (Lin et al., 2022) m-ARC (Lai et al., 2023b) TyDiQA (Clark et al., 2020) MLQA (Lewis et al., 2020) XQuAD (Artetxe et al., 2020) IndicQA (Doddapaneni et al., 2023) AfriQA (Ogundepo et al., 2023) TruthfulQA (Lai et al., 2023b) UDPOS 2.7 (de Marneffe et al., 2021) WikiANN (Pan et al., 2017) XLSum (Hasan et al., 2021) WikiLingua (Ladhak et al., 2020) Belebele (Bandarkar et al., 2024) AfriMMLU (Adelani et al., 2024) m-MMLU (Lai et al., 2023b) MMMLU (OpenAI, 2024) M3Exam (Zhang et al., 2023a) Natural Language Inference Natural Language Inference Natural Language Inference Natural Language Inference Paraphrase Identification Commonsense Reasoning Commonsense Reasoning Common Sense Reasoning Question Answering Question Answering Question Answering Question Answering Question Answering Multiple Choice Question Answering Part of Speech Tagging Name Entity Recognition Summarization Summarization Multiple Choice Reading Comprehension Multiple Choice Knowledge Question Answering Multiple Choice Knowledge Question Answering Multiple Choice Knowledge Question Answering Multimodal Multiple Choice Knowledge Question Answering 15 11 15 31 7 11 11 31 11 7 11 10 10 31 104 282 44 18 115 17 31 15 9 Table 4: Multilingual evaluation benchmarks A.3 SEMANTIC SIMILARITY IN MULTILINGUAL EMBEDDINGS There are other ways to compute similarity between languages, such as Representational Similarity Analysis (RSA) (Chrupała & Alishahi, 2019) and Central Kernel Alignment (CKA) (Kornblith et al., 2019). RSA involves first computing the cosine similarity for sentence embeddings within each language, then correlating these in-language similarities with those in other languages. CKA, another metric, is adopted by Conneau et al. (2020b) and Muller et al. (2021). Conneau et al. (2020b) show that the CKA similarity is highly correlated with sentence retrieval scores for four languages. In this paper, our focus is not on finding different ways to calculate similarity between languages, but on how helpful properly defined alignment score can be in estimating the multilingual capabilities of LLMs across multiple languages. A.4 ROBUSTNESS OF MEXA We show that the MEXA alignment score (µ(.)) is very robust, and the odds of this score randomly achieving high value are very slim. Recall that µ(cid:0)C(L1, L2, m, l)(cid:1) measures the fraction of diagonal elements in matrix C(L1, L2, m, l) that have the maximum value in their respective rows and columns. If this condition is met times out of diagonal elements, then µ(cid:0)C(L1, L2, m, l)(cid:1) is . In an random matrix, the probability of diagonal element being the maximum in its row and column (a total of 2n 1 elements) is = 1 2n1 . The probability that at least out of independent 21 Preprint variables are satisfied, given that the diagonal element is the maximum in its row and column, can be computed using the binomial distribution with Eq. 3. (X ) = 1 k1 (cid:88) i=0 (cid:19) (cid:18)n pi(1 p)ni (3) Figure 4: The probability that at least out of diagonal elements in an random matrix are the maximum elements in their respective rows and columns. In Figure 4, we plot (X ). This plot illustrates that, given sufficient number of parallel sentences (n), the probability of achieving high score by chance is very low. For example, with = 100, the chance of obtaining MEXA alignment score larger than 0.05 (k = 5) from 100 100 random matrix is (X 0.05) = 0.00016. A.5 MEXA FOR FLORES-200 We compute MEXA with weighted average embedding and max pooling for the FLORES parallel data for 203 langauge labels, multiplied by the performance of Belebele for each model in English. We show the results in Table 5, and color the cells based on 0.2 intervals from green (well-covered) to red (not covered): (1.0-0.8), (0.8-0.6), (0.6-0.4), (0.4-0.2), (0.2-0). Note that although FLORES is high-quality, human-translated dataset, we addressed two major issues before proceeding, as noted by Kargaran et al. (2023). First, the data labeled as Cantonese (Yue Chinese) is not actually Cantonese, so we removed it. Second, the code for Central Atlas Tamazight (tzm), which actually refers to Standard Moroccan Tamazight (zgh), was renamed accordingly. As Belebele is relatively an easy task since the models get good scores in English, and we are using max pooling, this gives high estimate of the coverage the LLMs have. If the score for language is not very high, it likely indicates that for more challenging tasks, it will remain low. In Table 5, we can clearly see that Llama 3.1-70B and Gemma 2-9B show higher level of multilinguality than other models. Gemma 2 Gemma 1 Llama 3.1 Llama 3.1 Llama 3 Llama 2 Llama 1 Mistral OLMo AVG eng Latn fra Latn por Latn deu Latn spa Latn ita Latn cat Latn nld Latn rus Cyrl zho Hans 9B 0.92 0.92 0.92 0.92 0.92 0.92 0.92 0.92 0.91 0.91 7B 0.85 0.84 0.84 0.84 0.83 0.83 0.82 0.82 0.82 0. 70B 0.95 0.94 0.94 0.94 0.95 0.92 0.94 0.95 0.94 0.94 8B 0.87 0.87 0.87 0.87 0.87 0.87 0.87 0.87 0.87 0.87 7B 0.48 0.37 0.41 0.35 0.37 0.35 0.39 0.34 0.34 0. 7B 0.42 0.41 0.41 0.42 0.42 0.42 0.42 0.42 0.41 0.34 7B 0.84 0.84 0.84 0.84 0.84 0.84 0.84 0.84 0.83 0.81 7B 0.77 0.70 0.63 0.65 0.56 0.56 0.50 0.52 0.51 0. 0.77 0.75 0.75 0.74 0.74 0.73 0.73 0.73 0.72 0.72 Continued on next page 8B 0.88 0.88 0.88 0.88 0.88 0.88 0.88 0.88 0.88 0.88 22 Preprint glg Latn swe Latn dan Latn ces Latn ron Latn nob Latn zho Hant pol Latn ast Latn ind Latn oci Latn bos Latn nno Latn ukr Cyrl zsm Latn hrv Latn slv Latn afr Latn slk Latn bul Cyrl jpn Jpan hun Latn vec Latn srp Cyrl tgl Latn fin Latn mkd Cyrl vie Latn epo Latn kor Hang arb Arab ars Arab lim Latn acq Arab acm Arab fur Latn pes Arab arz Arab ajp Arab lit Latn apc Arab ell Grek tur Latn est Latn pap Latn lmo Latn szl Latn prs Arab scn Latn heb Hebr lvs Latn als Latn lij Latn ceb Latn srd Latn hin Deva ltz Latn tha Thai aeb Arab bel Cyrl Gemma 2 Gemma 1 Llama 3.1 Llama 3.1 Llama 3 Llama 2 Llama 1 Mistral OLMo AVG 9B 0.92 0.92 0.92 0.92 0.92 0.91 0.91 0.92 0.90 0.92 0.89 0.91 0.92 0.92 0.92 0.91 0.91 0.91 0.91 0.91 0.90 0.91 0.87 0.91 0.91 0.91 0.90 0.91 0.87 0.88 0.91 0.91 0.76 0.91 0.90 0.73 0.91 0.88 0.88 0.90 0.89 0.90 0.89 0.90 0.79 0.73 0.77 0.90 0.77 0.91 0.90 0.87 0.74 0.83 0.73 0.90 0.79 0.90 0.82 0.88 7B 0.83 0.83 0.83 0.82 0.82 0.82 0.81 0.81 0.80 0.83 0.75 0.81 0.82 0.81 0.83 0.81 0.79 0.81 0.80 0.80 0.80 0.78 0.74 0.79 0.74 0.79 0.77 0.81 0.76 0.74 0.80 0.80 0.65 0.78 0.76 0.60 0.79 0.74 0.76 0.76 0.76 0.78 0.78 0.77 0.60 0.56 0.59 0.78 0.59 0.81 0.75 0.67 0.58 0.59 0.59 0.74 0.59 0.76 0.67 0. 70B 0.91 0.95 0.94 0.95 0.94 0.95 0.94 0.95 0.91 0.93 0.95 0.95 0.92 0.95 0.93 0.90 0.93 0.93 0.93 0.90 0.93 0.92 0.93 0.90 0.94 0.90 0.94 0.95 0.95 0.91 0.94 0.93 0.89 0.92 0.90 0.91 0.88 0.90 0.86 0.92 0.86 0.87 0.90 0.90 0.89 0.87 0.87 0.92 0.88 0.89 0.90 0.93 0.88 0.89 0.86 0.91 0.84 0.87 0.86 0.88 8B 0.87 0.87 0.87 0.87 0.87 0.87 0.87 0.87 0.86 0.87 0.87 0.87 0.84 0.87 0.87 0.86 0.86 0.87 0.85 0.86 0.82 0.83 0.83 0.86 0.82 0.85 0.86 0.87 0.85 0.83 0.85 0.85 0.83 0.82 0.82 0.77 0.85 0.83 0.83 0.80 0.83 0.86 0.81 0.83 0.73 0.74 0.74 0.84 0.77 0.83 0.79 0.80 0.70 0.72 0.72 0.79 0.74 0.83 0.75 0.79 7B 0.31 0.38 0.31 0.26 0.23 0.34 0.31 0.22 0.21 0.22 0.22 0.19 0.26 0.22 0.17 0.18 0.20 0.20 0.12 0.12 0.29 0.13 0.16 0.10 0.16 0.14 0.07 0.22 0.14 0.22 0.05 0.04 0.21 0.04 0.04 0.16 0.05 0.03 0.03 0.10 0.03 0.02 0.04 0.12 0.18 0.17 0.11 0.01 0.15 0.02 0.05 0.09 0.16 0.16 0.16 0.03 0.15 0.02 0.04 0. 7B 0.41 0.42 0.41 0.42 0.41 0.39 0.32 0.42 0.40 0.30 0.39 0.41 0.36 0.42 0.25 0.41 0.40 0.37 0.38 0.42 0.25 0.39 0.35 0.42 0.20 0.21 0.38 0.08 0.26 0.15 0.17 0.17 0.25 0.13 0.14 0.27 0.08 0.10 0.12 0.10 0.11 0.09 0.09 0.09 0.22 0.26 0.26 0.06 0.22 0.05 0.05 0.08 0.25 0.15 0.23 0.05 0.18 0.02 0.10 0.09 7B 0.82 0.84 0.82 0.84 0.83 0.81 0.79 0.84 0.77 0.82 0.81 0.84 0.78 0.84 0.81 0.83 0.84 0.79 0.82 0.84 0.76 0.81 0.76 0.84 0.77 0.74 0.80 0.79 0.67 0.71 0.70 0.69 0.59 0.67 0.67 0.59 0.59 0.63 0.60 0.56 0.64 0.58 0.61 0.45 0.56 0.60 0.64 0.46 0.57 0.47 0.55 0.53 0.53 0.49 0.55 0.44 0.44 0.32 0.55 0.50 7B 0.52 0.37 0.44 0.43 0.48 0.39 0.52 0.38 0.49 0.42 0.40 0.25 0.38 0.15 0.36 0.23 0.19 0.21 0.25 0.14 0.24 0.18 0.28 0.06 0.36 0.33 0.11 0.16 0.11 0.15 0.10 0.08 0.21 0.09 0.09 0.26 0.07 0.09 0.09 0.11 0.09 0.05 0.04 0.10 0.23 0.26 0.21 0.08 0.15 0.06 0.08 0.10 0.30 0.24 0.21 0.06 0.18 0.10 0.08 0. 0.72 0.72 0.71 0.71 0.71 0.71 0.71 0.70 0.69 0.69 0.68 0.68 0.68 0.67 0.67 0.67 0.66 0.66 0.66 0.65 0.65 0.64 0.64 0.64 0.64 0.64 0.63 0.63 0.61 0.60 0.60 0.59 0.58 0.58 0.57 0.57 0.56 0.56 0.56 0.56 0.56 0.56 0.55 0.55 0.55 0.55 0.55 0.54 0.54 0.54 0.54 0.54 0.54 0.53 0.53 0.53 0.52 0.52 0.52 0.51 Continued on next page 8B 0.88 0.88 0.88 0.88 0.88 0.88 0.88 0.88 0.88 0.87 0.88 0.88 0.84 0.88 0.88 0.86 0.86 0.87 0.86 0.86 0.83 0.84 0.84 0.86 0.82 0.85 0.87 0.88 0.86 0.84 0.86 0.86 0.83 0.83 0.86 0.81 0.85 0.84 0.85 0.78 0.82 0.87 0.82 0.82 0.75 0.75 0.73 0.84 0.79 0.83 0.81 0.79 0.72 0.73 0.75 0.80 0.75 0.83 0.78 0.79 23 Preprint isl Latn swh Latn mlt Latn war Latn cym Latn fao Latn urd Arab jav Latn eus Latn sun Latn kea Latn ary Arab hat Latn mag Deva min Latn ban Latn bjn Latn azj Latn npi Deva mar Deva awa Deva ben Beng uzn Latn bho Deva gle Latn hye Armn hne Deva kaz Cyrl tpi Latn hau Latn mai Deva crh Latn ilo Latn tat Cyrl kat Geor ydd Hebr kir Cyrl pag Latn pan Guru bak Cyrl guj Gujr tam Taml pbt Arab tgk Cyrl tel Telu snd Arab kan Knda mal Mlym ckb Arab gla Latn asm Beng tuk Latn san Deva kmr Latn lus Latn khk Cyrl ltg Latn azb Arab plt Latn ibo Latn Gemma 2 Gemma 1 Llama 3.1 Llama 3.1 Llama 3 Llama 2 Llama 1 Mistral OLMo AVG 9B 0.83 0.90 0.88 0.76 0.87 0.71 0.83 0.75 0.82 0.69 0.64 0.71 0.74 0.75 0.56 0.48 0.60 0.75 0.81 0.82 0.73 0.82 0.70 0.51 0.68 0.85 0.67 0.62 0.69 0.68 0.61 0.58 0.61 0.56 0.73 0.74 0.53 0.33 0.78 0.56 0.79 0.78 0.50 0.62 0.77 0.59 0.74 0.76 0.51 0.46 0.63 0.49 0.48 0.38 0.53 0.44 0.31 0.37 0.52 0.35 7B 0.62 0.74 0.63 0.55 0.59 0.53 0.66 0.54 0.66 0.48 0.51 0.60 0.47 0.52 0.45 0.32 0.51 0.53 0.59 0.58 0.53 0.58 0.47 0.46 0.31 0.59 0.44 0.47 0.38 0.41 0.43 0.42 0.32 0.38 0.45 0.45 0.33 0.23 0.50 0.36 0.52 0.54 0.23 0.27 0.52 0.30 0.47 0.50 0.20 0.14 0.35 0.31 0.26 0.15 0.09 0.18 0.23 0.28 0.17 0. 70B 0.88 0.86 0.87 0.88 0.88 0.86 0.88 0.84 0.84 0.86 0.78 0.80 0.86 0.88 0.81 0.82 0.78 0.86 0.82 0.87 0.83 0.82 0.84 0.89 0.82 0.79 0.80 0.87 0.69 0.77 0.86 0.77 0.74 0.83 0.72 0.78 0.81 0.63 0.75 0.82 0.67 0.72 0.82 0.78 0.67 0.78 0.66 0.68 0.78 0.71 0.70 0.63 0.71 0.69 0.56 0.73 0.61 0.60 0.59 0.64 8B 0.78 0.80 0.74 0.61 0.76 0.69 0.73 0.67 0.71 0.64 0.64 0.68 0.61 0.70 0.68 0.63 0.60 0.66 0.64 0.63 0.65 0.60 0.62 0.65 0.64 0.58 0.63 0.60 0.45 0.54 0.59 0.51 0.46 0.55 0.50 0.47 0.57 0.39 0.40 0.51 0.39 0.38 0.57 0.52 0.43 0.50 0.41 0.30 0.50 0.45 0.36 0.41 0.45 0.50 0.33 0.43 0.35 0.44 0.25 0.37 7B 0.09 0.11 0.12 0.26 0.11 0.12 0.02 0.16 0.10 0.14 0.16 0.03 0.15 0.02 0.16 0.17 0.14 0.01 0.02 0.02 0.02 0.01 0.04 0.02 0.09 0.01 0.01 0.02 0.19 0.12 0.02 0.03 0.17 0.03 0.01 0.03 0.01 0.25 0.01 0.01 0.02 0.02 0.02 0.02 0.01 0.02 0.01 0.01 0.01 0.08 0.00 0.05 0.00 0.05 0.14 0.01 0.08 0.00 0.14 0. 7B 0.10 0.09 0.11 0.20 0.10 0.13 0.02 0.12 0.08 0.12 0.19 0.09 0.12 0.05 0.12 0.15 0.12 0.03 0.02 0.01 0.05 0.01 0.05 0.03 0.08 0.02 0.02 0.06 0.15 0.11 0.03 0.07 0.15 0.04 0.04 0.02 0.04 0.21 0.00 0.04 0.01 0.01 0.03 0.03 0.01 0.01 0.01 0.01 0.01 0.07 0.01 0.04 0.01 0.06 0.10 0.02 0.06 0.01 0.12 0.08 7B 0.48 0.27 0.38 0.35 0.28 0.53 0.31 0.29 0.18 0.34 0.45 0.44 0.36 0.29 0.34 0.42 0.31 0.29 0.20 0.12 0.21 0.23 0.23 0.26 0.22 0.10 0.24 0.27 0.33 0.17 0.16 0.35 0.24 0.21 0.18 0.05 0.13 0.36 0.03 0.13 0.04 0.08 0.10 0.10 0.05 0.06 0.06 0.03 0.05 0.13 0.08 0.14 0.12 0.13 0.24 0.08 0.22 0.11 0.18 0.12 7B 0.06 0.08 0.12 0.20 0.08 0.08 0.03 0.16 0.05 0.23 0.18 0.10 0.09 0.07 0.17 0.30 0.21 0.02 0.05 0.03 0.08 0.05 0.05 0.08 0.09 0.01 0.08 0.03 0.15 0.06 0.08 0.05 0.13 0.02 0.01 0.02 0.02 0.24 0.05 0.02 0.07 0.03 0.07 0.05 0.04 0.04 0.05 0.03 0.02 0.07 0.04 0.02 0.02 0.05 0.11 0.03 0.06 0.02 0.05 0. 0.51 0.51 0.51 0.49 0.49 0.48 0.47 0.47 0.47 0.47 0.46 0.46 0.45 0.45 0.44 0.43 0.43 0.43 0.42 0.42 0.42 0.41 0.40 0.40 0.40 0.40 0.40 0.39 0.39 0.38 0.38 0.37 0.37 0.36 0.35 0.34 0.34 0.33 0.33 0.33 0.33 0.33 0.32 0.32 0.32 0.31 0.31 0.29 0.29 0.29 0.29 0.28 0.28 0.28 0.27 0.26 0.26 0.25 0.25 0.25 Continued on next page 8B 0.77 0.73 0.74 0.65 0.75 0.71 0.76 0.69 0.74 0.68 0.60 0.68 0.65 0.75 0.68 0.62 0.62 0.68 0.62 0.68 0.65 0.60 0.60 0.68 0.64 0.60 0.65 0.61 0.46 0.58 0.61 0.56 0.47 0.58 0.53 0.48 0.58 0.35 0.47 0.51 0.42 0.38 0.57 0.51 0.38 0.53 0.42 0.32 0.54 0.46 0.39 0.43 0.46 0.48 0.34 0.42 0.38 0.44 0.25 0.38 24 Preprint mri Latn som Latn ace Latn xho Latn nso Latn sot Latn zul Latn kin Latn sin Sinh smo Latn nya Latn twi Latn sna Latn uig Arab bug Latn luo Latn tsn Latn arb Latn khm Khmr lua Latn lug Latn grn Latn ssw Latn lin Latn ory Orya fij Latn fuv Latn kas Arab quy Latn aka Latn mya Mymr run Latn bem Latn kas Deva wol Latn kam Latn tso Latn kon Latn tum Latn kik Latn taq Latn mos Latn yor Latn amh Ethi sag Latn cjk Latn umb Latn dyu Latn kac Latn kmb Latn bam Latn ayr Latn lao Laoo dik Latn ewe Latn knc Latn kab Latn sat Olck gaz Latn bod Tibt Gemma 2 Gemma 1 Llama 3.1 Llama 3.1 Llama 3 Llama 2 Llama 1 Mistral OLMo AVG 9B 0.35 0.42 0.22 0.49 0.27 0.34 0.55 0.37 0.56 0.28 0.36 0.23 0.41 0.21 0.14 0.07 0.24 0.29 0.34 0.09 0.17 0.17 0.27 0.11 0.28 0.13 0.07 0.16 0.10 0.17 0.36 0.25 0.14 0.14 0.09 0.09 0.14 0.07 0.15 0.07 0.06 0.04 0.13 0.48 0.05 0.06 0.05 0.04 0.02 0.05 0.05 0.04 0.17 0.05 0.04 0.05 0.04 0.19 0.05 0.07 7B 0.11 0.14 0.13 0.19 0.11 0.12 0.19 0.10 0.27 0.09 0.13 0.08 0.17 0.09 0.12 0.07 0.10 0.07 0.15 0.08 0.07 0.09 0.10 0.08 0.03 0.06 0.08 0.10 0.06 0.06 0.13 0.07 0.08 0.09 0.07 0.08 0.06 0.08 0.07 0.04 0.06 0.04 0.04 0.16 0.07 0.06 0.05 0.04 0.03 0.06 0.05 0.04 0.04 0.06 0.03 0.06 0.02 0.02 0.03 0. 70B 0.60 0.60 0.49 0.47 0.48 0.53 0.44 0.53 0.49 0.66 0.41 0.46 0.40 0.71 0.35 0.40 0.42 0.46 0.59 0.33 0.41 0.44 0.37 0.43 0.66 0.38 0.30 0.50 0.42 0.37 0.46 0.37 0.29 0.37 0.30 0.26 0.35 0.27 0.32 0.32 0.28 0.25 0.30 0.24 0.22 0.21 0.20 0.22 0.22 0.20 0.18 0.20 0.22 0.18 0.18 0.15 0.17 0.32 0.20 0.22 8B 0.35 0.24 0.31 0.20 0.23 0.20 0.17 0.23 0.18 0.20 0.19 0.22 0.20 0.29 0.22 0.24 0.18 0.20 0.16 0.21 0.19 0.17 0.17 0.18 0.19 0.16 0.20 0.21 0.22 0.17 0.16 0.17 0.16 0.21 0.16 0.16 0.13 0.17 0.13 0.13 0.12 0.14 0.14 0.03 0.17 0.12 0.14 0.12 0.14 0.10 0.09 0.10 0.09 0.07 0.08 0.08 0.08 0.05 0.06 0.08 7B 0.12 0.11 0.14 0.12 0.17 0.14 0.11 0.11 0.01 0.10 0.13 0.14 0.10 0.00 0.14 0.15 0.12 0.05 0.01 0.14 0.14 0.12 0.11 0.12 0.01 0.14 0.13 0.02 0.10 0.11 0.00 0.08 0.13 0.02 0.12 0.13 0.11 0.09 0.09 0.10 0.11 0.11 0.08 0.01 0.07 0.10 0.10 0.06 0.06 0.10 0.08 0.06 0.02 0.08 0.09 0.07 0.06 0.00 0.05 0. 7B 0.10 0.08 0.10 0.10 0.13 0.10 0.06 0.09 0.01 0.08 0.10 0.13 0.08 0.01 0.11 0.12 0.11 0.05 0.02 0.13 0.09 0.09 0.08 0.11 0.01 0.11 0.10 0.02 0.06 0.08 0.00 0.06 0.11 0.03 0.10 0.10 0.08 0.07 0.08 0.09 0.08 0.09 0.06 0.02 0.07 0.08 0.08 0.07 0.06 0.07 0.08 0.05 0.02 0.07 0.07 0.06 0.06 0.00 0.04 0.01 7B 0.18 0.18 0.21 0.13 0.19 0.18 0.10 0.15 0.03 0.13 0.17 0.19 0.12 0.03 0.20 0.21 0.20 0.17 0.09 0.24 0.19 0.13 0.14 0.16 0.03 0.15 0.18 0.10 0.13 0.12 0.02 0.10 0.15 0.11 0.17 0.15 0.13 0.13 0.11 0.13 0.14 0.15 0.10 0.03 0.09 0.13 0.11 0.10 0.12 0.09 0.12 0.10 0.04 0.10 0.08 0.08 0.11 0.01 0.08 0.02 7B 0.07 0.09 0.15 0.05 0.07 0.05 0.05 0.06 0.02 0.06 0.06 0.07 0.07 0.02 0.12 0.09 0.06 0.08 0.06 0.12 0.06 0.10 0.05 0.08 0.03 0.08 0.10 0.05 0.05 0.10 0.02 0.04 0.06 0.08 0.07 0.08 0.06 0.09 0.05 0.12 0.05 0.07 0.04 0.02 0.06 0.07 0.05 0.08 0.08 0.05 0.04 0.06 0.09 0.05 0.04 0.05 0.04 0.01 0.03 0. 0.25 0.23 0.23 0.22 0.21 0.21 0.21 0.21 0.20 0.20 0.19 0.19 0.19 0.18 0.18 0.18 0.18 0.18 0.17 0.17 0.17 0.16 0.16 0.16 0.16 0.15 0.15 0.15 0.15 0.15 0.15 0.14 0.14 0.14 0.14 0.14 0.13 0.13 0.13 0.12 0.12 0.12 0.11 0.11 0.11 0.11 0.10 0.10 0.09 0.09 0.09 0.08 0.08 0.08 0.08 0.08 0.07 0.07 0.07 0.06 Continued on next page 8B 0.38 0.24 0.32 0.20 0.26 0.22 0.19 0.20 0.26 0.20 0.19 0.22 0.19 0.29 0.22 0.25 0.18 0.24 0.15 0.20 0.18 0.16 0.17 0.16 0.18 0.18 0.20 0.20 0.21 0.14 0.14 0.16 0.16 0.20 0.18 0.18 0.14 0.15 0.11 0.12 0.14 0.16 0.14 0.04 0.17 0.13 0.15 0.13 0.12 0.11 0.11 0.11 0.07 0.06 0.08 0.08 0.09 0.05 0.06 0.08 25 Preprint Gemma 2 Gemma 1 Llama 3.1 Llama 3.1 Llama 3 Llama 2 Llama 1 Mistral OLMo AVG fon Latn shn Mymr kbp Latn mni Beng ace Arab knc Arab bjn Arab nus Latn min Arab tir Ethi dzo Tibt taq Tfng zgh Tfng 9B 0.03 0.02 0.03 0.03 0.03 0.01 0.03 0.02 0.02 0.10 0.01 0.00 0.00 7B 0.02 0.01 0.02 0.02 0.02 0.01 0.02 0.02 0.01 0.02 0.00 0.00 0. 70B 0.14 0.21 0.14 0.12 0.15 0.13 0.11 0.07 0.13 0.05 0.08 0.04 0.02 8B 0.06 0.06 0.05 0.05 0.07 0.04 0.05 0.04 0.05 0.02 0.03 0.01 0.01 8B 0.06 0.06 0.04 0.06 0.07 0.04 0.08 0.03 0.04 0.02 0.03 0.01 0. 7B 0.03 0.01 0.03 0.01 0.00 0.02 0.01 0.03 0.01 0.01 0.00 0.00 0.00 7B 0.04 0.02 0.02 0.02 0.00 0.02 0.01 0.02 0.00 0.01 0.00 0.00 0.00 7B 0.05 0.03 0.08 0.08 0.03 0.05 0.05 0.04 0.03 0.02 0.01 0.01 0. 7B 0.07 0.07 0.05 0.02 0.01 0.05 0.01 0.05 0.01 0.02 0.01 0.03 0.01 0.06 0.05 0.05 0.05 0.04 0.04 0.04 0.03 0.03 0.03 0.02 0.01 0.01 Table 5: Adjusted performance of MEXA using max pooling with the English performance of models on the Belebele benchmark."
        }
    ],
    "affiliations": [
        "LMU Munich & Munich Center for Machine Learning",
        "Sorbonne Universite & CNRS, ISIR",
        "Technical University of Munich"
    ]
}