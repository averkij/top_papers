{
    "paper_title": "FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers",
    "authors": [
        "Yusuf Dalva",
        "Kavana Venkatesh",
        "Pinar Yanardag"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Rectified flow models have emerged as a dominant approach in image generation, showcasing impressive capabilities in high-quality image synthesis. However, despite their effectiveness in visual generation, rectified flow models often struggle with disentangled editing of images. This limitation prevents the ability to perform precise, attribute-specific modifications without affecting unrelated aspects of the image. In this paper, we introduce FluxSpace, a domain-agnostic image editing method leveraging a representation space with the ability to control the semantics of images generated by rectified flow transformers, such as Flux. By leveraging the representations learned by the transformer blocks within the rectified flow models, we propose a set of semantically interpretable representations that enable a wide range of image editing tasks, from fine-grained image editing to artistic creation. This work offers a scalable and effective image editing approach, along with its disentanglement capabilities."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 1 1 6 9 0 . 2 1 4 2 : r FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers"
        },
        {
            "title": "Yusuf Dalva",
            "content": "Kavana Venkatesh Virginia Tech https://fluxspace.github.io"
        },
        {
            "title": "Pinar Yanardag",
            "content": "Figure 1. FluxSpace. We propose text-guided image editing approach on rectified flow transformers [14], such as Flux. Our method can generalize to semantic edits on different domains such as humans, animals, cars, and extends to even more complex scenes such as an image of street (third row, first example). FluxSpace can apply edits described as keywords (e.g. truck for transforming car into truck) and offers disentangled editing capabilities that do not require manually provided masks to target specific aspect in the original image. In addition, our method does not require any training and can apply the desired edit during inference time."
        },
        {
            "title": "Abstract",
            "content": "Rectified flow models have emerged as dominant approach in image generation, showcasing impressive capabilities in high-quality image synthesis. However, despite their effectiveness in visual generation, rectified flow models often struggle with disentangled editing of images. This limitation prevents the ability to perform precise, attributespecific modifications without affecting unrelated aspects of the image. In this paper, we introduce FluxSpace, domainagnostic image editing method leveraging representation space with the ability to control the semantics of images generated by rectified flow transformers, such as Flux. By leveraging the representations learned by the transformer blocks within the rectified flow models, we propose set of semantically interpretable representations that enable wide range of image editing tasks, from fine-grained image editing to artistic creation. This work offers scalable and effective image editing approach, along with its disentanglement capabilities. 1 1. Introduction Recent research aimed at enhancing the interpretability of generative models has increasingly focused on disentangled editing capabilities, which allows precise control over specific features or attributes within generated images [20, 42, 44]. These capabilities are crucial for understanding and manipulating the outputs of generative models, such as Generative Adversarial Networks (GANs) and multi-step diffusion models like Stable Diffusion. In singlestep models such as GANs [17], the editing process is facilitated by structured and highly disentangled latent space. This space, composed of fixed-dimension vectors, linearly encodes meaningful concepts that can be individually adjusted to effectively alter specific aspects of the output images. However, achieving similar disentangled editing in multi-step diffusion models presents significant challenge. Unlike GANs, diffusion models do not utilize fixed-dimension latent space; instead, they generate images through multi-step refinement process. This process gradually transitions from random noise distribution to the data distribution, with each step involving complex interactions of learned noise patterns. These patterns are inherently difficult to map back to specific features in latent space. Moreover, the sequential nature of diffusion models complicates the internal dynamics and obscures the direct correlations between dimensions of hypothetical latent space and specific visual features in the generated output. Several studies have explored disentangled latent spaces in diffusion models, focusing on the UNet bottleneck layer [23, 30], text embedding space [3], noise space [8] and weight space [12, 16]. However, these approaches often suffer from significant limitations in terms of disentangled semantics. Manipulation of the UNet bottleneck layer [23, 30] may not fully capture the high-level semantic features necessary for fine-grained control, as bottleneck representations can still be highly entangled. Adjusting the text embedding space [3] relies heavily on alignment between textual and visual features, which may not always correspond accurately, leading to entangled semantics. Operating within the noise space [8] presents challenges, as it is difficult to associate specific noise patterns with distinct semantic attributes. Other works [12] explored the weight space to perform disentangled editing; however, they require training of LoRA models to create latent space for each domain, which is time and resource consuming. On the other hand, generative models based on flow-matching transformers (e.g. Flux) offer high-quality image generation; however, their disentanglement editing capabilities remain underexplored. In this paper, we focus on the latent spaces in flowmatching transformers, characterized by their ability to generate images with high fidelity [14]. Our analysis reveals that the joint transformer blocks, integral to the denoising network, are adept at encoding highly disentangled semantic information. Since the architecture we target involves transformers, the attention layer outputs gradually add image content to the latent representation that gets denoised, without any residual connections between different blocks, unlike UNet-based architectures. Furthermore, the attention layers included in these blocks enable enhanced control over the model outputs, as they control the image content in isolation compared to succeeding and preceding blocks of the denoising network. By implementing linear editing scheme across these attention outputs, we unlock the potential for semantic editing within flow-matching transformers, facilitating semantic navigation across their output space. Our approach supports precise, fine-grained edits, such as adding smile, as well as broader, coarse-level modifications, such as stylization. We perform qualitative and quantitative experiments with various state-of-the-art methods and demonstrate the effectiveness of our method in enabling fine-grained image editing tasks. Our contributions are as follows. We introduce FluxSpace, novel editing framework within flow-matching transformers using attention layer outputs, unlocking advanced semantic editing capabilities for precise navigation within the models output space. We demonstrate the capability of joint transformer blocks to encode highly disentangled semantic information through incremental content refinement. We introduce support for both fine-grained edits, such as smile addition, and coarse-level modifications such as stylization, improving the models versatility in image manipulation. Our method supports the editing of both real and generated images. We make our implementation public to facilitate research in this area. 2. Related Work Latent Space Exploration of Diffusion Models. Diffusion models, which have been the dominant approach for the tasks of text-to-image generation and editing, encapsulate rich semantics within their latent representations. To facilitate new applications, research has focused on leverIn addiaging the semantics encoded by such models. tion to the explorations performed on latent spaces, certain methods [23, 41] explored image editing techniques that alter the backward diffusion process, targeting the learned latent variables. Specifically, [23] bases its approach on the features learned by the bottleneck block of the denoising model, which has UNet-based architecture, while [41] modifies the intermediate latent variables using stochastic diffusion models. As further advancement toward this area, [30] introduces method that aims to identify latent-specific directions representing various semantics, inspired by latent space exploration methods in GANs 2 [9, 21, 31, 44]. Although such methods promise the discovery of semantic directions in domain-specific DDPMs [19], their application to large-scale diffusion models [14, 33, 36] is limited. In addition, various approaches have studied alternative latent spaces to propose directions for semantic editing, such as the noise space [8], the text embedding space [3], and the weight space [12, 16]. Despite these efforts that propose different formulations of semantic directions on diffusion models, such an approach still does not exist for flow-matching transformer models, which is addressed by our proposed FluxSpace framework. Image Editing with Diffusion Models. Due to their high-fidelity generation capabilities, text-to-image diffusion models have been used frequently for image generation and editing tasks. As straightforward strategy, one can perform image editing with target text prompt that effectively describes the desired visual effect. However, this strategy can lead to entangled edits, where the change described as text prompt modifies certain aspects of the image that are not intended to be edited. Addressing this problem, studies such as [18, 45] enabled increased precision within the editing process. Among these works, [45] uses conditional diffusion model that guides the generation process based on user-specified controls. Similarly, [40] attempts to preserve image content by fine-tuning the diffusion model. On the other hand, several methods benefit from the representations learned by the diffusion models. Among these methods, [18, 39] use attention control mechanisms within the diffusion model, [5, 6] use the semantic guidance mechanism over the noise space, and [3, 43] utilize the textembedding representations to perform edits. Furthermore, studies such as [10, 16] succeeded in applying disentangled edits by trained semantic directions. However, these methods are limited in terms of requiring training-per-edit to perform the desired image-to-image transformation, which is either specified as set of textual conditions or paired images reflecting the desired edit. Addressing the problems in both directions, we focus on disentangled editing on rectified flow transformers, without requiring any additional training. Rectified Flow-Based Models. Succeeding over diffusion models, transformers trained with the flow-matching objective [24, 25], such as Flux, now serve as the state-ofthe-art text-to-image generation models [14]. Despite their superior generation capabilities, the methods applied to diffusion models for image editing are not directly transferable to rectified flow models, since the MM-DiT architecture [32] involves continuous interaction between text and image features, which results in changes in both representations, unlike diffusion models [33, 36]. Furthermore, disentangled editing with existing methods is not possible in straightforward manner. As preliminary effort towards text-based image editing with rectified flows, [37] offers an editing method with target captions specifying the desired edit. However, existing methods are limited in terms of the types of edits available and edit scale adjustment, whereas our method offers such functionalities. 3. Preliminaries 3.1. Rectified-Flow Models Generative models aim to define mapping from samples x1 from noise distribution p1 to samples x0 from data distribution p0, where p0 represents real images in image generation tasks. Rectified flows [24, 25] define forward process that constructs paths between distributions p0 and p1 as straight trajectories, as shown in Eq. 1, where p1 = (0, 1). Here, the forward process is time-dependent due to timestep t. xt = (1 t)x0 + tϵ, ϵ (0, 1) (1) To learn this mapping, network is trained with parameters θ, to estimate the velocity of the rectified flow, represented by vθ. By adopting the reparameterization from [14], this velocity prediction network can serve as noise prediction network, ϵθ, optimized using the Conditional Flow Matching (CFM) objective formulated as Eq. 2. LCF = 1 2 EtU (t),ϵN (0,I)[wtλ tϵθ(xt, t) ϵ2] (2) Here, λ tio, and wt is time-dependent weighting function. represents the re-parametrized signal-to-noise ra3.2. Multi-Modal Diffusion Transformers The multi-modal diffusion transformer architecture, multimodal diffusion transformers (MM-DiT) [32], integrates both text and image modalities to generate images aligned with the semantics implied by text inputs. Among the stateof-the-art text-to-image generation models, rectified flow transformers (e.g. Flux) utilize the adaptation of this architecture introduced in [14], which involves modulated attention and MLP layers, followed by attention layers lθ parametrized by parameters θ, conditioning image generation on both pooled and token-wise text embeddings. To guide image generation with text inputs, rectified flow transformers use two text embeddings: cpool and cctxt. The pooled embedding cpool, derived from the CLIP Text Encoder [34], is used in the modulation mechanism to scale and shift features fed as input to the attention layers. The token-wise embeddings cctxt, obtained from T5 Text Encoder [35], ensure alignment with prompt semantics, thus enhancing the relevance of the generated images. The complete conditioning set, {cpool, cctxt}, improves prompt 3 Figure 2. FluxSpace Framework. The FluxSpace framework introduces dual-level editing scheme within the joint transformer blocks of Flux, enabling coarse and fine-grained visual editing. Coarse editing operates on pooled representations of base (cpool) and edit (ce,pool) conditions, allowing global changes like stylization, controlled by the scale λcoarse (a). For fine-grained editing, we define linear editing scheme using base, prior, and edit attention outputs, guided by scale λf ine (b). With this flexible design, our framework is both able to perform coarse-level and fine-grained editing, with linearly adjustable scale. alignment, as shown in previous work [14, 33]. Both embeddings are integrated into joint transformer blocks, where text and image features interact through distinct query (Q), key (K), and value (V ) transformations. These blocks enable influence across modalities (text and image) in bidirectional manner, forming the foundation for the image editing mechanism introduced in this work. ters. Our primary objective is to introduce an inference-time image editing algorithm with flexible levels of control. The first option focuses on detailed edits, allowing fine-grained adjustments, such as adding smile to face. The second option provides control over the images coarse appearance, such as changing the overall style. 4. Methodology Our approach leverages the architecture of flow-matching transformers, specifically Flux, to introduce structured method for controlled image editing. In Flux, image generation progresses by refining content from initial noise through multi-modal transformer blocks. At each timestep t, the noise prediction network ϵθ(xt, c, t) outputs prediction conditioned on noisy latent xt and text-based conditioning (composed of cpool and cctxt). This stepwise refinement occurs within the attention layers lθ, creating progressively evolving representation space suitable for semantic manipulations. For simplicity, we represent the conditional input to the attention layer as c, where it contains both the modulating text embedding cpool and the tokenwise text condition cctxt, which both influences the attention mechanism within Flux. Our method designates the outputs of Fluxs joint attention layers as FluxSpace, linear representation space where semantic image edits can be performed in disentangled manner. Within this space, we enable transformations on the attention outputs, allowing semantic modifications such as detailed adjustments to object attributes or overall changes in style, without altering Fluxs pretrained parame4.1. Fine-Grained Editing Given the pre-trained attention layer lθ and the image tokens from the noisy image input xt, we employ three different outputs obtained by this layer. With minor use of notation, we use for image tokens from the noisy input, both for simplicity and to illustrate the relation between xt and image tokens. First, we utilize the output lθ(x, c, t) where is the text condition used to generate the unedited image. Following, we compute two additional features, lθ(x, ce, t) and lθ(x, ϕ, t) corresponding to the outputs w.r.t. editing condition ce (e.g. eyeglasses for the addition of eyeglasses) and null text ϕ. In our editing scheme, we treat lθ(x, ϕ, t) as an image prior, which reflects the attention layers knowledge of the image features without any supplementary text conditions. Our framework relies on the linearity assumption of attention outputs, which enables us to define latent directions on given input condition ce. First, to isolate the conditional output lθ(xt, ce, t) from the image-related details, we apply an orthogonal projection with the image prior retrieved as the null prediction lθ(xt, ϕ, t), to obtain projϕlθ(x, ce, t) formulated over Eq. 3. 4 Figure 3. Qualitative Results on Face Editing. Our method can perform variety of edits from fine-grained face editing (e.g. adding eyeglasses) to changes over the overall structure of the image (e.g. comics style). As our method utilizes disentangled representations to perform image editing, we can precisely edit variety of attributes while preserving the properties of the original image. projϕlθ(x, ce, t) = lθ(x, ce, t) lθ(x, ϕ, t) lθ(x, ϕ, t)2 lθ(x, ϕ, t) (3) Given the projection of attention outputs, we identify the orthogonal component of lθ(x, ce, t) over lθ(x, ϕ, t) as θ(x, ce, t) in Eq. 4. With this vector, we identify semantic direction that is effective in latent pixels to shift image content w.r.t. editing prompt ce. θ(x, ce, t) = lθ(x, ce, t) projϕlθ(x, ce, t) (4) Using this linear direction, we formulate our editing scheme in Eq. 5. As we define our editing method in the form of linear interpolation between the editing vector θ(x, ce, t), our method is also able to perform edit interpolation with the editing scale λf ine, which controls the strength of the edit. ˆlθ(x, c, ce, t) = lθ(x, c, t) + λf ine(l θ(x, ce, t)) (5) Content Preservation with Attention Masking. To facilitate further disentanglement over the performed edit, we introduce self-supervised mask, based on the interaction of the image features and the editing condition. First, we introduce the mask Mi,edit based on the query features Qi, computed with the image features, and the key features Kedit with the editing condition in Eq. 6. Intuitively, we query the image for the pixels that respond strongly to the given text condition as an intermediate estimate, which is used to mask out the pixels with low attention values. Following the existing work on the T5 text encoder, which is used in Flux, we utilize the attention map of the first text token [28]. Mi,edit = sof tmax (cid:32) (cid:33) Qi edit (6) Given the mask Mi,edit, we introduce soft decision boundary i,edit with boundary coefficient = 10 [13], sigmoid operator σ, and min-max normalization operator normalize, formulated as Eq. 7. i,edit = σ(d (normalize(Mi,edit) 0.5)) (7) As the final step, we perform thresholding operation with the parameter τm to retrieve the thresholding mask i,edit in Eq. 8. i,edit = (cid:40) 1 0 i,edit τm if otherwise (8) Using this thresholding mask, we modify the content editing equation presented in Eq. 5 by masking out the latent pixels that receive low attention from the editing direction θ(x, ce, t). 4.2. Editing Coarse Level Details Before performing the attention calculation, Flux applies modulation based on the pooled CLIP embeddings that provides information about the coarse structure of the generated image [33]. Since certain edits need to change the overall structure and appearance of the image, we introduce an additional control mechanism for appearance-based Figure 4. Qualitative Comparisons. We compare our method both with latent diffusion-based approaches (LEDITS++ [6] and TurboEdit [11]) and flow-based methods (Sliders-FLUX [16] and RF-Inversion [37]) in terms of their disentangled editing capabilities. We present qualitative results for smile, eyeglasses, and age edits where our method succeeds over competing methods in both reflecting the semantic and preserving the input identity. changes. Specifically, we extend our editing approach based on the orthogonal projection of attention features in the CLIP embeddings, where we edit the pooled generation condition cpool in the direction of the pooled editing condition ce,pool to retrieve ˆcpool, which is used to normalize the attention features. Since we want to edit the coarse condition in disentangled manner, we perform our editing scheme based on the application of the linear representation hypothesis [1, 2, 15, 26, 29] on pooled CLIP embeddings [4]. To do so, we first perform an orthogonal projection in Eq. 9, for the editing concept ce,pool on cpool to retrieve the editing direction for the coarse representation of the image. However, different from the attention features, we use the base generation condition as the basis of the projection, as we now operate on textual representations rather than multimodal representations. e,pool = ce,pool cpool ce,pool cpool2 cpool (9) Given the linear direction for editing the coarse condition, we formulate the edit as linear interpolation between the original condition and the editing direction with scale parameter λcoarse, which controls the extent of editing of the modulation condition. ˆcpool = (1 λcoarse) cpool + λcoarse e,pool During generation, to preserve the image content and influence the image with the desired edit, we use different modulations for text and image features. Specifically, we normalize the image features inputted to the attention layer lθ with cpool, while we modulate the text features with ˆcpool, which enables changes in the overall appearance of the image. 5. Experiments To assess the effectiveness of FluxSpace in disentangled semantic editing, we conducted series of qualitative and quantitative experiments. Furthermore, we compared our approach with the state-of-the-art image editing methods in both flow-based and latent diffusion-based models to highlight our methods versatility and superior performance. 5.1. Experimental Setup We utilize FLUX.1-dev1 for all experiments, evaluating both the capabilities of the proposed framework and the control parameters introduced. For all of our experiments, we use 30 generation steps and λcoarse = 0.5 and τm = 0.5 unless stated otherwise. Since the required editing strength changes for every editing task, we use varying values of λf ine and starting timestep for every experiment. We pro1https://huggingface.co/black-forest-labs/FLUX. (10) 1-dev LEDITS++ [6] TurboEdit [11] Sliders-FLUX [16] RF-Inversion [37] Ours"
        },
        {
            "title": "Overall",
            "content": "CLIP-T CLIP-I DINO CLIP-T CLIP-I DINO User Pref. 0.3723 0.3626 0.2878 0.3046 0.3180 0.3634 0.3692 0.3618 0.3438 0.3503 0.8657 0.8479 0.8268 0.8834 0.9038 0.9111 0.9082 0.8683 0.9134 0.9347 0.8639 0.8315 0.8356 0.9354 0.9417 0.9177 0.8954 0.7858 0.9359 0. 3.04 2.75 3.34 2.91 4.19 Table 1. Quantitative Results. We quantitatively measure the editing performance of our method over competing approaches both in terms of text alignment using CLIP-T [34], and content preservation using CLIP-I [34] and DINO [7] metrics where higher is better for all metrics. We compare our method with both latent diffusion [6, 11], and flow-matching-based approaches [16, 37]. Overall, our method strikes good balance in terms of alignment with the editing prompt and content preservation. Supplementary to these metrics, we also present user study as perceptual evaluation that aligns with our claims regarding edit performance, where our method outperforms the competing approaches. vide the complete list of used hyperparameters in the supplementary material. To guarantee the reproducibility of our experiments, we maintained consistent random seed of 0. Our method requires approximately 20 seconds to generate an image with the desired edit on single NVIDIA L40 GPU. 5.2. Qualitative Results Fig. 3 and 4 show the qualitative results of our editing experiments. These results demonstrate our methods capability to execute disentangled edits on fine-grained attributes, such as adding eyeglasses or sunglasses, as well as beards, smiles, and detailed facial expressions such as surprise (see Fig. 3, top row). Additionally, our method can conduct broader transformations, including altering persons perceived age or gender and applying stylistic changes like converting images to comic or 3D cartoon styles (see Fig. 3, bottom row). Beyond faces, our method handles broader image contexts, where we provide examples in the supplementary material. Our approach also extends to other domains, including cars and complex scenes such as street views (see Fig. 1). Furthermore, we conducted qualitative comparison with several state-of-the-art methods, including approaches based on latent diffusion models like LEDITS++ [6] (with SDXL [33]), and TurboEdit [11] (with SDXL-Turbo [38]), as well as flow-based methods such as Sliders-FLUX [16] and RF-Inversion [37]. Fig. 4 illustrates that our method consistently achieves disentangled edits, such as adding eyeglasses or smiles without altering unrelated features, while other methods often struggle with this aspect. For instance, RF-Inversion incorrectly positions the eyeglasses, placing them in unnatural locations on the face. Similarly, while performing edits, Concept Sliders (Sliders-FLUX) and TurboEdit tend to significantly alter the subjects identities, as seen in the eyeglass and age transformations. Although LEDITS++ manages to execute reasonable smile and eyeglass edits, it inadvertently changes the subjects identity in age-related edits. These results highlight the ability of our method to perform disentangled edits and maintain the identity of the subject while applying targeted edits. More comparisons with other editing methods are provided in the supplementary material. 5.3. Real Image Editing To be able to apply FluxSpace to real images, we integrate our method into the inversion mechanism proposed by RFInversion [37] and replace their editing with our editing algorithm. We provide real image editing results in Fig. 5. However, we observe that, due to the nature of their inversion algorithm, the input image is not fully mapped into the latent distribution of the generator model (e.g. Flux), but rather consistency is enabled with correction term. Since we focus on the interpretability of the latent space of rectified flow transformers, we focus on the models output distribution and perform our experiments with images generated by Flux. 5.4. Quantitative Results To further validate the effectiveness of our method, we performed quantitative experiments comparing it with state-ofthe-art editing techniques, using Eyeglass and Smile semantics on set of 60 images (see Table 1). We used metrics such as CLIP-T, CLIP-I [34], and DINO [7] for evaluation. CLIP-T assesses textual alignment by measuring the semantic similarity between the target description and the generated image, ensuring that the intended edits align with textual descriptors. CLIP-I and DINO evaluate image fidelity by measuring the visual similarity between the original and edited images, focusing on maintaining the identity and context of the original subjects. The results indicate that our method excels in the CLIPI and DINO metrics, showcasing its superior ability to retain the original identity while applying disentangled edits. Although methods such as TurboEdit are able to achieve 7 Figure 5. Real Image Editing. By integrating FluxSpace on the inversion approach proposed by RF-Inversion [37], we extend our method for real image editing task. As we show qualitatively, our method achieves improved disentanglement over the performed edits compared to the baseline approach, where we use identical hyperparameters for the inversion task on both approaches. higher CLIP-T scores, they do so at the cost of significant alterations to the subjects original identity. For instance, TurboEdit, despite its high CLIP-T score for eyeglasses, completely changes the individuals identity, as evidenced in Figure 4. This underscores our methods strength in balancing identity preservation with accurate semantic edits. User Study. We conducted user study involving 50 participants recruited through the Prolific.com crowd-sourcing platform. Each participant was presented with an original image alongside its edited version and the corresponding target text. They were then asked to assess how well the edited image captured the intended semantics while maintaining the original facial characteristics of the subject. This was rated on Likert scale from 1 (strongly disagree) to 5 (strongly agree). The results in Table 1 demonstrate that our method significantly outperforms competing approaches. More details about our user study are provided in the supplementary material. 5.5. Ablation Studies We perform ablation studies to demonstrate the effect of hyperparameters introduced by our method. Overall, we present ablations on fine-grained editing scale λf ine, the coarse editing scale λcoarse, the masking coefficient τm, and the timesteps that the edit is applied. We provide qualitative results for each of these hyperparameters in Fig. 6. Ablations on coarse editing scale. We introduce the hyperparameter λcoarse to control the text embedding that determines the modulation parameters of the attention layer inputs [14]. In our experiments, we identified that this representation is heavily involved in the overall appearance of the image. In Fig. 6(a), we demonstrate the effect of λcoarse for controlling the pooled embedding for this modulation, where we use portrait photo of man as the generation prompt, and comics style as the editing prompt. Upon interpolation within the scale [0.0, 1.0], we see that λcoarse enables controlling the style of the image, given the editing prompt. As we would like to identify the effect of λcoarse in isolation, we set λf ine = 0 and τm = 0 for all generations. Ablations on fine-grained editing scale. We investigate the flexibility of content editing over the parameter λf ine. Since the text embedding utilized in feature modulation only provides an approximate overview of the image to be generated, we associate λf ine with fine-grained editing, which determines the scale of the edit applied to the attention outputs. We provide data on the effect of λf ine in Fig. 6(b), where we show semantic interpolation results on the attributes of gender and smile, with increasing values of the editing scale. Ablations on masking coefficient. We introduce an optional attention masking mechanism to improve the disentanglement properties of the editing process. To demonstrate the effect of our masking strategy, we provide ablations in Fig. 6(c), where the edit sunglasses is applied starting from the second denoising step, over 30-step generation process. As can be observed from the presented results, our masking strategy improves the disentanglement of the edit and preserves the semantic properties of the original image better (e.g. facial expression). Ablations on Editing Timesteps. Since our method allows the selection of editing timesteps, we perform an ablation over the effectiveness of the selection of timesteps where the edit is performed. As can also be observed from Fig. 6(d), starting the editing process in earlier timesteps allows 8 Figure 6. Ablation Study. We present ablations over the hyperparameters introduced within the FluxSpace framework. Specifically, we perform ablations on coarse editing scale λcoarse, fine-grained editing scale λf ine, masking coefficient τm and timestep when the editing is initiated. For all ablations, we report qualitative results for changing values of the specified hyperparameters. more drastic editing results, by sacrificing consistency with the original image. This is because the overall structure is still considered noisy from the perspective of the denoising model ϵθ. However, our method is still able to perform disentangled editing starting from early timesteps (e.g. skipping only 1-2 denoising steps). implement robust ethical guidelines and regulatory frameworks that ensure the responsible use of image manipulation technologies. Despite these risks, our approach highlights the internal knowledge of generative models for the task of image editing, which could facilitate research aimed at limiting harmful applications. 6. Limitation 7. Conclusion While FluxSpace offers significant benefits in controllable content generation and editing, it also introduces considerable ethical challenges. The ability of FluxSpace to perform precise image manipulation can lead to concerns over privacy, as individuals likenesses might be used or altered without their explicit consent, limitation that is commonly shared by other editing methods [22]. Furthermore, the potential for creating misleading or deceptive content raises issues regarding the authenticity and trustworthiness of digital media. These concerns are amplified by the capacity of FluxSpace to perform highly realistic and convincing edits that might be indistinguishable from the original images. This capability could be misused in scenarios such as creating fake news, impersonating individuals in harmful ways, or manipulating public opinion by altering perceptions of reality. Given these risks, it is imperative to develop and 9 In conclusion, we propose FluxSpace, novel method that demonstrates robust ability to perform targeted, disentangled edits across range of attributes and styles while maintaining the original identity of subjects in images. The qualitative and quantitative results, supplemented by user study, underscore its effectiveness in preserving the characteristics of the edited image and achieving intended semantic changes, outperforming several state-of-the-art methods. Moreover, FluxSpace excels in performing fine-grained edits across various domains, along with stylistic transformations, showcasing its versatility and wide applicative potential in the field of image editing."
        },
        {
            "title": "References",
            "content": "[1] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. latent variable model approach to pmibased word embeddings. Transactions of the Association for Computational Linguistics, 4:385399, 2016. 6 [2] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of word senses, with applications to polysemy. Transactions of the Association for Computational Linguistics, 6:483495, 2018. 6 [3] Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, and Bjorn Ommer. Continuous, subject-specific attribute control in t2i models by identifying semantic directions. arXiv preprint arXiv:2403.17064, 2024. 2, 3 [4] Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio Calmon, and Himabindu Lakkaraju. Interpreting CLIP with sparse linear concept embeddings (spliCE). In The Thirtyeighth Annual Conference on Neural Information Processing Systems, 2024. 6 [5] Manuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and Kristian Kersting. Sega: arXiv Instructing diffusion using semantic dimensions. preprint arXiv:2301.12247, 2023. 3 [6] Manuel Brack, Felix Friedrich, Katharia Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88618870, 2024. 3, 6, 7, 12, 13 [7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [8] Yusuf Dalva and Pinar Yanardag. Noiseclr: contrastive learning approach for unsupervised discovery of interpretable directions in diffusion models. arXiv preprint arXiv:2312.05390, 2023. 2, 3 [9] Yusuf Dalva, Hamza Pehlivan, Oyku Irmak Hatipoglu, Cansu Moran, and Aysegul Dundar. Image-to-image translation with disentangled latent vectors for face editing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. 3 [10] Yusuf Dalva, Hidir Yesiltepe, and Pinar Yanardag. Gantastic: Gan-based transfer of interpretable directions for disentangled image editing in text-to-image diffusion models. arXiv preprint arXiv:2403.19645, 2024. 3 [11] Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or. Turboedit: Text-based image editing using few-step diffusion models, 2024. 6, 7, 12, 13 [12] Amil Dravid, Yossi Gandelsman, Kuan-Chieh Wang, Rameen Abdal, Gordon Wetzstein, Alexei Efros, and Kfir Aberman. Interpreting the weight space of customized diffusion models. arXiv preprint arXiv:2406.09413, 2024. 2, [13] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. 2023. 5 [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 1, 2, 3, 4, 8, 12 [15] Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah Smith. Sparse overcomplete word vector representations. arXiv preprint arXiv:1506.02004, 2015. 6 [16] Rohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau. Concept sliders: Lora adaptors for precise control in diffusion models. arXiv preprint arXiv:2311.12092, 2023. 2, 3, 6, 7, 12 [17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 3, 14, 15 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [20] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. 2 [21] Umut Kocasari, Alara Dirik, Mert Tiftikci, and Pinar Yanardag. Stylemc: multi-channel based fast text-guided imIn Proceedings of the age generation and manipulation. IEEE/CVF Winter Conference on Applications of Computer Vision, pages 895904, 2022. 3 [22] Pavel Korshunov and Sebastien Marcel. Deepfakes: new threat to face recognition? assessment and detection. arXiv preprint arXiv:1812.08685, 2018. 9 [23] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have semantic latent space. arXiv preprint arXiv:2210.10960, 2022. 2 [24] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. [25] Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In The Eleventh International Conference on Learning Representations, 2023. 3 [26] Tomaˇs Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 746751, 2013. 6 [27] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real arXiv preprint images using guided diffusion models. arXiv:2211.09794, 2022. 14, 15 10 [41] Chen Henry Wu and Fernando De la Torre. latent space of stochastic diffusion models for zero-shot image editing and guidance. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 73787387, 2023. 2 [42] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace analysis: Disentangled controls for stylegan image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1286312872, 2021. 2 [43] Hidir Yesiltepe, Yusuf Dalva, and Pinar Yanardag. The curious case of end token: zero-shot disentangled image editing using clip. arXiv preprint arXiv:2406.00457, 2024. 3 [44] Oguz Kaan Yuksel, Enis Simsar, Ezgi Gulperi Er, and Pinar Yanardag. Latentclr: contrastive learning approach for unsupervised discovery of interpretable directions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1426314272, 2021. 2, 3 [45] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [28] Jianmo Ni, Gustavo Hernandez Abrego, Noah Constant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang. Sentencet5: Scalable sentence encoders from pre-trained text-to-text models. arXiv preprint arXiv:2108.08877, 2021. 5 [29] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language In Causal Representation Learning Workshop at models. NeurIPS 2023, 2023. 6 [30] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understanding the latent space of diffusion models through the lens of riemannian geometry. Advances in Neural Information Processing Systems, 36: 2412924142, 2023. 2 [31] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. arXiv preprint arXiv:2103.17249, 2021. 3 [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. 3, 12 [33] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3, 4, 5, 7, 12 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, [35] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):54855551, 2020. 3 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, 12 [37] Rout, Chen, Ruiz, Caramanis, Shakkottai, and Chu. Semantic image inversion and editing using rectified stochastic differential equations. 2024. 3, 6, 7, 8, 12 [38] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2025. 7, 13 [39] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. 3, 14, 15 [40] Dani Valevski, Matan Kalman, Eyal Molad, Eyal Segalis, Yossi Matias, and Yaniv Leviathan. Unitune: Text-driven image editing by fine tuning diffusion model on single image. ACM Transactions on Graphics (TOG), 42(4):110, 2023. 3 FluxSpace: Disentangled Semantic Editing in Rectified Flow Transformers"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Joint Transformer Block Architecture In our editing algorithm introduced in Sec. 4, we focus on joint transformer blocks in Flux, where text and image inputs are processed in their respective representation spaces. As noted in [14], this design enables each modality to operate within its own feature space, while an interaction mechanism - specifically, the attention layer in the MM-DiT architecture - combines the two. Unlike single transformer blocks, which do not distinguish between feature spaces of different modalities, joint transformer blocks explicitly assume distinct spaces for text and image representations. Based on this difference, we conceptualize these blocks as the areas in which text content semantically influences image content. Consequently, we define our edits within these blocks, leveraging their unique capacity for cross-modal integration. Below, we outline the key components of the joint transformer block architecture in Flux to provide better overview of our editing approach. Modulation Mechanism. The MM-DiT architecture, based on the DiT framework [32], begins by performing modulation operation utilizing coarse conditioning cpool, timestep embedding temb and guidance scale embedding gemb. These embeddings are combined to compute the modulation embedding m, as described in Eq. 11, within the MM-DiT architecture used in Flux. = cpool + temb + gemb (11) Attention Computation. Given the modulation embedding m, input image features x, and contextual text features cctxt, the features undergo modulation operation before the attention process. Subsequently, the text and image features are normalized using their respective layers. Using these normalized features, the joint attention block computes the attention for both modalities, using the query (Q), key (K), and value (V ) features for the text and image inputs. Throughout this paper, we refer to the combination of modulation, normalization, and attention computation as single pass through the attention layer, parameterized by θ, denoted as lθ(x, c, t) for timestep t. B. User Study Details To perceptually evaluate our method against competing approaches, we conducted user study with 50 participants on the Prolific platform, where the results are provided in Table 1. For further clarification of the user study conducted, we provide sample question in Fig. 7. In each question, we ask the users to rate the edit on scale of 1-to-5 (1 for unsatisfactory, 5 for very satisfactory), after providing the image before editing and the edited image. C. Details on Quantitative Comparisons In this section, we provide the details for the quantitative results provided in Table 1. For clarity, we explain the setup used for each of the compared approaches, the models used for evaluation, and the hyperparameters used for editing with FluxSpace. C.1. Competing Methods We compare the editing capabilities of FluxSpace with RF-Inversion [37], Sliders-FLUX [16], TurboEdit [11] and LEDITS++ [6]. Below, we explain the setup used for each of these methods along with implementation details where necessary. RF-Inversion [37]. Using the algorithm provided in [37], we re-implement RF-Inversion using diffusers library. For the editing hyperparameters, we use the parameter set provided by the authors for the eyeglasses edit. Even though the authors do not provide the exact hyperparameters for the smile edit in their paper, we utilize the hyperparameters used for the age editing task. Specifically, for the hyperparameters starting time (s), stopping time (τ ), and strength (η) we use the values 6, 25, 0.7 for the eyeglasses edit and 0, 5, 1.0 for the smile edit. For all generations, we use 30 steps, consistent with the setup used for FluxSpace. Comparing the results presented in Table 1 and the results provided in [37], our reported results are consistent with their quantitative evaluation, where we consider our implementation successful. As we also demonstrate qualitatively in Fig. 4 and 5, our approach succeeds over this baseline by improving the disentanglement and editing capabilities, where both approaches use FLUX.1-dev as the generative model. Sliders-FLUX [16]. Even though Concept Sliders was originally developed for UNet-based diffusion models [33, 36], we utilize the extension of this method on rectified flow transformers. To do so, we use the sliders implementation provided for Flux, by the authors2. In all of our experiments, we use text sliders which we train for eyeglasses and smile attributes. For training, we follow the default hyperparameters provided in the official implementation, where 2https : / / github . com / rohitgandikota / sliders / tree/main/flux-sliders Figure 7. User Study Setup. We conduct our user study on unedited-edited image pairs. For each editing method, we provide the original image where the edit is not applied, with the edited image, and ask the users to rate the edit from scale of 1-to-5. On the Likert scale that the users are asked to provide their preference on, 1 corresponds to unsatisfactory editing and 5 corresponds to satisfactory edit. the LoRA rank is set to 16 and training is performed for 1000 iterations for all experiments, using FLUX.1-dev. Demonstrated in Fig. 4 and Table 1, Sliders-FLUX struggles with the preservation of the input subject identity where significant alterations are observed during editing (see Fig. 4, middle row). We relate this issue with the training performed for the LoRA adapters and the fact that the edited concept cannot be clearly isolated. Note that our method does not require such training for the image editing task. TurboEdit [11]. As method based on few-step text-toimage generation models, we perform comparisons with TurboEdit [11] which uses SDXL-Turbo [38]. Following the official implementation of TurboEdit, we perform our comparisons with resolution 512 512 and 4 inference steps. Regarding the hyperparameters of the method, we use the pseudo-guidance scale as 1.5, and the random seed for edits as 23. Demonstrated in results presented in Fig. 4, TurboEdit succeeds in performing the edit but fails in disentanglement, where the edits age and eyeglasses result in significant edit-irrelevant changes (e.g. skin color in age edit). LEDITS++ [6]. We also compare our method with LEDITS++ [6], which is the state-of-the-art semantic editing method for text-to-image diffusion models. In our comparisons, we use the version of LEDITS++ that uses SDXL4. For all of our experiments, we use the editing guidance scale and the editing threshold values 5.0 and 0.75, as we found the guidance scale effective for eyeglasses and smile edit and we do not change the threshold value from the default setup. Despite performing well in identity preservation and semantic editing tasks, LEDITS++ results in artifacts in the edited images, which are not clearly identified in the quantitative evaluation, but acknowledged in the user study conducted. C.2. Hyperparameter Selection We present the quantitative results for our method in Table 1. In our experiments, we use fixed set of hyperparameters for each edit evaluated, which are coarse editing scale λcoarse, fine-grained editing scale λf ine, mask threshold 3https : / / github . com / GiilDe / turbo - edit / blob / 4https://github.com/ml-research/ledits_pp/tree/ master/main.py main τm, and starting iteration for edit i. The hyperparameter sets we used for the eyeglasses and smile edits are as follows: Eyeglasses: λcoarse = 0.8, λf ine = 5, τm = 0.5, = 3 Smile: λcoarse = 0.5, λf ine = 8, τm = 0.5, = 5 We use the editing prompts eyeglasses and smiling to perform these edits. Note that even though these edits require fine-grained changes in the image, our approach can apply these edits in disentangled way even when the editing process starts in early timesteps. For all of our experiments, we set the number of inference steps as 30. We use fixed seed of 0 in all of our experiments. C.3. Metrics Used As we also specify in Table 1, we use CLIP-T, CLIP-I, and DINO metrics to quantitatively evaluate our method. To enable the reproducibility of our experiments, we also share details on the model weights we use to obtain these scores. Specifically, we use CLIP ViT-bigG/145 variant of the CLIP model to calculate the CLIP-I and CLIP-T scores. For DINO scores, we use DINOv26. D. Supplementary Qualitative Results In this section, we provide supplementary qualitative results to further demonstrate the capabilities of our method. Supplementary Comparisons. In addition to the comparisons provided in the main paper, we also compare the editing capabilities of FluxSpace with methods based on Stable Diffusion. We compare our method with Prompt2Prompt [18] and PnP-Diffusion [39] as competing approaches, where we perform inversion with Null-Text inversion [27] for Prompt2Prompt. We provide qualitative comparisons in Fig. 8. Editing Examples. Supplementary to the results provided in the main paper, we provide additional editing results in the supplementary material. We provide additional results for gender and sunglasses edits in Figs. 9 and 10 for both portrait images and images in natural settings. Furthermore, we provide editing results for various concepts in Fig. 11 and with multiple subjects in Fig. 12. 5https : / / huggingface . co / laion / CLIP - ViT - bigG - 14-laion2B-39B-b160k 6https://huggingface.co/facebook/dinov2-base Figure 8. Additional Qualitative Comparisons. In addition to comparisons provided in the main paper, we provide additional comparisons with Prompt2Prompt [18] (with Null-Text Inversion [27]) and PnP-Diffusion [39], as Stable Diffusion based editing methods. As we demonstrate qualitatively, FluxSpace both achieves disentangled and semantically correct edits where competing methods contain artifacts in edited results (see the edit Eyeglasses for both methods), and significantly alter the subject identity (see Age edit). 15 Figure 9. Gender Editing Results. We provide additional editing results for editing the gender semantics. As shown in the examples, our method succeeds in both male-to-female and female-to-male translations. We provide editing results on both portrait images, where our edits preserve the facial details, and edits on complex scenes where we succeed in only editing the human subject. Both in terms of preserving the identity of the subject and the background details, FluxSpace succeeds in the disentanglement editing task. 16 Figure 10. Sunglasses Editing Results. We provide additional qualitative results for the edit adding sunglasses. As we demonstrate on human subjects in both portrait images and more complex scenes, our editing method can accurately target where the edit should be applied without any input mask. We show the editing capabilities of FluxSpace both in images where the human subject is the main focus of the image (first two rows) and with human subjects as part of scene (last two rows). In both cases, our method succeeds in performing the desired edit and preserving the edit-irrelevant details. Figure 11. Conceptual Editing Results. We provide editing results with abstract concepts, that affect the overall appearance of the image. Our method succeeds in performing edits that alter the content of the image (top row) by being able to interpret the structures in the unedited image (e.g. the trees on the back for the edit cherry blossom) and can change the style and overall appearance of the image (bottom row). Figure 12. Editing Results with Multiple Subjects. We present qualitative results on images with multiple subjects. In addition to images with only one subject to be edited, FluxSpace can apply edits by identifying semantics globally and editing multiple subjects at the same time. Note that our method does not use any external mask, and performs the edit completely with the semantic understanding of the rectified flow transformer."
        }
    ],
    "affiliations": [
        "Virginia Tech"
    ]
}