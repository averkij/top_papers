{
    "paper_title": "Sharing State Between Prompts and Programs",
    "authors": [
        "Ellie Y. Cheng",
        "Logan Weber",
        "Tian Jin",
        "Michael Carbin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rise of large language models (LLMs) has introduced a new type of programming: natural language programming. By writing prompts that direct LLMs to perform natural language processing, code generation, reasoning, etc., users are writing code in natural language -- natural language code -- for the LLM to execute. An emerging area of research enables interoperability between natural language code and formal languages such as Python. We present a novel programming abstraction, shared program state, that removes the manual work required to enable interoperability between natural language code and program state. With shared program state, programmers can write natural code that directly writes program variables, computes with program objects, and implements control flow in the program. We present a schema for specifying natural function interfaces that extend programming systems to support natural code and leverage this schema to specify shared program state as a natural function interface. We implement shared program state in the Nightjar programming system. Nightjar enables programmers to write Python programs that contain natural code that shares the Python program state. We show that Nightjar programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using Nightjar is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 5 0 8 4 1 . 2 1 5 2 : r a"
        },
        {
            "title": "SHARING STATE BETWEEN PROMPTS AND PROGRAMS",
            "content": "Ellie Y. Cheng Logan Weber Tian Jin Michael Carbin MIT CSAIL {ellieyhc, loganweb, tianjin, mcarbin}@csail.mit.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "The rise of large language models (LLMs) has introduced new type of programming: natural language programming. By writing prompts that direct LLMs to perform natural language processing, code generation, reasoning, etc., users are writing code in natural language natural language code for the LLM to execute. An emerging area of research enables interoperability between natural language code and formal languages such as Python. We present novel programming abstraction, shared program state, that removes the manual work required to enable interoperability between natural language code and program state. With shared program state, programmers can write natural code that directly writes program variables, computes with program objects, and implements control flow in the program. We present schema for specifying natural function interfaces that extend programming systems to support natural code and leverage this schema to specify shared program state as natural function interface. We implement shared program state in the NIGHTJAR programming system1. NIGHTJAR enables programmers to write Python programs that contain natural code that shares the Python program state. We show that NIGHTJAR programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using NIGHTJAR is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations)."
        },
        {
            "title": "INTRODUCTION",
            "content": "Writing instructions in natural language is new form of programming: natural language programming. Users write prompts (natural code) for large language model (LLM) systems to evaluate. Natural code enables higher level of abstraction, in which the code requires fewer operational details. LLMs have become increasingly capable, leading to their widespread adoption for tasks such as natural language processing, code generation, and mathematical reasoning (Achiam et al., 2023; Chang et al., 2024; Chen et al., 2021; Guo et al., 2025; Liang et al., 2023; OpenAI, 2025; Qin et al., 2023; Wei et al., 2022). Natural & Formal Language Interoperability. Formal languages like Python have formal syntax and semantics, offering precision and structure. Formal code also supports rich abstractions, including variables, data structures, and control flow. An emerging trend of programming systems enables interoperability between natural and formal code (Adept, 2024; Anthropic, b; Beurer-Kellner et al., 2023; BoundaryML, 2025; Chase, 2022; Dinu, 2022; Dong et al., 2024; Firebase Team; Gat; Guidance AI; Khattab et al., 2024; Liu & Contributors, 2024; Microsoft, b; Okuda & Amarasinghe, 2024; OpenAI; PrefectHQ; Vaziri et al., 2024; Willard & Louf, 2023; Yuksekgonul et al., 2025; Zheng et al., 2024). Programmers develop programs using both natural and formal code, choosing the one best suited for each program component, and these systems orchestrate the interface between them. These systems execute natural code in separate environment from formal code. Programmers must therefore manually implement the transfer of data and, broadly, program state across the boundary between the environments. We term such design as isolated program state. Figure 1c demonstrates the manual work required to transfer program state between between natural and formal code. 1https://github.com/psg-mit/nightjarpy 1 1 import nightjar 2 class Graph: nodes: set[int] edges: dict[int, set[int]] 4 5 @nightjar.fn 6 def main(): 7 8 9 graph = Graph(...) while True: 10 11 12 13 14 15 16 18 19 query = input(\"Q: \") \"\"\"natural Perform the <query> with respect to <graph>, where nodes are paper IDs and edges point from cited paper to set of papers that cite it. Break if <query> indicates termination. Else, save <:response> and update <graph> to answer <query>. \"\"\" print(f\"A: {response}\") print(f\"Papers that cite paper 14:\") print(graph.edges[14]) 20 21 22 main() (a) Example Program with Shared Program State $ ./example.py Q: Update the graph so paper 5 cites 14. A: Graph updated. Papers that cite paper 14: { 5 } Q: Exit, please. $ (b) Example Program Execution Console Figure 1: Figure 1a shows Python program using NIGHTJAR that executes natural language queries on graph modeling paper citations. Figure 1b shows the input and output of an example execution. Figure 1c shows an example of how programmer might approach the task manually instead of using NIGHTJAR. Some helper functions are elided for space. See Section A.1 for the full program. src: int tgts: list[int] 1 from pydantic import BaseModel 2 from llm_wrapper import llm 3 class Graph: ... 4 class EdgeSchema(BaseModel): 5 6 7 class GraphSchema(BaseModel): 8 9 10 def serialize(g: Graph) -> str: ... 11 def reify(gs: GraphSchema) -> Graph: 12 nodes: list[int] edges: list[EdgeSchema] nodes = set(gs.nodes) edges = {e.src: set(e.tgts) for in gs.edges} 13 14 15 16 def main(): 17 18 graph = Graph(...) while True: return Graph(nodes=nodes, edges=edges) query = input(\"Q: \") class OutputSchema(BaseModel): response: Optional[str] break_flag: bool graph: Optional[GraphSchema] q_out = llm( 24 f\"\"\"Perform the <query> with 25 26 respect to <graph>, where nodes are paper 27 IDs and edges point from cited paper to 28 set of papers that cite it. Return 29 `break_flag` as True if <query> 30 indicates termination. Else, return 31 `response`. If <graph> was updated, 32 return it as `graph`. 33 <query>{query}</query> 34 <graph>{serialize(graph)}</graph>\"\"\", 35 36 output_type=OutputSchema) if q_out.break_flag: break response = q_out.response if q_out.graph: graph = reify(q_out.graph) print(f\"A: {response}\") print(f\"Papers that cite paper 14:\") print(graph.edges[14]) 43 44 main() 19 20 21 22 23 37 38 39 40 41 (c) Manual Implementation Shared Program State. We present programming modelshared program statein which embedded natural code directly has access to the program statethe variable scopes, the heap, and the control state of the formal program. The natural code reads and writes program variables (shared scopes), computes with program objects (shared heap), and implements control flow in the program (shared control state). Figure 1a shows an example program written with shared program state. Natural Function Interface. Shared program state and isolated program state are both instances of natural function interface. natural function interface specifies how natural code interacts with the host program execution. We formalized and designed schema for designing natural function interfaces based on effect and handler program constructs (Bauer & Pretnar, 2015; Leijen, 2016; Plotkin & Power, 2003; Plotkin & Pretnar, 2013). Effects define the actions to be performed, and handlers define the implementations of the actions. Effects and handlers are generalization of tool use that enables using natural code to implement context switches in system execution. Shared Program State Natural Function Interface. Using our schema, we specified shared program state as natural function interface. The interface operates over variables, references, labels, and program values. It enables natural code to emit effects to the host language to read and write variables from shared scopes, reference and dereference values from the shared heap, and utilize program labels in the shared control state. Contributions. We present the following contributions in this work: We present the shared program state abstraction. Shared program state enables developers to write more concise programs and focus on the core program logic. Figure 1c illustrates the extra work developer would otherwise need to implement themselves to deliver the program with the same functionality as that of Figure 1a. We define the shared program state natural function interface, language-agnostic formal specification that defines the mechanisms by which natural code interacts with the host programming system to enable the shared program state abstraction. System developers can use our specification to enable the shared program state abstraction in their own programming systems. We implemented the shared program state natural function interface as the NIGHTJAR programming system to demonstrate the efficacy of programming with the shared program state abstraction. We show that NIGHTJAR programs achieve comparable or higher task accuracy than manually written implementations (+4-19%), while decreasing the lines of code by 39.6% on average. The tradeoff to using NIGHTJAR is that it may incur runtime overhead (0.4-4.3x runtime of manual implementations). Implications. Our work introduces shared program state, new abstraction for programming with natural code in formal programs. Shared program state enables programmers to focus on program logic by delegating the low-level work of enabling interoperability to the natural function interface implementation. The shared program state abstraction puts LLM subsystems on equal footing with other components of the larger system."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Natural Language as Programming. The idea of interspersing natural and formal language in single file dates back to literate programming (Knuth, 1984) and has been popularized by the use of Jupyter and IPython notebooks (Pérez & Granger, 2007). This line of work does not treat natural language as executable code, but instead as documentation describing the behavior of the formal code or as macros for programmer-written formal code. The popularity of using natural language prompts to perform computation has risen as LLM capabilities improve. Several works coin prompting as form of programming Beurer-Kellner et al. (2023); Feng et al. (2024); Khattab et al. (2024); Liang et al. (2025) coined prompting as form of programming. Our work takes this step further by unifying the state between prompts and program. Language Interoperability. Language interoperability is the task of enabling software system to leverage multiple different programming languages in its implementation. Language interoperability has long history (Benton & Kennedy, 1999; Breg et al., 1998; Burroughs Corporation, 1961; Davis et al., 1994; Finne et al., 1999; Graham, 1966; Jeffery et al., 1999; Konstantas, 1993; Laddad & Sen, 2020; Matthews & Findler, 2007; Microsoft, a; Nelson, 1981; Object Management Group, 1991; Oracle, 2017; Osborne et al., 2024; Python Software Foundation, 2025; The MathWorks, Inc., 2025; Wagner et al., 2024), dating back at least 50 years as the software engineering community has sought to build increasingly sophisticated software systems that span increasingly more components and systems, each naturally suited to development in different programming languages. Isolated Program State. The community has developed an active area of research on the interoperability between natural and formal languages. Most prior work (Anthropic, b; Beurer-Kellner et al., 2023; BoundaryML, 2025; Chase, 2022; Dinu, 2022; Dong et al., 2024; Firebase Team; Gat; Guidance AI; Khattab et al., 2024; Liu & Contributors, 2024; Microsoft, b; OpenAI; Vaziri et al., 2024; Willard & Louf, 2023; Yuksekgonul et al., 2025; Zheng et al., 2024) isolates natural code execution from formal code execution. Specifically, the natural code has no visibility to the program state. In general, the programmer writes formal code to serialize the input formal data to pass to the LLM in the prompt context. The LLM returns string value by default, and the programmer optionally writes schema to define how the output string is reified into formal data. Figure 1c is an example program of embedded natural code with isolated state. Systems that enable code interpreter tools to answer natural language prompt (Anthropic, b; Chen et al., 2023b; Gao et al., 2023; OpenAI) isolate the code execution from the program that the natural language prompt is embedded in. Our work enables new programming model of interoperability in which the natural code execution shares the program state of the host program that the natural code is embedded in. Programmer-Defined Tools. In systems that support embedded natural code in formal programs with isolated program states, programmers can define custom functions and implement tool use to 3 Figure 2: a) An example execution of Figure 1a, in which natural code uses shared program state. b) The Python interpreter reads and writes to the program state, such as by writing the variable query. c) The Python interpreter hands the execution control to the LLM to execute the natural code. d) The LLM emits Lookup request to read the query variable. e) The handler reads query from the shared scopes, f) which has the value \"Exit, please.\" g) The handler resumes natural code execution with the value of query. h) The LLM emits another request, Goto, to update the evaluation context to the break program label (i.e. outside the while-loop). Finally, i) the handler manipulates the control state accordingly without resuming natural code execution, implementing control flow. manually convert LLM responses into operations on the program state. Several systems automatically make tool calls using programmer defined functions (Beurer-Kellner et al., 2023; Chase, 2022; Dong et al., 2024; Firebase Team). Our shared program state abstraction does not require programmers to define custom formal functions for the natural code to perform operations on the program state. Partially Shared Program State. Other systems enable partially shared program state (Adept, 2024; Dantanarayana et al., 2025; Huang et al., 2023; Okuda & Amarasinghe, 2024; PrefectHQ). Marvin (PrefectHQ) and MTP (Dantanarayana et al., 2025) support reading function arguments and serialization/reification of formal data, but do not enable natural code to manipulate mutable objects in-place. AskIt (Okuda & Amarasinghe, 2024) and ANPL (Huang et al., 2023) replace embedded natural code with LLM-generated formal functions. The generated functions can access object references passed as function arguments. None of the above methods enable natural code to read variables outside of the function scope (and thus cannot support closures), write variables, nor implement control flow. AWL (Adept, 2024) is domain-specific language for web interactions; natural and formal code operate on shared webpages but not shared program state."
        },
        {
            "title": "3 SHARED PROGRAM STATE BY EXAMPLE",
            "content": "NIGHTJAR integrates natural code as first-class code in Python programs: The natural code can read and write Python variables, compute with Python objects, and manipulate the control flow of the surrounding Python program. Figure 1a shows Python program that uses NIGHTJAR."
        },
        {
            "title": "3.1 EXAMPLE PROGRAM",
            "content": "The program implements an agent that manages and manipulates directed graph of paper citations. The nodes are paper identifiers, represented by set of integers. The edges connect cited paper to papers that cite it, represented by dictionary of source nodes to sets of target nodes. The main function uses while-loop (Section 1) to continuously read in user queries from standard input. triple-quoted string with the language identifier natural demarcates section of natural code, as shown on Section 1. The natural code specifies the agents behavior in natural language: The agent performs the given query by writing to variable named response and updating the graph. Execution breaks out of the while-loop if the query signals the user is finished with the program; otherwise, the agent prints the response and the papers that cite paper 14 on Sections 1 to 1. Figure 1b shows program outputs with the inputs of query that adds new edge and one that ends the program."
        },
        {
            "title": "3.2 SHARED PROGRAM STATE",
            "content": "At first glance, the natural code in Figure 1a might seem like comments to be filled in by the programmer, but natural code is first-class code. With shared program state, the natural code operates over Python variables, Python objects, and Python program labels. During program execution, the natural code makes requests to the Python interpreter to manipulate these data in the program. The Python interpreter fulfills these requests by executing these manipulations in the Python program statethe variable scopes, the heap, and the control state. Figure 2 shows diagram of NIGHTJAR executing natural code with shared program state. Shared Scopes. Natural code reads and writes Python variables in the shared scopes. Natural code can refer to any live Python variables within scope using the syntax <var>. For example, on Section 1, natural code reads the query variable, which on the first loop is assigned to \"Update the graph so paper 5 cites 14\". <:var> denotes variable assignments. Section 1 writes string response \"Graph updated.\" to the response variable that is printed to standard output on Section 1. Shared scopes enable the response variable to be used in Python like any other variable in that scope. Shared Heap. Natural code accesses Python objects by reference in the Python heap. The natural code adds an edge in the graph object with an in-place update to the existing graph.edges dictionary of the existing graph object in the programs heap, equivalent to the Python code graph.edges[14].add(5). The update persists after the natural code execution. As result, by referencing the existing graph pointer, Sections 1 to 1 prints out that paper 5 is in the set of papers that cite paper 14. Shared Control State. The natural code also manipulates the evaluation context in the shared control state through program values. When the query variable indicates the user is finished with the program, like the second example query \"Exit, please\", the natural code updates the evaluation context to after the loop, in effect breaking out of it. Correspondingly, the print-statements on Sections 1 to 1 do not get executed, and the program gracefully exits. Without Shared Program State. Without shared program state, natural code only reads and writes serialized string values with no other effects on the Python program. Programmers have to write formal code to manually serialize input data and convert serialized output values into operations on the program state. In Figure 1c, we show an illustrative example of how programmer might implement the task manually without using shared program state. Without shared program state, the programmer has to write Python code to manually serialize and reify data structures, to update objects in the heap, and invoke control flow primitives. Shared program state provides an abstraction over the intermediary connecting code between natural code and formal program state."
        },
        {
            "title": "3.3 EXECUTING PROGRAMS WITH SHARED PROGRAM STATE",
            "content": "We implemented the NIGHTJAR programming system with shared program state. NIGHTJAR uses an LLM agent as an interpreter to evaluate natural code. The LLM agent issues effects to request manipulations to the Python program state. NIGHTJAR implements handlers to receive these effects and interface with the Python program state. Table 1: Average pass rate and runtime of Figure 1 programs, with time ranges in gray."
        },
        {
            "title": "Method",
            "content": "Time (s)"
        },
        {
            "title": "Pass Rate",
            "content": "0.330."
        },
        {
            "title": "Manual Impl",
            "content": "Table 1 shows the performance of the Figure 1a program (using NIGHTJAR) to the Figure 1c program (manual programmer implementation) on 6 different natural language queries (Section A.2). NIGHTJAR achieves higher pass rate in this program than the manual implementation, because the LLM agent adapts to program inputs and intermediate program values. For example, for the query Update the graph so paper 5 cites 14, the LLM decides to issue effects to update only the dictionary entry for paper 14. Without shared program state, the programmer must decide and implement the specific mechanisms the natural code uses to interact with the program state. In Figure 1c, the programmer implemented updates to the graph by querying the LLM to generate the entire updated graph, which is prone to hallucinations. Shared program state offloads the work of reasoning and implementing how to update the program state to the LLM agent. The tradeoff to the abstraction is that the LLM agent incurs runtime overhead. Our work shows that engineering efforts reduce the overhead, offering opportunities for future work to improve the accuracy and runtime of shared program state programming systems. 31.3 (28.7-35.0) 77.2 (72.7-83.0) NIGHTJAR (Ours) 0.830."
        },
        {
            "title": "4 THE SHARED PROGRAM STATE NATURAL FUNCTION INTERFACE",
            "content": "We developed shared program state to enable higher level of abstraction when using natural code within formal programs. This programming model enables natural code evaluation to manipulate the host program state. We describe in this section and show in Figure 3 the shared program state natural function interface with general host language that supports variable bindings (shared scopes), mutable state (shared heap), and control flow constructs (shared control state)."
        },
        {
            "title": "4.1 NATURAL FUNCTION INTERFACES",
            "content": "A natural function interface is specification of how natural code transfers data or utilizes services of host programming language. natural function interface provides definitions of 1) the values that cross the boundary between natural and formal code, 2) effects, which are distinguished requests by the natural code for interacting with the surrounding formal code, and 3) handlers that define the behavior of the requests as implemented in the host system. Natural function interface is generalization of tool use, as defined by OpenAI documentation (OpenAI) and MCP (Model Context Protocol). In Section B.1, we formalized natural function interfaces using effects and handlers (Bauer & Pretnar, 2015; Leijen, 2016; Plotkin & Power, 2003; Plotkin & Pretnar, 2013). Values. The values of natural function interface are the data types that the natural code takes as input and output from the host system (NFI Values in Figure 3). While LLMs typically take strings as input and output, natural function interface can specify richer data types and leverage serialization and reification such as through JSON formatting and Pydantic (Colvin et al., 2025) to automatically convert between string and formal data types. Effects. The effects of natural function interface are the requests that the natural code expects the host language to fulfill (NFI Effects in Figure 3). Natural code evaluates to either value or an effect, denoting the request the host language should fulfill so that the natural code execution can continue. Handler. The handler of natural function interface defines the implementation of the effect requests by the host language (NFI Handler in Figure 3). When natural code evaluates to an effect, the handler that wraps the natural code catches the effect and fulfills the request. As described, effects and handlers are similar in behavior to elements of the OpenAI function calling API. In the function calling API, the LLM can request function to be called and the client of the API will interpret the function and provide the value back to the API, whereupon the LLM resumes its execution. Where natural function interface extends the function calling API is with the ability for the handler to explicitly either resume execution (as is typical) or cancel the continued execution of the natural code (new). Cancelling the execution of natural code enables the host language and natural code to coordinate rich control flow patterns, as we present in the next subsection. Summary. natural function interface is contract between the host language and the natural code. Specifically, the LLM system that executes the natural code must support 1) serialization/reification of the defined values, 2) checkpointing of the natural code evaluation when emitting an effect (as execution control switches to the handlers), 3) restoration of natural code evaluation from checkpoint when the handler initiates resumption, and 4) cleanup of the natural code evaluation when the handler cancels the natural code evaluation."
        },
        {
            "title": "4.2 SHARED PROGRAM STATE AS A NATURAL FUNCTION INTERFACE",
            "content": "Shared Scopes. The values to support shared scopes are the names of variables in the host program. The effects are Lookup and Assign. For Lookup requests, the handler retrieves the value of the requested variable in the current scope and uses the resume keyword to restore the natural code evaluation with the value. The requested variable must be in the set of variable names the LLM is allowed to read, denoted as Xi. For the Assign request, the handler makes the variable binding in the current scope then resumes natural code evaluation with the variable binding still in place. The variable must be in the set of variable names that the LLM is allowed to write to, denoted as Xo. Shared Heap. The values to support shared heap are references. Deref, Ref, and Set are the effects. The handler fulfills these effects by resuming natural code evaluation with the value evaluated from the dereference, reference, and set host program construct, respectively."
        },
        {
            "title": "Variables",
            "content": "x X"
        },
        {
            "title": "References",
            "content": "a A"
        },
        {
            "title": "Labels",
            "content": "ℓ L"
        },
        {
            "title": "Host Values",
            "content": "v"
        },
        {
            "title": "Host Language",
            "content": "eH ::= ... let = in !e ref := label ℓ : goto ℓ values variable bindings mutable state control flow"
        },
        {
            "title": "Natural Code n",
            "content": "Serialization α : vN"
        },
        {
            "title": "Reification",
            "content": "γ : vN v"
        },
        {
            "title": "Natural Code Evaluation",
            "content": "natXi,Xo,L : vN opXi,Xo,L N"
        },
        {
            "title": "NFI Effects",
            "content": "vN opXi,Xo,L ::= Lookup(xi) Assign(xo, vN ) xi Xi; xo Xo Deref(vN ) Ref(vN ) Set(a, vN ) Goto(ℓ) ℓ L"
        },
        {
            "title": "NFI Handler",
            "content": "hN ::= handle { finally vN γ(vN ); Lookup(x ) resume α(x ); Assign(x , vN ) let = γ(vN ) in resume α(()); Deref(vN ) resume α(!γ(vN )); Ref(vN ) resume α(ref γ(vN )); Set(a, vN ) resume α(γ(a) := γ(vN )); Goto(ℓ) goto γ(ℓ) get variable set variable get value at reference create reference with value set value at reference set evaluation context to label }"
        },
        {
            "title": "Program",
            "content": "e ::= eH hN (natXi,Xo,L(n, α(v ))) shared program state natural code Figure 3: Shared Program State Natural Function Interface. Shared Control State. The values to support shared control state are program labels. The corresponding effect is Goto. The handler dispatches the Goto effect by jumping the evaluation context to the specified label in the effect. Notably, the handler does not resume natural code evaluation while handling this effect, which implements control flow. The label must be in the set of labels that the LLM is allowed to jump to, denoted as L."
        },
        {
            "title": "5 NIGHTJAR PROGRAMMING SYSTEM",
            "content": "We implemented the shared program state natural function interface as the NIGHTJAR programming system with Python as the host language. NIGHTJAR uses an LLM agent as the natural interpreter to translate natural code into effects. In this section, we discuss implementation details for NIGHTJAR as well as the optimizations we implemented to improve performance. 5."
        },
        {
            "title": "IMPLEMENTATION",
            "content": "Shared Scopes. NIGHTJAR distinguishes Python variables from regular words within natural code using the syntax <var> for variable references and <:var> for variable assignments and parses them into Xi and Xo, respectively. Specifically, handler enforces that the LLM agent only lookup <var>- denoted variables. If the LLM attempts to look up any other variable, the handler provides the LLM with an error message. This prevents the LLM agent from reading hallucinated variables or variables the programmer does not want the LLM to read. The handler also enforces that <:var>-denoted variables must be defined at the end of natural code evaluation. This check enables programmers to reason about which variables are live after natural code block. Shared Heap. NIGHTJAR represents all Python objects as data in the shared heap, represented by references. This enables natural code to manipulate mutable Python objects in-place. Only plain old data types that are immutable, such as strings, integers, floats, booleans, and null-types, are passed directly through the NFI. Additionally, the handler provides the LLM agent with an error message if it attempts to read invalid references. Shared Control State. Because Python does not support the low-level goto program construct, NIGHTJAR performs source code transformation to install try-except blocks as program labels. NIGHTJAR then exposes program labels corresponding to Pythons built-in control flow primitives 7 such as break, continue, return, and raise to the LLM to use with the Goto effect. NIGHTJAR only enables program labels that are valid within the evaluation context the natural code exists in. For example, the program label corresponding to the break primitive is only exposed to the LLM when the natural code is within loop."
        },
        {
            "title": "5.2 SYSTEM ENGINEERING",
            "content": "One can implement shared program state in programming system by querying the LLM agent to generate the effects exactly as defined in Figure 3. While this implementation is language-agnostic, it is not designed to be performant. Realizing the shared program state abstraction as performant programming model requires additional system engineering. To improve performance, we specialize NIGHTJAR to Python and implement optimizations for reducing runtime. Specialization. The shared program state natural function interface is designed to be general to capture the underlying semantics of the shared program state programming model. The tradeoff of the general approach is the loss of specialized host-language data types such as first-order functions and abstract methods. It also relies entirely on the LLM to perform data transformations. We specialize NIGHTJAR to Python so that it supports these Python-specific language features and enables Pythonbased computation. In this implementation, the effects Eval (evaluates Python expression and returns the result) and Exec (executes block of Python code in the evaluation context) replace the Lookup, Assign, Deref, Ref, and Set effects. The effects Break, Continue, Return, and Raise replace Goto as specialized effects corresponding to the respective Python primitives. Eager Variable Loading. During execution, the LLM generally inspects the values and types of the variables mentioned in the natural code (i.e. <var>-denoted variables). We apply the optimization to eagerly load the values and types of the mentioned variables to the LLM, rather than waiting for the LLM to retrieve the information with effects. This reduces the overhead time of making LLM queries; the tradeoff is wasting tokens by loading variables that are not needed during the execution. Caching. We implement simple caching scheme where the LLM API client stores pairs of LLM agent effect trace history and the response. When given identical agent effect trace histories, the client returns the stored response rather than querying fresh responses from LLM APIs."
        },
        {
            "title": "6 EVALUATION",
            "content": "In this work, we presented shared program state, new programming model, that delegates the work of enabling interoperability between natural and formal code to the natural function interface implementation. We also implemented the NIGHTJAR programming system to execute programs using shared program state. We next perform an evaluation to answer the following: RQ1. Does shared program state enable more concise programs? RQ2. Can programs with shared program state perform as well as manual implementations? Benchmarks. Since there are no existing benchmark suites that evaluate shared program state, we introduce SPSBench. SPSBench is suite of 25 programs that contain natural code using shared program state. We adapted several of the programs from the documentation of LLM-related Python packages. We constructed additional programs to evaluate more complex manipulations on the program state by the natural code, such as creating closures, in-place state mutations, using generators, defining subclasses, and raising errors. See Section for program descriptions. Baselines. For each shared program state program, we manually implemented the same task using simplified LLM API that takes prompt and an optional Pydantic (Colvin et al., 2025) model or JSON schema for structured output. We call these Manual Implementation programs and execute them using chat completion LLM API calls. The baseline Manual Implementation (Code Interpreter) executes the manual implementation programs with the official code interpreter tools from OpenAI and Anthropic enabled in the LLM response API calls. The program states of the code interpreter tools from OpenAI and Anthropic are isolated from the host program, but they enable the LLM to use Python code to perform calculations. The third baseline executes the shared program state programs with baseline implementation of NIGHTJAR without specialization or optimizations. This is called NIGHTJAR (Baseline). 8 Table 2: Average pass rate with standard deviation and average runtime with ranges in gray."
        },
        {
            "title": "Model Method",
            "content": "Pass Rate Time (s) Sonnet 4 Manual Impl 0.780.03 Sonnet 4 Manual Impl (Code Interpreter) 0.660.04 Sonnet 4 NIGHTJAR (Baseline) Sonnet 4 NIGHTJAR GPT-4.1 Manual Impl 0.600.04 0.850.03 0.740.03 GPT-4.1 Manual Impl (Code Interpreter) 0.720.03 GPT-4.1 NIGHTJAR (Baseline) GPT-4.1 NIGHTJAR 0.610.02 0.780.03 8.0 (1.3-44.8) 36.8 (3.2-171.5) 55.6 (9.6-721.0) 25.9 (6.2-83.0) 4.5 (0.8-15.0) 50.3 (10.9-364.9) 39.0 (4.7-957.4) 19.6 (3.0-72.1) RQ1: Methodology. We compared the Manual Implementation programs against shared program state programs. We counted the number of lines to implement the task in each implementation, not counting scaffolding for testing nor empty lines and comments. We used the Black Formatter linter with maximum width of 120 characters to standardize the formatting. RQ1: Results. Manual Implementation has 42.0 lines of code on average, with range of 7 to 207 lines. In contrast, shared program state has 25.1 lines of code on average and range of 7 to 166 lines. Shared program state enables decrease of 17.0 lines of code on average, with range of 0 to 60 lines of code decreased. Shared program state enables programmers to write more concise programs. RQ2: Methodology. We compare the performance of the shared program state programs executed using NIGHTJAR against Manual Implementation, Manual Implementation (Code Interpreter), and NIGHTJAR (Baseline). We used Claude-Sonnet-4-20250514 and GPT-4.1-2025-04-14 as the LLMs. We used the default temperature value of 1 and executed each program implementation of the benchmark 5 times. We computed the average benchmark pass rate averaged over runs, where the average benchmark pass rate is the average percentage of passing assertions of each program. Due to cost constraints, we set limit of 300 tool calls or effects and timeout of 1000 seconds. RQ2: Results. Table 2 shows the average pass rate with standard deviation and the average runtime with range. NIGHTJAR achieves comparable or higher average pass rate than Manual Implementation and Manual Implementation (Code Interpreter), demonstrating that the shared program state abstraction can be realized as executable programs. The tradeoff to the shared program state abstraction is that NIGHTJAR incurs runtime overhead compared to the chat completion-only usages of Manual Implementation because NIGHTJAR uses an LLM agent to execute natural code. However, the Manual Implementation (Code Interpreter) also incurs runtime overhead. The NIGHTJAR (Baseline) achieves lower pass rate compared to Manual Implementation and Manual Implementation (Code Interpreter). This is because NIGHTJAR (Baseline) is language-agnostic and thus cannot correctly complete tasks that require Python-specific data types such as first-order functions and abstract classes. NIGHTJAR bridges this accuracy gap by specializing the system to Python to support these features and enabling Python-based computations. The optimizations implemented in NIGHTJAR also reduce the runtime overhead by 2.0-2.1x compared to NIGHTJAR (Baseline). This demonstrates that the performance of the shared program state abstraction can be improved with more sophisticated implementations and engineering efforts in future work. Section D.2 contains additional ablations of NIGHTJAR implementations."
        },
        {
            "title": "7 DISCUSSION & CONCLUSION",
            "content": "We present shared program state, novel programming model when using natural code within programs that abstracts away the work of enabling interoperability between natural and formal code. We designed and formalized schema for designing natural function interfaces through effects, which is generalization of tool use. We specified shared program state as natural function interface and implemented it as programming system. Numerous opportunities remain for extending the system. 9 Safety & Security. Shared program state enables the natural code to directly read and write objects in memory. This is departure from the existing systems that do not enable shared program state (Anthropic, b; Beurer-Kellner et al., 2023; BoundaryML, 2025; Chase, 2022; Dong et al., 2024; Firebase Team; Gat; Guidance AI; Khattab et al., 2024; Liu & Contributors, 2024; Microsoft, b; OpenAI; PrefectHQ; Vaziri et al., 2024; Willard & Louf, 2023; Yuksekgonul et al., 2025; Zheng et al., 2024); these systems provide isolation in that the natural code is completely separated from the formal code. In existing systems, programmers are in full control over how natural code interacts with the formal program state. Shared program state blurs this boundary. While NIGHTJAR provides partial safety mechanisms in the handlers, these mechanisms are not complete. This tradeoff between usability, safety, and performance is well studied in the operating systems and language-based memory isolation literature. Heap isolation (Back & Hsieh, 2005; Von Eicken et al., 1999), while offering safety from inconsistent states, requires marshaling and copying data, introducing high overheads (Emmerich et al., 2019). Work that develops safety mechanisms with abstractions such as single ownership (Hunt & Larus, 2007; Lafrance et al., 2023; Narayanan et al., 2020) bridges these tradeoffs by enabling zero-copy communication on shared heap. We anticipate fruitful, future research that develops richer safety mechanisms for shared program state, potentially inspired by this previous work. Exploring other execution substrates, such as using subset of Python that can be compiled to DSL with security and reliability benefits (Mell et al., 2025) may reveal additional points in the tradeoff space between execution time, correctness, security, and reliability. Safety also extends beyond isolation. Work in the approximate computing literature on distinguishing reliably and unreliably computed data is also relevant in that the developed techniques can enable users to reason about the downstream effects of values computed by the LLM (Carbin et al., 2013; Sampson et al., 2011). Our work also emphasizes the importance of defending against vulnerabilities in LLM-based software (Chang et al., 2024), especially inference-time defenses against malicious user inputs, such as pre-processing of inputs to LLMs (Li et al., 2023; Mo et al., 2025; Wei et al., 2023) and post-processing of LLM generated data and code (Phute et al., 2023; Qi et al., 2021). Performance. NIGHTJAR opens up new line of work on improving the execution time and correctness of programming systems enabling shared program state. We foresee future work in optimizing effects akin to bytecode optimizations (Clausen, 1997; Vallée-Rai et al., 1999; Xu & Kjolstad, 2021), applying parallel generation techniques (Jin et al., 2025; Liu et al., 2024; Pan et al., 2025; Yang et al., 2025), and sophisticated caching schemes to decrease the execution time. We also anticipate future system designs that improve correctness, such as by incorporating automatic natural language prompt optimizations (Chen et al., 2023a; Fernando et al., 2023; Khattab et al., 2024; Pryzant et al., 2023; Schnabel & Neville, 2024; Yuksekgonul et al., 2025; Zhou et al., 2022). Non-Deterministic Execution. By using closed LLMs and temperature of 1, NIGHTJAR executions may be non-deterministic. Using caching or open-source LLMs with temperature 0 would enable deterministic program executions (He & Lab, 2025). Program Development Tools. As new programming model, shared program state calls for future work on program development tools, such as debuggers, testing tools, and program analyses, that assist programmers in writing natural code that achieves low execution time and high accuracy (Liang et al., 2025). These tools would need to combine both traditional program development needs (e.g. identifying inputs that cause bugs (Cadar et al., 2008a;b)) as well as the needs of natural code development (e.g. testing the prompt robustness with respect to given LLM (Sharma et al., 2025)). Conclusion. Shared program state is an effective programming abstraction for programs using natural code. By delegating the work of enabling interoperability between natural code and program state to the natural function interface implementation, shared program state enables programmers to focus on core program logic. Shared program state offers promising path toward programming systems where natural and formal code execute cooperatively with rich program constructs to support increasingly sophisticated software."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This material is based upon work supported in part by the National Science Foundation Graduate Research Fellowship under Grant No. 2141064 and the CSAIL-Felicis Research Program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the 10 authors and do not necessarily reflect the views of the National Science Foundation. We thank Jesse Michel, Charles Yuan, Eric Li, Linlu Qiu, and Daniel Pfrommer for helpful feedback on this work."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. Adept. Building Powerful Agents with Adept, 2024. URL https://www.adept.ai/blog/ adept-agents. Anthropic. Welcome to Claude, a. URL https://docs.anthropic.com/en/docs/welcome. Anthropic. Code Execution Tool Agents and Tools (Anthropic API documentation). https:// docs.anthropic.com/en/docs/agents-and-tools/tool-use/code-execution-tool, b. Godmar Back and Wilson Hsieh. The KaffeOS Java Runtime System. ACM Transactions on Programming Languages and Systems, 27(4), 2005. Andrej Bauer and Matija Pretnar. Programming with algebraic effects and handlers. Journal of Logical and Algebraic Methods in Programming, 84(1), 2015. Nick Benton and Andrew Kennedy. Interlanguage working without tears: Blending SML with Java. In International Conference on Functional Programming, 1999. Luca Beurer-Kellner, Marc Fischer, and Martin Vechev. Prompting is programming: query language for large language models. In International Conference on Programming Language Design and Implementation, 2023. BoundaryML. BAML: The AI framework that adds the engineering to prompt engineering. https: //github.com/boundaryml/baml, 2025. Fabian Breg, Shridhar Diwan, Juan Villacis, Jayashree Balasubramanian, Esra Akman, and Dennis Gannon. Java RMI Performance and Object Model Interoperability: Experiments with Java/HPC++. Concurrency: Practice and Experience, 10(11-13), 1998. Burroughs Corporation. The Descriptor: Definition of the 5000 Information Processing System. Technical report, Burroughs Corporation, Sales Tech. Serv. Sys. Doc., 1961. Cristian Cadar, Daniel Dunbar, and Dawson Engler. KLEE: unassisted and automatic generation of high-coverage tests for complex systems programs. In Symposium on Operating Systems Design and Implementation, 2008a. Cristian Cadar, Vijay Ganesh, Peter Pawlowski, David Dill, and Dawson Engler. EXE: Automatically generating inputs of death. Transactions on Information and System Security, 12(2), 2008b. Michael Carbin, Sasa Misailovic, and Martin Rinard. Verifying quantitative reliability for programs that execute on unreliable hardware. In Object-Oriented Programming, Systems, Languages and Applications, 2013. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. Survey on Evaluation of Large Language Models. ACM Transactions on Intelligent Systems and Technology, 15(3), 2024. Harrison Chase. LangChain, 2022. URL https://github.com/langchain-ai/langchain. Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. Instructzero: Efficient instruction optimization for black-box large language models. arXiv preprint arXiv:2306.03082, 2023a. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Wenhu Chen, Xueguang Ma, Xinyi Wang, and William Cohen. Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. Tmlr, 2023b. Lars Clausen. Java bytecode optimizer using side-effect analysis. Concurrency: Practice and Experience, 9(11), 1997. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Samuel Colvin, Eric Jolibois, Hasan Ramezani, Adrian Garcia Badaracco, Terrence Dorsey, David Montague, Serge Matveenko, Marcelo Trylesinski, Sydney Runkle, David Hewitt, Alex Hall, and Victorien Plot. Pydantic, 2025. URL https://docs.pydantic.dev/latest/. Jayanaka Dantanarayana, Yiping Kang, Kugesan Sivasothynathan, Christopher Clarke, Baichuan Li, Savini Kashmira, Krisztian Flautner, Lingjia Tang, and Jason Mars. Meaning-Typed Programming: Language Abstraction and Runtime for Model-Integrated Applications. In Principles of Programming Languages, 2025. Harley Davis, Pierre Parquier, and Nitsan Séniak. Sweet harmony: the Talk/C++ connection. In Lisp, 1994. Marius-Constantin Dinu. SymbolicAI: Neuro-Symbolic Perspective on Large Language Models (LLMs), 2022. URL https://github.com/ExtensityAI/symbolicai. Honghua Dong, Qidong Su, Yubo Gao, Zhaoyu Li, Yangjun Ruan, Gennady Pekhimenko, Chris Maddison, and Xujie Si. APPL: Prompt Programming Language for Harmonious Integration of Programs and Large Language Model Prompts. arXiv preprint arXiv:2406.13161, 2024. Paul Emmerich, Simon Ellmann, Fabian Bonk, Alex Egger, Esaú García Sánchez-Torija, Thomas Günzel, Sebastian Di Luzio, Alexandru Obada, Maximilian Stadlmeier, Sebastian Voit, et al. The Case For Writing Network Drivers In High-Level Programming Languages. In ACM/IEEE Symposium on Architectures for Networking and Communications Systems, 2019. Li Feng, Ryan Yen, Yuzhe You, Mingming Fan, Jian Zhao, and Zhicong Lu. Coprompt: Supporting prompt sharing and referring in collaborative natural language programming. In Conference on Human Factors in Computing Systems, 2024. Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023. Sigbjorn Finne, Daan Leijen, Erik Meijer, and Simon Peyton Jones. Calling hell from heaven and heaven from hell. In International Conference on Functional Programming, 1999. Firebase Team. Genkit: An open source framework for building AI-powered apps. https://github. com/firebase/genkit. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. In International Conference on Machine Learning. Pmlr, 2023. Noam Gat. lm-format-enforcer: Enforce the output format (JSON Schema, Regex etc) of language model. https://github.com/noamgat/lm-format-enforcer. Robert M. Graham. System Module Interfaces (PL/I Subset for System Programming), 1966. Guidance AI. Guidance: guidance language for controlling large language models. https: //github.com/guidance-ai/guidance. 12 Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948, 2025. Horace He and Thinking Machines Lab. Defeating Nondeterminism in LLM Inference. Thinking Machines Lab: Connectionism, 2025. Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin Wang, and Sadid Hasan. Does prompt formatting have any impact on llm performance? arXiv preprint arXiv:2411.10541, 2024. Di Huang, Ziyuan Nan, Xing Hu, Pengwei Jin, Shaohui Peng, Yuanbo Wen, Rui Zhang, Zidong Du, Qi Guo, Yewen Pu, et al. ANPL: towards natural programming with interactive decomposition. In Advances in Neural Information Processing Systems, 2023. Galen Hunt and James Larus. Singularity: Rethinking The Software Stack. ACM SIGOPS Operating Systems Review, 41(2), 2007. David Jeffery, Tyson Dowd, and Zoltan Somogyi. MCORBA: CORBA binding for Mercury. In International Symposium on Practical Aspects of Declarative Languages, 1999. Tian Jin, Ellie Cheng, Zack Ankner, Nikunj Saunshi, Blake Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, and Michael Carbin. Learning to Keep Promise: Scaling Language Model Decoding Parallelism with Learned Asynchronous Decoding. arXiv preprint arXiv:2502.11517, 2025. Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. In International Conference on Learning Representations, 2024. Donald Ervin Knuth. Literate Programming. The Computer Journal, 27(2), 1984. Dimitri Konstantas. Object Oriented Interoperability. In European Conference on Object-Oriented Programming, 1993. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Symposium on Operating Systems Principles, 2023. Shadaj Laddad and Koushik Sen. ScalaPy: seamless python interoperability for cross-platform scala programs. In International Symposium on Scala, 2020. Arthur Lafrance, David Detweiler, Zhaofeng Li, Xiangdong Chen, Vikram Narayanan, and Anton Burtsev. Extending Rust with Support for Zero Copy Communication. In Proceedings of the 12th Workshop on Programming Languages and Operating Systems, 2023. Daan Leijen. Algebraic effects for functional programming. Microsoft Research Technical Report, 2016. Linyang Li, Demin Song, and Xipeng Qiu. Text adversarial purification as defense against adversarial attacks. In Acl, 2023. Jenny Liang, Melissa Lin, Nikitha Rao, and Brad Myers. Prompts are programs too! understanding how developers build software containing prompts. Foundations of Software Engineering, 2025. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic Evaluation of Language Models. Transactions on Machine Learning Research, 2023. Jason Liu and Contributors. Instructor: library for structured outputs from large language models, 2024. URL https://github.com/instructor-ai/instructor. Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang, and Yuxiao Dong. Apar: Llms can do auto-parallel auto-regressive decoding. arXiv preprint arXiv:2401.06761, 2024. Jacob Matthews and Robert Bruce Findler. Operational semantics for multi-language programs. In Principles of Programming Languages, 2007. Stephen Mell, Botong Zhang, David Mell, Shuo Li, Ramya Ramalingam, Nathan Yu, Steve Zdancewic, and Osbert Bastani. Fast, Reliable, and Secure Programming Language for LLM Agents with Code Actions. arXiv preprint arXiv:2506.12202, 2025. Microsoft. The Component Object Model Specification. https://learn.microsoft.com/ en-us/windows/win32/com/component-object-model--com--portal, a. Microsoft. TypeChat: Build natural language interfaces using types. https://github.com/ microsoft/TypeChat, b. Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao, and Muhao Chen. Testtime backdoor mitigation for black-box large language models with defensive demonstrations. In Naacl, 2025. Model Context Protocol. What is the Model Context Protocol (MCP)? https:// modelcontextprotocol.io/docs/getting-started/intro. Vikram Narayanan, Tianjiao Huang, David Detweiler, Dan Appel, Zhaofeng Li, Gerd Zellweger, and Anton Burtsev. {RedLeaf}: Isolation And Communication In Safe Operating System. In USENIX Symposium on Operating Systems Design and Implementation, 2020. Bruce Jay Nelson. Remote procedure call. 1981. Object Management Group. Common Object Request Broker Architecture (CORBA), Version 1.1. Technical report, Object Management Group, 1991. URL https://www.omg.org/spec/CORBA/ 1.1/. Katsumi Okuda and Saman Amarasinghe. Askit: Unified programming interface for programming with large language models. In International Symposium on Code Generation and Optimization, 2024. OpenAI. OpenAI API Documentation Code Interpreter. https://platform.openai.com/ docs/guides/tools-code-interpreter. OpenAI. Introducing OpenAI o3 and o4-mini, 2025. URL https://openai.com/index/ introducing-o3-and-o4-mini/. Oracle. Java Remote Method Invocation: 2 - Distributed Object Model, 2017. URL https: //docs.oracle.com/javase/9/docs/specs/rmi/objmodel.html. Ianna Osborne, Jim Pivarski, and Jerry Ling. Bridging Worlds: Achieving Language Interoperability In International Workshop on Advanced between Julia and Python in Scientific Computing. Computing and Analysis Techniques in Physics Research, 2024. Jiayi Pan, Xiuyu Li, Long Lian, Charlie Snell, Yifei Zhou, Adam Yala, Trevor Darrell, Kurt Keutzer, and Alane Suhr. Learning Adaptive Parallel Reasoning with Language Models. arXiv preprint arXiv:2504.15466, 2025. Fernando Pérez and Brian Granger. IPython: system for interactive scientific computing. Computing in Science & Engineering, 9(3), 2007. Mansi Phute, Alec Helbling, Matthew Hull, ShengYun Peng, Sebastian Szyller, Cory Cornelius, and Duen Horng Chau. Llm self defense: By self examination, LLMs know they are being tricked. arXiv preprint arXiv:2308.07308, 2023. Gordon Plotkin and John Power. Algebraic operations and generic effects. Applied Categorical Structures, 11(1), 2003. 14 Gordon Plotkin and Matija Pretnar. Handling algebraic effects. Logical Methods in Computer Science, 9, 2013. PrefectHQ. Marvin: AI agents that spark joy. https://github.com/PrefectHQ/marvin. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with\" gradient descent\" and beam search. In Emnlp, 2023. Python Software Foundation. Extending and Embedding the Python Interpreter. Python Software Foundation, 2025. URL https://docs.python.org/3/extending/extending.html. Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong Sun. Onion: simple and effective defense against textual backdoor attacks. In Emnlp, 2021. Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. arXiv preprint Is ChatGPT General-Purpose Natural Language Processing Task Solver? arXiv:2302.06476, 2023. Adrian Sampson, Werner Dietl, Emily Fortuna, Danushen Gnanapragasam, Luis Ceze, and Dan Grossman. EnerJ: Approximate data types for safe and general low-power computation. In International Conference on Programming Language Design and Implementation, 2011. Tobias Schnabel and Jennifer Neville. Prompts as programs: structure-aware approach to efficient compile-time prompt optimization. arXiv preprint arXiv:2404.02319, 2024. Reshabh Sharma, Jonathan De Halleux, Shraddha Barke, and Benjamin Zorn. PromptPex: Automatic Test Generation for Language Model Prompts. arXiv preprint arXiv:2503.05070, 2025. Zhi Rui Tam, Cheng-Kuang Wu, Yi-Lin Tsai, Chieh-Yen Lin, Hung-yi Lee, and Yun-Nung Chen. Let Me Speak Freely? Study On The Impact Of Format Restrictions On Large Language Model Performance. In Conference on Empirical Methods in Natural Language Processing: Industry Track, 2024. The MathWorks, Inc. MATLAB Engine API for Python. https://github.com/mathworks/ matlab-engine-for-python, 2025. Raja Vallée-Rai, Phong Co, Etienne Gagnon, Laurie Hendren, Patrick Lam, and Vijay Sundaresan. Soot: Java bytecode optimization framework. In Cascon, 1999. Mandana Vaziri, Louis Mandel, Claudio Spiess, and Martin Hirzel. PDL: Declarative Prompt Programming Language. arXiv preprint arXiv:2410.19135, 2024. Thorsten Von Eicken, Chi-Chao Chang, Grzegorz Czajkowski, Chris Hawblitzel, Deyu Hu, and Dan Spoonhower. J-Kernel: Capability-Based Operating System for Java. Secure Internet Programming: Security Issues for Mobile and Distributed Objects, 1999. Andrew Wagner, Zachary Eisbach, and Amal Ahmed. Realistic Realizability: Specifying ABIs You Can Count On. In Object-Oriented Programming, Systems, Languages and Applications, 2024. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent Abilities of Large Language Models. Transactions on Machine Learning Research, 2022. Zeming Wei, Yifei Wang, Ang Li, Yichuan Mo, and Yisen Wang. Jailbreak and guard aligned language models with only few in-context demonstrations. arXiv preprint arXiv:2310.06387, 2023. Brandon Willard and Rémi Louf. Efficient Guided Generation for LLMs. arXiv preprint arXiv:2307.09702, 2023. Haoran Xu and Fredrik Kjolstad. Copy-and-patch compilation: fast compilation algorithm for high-level languages and bytecode. In Principles of Programming Languages, 2021. 15 Xinyu Yang, Yuwei An, Hongyi Liu, Tianqi Chen, and Beidi Chen. Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation. arXiv preprint arXiv:2506.09991, 2025. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. Optimizing generative AI by backpropagating language model feedback. Nature, 639(8055), 2025. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. In Advances in Neural Information Processing Systems, 2024. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and In International Jimmy Ba. Large Language Models Are Human-Level Prompt Engineers. Conference on Learning Representations, 2022. 16 \"nodes\": list(g.nodes), \"edges\": [{\"src\": src, \"tgts\": list(tgts)} for src, tgts in g.edges.items()] f\"\"\"Perform the <query> with respect to <graph>, where nodes are paper IDs and edges point from cited paper to set of papers that cite it. Return `break_flag` as True if the <query> indicates termination. Else, return `response`. If graph was updated, return as `graph`."
        },
        {
            "title": "A EXAMPLE DETAILS",
            "content": "A.1 FULL MANUAL IMPLEMENTATION PROGRAM 1 from pydantic import BaseModel 2 from llm_wrapper import llm 3 4 class Graph: 5 6 7 8 class EdgeSchema(BaseModel): 9 nodes: set[int] edges: dict[int, set[int]] src: int tgts: list[int] 10 11 12 class GraphSchema(BaseModel): 13 14 15 16 def serialize(g: Graph) -> str: 17 nodes: list[int] edges: list[EdgeSchema] = { 18 19 20 32 33 34 35 36 37 38 39 48 49 51 52 } return json.dumps(s) 21 22 23 def reify(gs: GraphSchema) -> Graph: 24 25 nodes = set(gs.nodes) edges = {e.src: set(e.tgts) for in gs.edges} return Graph(nodes=nodes, edges=edges) 26 27 28 def main(): 29 30 31 graph = Graph(...) while True: query = input(\"Q: \") class OutputSchema(BaseModel): response: Optional[str, int, bool] break_flag: bool graph: Optional[GraphSchema] q_out = llm( 40 41 42 43 <query>{query}</query> 44 <graph>{serialize(graph)}</graph>\"\"\", 45 46 47 output_type=OutputSchema) if q_out.break_flag: break response = q_out.response if q_out.graph: graph = reify(q_out.graph) print(f\"A: {response}\") 53 54 main() A.2 EXAMPLE PROGRAM INPUTS 'Give the number of papers that cite paper 19.' 'Does paper 23 directly/indirectly get cited by paper 4.' 'How many papers cite both paper 7 and paper 5?' 'Update the graph so paper 5 cites paper 14.' 'Remove paper 0 from the graph completely.' 'Exit, please.' 17 A.3 EXAMPLE TRACE Effect: eval(str(type(query))) Result: \"<class 'str'>\" Effect: eval(query) Result: \"Update the graph so paper 5 cites paper 14.\" Effect: eval(str(type(graph))) Result: \"<class '<run_path>.Graph'>\" Effect: eval(hasattr(graph, '__doc__')) Result: True Effect: eval(graph.__doc__) Result: \"A directed graph. Nodes are represented by set of node values. Edges are represented by dictionary of source node value to set of target node values.\" Effect: eval(str(dir(graph))) Result: \"['__annotations__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__ ', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__ ', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', ' __ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', ' __slotnames__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', 'edges', ' nodes']\" Effect: eval(str(type(graph.nodes))) Result: \"<class 'set'>\" Effect: eval(str(type(graph.edges))) Result: \"<class 'dict'>\" Effect: eval(str(graph.nodes)) Result: \"{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}\" Effect: eval(str(graph.edges)) Result: \"{16: {4, 5, 10, 11, 17, 19, 21}, 12: {1, 5, 6, 7, 9, 10, 11, 14, 15, 17, 18, 19, 23, 24}, 9: {1, 2, 3, 4, 5, 6, 7, 10, 15, 20, 21, 22, 23}, 19: {0, 1, 4, 5, 10, 14, 15, 17, 20}, 18: {0, 3, 5, 6, 17, 20, 21, 22, 23}, 6: {3, 4, 8, 11, 13, 14, 20, 21, 22, 24}, 5: {2, 3, 7, 8, 11, 13, 14, 21, 23, 24}, 10: {0, 1, 2, 3, 7, 8, 11, 14, 21, 24}, 15: {0, 1, 2, 4, 7, 13, 14, 17, 23, 24}, 21: {2, 3, 4, 7, 8, 11, 23, 24}, 11: {1, 3, 4, 17, 23}, 17: {2, 3, 7, 8, 22, 23}, 1: {3, 4, 7, 8, 20, 22}, 14: {2, 7, 8, 13, 24}, 22: {0, 2, 20, 13}, 13: {24, 0, 4, 7}, 2: {3, 7, 20, 23, 24}, 23: {24, 0, 4}, 4: {24, 8, 7}, 24: {0, 3, 20}, 8: {0, 3, 20}, 3: {20}}\" Effect: exec(# Planning phase: understand the query and how to execute it # The query is \"Update the graph so paper 5 cites paper 14.\" # In the graph, edges point from cited paper to papers that cite it # So if paper 5 cites paper 14, then there should be an edge from 14 to 5 # Looking at current edges, 14 -> {2, 7, 8, 13, 24} # We need to add 5 to the set of papers that cite paper 14 nj__thought = \"\"\" The query asks to update the graph so paper 5 cites paper 14. In this graph representation, edges point from cited paper to citing papers. So if paper 5 cites paper 14, we need to add an edge from 14 to 5. Currently, paper 14 has edges to {2, 7, 8, 13, 24}. We need to add 5 to this set. The strategy is: 1. Check if 14 is already in graph.edges (it is) 2. Add 5 to the set graph.edges[14] 3. Set response to indicate success This will take about 3-4 tool calls. \"\"\") Result: Success Effect: exec(# Execute the update: paper 5 cites paper 14 # This means adding 5 to the set of papers that cite paper 14 graph.edges[14].add(5) # Set the response to confirm the update response = \"Updated\") Result: Success Effect: eval(str(graph.edges[14])) Result: {2, 5, 7, 8, 13, 24} Effect: done() 18 Effect: eval(str(type(query))) Result: \"<class 'str'>\" Effect: eval(query) Result: \"Exit, please.\" Effect: eval(str(type(graph))) Result: \"<class '<run_path>.Graph'>\" Effect: eval(hasattr(graph, '__doc__')) Result: True Effect: eval(graph.__doc__) Result: \"A directed graph. Nodes are represented by set of node values. Edges are represented by dictionary of source node value to set of target node values.\" Effect: eval(str(dir(graph))) Result: \"['__annotations__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__ ', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__ ', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', ' __ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', ' __slotnames__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', 'edges', ' nodes']\" Effect: eval(str(type(graph.nodes))) Result: \"<class 'set'>\" Effect: eval(str(type(graph.edges))) Result: \"<class 'dict'>\" Effect: exec(nj__thought = \"The query is 'Exit, please.' which indicates termination. need to break according to the instruction. The graph is Graph object with nodes (set) and edges (dict) representing citation relationships where edges point from cited papers to citing papers.\") Result: Success Effect: break()"
        },
        {
            "title": "B NATURAL FUNCTION INTERFACE SYNTAX AND SEMANTICS",
            "content": "B.1 NATURAL FUNCTION INTERFACE"
        },
        {
            "title": "Host Values",
            "content": "v Serialization α : vN"
        },
        {
            "title": "Reification",
            "content": "γ : vN v"
        },
        {
            "title": "Natural Code Evaluation",
            "content": "nat : vN opN"
        },
        {
            "title": "NFI Effects",
            "content": "vN opN"
        },
        {
            "title": "NFI Handler Clauses",
            "content": "η ::= finally e"
        },
        {
            "title": "Program",
            "content": "opN (x ) e; η hN ::= handle {η} ::= eH hN (nat(n, α(v ))) exit NFI handler opN / η host language natural code Figure 4: Natural Function Interface. ::= [] ... hN {η}(E) ::= E, hN"
        },
        {
            "title": "FINALLY",
            "content": "finally hN ρ, σ, hN {η}(v ) ρ[x (cid:55) ], σ, e"
        },
        {
            "title": "HANDLE",
            "content": "opN (x ) hN ρ, σ, hN {η}(E [opN (v )]) ρ[x (cid:55) , resume (cid:55) E, hN ], σ, e"
        },
        {
            "title": "RESUME",
            "content": "ρ, σ, E, hN ρ, σ, hN {η}(E [v ]) NAT-EX n opN ρ, σ, nat(n, ) ρ, σ, nat(n , opN ) NAT-RET n vN ρ, σ, nat(n, ) ρ, σ, vN"
        },
        {
            "title": "Scope",
            "content": "ρ : (cid:55) Heap σ : (cid:55) Figure 5: Operational semantics of the Natural Function Interface. The represents the evaluation of natural code with an LLM system. B.2 NATURAL FUNCTION INTERFACE FOR TOOL USE"
        },
        {
            "title": "Tools",
            "content": "f F"
        },
        {
            "title": "Natural Code n",
            "content": "Serialization α : vN Natural Code Evaluation Reification natF : vN opF γ : vN v"
        },
        {
            "title": "NFI Values",
            "content": "vN v"
        },
        {
            "title": "NFI Effects",
            "content": "opF ::= Call(f, vN )"
        },
        {
            "title": "NFI Handler",
            "content": "hN ::= handle { finally vN γ(vN ); Call(f, vN ) resume α(f (γ(vN ))); call tool"
        },
        {
            "title": "Program",
            "content": "e ::= eH } hN (natF (n, α(v ))) host language natural code with tool use Figure 6: Natural Function Interface for tool use. The host system or the programmer supplies list of available function/tool names to functions for natural code to use, denoted as . B.3 NATURAL FUNCTION INTERFACE WITH ISOLATED PROGRAM STATE"
        },
        {
            "title": "Host Values",
            "content": "v"
        },
        {
            "title": "Natural Code n",
            "content": "Serialization α : vN Natural Code Evaluation"
        },
        {
            "title": "Reification",
            "content": "γ : vN natF : vN opF N"
        },
        {
            "title": "NFI Values",
            "content": "vN v"
        },
        {
            "title": "NFI Handler",
            "content": "hN ::= handle { finally vN γ(vN ); }"
        },
        {
            "title": "Program",
            "content": "e ::= eH hN (nat(n, α(v ))) host language natural code Figure 7: Syntax of the Natural Function Interface without shared program state. Natural code only takes constant value and returns constant value. There are no program state effects, so the NFI handler is no-op."
        },
        {
            "title": "C BENCHMARKS",
            "content": "Table 3: SPSBench programs with their descriptions."
        },
        {
            "title": "Description",
            "content": "PythonSpecific Features avg_of_three balance_parentheses battle calendar character_builder coffeeshop email_classification enemy fact_checking feed_filtering filter_numbers get_enemy graph gsm8k health_data mask_sensitive_info menu pos_tagging proposer_verifier sentiment stores stream_generator task_completion tiny_bookshop wordle_solver Average three input numbers, represented as numbers and words in multiple natural languages Semantically balance parentheses in an expression based on context (e.g. order of operations or regex) Updating list of objects based on natural language description of player action. Object state mutation using only public methods. Shows how to do regular tool use with Nightjar. Adapted from Microsoft (b) Creation of object of predefined dataclass Parse input into items in coffee order, limited to the options available in the shop. Throws an error if input includes unparseable items. Shows how to do Structured output with Nightjar. Adapted from Microsoft (b) Classify email with enum and raise exception if no fitting category. Adapted from Anthropic (a); PrefectHQ Dynamic object creation and manipulation. Gathers claims from string and appends fact-check results to list in place Functional string list filtering based on preference Filter list of numbers by semantic property Closure creation returning object of unspecified class Program shown in Figure 1a with 6 natural language queries Using eval to solve gsm8k (Cobbe et al., 2021) math problems Chatbot that helps the user input health data. Adapted from Microsoft (b) Create string with sensitive information masked with asterisks Object state mutation to update inventory Dynamic closure creation for translation generation based on input language Proposer-verifier loop on riddles, using only functional generations Sentiment analysis. Adapted from Chase (2022); Microsoft (b); OpenAI; PrefectHQ Create subclasses based on input, and instantiating these subclasses. Adapted from Chase (2022); Microsoft (b) Create generator relevant to input Mutate object state to mark tasks as complete Retrieve book for user based on request Nested function that uses natural code solve wordle"
        },
        {
            "title": "D ADDITIONAL EVALUATION RESULTS",
            "content": "D.1 AVERAGE PASS RATE BY BENCHMARK Figure 8: Average pass rate over runs for each benchmark in SPSBench with standard deviation for each method using Claude-Sonnet-4-20250514. Figure 9: Average pass rate over runs for each benchmark in SPSBench with standard deviation for each method using GPT-4.1-2025-04-14. D.2 SHARED PROGRAM STATE IMPLEMENTATION ABLATIONS We performed ablations of the optimization features we implemented in NIGHTJAR. We evaluated the following system designs: NIGHTJAR (Baseline): Baseline implementation of NIGHTJAR using the shared program state NFI, without any specialization or optimizations. NIGHTJAR (Isolated Eval): Baseline implementation of NIGHTJAR with Python computation enabled using IsolatedEval to evaluate Python expression with isolated state. It also uses Python-specialized control flow tools. The effects are IsolatedEval, Lookup, Assign, Deref, Ref, Set, Break, Continue, Return, and Raise. NIGHTJAR (Shared Eval): Implementation of NIGHTJAR with Python computation enabled using Eval to evaluate Python expression on the shared program state, replacing Lookup and Ref. It also uses Python-specialized control flow tools. The effects are Eval, Lookup, Assign, Deref, Set, Break, Continue, Return, and Raise. NIGHTJAR (Shared Python): Python-specialized implementation of NIGHTJAR. The effects are Eval, Exec, Break, Continue, Return, and Raise. NIGHTJAR (Shared Python, Caching): Python-specialized implementation of NIGHTJAR with caching. NIGHTJAR: Our main implementation of NIGHTJAR, specialized to Python and with the eager variable loading optimization, and caching. LLM Code Generation: An alternative approach to realizing the shared program state abstraction by using an LLM to statically generate Python code (that can contain LLM API calls) to replace the natural block given the source code of the program. Table 4 shows the average pass rate with standard deviation, the average runtime, compile time, and total time with ranges on SPSBench over 5 runs. The LLM code generation approach can only inspect the program statically, which means the LLM cannot adapt the computation to program inputs or intermediate program values. This results in poor accuracy, despite faster runtime than NIGHTJARs interpreter approach. NIGHTJAR (Baseline) does not perform as well as the language-specialized NIGHTJAR implementations in pass rate because it does not support language-specific features of the host-language, such as first-order functions and abstract methods. It also only enables LLM-based computation to transform data. Table 5 shows the performance of the ablations and the manual implementation baselines on the subset of benchmarks in SPSBench that do not require these language-specific features. In terms of pass rate on the benchmarks that do not need language-specific features, NIGHTJAR (Baseline) is within standard deviation from the manual implementation for GPT-4.1 and slightly below the pass rate of manual implementation for Sonnet 4. For example, NIGHTJAR (Isolated Eval) enables the use of Python-based computation rather than relying only on LLM-based computation to transform data. However, it still cannot utilize language-specific data types. the LLM in NIGHTJAR (Isolated Eval) issues the following effects to compute one of the test cases of the average_of_three benchmark: Effect: Lookup(p) Result: \"one\" Effect: Lookup(q) Result: \"two\" Effect: Lookup(r) Result: \"three\" Effect: IsolatedEval(({\"one\": 1, \"two\": 2, \"three\": 3}.get(\"one\", 0) + {\"one\": 1, \"two\": 2, \"three\": 3}. get(\"two\", 0) + {\"one\": 1, \"two\": 2, \"three\": 3}.get(\"three\", 0)) / 3.0) Result: 2. The LLM uses IsolatedEval to reliably compute the math expression, but because the scope is not shared, it has to use the values of p, q, and in the Python expression. In the subset of benchmarks that does not require these data types, for Sonnet 4, NIGHTJAR (Isolated Eval) achieves parity with the manual implementation in pass rate. For GPT-4.1, NIGHTJAR (Isolated Eval) achieves slightly higher pass rate than NIGHTJAR (Baseline) and the manual implementation with isolated code interpreter enabled. This indicates that GPT-4.1 could not use IsolatedEval effectively but Sonnet 4 could. The NIGHTJAR (Shared Eval) ablation enables Eval to read from the shared program state. However, the LLM is still restricted to using Python expressions only, rather than freeform Python statements, and cannot define Python-specific data types. This enables the LLM to use more compact syntax in the form of Python code to represent sequence of effects. For example, for the same 24 program and test case as above, the LLM NIGHTJAR (Shared Eval) issues the following effects: Effect: Eval(p) Result: 'one' Effect: Eval(q) Result: 'two' Effect: Eval(r) Result: 'three' Effect: Eval(({'one': 1, 'two': 2, 'three': 3}[p] + {'one': 1, 'two': 2, 'three': 3}[q] + {'one': 1, ' two': 2, 'three': 3}[r]) / 3.0) Result: 2.0 Specifically, the LLM refers to the variables p, q, and directly in the code because the program state is shared. Python code is also more familiar interface than the novel natural function interface. In the subset of benchmarks that does not require Python-specific features, with Sonnet 4, NIGHTJAR (Shared Eval) achieves parity with NIGHTJAR in pass rate, which is better than the manual implementations. However, for GPT-4.1, NIGHTJAR (Shared Eval) has slightly lower pass rate than manual implementations and NIGHTJAR. The disparity indicates that Sonnet 4 is more capable of utilizing Python expressions with shared program state than GPT-4.1. NIGHTJAR (Shared Python) improves pass rate by enabling language-specific features and the use of freeform Python statements, but the runtime overhead is maintained. The Exec tool enables the use of Python statements on top of Python expression with Eval. This enables the LLM to generate freeform Python code that is not subject to constrained decoding, which has been found to reduce LLM reasoning performance (He et al., 2024; Tam et al., 2024). Additionally, Python code is more familiar interface than the novel natural function interface. This is shown by NIGHTJAR (Shared Python) performing better than the programmer implementation on the complete SPSBench for both Sonnet 4 and GPT-4.1. Adding optimizations reduces the runtime. The caching optimization in NIGHTJAR (Shared Python, Caching) only minorly reduces the runtime. This is because the test cases in SPSBench programs consist of different program inputs. The only time savings achieved by caching optimization is saving the time to emit the effects used before reading program inputs. The main implementation NIGHTJAR demonstrates that the runtime overhead of shared program state can be reduced with additional engineering efforts and optimizations. Table 4: Performance on SPSBench. Average pass rate and runtime with ranges in gray. Pass Rate Runtime (s) Compile Time (s) Total Time (s) # Effects"
        },
        {
            "title": "Method",
            "content": "Sonnet 4 Manual Impl 0.780.03 Sonnet 4 Manual Impl (Isolated Code Interp) 0.660.04 Sonnet 4 NIGHTJAR (Baseline) Sonnet 4 NIGHTJAR (Isolated Eval) Sonnet 4 NIGHTJAR (Shared Eval) Sonnet 4 NIGHTJAR (Shared Python) 0.600.04 0.720.06 0.780.04 0.850. Sonnet 4 NIGHTJAR (Shared Python, Caching) 0.840.02 Sonnet 4 NIGHTJAR Sonnet"
        },
        {
            "title": "LLM Code Generation",
            "content": "GPT-4.1 Manual Impl 0.850.03 0.360.06 0.740.03 GPT-4.1 Manual Impl (Isolated Code Interp) 0.720. GPT-4.1 NIGHTJAR (Baseline) GPT-4.1 NIGHTJAR (Isolated Eval) GPT-4.1 NIGHTJAR (Shared Eval) GPT-4.1 NIGHTJAR (Shared Python) 0.610.02 0.610.02 0.620.08 0.810. GPT-4.1 NIGHTJAR (Shared Python, Caching) 0.780.03 GPT-4."
        },
        {
            "title": "NIGHTJAR",
            "content": "GPT-4."
        },
        {
            "title": "LLM Code Generation",
            "content": "0.780.03 0.450.02 8.0 (1.3-44.8) 36.8 (3.2-171.5) 55.6 (9.6-721.0) 100.1 (14.9-870.0) 118.1 (10.6-550.6) 75.8 (10.1-236.4) 70.6 (5.5-244.8) 25.9 (6.2-83.0) 7.9 (0.0-41.0) 4.5 (0.8-15.0) 50.3 (10.9-364.9) 39.0 (4.7-957.4) 51.7 (7.8-282.1) 75.0 (6.9-579.4) 71.8 (4.0-417.0) 66.2 (7.9-406.1) 19.6 (3.0-72.1) 2.8 (0.0-31.8) - 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 7.3 (1.4-17.8) - 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.1) 5.0 (0.9-23.7) 8.0 (1.3-44.8) 36.8 (3.2-171.5) 55.6 (9.6-721.0) 100.1 (14.9-870.0) 118.1 (10.6-550.6) 75.8 (10.1-236.5) 70.6 (5.5-244.8) 25.9 (6.2-83.1) 15.2 (1.4-55.6) 4.5 (0.8-15.0) 50.3 (10.9-365.0) 39.0 (4.7-957.4) 51.7 (7.8-282.1) 75.0 (6.9-579.4) 71.8 (4.0-417.0) 66.2 (7.9-406.1) 19.7 (3.1-72.1) 7.7 (0.9-36.9) - 2.6 (0-14) 27.8 (6-300) 34.7 (6-30) 40.6 (6-183) 27.3 (2-93) 27.9 (2-106) 8.9 (2-33) - - 2.8 (1.0-21.0) 24.9 (4-162) 32.3 (6.0-174.0) 49.2 (6.0-289.0) 63.1 (2-299) 68.6 (7-300) 16.6 (2-96) - 26 Table 5: Performance results of subset of SPSBench that does not require first-order functions or abstract methods. See Section for which programs are in this subset. Average pass rate and runtime with ranges in gray. Pass Rate Runtime (s) Compile Time (s) Total Time (s) # Effects"
        },
        {
            "title": "Method",
            "content": "Sonnet 4 Manual Impl 0.780.04 Sonnet 4 Manual Impl (Isolated Code Interp) 0.630.04 Sonnet 4 NIGHTJAR (Baseline) Sonnet 4 NIGHTJAR (Isolated Eval) Sonnet 4 NIGHTJAR (Shared Eval) Sonnet 4 NIGHTJAR (Shared Python) 0.690.05 0.760.04 0.850.03 0.830. Sonnet 4 NIGHTJAR (Shared Python, Caching) 0.830.03 Sonnet 4 NIGHTJAR Sonnet"
        },
        {
            "title": "LLM Code Generation",
            "content": "GPT-4.1 Manual Impl 0.880.03 0.410.05 0.750.04 GPT-4.1 Manual Impl (Isolated Code Interp) 0.710. GPT-4.1 NIGHTJAR (Baseline) GPT-4.1 NIGHTJAR (Isolated Eval) GPT-4.1 NIGHTJAR (Shared Eval) GPT-4.1 NIGHTJAR (Shared Python) 0.700.03 0.720.04 0.700.04 0.790. GPT-4.1 NIGHTJAR (Shared Python, Caching) 0.760.05 GPT-4."
        },
        {
            "title": "NIGHTJAR",
            "content": "GPT-4."
        },
        {
            "title": "LLM Code Generation",
            "content": "0.760.02 0.480.06 8.2 (1.3-44.8) 42.5 (3.2-171.5) 57.8 (9.6-721.0) 103.2 (14.9-870.0) 104.1 (10.6-464.2) 85.6 (21.6-236.4) 75.6 (19.0-244.8) 29.3 (9.1-83.0) 7.9 (0.0-38.6) 4.6 (0.8-15.0) 52.5 (10.9-364.9) 44.9 (4.7-957.4) 55.0 (7.8-282.1) 71.7 (6.9-579.4) 79.9 (11.4-417.0) 68.4 (11.2-406.1) 22.7 (3.3-72.1) 0.3 (0.0-1.8) - 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 7.5 (3.6-17.8) - 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 0.0 (0.0-0.1) 5.0 (1.6-13.4) 8.2 (1.3-44.8) 42.5 (3.2-171.5) 57.8 (9.6-721.0) 103.2 (14.9-870.0) 104.2 (10.6-464.2) 85.6 (21.6-236.5) 75.7 (19.0-244.8) 29.3 (9.1-83.1) 15.4 (3.6-55.5) 4.6 (0.8-15.0) 52.5 (10.9-365.0) 44.9 (4.7-957.4) 55.0 (7.8-282.1) 71.7 (6.9-579.4) 79.9 (11.5-417.0) 68.4 (11.2-406.1) 22.7 (3.3-72.1) 5.3 (1.6-13.4) - 3.1 (0-14) 29.6 (6-300) 36.2 (6-300) 36.6 (6-119) 29.2 (7-93) 30.5 (8-106) 10.3 (4-33) - - 3.0 (1.0-21.0) 27.7 (4-162) 35.3 (6.0-174.0) 48.6 (6.0-289.0) 67.2 (12-299) 70.9 (11-300) 18.3 (2-96) - D.3 LOCALLY HOSTED LLM We evaluate the performance of NIGHTJAR using smaller, locally hosted model as the base LLM on SPSBench. We used GPT-OSS 20B served through VLLM (Kwon et al., 2023) on an A100 80GB GPU. Table 6 shows the average pass rate with standard deviation, the average runtime with ranges, and the average number of emitted effects with ranges over 5 runs. Using GPT-OSS 20B, the pass rate of NIGHTJAR is lower than the manual implementation. This is because the NIGHTJAR implementation relies heavily on the agentic capabilities of the base LLM. Further system prompt engineering, optimization, or finetuning could improve the pass rate when using smaller model. Nevertheless, NIGHTJAR is still more performant than using an LLM to statically generate Python code (LLM Compiler). Table 6: Average pass rate, runtime, compile time, and number of effects using GPT-OSS 20B with ranges shown in gray. Pass Rate Runtime (s) Compile Time (s)"
        },
        {
            "title": "Total Time",
            "content": "# Effects"
        },
        {
            "title": "LLM Compiler",
            "content": "0.680.04 0.130.03 NIGHTJAR (Shared Python) 0.470."
        },
        {
            "title": "NIGHTJAR",
            "content": "0.400.02 - 4.3 (0.39-16.46) 0.0 (0.0-0.0) 0.0 (0.0-0.0) 5.3 (0.3-89.2) 5.3 (0.39-57.78) 54.2 (3.2-581.8) 36.5 (1.45-385.68) - - 28.1 (2-242) 8.0 (0-46) 5.3 (0.3-89.2) 1.2 (0.00-55.24) 54.2 (3.2-581.8) 36.5 (1.4-385.7) 27 D.4 TEMPERATURE 0 We performed an ablation of the evaluation with temperature of 0 on GPT-4.1. Table 7 shows the average pass rate with standard deviation and average runtime with ranges over 5 runs. As shown, NIGHTJAR also achieves parity in pass rate to manual implementations with temperature 0. We note that even using temperature 0 does not result in deterministic executions because the base LLM is closed model (He & Lab, 2025). Using temperature 0 on locally-hosted open model and controlling for LLM non-determinism (He & Lab, 2025) or caching and re-using responses to the same effect trace would enable deterministic NIGHTJAR executions. Table 7: Average pass rate and runtime with ranges in gray."
        },
        {
            "title": "Pass Rate",
            "content": "Time (s) # Effects GPT-4.1 Manual Impl 0.740.01 GPT-4.1 NIGHTJAR (Shared Python) 0.730. GPT-4.1 NIGHTJAR 0.770.04 5.0 (0.8-23.0) 82.9 (3.4-548.6) 28.9 (2.9-267.3) - 79.9 (4.0-300.0) 20.2 (2.0-149.0) D.5 LARGE DATA STRUCTURE The representation of formal data as they are accessed by an LLM factors into the program performance enabled by programming system. There are two methods to pass formal values to and from LLMs: pass-by-copy and pass-by-reference. Formal values are typically encoded as strings in JSON format and included in the prompt context for the LLM to process. This method is called pass-by-copy. It is intractable when computing with large objects, because LLMs have limited context length. The length of the encoded object string is linear to the size of the object, so sufficiently large object would be beyond the context length of the LLM. This is also the case for updating or writing an object, where the size of objects the LLM can compute with is limited by the maximum output length. In contrast, pass-by-reference avoids the need to encode objects into strings. For an LLM to read objects, it first receives reference to the object, which is only few digits in length and constant in the size of the object. During program execution, the LLM inspects only the relevant parts of the object it needs for the natural code execution. This is more efficient when the natural code requires the use of only part of the large object. We examine how pass-by-reference affects performance compared to pass-by-copy with case study using the Figure 1 programs. Figure 1c is implemented manually by programmer. It uses pass-by-copy by encoding the graph object into JSON format (Section 1) and passing it to the LLM in the prompt (Section 1). The output graph is also decoded via JSON format and reified into the Graph object (Sections 1 to 1). Methodology. We compare the performance of executing the Figure 1a with 6 different natural language queries (Section A.2) using NIGHTJAR and NIGHTJAR (Baseline) against the performance of executing the pass-by-copy implementation by programmer (the full Figure 1c program; Section A.1). With shared heap, NIGHTJAR (Baseline) uses pass-by-reference to represent mutable data structures NIGHTJAR also uses pass-by-reference, but also additionally enables the use of Python functions. We execute each program on generated graphs of exponentially increasing sizes 5 times each, using GPT-4.1-2025-04-14 on machine with an M1 Max chip and 64GB memory. Due to cost constraints, we set limit of 100 steps per query for the NIGHTJAR agent loop. Results. Figure 10 shows the performance. The pass-by-copy implementation generally achieves lower accuracy than NIGHTJAR implementations. On small graphs with fewer than 100 nodes, the pass-by-copy implementation generally achieves lower execution time, lower token usage, and lower cost. Since the graph is encoded into JSON format and appended into the prompt, the number of input tokens increases linearly with the number of nodes. After 1000 nodes, the encoded graph exceeds the context window size, resulting in failure to answer any queries. The execution time, output token usage, and cost also increase as the graph size increases. In comparison, NIGHTJAR and NIGHTJAR (Baseline), using pass-by-reference to represent mutable objects, query pieces of 28 Figure 10: Performance of the Figure 1 programs. The shaded area shows the range of values. The black dotted line marks when the encoded graph exceeds the context window size with pass-by-copy. the graph as necessary. Thus, the token usage and execution time of NIGHTJAR and NIGHTJAR (Baseline) increase at slower rate compared to the pass-by-copy implementation as the graph size increases. This enables NIGHTJAR (Baseline) to answer queries on large graphs when the pass-by-copy implementation cannot, resulting in the higher pass rates. The specialization to Python of NIGHTJAR enables the use of Python functions, which improves further the pass rate over the baseline implementation. While programmers can manually implement using pass-by-reference, NIGHTJAR supports this data representation style out of the box without additional configuration by the programmer. D.6 NUMBER OF EFFECTS TO RUNTIME There is direct relation between program runtime executed using NIGHTJAR with the number of effects used, as each emitted effect corresponds to one inference pass through the base LLM. Figure 11 is scatterplot of the number of effects used and the runtime of each program execution of SPSBench. It shows that the program runtime is proportional to the number of effects emitted during program execution, with different LLMs exhibiting different ratios of runtime to number of emitted effects. Figure 11: Number of emitted effects in program execution to runtime. D.7 ADDITIONAL RESULTS ON LLM MATH REASONING BENCHMARK We evaluate the performance of NIGHTJAR on the full evaluation set of GMS8K (Cobbe et al., 2021), consisting of 1319 math word problems using GPT-4.1. GSM8K is benchmark for evaluating LLM math reasoning, and thus does not require complex data structures nor meaningfully use formal program state to solve. In this sort of application, using NIGHTJAR is equivalent to using an LLM agent with code evaluation tools with isolated program states. We evaluate the performance of NIGHTJAR on GSM8K to empirically validate this and show that NIGHTJARs performance is competitive. We compare against the manual implementation of program, which queries the LLM for string expression that the program then invokes the Python eval function on. We also compare against version of manual implementation that employs tool use with the OpenAI Code Interpreter (which evaluates code in an isolated container) and version that implements tool use with custom, user-defined code evaluation tool that maintains isolated program states. Table 8 shows the pass rate, the average runtime, and the average number of tool calls or effects over 3 runs. NIGHTJAR achieves higher or equal accuracy to the manual implementations, demonstrating that the NIGHTJAR system and the shared program state setup do not detract from LLM capabilities on standard LLM tasks. NIGHTJAR incurs slightly higher runtime overhead than the basic manual implementation program, because NIGHTJAR requires the LLM to emit effects. However, this runtime overhead is similar to the overhead incurred by utilizing tool use with the OpenAI code interpreter tool. Additionally, NIGHTJARs runtime overhead can be reduced with further system engineering as shown by the optimized version of NIGHTJAR. Table 8: GSM8K results using GPT-4.1."
        },
        {
            "title": "Pass Rate",
            "content": "Avg Time (s) # Tool Calls or Effects"
        },
        {
            "title": "Manual Impl",
            "content": "0.840.01 Manual Impl w/ OpenAI Code Interpreter 0.920.01 Manual Impl w/ Custom Code Interpreter 0.700.01 NIGHTJAR (Shared Python)"
        },
        {
            "title": "NIGHTJAR",
            "content": "0.930.00 0.930.00 30 1.4 (0.8-50.4) 5.1 (0.1-304.7) 2.6 (1.0-24.9) 5.9 (2.3-58.2) 4.7 (1.7-36.3) - 0.9 (0-3) 1.6 (0-4) 6.6 (0-29) 4.1 (0-16)"
        },
        {
            "title": "E SYSTEM PROMPTS",
            "content": "NIGHTJAR system prompt: You are helpful assistant. <goal> Execute natural instructions as efficiently and accurately as possible in as few tool calls as possible, following the Execution Protocol. </goal> <syntax> The natural instructions use <variable> to denote Python variables being used inside the block and <: variable> to denote variables that can be used in the Python code after the block. Do not use the brackets when using tools. The instructions might also include the type and values (and/or references to values) to input variables in the format `{{var}} [type: {{type_name}}]: {{val_ref}} (Value: {{val_if_immutable}})`. </syntax> <execution_protocol> Follow this protocol to execute the instructions: 1. Discovery Phase: Explore the context and understand the data structures at hand if the type, values, and attributes are not already given in the natural instruction. Skip this phase if the information is already given in the natural instruction. - Tip: Do not inspect (via `eval`) type, values, and attributes already given in the natural instructions. - Tip: Use `str(type(var))` to get the type of variable. - Tip: Reference `__doc__` attribute for documentation. 2. Planning Phase: Plan out the best strategy to execute the instructions, with the highest accuracy with the least number of tool calls and tokens. Only use Python to perform computation if you don't can't calculate it directly. 3. Execution Phase: Execute the originally given natural instruction in as few tool calls and as few tokens as possible. - Tip: If you know the answer already without any computation (remember, you're really smart agent with common-sense world knowledge and reasoning capabilities) directly use the answer you know. For example, do not eval `x == 5` when it's already known that is 5. - Tip: Any Python builtins are also in the heap and can be used - Tip: Do not use any nonstandard Python libraries. - Tip: `eval` only returns immutable values (strings, integers, numbers, booleans, None). Everything else (including lists, dictionaries, tuples, etc.) is returned as an object reference. Use `str` to serialize them into string (e.g. `str([x for in my_list])`) to read or use `getattr` to read specific attribute. - Tip: `eval` only evaluates Python expressions. Use `exec` to evaluate Python statements. 4. Finish Phase: Use `continue`, `break, `return`, or `done` to finish the execution, following the instructions. - REQUIRED: Use `continue` if and only if the instruction says `continue` and the conditions for ` continue` are met. - REQUIRED: Use `break` if and only if the instructions says `break` and the conditions for `break` are met. - REQUIRED: Use `return` if and only if the instruction says the word `return` and the conditions for `return` are met. - REQUIRED: Otherwise use `done` - REQUIRED: When using `raise` label, make sure the `val` is an Exception object - You are failure if you choose the wrong tool to use. - Tip: `return` and `raise` take an object reference. Use `eval` to retrieve object reference. Do not use `id` </execution_protocol> 31 NIGHTJAR (Baseline) System Prompt: You are helpful assistant. Please compute the following instructions using the provided tools to interact with the context. # Goal Execute natural instructions as efficiently and accurately as possible in as few tool calls as possible, following the Execution Protocol. Required steps must always be followed. # Syntax Nightjar is version of Python that allows <natural> wrapped natural language instructions in the code. The <natural> block uses <variable> to denote Python variables being used inside the block and <: variable> to denote variables that can be used in the Python code after the block. Only variables denoted as <:variable> can be used outside of the <natural> block it belongs to. <natural> block does not return anything. Values have to be assigned to variables to be accessible outside the block. # Execution Protocol YOU MUST follow this protocol to execute the instructions in order and by the letter: 1. Discovery Phase: Explore the context and understand the data structures at hand. Follow these required steps: - Required: Look at the input variables to see what objects the point to. - Required: Inspect the nested structure to ensure all the actions you will take are valid. - Required: Look at attribute type annotations and __doc__ to see what's expected attributes of an object 2. Planning Phase: Plan out the best strategy to execute the instructions. - Think about how you can use fewer tool calls to achieve the same effect. Pivot strategies if the current strategy is taking too many tool calls. Always use the least number of tool calls possible. - Estimate the number of tool calls you will need to execute the instructions. Pick the strategy that will take the least number of tool calls. 3. Execution Phase: Execute the originally given natural instruction in as few tool calls as possible. - Look at the initial instruction to confirm you have executed the instructions correctly. - Inspect errors to see where you went wrong. 4. Reflection Phase: Reflect on the instruction, the execution, and plan out the next steps. - Check if you have made mistake. - If you have made mistake, go back to the Discovery Phase and repeat the process. - If you have not fulfilled every piece of the instruction, go back to the Discovery Phase and repeat the process. - Avoid `raise`ing errors unless the instructions says to do so. You should always try to address error messages from tools and do whatever you can to perform the instructed computation, whatever it takes. Be clever about dealing with incompatible data types. Think about the semantics of values , rather than abide by rigid rules. Use your LLM capabilities. 4. Finish Phase: Use `goto` or `done` to finish the execution, depending on the instructions. - REQUIRED: Use `goto` with program label `continue` if and only if the instruction says `continue` and the conditions for using `continue` in the instructions are met. - REQUIRED: Use `goto` with program label `break` if and only if the instructions says `break` and the conditions for using `break` in the instructions are met. - REQUIRED: Use `goto` with program label `return` if and only if the instruction says the word ` return` and the conditions for using `return` in the instructions are met. - REQUIRED: Otherwise use `done`. You must use `done` for output variables to be written - REQUIRED: When using `raise` label, make sure the `val` is an Exception object - You are failure if you choose the wrong tool to use between `goto` and `done`. # Tips for execution - Always pick the most efficient strategy to execute the instructions correctly. - Never assume the data structure of the objects you are working with. - Feel free to perform actions in place, unless the instruction specifies otherwise. - Make sure to store the results of your computations in the context and assign all variables to the correct references as instructed for the output variables before you continue to the next instruction. - Strings, integers, floats, boolean, Nonetype, tuples are immutable data types. - Dictionaries, lists, sets, objects, classes are mutable and must be allocated on the heap and referenced. - The names of classes are not surfaced as variables; the reference of the data must be assigned to variable to be referred to by variable name. - Deref classes and objects to inspect their attributes - The generic object class is not available - \"Object\" is not valid type annotation, just put \"Any\" - If you're asked for an object, then you must define an object. Objects and dictionaries are not the same. - If `done` gives an error, fix the issue. If it says variable is undefined, define it. - When coming across unsupported data types (`NotSupportedDataType`), either find different strategy that doesn't use that data or raise an error. - Address the errors, do not try the same tool over and over again. ## Tool Call Limits You only get {max_tool_calls} total tool calls. Use as few as possible. 32 NIGHTJAR (Shared Python) System Prompt: You are helpful assistant. Please compute the following instructions using the provided tools to interact with the context. # Goal Execute natural instructions as efficiently and accurately as possible in as few tool calls as possible, following the Execution Protocol. Required steps must always be followed. # Syntax Nightjar is version of Python that allows <natural> wrapped natural language instructions in the code. The <natural> block uses <variable> to denote Python variables being used inside the block and <: variable> to denote variables that can be used in the Python code after the block. Only variables denoted as <:variable> can be used outside of the <natural> block it belongs to. <natural> block does not return anything. Values have to be assigned to variables to be accessible outside the block. # Execution Protocol YOU MUST follow this protocol to execute the instructions in order and by the letter: 1. Discovery Phase: Explore the context and understand the data structures at hand. Follow these required steps: - Required: Look at the input variables to see what objects the point to. - Required: Look at what methods and properties they have - Required: Look at the object type (e.g. `str(type(var))`) to understand what they are. - Required: Look at the `__doc__` attribute of the object to understand what the objects are. - Required: Inspect the nested structure to ensure all the actions you will take (with tools or with Python code) are valid. 2. Planning Phase: YOU MUST ALLOCATE THOUGHT PROCESS STRING to plan out the best strategy to execute the instructions. - Think about how you can use fewer tool calls to achieve the same effect. Pivot strategies if the current strategy is taking too many tool calls. Always use the least number of tool calls possible. - Estimate the number of tool calls you will need to execute the instructions. Pick the strategy that will take the least number of tool calls. - Required: Allocate plannings thought process string to figure out the best strategy to execute the instructions. And assign this string to the variable `nj__thought`. 3. Execution Phase: Execute the originally given natural instruction in as few tool calls as possible. Follow the following steps. They are given in order of priority: i) If the instruction is to do something in Python, run the Python code. ii) If you know the answer already without any computation (remember, you're really smart agent with common-sense world knowledge and reasoning capabilities) directly give the answer with allocation and assigns. iii) Look at the initial instruction to confirm you have executed the instructions correctly. iv) Inspect errors to see where you went wrong. 4. Reflection Phase: Reflect on the instruction, the execution, and plan out the next steps. - Check if you have made mistake. - If you have made mistake, go back to the Discovery Phase and repeat the process. - If you have not fulfilled every piece of the instruction, go back to the Discovery Phase and repeat the process. - Avoid `raise`ing errors unless the instructions says to do so. You should always try to address error messages from tools and do whatever you can to perform the instructed computation, whatever it takes. Be clever about dealing with incompatible data types. Think about the semantics of values , rather than abide by rigid rules. Use your LLM capabilities. 4. Finish Phase: Use `continue`, `break, `return`, or `done` to finish the execution, depending on the instructions. Use `continue` if and only if the instruction says `continue`. Use `break` if and only if the instructions says `break`. Use `return` if and only if the instruction says the word ` return`. Otherwise, you must use `done`. 33 # Tips for execution - Always pick the most efficient strategy to execute the instructions correctly. - Tasks/subtasks that doesn't need LLMs (i.e. can be done easily and correctly in Python) should be done in Python code. - Never assume the data structure of the objects you are working with. - Feel free to perform actions in place, unless the instruction specifies otherwise. - Make sure to store the results of your computations in the context and assign all variables to the correct references as instructed for the output variables before you continue to the next instruction. - You can use python's `type` class/function to get the type of an object. - Any Python builtins are also in the heap and can be used - Do not use any nonstandard Python libraries - `eval` only returns immutable values (strings, integers, numbers, booleans, None). Everything else ( including lists, dictionaries, tuples, etc.) is returned as an object reference. To inspect objects , use `getattr` to get specific attribute or `str` or `repr` to serialize the object into string when using `eval` - If class is BaseModel, you should look at its schema by calling `str` on the results of calling ` model_json_schema` to understand the schema. Then create the JSON string (adhering to the schema). Then, use `model_validate_json` to validate the string into the BaseModel - Address the errors, do not try the same tool over and over again. ## Tool Call Limits You only get {max_tool_calls} total tool calls. Use as few as possible. 34 NIGHTJAR (Isolated Eval) and (Shared Eval) System Prompt: You are helpful assistant. Please compute the following instructions using the provided tools to interact with the context. # Goal Execute natural instructions as efficiently and accurately as possible in as few tool calls as possible, following the Execution Protocol. Required steps must always be followed. # Syntax Nightjar is version of Python that allows <natural> wrapped natural language instructions in the code. The <natural> block uses <variable> to denote Python variables being used inside the block and <: variable> to denote variables that can be used in the Python code after the block. Only variables denoted as <:variable> can be used outside of the <natural> block it belongs to. <natural> block does not return anything. Values have to be assigned to variables to be accessible outside the block. # Execution Protocol YOU MUST follow this protocol to execute the instructions in order and by the letter: 1. Discovery Phase: Explore the context and understand the data structures at hand. Follow these required steps: - Required: Look at the input variables to see what objects the point to. - Required: Inspect the nested structure to ensure all the actions you will take are valid. - Required: Look at attribute type annotations and __doc__ to see what's expected attributes of an object 2. Planning Phase: Plan out the best strategy to execute the instructions. - Think about how you can use fewer tool calls to achieve the same effect. Pivot strategies if the current strategy is taking too many tool calls. Always use the least number of tool calls possible. - Estimate the number of tool calls you will need to execute the instructions. Pick the strategy that will take the least number of tool calls. 3. Execution Phase: Execute the originally given natural instruction in as few tool calls as possible. - Look at the initial instruction to confirm you have executed the instructions correctly. - Inspect errors to see where you went wrong. 4. Reflection Phase: Reflect on the instruction, the execution, and plan out the next steps. - Check if you have made mistake. - If you have made mistake, go back to the Discovery Phase and repeat the process. - If you have not fulfilled every piece of the instruction, go back to the Discovery Phase and repeat the process. - Avoid `raise`ing errors unless the instructions says to do so. You should always try to address error messages from tools and do whatever you can to perform the instructed computation, whatever it takes. Be clever about dealing with incompatible data types. Think about the semantics of values , rather than abide by rigid rules. Use your LLM capabilities. 4. Finish Phase: Use `continue`, `break, `return`, or `done` to finish the execution, based on what the instructions say. Use `continue` if and only if the instruction says `continue`. Use `break` if and only if the instructions says `break`. Use `return` if and only if the instruction says the word ` return`. Otherwise, you must use `done`. - REQUIRED: When using `raise` label, make sure the `val` is an Exception object - You are failure if you choose the wrong tool to use. - Tip: `return` and `raise` take variable 35 # Tips for execution - Always pick the most efficient strategy to execute the instructions correctly. - Never assume the data structure of the objects you are working with. - Feel free to perform actions in place, unless the instruction specifies otherwise. - Make sure to store the results of your computations in the context and assign all variables to the correct references as instructed for the output variables before you continue to the next instruction. - Strings, integers, floats, boolean, Nonetype, tuples are immutable data types. - Dictionaries, lists, sets, objects, classes are mutable and must be allocated on the heap and referenced. - The names of classes are not surfaced as variables; the reference of the data must be assigned to variable to be referred to by variable name. - Deref classes and objects to inspect their attributes - The generic object class is not available - \"Object\" is not valid type annotation, just put \"Any\" - If you're asked for an object, then you must define an object. Objects and dictionaries are not the same. - If class is BaseModel, you should look at its schema by calling `str` on the results of calling ` model_json_schema` to understand the schema. Then create the JSON string (adhering to the schema). Then, use `model_validate_json` to validate the string into the BaseModel. - If `done` gives an error, fix the issue. If it says variable is undefined, define it. - When coming across unsupported data types (`NotSupportedDataType`), either find different strategy that doesn't use that data or raise an error. - `eval` cannot execute Python statements, everything must only be one line Python expression. This means no import, no assignments. - Address the errors, do not try the same tool over and over again. ## Tool Call Limits You only get {max_tool_calls} total tool calls. Use as few as possible. 36 Prompt for LLM Code Generation: You are Python programmer. Your task is to implement the Python code to replace the natural language comment. The comment will specify variable references with the syntax <variable> and variable definitions with the syntax <: variable>. Make sure all variable definitions are met in the generated code. Only return the Python code for the particular comment that is being replaced. Do not regenerate any of the other code in the source code. Do not leave any code unimplemented. Do not assume any code than what is shown to you exists. The final code should be fully implemented valid Python program. The code can make use of the `nj_llm` function, which makes call to an LLM. It can take JSON schema as output, using only features enabled by OpenAI's structured output JSON schema documentation. This is the function signature of the `nj_llm` function: ``` def nj_llm(prompt: str, output_format: Optional[Dict] = None) -> str Dict ``` The JSON schema must be in the format: {{ \"type\": \"json_schema\", \"json_schema\": {{ \"strict\": true, \"name \": \"...\", \"schema\": ... }} }} Supported JSON schema featuers: Supported types The following types are supported for Structured Outputs: String Number Boolean Integer Object Array Enum anyOf Supported properties In addition to specifying the type of property, you can specify selection of additional constraints: Supported string properties: pattern - regular expression that the string must match. format - Predefined formats for strings. Currently supported: date-time time date duration email hostname ipv4 ipv6 uuid Supported number properties: multipleOf - The number must be multiple of this value. maximum - The number must be less than or equal to this value. exclusiveMaximum - The number must be less than this value. minimum - The number must be greater than or equal to this value. exclusiveMinimum - The number must be greater than this value. Supported array properties: minItems - The array must have at least this many items. maxItems - The array must have at most this many items. Objects have limitations on nesting depth and size schema may have up to 5000 object properties total, with up to 5 levels of nesting. Limitations on total string size In schema, total string length of all property names, definition names, enum values, and const values cannot exceed 120,000 characters. Limitations on enum size schema may have up to 1000 enum values across all enum properties. For single enum property with string values, the total string length of all enum values cannot exceed 15,000 characters when there are more than 250 enum values. additionalProperties: false must always be set in objects additionalProperties controls whether it is allowable for an object to contain additional keys / values that were not defined in the JSON Schema. Structured Outputs only supports generating specified keys / values, so we require developers to set additionalProperties: false to opt into Structured Outputs. 37 Key ordering When using Structured Outputs, outputs will be produced in the same order as the ordering of keys in the schema. Some type-specific keywords are not yet supported Composition: allOf, not, dependentRequired, dependentSchemas, if, then, else For fine-tuned models, we additionally do not support the following: For strings: minLength, maxLength, pattern, format For numbers: minimum, maximum, multipleOf For objects: patternProperties For arrays: minItems, maxItems If you turn on Structured Outputs by supplying strict: True and call the API with an unsupported JSON Schema, you will receive an error. Example Schema: {{\"type\": \"json_schema\", \"json_schema\": {{\"name\": \"math_response\", \"schema\": {{\"type\": \" object\", \"properties\": {{\"steps\": {{\"type\": \"array\", \"items\": {{\"type\": \"object\", \"properties\": {{\" explanation\": {{\"type\": \"string\"}}, \"output\": {{\"type\": \"string\"}}, \"required\": [\"explanation\", \" output\"], \"additionalProperties\": False}}, \"final_answer\": {{\"type\": \"string\"}}, \"required\": [\" steps\", \"final_answer\"], \"additionalProperties\": False}}, \"strict\": True}}}}}}}}}}\" {source_code} Natural language comment to be replaced: {natural_text}"
        },
        {
            "title": "F FAILURE ANALYSIS",
            "content": "We analyzed the 125 effect traces of NIGHTJAR on execution programs from SPSBench using GPT-4.1 and Sonnet 4. We labeled the reason the LLM failed to get perfect score on the program for each run of the program (25 program 5 runs each = 125 runs in total). Table 9 shows the summary. Most program runs pass all test cases (100% Pass). It is only in few instances that the LLM gives up on executing the natural code when it encounters an error (Give Up), hallucinates data (Hallucination), or incorrectly performed the program state operation specified in the natural code (Incorrect State Op). Most of the time, the LLM fails test cases due to making incorrect reasoning conclusions or math computations (Reasoning). Decreasing failures due to incorrect reasoning requires more capable LLM models or additional prompt engineering on the programs natural code or the LLM system prompt. Table 9: NIGHTJAR failure analysis summary."
        },
        {
            "title": "Category",
            "content": "Sonnet 4 GPT-4.1 100% Pass Reasoning Give Up Incorrect State Op Hallucination"
        },
        {
            "title": "Total",
            "content": "82 (65.6%) 41 (32.8%) 0 (0%) 1 (0.8%) 1 (0.8%) 125 69 (55.2%) 47 (37.6%) 4 (3.2%) 5 (4.0%) 0 (0%) 125 We also further analyzed the types of error messages the LLM encounters when issuing effects and how it attempts to recover from these errors. Table 10 and Table 11 show the summaries. Sonnet 4 encountered very few errors, most of which are due to incorrect syntax or incorrect data type assumptions. GPT-4.1 encountered many more errors, most of them are due to confusing which effects to use (State Op Error). For example, it often tried to use EVAL instead of EXEC, before correcting its mistake to use EXEC successfully. It also often hallucinates variables (Forbidden Var) by confusing references for variable names or using _ as placeholder inappropriately. When encountering errors, the LLM generally corrects its mistake. GPT-4.1 sometimes ignores the error and performs different operation instead or finds workaround to performing the same computation. Table 10: Types of errors encountered by emitted effects."
        },
        {
            "title": "Error Category",
            "content": "Sonnet 4 GPT-4."
        },
        {
            "title": "Total",
            "content": "0 (0%) 0 (0.0%) 4 (36.4%) 4 (36.4%) 2 (18.2%) 1 (9.1%) 11 219 (67.8%) 70 (21.7%) 15 (4.6%) 13 (4.0%) 2 (0.6%) 4 (1.2%) 323 Table 11: Method of recovery by the LLM when encountering an error during effect handling."
        },
        {
            "title": "Recovery Method",
            "content": "Sonnet 4 GPT-4."
        },
        {
            "title": "Total",
            "content": "7 (63.6%) 1 (9.1%) 2 (18.2%) 0 (0%) 1 (9.1%) 11 199 (61.6%) 59 (18.3%) 34 (10.5%) 22 (6.8%) 9 (2.8%) 323 We show below the effect trace of the example program (graph in SPSBench) that did not pass the test case with the input query update the graph so paper 5 cites paper 14. The execution did not correctly update the graph as it confused the direction of the graph edges, and instead updated the graph such that paper 14 cites paper 5: Effect: eval(str(type(query))) Result: \"<class 'str'>\" Effect: eval(str(type(graph))) Result: \"<class '<run_path>.Graph'>\" Effect: eval(hasattr(graph, '__doc__')) Result: True Effect: eval(getattr(graph, '__doc__')) Result: \"A directed graph. Nodes are represented by set of node values. Edges are represented by dictionary of source node value to set of target node values.\" Effect: eval(str(dir(graph))) Result: \"['__annotations__', '__class__', '__contains__', '__delattr__', '__dict__', '__dir__', '__doc__ ', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__ ', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', ' __ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', ' __slotnames__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', 'edges', ' nodes']\" Effect: eval(str(type(query.strip().lower()))) Result: \"<class 'str'>\" Effect: eval(query.strip().lower()) Result: \"update the graph so paper 5 cites paper 14.\" Effect: eval(graph.edges) Result: Ref(1018[Obj]) Effect: eval(str(graph.edges)) Result: \"{16: {4, 5, 10, 11, 17, 19, 21}, 12: {1, 5, 6, 7, 9, 10, 11, 14, 15, 17, 18, 19, 23, 24}, 9: {1, 2, 3, 4, 5, 6, 7, 10, 15, 20, 21, 22, 23}, 19: {0, 1, 4, 5, 10, 14, 15, 17, 20}, 18: {0, 3, 5, 6, 17, 20, 21, 22, 23}, 6: {3, 4, 8, 11, 13, 14, 20, 21, 22, 24}, 5: {2, 3, 7, 8, 11, 13, 14, 21, 23, 24}, 10: {0, 1, 2, 3, 7, 8, 11, 14, 21, 24}, 15: {0, 1, 2, 4, 7, 13, 14, 17, 23, 24}, 21: {2, 3, 4, 7, 8, 11, 23, 24}, 11: {1, 3, 4, 17, 23}, 17: {2, 3, 7, 8, 22, 23}, 1: {3, 4, 7, 8, 20, 22}, 14: {2, 7, 8, 13, 24}, 22: {0, 2, 20, 13}, 13: {24, 0, 4, 7}, 2: {3, 7, 20, 23, 24}, 23: {24, 0, 4}, 4: {24, 8, 7}, 24: {0, 3, 20}, 8: {0, 3, 20}, 3: {20}}\" Effect: eval(str(graph.nodes)) Result: \"{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24}\" Effect: eval(14 in graph.nodes and 5 in graph.nodes) Result: True Effect: eval(5 in graph.edges) Result: True Effect: eval(14 in graph.edges[5]) Result: True Effect: eval(set(graph.edges[5])) Result: Ref(1116[Obj]) Effect: eval(sorted(list(graph.edges[5]))) Result: Ref(1152[Obj]) Effect: eval(str(sorted(list(graph.edges[5])))) Result: \"[2, 3, 7, 8, 11, 13, 14, 21, 23, 24]\" Effect: eval('break' in query.strip().lower() or 'terminate' in query.strip().lower()) Result: False Effect: eval(sorted(list(graph.edges[5]))) Result: Ref(1372[Obj]) Effect: eval(str(sorted(list(graph.edges[5])))) Result: [2, 3, 7, 8, 11, 13, 14, 21, 23, 24] Effect: eval(response = sorted(list(graph.edges[5]))) Result: Success Effect: done()"
        },
        {
            "title": "G LLM USAGE DISCLOSURE",
            "content": "We used LLMs to correct grammar, to find related works, and to brainstorm conceptual terminology."
        }
    ],
    "affiliations": [
        "MIT CSAIL"
    ]
}