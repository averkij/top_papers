{
    "paper_title": "Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision",
    "authors": [
        "Aadarsh Sahoo",
        "Georgia Gkioxari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., \"left-most apple\") and overlooks functional and physical reasoning (e.g., \"where can I safely store the knife?\"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/"
        },
        {
            "title": "Start",
            "content": "Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision"
        },
        {
            "title": "Georgia Gkioxari\nCalifornia Institute of Technology",
            "content": "6 2 0 2 3 1 ] . [ 1 5 9 1 3 1 . 2 0 6 2 : r Figure 1. Conversational Image Segmentation requires reasoning beyond object categories. Left: Referring Image Segmentation (RIS) grounds descriptive phrases about object identity and spatial relations (e.g., left-most apple). Right: Conversational Image Segmentation (CIS) grounds abstract, intent-oriented concepts that require relational reasoning, physical understanding, and implicit constraints."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., left-most apple) and overlooks functional and physical reasoning (e.g., where can safely store the knife?). We address this gap and introduce Conversational Image Segmentation (CIS) and CONVERSEG, benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present CONVERSEG-NET, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates promptmask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while CONVERSEGNET trained on our data engine achieves significant gains on CONVERSEG and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg. Which suitcases can take without disturbing the stack? For humans, the answer is immediate: we identify which pieces are load-bearing versus accessible, anticipate redistribution of weight, and filter candidates by the constraint of easy removal. Yet segmentation model, trained to predict suitcase and cart, lacks any representation of support relations, occlusion ordering, or physical stability. Selecting easily removable luggage requires reasoning jointly over geometry, physics, and user intent not merely recognizing object categories. This type of conversational, intent-driven language instruction reflects how humans naturally interact with their environments, yet remains beyond the reach of current perception systems. In computer vision, grounding images using natural language expressions was first explored through the task of Referring Image Segmentation (RIS) [41]. However, existing benchmarks for this task, RefCOCO variants [41], primarily emphasize categorical and spatial references, such as the white umbrella or the left-most apple, shown on the left of Fig. 1. By contrast, functional or physical reasoning about 1 objects and environments such as what object is prone to rolling if unsecured or where can safely store the knife? is largely underrepresented. We address this gap by introducing Conversational Image Segmentation (CIS), task that grounds high-level conversational concepts into pixel-accurate masks in natural images. Examples are shown in Fig. 1. We call these concepts conversational because they mirror how humans naturally talk about objects and their surroundings. They span five families, inspired by human vision science [9, 27] and intuitive physics [2]: (1) Entities with open-vocabulary descriptions (weathered wooden furniture); (2) Spatial & Layout capturing complex geometric relations (items blocking the walkway); (3) Relations & Events describing interactions (the player about to catch the ball); (4) Affordances & Functions requiring use-case reasoning (surfaces suitable for hot cookware); and (5) Physics & Safety involving stability or hazard assessment (objects likely to tip over). To measure progress in CIS, we introduce the CONVERSEG benchmark, featuring 1,687 human-verified imagemask pairs. Unlike previous benchmarks that focus on categorical entities and simple spatial relations, CONVERSEG offers coverage across all five concept families and broader representation of conversational reasoning. We hope that advancing CIS with CONVERSEG will advance perception systems in assistive robotics, humanrobot interaction and augmented reality domains that require grounding abstract concepts. We further introduce CONVERSEG-NET, conversational segmentation model that maps an image and prompt to grounding mask. Training it demands large-scale supervision over diverse prompts in natural images costly and cognitively intensive effort for human annotators who must produce pixel-accurate masks and natural, reasoningrich prompts. To bypass this bottleneck, we build an automated, VLM-driven data engine that synthesizes high-quality promptmask pairs without human supervision via an iterative generate-and-verify loop, yielding 106K imagemask pairs across all five concept families. Trained on this data, CONVERSEG-NET achieves strong results on CONVERSEG and remains competitive on standard referring expression benchmarks, demonstrating both data quality and scalability. In summary, our contributions are: We introduce Conversational Image Segmentation (CIS) and CONVERSEG, benchmark of human-verified imagemask pairs targeting grounding of affordances, physics, and functional reasoning. We build an AI-powered data engine that synthesizes diverse, high-quality conversational promptmask pairs without human supervision. We design baseline model, CONVERSEG-NET; trained on our engines data, it excels on CONVERSEG and remains strong on RIS benchmarks. 2. Related Work Referring Expression Segmentation. Referring expression segmentation (RIS) localizes regions described by language. RefCOCO/+/g [41] are standard benchmarks but are dominated by object-centric, low-level spatial phrases (e.g., person on the left, red cup). Early methods used multi-stage languagevision pipelines [12, 20]; recent work adopts Transformer-based vision-language encoders [15, 39]. Despite strong results on entities and simple spatial relations, these benchmarks seldom test affordances, stability, or user intent. Our CIS task and CONVERSEG explicitly target these gaps via five conversational concept families. Reasoning and Implicit Segmentation. ReasonSeg [17] pairs images with implicit, reasoning-heavy instructions and masks, but queries still target entities or spatial relations, with limited coverage of affordances, safety, or physical constraints. Multi-modal LLM systems (LISA [17], GLaMM [31], PixelLM [34]) can perform multi-step reasoning and produce masks, yet rely on heavy backbones and multi-stage inference (chain-of-thought, tool calls), making deployment costly. In contrast, we pursue single-pass architectures that directly ground conversational concepts. Promptable Segmentation Models. The Segment Anything Model (SAM) [16] enables promptable, class-agnostic segmentation from points or boxes; SAM2 [32] extends this to streaming video. These offer strong priors but lack native text conditioning. Bridging this, some [24, 33] pair SAM with text-conditioned detectors, while others integrate SAM-like decoders into VLMs [31, 43]. We leverage SAMs learnt priors and combine them lightweight visionlanguage adapters to enable end-to-end conversational grounding without sacrificing segmentation quality. Vision-Language Models for Dense Prediction. Recent VLMs add heads for dense prediction: LISA [17] augments LLaVA [21] with mask decoder, GLaMM [31] supports multi-turn grounded dialogue, and GroundHog [43], Kosmos-2 [29], and xGen-MM/BLIP-3 [37] push pixel-level grounding in ever-larger models. These systems excel at complex reasoning but demand substantial compute and often multiple passes per query. We take complementary approach: rather than scaling model capacity, we scale training data diversity through automated data synthesis. lightweight 3B VLM + SAM2 decoder, trained on 106K auto-generated promptmask pairs across five reasoning concepts, achieves competitive CIS and RIS performance. Automated Data Synthesis. Synthetic data is strong alternative to manual annotation, with synthetic pre-training approaching real-image performance for representation learning [35]. Pipelines like ELEVATER [18], synthetic region captioning [7, 40], and grounding verification [43] scale supervision, but mostly target literal descriptions or category la2 Figure 2. CONVERSEG Qualitative Examples. Representative samples across five concept categories. Prompts require reasoning about attributes, spatial relations, interactions, functional properties, and physical constraints beyond standard object reference. how humans naturally query their surroundings and reveal reasoning abilities that extend beyond object-centric reference. We draw inspiration from human vision science [9, 27] and intuitive physics [2], which demonstrate that people infer functional properties and physical constraints directly from visual input capabilities largely missing from current vision tasks and benchmarks. 1. Entities. Prompts that identify entities by category or attributes (e.g., the bicycle with basket). While overlapping with RIS, we include open-vocabulary categories and complex attribute compositions. 2. Spatial & Layout. Prompts about spatial relations, ordering, and occupancy (e.g., the rightmost orange in the bowl, the lamp behind the sofa). 3. Relations & Events. Prompts targeting interactions or transient states (e.g., the player serving the ball, the door being opened). 4. Affordances & Functions. Prompts requiring functional reasoning about object use (e.g., surfaces you could cut on, items that could serve as shovel). 5. Physics & Safety. Prompts invoking stability, support, and hazard assessment (e.g., objects likely to tip, sharp objects posing hazard). 4. The CONVERSEG Benchmark In this section, we introduce the CONVERSEG benchmark for conversational image segmentation and describe our data collection and annotation process. Fig. 3 compares concept distributions across benchmarks. Existing datasets are heavily skewed toward entities and spatial relations (> 50%), with minimal coverage of affordances, events, or physical reasoning. In contrast, CONVERSEG provides balanced representation across all five concepts. Figure 3. Concept Coverage in Benchmarks. Distribution of concepts across five existing benchmarks versus CONVERSEG. Prior datasets primarily focus on entities/spatial relations, whereas CONVERSEG offers near-uniform coverage across all five concepts. bels. Our engine instead synthesizes conversational prompts aimed at affordances, layout constraints, and physical safety, then filters them via multi-stage visual verification. 3. Conversational Image Segmentation We introduce the task of Conversational Image Segmentation (CIS) and the family of concepts that define it. 3.1. Task Definition Given an image and natural language prompt p, the task is to predict binary mask Mp identifying the pixels in that satisfy the query p. Unlike referring image segmentation (RIS), prompts in CIS may require functional or physical reasoning (e.g., surfaces stable enough to stack books), target non-visible properties such as affordances or safety (e.g., objects that might be hot), and have contextdependent groundings (e.g., comfortable seating areas). 3.2. The Five Conversational Concepts We organize prompts into five concept families according to the type of reasoning they require. These families reflect 3 Figure 4. Conversational Data Engine. Five-stage pipeline for automated concept-mask synthesis: (1) VLM generates region descriptions; (2) Object detector and SAM2 produce masks, VLM filters and refines them. (3) Concept-driven meta-prompts generate conversational queries assigned to region-mask pairs; (4) Alignment verification ensures prompt-mask correctness. 4.1. Benchmark Construction and Verification CONVERSEG is constructed through two-stage pipeline: automated candidate generation followed by human verification. We source images from the COCO [19] validation set and generate candidate tuples (I, p, Mp, c) where denotes one of five concept types. Segmentation masks Mp in CONVERSEG are either machine or human drawn, which lead to two evaluation sets: Human-annotated Set. We initialize masks with humandrawn annotations from COCO, sourced from instance (objects) and panoptic annotations (objects and stuff). This split has high-quality mask annotations but is restricted to annotations within COCO. SAM-seeded Set. Masks are extracted with SAM2 [32] prompted with bounding boxes generated by an object detector. This mode eliminates reliance on closed-vocabulary annotations from COCO and allows us to scale our evaluation data with minimal human supervision. Conversational prompts are collected with the help of our data engine, described in detail in Sec. 5, which proposes prompts with corresponding masks Mp across all five concept families via generate-then-verify process. Human Verification. Each sample undergoes human verification for (i) prompt quality and correct concept assignment and (ii) mask accuracy. Verifiers accept/reject with single click. The engine supplies image-tailored, diverse prompts across all five concept families; human checks ensure prompt and mask quality. 4.2. CONVERSEG Statistics. CONVERSEG comprises 1,687 total samples across two splits: 1,194 SAM-seeded and 493 human-annotated. Prompts average 7.6 words (std: 1.2). Fig. 2 shows representative examples from each concept category. Additional statistics, annotation protocols, and qualitative examples are provided in the Appendix. 5. The Conversational Data Engine We introduce fully automatic data engine for conversational image segmentation. Collecting pixel-accurate masks and, more critically, realistic prompts is prohibitively expensive at scale with human annotators. Our data engine synthesizes high-quality promptmask pairs without human supervision. Instead, it leverages high-performing VLMs that iteratively generate outputs and verify their quality. 5.1. Data Engine Architecture Fig. 4 shows our pipeline, featuring modules for scene recognition, mask generation, prompt creation and verification. Stage 1: Scene Understanding. Given an image I, VLM generates 57 region descriptions in natural language, {d1, . . . , dn}. Each di specifies category, attributes, location, and relations in 15 words, e.g., large elephant walking toward left foreground. These descriptions serve as targets for mask generation. Stage 2: Mask Generation. For each description di, we localize its segment in image I: Moondream3 [28] predicts box bi from (I, di); SAM2, prompted with bi, returns the mask mi. We choose Moondream3 for open-vocabulary detection and SAM2 for box-conditioned segmentation. Stage 3: Mask Quality Verification. Pipeline systems suffer from error propagation, so we enforce two checks for mask fidelity: Masktext consistency check. VLM verifies that (bi, mi) matches di in identity, attributes, and spatial location. It returns accept/reject; only accepts proceed to the next stage. 4 Mask refinement and selection. Passing masks may still have under/over-coverage or holes, often from noisy boxes. We sample SAM2 with dense point grid to form candidates and pick with highest IoU to mi. VLM then chooses the better of the two based on coverage, boundary precision, artifacts. The selected ( ˆmi) is the final mask. Now, each image has verified descriptions, di, paired with high-quality masks, ˆmi. Stage 4: Concept-Driven Prompt Generation. This stage derives image-tailored conversational prompts for and its discovered regions. Prompts must span range of reasoning types within each concept c. For example, for affordances they should assess functional properties and context-dependent use (\"surfaces safe for hot items\"), canonical functions (\"sources of water\"), or counterfactual uses (\"items that could prop door\"); for spatial relations, containment (\"items inside containers\") or ordinality (\"the three leftmost cups\"). We enforce this via concept-specific meta-prompts πc (found in the Appendix). For each c, VLM receives: (1) indexed descriptions {di}, (ii) set-of-marks numbered overlay [38], and (iii) metaprompt πc. It generates up to three prompts per concept and selects the corresponding regions. Trivial prompt-mask pairs (e.g., an image with one car and segment the car) or prompts applicable to undiscovered regions are pruned. Stage 5: PromptMask Alignment Verification. For each tuple (I, p, Mp, c), VLM verifies that Mp: (1) matches ps target, (2) excludes irrelevant content, and (3) is reasonably described by p. It returns accept/reject; only accepts proceed. This five-stage pipeline combines VLM-driven generation with multi-stage verification to produce high-quality promptmask pairs without human annotation. All VLM components use Gemini-2.5-Flash [5]. Negative Data. Beyond the positives, we also generate concept-specific negative prompts to improve robustness against plausible hallucinations. For each concept category, VLM creates adversarial prompts using dedicated metaprompts (see Appendix) that employ two strategies: (1) object-level neighbors reference contextually plausible but absent objects (e.g., \"segment the wine glass\" at dinner table setting without one), and (2) concept-level neighbors describe present objects with incorrect attributes (e.g., \"segment the wooden chair\" for metal one, or \"segment the person standing\" when they are sitting). Each negative prompt is verified by VLM (Gemini-2.5-Flash) to ensure no valid mask exists in the image. 5.2. Dual-Purpose Design Our data engine is crafted to generate diverse conversational prompt-mask pairs in images. We use it for two purposes: (1) Benchmark curation (Sec. 4.1). We run the pipeline on COCO val images, seeding mask candidates from COCO anFigure 5. Model Architecture. An image encoder processes the image, while VLM jointly encodes image and text. Lightweight adapters map the text-token embeddings from the VLM to mask decoder that predicts the target segment. notations or SAM2. For benchmark-grade quality, generated promptmask pairs receive final human check. (2) Training data synthesis (Sec. 6.2). We run the data engine at scale on COCO train and SA-1B [16] images, producing training pairs without human supervision. Ablations on pipeline components, failure mode analysis, and additional implementation details are in the Appendix. 6. Model In this section, we present single-pass model for conversational image segmentation that grounds conversational concepts into pixels. Our design philosophy is to avoid multi-component workflows like iterative tool use or multiturn refinement to build strong baseline for CIS. To this end, we combine advances in promptable image segmentation, but lack text conditioning, with VLMs that integrate vision and language but do not perform segmentation. We describe our model architecture (Sec. 6.1), training strategy and implementation details (Sec. 6.2). 6.1. Model Architecture Fig. 5 illustrates our architecture. It fuses SAM2 [32] (imageonly components) with compact visionlanguage backbone (Qwen-2.5-VL-3B [1]), connected via lightweight prompt adapters. Image Encoder. We adopt the SAM2 image encoder which is an MAE [10] pre-trained Vision Transformer (ViT) [8] adapted for high-resolution inputs and keep it frozen. The encoder processes each image once, independent of the prompt, producing spatial image embedding zimg RH Dimg. Prompt Encoder. We use Qwen2.5-VL-3B [1] as the prompt encoder, which jointly processes the image and text prompt to produce per-token hidden states at the final layer. We extract hidden states corresponding to text tokens only (they have attended to the image tokens through Qwens backbone), yielding sequence {h1, . . . , hT , hEOS} RDt , where is the text length and Dt is the hidden dimension. Following SAMs design, we represent prompts as sparse and dense embeddings. The text token sequence {h1, . . . , hT } serves as sparse embeddings, capturing 5 fine-grained text information. The hidden state at the EOS position serves as the dense embedding, capturing global image-text context. Two lightweight adapters project these representations to the decoders input space: esparse = LinearDtDdec({h1, . . . , hT }) and edense = MLPDtDdec (hEOS), where the dense adapter is 2-layer MLP with SiLU activation. The Qwen backbone is finetuned using LoRA [11] with rank 16 and α = 32. Mask Decoder. We adopt SAM2s mask decoder and fully fine-tune it. The decoder uses modified Transformer blocks [36] with bidirectional cross-attention between prompt and image embeddings. After two blocks, the image embedding is upsampled and an MLP maps the output token to per-pixel foreground probabilities, producing the final mask. 6.2. Training Curriculum To tackle the reasoning-heavy task of conversational image segmentation, we use curriculum that gradually increases task complexity: the model first learns to segment literal concepts (e.g., segment the cat) before advancing to more abstract referential and conversational concepts. This approach is critical because SAM2 has no prior exposure to language and thus requires careful integration of text conditioning. We validate this design via ablations in Sec. 7.3. 6.2.1. Training Data We organize training data into four groups with increasing complexity: (1) Literal concepts. We leverage the COCO train set reformulated as category-level segmentation using refined masks from COCONut [6]. For each image, we randomly sample category and treat all corresponding masks as ground truth with prompts like Segment all the [category] in the image. In addition, we also get part-level segmentation masks from PACO [30]. This yields 440K prompt-mask pairs. (2) Basic referring expressions. We utilize the RefCOCO family datasets [41] (RefCOCO, RefCOCO+, RefCOCOg) train splits, providing 321K object-centric references. (3) Open-vocabulary regions. Region descriptions from Stage 3 of our data engine (Sec. 5), providing 48K diverse region descriptions beyond COCOs closed vocabulary. Examples are shown in Fig. 4. (4) Conversational concepts. This is the output from our data engine which generates 106K concept-mask pairs, covering all five concept families, plus an equal number of concept-specific negative prompts with empty masks to improve robustness. 1-3, producing base model proficient at basic referring segmentation. Phase 2: Conversational post-training. Initialized from Phase 1, we fine-tune on group 4 mixed with samples randomly drawn from groups 1 through 3 such that positives, negatives, and pretraining data are equal in proportion. This mixing strategy maintains performance on foundational segmentation tasks while adapting to conversational concepts, empirically validated in Sec. 7.3. 6.2.3. Training Objective Given binary ground-truth mask {0, 1}HW and predicted probability mask [0, 1]HW , we minimize weighted combination of binary cross-entropy and Dice loss: = LBCE(M, ) + λLDice(M, ) with λ = 0.25. 6.2.4. Implementation Details We train with AdamW [26], batch size 6 with gradient accumulation of 8 steps, and cosine schedule with warmup. Phase 1 runs for 100K steps and Phase 2 runs for 90K steps. We use learning rate of η=1e4. Training on single NVIDIA A100 80GB takes about 96 hours. Additional details are in the Appendix. 7. Experiments We evaluate our model against baselines on conversational image grounding, outlining the setup (Sec. 7.1), results (Sec. 7.2), and ablation studies and analysis (Sec. 7.3). 7.1. Experimental Setup Benchmarks. We evaluate on three benchmarks: (1) CONVERSEG: our benchmark with SAM-seeded and humanannotated splits across five concept categories (entities, spatial relations, affordances, relations & events, physics & safety); (2) RefCOCO/+/g [41]: standard referring expression benchmarks; (3) ReasonSeg [17]: segmentation benchmark of complex implicit reasoning and understanding. Baselines. We compare against state-of-the-art feed-forward methods of diverse architectural designs: LISA [17], an MLLM-based model with embedding-as-mask paradigm; UniLSeg [22], universal model for arbitrary semantic granularity; EVF-SAM [42], SAM-based model with early vision-language fusion; and Seg-Zero [23], reasoningchain guided framework with decoupled reasoning and segmentation modules. Qualitative comparisons with LISA appear in Fig. 6; additional baselines are in the Appendix. Evaluation Metrics. We report generalized IoU (gIoU) as our primary metric following [13, 41]. Cumulative IoU (cIoU) results are provided in the Appendix. 6.2.2. Two Phase Training 7.2. Main Results We train in two phases with increasing task complexity: Phase 1: Pretraining. We pretrain on mixture of groups Results on CONVERSEG. Tab. 1 reports overall and perconcept gIoU on the SAM-seeded and human-annotated 6 Model Prompt Encoder SAM-seeded (gIoU) Human-annotated (gIoU) All Ent. Spat. Rel. Aff. Phys. All Ent. Spat. Rel. Aff. Phys. LISA LISA UniLSeg-20 EVF-SAM EVF-SAM Seg-Zero 48.6 52.3 55.0 54.5 41.5 LLaVA 7B 55.2 60.0 57.1 60.3 50.1 Llama2 13B 32.6 39.0 36.0 36.5 26.4 CLIP ViT-B/16 42.2 47.1 49.3 46.0 37.3 BEIT-3-Large BEIT-3-Large 47.7 53.8 54.7 50.9 40.9 Qwen2.5-VL 7B 69.2 74.1 71.7 72.3 65.1 CONVERSEG-NET (Base) Qwen2.5-VL 3B 58.0 66.0 60.5 64.6 52.3 Qwen2.5-VL 3B 70.8 74.0 70.9 74.1 68.7 CONVERSEG-NET Qwen2.5-VL 7B 72.4 76.1 71.1 77.5 70.4 CONVERSEG-NET 39.1 46.6 22.8 29.8 36.6 60. 41.8 64.2 63.7 45.9 46.5 50.4 50.9 42.6 53.8 54.3 60.1 56.8 53.6 31.6 36.6 35.4 27.4 27.6 39.1 42.1 50.1 41.3 29.5 45.4 51.9 52.2 44.2 41.4 61.1 62.8 63.6 61.1 60.2 56.5 61.9 59.9 59.4 52.7 67.4 71.6 68.7 67.0 64.4 67.9 70.0 71.5 69.3 63.5 37.7 42.1 29.4 31.2 34.1 56.6 45.5 63.8 64.0 Table 1. CONVERSEG benchmark results (gIoU, %). Each subset reports performance across the five concept categories Entities, Spatial, Relations, Affordances, and Physics & Safety and summarizes across all (All). indicates models fine-tuned on ReasonSeg training data. trained on RefCOCO only; trained on RefCOCO and additional datasets (Objects365, PASCAL-Part, etc). Figure 6. Qualitative results on CONVERSEG. CONVERSEG-NET produces more accurate masks for spatial, affordance, and physics concepts than LISA, popular leading reasoning segmentation model. splits of CONVERSEG. Our Phase-1 model (CONVERSEGNET Base, Qwen2.5-VL 3B, without conversational training) already achieves 58.0% on the SAM-seeded split surpassing the strongest LISA variant (55.2% with Llama2-13B) by +2.8%, despite using 4 smaller backbone and without ReasonSeg fine-tuning. Among existing methods, Seg-Zero is the strongest baseline with 69.2% overall. Our full 3B model (CONVERSEG-NET) reaches 70.8%, improving over Seg-Zero by +1.6% on the SAM-seeded split. Scaling the prompt encoder to 7B further boosts performance to 72.4%, +3.2% absolute gain over the best baseline. The same trends hold on the human-annotated split. Per-Concept Analysis. Tab. 1 reports per-concept performance. Key takeaways: (1) baselines score highest on entities and spatial and lowest on affordances, physics & safety concepts. For example, LISA-Llama2-13B on the SAMseeded split achieves 60.0% on entities and 46.6% on physics & safety (-13.4%). (2) Our base model shows an even larger gap: 66.0% vs. 41.8% (24.2%). (3) Phase-2 conversational training boosts performance for all concepts, with biggest gains for physics & safety (from 41.8% to 64.2%) narrowing the the gap with entities to 9.8% (74.0% vs. 64.2%). (4) Scaling the backbone (CONVERSEG-NET-7B) achieves the best results across all families. Overall, later-stage training and larger models especially benefit abstract concepts while preserving strong entity-level performance. Qualitative Examples. Fig. 6 compares CONVERSEG-NET and LISA predictions on CONVERSEG. CONVERSEG-NET better localizes abstract concepts: it focuses on the bicycle as the mechanism requiring physical pedaling force, isolates the newspaper as the object being actively examined, selects only vessels containing liquid at high spill risk, and segments the bell as the object used to gain attention, whereas LISA often includes nearby distractors or misses subtle physical cues. For surfaces affording comfortable full-body rest, however, LISAs decision to segment the full bed is arguably more canonical than CONVERSEG-NET blanket-focused mask. Comparison on Referring Expression Benchmarks. Tab. 2 reports gIoU on RefCOCO/+/g [41] and ReasonSeg [17]. While the primary goal of CONVERSEG-NET is to establish strong baseline for conversational image segmentation with abstract concepts, we evaluate on standard referring expression benchmarks to demonstrate robustness on literal concepts. On RefCOCO, CONVERSEG-NET achieves 78.4% on val, competitive with models like GSVA (79.2%) and EVF-SAM (82.4%) that use substantially more training data. We note that RefCOCO/+/g datasets contain noisy 7 Model Prompt Encoder RefCOCO RefCOCO+ RefCOCOg ReasonSeg val testA testB val testA testB val(U) test(U) val(G) val test test(short) test(long) LISA LISA LISA Seg-Zero Seg-Zero GSVA GLaMM UniLSeg-20 EVF-SAM EVF-SAM HyperSeg HyperSeg Gemini Seg X-SAM RSVP RSVP LLaVA 7B LLaVA 7B Llama2 13B Qwen2.5-VL 3B Qwen2.5-VL 7B Vicuna 13B Vicuna 7B CLIP ViT-B/16 BEIT-3-Large BEIT-3-Large Phi2 2.7B Phi2 3B Gemini2.5 Flash Phi3 3.8B LLaVA1.6 7B Qwen2-VL 7B 74.9 79.2 79.5 80.5 82.1 82.4 84.8 85.1 CONVERSEG-NET (Base) Qwen2.5-VL 3B 78.4 Qwen2.5-VL 3B 78.2 CONVERSEG-NET Qwen2.5-VL 7B 79.4 CONVERSEG-NET 79.1 81.7 83.2 81.8 83.7 84.2 85.7 87.1 80.8 80.3 81.6 72.3 77.1 76.9 78.4 80.0 80.2 83.4 83.4 75.8 74.7 76.4 65.1 70.3 72.6 72.7 75.2 76.5 79.0 78.0 72.5 72.0 74.3 70.8 73.8 78.7 77.0 78.3 80.0 83.5 81.0 77.7 77.5 79.1 58.1 63.6 64.6 67.0 70.1 71.9 75.2 74.4 66.4 66.3 69.2 67.9 75.7 74.2 78.4 76.8 78.2 79.4 83.8 75.1 74.1 74.9 70.6 77.0 74.9 79.5 77.4 78.3 78.9 83.9 74.7 73.9 75.5 74.7 73.7 75.0 44.4 52.9 60.0 58.2 62.6 47.4 59.2 28.3 56.6 59.2 58. 51.1 56.4 61.9 36.8 47.3 51.5 56.1 57.5 30.6 57.8 56.9 56.1 48.3 52.2 57.0 37.6 40.6 43.9 16.5 47.7 47.9 48.5 47.2 53.8 54.2 36.6 49.4 54.0 35.0 56.0 58.4 57. 48.6 51.7 57.9 Table 2. Referring expression segmentation (gIoU, %). CONVERSEG-NET is competitive on RefCOCO/+/g and achieves strong zero-shot performance on ReasonSeg with the 7B model, surpassing some methods fine-tuned on ReasonSeg (). trained on RefCOCO only; on RefCOCO plus additional datasets (Objects365, PASCAL-Part, etc). ground truth annotations with incorrect or inaccurate masks, which can artificially lower performance metrics even when model predictions are reasonable. We provide qualitative examples of such cases in the Appendix. On ReasonSeg, our 3B model reaches 52.2% on the test set without training on any ReasonSeg data, few points ahead of LISA-13B (51.5%), which was fine-tuned on its training set and is 4 larger. Scaling our model to 7B improves performance (57.0% on ReasonSeg test), showing that our conversational training effectively transfers to complex reasoning scenarios zero-shot (i.e., without task-specific supervision). 7.3. Ablation Studies and Analysis Curriculum Learning. Tab. 3 analyzes our training curriculum on RefCOCO/+/g (avg. over 9 splits) and CONVERSEGs human-annotated split. Training only on conversational data yields reasonable CONVERSEG performance (66.0%) but poor RefCOCO/+/g results (56.1%), indicating overfitting. Jointly training on all data without curriculum improves RefCOCO/+/g to 75.5% but degrades CONVERSEG to 65.4%. two-phase curriculum (Phase 1: basic referring data; Phase 2: conversational data) achieves 57.7% and 65.2%. Our final strategy Phase 1 on basic data followed by Phase 2 with mix of both data types achieves the highest overall performance (74.5% on RefCOCO/+/g, 67.4% on CONVERSEG), showing that curriculum learning with mixed fine-tuning effectively balances performance across benchmarks. Architectural Ablations. Tab. 4 ablates key architectural choices. Freezing the prompt encoder backbone instead of using LoRA fine-tuning causes drastic performance drop (48.3% vs. 67.4%) adapting the prompt encoder is crucial for language grounding. Providing only text input to the prompt encoder, rather than both image and text, reduces performance by 17.9% (67.4% to 49.5%), showing that visual context is essential for text-conditioned segmentation. Finally, removing dense prompt embeddings while retaining only sparse embeddings causes modest 0.1% drop. VLM Backbone Comparison. Tab. 5 evaluates PerceptionLM-3B [4], recent VLM with strong visual reasoning capabilities. Performance remains comparable (66.5 vs. 67.4), demonstrating that CONVERSEG-NET can work with any performant VLM backbone. Attention Map Visualization. Fig. 7 shows cross-attention between text tokens and image regions in the mask decoder. For each image we visualize maps for two prompts; attention concentrates on the referred region (e.g. Vitamin on carrots) and is sparse and point-like rather than diffuse. We hypothesize this stems from conditioning the SAM mask decoder with language embeddings in place of its point embeddings, so each token behaves like soft point prompt. Figure 7. Cross-attention maps showing that each text prompt focuses on its corresponding image region. 8 Training Strategy RefCOCO/+/g CONVERSEG Architecture Configuration CONVERSEG Prompt Encoder CONVERSEG No curriculum (only conversational) No curriculum (all data) Phase 1 + Phase 2 (only conversational) Phase 1 Phase 1 + Phase 2 (full curriculum) 56.1 75.5 57.7 75.1 74.5 66.0 65.4 65.2 56.5 67. w/o LoRA finetuning Text-only input to Qwen Sparse embeddings only CONVERSEG-NET 48.3 49.5 67.3 67.4 -19.1 -17.9 -0.1 Table 3. Curriculum learning ablation on RefCOCO/+/g and CONVERSEG. Table 4. Architectural ablations. Each row removes one component from the final design. Perception-LM-3B Qwen2.5-VL-3B 66.5 67.4 Table 5. Prompt encoder backbone comparison on CONVERSEG. 8. Conclusion We introduced Conversational Image Segmentation (CIS), grounding high-level concepts about affordances, physics, and function into pixel-accurate masks. Our CONVERSEG benchmark provides 1,687 human-verified samples with balanced coverage across five concept families underrepresented in prior work. To scale beyond manual annotation, we built an automated data engine synthesizing 106K promptmask pairs via iterative VLM generation and verification. CONVERSEG-NET, trained on this data, achieves state-ofthe-art performance on CONVERSEG while remaining competitive on standard benchmarks, showing that curriculum from literal to conversational concepts effectively adapts promptable segmentation to language conditioning."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Damiano Marsili and Ilona Demler for their valuable feedback. Aadarsh is supported by the Kortschak Scholarship and Caltechs CAST program. Georgia is supported by the Powell Foundation, Meta through the LLM evaluation research grant, Google, and Amazon."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 5, 15 [2] Peter Battaglia, Jessica Hamrick, and Joshua Tenenbaum. Simulation as an engine of physical scene understanding. Proceedings of the national academy of sciences, 110 (45):1832718332, 2013. 2, 3 [3] Nicolas Carion, Laura Gustafson, Yuan-Ting Hu, Shoubhik Debnath, Ronghang Hu, Didac Suris, Chaitanya Ryali, Kalyan Vasudev Alwala, Haitham Khedr, Andrew Huang, et al. Sam 3: Segment anything with concepts. arXiv preprint arXiv:2511.16719, 2025. 15 [4] Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Suyog Jain, et al. Perceptionlm: Open-access data and models for detailed visual understanding. arXiv preprint arXiv:2504.13180, 2025. 8 [5] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 5 [6] Xueqing Deng, Qihang Yu, Peng Wang, Xiaohui Shen, and Liang-Chieh Chen. Coconut: Modernizing coco segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2186321873, 2024. [7] Xueqing Deng, Qihang Yu, Ali Athar, Chenglin Yang, Linjie Yang, Xiaojie Jin, Xiaohui Shen, and Liang-Chieh Chen. Coconut-pancap: Joint panoptic segmentation and grounded captions for fine-grained understanding and generation. arXiv preprint arXiv:2502.02589, 2025. 2 [8] Alexey Dosovitskiy. An image is worth 16x16 words: TransarXiv preprint formers for image recognition at scale. arXiv:2010.11929, 2020. 5 [9] JJ Gibson. The ecological approach to visual perception: classic edition, 2014. 2, 3 [10] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. [11] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 6 [12] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Segmentation from natural language expressions. In European conference on computer vision, pages 108124. Springer, 2016. 2 [13] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 6 [14] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity 9 Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, Vitor Guizilini, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Muhammad Zubair Irshad, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph Lim, Jitendra Malik, Roberto Martín-Martín, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. Droid: large-scale in-the-wild robot manipulation dataset. 2024. 12, [15] Namyup Kim, Dongwon Kim, Cuiling Lan, Wenjun Zeng, and Suha Kwak. Restr: Convolution-free referring image In Proceedings of the segmentation using transformers. IEEE/CVF conference on computer vision and pattern recognition, pages 1814518154, 2022. 2 [16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 2, 5 [17] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 2, 6, 7, 12 [18] Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, et al. Elevater: benchmark and toolkit for evaluating language-augmented visual models. Advances in Neural Information Processing Systems, 35:92879301, 2022. 2 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. 4 [20] Chenxi Liu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, and Alan Yuille. Recurrent multimodal interaction for referring image segmentation. In Proceedings of the IEEE international conference on computer vision, pages 12711280, 2017. 2 [21] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [22] Yong Liu, Cairong Zhang, Yitong Wang, Jiahao Wang, Yujiu Yang, and Yansong Tang. Universal segmentation at arbitrary In Proceedings of granularity with language instruction. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 34593469, 2024. 6 [23] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. 6 [24] Yang Liu, Muzhi Zhu, Hao Chen, Xinlong Wang, Bo Feng, Hao Wang, Shiyu Li, Raviteja Vemulapalli, and Chunhua Shen. Segment anything in context with vision foundation models. International Journal of Computer Vision, 133(10): 74607485, 2025. 2 [25] Christoffer Löffler, Sascha Riechel, Janina Fischer, and Christopher Mutschler. Evaluation criteria for inside-out indoor positioning systems based on machine learning. In 2018 International Conference on Indoor Positioning and Indoor Navigation (IPIN), pages 18. IEEE, 2018. 12, 13 [26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [27] David Marr. Vision: computational investigation into the human representation and processing of visual information. MIT press, 2010. 2, 3 [28] Moondream. moondream3, 2025. 4 [29] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824, 2023. 2 [30] Vignesh Ramanathan, Anmol Kalia, Vladan Petrovic, Yi Wen, Baixue Zheng, Baishan Guo, Rui Wang, Aaron Marquez, Rama Kovvuri, Abhishek Kadian, et al. Paco: Parts and attributes of common objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71417151, 2023. 6 [31] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300913018, 2024. 2 [32] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 2, 4, 5, 15 [33] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. [34] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2637426383, 2024. 2 [35] Mert Bülent Sarıyıldız, Karteek Alahari, Diane Larlus, and Yannis Kalantidis. Fake it till you make it: Learning transferable representations from synthetic imagenet clones. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 80118021, 2023. 2 [36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 6 10 [37] Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael Ryoo, et al. xgen-mm (blip-3): family of open large multimodal models. arXiv preprint arXiv:2408.08872, 2024. 2 [38] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [39] Zhao Yang, Jiaqi Wang, Yansong Tang, Kai Chen, Hengshuang Zhao, and Philip HS Torr. Lavt: Language-aware vision transformer for referring image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1815518165, 2022. 2 [40] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. 2 [41] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In European conference on computer vision, pages 6985. Springer, 2016. 1, 2, 6, 7 [42] Yuxuan Zhang, Tianheng Cheng, Lianghui Zhu, Rui Hu, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, and Xinggang Wang. Evf-sam: Early vision-language fusion for text-prompted segment anything model. arXiv preprint arXiv:2406.20076, 2024. 6 [43] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog: Grounding large language models to holistic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1422714238, 2024."
        },
        {
            "title": "Appendix",
            "content": "A Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 A.1 Qualitative Predictions by CONVERSEG-NET 12 A.2 Failure Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 A.3 Annotation Quality in RefCOCO/+/g . . . . . . . 12 Conversational Data Engine Details . . . . . . . . . . . . . 12 B.1 Meta-Prompts and Stage Details . . . . . . . . . . . . 12 B.2 Negative Data Generation . . . . . . . . . . . . . . . . . . 13 Benchmark Construction and Analysis . . . . . . . . . 14 C.1 Annotation Protocols . . . . . . . . . . . . . . . . . . . . . . 14 C.2 VLM Verifier Reliability for CONVERSEG . . 14 C.3 Additional Statistics and Visualizations . . . . . 15 C.4 Additional Qualitative Examples . . . . . . . . . . . 15 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . 15 D.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 D.2 Training Hyperparameters . . . . . . . . . . . . . . . . . 15 Additional Quantitative Results . . . . . . . . . . . . . . . . 15 E.1 Cumulative IoU (cIoU) . . . . . . . . . . . . . . . . . . . . 15 E.2 Additional Baselines . . . . . . . . . . . . . . . . . . . . . . 15 A. Qualitative Results In this section, we provide additional qualitative examples of predictions by CONVERSEG-NET. A.1. Qualitative Predictions by CONVERSEG-NET We compare CONVERSEG-NET with LISA [17] instantiated with LLaVA-7B and Llama2-13B backbones. Note that CONVERSEG-NET uses Qwen2.5-VL 3B backbone, which is considerably smaller than both LISA variants. In Figs. 24 to 26, we show model predictions on images from the human-annotated split of CONVERSEG. In Figs. 27 to 29, we show predictions on the SAM-seeded split of CONVERSEG. Across both splits, CONVERSEG-NET typically produces masks that more closely match the conversational intent of the prompt, accurately identifying and segmenting the requested regions despite its smaller backbone size. We also explore out-of-distribution (OOD) behavior in Fig. 8, where we show qualitative predictions on images from the DROID dataset [14] and the Warehouse dataset [25]. In both settings, CONVERSEG-NET often localizes the regions implied by the prompts, suggesting prospective applications in domestic and warehouse robotics. A.2. Failure Cases In Figs. 30 and 31, we present representative failure cases of CONVERSEG-NET on the human-annotated and SAMseeded splits of CONVERSEG, respectively. We observe several recurring failure modes. For ambiguous prompts such as Segment the object reflected by the window glass in Fig. 31, CONVERSEG-NET segments the reflection of the person rather than the person itself. In other cases, the model selects only one of several valid targets, yielding high precision but low recall; examples include Identify cylindrical vessels designed for dry ingredient storage in Fig. 30 and Segment signs pointing diagonally upward in Fig. 31. Additional diverse failure cases illustrating similar behaviors are shown in the figures below. A.3. Annotation Quality in RefCOCO/+/g As noted in the main paper, RefCOCO/+/g datasets contain noisy ground truth annotations with incorrect or inaccurate masks. Fig. 9 shows representative examples where CONVERSEG-NET produces semantically reasonable predictions that receive low gIoU scores due to problematic ground truth annotations. Common issues include ground truth masks including irrelevant regions, or have poor boundary alignment despite correct semantic interpretation. In such cases, the gIoU metric penalizes reasonable model predictions, leading to artificially deflated performance numbers. These examples illustrate that numerical results on RefCOCO/+/g should be interpreted with caution, as the annotation quality does not always reflect the true segmentation difficulty or model capability. B. Conversational Data Engine Details In this section, we expand on the five-stage conversational data engine introduced in Section 5.1 of the main paper. Recall that the engine automatically constructs conversational segmentation triplets (image, prompt, mask) from COCO images, and is used to generate the 106K training examples for CONVERSEG-NET. Below we provide the meta-prompts and additional implementation details for each stage. B.1. Meta-Prompts and Stage Details Stage 1: Scene Understanding. In Stage 1 we obtain rich region-level descriptions that serve as the semantic backbone for subsequent stages. Figure 17 shows the meta-prompt used to query Gemini-2.5-Flash for these region descriptions, given the input image. Stage 2: Mask Generation. In Stage 2 we convert textual region descriptions into segmentation masks. We query the Moondream model with its default API configuration to predict bounding boxes, and then pass these boxes to SAM2 to obtain corresponding masks. Since this stage does not rely on any additional natural-language control, we do not use any dedicated meta-prompt here. Stage 3: Mask Quality Verification. Stage 3 filters and refines the SAM2 masks using VLM-based checks. Figure 18 shows the meta-prompt used for the masktext consistency check, where the VLM judges whether candidate mask 12 Figure 8. Out-of-distribution qualitative examples. Predictions of CONVERSEG-NET on images from the DROID dataset [14] and the Warehouse dataset [25]. For each example, we show the input image, conversational prompt, and the predicted mask overlaid. CONVERSEGNET often localizes the regions implied by the prompts despite the distribution shift, hinting at prospective applications in domestic and warehouse robotics. Figure 9. Noisy annotations in RefCOCO/+/g. CONVERSEG-NET predictions (blue) are semantically reasonable but receive low gIoU due to problematic ground truth masks (pink): incomplete coverage, irrelevant regions, or poor boundaries. matches the associated region description. Figure 19 shows the meta-prompt used for mask refinement and selection, where the VLM compares two candidate masks and selects the best one. Stage 4: Concept-Driven Prompt Generation. Stage 4 converts region descriptions into conversational prompts anchored in our five concept families. We use separate concept-specific meta-prompt for each family. Fig. 20 shows the meta-prompt for the affordances & functions concept. The meta-prompts for the remaining concepts follow the same structure; we omit them here to avoid redundancy. Stage 5: PromptMask Alignment Verification. Finally, Stage 5 verifies that the generated conversational prompt is aligned with the selected mask. Fig. 21 shows the metaprompt used for this verification step, where the VLM judges whether the prompt correctly and unambiguously describes the masked region. B.2. Negative Data Generation Beyond positive examples, our data engine generates concept-specific negative prompts to improve model robustness against plausible hallucinations. These examples train the model to reject prompts that sound contextually reasonable but do not correspond to any valid regions in the image. Negative Prompt Generation Strategy. For each concept family, we use dedicated meta-prompt that instructs the VLM to generate adversarial prompts using two strategies. First, object-level neighbors reference contextually plausible but absent objects (e.g., \"segment the wine glass\" at dinner table without one). Second, concept-level neighbors describe present objects with incorrect attributes (e.g., \"segment the wooden chair\" for metal one, or \"segment the person standing\" when sitting). Fig. 22 shows the generation meta-prompt for the affordances and functions concept family. Each generated prompt is verified to ensure no valid mask exists. Fig. 23 shows the verification meta-prompt. 13 Only verified negatives are included in training. Qualitative Examples of Negative Prompts. Fig. 13 shows representative negative prompts across concept families, demonstrating how they exploit contextual priors or attribute mismatches to create cases where the correct answer is an empty mask. Impact of Negative Data on Model Predictions. Fig. 14 compares predictions before and after negative training. Without negative data, the model hallucinates plausible but incorrect masks. After training, it correctly rejects adversarial prompts by producing empty masks. Interestingly, training with conversational negatives also improves robustness on simpler literal negatives. For instance, the model better handles \"segment the tiger\" in images with only zebras, even though such literal mismatches were not in the training set. This suggests the model develops general verification capability rather than memorizing specific patterns. C. Benchmark Construction and Analysis C.1. Annotation Protocols We describe the interface and instructions given to human annotators for constructing CONVERSEG. As discussed in Section 4 of the main paper, CONVERSEG is obtained via human verification of examples produced by the conversational data engine. For each candidate example, annotators were shown the input image with the AI-generated mask overlaid and the corresponding conversational prompt. The original image without the mask overlaid was also provided for context. The user interface was intentionally kept simple. Annotators were asked to judge whether the prompt and mask were semantically aligned, i.e., whether the mask accurately and sufficiently covered all regions referred to by the prompt without including substantial irrelevant areas. They then chose between two options: Accept (if the example was valid) or Reject (otherwise). The decision proposed by the AI verifier was also displayed as suggested label, but annotators were free to override it. Rejected examples were simply discarded; we did not ask annotators to refine or edit masks. screenshot of the annotation interface is shown in Fig. 10. C.2. VLM Verifier Reliability for CONVERSEG To better understand how the VLM verifier behaves, Fig. 15 shows qualitative examples from three categories: (1) the VLM accepts but the human rejects; (2) both the VLM and the human reject; (3) the VLM rejects but the human accepts. In the first case, the VLM typically accepts examples where the mask partially satisfies the prompt. For instance, for the prompt Identify the luggage sturdy enough to use as step, the VLM accepts mask highlighting two pieces of luggage, even though additional items could also reasonably Figure 10. Annotation interface for constructing CONVERSEG. Annotators are shown the input image with the AI-generated mask overlaid, along with the corresponding conversational prompt and the suggested decision from the AI verifier. They then decide whether the prompt and mask are semantically aligned and select either Accept/Select or Reject/Discard; rejected examples are discarded without further editing. satisfy the prompt. In other cases, disagreement is driven by mask quality rather than semantics; for example, for Segment the objects currently providing thermal insulation, the mask includes the blanket but also the person, leading the human to reject the example despite the VLM accepting it. In the second case, where both the VLM and the human reject an example, the VLM reliably identifies clear errors (e.g., severe under-/over-coverage or obvious semantic mismatch), and its decision closely matches the human judgment. In the third case, where the VLM rejects but the human accepts, the prompts are often somewhat ambiguous. For example, for Segment the object reflected by the window glass, the VLM expects the reflection itself to be masked and therefore rejects the example, while the human accepts mask covering the physical object. For benchmark construction, and to avoid any single instance dominating the dataset with many similar prompts, annotators were also instructed to reject prompts referring to duplicate objects, even if the promptmask pair was otherwise accurate. These rejected pairs remain useful for training but are excluded from CONVERSEG to preserve diversity. Aggregating across these conditions, the VLM verifier and human annotators make the same decision on about 70% 14 of examples. In the common disagreement case where the VLM accepts but the human rejects, the VLM decision is often not semantically incorrect (e.g., partial coverage or duplicate prompts), so these examples remain valuable as training data. This behavior is appropriate for automatically generating large pool of candidate examples, while human verification is used to ensure benchmark-quality data. In practice, the verifier provides strong starting set from which annotators can efficiently curate high-quality examples for CONVERSEG. Hyperparameter Value Optimizer Learning rate (η) LR schedule Weight decay Batch size / grad. accum. Steps (Stage 1, Stage 2) Image resolution LoRA rank and alpha (r, α) AdamW 1 104 Warmup + cosine (min 106) 0.05 6 / 1 100 000, 90 000 1024 1024 (longer side) 16, 32 C.3. Additional Statistics and Visualizations Table 6. Training hyperparameters for CONVERSEG-NET. In Fig. 11, we show bar charts indicating the number of examples per concept family (entities, spatial & layout, relations & events, affordances & functions, physics & safety) for each split of CONVERSEG. These statistics complement the distributional analysis in the main paper and confirm that all concept families are well represented. Region Type Diversity. Beyond whole-object instances, CONVERSEG includes diverse region types such as object parts, surfaces, and functional areas. The SAM-seeded split naturally incorporates these non-instance regions because SAM2 can generate masks for parts and surfaces in addition to complete objects. The human-annotated split further incorporates stuff regions from COCO-Panoptic, such as sky, grass, walls, and other amorphous areas. Fig. 12 shows representative examples from the SAM-seeded split, including object parts, surfaces, and functional regions. C.4. Additional Qualitative Examples We provide additional qualitative examples from CONVERSEG in Fig. 16, illustrating the diversity of conversational prompts and corresponding masks across concept families and splits. D. Implementation Details D.1. Architecture Prompt encoder. We use Qwen2.5-VL-3B [1] as frozen multimodal backbone: it takes both the RGB image and the conversational prompt as input. From the final hidden states we keep only positions corresponding to text tokens (padding, special tokens, and image placeholders are discarded). These token embeddings are layer-normalized and linearly projected to the SAM2 decoder width to form sparse language tokens. The EOS embedding is passed through small MLP and broadcast as dense bias map. Only the adapter projections and LoRA weights on top of Qwen are trained. Mask decoder. We use the SAM2 [32] Hiera-L configuration (sam2_hiera_l.yaml). The image encoder is frozen. The mask decoder takes the sparse and dense language embeddings as prompt inputs. We supervise only the first output mask. D.2. Training Hyperparameters We fine-tune the SAM2 mask decoder, SAM2 prompt encoder, and language adapter in two stages (pretraining and conversational post-training). Each stage is trained for 35 000 steps with AdamW and cosine schedule with linear warmup. Images are resized so that the longer side is 1024 pixels; masks are binarized and eroded with 5 5 kernel (one iteration). We use batch size of 6 with no gradient accumulation. The main optimization and model hyperparameters are summarized in Tab. 6. E. Additional Quantitative Results In this section, we present additional quantitative comparisons between CONVERSEG-NET and existing baselines. E.1. Cumulative IoU (cIoU) In the main paper, we reported gIoU performance of CONVERSEG-NET on the RefCOCO/+/g and ReasonSeg benchmarks. In Tab. 9, we report the corresponding cumulative IoU (cIoU) for CONVERSEG-NET on the same benchmarks, complementing the gIoU results. In Tab. 8, we additionally report cIoU performance of CONVERSEG-NET on CONVERSEG. E.2. Additional Baselines We extend Table 2 of the main paper to include additional baselines and report gIoU performance in Tab. 7. This expanded comparison situates CONVERSEG-NET among broader set of contemporary referring and reasoning segmentation approaches and provides more complete view of the current landscape. Comparison with SAM3. The recent work, SAM3 [3], is new variant of SAM that supports natural language promptable segmentation. We evaluate SAM3 on CONVERSEG and report results in Tab. 10. SAM3 achieves 39.7% gIoU on the SAM-seeded split and 35.4% on the human-annotated 15 (a) Human-annotated split (b) SAM-seeded split Figure 11. Distribution of examples per concept in the two splits of CONVERSEG. Figure 12. Examples of non-instance regions in CONVERSEG. The masks capture object parts, surfaces, and functional areas, demonstrating coverage of diverse region types beyond complete object instances. split, substantially lower than CONVERSEG-NET (70.8% and 67.4% respectively with the 3B backbone). This demonstrates that our conversational training approach and conceptdriven data engine provide significant gains for abstract reasoning in conversational image segmentation. 16 Figure 13. Examples of negative prompts generated by our data engine. Each row shows an image with its corresponding negative prompt. The correct model response for all these prompts is an empty mask. Figure 14. Impact of negative training data. Model predictions before and after negative training on adversarial prompts. After training, the model correctly produces empty masks for invalid prompts. Robustness also transfers to simpler literal negatives (e.g. \"Segment the tiger\"), despite not being explicitly trained on them. 17 Figure 15. Qualitative examples of VLM verifier behavior, illustrating agreement and disagreement with human annotators on candidate promptmask pairs in CONVERSEG. 18 Figure 16. Additional qualitative examples from CONVERSEG. Each panel shows an input image, its conversational prompt, and the corresponding ground-truth mask overlaid. Examples span all concept families (entities, spatial & layout, relations & events, affordances & functions, physics & safety) and belong to the human-annotated split. Model Prompt Encoder RefCOCO RefCOCO+ RefCOCOg ReasonSeg val testA testB val testA testB val(U) test(U) val(G) val test test(short) test(long) LISA LISA LISA LISA LISA LISA LISA LISA LISA SEEM Grounded SAM OVSeg Seg-Zero Seg-Zero GSVA GLaMM SAM4MLLM SAM4MLLM SAM4MLLM GLEE GLEE DETRIS-L UniLSeg-20 UniLSeg-100 PSALM EVF-SAM EVF-SAM RICE MLCD-seg HyperSeg HyperSeg Gemini Seg X-SAM RSVP RSVP RSVP RSVP LLaVA 7B LLaVA 7B LLaVA 13B LLaVA 13B Llama2 13B LLaVA1.5 7B LLaVA1.5 7B LLaVA1.5 13B LLaVA1.5 13B Qwen2.5-VL 3B Qwen2.5-VL 7B Vicuna 13B Vicuna 7B Qwen-VL 7B LLaVA1.6 7B LLaVA1.6 8B CLIP CLIP CLIP CLIP ViT-B/16 CLIP ViT-B/16 Phi1.5 1.3B BEIT-3-Large BEIT-3-Large Qwen2.5-7B Qwen2.5-7B Phi2 2.7B Phi2 3B Gemini2.5 Flash Phi3 3.8B LLaVA1.6 7B Qwen2-VL 7B Gemini1.5-Flash GPT-4o 74.9 79.2 79.5 79.6 79.8 79.5 80.0 81.0 80.5 81.7 83.6 82.1 82.4 83.5 83.6 84.8 85.1 CONVERSEG-NET (Base) Qwen2.5-VL 3B 78.4 Qwen2.5-VL 3B 78.2 CONVERSEG-NET Qwen2.5-VL 7B 79.4 CONVERSEG-NET 79.1 81.7 83.2 82.8 82.7 81.9 81.8 83.2 84.7 83.7 84.2 85.3 85.3 85.7 87.1 80.8 80.3 81.6 72.3 77.1 76.9 76.1 74.7 79.0 78.4 79.9 81.6 80.0 80.2 81.7 81.5 83.4 83.4 75.8 74.7 76.4 65.1 70.3 72.6 73.5 74.6 68.3 69.6 75.2 72.7 73.2 72.9 75.2 76.5 79.4 79.4 79.0 78.0 72.5 72.0 74.3 70.8 73.8 78.7 77.8 80.0 78.6 77.0 78.3 75.5 78.3 80.0 82.8 82.9 83.5 81.0 77.7 77.5 79.1 58.1 63.6 64.6 65.8 67.2 70.2 67.0 68.2 70.1 70.1 71.9 75.4 75.6 75.2 74.4 66.4 66.3 69.2 67.9 75.7 74.2 74.5 75.5 70.6 72.9 74.6 78.4 79.3 73.8 76.8 78.2 79.8 79.7 79.4 83.8 75.1 74.1 74.9 70.6 75.1 77.0 74.9 75.6 76.4 75.3 79.5 80.5 74.4 77.4 78.3 80.4 80.5 78.9 83.9 74.7 73.9 75.5 74.7 73.7 75.0 44.4 52.9 48.9 56.2 60.0 53.6 61.3 57.7 65.0 25.5 26.0 28.5 58.2 62.6 47.4 46.7 58.4 59.2 28.3 56.6 59.2 58.6 56.9 64. 51.1 56.4 61.9 36.8 47.3 44.8 51.7 51.5 48.8 55.6 53.8 61.3 24.3 21.3 26.1 56.1 57.5 30.6 57.8 56.9 56.1 57.1 60.3 48.3 52.2 57.0 37.6 40.6 39.9 44.3 43.9 48.3 48.3 50.8 55.4 20.1 17.8 18.0 16.5 47.7 47.9 48.5 47.3 55.4 47.2 53.8 54.2 36.6 49.4 46.4 54.0 54.0 49.2 57.9 54.7 63.2 25.6 22.4 28.7 35.0 56.0 58.4 57.1 60.2 61. 48.6 51.7 57.9 Table 7. Referring expression segmentation (gIoU, %). CONVERSEG-NET is competitive on RefCOCO/+/g and achieves strong zero-shot performance on ReasonSeg, surpassing methods fine-tuned on ReasonSeg (). trained on RefCOCO only; on RefCOCO plus additional datasets (Objects365, PACO-LVIS, PASCAL-Part, etc)."
        },
        {
            "title": "Prompt Encoder",
            "content": "SAM-seeded (cIoU) Human-annotated (cIoU) All Ent. Spat. Rel. Aff. Phys. All Ent. Spat. Rel. Aff. Phys. CONVERSEG-NET Qwen2.5-VL 3B 70.8 76.7 73.6 75.5 65.7 CONVERSEG-NET Qwen2.5-VL 7B 72.9 78.7 71.9 78.2 68.0 57.7 62.3 67.8 73.4 65.5 67.3 68.0 67.5 66.9 72.3 66.1 68. 59.2 64.6 Table 8. CONVERSEG benchmark results (cIoU, %). Each subset reports performance across the five concept categories Entities, Spatial, Relations, Affordances, and Physics & Safety and summarizes across all (All). 20 Model Prompt Encoder RefCOCO RefCOCO+ RefCOCOg ReasonSeg val testA testB val testA testB val(U) test(U) val(G) val test test(short) test(long) CONVERSEG-NET Qwen2.5-VL 3B 77.6 CONVERSEG-NET Qwen2.5-VL 7B 79. 79.4 81.0 73.9 75.9 71.7 74.4 76.1 78.1 65.6 68.6 74.1 75. 73.7 75.7 75.5 76.8 64.0 64.3 56.6 60.4 51.6 53.5 58.0 62. Table 9. Referring expression segmentation (cIoU, %). CONVERSEG-NET is competitive on RefCOCO/+/g and shows strong zero-shot performance on ReasonSeg. Model SAM3 Prompt Encoder SAM-seeded (cIoU) Human-annotated (cIoU) All Ent. Spat. Rel. Aff. Phys. All Ent. Spat. Rel. Aff. Phys. Perception Encoder 39.7 47.5 40.2 44.1 35.7 25.9 35.4 45.8 27.0 32.6 32.5 36.6 CONVERSEG-NET Qwen2.5-VL 3B CONVERSEG-NET Qwen2.5-VL 7B 70.8 74.0 70.9 74.1 68.7 72.4 76.1 71.1 77.5 70.4 64.2 63.7 67.4 71.6 68.7 67.0 64.4 67.9 70.0 71.5 69.3 63.5 63.8 64.0 Table 10. Comparison with SAM3 on CONVERSEG (gIoU %). Each subset reports performance across the five concept categories Entities, Spatial, Relations, Affordances, and Physics & Safety and summarizes across all (All). Figure 17. Meta-prompt for Stage 1 (Scene Understanding). Prompt template used to query Gemini-2.5-Flash to produce detailed region-level descriptions of the scene, which later serve as the semantic basis for mask generation and concept-driven prompt construction. Figure 18. Meta-prompt for Stage 3 masktext consistency checking. Prompt template used to ask the VLM whether candidate mask is consistent with its associated region description, enabling automatic filtering of low-quality or mismatched masks. Figure 19. Meta-prompt for Stage 3 mask refinement and selection. Prompt template used to compare two candidate masks for the same region description and select the most appropriate one, based on coverage, tightness, and semantic alignment. 22 Figure 20. Meta-prompt for Stage 4 (Affordances & Functions). Concept-specific prompt template used to turn region descriptions into conversational queries about object affordances and functional roles; analogous templates are used for the other concept families. Figure 21. Meta-prompt for Stage 5 promptmask alignment verification. Prompt template used to ask the VLM whether generated conversational prompt correctly and unambiguously describes the masked region, providing final quality gate for training examples. 24 Figure 22. Meta-prompt for Negative Data Generation (Affordances & Functions). Concept-specific template used to generate adversarial negative prompts that describe plausible but absent affordances or incorrect functional states. Similar templates are used for other concept families. 25 Figure 23. Meta-prompt for Negative Prompt Verification. Template used to verify that generated negative prompts have no valid corresponding masks in the image. The VLM checks whether any regions satisfy the prompt requirements before the negative example is included in training. Figure 24. Qualitative comparisons on the human-annotated split of CONVERSEG (1/3). Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions from CONVERSEG-NET (Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right. CONVERSEG-NET more reliably segments the regions implied by the conversational intent despite using smaller 3B backbone. 27 Figure 25. Qualitative comparisons on the human-annotated split of CONVERSEG (2/3). Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions from CONVERSEG-NET (Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right. CONVERSEG-NET more reliably segments the regions implied by the conversational intent despite 28 using smaller 3B backbone. Figure 26. Qualitative comparisons on the human-annotated split of CONVERSEG (3/3). Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions from CONVERSEG-NET (Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right. CONVERSEG-NET more reliably segments the regions implied by the conversational intent despite using smaller 3B backbone. 29 Figure 27. Qualitative comparisons on the SAM-seeded split of CONVERSEG (1/3). Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions from CONVERSEG-NET (Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right. CONVERSEG-NET more reliably segments the regions implied by the conversational intent despite using smaller 3B backbone. 30 Figure 28. Qualitative comparisons on the SAM-seeded split of CONVERSEG (2/3). Each row shows an image with its conversational 31 prompt (between rows), the ground-truth mask (left), and predictions from CONVERSEG-NET (Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right. CONVERSEG-NET more reliably segments the regions implied by the conversational intent despite using smaller 3B backbone. Figure 29. Qualitative comparisons on the SAM-seeded split of CONVERSEG (3/3). Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions from CONVERSEG-NET (Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right. CONVERSEG-NET more reliably segments the regions implied by the conversational intent despite using smaller 3B backbone. 32 Figure 30. Representative failure cases of CONVERSEG-NET on the human-annotated split of CONVERSEG. Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions from CONVERSEG-NET (Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right. Figure 31. Representative failure cases of CONVERSEG-NET on the SAM-seeded split of CONVERSEG. Each row shows an image with its conversational prompt (between rows), the ground-truth mask (left), and predictions from CONVERSEG-NET (Qwen2.5-VL-3B), LISA (LLaVA-7B), and LISA (Llama2-13B) from left to right."
        }
    ],
    "affiliations": [
        "Caltech"
    ]
}