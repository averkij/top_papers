{
    "paper_title": "Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence",
    "authors": [
        "Ling-Hao Chen",
        "Yuhong Zhang",
        "Zixin Yin",
        "Zhiyang Dou",
        "Xin Chen",
        "Jingbo Wang",
        "Taku Komura",
        "Lei Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, a novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or a few example motions on the target skeleton, by accessing a sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of our approach is further evidenced by its successful integration in downstream applications and user interfaces, highlighting its potential for industrial applications. Code and data are available at https://lhchen.top/Motion2Motion."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 9 3 1 3 1 . 8 0 5 2 : r Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence LING-HAO CHEN, Tsinghua University, International Digital Economy Academy, China YUHONG ZHANG, Tsinghua University, China ZIXIN YIN, The Hong Kong University of Science and Technology, China ZHIYANG DOU, The University of Hong Kong, China XIN CHEN, ByteDance, United States of America JINGBO WANG, Shanghai Artificial Intelligence Laboratory, China TAKU KOMURA, The University of Hong Kong, China LEI ZHANG, International Digital Economy Academy, China Fig. 1. We propose Motion2Motion, enabling motion transfer across characters with vastly different topologies. From left to right, we show motion transfer results across increasingly different target characters, anaconda king-kobra (in-species transfer) T-rex (cross-species transfer). This work studies the challenge of transfer animations between characters whose skeletal topologies differ substantially. While many techniques have advanced retargeting techniques in decades, transfer motions across diverse topologies remains less-explored. The primary obstacle lies in the inherent topological inconsistency between source and target skeletons, which restricts the establishment of straightforward one-to-one bone correspondences. Besides, the current lack of large-scale paired motion datasets spanning different topological structures severely constrains the development of data-driven approaches. To address these limitations, we introduce Motion2Motion, novel, training-free framework. Simply yet effectively, Motion2Motion works with only one or few example motions on the target skeleton, by accessing sparse set of bone correspondences between the source and target skeletons. Through comprehensive qualitative and quantitative evaluations, we demonstrate that Motion2Motion achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios. The practical utility of our approach is further evidenced by its successful integration in downstream applications and user interfaces, highlighting its potential for industrial applications. Code and data are available at https://lhchen.top/Motion2Motion. CCS Concepts: Computing methodologies Animation; Computer vision. Additional Key Words and Phrases: Motion transfer, Motion synthesis"
        },
        {
            "title": "INTRODUCTION",
            "content": "Transferring motion from one character to another with different topology is long-standing research problem in computer animation. Motion transfer occupies pivotal position in character animations, especially when mapping motion to unseen characters. Work done during Ling-Hao Chens internship at IDEA Research. evan@lhchen.top. To tackle this fundamental problem, series of methods have been proposed [Aberman et al. 2020; Zhang et al. 2023b] in recent years. One category is skeleton-based motion transfer for various shapes of skeletons, the other takes the geometry of two characters into consideration for more fine-grained motion transfer. Despite these progresses, previous methods face significant challenges in real applications, especially transfer motions between two characters with different topologies. Regardless of whether skeleton-based or geometric-based methods, most of these methods [Ye et al. 2024; Zhang et al. 2023b] rely on quite amount of data. However, due to the limited accessibility of motion datasets with different characters, these methods often fail at test time. Another trickier issue is that the skeleton topologies and geometries of the target characters are often more complex than the training data. For instance, methods trained on humanoid-like data, such as Mixamo [Adobe 2024], struggle to retarget motions to characters with complex dynamics like skirts or hair, let alone transfer from bipedal to quadrupeds. Although recent attempts [Li et al. 2024] tried to retarget animation between humans and quadrupeds, they still need tailored training to accommodate diverse morphologies, making generalization to more complex characters challenging. There are two key challenges to transfer animations from one topology to another. The first challenge is the limited availability of animation data across diverse topologies. In practical applications, only few motion examples exist for the target skeleton, forcing data-driven methods to rely on large-scale datasets. Unfortunately, even the most readily accessible human motion data, such as SMPLbased motion [Loper et al. 2023], suffers from an insufficient volume of high-quality examples. The second challenge involves binding ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025. 2 Ling-Hao Chen, et al. correspondences for all bones between source and target skeletons, especially when the target one has significantly different topology or number of joints. This makes it difficult to define consistent joint-level mappings, which are critical for motion transfer. To make this problem tractable, we introduce two mild assumptions that align with real-world constraints. First, we assume access to small number of motion sequences on the target skeleton, few-shot setting that reflects the scarcity of data in most practical scenarios. If there is no motion example at all, transfer becomes ill-posed, as there is no signal to anchor temporal or spatial motion patterns on target skeletons. This minimal data availability provides satisfying contexts to guide meaningful transfer while avoiding the need for large-scale annotation. Second, we assume that sparse joint correspondence between source and target skeletons is available, which can be specified by users or automatic matching (Appendix A). This minimal bone mapping offers rough semantic alignment and serves as useful prior for motion-level matching, step that is commonly required in both academic and commercial systems (e.g. , AutoRig [Artell 2025], and Rokoko [Rokoko 2021]). Considering these issues, we introduce Motion2Motion, novel approach for animation transfer under sparse joint correspondences (sparse bone binding). Specifically, Motion2Motion models the cross-topology motion transfer problem as conditional patch-based motion matching problem. The bone correspondences specified by users provide coarse motion semantics, serving as the input spatial conditions for transfer. Departing from traditional neural transfer pipelines, Motion2Motion synthesizes motions for the target skeleton by matching motion patches of the bound joints from few example animations. This design is motivated by straightforward observation: motion coordination between bound and unbound joints can often be inferred by observing only few examples. For instance, in quadruped locomotion, the movement of the hind legs can often be extrapolated from the motion patterns of the front legs alone by some examples, even in one/few-shot settings. This simple yet effective matching-based transfer allows Motion2Motion to outperform current state-of-the-art methods. key application of Motion2Motion is its capacity to retarget SMPL-based motion [Loper et al. 2023] to those more complex topologies, such as characters featuring dynamic elements like skirts or hair. In addition to this capability, Motion2Motion further enables seamless motion transfer across structurally diverse skeletons, even between different species, e.g. anaconda v.s. raptor (in Fig. 8). Our framework is also flexible in its choice of motion representation. Specifically, it supports variety of features for transfer, including velocity fields, and the local pose positions, demonstrating its broad generalizability and robustness. This work paves the way for topology-flexible motion transfer, enabling motion adaptation across different character structures in real time. To support practical use, we also develop Blender add-on (Section 6), demonstrating the applicability in real-world animation workflows with good interpretability. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Motion Retargeting. Motion transfer was first introduced by Gleicher [1998], who approached the problem as space-time optimization challenge subject to kinematic constraints. This seminal work paved the way for various optimization-based methods that incorporated additional constraints [Bernardin et al. 2017; Feng et al. 2012; Lee and Shin 1999]. More recently, advances in deep learning have enabled more flexible, data-driven solutions for motion retargeting. For instance, Jang et al. [2018] proposed an autoencoder-based method for motion generation, Aberman et al. [2020] introduced Skeleton-Aware Network to address differences in skeletal topology, and Lim et al. [2019] developed framework that decouples the learning of local poses from global motion. Li et al. [2022b] presented an iterative retargeting approach that uses motion autoencoders to refine results over multiple steps. Some methods have incorporated character geometry into motion retargeting by preserving contacts or avoiding interpenetration [Ho et al. 2010; Jin et al. 2018; Lyard and Magnenat-Thalmann 2008; Zhang et al. 2024b, 2023b]. Unlike previous methods that rely on dense correspondences or large-scale data, our approach works under sparse joint mappings and limited target motions. We formulate motion transfer as matching-andblending process over motion patches, enabling flexible transfer across structurally different skeletons. This design allows generalization to complex topologies with minimal samples. Generative motion models and motion matching. Human motion generation [Athanasiou et al. 2022; Barquero et al. 2024; Bhattacharya et al. 2021; Dabral et al. 2023; Lu et al. 2024; Petrovich et al. 2022; Shafir et al. 2024; Tevet et al. 2022; Wang et al. 2022; Xie et al. 2024; Yuan et al. 2023; Zhang et al. 2024a, 2023a] reaches significant progress in past years, benefiting from the foundational humancentric vision technologies [Loper et al. 2023; Mahmood et al. 2019]. Researchers have extensively explored the generation of human motion using various modalitiesincluding audio [Bhattacharya et al. 2021; Chen et al. 2024], music [Li et al. 2023b; Tseng et al. 2023], and text [Chen et al. 2023a; Dai et al. 2024; Tevet et al. 2022; Zhou et al. 2024]as well as through unconditional approaches [Chen et al. 2023b; Li et al. 2022a]. In addition to neural synthesis methods, techniques based on generative motion matching have also shown strong performance on synthesizing high-quality motions [Bergamin et al. 2019; BÃ¼ttner and Clavet 2015; Holden et al. 2020; Li et al. 2023a]. Motion matching [Agata and Igarashi 2025] itself is high-level concept in animation creation, primarily employed for character control. However, the application of motion matching and blending to motion transfer, especially across different skeletal topologies, remains underexplored from both research and engineering perspectives. The latest progress in generating motions across diverse topologies is represented by AnyTop [Gat et al. 2025]. However, AnyTops architecture does not include motion transfer module, and its generalization capability is restricted by the relatively limited scale of its training data. In contrast, Motion2Motion takes an early step by transfer source motion to target skeleton using one or few sample target motions, marking the first attempt to harness motion matching and blending in cross-topology transfer. ğ·ğ‘  ğ¹ğ‘  ğ‘ƒ Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence 3 ğ·ğ‘¡ ğ¹ğ‘  Fig. 2. System overview of Motion2Motion. (A) The source motion sequence Rğ¹ğ‘  ğ·ğ‘  . (B) The source sequence is divided into overlapping motion patches (ğ‘  ) . (C) Each source patch is projected to the target skeleton space via sparse mapping and noise initialization, serving as the query for retrieval. For each source patch, we retrieve target patches (D) from pre-built motion patch database (ğ‘¡ ) , based on sparse correspondences. (E) The matched target patches are averaged for blending. (F) The retargeted motion (cid:98)T Rğ¹ğ‘  ğ·ğ‘¡ is reconstructed from the blended target patches. (C)-(F) are executed in ğ¿ times."
        },
        {
            "title": "3.2 Motion Patching",
            "content": "As illustrated in Fig. 2-(A) and (B), prior to transfer, we perform motion patching on the source motion along the temporal axis without padding. In contrast to the patching operation in the image domain, we apply sliding window to patchify the motion, ensuring the preservation of meaningful motion features. This process is grounded in the observation that motion patches capture significant temporal dynamics, and the patch size (ğ‘ƒğ‘† ) is relatively atomic. After the motion patching operation, we obtain ğ‘ƒ patches of the motion, where ğ‘ƒ = . Similarly, we apply the same operation to the target motion set using the same patch and step size, resulting in target motion patches in Fig. 2-(C). The source and target motion patches are denoted as set (ğ‘  ) and set (ğ‘¡ ) , respectively. (cid:106) ğ¹ğ‘  patch size+1 step size (cid:107)"
        },
        {
            "title": "3.3 Spatial Correspondence between Skeletons\nLet M = {(ğ‘¡, ğ‘ ) | ğ‘¡ âˆˆ Jğ‘¡ , ğ‘  âˆˆ Jğ‘  } denote the set of sparse keypoint\ncorrespondences between the target and source skeletons, where\nJğ‘¡ and Jğ‘  represent the index sets of keypoints in the target and\nsource skeletons, respectively. We denote the number of correspon-\ndences by ğ¾ = |M|. A correspondence (ğ‘¡, ğ‘ ) âˆˆ M indicates that\nthe keypoint ğ‘¡ in the target skeleton is semantically aligned with\nthe keypoint ğ‘  in the source skeleton. The correspondence of bone",
            "content": "binding can be specified by the user or be bound in an automatic way via fuzzy subgraph matching (algorithm detail in Appendix A). To specify the indexes of each keypoint within the motion representation vectors, we introduce two index functions I1 (), I2 () : N, such that for any keypoint ğ‘¡ (or ğ‘ ), I1 (ğ‘¡) and I2 (ğ‘ ) denote the start and end indices of the channels in the corresponding bones (e.g., Rğ¹ ğ·ğ‘¡ for the target or Rğ¹ ğ·ğ‘  for the source). Based on these definitions, we construct correspondence matrix Rğ·ğ‘¡ ğ·ğ‘  whose block components are defined as follows: (cid:104) I1 (ğ‘¡) : I2 (ğ‘¡), I1 (ğ‘ ) : I2 (ğ‘ ) (cid:105) = (cid:40) , , if (ğ‘¡, ğ‘ ) M, otherwise, (1) where denotes an identity matrix of appropriate dimensions and denotes zero matrix of corresponding size. This formulation ensures that, in motion matching, only the segments of the motion representations corresponding to the predefined correspondences are aligned, thereby effectively capturing the core kinematic characteristics of the motion while discarding redundant information. We further introduce mask vector to identify target skeletal components that lack any correspondence in M. Specifically, the mask is computed as m[ğ‘–] = (cid:205)ğ·ğ‘  ğ‘—=1 C[ğ‘–, ğ‘—], which effectively indicates, for each target dimension ğ‘–, whether valid mapping from the source exists. Note that the vector can be sparse, that is small number of joint correspondences. 3.4 Iterative Matching-based Transfer With the sparse correspondence established above, we aim to transfer motion information between structurally different skeletons. Motivated by motion matching, we formulate the transfer process as retrieval-and-blending procedure over motion patches. This allows us to explore the application of motion matching under crosstopology settings with sparse features, where the source and target skeletons differ significantly in structure and representation. Motion projection and initialization. Using this mask, the mapping from the source motion to the target skeleton is defined as ğ‘ ğ‘¡ = SC + (cid:0)1 m(cid:1) N, (2) where (O, I) Rğ¹ ğ·ğ‘¡ denotes noise term drawn from standard normal distribution. The first term, SC, represents the motion mapping from the source skeleton to the target skeleton ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025. 4 Ling-Hao Chen, et al. Algorithm 1: Iterative Matching-based Motion Retargeting Input: Source motion S, target patch set (ğ‘¡ ) , weight ğ›¼, correspondence matrix C, mask m, iterations ğ¿. this, we repeat the matching-and-blending process ğ¿ times, progressively refining the result to ensure temporal coherence across the entire sequence. The whole algorithm is in Algorithm 1. Output: Retargeted target motion (cid:98)T. 1 Project source motion: Pğ‘ ğ‘¡ SC + (1 m) N. 2 Initialize (cid:98)T Pğ‘ ğ‘¡ . 3 for â„“ 1 to ğ¿ do Initialize matched target set = {}. Pathify (cid:98)T into ((cid:98)ğ‘¡ ) . foreach P((cid:98)ğ‘¡ ) ((cid:98)ğ‘¡ ) do from (ğ‘¡ ) with mask by Eq. (3). 5 6 7 9 10 11 end 12 return (cid:98)T ğ‘šğ‘ğ‘¡ğ‘â„ Retrieve Append Pğ‘šğ‘ğ‘¡ğ‘â„ into M. end Blend patches in as (cid:98)T. based on predefined keypoint correspondences. The second term, (cid:0)1 m(cid:1) N, is initialized by noise for target dimensions without corresponding source mapping. The transfer process can be viewed as the transformation of these noisy components into the desired locomotion, conditioned on the first term. After the procedure, the noisy parts will be replaced by the retargeted locomotion. This formulation ensures that for target dimensions without corresponding source mapping, the motion is augmented by controlled stochastic variation. For convenience, we rename Pğ‘ ğ‘¡ as (cid:98)T in following sections. Masked motion matching. After obtaining the projected motion (cid:98)T in Eq. (2), we formulate the synthesis process of the noisy part as generative process conditioning on SC. We also patchify the the projected motion (cid:98)T as ((cid:98)ğ‘¡ ) . Motivated by generative motion matching [BÃ¼ttner and Clavet 2015; Li et al. 2023a], the generation process of target motion patches in Motion2Motion is masked motion patch retrieval. Specifically, for each P((cid:98)ğ‘¡ ) ((cid:98)ğ‘¡ ) , ğ‘šğ‘ğ‘¡ğ‘â„ arg min (ğ‘¡ ) ğ›¼ (cid:16) P, P((cid:98)ğ‘¡ ) (cid:17) + (1 ğ›¼)L (cid:16) (1 m) P, (1 m) P((cid:98)ğ‘¡ ) (cid:17) , (3) where (, ) denotes the mean squared error (MSE) loss, and ğ›¼ [0, 1] balances the contributions from mapped and unmapped parts in (cid:98)T. larger ğ›¼ in Eq. (3) places more emphasis on the sparse correspondences, enforcing stronger semantic alignment between source and target motions. In contrast, the second term leverages noise in unmapped dimensions, introducing diversity and flexibility into the generated motion. Motion blending. For each source motion patch P(ğ‘  ) (ğ‘  ) , we (ğ‘¡ ) according to Eq. (3), match similar target patch resulting matching set M. We blend all retrieved target motion patches of in average, as illustrated in Fig. 2-(E). ğ‘šğ‘ğ‘¡ğ‘â„ However, directly blending the retrieved patches may compromise motion naturalness or distort key actions, especially in the regions corresponding to the sparse correspondences. To address ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025."
        },
        {
            "title": "Implementation Details",
            "content": "Data. In this section, we collect set of animal motions and human motions to present motion transfer across skeletons. The animal animation dataset is the Truebones-Zoo dataset [Truebones 2022]. The number of joints ranges from 9 to 143. The human animation data is borrowed from LAFAN [Harvey et al. 2020]. For qualitative experiments, we choose subset of Truebones-Zoo as the benchmark for comparison. Implementation. The patch size of the motion is set as 11 frames, ğ›¼ is set as 0.85 by default. We follow Li et al. [2022a], generating motion in an inverted pyramidal way. All of the baselines mentioned in the paper is run on 1 NVIDIA RTX-3090-24GB GPU, while our method is run on 1 MacBook-M1 chip without GPU."
        },
        {
            "title": "4.2 Evaluation Protocol",
            "content": "Metrics. We evaluate the transfer result from the following aspects. (1) Motion quality is evaluated via the FrÃ©chet Inception Distance (FID) between retargeted and real target skeleton motions. The FID metric measures the motion quality and the style consistency with the target motion distribution. We use the kinematic features [Siyao et al. 2022] as extracted features when calculating FID. (2) The temporal alignment between the source and retargeted motion is evaluated by the cosine similarity of the Power Spectral Density (PSD) across all joints. We also follow Zhao et al. [2024] using contact consistency as the metric to evaluate the temporal coherence, where the contact bones are labeled manually by researchers. (3) Transfer diversity is the average joint distance between each two samples among the 5 retargeted results. (4) We use inference FPS to evaluate efficiency. In remaining text, we calculate the binding rate as 2 100%, ğ½ğ‘† +ğ½ğ‘‡ where ğ½ğ‘† and ğ½ğ‘‡ are bone numbers of source and target skeletons. Baselines. We introduce two latest baselines for comparison. Poseto-Motion [Zhao et al. 2024] transfers motion from motion-rich source to target with only pose data, using learned pose prior to generate temporally coherent motions. It performs well with sparse or noisy poses and in cross-skeleton settings. In our comparison, we split the motion data into poses set in ahead. WalkTheDog [Li et al. 2024] aligns motions across different morphologies by shared phase manifold. It enables semantic and temporal alignment between structurally different characters (e.g., human to quadruped) and supports tasks like motion transfer and stylization. Benchmark. Inspired by previous in-topological motion transfer evaluation, we collect 14 character animations to evaluate the crosstopology motion transfer results. The evaluation benchmark comes up with 1,167 frames in total, covering running, walking, jumping, and attacking actions from Truebones-Zoo [Truebones 2022] and LAFAN [Harvey et al. 2020]. To thoroughly evaluate the algorithm bounds, we categorize the skeleton gap between the source and target characters as similar and cross-species transfer. In detail, we categorize the source-target characters into similar skeletons (e.g., Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence 5 Method training FID freq. align(%) contact con.(%) diversity FPS FID Similar Skeleton freq. align(%) Cross-species Skeleton contact con.(%) diversity FPS Li et al. [2024] Zhao et al. [2024] Motion2Motion (cid:34) (cid:34) (cid:37) 0.507 0.389 0.033 72.0 80.1 96.2 70.5 78.6 93.5 0.02 0.33 3.20 833 234 2.250 1.68 0.492 66.9 72.4 90.3 62.4 64.5 79.7 0.03 0.30 1.90 729 215 752 Table 1. Main evaluation results for motion transfer. Our method (Motion2Motion), in both similar skeleton and cross-species settings, achieves the best performance on synthesis quality, temporal coherence, and diversity. Different from baselines, our method runs without GPUs and deep model training. Li et al. [2024] Zhao et al. [2024] Fig. 3. Quantitative comparison with baselines. Each animation sequence is listed as frames from left to right. (Highlighted gray frames as comparison for baselines.) (A) Input source motion of dragon character. (BD) Retargeted sequences on the bat skeleton produced by (B) our method, (C) Li et al. [2024], and (D) Zhao et al. [2024]. Compared to baselines, our approach more faithfully preserves the original motion style, coherence, and frequency. biped-to-biped or crawling-to-crawling) and cross-species skeletons (e.g., biped-to-quadruped or crawling-to-biped)."
        },
        {
            "title": "4.3 Comparison with Baselines",
            "content": "Qualitative results. As shown in Fig. 3, we present visual comparison of motion transfer results across different methods. The top row (A) illustrates the source motion performed by dragon (143 joints), showcasing large-amplitude wing flaps and dynamic movement patterns. The following rows show the motion retargeted onto bat (48 joints) using (B) our method (binding 2 pairs, left and right UpperArm), (C) Walk-The-Dog [Li et al. 2024], and (D) Poseto-Motion [Zhao et al. 2024]. Our method (B) clearly follows the full range of the source motion, preserving both temporal structure and stylistic characteristics such as up and down wing spread events. In contrast, motions produced by baselines suffer from artifacts and temporal inconsistency. Furthermore, our method shows consistent spatial-temporal transitions across frames, demonstrating superior coherence and fidelity in cross-topology transfer. Quantitative results. Table 1 reports the quantitative comparison between our method and two recent baselines, Walk-The-Dog and Pose-to-Motion. Across both similar skeleton and cross-species settings, our method consistently achieves the best performance in all quality and diversity metrics. Specifically, we obtain the lowest FID, indicating high motion quality and style consistency. Our method also achieves the highest frequency alignment and contact consistency, demonstrating strong temporal coherence with the source motion. In terms of diversity, we outperform baselines by large margin, suggesting the ability to generate varied retargeted motions. Despite being training-free, our method maintains high inference FPS, comparable to Walk-The-Dog and significantly faster than Pose-to-Motion. Notably, two baselines are tested on one GPU, while ours is without GPU. Moreover, our method is model-free algorithm, without any model training. Analysis. The reason why two baselines work worse than the proposed method is that the model-based methods are highly datadriven, requiring large amount of data. This not only results in some artifacts on some OOD scenarios, but also easily leads to over-fitting issues and makes the result less diverse (see Table 1). Moreover, baselines are highly reliant on the trained data distribution. If the desired motion frequency is unobserved in training, it is hard to retarget motion to novel frequency provided by the source motion (see freq. align & contact con. in Table 1). However, our matching-based method can flexibly compose the motion patches by motion patch blending, simulating as frequency interpolator. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025. 6 Ling-Hao Chen, et al. Fig. 4. Motion matching visualization. A) Source motion on bear skeleton. (B) Target dog skeleton used for matching. (C) The retargeted motion result on the dog skeleton. The 2nd and 4th frames in (C) correspond to the 4th and 6th frames in (B), respectively, illustrating temporally non-linear yet semantically aligned motion matching. Fig. 5. Motion temporal coherence visualization. (A) Source running motion from Raptor character (36 joints). (B) Retargeted motion to human character (22 joints). (C) Retargeted motion to fox character (40 joints). The retargeted motions share the same temporal coherence and movement periodicity. Binding bones are in purple (right side)."
        },
        {
            "title": "4.4 Temporal Matching Visualization",
            "content": "Matching patches visualization. Fig. 4 provides qualitative visualization of our motion transfer process. The source and target skeletons correspond to bear (76 joints) and dog (55 joints), respectively. As shown on the left side of panels (A) and (B), the correspondences between the two skeletons are limited to 6 hind leg joints. Notably, the second and fourth frames in panel (C) correspond to the fourth and sixth frames in panel (B), demonstrating effective temporal alignment. Moreover, the motion of the dogs front legs is conditionally inferred from its hind leg motion, and the dynamic motion of the dogs long tail is not directly provided by the source. This flexible matching is achieved by leveraging sparse keypoint bindings and masked patch comparisons, enabling robust motion transfer across diverse topology skeletons. Phase manifold coherence. In addition to maintaining temporal coherence, our method ensures that the retargeted motions lie on smooth and consistent phase manifold. To present the robustness of the algorithm to human characters (from LAFAN dataset [Harvey et al. 2020]) outside the animal motion dataset. We randomly clip 5 motion sequences from the LAFAN dataset, each of which is no more than 80 frames (data with 60 FPS). Previous research shows the periodicity of the motion, especially in the phase manifold. To verify the motion coherence of the source motion and the retargeted one, we visualize the periodic phase of the joint positions. In Fig. 6, we visualize the phase of the dominant frequency component (a.k.a. the frequency component with the largest amplitude in the FFT). The constant phase bias shown in Fig. 6 means the coherent motion frequency. As visualized in Fig. 5, the phase progression in the retargeted motions (B and C) follows the same periodicity as the source Raptor motion (A), despite substantial differences in skeleton topology and motion dynamics. This coherence across species and motion styles highlights the robustness of our framework in preserving underlying phase structures during motion transfer. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025. Fig. 6. Phase visualization of the motion. (A) and (B) present the phases of RightToe and LeftToe. The bar figure is the phase variation curve, and the clock figure is the phase visualization at the 1-st and 10-th frames. The blue and orange colors denoted retargeted and source motion, respectively. Note that there is consistent phase bias between the source and target. Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence Settings FID freq. align(%) contact con.(%) diversity Ours (3 samples) Ours (2 samples) Ours 0.230 0.237 0.263 77.5 75.9 73.3 89.7 88.0 86.6 4.50 3.16 2.55 Table 2. Test time scaling property. The comparison with different number of target samples. quality alignment Li et al. [2024] 3.55.22 3.30.35 Zhao et al. [2024] 2.95.10 2.84.19 Ours 4.36.18 4.60.11 Table 3. User study results. Our method leads baselines over significant margin on both motion quality and alignment. the retargeted kingcobra motion keeps its original huddling style. As shown in Fig. 8-(A)/(C), we retarget motion from limbless species, an anaconda, to bipedal raptor skeleton with 36 joints. In this case, we establish sparse keypoint correspondences by binding only 4 key vertebral points between source and target skeletons. These sparse bindings are crucial in guiding the transfer process, ensuring motion alignment across skeletons. Moreover, as discussed in Section 3.4, the transfer diversity is controlled by the noise term. As shown in Fig. 8-(B)/(C), different colors for retargeted skeletons represent diverse results driven by the same source motion. Diversity of retargeted motions. In Eq. (3), ğ›¼ [0, 1] controls the weights of joint correspondence matching and random noise matching. The larger ğ›¼ means more accurate local motion matching on specified joints, that is, less diversity. Fig. 8 shows diverse results of the cross-species motion transfer. As can be seen in Table 1, our method enjoys the best diversity over baselines, qualitatively verifying the superiority of our algorithm. 4. Test-time Scaling Property Table 2 reports the performance of our method when varying the number of target samples in the dataset during inference. We observe that increasing the number of samples improves the overall performance across all metrics. Specifically, generating 3 samples significantly reduces the FID score and enhances both frequency alignment and contact consistency, indicating better quality and realism. Furthermore, the diversity score increases notably, showing that more samples lead to more diverse motion generations. Although the larger number of target examples benefits the transfer quality, the more #frames of the target skeletons help the overall quality. This demonstrates that our method benefits from test-time (inference) sampling without training, showcasing strong scalability."
        },
        {
            "title": "4.7 User Study",
            "content": "We evaluate the motion transfer result across the following aspects through user study. Users are required to evaluate the transfer result through (1) the quality of retargeted motion. (2) action alignment with the source motion. In this study, 50 users rate 10 source retargeted motion pairs with score 1-5. Baselines are introduced in Section 4.2. As shown in Table 3, our method leads baseline with significant margin on both quality and action alignment. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025. Fig. 7. Cross skeletal organisms transfer (biped quadruped). walking motion is transferred from flamingo (41 joints) to monkey (76 joints) by binding only 6 corresponding hind limb bones. The hind limb motions are kept synchronous, while the monkeys forelimb and tail movements are inferred based on the retargeted hind limb dynamics. Fig. 8. Cross skeletal organisms transfer (limbless biped). The diverse retargeted results of the same skeleton are shown in different colors. (A) Source motion of an anaconda. (B) Retargeted motion on the limbless skeleton (king-cobra). (C) Retargeted motion on the biped skeleton (raptor). The frames in (B) and (C) correspond to specific moments from (A), demonstrating the transformation from limbless to bipedal structure with semantically and temporally aligned motion."
        },
        {
            "title": "4.5 Cross- Skeleton and Species Transfer",
            "content": "Cross-skeletal motion transfer between species with significantly different anatomical structures is challenging problem. Our approach resolve this issue with very sparse correspondences. Biped quadruped. As illustrated in Fig. 7, we transfer walking motion from flamingo, bipedal species with 41 joints, to quadrupedal monkey skeleton comprising 76 joints. In this case, we establish sparse keypoint correspondences by binding only 6 hind limb bones between the source and target skeletons. These sparse bindings serve as anchors for transfer motion cues. In our framework, the synchronous motion of the hind limbs is maintained between the two skeletons; that is, the source motion directly drives the retargeted hind limb dynamics in the monkey. Meanwhile, the movements of the monkeys forelimbs, head, fingers, and tail are inferred from these sparse hind limb dynamics, ensuring natural and coherent overall gait despite the structural disparity. Limbless biped. We first retarget an attack motion from anaconda (27 joints) to the kingcobra (19 joints) in Fig. 8-(A)/(B), where 8 Ling-Hao Chen, et al. Fig. 9. Key-frame Cross-topology Retargeting. The frame number index is denoted as <X>. (A) The source motion of bat. The purple frames are given key frames. The transparent blue frames are ground-truth frames of the source motion. (B) Retargeted motion of dragon character. Settings FID freq. align(%) contact con.(%) diversity position velocity Ours 0.330 0.188 0.263 70.0 76.1 73. 84.4 89.3 86.6 2.41 1.95 2.55 Table 4. Flexible transfer features. Our method support diverse motion matching features. to Algorithm 1. As shown in Fig. 9, our method robustly works even when the source animation is very sparse in the temporal axis, not only in the spatial axis. Specifically, the retargeted unseen frames are well aligned with the source motion. This capability demonstrates that our method remains effective even when the source motion is corrupted or poorly crafted."
        },
        {
            "title": "4.8 Flexible Transfer Features",
            "content": "One of the key component of our method is the motion feature matching, whose matching feature is quite flexible. As shown in Alg. 1, the defined motion feature of 6D rotation is the default setting. However, the 3D loco-position or joint velocity features can also be used as matching features. As shown in Table 4, our method enjoys good performance with diverse matching features. Especially, the velocity feature even works better than 6D rotations and with less diversity. This is mainly because velocity is straightforward indicator in animation, capturing motion details sensitively, which also makes the matching process more deterministic."
        },
        {
            "title": "4.9 Key-frame Cross-topology Transfer",
            "content": "We found that our method can also be used in key-frame interpolation when given only some animation frames from the source skeletons. In this case, the invisible frames are initialized with the noise. Although these frames are matched blindly initially, the completion of the whole sequence can be reached in ğ¿ turns, according ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025."
        },
        {
            "title": "4.10 Ablation Study\nHow can ğ›¼, patch size ğ‘ƒğ‘ , and ğ¿ affect the quality and diversity?\nWe conducted ablations over the patch-matching hyperparameters\nto evaluate their impact on reconstruction quality and output diver-\nsity. Here, we do not distinguish two types of skeletal differences.\nThe blending weight ğ›¼ controls the strength of adherence to the\nbound jointsâ€™ motion. When ğ›¼ is set too small (e.g. 0.3), retargeted\nmotion becomes erratic and contact consistency drops. When set too\nlarge (e.g. 0.95), motion becomes overly â€œlockedâ€ to the source, re-\nducing diversity significantly. Varying the patch size ğ‘ƒğ‘† shows that\nincreasing ğ‘ƒğ‘† results in comparable quality, and less diversity. This\nindicates that a too large horizon may include more complex motion\npatterns for matching. The decrease in ğ‘ƒğ‘† compromises the consis-\ntency and quality, because the short temporal window size cannot\ncapture the semantically fruitful motion feature. Increasing ğ¿ (from\n1 to 5 iterations) improves temporal consistency and motion and\nmotion quality. However, excessively large ğ¿ offers minimal further\nimprovement and incurs greater computational cost. We therefore",
            "content": "Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence 9 Fig. 10. Key frames of the source captured SMPL-based motion (A) and the retargeted motion (B) on character. Settings Ours (ğ¿ = 1) Ours (ğ¿ = 5) Ours (ğ‘ƒğ‘  = 3) Ours (ğ‘ƒğ‘  = 17) Ours (ğ›¼ = 0.6) Ours (ğ›¼ = 0.7) Ours (ğ›¼ = 0.8) Ours (ğ›¼ = 0.95) Ours (default) FID freq. align(%) contact con.(%) diversity 0.412 0.255 0.309 0.264 0.359 0.344 0.298 0.257 0.263 65.4 73.8 71.1 70.0 60.1 65.5 71.8 75.5 73.3 70.4 87.0 83.3 80.5 67.1 75.4 83.3 89.1 86. 2.64 2.50 2.60 1.95 4.19 3.51 2.60 0.12 2.55 Table 5. Abalation study of different settings. The default setting of the Motion2Motion is ğ¿ = 3, ğ‘ƒğ‘† = 11, and ğ›¼ = 0.85. Similar Skeleton freq. align(%) contact con.(%) diversity Ours (auto.) Ours (directly copy) Ours (default) FID 0.039 0.024 0.033 94.5 97.1 96.2 91.7 95.9 93.5 Cross-species Skeleton freq. align(%) FID contact con.(%) Ours (auto.) Ours (directly copy) Ours (default) 0.666 0.702 0.492 86.7 88.0 90.3 72.4 75.3 79.7 3.25 2.99 3. diversity 1.98 1.66 1.90 Table 6. Ablation study of binding mechanism and transfer strategies. We compare automatic and manual bone binding, as well as direct copying bound motion features from the source after executing Algorithm 1. adopt ğ¿ = 3 as reliable default across both similar-skeleton and cross-species settings. Bone binding automatically v.s. manually? Table 6 compares our default (manual) binding setting with the automatic mode using fuzzy graph matching (algorithm in Appendix A). For similar skeletons, automatic binding incurs minor quality drop, and contact consistency lightly falls. This shows flexible binding choices. For cross-species transfers, the gap widens, denoting the value of expertverified correspondences when topologies differ greatly. Nonetheless, the automatic mode still produces reasonable results without user effort, making it practical solution. Can binding bones be directly copied with source motion features? We also evaluated naÃ¯ve strategy that directly copies source binding joint features to bound target joints (Ours (directly copy)) after patch matching in Algorithm 1. As shown in Table 6, direct copying achieves slightly higher transfer quality on cross-similarskeleton transfer. In cross-species transfer, the drawback is amplified. These results confirm that we can directly copy the bound Fig. 11. Application: Lifting SMPL-based motion to any characters. (A) The SMPL-based source motion captured from video. (B) The character with 331 joints was retargeted by the SMPL-based motion. The bound 21 joints are in purple. The retargeted frames are shown in Fig. 10. joint motion to the target skeleton when two skeletons share similar skeleton topologies (e.g. SMPL to other human characters). What is the best binding rate for bone correspondence? Due to page limits, we answer this question in Appendix B."
        },
        {
            "title": "5 APPLICATION: LIFTING A SMPL-BASED MOTION TO",
            "content": "ANY CHARACTER direct and realistic application of Motion2Motion is lifting the topological skeleton structure into more complex types. For instance, the generated SMPL-based motion with text or music [Jiang et al. 2024; Li et al. 2023b], even captured from videos [Shen et al. 2024; Zhang et al. 2025], cannot be directly retargeted to the characters used in industrial characters, whose skeletons have higher DoFs. As shown in Fig. 11, we retarget SMPL-based motion captured by HumanMM [Zhang et al. 2025] to new character with 331 joints, including skirt and hair points. This shows strong application of retarget motion to private characters from generated/captured SMPL [Loper et al. 2023] motions, whose algorithm is the most widely studied by the research community."
        },
        {
            "title": "6 USER INTERFACE",
            "content": "To demonstrate the practical viability of our Motion2Motion, we develop Blender add-on that integrates seamlessly with the native animation workflow, shown in Fig. 12. The user first loads source motion (A), for example flamingo walking motion, then selects few target-skeleton reference clips (B). The global panel (D) lets users specify the source armature, target motions, and blending ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025. 10 Ling-Hao Chen, et al. Fig. 12. User interface: Blender add-on for Motion2Motion. (A) Source motion of the flamingo. (B) Users can select few-shot samples from three motions from the target monkey skeleton. (C) The retargeted motion of the monkey. (D) Global settings of the skeleton choices. (E) The bone binding module (options: automatic or manual binding modes). weight ğ›¼, while the bone-binding module (E) supports both automatic binding and manual adjustment of bone pairs. After clicking the transfer button, the retargeted motion (C) will be synthesized in real time. This intuitive interface streamlines the cross-topology motion transfer application and indicates our method can be deployed directly within the animation creation workflow."
        },
        {
            "title": "7 CONCLUSION",
            "content": "Conclusion. In this paper, we propose novel framework, Motion2Motion, to retarget an animation from source skeleton to target one, requiring only very sparse bone correspondence. The proposed framework works in training-free fashion with realtime efficiency on CPU-only devices. Our algorithm works in patchwise motion matching mechanism, supporting flexible motion features for matching. Extensive experimental results indicate that our method is quite robust in in-species and cross-species motion transfer, demonstrating its applicability in diverse downstream tasks, especially transfer SMPL-based motion to any character. We also show the interaction interface of Motion2Motion in applications of animation creation pipelines. Failure cases. Though our method works well in some scenarios, it may fail in some cases. If the target examples are quite semantically different from the source motion (kungfu v.s. dancing), the retargeted result fails. Despite this, the community still lacks reasonable solution to this. At present, requiring some key frames as target examples by humans is relatively applicable method for this. Limitations and future work. While this work represents pioneering effort in cross-topology motion transfer, it still has limitations. Our algorithm relies on oneor few-shot target motions, which imposes laborious demands on the animation creation pipeline. Although this level of data requirement is minimal by current community standards, we will continue to explore more data-efficient and lightweight approaches in future work. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "The Motion2Motion author team would like to acknowledge all program committee members for their extensive efforts and constructive suggestions. In addition, Weiyu Li (HKUST), Shunlin Lu (CUHK-SZ), and Bohong Chen (ZJU) had discussed with the author team many times throughout the process. The author team would like to convey sincere appreciation to them as well."
        },
        {
            "title": "REFERENCES",
            "content": "Kfir Aberman, Peizhuo Li, Dani Lischinski, Olga Sorkine-Hornung, Daniel Cohen-Or, and Baoquan Chen. 2020. Skeleton-aware networks for deep motion retargeting. ACM TOG 39, 4 (2020), 621. Adobe. 2024. Mixamo. https://www.mixamo.com/. Accessed: 2025-05-21. Naoki Agata and Takeo Igarashi. 2025. Motion Control via Metric-Aligning Motion Matching. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers (SIGGRAPH Conference Papers 25). Association for Computing Machinery, New York, NY, USA, Article 11, 12 pages. https://doi.org/10.1145/3721238.3730665 Artell. 2025. Auto-Rig Pro. https://superhivemarket.com/products/auto-rig-pro. Accessed: 2025-05-21. Nikos Athanasiou, Mathis Petrovich, Michael Black, and GÃ¼l Varol. 2022. Teach: Temporal action composition for 3d humans. In 3DV. 414423. German Barquero, Sergio Escalera, and Cristina Palmero. 2024. Seamless human motion composition with blended positional encodings. In CVPR. 457469. Kevin Bergamin, Simon Clavet, Daniel Holden, and James Richard Forbes. 2019. DReCon: data-driven responsive control of physics-based characters. ACM Transactions On Graphics (TOG) 38, 6 (2019), 111. Antonin Bernardin, Ludovic Hoyet, Antonio Mucherino, Douglas GonÃ§alves, and Franck Multon. 2017. Normalized Euclidean distance matrices for human motion retargeting. In Proceedings of the 10th International Conference on Motion in Games. 16. Uttaran Bhattacharya, Nicholas Rewkowski, Abhishek Banerjee, Pooja Guhan, Aniket Bera, and Dinesh Manocha. 2021. Text2gestures: transformer-based network for generating emotive body gestures for virtual agents. In VR. 110. Michael BÃ¼ttner and Simon Clavet. 2015. Motion Matching - The Road to Next Gen Animation. In Proceedings of Nucl.ai Conference 2015. https://www.youtube.com/ watch?v=z_wpgHFSWss&t=658s Presentation Video. Bohong Chen, Yumeng Li, Yao-Xiang Ding, Tianjia Shao, and Kun Zhou. 2024. Enabling synergistic full-body control in prompt-based co-speech motion generation. In Proceedings of the 32nd ACM International Conference on Multimedia. 67746783. Ling-Hao Chen, Jiawei Zhang, Yewen Li, Yiren Pang, Xiaobo Xia, and Tongliang Liu. 2023b. Humanmac: Masked motion completion for human motion prediction. In ICCV. 95449555. Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. 2023a. Executing your commands via motion diffusion in latent space. In CVPR. 18000 18010. Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. 2023. Mofusion: framework for denoising-diffusion-based motion synthesis. In CVPR. 97609770. Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. 2024. MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model. ECCV (2024). Andrew Feng, Yazhou Huang, Yuyu Xu, and Ari Shapiro. 2012. Automating the transfer of generic set of behaviors onto virtual character. In Motion in Games: 5th International Conference, MIG 2012, Rennes, France, November 15-17, 2012. Proceedings 5. Springer, 134145. Inbar Gat, Sigal Raab, Guy Tevet, Yuval Reshef, Amit Bermano, and Daniel Cohen-Or. 2025. AnyTop: Character Animation Diffusion with Any Topology. arXiv preprint arXiv:2502.17327 (2025). Michael Gleicher. 1998. Retargetting motion to new characters. In Proceedings of the 25th annual conference on Computer graphics and interactive techniques. 3342. FÃ©lix G. Harvey, Mike Yurick, Derek Nowrouzezahrai, and Christopher Pal. 2020. Robust Motion In-Betweening. 39, 4 (2020). Edmond SL Ho, Taku Komura, and Chiew-Lan Tai. 2010. Spatial relationship preserving character motion adaptation. In ACM SIGGRAPH 2010 papers. 18. Daniel Holden, Oussama Kanoun, Maksym Perepichka, and Tiberiu Popa. 2020. Learned motion matching. ACM Transactions on Graphics (ToG) 39, 4 (2020), 531. Hanyoung Jang, Byungjun Kwon, Moonwon Yu, Seong Uk Kim, and Jongmin Kim. 2018. variational u-net for motion retargeting. In SIGGRAPH Asia 2018 Posters. 12. Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. 2024. Motiongpt: Human motion as foreign language. NeurIPS (2024). Taeil Jin, Meekyoung Kim, and Sung-Hee Lee. 2018. Aura mesh: Motion retargeting to preserve the spatial relationships between skinned characters. In Computer Graphics Forum, Vol. 37. Wiley Online Library, 311320. Jehee Lee and Sung Yong Shin. 1999. hierarchical approach to interactive motion editing for human-like figures. In Proceedings of the 26th annual conference on Computer graphics and interactive techniques. 3948. Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, and Olga Sorkine-Hornung. 2022a. GANimator: Neural Motion Synthesis from Single Sequence. ACM TOG 41, 4 (2022), 138. Peizhuo Li, Sebastian Starke, Yuting Ye, and Olga Sorkine-Hornung. 2024. WalkTheDog: Cross-Morphology Motion Alignment via Phase Manifolds. In SIGGRAPH, Technical Papers. https://doi.org/10.1145/3641519. Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su, Zeping Ren, Han Zhang, Yansong Tang, and Xiu Li. 2023b. Finedance: fine-grained choreography dataset for 3d full body dance generation. In ICCV. 1023410243. Shujie Li, Lei Wang, Wei Jia, Yang Zhao, and Liping Zheng. 2022b. An iterative solution for improving the generalization ability of unsupervised skeleton motion retargeting. Computers & Graphics 104 (2022), 129139. Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, and Baoquan Chen. 2023a. Example-based Motion Synthesis via Generative Motion Matching. ACM TOG 42, 4, Article 94 (2023). https://doi.org/10.1145/3592395 Jongin Lim, Hyung Jin Chang, and Jin Young Choi. 2019. Pmnet: Learning of disentangled pose and movement for unsupervised motion retargeting. In 30th British Machine Vision Conference (BMVC 2019). British Machine Vision Association, BMVA. Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael In Seminal Graphics Black. 2023. SMPL: skinned multi-person linear model. Papers: Pushing the Boundaries, Volume 2. 851866. Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, and Heung-Yeung Shum. 2024. Humantomato: Text-aligned whole-body motion generation. ICML (2024). Etienne Lyard and Nadia Magnenat-Thalmann. 2008. Motion adaptation based on character shape. Computer Animation and Virtual Worlds 19, 3-4 (2008), 189198. Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. 2019. AMASS: Archive of motion capture as surface shapes. In ICCV. 5442 5451. Mathis Petrovich, Michael Black, and GÃ¼l Varol. 2022. TEMOS: Generating diverse human motions from textual descriptions. In ECCV. 480497. Rokoko. 2021. Rokoko Motion Capture Solutions. https://www.rokoko.com/. Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit Bermano. 2024. Human motion diffusion as generative prior. In ICLR. Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. 2024. World-Grounded Human Motion Recovery via GravityView Coordinates. In SIGGRAPH Asia 2024 Conference Papers. 111. Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. 2022. Bailando: 3d dance generation by actor-critic gpt with choreographic memory. In CVPR. 1105011059. Shixuan Sun, Xibo Sun, Yulin Che, Qiong Luo, and Bingsheng He. 2020. Rapidmatch: holistic approach to subgraph query processing. Proceedings of the VLDB Endowment 14, 2 (2020), 176188. Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit Bermano. 2022. Human motion diffusion model. In ICLR. Truebones. 2022. Truebones. Free FBX/BVH Zoo. Over 75 Animals with Animations and Textures. https://truebones.gumroad.com/l/skZMC/. Accessed: 2024-07-01. Jonathan Tseng, Rodrigo Castellon, and Karen Liu. 2023. Edge: Editable dance generation from music. In CVPR. 448458. Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. 2022. Humanise: Language-conditioned human motion generation in 3d scenes. NeurIPS (2022), 1495914971. Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. 2024. Omnicontrol: Control any joint at any time for human motion generation. In ICLR. Zijie Ye, Jia-Wei Liu, Jia Jia, Shikun Sun, and Mike Zheng Shou. 2024. Skinned Motion Retargeting with Dense Geometric Interaction Perception. Advances in Neural Information Processing Systems 37 (2024), 125907125934. Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. 2023. Physdiff: Physics-guided human motion diffusion model. In ICCV. 1601016021. Haodong Zhang, Zhike Chen, Haocheng Xu, Lei Hao, Xiaofei Wu, Songcen Xu, Zhensong Zhang, Yue Wang, and Rong Xiong. 2024b. Semantics-aware motion retargeting with vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 21552164. Jiaxu Zhang, Junwu Weng, Di Kang, Fang Zhao, Shaoli Huang, Xuefei Zhe, Linchao Bao, Ying Shan, Jue Wang, and Zhigang Tu. 2023b. Skinned motion retargeting with residual perception of motion semantics & geometry. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1386413872. Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. 2024a. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE TPAMI (2024). Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. 2023a. ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model. In ICCV. Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence 11 Yuhong Zhang, Guanlin Wu, Ling-Hao Chen, Zhuokai Zhao, Jing Lin, Xiaoke Jiang, Jiamin Wu, Zhuoheng Li, Hao Frank Yang, Haoqian Wang, et al. 2025. HumanMM: Global Human Motion Recovery from Multi-shot Videos. arXiv preprint arXiv:2503.07597 (2025). Qingqing Zhao, Peizhuo Li, Wang Yifan, Olga Sorkine-Hornung, and Gordon Wetzstein. 2024. Pose-to-Motion: Cross-Domain Motion Retargeting with Pose Prior. SCA. Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, and Lingjie Liu. 2024. Emdm: Efficient motion diffusion model for fast, high-quality motion generation. ECCV (2024). ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025. 12 Ling-Hao Chen, et al."
        },
        {
            "title": "B DESIRED BINDING RATE",
            "content": "Motivated by Sun et al. [2020], we design an automatic bone binding method for users. Let be the number of nodes in the tree and ğ¿ be the maximum path length. For each node, the algorithm traces upward for at most ğ¿ steps, resulting in time complexity of (N ğ¿). Each valid path contains at most ğ¿ nodes, and up to such paths can be collected, leading to space complexity of (N ğ¿). The algorithm is detailed in Algorithm 2. Before excusing the matching algorithm above, we calculate the unit direction vector of each bone for the source and target skeletons. For the source and target skeletons, given the same path length ğ¿, we construct two node sets for each (Cğ‘  and Cğ‘¡ ). We calculate the cosine similarity of two direction vectors from two sets pairwisely. Accordingly, we calculate all L-length subgraph similarities by the proposed similarity measurement. We return the pair with the highest similarity to the user as the binding bones, and feed them into the Motion2Motion. Algorithm 2: Trace ğ¿-length Paths of skeleton tree Input: Skeleton tree with node set , path length ğ¿. Output: set of valid chains with length ğ¿. Here, we raise research question: What is the best binding rate for bone correspondence? To answer this question, we ablate group of binding joint numbers. To simplify and align the setting, we set the binding mode to automatic. We calculate the bonding rate from 2% to 15%, and our default setting in the automatic binding mode is 6.1%. As can be seen in Fig. 13, different binding rates have varying effects on both similar skeletons and cross-species motion transfer. Our default setting of 6.1% achieves favorable balance between motion fidelity and anatomical plausibility. When the binding rate is too small, it is hard for the algorithm to find the coherence between two skeletons. If the rate is too large, it will introduce some mismatched correspondences, resulting in some inaccuracy. That is to say, sometimes less is more. Fortunately, our binding algorithm is interactive to choose the bindings, which is adjustable for users. 1 Function TraceUp(node): path {node} 2 current node while path < ğ¿ and has_parent(current) do 4 parent get_parent(current) path.append(parent) current parent 5 6 7 9 10 16 17 end if path = ğ¿ then return path ; end 11 12 return { } 13 { } ; 14 foreach node do 15 path = TraceUp(node) if path > 0 then (a) (A) Similar skeleton motion transfer. // find path // initialize C.append(path) ; // append the path end 18 19 end 20 return (b) (B) Cross-species skeleton motion transfer. Fig. 13. Comparison of different binding rates. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025. Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence"
        },
        {
            "title": "C USER STUDY TEMPLATE",
            "content": "We provide the template of the user study cases in Fig. 14. The participants are asked to watch the source motion (top-left) and evaluate three retargeted results (A/B/C) based on motion quality and coherence. Each result is scored on 5-point Likert scale, where 5 indicates the highest quality. This setup allows for consistent and fair comparison of different transfer methods. Fig. 14. User study template. ACM Trans. Graph., Vol. 1, No. 1, Article . Publication date: August 2025."
        }
    ],
    "affiliations": [
        "ByteDance, United States of America",
        "International Digital Economy Academy, China",
        "Shanghai Artificial Intelligence Laboratory, China",
        "The Hong Kong University of Science and Technology, China",
        "The University of Hong Kong, China",
        "Tsinghua University, China",
        "Tsinghua University, International Digital Economy Academy, China"
    ]
}