{
    "paper_title": "ScanBot: Towards Intelligent Surface Scanning in Embodied Robotic Systems",
    "authors": [
        "Zhiling Chen",
        "Yang Zhang",
        "Fardin Jalil Piran",
        "Qianyu Zhou",
        "Jiong Tang",
        "Farhad Imani"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce ScanBot, a novel dataset designed for instruction-conditioned, high-precision surface scanning in robotic systems. In contrast to existing robot learning datasets that focus on coarse tasks such as grasping, navigation, or dialogue, ScanBot targets the high-precision demands of industrial laser scanning, where sub-millimeter path continuity and parameter stability are critical. The dataset covers laser scanning trajectories executed by a robot across 12 diverse objects and 6 task types, including full-surface scans, geometry-focused regions, spatially referenced parts, functionally relevant structures, defect inspection, and comparative analysis. Each scan is guided by natural language instructions and paired with synchronized RGB, depth, and laser profiles, as well as robot pose and joint states. Despite recent progress, existing vision-language action (VLA) models still fail to generate stable scanning trajectories under fine-grained instructions and real-world precision demands. To investigate this limitation, we benchmark a range of multimodal large language models (MLLMs) across the full perception-planning-execution loop, revealing persistent challenges in instruction-following under realistic constraints."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 5 9 2 7 1 . 5 0 5 2 : r ScanBot: Towards Intelligent Surface Scanning in"
        },
        {
            "title": "Embodied Robotic Systems",
            "content": "Zhiling Chen, Yang Zhang, Fardin Jalil Piran, Qianyu Zhou, Jiong Tang, Farhad Imani University of Connecticut Project Page Figure 1: Overview and motivation behind the ScanBot dataset. (a) Embodied AI must generalize not only across tasks and environments, but also across tools, each with distinct control and perception demands. (b) Gripper tasks involve discrete object interaction, while scanner tasks require precise region localization and smooth, continuous motion. (c) Traditional laser scanning follows fixed, task-agnostic paths, leading to inefficient coverage and wasted time on irrelevant areas. (d) ScanBot includes 6 real-world components and 6 3D-printed shapes, enabling 6 task types and 4 evaluation capabilities for instruction-conditioned surface scanning."
        },
        {
            "title": "Abstract",
            "content": "We introduce ScanBot, novel dataset designed for instruction-conditioned, highprecision surface scanning in robotic systems. In contrast to existing robot learning datasets that focus on coarse tasks such as grasping, navigation, or dialogue, ScanBot targets the high-precision demands of industrial laser scanning, where submillimeter path continuity and parameter stability are critical. The dataset covers laser scanning trajectories executed by robot across 12 diverse objects and 6 task types, including full-surface scans, geometry-focused regions, spatially referenced parts, functionally relevant structures, defect inspection, and comparative analysis. Each scan is guided by natural language instructions and paired with synchronized RGB, depth, and laser profiles, as well as robot pose and joint states. Despite recent progress, existing vision-language action (VLA) models still fail to generate stable scanning trajectories under fine-grained instructions and real-world precision demands. To investigate this limitation, we benchmark range of multimodal large Corresponding author: farhad.imani@uconn.edu language models (MLLMs) across the full perceptionplanningexecution loop, revealing persistent challenges in instruction-following under realistic constraints."
        },
        {
            "title": "Introduction",
            "content": "Recently, embodied-AI systems have demonstrated remarkable progress on tasks such as navigation, pick-and-place, and natural-language manipulation [1, 2, 3, 4]. Yet general-purpose robots encounter tools that impose motion-control requirements far stricter than those of simple parallel gripper. paint sprayer must hold 2 mm standoff and 50 mm s1 sweep-speed tolerance to avoid orangepeel artifacts; TIG-welding torch must track bead within 0.2 mm; and laser line-scanner, our focus, must keep trajectory jitter below 0.1 mm, the sensors depth resolution, as shown in Figure 1 (a). No public benchmark currently tests whether embodied models can satisfy such sub-millimeter, instruction-conditioned trajectories. The gap is felt most acutely in non-destructive evaluation of large-scale industrial components, including aircraft skins, turbine blades, and battery housings, where exhaustive full-surface scans scale quadratically with part size and waste sensor bandwidth on regions that rarely fail. For example, scanning 1 0.5 turbine blade at 0.1 mm resolution demands 50 million points and over 45 minutes with 1 kHz sensor, untenable for in-line quality control. Engineers instead seek target-aware scanning, inspect the leading-edge weld, where the robot localizes the phrase to mesh region and adapts standoff, speed, and exposure on the fly. Mounting scanner inside gripper amplifies vibration and occlusion; rigid flange mounting eliminates these artifacts but transforms brief grasp into thousands of continuous waypoints whose geometry, timing, and illumination each influence data quality [5]. Figure 1 (b) illustrates this shift through four representative steps performed by robot using either gripper or scanner. In Step (1), both systems begin from an initial state by recognizing the object of interest. However, the grippers target is cube, while the scanner must treat the object as surface to be profiled. In Step (2), the gripper locates the cube and identifies stable grasp pose based on geometry, whereas the scanner must interpret natural-language instruction (e.g., scan the fan), adjust scanning parameters according to object appearance and environmental conditions, and ground it to specific region. In Step (3), the gripper performs short motion to grasp the object, but the scanner must move precisely to well-aligned start point, maintaining appropriate height and orientation to avoid occlusion and measurement artifacts. Finally, in Step (4), the gripper completes the task with discrete place action, while the scanner executes smooth and continuous motion toward the end point. Unlike grasping, the success of scanning is judged not by task completion, but by the quality and completeness of the resulting 3D surface reconstruction. Both are highly sensitive to motion jitter or misalignment. Existing embodied datasets and models fall short. They either (1) assume static scanner observing moving part [6, 7], (2) restrict data to sparse point clouds without joint-state supervision, or (3) confine evaluation to successfail task completion. Consequently, popular vision language action (VLA) models [1, 2, 8, 9, 10, 11, 12] that excel at grasping or navigation offer little insight into their capacity for precision scanning. These models reason over bounding boxes and discrete actions rather than the centimeter-scale pose curves required for continuous scanning. This fundamental shift in action dynamics also reveals the limitations of how scanning is performed in real-world industrial settings. As illustrated in Figure 1 (c), traditional laser scanning is typically executed in one of two ways: either with static scanner setup scanning objects as they pass on conveyor [6], or using robot arm that follows hard-coded trajectories across the entire surface [7]. While methods may suffice for simple, uniform parts, they become inefficient and impractical in realistic inspection scenarios. When dealing with large components, exhaustive surface coverage becomes prohibitively time-consuming. More critically, defects often appear in specific regions, such as connectors, weld seams, or edgesrendering full-surface scans wasteful. Furthermore, defects (e.g., micro-cracks or delaminations) are only detectable under fine-tuned conditions like slower scan speeds or higher exposure settings, which cannot be uniformly applied across an entire part. Motivated by these practical challenges, we introduce ScanBot, the first instruction-conditioned multimodal dataset designed explicitly for high-precision surface scanning tasks, as shown in Figure 1 (d). ScanBot consists of twelve objects, including both real-world electronic components and analytically structured 3D-printed shapes, each annotated with multiple task instructions and 2 Table 1: Comparison of robot learning datasets. ScanBot is the only dataset using laser scanner instead of gripper, enabling structured language-guided scanning. Dataset Year Tasks Collection Lang. Instruct. Public End-effector Tool RoboTurk [17] MIME [18] RoboNet [19] BridgeData [20] BC-Z [21] RT-1 [22] MT-Opt [23] RoboSet [24] RH20T [25] BridgeData V2 [3] DROID [26] Open X-Embodiment [4] RobotMind [27] ScanBot (Ours) 2018 2018 2019 2021 2022 2022 2022 2023 2023 2024 2024 2024 2024 2025 3 20 n/a 71 100 700 12 38 140 n/a n/a 160k 197 human human scripted human human human scripted & learned 29% human, 71% scripted human 84% human, 16% scripted human dataset aggregation human scripted gripper gripper gripper gripper gripper gripper gripper gripper gripper gripper gripper gripper gripper + dexterous hands laser scanner corresponding high-resolution scanning trajectories. We define six representative scanning tasks that range from broad surface coverage to fine-grained defect inspection and comparative region analysis. Each task includes synchronized multimodal observationsfirst-person RGB-D imagery, third-person overview video, laser height profiles, robot pose and joint data, and detailed sensor parameter configurationsproviding comprehensive supervision for model training and evaluation. To systematically evaluate model performance on the ScanBot dataset, we design set of challenging experiments targeting four key capabilities: (1) predicting scanner parameters based on object appearance, (2) grounding natural-language instructions to precise object features, (3) generating target-aware scan trajectories, and (4) reconstructing accurate surface geometry from acquired data. However, current VLA models are primarily designed for coarse-grained tasks such as grasping, navigation, or tool-free manipulation. When applied to scanner-based tasks, these models often struggle with unstable joint-level control and waypoint jitter, resulting in poor path continuity and degraded scan quality, making them ill-suited for surface scanning, where even small deviations can compromise the outcome. To assess how well existing models generalize to this new setting, we evaluate four state-of-the-art multimodal large language models (MLLMs): GPT-4.1 [13], OpenAI o3 [14], Gemini 2.5 Pro [15], and Gemini 2.5 Flash [16]. Our experiments test whether these models can overcome these challenges and achieve the precise spatial reasoning and motion control demanded by realistic industrial scanning scenarios."
        },
        {
            "title": "2 Background and Related Work",
            "content": "2.1 Robot Learning Datasets The development of general-purpose robotic policies depends critically on the availability of largescale, diverse robot learning datasets [17, 18, 19, 20, 21, 22, 23, 24, 25]. Inspired by successful pretraining in vision and language domains [28, 29, 30, 31, 32], recent efforts have emphasized the importance of heterogeneous datasets spanning diverse tasks, objects, and robot embodiments. Existing large-scale robotic datasets such as Open X-Embodiment [4] and RoboMIND [27] have made significant strides in this direction. Open X-Embodiment aggregates over one million trajectories collected from 22 robot embodiments, enabling cross-embodiment generalization. RoboMIND establishes unified teleoperation pipeline, collecting over 107,000 trajectories across multiple robotic platforms, including single-arm, dual-arm, and humanoid robots. These datasets collectively mark transition from narrowly-scoped, lab-specific collections toward community-driven repositories for robotic learning. However, as summarized in Table 1, the vast majority of existing datasets focus exclusively on manipulation-centric tasks with end-effectors such as grippers or dexterous hands. Even as these datasets grow in size and diversity, they largely ignore perception-driven tasks involving non-manipulative tools. To address this underexplored area and move beyond conventional manipulation settings, our work introduces dataset centered on active perception with robot-mounted 3D laser scanner. Unlike manipulation tasks that optimize physical interactions, sensor-guided tasks prioritize precise positioning and orientation to maximize sensor coverage and measurement quality. This distinction fundamentally alters the embodiment-task interface, as the 3 robots primary goal becomes coverage-aware motion planning rather than object manipulation. By introducing robot-guided active scanning trajectories, pose-aligned surface coverage metrics, and task-specific scan directives, our dataset complements existing manipulation-focused datasets. We hope this direction broadens the scope of robot learningfrom what to do to what to sense, and from manipulation tools to sensors as active perception tools. 2.2 Laser Scanner Laser scanners have become indispensable in industrial inspection, offering high-resolution 2D and 3D surface measurements for tasks such as defect detection, geometric tolerance verification, and reverse engineering. Traditional 2D laser systems are widely used for fast inline inspection but cannot recover depth or out-of-plane geometry. As result, many industries have transitioned to 3D laser profilers, which aggregate dense 2D scan lines into 3D surfaces to enable comprehensive shape analysis. These systems are deployed extensively across electronics, automotive, and semiconductor manufacturing to support 100% quality assurance where conventional vision systems fall short. To automate such workflows, laser scanners are increasingly integrated into robotic systems. In traditional deployments, robot arm is programmed to follow fixed trajectories to scan known parts. However, this approach relies heavily on manual programming and is difficult to scale to new geometries. Recent research has begun to address this by developing automated methods for viewpoint planning and trajectory generation tailored to line-based scanners. For example, coverage path planning (CPP) algorithms have been proposed to decompose complex surfaces into scannable subregions and generate efficient trajectories under sensor constraints[33]. Some works apply region segmentation and global optimization (e.g., PSO) to minimize redundant motion while ensuring full coverage[34]. More recently, learning-based methods have been introduced to improve scanner control. reinforcement learning framework has been proposed that dynamically adjusts the scanners tilt and position based on surface geometry and sensor feedback, improving coverage and data quality over static or scripted scans [35]. However, as these approaches remain domain-specific and are often validated only in isolated setups, the broader space of scanner-based active perception remains underexplored. Most existing datasets and benchmarks in robotic learning focus on manipulation tasks, where grippers or dexterous hands act on the environment. In contrast, robotic tasks where the sensor itself is the actuator still lack structured evaluation environments. ScanBot addresses this gap by introducing the first instruction-conditioned, multimodal dataset centered on high-resolution surface scanning. Unlike prior works that treat laser scanners as passive data sources, ScanBot frames the scanner as an active agent that must interpret natural-language instructions, localize target regions, select appropriate sensor parameters, and execute smooth scanning trajectories."
        },
        {
            "title": "3 ScanBot Dataset",
            "content": "Our goal is to develop dataset that enables research on tool-specific embodied perception beyond traditional manipulation. The dataset should support generalization across diverse scan tasks, object geometries, and tool configurations. It should also facilitate instruction-conditioned policy learning, where the robot interprets high-level language commands to perform surface scanning under realworld constraints. In this section, we describe the design of ScanBot, including our data collection setup, task structure, and dataset composition. 3.1 Hardware Setup and System Overview As shown in Figure 2, the ScanBot system integrates 6-DOF UR3 collaborative robotic arm with multi-sensor payload designed for high-precision surface profiling. Mounted on the robots end-effector is Keyence LJ-X8200 2D/3D laser displacement sensor [36]. This model operates with 405 nm blue laser and captures 3200 data points per profile across an 80 mm field of view. It achieves sub-millimeter resolution, with 1 µm Z-axis repeatability and 3 µm X-axis precision, enabling accurate surface geometry reconstruction even for reflective or high-contrast materials. To complement the profiler with visual and depth information, an Intel RealSense D435i RGB-D camera is rigidly co-mounted next to the laser head. Additionally, GoPro HERO8 Black camera is positioned on fixed tripod to record third-person views of the scanning workspace. 4 Figure 2: Hardware setup of the ScanBot system. UR3 robotic arm is equipped with Keyence LJ-X8200 laser profiler and an Intel RealSense D435i RGB-D camera mounted on the end-effector. GoPro HERO8 captures third-person views from fixed tripod. The entire setup operates within black-curtained environment to ensure consistent and interference-free measurements. All components are deployed in controlled environment enclosed by matte black curtains. The workspace includes flat platform for object placement and is designed to reduce ambient light interference, ensuring consistent laser and RGB-D measurements. 3.2 Scanning Objects Categories To better align with practical applications of laser profilers in industrial inspection and robotic surface analysis, we construct dataset that includes both real-world components and analytically designed geometric parts, as shown in Figure 3. This object set enables controlled evaluation of surface profiling under varied material, texture, and geometric conditions. The dataset contains total of 12 objects: 6 real-world electronic parts and 6 3D-printed geometric primitives. The real-world set includes four GPU boards (with distinct form factors and cooling designs), one RAM module, and one WiFi card. These components feature diverse surface properties such as matte plastic covers, glossy heatsinks, exposed soldered circuits, and reflective metallic connectors, making them representative of common challenges in industrial surface profiling. The 3D-printed set is structured into three comparison groups. The first group contains two equilateral triangles in black and white, both with flat surfaces and no surface features. The second group includes two cubes with identical dimensions and color but distinct embossed and recessed patterns. The third group consists of two cylinders with identical geometric features but different surface colors. These parts are designed to isolate the effects of color, material, and geometry on sensor response, while maintaining known ground-truth shapes for reference evaluation. For detailed visualizations of the point clouds corresponding to each feature on each part, please refer to Appendix A.1. 3.3 Data Collection To reflect real-world industrial inspection scenarios, we design data collection protocol that supports multi-object, multi-task surface scanning. Each object in the dataset is associated with multiple scanning tasks, targeting different geometric features or surface regions. This design enables the evaluation of multi-task and instruction-driven learning methods under diverse conditions. Our setup integrates three sensor modalities. The GoPro HERO8 Black connects via WiFi to record third-person video, while the Intel RealSense D435i is linked over USB to capture RGB and RGB-D data. The Keyence LJ-X8200 profiler outputs live profile images through the LJ-X Navigator interface, 5 Figure 3: Overview of the 12 scanned objects in the ScanBot dataset. The top two rows show six real-world electronic components (four GPU boards, one RAM module, and one WiFi card) alongside their corresponding point clouds. The bottom two rows present six 3D-printed parts grouped into three comparison sets: (1) black and white triangles with no surface features, (2) two cubes with identical shape and color but different embossed patterns, and (3) two cylinders with identical features but different colors. Table 2: Task types in the ScanBot dataset. ID Type Target Region Objective T1 Surface Scan T2 Geometry Focus Spatial Reference T3 T4 Functional Target T5 Defect Inspection T6 Comparative Analysis Repetitive regions Compare similar areas for consistency or alignment Capture full object geometry Scan distinct geometric features Scan based on spatial position Focus on operational parts Identify surface defects or anomalies Entire surface Specific structure Relative location Functional part Flawed region Example \"Scan the full board.\" \"Scan the small fan.\" \"Scan the right slot.\" \"Scan the port.\" \"Scan the scratches.\" \"Compare both fans.\" which we use to manually verify that the object is positioned within the sensors optimal working distance before each scan begins. Although initial scanning paths are pre-defined for each object and feature, we refine sensor and motion parameters through an iterative calibration process. For each trajectory, we adjust laser profiler settings, such as exposure time, control range, and laser power, based on the material properties and reflectivity of the target object. For detailed scanner settings used for each part, please refer to Appendix A.2. In parallel, we tune the robots motion speed and acceleration to match the profilers scan frequency. This alignment is critical: insufficient synchronization can cause undersampling or motion blur in the profile output, while proper tuning ensures dense, temporally aligned scans. Once optimal parameters are identifiedincluding scanning height, motion profile, and sensor configurationwe proceed with data recording. This process is repeated across all object-task combinations, yielding dataset that captures rich variations in surface geometry and supports fine-grained reasoning across tasks. For detailed information on the measurement geometry of our scanner at the reference distance and its vertical measurement range, as well as our procedure for determining the optimal scanner-to-object distance, please refer to Appendix A.3. 3.4 Dataset Composition To better align with the practical demands of industrial inspection, we define six types of scanning tasks that represent different levels of spatial, functional, and semantic reasoning, as shown in Table 2. Figure 4: Multiview examples and annotated features in the ScanBot dataset. The first column shows first-person views captured by the Intel RealSense D435i mounted on the robots end-effector. The second column presents third-person overviews recorded by fixed GoPro camera. The third column highlights annotated object features. These include: surface scan (T1), which captures the full geometry of an object; geometry focus (T2), which targets specific structural components such as fans or grooves; spatial reference (T3), which refers to scanning regions defined by relative positions such as the left or right side; functional target (T4), which focuses on semantically meaningful parts like ports or connectors; defect inspection (T5), which directs the scan toward flawed or damaged areas; and comparative analysis (T6), which involves comparing multiple regions for consistency or alignment. Figure 5 illustrates the distribution of the 896 total scanning paths across these six categories. Each task is paired with one or more natural language instructions and executed through corresponding motion trajectory. Each trajectory produces multimodal data sample composed of synchronized sensor streams and detailed metadata. The metadata records the natural language instruction, task ID, robot joint states, end-effector poses, scanner parameters such as exposure and scan range. Figure 4 shows example samples from the dataset. The first column presents first-person RGB views captured during scanning, the second column shows third-person views, and the third column highlights feature annotations on the scanned objects. See Appendix A.4 for additional examples."
        },
        {
            "title": "4 Experiments",
            "content": "To demonstrate the utility of ScanBot for embodied robot learning, we benchmark set of vision language and control models on instructionconditioned surface scanning. Existing VLA models are mainly designed for coarse-grained robotics tasks. When applied to high-precision laser scanning they often suffer from waypoint jitter, irregular interpolation, and unstable parameter controlfailing to meet the strict industrial demands for path continuity and consistent scan quality. To quantify this gap, we evaluate several state-ofthe-art MLLMs, including GPT-4.1, OpenAI o3, Gemini 2.5 Pro, and Gemini 2.5 Flash, on the ScanBot dataset. Our evaluation spans the entire perFigure 5: Distribution of scanning tasks across six instruction types in the ScanBot dataset. The pie chart shows the number of tasks belonging to each task type, with values labeled inside each segment. 7 Table 3: Evaluation of model performance on selecting scanner parameters. Each column shows the accuracy (%) of predicting each parameter. The final column reports the average accuracy. Model Sampling Freq. (Hz) Range Center (%) CMOS Range Exposure (µs) Light Intensity Range Average Accuracy (%) GPT-4.1 OpenAI o3 Gemini 2.5 Pro Gemini 2.5 Flash 50.0 50.0 91.7 50.0 75.0 33.3 41.7 75.0 0.0 16.7 0.0 25.0 0.0 0.0 8.33 0.0 33.3 25.0 25.0 33. 50.0 83.3 75.0 66.7 34.7 34.7 40.3 41.7 ception planning execution loop: the model first observes multiple sensor data from an unseen object, then interprets an instruction, plans laser-scanning trajectory, and finally reconstructs surface geometry from the acquired profiles. Our experiments are designed to answer the following questions: 1. Can the model pick scanner parameters (frequency, exposure, power, range) that give clear scan based on the visual appearance of the object? 2. Can the model locate the region or feature referenced by natural-language instruction? 3. Can the model generate scanning path that covers the target well without extra moves? 4. After scanning, does the recovered 3-D shape closely match the ground-truth surface? 4.1 Can the model pick scanner parameters (frequency, exposure, power, range) that give clear scan based on the visual appearance of the object? We begin by evaluating whether state-of-the-art MLLMs can select appropriate scanner parameters based on the visual appearance of the object. Accurate parameter configuration is essential in laserbased surface profiling, where small deviations can lead to noise, clipping, or signal loss. Given the sensitivity of the scanning process to material properties and geometric variations, this task serves as strong test of models ability to perform fine-grained physical reasoning. For each object, the model is provided with two images as visual input: first-person view and third-person view. Based on these inputs, the model must predict six discrete configuration parameters for the laser profiler: sampling frequency, range, center, CMOS dynamic range, exposure time, and light intensity range. We evaluate model predictions against ground-truth values determined by expert users during data collection. prediction is marked correct only when it exactly matches the true setting. As shown in Table 3, Gemini 2.5 Flash achieves the highest overall accuracy at 41.7%, followed by Gemini 2.5 Pro at 40.3%. GPT-4.1 and OpenAI o3 perform similarly, both reaching 34.7% accuracy. Prediction accuracy varies significantly across parameters. Sampling frequency and light intensity range are predicted with relatively high consistency, showing that models can pick up on general object size and brightness. In contrast, center and CMOS range are especially difficult, with average accuracies below 25%. These two parameters require understanding fine-scale depth cues and reflectance variationtasks that go beyond general scene recognition. For example, predicting range and center involves estimating the vertical distance between the scanner and the object surface, which depends on both object geometry and sensor mounting. Exposure time and light intensity range must be adapted to the objects surface materials. CMOS dynamic range requires reasoning about reflectivity contrast across the entire field of view. These results highlight the current limitations of MLLMs in high-precision, vision-based parameter tuning. While the models show some ability to generalize from object appearance, they still fall short of the robustness and reliability required for industrial scanning tasks. 4.2 Can the model locate the region or feature referenced by natural-language instruction? To evaluate whether models can accurately generate scan trajectories, we must first assess their ability to identify the correct target region described by natural-language instruction. Region localization is prerequisite for instruction-conditioned scanning, particularly in tasks that require fine-grained focus on object subparts. In this evaluation, the model is provided with single first-person image of the object and task instruction, and is asked to predict 2D bounding box that localizes the intended scanning area. We then compare the predicted bounding box with human-annotated ground truth using the Intersection-over-Union (IoU) metric. To better understand performance across different types of spatial reasoning, we compute the mean IoU separately for each of the six level task categories. As shown in Table 4, overall localization performance remains low. OpenAI o3 achieves the highest average IoU at 0.129, followed by GPT-4.1 at 0.073. Both Gemini 2.5 Pro and Gemini Table 4: Mean IoU across different task types (T1T6) for each model Model T1 T2 T3 T5 T6 Avg. GPT-4.1 0.310 0.041 0.041 0.058 0.093 0.034 0.073 OpenAI o3 0.477 0.084 0.077 0.141 0.172 0.039 0.129 0.022 0.000 0.000 0.000 0.007 0.000 0.003 Gemini 2.5 Pro Gemini 2.5 Flash 0.064 0.013 0.000 0.010 0.031 0.011 0."
        },
        {
            "title": "2.5 Flash perform poorly, with average IoUs below 0.02. The results show that models perform best\non T1 (full-object scan), where instructions map to the entire object and are less ambiguous. For\ntasks involving fine-grained features—such as T2 (geometry focus), T3 (spatial reference), and T4\n(functional targets)—performance drops sharply, with IoUs often approaching zero. Based on these\nresults, we found that existing MLLMs struggle to resolve spatial references or semantic attributes.\nWhile they may understand general scene content, precise instruction grounding remains a major\nbottleneck for instruction-conditioned robotic scanning.",
            "content": "4.3 Can the model generate an efficient scanning path that adequately covers the target region and leads to an accurate 3D surface reconstruction? Scanning path selection is critical step that turns language and spatial understanding into precise robot actions and accurate scan execution. Because our current tasks involve flat-surface profiling, we simplify trajectory prediction by requiring the model to output only the start and end end-effector poses. We evaluate this capability on two levels: (i) the Euclidean deviation of the predicted start and end points from expert ground truth and (ii) the Chamfer distance between the point cloud produced by executing the predicted path and the reference scan. To test the influence of sensory context and prompting, we evaluate MLLMs with several input combinations: first-person RGB alone; first-person RGB plus depth; first-person RGB, depth and third-person RGB. Across all models and input regimes, the predicted way-points consistently fall off the object footprint, leaving the laser to sweep background and yielding point clouds with maximal reconstruction error. These results illustrate that current MLLMs, even when given additional depth cues or an external view, lack the millimetre-scale spatial grounding needed to convert free-form instructions into viable scanning trajectories."
        },
        {
            "title": "5 Limitations and Future Work",
            "content": "While ScanBot provides structured benchmark for instruction-conditioned surface scanning, it has several limitations. First, all trajectories assume flat surfaces, limiting its applicability to objects with curved or irregular geometries. Second, the scanning process is open-loop: once the trajectory and parameters are predicted, the system does not adapt based on scan quality or feedback during execution. Third, the framework assumes one-pass scanning; however, in practical scenarios, some regions may require multiple scans under different angles or settings to achieve sufficient coverage or resolution. Future work will address these limitations by extending ScanBot to support curved surface scanning, where the robot must continuously adjust pose and orientation to follow nonplanar geometry. We also aim to enable multi-pass scanning, allowing repeated surface coverage for challenging regions. Finally, while this work focuses on laser scanners, the broader paradigm of instruction-guided continuous control applies to other tools such as welders, sprayers, and polishers. We plan to expand ScanBot toward multi-tool benchmark to further explore tool-conditioned perception and control."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced ScanBot, the first instruction-conditioned, multimodal robotic dataset designed explicitly for precision surface scanning tasks. By highlighting fundamental differences between scanner-based tasks and traditional gripper-based manipulation, we identified critical gaps in existing VLA models, particularly in stable trajectory planning, fine-grained region grounding, and sensor parameter adaptation. Our experiments using state-of-the-art MLLMs demonstrated that current approaches significantly underperform in the continuous and high-precision control 9 scenarios required by industrial surface scanning. By addressing these challenges, ScanBot opens new avenues for embodied AI research, emphasizing tool-level generalization, adaptive control, and precise multimodal understanding for realistic industrial applications."
        },
        {
            "title": "References",
            "content": "[1] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [2] Jiaming Liu, Mengzhen Liu, Zhenyu Wang, Pengju An, Xiaoqi Li, Kaichen Zhou, Senqiao Yang, Renrui Zhang, Yandong Guo, and Shanghang Zhang. Robomamba: Efficient visionlanguage-action model for robotic reasoning and manipulation. Advances in Neural Information Processing Systems, 37:4008540110, 2024. [3] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe HansenEstruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pages 17231736. PMLR, 2023. [4] Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open xembodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 68926903. IEEE, 2024. [5] Toufik Al Khawli, Muddasar Anwar, Dongming Gan, and Shafiqul Islam. Integrating laser profile sensor to an industrial robotic arm for improving quality inspection in manufacturing processes. Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science, 235(1):417, 2021. [6] Gordon Petrie and Charles Toth. Introduction to laser ranging, profiling, and scanning. In Topographic laser ranging and scanning, pages 128. CRC Press, 2018. [7] Jing Xu, Jian Li Hoo, Stylianos Dritsas, and Javier Gomez Fernandez. Hand-eye calibration for 2d laser profile scanners using straight edges of common objects. Robotics and ComputerIntegrated Manufacturing, 73:102221, 2022. [8] Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. Univla: Learning to act anywhere with task-centric latent actions, 2025. [9] Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Heming Cui, Zhizheng Zhang, and He Wang. Graspvla: grasping foundation model pre-trained on billion-scale synthetic action data, 2025. [10] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. π0.5: vision-language-action model with open-world generalization, 2025. [11] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, and Tsung-Yi Lin. Cot-vla: Visual chain-of-thought reasoning for vision-languageaction models, 2025. [12] Chengmeng Li, Junjie Wen, Yan Peng, Yaxin Peng, Feifei Feng, and Yichen Zhu. Pointvla: Injecting the 3d world into vision-language-action models, 2025. [13] OpenAI. GPT-4.1. https://openai.com/index/gpt-4-1/, 2025. Accessed: 2025-05-13. [14] OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, 2025. Accessed: 2025-05-13. [15] Google. Gemini 2.5 pro preview model card. Technical report, Google, 2025. Accessed: 2025-05-13. 10 [16] Google. Gemini 2.5 flash preview model card. Technical report, Google, 2025. Accessed: 2025-05-13. [17] Ajay Mandlekar, Yuke Zhu, Animesh Garg, Jonathan Booher, Max Spero, Albert Tung, Julian Gao, John Emmons, Anchit Gupta, Emre Orbay, et al. Roboturk: crowdsourcing platform for robotic skill learning through imitation. In Conference on Robot Learning, pages 879893. PMLR, 2018. [18] Pratyusha Sharma, Lekha Mohan, Lerrel Pinto, and Abhinav Gupta. Multiple interactions made easy (mime): Large scale demonstrations data for imitation. In Conference on robot learning, pages 906915. PMLR, 2018. [19] Sudeep Dasari, Frederik Ebert, Stephen Tian, Suraj Nair, Bernadette Bucher, Karl Schmeckpeper, Siddharth Singh, Sergey Levine, and Chelsea Finn. Robonet: Large-scale multi-robot learning. arXiv preprint arXiv:1910.11215, 2019. [20] Frederik Ebert, Yanlai Yang, Karl Schmeckpeper, Bernadette Bucher, Georgios Georgakis, Kostas Daniilidis, Chelsea Finn, and Sergey Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets. arXiv preprint arXiv:2109.13396, 2021. [21] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, and Chelsea Finn. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pages 9911002. PMLR, 2022. [22] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. [23] Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman. Mt-opt: Continuous multi-task robotic reinforcement learning at scale. arXiv preprint arXiv:2104.08212, 2021. [24] Vikash Kumar, Rutav Shah, Gaoyue Zhou, Vincent Moens, Vittorio Caggiano, Abhishek Gupta, and Aravind Rajeswaran. Robohive: unified framework for robot learning. Advances in Neural Information Processing Systems, 36:4432344340, 2023. [25] Hao-Shu Fang, Hongjie Fang, Zhenyu Tang, Jirong Liu, Chenxi Wang, Junbo Wang, Haoyi Zhu, and Cewu Lu. Rh20t: comprehensive robotic dataset for learning diverse skills in one-shot. arXiv preprint arXiv:2307.00595, 2023. [26] Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. [27] Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, et al. Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation. arXiv preprint arXiv:2412.13877, 2024. [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [29] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. [30] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features, 2025. [31] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers, 2021. [32] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, 11 Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. [33] Ondˇrej Vaníˇcek, Michal Chaluš, and Jindˇrich Liška. Coverage path planning for robotic laser surfacing applications based on 3d scanning. In 2024 21st International Conference on Mechatronics-Mechatronika (ME), pages 16. IEEE, 2024. [34] Hongpeng Chen, Shengzeng Huo, Muhammad Muddassir, Hoi-Yin Lee, Yuli Liu, Junxi Li, Anqing Duan, Pai Zheng, and David Navarro-Alarcon. Pso-based optimal coverage path planning for surface defect inspection of 3c components with robotic line scanner. IEEE Transactions on Instrumentation and Measurement, 2025. [35] Sara Roos-Hoefgeest, Mario Roos-Hoefgeest, Ignacio Álvarez, and Rafael González. Reinforcement learning approach to optimizing profilometric sensor trajectories for surface inspection. Sensors (Basel, Switzerland), 25(7):2271, 2025. [36] Keyence Corporation. LJ-X8200 High-Speed 2D/3D Laser Profiler. https://www.keyence. com/products/measure/laser-2d/lj-x8000/models/lj-x8200/, 2025. Accessed: May 5, 2025."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Feature Level Ground Truth Scan Visualizations To facilitate detailed inspection and evaluation of scan quality across different objects and features, we provide comprehensive visualization of the ground-truth scan results for each annotated feature in the ScanBot dataset. Figure 6 and Figure 7 present examples for both 3D-printed and real-world parts. Note that minor variations in scan results may occur across different scanning setups due to differences in sensor configuration, lighting conditions, or surface reflectivity. Figure 6: Feature-level ground-truth scans for 3D-printed objects. A.2 Scanner Configuration Details Accurate configuration of scanner parameters is essential for producing high-fidelity height profiles, particularly in tasks involving reflective, curved, or low-contrast surfaces. During ScanBot data collection, we manually tuned the settings of the Keyence LJ-X8200 2D/3D laser profiler to ensure optimal scan quality for each object. Table 5 summarizes the parameters used for all 12 objects in the dataset. For each object, we document six key settings: (1) scanning frequency (Hz), (2) Z-direction range mode (either 1/2 or 1/4), (3) the center of the measurement window along the axis, (4) CMOS dynamic range level, (5) laser exposure time (in µs), and (6) the light intensity control range. We observe that real-world electronic components such as GPUs, RAM, and WiFi cards tend to use consistent configuration: 1kHz sampling frequency, 1/2 direction, 26.3% measurement center, and control range between 7085. In contrast, 3D-printed shapes (e.g., cubes, triangles, cylinders) require more variation in center and exposure settings due to differences in height and surface reflectivity. For example, Cube 1 is scanned using 1/4 Z-range and 43.5% center, while reflective parts such as the red cylinder require increased exposure time (240 µs) and CMOS gain adjustment to mitigate noise. 13 Figure 7: Feature-level ground-truth scans for real-world electronic components. 14 Table 5: Scanner parameters used for each object during data collection. Parameters include sampling frequency, Z-direction mode, measurement center, CMOS dynamic range, exposure time (in µs), and control range (light intensity). Object Freq (Hz) Dir. Center (%) CMOS Range Exposure (µs) Control Range Cube 1 Cube 2 Triangle black Triangle white Cylinder white Cylinder red GPU red GPU blue GPU green 1 GPU green 2 RAM WiFi card 1k 1k 1k 1k 1k 1k 1k 1k 1k 1k 1k 1k 1/4 1/2 1/2 1/2 1/2 1/2 1/2 1/2 1/2 1/2 1/2 1/2 43.5% 45.0% 45.0% 45.0% 45.0% 45.0% 26.3% 26.3% 26.3% 26.3% 39.8% 26.3% 1 1 3 1 1 1 3 3 3 3 3 60 60 240 60 60 240 120 120 480 240 120 120 6075 6075 6075 6075 6075 6075 7085 7085 7085 7085 7085 7085 A.3 Height Control During Data Collection To ensure high-quality surface scans, it is critical that the scanner operates within its calibrated measurement range and that the target region remains centered in the field of view (FOV). During ScanBot data collection, we carefully calibrated the vertical position of the Keyence LJ-X8200 scanner relative to each object before recording any trajectory. The LJ-X8200s reference distance is 245mm, with measurement range of 68mm (i.e., approximately 34mm). At this height, the scanners field of view spans 72mm in width. As illustrated in Figure 8, if the scanner is positioned too high or too low, the laser line projected onto the object surface may shift out of range, causing missing or clipped scan profiles. Therefore, we manually adjusted the robots Z-axis to ensure that the scanned surface lay within the optimal range. Figure 9 demonstrates how we verified scanner alignment using the live profile display in Keyences configuration software. When the scanner is positioned correctly (Figure 9 (a)), the surface profile appears centered in the preview window, indicating that the surface lies near the reference distance. If the scanner is too far from the surface (Figure 9 (b)), the profile drops toward the bottom of the window and becomes truncated. Conversely, when the scanner is too close (Figure 9 (c)), the profile shifts upward and may exceed the capture range. These live previews enabled precise alignment of the scanners vertical offset prior to each scan. Once the optimal distance was confirmed, we fixed the Z-height for that object and recorded all scanning trajectories under consistent configuration settings. Figure 8: Measurement geometry of the Keyence LJ-X8200 scanner. At reference distance of 245 mm, the scanner captures 72 mm-wide region with 68 mm vertical measurement range. 15 Figure 9: Scanner profile previews used for height calibration. (a) Profile centered at reference height. (b) Profile drops low when the scanner is too far. (c) Profile shifts upward when too close. A.4 Example of Scanning Process Figure 10 illustrates complete instruction-conditioned scanning process for the task Scan the top surface of the white cube. The figure presents both the first-person view (from the scanner-mounted camera) and the third-person view (from an external overview camera), offering step-by-step visualization of the perceptionplanningexecution loop. Upon receiving the instruction, the model should first interpret the language command and visually recognize the target object. Based on the objects appearance, the model is expected to select appropriate scanner parameters, including sampling frequency, exposure time, Z-range, and laser power. Simultaneously, the robot must compute and execute precise motion plan that positions the scanner at an appropriate height and orientation for surface profiling. This includes adjusting the Z-axis clearance to maintain optimal laser focus and avoiding occlusion or clipping during scanning. Figure 10: Example of an instruction-conditioned scanning trajectory. The task Scan the top surface of the white cube is executed using first-person (top row) and third-person (bottom row) camera views, showing each stage of the scanning process. A.5 Failure Cases from Improper Settings Despite careful motion planning and object alignment, inappropriate scanner or robot control parameters can still result in severe degradation of surface scan quality. To illustrate the importance of correct configuration, we present series of failure cases in Figure 11. Under optimal conditions (Figure 11 (a)), the resulting point cloud shows clean geometry and high surface fidelity. However, when the robot moves too slowly (Figure 11 (b)), the accumulated scan data becomes overly dense, causing vertical stretching and geometric artifacts. Conversely, overly fast motion (Figure 11 (c)) leads to sparse and incomplete coverage due to insufficient scan-line density. Exposure settings are particularly critical for material-sensitive surfaces. As shown in Figure 11 (d), short exposure time results in missing geometryespecially on darker or less reflective areaswhile an excessively long exposure (Figure 11 (e)) causes oversaturation and loss of detail, producing washed-out appearance. Finally, using low scanning frequency (Figure 11 (f)) leads to extreme undersampling. The resulting 16 Figure 11: Examples of failed surface reconstructions caused by inappropriate configurations. (a) clean, high-fidelity result under optimal settings. (b) Low robot speed causes vertical stretching and noise accumulation. (c) High robot speed results in sparse, incomplete coverage. (d) Short exposure time leads to missing geometry, particularly on darker surfaces. (e) Excessively long exposure creates oversaturation and color washout. (f) Low scanning frequency (10 Hz) produces distorted, flattened results due to motion undersampling. point cloud becomes flattened and distorted, as too few scan lines are captured to reconstruct meaningful geometry. These examples underscore the necessity of task-specific scanner calibration and the importance of parameter-aware learning strategies in instruction-conditioned surface profiling."
        }
    ],
    "affiliations": [
        "University of Connecticut"
    ]
}