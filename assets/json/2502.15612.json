{
    "paper_title": "LaTIM: Measuring Latent Token-to-Token Interactions in Mamba Models",
    "authors": [
        "Hugo Pitorro",
        "Marcos Treviso"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. While recent efforts provide insights into Mamba's internal mechanisms, they do not explicitly decompose token-wise contributions, leaving gaps in understanding how Mamba selectively processes sequences across layers. In this work, we introduce LaTIM, a novel token-level decomposition method for both Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively evaluate our method across diverse tasks, including machine translation, copying, and retrieval-based generation, demonstrating its effectiveness in revealing Mamba's token-to-token interaction patterns."
        },
        {
            "title": "Start",
            "content": "LATIM: Measuring Latent Token-to-Token Interactions in Mamba Models Hugo Pitorro, Marcos Treviso Instituto de Telecomunicações, Lisbon hugo.pitorro@gmail.com 5 2 0 2 4 2 ] . [ 2 2 1 6 5 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "State space models (SSMs), such as Mamba, have emerged as an efficient alternative to transformers for long-context sequence modeling. However, despite their growing adoption, SSMs lack the interpretability tools that have been crucial for understanding and improving attention-based architectures. While recent efforts provide insights into Mambas internal mechanisms, they do not explicitly decompose token-wise contributions, leaving gaps in understanding how Mamba selectively processes sequences across layers. In this work, we introduce LATIM, novel token-level decomposition method for both Mamba-1 and Mamba-2 that enables fine-grained interpretability. We extensively evaluate our method across diverse tasks, including machine translation, copying, and retrieval-based generation, demonstrating its effectiveness in revealing Mambas token-totoken interaction patterns. Our code is available at https://github.com/deep-spin/latim."
        },
        {
            "title": "Introduction",
            "content": "State space models (SSMs), such as S4 (Gu et al., 2022), have emerged as promising alternative to transformers for long-context modeling. Unlike transformers (Vaswani et al., 2017), which explicitly compute pairwise token interactions and require quadratic memory, SSMs leverage structured recurrence mechanisms that enable more efficient sequence processing. Among them, the Mamba architecture (Gu and Dao, 2023; Dao and Gu, 2024) has demonstrated strong performance in language modeling and other modalities while significantly reducing runtime and memory requirements (Xu et al., 2024). Additionally, hybrid architectures that integrate both Mamba and attention mechanisms often outperform purely homogeneous models by combining the efficiency of recurrence with the expressivity of attention (Lenz et al., 2025; Dong et al., 2025; Pitorro et al., 2024). While these findings highlight the relevance of Mamba models, 1 their internal decision-making processes remain opaque, hindering their reliability. Interpretability techniques have played key role in the widespread adoption of transformers, enabling researchers to analyze token interactions and information flow (Mohebbi et al., 2024; Ferrando et al., 2024). However, in contrast to transformers, where attention scores offer direct visualization of how the model distributes importance across tokens, Mamba lacks an explicit mechanism to reveal where it is attending at each step. Existing interpretability efforts for Mamba attempt to bridge this gap by reformulating its computations into attention-like representations. For instance, MambaAttention (Ali et al., 2024) reformulates the models computation in terms of implicit attention matrices, while MambaLRP (Jafari et al., 2024) uses layer-wise propagation analysis to track gradient flow. However, these methods do not explicitly decompose contributions into fine-grained elements across layers, leaving gaps in understanding how Mamba selectively processes sequences. In this work, we bridge this gap by introducing LATIM, novel token-level decomposition method for both Mamba-1 and Mamba-2. Our approach reformulates the SSM computation to enable token-by-token analysis, allowing us to adapt attention-based interpretability techniques, such as ALTI (Ferrando et al., 2022), to the Mamba architecture. We extensively evaluate our method across diverse tasks, including the copying task (Jelassi et al., 2024) in 4.1, which features well-defined diagonal attention pattern; machine translation in 4.2, where precise sourcetarget alignment is essential; and retrieval-based generation (Hsieh et al., 2024) in 4.3, where ground-truth context allows direct evaluation of token importance. Our method not only improves Mambas interpretability but also defines robust framework for analyzing token interactions in SSMs, paving the way for more transparent models."
        },
        {
            "title": "2 Background",
            "content": "2.1 Transformers key component in the transformer architecture is the attention mechanism, which is responsible for mixing input sequences = x1, ..., xN , where each xi RD. Concretely, given query Qh = RN RN XW , and value = XW matrices as input, where 1 is the head dimension, the multi-head attention mechanism is defined as follows (Vaswani et al., 2017): (cid:33) , key Kh = XW RN (cid:32) Attn(X)h = π RN QhKh (cid:124) (cid:123)(cid:122) AhRN (cid:125) where the transformed contribution of xj to yi is Ti(xj) = (cid:88) h=1 Ah i,jW LN(xj)+δi,jxi, (4) with δi,j denoting the Kronecker delta. Token-to-Token Importance. Using this decomposition, we can obtain token-to-token importance scores via vector norms (Kobayashi et al., 2021): Ci,j = Ti(xj)2 , (5) , (1) or via ALTIs contextual mixing approach (Ferrando et al., 2022): where π maps rows to distributions, with π := softmax being common choice. Ci,j = [yi1 yi Ti(xj)1]+ [yi1 yi Ti(xk)1]+ (cid:80) , (6) Transformer block. The attention is combined with other modules in order to form transformer block. The full block, with pre LayerNorm (LN, Ba et al. 2016), can be described as follows: RN D, ), RN D, RN D, Xl = LN(X) Ya = Concat(Attn(Xl)hW = Ya + (2) where we denote Concat() as the concatenation of all heads 1 H, and Wo RDD. In words, the attention output is projected through and, together with residual stream and prelayer norm, forms the output of the block."
        },
        {
            "title": "2.2 Attention Decomposition",
            "content": "Transformers benefit from attention maps for interpretability, but these do not fully capture token influence on predictions. Token attribution methods address this by decomposing the forward pass into token-wise contributions (Kobayashi et al., 2021). This section presents two key approachesdirect token-to-token decomposition and logit attributionwhich motivate our interpretability method for Mamba. Token Contributions. To determine the influence of token on the representation of token i, we express the output at certain layer as follows:1 where []+ represents the ReLU function. Logit Contributions. While token-wise decomposition methods capture interactions within layer, they do not measure tokens direct contribution to the final output. To bridge this gap, ALTI-Logit (Ferrando et al., 2023) traces token contributions through the residual stream up to the final prediction. Formally, given token w(i) V, the contribution of token at layer is given by: (l) i,j = (l) (x(l1) )Uw(i), (7) where RVD is the output embedding matrix. Let R(l) = (l) (2)P (1) denote the residual stream at layer l, where (l) i,j refers to the contribution of x(l1) i,j = 1. Then, the final pairwise contribution score aggregated from all layers is such that (cid:80) to x(l) (l) Ci,j = (cid:88) l= (l) i,jR(l1) . (8) ALTI-Logit provides final-layer attribution score, making it particularly useful for output-sensitive interpretability. In Section 3, we follow these principles to design attribution methods for Mamba. yi = (cid:88) j=1 Ti(xj) RD, (3)"
        },
        {
            "title": "2.3 State Space Models (SSMs)",
            "content": "SSMs (Gu et al., 2020) are type of sequence mixing layer that process sequences through linear recurrence. Letting Hi RRD denote the state 1We ignore the bias terms for clarity (w.l.o.g). Moreover, in decoder-only model we have 1 i. 2 at the ith time step, discrete SSM can be formulated as follows (Pitorro et al., 2024):2 Hi = AHi1 + bx υi = + Dxi RRD, RD, (9) where RRR, RR, RR, RDD are (discrete) parameters shared for all i. Mamba-1. The first version of Mamba (Gu and Dao, 2023) extends the previous formulation into an input-dependent SSM by turning the parameters into learnable projections of the current input xi: Hi = Ai Hi1 + Bi Xi RRD, RD, υi = ci + Dxi (10) RRD is an R-sized stack where Xi = 1rx of the input, Ai RRD represents diagonal matrices of size R, Bi RRD, ci RR, and is the Hadamard product. Mamba-1 block. Analogously to transformers, the Mamba-1 model is collection of stacked blocks containing sequence mixing layer and gating mechanism. Concretely, the sequence mixing layer can be fully described as: (11) Φ = SiLU(Ψ) Ψ = Conv1D(XWx) RN 2D, RN 2D, RN R, A, B, = Linear(Φ) Υ = SSM(Φ; A, B, C, D) RN 2D, where Wx RD2D and Linear : RN 2D RN represents set of low-rank projections. The gating mechanism is employed as follows: = Υ = SiLU(XWz) RN 2D, RN 2D, RN D, where Wz RD2D and Wo R2DD. = Wo (12) Mamba-2. Mamba-2 (Dao and Gu, 2024) introduces simpler SSM formulation by defining as scalar times identity Ai = aiIRR. This leads to the following input-dependent model: Hi = AiHi1 + Bi Xi RRD, RD. υi = ci + Dxi (13) In contrast to Mamba-1 (c.f. Equation 10), the input-dependent parameter Ai RRR represents single diagonal matrix. 2A discretization step is required to obtain discrete parameters (e.g., via the zero-order hold rule); however, we follow Pitorro et al. (2024) and omit this step for clarity. 3 Mamba-2 block. Regarding block structure, Mamba-2 draws the parameters A, B, directly from the initial input X, and further introduces GroupNorm layer (Wu and He, 2018) after the gating mechanism for additional stability: = GroupNorm(Υ Z) RN 2D. (14) 2.4 Hidden Attention in Mamba As noted by Ali et al. (2024) and Dao and Gu (2024), by unrolling Mambas recurrence we can interpret the sequence mixing layer as multiplying lower-triangular matrix with the entire input Υ = (independently for each channel/head). More generally, by unrolling Mamba-1s recurrence defined in Equations 10, we can show that Mi,j RDD has the following form: Mi,j = Diag k=j+1 Ak Bj ci , (15) for all i, and Mi,j = 0 otherwise. similar expression can be derived for Mamba-2 by noticing that Ak RRR is, by definition, diagonal matrix. Importantly, for each dimension [D], this is an implicit attention matrix akin to transformers attention matrix. We provide more details on this derivation in A."
        },
        {
            "title": "3 LATIM",
            "content": "While the attention mechanism found in transformers allows us to decompose the contributions of different input tokens, decomposing individual token contributions is challenging for Mamba. Additionally, in Mamba-1 the channel dimensionality is often large in practice, and therefore manual inspection of all attention maps per layer and sample quickly becomes unfeasible (e.g., 370M model has 48 layers with = 1024). Although Mamba-2 alleviates this issue by using smaller number of heads, it remains unclear how to obtain single attention plot for each layer or for an entire sample. Overall, our goal is to rearrange the forward pass from both Mamba-1 and Mamba-2 such that we can measure the total contribution of token xj towards the output yi, similar to the definition of Ti(xj) in Equation 4 tailored for transformers."
        },
        {
            "title": "3.1 Mamba-1 Decomposition",
            "content": "In this direction, we start by revisiting Mambas forward pass at step in Equation 11. The first component of Mamba-1 block is the 1D convolution layer. Concretely, letting denote the kernel size, the 1D causal convolution output for token can be described as: Method Expression LATIM (ℓp) LATIM (ALTI) LATIM (ALTI-Logit) Cij = Ti(xj)Uw(i)Rj Cij = Ti(xj)p Cij [yi1 yi Ti(xj)1]+ ψi = Conv1D (XWx; w)i = (cid:88) k=1 (cid:16) (k) W xiw+k (cid:17) (16) Table 1: Overview of LATIM-based methods for obtaining contribution scores for (i, j) token interactions. + bc, (17) where (k) Rdd and bc represents the convolution kernel and bias, respectively. Next, ψi is transformed via SiLU activation ϕi = SiLU(ψi), which, in turn, is passed to the SSM module, υi = SSM(ϕi). Therefore, in order to compute the contribution of token-to-token interactions, we first need to unroll the SSM recurrence from Equation 10. To that end, we leverage the tensor defined in Equation 15 and treat the term Dϕi as skip-connection, leading to: υi = (cid:88) j=1 (Mi,j + δi,jD) ϕj (cid:124)(cid:123)(cid:122)(cid:125) SiLU(ψj ) , (18) where δi,j is the Kronecker delta. Unfortunately, the non-additivity of the SiLU activation prevents the decomposition of υi as sum of previous token interactions. That is, we cannot rearrange the above expression such that we use the jth token only at the jth iteration, prohibiting us from deriving tokento-token contributions as done in transformers (see Section 2.2). However, if we assume the existence of an additive function that approximates well the SiLU activation, we can decompose ϕj as follows:3 ϕj = (cid:88) k= (W (k) (cid:124) xjw+k + δk,0bc ). (cid:125) (cid:123)(cid:122) φ(k) (19) This decomposition allows us to derive more interpretable output for Mambas recurrent module: υi = (cid:88) (cid:88) j=1 k=1 (Mi,j+k + δi,j+kD) φ(k) . (20) Importantly, we can modify the above expression in order to obtain the vector representation that stems from interactions with the jth token as follows: υij = (cid:88) k=1 (Mi,j+k + δi,j+kD) φ(k) . (21) Finally, after considering the gating mechanism and the output projection from Equation 12, we obtain the (i, j) contribution vector: Ti(xj) = (Zi υij) . (22) And similarly to the way attention is decomposed in transformers (see Equations 3 and 4), the final output can be computed by integrating the contribution from all previous tokens: yi = (cid:88) j=1 Ti(xj). (23)"
        },
        {
            "title": "3.2 Mamba-2 Decomposition",
            "content": "Recall from Equation 14 that Mamba-2 places GroupNorm layer on the output of the SSM module. Let υi R2D be the SSM output at token i, and define ui = Zi υi. At test time, GroupNorm can be viewed as an affine map around ui, GroupNorm(ui) = γi(ui) ui + βi, (24) where γi(ui) is (fixed) linear operator once ui is known, and βi is an offset.4 Hence, if uij denotes the portion of ui that originates from token j, its contribution passes through GroupNorm in the same linear fashion. Finally, applying the output projection Wo yields the token decomposition: Ti(xj) = (cid:104) (cid:0)ui (cid:1) uij γi (cid:105) . (25) As we are interested in obtaining token-to-token interpretability scores, we can apply various scalar aggregation functions to Ti(xj). Common examples include ℓ1 or ℓ2 norms (Kobayashi et al., 2021), as well as the ALTI (Ferrando et al., 2022) and ALTI-Logit (Ferrando et al., 2023) approaches. We provide summary of LATIM variants that leverage these aggregations in Table 1. 3We explicitly include δj,0 into the expression to account for the convolution bias, which is only added once per channel. 4We follow Ferrando et al. (2022) and ignore the offset term as it not attributed to any token. 4 Method AUC AP R@K Mamba-1: Mamba-Attention Mamba-Attribution MambaLRP LATIM (ℓ2) LATIM (ALTI) LATIM (ALTI-Logit) Mamba-2: Mamba-Attention Mamba-Attribution LATIM (ℓ2) LATIM (ALTI) LATIM (ALTI-Logit) 0.84 0.83 0.40 0.88 0.86 0.85 0.79 0.79 0.98 0.85 0. 0.36 0.31 0.22 0.41 0.47 0.44 0.49 0.47 0.86 0.71 0.70 0.22 0.19 0.20 0.27 0.36 0.31 0.39 0.39 0.74 0.63 0.61 Table 2: Faithfulness evaluation on the copying task in terms of Area Under the Curve (AUC), Average Precision (AP), and Recall at (R@K). Task, synthetic benchmark that tests sequence recall and allows us to faithfully assess how different methods capture token interactions (Bastings et al., 2022). Next, we follow (Kobayashi et al., 2020) and (Ferrando et al., 2022) and analyze machine translation (MT), where we use the alignment error rate (AER) metric to quantitatively compare the performance of interpretability approaches. Finally, we explore retrieval-based generation, leveraging the RULER benchmark (Hsieh et al., 2024) to investigate Mambas selective processing in real-world recall-intensive tasks. Models. For machine translation and retrievalbased generation, we use pre-trained versions of Mamba-1 and Mamba-2. For the copying task, we train our models from scratch. Training details for all tasks are provided in B. Methods. To evaluate the effectiveness of LATIM, we conduct both qualitative and quantitative assessments, comparing it against existing interpretability techniques for Mamba. Namely, we compare our approach against MambaLRP (Jafari et al., 2024) when using Mamba-1,5 and with Mamba-Attention/Attribution (Ali et al., 2024) for both Mamba-1 and Mamba-2. Regarding LATIM, we experiment with the variants shown in Table 1."
        },
        {
            "title": "4.1 Copying",
            "content": "The synthetic copying task (Jelassi et al., 2024) serves as controlled setting for testing memory recall in SSM-based models, which traditionally struggle with maintaining long-range dependencies (Arora et al., 2024). Recent advances, such as 5MambaLRP is only defined for Mamba-1. Figure 1: Heatmaps generated by different interpretability methods for Mamba-2. The interaction between source and copied tokens (along the diagonal line) is more clearly highlighted with LATIM."
        },
        {
            "title": "3.3 Decomposition Error",
            "content": "Approximated Strategy. Unlike attention decomposition in transformers, Mamba requires an additive function in Equation 19 to linearly decompose pairwise interactions. Ideally, should closely approximate the original non-additive expression ϕi = SiLU(ψi). To assess this, we explore different approximation strategies in C, including firstand second-order Taylor expansions around zero. Surprisingly, we find that directly setting as SiLU yields the lowest approximation error across all layers. Therefore, unless explicitly stated otherwise, LATIM refers to our decomposition method using := SiLU. Exact Strategy. While well-chosen approximation function enables interpretability without requiring model retraining, it does not fully recover the exact Mamba blocks output. To eliminate this discrepancy, we propose modified version of Mamba that removes the SiLU activation, simplifying the computation to ϕi = ψi, which effectively turns into the identity function in Equation 19. Though this approach requires retraining, we demonstrate in Section 4.4 that it achieves zero decomposition error while maintaining the same level of interpretability and task performance."
        },
        {
            "title": "4 Experiments",
            "content": "Tasks and Metrics. We adopt diverse set of tasks to provide rigorous evaluation. Following Jelassi et al. (2024) we experiment on the Copying 5 Figure 2: Interpretability heatmaps for Mamba-1 (370M) fine-tuned on DEEN data from the IWSLT17 dataset. LATIM (ℓ2) produces alignments that more closely match the ground truth. GoldAlign (DEEN) IWSLT17 (DEEN) IWSLT17 (FREN) Method M1S M1L M2S M2L M1S M1L M2S M2L M1S M1L M2S M2L Aggregating layers: MambaLRP LATIM (ALTI-Logit) Best layer: Mamba-Attention Mamba-Attribution LATIM (ℓ2) LATIM (ALTI) 0.50 0.68 0.47 0. - 0.63 - 0.69 0.65 0.67 0.68 0.71 - 0.60 - 0. 0.65 0.71 0.66 0.69 - 0.62 - 0.76 0.84 0.86 0.46 0.55 0.85 0.87 0.44 0. 0.84 0.78 0.49 0.51 0.85 0.70 0.52 0.51 0.79 0.81 0.47 0.52 0.79 0.82 0.49 0.53 0.72 0.81 0.43 0.47 0.79 0.81 0.49 0. 0.80 0.73 0.46 0.53 0.79 0.68 0.48 0.53 0.69 0.72 0.35 0.38 0.78 0.66 0.37 0.38 Table 3: Alignment Error Rate (AER) per interpretability method. M1 and M2 stand for Mamba-1 and Mamba-2, with subscript and denoting the small (130M) and large (370M) versions, respectively. the mimetic initialization proposed by Trockman et al. (2024), have significantly improved Mambas performance on this task. We replicate this setup in smaller-scale experiment, where 13M models (Mamba-1 and 2) are trained to repeat 50-token string after separator token: source <SEP> copy. Qualitative Analysis. Our interpretability analysis focuses on whether different methods can recover the expected diagonal interaction pattern between source and copied tokens. To that end, we start by qualitatively inspecting each methods heatmap in Figure 1 for Mamba-2.6 We observe that Mamba-Attention produces coarse representation of the copy mechanism, lacking the precision needed to capture token-level dependencies. In contrast, all LATIM variants better highlight sourcecopy interactions, making it the superior choice for visualizing the copying mechanism. Faithfulness Evaluation. To quantitatively assess the reliability of each method, we use groundtruth matrix with ones along the three main diagonals. This means that faithful interpretability method should produce well-defined diagonal pattern, indicating that the model correctly attends 6We empirically observed that Mamba-1 learns to copy at layer 4, while Mamba-2 shifts this behavior to layer 3. Thus, we extract heatmaps at these layers for the copying task. to preceding tokens, even when shifted, during the copying process. Leveraging the interpretability metrics from Fomicheva et al. (2021), we report faithfulness evaluation in Table 2. The results show that all variants of LATIM outperform the baselines, with LATIM (ℓ2 and ALTI) consistently achieving the top results across all metrics for both Mamba-1 and Mamba-2."
        },
        {
            "title": "4.2 Machine Translation",
            "content": "We evaluate our method in machine translation (MT) by fine-tuning Mamba models (130M and 370M) on the IWSLT17 dataset DEEN (Cettolo et al., 2017a), following the setup from (Pitorro et al., 2024). This setup allows us to compare interpretability methods using the alignment error rate (AER), widely used metric for measuring the accuracy of token alignments in translations. Qualitative Analysis. We start by showing the alignments produced by Mamba-1 with the different approaches in Figure 2, along with the golden alignments provided by Vilar et al. (2006). We present additional heatmaps for all methods, including Mamba-2 plots, in Figure 7 (D.2). We find that token contribution heatmaps produced by LATIM (ℓ2) are sparser and more informative than Mamba-Attention and MambaLRP, which captures the general structure but lacks token-level preci6 Figure 3: Left: Attention map from LATIM (ℓ2) for Passkey Retrieval sample where the key is itchy-obligation Instead of predicting 5661907, the model incorrectly produces 4612365. Right: Average contribution scores for token ranges preceding each extracted frequent word. Notably, the focus over the token ranges fdcv and vgpn aligns well with the two most frequent tokens in the sample (fdcvcu, vgpnki). However, when generating uqbcr, it fails to focus on the 3rd most frequent token, suggesting that it relies more on morphological patterns than frequency. sion. Moreover, we also note that methods that aggregate input relevances across the entire model, such as LATIM (ALTI-Logit), retain sparsity but fail to capture the gold alignments. Alignment Error Rate. To quantitatively compare methods, we further compute AER on IWSLT17 DEEN and FREN using candidate alignments generated with AwesomeAlign (Dou and Neubig, 2021). As seen in Table 3, among the layer-wise aggregation methods, we note that MambaLRP consistently outperforms LATIM (ALTILogit). However, when looking at layer-wise methods, we find that LATIM (ℓ2) achieves the lowest AER among all methods, reinforcing again its effectiveness in capturing token-to-token interactions, and also suggesting that translation alignments obtained on per-layer basis might be preferable than those collapsed into global representation."
        },
        {
            "title": "4.3 Retrieval-based Generation",
            "content": "Mambas efficiency in handling long contexts makes it an attractive candidate for retrieval-based generation. However, its ability to selectively recall relevant information remains an open question. We investigate this issue using pre-trained Mamba-2 checkpoints with various sizes and experimenting on the RULER benchmark (Hsieh et al., 2024), focusing on two recall-intensive tasks: Passkey Retrieval and Frequent Word Extraction (FWE). Passkey Retrieval. In this task, the model must extract numeric value associated with key from surrounding distractor text. In our experiments, Mamba-2 consistently performed well in the simpler, single-passkey setting. However, as shown in Table 5 (D.3), increasing model size, sequence length, and the number of key-value pairs leads to significant drop in recall. When analyzing attention maps for multi-key retrieval using LATIM (ℓ2) in Figure 3 (left), we observe that the 370M model struggles to consistently focus on the correct key, revealing potential weakness in the multi-key setting. Additionally, in D.3 we show that when the gold key appears in the first position, model accuracy declines by 38% in the 2-key setting and up to 101% in the 4-key setting. Despite these clear limitations, MambaAttention visualization fails to capture this inconsistency, often misattributing focus to misleading tokens. Frequent Word Extraction. The FWE task requires the model to extract the three most frequent synthetic words in passage. In D.3, we show that Mamba models, even at the 1.4B parameter scale, struggle with this task. Our analysis in Figure 3 (right), using LATIM (ℓ2), reveals that the model frequently misidentifies the correct 3rd most frequent token, highlighting its difficulty in tracking long-range token occurrences. We also note that Mambas attention on repeated words decays over time, which may explain its failure to accurately count word frequency."
        },
        {
            "title": "4.4 Approximation Error Analysis",
            "content": "As noted in Section 3.3, our current method involves an approximate decomposition of Mambas computations due to the non-linearity introduced by the SiLU activation. To measure the impact of this approximation, we experiment with alternative activations by retraining Mamba with ReLU or disabling activations entirely, which casts as the identity function and, more importantly, yields an exact method. We perform continued pretraining of Mamba-2 (370M) on the FineWeb7 Edu dataset (Penedo et al., 2024) and evaluate on the IWSLT17 DEEN dataset using AER to assess interpretability and COMET (Rei et al., 2020) to asses translation quality. Results are shown in Table 4. Interestingly, model trained without non-linear activation achieves not only 0 approximation error but also leads to the best AER scores along with high COMET. As noted by Bick et al. (2024), who also disable the activation before SSM distillation, purely linear variant of Mamba can be an effective alternative for more interpretable architectures. Nonetheless, we highlight that our approximated version with := SiLU leads to similar AER and COMET scores as := identity."
        },
        {
            "title": "5 Related Work",
            "content": "Input Attribution Methods. large body of work focuses on interpretability via input attribution, particularly in transformers, where attention maps serve as widely used technique (Fantozzi and Naldi, 2024). While attention weights alone can be unfaithful indicators of model decisions (Jain and Wallace, 2019; Bastings and Filippova, 2020), they remain useful in many applications, including machine translation (Wiegreffe and Pinter, 2019; Treviso and Martins, 2020). Recent methods go beyond simple attention analysis by explicitly decomposing internal model computations, such as integrating value-weighted norms (Kobayashi et al., 2020) or using vector distances to estimate token contributions (Ferrando et al., 2022). Additionally, aggregation-based techniques, including Attention Rollout (Abnar and Zuidema, 2020), DiffMask (De Cao et al., 2020), and ALTI-Logit (Ferrando et al., 2023), consolidate relevance scores across multiple layers to provide more holistic view of information flow. While these methods have substantially improved transformer interpretability, state space models (SSMs) remain comparatively underexplored. Theoretical Insights into SSMs. Beyond interpretability, several studies have analyzed the internal mechanisms of SSMs. Vo et al. (2025) investigate the asymptotic behavior of token states, revealing conditions under which tokens either converge or diverge, affecting memory retention. Sieber et al. (2024) introduce framework that unifies different sequence modeling paradigms, including SSMs, under common mathematical representation. Meanwhile, Trockman et al. (2024) propose an initialization technique that improves Mambas Activation Error per Layer 0-16 16-32 32-48 AER COMET SiLU 0.21 SiLU + CP 0.21 ReLU 0.35 Identity 0.00 0.45 0.43 0.83 0.00 0.57 0.54 1.07 0.00 0.47 0.46 0.51 0.46 83.4 83.6 82.8 83.3 Table 4: Approximation error analysis with different activations for computing ϕi in Equation 18. CP indicates continued pretraining. recall ability inspired by attention-like patterns. Interpreting Mamba. Despite the growing adoption of Mamba, only few works have explicitly addressed its interpretability. Ali et al. (2024) introduce Mamba-Attention and Mamba-Attribution, which approximate token interactions by extracting implicit attention patterns in Mamba-1. Similarly, MambaLRP (Jafari et al., 2024) applies Layer-wise Relevance Propagation to Mamba-1, ensuring stable attribution propagation. However, these approaches do not provide direct decomposition of individual token contributions, leaving gaps in understanding how Mamba selectively processes information. LATIM bridges this gap by providing fine-grained, token-level interpretability for both Mamba-1 and Mamba-2. Additionally, we note that LATIM is adaptable and can be applied to other linear recurrent architectures, such as DeltaNet (Yang et al., 2024) and mLSTM (Beck et al., 2024), making it valuable interpretability tool for long-context models."
        },
        {
            "title": "6 Conclusion",
            "content": "Our experiments demonstrate that our token-level decomposition approach significantly improves interpretability for Mamba models. Across copying, machine translation, and retrieval-based generation tasks, we show that our method, LATIM, provides clearer insights into Mambas selective processing mechanisms. For example, our findings suggest that Mambas recall limitations in long-context tasks may stem from its sparse and decaying focus on relevant tokens. Moreover, our study confirms that while LATIM introduces minimal approximation error, its exact counterpart eliminates this error entirely while maintaining interpretability and task performance. Together, these contributions improve our understanding of Mamba and open new directions for improving its reliability and effectiveness in real-world applications."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank André Martins, Pavlo Vasylenko, Giuseppe Attanasio and Saúl Santos for their helpful and constructive feedback. This work was supported by EUs Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by the project DECOLLAGE (ERC-2022-CoG 101088763), by the Portuguese Recovery and Resilience Plan through project C64500888200000055 (Center for Responsible AI), and by FCT/MECI through national funds and when applicable co-funded EU funds under UID/50008: Instituto de Telecomunicações."
        },
        {
            "title": "Limitations",
            "content": "We point out some limitations of the presented study. Our method, LATIM, relies on an approximation strategy to decompose token contributions due to the non-linearity introduced by the SiLU activation. Although our empirical analysis suggests that this approximation does not meaningfully impact interpretability quality, an exact decomposition requires model modifications, such as removing non-linearities, requiring re-training. Additionally, our evaluation focuses primarily on tasks like Copying and Machine Translation, where token interactions are well understood. In more complex tasks such as Retrieval-based Generation, assessing interpretability quality is harder, and further validation with human evaluations could provide more robust assessment. Furthermore, LATIM is specifically designed for Mamba-1 and Mamba-2, and while the principles behind it could easily be extended to other state space models or linear recurrent models, some additional modifications may be necessary. Architectures incorporating more complex gating mechanisms or hybrid attention-SSM layers might require adapted decomposition techniques. Additionally, while LATIM helps visualize token interactions, its impact on improving model robustness and trustworthiness remains an open question."
        },
        {
            "title": "Potential Risks",
            "content": "Although our token-level decomposition provides valuable insights, it may also be misused. An overreliance on the generated token maps could lead users to assume these partial explanations capture all aspects of the models reasoning. This false confidence may mask biases in the model or data, and encourage trust in outputs without adequate scrutiny, particularly in sensitive domains. Additionally, exposing how Mamba selectively processes tokens could aid malicious actors in crafting targeted adversarial inputs. By identifying which tokens or positions most influence the model, adversaries could exploit these patterns to degrade performance or manipulate outputs. Such misuse risks undermining the reliability of Mamba-based systems, especially when high-stakes decisions rely on accurate and fair model predictions."
        },
        {
            "title": "References",
            "content": "Samira Abnar and Willem Zuidema. 2020. Quantifying attention flow in transformers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 41904197, Online. Association for Computational Linguistics. Ameen Ali, Itamar Zimerman, and Lior Wolf. 2024. The hidden attention of mamba models. Preprint, arXiv:2403.01590. Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Re. 2024. Zoology: Measuring and improving recall in efficient language models. In The Twelfth International Conference on Learning Representations. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Preprint, Layer normalization. Hinton. 2016. arXiv:1607.06450. Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, and Katja Filippova. 2022. will you find these shortcuts? protocol for evaluating the faithfulness of input salience methods for text classification. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 976991, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Jasmijn Bastings and Katja Filippova. 2020. The elephant in the interpretability room: Why use attention as explanation when we have saliency methods? In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 149155, Online. Association for Computational Linguistics. Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. xLSTM: Extended long shortterm memory. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Aviv Bick, Kevin Li, Eric P. Xing, Zico Kolter, and Albert Gu. 2024. Transformers to SSMs: Distilling quadratic knowledge to subquadratic models. In The 9 Thirty-eighth Annual Conference on Neural Information Processing Systems. Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Niehues Jan, Stüker Sebastian, Sudoh Katsuitho, Yoshino Koichiro, and Federmann Christian. 2017a. Overview of the iwslt 2017 evaluation campaign. In Proceedings of the 14th International Workshop on Spoken Language Translation (IWSLT), pages 214. Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. 2017b. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conference on Spoken Language Translation, pages 214, Tokyo, Japan. International Workshop on Spoken Language Translation. Tri Dao and Albert Gu. 2024. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. Preprint, arXiv:2405.21060. Nicola De Cao, Michael Sejr Schlichtkrull, Wilker Aziz, and Ivan Titov. 2020. How do decisions emerge across layers in neural models? interpretation with differentiable masking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 32433255, Online. Association for Computational Linguistics. Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, ShihYang Liu, Matthijs Van keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Celine Lin, Jan Kautz, and Pavlo Molchanov. 2025. Hymba: hybrid-head architecture for small language models. In The Thirteenth International Conference on Learning Representations. Zi-Yi Dou and Graham Neubig. 2021. Word alignment by fine-tuning embeddings on parallel corpora. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 21122128, Online. Association for Computational Linguistics. Paolo Fantozzi and Maurizio Naldi. 2024. The explainability of transformers: Current status and directions. Computers, 13(4):92. Javier Ferrando, Gerard I. Gállego, and Marta R. Costajussà. 2022. Measuring the mixing of contextual information in the transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 86988714, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Javier Ferrando, Gerard I. Gállego, Ioannis Tsiamas, and Marta R. Costa-jussà. 2023. Explaining how transformers use context to build predictions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 54865513, Toronto, Canada. Association for Computational Linguistics. Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta Costa-jussà. 2024. primer on the inner workings of transformer-based language models. arXiv preprint arXiv:2405.00208. Marina Fomicheva, Piyawat Lertvittayakumjorn, Wei Zhao, Steffen Eger, and Yang Gao. 2021. The Eval4NLP shared task on explainable quality estiIn Proceedings of mation: Overview and results. the 2nd Workshop on Evaluation and Comparison of NLP Systems, pages 165178, Punta Cana, Dominican Republic. Association for Computational Linguistics. Albert Gu and Tri Dao. 2023. Mamba: Lineartime sequence modeling with selective state spaces. Preprint, arXiv:2312.00752. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Ré. 2020. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems, volume 33, pages 14741487. Curran Associates, Inc. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. 2024. RULER: Whats the real context size of your long-context language models? In First Conference on Language Modeling. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. Minicpm: Unveiling the potential of small language modPreprint, els with scalable training strategies. arXiv:2404.06395. Farnoush Rezaei Jafari, Grégoire Montavon, Klaus Robert Muller, and Oliver Eberle. 2024. MambaLRP: Explaining selective state space In The Thirty-eighth Annual sequence models. Conference on Neural Information Processing Systems. Sarthak Jain and Byron C. Wallace. 2019. Attention is not Explanation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 35433556, Minneapolis, Minnesota. Association for Computational Linguistics. Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and eran malach. 2024. Repeat after me: Transformers are better than state space models at copying. In Forty-first International Conference on Machine Learning. 10 Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2020. Attention is not only weight: Analyzing transformers with vector norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 70577075, Online. Association for Computational Linguistics. Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Kentaro Inui. 2021. Incorporating Residual and Normalization Layers into Analysis of Masked Language Models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 45474568, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Barak Lenz, Opher Lieber, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, Daniel Gissin, Daniel Jannai, Dor Muhlgay, Dor Zimberg, Edden M. Gerber, Elad Dolev, Eran Krakovsky, Erez Safahi, Erez Schwartz, Gal Cohen, Gal Shachaf, Haim Rozenblum, Hofit Bata, Ido Blass, Inbal Magar, Itay Dalmedigos, Jhonathan Osin, Julie Fadlon, Maria Rozman, Matan Danos, Michael Gokhman, Mor Zusman, Naama Gidron, Nir Ratner, Noam Gat, Noam Rozen, Oded Fried, Ohad Leshno, Omer Antverg, Omri Abend, Or Dagan, Orit Cohavi, Raz Alon, Roi Belson, Roi Cohen, Rom Gilad, Roman Glozman, Shahar Lev, Shai Shalev-Shwartz, Shaked Haim Meirom, Tal Delbari, Tal Ness, Tomer Asida, Tom Ben Gal, Tom Braude, Uriya Pumerantz, Josh Cohen, Yonatan Belinkov, Yuval Globerson, Yuval Peleg Levy, and Yoav Shoham. 2025. Jamba: Hybrid transformer-mamba language models. In The Thirteenth International Conference on Learning Representations. Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations. Hosein Mohebbi, Jaap Jumelet, Michael Hanna, Afra Alishahi, and Willem Zuidema. 2024. Transformerspecific interpretability. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts, pages 2126, St. Julians, Malta. Association for Computational Linguistics. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Hugo Pitorro, Pavlo Vasylenko, Marcos Treviso, and André Martins. 2024. How effective are state space models for machine translation? In Proceedings of the Ninth Conference on Machine Translation, pages 11071124, Miami, Florida, USA. Association for Computational Linguistics. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702, Online. Association for Computational Linguistics. Jerome Sieber, Carmen Amo Alonso, Alexandre Didier, Melanie Zeilinger, and Antonio Orvieto. 2024. Understanding the differences in foundation models: Attention, state space models, and recurrent neural networks. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):19291958. Marcos Treviso and André F. T. Martins. 2020. The explanation game: Towards prediction explainability through sparse communication. In Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 107118, Online. Association for Computational Linguistics. Asher Trockman, Hrayr Harutyunyan, J. Zico Kolter, Sanjiv Kumar, and Srinadh Bhojanapalli. 2024. Mimetic initialization helps state space models learn to recall. Preprint, arXiv:2410.11135. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. David Vilar, Maja Popovic, and Hermann Ney. 2006. AER: do we need to improve our alignments? In Proceedings of the Third International Workshop on Spoken Language Translation: Papers, Kyoto, Japan. Thieu Vo, Duy-Tung Pham, Xin T. Tong, and Tan Minh Nguyen. 2025. Demystifying the token dynamics of deep selective state space models. In The Thirteenth International Conference on Learning Representations. Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1120, Hong Kong, China. Association for Computational Linguistics. Yuxin Wu and Kaiming He. 2018. Group normalization. In Proceedings of the European Conference on Computer Vision (ECCV). Rui Xu, Shu Yang, Yihui Wang, Bo Du, and Hao Chen. 2024. survey on vision mamba: Models, applications and challenges. arXiv preprint arXiv:2404.18861v1. 11 Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. 2024. Parallelizing linear transformers with the delta rule over sequence length. In The Thirty-eighth Annual Conference on Neural Information Processing Systems."
        },
        {
            "title": "A Hidden Attention Derivation in Mamba",
            "content": "This appendix provides detailed derivation of the hidden-attention matrix in both Mamba-1 and Mamba-2, showing how their element-wise recurrences can be written in the form Υ = X. A.1 Mamba-1 Derivation Recall the Mamba-1 recurrence (ignoring skip connections) for each time step 1: Hi = Ai Hi1 + Bi Xi υi = ci RRD, RD, RRD is an R-sized stack where Xi = 1rx of the input, Ai RRD represents diagonal matrices of size R, Bi RRD, ci RR, and is the Hadamard product. Setting H0 = 0, we can unroll the recurrence to see how past tokens contribute: H1 = A1 0 + B1 X1. H2 = A2 H1 + B2 X2 = A2 (A1 0 + B1 X1) + B2 X2 = A2 B1 X1 + B2 X2. H3 = A3 H2 + B3 X3 = A3 [A2 B1 X1 + B2 X2] + B3 X3 = A3 A2 B1 X1 + A3 B2 X2 + B3 X3. Hence, in general for any i, we have: (cid:88) Hi = Bj Xj RRD, Ak k=j+ ci j=1 υi = RD, where we write to indicate an element-wise product over the indices k. Block-matrix expression. To capture this in matrix form, observe that each coordinate of Xj gets multiplied by chain of element-wise factors Ak and Bj, then finally projected by ci. Aggregating these dimension-wise scalars into diagonal matrix Mi,j RDD yields for all i, and Mi,j = 0 otherwise. Stacking these Mi,j blocks into 4D tensor RN DD gives us Υ = X, (26) once we interpret as an grid of blocks and flatten RN to length-(N D) vector, as explained below in A.3. A.2 Mamba-2 Derivation Mamba-2 uses similar idea but modifies Ai into diagonal matrix of size R, rather than an element-wise parameter array. Formally, Hi = AiHi1 + Bi Xi υi = ci RRD, RD, where Ai = ai IRR. Unrolling similarly, we get Bj Xj RRD, Ht = (cid:88) (cid:89) Ak k=j+1 j=1 υt = ct RD. Since each Ak is diagonal matrix, the product (cid:81)i k=j+1 Ak remains diagonal. Aggregating the resulting dimension-wise multipliers again forms Mi,j RDD, leading to Mi,j = Diag (cid:89) k=j+1 Ak Bj ci , for all i, and Mi,j = 0 otherwise. The shapeflattening for and then follows the same block-matrix logic as in Mamba-1. A.3 Block-Matrix Implementation Define the overall 4D tensor RN DD by gathering the blocks Mi,j from above. In matrix form, we can treat as an grid of blocks, thus flattening to R(N D)(N D). Simultaneously, reshape RN into vector of length (N D) by stacking each token row. Then the usual matrixvector product recovers the unrolled recurrence: Υ = υi = (cid:88) j=1 Mi,j xj. Mi,j = Diag k=j+1 Ak Bj ci , Concretely, the ith block row of multiplies the token embeddings {xj}N j=1, and the result is then reshaped back to produce an matrix, whose ith row is precisely υ . 13 Figure 4: Error amounting to the average difference between the regular Mamba-1 (left) and Mamba-2 (right) layer output and the interpretable version with different approximations in Equation 19."
        },
        {
            "title": "B Experimental Details",
            "content": "B.1 Copying We use 8-layer Mamba 1 and 2 models with 512 as the hidden size and 32 as the vocabulary size, the state dimension is set to 16 and 128 for Mamba 1 and 2, respectively. Only layer 4 is initialized as per Trockman et al. (2024), with their optimal configuration (which differs from Mamba 1 to 2). Optimization: AdamW (Loshchilov and Hutter, 2019) optimizer with the inverse square root (Vaswani et al., 2017) learning rate scheduler (500 warmup steps, 5000 total steps, 256 samples per batch) and learning rate of 7e 4. No dropout or gradient clipping was used. The copying dataset was generated as per (Jelassi et al., 2024) and contains 5000 training samples and 128 evaluation samples. B.2 Machine Translation All model dimensions are coupled to their officially released checkpoints. Optimization: AdamW (Loshchilov and Hutter, 2019) optimizer with cosine learning rate scheduler (2000 warmup steps, 18000 steps, 64 samples per batch) and learning rate of 7e 4. Dropout (Srivastava et al., 2014) rate was set to 0.3 and no gradient clipping was used. The IWSLT17 (Cettolo et al., 2017b) dataset contains 232825 training samples, 890 validation samples and 8597 samples for both the DEEN and FREN versions. B.3 Approximation Error All model dimensions are coupled to their officially released checkpoints when performing continued language pretraining. Optimization: AdamW (Loshchilov and Hutter, 2019) optimizer with WSD (Hu et al., 2024) learning rate scheduler (2000 warmup steps, 27900 stable steps, 3100 decay steps, 32k tokens per batch) and learning rate of 5e 5. We used gradient clipping set to 5.0 and no dropout. Moreover, we employed an α parameter in order to smoothly interpolate between the old (SiLU) and the new activations (ReLU or identity). The value of α followed power law during training: min(1, current_step/(total_steps decay_steps))2. Note that the learning rate decay period coincides with the phase where the model relies only on the new activation. B.4 Computational Details All experiments involving LATIM were carried on Nvidia RTX A6000 GPUs with 48GB VRAM."
        },
        {
            "title": "C Extended Approximation Error",
            "content": "Following 4.4, we include additional data which details how the decomposition error changes with different SiLU approximations on each layer. This experiment has been conducted over the GoldAlign (Vilar et al., 2006) dataset. The results can be seen in Figure 4. Overall, casting as SiLU leads to the lowest approximation errors across all models and layers."
        },
        {
            "title": "D Additional Experiments",
            "content": "D.1 Copying In addition to the Mamba-2 visualizations in 4.1, we further include Mamba-1-based versions in Figure 5. These include comparison with MambaLRP which performs especially poorly for this experiment as previously observed in Table 2. Moreover, in Figure 6 we show filtered version of these plots with just the sourcecopy interactions (left-bottom block). We highlight how models learn pattern centered around the diagonal. As per this argument, Table 2 relies on the three main diagonals as its gold label. D.2 Machine Translation In addition to the Mamba-1 visualizations in 4.2, we further include Mamba-2-based versions in Figure 7. 14 Figure 5: Heatmaps produced by different interpretable approaches for Mamba-1. The interaction between source and copied tokens (along the diagonal line) becomes clearer with LATIM. Figure 6: Heatmaps produced by the different interpretability methods for Mamba-1 (top) and Mamba-2 (bottom) on copying sample after filtering the sourcecopy interaction. Note how both models learned to focus over off-diagonal pattern instead of direct token-copy map. Size 1024 2048 Size 2 Passkeys 4 Passkeys = 1 = 2 = 4 = 1 = 2 = 4 First Second First Second+ 130M 99.8 370M 100.0 780M 99.8 99.3 1.4B 58.2 57.6 68.1 63.2 28.1 33.1 60.2 39. 99.7 98.0 84.5 99.7 57.3 55.1 59.3 60.8 30.8 34.1 51.4 38.9 130M 74.3 370M 65.4 780M 76.9 81.7 1.4B 41.2 47.3 59.1 43.6 46.9 53.6 82.0 64. 22.2 26.7 53.4 31.8 Table 5: Mamba-2 accuracy (%) in the Passkey Retrieval task at recovering the correct output. We vary the model size, sequence length (1024 and 2048) and the number of keys {1, 2, 4}. Computed over 1000 samples. Table 6: Mamba-2 accuracy (%) in the Passkey Retrieval task at recovering the correct key if the correct key is the First to appear or the Second+ to appear. Computed over 1000 samples of length 1024. can see, only some words get attended to, making it difficult for the model to track word frequencies. To strengthen this effect, the average attention per word instance decreases heavily. For example, the word fdcvcu occurs 68 times and its first few occurrences have an average attention score across layers substantially higher than the remainder."
        },
        {
            "title": "E AI Assistants",
            "content": "We used Cursor during development, and ChatGPT during paper writing for grammar correction. D.3 Retrieval-based Generation Passkey Retrieval. We compute accuracy statistics in the passkey retrieval task for Mamba-2 370M for each variation (1, 2 and 4 passkeys). We observe that the model has heavy bias towards the first passkey that appears in context as the average accuracy decreases as more keys get introduced  (Table 5)  . To strengthen our argument, accuracy heavily depends on whether the desired passkey is the first that appears  (Table 6)  . Frequent Word Extraction. In Figure 9 (left) we plot Mamba-2s focus over the context tokens in the Frequent Word Extraction task when we only consider the and uqbc (underlined) tokens. As we 15 Figure 7: Heatmaps produced by the different interpretability methods for Mamba-1 (top) and Mamba-2 (bottom) fine-tuned on DEEN data. Figure 8: Attention plots obtained by LATIM (ℓ2) (left) and MambaAttention (right) on Passkey Retrieval sample, showing that MambaAttention focuses on several misleading tokens, such as determined, number for, and mentioned. In contrast, LATIM (ℓ2) focuses only on meaningful strings, like 4612365 (the predicted key) and 5661907 (the correct key). Figure 9: Left: Attention map obtained by LATIM (ℓ2) on the Frequent Word Extraction task, showing that the model is focusing on the incorrectly generated and uqbcr token range (Mamba-2 370M layer 23). Right: Average attention score per word instance, showing that the models focus reduces heavily after the first few word occurrences."
        }
    ],
    "affiliations": [
        "Instituto de Telecomunicações, Lisbon"
    ]
}