{
    "paper_title": "VLA-Adapter: An Effective Paradigm for Tiny-Scale Vision-Language-Action Model",
    "authors": [
        "Yihao Wang",
        "Pengxiang Ding",
        "Lingxiao Li",
        "Can Cui",
        "Zirui Ge",
        "Xinyang Tong",
        "Wenxuan Song",
        "Han Zhao",
        "Wei Zhao",
        "Pengxu Hou",
        "Siteng Huang",
        "Yifan Tang",
        "Wenhui Wang",
        "Ru Zhang",
        "Jianyi Liu",
        "Donglin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training a large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, a novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose a lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only a 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of a powerful VLA model in just 8 hours on a single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 2 7 3 9 0 . 9 0 5 2 : r VLA-ADAPTER: AN EFFECTIVE PARADIGM FOR TINY-SCALE VISION-LANGUAGE-ACTION MODEL Pengxiang Ding2,3,4,, Lingxiao Li1,4,5 Can Cui2,4 Siteng Huang2 Yifan Tang1 Wenhui Wang1 Ru Zhang1,(cid:66) Yihao Wang1,2,4,, Zirui Ge3,4 Xinyang Tong2,4 Wenxuan Song4,6 Han Zhao2,3,4 Wei Zhao2,4 Pengxu Hou6 Jianyi Liu1 Donglin Wang2,(cid:66) 1Beijing University of Posts and Telecommunications 2Westlake University 3Zhejiang University 4OpenHelix Team 5State Key Laboratory of Networking and Switching Technology 6The Hong Kong University of Science and Technology (Guangzhou) Equal contribution: yh-wang@bupt.edu.cn; dingpx2015@gmail.com (cid:66)Corresponding Author Project Lead Work done during interning at Westlake University"
        },
        {
            "title": "ABSTRACT",
            "content": "Vision-Language-Action (VLA) models typically bridge the gap between perceptual and action spaces by pre-training large-scale Vision-Language Model (VLM) on robotic data. While this approach greatly enhances performance, it also incurs significant training costs. In this paper, we investigate how to effectively bridge vision-language (VL) representations to action (A). We introduce VLA-Adapter, novel paradigm designed to reduce the reliance of VLA models on large-scale VLMs and extensive pre-training. To this end, we first systematically analyze the effectiveness of various VL conditions and present key findings on which conditions are essential for bridging perception and action spaces. Based on these insights, we propose lightweight Policy module with Bridge Attention, which autonomously injects the optimal condition into the action space. In this way, our method achieves high performance using only 0.5B-parameter backbone, without any robotic data pre-training. Extensive experiments on both simulated and real-world robotic benchmarks demonstrate that VLA-Adapter not only achieves state-of-the-art level performance, but also offers the fast inference speed reported to date. Furthermore, thanks to the proposed advanced bridging paradigm, VLA-Adapter enables the training of powerful VLA model in just 8 hours on single consumer-grade GPU, greatly lowering the barrier to deploying the VLA model. Project page: https://vla-adapter.github.io/."
        },
        {
            "title": "INTRODUCTION",
            "content": "In the past two years, with significant breakthroughs in multimodal LLMs (Karamcheti et al., 2024; Steiner et al., 2024; Liu et al., 2023b; Li et al., 2025), developing robot systems with general perception, understanding, and behavior capabilities has become key research direction in artificial intelligence. In particular, the emergence of the VisionLanguage-Action (VLA) model offers new solution for enabling robot operations driven by instructions (Kim et al., 2024; Cui et al., 2025; Zhao et al., 2025b; Kim et al., 2025; Song et al., 2025b; Cen et al., 2025; Zhang et al., 2025b; Shi et al., 2025). Research on VLA primarily focuses Figure 1: Characteristics of VLA-Adapter. is that smaller values are better, and vice versa. Our paradigm can effectively obtain the SOTA-level VLA model using tiny-scale backbone. 1 on extracting multimodal information and aligning it with the action decision space to generate the high-quality actions (Team et al., 2024; Liu et al., 2025; Zhong et al., 2025; Fan et al., 2025). Current VLA models typically require large-scale embodied data (e.g., Open X-Embodiment (Collaboration et al., 2024) and DROID (Khazatsky et al., 2024)) to pre-train Multimodal Large Language Models (MLLMs) (Especially, VLMs) for task adaptability (Cheang et al., 2024), which is then passed to the designed Policy network (Bu et al., 2024a; Li et al., 2024b) to decode or generate actions for handling the tasks in the diverse environments (Liu et al., 2023a; Mees et al., 2022). However, when confronted with high-dimensional control environments, VLA models still face several bottlenecks, including reliance on large-scale VLMs, slow fine-tuning speed, high GPU memory (VRAM) consumption, and low inference efficiency (throughput), as shown in Figure 1. To this end, it is necessary to explore the most essential but rarely discussed question in the VLA field: How to bridge the gap of VL (vision-language representations) to (action) more effectively? To answer this question, we propose VLA-Adapter, novel bridging paradigm for VLA. We systematically explore how different conditions influence action generation and give some key findings for VLA design. On this basis, we built Policy network with Bridge Attention to autonomously inject the optimal condition into the action space. Experiments show that VLA-Adapter has superior performance, high inference efficiency, and fast throughput with tiny-scale backbone. It significantly lowers the barrier to VLA deployment. The main contributions are summarized as follows. To our knowledge, this work is the first systematic analysis of bridging paradigms effects on action generation. And we also give some key findings of the VLA model design. VLA-Adapter transfers the sufficient multimodal information to the proposed Policy Network for action generation, effectively bridging the modality gap from VL to A. Rich experiments show that VLA-Adapter has higher success rate, smaller scale, lower tuning cost, and faster inference in diverse simulated and real-world robotic tasks."
        },
        {
            "title": "2.1 VISION-LANGUAGE-ACTION (VLA) MODELS",
            "content": "Recently, leveraging pre-trained Vision-Language Models (VLMs) (Karamcheti et al., 2024; Steiner et al., 2024; Liu et al., 2023b; Li et al., 2025) to control robots for performing various daily tasks has substantially accelerated research in embodied intelligence. This has emerged as prominent research focus (Black et al., 2025; Intelligence et al., 2025; Shukor et al., 2025; NVIDIA et al., 2025; Liu et al., 2025; Luo et al., 2025; Cheang et al., 2025; Jiang et al., 2025; Ding et al., 2024; Bu et al., 2024b; Fan et al., 2025; Tong et al., 2025). These models are referred to as the VLA models. Typically, VLA models require large-scale embodied datasets, such as Open X-Embodiment (Collaboration et al., 2024), for pre-training (Liu et al., 2025; Cheang et al., 2025). This process integrates VLMs with dedicated Policy network (Song et al., 2025a; Li et al., 2024b), allowing the system to decode or generate action sequences for diverse tasks in an end-to-end manner. Moreover, dual-system VLA architectures (Shentu et al., 2024; Bu et al., 2024a; Cui et al., 2025) have recently garnered attention. These methods typically introduce an intermediate latent token to connect the VLMs and the Policy, using an asynchronous mechanism to enhance coordination between the two systems (Zhang et al., 2024a). This design mitigates latency issues during action generation. Consequently, how to effectively and efficiently bridge the gap from the vision-language perception space to the action space has become key challenge in the design of VLA models."
        },
        {
            "title": "2.2 BRIDGING FROM PERCEPTION TO ACTION SPACE",
            "content": "Earlier studies (Kim et al., 2024; Brohan et al., 2023a;b) attempted to directly align perception and action spaces by discretizing actions into tokens. However, this discretization inevitably introduces inherent loss. Recent studies have shifted their focus toward continuous action spaces (Li et al., 2024a; NVIDIA et al., 2025; Black et al., 2025; Shukor et al., 2025; Kim et al., 2025). Based on the types of perceptual features utilized to bridge to the action space, they can be categorized: 2 1) Raw Features from VLMs. Raw features (refer to vision and language representations) are extracted directly from the VLM. Early methods extract representations from the final-layer VLM, operating under the assumption that it encodes the most task-relevant semantic information (Li et al., 2024a; Zhang et al., 2024a). More recent methods leverage the intermediate-layer features within the VLM (Black et al., 2025). They believe that such representations may retain richer multimodal information, thereby benefiting Policy in tasks that demand fine-grained perception or complex reasoning. For example, some studies use features from middle layer (NVIDIA et al., 2025), the first-half layers (Shukor et al., 2025), or all intermediate-layer features (Black et al., 2025). 2) Additional Query as Interface. Furthermore, recent studies (Kim et al., 2025; Cui et al., 2025) have introduced novel interface that employs additional queries as bridges between VLMs and Policy, rather than transmitting Raw features. The query is learnable and can incorporate multimodal information, showing superior performance. The existing bridge paradigms are shown in Figure 2. Figure 2: Existing representative bridge paradigms from VL to A."
        },
        {
            "title": "3.1 PRELIMINARY",
            "content": "We present the VLA-Adapter framework, as illustrated in Figure 3. This VLM follows the PrismaticVLMs architecture (Karamcheti et al., 2024). It has layers. At timestep t, the input into VLM , Lt, AQt}: the 3rd-view image consists of {X , the instruction Lt, and additional ActionQuery AQt. After inputting , the DINOv2 (Oquab et al., 2024) and SigLIP (Zhai et al., 2023) extract vision embeddings. Lt is tokenized. The outputs are the specified-layer Raw latent CR . They serve as the conditions for Policy. , the gripper image and and ActionQuery latent CAQ , Backbone. To build solid basis for research, we perform experiments of VLA-Adapter on different-scale backbones. The backbones select the Prismatic VLM trained on Qwen2.5-0.5B (Yang et al., 2024), the Prismatic VLM trained on LLaMA2-7B (Touvron et al., 2023), and OpenVLA-7B (Kim et al., 2024) pre-trained on robotic data. The benefit gained from increasing backbone scale is limited in VLA-Adapter. The results are shown in Table 2 of Section 4.1. Therefore, to ensure efficiency, Qwen2.5-0.5B is our default backbone unless otherwise specified."
        },
        {
            "title": "3.2 WHICH CONDITION IS ESSENTIAL FOR BRIDGING FROM VL TO A?",
            "content": "Although existing methods have adopted various bridging paradigms from VL to A, their relative effectiveness remains inconclusive. This is mainly due to the differences in the design of the VLM and the Policy. To address this gap, we explore which type of perception information is essential for action generation in the Policy network. In summary, we mainly focus on the following questions: Question 1.1. Which layer of features within the VLM is more effective for the Policy network? Question 1.2. Are the ActionQuery features better choice than the Raw features? 3 Figure 3: The proposed VLA framework. The key components are the effective condition exploration and Attention design. Attention specifically includes cross attention with conditions and self attention with itself. In the Unified VLA-Adapter Framework, Attention is the Bridge Attention as shown in Section 3.3. Four conditions about layer and type are given on the right. To ensure compatibility with existing experimental protocols for representative work (e.g., π0 (Black et al., 2025)), we let the number of Policy layers be equal to that of VLM. At each layer of Policy, the action latent undergoes cross-attention with conditions and self-attention with itself. This iterative process ultimately yields the action output. Details of the Policy can be seen in Section 3.3. For Question 1.1, Experimental Setting. We evaluate four conditions in our framework. to evaluate the effectiveness of the individual-layer information, we employ the single-layer latent as the conditions for the all-layer Policy, as shown in Figure 3a) and 3c). To evaluate the effectiveness of all-layer information, we feed each-layer latent into the corresponding-layer Policy, as shown in Figure 3b) and 3d). For Question 1.2, to compare the effectiveness of the feature types, we use the CR as conditions. The comparison on the LIBERO-Long (Liu et al., 2023a), which is the long-horizon and complex benchmark, the results are as shown in Figure 4. We give the following key findings. Figure 4: Comparison of four conditions in the VLA-Adapter framework on the LIBERO-Long. Blue and Green lines are single-layer CR , as in Figure 3a) and 3b). and all-layer CAQ Blue and Green columns are all-layer CR , as in Figure 3c) and 3d). The detailed results are shown in Appendix C. Please note: the number of ActionQuery is 64 here. Its number is variable, similar to MetaQueries (Pan et al., 2025) in MLLM research; we will explore it in Section 4.5. and single-layer CAQ or CAQ t Key Finding 1. Regarding CR Deep-layer CR tion. The middle-layer CR multimodal details, and facilitates action generation. , the middle-layer latent performs better than the deep-layer latent. is biased towards semantic information and less effective in action generat effectively integrates image and text information, retains richer Key Finding 2. Regarding CAQ ActionQuery is trained from scratch, and deep-layer CAQ details and is more effectively promoting action generation than the shallow layers. , deep-layer latent performs better than other-layer latent. Since aggregates richer multimodal Key Finding 3. Multi-layer features perform better. We observed that using all-layer features generally outperforms single layer. Not only does it improve performance, but it also saves time on best layer selection during design. This design can be more universal. 4 Condition Determination. Does VLA-Adapter rely exclusively CAQ is no. While all-layer CAQ outperforms CR is shown in Table 1. So, we aim to enhance performance by using certain knowledge from CR . as conditions? The answer excels in some hard tasks. Comparison , middle-layer CR Table 1: Comparison of the ith-layer CR and CAQ CR Subtask 7 Subtask 9 9 90 74 13 82 84 CAQ Subtask 7 Subtask 9 1"
        },
        {
            "title": "3.3 POLICY WITH BRIDGE ATTENTION",
            "content": "t 13 66 62 in subtasks of LIBERO-Long. 17 74 58 70 72 23 70 72 24 All 74 84 76 Overall. For the simplicity of the model, we designed an L1-based Policy network. At t-th timestep, the input to Policy includes: {CR , Pt}. τ is the layer of Policy, and it has τ Z+, 0 τ 1. A0 is the H-step initial action of all zeros, it is processed by LayerNorm (cid:3). Pt is the proprit , (cid:101)a0 (LN) and Multi Layer Perceptron (MLP) to obtain (cid:101)A0 (cid:101)a0 oceptive state, and it is mapped through two-layer MLP to obtain the proprio embedding σ0(Pt). The output is the H-step action chunk AM 1 . Each layer is composed of Bridge Attention module and Feed-Forward Network (FFN). The Bridge Attention architecture is shown in Figure 5. t+1, . . . , (cid:101)a0 , CAQ , Aτ =0 = (cid:2) t+H Figure 5: The Policy with Bridge Attention. The Policy parameters are only 97M when the and CAQ backbone is Qwen2.5-0.5B. Each-layer CR are integrated in Bridge Attention with the corresponding-layer action latent. Bridge Attention maps VL to Action to the greatest extent. The degree of CR injection is learnable, ensuring the performance and stability of training. Bridge Attention. The proposed Bridge Attention hopes to guide action generation to the greatt and CAQ est extent possible through the conditions CR . Each Bridge Attention consists of two In the first cross attention, CR cross attentions and one self attention. is processed through an MLP σ1 to obtain K1, V1. The action latent (cid:101)Aτ is used as the Q1, and perform attention (cid:17) needs to be concatenated with . to get CA1 (cid:101)Aτ In the second cross attention, CAQ the σ0(Pt) and passed through an MLP σ2 to obtain K2, V2. CA is as Q, K, , and there is SA( (cid:101)Aτ . In the self attention, (cid:101)Aτ is used as the Q2 to get , σ0(Pt)(cid:3)(cid:17) , σ1(CR ) (cid:2)CAQ , (cid:101)Aτ , σ2 (cid:101)Aτ (cid:101)Aτ ). (cid:16) (cid:16) To selectively inject certain CR into the action space of the Policy, we introduce learning parameter , σ1(CR Ratio to modulate the influence of CA1 . is initialized to 0 value, and the tanh ) activation function is utilized tanh(g) [1, 1] to prevent extreme values from destabilizing the distribution (Zhang et al., 2024b). And then, the three attentions are concatenated to obtain (cid:98)Aτ : (cid:101)Aτ (cid:17) (cid:16) (cid:98)Aτ = [CA1 (cid:16) (cid:101)Aτ , σ1(CR ) (cid:17) tanh(g), CA2( (cid:101)Aτ , σ2[CAQ , σ0(Pt)]), SA (cid:16) (cid:101)Aτ , (cid:101)Aτ (cid:17) ]. (1) 5 passes through residual FFN to obtain (cid:101)Aτ +1 After Bridge Attention, (cid:98)Aτ process, we finally obtain (cid:101)AM 1 Additionally, we also design DiT-based (Diffusion Transformer (Peebles & Xie, 2023)) Policy. Since the diversity of Policy is not the focus of this paper, we put its details and the brief results in Appendix B. The results show that L1-based performance and inference speed are generally superior to those of the DiT-based approach. Therefore, VLA-Adapter chose the L1 architecture as the Policy. is yielded by an LN and MLP layer. . The action chunk AM 1 . Repeating the above t"
        },
        {
            "title": "3.4 TRAINING",
            "content": "The training is conducted end-to-end, with the Policy trained from scratch. Given ground truth action trajectory At and action latent Aτ . We train VLA-Adapter model πθ() with the objective: (θ) = min θ At,CR ,CAQ ,σ0(Pt),τ (cid:104)(cid:13) (cid:13)πθ(Aτ , CR , CAQ , σ0(Pt), τ ) At (cid:105) . (cid:13) (cid:13)1 (2) For more details of training, please see Appendix F.1."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "All experiments are run on 4 NVIDIA H100 GPUs. For more details of the hyperparameters, please see Appendix F.2. We perform rich experiments to answer the following questions: Question 2.1. What are the advantages of the VLA-Adapter compared to other bridge paradigms? Question 2.2. How does VLA-Adapter perform compared to existing methods? Question 2.3. What else key components in the VLA-Adapter paradigm are worth exploring? Experiment Overview. In Section 4.1, we use the long-horizon and complex LIBERO-Long (Liu et al., 2023a), which typically has low success rate, to investigate the necessity of VLA-Adapter. From Section 4.2 to Section 4.4, we use LIBERO (Liu et al., 2023a) and CALVIN (Mees et al., 2022), which are widely used in VLA, as well as real-world data, to compare the performance comprehensively. In Section 4.5, we use LIBERO-Long to explore key parts of VLA-Adapter."
        },
        {
            "title": "4.1 NECESSITY OF VLA-ADAPTER",
            "content": "Effectiveness of our bridge paradigm. To validate the effectiveness, we compare three kinds of backbones: B1: The Prismatic VLM (Karamcheti et al., 2024) trained on Qwen2.5-0.5B (Yang et al., 2024). B2: The Prismatic VLM trained on LLaMA2-7B (Touvron et al., 2023). The first two are different-scale backbones without pre-training on robotic data. B3: The OpenVLA-7B (Kim et al., 2024) pre-trained on robotic data. We adopted the OpenVLA-OFT bridging paradigm (Kim et al., 2025) for comparison. It is the existing state-of-the-art level method on major benchmarks, including LIBERO-Long (Liu et al., 2023a). The comparison results are shown in Table 2. Table 2: Effectiveness comparison with OpenVLA-OFT (Kim et al., 2025) on the LIBERO-Long (Liu et al., 2023a). Fine-tuned is by LoRA fine-tuning (Hu et al., 2022). Bold represents the best performance. Please note, comparison with the bridge paradigms of π0 (Black et al., 2025) and GR00T N1 (NVIDIA et al., 2025) has been included in Section 3, so we will not compare it here. Fine-tuned B1 +OFT B1 +Ours B2 +OFT B2 +Ours B3 +OFT B3 +Ours Success Rate (%) 85.8 95.0 (9.2% ) 87.5 95.2 (7.7% ) 94.5 95.4 (0.9% ) Fortunately, VLA-Adapter remains effective when the backbone is frozen. Only the ActionQuery and Policy are trained from scratch. SmolVLA (Shukor et al., 2025) is the VLA dedicated to studying frozen VLMs. So, we compare with OpenVLA-OFT and SmolVLA. The results are shown in Table 3. Since the results of GR00T N1 come from (Song et al., 2025a), it did full-params tuning, so we will not compare with it here. Based on Tables 2 and 3, we summarize two conclusions: Table 3: Effectiveness comparison when the backbone is frozen. Benchmark is the same as Table 2. For detailed analysis of OpenVLA-OFT (Kim et al., 2025) does not work, please see Appendix H. Frozen OpenVLA-OFT SmolVLA VLA-Adapter Success Rate (%) 0. 77.0 86.4 Conclusion 1. VLA-Adapter improvement is obvious when VLMs without robotic pre-training. Conclusion 2. Even if the backbone freezes, VLA-Adapter still performs strongly. This can be attributed to the fact that, after pre-training on robotic data, the last-layer features are already adapted to the action domain, enabling efficient fine-tuning with simple MLP. However, when VLMs without pre-training, relying solely on the last-layer latents, are insufficient for effective action mapping. So, adopting the VLA-Adapter becomes crucial to achieve efficient fine-tuning. These insights highlight key advantage: VLA-Adapter facilitates efficient fine-tuning of VLMs without robotic pre-training, achieving performance that surpasses baselines using tiny backbone. Efficiency. VLA-Adapter attains faster inference speed. The comparison is shown in Table 4. Table 4: Inference efficiency comparison with OpenVLA (Kim et al., 2024) and OpenVLA-OFT (Kim et al., 2025). The action chunk is 8 dimensions, consistent with most VLA. OpenVLA-OFT (wo , P) is the L1-based version where the input is without the gripper image and proprioceptive state. It is the fastest version of OpenVLA-OFT. Benchmark is the same as Table 2. OpenVLA OpenVLA-OFT (wo , P) OpenVLA-OFT VLA-Adapter Efficiency Throughput (Hz) Latency (Sec) 4.2 0.2396 109.7 0. 71.4 0.1120 219.2 0."
        },
        {
            "title": "4.2 OVERALL PERFORMANCE ON VARIOUS TASKS",
            "content": "Benchmark. We selected the widely adopted LIBERO benchmark (Liu et al., 2023a) to evaluate performance across various types of tasks. LIBERO1 provides multiple suites, including Spatial, Object, Goal, and Long. For detailed settings and examples of LIBERO, please see Appendix A. Baselines. We selected recently published, comprehensive, and high-performance VLA works as comparison baselines. They are Large: 1. FlowVLA (Zhong et al., 2025), 2. UnifiedVLA (Wang et al., 2025), 3. OpenVLA (Kim et al., 2024), 4. OpenVLA-OFT (Kim et al., 2025), 5. UniVLA (Bu et al., 2025), 6. CoT-VLA (Zhao et al., 2025a), 7. WorldVLA (Cen et al., 2025), 8. TraceVLA (Zheng et al., 2025), 9. MolmoAct (Lee et al., 2025), 10. ThinkAct (Huang et al., 2025), and 11. PD-VLA (Song et al., 2025b); Small: 12. 4D-VLA (Zhang et al., 2025a), 13. SpatialVLA (Qu et al., 2025), 14. π0 (Black et al., 2025), 15. π0-FAST (Pertsch et al., 2025), 16. NORA (Hung et al., 2025), 17. SmolVLA (Shukor et al., 2025), 18. GR00T N1 (NVIDIA et al., 2025), and 19. GraspVLA (Deng et al., 2025); Tiny: 20. Seer (Tian et al., 2025), 21. VLA-OS (Gao et al., 2025), and 22. Diffusion Policy (Chi et al., 2023). Their performances are all derived from original references or the reproduction of other published works, ensuring objectivity and accuracy. Metrics. Each subtask is repeated 50 times to evaluate. We use the commonly used metric Success Rate, reported as ranging from 0 to 100, with higher values meaning better performance. Results. Comparison on the LIBERO is shown in Table 5. The results in Table 5 demonstrate that VLA-Adapter, using only tiny-scale backbone, can achieve performance comparable to OpenVLAOFT with 14 larger. It surpasses representative works such as π0, SmolVLA, and GR00T N1. In addition, VLA-Adapter has notable advantage of 29.0% over VLA-OS with the same-scale backbone on LIBERO-Long. These demonstrate the VLA-Adapter superiority on various tasks. 1https://libero-project.github.io/datasets Table 5: Comparison on the LIBERO benchmark. Bold is the best performance. Italics* is suboptimal performance. represents that the non-based-VLM baselines. Scratch is the work without pre-training on robotic data. Params is the backbone scale, and its unit is Billion. Furthermore, we give the performance of VLA-Adapter on the subtasks. It is shown in Table D1 of Appendix D. LIBERO Params Spatial Object Goal Long Avg. FlowVLA (Zhong et al., 2025) (ArXiv) UnifiedVLA (Wang et al., 2025) (ArXiv) OpenVLA (Kim et al., 2024) (CoRL) OpenVLA-OFT (Kim et al., 2025) (RSS) UniVLA (Bu et al., 2025) (RSS) CoT-VLA (Zhao et al., 2025a) (CVPR) WorldVLA (Cen et al., 2025) (ArXiv) TraceVLA (Zheng et al., 2025) (ArXiv) MolmoAct (Lee et al., 2025) (ArXiv) ThinkAct (Huang et al., 2025) (ArXiv) PD-VLA (Song et al., 2025b) (ArXiv) 4D-VLA (Zhang et al., 2025a) (ArXiv) SpatialVLA (Qu et al., 2025) (RSS) π0 (Black et al., 2025) (RSS) π0-FAST (Pertsch et al., 2025) (RSS) NORA (Hung et al., 2025) (ArXiv) SmolVLA (Shukor et al., 2025) (ArXiv) GR00T N1 (NVIDIA et al., 2025) (ArXiv) GraspVLA (Deng et al., 2025) (ArXiv) Seer (Tian et al., 2025) (Scratch) (ICLR) VLA-OS (Gao et al., 2025) (ArXiv) Diffusion Policy (Chi et al., 2023) (RSS) VLA-Adapter (Ours) Large Small Tiny 8.5 8.5 7 7 7 7 7 7 7 7 7 4 4 3 3 3 2.2 2 1. 0.57 0.5 - 0.5 93.2 95.4 84.7 97.6* 96.5 87.5 87.6 84.6 87.0 88.3 95.5 88.9 88.2 96.8 96.4 92.2 93.0 94.4 - - 87.0 78.3 97.8 95.0 98.8* 88.4 98.4 96.8 91.6 96.2 85.2 95.4 91.4 96.7 95.2 89.9 98.8* 96.8 95.4 94.0 97.6 94. - 96.5 92.5 99.2 91.6 93.6 79.2 97.9 95.6 87.6 83.4 75.1 87.6 87.1 94.9 90.9 78.6 95.8 88.6 89.4 91.0 93.0 91.2 - 92.7 68.3 97.2* 72.6 94.0 53.7 94.5* 92.0 69.0 60.0 54.1 77.2 70.9 91.7 79.1 55.5 85.2 60.2 74.6 77.0 90.6 82. 78.7 66.0 50.5 95.0 88.1 95.5 76.5 97.1* 95.2 81.1 81.8 74.8 86.6 84.4 94.7 88.6 78.1 94.2 85.5 87.9 88.8 93.9 89.1 78.7 85.6 72.4 97."
        },
        {
            "title": "4.3 PERFORMANCE ON GENERALIZATION TASKS",
            "content": "We used the CALVIN ABCD (Mees et al., 2022) to evaluate the performance on the zero-shot generalization tasks. CALVIN consists of four environments (Env A, B, C, and D)2 . ABCD means it trains on Env A, B, and and evaluates on Env D. VLA needs to execute preset sequence of 1,000 tasks in sequence. Each task row consists of five subtasks. The model can only proceed to the next subtask after completing the current one. Please see Appendix for more settings. Baselines. We selected recently published works as baselines. They are Large: 1. UniVLA (Bu et al., 2025), 2. OpenVLA (Kim et al., 2024), 3. OpenVLA-OFT (Kim et al., 2025), 4. VLAS (Zhao et al., 2025b), 5. LCB (Shentu et al., 2024), 6. RoboDual (Bu et al., 2024a), 7. OpenHelix (Cui et al., 2025), and 8. ReconVLA (Song et al., 2025c); Small: 9. DeeR (Yue et al., 2024), 10. RoboFlamingo (Li et al., 2024b), 11. VPP (Hu et al., 2025), and 12. SuSIE (Black et al., 2024); Tiny: 13. MoDE (Reuss et al., 2025) and 14. Seer (Tian et al., 2025). The results of these baselines are based on original references or other published works, ensuring objectivity and correctness. Since the original OpenVLA-OFT paper (Kim et al., 2025) did not perform experiments on CALVIN ABCD, we used its source codes to run 150,000 steps and took the best performance. Metrics. We use the widely used Success Rate (the same in LIBERO (Liu et al., 2023a)) and Avg. len of completed tasks (the larger the better, with values between 0-5) as metrics. Results. Comparison on the CALVIN is shown in Table 6. The results in Table 6 show that VLAAdapter has strong generalization ability, and its average length is better than SOTA baselines. 2http://calvin.cs.uni-freiburg.de/ Table 6: Comparison on the CALVIN ABCD benchmark. Bold is the best performance. Italics* is the suboptimal performance. represents that the non-based-VLM method. CALVIN ABCD Params 1 Task completed in row 3 4 Avg. len 5 UniVLA (Bu et al., 2025) (RSS) OpenVLA (Kim et al., 2024) (CoRL) OpenVLA-OFT (Kim et al., 2025) (RSS) VLAS (Zhao et al., 2025b) (ICLR) LCB (Shentu et al., 2024) (IROS) RoboDual (Bu et al., 2024a) (ArXiv) OpenHelix (Cui et al., 2025) (ArXiv) ReconVLA (Song et al., 2025c) (ArXiv) DeeR (Yue et al., 2024) (NeurIPS) RoboFlamingo (Li et al., 2024b) (ICLR) VPP (Hu et al., 2025) (ICML) SuSIE (Black et al., 2024) (ICLR) (Tian et al., 2025) (ICLR) SeerLarge MoDE (Reuss et al., 2025) (ICLR) Seer (Tian et al., 2025) (ICLR) VLA-Adapter (Ours) 7 7 7 7 7 7 7 3 3 1.5 1.3 0.57 0.44 0.32 0.5 95.5 91.3 96.3 87.2 73.6 94.4 97.1* 95.6 86.2 82.4 95.7 87.0 96.3 96.2 94.4 99.1 85.8 77.8 89.1 64.2 50.2 82.7 91.4 87. 70.1 61.9 91.2 69.0 75.4 62.0 82.4 40.9 28.5 72.1 82.8 76.9 66.9 52.1 75.8 28.1 16.0 62.4 72.6 69.3 56.5 43.5 66.5 19.6 9.9 54.4 64.1 64.1 30.4 41.5 51.8 46.6 23.5 33.1 86.3* 81.0* 75.0* 26.0 38.0 49.0 91.6* 88.9 87.2 94. 86.1 81.1 79.9 88.8 80.3 71.8 72.2 82.8 74.0 63.5 64.3 76.5 Large Small Tiny 3.80 3.27 4.10 2.40 1.78 3.66 4.08 3.95 2.82 2.48 4.33* 2.69 4.28 4.01 3.98 4."
        },
        {
            "title": "4.4 PERFORMANCE ON REAL-WORLD TASKS",
            "content": "Experimental settings. We use robotic system to perform real-world tasks. 6-DOF Synria Alicia-D equipped with 1-DOF gripper is employed, and it uses Logitech C920e and RealSense D405 cameras to capture the third-view and gripper images. The real-world robotic system is shown in Figure 6. We evaluate the VLA-Adapter method across four experimental categories: 1) Simple pick-and-place tasks with objects spanning diverse materials and geometries. 2) CALVIN-inspired challenging task II: lateral block relocation (e.g. Move <obj> left/right). 3) CALVIN-inspired challenging manipulation task I: Block stacking. 4) LIBERO-inspired complex and long-horizon task: (e.g. Pick up the spoon and place it on the cup, then place the cup on the plate). To strengthen evaluation rigor and assess generalization performance, we randomize the object positions at test time to induce distribution shift and increase task difficulty. Figure 6: Real-world system Synria Alicia-D and the task examples. Baselines. ACT (Zhao et al., 2023) and OFT-style variant (Kim et al., 2025) are as baselines. 9 Results. The comparison results are shown in Figure 7. Each result is obtained by averaging the results of 10 executions. Experimental results show that VLA-Adapter has better generalization capabilities in various scenarios. Therefore, VLA-Adapter greatly lowers the barrier to adopting VLA in practical applications. More real-world experiments are detailed in Appendix G. Figure 7: Comparison on real-world tasks."
        },
        {
            "title": "4.5 ABLATION EXPERIMENTS",
            "content": "We explore three key components in the VLA-Adapter: 1. Number of ActionQuery, 2. Condition type, and 3. Injection degree for Policy. The benchmark is LIBERO-Long (Liu et al., 2023a). Number of ActionQuery. In our paradigm, the number of ActionQuery is not fixed. To explore the impact of this number on performance, we conducted the following experiments by varying the number of ActionQuery to 1, 4, 8, 16, 64, 128, 256, and 512. The results are shown in Figure 8. Thus, using too few ActionQuery tokens weakens multimodal aggregation and makes it challenging to condition the Policy. Conversely, employing too many ActionQuery tokens introduces redundancy, interfering with the performance. Therefore, we selected 64 ActionQuery tokens. This number provides the optimal balance between performance and efficiency. Figure 8: Comparison of the different numbers of ActionQuery. The blue line shows the result of using only the last-layer ActionQuery. The red star shows the result of the full VLA-Adapter. Condition Type. In Section 3, we analyzed the overall effects of different conditions on action generation. Here, we present the complete comparison results based on the four classic paradigms in Section 2, as shown in Table 7. This result demonstrates that using both all-layer Raw and ActionQuery achieves superior performance, indirectly validating the superiority of our bridge paradigm. Injection Degree for Policy. In the Bridge Attention, we use learnable parameters to control the and set the injection degree of ActionQuery features CAQ injection degree of Raw features CR to 1. Here, we explore other injection degrees, and the comparison results are shown in Table 8. Two conclusions can be drawn from the results in Table 8: From 1) and 2), the performance of CR is inferior to CAQ should inject some effective information into Policy through learning. From 1) and 4), CAQ aggregates multimodal information, which is beneficial for action generation; it needs to be injected fully into Policy. This result confirms that the Bridge Attention is effective. , so CR t 10 Table 7: Comparison with different condition types. The style can be summarized as representative works in Figure 2 of Section 1. N/A represents no such method. Bold is the best performance."
        },
        {
            "title": "All",
            "content": "RoboVLMs (Li et al., 2024a) OpenVLA-OFT (Kim et al., 2025) GR00T N1 (NVIDIA et al., 2025) π0 (Black et al., 2025) N/A VLA-Adapter (Ours) SR 85.8 90.2 88. 90.6 92.6 95.0 Table 8: Ablation of other injection degrees."
        },
        {
            "title": "ActionQuery",
            "content": "Success Rate (%) 1) (VLA-Adapter) 2) 3) 4) tanh(g) 1 1 tanh(g) 1 1 tanh(g) tanh(g) 95.0 91.4 91.0 92."
        },
        {
            "title": "5 CONCLUSION",
            "content": "We propose VLA-Adapter, novel and efficient bridging paradigm for VLA. By leveraging Raw and ActionQuery latent, this method effectively transfers multimodal knowledge to the Policy to generate action. Experiments show that VLA-Adapter achieves SOTA performance using tinyscale backbone. Even when the VLM is frozen, it has strong performance. In addition, our method has low VRAM usage and high inference speed. These results suggest that VLA-Adapter alleviates VLAs reliance on large-scale VLMs and huge training costs, lowering the barrier to deploying VLA. Ultimately, we hope the VLA-Adapter method and key findings of this study can provide solid basis for future research in the VLA and inspire the development of more advanced VLA methods!"
        },
        {
            "title": "6 LIMITATIONS",
            "content": "While VLA-Adapter achieves lightweight and excellent performance, it also has some limitations. First, because VLA-Adapter is not pre-trained on large amount of embodied data and the scale is tiny, its generalization in real-world systems needs to be improved. Secondly, the quality of the actions generated by the Policy networks depends on the conditions provided by the VLM and how they are used. Therefore, future work can further explore these conditions to improve its representation and ensure its efficient use. Finally, the fundamental training process of the VLA-Adapter is still relatively simple, and the complex processes, such as reinforcement learning, can be explored."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work was supported in part by the National Natural Science Foundation of China under Grant U21B2020, and the BUPT Excellent Ph.D. Students Foundation under Grant CX20241055."
        },
        {
            "title": "REFERENCES",
            "content": "Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-Shot Robotic Manipulation with Pretrained Image-Editing Diffusion Models. In ICLR, 2024. 8, 9 Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, 11 Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: VisionLanguage-Action Flow Model for General Robot Control. In RSS, 2025. 2, 3, 4, 6, 7, 8, 11 Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. In ArXiv, 2023a. 2 Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: Robotics Transformer for Real-World Control at Scale. In RSS, 2023b. Qingwen Bu, Hongyang Li, Li Chen, Jisong Cai, Jia Zeng, Heming Cui, Maoqing Yao, and Yu Qiao. Towards Synergistic, Generalized and Efficient Dual-System for Robotic Manipulation. In ArXiv, 2024a. 2, 8, 9 Qingwen Bu, Jia Zeng, Li Chen, Yanchao Yang, Guyue Zhou, Junchi Yan, Ping Luo, Heming Cui, Yi Ma, and Hongyang Li. Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation. In NeurIPS, 2024b. 2 Qingwen Bu, Yanting Yang, Jisong Cai, Shenyuan Gao, Guanghui Ren, Maoqing Yao, Ping Luo, and Hongyang Li. UniVLA: Learning to Act Anywhere with Task-Centric Latent Actions. In RSS, 2025. 7, 8, 9 Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, Deli Zhao, and Hao Chen. WorldVLA: Towards Autoregressive Action World Model. In ArXiv, 2025. 1, 7, 8 Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, and Minzhao Zhu. GR-2: Generative VideoLanguage-Action Model with Web-Scale Knowledge for Robot Manipulation. In ArXiv, 2024. 2 Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, Jiafeng Xu, and Yichu Yang. GR-3 Technical Report. In ArXiv, 2025. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor Policy Learning via Action Diffusion. In RSS, 2023. 7, 8 Open X-Embodiment Collaboration, Abhishek Padalkar, Acorn Pooley, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anikait Singh, Animesh Garg, Anthony Brohan, Antonin Raffin, Ayzaan Wahid, Ben BurgessLimerick, Beomjoon Kim, Bernhard Scholkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, 12 Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Giulio Schiavi, Gregory Kahn, Hao Su, Hao-Shu Fang, Haochen Shi, Heni Ben Amor, Henrik I. Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Joao Silverio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, Krishnan Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Li FeiFei, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Max Spero, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J. Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Pannag R. Sanketi, Paul Wohlhart, Peng Xu, Pierre Sermanet, Priya Sundaresan, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martın-Martın, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, and Zichen Jeff Cui. Open X-Embodiment: Robotic learning datasets and RT-X models. In ICRA, 2024. 2 Can Cui, Pengxiang Ding, Wenxuan Song, Shuanghao Bai, Xinyang Tong, Zirui Ge, Runze Suo, Wanqi Zhou, Yang Liu, Bofang Jia, Han Zhao, Siteng Huang, and Donglin Wang. OpenHelix: Short Survey and Empirical Analysis and Open-Source Dual-System VLA Model for Robotic Manipulation. In ArXiv, 2025. 1, 2, 3, 8, 9 Shengliang Deng, Mi Yan, Songlin Wei, Haixin Ma, Yuxin Yang, Jiayi Chen, Zhiqi Zhang, Taoyu Yang, Xuheng Zhang, Wenhao Zhang, Heming Cui, Zhizheng Zhang, and He Wang. GraspVLA: Grasping Foundation Model Pre-trained on Billion-scale Synthetic Action Data. In ArXiv, 2025. 7, Pengxiang Ding, Han Zhao, Wenjie Zhang, Wenxuan Song, Min Zhang, Siteng Huang, Ningxi Yang, and Donglin Wang. QUAR-VLA: Vision-language-action model for quadruped robots. In ECCV, 2024. 2 Yiguo Fan, Pengxiang Ding, Xinyang Tong Shuanghao Bai, Yuyang Zhu, Hongchao Lu, Fengqi Dai, Wei Zhao, Yang Liu, Zhaoxin Fan Siteng Huang, Badong Chen, and Donglin Wang. Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation. In CoRL, 2025. 2 Chongkai Gao, Zixuan Liu, Zhenghao Chi, Junshan Huang, Xin Fei, Yiwen Hou, Yuxuan Zhang, Yudi Lin, Zhirui Fang, Zeyu Jiang, and Lin Shao. VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models. In ArXiv, 2025. 7, 8 Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR, 2022. 6, 21 Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yenjen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video Prediction Policy: Generalist Robot Policy with Predictive Visual Representations. In ICML, 2025. 8, 9 Chi-Pin Huang, Yueh-Hua Wu, Min-Hung Chen, Yu-Chiang Frank Wang, and Fu-En Yang. ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning. In ArXiv, 2025. 7, 13 Chia-Yu Hung, Qi Sun, Pengfei Hong, Amir Zadeh, Chuan Li, U-Xuan Tan, Navonil Majumder, and Soujanya Poria. NORA: Small Open-Sourced Generalist Vision Language Action Model for Embodied Tasks. In ArXiv, 2025. 7, 8 Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. π0.5: Vision-Language-Action Model with Open-World Generalization. In ArXiv, 2025. 2 Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, and Hang Zhao. Galaxea Open-World Dataset and G0 Dual-System VLA Model. In ArXiv, 2025. 2 Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models. In ICML, 2024. 1, 2, 3, 6 Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Yecheng Jason Ma, Patrick Tree Miller, Jimmy Wu, Suneel Belkhale, Shivin Dass, Huy Ha, Arhan Jain, Abraham Lee, Youngwoon Lee, Marius Memmel, Sungjae Park, Ilija Radosavovic, Kaiyuan Wang, Albert Zhan, Kevin Black, Cheng Chi, Kyle Beltran Hatch, Shan Lin, Jingpei Lu, Jean Mercat, Abdul Rehman, Pannag Sanketi, Archit Sharma, Cody Simpson, Quan Vuong, Homer Rich Walke, Blake Wulfe, Ted Xiao, Jonathan Heewon Yang, Arefeh Yavary, Tony Z. Zhao, Christopher Agia, Rohan Baijal, Mateo Guaman Castro, Daphne Chen, Qiuyu Chen, Trinity Chung, Jaimyn Drake, Ethan Paul Foster, Jensen Gao, Vitor Guizilini, David Antonio Herrera, Minho Heo, Kyle Hsu, Jiaheng Hu, Muhammad Zubair Irshad, Donovon Jackson, Charlotte Le, Yunshuang Li, Kevin Lin, Roy Lin, Zehan Ma, Abhiram Maddukuri, Suvir Mirchandani, Daniel Morton, Tony Nguyen, Abigail ONeill, Rosario Scalise, Derick Seale, Victor Son, Stephen Tian, Emi Tran, Andrew E. Wang, Yilin Wu, Annie Xie, Jingyun Yang, Patrick Yin, Yunchu Zhang, Osbert Bastani, Glen Berseth, Jeannette Bohg, Ken Goldberg, Abhinav Gupta, Abhishek Gupta, Dinesh Jayaraman, Joseph Lim, Jitendra Malik, Roberto Martın-Martın, Subramanian Ramamoorthy, Dorsa Sadigh, Shuran Song, Jiajun Wu, Michael C. Yip, Yuke Zhu, Thomas Kollar, Sergey Levine, and Chelsea Finn. DROID: Large-Scale In-The-Wild Robot Manipulation Dataset. In RSS, 2024. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. OpenVLA: An Open-Source Vision-Language-Action Model. In CoRL, 2024. 1, 2, 3, 6, 7, 8, 9 Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success. In RSS, 2025. 1, 2, 3, 6, 7, 8, 9, 11, 20 Jason Lee, Jiafei Duan, Haoquan Fang, Yuquan Deng, Shuo Liu, Boyang Li, Bohan Fang, Jieyu Zhang, Yi Ru Wang, Sangho Lee, Winson Han, Wilbert Pumacay, Angelica Wu, Rose Hendrix, Karen Farley, Eli VanderBilt, Ali Farhadi, Dieter Fox, and Ranjay Krishna. MolmoAct: Action Reasoning Models that can Reason in Space. In ArXiv, 2025. 7, 8 Xinghang Li, Peiyan Li, Minghuan Liu, Dong Wang, Jirong Liu, Bingyi Kang, Xiao Ma, Tao Kong, Hanbo Zhang, and Huaping Liu. Towards Generalist Robot Policies: What Matters in Building Vision-Language-Action Models. In ArXiv, 2024a. 2, 3, 11 Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, Hang Li, and Tao Kong. Vision-Language Foundation Models as Effective Robot Imitators. In ICLR, 2024b. 2, 8, Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, Nadine Chang, Karan Sapra, Amala Sanjay Deshmukh, 14 Tuomas Rintamaki, Matthieu Le, Ilia Karmanov, Lukas Voegtle, Philipp Fischer, De-An Huang, Timo Roman, Tong Lu, Jose M. Alvarez, Bryan Catanzaro, Jan Kautz, Andrew Tao, Guilin Liu, and Zhiding Yu. Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier VisionLanguage Models. In ArXiv, 2025. 1, 2 Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning. In NeurIPS, 2023a. 2, 4, 6, 7, 8, 10, 18, 20, 21 Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. In NeurIPS, 2023b. 1, Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. RDT-1B: Diffusion Foundation Model for Bimanual Manipulation. In ICLR, 2025. 2 Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In ICLR, 2019. 21 Hao Luo, Yicheng Feng, Wanpeng Zhang, Sipeng Zheng, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, and Zongqing Lu. Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos. In ArXiv, 2025. 2 Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. CALVIN: Benchmark for Language-Conditioned Policy Learning for Long-Horizon Robot Manipulation Tasks. In IEEE RA-L, 2022. 2, 6, 8, 21 NVIDIA, Johan Bjorck, Fernando Castaneda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Jim Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith Llontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. GR00T N1: An Open Foundation Model for Generalist Humanoid Robots. In ArXiv, 2025. 2, 3, 6, 7, 8, 11 Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Poyao Huang, Shangwen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning Robust Visual Features without Supervision. In TMLR, 2024. Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, Ji Hou, and Saining Xie. Transfer between Modalities with MetaQueries. In ArXiv, 2025. 4 William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, 2023. 6, 18 Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. FAST: Efficient Action Tokenization for Vision-LanguageAction Models. In RSS, 2025. 7, 8 Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, and Xuelong Li. SpatialVLA: Exploring Spatial Representations for Visual-Language-Action Model. In ArXiv, 2025. 7, 8 Moritz Reuss, Jyothish Pari, Pulkit Agrawal, and Rudolf Lioutikov. Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning. In ICLR, 2025. 8, 9 Yide Shentu, Philipp Wu, Aravind Rajeswaran, and Pieter Abbeel. From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control. In IROS, 2024. 2, 8, 9 15 Hao Shi, Bin Xie, Yingfei Liu, Lin Sun, Fengrong Liu, Tiancai Wang, Erjin Zhou, Haoqiang Fan, Xiangyu Zhang, and Gao Huang. MemoryVLA: Perceptual-Cognitive Memory in Vision-LanguageAction Models for Robotic Manipulation. In ArXiv, 2025. 1 Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Michel Aractingi Adil Zouitine, Caroline Pascal, Martino Russi, Andres Marafioti, Simon Alibert, Matthieu Cord, Thomas Wolf, and Remi Cadene. SmolVLA: Vision-Language-Action Model for Affordable and Efficient Robotics. In ArXiv, 2025. 2, 3, 6, 7, Haoming Song, Delin Qu, Yuanqi Yao, Qizhi Chen, Qi Lv, Yiwen Tang, Modi Shi, Guanghui Ren, Maoqing Yao, Bin Zhao, Dong Wang, and Xuelong Li. Hume: Introducing System-2 Thinking in Visual-Language-Action Model. In ArXiv, 2025a. 2, 6 Wenxuan Song, Jiayi Chen, Pengxiang Ding, Han Zhao, Wei Zhao, Zhide Zhong, Zongyuan Ge, Jun Ma, and Haoang Li. Accelerating Vision-Language-Action Model Integrated with Action Chunking via Parallel Decoding. In ArXiv, 2025b. 1, 7, 8 Wenxuan Song, Ziyang Zhou, Han Zhao, Jiayi Chen, Pengxiang Ding, Haodong Yan, Yuxin Huang, Feilong Tang, Donglin Wang, and Haoang Li. ReconVLA: Reconstructive Vision-LanguageAction Model as Effective Robot Perceiver. In ArXiv, 2025c. 8, 9 Andreas Steiner, Andre Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, Siyang Qin, Reeve Ingle, Emanuele Bugliarello, Sahar Kazemzadeh, Thomas Mesnard, Ibrahim Alabdulmohsin, Lucas Beyer, and Xiaohua Zhai. PaliGemma 2: Family of Versatile VLMs for Transfer. In ArXiv, 2024. 1, 2 Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An Open-Source Generalist Robot Policy. In RSS, 2024. 2 Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation. In ICLR, 2025. 7, 8, Xinyang Tong, Pengxiang Ding, Yiguo Fan, Donglin Wang, Wenjie Zhang, Can Cui, Mingyang Sun, Han Zhao, Hongyin Zhang, Yonghao Dang, Siteng Huang, and Shangke Lyu. QUART-Online: Latency-Free Large Multimodal Language Model for Quadruped Robot Learning. In ICRA, 2025. 2 Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open Foundation and Fine-Tuned Chat Models. In ArXiv, 2023. 3, 6 Yuqi Wang, Xinghang Li, Wenxuan Wang, Junbo Zhang, Yingyan Li, Yuntao Chen, Xinlong Wang, and Zhaoxiang Zhang. Unified Vision-Language-Action Model. In ArXiv, 2025. 7, 8 An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 Technical Report. In ArXiv, 2024. 3, 6 Yang Yue, Yulin Wang, Bingyi Kang, Yizeng Han, Shenzhi Wang, Shiji Song, Jiashi Feng, and Gao Huang. DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for Efficient Robot Execution. In NeurIPS, 2024. 8, 9 Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image Pre-Training. In ICCV, 2023. 3 Jiahui Zhang, Yurui Chen, Yueming Xu, Ze Huang, Yanpeng Zhou, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, and Li Zhang. 4D-VLA: Spatiotemporal VisionLanguage-Action Pretraining with Cross-Scene Calibration. In ArXiv, 2025a. 7, 8 Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, and Jianyu Chen. HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers. In CoRL, 2024a. 2, Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu, Yu Qiao, Hongsheng Li, and Peng Gao. LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention. In ICLR, 2024b. 5 Wenyao Zhang, Hongsi Liu, Zekun Qi, Yunnan Wang, Xinqiang Yu, Jiazhao Zhang, Runpei Dong, Jiawei He, Fan Lu, He Wang, Zhizheng Zhang, Li Yi, Wenjun Zeng, and Xin Jin. DreamVLA: Vision-Language-Action Model Dreamed with Comprehensive World Knowledge. In ArXiv, 2025b. 1 Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, and Tsung-Yi Lin. SpatialVLA: Exploring Spatial Representations for Visual-LanguageAction Model. In CVPR, 2025a. 7, 8 Tony Z. Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware. In RSS, 2023. 9 Wei Zhao, Pengxiang Ding, Min Zhang, Zhefei Gong, Shuanghao Bai, Han Zhao, and Donglin Wang. VLAS: Vision-language-action model with speech instructions for customized robot manipulation. In ICLR, 2025b. 1, 8, Ruijie Zheng, Yongyuan Liang, Shuaiyi Huang, Jianfeng Gao, Hal Daume III, Andrey Kolobov, Furong Huang, and Jianwei Yang. TraceVLA: Visual Trace Prompting Enhances SpatialTemporal Awareness for Generalist Robotic Policies. In ICLR, 2025. 7, 8 Zhide Zhong, Haodong Yan, Junfeng Li, Xiangchen Liu, Xin Gong, Wenxuan Song, Jiayi Chen, and Haoang Li. FlowVLA: Thinking in Motion with Visual Chain of Thought. In ArXiv, 2025. 2, 7, 8 17 Appendix of VLA-Adapter"
        },
        {
            "title": "A SETUP DETAILS OF LIBERO SIMULATION BENCHMARKS",
            "content": "The LIBERO benchmark (Liu et al., 2023a) comprises four distinct task suites: LIBERO-Spatial, LIBERO-Object, LIBERO-Goal, and LIBERO-100. The first three suites each contain 10 tasks, and LIBERO-100 contains 90 short-term tasks (LIBERO-90) and 10 long-horizon tasks (LIBEROLong). The strategy for each task depends solely on the current instructions provided. Each task is repeated multiple times (50 repetitions in this paper) to obtain the average success rate for each subtask. The examples and the instructions in the LIBERO benchmark are shown in Figure A1. Figure A1: The examples and the task instructions on the LIBERO benchmark. In the LIBERO benchmark, we use third-person images (resolution 2242243, RGB) and wrist images (resolution 2242243, RGB) as visual input. The task instruction is first constructed as prompt in specific format: In: What action should the robot take to instruction.lower()?nOut:, and then input into the VLM module together with the image information. Its output is 7-dimensional action vector, which is used to control the 7-DOF Franka Emika Panda simulated robot arm to perform the corresponding action sequence. DIT-BASED POLICY NETWORK B.1 OVERALL ARCHITECTURE This architecture is shown in Figure B1. It consists of τ -DiT blocks, 1 τ . It has the same number of layers as VLM. Each DiT block consists of three components: conditional modulation, conditional attention, and conditional feedforward network. At timestep t, the input chunk to the first-DiT block, A1 , is noisy action sequence. Since the input contains random noise, to facilitate the transition from pure noise fine-grained prediction, we adopt the AdaLN-Zero layer (Peebles & Xie, 2023) to modulate the activation amplitude at each layer. The AdaLN-Zero consists of LayerNorm, modulation, and gated residual. Specifically, CM will be obtained by Pt and CR ) + σ0(Pt). It is used to generate Scale and Shift vectors, which guide the activation direction of intermediate features and inject automatic modulation amplitude = (cid:2) via gated residual control. After modulation, (cid:101)A (cid:3) is obtained: , i.e., CM = σ 1(CR t+H1 (cid:101)a1 , (cid:101)a t+1, . . . , (cid:101)a1 (cid:101)A1 = A1 +ατ ε(γτ LN(A1 )+βτ ) = A1 +σ 2(CM )ε(σ 1 (2)(CM )LN(A1 )+σ 1 (1)(CM )), [βτ ; γτ ] = σ 1(CM ), 18 (B-1) Figure B1: The DiT-based policy network. ). is element-wise multiplication, and ε() is self-attention and projection modules. where, βτ and γτ are scaling factors and offset factors, which are dynamically generated by CM through projection new with the SiLU. ατ is the gated residual coefficient, used to adjust the injection amplitude. It is dynamically generated by CM through σ0 with the SiLU, and has ατ = σ 3(CM After conditional modulation, (cid:101)A1 are used as the KV vectors for Bridge Attention. The details of Bridge Attention are shown in Section 3.3. And then, the attention latent (cid:98)A1 is input into the conditional feedforward network. The first-DiT block output A2 , which is passed by LayerNorm and MLP layer to generate the current action chunk AM . is obtained. After passing through DiT blocks, we get the (cid:98)AM is used as the QKV vector, and CR will be obtained. (cid:98)A1 , CAQ t B.2 TRAINING OF DIT-BASED POLICY This Policy is also trained from scratch. Given ground truth action trajectory At, noisy action in DiT-based Policy Aτ ατ is the cumulative product of noise 1 ατ ϵ, where, i=1 (1 βi), βi is the variances used at each step, and coefficients, and has ϵ (0, I) is Gaussian noise. We train DiT-based model πθ() with the training objectives: ατ At + i=1 αi = (cid:81)T = ατ = (cid:81)T (θ) = min θ At,ϵN (0,I),CAQ ,σ1(Pt),τ (cid:104)(cid:13) (cid:13)πθ (cid:0) ατ At + 1 ατ , CAQ , σ1(Pt), τ (cid:1) ϵ(cid:13) 2 (cid:13) 2 (cid:105) . (B-2) B.3 BRIEF COMPARISON WITH L1-BASED POLICY As exploring the Policy architecture is not the primary focus of this paper, we briefly compare the performance of the L1-based and DiT-based Policy networks on the LIBERO-Long benchmark. Table B1: Comparison of the L1-based and DiT-based Policy networks of VLA-Adapter. Bold represents the best results. For detailed task instructions, please see Figure A1 in Appendix A. Task instructions 1 L1-based DiT-based 96.0 96.0 2 96.0 92. 3 100.0 98.0 4 98.0 96.0 5 100.0 90.0 100.0 98.0 7 84.0 82.0 8 10 Avg. 96.0 100.0 84.0 74.0 96.0 90.0 95.0 91. 19 Results presented in Table B1 indicate that the L1-based Policy achieves superior performance compared to the DiT-based Policy. This phenomenon coincides with the conclusions in OpenVLA-OFT (Kim et al., 2025): Diffusion-type Policy performs better during pre-training, but L1-based Policy outperforms Diffusion-type Policy during fine-tuning because their actions are less redundant. Furthermore, consistent with findings from OpenVLA-OFT, the L1-based Policy achieves higher throughput compared to the Diffusion-type Policy. Therefore, we chose the L1-based Policy."
        },
        {
            "title": "C DETAILED COMPARISON RESULTS OF DIFFERENT CODITIONS",
            "content": "In this section, we give the specific performance of the ten subtasks on the LIBERO-Long benchmark in Section 3 to explore different conditions. The specific results are shown in Tables C1 and C2. Table C1: The specific performance of different layers of Raw features on 10 subtasks. For detailed task instructions, please see Figure A1 in Appendix A. Bold represents the best performance of the average success rate. Italics* represents the suboptimal performance of the average success rate. Raw feature Single-layer 1 5 9 13 17 21 24 All-layer 124 1 78 82 94 90 82 78 84 92 2 96 94 94 94 92 94 98 3 94 84 84 86 92 98 94 96 4 100 98 94 92 96 90 100 Subtasks 6 5 96 94 90 86 92 68 94 84 98 96 98 100 90 92 100 7 62 68 90 82 66 66 64 76 8 90 94 90 96 72 94 88 9 68 66 74 84 62 78 56 84 10 88 90 90 74 86 88 88 Avg. 87.6 86.6 89.8* 88.4* 84.4 83.2 85.8 90.6 Table C2: The specific performance of different layers of ActionQuery features on subtasks. For task instructions, please see Figure A1 in Appendix A. Bold represents the best performance of the average success rate. Italics* represents the suboptimal performance of the average success rate. ActionQuery feature Single-layer 1 13 17 21 23 24 All-layer 124 1 28 16 90 88 92 92 2 50 52 94 92 98 88 94 3 98 98 86 94 94 100 4 96 94 88 98 96 98 98 Subtasks 6 5 80 94 82 92 96 90 92 100 100 92 100 96 98 7 76 66 74 70 70 74 76 94 98 100 96 98 98 98 9 78 62 58 72 72 84 78 90 86 96 94 82 82 96 Avg. 78.2 76.6 86.8 88.8 89.6* 90.2* 92.6 In Table C1, the middle-layer Raw features generally outperform other-layer Raw features. In Table C2, deep-layer ActionQuery features generally perform better than shallow-layer ActionQuerys. In addition, in Table C1 and Table C2, the performance of all layers is the best. Therefore, in VLAAdapter, we use the Raw feature and ActionQuery feature of all layers as conditions for Policy."
        },
        {
            "title": "D PERFORMANCE ON LIBERO SUBTASKS",
            "content": "In this section, we demonstrate the performance of VLA-Adapter on 40 (410) subtasks on the LIBERO benchmark (Liu et al., 2023a). The detailed performance is shown in Table D1. 20 Table D1: The specific performance of VLA-Adapter on the 40 subtasks of four LIBERO (Liu et al., 2023a) suites. For detailed task instructions, please see Figure A1 in Appendix A. LIBERO 1 3 4 5 6 7 9 10 Avg. Spatial Object Goal Long 98.0 98.0 92.0 96.0 100.0 98.0 100.0 96. 100.0 100.0 98.0 100.0 90.0 100.0 96.0 98.0 96.0 98.0 100.0 100.0 100.0 100.0 98.0 100.0 100.0 98.0 94.0 84.0 100.0 100.0 100.0 96. 98.0 100.0 98.0 84.0 96.0 100.0 96.0 96.0 97.8 99.2 97.2 95."
        },
        {
            "title": "E SETUP DETAILS OF CALVIN SIMULATION BENCHMARK",
            "content": "Benchmark. We used the CALVIN ABCD (Mees et al., 2022) to evaluate the performance on the zero-shot generalization tasks. CALVIN consists of four environments (Env A, B, C, and D). ABCD means it trains on Env A, B, and and evaluates on Env D. These environments collectively include over two million human demonstration trajectories totaling approximately six hours. The CALVIN benchmark contains 34 different subtasks. By screening the combination of five consecutive subtasks, 1,000 unique instruction chains with rationality and diversity are finally generated. In each instruction chain, the agent needs to complete five subtasks in sequence, and can only proceed to the next subtask after successfully completing the current subtask. The benchmark aims to evaluate generalization capabilities and task execution performance under diverse conditions. Examples of each environment and the task instructions are shown in Figure E1. Figure E1: The example and task completion conditions on the CALVIN ABCD. In the CALVIN ABCD benchmark, we use third-person images (resolution 2242243, RGB) and Gripper images (resolution 84843, RGB) as visual input. The task instruction is first conWhat action should the robot take structed as prompt in specific format: In: to {Task instruction}?nOut:, and then input into the VLM module together with the image information. Its output is 7-dimensional action vector, which is used to control the 7-DOF Franka Emika Panda simulated robot arm to perform the corresponding action sequence."
        },
        {
            "title": "F SUPPLEMENTARY DETAILS OF TRAINING AND HYPERPARAMETERS",
            "content": "F.1 TRAINING DETAILS During VLA-Adapter training, we use the AdamW (Loshchilov & Hutter, 2019) optimizer and LoRA scheme (Hu et al., 2022). To ensure the stability of training, the learning rate is set to 1e-4, and the cosine-annealing scheduler with warm-up steps is used. Our batch size is set to 16. 21 Table F1: The detail settings of Training."
        },
        {
            "title": "Batch size\nMax training step\nLearning rate\nWarmup step",
            "content": "value 16 150,000 1e-4 10% F.2 HYPERPARAMETER DETAILS We list the hyperparameters of VLA-Adapter. Their corresponding values are shown in Table F2. Table F2: Specific hyperparameters of VLA-Adapter and their corresponding values. Backbone Layer (τ / ) Number of ActionQuery Hidden size Attention head Action chunk (H) Intermediate layers of VLM Qwen2.5-0.5B 24 64 896 8 8 124 Total trainable parameters of Policy Total trainable parameters of VLA-Adapter 97.3M 197.2M"
        },
        {
            "title": "G EXECUTION EXAMPLES",
            "content": "We provide some execution examples, please see Figure G1 and Figure G2 for details. G.1 REAL-WORLD EXAMPLES These include long-horizon tasks: Pick up the spoon and place it on the cup, then place the cup on the plate and short-horizon tasks: Stack red blocks on top of blue blocks, Move the blue block to the right, and Pick up the duck and place it on plate. The settings of real-world experiments are shown in Section 4.4. 22 Figure G1: Execution example on the real-world tasks. G.2 SIMULATION EXAMPLES Figure G2: Execution example on the LIBERO and CALVIN ABCD tasks."
        },
        {
            "title": "H EFFECTIVENESS ANALYSIS OF FROZEN BACKBONE",
            "content": "Section 4.1 compares the effectiveness of the frozen backbone. The results show that OpenVLAOFT does not work. Although it also uses learnable tokens, it is implemented (line 620 in 3): # === Handle Multimodal Forward === elif (input_ids.shape[0] == pixel_values.shape[0]) or (inputs_embeds.shape[0] == pixel_values.shape[0]): ... # Process action embeddings if noisy_actions is not None: ... else: # Replace the embeddings of the action tokens with zeros # (Later on, the positional embeddings will be added to them) all_actions_mask = all_actions_mask.unsqueeze(-1) input_embeddings = input_embeddings * all_actions_mask The tokens added in the else: (L1 architecture) are input to the VLM in the form of mask. It is initially all zeros, and when the VLM backbone is frozen, it is not trained. Our ActionQuery is: 3https://github.com/moojink/openvla-oft/blob/main/prismatic/extern/hf/ modeling_prismatic.py 24 # === Handle Multimodal Forward === elif (input_ids.shape[0] == pixel_values.shape[0]) or (inputs_embeds.shape[0] == pixel_values.shape[0]): ... # Process action embeddings if noisy_actions is not None: ... else: action_queries = self.action_queries.weight action_queries = action_queries.view(1, action_queries.shape[0], # (1, h) action_queries.shape[1]).repeat(input_embeddings.shape[0], 1, 1) all_actions_mask = self._process_action_masks(labels) input_embeddings = self._replace_input_embeddings(input_embeddings, all_actions_mask, action_queries) Instead of inputting the VLM in the form of mask, essentially learnable tokens (or multiple tokens) that is inserted into the specified position in the sequence and participate in attention. Therefore, when the VLM is frozen, the VL information is indeed not trained. Still, ActionQuery is not an original part of the VLM, and its parameters can be learned from scratch, so the VLA-Adapter still works. Below, we give examples of OpenVLA-OFT and VLA-Adapter, as shown in Figure H1. Figure H1: Execution example when the backbone is frozen."
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications",
        "OpenHelix Team",
        "State Key Laboratory of Networking and Switching Technology",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "Westlake University",
        "Zhejiang University"
    ]
}