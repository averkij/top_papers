{
    "paper_title": "Motif 2 12.7B technical report",
    "authors": [
        "Junghwan Lim",
        "Sungmin Lee",
        "Dongseok Kim",
        "Taehyun Kim",
        "Eunhwan Park",
        "Jeesoo Lee",
        "Jeongdoo Lee",
        "Junhyeok Lee",
        "Wai Ting Cheung",
        "Dahye Choi",
        "Jaeheui Her",
        "Jaeyeon Huh",
        "Hanbin Jung",
        "Changjin Kang",
        "Beomgyu Kim",
        "Minjae Kim",
        "Taewhan Kim",
        "Youngrok Kim",
        "Hyukjin Kweon",
        "Haesol Lee",
        "Kungyu Lee",
        "Dongpin Oh",
        "Yeongjae Park",
        "Bokki Ryu",
        "Dongjoo Weon"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 4 6 4 7 0 . 1 1 5 2 : r Motif 2 12.7B technical report Motif Technologies"
        },
        {
            "title": "Abstract",
            "content": "We introduce Motif-2-12.7B, new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models. Hugging Face: https://huggingface.co/Motif-Technologies Chat service: https://chat.motiftech.io/"
        },
        {
            "title": "Introduction",
            "content": "Recent years have seen rapid progress in large language models (LLMs), with both proprietary and open-weight systems such as GPT-5 [34], Claude-4 [3], Grok-4 [50], Gemini-2.5 [10], Qwen-3 [52], DeepSeek-r1 [15], and Kimi K2 [45] achieving remarkable results across diverse domains, including mathematics, programming, STEM reasoning, medicine, and law. The scale and breadth of these systems have begun to redefine the boundaries of what language models can represent and reason about, signaling shift from pattern recognition toward more structured and analytical forms of intelligence. Despite these advances, research on competitive capability under constrained model sizes and limited computational resources remains relatively sparse. Understanding how to push the limits of LLMs ability within efficiency constraints has emerged as an important challenge for the next generation of language models. We introduce Motif-2-12.7B(-Base/Instruct), an open-weight foundational language model designed to support instruction following, multi-step reasoning, and general-purpose language understanding. At its core is Grouped Differential Attention (GDA) [26], new mechanism that reorganizes attention computation to better capture multi-granular token interactions while preserving scalability. Motif-2-12.7B achieves competitive results across broad range of reasoning and instructionfollowing tasks, showing that architectural innovation and effective training strategies can lead to meaningful improvements even without relying on extremely large parameter sizes. With its open availability and transparent training methodology, Motif-2-12.7B would serve as strong baseline for future research on scalable attention and efficient reasoning. Compared to its predecessor Motif-2.6B [27], Motif-2-12.7B represents significant evolution in scale and training methodology. It is obtained by upscaling Motif-2.6B and pre-training on substantially larger and more diverse corpus, yielding broader linguistic coverage and deeper knowledge representation across general English, Question Answering, chat, multilingual, scientific, mathematical, and coding domains. Most importantly, we introduce Grouped Differential Attention (GDA), which enhances the models capacity to capture complex token dependencies and improves efficiency without additional computational cost. Complemented by strengthened data curation (rigorous filtering, deduplication, novel synthetic data generation, and domain-balanced sampling) and an expanded token budget, Motif-2-12.7B exhibits consistently stronger performance on standard evaluations while preserving continuity with Motif-2.6B. Pre-training of Motif-2-12.7B-Base was conducted over corpus total 5.5 trillion tokens, designed to support scalable reasoning and generalization. We employed linear curriculum scheduler to modulate the data composition throughout training. Early phases consisted predominantly of generaldomain English text, with gradual and controlled increase in the proportion of STEM, mathematical, and programming data. This approach encouraged early convergence on core linguistic fluency while promoting progressive adaptation to more structured reasoning tasks. Optimization was carried out using the MuonClip optimizer [45], which was chosen for its adaptive gradient scaling properties under large-batch conditions. To fully leverage hardware efficiency, we implemented set of high-performance parallelism kernels for optimizing MuonClips update rule, significantly improves throughput and stability for long-sequence training. This combination of curriculum-aware data scheduling, specialized optimization, and systems-level acceleration enabled us to scale pretraining effectively without compromising convergence quality or compute efficiency. Following pre-training, Motif-2-12.7B-Instruct further went through post-training phase consisting of three-stage supervised fine-tuning (SFT) to enhance its general instruction-following and domainspecific abilities. The process began with large-scale alignment on diverse instruction datasets to establish broad conversational competence, then progressively incorporated curated and synthetic data emphasizing compositional reasoning, algorithmic thinking, mathematics, and code generation. In the final refinement stage, selective data pruning was applied to improve diversity and maintain linguistic fluency. Through this gradual and data-driven approach, Motif-2-12.7B-Instruct developed stronger generalization and more coherent reasoning behaviors across wide range of tasks. Organization of the technical report. Section 2 introduces the architectural design of Motif-212.7B, detailing its scaling methodology and integration of key components. Section 3 describes the pretraining data composition, training procedure, and evaluation results of the base model. Section 4 presents system-level optimizations developed to improve large-scale training efficiency. Section 5 outlines the three-stage supervised fine-tuning pipeline used to construct the Motif-2-12.7B-Instruct model, along with its corresponding evaluation results. Finally, Section 6 concludes the report with summary of findings and future directions."
        },
        {
            "title": "2 Architecture",
            "content": "To expand model capacity while preserving the pretrained representation space, we employed width expansion strategy that scales internal dimensions without altering the models learned function. Motif-2-12.7B was derived from Motif-2.6B through the Scaling Smart Hypercloning procedure [38], in which the model is scaled by an integer multiple through direct weight replication. This method enlarges the network while strictly preserving the original parameter topology and initialization statistics, thereby ensuring functional continuity between generations. Rather than re-initializing or retraining new components from scratch, hypercloning reproduces the pretrained 2 Model size Hidden dimension Number of layers Feed-Forward dimension Number of heads Number of KV heads Number of Noise heads"
        },
        {
            "title": "Attention Mechanism\nActivation Function\nMax Sequence Length\nPositional Embeddings\nVocab size\nTied word embedding",
            "content": "12.7B 4,096 40 16,384 40 16 8 Grouped Differential Attention PolyNorm Activation 32,768 RoPE (Î¸ = 1, 000, 000) 219,520 False Table 1: The detailed hyper-parameters of Motif-2-12.7B. activation tensors exactly, allowing the expanded model to inherit both the structural and representational properties of Motif-2.6B. To improve representational disentanglement and enhance fine-grained control over information routing within attention, we incorporated the Grouped Differential Attention (GDA) mechanism [26] during the hypercloning stage. This mechanism enables different subsets of attention heads to specialize in either amplifying salient signals or suppressing residual noise, thereby improving the overall quality and efficiency of learned representations. During this stage, we integrated GDA directly into the cloned architecture, organizing the attention heads into asymmetric groups with 4:1 ratio between signal-preserving and noise-control groups, respectively. Determining this ratio was critical design choice, as the GDA mechanism requires balancing the proportion of signal and noise heads for optimal performance. Through preliminary experiments exploring various configurations, we empirically found that 4:1 ratio provided the best balanceallocating more heads to salient information propagation while maintaining stability and regularization through the smaller noisecontrol group. This integration represents the first large-scale application of GDA within the Motif architecture, validating its scalability and effectiveness in production-scale training. Following the width-preserving hypercloning and GDA integration, we expanded the model depth using the LLaMA-Pro framework [49]. This step increased the number of layers in structurally consistent manner while preserving RMS normalization [55], Rotary Positional Embedding [41], and cross-layer residual scaling inherited from Motif-2.6B. The activation PolyNorm [56] and the tokenizer from Motif-2.6B were retained without modification. The resulting architecture, referred to 2 in the middle of its name Motif-2-12.7B, therefore represents continuous and functionally extended successor of its predecessor Motif-2.6B. The detailed architectural configurations are summarized in Table 1."
        },
        {
            "title": "3 Pre-training",
            "content": "This section provides an overview of the pre-training data construction, describes the pre-training methodology, and presents benchmark evaluations of the base models. 3.1 Pre-training Data We constructed large-scale and high-quality pre-training corpus for Motif-2-12.7B by aggregating broad spectrum of open-source and in-house datasets spanning diverse domains and modalities. The corpus encompasses general web data (e.g., Nemotron-CC [40], Wikipedia, Stack Exchange [43], and Facebook Recycling the Web [33]), multilingual and Korean-language resources (Fineweb2 [35], Translated DiverseQA [40], and extended in-house Korean corpus), as well as specialized domains such as reasoning and knowledge-intensive tasks (DiverseQA [40], Synthetic QA Dataset), scientific and mathematical content (FineWeb2 [35], OctoThinker MegaMath [47], Yulan Math [19], arXiv), and code-centric corpora (OpenCoder-LLM Dataset [20], Yulan Code [19]). 3 The model was trained on 5.5 trillion tokens drawn from this corpus, encompassing general English, question answering, conversational, multilingual, scientific, mathematical, and programming domains. This large-scale training setup provides balanced and robust foundation for advancing both linguistic and reasoning capabilities. 3.2 Pre-training Stage and Recipe The pre-training of the Motif-2-12.7B series followed three core strategies designed to balance efficiency, diversity, and reasoning capability. Dataset Mixture Scheduling We adopted progressive dataset mixture scheduling strategy, similar to that used in Motif-2.6B. The early training phase primarily consisted of English web-scale corpora, while reasoning, mathematical, and code-related datasets were introduced in small proportions. Their relative ratios were then gradually increased over the course of training. The dataset mixture was dynamically adjusted at each training step, analogous to how learning-rate scheduler modulates optimization dynamics. This approach enabled smooth transition from general linguistic learning to higher-order reasoning without destabilizing early-stage convergence. Large-Batch Training Training was conducted with large effective batch sizes, starting from 16 million tokens and progressively scaling up to 80 million tokens. We employed the Muon-Clip optimizer, originally introduced in the Kimi K2 [45], and further developed an internally optimized variant tailored for parallel and distributed training settings. This enhanced Muon implementation features improved gradient stability, communication efficiency, and large-batch scalability, which together enable high-throughput optimization under multi-node environments. Details of the parallel and distributed optimization design are provided in later section. Reasoning Annealing Stage In the final annealing phase, we gradually increased the proportion of reasoning data while capping it below 10% of total tokens to prevent overfitting and mode collapse toward narrow reasoning distributions. Notably, within this reasoning subset, mathematical data were weighted more heavily than code data, which is deliberate choice aimed at strengthening symbolic abstraction and quantitative reasoning. Although this allocation is unconventional, we found it conducive to improved general reasoning robustness without degrading linguistic fluency. We used peak learning rate of 4 104, which was decayed to approximately 10% of the value using the WarmupStableDecay scheduler [18]. The annealing stage was conducted over roughly 1 trillion training tokens, during which the maximum sequence length was gradually increased from 4,096 to 16,384 tokens toward the end of training to enable smooth adaptation to longer-context understanding. 3.3 Pre-training Evaluation Evaluation Setup To evaluate the performance of the Motif-2-12.7B-Base, we conducted comprehensive evaluation across multiple capability areas. We compared the models developed in this study against two leading open-source foundation models, Qwen3 and Gemma3 [44]. These two models represent state-of-the-art open weight models similar scale and learning methods. To ensure fair and consistent comparison, we used the officially reported scores from each models technical report. All evaluations for Motif-2-12.7B-Base were conducted using greedy decoding. General Knowledge We evaluate benchmarks such as MMLU [16], MMLU-Redux [13], MMLU-Pro [46], BBH [42], ARC-Challenge, and ARC-Easy [8] to measure the models breadth of factual knowledge, general reasoning ability. Motif-2-Base achieves solid performance on the MMLU and MMLU-Redux, surpassing Gemma-3 12B and approaching Qwen-3 14B/32B levels. Notably, it records the highest score on MMLU-Pro highlighting strong reasoning capability. Math & Scientific Reasoning GSM8K [9], MATH [17], GPQA, GPQA-Diamond [36], and SuperGPQA [11] are used to assess mathematical reasoning, numerical precision, and scientific understanding requiring multi-step logical inference. Motif-2-Base exhibits clear strength, recording 94.9 on GSM8K and 73.6 on MATH, exceeding all open-weight baselines of comparable scale. It also attains competitive results on GPQA and SuperGPQA, showing superior performance among models of similar scale. 4 Code Tasks HumanEval [6], MBPP [4], EvalPlus [30], and CRUX-O [14] evaluate code generation, reasoning about programs, and functional correctness through test-based execution. Motif-2Base leads with 65.9 on HumanEval and 81.5 on MBPP, establishing itself as one of the strongest code-capable models among non-proprietary peers. Common-sense HellaSwag [54], BoolQ [7], PIQA [5], SIQA [39], WinoGrande [37], DROP [12], TriviaQA [22], and Natural Questions [23] are employed to test everyday reasoning, contextual comprehension, and the models ability to infer causal or pragmatic relationships in natural language. Although slightly lower than larger counterparts, the scores remain competitive and do not show significant degradation. Evaluation Results Across diverse benchmarks spanning knowledge, reasoning, coding, and commonsense understanding, Motif-2-12.7B-Base demonstrates strong overall competitiveness with leading open-source foundation models. Overall, Motif-2-12.7B-Base outperforms both Gemma-3 12B and 27B, as well as Qwen models of comparable size (14B), while approaching the performance of much larger variants such as Qwen-3 32B. These results confirm that Motif-212.7B-Base stands as high-performing and broadly capable foundation model among open-weight models. Benchmark General knowledge MMLU MMLU-Redux MMLU-Pro BBH ARC-Easy ARC-Challenge Math & Scientific Reasoning GSM8k MATH GPQA GPQA-Diamond SuperGPQA Coding Tasks HumanEval MBPP EvalPlus CRUX-O Common Sense HellaSwag BoolQ PIQA SIQA DROP TriviaQA Natural Questions WinoGrande Average Metric Motif-2 12.7B Gemma3 12B 27B Qwen2.5 14B 32B Qwen3 32B 30B-A3B 14B 5-shot 5-shot 5-shot, CoT 3-shot, CoT 0-shot 25-shot 4-shot, CoT 8-shot, CoT 4-shot, CoT 5-shot, CoT 5-shot, CoT 5-shot, CoT 0-shot 3-shot 0-shot 1-shot 10-shot 0-shot 0-shot 0-shot 1-shot 5-shot 5-shot 5-shot 78.1 78.68 66.38 81.34 84.1 69.6 93.85 94.92 73.62 42.18 42.92 32. 65.9 81.5 72.22 63.1 84.0 78.5 81.6 53.8 69.9 72.2 29.6 79.6 74.5 - 45.3 72.6 88.3 68.9 - 71.0 43.3 - 25.4 - 48.8 60.4 - - 84.2 78.8 81.8 53.4 72.2 78.2 31.4 74. 78.6 - 52.2 77.7 89.0 70.6 - 82.6 50.0 - 24.3 - 45.7 65.6 - - 85.6 82.4 83.3 54.9 77.2 85.5 36.1 78.8 79.66 76.64 51.16 78.18 - - 90.22 - 55.64 32.83 - 30. - 69.0 60.7 61.1 - - - - - - - - 83.32 81.97 55.1 84.48 - - 92.87 - 57.7 47.97 - 33.55 - 73.6 66.25 67.8 - - - - - - - - 81.05 79.88 61.03 81.07 - - 92.49 - 62.02 39.9 - 34.27 - 73.4 72.05 68.6 - - - - - - - - 83.61 83.41 65.54 87.38 - - 93.40 - 61.62 49.49 - 39. - 78.2 72.23 72.5 - - - - - - - - 81.38 81.17 61.49 81.54 - - 91.81 - 59.04 43.94 - 35.72 - 74.4 71.45 67.2 - - - - - - - - 71.53 69.42 63.87 - 67.96 - - 62.35 - 67.69 - 67. - 71.54 - 68.10 Table 2: Performance comparison across Motif-2, Gemma3, Qwen2.5, and Qwen3 families. Bold indicates the highest score per row; underline indicates the second-highest."
        },
        {
            "title": "4 Training System Optimization",
            "content": "In this section, we describe how training efficiency was significantly enhanced through the integration of novel kernel fusion and parallel muon techniques. 5 Operation Forward Speedup Backward Speedup vs Naive vs torch.compile vs Naive vs torch.compile PolyNorm PolyNorm + Elementwise Multiplication 30.29 23.38 1.53 1. 43.90 27.92 4.77 3.33 Table 3: Performance comparison of fused kernels against baseline implementations on an H200 GPU. All experiments use BF16 precision under PyTorch 2.8. Benchmarks were conducted with hidden sizes of 8K and 16K, sequence lengths from 1K to 8K, and batch sizes between 1 and 4. Reported speedups are geometric means across all configurations. Naive refers to direct implementation using standard PyTorch APIs. 4.1 Infrastructure and Precision We employed SkyPilot [53] as the orchestration framework, enabling seamless multi-node and multi-cluster provisioning, and unified resource abstraction across multiple compute environments. For the core training stack, we adopted TorchTitan [24], PyTorch-native distributed training platform that provides modular 3D/4D parallelism (data, tensor, pipeline, and context), advanced optimizations such as FP8 all-gather and activation checkpointing. Pretraining was performed on 400 H100 gpus for approximately 272K hours, using FP8 precision to maximize computational efficiency. For FP8 training, we applied row-wise scaling and keeps gradients in BF16 precision during the backward pass. 4.2 Kernel Fusion During training, we implemented PolyNorm, polynomial activation function introduced at Motif2.6B [27], as fused CUDA kernel and further fused it with the subsequent elementwise multiplication in the FeedForward layer. PolyNorm is composed of lightweight sub-operationselementwise scaling, reduction, and normalizationthat are typically memory-bound and hence benefit from kernel fusion, which reduces redundant memory traffic and kernel launch overhead. torch.compile 1 is high-level API that enables graph-level optimization and automatic GPU kernel fusion with almost no code modification. While it provides substantial performance gains, we observed that hand-written fused CUDA kernels can still deliver superior throughput. Table 3 compares our fused CUDA kernels against both the naive PyTorch implementation and the torch.compile-optimized version. While torch.compile already provides substantial accelerationyielding up to 12.9 speedup over the naive baselineour hand-written fused kernels achieve even higher throughput in both forward and backward passes. This demonstrates that manual fusion can further exploit the remaining optimization headroom, particularly for normalization layers dominated by small, memory-bound operations. The full implementation of Fused PolyNorm kernels are available at https://huggingface.co/Motif-Technologies/activation. 4.3 Parallel Muon Liu et al. introduced Distributed Muon [31], which applies ZeRO-1based parameter partitioning scheme to the Muon algorithm. Unlike element-wise optimizers such as AdamW, Muon requires access to the full gradient matrices to perform the NewtonSchulz iterations. Distributed Muon addresses this by performing an all-gather to reconstruct the full matrix before each NewtonSchulz iteration, computing the update locally, and then discarding all shards except the one that belongs to each rank. While this approach resolves the full-matrix dependency of Muon, it does not parallelize the NewtonSchulz computation itself; as result, the iteration steps are redundantly executed on all ranks. There exist several ideas and implementations that aim to overcome the limitations of Distributed Muon. One notable approach is from Essential AI2, where they propose employing All-to-All communication to re-shard gradients across ranks, thereby eliminating the redundant computations inherent in Distributed Muon. 1https://docs.pytorch.org/docs/stable/generated/torch.compile.html 2https://www.essential.ai/research/infra 6 Figure 1: Illustration of the All-to-All gather and scatter process in Parallel Muon. Each rank exchanges its sharded gradients with all other ranks during the gather phase and redistributes the computed results in the scatter phase. We began developing our own implementation of Muon, extending Essential AIs idea with pipelining and general sharding support (e.g., TP + HSDP), which we refer to as Parallel Muon. Parallel Muon distributes computational workloads across shard ranks and executes them concurrently. In addition, it schedules communication and computation operations to fully utilize the accelerators. While some existing implementations of Muon support FSDP, none provide support for hybrid parallel configurations. full all2all gather(recv=G, send=G G) Divide into subsets and assign to each rank Algorithm 1 Parallel Muon Require: DP sharded gradients G, DP sharded parameters P, rank 1: // 1. 2: assign(G, r) 3: // 2. All2All communicate to gather across DP shard group 4: // As result, each rank get unsharded matrices of 5: 6: // 3. Calculate Newton-Schulz of unsharded gradients assigned to rank 7: 8: // 4. All2All communicate to scatter 9: // As result, earch rank get sharded matrices of 10: all2all scatter(recv=U 11: // 5. 12: apply update(P, U) 13: return other, send=U Apply DP sharded to full across DP shard group full Newton-Schulz(G full) full) full Algorithm 1 describes the parallel version of Muon under Fully Sharded Data-Parallel (FSDP) training. Each rank first takes subset of gradients and then participates in an All2All communication to gather the full unsharded gradients assigned to it. The NewtonSchulz iteration is then applied independently on each gathered subset to compute the update matrices. The resulting unsharded updates are scattered back to the corresponding ranks via another All2All operation, producing sharded update tensors. Finally, each rank applies its local update to the corresponding sharded parameters. This procedure enables concurrent computation across DP-shard ranks without additional memory overhead. All-to-All Gather and Scatter As discussed earlier in Essential AIs idea, Parallel Muon also employs All-to-All communication to redistribute both sharded and unsharded gradients across ranks. The All-to-All gather operation collects the sharded gradients so that each rank obtains its assigned unsharded gradients, while the All-to-All scatter operation redistributes the computed results back to all ranks, as illustrated in Figure 1. Pipelined Execution Although Algorithm 1 is sound on its own, pipelining its execution enables better performance through communicationcomputation overlap. As illustrated in Figure 2, we group the parameters into chunks and schedule their all-to-all gather, computation, and scatter phases in pipelined fashion. Specifically, we first issue the gather and computation operations for few initial chunks to warm up the pipeline. Then, for each subsequent chunk, we overlap scatter of the previous results, gather of the next inputs, and the computation of the current chunk. 7 Figure 2: Illustration of pipelined execution of all-to-all gather, computation, and scatter phases. total of eight gradients are distributed across two ranks and partitioned into chunks of size two. Computation and communication are scheduled in pipelined manner, allowing overlap between them and improving overall efficiency. This scheduling can hide communication latency by maintaining steady flow of computation and communication, thereby improving throughput and reducing idle time. In addition to performance improvements, pipelining also helps reduce peak memory usage. Since the full output matrices produced by each computation can only be deallocated after their corresponding scatter phase completes, non-pipelined execution must keep multiple full matrices in memory simultaneously. By overlapping computation and scatter, the pipelined schedule allows completed full matrices to be released earlier, thus lowering the overall memory footprint. Chunk Size Trade-off While pipelining improves both performance and memory efficiency, one important consideration is the choice of chunk size. smaller chunk size allows finer-grained overlap between computation and communication, which tends to improve performance and reduce memory usage. However, all-to-all communication becomes less efficient when messages are too small, making larger chunks more favorable for bandwidth utilization. In practice, the benefit from improved bandwidth utilization with larger chunks often outweighs the performance gain from finer overlap, resulting in an optimal chunk size that balances both effects. We empirically find that chunk size of 32 achieves the best overall trade-off between performance and memory usage. Mitigating Rank Imbalance To mitigate workload imbalance among ranks, we balanced the total computational load assigned to each rank. Because the all-to-all communication in gather and scatter stages requires synchronization among all participating ranks, any imbalance in computation time would lead to idle waiting and reduced throughput. We therefore sorted all gradients according to the number of floating-point operations (FLOPs) required for their respective NewtonSchulz computations and assigned them to ranks in round-robin fashion. This simple yet effective heuristic ensures that each rank receives roughly equal total workload, reducing idle time and improving parallel efficiency. While this strategy does not guarantee perfectly balanced execution, it provides good practical trade-off between load balance and scheduling overhead. Support for Hybrid Parallel Configurations To support various strategies beyond FSDP, such as TP or TP + HSDP, Parallel Muon dynamically inspects the placement of gradient tensors at runtime, constructs corresponding subprocess groups for gather and scatter operations, and computes the shard mapping that defines which tensor slice is held by each rank. Since Parallel Muon preserves the semantics of the original NewtonSchulz iterations, it directly leverages existing kernel optimizations, including Flash Muon [29]. The full implementation of Parallel Muon is available at https://huggingface.co/Motif-Technologies/optimizer. Parallel Muon Evaluation We evaluate the performance of Parallel Muon under various configurations and compare it against the Distributed Muon baseline. All experiments are conducted on 8 single node equipped with eight NVIDIA H200 GPUs, using Fully Sharded Data Parallel (FSDP) with eight ranks. The model used for evaluation is Motif-2-12.7B, and all communication and GEMM operations are performed in BF16 precision. We consider the following configurations: 1. Distributed Muon the baseline implementation, extended with Flash Muon[29] for fair comparison with Parallel Muon. Distributed Muon reconstructs the full parameter matrix via all-gather and redundantly executes the NewtonSchulz iterations on all ranks. 2. Parallel Muon (non-pipelined) our implementation of Parallel Muon without pipelining, where computation and communication are executed sequentially. 3. Parallel Muon (pipelined) Parallel Muon with pipelined gathercomputescatter scheduling to enable communicationcomputation overlap. 4. Parallel Muon (pipelined + FLOPs-sorted) the full version, which additionally applies FLOPs-based gradient sorting and round-robin scheduling to balance computation across ranks. We measure throughput, step time, and peak memory usage for each configuration to quantify the impact of pipelining and load balancing. Table 4 summarizes the performance of Distributed Muon and the different Parallel Muon configurations. First, simply adopting the non-pipelined Parallel Muon configuration (row 2) achieves substantial performance of 571 Tera floating operations per second per GPU (TFLOPS/GPU), 7.1 throughput improvement over the baseline Distributed Muon (80 TFLOPS/GPU). This significant gain is attributed to distributing the computational workload across the 8 GPUs. Interestingly, introducing pipelining (row 3) initially results in performance degradation, with throughput dropping by approximately 15.8% compared to the non-pipelined version. This is caused by workload imbalance, as the per-chunk all-to-all communication (for the small, size 32 chunks) acts as synchronization barrier across ranks. This is strongly supported by the results from the fourth configuration: applying FLOPs-based parameter sorting (row 4) rectifies this imbalance, boosting performance significantly by over 21%. It is also important to note that the throughput of the fully optimized (pipelined + sorted) version is only marginally higherby about 2.1%than that of the non-pipelined version. This relates to the chunk size trade-off; the non-pipelined approach effectively uses single, large chunk (equal to the total number of parameters), thus benefiting from high bandwidth utilization on its single communication step while also avoiding the repeated synchronization overheads inherent to finegrained pipelining. Despite this small throughput difference, the primary benefit of the pipelined approach is its drastically lower memory footprint. The non-pipelined versions peak memory usage is over 4.1 times that of the pipelined (non-sorted) configuration and over 3 times that of the fully optimized version, making pipelining crucial optimization for memory efficiency. Another open-sourced Muon implementation example3 is provided in Dion [1]. This work is also inspired by Essential AIs approach, and its key idea is to avoid the full-matrix reconstruction required by Muon. However, this implementation does not support hybrid configurations such as FSDP2 combined with tensor parallelism (TP). Table 4: Performance comparison of Distributed Muon and Parallel Muon configurations on 8H200 GPUs. All experiments use BF16 precision under FSDP. Configuration Pipelining Sort Param. Chunk Size Time (ms) Peak Mem (MB) TFLOPS per GPU Distributed Muon Parallel Muon Parallel Muon Parallel Muon X 32 32 1574.67 221.27 262.83 216.46 832 11904 2894 3904 80 571 481 583 3https://github.com/microsoft/dion/blob/main/dion/muon.py"
        },
        {
            "title": "5 Post-training",
            "content": "5.1 Supervised Fine-tuning Following the completion of pre-training, Motif-2-12.7B-Instruct went through three-stage supervised fine-tuning (SFT) process designed to enhance its instruction-following, reasoning, and domain specialization capabilities. Unlike Motif-2.6B, which primarily focused on general instruction adherence, each stage explicitly targeted multi-step reasoning, code synthesis, and mathematical problem-solving through carefully curated and synthesized datasets. The three-stage SFT pipeline progressively refines the models capabilitiesfrom large-scale general instruction learning (Stage 1), to reasoningand domain-oriented enhancement through synthetic data (Stage 2), and finally to data-pruned targeted refinement (Stage 3). 5.1.1 Stage 1: Large-Scale Supervised Fine-tuning The first fine-tuning stage focused on establishing strong general instruction-following performance through large-scale training. We trained on approximately 28 million samples, drawn from both open-source and proprietary datasets, with domain ratios adjusted to maintain balanced representation across categories. Training was conducted with sequence packing up to 16,384 tokens to enable efficient utilization of long-context information. All parameters were unfrozen during this stage, allowing full adaptation of model representations to downstream supervision. Training Setup Training was performed using the Muon-Clip optimizer with base learning rate of 2 105, weight decay of 0.1, and global batch size of 32 million tokens. cosine learning rate scheduler was employed with warm-up phase corresponding to 5% of the total training steps, followed by smooth decay toward zero. Sequence packing was enabled up to maximum sequence length of 16,384 tokens to ensure efficient utilization of GPU memory and to improve long-context training stability. All training was conducted in FP8 precision under Fully Sharded Data Parallel (FSDP) with the Parallel Muon optimization kernels as described in Section 4.3, thereby enabling stable and high-throughput large-batch training. 5.1.2 Stage 2: Synthetic and Targeted Fine-tuning To further enhance higher-order reasoning and domain-specific capabilities, the second fine-tuning phase employed mixture of open and synthetic instruction datasets. We first consolidated diverse mixture of high-quality open instruction datasets spanning general conversation, STEM, mathematics, and code generation. Each dataset was evaluated based on reasoning density, response completeness, and structural consistency. The resulting dataset was designed to maintain domain balance while providing comprehensive coverage of general instruction-following and structured reasoning tasks. Building upon this foundation, we introduced additional synthetic data generated from both internal and external models to strengthen compositional reasoning, algorithmic problem-solving, and multistep instruction-following performance. The synthetic datasets consisted of: Compositional Reasoning: Synthetic examples designed to enhance multi-step logical reasoning and structured task decomposition. Algorithmic and Mathematical Skills: Data emphasizing algorithmic reasoning, code generation, and quantitative problem-solving. Quality-Centric Selection: High-quality samples curated through multi-stage filtering and automatic quality scoring. This stage adopted lower learning rate and shorter training schedule relative to Stage 1, functioning as form of curriculum continuation. Through the combination of large-scale instruction data and carefully designed synthetic augmentation, Motif-2-12.7B-Instruct achieved significant improvements in reasoning quality while preserving conversational fluency. 10 5.1.3 Stage 3: Data-Pruned Refinement In the final stage, we conducted focused fine-tuning process by selectively excluding subset of the Stage 2 synthetic data identified as redundant or low-utility. This data-pruned refinement stage aimed to reinforce domain diversity and reduce potential overfitting to synthetic distributions by re-training on carefully filtered subset of Stage 2 data. The pruning criteria emphasized diversity, reasoning coherence, and linguistic fidelity, resulting in leaner but higher-quality dataset. This stage served as lightweight continuation of Stage 2, stabilizing the models reasoning and alignment behavior without altering its general conversational fluency. 5.2 Post-training Evaluation Evaluation Setup Following the same evaluation protocol as Motif-2-12.7B-Base, we conducted comprehensive assessment of Motif-2-12.7B-Instruct to evaluate its generalization, reasoning, and instruction-following capabilities. To ensure comparability and fairness, we benchmarked our model against strong open-weight baselines, including the Qwen3 and Gemma3 series, using their officially reported scores from the respective technical reports. All evaluations for Motif-2-12.7B-Instruct were performed with sampling temperature of 0.6 and maximum sequence length of 32,768 tokens. Benchmark Composition Our evaluation suite spans wide range of domains to measure both breadth and depth of model ability. The general task benchmarks include MMLU [16], MMLURedux [13], GPQA-Diamond [36], LiveBench [48], and IFEval [32], which collectively test factual knowledge, reasoning, and alignment quality across diverse subjects. The mathematics and text reasoning category comprises MATH, MATH-500 [25], AIME24, AIME25 [2], ZebraLogic [28], and BBH [42], emphasizing symbolic reasoning, structured problem-solving, and compositional understanding. Finally, agentic and programming-oriented benchmarks, including BFCL v3 [51], LiveCodeBench [21], MBPP [4], and HumanEval [6], assess multi-step reasoning, code synthesis, and executional accuracy in practical scenarios. This diverse set of tasks allows us to holistically evaluate the instruction-following ability and analytical robustness of Motif-2-12.7B-Instruct across both linguistic and technical domains. Evaluation Results Detailed quantitative results are summarized in Table 5 and Table 6. Overall, Motif-2-12.7B-Instruct achieves performance comparable to or exceeding that of similarly sized or larger open-weight models, despite being trained on substantially smaller dataset. Notably, without relying on computationally heavy and reasoning targeted reinforcement learning, the model demonstrates strong results on high-difficulty reasoning and code-oriented benchmarks such as MATH500, AIME25, and LiveCodeBench, underscoring the effectiveness of its three-stage SFT curriculum. While Qwen3 and Gemma3 models benefit from significantly larger training corpora, with Qwen3 trained on 36T tokens and Gemma3 on 12 to 14T tokens, our model attains similar or higher accuracy across several benchmarks using only 5.5T tokens, highlighting the data efficiency and architectural effectiveness of our approach. The integration of Grouped Differential Attention (GDA), curriculum-aware data scheduling, and high-throughput optimization via Parallel Muon collectively Benchmark Metric Motif-2 Qwen2. Qwen3 12.7B 72B 14B 14B 32B 32B Instruct Non-Think Non-Think Think Non-Think Think Non-Think 30B-A3B 30B-A3B Think MMLU-Redux GPQA-Diamond LiveBench 2024-11-25 MATH-500 AIME24 AIME25 ZebraLogic BFCLv3 LiveCodeBench v5 (2024.10 - 2025.2) IFEval Average 0-shot 0-shot, CoT - 0-shot 0-shot 0-shot - - 0-shot strict prompt 90.02 63.6 33. 96.8 72.3 63.6 69.5 55.34 50.03 75.78 67.08 86.8 49 51. 83.6 18.9 15 26.6 63.4 30.7 84.1 82 54.8 59.6 90 31.7 23.3 33 61. 29 84.8 88.6 64 71.3 96.8 79.3 70.4 88.5 70.4 63. 85.4 85.7 54.6 59.8 88.6 31 20.2 29.2 63 31.3 83. 90.9 68.4 74.9 97.2 81.4 72.9 88.8 70.3 65.7 85 84.1 54. 59.4 89.8 32.8 21.6 33.2 58.6 29.8 83.7 89.5 65.8 74. 98 80.4 70.9 89.5 69.1 62.6 86.5 50.95 54.97 77. 54.66 79.55 54.78 78.66 Table 5: Performance comparison across Motif-2, and Qwen families. Benchmark Metric Motif-2 Gemma3 MMLU BBH GSM8k MATH MBPP IFEval LiveCodeBench v5 HumanEval Average 0-shot 0-shot 0-shot, CoT 0-shot 3-shot soft prompt 0-shot 0-shot 12.7B 86.11 85.78 96.13 97 91 76.52 61.66 93.2 12B 71.9 85.7 94.4 83.8 73 88.9 32 85.4 27B 76.9 87.6 95.9 89 74.4 90.4 39 87. 83.44 72.89 75.93 Table 6: Performance comparison across Motif-2, Gemma3. contribute to this efficiency, enabling more capable instruction models at fraction of the compute cost. These results illustrate that careful architectural design, optimization-aware training infrastructure, and balanced fine-tuning strategies can bridge much of the performance gap traditionally attributed to massive scale, reaffirming the viability of efficient frontieroriented model design as path toward high-quality open-weight systems."
        },
        {
            "title": "6 Conclusion",
            "content": "Motif-2-12.7B demonstrates that carefully targeted architectural changes and systems-aware training can deliver strong performance without resorting to extreme parameter counts. By scaling Motif2.6B with width-preserving hypercloning and Llama Pro depth scaling, integrating Grouped Differential Attention for finer-grained information routing, and coupling curriculum-aware pre-training with three-stage SFT pipeline, we obtain model that is both efficient and broadly capable. Our fused PolyNorm kernels and Parallel Muon further show how bespoke systems work unlocks practical throughput and memory gains at long context, making large-scale training more accessible. Looking ahead, we will release Motif-2-12.7B-Reasoning, reinforcement-learningenhanced variant that explicitly optimizes multi-step reasoning quality (with focus on mathematics and code) while maintaining conversational fluency. We hope these open weights, transparent recipes, and forthcoming RL results provide solid baseline for the community to study scalable attention, efficient training, and principled pathways to stronger reasoning."
        },
        {
            "title": "References",
            "content": "[1] Kwangjun Ahn, Byron Xu, Natalie Abreu, Ying Fan, Gagik Magakyan, Pratyusha Sharma, Zheng Zhan, and John Langford. Dion: Distributed orthonormalized updates, 2025. [2] AIME. AIME problems and solutions. https://artofproblemsolving.com/wiki/ index.php/AIME_Problems_and_Solutions., 2025. [3] Anthropic. System Card: Claude Opus 4 & Claude Sonnet 4. anthropic.com/4263b940cabb546aa0e3283f35b686f4f3b2ff47.pdf, 2025. https://www-cdn. [4] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [5] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439, 2020. [6] Mark Chen. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 12 [7] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [8] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [10] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [11] Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, King Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. arXiv preprint arXiv:2502.14739, 2025. [12] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019. [13] Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are we done with mmlu? In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 50695096, 2025. [14] Alex Gu, Baptiste Rozi`ere, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024. [15] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. URL https://arxiv. org/abs, page 20, 2009. [17] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv. org/abs/2103.03874, 2, 2024. [18] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm: Unveiling the potential of small language models with scalable training strategies, 2024. [19] Yiwen Hu, Huatong Song, Jia Deng, Jiapeng Wang, Jie Chen, Kun Zhou, Yutao Zhu, Jinhao Jiang, Zican Dong, Wayne Xin Zhao, et al. Yulan-mini: An open data-efficient language model. arXiv preprint arXiv:2412.17743, 2024. [20] Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, Yang, Jiaheng Liu, Chenchen Zhang, Linzheng Chai, et al. Opencoder: The open cookbook for top-tier code large language models. arXiv preprint arXiv:2411.04905, 2024. [21] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [22] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large arXiv preprint scale distantly supervised challenge dataset for reading comprehension. arXiv:1705.03551, 2017. [23] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453466, 2019. [24] Wanchao Liang, Tianyu Liu, Less Wright, Will Constable, Andrew Gu, Chien-Chin Huang, Iris Zhang, Wei Feng, Howard Huang, Junjie Wang, et al. Torchtitan: One-stop pytorch native solution for production ready llm pre-training. arXiv preprint arXiv:2410.06511, 2024. [25] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [26] Junghwan Lim, Sungmin Lee, Dongseok Kim, Wai Ting Cheung, Beomgyu Kim, Taehwan Kim, Haesol Lee, Junhyeok Lee, Dongpin Oh, and Eunhwan Park. Grouped differential attention. arXiv preprint arXiv:2510.06949, 2025. [27] Junghwan Lim, Sungmin Lee, Dongseok Kim, Eunhwan Park, Hyunbyung Park, Junhyeok Lee, Wai Ting Cheung, Dahye Choi, Jaeheui Her, Jaeyeon Huh, et al. Motif 2.6 technical report. arXiv preprint arXiv:2508.09148, 2025. [28] Bill Yuchen Lin, Ronan Le Bras, Kyle Richardson, Ashish Sabharwal, Radha Poovendran, Peter Clark, and Yejin Choi. Zebralogic: On the scaling limits of llms for logical reasoning. arXiv preprint arXiv:2502.01100, 2025. [29] Tianyang Lin. Flash-muon: An efficient implementation of muon optimizer, 2025. [30] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. [31] Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al. Muon is scalable for llm training. arXiv preprint arXiv:2502.16982, 2025. [32] Zhengxiao Liu et al. Ifeval: Instruction-following evaluation for large language models. arXiv preprint arXiv:2404.01927, 2024. [33] Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, and Xian Li. Recycling the web: method to enhance pre-training data quality and quantity for language models. arXiv preprint arXiv:2506.04689, 2025. [34] OpenAI. GPT-5 System Card. https://openai.com/index/gpt-5-system-card/, 2025. [35] Guilherme Penedo, Hynek KydlÄ±Ëcek, Vinko SabolËcec, Bettina Messmer, Negar Foroutan, Amir Hossein Kargaran, Colin Raffel, Martin Jaggi, Leandro Von Werra, and Thomas Wolf. Fineweb2: One pipeline to scale them all adapting pre-training data processing to every language, 2025. [36] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [37] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. 14 [38] Mohammad Samragh, Iman Mirzadeh, Keivan Alizadeh Vahid, Fartash Faghri, Minsik Cho, Moin Nabi, Devang Naik, and Mehrdad Farajtabar. Scaling smart: Accelerating large language model pre-training with small model initialization. arXiv preprint arXiv:2409.12903, 2024. [39] Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. [40] Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset. arXiv preprint arXiv:2412.02595, 2024. [41] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [42] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1300313051, 2023. [43] Liping Tang, Nikhil Ranjan, Omkar Pangarkar, Xuezhi Liang, Zhen Wang, Li An, Bhaskar Rao, Linghao Jin, Huijuan Wang, Zhoujun Cheng, et al. Txt360: top-quality llm pre-training dataset requires the perfect blend, 2024. [44] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [45] Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. [46] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. [47] Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025. [48] Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 4, 2024. [49] Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ying Shan, and Ping Luo. Llama pro: Progressive llama with block expansion. arXiv preprint arXiv:2401.02415, 2024. [50] xAI. Grok 4 Model Card. Technical report, xAI, aug 2025. Last updated August 20, 2025. [51] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36:69798 69818, 2023. [52] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [53] Zongheng Yang, Zhanghao Wu, Michael Luo, Wei-Lin Chiang, Romil Bhardwaj, Woosuk Kwon, Siyuan Zhuang, Frank Sifei Luan, Gautam Mittal, Scott Shenker, et al. {SkyPilot}: An intercloud broker for sky computing. In 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23), pages 437455, 2023. 15 [54] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [55] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in neural information processing systems, 32, 2019. [56] Zhijian Zhuo, Ya Wang, Yutao Zeng, Xiaoqing Li, Xun Zhou, and Jinwen Ma. Polynomial composition activations: Unleashing the dynamics of large language models. arXiv preprint arXiv:2411.03884, 2024."
        },
        {
            "title": "7 Appendix",
            "content": "7.1 Contributions All authors are alphabetically sorted by last name. Technical and management leadership: Sungmin Lee, Junghwan Lim Core contributors: Dongseok Kim, Taehyun Kim, Eunhwan Park, Jeesoo Lee, Jeongdoo Lee, Junhyeok Lee Contributors: Wai Ting Cheung, Dahye Choi, Jaeheui Her, Jaeyeon Huh, Hanbin Jung, Changjin Kang, Beomgyu Kim, Minjae Kim, Taewhan Kim, Youngrok Kim, Hyukjin Kweon, Haesol Lee, Kungyu Lee, Dongpin Oh, Yeongjae Park, Bokki Ryu, Dongjoo Weon"
        }
    ],
    "affiliations": [
        "Motif Technologies"
    ]
}