{
    "paper_title": "One to rule them all: natural language to bind communication, perception and action",
    "authors": [
        "Simone Colombani",
        "Dimitri Ognibene",
        "Giuseppe Boccignone"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In recent years, research in the area of human-robot interaction has focused on developing robots capable of understanding complex human instructions and performing tasks in dynamic and diverse environments. These systems have a wide range of applications, from personal assistance to industrial robotics, emphasizing the importance of robots interacting flexibly, naturally and safely with humans. This paper presents an advanced architecture for robotic action planning that integrates communication, perception, and planning with Large Language Models (LLMs). Our system is designed to translate commands expressed in natural language into executable robot actions, incorporating environmental information and dynamically updating plans based on real-time feedback. The Planner Module is the core of the system where LLMs embedded in a modified ReAct framework are employed to interpret and carry out user commands. By leveraging their extensive pre-trained knowledge, LLMs can effectively process user requests without the need to introduce new knowledge on the changing environment. The modified ReAct framework further enhances the execution space by providing real-time environmental perception and the outcomes of physical actions. By combining robust and dynamic semantic map representations as graphs with control components and failure explanations, this architecture enhances a robot adaptability, task execution, and seamless collaboration with human users in shared and dynamic environments. Through the integration of continuous feedback loops with the environment the system can dynamically adjusts the plan to accommodate unexpected changes, optimizing the robot ability to perform tasks. Using a dataset of previous experience is possible to provide detailed feedback about the failure. Updating the LLMs context of the next iteration with suggestion on how to overcame the issue."
        },
        {
            "title": "Start",
            "content": "One to rule them all: natural language to bind communication, perception and action Simone Colombani1,3, Dimitri Ognibene2 and Giuseppe Boccignone1 2University of Milan, Italy 1University of Milano-Bicocca, Milan, Italy 3Oversonic Robotics, Carate Brianza, Italy Abstract In recent years, research in the area of human-robot interaction has focused on developing robots capable of understanding complex human instructions and performing tasks in dynamic and diverse environments. These systems have wide range of applications, from personal assistance to industrial robotics, emphasizing the importance of robots interacting flexibly, naturally and safely with humans. This paper presents an advanced architecture for robotic action planning that integrates communication, perception, and planning with Large Language Models (LLMs). Our system is designed to translate commands expressed in natural language into executable robot actions, incorporating environmental information and dynamically updating plans based on real-time feedback. The Planner Module is the core of the system where LLMs embedded in modified ReAct framework are employed to interpret and carry out user commands like Go to the kitchen and pick up the blue bottle on the table. By leveraging their extensive pre-trained knowledge, LLMs can effectively process user requests without the need to introduce new knowledge on the changing environment. The modified ReAct framework further enhances the execution space by providing real-time environmental perception and the outcomes of physical actions. By combining robust and dynamic semantic map representations as graphs with control components and failure explanations, this architecture enhances robots adaptability, task execution efficiency, and seamless collaboration with human users in shared and dynamic environments. Through the integration of continuous feedback loops with the environment the system can dynamically adjusts the plan to accommodate unexpected changes, optimizing the robots ability to perform tasks. Using dataset of previous experience is possible to provide detailed feedback about the failure. Updating the LLMs context of the next iteration with suggestion on how to overcame the issue. This system has been implemented on RoBee, the cognitive humanoid robot developed by Oversonic Robotics, showcasing its adaptability and potential for integration across diverse environments. By leveraging LLMs and semantic mapping, the architecture enables RoBee to navigate and respond to real-time changes. Keywords Human-Robot interaction, Robot task planning, Large Language Models, Automated planning 1. Introduction The integration of LLMs in robotic systems has opened new avenues for autonomous task planning and execution [2, 3]. These models demonstrate exceptional natural language understanding and commonsense reasoning capabilities, enhancing robots ability to comprehend contexts and execute commands [4, 5]. However LLMs are not be able to plan autonomously, they need to be integrated in architectures that enable them to understand the environment, the robot capabilities and state [6]. This research aims to empower robots to comprehend user requests and autonomously generate actionable plans in diverse environments. The efficacy of these plans relies on the robots understanding of its operating environment [7]. To bridge this gap, our work employs scene graphs [8] as semantic mapping tool, offering structured representation of spatial and semantic information within scene. AI4CC-IPS-RCRA-SPIRIT 2024: International Workshop on Artificial Intelligence for Climate Change, Italian Workshop on Planning and Scheduling, RCRA Workshop on Experimental evaluation of algorithms for solving problems with combinatorial explosion, and SPIRIT Workshop on Strategies, Prediction, Interaction, and Reasoning in Italy. November 25-28th, 2024, Bolzano, Italy [1]. $ simone.colombani@studenti.unimi.it (S. Colombani); dimitri.ognibene@unimib.it (D. Ognibene); giuseppe.boccignone@unimi.it (G. Boccignone) 2024 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). 4 2 0 2 2 2 ] . [ 1 3 3 0 5 1 . 1 1 4 2 : r In our approach, we leverage LLMs through in-context [9], which enables the models to learn and adapt based on the information provided in the context. Our work implements modified version of the ReAct [10] framework that expand the context of LLMs with environmental information and execution feedback, allowing the model to plan and execute skills [11] translating them into physical actions. Motivation The primary focus of our work is to enable robot to interact flexibly and robustly in dynamic and diverse environments with limited human intervention. Traditional robotic systems usually rely on static, pre-programmed instructions or closed world predefined knowledge and settings, limiting their adaptability to dynamic environments. Interacting with humans in daily tasks within complex environments disrupts these assumptions. LLMs and VLM can provide open-domain knowledge to represent novel conditions without human intervention. However, these models are not informed of the specific robot, task and settings at hand, that define what information can be relevant and necessary to find and reason about [12]. Exceeding in the level of detail may lead to impractical computational requirements and response time. Discarding crucial information, spatial or semantic, may lead to repeated failures due to the introduced non-managed partial observability [13]. To find the relevant information may be too slow [14]. LLMs can still produce outputs that are logically inconsistent or impractical [15], expecially if they are not integrated into systems that allow them to adapt to changes in the environment and the physical capabilities of the robots. Finally task execution, robots may encounter unexpected situations, such as unanticipated obstacles, sensor errors, or changes in the environment that were not accounted for in the initial plan. Such scenarios necessitate robust error handling mechanisms and adaptive planning strategies that enable the system to reassess and modify its actions in real-time [16]. By introducing execution controlling and failure management into the planning process at different levels as well as retrieval of previous successful plans, we propose solution to enhance the robustness and flexibility of LLM-based robotic systems. This approach ensures that the robot can effectively perceive changes in the environment and the failures that may arise from them, allowing it to adapt strategies in response to new challenges. Proposed approach Our system addresses the challenges of dynamic environments through realtime perception module and Planner module that integrates execution control, and failure management. It comprise Controller that monitors the execution of tasks and detects errors, while the Explainer analyzes failures and suggests adjustments based on past experiences. This feedback loop enables adaptive re-planning, allowing the system to modify its actions as needed. Specifically, we propose the use of the ReAct [10] framework, expanding its operational space with skills, physical actions of the robot and with perception action, to access information from the environment. By leveraging LLMs for natural language understanding and perception system, the architecture supports autonomous task execution in dynamic scenarios. 2. Related works substantial body of literature explores the utilization of LLMs for robotic task planning [4, 5]. LLM for robot planning Recent works highlight the potential of Large Language Models (LLMs) in robotic planning [17, 18, 19]. DEPS [20] introduces an iterative planning approach for agents in open-world environments, such as Minecraft. It utilizes LLMs to analyze errors during execution and refine plans, improving both reasoning and goal selection processes. However, this approach has been primarily developed and tested in virtual environments, with notable differences in comparison to real-world settings due to the dynamic and unpredictable nature of physical environments. Additionally, DEPS does not leverage previous issues and solutions but relies solely on feedback from humans and vision-language models (VLMs). Figure 1: Architecture of the system. Scene graph as environemental representation The use of scene graphs [21] as means to represent the robots environment has gained traction. [22] employs 3D scene graphs to represent environments and uses LLMs to generate Planning Domain Definition Language (PDDL) files. This method decomposes long-term goals into natural language instructions and enhances computational efficiency by addressing sub-goals. However, it lacks mechanism for replanning based on feedback during execution, which could limit its adaptability in dynamic scenarios. SayPlan [23] integrates semantic search with scene graphs and path planning to aid robots in navigating complex environments through natural language. By combining these techniques, SayPlan simulates various scenarios to refine task sequences, which helps improve overall task performance in complex environments. However, it is reliance on static pre-built 3D scene graphs, hindering adaptability to dynamic real-world environments. Replanning Replanning enables long-term autonomous task execution in robotics [24]. DROC [25] empowers robots to process natural language corrections and generalize that information to new tasks. It introduces mechanism to distinguish between high-level and low-level errors, allowing more flexible plan corrections. However, DROC does not address the types of failures that may occur during plan execution, focusing instead on high-level corrections provided by users. [26] supports autonomous long-term task execution by integrating LLMs for planning and VLMs for feedback. This approach adapts to changes in the environment through structured component system that verifies and corrects plans as needed. Yet, the feedback is limited to what is visible to the robots camera, potentially overlooking other significant environmental changes. 3. Architecture Our system is based on two components: Perception Module: it is responsible for sensing and interpreting the environment. It builds and mantains semantic map in the form of directed graph that integrates both geometric and semantic information. Planner Module: it takes the information provided by the Perception Module to formulate plans and actions that allow the robot to perform specific tasks. Figure 1 show how these components interact to allow the robot to understand its environment and act accordingly to satisfy user requests. The Perception module uses data provided by the robots sensors to supply the semantic map to the Planner module, which in turn processes it to generate specific action plans. In what follows we precisely address the Planner Module while details on the Figure 2: Architecture of the planner module. Perception Module will be provided in separate article. 3.1. Planner module The architecture of the Planner module is designed to translate user requests, expressed in natural language, into specific actions executable by robot. This module is responsible for understanding instructions, planning appropriate actions, and managing the execution of those actions in dynamic environment. The Planning module is composed by five sub-modules: Task Planner: Translates user requests, expressed in natural language, into sequence of high-level skills. Skill Planner: Translates high-level skills into specific, low-level executable commands. Executor: Executes the low-level actions generated by the Skill Planner. Controller: Monitors the execution of actions and manages any errors or unexpected events during the process. Explainer: Interprets the causes of execution failures by analyzing data received from the Controller and provides suggestions to the Task Planner on how to adjust the plan. The architecture of the planner module is shown in Figure 2. The main component of the system is the Task Planner, which receives the users request and translates it into list of high-level \"skills\" that represent the robots capabilities. These skills include actions such as \"PICK\" (grasp an object), \"PLACE\" (place an object), and \"GOTO\" (move to position). 3.1.1. Task Planner The decision-making process of the Task Planner is driven by policy, which is implemented as LLM. policy is strategy or rule that defines how actions are selected based on the current state or context,[27]. Task Planner is implemented using the ReAct framework [10], which alternates between reasoning and action phases during the process. In the reasoning phase, the Task Planner can access various \"perception\" actions to gather information from the environment, such as the semantic map and the current state of the robot, and can execute one or more \"skill\" actions to perform physical actions. The classical idea of ReAct is to augment the agents action space to ğ´Ë† = ğ´ ğ¿, where ğ¿ is the space of language-based reasoning actions. An action ğ‘Ë†ğ‘¡ ğ¿, referred to as \"thought\" or reasoning trace, does not directly affect the external environment but instead updates the current context ğ‘ğ‘¡+1 = (ğ‘ğ‘¡, ğ‘Ë†ğ‘¡) by adding useful information to support future decision-making [10]. In the classical idea there could be various types of useful thoughts, such as decomposing task goals and creating action plans, injecting commonsense knowledge relevant to task solving, extracting important parts from observations, tracking progress and transitioning action plans, handling exceptions and adjusting action plans, and so on, but always without modifying the physical environment, only embedding it within the context. Interestingly, this approach mixes reasoning and action in flexible manner. In the future, we will analyse the potential of this approach also connecting to the planning-to-plan [28, 29] and meta-reasoning [30, 31, 32] concepts. In our work, we augment the agents [33] action space with two types of actions: skill action ğ‘ğ‘¡ ğ´skill, which involves physically interacting with the environment, such as manipulating objects or navigating. The result of skill action provides new feedback that updates the current context. perception action ğ‘ğ‘¡ ğ´perception, which involves accessing information from the environment, such as querying the semantic map or sensors, and integrating that information into the context. The augmented action space is defined as: ğ´Ë† = ğ´skill ğ´perception ğ¿ Thus, the LLM serves as the policy ğœ‹ that selects different types of actions from the augmented action space and dynamically adapting the current context ğ‘ğ‘¡ used to plan based on real-time information and reasoning. Formal Description: The Task Planners policy ğœ‹, represented by the LLM, can be formalized as function that maps the current context ğ‘ğ‘¡ to an action ğ‘Ë†ğ‘¡ from the augmented action space ğ´Ë† : Where: ğœ‹ : ğ¶ ğ´Ë† , ğœ‹(ğ‘ğ‘¡) = ğ‘Ë†ğ‘¡ ğ¶ is the set of all possible contexts. ğ´Ë† is the augmented action space ğ´Ë† = ğ´skill ğ´perception ğ¿. ğ‘ğ‘¡ represents the current context at time ğ‘¡, which includes the state of the robot, the environment, and any past actions or thoughts. ğ‘Ë†ğ‘¡ ğ´Ë† is the action chosen by the policy, which can be skill action ğ‘ğ‘¡ ğ´skill, perception action ğ‘ğ‘¡ ğ´perception, or reasoning trace ğ‘Ë†ğ‘¡ ğ¿. The context ğ‘ğ‘¡ is updated based on the chosen action: If ğ‘Ë†ğ‘¡ ğ¿ (a reasoning action), the context updates to: ğ‘ğ‘¡+1 = (ğ‘ğ‘¡, ğ‘Ë†ğ‘¡) This represents the thought process, where reasoning contributes new information without affecting the external environment. If ğ‘Ë†ğ‘¡ ğ´perception (a perception action), the result of querying the environment updates the context: ğ‘ğ‘¡+1 = (ğ‘ğ‘¡, ğ‘“perception(ğ‘Ë†ğ‘¡)) Here, ğ‘“perception represents the function that gathers information and modifies the context based on the perception actions outcome. If ğ‘Ë†ğ‘¡ ğ´skill (a skill action), the robot interacts with the environment, and the context updates based on feedback from the physical action: ğ‘ğ‘¡+1 = (ğ‘ğ‘¡, ğ‘“skill(ğ‘Ë†ğ‘¡)) Where ğ‘“skill is the function that captures the result of executing physical skill, such as manipulating an object or moving to location. 3.1.2. Skill Planner Once high-level request for the execution of skill is made, the Skill Planner is responsible for translating the high-level skills, provided by the Task Planner, into sequences of low-level commands executable by the robot. While the Task Planner focuses on understanding natural language and creating general plan, the Skill Planner deals with the specific details of how each skill should be executed, considering the robots state and the environment. Let skill be represented in the following general form, defined by the Task Planner with specific syntax: ğ‘†ğ¾ğ¼ğ¿ğ¿_ğ‘ ğ´ğ‘€ ğ¸(ğ‘ğ‘ğ‘Ÿğ‘ğ‘š1, ğ‘ğ‘ğ‘Ÿğ‘ğ‘š2, . . . , ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘ ) Where: SKILL_NAME is the name of the skill to be executed (e.g., PICK, PLACE, GOTO). param_1, param_2, . . . , param_N are parameters for the skill, such as the object to manipulate or the destination to navigate to. Using strict syntax ensures that the Skill Planner can correctly interpret the high-level commands without ambiguity. For instance, natural language command like \"Move near the table and grab the bottle\" would lack precision. The Skill Planner needs concrete parameters for the robot to act effectively. Skill Planner workflow: The Skill Planner operates by performing three functions: 1. Precondition Verification: Before translating skill into low-level commands, the Skill Planner verifies that the necessary preconditions for execution are met. Let ğ‘ ğ‘¡ represent the current state of the robot and the environment at time ğ‘¡, and ğ‘ƒ (skill, ğ‘ ğ‘¡) denote function for every skill that evaluates the preconditions for given skill. The precondition check can be expressed as: ğ‘ƒ (skill, ğ‘ ğ‘¡) = if all preconditions are met { 1, 0, otherwise For example, before executing the PICK skill, the following checks may be performed: The object is visible by the robot. The object is reachable for the robotic arm. The robotic arm is free. If any of these conditions are not met (ğ‘ƒ (skill, ğ‘ ğ‘¡) = 0), the Skill Planner reports failure to the Task Planner. 2. Target nodes extraction: Based on the parameters of the skill, the Skill Planner extracts the target nodes from the semantic map â„³, which contains geometric and semantic information about the environment. Every node provides geometric information such as objects position and relevant context, which is then used to generate low-level commands. 3. Generation of Low-Level Commands: When ğ‘ƒ (skill, ğ‘ ğ‘¡) = 1, the Skill Planner translate the skill into sequence of low-level commands to control the robot behavior. In this system, we represent skill decomposition in commands as Hierarchical Task Networks (HTNs) that contains low-level commands executable by the robot. Let ğ¶ğ‘€ (skill, node, ğ‘ ğ‘¡) denote the function that translates the given skill into low-level commands based on the target nodes extracted from the semantic map and current state. The output is sequence of pre-modeled commands parameterized with the information of the robot state and the target nodes, {ğ‘ğ‘š1, ğ‘ğ‘š2, . . . , ğ‘ğ‘šğ‘˜}, where each command ğ‘ğ‘šğ‘– directs specific components of the robot. Our implementation use HTNs solely on the breakdown of skills into commands without using them with advanced features like re-planning or error recovery of the commands. In this case, if any command fails, the entire skill fails, with no attempt at re-planning at the skill planner level. The process can be represented as: {ğ‘ğ‘š1, ğ‘ğ‘š2, . . . , ğ‘ğ‘šğ‘˜} = ğ¶ğ‘€ (skill, node, ğ‘ ğ‘¡) The Skill Planner is designed to be flexible and extendable. The skill functions ğ‘ƒ , and ğ¶ğ‘€ can be adapted or extended to accommodate new skills, hardware, or environments. 3.1.3. Executor The Executor is responsible for directly interacting with the robots hardware to execute the commands provided by the Skill Planner. It translates the low-level commands into physical actions by controlling various hardware elements such as motors, robotic arm grippers, and other actuators required for task execution. Let the set of low-level commands generated by the Skill Planner be represented as above, i.e., ğ‘ğ‘š1, ğ‘ğ‘š2, . . . , ğ‘ğ‘šğ‘˜ = ğ¶ğ‘€ (skill, node, ğ‘ ğ‘¡), where ğ¶ğ‘€ (skill, node, ğ‘ ğ‘¡) defines the sequence of commands based on the skill, the target node, and the current state of the robot and the environment. The Executor is tasked with executing these commands on the physical robot. Let the state of the robot at time ğ‘¡ be denoted by â„ğ‘¡, and the function that maps low-level command ğ‘ğ‘– to an effect on the robots state be denoted as ğ»(ğ‘ğ‘šğ‘–, â„ğ‘¡). The execution of command at time ğ‘¡ can be described as: where â„ğ‘¡+1 is the updated state after executing the command ğ‘ğ‘šğ‘–. This process is repeated for each command in the sequence {ğ‘ğ‘š1, ğ‘ğ‘š2, . . . , ğ‘ğ‘šğ‘˜} until the entire skill is executed. â„ğ‘¡+1 = ğ»(ğ‘ğ‘šğ‘–, â„ğ‘¡) Executor workflow: Command reception: The Executor receives set of low-level commands {ğ‘ğ‘š1, ğ‘ğ‘š2, . . . , ğ‘ğ‘šğ‘˜} from the Skill Planner. Each command specifies concrete action to be performed by the robots hardware components. Hardware interaction: For each command ğ‘ğ‘šğ‘–, the Executor interacts with the robots hardware, adjusting the motors, grippers, and other actuators. This interaction can be represented by the function ğ»(ğ‘ğ‘šğ‘–, â„ğ‘¡) that determines the effect of command on the robots state â„ğ‘¡. Command execution: The Executor executes each command ğ‘ğ‘šğ‘– in the sequence, ensuring that the robots state transitions from state â„ğ‘¡ to â„ğ‘¡+1. Formally: â„_ğ‘¡ + 1 = ğ»(ğ‘ğ‘š_ğ‘–, â„_ğ‘¡), ğ‘– = 1, 2, . . . , ğ‘˜ After executing all commands, the robots reaches the final state â„ğ‘¡+ğ‘˜, corresponding to the completion of the skill. Real-Time feedback: During execution, the robots provides feedback on its current state. Let ğ‘“ğ‘¡ denote the feedback at time ğ‘¡, and ğ‘“ğ‘¡+1 be the updated feedback after executing command ğ‘ğ‘šğ‘–: ğ‘“ğ‘¡+1 = ğ¹ (ğ‘ğ‘šğ‘–, â„ğ‘¡) where ğ¹ is the feedback function. If unexpected feedback ğ‘“ğ‘¡+1 is received, the Executor can trigger adjustments to the plan or inform the Skill Planner of potential issue. Different robots may use different communication protocols, and hardware configurations. Therefore, the Executor must be adapted for each specific robot system, ensuring that it correctly interacts with the robots hardware. 3.1.4. Controller The Controller is responsible for monitoring the robots status and the environment during command execution, ensuring that they are carried out as planned. After each command is executed, the Executor sends feedback indicating either success or failure. If failure occurs, it results in the failure of the entire skill. Upon the completion of all commands, success feedback will indicate the successful execution of the skill. Denote ğ‘“ğ‘¡ the feedback from the Executor at time ğ‘¡. The Controller processes ğ‘“ğ‘¡ to determine the outcome of the executed skills. The feedback can be classified into two categories: success and failure. Feedback processing: Success: If the feedback ğ‘“ğ‘¡ indicates successful execution of command and it is the last command to execute, the skill is considered successfully completed, the Controller sends positive acknowledgment to the Task Planner to continue the planning process. However, if the feedback indicates success but the command is not the last one, the Controller waits for the execution of the next command: if ğ‘“ğ‘¡ = Success = Task Planner continues Failure: If failure occurs during the execution of any command, the planned skill fails and the Controller generates failure message ğ‘šğ‘“ that includes the reason for the failure. This message is sent to the Explainer. Let ğ‘’ğ‘¡ represent the specific error detected at time ğ‘¡. The failure message can be represented as: ğ‘šğ‘“ = Failure(ğ‘’ğ‘¡) where ğ‘’ğ‘¡ can include various error reasons such as obstacles detected, non-executable trajectories, or environmental changes. The Controllers operation is highly dependent on the specific robot system in use, as it relies on the characteristics of the robot and the employed software system. In ROS environment, for example, the Controller interacts with ROS nodes that control the robots hardware. In our work, RoBee, described in section 5, has system that allows to obtain feedback on the execution of commands. 3.1.5. Explainer The Explainer component plays critical role in enhancing the planning process by providing insights to the Task Planner when failures occur during the execution phase. After receiving the failure reason, the Explainer searches dataset ğ’Ÿ for previous instances of similar failures. This dataset comprises denote the subset of the records of failures associated with specific skills and user requests. Let ğ’Ÿğ‘Ÿğ‘“ dataset containing records of failures and solutions related to the same skill and error message. The dataset has been manually built based on previous experiences, desired behaviors, and expected failures. The search can be expressed as: ğ’Ÿğ‘Ÿğ‘“ = {(ğ‘ ğ‘˜, ğ‘¢ğ‘Ÿ, ğ‘Ÿğ‘“ ) ğ’Ÿ ğ‘ ğ‘˜ = skill_name, ğ‘’ğ‘Ÿ = ğ‘Ÿğ‘“ , ğ‘¢ğ‘Ÿ user_request} where: ğ‘ ğ‘˜ is the skill being executed (e.g., PICK). ğ‘¢ğ‘Ÿ represents the specific user request associated with the failure. ğ‘Ÿğ‘“ is the failure reason provided by the Controller ğ‘¢ğ‘Ÿ user_request indicates that the user request in the dataset is similar to the current user request. Rather than searching for an exact match to the users request, the Explainer assesses the similarity of the users request (ğ‘¢ğ‘Ÿ) to the instances in the dataset linked to the suggestion, using cosine similarity in our approach [34]. This method enables the system to identify the most relevant past instances, even when the users requests are not identical. Once relevant instances are identified, the Explainer analyzes these cases to generate suggestion ğ‘ ğ‘” for the Task Planner on how to proceed. The suggestion is structured as follows: ğ‘ ğ‘” = Suggest(ğ’Ÿğ‘Ÿğ‘“ ) For instance, if the Controller reports the failure reason: ğ‘Ÿğ‘“ = \"Cannot execute the approach movement for the PICK skill, object too far\" The Explainer analyzes this failure and may find previous instance where the robot successfully resolved similar issue. It could recommend command to the Task Planner: ğ‘ ğ‘” = \"Use the GOTO skill to move near the object to pick\" This suggestion enables the Task Planner to adjust its strategy effectively, moving the robot closer to the object before attempting the PICK action again. The suggestions provided by the Explainer can be tailored to accommodate specific behaviors of the robot. This adaptability can be achieved by modifying the parameters of the data utilized to generate the suggestions. Thus, the Explainer enhances the resilience of the system, facilitating its ability to adapt to changing conditions and recover from errors efficiently. 4. Behavior example of the system To illustrate the proposed systems behavior in dynamic environment, consider scenario where the robot is tasked with picking up bottle from table in room and placing it in designated area in other room. In this example, the system must react to unexpected changes, such as the bottle being moved by an external agent during the task. Example User Request: The user provides the instruction: \"Go to the table in the kitchen, pick up the bottle, and place it on the table in the bedroom.\" Perception Module: The robot senses the environment and generateds semantic map, which includes the pose and types of objects. Task Planner: The request is interpreted, and the Task Planner begins the planning procedure. It starts by calling perception actions to gather information about the environment and the robots state. Using the results of these perception actions, the Task Planner determines the next steps in its execution by calling skill actions and incorporating the outcomes of each skill into the planning process. LLM interprets the request and reasons as follows. User Request: \"Go to the table in the kitchen, pick up the bottle, and place it on the table in the bedroom.\" Robot State: The robot is currently in the bedroom and has both the right and left arms empty. Thought: The task is to fetch the bottle from the kitchen and place it on the table in the bedroom. need to know all the rooms where the robot can go to. Perception action: GetMapRooms() Observation: The robot identifies the rooms: [kitchen, bedroom]. Thought: Now, will navigate to the kitchen to find the bottle. Skill action: GOTO(kitchen) Observation: The robot moves to the kitchen. Thought: should identify the specific location of the bottle within the kitchen. Perception action: GetObjectInRoom(kitchen) Observation: The robot finds the following objects in the kitchen: [sink on the table_1, stove on the table_1, bottle, table_1, table_2]. Thought: will now move to the location of the bottle. Skill action: GOTO(bottle) Observation: The robot moves to the bottles location in the kitchen. Thought: will pick up the bottle now. Skill action: PICK(bottle, right) Observation: The robot cant see the bottle to pick, find bottle in the semantic map and try again to pick it. Thought: The bottle is not visible where expected, need to check the location of the bottle again. Perception action: GetObjectInRoom(kitchen) Observation: The robot finds the following objects in the kitchen: [sink on the table_1, stove on the table_1, bottle on the table_2, table_1, table_2 ]. Thought: will go to the table to pick up the bottle instead. Skill action: GOTO(table_2) Observation: The robot moves to the table. Thought: will pick up the bottle from the table now. Skill action: PICK(bottle, right) Observation: The robot successfully picks up the bottle. Thought: Next, need to navigate to the bedroom to place the bottle. Skill action: GOTO(bedroom) Observation: The robot moves to the bedroom. Thought: need to locate the table in the bedroom where will place the bottle. Perception action: GetObjectInRoom(bedroom) Observation: The robot sees: [table, bed, lamp on the table] in the bedroom. Thought: will approach the table to place the bottle. Skill action: GOTO(table) Observation: The robot approaches the table. Thought: Finally, will place the bottle on the table. Skill action: PLACE(bottle, right) Observation: The robot places the bottle on the table successfully. Skill Planner: For each high-level skill, once the preconditions are met, the Skill Planner translates the choosen skill in sequence of low-level commands, such as motor movements for navigation, arm articulation for picking, and placing actions. For example once the PICK(bottle, right) skill is planned, it can be translated and divided into the following phases with relative commands: Approach: The robot arm moves towards the objects position, making any necessary adjustments to align correctly, and opens the gripper. Grasp: The robot activates the gripping mechanisms to seize the object. This phase includes closing the gripper and verifying the grasp. Lifting: The robot lifts the object from the surface it is on. Execution: The Executor begins executing the planned skill, which is composed of sequence of commands by the Skill Planner. The Executor follows the ordered steps to achieve the goal. For example with the skill PICK(bottle, right), the Executor receive the list of command and execute: Execute approach: The robot arm moves towards the objects position and open the gripper. Execute grasp: This phase includes closing the gripper and verifying the grasp. Execute lifting: The robot lifts the object from the surface it is on. Thus, when an unexpected event occurs, such as the bottle being moved or is not reachable the executor may raise failure message. Controller and Explainer interaction: The Controller detects that the object is no longer in the expected location and sends failure message to the Explainer. The Explainer analyzes the failure, referencing previous instances where objects were moved unexpectedly. It suggests the Task Planner to re analyse the semantic map and update the objects location. Re-planning: Based on the suggestion, the Task Planner issues new plan: Execute GOTO(table) to go near the identified bottle. After locating the bottle on the table, the robot updates its actions and proceeds to execute the remaining tasks. This example demonstrates how the system adapts in real-time, allowing for continuous task execution even in dynamic and unpredictable environments. Planning algorithm We now formalize this process in the form of an adaptive planning algorithm. In this algorithm, the used LLM is generalist model such as Llama 3 70B Instruct [35], whose behavior we influence through in-context learning [9]. Figure 3: Robee, humaniod robot developed by Oversonic Robotics. Algorithm 1 Planning with extedend ReAct Framework 1: Input: User request ğ‘Ÿ, Robot state ğ‘…ğ‘  2: Output: Execution of user request 3: procedure Planning(ğ‘Ÿ, ğ‘€ ) 4: ğ¶0 InitializeLLMContext(ğ‘Ÿ, ğ‘€, ğ‘…ğ‘ ) while not goal achieved do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› TaskPlanner(ğ‘Ÿ, ğ¶0) if ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› = \"Skill\" then ğ‘ğ‘œğ‘šğ‘šğ‘ğ‘›ğ‘‘ğ‘  SkillPlanner(ğ‘ ğ‘˜ğ‘–ğ‘™ğ‘™, ğ¶ğ‘¡) ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘  Executor(ğ‘ğ‘œğ‘šğ‘šğ‘ğ‘›ğ‘‘ğ‘ ) if ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘  = False then ğ‘“ ğ‘ğ‘–ğ‘™ğ‘¢ğ‘Ÿğ‘’ğ‘€ ğ‘ ğ‘” Controller(ğ¶ğ‘¡) ğ‘ğ‘¡ Explainer(ğ‘“ ğ‘ğ‘–ğ‘™ğ‘¢ğ‘Ÿğ‘’ğ‘€ ğ‘ ğ‘”) else ğ‘ğ‘¡ Skill succesfully executed end if else Get first skill Translate skill into low-level commands Execute commands Detect failure Generate suggestion ğ‘ğ‘¡ CallPerceptionAction() Reading semantic map from Perception Module end if ğ¶ğ‘¡+1 UpdateContext(ğ¶ğ‘¡) ğ‘ ğ‘˜ğ‘–ğ‘™ğ‘™ TaskPlanner(ğ‘Ÿ, ğ¶ğ‘¡+1) Update context Get next skill based on updated context end while 21: 22: end procedure This algorithm shows the adaptive behavior of the system by incorporating feedback loops that facilitate real-time re-planning. By alternating between action and reasoning phases, the robot can continuously adapt to changes, ensuring task success even in unpredictable environments. Figure 4: Environment used during the execution of experiments. 5. Robot Hardware The system was implemented using RoBee, cognitive humanoid robot developed by Oversonic Robotics. RoBee measures 160 cm in height and weighs 60 kg. It has 32 degrees of freedom, enabling highly flexible movement. The robot is equipped with multiple sensors, including cameras, microphones, and force sensors. The cameras provide real-time visual data, supporting navigation and object recognition tasks. The microphones facilitate audio input, enabling speech recognition and interaction through natural language processing. The force sensors are used for handling objects, allowing RoBee to adjust grip force based on the characteristics of the item being manipulated, enhancing precision and safety during interactions. RoBees mechanical structure includes two arms capable of bimanual manipulation, each capable of handling objects weighing up to 5 kg. The system includes torso and leg system designed for balance and mobility. RoBee is equipped with LIDAR sensors for real-time environment mapping and obstacle detection. These LIDAR sensors enable the robot to navigate autonomously through complex environments, ensuring safe operation in shared spaces. The combination of autonomous navigation technologies and LIDAR-based detection enhances the ability of RoBee to move efficiently and avoid collisions in dynamic industrial environments. In addition to its physical capabilities, RoBee integrates with cloud-based systems, allowing for remote monitoring, task scheduling, and data analytics. The Planner-module takes into account RoBees embodiment, ensuring that the system is aligned with the robots capabilities such as its degrees of freedom, sensor suite, and ability to perform manipulation and navigation. 6. Preliminary results Preliminary experiments were conducted in simulated environment replicating two main rooms: kitchen and bedroom, as illustrated in Figure 6. During the experiments, three types of requests were tested, each varying in complexity: Simple requests: direct commands that involve only one skill. For example, \"Pick up the bottle in front of you\", where the task planner needs only to identify the parameters and activate the appropriate skill. Moderately complex requests: tasks that require the robot to perform multiple skills in sequence, as explicitly described in the command. An example is \"Go to the kitchen, pick up the bottle, and bring it to the table in the bedroom\", which involves multiple skills. These tasks require higher level of complexity, with planning across several steps and handling potential failures. Complex requests: such as \"Im thirsty, can you help me?\", which were more open-ended and required the robot to interpret the task and break it down into multiple steps. The results in table 1 showed that the system performed well with simple requests, followed by moderately complex ones. However, the success rate for complex requests was significantly lower, with only 25% of the tasks completed correctly. This lower performance was attributed to the systems difficulty in understanding and managing ambiguous or under-specified instructions. It is important to note that these are preliminary results, and further analysis is ongoing. thorough evaluation of the data is currently underway, including comparison with the state of the art in robot task execution and natural language understanding. This will allow for deeper understanding of the systems strengths and areas for improvement. Request type Simple requests Moderately complex requests Complex requests Number of attempts 30 20 10 Success rate 90% 75% 25% Table 1 Number of attempts and success rate for each request type 7. Conclusions The proposed planning system exhibits notable strengths, particularly its adaptability and seamless with the robots diverse set of skill for executing complex tasks. The systems core advantage lies in its ability to interpret user commands through natural language processing, converting them into high-level actions that are further refined into low-level, executable tasks. By integrating real-time environmental feedback from the Perception Module through an extended version of ReAct framework, the system can dynamically adjust to unexpected situations, such as obstacles or execution failures. This adaptability is supported by an architecture, where the Task Planner, Skill Planner, Controller, and Explainer components work in harmony to ensure smooth task execution even in changing environments. One of the systems key strengths is its ability to manage error recovery through feedback loops, allowing the robot to adapt quickly to failures during task execution. The Explainer module provides on the fly suggestions to modify the plan based on past errors, enhancing the systems validity. The use of semantic maps and scene graphs provides the robot with structured understanding of its environment, ensuring that actions are contextually accurate and responsive to real-world conditions. The integration of LLMs, perceptual feedback, and flexible task planning mechanisms makes the system highly versatile for complex, dynamic environments. Its implementation on RoBee, the humanoid robot developed by Oversonic Robotics, has demonstrated its practical potential, positioning it as valuable tool for applications requiring advanced human-robot interaction and adaptability in unpredictable settings. In the future, other than extending the low level skill set available, we will investigate the possibility to autonomously expand the Explainer dataset as well as providing similar information directly to the Task Planner, increasing flexibility and reliability and reducing the number of re-planning events. We will also study capability of the system to proactively acquire information about the environment [14] and human partners both through sensors [36] and communication strategies, leveraging the potential for proactive information gathering behaviours of LLMs [37, 38, 39]. Moreover, it will be crucial to assess the reliability of the system both at the planning level as well as the communication level, considering the introduction of embodiment and environment while the limitation in pragmatic understanding of LLM are still to be understood [39, 40, 41]."
        },
        {
            "title": "Acknowledgments",
            "content": "Special thanks to Oversonic Robotics for enabling the implementation of this project using their humanoid robot, RoBee."
        },
        {
            "title": "References",
            "content": "[1] D. Aineto, R. De Benedictis, M. Maratea, M. Mittelmann, G. Monaco, E. Scala, L. Serafini, I. Serina, F. Spegni, E. Tosello, A. Umbrico, M. Vallati (Eds.), Proceedings of the International Workshop on Artificial Intelligence for Climate Change, the Italian workshop on Planning and Scheduling, the RCRA Workshop on Experimental evaluation of algorithms for solving problems with combinatorial explosion, and the Workshop on Strategies, Prediction, Interaction, and Reasoning in Italy (AI4CC-IPS-RCRA-SPIRIT 2024), co-located with 23rd International Conference of the Italian Association for Artificial Intelligence (AIxIA 2024), CEUR Workshop Proceedings, CEUR-WS.org, 2024. [2] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, in: Proceedings of the 40th T. Yu, et al., Palm-e: an embodied multimodal language model, International Conference on Machine Learning, 2023, pp. 84698488. [3] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman, et al., Do as can, not as say: Grounding language in robotic affordances, arXiv e-prints (2022) arXiv2204. [4] J. Wang, Z. Wu, Y. Li, H. Jiang, P. Shu, E. Shi, H. Hu, C. Ma, Y. Liu, X. Wang, et al., Large language models for robotics: Opportunities, challenges, and perspectives, arXiv preprint arXiv:2401.04334 (2024). [5] F. Zeng, W. Gan, Y. Wang, N. Liu, P. S. Yu, Large language models for robotics: survey, arXiv e-prints (2023) arXiv2311. [6] S. Kambhampati, K. Valmeekam, L. Guan, K. Stechly, M. Verma, S. Bhambri, L. Saldyt, A. Murthy, Llms cant plan, but can help planning in llm-modulo frameworks, arXiv preprint arXiv:2402.01817 (2024). [7] S. Tellex, N. Gopalan, H. Kress-Gazit, C. Matuszek, Robots that use language, Annual Review of Control, Robotics, and Autonomous Systems 3 (2020) 2555. [8] G. Zhu, L. Zhang, Y. Jiang, Y. Dang, H. Hou, P. Shen, M. Feng, X. Zhao, Q. Miao, S. A. A. Shah, et al., Scene graph generation: comprehensive survey, arXiv e-prints (2022) arXiv2201. [9] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, Z. Sui, survey on in-context learning, arXiv preprint arXiv:2301.00234 (2022). [10] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, Y. Cao, React: Synergizing reasoning and acting in language models, in: International Conference on Learning Representations (ICLR), 2023. [11] L. Heuss, D. Gebauer, G. Reinhart, Concept for the automated adaption of abstract planning domains for specific application cases in skills-based industrial robotics, Journal of Intelligent Manufacturing (2023) 126. [12] M. Shanahan, Frame problem, the, Encyclopedia of Cognitive Science (2006). [13] L. P. Kaelbling, M. L. Littman, A. R. Cassandra, Planning and acting in partially observable stochastic domains, Artificial intelligence 101 (1998) 99134. [14] D. Ognibene, G. Baldassare, Ecological active vision: four bioinspired principles to integrate bottomup and adaptive topdown attention tested with simple camera-arm robot, IEEE transactions on autonomous mental development 7 (2014) 325. [15] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, P. Fung, Survey of hallucination in natural language generation, ACM Computing Surveys 55 (2023) 138. [16] O. Ruiz, J. Rosell, M. Diab, Reasoning and state monitoring for the robust execution of robotic manipulation tasks, in: 2022 IEEE 27th International Conference on Emerging Technologies and Factory Automation (ETFA), IEEE, 2022, pp. 14. [17] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, A. Zeng, Code as policies: in: 2023 IEEE International Conference on Language model programs for embodied control, Robotics and Automation (ICRA), IEEE, 2023, pp. 94939500. [18] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, P. Stone, Llm+ p: Empowering large language models with optimal planning proficiency, arXiv e-prints (2023) arXiv2304. [19] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, Y. Su, Llm-planner: Few-shot grounded in: Proceedings of the IEEE/CVF planning for embodied agents with large language models, International Conference on Computer Vision, 2023, pp. 29983009. [20] Z. Wang, S. Cai, G. Chen, A. Liu, X. Ma, Y. Liang, Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents, arXiv e-prints (2023) arXiv2302. [21] I. Armeni, Z.-Y. He, J. Gwak, A. R. Zamir, M. Fischer, J. Malik, S. Savarese, 3d scene graph: structure for unified semantics, 3d space, and camera, in: Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 56645673. [22] Y. Liu, L. Palmieri, S. Koch, I. Georgievski, M. Aiello, Delta: Decomposed efficient long-term robot task planning using large language models, arXiv e-prints (2024) arXiv2404. [23] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, N. Suenderhauf, Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning, in: 7th Annual Conference on Robot Learning, 2023. [24] M. Cashmore, A. Coles, B. Cserna, E. Karpas, D. Magazzeni, W. Ruml, Replanning for situated robots, in: Proceedings of the International Conference on Automated Planning and Scheduling, volume 29, 2019, pp. 665673. [25] L. Zha, Y. Cui, L.-H. Lin, M. Kwon, M. G. Arenas, A. Zeng, F. Xia, D. Sadigh, Distilling and retrieving generalizable knowledge for robot manipulation via language corrections, in: 2024 IEEE International Conference on Robotics and Automation (ICRA), IEEE, 2024, pp. 1517215179. [26] M. Skreta, Z. Zhou, J. L. Yuan, K. Darvish, A. Aspuru-Guzik, A. Garg, Replan: Robotic replanning with perception and language models, arXiv e-prints (2024) arXiv2401. [27] H. Geffner, Non-classical planning with classical planner: The power of transformations, in: European Workshop on Logics in Artificial Intelligence, Springer, 2014, pp. 3347. [28] D. Ognibene, G. Pezzulo, H. Dindo, Resources allocation in bayesian, schema-based model of distributed action control, in: NIPS-Workshop on Probabilistic Approaches for Robotics and Control, 2009. [29] M. Ho, D. Abel, J. Cohen, M. Littman, T. Griffiths, People do not just plan, they plan to plan, in: Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, 2020, pp. 13001307. [30] S. Russell, E. Wefald, Principles of metareasoning, Artificial intelligence 49 (1991) 361395. [31] S. Zilberstein, S. J. Russell, Anytime sensing, planning and action: practical model for robot control, in: IJCAI, volume 93, 1993, pp. 14021407. [32] R. Ackerman, V. A. Thompson, Meta-reasoning: Monitoring and control of thinking and reasoning, Trends in cognitive sciences 21 (2017) 607617. [33] S. J. Russell, P. Norvig, Artificial intelligence: modern approach, Pearson, 2016. [34] F. Rahutomo, T. Kitasuka, M. Aritsugi, et al., Semantic cosine similarity, in: The 7th international student conference on advanced science and technology ICAST, volume 4, University of Seoul South Korea, 2012, p. 1. [35] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al., The llama 3 herd of models, arXiv. org (????). [36] D. Ognibene, Y. Demiris, Towards active event recognition., in: IJCAI, 2013, pp. 24952501. [37] S. Patania, E. Masiero, L. Brini, G. Donabauer, U. Kruschwitz, V. Piskovskyi, D. Ognibene, Large language models as an active bayesian filter: information acquisition and integration, in: Proceedings of the 28th Workshop on the Semantics and Pragmatics of Dialogue - Full Papers, SEMDIAL, Trento, Italy, 2024. URL: http://semdial.org/anthology/Z24-Patania_semdial_0006.pdf. [38] A. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu, L. Takayama, F. Xia, J. Varley, et al., Robots that ask for help: Uncertainty alignment for large language model planners, Proceedings of Machine Learning Research 229 (2023). [39] B. Magnini, Toward collaborative llms: Investigating proactivity in task-oriented dialogues, in: Proceedings of the 28th Workshop on the Semantics and Pragmatics of Dialogue - Invited Talks, SEMDIAL, Trento, Italy, 2024. URL: http://semdial.org/anthology/Z24-Magninini_semdial_0003a. pdf. [40] A. Martinenghi, C. Koyuturk, S. Amenta, M. Ruskov, G. Donabauer, U. Kruschwitz, D. Ognibene, Von neumidas: Enhanced annotation schema for human-llm interactions combining midas with von neumann inspiredsemantics, in: Proceedings of the 28th Workshop on the Semantics and Pragmatics of Dialogue - Poster Abstracts, SEMDIAL, Trento, Italy, 2024. URL: http://semdial.org/ anthology/Z24-Martinenghi_semdial_0045.pdf. [41] A. Martinenghi, G. Donabauer, S. Amenta, S. Bursic, M. Giudici, U. Kruschwitz, F. Garzotto, D. Ognibene, Llms of catan: Exploring pragmatic capabilities of generative chatbots through prediction and classification of dialogue acts in boardgames multi-party dialogues, in: Proceedings of the 10th Workshop on Games and Natural Language Processing@ LREC-COLING 2024, 2024, pp. 107118. 8. Online Resources More information about RoBee and Oversonic Robotics are available: RoBee, Oversonic Robotics"
        }
    ],
    "affiliations": [
        "Oversonic Robotics, Carate Brianza, Italy",
        "University of Milan, Italy",
        "University of Milano-Bicocca, Milan, Italy"
    ]
}