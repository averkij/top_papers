{
    "paper_title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems",
    "authors": [
        "Shaina Raza",
        "Ranjan Sapkota",
        "Manoj Karkee",
        "Christos Emmanouilidis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and introduce a comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with a roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment."
        },
        {
            "title": "Start",
            "content": "TRISM FOR AGENTIC AI: REVIEW OF TRUST, RISK, AND SECURITY MANAGEMENT IN LLM-BASED AGENTIC MULTI-AGENT SYSTEMS 5 2 0 2 4 ] . [ 1 3 3 1 4 0 . 6 0 5 2 : r Shaina Raza1, Ranjan Sapkota2, Manoj Karkee2, Christos Emmanouilidis3 1Vector Institute, Toronto, Canada 2Cornell University, USA 3University of Groningen, Netherlands"
        },
        {
            "title": "ABSTRACT",
            "content": "Agentic AI systems, built on large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligent autonomy, collaboration and decision-making across enterprise and societal domains. This review presents structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS). We begin by examining the conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and the emerging system designs that enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is then detailed through four pillars governance, explainability, ModelOps, and privacy/security each contextualized for agentic LLMs. We identify unique threat vectors and introduce comprehensive risk taxonomy for the agentic AI applications, supported by case studies illustrating real-world vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies in distributed LLM agent systems. Additionally, metrics for evaluating trust, interpretability, and human-centered performance are reviewed alongside open benchmarking challenges. Security and privacy are addressed through encryption, adversarial defense, and compliance with evolving AI regulations. The paper concludes with roadmap for responsible agentic AI, proposing research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment. Keywords Agentic AI, LLM-based Multi-Agent Systems, TRISM, AI Governance, Explainability, ModelOps, Application Security, Model Privacy, Autonomous Agents, Trustworthy AI, Risk Management, AI Safety, Privacy-Preserving AI, Adversarial Robustness, Human-in-the-Loop"
        },
        {
            "title": "Introduction",
            "content": "1.1 Background and Motivation The global market for AI agents is projected to grow from $5.4 billion in 2024 to $7.6 billion in 2025 [1]. By mid-2025, over 70% of enterprise AI deployments are expected to involve multi-agent or action-based systems, reflecting dramatic shift from traditional single-agent or rule-based conversational models [2]. An AI agent, typically defined as computational entity that perceives its environment and takes actions to achieve goals [3], has undergone fundamental transformation. Whereas early agents were task-specific and deterministic, modern systems, referred to as Agentic AI, now integrate large language models (LLMs), tools, and persistent memory to support complex planning and coordination [4, 5]. *Shaina Raza and Ranjan Sapkota contributed equally and are corresponding authors. Emails: shaina.raza@torontomu.ca, rs2672@cornell.edu, mk2684@cornell.edu, c.emmanouilidis@rug.nl."
        },
        {
            "title": "A PREPRINT",
            "content": "From isolated agents to Agentic AI. The autonomous software agents date back to the 1990s [6], the advent of LLM-driven multi-agent ecosystems [7] since 2023 has made Agentic AI distinctly new and rapidly expanding research focus [4]. Traditional agents automated narrow tasks, such as information retrieval [8, 9], data summarization [10, 11, 12], and dialogue response [13]. Driven by single step logic [14] or scripted rules, they lacked deep reasoning, adaptability, and persistence [15, 16]. In contrast, Agentic AI consists of collaborative agents with specialized roles (e.g., planner, coder, analyst), enabled by LLMs and tool use [17]. These systems dynamically decompose tasks, share context, and pursue high-level goals across long timelines [18, 19, 20]. This shift represents not just technological upgrade but qualitative leap in complexity and autonomy, marking the emergence of machine collectives capable of emergent, decentralized behavior. Trust, risk, and security challenges. However, this evolution introduces serious challenges. Unlike deterministic agents [21], Agentic AI systems can produce non-linear, opaque decisions, increasing the risks of failure, bias, and unintended consequences. For example, multi-agent supply chain optimizer may autonomously coordinate between procurement and logistics agents, yet inadvertently leak sensitive information or violate compliance protocols if safeguards are absent [22]. Traditional evaluation and safety frameworks, built for static or single-function AI , are no longer sufficient. This underscores the urgent need for new paradigm that integrates trust, risk, and security as core design principles. To address this gap, recent frameworks like AI TRiSM (Trust, Risk, and Security Management) [23, 24] propose lifecycle-level controls, including explainability, secure model orchestration, and privacy management. These are essential for deploying agentic systems in high-stakes domains such as finance, healthcare, and defense. Problem Statement Agentic AI systems now coordinate multiple LLM-powered agents, yet no unified framework exists for managing their trust, risk, and security (TRiSM). High-profile lapses, e.g., an autonomous research agent citing unverifiable medical sources or forecasting swarm drifting off-target due to memory inconsistencies show that traditional safeguards do not scale to dynamically evolving, multi-agent workflows. structured TRiSM perspective is therefore crucial to mitigate orchestration failures, collusion, data leakage, and opaque decision paths. This review distills current findings and outlines practical guidelines to build secure, reliable, and trustworthy Agentic AI ecosystems. Scope and Objectives This survey maps the TRiSM (Trust, Risk, and Security Management) landscape for LLM-powered, multi-agent agentic AI. We frame four pillars explainability, security, lifecycle governance, and privacy and show why safeguards designed for bounded, rule-based agents cannot contain open-world, self-orchestrating collectives [4, 17]. Our goal is to give researchers and practitioners concise, unified reference for building and governing trustworthy Agentic AI systems. Necessity of this Survey Agentic AI research often concentrates on agent modelling or task coordination and rarely tackles adversarial threats, lifecycle governance, or large-scale explainability. As deployments reach high-stakes arenas healthcare, science, finance the absence of unified TRiSM framework exposes stakeholders to opaque reasoning and unmanaged risks. This survey closes that gap by synthesising TRiSM principles for LLM-driven multi-agent systems and providing actionable guidance for researchers, engineers, and policy makers. comparison with related surveys will be presented in Table 1. 1.2 Contributions In this survey, we address the urgent need for unified governance framework for LLM-based Agentic AI systems. Our framework is shown in Figure 1. Our key contributions are as follows: Conceptual TRISM Framework Tailored to Agentic AI We introduce structured Trust, Risk, and Security Management (TRISM) framework specifically designed to accommodate the unique characteristics of multi-agent, LLM-driven systems. This framework defines four pillars: Explainability, ModelOps, Application Security, and Model Privacy, and explains how each pillar addresses the challenges of autonomy, coordination, and persistent memory in Agentic AI. Threat Taxonomy for LLM-Powered Agents We develop detailed taxonomy of risks and threat vectors that arise when multiple LLM agents collaborate. By categorizing vulnerabilities, such as prompt injection, memory poisoning, collusive failures, and emergent misbehavior, we clarify how attack surfaces expand in agentic environments and why traditional LLM security measures are insufficient. Comparative Evaluation of Existing Methods We survey and compare state-of-the-art techniques for each TRISM pillar as applied to Agentic AI. For example, we examine how explainable-AI methods (e.g., LIME, SHAP, decision provenance) have been adapted for multi-agent workflows, how ModelOps best practices must be extended to manage multiple LLM instances, and which security/hardening measures (e.g., prompt hygiene, sandboxing, access controls) are most effective."
        },
        {
            "title": "A PREPRINT",
            "content": "Table 1: Comparison of Related Surveys on LLM-based Multi-Agent Systems and TRiSM Aspects Survey Adversarial Threats Lifecycle Governance Large-Scale Explainability TRiSM Integration LLMSpecific Guo et al. (2024) [25] Chen et al. (2025) [26] Yan et al. (2025) [27] Tran et al. (2025) [28] Lin et al. (2025) [29] Fang et al. (2025) [30] This Survey (2025) Application Domains (simulated envts) (task/simulation focus) (mentions diverse) (networks, QA, etc.) (creative tasks) (health, finance cited) (high-stakes domains) Actionable Guidance (research challenges) (future research areas) (future work directions) (open challenges) (roadmap for research) (technical next steps) (practical guidance) Legend: = explicitly addressed; = partially/indirectly addressed; = not addressed. Future Research Directions for Trustworthy, Secure Agentic AI Building on our TRISM analysis, we outline concrete directions for next-generation research and setup roadmap items aim to guide researchers and practitioners toward robust, scalable, and compliant deployments of Agentic AI. This study provide unified reference for researchers, engineers, and policymakers who seek to design, evaluate, and govern LLM-based Agentic AI systems in high-stakes domains. 1.3 Paper Organization The remainder of this paper is structured as follows. In Section 2, we present our literature-search methodology, including databases queried, inclusion criteria, and classification strategies. Section 3 reviews the fundamental characteristics and typical architectures of Agentic AI systems, establishing the groundwork for subsequent TRISM analysis. Section 4 introduces the TRISM framework and its four core pillars : Explainability, ModelOps, Application Security, and Model Privacy. Building on this foundation, Section 5 identifies the primary risks, vulnerabilities, and attack surfaces inherent in multi-agent AI ecosystems. In Section 6, we explore trust-building strategies and specialized explainability techniques tailored to Agentic AI, while Section 7 details advanced security mechanisms and privacy-preserving methods. Section 8 synthesizes our findings and outlines future research directions and policy recommendations. Section 9 concludes the paper. list of key terms used throughout this paper is provided in Table 9. 1.4 Related Work LLM-Based Multi-Agent Surveys (Technical Focus): Prior surveys on LLM-driven multi-agent systems primarily emphasize system architectures, agent capabilities, and domain-specific applications of agentic AI, but often overlook critical TRiSM considerations. For instance, existing reviews [25, 26] focus on simulated environments, inter-agent communication, and performance benchmarks, yet fail to address adversarial threats, lifecycle governance, or systemwide explainability. Other targeted surveys, such as [27] on natural language communication and [28] on collaboration mechanisms, explore interaction protocols and use cases in domains like QA systems, 5G, and Industry 5.0. While the former briefly acknowledges security concerns and the latter highlights deployment contexts, both narrowly center on coordination strategies without examining robustness, governance structures, or explainability at scale. Trustworthy/Responsible AI Surveys (Broad Trust Focus): Surveys in the TRiSM domain address ethical and riskrelated concerns such as alignment, fairness, and privacy attacks [30], but typically overlook explainability, governance, and multi-agent dynamics. While they offer actionable recommendations for general ML systems, they do not account for LLM-based coordination in high-stakes domains. Current Survey (LLM Multi-Agent + TRiSM in High-Stakes Domains): This survey uniquely integrates TRiSM principles into the analysis of LLM-based multi-agent systems, specifically targeting high-stakes domains such as healthcare, science, and finance. In contrast to earlier surveys (shown in Table 1) that underemphasize governance and security, this work foregrounds adversarial threats (e.g., vulnerabilities in agent interaction), end-to-end lifecycle governance (from training data to deployment oversight), and large-scale explainability. These concerns are treated as"
        },
        {
            "title": "A PREPRINT",
            "content": "integral to system design, not peripheral. Additionally, this survey provides actionable recommendations for researchers, engineers, and policymakers, bridging gaps left by both technical and responsible-AI surveys."
        },
        {
            "title": "2 Literature Review Methodology",
            "content": "To ensure comprehensive review of the literature on trust, risk, and security in Agentic AI systems, we adopted structured methodology inspired by best practices in systematic reviews [31, 32]. This section outlines our research objectives, data sources, inclusion criteria, and classification strategy. Research Objectives The review was guided by the following questions: RQ1: What are the key trust, risk, and security challenges posed by LLM-enabled Agentic AI systems? RQ2: What technical and governance strategies have been proposed to address these challenges? RQ3: How do existing approaches map onto the TRiSM pillars: explainability, model operations, security, and privacy? RQ4: What gaps remain in current research, and what directions are promising for future work? Data Sources and Search Strategy We searched the following major digital libraries: IEEE Xplore, ACM Digital Library, SpringerLink, arXiv, ScienceDirect, and Google Scholar, covering publications from January 2020 to May 2025. The search was executed using Boolean combinations of keywords relevant to Agentic AI and TRiSM, including: \"agentic AI\" OR multi-agent systems \"(AMAS)\" OR \"multi-agent LLMs\" OR \"AI agents\" OR \"autonomous agents\" OR \"intelligent agents\") AND (\"trust\" OR \"trustworthiness\" OR \"risk\" OR \"security\" OR \"safety\" OR \"governance\" OR \"oversight\" OR \"compliance\" OR \"explainability\" OR \"interpretability\" OR \"transparency\" OR \"privacy\" OR \"data protection\") In total, 150 unique papers were initially retrieved. Inclusion and Exclusion Criteria We applied the following criteria during the screening process: Inclusion: Studies published between 2022 and 2025, to capture developments following the introduction of LLMs (mostly post-chatgpt). Studies that explicitly discuss agentic, multi-agent, or LLM-powered AI systems in the context of at least one TRiSM dimension: trust, risk, security, governance, explainability, or privacy. Peer-reviewed articles, preprints, or whitepapers from credible sources (e.g., arXiv, NeurIPS, IEEE, ACM, Nature, or governmental standards bodies). Exclusion: Papers focused solely on traditional, rule-based, or symbolic agents without any integration of LLMs or emergent coordination. Studies addressing only low-level ML components (e.g., training optimization, architecture design) without reference to agentic behavior or TRiSM concerns. Non-English papers or those lacking sufficient metadata (e.g., missing abstracts or publication details). After screening titles and abstracts, we shortlisted 250 papers. subsequent full-text review narrowed the selection to 180 primary studies that directly addressed one or more TRiSM pillars. In addition to peer-reviewed literature, we also incorporated relevant white papers, technical blogs, and practitioner reports to contextualize and map the emerging landscape of Agentic AI within the TRiSM framework. Quality Assurance To ensure the rigor and reliability of included studies, we conducted quality assessment adapted from established systematic review guidelines [33, 32]. Each paper was evaluated against the following criteria: (1) Does the study clearly define its objectives in the context of agentic or LLM-based systems? (2) Are the methods, experiments, or architectural designs well-documented and reproducible? (3) Does the paper substantively engage with at least one of the trust, risk, security, explainability, or privacy aspects? (4) Does the study offer empirical, theoretical, or normative contributions relevant to Agentic AI governance? Each criterion was rated as low, medium, or high. Studies scoring low in more than two dimensions were excluded or flagged for contextual relevance only. Quality ratings were assigned independently by two reviewers, with discrepancies resolved through discussion."
        },
        {
            "title": "A PREPRINT",
            "content": "Figure 1: Taxonomy of Agentic AI systems presented in this review. It includes five major sections: Fundamentals of Agentic AI Systems (Agentic AI vs. Traditional AI Agents, LLM-Based Architecture), The TRISM Framework (Governance, Explainability, ModelOps, Application Security, Privacy), Threats and Risks (Unique Threat Vectors, Taxonomy of Risks, Case Studies), Trust and Explainability, and Evaluation Metrics (including Human-Centered and Composite Metrics)."
        },
        {
            "title": "3 Fundamentals of Agentic AI Systems",
            "content": "3.1 Agentic AI vs. Traditional AI Agents The distinction between traditional AI agents and Agentic AI systems represents paradigm shift in autonomous system design. Traditional agents operate through predefined rules [34], heuristic workflows [35, 36], or deterministic logic [37], excelling in narrow, bounded environments. In contrast, Agentic AI systems leverage foundation models (LLMs) to achieve adaptive goal-oriented behavior through multi-agent coordination and emergent reasoning capabilities. Agentic AI systems fundamentally redefine autonomy through three core innovations: 1) Multi-agent coordination where specialized agents (e.g., planners, verifiers) collaborate via structured protocols [18, 20] 2) Persistent context through memory architectures that maintain state across workflows [38] 3) Dynamic meta-orchestration that delegates tasks and resolves conflicts [39] We present comparison of AI agents and Agentic AI in Table 2. This architecture enables longitudinal task execution and cross-domain generalization, overcoming the context fragmentation inherent in traditional approaches [40]. Systems like AutoGen [19] demonstrate how LLM-backed agents collectively decompose complex problems through multi-step reasoning [41], marking qualitative leap from reactive agents to proactive problem-solving systems."
        },
        {
            "title": "A PREPRINT",
            "content": "Table 2: Traditional AI Agents vs. Agentic AI Systems Dimension Traditional AI Agents Autonomy Model Cognitive Foundation Intelligence Scope Reasoning Approach Collaboration Temporal Context Orchestration Tool Utilization Context Awareness Scalability Exemplars Reactive execution via scripts/rules Fixed action sequences Symbolic logic & FSMs Hand-coded KBs Narrow & task-specific Single-domain focus Deterministic/single-step Rule-based inference Isolated execution Manual decomposition Episodic & stateless Session-bound Hard-coded pipelines Sequential workflows Static integrations Handcrafted interfaces Bounded inputs Limited context Domain-specific High maintenance MYCIN ELIZA Finite-state bots Agentic AI Systems Goal-driven planning & adaptation Recursive self-improvement Foundation models (LLMs/LIMs) Emergent reasoning Broad & compositional Cross-domain transfer Multi-step & contextual Chain-of-Thought (CoT) Role-specialized coordination Automated hierarchy Persistent memory VectorDB/LTM Dynamic meta-agents Conflict resolution Planned API invocation Function calling Memory-augmented RAG architectures Cross-domain transfer Modular composition AutoGen ChatDev MetaGPT AgentVerse = Core strength = Fundamental limitation = Implementation characteristic = Modern framework = Classical system 3.2 LLM-Based Agentic AI System Architecture Agentic Multi-Agent System (AMAS) represent an emerging paradigm in AI where multiple LLM-powered agents operate semi-autonomously, interact with external tools, and collaborate to achieve complex tasks. As illustrated in Figure 2, typical AMAS architecture comprises several key components that together form flexible yet highly dynamic ecosystem. At the core are multiple LLM-Based Agents, each capable of reasoning, planning, and tool invocation [42]. These agents access Shared Toolchain Interface [43] to execute code, perform searches, or interact with domain-specific APIs. Communication and coordination are facilitated through Communication Middleware [44], allowing agents to share goals, observations, or intermediate results. Task Manager or Orchestrator [45] governs high-level planning, delegating subtasks to agents based on their roles or specializations. Agents can read from and write to World Model or Shared Memory [25], which stores contextual knowledge, system state, or evolving task data. Human oversight is supported through Human-in-the-Loop Interface [36], enabling users to prompt, correct, or halt agent behavior. To ensure accountability, Trust and Audit Module monitors agent actions, logs tool usage, and generates behavioral traces [46]. However, this modular and distributed structure introduces significant TRiSM [47] challenges. With multiple autonomous agents accessing external resources, the Security Gateway becomes critical for enforcing access controls, authentication, and sandboxing [48]. Likewise, dedicated Privacy Management Layer is essential to prevent leakage of sensitive or personally identifiable information [49], especially when data traverses multiple agents or tools. Finally, an Explainability Interface must provide interpretable rationales for multi-agent decisions, supporting transparency and trust calibration [50]. Together, these architectural elements make AMAS powerful yet complex, raising unique and urgent questions about how to ensure their trustworthiness, mitigate systemic risks, and secure them against adversarial behaviors. Below, we discuss the architecture of AMAS. Language Model Core (Agent Brain). At the center of an Agentic AI system lies LLM serving as the primary decision-making controller or brain [51]. The core LLM is initialized with user goal and structured agent prompt (defining its role, capabilities, and tool access). It then generates step-by-step decisions or actions, interpreting instructions, producing reasoning traces, and selecting next steps in either natural language or structured action formats. In many agent frameworks, such as [52], Baby AGI [53] and GPT Engineer [54], the LLM governs the full control loop, orchestrating the overall system behavior. Planning and Reasoning Module. To handle complex goals, an explicit planning mechanism decomposes tasks into manageable sub-goals. This can be done internally via chain-of-thought (CoT) or tree-of-thoughts prompting [55], where the model performs intermediate reasoning before arriving at final decision. Some implementations employ external planning systems by translating goals into structured planning languages and using classical planners for long-term decision-making. Planning is often interleaved with execution and feedback: the agent refines its plan based on outcomes, alternating between reasoning, acting, and integrating observations. Techniques like ReAct [56] exemplify this loop by guiding the agent through repeated reasoningactionobservation cycles, improving performance on complex tasks."
        },
        {
            "title": "A PREPRINT",
            "content": "Figure 2: Architecture of LLM-based Agentic Multi-Agent System (AMAS), illustrating key functional layers: Perception/Input Layer (text, image, audio processing), Cognitive/Reasoning Layer (goal-setting, planning, decision-making), Action/Execution Layer (digital and physical task execution), Learning Module (supervised and reinforcement learning), Communication and Collaboration (agent messaging and coordination), Data Storage & Retrieval (centralized/distributed databases), and Monitoring & Governance (ethical oversight, observability, and compliance mechanisms). The modular design highlights adaptive intelligence and interagent synerg Memory Module. Agentic AI systems integrate memory to maintain context across iterations. This includes short-term memory (recent interactions held within the prompt context) and long-term memory (accumulated knowledge or experiences) [7]. Long-term memory is often implemented using vector databases, where key facts or past events are stored and retrieved by similarity search. By reintegrating past data into the LLMs prompt, the agent can recall relevant information across sessions, avoid repetition, and support coherent long-term planning. Effective memory management enables adaptive, learning-driven behavior. Tool-Use Interface. To extend its capabilities beyond text generation, the agent is equipped with tool-use interface [57]. This layer allows invocation of external tools such as web search, APIs, code interpreters, or databases. The available tools are specified in the agent prompt with command schemas. When the LLM determines tool is needed, it emits structured command, which is executed externally. The result is fed back into the LLM as new observation. This mechanism enables the agent to access real-time information, perform computations, and interact with external systems dynamically. Tool-use frameworks like MRKL [58] illustrate this design by routing queries to different expert modules (symbolic or learned) with an LLM as the router. Similarly, approaches like Toolformer [57] train the model to insert API calls in its text generation. Perception and Environment Interface. For agents interacting with dynamic environments, such as web interfaces, simulated worlds, or physical systems, an observation - action interface is essential [59]. Perception modules translate raw inputs (e.g., sensor data, images, or textual states) into representations the LLM can process. Conversely, the agents chosen actions are executed within the environment and the resulting state changes are returned to the agent as 7 Table 3: Representative LLM-Based Agentic AI Frameworks Framework (Year) Core LLM Planning Memory Tool Use Notable Features"
        },
        {
            "title": "A PREPRINT",
            "content": "AutoGPT [60] BabyAGI [53] GPT Engineer [61] LangGraph [62] AutoGen [19] Modelagnostic GPT-4 GPT-4 GPT-3.5/4 Task queue GPT-4V Self-looped CoT Vector DB In-memory File cache Spec-to-code OS shell + web Web search Python REPL Fully autonomous goal loop Minimal task generator End-to-end code generation pipeline FSM via graph Persistent nodes Custom modules Visual orchestration of agent Multi-agent PDDL JSON/DB API calls MRKL [58] LLaMA/GPT Prompt router N/A Reflexion [63] GPT-3.5/4 Retryreflect Episodic buffer MetaGPT [64] GPT-4 loop SOP workflow YAML state Math + search tools Same as base agent Git CLI graphs Modular, reusable agent templates Neuro-symbolic expert routing Verbal self-improvement via reflection Structured roles for software engineering Voyager [65] GPT-4 Auto skill tree Task DB WebVoyager [66] LLaVA-1.6 ReAct JSON store HuggingGPT [67] ChatGPT Task-plan-select Log store CAMEL [68] GPT-4 Role-play CoT Dialogue log ChatDev [69] GPT-4 Chat chain File repo CrewAI [70] Any Declarative plan Optional DB AgentVerse [71] OpenAgents [72] SuperAGI [73] Modelagnostic Modelagnostic GPTConfig graph Redis/KV store Agent scripts MongoDB Minecraft API Hugging Face models Chat only Lifelong open-ended learning in environments Browser actions Multimodal web interaction via vision and text External model orchestration by LLMs Multi-agent role simulation via dialogue Simulated software development workflow Python modules Lightweight, LangChain-free framework Multi-agent simulation and task solving Open platform with public hosting Web plugins Plugin API Unix tools DAG workflow Postgres Pinecone Concurrent agents for production use observations. This loop supports sense-plan-act cycles that continue until the task is completed or halted. In robotic or multimodal settings, the interface may include additional sensory models (e.g., vision transformers), but the core flow remains consistent. Integration and Autonomy. These modules together form closed-loop architecture. The LLM plans and reasons over tasks, guided by memory and tools, and interfaces with the environment to execute decisions and observe results [17]. Each iteration enriches the agents context, enabling it to self-prompt, generate subtasks, assess progress, and adapt strategies over time. This integrated design empowers agentic systems to operate autonomously, pursue long-range goals, and exhibit adaptive behavior across dynamic environments. Table 3 presents representative systems and shows how each one maps these five design axes (LLM core, planning/ reasoning, memory, tool-use, environment interface) into practice. For example, AutoGPT pairs GPT-4 core with self-looped chain-of-thought planner and vector-database memory, while Voyager couples the same core with an embodied Minecraft API that serves as both tool layer and environment interface. We comparing current Agentic AI implementations and identifying open design trade-offs in this table."
        },
        {
            "title": "4 The TRISM Framework for Agentic AI",
            "content": "4.1 Governance for Agentic LLM Systems AI Trust, Risk, and Security Management (AI TRISM) is comprehensive governance framework designed to ensure that AI systems are trustworthy, robust, and compliant with safety standards [24]. Originally highlighted in industry guidelines for AI governance [74], TRISM provides structured approach to manage the unique challenges of LLMbased agentic AI systems. Such systems consist of autonomous LLM agents that can make independent decisions,"
        },
        {
            "title": "A PREPRINT",
            "content": "collaborate with other agents, and adapt their behavior over time [46]. These properties: autonomy, multi-agent interaction, and evolving behavior, introduce new risks not seen in traditional single-model deployments [75]. For example, an agent acting in isolation might be benign, but when interacting with others across organizational or trust boundaries, it could manipulate peers or leak confidential information. The TRISM framework addresses these concerns by focusing on four key pillars: Explainability, Model Operations (ModelOps), Application Security, and Model Privacy [46, 24]. Each pillar targets critical aspect of safety or risk management, ensuring that an agentic LLM system remains transparent, reliable, secure, and compliant with ethical and regulatory requirements. Below, we define each pillar and explain how it applies to LLM-based agentic systems, grounding the discussion in current research and best practices. 4.2 Explainability in Multi-Agent Decision Making Definition and Importance Explainability refers to making the inner workings and decisions of AI agents interpretable to humans. In the context of agentic LLM systems, explainability is paramount for building user trust, as outcomes often emerge from complex interactions among multiple agents rather than single models prediction [76]. Accordingly, the TRISM framework elevates explainability as core pillar to ensure that each agents actions and the overall systems behavior can be understood and audited. Techniques for Explainability in Agentic Systems Achieving explainability in multi-agent LLM systems is challenging because one must interpret not only individual model decisions, but also the inter-agent dynamics that lead to final outcomes. Established explainable AI techniques provide starting point. For instance, Local Interpretable ModelAgnostic Explanations (LIME) [77] and Shapley Additive Explanations (SHAP) [78] can be adapted to analyze LLM decisions. These techniques identify which features or input factors most influenced an agents output, offering insight into why particular action or response was taken. In an agentic setting, feature might be component of the agents input context or signal from another agent. Beyond local explanations, counterfactual analysis is increasingly important for multi-agent explainability. Counterfactual techniques examine how the systems behavior would change if certain conditions were altered [79], for example, if particular agents contribution were removed or modified. This approach, rooted in causal inference, helps isolate each agents role in collaborative decision-making. For instance, by systematically toggling an agent off or varying its outputs, one can observe changes in the collective outcome and thus determine that Agent was critical in influencing decision Y. Such analyses surface the dependencies and influences among agents, effectively explaining emergent behaviors at the system level. Moreover, recent research on explainable AMAS suggests logging the intermediate reasoning steps (e.g. chain-of-thought prompts or dialogue between agents) can further enhance transparency. Human auditors can also help with trace of how agents arrived at decision, e..g, which agent contributed what information and why, to produce narrative explanations for its outcomes. Summary In summary, the explainability pillar of TRISM compels the use of these techniques (surrogate models, feature attributions, counterfactual testing, and transparent reasoning traces) to ensure that even highly autonomous LLM agents remain interpretable and accountable to human oversight. 4.3 ModelOps : Lifecycle Management for LLM Agents Definition and Scope ModelOps is the discipline of managing AI models through their entire lifecycle, from development and deployment to monitoring, maintenance, and eventual retirement [80]. It extends the principles of MLOps (Machine Learning Operations) to focus specifically on model governance and reliable operation in production. Within agentic LLM systems, ModelOps encompasses not just individual models, but the orchestration of multiple agents and the supporting infrastructure that keeps them running safely [46]. Effective ModelOps is crucial for maintaining consistency, performance, and regulatory compliance as LLM agents evolve or as new agents are added to system. Application to Agentic Systems LLM-based agents require rigorous lifecycle governance because their behavior can change with model updates, prompt adjustments, or environmental drift. cornerstone of ModelOps in this context is version control, i.e., tracking and managing versions of each agents model and prompt configurations. Additionally, robust CI/CD pipelines (Continuous Integration/Continuous Deployment) are employed to automatically test agents performance and safety whenever model is fine-tuned or an agents logic is modified. Before deployment, multi-agent simulations and unit tests validate that new agent behaviors do not introduce regressions or unsafe interactions. This aligns with best practices in LLM operations (LLMOps) [81], which integrate MLOps principles tailored to the challenges of large language models Challenge One challenge with this method is model drift, i.e., over time, an agents responses may become less accurate or relevant as data distributions shift or real-world conditions change. Continuous monitoring is therefore required"
        },
        {
            "title": "A PREPRINT",
            "content": "to detect performance degradation or deviations from expected behavior, triggering retraining or recalibration when needed. Moreover, real-time monitoring and logging are fundamental in multi-agent settings. Each agents actions (e.g. API calls, decisions taken, errors encountered) are logged and analyzed to provide observability into the systems functioning. In large-scale agent ecosystem, orchestration services may oversee the agents, scheduling their tasks and managing inter-agent communication. ModelOps must govern these orchestration layers as well, ensuring, for instance, that if one agent fails or produces suspicious output, it can be isolated or shut down without collapsing the whole system. Summary In line with TRISM objectives, robust ModelOps thereby ensures that an Agentic AI system remains reliable and maintainable. It formalizes change management (so updates do not introduce new risks), provides continuous validation of model behavior, and supports compliance by logging data for audits and enforcing policies (e.g. preventing unauthorized model changes). 4.4 Application Security for LLM Agents Threats in Agentic Environments Application security in the TRISM framework focuses on protecting AI agents and their ecosystem from malicious attacks and misuse. LLM-based agents are susceptible to range of novel security threats that exploit their language-based interfaces and cooperative behaviors. One well-documented threat is prompt injection [82], wherein an attacker designs input data that contains hidden or malicious instructions . Recent studies [83] have shown that in AMAS, such prompt injections can propagate from one agent to another, phenomenon dubbed prompt infection is analogous to computer virus spreading across networks [84]. In prompt infection scenario, malicious prompt introduced to Agent might covertly modify Agent As output, which then becomes part of Agent Bs input, thereby tricking Agent as well, and so on. This cascading attack can lead to widespread data leaks, fraudulent transactions, misinformation, or coordinated misbehavior across an agent society. Another critical vulnerability is identityspoofing and impersonation [85]. In multi-agent system, agents often communicate or coordinate tasks, and they may rely on credentials or tokens to authenticate one another. For example, if an adversary steals an agents API key or tricks the system into treating rogue model as trusted peer, they could issue commands or receive information under false identity. Defensive Measures: To mitigate these threats, TRISMs security pillar mandates defense-in-depth approach tailored to LLM agents: 1. Prompt hygiene: agents should treat inputs defensively by sanitizing and filtering prompts, and using guardrails or content policies to detect and refuse suspicious instructions. Prompt hardening (e.g., adding secure prefixes or validation steps) is one such method to make agents less susceptible to injection [86]. 2. Strong authentication and access controls Each agent and human user must be securely authenticated, and least-privilege principles [87] should constrain what actions an agent can perform autonomously. 3. Continuous monitoring If an agent suddenly starts issuing unusual requests or deviates from its normal pattern of behavior, automated monitors can flag this for investigation or trigger an automatic shutdown of the agents actions. Recent frameworks, such as LangChain/LangFlow [88], AutoGen [89], CrewAI [70], introduce the idea of trust scores or reputation among agents, where agents verify each others outputs and cross-check decisions to catch inconsistencies or signs of compromise. Moreover, training LLM agents with adversarial robustness in mind (e.g. fine-tuning on adversarial examples, employing adversarial training regimes) [84] can improve their resistance to malicious inputs Summary In sum, the Application Security pillar of TRISM emphasizes proactive safeguards against both external attackers and potential rogue agents. By implementing strict authentication, input validation, encrypted communication, sandboxing of execution (for agents that can use tools or code), and comprehensive monitoring, organizations can significantly reduce the risk of prompt-based exploits, impersonation, and other lateral vulnerabilities that are unique to autonomous multi-agent AI systems hal.science. This layered security approach is essential to maintain the integrity and reliability of agentic LLM deployments in adversarial environments. 4.5 Model Privacy and Data Protection Challenges in Data Sharing: The Model Privacy pillar addresses the protection of sensitive data within AI agent systems, ensuring that the use of personal or confidential information complies with privacy regulations and ethical norms. LLM-based agents often need to handle user data, proprietary business information, or other sensitive inputs to fulfill their tasks [75]. In multi-agent context, this challenge is amplified by the fact that agents may share information 10 Table 4: TRISM pillars for LLM-based Agentic AI and associated governance references. Pillar Primary Objective Key Practices & Agentic-LLM Considerations Exemplar Standards / Metrics"
        },
        {
            "title": "A PREPRINT",
            "content": "Explainability Transparent multi-agent decisions ModelOps Lifecycle governance of agents, models, orchestration Application Security Prevent prompt tion, spoofing, exploits Safeguard sensitive data and shared memory Model Privacy injeclateral LIME, SHAP, reasoning-trace dependency mapping logs; counterfactuals; cross-agent sanitation; Version-controlled prompts; CI/CD w/ multi-agent sims; drift monitoring; rollback policies Prompt least-privilege scopes; encrypted comms; sandboxed tool calls; anomaly detectors Differential-privacy training; data minimization; homomorphic encryption; secure enclaves; audit logs NIST AI RMF Explain [96] ISO/IEC 24029-1 [97] Metrics: fidelity, sufficiency, humantrust score ISO/IEC 42001 (AI MS) [98] SRE MTTR/MTTD; -accuracy (model drift); pipeline pass-rate OWASP Top-10 for LLM Apps [99] Jailbreak-success%; mean-time-todetect; exploit CVSS GDPR Art. 25 [100] HIPAA 164 [101] Metrics: ϵ-DP budget, leakage rate, access-audit pass-rate with each other (e.g. via shared memory store or message passing) to collaborate. Without strict privacy controls, there is risk that an agent could inadvertently expose private data to unauthorized parties or that sensitive information could leak through the language models outputs. Therefore, TRISMs privacy pillar compels organizations to institute measures that safeguard data throughout the AI lifecycle, from training and inference to inter-agent communication. Techniques for Privacy Preservation Differential Privacy (DP): Injects calibrated noise during model training to prevent memorization of individual data entries, ensuring no single record significantly influences the output [90]. Data Anonymization and Minimization: Limits inter-agent data sharing to only what is necessary [91], often using aggregated or pseudonymized formats (e.g., age bracket 30-40 instead of exact birthdate). Secure Multi-Party Computation (SMPC): Enables agents to compute joint functions without exposing private inputs, useful in cross-organization tasks like collaborative fraud detection [92]. Homomorphic Encryption (HE): Allows agents to compute on encrypted data [93]. With Fully Homomorphic Encryption (FHE), even the plaintext query and response remain unseen by the agent. Trusted Execution Environments (Secure Enclaves): Hardware-based isolation ensures that even privileged system users cannot access the data being processed by agents [94]. Useful for secure memory sharing and execution. Model Privacy Policies and Compliance: Enforces data retention limits, maintains audit logs, and ensures compliance with regulations (e.g., GDPR, HIPAA) governing agent behavior and data usage [95]. Summary By implementing these layers of privacy defense , from differential privacy in model training to homomorphic encryption for data sharing, and stringent access control policies, Agentic AI systems can protect user data and proprietary information even as they leverage that data for intelligent decision-making. The TRISM framework offers comprehensive governance model for LLM-based Agentic AI, integrating Explainability, ModelOps, Application Security, and Model Privacy to manage the complexity of autonomous agent systems. Grounded in proven methods, such as SHAP, CI/CD, adversarial defenses, and homomorphic encryption, TRISM enhances safety, transparency, and trust. As AI systems evolve, TRISM provides stable foundation to ensure responsible and secure agent behavior, aligning advanced capabilities with human values and operational integrity. Table 4 condenses each pillar into its governing goal."
        },
        {
            "title": "5 Threats and Risks in LLM-based (Agentic) AMAS",
            "content": "In this section, we discuss the threats and risks in AMAS. 5.1 Unique Threat Vectors Agentic AI systems introduce distinct set of security and reliability concerns compared to traditional single-agent LLM architectures [102]. These risks primarily arise from agents autonomy, persistent state management, and the complex demands of multi-agent coordination [4]. Below, we discuss these threats"
        },
        {
            "title": "A PREPRINT",
            "content": "Autonomy abuse The foremost threat is autonomy abuse, whereby agents with significant decision-making authority might misinterpret objectives or implement harmful plans due to erroneous reasoning or manipulated inputs [103, 104]. Unlike deterministic models, agentic systems dynamically generate actions, complicating efforts to define and enforce safe operational states [105, 106]. Persistent memory Another threat is the persistent memory, which while, crucial for context retention, introduces unique vulnerabilities through potential adversarial injections and accumulations [107, 108]. Such contamination can propagate subtly via shared memory, especially in the absence of detailed version control and robust audit mechanisms [103, 102]. Agent orchestration This risk involves central or distributed control mechanisms for role assignment and workflow mediation. compromised orchestrator could distort task distribution or misroute information, triggering cascading failures, issues exemplified by documented vulnerabilities in MetaGPT [20] and AutoGen [19]. These orchestration vulnerabilities differentiate agentic systems markedly from conventional stateless, single-threaded LLM deployments. 5.2 Taxonomy of Risks To systematically understand the security landscape in Agentic AI, we categorize risks into four broad classes: adversarial attacks, data leakage, agent collusion, and emergent behaviors. Adversarial Attacks Agents remain vulnerable to prompt injections, gradient-based manipulations, and engineered reasoning traps, risks that are magnified in AMAS due to propagation across agent interactions [109, 110]. An illustrative instance is the role-swapping attack observed in ChatDev [18]. Data Leakage Persistent memory and extensive inter-agent communication elevate the likelihood of accidental exposure of sensitive information [111, 112]. In sensitive domains like financial services and HR, inadequate boundary enforcement and ineffective sanitization amplify these leakage risks [113, 114]. Agent Collusion and Mode Collapse Coordination mechanisms can inadvertently lead agents to reinforce mutual errors, precipitating groupthink or echo chambers [115, 116]. AutoGen experiments illustrate how iterative dialogues among agents can amplify flawed designs, highlighting the risk of emergent misalignment [19, 117, 118]. Emergent Behavior Complex interactions among agents, memory components, tools, and tasks yield unpredictable behaviors that evade traditional testing and validation methods [119, 120]. Agents optimizing for efficiency may unintentionally bypass critical verification steps or suppress contradictory information, scenarios exemplified in blockchain [121] and audio verification contexts [122]. 5.3 Case Studies Several real-world and research-based examples illustrate the tangible impact of these risks in deployed or experimental agentic systems. Case Study 1: Prompt Leakage in Agentic Systems. Instances of prompt leakage have been observed in LLM-based agent frameworks such as AutoGPT, where recursive prompt augmentation and insufficient memory controls can lead to the unintentional exposure of sensitive information. In one reported scenario, sensitive tokens were stored in persistent memory and later surfaced in planning summaries or external logs. Such vulnerabilities underscore the critical importance of implementing memory sanitization, access controls, and prompt boundary protections to safeguard agentic systems from cascading information leaks [123]. Case Study 2: Collusive Failure in ChatDev. In collaborative code generation session involving planner, coder, and tester agents within the ChatDev framework, an error in shared planning module led to the propagation of faulty design assumptions. Due to the absence of external ground-truth or objective feedback loops, all agents validated each others outputs, resulting in feedback loop of erroneous confirmations. This scenario underscores the necessity of incorporating diverse information sources and adversarial checks within agent loops to prevent such collusive failures [69, 83]. Case Study 3: Simulation Attack in Swarm Robotics. In simulated swarm robotics experiment utilizing LLM-based planning strategies, an agent was provided with misleading environmental assumption, leading to coordination failure characterized by spatial congestion and task incompletion. This incident underscores the potential vulnerabilities in real-world deployments, particularly in critical infrastructure or logistics, where such failures could have significant consequences. The case highlights the importance of robust validation mechanisms and the integration of diverse information sources to ensure reliable swarm behavior [124, 125]. Case Study 4: Memory Poisoning in Multi-Agent Chatbots. In multi-agent customer support system, customerfacing agent injected sarcastic feedback into persistent feedback buffer. This buffer was later utilized by the"
        },
        {
            "title": "A PREPRINT",
            "content": "policy improvement agent to adapt dialogue strategies, resulting in responses with inappropriate tones. This incident underscores the importance of implementing validation filters, sentiment monitoring, and robust feedback loop governance in self-adapting systems to prevent such memory poisoning vulnerabilities [126]. Case Study 5: System Prompt Drift in Autonomous Memory Agents In experiments with agents using system-level memory (e.g., LangGraph or BabyAGI), over time, system prompts began drifting due to self-appended contextual memory that wasnt properly versioned or validated. This led to hallucinated goals and emergent behaviors misaligned with initial intentions [127]. Points to risks from prompt accumulation and the need for memory version control, audit trails, and reset mechanisms. These cases illustrate that the introduction of autonomy, memory, and orchestration into LLM-based AI introduces an expanded threat surface that cannot be mitigated with traditional LLM security protocols alone. As agentic systems evolve, new methodologies are needed for rigorous, system-wide threat modeling and runtime assurance that span multiple agents, roles, and memory contexts."
        },
        {
            "title": "6 Trust and Explainability in Agentic AI",
            "content": "Agentic AI systems are highly autonomous agents capable of making decisions and taking actions without continuous human oversight. These systems also pose unique challenges and opportunities for human trust. Ensuring that users and stakeholders have confidence in such systems is crucial for their adoption in real-world settings. Two key factors that influence trust in Agentic AI are the transparency of the agents decision-making processes and the ability to explain or justify its actions in human-understandable terms. 6.1 Building Trust in Autonomous Agents In Agentic AI systems, building trust is foundational to user acceptance, system reliability, and responsible deployment especially as these systems begin to make autonomous decisions in critical domains such as healthcare, finance, and scientific research [128]. Unlike traditional software agents, autonomous LLM-based agents are characterized by self-directed reasoning, adaptive memory, and dynamic collaboration, which make their operations opaque and often unpredictable [129]. Establishing trust in such systems, therefore, requires combination of technical transparency, user feedback integration, and robust oversight mechanisms. Table 5: Trust-Enabling Mechanisms in Agentic AI Systems Mechanism Purpose Example System(s) Trust Contribution Transparency via Reasoning Traceability Status Reporting and Intent Disclosure Human-in-the-Loop (HITL) Behavioral Consistency & Bounded Autonomy Social Trust Cues Make agent decisions observable through intermediate steps or explanations Provide real-time updates or clarification of progress and errors Enable user oversight over high-stakes or irreversible actions Ensure agents operate within predefined roles and limits Use polite language, uncertainty expression, and turn-taking SciAgent [130], MetaGPT [131] Enhances interpretability and debuggability ChatDev [18], AutoGen [89] Mimics human team transparency; reduces uncertainty ChemCrow [132], enterprise dashboards Prevents unsafe execution; reinforces control Adaptive tutors [133, 134] GPT-4 safety-tuned profiles [104, 135] Improves predictability; prevents overreach Enhances user comfort and trust in uncertain scenarios Transparency and Decision Traceability Transparency is one of the core enablers of trust. For users to understand and evaluate agentic decisions, the reasoning chains, decision states, and action triggers of agents must be made observable. Several agentic systems are now integrating decision traceability through mechanisms such as CoT prompting and self-explanation modules. For example, SciAgent [130] generates scientific summaries and provides justifications by linking outputs to source documents retrieved via retrieval-augmented generation (RAG). Similarly, MetaGPT[131] structures its reasoning using role-based outputs, where each agent (e.g., planner, coder) explicitly states the logic behind its task execution, creating modular interpretability."
        },
        {
            "title": "A PREPRINT",
            "content": "Status Reporting and Progress Visibility Clear reporting of intent and intermediate status is also essential. Human collaborators often require updates about what an agent is doing, why task is taking longer, or how an agent interprets ambiguous instructions. Tools like AutoGen[89] and ChatDev[18] have incorporated structured chat interfaces where agents summarize their intermediate progress, decisions, and encountered errors. Human-in-the-Loop Oversight Human oversight and intervention mechanisms further reinforce trust. Allowing human users to review, edit, or approve agent-generated outputs not only prevents missteps but also signals that the system respects user authority [136, 137, 138]. Many systems adopt human-in-the-loop (HITL) paradigm [139], where agents request confirmation before executing high-risk or irreversible actions. Behavioral Consistency and Bounded Autonomy Trust requires predictability. Agents should follow defined roles, output in expected formats, and remain within delegated authority. Example In enterprise AI platforms used for automated data analytics, agents may generate insights or dashboards but defer publishing until domain expert reviews the material. Similarly, in autonomous research assistants like ChemCrow [132], agents pause to allow chemists to validate proposed reactions or data pipelines before proceeding, reinforcing safe deployment. Social Trust Cues and Language Behaviors Beyond system-level mechanisms, behavioral consistency and bounded autonomy are crucial. If an agent behaves unpredictably or inconsistently, even if technically correct, users are less likely to trust it. Behavioral alignment mechanisms such as predefined role protocols, output style consistency, and language modeling constraints help standardize responses [140, 17, 141]. Example In adaptive educational platforms using AI tutors, agents may be allowed to revise lesson plans but not change grading criteria, preserving institutional trust boundaries [133, 134]. Lastly, social trust cues, such as polite language, turn-taking, and cooperative gestures, have shown promise in reinforcing user trust even in non-expert settings [142]. Studies have found that users trust agents more when they express uncertainty (\"Im not sure, but heres what found\") rather than overconfidence [104, 135]. This has been implemented in models like GPT-4 when configured with safety-tuned instruction sets, improving reliability perception without undermining capability. Together, these mechanisms form layered trust strategy for Agentic AI, as summarized in Table 5. As autonomy and complexity increase, combining transparency, oversight, and social alignment will be essential to sustain user confidence. 6.2 Explainability Techniques Explainability remains cornerstone in fostering trust, accountability, and reliability in Agentic AI systems, particularly as they operate in high-stakes environments where multi-agent coordination and autonomous decisions directly impact human lives [17]. In contrast to conventional AI systems, Agentic AI introduces unique challenges for explainability due to its decentralized architecture, dynamic role assignment, and evolving task decomposition among multiple interacting agents [4]. Technique LIME [77] SHAP [78] Decision Provenance Graphs [143, 144] Multi-agent SHAP [145] Symbolic/Rule-Based Agents [4] Attention Maps [146, 147] Prompt Audit Trails [148, 149] Table 6: Explainability Techniques in Agentic AI Systems Purpose Challenges in Agentic AI Example Use Cases Local post-hoc explanation using surrogate models Attribution of prediction to input features via Shapley values Trace inter-agent decision paths and data flows Attribution extended to agents and shared memory Built-in interpretability via logic or rule-based models Visualize focus of language/image models Log input prompts, outputs, and agent actions Fails to capture cross-agent dependencies in multi-agent setups Difficult to aggregate across agents with divergent contexts Feature-level auditing in fraud detection Financial risk analysis, regulatory explanations Can be complex to interpret at scale High complexity, lack of standard frameworks Limited flexibility compared to LLMs Only reflects internal focus, not logic or intent Requires infrastructure, may miss implicit state changes Collaborative document generation, planning chains Strategic gameplay, collaborative writing Hybrid planning systems, educational tutors Multimodal VQA, image captioning Debugging multi-agent sequences, fine-tuning"
        },
        {
            "title": "A PREPRINT",
            "content": "Local Post-hoc Techniques (LIME and SHAP) Local Interpretable Model-Agnostic Explanations (LIME) [77] and SHapley Additive exPlanations (SHAP) [78] are widely adopted techniques that offer post-hoc interpretability. LIME approximates black-box model locally using an interpretable surrogate model, while SHAP attributes predictions to input features via Shapley values. These techniques have been integrated into agentic pipelines, particularly in finance and multi-agent fraud detection systems, where feature-level transparency supports regulatory compliance. However, their direct application in Agentic AI is limited. Each agent may operate with its own objectives, context, and tool access, leading to divergent decision paths that local techniques struggle to reconcile [76, 150]. Explainability in AMAS Emergent behavior poses another challenge: the interpretability of an individual agent does not necessarily imply the interpretability of the overall system. In platforms such as ChatDev [18] or AutoGen [19], agents emulate specialized roles (e.g., engineer, reviewer), and tracing final action back to its source agent is often non-trivial. To address this, researchers have proposed composite frameworks that combine local explanations with global decision traceability [151, 152, 153]. For example, decision provenance graphs visualize communication flows and interdependencies across agents [143, 144], while causal influence chains track how actions propagate between roles [154, 155]. Adapted SHAP techniques for multi-agent setups now aim to attribute outcomes to shared memory and agent collaboration [145]. Symbolic and Hybrid Architectures Another promising direction is the use of inherently interpretable modules such as rule-based planners and decision trees within hybrid architectures [4]. These agents offer built-in explainability but retain the generative capabilities of LLMs for broader context understanding. Such designs are increasingly used in domains where structure and interpretability are prioritized, such as educational AI or mission planning. Lightweight Interpretability: Attention and Prompts Attention map visualizations have been used to highlight focus areas in multimodal language agents [146, 156, 147], offering lightweight but informative views into model behavior. Prompt audit trails logging prompt history, agent actions, and response metadata have also gained traction [148, 149]. These mechanisms support system debugging, safety evaluations, and human-in-the-loop fine-tuning in multi-agent environments. Ongoing Challenges and Research Directions Despite these advancements, achieving robust explainability in Agentic AI systems remains an open research problem. Many techniques focus on isolated predictions or modules and fail to capture system-level dynamics. Future work should prioritize longitudinal interpretability across agent interactions, causal reasoning pipelines, and interactive querying interfaces that support transparency in real time. comparative summary of current techniques is shown in Table 6. 6.3 Evaluation Metrics for Agentic AI Systems Agentic AI systems demand comprehensive evaluation across multiple dimensions beyond traditional accuracy. We outline five key categories of metrics: trustworthiness, explainability, user-centered performance, coordination, and composite scores, each capturing distinct aspect of an Agentic AIs performance and its real-world implications.. Below we discuss these metrics and summarize in Table 7. Trustworthiness: This category evaluates the reliability, safety, and ethical alignment of the AI agent. trustworthy agent consistently produces correct and unbiased results, adheres to constraints, and avoids harmful or unpredictable behavior. Metrics for trustworthiness include success rates on tasks under varying conditions (measuring robustness), violation rates of safety or ethical guidelines (which should be minimal), and calibration of the agents confidence (how well the agents self-reported confidence aligns with actual accuracy) [157]. Some approaches combine such factors into an overall trust score. For example, one model defines trustworthiness score as = + + , (1) where is the agents credibility (accuracy and correctness of its outputs), is reliability (consistency of performance over time), is the level of user alignment or rapport (analogous to intimacy in trust modeling), and is selforientation (the degree to which the agent pursues its own goals over the users goals). higher indicates an agent that is accurate, consistent, user-aligned, and not self-serving, which corresponds to greater trustworthiness. In practice, achieving high trustworthiness means the agent behaves predictably and transparently in accordance with ethical AI principles (such as fairness and accountability). Explainability: Explainability metrics assess how well the agents decisions can be understood and traced by humans [158]. These metrics focus on the clarity and completeness of the rationale the agent provides for its actions. For"
        },
        {
            "title": "A PREPRINT",
            "content": "instance, one can measure the explanation coverage (the percentage of decisions or outputs that come with an adequate explanation) and the fidelity of explanations (how accurately the explanation reflects the true reasoning or model logic). Consistency of explanations for similar scenarios is another important metric: the agent should explain comparable decisions in similar way, indicating stable reasoning process. Quantitatively, methods like OpenXAI provide suite of metrics to evaluate explanation quality across dimensions such as faithfulness, stability, and fairness of explanations [159]. High explainability builds user trust, as users can follow why the agent made decision, and it aids debugging by revealing the agents internal decision process. In regulated domains (e.g., healthcare or finance), explainability is often essential for compliance and user acceptance. User-Centered Performance: User-centered metrics capture how effectively the AI agent interacts with and satisfies the end-users needs. These criteria emphasize the users experience and outcomes [160]. Key metrics include user satisfaction ratings, typically collected via surveys or feedback after interaction, which reflect whether the users goals were met and how comfortable they felt with the agents behavior. Task success from the users perspective (did the agent fulfill the users request or solve the users problem?) is fundamental measure. Additionally, interaction metrics like the number of back-and-forth clarification queries needed (fewer indicates the agent understood the user well) and the coherence or naturalness of the conversational flow contribute to user-centered evaluation. Human-in-the-loop evaluations are often employed here: for example, user studies might rate the agent on criteria such as helpfulness, clarity and naturalness of language, and adherence to user instructions. Ultimately, user-centered agentic system should align its actions with user intent and preferences. Benchmarks like ChatDev [18], which simulates multi-agent software development team interacting via natural language, implicitly evaluate how well agents fulfill user-defined roles and requirements in collaborative project. This highlights the importance of user-oriented success in complex, realistic tasks. Table 7: Summary of key evaluation metrics for Agentic AI systems. Each category addresses different facet of an autonomous agents performance and behavior. Metric Aspect Evaluation Focus (Examples of Metrics) Trustworthiness Reliability and safety of the agents behavior. Example metrics: task success rate across diverse scenarios; frequency of rule or safety violations (lower is better); calibration of confidence vs. outcomes; fairness and bias indices. Explainability User-Centered Coordination Composite Transparency and interpretability of the agents decisions. Example metrics: percentage of outputs with an accompanying explanation; explanation fidelity to actual model reasoning; consistency of explanations for similar cases; human interpretability ratings of explanations. User satisfaction and alignment with user needs. Example metrics: user satisfaction score (postinteraction survey); rate of fulfilling user-defined goals; number of clarification questions needed (lower is better); naturalness and coherence of dialog from the users perspective. Effective collaboration in multi-agent or modular systems. Example metrics: multi-agent task completion rate; communication overhead (e.g., messages exchanged); consistency of shared plans or beliefs among agents; synergy score quantifying complementary actions. Overall performance across metrics. Example metrics: weighted aggregate score encompassing trust, explainability, user-centric, and coordination measures; specialized composite indices (e.g., TUE for tooluse efficacy); cross-domain benchmark results (aggregate success across diverse tasks, as in AgentBench). Coordination (Multi-Agent or Modular): In scenarios where an Agentic AI system consists of multiple cooperating agents or modular components, coordination metrics gauge how effectively these parts work together. Good coordination means agents share information, divide labor without conflict or redundancy, and converge on solutions efficiently. Quantitative measures include the team success rate on collaborative tasks (whether the group of agents achieves the overall goal) and communication efficiency metrics (e.g., the number of messages or iterations required among agents to reach decision, with fewer often indicating more efficient interaction). One specific example is the Component Synergy Score (CSS), which counts or weights effective interactions between agents, reflecting how well each agents actions complement the others (a higher CSS means agents are synergistic rather than working at cross-purposes). Multi-agent frameworks such as ChatDev and MetaGPT provide practical testbeds for these metrics: they orchestrate specialized agents (e.g., different roles in software engineering pipeline) that must cooperate to complete complex projects. Evaluations on such frameworks examine whether agents maintain consistent shared plan, handle inter-agent dependencies smoothly, and recover from misunderstandings. For instance, if one agent generates plan and another"
        },
        {
            "title": "A PREPRINT",
            "content": "executes it, coordination metric would assess if the executing agent follows the planners intent correctly and whether both agents remain in agreement throughout the process. High coordination scores indicate that the agentic system functions as cohesive unit, which is crucial for complex tasks beyond the capability of any single agent. Composite Metrics: Composite metrics aggregate multiple evaluation aspects into single overall score. These are useful for summarizing an agents performance holistically, especially when comparing different systems. composite metric is often weighted combination of the above categories, for example: Mcomposite = wT MT + wEME + wU MU + wCMC , (2) where MT , ME, MU , MC are the normalized scores (on common scale) for trustworthiness, explainability, usercentered performance, and coordination respectively, and wT , wE, wU , wC are weights reflecting the relative importance of each aspect for given application. The choice of weights wi can be domain-specific (for instance, in healthcare applications, trustworthiness and explainability might be weighted more heavily than raw efficiency). An example of specialized composite metric in agentic contexts is the Tool Utilization Efficacy (TUE) score, which combines sub-metrics evaluating how correctly and efficiently an agent uses external tools (including proper tool selection and correct parameter usage in tool calls) into one measure. By condensing multiple criteria, composite metrics enable high-level comparison and benchmarking of agentic systems. For instance, AgentBench [161] is comprehensive benchmark that evaluates agents across diverse range of tasks and environments (from operating system manipulation to web shopping), effectively providing composite performance profile of an agent. Such aggregated scores highlight if an agent performs strongly across the board or if it excels in some dimensions while underperforming in others. It is important to interpret composite scores in light of their components: single number can mask specific weaknesses (e.g., an agent might achieve high overall score by doing well in task completion and coordination, yet still have poor explainability). Therefore, composite metrics are most informative when accompanied by breakdown of the agents performance per category."
        },
        {
            "title": "7 Security and Privacy in LLM-based Agentic Multi-Agent Architectures",
            "content": "7.1 Security Mechanisms Agentic AI systems, composed of loosely coupled yet collaboratively functioning LLM-based agents, introduce an expanded attack surface relative to conventional AI agents [162]. Ensuring the security of such systems necessitates multi-layered defense architecture that addresses data protection, execution integrity, inter-agent communication, and model robustness [163]. Among the foundational techniques employed are encryption [164, 165], access control [166, 167], adversarial defense [168, 169], and runtime monitoring[170, 171] each adapted to the unique demands of decentralized multi-agent environments. Encryption plays an important role in safeguarding data exchanged between agents, especially when sensitive or regulated content (e.g., healthcare records, financial data) is involved [165, 49, 172]. Agentic workflows often include inter-agent handoff of partially processed results, models, or prompts. Implementations such as SSL/TLS, homomorphic encryption [173, 172], and secure enclaves (e.g., Intel SGX ) are increasingly integrated into Agentic AI pipelines to ensure confidentiality across message-passing protocols. Access control becomes crucial when orchestrators or shared memory modules manage permissions for agents with distinct capabilities and responsibilities. For instance, in systems like AutoGen[19] and CrewAI [70] where agents take on specialized roles (e.g., summarizer, planner, coder) [174], enforcing principle-of-least-privilege access prevents privilege escalation and unauthorized tool invocation [19, 170, 175]. Agent-based access control policies often aligned with Role-Based Access Control (RBAC) [176, 177] and Attribute-Based Access Control (ABAC) [178, 177] paradigms can dynamically restrict which agents may access sensitive APIs, files, or memory buffers, based on contextual trust levels [4]. Adversarial robustness is growing concern as LLM-based agents are susceptible to prompt injection, manipulation through poisoned tool outputs, or coordination disruption via malformed intermediate results [179, 180]. Recent studies have shown that multi-agent LLM frameworks can be destabilized by adversarially crafted outputs from one compromised agent propagating misleading information to others [180, 163, 102, 42, 103, 181]. Adversarial training methods, such as input perturbation, reward shaping, and contrastive learning, can partially mitigate these vulnerabilities [182, 102]. Integrating safety constraints and verifying tool responses before execution are also effective mitigation strategies. https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/ software-guard-extensions.html"
        },
        {
            "title": "A PREPRINT",
            "content": "Runtime monitoring systems support the detection of anomalous agent behaviors, especially in high-stakes domains like automated healthcare or cybersecurity [183]. Log-based auditing, anomaly detection with LSTM or autoencoderbased detectors, and trust scoring among agents are becoming essential components of real-time surveillance layers [184, 185, 114]. For example, Microsofts Copilot governance layers monitor anomalous agent behavior across sessions to ensure compliant execution and flag potentially harmful interactions. As Agentic AI continues to expand into mission-critical domains, developing standardized, scalable security mechanisms will be paramount. Future approaches must include zero-trust frameworks, secure multi-party computation, and formal verification of inter-agent protocols to ensure safe and resilient operation across decentralized autonomous agent collectives. 7.2 Privacy-Preserving Techniques The decentralized and interactive nature of LLM-based Agentic AI systems introduces new challenges for preserving privacy, especially as agents continuously communicate, access external data sources, and store episodic or shared memory. To ensure data confidentiality and protect personally identifiable information (PII), Agentic AI systems must adopt robust privacy-preserving techniques such as differential privacy, data minimization, and secure computation. Differential privacy (DP) offers mathematically grounded guarantees by injecting statistical noise into outputs, ensuring that individual user contributions cannot be re-identified. In multi-agent LLM systems, DP can be applied during training or at inference-time when agents exchange information. For example, Googles implementation of DP in federated learning frameworks can be extended to distributed agentic systems, where agents collaboratively train or fine-tune local models without exposing raw data [186, 187]. DP-SGD and privacy budgets (ϵ-differentials) can regulate information exposure during policy updates or collaborative planning in real-time decision-making agents. Data minimization is another cornerstone of privacy preservation. Agentic AI systems can mitigate exposure risks by limiting the scope, granularity, and duration of data collected or retained during task execution. For instance, temporary memory buffers used in systems like ChatDev or ReAct-based pipelines are cleared once subgoals are completed, preventing persistent storage of unnecessary user data. Furthermore, anonymization and pseudony [187], techniques can help remove identifying features before data is passed between agents or stored in shared memory repositories. Secure computation techniques including secure multi-party computation (SMPC), homomorphic encryption, and trusted execution environments (TEEs) enable agents to perform computations over encrypted or obfuscated data without compromising privacy. In scenarios where agents collaborate across different organizational boundaries (e.g., federated medical agents or cross-silo industrial agents), SMPC allows joint computations such as diagnostics or anomaly detection without data leakage. Homomorphic encryption, while computationally expensive, is increasingly being explored to allow arithmetic operations on encrypted vectors used in RAG workflows. Privacy-by-design principles are becoming central to the engineering of next-generation agentic systems. Architectures now embed user consent layers, configurable privacy settings, and memory redaction modules that allow end-users or system administrators to control what agents can remember or share. As Agentic AI expands into domains such as personalized education, healthcare, and finance, ensuring privacy-respecting behaviors will be essential for regulatory compliance (e.g., GDPR, HIPAA) and public trust. 7.3 Compliance and Governance As Agentic AI systems grow in capability and autonomy, ensuring regulatory compliance and instituting robust governance mechanisms become imperative. Unlike traditional AI agents, agentic systems operate with greater autonomy, persistent memory, and complex decision flows necessitating layered oversight to manage legal, ethical, and societal implications. Effective governance in this context spans three critical dimensions: adherence to regulatory standards, system-level auditability, and enforceable policy frameworks. Regulatory standards provide the baseline requirements that all AI systems, including agentic architectures, must meet. Frameworks such as the NIST AI Risk Management Framework (AI RMF) and the EU AI Act define principles for trustworthy AI including transparency, accountability, and fairness. These standards are especially relevant for LLM-based AMAS that interact in high-stakes domains like healthcare, finance, defense, or transportation. For example, the EU AI Act classifies certain autonomous systems as high-risk, requiring continuous risk monitoring, documentation of decision logic, and human oversight mechanisms attributes directly relevant to Agentic AI. Auditability is critical in ensuring transparency and facilitating post-hoc accountability. Each decision, plan, or interaction within an agentic system should be logged with timestamps, context, agent role, and justification. Techniques such as decision provenance and action traceability enable this, allowing regulators or internal auditors to reconstruct"
        },
        {
            "title": "A PREPRINT",
            "content": "Table 8: Governance Dimensions for Agentic AI Systems Governance Component Description Example Tools or Methods Regulatory Compliance Auditability and Logging Policy Enforcement Human Oversight Risk Monitoring Aligning with legal and industry standards for responsible AI [188] Capturing traceable records of agent actions and decisions [167] Enforcing system-level operational, ethical, and security constraints [40, 190] Human-in-the-loop [139] decision checkpoints and override capabilities Continuous identification and mitigation of systemic or emergent risks [194] Explainability Governance Ensuring interpretability standards are met in decision pipelines [195] Adaptive Governance Incident Response and Recovery Updating governance mechanisms based on model evolution and deployment context [104, 128] Procedures for handling, logging, and recovering from failures or breaches NIST AI RMF [96], EU AI Act [189], GDPR [100], HIPAA [101] Decision provenance, immutable logs, blockchain-based traceability RBAC/ABAC policies [191], formal logic rules, dynamic sandboxing [48, 192, 193] Human review agents, interactive dashboards, compliance checkpoints Model risk scanners, out-of-distribution detectors, reinforcement learning audit modules LIME/SHAP [196, 197, 198], decision trees [199, 200], interpretable surrogate models [201, 202, 203] Governance-as-code [95, 204], auto-updated rule engines [205, 206, 207], learning-based policy adaptation [115, 208] Real-time alerting, kill-switch mechanisms, secure rollback protocols how decisions were reached. For instance, in systems like AutoGen or MetaGPT, where agents assume specialized roles (e.g., researcher, coder, reviewer), audit trails can capture role-specific actions and flag inconsistencies, bias amplification, or security violations. Blockchain-based audit logs are also being explored to ensure immutability and verifiability of multi-agent interactions. Policy enforcement governs what agentic systems can and cannot do, and under which conditions. These policies must be codified into the orchestration layers or meta-agent governance modules that manage agent interactions. Examples include enforcing memory expiration policies to avoid data retention violations or restricting access to external tools based on role and authentication level. Role-based access control (RBAC) and attribute-based access control (ABAC) are essential for enforcing differentiated privileges across agent subcomponents. Furthermore, real-time monitoring systems can halt or flag agent activity that deviates from pre-specified ethical constraints or operational bounds, using formal verification tools like TLA+ or symbolic execution engines. Emerging best practices also include the creation of AI governance boards, the adoption of governance-as-code platforms, and the integration of adaptive governance layers that evolve as the agentic system scales or changes context. These practices are designed to meet not only current standards like ISO/IEC 42001 for AI management systems, but also prepare for future regulatory evolutions. Below, we summarize the core governance components required for secure and compliant Agentic AI deployment. Thus, governance in Agentic AI systems is not static compliance checkbox but dynamic, adaptive layer embedded into the orchestration and operational pipelines. As these systems become more autonomous and integrated across sectors, aligning governance with system behavior, human values, and evolving legal standards is imperative for safe and trustworthy deployment, as summarized in Table 8."
        },
        {
            "title": "8 Discussion",
            "content": "our exploration of TRiSM-based governance for LLM-powered Agentic AI systems reveals critical insights into technical design, ethical oversight, regulatory alignment, and future challenges. Below, we discuss the broader implications of our findings, structured into key areas for clarity. 8.1 Technical Implications of TRiSM for LLM Agent Design The AI TRiSM framework (Trust, Risk, and Security Management) imposes concrete technical requirements on how autonomous LLM-driven agents are built and deployed. core implication is the need to embed real-time monitoring and control mechanisms into agent architectures. Rather than treating LLMs agents as black-box decision-makers, TRiSM encourages instrumenting them with continuous oversight guardrails [209]. For example, there is discusison on designing specialized guardian agents within AMAS [210]. In this paradigm, such agents serve as proactive monitors that filter sensitive data and establish baselines of normal behavior, while operator agents dynamically enforce"
        },
        {
            "title": "A PREPRINT",
            "content": "policies at runtime (e.g. blocking disallowed actions such as outputting personally identifiable information). This layered control strategy transforms the technical architecture: an autonomous LLM agent is now supplemented by meta-agents that supervise its inputs, outputs, and tool use in real time. Prior research highlights risks like excessive agency [87] where an LLM given too much autonomy or tool access can produce unintended harmful actions (for instance, via hallucination or misinterpreted goals). TRiSM-driven agentic design mitigates these failure modes by constraining agent autonomy within well-defined safety bounds. Likewise, emerging threats specific to Agentic AI such as prompt injection [82] attacks, memory poisoning [126], or cascading hallucinations [211]underscore the need for built-in risk controls. By incorporating anomaly detection and policychecking modules (as per the Sentinel/Operator model), an LLM agent can detect deviations from normal behavior and either alert humans or automatically neutralize the threat (e.g. masking sensitive datum or stopping an unsafe action). This aligns with calls in the security community for proactive measures in Agentic AI, blending traditional cybersecurity techniques (access control, logging, sandboxing) with AI-specific safeguards like LLM firewalls and adversarial robustness checks [212]. In summary, TRiSMs technical implications mean that autonomous LLM agents should no longer be deployed as stand-alone intelligent actors; instead, they operate under an active governance fabric of monitors, validators, and enforcement agents that ensure trustworthiness and safety by design. 8.2 Ethical and Societal Ramifications of Multi-Agent AI Beyond technical matters, deploying networks of autonomous LLM agents raises pressing ethical and societal questions. Applying TRiSM in this context emphasizes principles of accountability, human oversight, and fairness [188] all of which are vital for public trust in AI systems. central concern is accountability: when AI agents make autonomous decisions that affect humans, who is answerable for the outcomes? TRiSM-based governance insists that organizations retain clear responsibility for their AIs actions, rather than obscuring blame behind algorithmic black boxes. This implies implementing audit trails and explicable decision logs so that any harmful or biased outcome can be traced and attributed [148]. Recent guidance on trustworthy AI [190] often highlights accountability and explainability as key pillars of trust. In practice, our approach means each autonomous agents decisions should be transparent enough to be understood and challenged by human reviewers when necessary. Human oversight is another ethical imperative tightly coupled with accountability. TRiSM does not seek to eliminate humans from the loop; rather, it provides structured way for humans and AI agents to collaborate under defined governance. Human operators or AI managers must have the ability to intervene or override when an agents behavior deviates from acceptable bounds or when moral judgment is required. Indeed, high-level policy frameworks (such as the EUs AI ethics guidelines [189]) explicitly call for human agency and oversight in AI systems. In multi-agent setups, this may involve dashboard interfaces where humans can monitor agent swarms in real time, pause or shut down agents exhibiting anomalies, and adjust policies on the fly [115]. The risk of user complacency, trusting an autonomous agent too much , has been noted as hazard [213]. TRiSM governance counteracts this by formalizing oversight roles and ensuring no AI operates without appropriate human or regulatory supervision. Fairness and bias mitigation are also critical societal considerations. Our governance approach therefore incorporates bias audits and fairness checks throughout the agent lifecycle [214]. Techniques like pre-deployment bias testing, continuous monitoring for disparate impacts, and diverse stakeholder evaluation panels can be employed. These measures echo regulatory expectations; the EUs AI Act [189] and related guidelines enumerate diversity, non-discrimination and fairness as core requirements for trustworthy AI. In deploying LLM-based agents, we must ensure they do not treat individuals or groups inequitably, for example, content-filtering agents should apply policies uniformly across demographic groups, and task-planning agents should not propagate historical biases in resource allocation decisions. In sum, TRiSM-oriented governance extends beyond preventing technical failures: it seeks to uphold ethical norms and human rights, ensuring that autonomy in AI does not come at the expense of justice, transparency, or human dignity. 8.3 Alignment with Emerging AI Regulations and Standards The principles embedded in TRiSM align closely with emerging regulatory frameworks for AI. This convergence means that adopting TRiSM-based governance can help organizations meet new legal obligations and industry standards. For example, the European Unions AI Act [215] (set to fully apply in 2026) mandates rigorous risk management, transparency, data governance, and human oversight for high-risk AI systems. These are precisely the capabilities that TRiSM approach cultivates."
        },
        {
            "title": "A PREPRINT",
            "content": "By instituting continuous risk assessment, documentation of AI decision processes, and oversight mechanisms, TRiSM-governed multi-agent system inherently addresses many of the EU Acts requirements (e.g. having risk management system and post-market monitoring for AI. Notably, the Act also stresses accuracy, robustness, and cybersecurity for high-risk AI, qualities that TRiSMs security management component is designed to ensure (through adversarial resilience, access control, etc.). Similarly, international AI governance standards are emerging that mirror TRiSMs tenets. ISO/IEC 42001:2023, the first global standard for AI management systems, highlights requirements such as transparency, accountability, bias mitigation, safety, and privacy in AI development TRiSMs trust and risk management focus naturally encompasses these elements: for instance, trust in TRiSM relates to reliable, truthful outputs (promoting transparency), while explicit risk management aligns with accountability for negative outcomes. By implementing TRiSM, organizations essentially put in place the processes that ISO 42001 [98] and similar standards call for (e.g. leadership oversight, documented risk controls, ongoing monitoring and improvement cycles). Another example is the U.S. NIST AI Risk Management Framework [96], which emphasizes many of the same concepts: identifying risks, embedding governance, and cultivating trustworthiness in AI. By following TRiSM guidelines: for example, maintaining an AI catalog of all models/agents in use and their purposes, enforcing policies via sentinel/operator agents, and logging every AI decision, organizations create an audit-ready environment. In the event of an incident or an inquiry by authorities, they can demonstrate traceability and control over their autonomous agents, which will be crucial for regulatory compliance and liability management. 8.4 Limitations and Current Research Gaps While the TRiSM-based approach appears promising, our work also revealed several limitations and open challenges in current research. First, limited benchmark evaluations pose problem. The AI safety and agent governance community lacks widely accepted benchmarks to quantitatively assess trustworthiness or risk in multi-agent LLM systems. Unlike classical AI domains (vision, NLP) that have standard test suites, there is no consensus on how to measure an AI agent ability to operate safely under TRiSM principles. This makes it difficult to compare different governance strategies or to track progress objectively. We encourage future work to develop evaluation frameworks, possibly extending from adversarial attack simulations [182] or red-teaming exercises [216] that can stress-test agentic systems and score their resilience (e.g. measuring success rates of prompt injection attacks or frequency of policy violations caught by oversight agents). Secondly, there is paucity of real-world validation for many TRiSM-inspired controls. Much of the existing literature and tooling for LLM agent safety has been demonstrated in laboratory settings or on narrowly scoped tasks [47]. It remains uncertain how these governance mechanisms perform in complex, open-ended real-world environments. Additionally, integrating TRiSM with legacy systems poses practical challenges , for example, the earlier work notes compatibility issues when bolting on trust/security layers to existing AI pipelines. This suggests limitation in how easily current AI deployments can fit TRiSM controls, topic that merits further engineering research. Another critical gap is adversarial robustness. As we improve defenses, attackers will inevitably adapt. Recent findings show that LLM-based systems remain vulnerable to cleverly crafted attacks (for example, hidden prompt injections or subtle data poisoning) that can bypass superficial guardrails [84]. TRiSM solutions today are not foolproof against these. For example, an agent designed to mask secret data might itself be tricked into revealing it if the oversight logic fails to anticipate new attack pattern. The literature identifies evolving threats and adversarial attacks as ongoing obstacles to trustworthy AI [210]. This underlines need for continuous updates and adaptive security in any TRiSM implementation. Finally, organizational and human factors present limitations: implementing TRiSM requires interdisciplinary expertise (AI specialists, security experts, ethicists, legal advisors) and clear governance structures. Many organizations lack the necessary skill sets or frameworks, making TRiSM adoption superficial or inconsistent. Without strong organizational commitment, even the best technical framework can falter. 8.5 Future Roadmap for Agentic AI TRiSM Drawing on our findings and best practices from multiple disciplines, we propose several actionable directions for future research and implementation, as shown in Figure 3. These recommendations span both technical system design improvements and governance-level policy initiatives: Develop Standardized Evaluation Benchmarks: The community should create open benchmarks and challenge environments to test multi-agent AI governance. For instance, suite of scenario-based tasks (with built-in threats and ethical dilemmas) could be used to evaluate how well TRiSM-governed agent system performs relative to one"
        },
        {
            "title": "A PREPRINT",
            "content": "Benchmarking & Evaluation Ethics & Governance Cognitive Capability Expansion Autonomous Lifecycle Management"
        },
        {
            "title": "Future\nRoadmap\nfor Agentic AI",
            "content": "Scalable Agent Architectures Human-Centered Trust Design Secure Multi-Agent Collaboration Explainable Multi-Agent Decisions Figure 3: Strategic roadmap for LLM-enabled Agentic AI, grouped into eight priority research and development domains. without such controls. This will enable direct comparisons and drive progress on measurable metrics of trust (e.g. frequency of prevented failures or fairness outcomes). Advance Adversarial Robustness Techniques: Future system design must anticipate continually evolving threat landscape. Techniques from cybersecurity (e.g. adversarial training, AI model penetration testing [217], and formal verification) should be integrated into the LLM agent development pipeline. Cross-disciplinary collaboration with security experts can yield LLM-specific hardening methods, such as dynamic prompt anomaly detectors or robust tool APIs that constrain agent actions. Additionally, creating red-team/blue-team exercises for AMAS , akin to cyber wargames [218] , can help discover vulnerabilities in controlled way before real adversaries do. Human-Centered Oversight Tools: We encourage designing better interfaces and protocols for human oversight of Agentic AI. Borrowing from human-computer interaction [219]and cognitive engineering [220], researchers could devise dashboards that visualize an agent societys state, flag important decisions, and allow intuitive human intervention (pausing agents, rolling back actions, etc.). Regulatory Sandboxes and Compliance-by-Design: Policymakers and industry should collaborate to create regulatory sandboxes for multi-agent AI trials. These would be controlled environments where innovators can deploy Agentic AI under supervision, demonstrating TRiSM controls to regulators. Insights from such pilots can inform refinements in both technical standards and regulations. Moreover, adopting compliance-by-design mindset is key: future AI system designs should bake in the requirements of frameworks like the EU AI Act and ISO 42001 from the startrather than retrofit them. Cross-Domain Best Practices and Ethical Governance: There is much to learn from other high-stakes domains. For example, the safety engineering field (e.g. aerospace, automotive) has mature practices for redundant controls and failure mode analysis; these could inspire analogous practices in AI agent design. Likewise, ethics boards in biomedical research provide template for AI ethics committees that review agent behaviors and approve high-risk deployments. We advocate establishing multidisciplinary governance boards that include ethicists, legal experts, domain specialists, and community representatives to oversee significant deployments of autonomous AI."
        },
        {
            "title": "9 Conclusion",
            "content": "TRiSM-based governance offers promising scaffold to ensure that autonomous LLM-powered agents are trustworthy, accountable, and secure. Our discussion has analyzed how this framework influences technical design decisions, mandates ethical guardrails, and dovetails with emerging regulatory regimes. While current research is nascent and"
        },
        {
            "title": "A PREPRINT",
            "content": "not without limitations, the path forward is clear. By rigorously testing these systems, strengthening them against adversaries, and crafting policies and standards in tandem with technological advances, we can enable powerful multi-agent AI systems to operate beneficially under robust oversight. The stakes are high but with proactive, interdisciplinary approach, we can achieve balance where innovation in AI goes hand-in-hand with responsibility and trust. As future work tackles the open challenges identified, we anticipate that TRiSM principles will transition from conceptual best-practice to standard operating procedure for Agentic AI, ensuring these systems earn and maintain the confidence of all stakeholders involved."
        },
        {
            "title": "References",
            "content": "[1] Chris Miller. 30+ powerful ai agents statistics in 2025: Adoption & insights, 2025. Accessed: 2025-06-02. [2] Lyzr AI. The state of ai agents in enterprise: H1 2025, 2025. Accessed: 2025-06-02. [3] Stuart Russell. Rationality and intelligence. Artificial intelligence, 94(1-2):5777, 1997. [4] Ranjan Sapkota, Konstantinos Roumeliotis, and Manoj Karkee. Ai agents vs. agentic ai: conceptual taxonomy, applications and challenge. arXiv preprint arXiv:2505.10468, 2025. [5] De Zarzà, De Curtò, Gemma Roig, Pietro Manzoni, and Carlos Calafate. Emergent cooperation and strategy adaptation in multi-agent systems: An extended coevolutionary theory with llms. Electronics, 12(12):2722, 2023. [6] Cristiano Castelfranchi. Modelling social action for ai agents. Artificial intelligence, 103(1-2):157182, 1998. [7] Yingxuan Yang, Qiuying Peng, Jun Wang, and Weinan Zhang. Multi-llm-agent systems: Techniques and business perspectives. arXiv preprint arXiv:2411.14033, 2024. [8] Ivey Chiu and LH Shu. Biomimetic design through natural language analysis to facilitate cross-domain information retrieval. Ai Edam, 21(1):4559, 2007. [9] Michael Lew, Nicu Sebe, Chabane Djeraba, and Ramesh Jain. Content-based multimedia information retrieval: State of the art and challenges. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 2(1):119, 2006. [10] Doru Tanasa and Brigitte Trousse. Advanced data preprocessing for intersites web usage mining. IEEE Intelligent Systems, 19(2):5965, 2005. [11] David DeVault, Ron Artstein, Grace Benn, Teresa Dey, Ed Fast, Alesia Gainer, Kallirroi Georgila, Jon Gratch, Arno Hartholt, Margaux Lhommet, et al. Simsensei kiosk: virtual human interviewer for healthcare decision support. In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, pages 10611068, 2014. [12] Dapeng Hao, Tao Ai, Frank Goerner, Xuemei Hu, Val Runge, and Michael Tweedle. Mri contrast agents: basic chemistry and safety. Journal of Magnetic Resonance Imaging, 36(5):10601071, 2012. [13] David Griol, Javier Carbó, and José Molina. An automatic dialog simulation technique to develop and evaluate interactive conversational agents. Applied Artificial Intelligence, 27(9):759780, 2013. [14] Michael Genesereth and Nils Nilsson. Logical foundations of artificial intelligence. Morgan Kaufmann, 2012. [15] Pieter Spronck, Marc Ponsen, Ida Sprinkhuizen-Kuyper, and Eric Postma. Adaptive game ai with dynamic scripting. Machine Learning, 63:217248, 2006. [16] Daniel Szafir and Bilge Mutlu. Pay attention! designing adaptive agents that monitor and improve user engagement. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 1120, 2012. [17] Deepak Bhaskar Acharya, Karthigeyan Kuppan, and Divya. Agentic ai: Autonomous intelligence for complex goalsa comprehensive survey. IEEE Access, 2025. [18] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023. [19] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversation. arXiv preprint arXiv:2308.08155, 2023. [20] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 3(4):6, 2023."
        },
        {
            "title": "A PREPRINT",
            "content": "[21] Bernard Zeigler. Object-oriented simulation with hierarchical, modular models: intelligent agents and endomorphic systems. Academic press, 2014. [22] Mei Yii Lim. Memory models for intelligent social companions. In Human-computer interaction: The agency perspective, pages 241262. Springer, 2012. [23] Alice Gomstyn and Alexandra Jonker. What is ai trism?, March 2025. Accessed: 2025-06-02. [24] Adib Habbal, Mohamed Khalif Ali, and Mustafa Ali Abuzaraida. Artificial intelligence trust, risk and security management (ai trism): Frameworks, applications, challenges and future research directions. Expert Systems with Applications, 240:122442, 2024. [25] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges, 2024. [26] Shuaihang Chen, Yuanxing Liu, Wei Han, Weinan Zhang, and Ting Liu. survey on llm-based multi-agent system: Recent advances and new frontiers in application, 2025. [27] Bingyu Yan, Xiaoming Zhang, Litian Zhang, Lian Zhang, Ziyi Zhou, Dezhuang Miao, and Chaozhuo Li. Beyond self-talk: communication-centric survey of llm-based multi-agent systems, 2025. [28] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry OSullivan, and Hoang D. Nguyen. Multi-agent collaboration mechanisms: survey of llms, 2025. [29] Yi-Cheng Lin, Kang-Chieh Chen, Zhe-Yan Li, Tzu-Heng Wu, Tzu-Hsuan Wu, Kuan-Yu Chen, Hung yi Lee, and Yun-Nung Chen. Creativity in llm-based multi-agent systems: survey, 2025. [30] Xingli Fang, Jianwei Li, Varun Mulchandani, and Jung-Eun Kim. Trustworthy ai on safety, bias, and privacy: survey, 2025. [31] Angela Boland, Gemma Cherry, and Rumona Dickson. Doing systematic review: student guide. 2017. [32] Staffs Keele et al. Guidelines for performing systematic literature reviews in software engineering. Technical report, Technical report, ver. 2.3 ebse technical report. ebse, 2007. [33] Barbara Kitchenham. Procedures for performing systematic reviews. Keele, UK, Keele University, 33(2004):126, 2004. [34] Nicola Muscettola, Pandurang Nayak, Barney Pell, and Brian Williams. Remote agent: To boldly go where no ai system has gone before. Artificial intelligence, 103(1-2):547, 1998. [35] Markus Hannebauer. From formal workflow models to intelligent agents. In Proceedings of the AAAI-99 Workshop on Agent Based Systems in the Business Context, pages 1924, 1999. [36] Justin Cranshaw, Emad Elwany, Todd Newman, Rafal Kocielnik, Bowen Yu, Sandeep Soni, Jaime Teevan, and Andrés Monroy-Hernández. Calendar. help: Designing workflow-based scheduling agent with humans in the loop. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, pages 23822393, 2017. [37] Paul Govaerts, Bart Vaerenberg, Geert De Ceulaer, Kristin Daemers, Carina De Beukelaer, and Karen Schauwers. Development of software tool using deterministic logic for the optimization of cochlear implant processor programming. Otology & Neurotology, 31(6):908918, 2010. [38] Uwe Borghoff, Paolo Bottoni, and Remo Pareschi. Human-artificial interaction in the age of agentic ai: system-theoretical approach. Frontiers in Human Dynamics, 7:1579166, 2025. [39] Johannes Schneider. Generative to agentic ai: Survey, conceptualization, and challenges. arXiv preprint arXiv:2504.18875, 2025. [40] Christian Schroeder de Witt. Open challenges in multi-agent security: Towards secure systems of interacting ai agents. arXiv preprint arXiv:2505.02077, 2025. [41] Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Agent planning with world knowledge model. Advances in Neural Information Processing Systems, 37:114843114871, 2024. [42] Kun Wang, Guibin Zhang, Zhenhong Zhou, Jiahao Wu, Miao Yu, Shiqian Zhao, Chenlong Yin, Jinhu Fu, Yibo Yan, Hanjun Luo, et al. comprehensive survey in llm (-agent) full stack safety: Data, training and deployment. arXiv preprint arXiv:2504.15585, 2025. [43] April Speight. Build ai agents end-to-end in vs code. https://techcommunity.microsoft.com/blog/ azuredevcommunityblog/build-ai-agents-end-to-end-in-vs-code/4418117, May 2025. Microsoft Developer Community Blog."
        },
        {
            "title": "A PREPRINT",
            "content": "[44] Danny Weyns, Alexander Helleboogh, Tom Holvoet, and Michael Schumacher. The agent environment in multi-agent systems: middleware perspective. Multiagent and Grid Systems, 5(1):93108, 2009. [45] Microsoft Corporation. Langgraph: Agent orchestration framework for llms. https://www.langchain.com/ langgraph, 2024. Accessed: 2025-06-03. [46] Jagreet Kaur. Building trust with ai trism: Managing risks in the era of agentic ai. Akira AI Blog, December 2024. Accessed: 2025-06-03. [47] CharliAI Inc. Trust, Risk and Security Management in AI Systems. https://charliai.com/wp-content/ uploads/2024/01/TRiSM-Position-Paper-January-2024.pdf, January 2024. Accessed: 2025-06-03. [48] Bingzhuo Zhong, Hongpeng Cao, Majid Zamani, and Marco Caccamo. Towards safe ai: Sandboxing dnns-based controllers in stochastic games. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1534015349, 2023. [49] Georgios Feretzakis, Konstantinos Papaspyridis, Aris Gkoulalas-Divanis, and Vassilios Verykios. Privacypreserving techniques in generative ai and large language models: narrative review. Information, 15(11):697, 2024. [50] Madhuri Singh, Amal Alabdulkarim, Gennie Mansi, and Mark Riedl. Explainable reinforcement learning agents using world models. arXiv preprint arXiv:2505.08073, 2025. [51] Elvis Saravia. Llm agents. https://www.promptingguide.ai/research/llm-agents, 2024. Accessed: 2025-06-03. [52] Significant Gravitas. Autogpt. https://github.com/Significant-Gravitas/AutoGPT, 2023. Accessed: 2025-06-03. [53] Yohei Nakajima. Babyagi (archived version). https://github.com/yoheinakajima/babyagi_archive, 2024. Accessed: 2025-06-03. [54] Anton Osika. gpt-engineer. https://github.com/AntonOsika/gpt-engineer, 2023. Accessed: 2025-0603. [55] Elvis Saravia. Prompt engineering guide. https://www.promptingguide.ai/, 2024. Accessed: 2025-06-03. [56] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. [57] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023. [58] Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai ShalevShwartz, Amnon Shashua, and Moshe Tenenholtz. Mrkl systems: modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning, 2022. [59] Kaiming Liu, Xuanyu Lei, Ziyue Wang, Peng Li, and Yang Liu. Agent-environment alignment via automated interface generation. arXiv preprint arXiv:2505.21055, 2025. [60] Hui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional opinions. arXiv preprint arXiv:2306.02224, 2023. [61] Anton Osika. gpt-engineer: Cli platform to experiment with codegen. https://github.com/AntonOsika/ gpt-engineer, 2023. MIT License. [62] Jialin Wang and Zhihua Duan. Agent ai with langgraph: modular framework for enhancing machine translation using large language models. arXiv preprint arXiv:2412.03801, 2024. [63] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning, 2023. [64] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. MetaGPT: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2024. [65] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models, 2023. [66] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024."
        },
        {
            "title": "A PREPRINT",
            "content": "[67] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. [68] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. [69] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative agents for software development, 2024. [70] João Moura and contributors. Crewai: Framework for orchestrating role-playing, autonomous ai agents. https://github.com/crewAIInc/crewAI, 2023. MIT License. [71] Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi Lu, Ruobing Xie, et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2023. [72] Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu, Hongjin Su, Dongchan Shin, Caiming Xiong, and Tao Yu. Openagents: An open platform for language agents in the wild, 2023. [73] TransformerOptimus and contributors. Superagi: dev-first open source autonomous ai agent framework. https://github.com/TransformerOptimus/SuperAGI, 2023. MIT License. [74] Avivah Litan. Ai trust and ai risk: Tackling trust, risk and security in ai models. Gartner, December 2024. Accessed: 2025-06-03. [75] Ronny Ko, Jiseong Jeong, Shuyuan Zheng, Chuan Xiao, Taewan Kim, Makoto Onizuka, and Wonyong Shin. Seven security challenges that must be solved in cross-domain multi-agent llm systems, 2025. [76] Avi Rosenfeld and Ariella Richardson. Explainability in humanagent systems. Autonomous agents and multi-agent systems, 33:673705, 2019. [77] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \" why should trust you?\" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 11351144, 2016. [78] Scott M. Lundberg and Su-In Lee. unified approach to interpreting model predictions. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 47654774. Curran Associates Inc., 2017. [79] Matt Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. Advances in Neural Information Processing Systems, 2017-Decem:40674077, 2017. [80] Kay Lefevre, Chetan Arora, Kevin Lee, Arkady Zaslavsky, Mohamed Reda Bouadjenek, Ali Hassani, and Imran Razzak. Modelops for enhanced decision-making and governance in emergency control rooms. Environment Systems and Decisions, 42(3):402416, September 2022. [81] Dominik Kreuzberger, Niklas Kühl, and Sebastian Hirschl. Machine learning operations (mlops): Overview, definition, and architecture, 2022. [82] Matthew Kosinski and Amber Forrest. What is prompt injection attack? IBM Think, March 2024. Accessed: 2025-06-03. [83] Donghyun Lee and Mo Tiwari. Prompt infection: Llm-to-llm prompt injection within multi-agent systems, 2024. [84] Shaina Raza, Rizwan Qureshi, Marcelo Lotif, Aman Chadha, Deval Pandya, and Christos Emmanouilidis. Just as humans need vaccines, so do models: Model immunization to combat falsehoods, 2025. [85] Jay Chen and Royce Lu. Ai agents are here. so are the threats. Unit 42 Blog, May 2025. Accessed: 2025-06-03. [86] Ante Gojsalic. System prompt hardening: The backbone of automated ai security. SplxAI Blog, December 2024. Accessed: 2025-06-03. [87] OWASP Agentic Security Initiative. Agentic ai threats and mitigations. Technical report, Open Worldwide Application Security Project (OWASP), February 2025. Accessed: 2025-06-03. [88] LangChain Team. Langchain and langgraph: Comparing function and tool calling capabilities. LangChain Blog, April 2024. Accessed: 2025-06-03. [89] Microsoft Open Source. Agents autogen agentchat user guide. https://microsoft.github.io/autogen/ stable//user-guide/agentchat-user-guide/tutorial/agents.html, 2024. Accessed: 2025-06-02."
        },
        {
            "title": "A PREPRINT",
            "content": "[90] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography Conference, pages 265284. Springer, 2006. [91] Latanya Sweeney. k-anonymity: model for protecting privacy. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 10(05):557570, 2002. [92] Yehuda Lindell and Benny Pinkas. Secure two-party computation via cut-and-choose oblivious transfer. In Advances in CryptologyEUROCRYPT 2007, pages 329346. Springer, 2007. [93] Craig Gentry. Fully homomorphic encryption using ideal lattices. In Proceedings of the 41st annual ACM symposium on Theory of computing, pages 169178. ACM, 2009. [94] Victor Costan and Srinivas Devadas. Intel sgx explained. In IACR Cryptology ePrint Archive, volume 2016, page 86, 2016. [95] Google AI. Responsible AI practices. url{https://ai.google/responsibilities/responsible-ai-practices/}, 2021. [96] Ravit Dotan, Borhane Blili-Hamelin, Ravi Madhavan, Jeanna Matthews, and Joshua Scarpino. Evolving ai risk management: maturity model based on the nist ai risk management framework, 2024. [97] International Organization for Standardization. Iso/iec tr 24029-1:2021 artificial intelligence (ai) assessment of the robustness of neural networks part 1: Overview. Technical report, ISO/IEC, 2021. Available at https://www.iso.org/standard/77608.html. [98] International Organization for Standardization. Iso/iec 42001:2023 artificial intelligence management system (ai ms) requirements. Technical report, ISO/IEC, 2023. Available at https://www.iso.org/standard/ 81230.html. [99] Open Worldwide Application Security Project (OWASP). OWASP Top 10 for Large Language Model Applications. https://owasp.org/www-project-top-10-for-large-language-model-applications/, 2024. Accessed: 2025-06-03. [100] European Union. General Data Protection Regulation (GDPR) Article 25: Data protection by design and by default. https://gdpr-info.eu/art-25-gdpr/, 2016. Accessed: 2025-06-03. [101] U.S. Department of Health and Human Services. HIPAA Privacy Rule 45 CFR Part 164: Security and Privacy Protections for Health Information. https://www.ecfr.gov/current/title-45/subtitle-A/ subchapter-C/part-164, 2003. Accessed: 2025-06-03. [102] Zehang Deng, Yongjian Guo, Changzhou Han, Wanlun Ma, Junwu Xiong, Sheng Wen, and Yang Xiang. Ai agents under threat: survey of key security challenges and future pathways. ACM Computing Surveys, 57(7):136, 2025. [103] Atharv Singh Patlan, Peiyao Sheng, Ashwin Hebbar, Prateek Mittal, and Pramod Viswanath. Real ai agents with fake memories: Fatal context manipulation attacks on web3 agents. arXiv preprint arXiv:2503.16248, 2025. [104] Noam Kolt. Governing ai agents. arXiv preprint arXiv:2501.07913, 2025. [105] Lewis Hammond, Alan Chan, Jesse Clifton, Jason Hoelscher-Obermaier, Akbir Khan, Euan McLean, Chandler Smith, Wolfram Barfuss, Jakob Foerster, Tomáš Gavenˇciak, et al. Multi-agent risks from advanced ai. arXiv preprint arXiv:2502.14143, 2025. [106] Robb Wilson and Josh Tyson. Age of Invisible Machines: Guide to Orchestrating AI Agents and Making Organizations More Self-Driving, Revised and Updated. John Wiley & Sons, 2025. [107] Yiming Du, Wenyu Huang, Danna Zheng, Zhaowei Wang, Sebastien Montella, Mirella Lapata, Kam-Fai Wong, and Jeff Pan. Rethinking memory in ai: Taxonomy, operations, topics, and future directions. arXiv preprint arXiv:2505.00675, 2025. [108] Chad DeChant. Episodic memory in ai agents poses risks that should be studied and mitigated. arXiv preprint arXiv:2501.11739, 2025. [109] Honglei Miao, Fan Ma, Ruijie Quan, Kun Zhan, and Yi Yang. Autonomous llm-enhanced adversarial attack for text-to-motion. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 61446152, 2025. [110] Aishan Liu, Yuguang Zhou, Xianglong Liu, Tianyuan Zhang, Siyuan Liang, Jiakai Wang, Yanjun Pu, Tianlin Li, Junqi Zhang, Wenbo Zhou, et al. Compromising llm driven embodied agents with contextual backdoor attacks. IEEE Transactions on Information Forensics and Security, 2025. [111] Liwen Wang, Wenxuan Wang, Shuai Wang, Zongjie Li, Zhenlan Ji, Zongyi Lyu, Daoyuan Wu, and Shing-Chi Cheung. Ip leakage attacks targeting llm-based multi-agent systems. arXiv preprint arXiv:2505.12442, 2025."
        },
        {
            "title": "A PREPRINT",
            "content": "[112] Junyuan Mao, Fanci Meng, Yifan Duan, Miao Yu, Xiaojun Jia, Junfeng Fang, Yuxuan Liang, Kun Wang, and Qingsong Wen. Agentsafe: Safeguarding large language model-based multi-agent systems via hierarchical data management. arXiv preprint arXiv:2503.04392, 2025. [113] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry OSullivan, and Hoang Nguyen. Multi-agent collaboration mechanisms: survey of llms. arXiv preprint arXiv:2501.06322, 2025. [114] Minrui Xu, Jiani Fan, Xinyu Huang, Conghao Zhou, Jiawen Kang, Dusit Niyato, Shiwen Mao, Zhu Han, Kwok-Yan Lam, et al. Forewarned is forearmed: survey on large language model-based agents in autonomous cyberattacks. arXiv preprint arXiv:2505.12786, 2025. [115] Zhaohan Feng, Ruiqi Xue, Lei Yuan, Yang Yu, Ning Ding, Meiqin Liu, Bingzhao Gao, Jian Sun, and Gang Wang. Multi-agent embodied ai: Advances and future directions. arXiv preprint arXiv:2505.05108, 2025. [116] Jiaxun Cui, Chen Tang, Jarrett Holtz, Janice Nguyen, Alessandro Allievi, Hang Qiu, and Peter Stone. Towards natural language communication for cooperative autonomous driving via self-play. arXiv preprint arXiv:2505.18334, 2025. [117] Chilukuri Riktha Reddy, Madhavi Devi, Kura Pranav Reddy, and Lahari Medarametla. Enhancing mental wellbeing among college students using autogen. In 2025 IEEE International Students Conference on Electrical, Electronics and Computer Science (SCEECS), pages 17. IEEE, 2025. [118] Peter Cihon, Merlin Stein, Gagan Bansal, Sam Manning, and Kevin Xu. Measuring ai agent autonomy: Towards scalable approach with code inspection. arXiv preprint arXiv:2502.15212, 2025. [119] Herbert Dawid, Philipp Harting, Hankui Wang, Zhongli Wang, and Jiachen Yi. Agentic workflows for economic research: Design and implementation. arXiv preprint arXiv:2504.09736, 2025. [120] Emanuele La Malfa, Gabriele La Malfa, Samuele Marro, Jie Zhang, Elizabeth Black, Micheal Luck, Philip Torr, and Michael Wooldridge. Large language models miss the multi-agent mark. arXiv preprint arXiv:2505.21298, 2025. [121] Md Monjurul Karim, Dong Hoang Van, Sangeen Khan, Qiang Qu, and Yaroslav Kholodov. Ai agents meet blockchain: survey on secure and scalable collaboration for multi-agents. Future Internet, 17(2):57, 2025. [122] Kai Li, Can Shen, Yile Liu, Jirui Han, Kelong Zheng, Xuechao Zou, Zhe Wang, Xingjian Du, Shun Zhang, Hanjun Luo, et al. Audiotrust: Benchmarking the multifaceted trustworthiness of audio large language models. arXiv preprint arXiv:2505.16211, 2025. [123] Divyansh Agarwal, Alexander Fabbri, Ben Risher, Philippe Laban, Shafiq Joty, and Chien-Sheng Wu. Prompt leakage effect and mitigation strategies for multi-turn LLM applications. In Franck Dernoncourt, Daniel PreotiucPietro, and Anastasia Shimorina, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 12551275, Miami, Florida, US, November 2024. Association for Computational Linguistics. [124] Volker Strobel, Marco Dorigo, and Mario Fritz. Llm2swarm: Robot swarms that responsively reason, plan, and collaborate through llms. In NeurIPS 2024 Workshop on Open-World Agents (OWA-2024), 2024. [125] Antoine Ligot and Mauro Birattari. On using simulation to predict the performance of robot swarms. Scientific Data, 9(1):788, 2022. [126] Conor Atkins, Benjamin Zi Hao Zhao, Hassan Jameel Asghar, Ian Wood, and Mohamed Ali Kaafar. Those arent your memories, theyre somebody elses: Seeding misinformation in chat bot memories. arXiv preprint arXiv:2304.05371, 2023. [127] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts, 2023. [128] Laurie Hughes, Yogesh Dwivedi, Tegwen Malik, Mazen Shawosh, Mousa Ahmed Albashrawi, Il Jeon, Vincent Dutot, Mandanna Appanderanda, Tom Crick, Rahul De, et al. Ai agents and agentic systems: multi-expert analysis. Journal of Computer Information Systems, pages 129, 2025. [129] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. [130] Alireza Ghafarollahi and Markus Buehler. Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. arXiv preprint arXiv:2409.05556, 2024. [131] Zixuan Ke, Austin Xu, Yifei Ming, Xuan-Phi Nguyen, Caiming Xiong, and Shafiq Joty. Meta-design matters: self-design multi-agent system. arXiv preprint arXiv:2505.14996, 2025."
        },
        {
            "title": "A PREPRINT",
            "content": "[132] Andres Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew White, and Philippe Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023. [133] Diana-Margarita Córdova-Esparza. Ai-powered educational agents: Opportunities, innovations, and ethical challenges. Information, 16(6):469, 2025. [134] Wadim Strielkowski, Veronika Grebennikova, Alexander Lisovskiy, Guzalbegim Rakhimova, and Tatiana Vasileva. Ai-driven adaptive learning for sustainable educational transformation. Sustainable Development, 33(2):19211947, 2025. [135] Michael Kirchhof, Gjergji Kasneci, and Enkelejda Kasneci. Position: Uncertainty quantification needs reassessment for large-language model agents. arXiv preprint arXiv:2505.22655, 2025. [136] Han Wang, An Zhang, Nguyen Duy Tai, Jun Sun, Tat-Seng Chua, et al. Ali-agent: Assessing llms alignment with human values via agent-based evaluation. Advances in Neural Information Processing Systems, 37:9904099088, 2024. [137] Jam Kraprayoon, Zoe Williams, and Rida Fayyaz. Ai agent governance: field guide. arXiv preprint arXiv:2505.21808, 2025. [138] Joseph Sifakis, Dongming Li, Hairong Huang, Yong Zhang, Wenshuan Dang, River Huang, and Yijun Yu. reference architecture for autonomous networks: An agent-based approach. arXiv preprint arXiv:2503.12871, 2025. [139] Hongpeng Chen, Shufei Li, Junming Fan, Anqing Duan, Chenguang Yang, David Navarro-Alarcon, and Pai IEEE Zheng. Human-in-the-loop robot learning for smart manufacturing: human-centric perspective. Transactions on Automation Science and Engineering, 2025. [140] Isabella Seeber, Lena Waizenegger, Stefan Seidel, Stefan Morana, Izak Benbasat, and Paul Benjamin Lowry. Collaborating with technology-based autonomous agents: Issues and research opportunities. Internet Research, 30(1):118, 2020. [141] Amit Shukla, Vagan Terziyan, and Timo Tiihonen. Ai as user of ai: Towards responsible autonomy. Heliyon, 10(11), 2024. [142] Giulio Sandini, Alessandra Sciutti, and Pietro Morasso. Artificial cognition vs. artificial intelligence for nextgeneration autonomous robotic agents. Frontiers in Computational Neuroscience, 18:1349408, 2024. [143] Jose Tupayachi, Haowen Xu, Olufemi Omitaomu, Mustafa Can Camur, Aliza Sharmin, and Xueping Li. Towards next-generation urban decision support systems through ai-powered construction of scientific ontology using large language modelsa case in optimizing intermodal freight transportation. Smart Cities, 7(5):2392 2421, 2024. [144] Jiaru Bai, Kok Foong Lee, Markus Hofmeister, Sebastian Mosbach, Jethro Akroyd, and Markus Kraft. derived information framework for dynamic knowledge graph and its application to smart cities. Future Generation Computer Systems, 152:112126, 2024. [145] Fangqiao Tian, An Luo, Jin Du, Xun Xian, Robert Specht, Ganghua Wang, Xuan Bi, Jiawei Zhou, Jayanth Srinivasa, Ashish Kundu, et al. An outlook on the opportunities and challenges of multi-agent ai systems. arXiv preprint arXiv:2505.18397, 2025. [146] Shusen Liu, Haichao Miao, Zhimin Li, Matthew Olson, Valerio Pascucci, and P-T Bremer. Ava: Towards autonomous visualization agents through visual perception-driven decision-making. In Computer Graphics Forum, volume 43, page e15093. Wiley Online Library, 2024. [147] Kai Hu, Keer Xu, Qingfeng Xia, Mingyang Li, Zhiqiang Song, Lipeng Song, and Ning Sun. An overview: Attention mechanisms in multi-agent reinforcement learning. Neurocomputing, page 128015, 2024. [148] Hanchi Gu, Marco Schreyer, Kevin Moffitt, and Miklos Vasarhelyi. Artificial intelligence co-piloted auditing. International Journal of Accounting Information Systems, 54:100698, 2024. [149] Jia Hui Chin, Pu Zhang, Yu Xin Cheong, and Jonathan Pan. Automating security audit using large language model based agent: An exploration experiment. arXiv preprint arXiv:2505.10732, 2025. [150] Claire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao, and Wulong Liu. survey on interpretable reinforcement learning. Machine Learning, 113(8):58475890, 2024. [151] Mostafa Yossef, Mohamed Noureldin, and Aghyad Alqabbany. Explainable artificial intelligence framework for frp composites design. Composite Structures, 341:118190, 2024. [152] Bhanu Chander, Chinju John, Lekha Warrier, and Kumaravelan Gopalakrishnan. Toward trustworthy artificial intelligence (tai) in the context of explainability and robustness. ACM Computing Surveys, 57(6):149, 2025."
        },
        {
            "title": "A PREPRINT",
            "content": "[153] Bingcheng Wang, Tianyi Yuan, and Pei-Luen Patrick Rau. Effects of explanation strategy and autonomy of explainable ai on humanai collaborative decision-making. International Journal of Social Robotics, 16(4):791 810, 2024. [154] Atul Rawal, Adrienne Raglin, Danda Rawat, Brian Sadler, and James McCoy. Causality for trustworthy artificial intelligence: status, challenges and perspectives. ACM Computing Surveys, 57(6):130, 2025. [155] Yunxiang Qiu, Yuting Tang, Liangguo Chen, Shuyu Jiang, Ying Huang, and Xingshu Chen. Reinforcement learning-driven temporal knowledge graph reasoning for secure data provenance in distributed networks. Peer-toPeer Networking and Applications, 18(4):116, 2025. [156] Yilin Ye, Jianing Hao, Yihan Hou, Zhan Wang, Shishi Xiao, Yuyu Luo, and Wei Zeng. Generative ai for visualization: State of the art and future directions. Visual Informatics, 2024. [157] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment, 2023. [158] Cristian Munoz, Kleyton da Costa, and Franklin Cardenoso Fernandez. Enhancing transparency in ai: Explainability metrics for machine learning predictions. Holistic AI Blog, 2024. [159] Chirag Agarwal, Dan Ley, Satyapriya Krishna, Eshika Saxena, Martin Pawelczyk, Nari Johnson, Isha Puri, Marinka Zitnik, and Himabindu Lakkaraju. Openxai: Towards transparent evaluation of model explanations, 2024. [160] Kerry Rodden, Hilary Hutchinson, and Xin Fu. Measuring the user experience on large scale: user-centered metrics for web applications. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 23952398, 2010. [161] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688, 2023. [162] Raihan Khan, Sayak Sarkar, Sainik Kumar Mahata, and Edwin Jose. Security threats in agentic ai system. arXiv preprint arXiv:2410.14728, 2024. [163] Yuntao Wang, Yanghe Pan, Quan Zhao, Yi Deng, Zhou Su, Linkang Du, and Tom Luan. Large model agents: State-of-the-art, cooperation paradigms, security and privacy, and future trends. arXiv preprint arXiv:2409.14457, 2024. [164] Yifeng He, Ethan Wang, Yuyang Rong, Zifei Cheng, and Hao Chen. Security of ai agents. arXiv preprint arXiv:2406.08689, 2024. [165] Sumeet Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip Torr, Lewis Hammond, and Christian Schroeder de Witt. Secret collusion among ai agents: Multi-agent deception via steganography. Advances in Neural Information Processing Systems, 37:7343973486, 2024. [166] Ken Huang, Vineeth Sai Narajala, Idan Habler, and Akram Sheriff. Agent name service (ans): universal directory for secure ai agent discovery and interoperability. arXiv preprint arXiv:2505.10609, 2025. [167] Tobin South, Samuele Marro, Thomas Hardjono, Robert Mahari, Cedric Deslandes Whitney, Dazza Greenwood, Alan Chan, and Alex Pentland. Authenticated delegation and authorized ai agents. arXiv preprint arXiv:2501.09674, 2025. [168] Ghadeer Ghazi Shayea, Mohd Hazli Mohammed Zabil, Mustafa Abdulfattah Habeeb, Yahya Layth Khaleel, and AS Albahri. Strategies for protection against adversarial attacks in ai models: An in-depth review. Journal of Intelligent Systems, 34(1):20240277, 2025. [169] Jasmita Malik, Raja Muthalagu, and Pranav Pawar. systematic review of adversarial machine learning attacks, defensive controls and technologies. IEEE Access, 2024. [170] Vineeth Sai Narajala and Om Narayan. Securing agentic ai: comprehensive threat model and mitigation framework for generative ai agents. arXiv preprint arXiv:2504.19956, 2025. [171] Chi-Min Chan, Jianxuan Yu, Weize Chen, Chunyang Jiang, Xinyu Liu, Weijie Shi, Zhiyuan Liu, Wei Xue, and Yike Guo. Agentmonitor: plug-and-play framework for predictive and secure multi-agent systems. arXiv preprint arXiv:2408.14972, 2024. [172] RM Bommi, TJ Nandhini, et al. Enhancing healthcare data security and privacy through ai-driven encryption and privacy-preserving techniques. In 2025 International Conference on Data Science, Agents & Artificial Intelligence (ICDSAAI), pages 16. IEEE, 2025. [173] Jency and RP Anto Kumar. Finquaxbot: enhancing trust and security in personalized investment and tax forecasting using homomorphic encryption and meta-reinforcement learning with explainability. Expert Systems with Applications, page 128136, 2025."
        },
        {
            "title": "A PREPRINT",
            "content": "[174] Will Epperson, Gagan Bansal, Victor Dibia, Adam Fourney, Jack Gerrits, Erkang Zhu, and Saleema Amershi. Interactive debugging and steering of multi-agent ai systems. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 115, 2025. [175] Ismail, Rahmat Kurnia, Zilmas Arjuna Brata, Ghitha Afina Nelistiani, Shinwook Heo, Hyeongon Kim, and Howon Kim. Toward robust security orchestration and automated response in security operations centers with hyper-automation approach using agentic artificial intelligence. Information, 16(5):365, 2025. [176] Javed Akhtar Khan. Role-based access control (rbac) and attribute-based access control (abac). In Improving security, privacy, and trust in cloud computing, pages 113126. IGI Global Scientific Publishing, 2024. [177] Ken Huang, Vineeth Sai Narajala, John Yeoh, Ramesh Raskar, Youssef Harkati, Jerry Huang, Idan Habler, and Chris Hughes. novel zero-trust identity framework for agentic ai: Decentralized authentication and fine-grained access control. arXiv preprint arXiv:2505.19301, 2025. [178] Subash Neupane, Shaswata Mitra, Sudip Mittal, and Shahram Rahimi. Towards hipaa compliant agentic ai system in healthcare. arXiv preprint arXiv:2504.17669, 2025. [179] Shuyuan Liu, Jiawei Chen, Shouwei Ruan, Hang Su, and Zhaoxia Yin. Exploring the robustness of decision-level through adversarial attacks on llm-based embodied models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 81208128, 2024. [180] Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes, Savvas Zannettou, and Yang Zhang. Breaking agents: Compromising autonomous llm agents through malfunction amplification. arXiv preprint arXiv:2407.20859, 2024. [181] Atharv Singh Patlan, Peiyao Sheng, Ashwin Hebbar, Prateek Mittal, and Pramod Viswanath. Ai agents in cryptoland: Practical attacks and no silver bullet. Cryptology ePrint Archive, 2025. [182] Maxwell Standen, Junae Kim, and Claudia Szabo. Adversarial machine learning attacks and defences in multi-agent reinforcement learning. ACM Computing Surveys, 57(5):135, 2025. [183] Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond, Herbie Bradley, Emma Bluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt, et al. Visibility into ai agents. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, pages 958973, 2024. [184] Yewei Zhen, Donghai Tian, Xiaohu Fu, and Changzhen Hu. novel malware detection method based on audit logs and graph neural network. Engineering Applications of Artificial Intelligence, 152:110524, 2025. [185] Ranjan Sapkota, Konstantinos Roumeliotis, and Manoj Karkee. Vibe coding vs. agentic coding: Fundamentals and practical implications of agentic ai. arXiv preprint arXiv:2505.19443, 2025. [186] Konstantinos Lazaros, Dimitrios Koumadorakis, Aristidis Vrahatis, and Sotiris Kotsiantis. Federated learning: Navigating the landscape of collaborative intelligence. Electronics, 13(23):4744, 2024. [187] Victoria Luzón, Nuria Rodríguez-Barroso, Alberto Argente-Garrido, Daniel Jiménez-López, Jose Moyano, Javier Del Ser, Weiping Ding, and Francisco Herrera. tutorial on federated learning from theory to practice: Foundations, software frameworks, exemplary use cases, and selected trends. IEEE/CAA Journal of Automatica Sinica, 11(4):824850, 2024. [188] Shaina Raza, Rizwan Qureshi, Anam Zahid, Joseph Fioresi, Ferhat Sadak, Muhammad Saeed, Ranjan Sapkota, Aditya Jain, Anas Zafar, Muneeb Ul Hassan, et al. Who is responsible? the data, models, users or regulations? comprehensive survey on responsible generative ai for sustainable future. arXiv preprint arXiv:2502.08650, 2025. [189] European Parliament and Council. Regulation (EU) 2024/1244 of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (AI Act). https://eur-lex.europa.eu/ legal-content/EN/TXT/?uri=CELEX%3A32024R1244, 2024. Official EU legislation adopted in 2024 establishing the legal framework for AI systems. [190] Chen Chen, Xueluan Gong, Ziyao Liu, Weifeng Jiang, Si Qi Goh, and Kwok-Yan Lam. Trustworthy, responsible, and safe ai: comprehensive architectural framework for ai safety with challenges and mitigations. arXiv preprint arXiv:2408.12935, 2024. [191] American Bar Association. Aba house adopts 3 guidelines to improve use of artificial intelligence. https://www.americanbar.org/advocacy/governmental_legislative_work/publications/ washingtonletter/may-23-wl/ai-0523wl/, May 2023. accountability, transparency, and human oversight in AI systems. Resolution 604 outlines guidelines for [192] Muhammad Ali, Stavros Shiaeles, Maria Papadaki, and Bogdan Ghita. Agent-based vs agent-less sandbox for dynamic behavioral analysis. In 2018 Global Information Infrastructure and Networking Symposium (GIIS), pages 15. IEEE, 2018."
        },
        {
            "title": "A PREPRINT",
            "content": "[193] Satvik Golechha and Adrià Garriga-Alonso. Among us: sandbox for measuring and detecting agentic deception. arXiv preprint arXiv:2504.04072, 2025. [194] Peter Slattery, Alexander K. Saeri, Emily A. C. Grundy, Jess Graham, Michael Noetel, Risto Uuk, James Dao, Soroush Pour, Stephen Casper, and Neil Thompson. The ai risk repository: comprehensive meta-review, database, and taxonomy of risks from artificial intelligence, 2025. [195] Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, et al. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Information fusion, 58:82115, 2020. [196] Ahmed Salih, Zahra Raisi-Estabragh, Ilaria Boscolo Galazzo, Petia Radeva, Steffen Petersen, Karim Lekadir, and Gloria Menegaz. perspective on explainable artificial intelligence methods: Shap and lime. Advanced Intelligent Systems, 7(1):2400304, 2025. [197] Turker Berk Donmez, Mustafa Kutlu, Mohammed Mansour, and Mustafa Zahid Yildiz. Explainable ai in action: comparative analysis of hypertension risk factors using shap and lime. Neural Computing and Applications, 37(5):40534074, 2025. [198] Patrick Knab, Sascha Marton, Udo Schlegel, and Christian Bartelt. Which lime should trust? concepts, challenges, and solutions. arXiv preprint arXiv:2503.24365, 2025. [199] Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. arXiv preprint arXiv:2504.08066, 2025. [200] Guneet Kaur Walia and Mohit Kumar. Computational offloading and resource allocation for iot applications using decision tree based reinforcement learning. Ad Hoc Networks, page 103751, 2025. [201] Luis Fonseca, Lucas Böttcher, Borna Mehrad, and Reinhard Laubenbacher. Optimal control of agent-based models via surrogate modeling. PLOS Computational Biology, 21(1):e1012138, 2025. [202] Shekhar Mahmud, Mustafa Kutlu, and Alper Turan Alan. Optimizing multi-agent system swarm performance through explainable ai. In 2025 11th International Conference on Mechatronics and Robotics Engineering (ICMRE), pages 175180. IEEE, 2025. [203] Zhang Xi-Jia, Yue Guo, Shufei Chen, Simon Stepputtis, Matthew Gombolay, Katia Sycara, and Joseph Campbell. Model-agnostic policy explanations with large language models. arXiv preprint arXiv:2504.05625, 2025. [204] Busra Ozdenizci Kose. Modern software challenges: Innovations in data governance and devsecops. In Data Governance, DevSecOps, and Advancements in Modern Software, pages 113130. IGI Global Scientific Publishing, 2025. [205] Tuan-Jun Goh, Lee-Ying Chong, Siew-Chin Chong, and Pey-Yun Goh. campus-based chatbot system using natural language processing and neural network. Journal of Informatics and Web Engineering, 3(1):96116, 2024. [206] Jingru Yu, Yi Yu, Xuhong Wang, Yilun Lin, Manzhi Yang, Yu Qiao, and Fei-Yue Wang. The shadow of fraud: The emerging danger of ai-powered social engineering and its possible cure. arXiv preprint arXiv:2407.15912, 2024. [207] Bhada Yun, Dana Feng, Ace Chen, Afshin Nikzad, and Niloufar Salehi. Generative ai in knowledge work: Design implications for data navigation and decision-making. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 119, 2025. [208] Tongyang Li, Jiageng Ruan, and Kaixuan Zhang. The investigation of reinforcement learning-based end-to-end decision-making algorithms for autonomous driving on the road with consecutive sharp turns. Green Energy and Intelligent Transportation, page 100288, 2025. [209] Guardrails AI. Guardrails AI Your Enterprise AI needs Guardrails guardrailsai.com, February 2024. [210] Securiti. What is AI TRiSM and why its essential in the era of genai, May 2025. Accessed: 2025-06-03. [211] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions, November 2023. arXiv:2311.05232 [cs]. [212] Shaina Raza, Syed Raza Bashir, and Usman Naseem. Accuracy meets Diversity in News Recommender System. In Proceedings of the 29th International Conference on Computational Linguistics, pages 37783787, Gyeongju, Republic of Korea, 10 2022. International Committee on Computational Linguistics."
        },
        {
            "title": "A PREPRINT",
            "content": "[213] Raja Parasuraman and Dietrich H. Manzey. Complacency and bias in human use of automation: An attentional integration. Human Factors, 52(3):381410, 2010. [214] Shaina Raza, Shardul Ghuge, Chen Ding, and Deval Pandya. FAIR Enough: How Can We Develop and Assess FAIR-Compliant Dataset for Large Language Models Training? arXiv preprint arXiv:2401.11033, 2024. [215] European Commission. AI Act Shaping Europes digital future, 2024. Accessed: 2025-06-03. [216] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned, November 2022. arXiv:2209.07858 [cs]. [217] Aileen G. Bacudio, Xiaohong Yuan, Bei-Tseng Bill Chu, and Monique Jones. An overview of penetration testing. International Journal of Network Security & Its Applications (IJNSA), 3(6):1938, November 2011. [218] Jacquelyn Schneider. Wargaming cyber security. War on the Rocks, September 2020. [219] Saleema Amershi, Daniel S. Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi T. Iqbal, Paul N. Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, and Eric Horvitz. Guidelines for human-ai interaction. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, 2019. [220] Henry Hexmoor, Johan Lammens, Guido Caicedo, and Stuart Shapiro. Behaviour based AI, cognitive processes, and emergent behaviors in autonomous agents, volume 1. WIT Press, 2025."
        },
        {
            "title": "A PREPRINT",
            "content": "Figure 4: Traditional AI Agent vs. Agentic AI. Term Table 9: Key terminologies for LLM-based agentic AI systems. Definition Explainability Autonomy Model Agentic AI System Application Security Counterfactual Analysis Chain of Thought (CoT) Shared Memory (Persistent Memory) ModelOps multi-agent architecture powered by large language models (LLMs), where autonomous agents collaborate, plan, and execute tasks over extended horizons with persistent memory and dynamic role assignment. The mechanism by which an agent decides and acts without direct human intervention, often using goal-driven planning and chain-of-thought reasoning. prompting strategy in which an LLM generates intermediate reasoning steps before producing final answer or action, enhancing interpretability and multi-step planning. An interpretability technique that examines how altering certain inputs or agent contributions would change the overall system outcome, revealing causal dependencies among agents. The capacity of an AI system (or individual agent) to produce human-understandable justifications or rationales for its decisions and actions, often via LIME, SHAP, or decision provenance graphs. Foundation Model (LLM) pretrained large language model (e.g., GPT-4, LLaMA) that serves as the brain of each agent, providing generative capabilities, reasoning, and tool-calling support. centralized or distributed store (often vector database) where agents write and retrieve contextual information, enabling long-term planning and consistency across iterations. The practice of managing AI models (and agent prompts) throughout their lifecycledevelopment, deployment, monitoring, and retirementwith version control, CI/CD testing, and drift detection. Safeguards and best practices (e.g., prompt sanitation, authentication, sandboxing) designed to protect agentic systems from prompt injection, identity spoofing, and lateral exploits. Techniques (e.g., differential privacy, homomorphic encryption, secure enclaves) that ensure sensitive dataeither during training or inter-agent communicationremains protected in multi-agent workflows. security exploit in which an attacker crafts input containing hidden instructions that corrupt an agents reasoning or propagate malicious commands through agent interactions (prompt infection). framework where agents query an external knowledge store (e.g., vector database) to fetch relevant documents or facts, then condition their LLM responses on that retrieved context. An architectural pattern in which each agent is assigned specific function (e.g., planner, verifier, coder) and collaborates via structured communication protocols to achieve complex tasks. graph-based representation that traces data flows and decision steps across multiple agents, enabling post-hoc auditing and system-level interpretability. The mechanism by which an agent issues structured commands (e.g., API calls, code execution) to external services or environments and incorporates the results back into its reasoning. composite metric that quantifies an agents reliability, alignment with user goals, and consistency over time, often combining accuracy, safety-violation rates, and calibration of confidence. An aggregate evaluation score (e.g., weighted sum of trustworthiness, explainability, user-centered performance, and coordination metrics) used to benchmark different agentic systems. Retrieval-Augmented Generation (RAG) Role-Specialized Coordination Decision Graph Tool-Use Interface Composite Metric Prompt Injection Model Privacy Provenance Trust Score"
        }
    ],
    "affiliations": [
        "Cornell University, USA",
        "University of Groningen, Netherlands",
        "Vector Institute, Toronto, Canada"
    ]
}