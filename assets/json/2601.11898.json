{
    "paper_title": "RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection",
    "authors": [
        "Yilmaz Korkmaz",
        "Vishal M. Patel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Remote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, a new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing a competitive autoregressive alternative for remote sensing change detection. Code will be available \\href{https://github.com/yilmazkorkmaz1/RemoteVAR}{\\underline{here}}."
        },
        {
            "title": "Start",
            "content": "RemoteVAR: Autoregressive Visual Modeling for Remote Sensing Change Detection Yilmaz Korkmaz Johns Hopkins University Baltimore, Maryland, USA ykorkma1@jhu.edu Vishal M. Patel Johns Hopkins University Baltimore, Maryland, USA vpatel36@jhu.edu 6 2 0 2 7 ] . [ 1 8 9 8 1 1 . 1 0 6 2 : r AbstractRemote sensing change detection aims to localize and characterize scene changes between two time points and is central to applications such as environmental monitoring and disaster assessment. Meanwhile, visual autoregressive models (VARs) have recently shown impressive image generation capability, but their adoption for pixel-level discriminative tasks remains limited due to weak controllability, suboptimal dense prediction performance and exposure bias. We introduce RemoteVAR, new VAR-based change detection framework that addresses these limitations by conditioning autoregressive prediction on multi-resolution fused bi-temporal features via cross-attention, and by employing an autoregressive training strategy designed specifically for change map prediction. Extensive experiments on standard change detection benchmarks show that RemoteVAR delivers consistent and significant improvements over strong diffusion-based and transformer-based baselines, establishing competitive autoregressive alternative for remote sensing change detection. Code will be available here. Index TermsRemote Sensing, Change Detection, Autoregressive Models, Genarative Models. I. INTRODUCTION In remote sensing, Change Detection (CD) aims to identify alterations on the Earths surface over time by comparing satellite observations of the same area acquired at different time points [1]. CD serves as core component in broad set of real-world applications, including monitoring and assessing natural disasters and climate variability [24], supporting policy and urban planning decisions [5], mapping land use and agricultural cover [6, 7], and military-oriented analysis [8]. In practice, the main difficulty in CD is not the definition of the task but the nuisance differences that arise between acquisitions. Multi-temporal satellite images frequently vary due to changing illumination conditions [9, 10], misalignment from imperfect registration [11, 12], sensor-specific spatial resolution [13]. and measurement noise [14, 15]. These effects can mimic real changes and lead to false detections, making robustness to cross-time inconsistencies central challenge for reliable CD. Motivated by the success of deep learning, many modern CD pipelines rely on deep neural networks and have reported substantial improvements across multiple benchmarks [16 21]. Representative directions include convolutional architectures [16, 22, 17], transformer-based models [19, 18], and more recently diffusion-based approaches [20, 23]. Most supervised change detection (CD) methods are discriminative: given bi-temporal pair (pre-change and postchange images), they directly regress change segmentation mask in single shot. However, CD inherently couples global reasoning with precise localization: the model must first reconcile the overall scene context across time (e.g., viewpoint, illumination, and background content) and then resolve finegrained change boundaries. This structure makes CD naturally compatible with coarse-to-fine prediction process that resembles how humans inspect changes, forming an initial global hypothesis and progressively refining local details. Accordingly, we formulate CD as conditional autoregressive prediction in discrete token space, where the change map is generated stage by stage from low to high resolution. This view is timely given the rapid progress of visual autoregressive models (VARs) [24]. Compared with diffusionbased generative approaches, VAR-style generation provides practical efficiency advantage at inference: diffusion models typically require many iterative denoising steps, whereas VAR produces outputs in small number of coarse-to-fine autoregressive stages. VARs have demonstrated strong generative capabilities, yet their adaptability to discriminative, pixel-level tasks has been rarely explored and has never been studied for remote sensing change detection. To bridge this gap, we propose RemoteVAR, VAR-based CD framework that redesigns conditioning and training for controllable, highaccuracy change map generation. RemoteVAR produces the change map directly via coarse-to-fine autoregressive decoding conditioned on multi-resolution bi-temporal cues. II. BACKGROUND Visual Autoregressive Models (VAR) [24] predict discrete visual tokens using scale-wise autoregression, generating tokens from coarse to fine resolutions to form global structure before refining details. These tokens are obtained with residual multi-scale VQ-VAE, which constructs tokenization scheme with varying token resolutions to encode an image into pyramid of multi-scale tokens from coarse to fine, and quantizes latent grids into codebook indices where finer scales capture remaining details. ControlVAR [25] extends VAR to conditional prediction by guiding token generation with additional context tokens (e.g., segmentation masks), improving controllability of the output. Fig. 1. Overview of the RemoteVAR architecture and training pipeline. For clarity, we visualize only the first three token scales with grid sizes 11, 22, and 33. Pre-image, post-image, and fused feature streams are color-coded in red, blue, and green, respectively. Trainable modules are marked with fire icon, while frozen components are marked with an ice icon. III. METHODOLOGY Given bi-temporal pair consisting of pre-change and post-change image, we perform change detection by generating discrete mask tokens rather than directly regressing pixels. We adopt the original VAR residual VQ-VAE as frozen, scale-wise residual tokenizer with 10 token resolutions (from 1 1 to 16 16 across 10 scales: {1, 2, 3, 4, 5, 6, 8, 10, 13, 16} with token grids s) to map each pre/post image (and the ground-truth mask during training) into token IDs from fixed vocabulary of size =4096. For each scale s, we obtain token grids zpre ; token IDs are converted to embeddings via the shared codebook and projected to the VAR embedding dimension. To mitigate severe foregroundbackground imbalance, we convert binary mask tokens into location aware RGB-coded representations following [25] to increase token diversity, while keeping an efficient inverse mapping back to binary mask. , and zm , zpost s , zpost We then construct scale-wise sequence by interleaving tokens per scale as [zpre , zm ], and enrich it with absolute 2D positional embeddings as well as scale embeddings so the model can distinguish pre/post/mask streams while still sharing parameters across scales. During training, we use teacher forcing and compute the loss only on mask tokens, treating preand post-image tokens as context for selfattention. In contrast to ControlVAR [25], which relies only on selfattention among discrete tokens for conditioning, we introduce an explicit cross-attention mechanism that injects fused continuous features into the causal transformer to improve spatial grounding for mask prediction. Specifically, beyond discrete token conditioning, we derive continuous, pixel-level feature maps directly from the VQ-VAE encoder that is already used for tokenization, removing the need for an additional feature-extraction backbone. The VQ-VAE encoder is reused as shared-weight pre/post encoder, analogous to the Siamese encoders that are widely adopted in change detection for modeling bi-temporal inputs. To explicitly combine information across time, we apply lightweight fusion modules adapted from CMX [26] to fuse the continuous features from the preand post-image streams into unified conditioning representation, which is then used as the cross-attention memory, i.e., its projected embeddings serve as the keys and values, while the causal transformer token states act as queries. Unlike discrete tokens, these continuous features do not suffer from tokenization/discretization artifacts, providing crucial finegrained information for accurate boundary localization and small-object changes. Moreover, to reduce exposure bias from teacher forcing, we randomly replace subset of early-scale mask tokens with random codebook tokens during training, so the model learns to recover the ground-truth tokens under imperfect coarse predictions that better match autoregressive inference. The overall training procedure and architecture is illustrated in Fig. 1. At inference time, we provide zpre via teacher forcing and autoregressively predict zm in coarse-to-fine, scale-wise manner. To transition between successive scales, and zpost Fig. 2. Scale-wise autoregressive mask generation shown across progressively finer token resolutions from 1 1 to 16 16. detection datasets WHU-CD [27], LEVIR-CD [28], LEVIRCD+ [28], and S2Looking[33]. All experiments are run on 8 NVIDIA A5000 GPUs with 24 GB memory each. B. Experimental setup. We evaluate RemoteVAR on two widely used building change detection benchmarks, LEVIR-CD [28] and WHUCD [27]. LEVIR-CD contains VHR Google Earth image pairs with building changes, while WHU-CD provides large-scale aerial imagery collected at different years for building change detection. Following standard practice, we report performance on the official splits using F1 score, IoU, and overall pixel accuracy (OA) as the primary metrics, where F1/IoU emphasize the changed class under heavy class imbalance. C. Baselines We compare against representative diffusion-, Mamba/SSM- , transformer-, CNN-, and self-supervised approaches. DDPMCD [20] is diffusion-feature-based change detector, RSMamba [32] uses Siamese Mamba/SSM backbone, and ChangeFormer [19] and BiT [18] are transformer-based bi-temporal change detection models. For CNN baselines, SNUNet [16] and STANet [29] follow Siamese encoder decoder designs with enhanced fusion/attention. We also include self-supervised pretraining baselines, SeCo [30] and SaDL-CD [31], which learn representations from unlabeled temporal imagery and are then fine-tuned for change detection. D. Results Quantitative results are reported in Table I. On WHUCD, RemoteVAR achieves the best performance with F1 = 0.930 and IoU = 0.870, slightly improving over strong baselines such as DDPM-CD [20] (0.927/0.863) and RSMamba [32] (0.927/0.865). On LEVIR-CD, RemoteVAR remains competitive and reaches the top performance with F1 = 0.910 and IoU = 0.834, marginally exceeding DDPM-CD (0.909/0.833) and outperforming the remaining transformer/CNN/self-supervised baselines by varying margins. Qualitative comparisons in Fig. 4 further show that RemoteVAR produces cleaner masks and detects subtle structural changes with more accurate localization, which we attribute Fig. 3. Decoder refinement procedure is illustrated. we upsample the predicted mask tokens from the previous resolution to the next target grid before generation (e.g., the 1010 prediction is upsampled to 1313 and used as the starting context for the 9th scale), enabling progressive refinement of the change map as spatial resolution increases (see Fig. 2). After autoregressive inference, the predicted mask tokens are mapped back through the codebook and decoded to pixel space by the VQ-VAE decoder. We then perform decoder refining to further sharpen boundaries and recover fine details: we augment the decoder with UNet-style skip connections, where the skip features come from our multi-scale fusion modules (rather than directly from the encoder), and finetune the decoder with binary cross-entropy objective on the change map. This refinement improves boundary localization and small-object changes while keeping the autoregressive generator fixed. This process is illustrated in Fig. 3. IV. EXPERIMENTS AND RESULTS A. Implementation Details All images are resized to 256 256, and models are trained for 100 epochs with batch size of 48 using AdamW (learning rate 1 104). Weight decay is set to 1 104 and annealed to 0 with cosine schedule. The learning rate follows cosine schedule with linear warmup, and gradients are clipped to norm of 2.0. Mixed-precision (FP16) training is applied. To increase the number of training samples and promote token diversity, training is performed on the union of binary change TABLE QUANTITATIVE COMPARISON ON WHU-CD [27] AND LEVIR-CD [28] USING F1, IOU, AND OVERALL ACCURACY (OA). WHU-CD [27] LEVIR-CD [28] Method F1 () IoU () OA () F1 () IoU () OA () SNUNet [16] DT-SCN [17] STANet [29] SeCo [30] SaDL-CD [31] BIT [18] ChangeFormer [19] RSMamba [32] DDPM-CD [20] RemoteVAR (Ours) 0.835 0.914 0.823 0.883 0.909 0.905 0.886 0.927 0.927 0.930 0.717 0.842 0.700 0.790 0.833 0.834 0.795 0.865 0.863 0.870 98.7 99.3 98.5 - - 99.3 99.1 99.4 99.4 99. 0.882 0.877 0.873 0.881 0.899 0.893 0.904 0.897 0.909 0.910 0.788 0.781 0.774 0.787 0.818 0.807 0.825 0.814 0.833 0.834 98.8 98.8 98.7 - - 98.9 99.0 98.9 99.1 99. Fig. 4. Qualitative prediction comparisons are shown for the WHU-CD [27] (top row) and LEVIR-CD [28] (bottom row) datasets. True positives are colored white, false positives green, and false negatives red. to its coarse-to-fine autoregressive prediction and progressive refinement across scales. TABLE II ABLATION RESULTS ARE PRESENTED IN WHU-CD [27]. E. Ablation Studies We analyze the contribution of each design choice on WHU-CD [27], with results summarized in Table II. No Cross-Att removes our cross-attention conditioning and yields that conditions only through ControlVAR-style variant self-attention over discrete tokens; unlike the original ControlVAR [25] (single-condition), our setting is bi-conditioned on both preand post-image tokens. No Location Aware RGB Masks disables our RGB-based mask token conversion and uses binary mask tokens directly, which increases token imbalance and reduces mask token diversity. No TokRand removes the early-scale token randomization used during teacher forcing; this variant is directly related to exposure bias because the model is trained only on ground-truth coarse tokens and is not exposed to imperfect coarse predictions that arise at autoregressive inference. No DecRef disables the decoder refining stage and outputs the change map using the fixed VQ-VAE decoder (pure autoregressive prediction without refinement). Overall, each component provides complementary gains, and the full RemoteVAR achieves the best performance. Variant F1 IoU OA (%) No Cross-Att (ControlVAR[25]) No Location Aware RGB Masks No TokRand (exposure bias) No DecRef (pure autoregressive) 0.145 0.643 0.877 0.894 0.078 0.474 0.781 0. RemoteVAR 0.930 0.870 96.0 96.8 99.0 99.1 99.4 V. CONCLUSION We introduce RemoteVAR, bi-temporal autoregressive change detection framework that generates multi-scale mask tokens in coarse-to-fine manner. By combining discrete token generation with fused continuous pre/post features injected via cross-attention, RemoteVAR achieves accurate localization and strong performance on public benchmarks, matching or surpassing competitive Diffusion-, Transformer-, Mamba-, and CNN-based baselines. The results suggest that autoregressive modeling is practical and effective alternative for dense remote sensing change detection."
        },
        {
            "title": "REFERENCES",
            "content": "[1] L. Khelifi and M. Mignotte, Deep learning for change detection in remote sensing images: Comprehensive review and metaanalysis, IEEE Access, vol. 8, pp. 126 385126 400, 2020. [2] Q. Liu, S. Wan, and B. Gu, review of the detection methods for climate regime shifts, Discrete Dynamics in Nature and Society, vol. 2016, pp. 110, 01 2016. [3] E. Hamidi, B. G. Peter, D. F. Munoz, H. Moftakhari, and H. Moradkhani, Fast flood extent monitoring with sar change detection using google earth engine, IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp. 119, 2023. [4] X. He, S. Zhang, B. Xue, T. Zhao, and T. Wu, Cross-modal change detection flood extraction based on Journal of convolutional neural network, Applied Earth Observation and Geoinformation, vol. 117, p. 103197, 2023. [Online]. Available: https://www.sciencedirect. com/science/article/pii/S"
        },
        {
            "title": "International",
            "content": "[5] M. K. Ridd and J. Liu, comparison of four algorithms for change detection in an urban environment, Remote Sensing of Environment, vol. 63, no. 2, pp. 95100, 1998. https://www.sciencedirect.com/ [Online]. Available: science/article/pii/S0034425797001120 [6] R. Kaur, R. Tiwari, R. Maini, and S. Singh, framework for crop yield estimation and change detection using image fusion of microwave and optical satellite dataset, Quaternary, vol. 6, p. 28, 04 2023. [7] N. Coops, P. Tompalski, T. Goodbody, A. Achim, and C. Mulverhill, Framework for near real-time forest inventory using multi source remote sensing data, Forestry, pp. 119, 05 2022. [8] P. Tueller, R. Ramsey, T. Frank, R. Washington-Allen, and S. Tweddale, Emerging and contemporary technologies in remote sensing for ecosystem assessment and change detection on military reservations, p. 74, 12 1998. [9] X. Wan, J. Liu, S. Li, J. Dawson, and H. Yan, An illuminationinvariant change detection method based on disparity saliency map for multitemporal optical remotely sensed images, IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 3, pp. 13111324, 2019. [10] J. Liu, X. Wang, M. Chen, S. Liu, Z. Shao, X. Zhou, and P. Liu, Illumination and contrast balancing for remote sensing images, Remote. Sens., vol. 6, pp. 11021123, 2014. [Online]. Available: https://api.semanticscholar.org/CorpusID:16993816 [11] J. Inglada, V. Muron, D. Pichard, and T. Feuvrier, Analysis of artifacts in subpixel remote sensing image registration, IEEE Transactions on Geoscience and Remote Sensing, vol. 45, no. 1, pp. 254264, 2007. [12] Y. Bentoutou, N. Taleb, K. Kpalma, and J. Ronsin, An automatic image registration for applications in remote sensing, IEEE Transactions on Geoscience and Remote Sensing, vol. 43, no. 9, pp. 21272137, 2005. [13] J. A. Benediktsson, J. Chanussot, and W. M. Moon, Very highresolution remote sensing: Challenges and opportunities [point of view], Proceedings of the IEEE, vol. 100, no. 6, pp. 1907 1910, 2012. [14] S. S. Al-amri, N. V. Kalyankar, and S. D. Khamitkar, comparative study of removal noise from remote sensing image, 2010. [15] D. A. Landgrebe and E. Malaret, Noise in remote-sensing systems: The effect on classification error, IEEE Transactions on Geoscience and Remote Sensing, vol. GE-24, no. 2, pp. 294 300, 1986. [16] S. Fang, K. Li, J. Shao, and Z. Li, Snunet-cd: densely connected siamese network for change detection of vhr images, IEEE Geoscience and Remote Sensing Letters, vol. 19, pp. 15, 2022. [17] Y. Liu, C. Pang, Z. Zhan, X. Zhang, and X. Yang, Building change detection for remote sensing images using dual-task constrained deep siamese convolutional network model, IEEE Geoscience and Remote Sensing Letters, vol. 18, no. 5, pp. 811 815, 2021. [18] H. Chen, Z. Qi, and Z. Shi, Remote sensing image change detection with transformers, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 114, 2022. [19] W. G. C. Bandara and V. M. Patel, transformer-based siamese network for change detection, in IGARSS 2022 - 2022 IEEE International Geoscience and Remote Sensing Symposium, 2022, pp. 207210. [20] W. G. C. Bandara, N. G. Nair, and V. M. Patel, Ddpm-cd: Denoising diffusion probabilistic models as feature extractors for change detection, 2024. and G. Liu, [21] C. Zhang, P. Yue, D. Tapete, L. Jiang, B. Shangguan, L. Huang, deeply supervised image fusion network for change detection in high resolution ISPRS Journal of bi-temporal sensing images, Photogrammetry and Remote Sensing, vol. 166, pp. 183 200, 2020. [Online]. Available: https://www.sciencedirect.com/ science/article/pii/S remote [22] R. Caye Daudt, B. Le Saux, and A. Boulch, Fully convolutional siamese networks for change detection, in 2018 25th IEEE International Conference on Image Processing (ICIP), 2018, pp. 40634067. [23] Y. Wen, X. Ma, X. Zhang, and M.-O. Pun, Gcd-ddpm: generative change detection model based on difference-featureguided ddpm, IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 116, 2024. [24] K. Tian, Y. Jiang, Z. Yuan, B. Peng, and L. Wang, Visual autoregressive modeling: Scalable image generation via nextscale prediction, Advances in neural information processing systems, vol. 37, pp. 84 83984 865, 2024. [25] X. Li, K. Qiu, H. Chen, J. Kuen, Z. Lin, R. Singh, and B. Raj, Controlvar: Exploring controllable visual autoregressive modeling, arXiv preprint arXiv:2406.09750, 2024. [26] J. Zhang, H. Liu, K. Yang, X. Hu, R. Liu, and R. Stiefelhagen, Cmx: Cross-modal fusion for rgb-x semantic segmentation with transformers, IEEE Transactions on intelligent transportation systems, vol. 24, no. 12, pp. 14 67914 694, 2023. [27] S. Ji, S. Wei, and M. Lu, Fully convolutional networks for multisource building extraction from an open aerial and satellite imagery data set, IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no. 1, pp. 574586, 2019. [28] H. Chen and Z. Shi, spatial-temporal attention-based method and new dataset for remote sensing image change detection, Remote Sensing, vol. 12, no. 10, 2020. [Online]. Available: https://www.mdpi.com/2072-4292/12/10/1662 [29] , spatial-temporal attention-based method and new dataset remote sensing image change detection, Remote Sensing, vol. 12, no. 10, 2020. [Online]. Available: https://www.mdpi.com/2072-4292/12/10/1662 for [30] O. Manas, A. Lacoste, X. Giro-i Nieto, D. Vazquez, and P. RodrÄ±guez, Seasonal contrast: Unsupervised pre-training from uncurated remote sensing data, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2021, pp. 94149423. [31] H. Chen, W. Li, S. Chen, and Z. Shi, Semantic-aware dense representation learning for remote sensing image change detection, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 118, 2022. [32] S. Zhao, H. Chen, X. Zhang, P. Xiao, L. Bai, and W. Ouyang, Rs-mamba for large remote sensing image dense prediction, 2024. [33] L. Shen, Y. Lu, H. Chen, H. Wei, D. Xie, J. Yue, R. Chen, S. Lv, and B. Jiang, S2looking: satellite side-looking dataset for building change detection, Remote Sensing, vol. 13, 2021."
        }
    ],
    "affiliations": [
        "Johns Hopkins University"
    ]
}