{
    "paper_title": "Large Language Model Guided Self-Debugging Code Generation",
    "authors": [
        "Muntasir Adnan",
        "Zhiwei Xu",
        "Carlos C. N. Kuhn"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose a novel framework, PyCapsule, with a simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the state-of-art methods. We also observe a decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 8 2 9 2 0 . 2 0 5 2 : r Large Language Model Guided Self-Debugging Code Generation MUNTASIR ADNAN, ZHIWEI XU, and CARLOS C. N. KUHN, Open Source Institute, Faculty of Science and Technology, University of Canberra, Australia Automated code generation is gaining significant importance in intelligent computer programming and system deployment. However, current approaches often face challenges in computational efficiency and lack robust mechanisms for code parsing and error correction. In this work, we propose novel framework, PyCapsule, with simple yet effective two-agent pipeline and efficient self-debugging modules for Python code generation. PyCapsule features sophisticated prompt inference, iterative error handling, and case testing, ensuring high generation stability, safety, and correctness. Empirically, PyCapsule achieves up to 5.7% improvement of success rate on HumanEval, 10.3% on HumanEval-ET, and 24.4% on BigCodeBench compared to the stateof-art methods. We also observe decrease in normalized success rate given more self-debugging attempts, potentially affected by limited and noisy error feedback in retention. PyCapsule demonstrates broader impacts on advancing lightweight and efficient code generation for artificial intelligence systems. CCS Concepts: Software and its engineering Automatic programming; Computing methodologies Natural language processing. Additional Key Words and Phrases: Python Code Generation, Self-debugging, Large Language Model, Programming Agent, Artificial Intelligence Automation M. Adnan, Z. Xu, and C. C.N. Kuhn"
        },
        {
            "title": "1 Introduction\nThe evolution of generative artificial intelligence has profoundly influenced multiple domains,\nnotably in automated code generation. Particularly, Large Language Models (LLMs) have demon-\nstrated remarkable proficiency in code generation given natural language prompts, streamlining\nsoftware development processes and enhancing productivity [7, 24, 30]. This transformative po-\ntential has sparked significant research in this field. However, several critical challenges emerge\nas these systems scale to handle increasingly complex programming tasks. Ensuring reliability,\naccuracy, and autonomy in code generation remains critical but challenging, particularly in error\ndetection and debugging, that align with human developers‚Äô intentions [9, 10, 23]. Recent research\nhas introduced frameworks consisting of multiple LLM-based agents to emulate human problem-\nsolving capability in code generation. For instance, MapCoder [17], AgentCoder [14], and LDB [37]\nemploy agents dedicated to retrieval, planning, coding, and debugging to enhance code quality.\nWhile these approaches have shown remarkable improvements, they often require substantial\ncomputational resources and complex agent coordination. This poses challenges for real-world\ndeployment and scalability.",
            "content": "To address these concerns, we propose novel framework, PyCapsule, that enhances the efficiency and accuracy of LLM-based Python code generation through two-agent architecture. This simple yet effective two-agent interaction ensures functionality independence without overcrowding with AI agents, thereby minimizing its computational consumption. PyCapsule employs programmer agent responsible for code generation and debugging, interacting with an executor agent that handles code validation, case testing, and error analysis. The frameworks effectiveness is further Equal contributions. Authors Contact Information: Muntasir Adnan, adnan.adnan@canberra.edu.au; Zhiwei Xu, danny.xu@canberra.edu.au; Carlos C. N. Kuhn, carlos.noschangkuhnu@canberra.edu.au, Open Source Institute, Faculty of Science and Technology, University of Canberra, Bruce, ACT, Australia. 2 M. Adnan, Z. Xu, and C. C.N. Kuhn enhanced by three specialized modules: an error handler that refines the debugging feedback, an example call detector that prevents runtime issues, and function signature converter that provides uniform template for the function head description. This structure enables PyCapsule to achieve high performance on widely used datasets, including HumanEval [4], HumanEval-ET [8], Mostly Basic Python Programming (MBPP) [2], MBPP-ET [8], and BigCodeBench [38] with high computational efficiency. The main contributions of our work are summarized as Our PyCapsule is resource-efficient framework that achieves high performance with significantly reduced computational overhead compared to existing multi-agent approaches. Our two-agent architecture demonstrates that complex coordination between multiple agents can be replaced by well-structured language prompts and modules without sacrificing accuracy. Our efficient debugging mechanism aids the programmer agent through three specialized modules: an error handler, an example call detector, and function signature converter. These achieve up to 5% accuracy improvement on HumanEval, 10% on HumanEval-ET using GPT models, and 25% on BigCodeBench using the Qwen 7B model compared to its 32B variant. Through comprehensive analysis across multiple datasets and models, we observe consistent decrease in the normalized success rate as we increase the number of self-debugging attempts. We hypothesize that this is caused by the increased problem complexity due to limited and noisy error messages for the self-debugging process."
        },
        {
            "title": "2 Related Work\nAutomated high-quality code generation has evolved in several key directions related to prompt\nengineering [20, 33], multi-agent systems [3, 34], and iterative debugging with runtime feedback\nmechanisms [6, 14, 17, 22, 27]. While LLM has demonstrated increasing capability in code generation,\nthe code quality and correctness can be further enhanced by using intelligent debugging strategies.\nTherefore, previous works in self-debugging [6, 15] show their capability in Automated Program\nRepair (APR) by identifying and fixing significant code flaws and bugs. Researchers have shown that\n\"large language models of code\" [15] often struggle to directly fix bugs in the generated code due to\nthe lack of task-specific fine-tuning. These fine-tuned models outperform the state-of-the-art APR\ntools [11, 19, 36] but still suffer from the loss of pre-trained knowledge [1, 35], lack of debugging\ningredients, inferior long-sequence handling ability, high resource constraints, long-tail problems,\nand multi-hunk error fixing [15].",
            "content": "Particulary, MapCoder [17] presents multi-agent framework designed to emulate human-level programming processes. It employs four agents for retrieval, planning, coding, and debugging to perform iterative code generation and debugging. AgentCoder [14] introduces three-agent framework to address high token usage and coordination overhead observed in other multi-agent systems like MapCoder [17], MetaGPT [12], and ChatDev [29]. The framework comprises programmer agent, test designer agent, and test executor agent. The \"Debug Like Human\" framework [37] introduces Large Language Model Debugger (LDB), which enhances the code quality by leveraging runtime execution information. LDB is performed iteratively through segmenting code, tracking intermediate variable states, and performing step-by-step verification with breakpoints. Moreover, some approaches have employed prompt engineering to improve the quality of code generation and debugging. For instance, CodeCoT [13] leverages iterative refinement through Chain of Thought (CoT) [33] and logical reasoning. However, its capability is limited to refining syntax errors. While these approaches have advanced automated code generations, they share several notable limitations, particularly the resource intensity for complex problems. Additionally, they often Large Language Model Guided Self-Debugging Code Generation 3 struggle to fully leverage error messages, which impairs their adaptability to nuanced debugging scenarios. Although this issue can be alleviated by generating complementary test cases, it can further increase resource consumption due to the reliance on preemptive test cases such that generation may become less reliable [21, 22] and potentially require more complex agents. To address these challenges, we introduce PyCapsule as streamlined modular architecture with robust and optimized modules. It combines the knowledge from the operation research field and computer science to optimize the processing pipeline with merely two AI agents. This greatly reduces the computational overhead and facilitates more efficient and reliable code generation."
        },
        {
            "title": "3.1 Programmer and Executor Agents",
            "content": "Fig. 1. PyCapsule framework as Python code generation service of OpenSI-CoSMIC [1]. PyCapsule consists of iterative code generation with debugging, error handling, and code execution within Docker container. PyCapsule employs two distinct types of agents as defined in the agent-based computing literature [18, 25]: smart agents and collaborative agents. The system consists of two agents: programmer agent and an executor agent. The programmer agent generates code and handles debugging while the executor agent acts as an autonomous validator that continuously monitors and responds to code execution outcomes. When execution succeeds, it forwards the validated code to the user; otherwise, it generates detailed diagnostic reports and initiates real-time feedback loop with the programmer agent for self-debugging. The programmer agent leverages Ollama and GPT infrastructure embedded in OpenSI-CoSMIC [1] to facilitate self-debugging and code generation through structured pipeline. The programmer agent employs tailored system prompts with two programming modes, generation mode and fix mode. These prompts, detailed in Appendix E, define the programmer agents persona [31], provide task-specific background, and structurally simplify code extraction. In the generation mode, the programmer agent uses CoT [33] to analyze the problem description and devise structured solution to generate executable code. In the fix mode, the executor agent uses error information from the previous debugging attempt. This mode is supported by the original problem description, prior solution, and error messages processed by the error-handling module. The conversation history 4 M. Adnan, Z. Xu, and C. C.N. Kuhn includes the most recent problem-solution pair. For MBPP with Qwen2.5-Coder-7B-Instruct, tests with 2 and 3 conversation pairs decrease the accuracy from 80.7% to 77.6% and 76.7%, respectively. We implemented the executor agent using lightweight Docker container. The containers entry point is shell script for installing required Python libraries and testing the generated code, which are generated by the programmer agent. Once the problem is solved, the container returns success or failure status. The success status indicates that the generated code is correct by passing all the test cases while the failure status will trigger the error-handling module to refine and send the error message to the programmer agent. The programmer agent will then enter the fix mode to fix code flaws iteratively with self-debugging attempts. Particularly, with our analysis in Section 4.2 showing the diminishing influence of successive debugging attempts, the programmer agent is capable of refining the incorrect code with up to 5 self-debugging attempts. This seamless interaction between the two agents ensures robust and efficient feedback-driven debugging."
        },
        {
            "title": "3.2 Supportive Modules\nExisting approaches like MapCoder [17] and AgentCoder [14] rely heavily on LLM agents for code\ngeneration. However, this requires a high resource consumption. In contrast, PyCapsule employs\nthree specialised modules to handle each task deterministically, including a signature converter, an\nexample call detector, and an error handling module.",
            "content": "The signature converter mitigates the instability of code generation by parsing problem descriptions provided in datasets through inferring structured function signatures. This module generates function name, robust signature, and an example call using the first test case for given problem without revealing the case-testing result, thereby avoiding extra LLM calls for signature inference. The example call detector ensures the safety of code execution by removing any internal example calls, which otherwise can lead to an infinite loop of function calls. This automatic detection replaces dedicated LLM agent to check code execution safety. The error-handling module refines error messages provided by the executor agent, giving concise and highly relevant natural language feedback. It also identifies the error type, filters irrelevant tracebacks, and resolves critical issues such as verbose recursion errors that can disrupt the programmer agent due to the LLMs limited context length. This systematic approach requires fewer tokens in self-debugging iterations compared to the raw but redundant error messages. These modules collectively enhance the code generation stability, safety, and self-debugging efficiency while reducing the computational overhead. More details are provided in Appendix C."
        },
        {
            "title": "3.3 Code Generation Workflow\nPyCapsule consists of three phases: code generation, execution, and self-debugging, shown in\nFigure 1. This pipeline integrates the programmer agent and executor agent with the self-bugging\nmechanism to ensure reliable code generation and case testing with high efficiency and accuracy.\nThe workflow starts with the programmer agent that receives a problem description, typically\nsupplemented by a function signature either from the dataset or the signature converter module.\nSubsequently, the agent generates a response with CoT reasoning, environment setup instructions,\nand code synthesis. Then, the agent‚Äôs response is parsed to extract the synthesized code and envi-\nronment setup instructions, with the latter automatically generating a text file, requirements.txt,\ncontaining all required Python libraries.",
            "content": "The extracted code is further processed through the example call detector to remove any unintended example calls. At the generation phase, the processed code with associated test cases from the dataset is saved in Python entry file main.py for code execution. To ensure code execution safety, we add timer with maximum execution time to prevent infinite loops and raise an out-of-time error for those requiring longer running time. Large Language Model Guided Self-Debugging Code Generation 5 At the execution phase, the executor agent runs main.py in Docker container with Python libraries from requirements.txt installed. The corresponding code execution status, either successful or failed, determines whether the current generated code requires to be fixed by using the programmer agents fix mode. If so, error messages extracted from the executor agent are refined by the error-handling module and will used by the programmer agent to fix code flaws. This mode allows up to 5 self-debugging attempts, each incorporating code execution feedback from the previous bug-fixing process. The pseudo-code for the pipeline is provided in Appendix D."
        },
        {
            "title": "4.1 Dataset Overview\nWe used five benchmark datasets to evaluate PyCapsule: HumanEval [4], HumanEval-ET [8],\nMBPP [2], MBPP-ET [8], and BigCodeBench [38], where the \"ET\" variants provide more test cases\nand enriched problem descriptions. These datasets encompass diverse programming challenges.",
            "content": "Particularly, HumanEval [4] consists of 164 human-curated programming problems for string manipulation, arithmetic, and basic data structures. Each problem includes natural language description, function signature, and test cases. MBPP [2] includes 974 programming problems spanning algorithms, data structures, and general-purpose tasks. It provides natural language descriptions and test cases but no function signatures to guarantee code generation stability. BigCodeBench [38] includes 1,140 problems with two splits: the complete split for code completion with specific docstrings and the instruct split with concise instructions for advanced reasoning."
        },
        {
            "title": "4.2 Results and Analysis\nUnlike other multi-agent architectures discussed in Section 2 that heavily rely on code generation\nagents, PyCapsule has more effective coordination among the computational overhead reduction,\nLLM‚Äôs API calls, and token processing.",
            "content": "In Table 1, PyCapsule achieves the state-of-the-art code generation tasks across HumanEval, HumanEval-ET, and BigCodeBench. While showing strong performance on MBPP, it falls short in comparison to AgentCoder and Qwen-2.5 [16] which employs three-shot prompt engineering to stabilize code generation. Although it shows remarkable potential, we maintain consistent prompting approach across datasets to ensure comparability and utilise our signature converter module to generate actionable function signatures. Remarkably, on HumanEval, the integration of Qwen-2.5-Coder-Instructs 7B model with PyCapsule achieves 94.1% success rate compared to 92.7% by its 32B model. Furthermore, on BigCodeBench, PyCapsule using Qwen-2.5-CoderInstructs 7B shows 25% improvement over the 49.6% reported by the 32B model variant. This demonstrates significant savings in computational power while exceeding the performance. Influence of Self-debugging Attempts. To evaluate the influence of each self-debugging attempt on the success rate, we analyze the normalized and accumulated impacts of each incremental attempt using GPT-4-1106-preview, GPT-3.5-Turbo-1106, and Qwen-2.5-Coder-Instruct models as the LLM for code generation. The success rate normalization was performed as follows: given ùëÅ problems, the number of successfully solved problems is denoted as ùëÜ0 while the rest ùëÅ1 = ùëÅ ùëÜ0 unsolved problems will be further resolved with self-debugging attempt. Accordingly, the number 1Could not be reproduced during our experiments. 6 M. Adnan, Z. Xu, and C. C.N. Kuhn Table 1. Success rate comparison on three popular LLMs. We provide the mean value and standard deviation for PyCapsule with three experiment repeats. The results of other methods are from their official reports."
        },
        {
            "title": "Method",
            "content": "HumanEval HumanEval-ET"
        },
        {
            "title": "MBPP",
            "content": "MBPP-ET"
        },
        {
            "title": "BigCodeBench",
            "content": "Direct CoT [17] Self-Planning [17] AgentCoder1 [14] MapCoder [17] PyCapsule (ours) Direct Direct (LDB [37]) CoT [17] Self-Planning [32] Reflexion [32] AgentCoder1 [14] MapCoder [17] LDB [37] PyCapsule (ours) Direct [16] PyCapsule (ours) 80.1 [32] 89.0 85.4 96.3 93.9 96.50.7 48.1 [26] 73.8 68.9 60.3 67.1 79.9 80.5 82.9 85.20.7 88.4 94.11. GPT-4-Preview-1106 86.0 82.9 96.31.2 81.1 [17] 82.4 75.8 91.8 82.1 88.20.3 GPT-3.5-Turbo-1106 77.4 70.1 84.70. 49.8 [17] 67.6 54.5 55.7 73.0 89.9 78.3 76.0 78.51.2 Qwen2.5-Coder-7B-Instruct 83.5 80.70.9 93.30.6 91.8 57.7 73.00.8 89.1 54.4 62.40.6 63.60.6 41.0 65.40. Fig. 2. Distribution of relative success ratios along the self-debugging attempts. The relative success ratio refers to the number of successful test cases with given attempts over the total number of successful test cases, measured by unit %. Results on all the datasets consistently show decreasing relative success ratio as the number of attempts increases. of problems solved with ùëñ 1 attempts is denoted as ùëÜùëñ , and ùëÅùëñ+1 = ùëÅùëñ ùëÜùëñ for the rest. The independent influence is defined as ùêºùëñ = ùëÜùëñ ùëÅùëñ , where ùëÅùëñ = ùëÅ ùëñ 1 ùëó=0 ùëÜ ùëó . This metric highlights the independent contribution of each attempt. In Figure 3, the descending trend in the independent influence indicates diminishing effectiveness of debugging attempts, where problems requiring more attempts may need enriched conversation history or enhanced prompts to improve the accuracy. We hypothesize that the first few self-debugging attempts address the most significant and easily identifiable errors while substantially improving the success rate. Large Language Model Guided Self-Debugging Code Generation 7 Fig. 3. The left subfigure shows the normalized influence of each self-debugging attempt through independent accuracy improvements. The right subfigure highlights the mean influence of each self-debugging attempt on the overall accuracy through the accumulated influence without normalization. However, since these primary issues are resolved, more subtle and complex issues occur, and they are difficult to be detected and solved even with more attempts, resulting in diminishing normalised accuracy while increasing the overall accuracy by small amounts, as demonstrated by the right plot of Figure 3 . Additionally, the accumulation of minor fixes may introduce unseen issues that further degrade the effect of self-debugging attempts."
        },
        {
            "title": "4.3 Discussion\nPyCapsule‚Äôs single-attempt accuracy is significantly lower than Qwen-2.5‚Äôs reported performance,\nand it varies considerably across datasets, as shown in Figure 2 and detailed in Appendix A. Never-\ntheless, PyCapsule achieves higher single-attempt accuracy compared to GPT models implemented\nin MapCoder and AgentCoder. This nuanced performance difference highlights the varying impacts\nof prompt engineering and framework structure on code generation without self-debugging. With\nmore debugging attempts, PyCapsule shows overall remarkable improvements, surpassing all the\nother self-debugging methods. This iterative improvement highlights the effectiveness of prompt\nengineering and error-handling ability of PyCapsule. Prior works [5, 28, 33] have demonstrated\nhow prompt design significantly influences model performance, especially for generative AI tasks.\nPyCapsule leverages sophisticated system prompts that are dynamically augmented in the code\ngeneration and fix modes. Furthermore, an important advantage of PyCapsule is its reduced re-\nliance on LLM-based agents, leading to reduced token consumption and improved computational\nefficiency. Existing frameworks like MapCoder can make up to 17 API calls with GPT models per\nproblem on HumanEval and 12 calls on MBPP, following ùëò √ó ùë° debugging attempts, where ùëò is the\nnumber of retrieved exemplars and ùë° is the number of self-debugging attempts. In contrast, PyCap-\nsule streamlines this process by limiting the programmer agent with a maximum of 5 attempts,\nrequiring at most 6 LLM API calls for each problem because planning and code generation are\nhandled within a single API call. Our empirical analysis shows that PyCapsule achieves superior\ncode generation ability while maintaining efficient API usage, requiring an average of only 2.09,\n1.38, and 1.55 API calls for HumanEval using GPT-3.5-Turbo-1106, GPT-4-Preview-1106, and Qwen-\n2.5-Coder-Instruct-7B, respectively. This optimization demonstrates the possibility of achieving\nhigh performance with significantly reduced computational overhead.",
            "content": "8 M. Adnan, Z. Xu, and C. C.N. Kuhn"
        },
        {
            "title": "5 Conclusion\nWe propose PyCapsule for significantly advancing reliable and automated code generation by\nintegrating two AI agents with high-efficiency self-debugging modules. Its two-agent architecture\neffectively balances code generation automation and control while reducing resource consumption.\nExperimental results show that PyCapsule achieves high performance across multiple datasets, sur-\npassing existing debugging-based approaches. Notably, PyCapsule enhances accuracy while using\nsmaller language models and fewer API calls, offering a cost-effective alternative without compro-\nmising its performance. The iterative self-debugging feedback and dynamic prompt augmentation\ncontribute to its robustness and improvements.",
            "content": "References [1] Muntasir Adnan, Buddhi Gamage, Zhiwei Xu, Damith Herath, and Carlos C. N. Kuhn. 2024. Unleashing Artificial Cognition: Integrating Multiple AI Systems. ACIS 2024 Proceedings 31 (2024). [2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021). [3] Mark Burgin and Gordana Dodig Crnkovic. 2009. Systematic Approach to Artificial Agents. (03 2009). [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, et al. 2021. Evaluating Large Language Models Trained on Code. arXiv preprint arXiv:2107.03374 (2021). [5] Xi Chen, XiangWen Deng, Hao Wen, MingKe You, Weizhi Liu, Qi Li, and Jian Li. 2024. Prompt engineering in consistency and reliability with the evidence-based guideline for LLMs. npj Digital Medicine 7 (02 2024). doi:10.1038/s41746-02401029- [6] Xinyun Chen, Maxwell Lin, Nathanael Sch√§rli, and Denny Zhou. 2023. Teaching Large Language Models to Self-Debug. arXiv:2304.05128 [cs.CL] https://arxiv.org/abs/2304.05128 [7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sashank Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2024. PaLM: scaling language modeling with pathways. In Journal of Machine Learning Research. [8] Yihong Dong, Jiazheng Ding, Xue Jiang, Zhuo Li, Ge Li, and Zhi Jin. 2023. Codescore: Evaluating code generation by learning code execution. arXiv preprint arXiv:2301.09043 (2023). [9] Shihan Dou, Haoxiang Jia, Shenxi Wu, Huiyuan Zheng, Weikang Zhou, Muling Wu, Mingxu Chai, Jessica Fan, Caishuang Huang, Yunbo Tao, Yan Liu, Enyu Zhou, Ming Zhang, Yuhao Zhou, Yueming Wu, Rui Zheng, Ming Wen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Tao Gui, Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2024. Whats Wrong with Your Code Generated by Large Language Models? An Extensive Study. arXiv:2407.06153 [cs.SE] https://arxiv.org/abs/2407. [10] Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, and Jie M. Zhang. 2023. Large Language Models for Software Engineering: Survey and Open Problems. In 2023 IEEE/ACM International Conference on Software Engineering: Future of Software Engineering (ICSE-FoSE). 3153. doi:10.1109/ICSE-FoSE59343.2023.00008 [11] Michael Fu, Chakkrit Tantithamthavorn, Trung Le, Van Nguyen, and Dinh Phung. 2022. VulRepair: T5-based automated software vulnerability repair. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Singapore, Singapore) (ESEC/FSE 2022). Association for Computing Machinery, New York, NY, USA, 935947. doi:10.1145/3540250.3549098 [12] Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and J√ºrgen Schmidhuber. 2024. MetaGPT: Meta Programming for Multi-Agent Collaborative Framework. arXiv:2308.00352 [cs.AI] https: //arxiv.org/abs/2308.00352 [13] Dong Huang, Qingwen Bu, Yuhao Qing, and Heming Cui. 2024. CodeCoT: Tackling Code Syntax Errors in CoT Reasoning for Code Generation. arXiv:2308.08784 [cs.SE] https://arxiv.org/abs/2308.08784 Large Language Model Guided Self-Debugging Code Generation [14] Dong Huang, Jie M. Zhang, Michael Luck, Qingwen Bu, Yuhao Qing, and Heming Cui. 2024. AgentCoder: MultiAgent-based Code Generation with Iterative Testing and Optimisation. arXiv:2312.13010 [cs.CL] https://arxiv.org/abs/ 2312.13010 [15] Kai Huang, Xiangxin Meng, Jian Zhang, Yang Liu, Wenjie Wang, Shuhao Li, and Yuqing Zhang. 2023. An Empirical Study on Fine-Tuning Large Language Models of Code for Automated Program Repair. 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE) (2023), 11621174. https://api.semanticscholar.org/CorpusID: 265054960 [16] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. 2024. Qwen2.5-Coder Technical Report. arXiv:2409.12186 [cs.CL] https://arxiv.org/abs/2409.12186 [17] Md. Ashraful Islam, Mohammed Eunus Ali, and Md Rizwan Parvez. 2024. MapCoder: Multi-Agent Code Generation for Competitive Problem Solving. arXiv:2405.11403 [cs.CL] https://arxiv.org/abs/2405.11403 [18] Nicholas Jennings, Katia Sycara, and Michael Wooldridge. 1998. Roadmap of Agent Research and Development. Autonomous Agents and Multi-Agent Systems 1 (03 1998), 738. doi:10.1023/A:1010090405266 [19] Harshit Joshi, Jos√© Cambronero Sanchez, Sumit Gulwani, Vu Le, Ivan Radiƒçek, and Gust Verbruggen. 2023. Repair is nearly generation: multilingual program repair with LLMs. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence (AAAI23/IAAI23/EAAI23). AAAI Press, Article 573, 10 pages. doi:10.1609/aaai.v37i4.25642 [20] Nils Knoth, Antonia Tolzin, Andreas Janson, and Jan Marco Leimeister. 2024. AI literacy and its implications for prompt engineering strategies. Computers and Education: Artificial Intelligence (2024). [21] Kefan Li and Yuan Yuan. 2024. Large Language Models as Test Case Generators: Performance Evaluation and Enhancement. arXiv:2404.13340 [cs.SE] https://arxiv.org/abs/2404. [22] Yihao Li, Pan Liu, Haiyang Wang, Jie Chu, and W. Eric Wong. 2025. Evaluating large language models for software testing. Computer Standards & Interfaces 93 (2025), 103942. doi:10.1016/j.csi.2024.103942 [23] Xiaoli Lian, Shuaisong Wang, Jieping Ma, Xin Tan, Fang Liu, Lin Shi, Cuiyun Gao, and Li Zhang. 2024. Imperfect Code Generation: Uncovering Weaknesses in Automatic Code Generation by Large Language Models (ICSE-Companion 24). Association for Computing Machinery, New York, NY, USA, 422423. doi:10.1145/3639478.3643081 [24] Mingxing Liu, Junfeng Wang, Tao Lin, Quan Ma, Zhiyang Fang, and Yanqun Wu. 2024. An Empirical Study of the Code Generation of Safety-Critical Software Using LLMs. Applied Sciences 14, 3 (2024). doi:10.3390/app14031046 [25] Hyacinth S. Nwana. 1996. Software agents: an overview. The Knowledge Engineering Review 11, 3 (1996), 205244. doi:10.1017/S026988890000789X [26] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, et al. 2024. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] https://arxiv.org/abs/2303. [27] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra of Human Behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (San Francisco, CA, USA) (UIST 23). Association for Computing Machinery, New York, NY, USA, Article 2, 22 pages. doi:10.1145/3586183.3606763 [28] Dhavalkumar Patel, Ganesh Raut, Eyal Zimlichman, Satya Cheetirala, Girish Nadkarni, Benjamin Glicksberg, Donald Apakama, Elijah Bell, Robert Freeman, Prem Timsina, and Eyal Klang. 2024. Evaluating prompt engineering on GPT-3.5s performance in USMLE-style medical calculations and clinical scenarios generated by GPT-4. Scientific Reports 14 (07 2024). doi:10.1038/s41598-024-66933-x [29] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2024. ChatDev: Communicative Agents for Software Development. arXiv:2307.07924 [cs.SE] https://arxiv.org/abs/2307.07924 [30] Sanka Rasnayaka, Guanlin Wang, Ridwan Shariffdeen, and Ganesh Neelakanta Iyer. 2024. An Empirical Study on Usage and Perceptions of LLMs in Software Engineering Project. In Proceedings of the 1st International Workshop on Large Language Models for Code (Lisbon, Portugal) (LLM4Code 24). Association for Computing Machinery, New York, NY, USA, 111118. doi:10.1145/3643795.3648379 [31] Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. Role play with large language models. Nature 623 (11 2023). doi:10.1038/s41586-023-06647- [32] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs.AI] https://arxiv.org/abs/2303. 11366 10 M. Adnan, Z. Xu, and C. C.N. Kuhn [33] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. ArXiv abs/2201.11903 (2022). https: //api.semanticscholar.org/CorpusID:246411621 [34] Michael Wooldridge and Nicholas Jennings. 1995. Intelligent agents: Theory and practice. The knowledge engineering review 10, 2 (1995), 115152. [35] Chun Xia and Lingming Zhang. 2022. Less training, more repairing please: revisiting automated program repair via zero-shot learning. Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (2022). https://api.semanticscholar.org/CorpusID:250627519 [36] Chunqiu Steven Xia and Lingming Zhang. 2022. Less training, more repairing please: revisiting automated program repair via zero-shot learning. In Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (Singapore, Singapore) (ESEC/FSE 2022). Association for Computing Machinery, New York, NY, USA, 959971. doi:10.1145/3540250.3549101 [37] Li Zhong, Zilong Wang, and Jingbo Shang. 2024. Debug like Human: Large Language Model Debugger via Verifying Runtime Execution Step-by-step. arXiv:2402.16906 [cs.SE] https://arxiv.org/abs/2402.16906 [38] Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. 2024. BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions. arXiv preprint arXiv:2406.15877 (2024). Large Language Model Guided Self-Debugging Code Generation"
        },
        {
            "title": "Appendix",
            "content": "A Contribution of Each Debugging Attempt To further illustrate the influence of iterative debugging, we present detailed breakdown of the success rate at each debugging attempt. Table 2 quantifies the proportion of problems successfully resolved at each stage. While the independent influence of successive attempts gradually decreases, the cumulative effect leads to an overall increase in the accuracy of problem-solving. Each debugging attempt contributes to solving additional problems, underscoring the value of iterative refinement in improving outcomes. This dual perspectiverising cumulative accuracy alongside diminishing returns per attemptdemonstrates the significance of leveraging efficient debugging strategies, such as enriched conversation history and refined prompts, to maximize the potential of LLM-based code generation systems. Table 2. Accuracy of PyCapsule framework across code generation and debugging attempts (05) for OpenAI GPT-4-Preview-1106, GPT-3.5-Turbo-1106 and Qwen 2.5 Coder Instruct. Attempt 0 represents the initial solution generated by the Programmer agent, while subsequent attempts (15) indicate iterative fixes during the debugging process. The table reports accuracy with associated sample standard deviation (ùë†). The results also showcase the incremental improvement achieved through PyCapsules iterative feedback mechanism."
        },
        {
            "title": "HumanEval",
            "content": "HumanEval-ET"
        },
        {
            "title": "MBPP",
            "content": "MBPP-ET"
        },
        {
            "title": "BigCodeBench",
            "content": "0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5 92.0 1.4 3.8 0.7 1.9 1.1 1.1 0.4 1.1 0.7 0.2 0.4 81.9 0.6 9.7 0.3 4.0 0.4 1.2 0.8 2.1 0.0 1.0 0.4 79.3 1.4 14.9 2.5 2.6 0.7 1.5 0.4 0.6 0.6 1.1 1. GPT-4-Preview-1106 88.0 3.7 8.2 4.3 2.3 1.4 0.8 0.4 0.6 0.0 0.0 0.0 70.6 1.0 18.0 1.1 5.9 1.5 3.0 0.3 1.4 0.5 1.2 0.4 GPT-3.5-Turbo-1106 77.2 1.5 13.2 0.4 5.3 3.0 1.9 1.1 1.4 0.7 1.0 1.1 74.6 1.8 15.1 0.9 5.2 0.7 2.6 0.5 1.3 0.4 1.3 0. Qwen2.5-Coder-7B-Instruct 60.9 0.7 81.3 0.4 22.4 0.4 13.1 2.0 7.5 0.2 2.2 0.8 4.5 0.5 1.7 1.0 2.9 0.3 1.5 0.7 1.8 0.3 0.2 0.4 67.5 0.8 17.2 1.6 8.6 0.5 3.2 0.3 1.9 0.2 1.6 0.2 70.3 1.3 16.2 0.2 6.6 1.5 3.3 0.1 2.0 0.1 1.5 0.2 59.7 1.2 21.6 0.7 8.7 0.4 5.0 0.3 2.9 0.3 2.2 0.5 - - - - - - - - - - - - 67.2 0.9 14.9 0.9 7.1 0.5 4.7 0.8 3.4 0.7 2.7 0.2 12 M. Adnan, Z. Xu, and C. C.N. Kuhn HumanEval Experiments Figure 4 and Figure 5 provide detailed analysis of debugging effectiveness across multiple attempts in the HumanEval benchmark. By tracking the resolution process of individual tasks, they highlight the impact of iterative debugging in improving accuracy. Fig. 4. Visualisation of Debugging Attempts. The left column represents the number of debugging attempts on the HumanEval benchmark. Each point represents problem, with failed tasks highlighted in red. The right column displays the mean number of debugging attempts and the associated sample standard deviations across three experiments for each model. The shaded areas in the right column indicate the variability in performance, illustrating the consistency and reliability of each model. Fig. 5. HumanEval experiment results using GPT-4-1106 across three repeats. Each line represents the progression of debugging attempts (up to 5) for tasks in the HumanEval dataset. Red markers denoting failed attempts. The coloured lines correspond to Experiment 1 (blue), Experiment 2 (green), and Experiment 3 (orange). The y-axis reflects the number of attempts taken to resolve each task. Large Language Model Guided Self-Debugging Code Generation 13 Modules (Detailed) C.1 Signature Converter The signature converter addresses code generation instability due to the lack of structured function signatures, particularly in the MBPP dataset (discussed in Section 4). Using the first test case, it infers the function name, input arguments, and constructs robust function signature without revealing the expected output of the test case. The generated output includes: The inferred function name. structured function signature (e.g., argument names and types). An example function call derived from the test case. For instance, given test case like assert foo(4) == 16, the module generates: ### Required function name for your reference foo() ### Function signature for your reference - foo(arg_int: int) ### An example function call from private test cases - foo(4) While return types are excluded from the function signature, they can be inferred from the test cases data type. This module enhances the clarity and consistency of code generation. C.2 Example Call Detection The example call detection module identifies and removes instances where the programmer agent includes sample function invocations within its generated code output. Despite explicit system prompts to avoid such calls, they frequently appear in outputs, as observed in 21 of 164 HumanEval problems and 44 of 344 MBPP problems in one of our experiments with GPT-3.5-Turbo-1106. These embedded example calls pose security risk as they can execute automatically during test case evaluation, potentially bypassing the runtime safety mechanisms weve implemented for controlled code execution. This module scans the generated solution and removes any detected example calls before execution, ensuring system reliability. For example, calls like - foo(4) embedded in code are automatically removed. This safeguard prevents unintentional execution of potentially unsafe code outside the controlled environment. C.3 Error-Handling The error-handling module refines error messages returned by the executor agent, making them concise and supplemented with natural language feedback. It mimics human debugging practices by streamlining error messages into relevant components. The module processes error messages in the following steps: Error Type Analysis: First, the module identifies error types and provides contextual feedback. For example, in the case of an AssertionError, the message might include: \"Your generated solution failed test case. Please improve the logic of your solution.\" Relevance Filtering: It shortens error messages to focus on errors within the main.py file, truncating irrelevant tracebacks referencing external files. Critical error-handling: For issues like RecursionError, the module truncates verbose error messages (e.g., maximum recursion depth exceeded) to ensure compatibility with the LLMs context length. Without this, such errors could disrupt the LLMs input pipeline. This module improves the debugging process by filtering and contextualizing errors, ensuring that the Programmer Agent receives actionable and relevant feedback. 14 M. Adnan, Z. Xu, and C. C.N. Kuhn Pseudo-code of PyCapsule Algorithm 1 Pseudo-code of PyCapsule Input: user-defined query for code generation with problem description, test cases, and the maximum number of self-debugging attempts ùëÅ = 5. Output: generated code satisfying the request in the problem description if all test cases passed, otherwise failed status message. Procedure: 1: Set and initialize self-debugging attempt counter ùëñ = 0. 2: Set an initial system prompt for generating code, buffer for storing conversation history, and user prompt initialized by Q. Return the extracted code as the optimal generated code. 3: Use and the LLM in PyCapsule to generate the function signature required by case testing. 4: Step 1: Generate code (\"Generation Mode\"). 5: Generate prompt by integrating the system prompt and user prompt. 6: Use the prompt and the same LLM to generate response for code generation. 7: Extract the required code, that is the function implementation, from the response. 8: Create Python container with the required packages installed. If it exists, skip this step. 9: Test all the cases in the container using the extract code, and return the code execute status. 10: if all the test cases passed then 11: 12: else if ùëñ > ùëÅ then 13: 14: else 15: 16: 17: 18: 19: 20: Step 2: Code Debugging (\"Fix Mode\"). Extract the error message from the code execute status and add it to B. Generate an error-handling prompt using the error-handling module. Update the system prompt for code debugging. Update the user prompt by integrating the error-handling prompt and messages in B. Update ùëõ = ùëõ + 1. Repeat Step 1. Return the failed message extracted from the code execute status. 21: 22: end if Large Language Model Guided Self-Debugging Code Generation"
        },
        {
            "title": "E System Prompts",
            "content": "E.1 Code Generation Mode You are an experienced Python developer. Your task is to complete the function based on the provided function signature and description - Analyze the description and provide step by step reasoning on how to solve the problem. - Maintain the function signature: Do not alter/modify the function signature. - Avoid example calls: Do not include any example call in your response. - If external libraries are needed, add '### Requirements' Section listing them separated by ','. e.g. pandas, pyhton-dotenv ). If no external libraries are needed, add 'None'. - Import all required libraries and complete the function in '### Code' Section, enclosed with triple backticks. Example Response Structure- ### Step-by-step reasoning $reasoning ### Requirements $external_libraries ### Code ``` $code ``` E.2 Fix Mode You are an experienced Python developer. - Your previous solution resulted an error. - Error message from python compiler and Conversation history has been added for your reference. - Analyze the description and provide step by step reasoning on how to solve the problem. - Maintain the function signature: Do not alter/modify the function signature. - Avoid example calls: Do not include any example call in your response. - If external libraries are needed, add '### Requirements' Section listing them separated by ','. e.g. pandas, pyhton-dotenv ). If no external libraries are needed, add 'None'. - Import all required libraries and complete the function in '### Code' Section, enclosed with triple backticks. Example Response Structure- ### Step-by-step reasoning $reasoning 16 M. Adnan, Z. Xu, and C. C.N. Kuhn ### Requirements $external_libraries ### Code ``` $code ``` Received XX; revised XX; accepted XX"
        }
    ],
    "affiliations": [
        "Open Source Institute, Faculty of Science and Technology, University of Canberra, Australia"
    ]
}