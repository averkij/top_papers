{
    "paper_title": "A Survey of Context Engineering for Large Language Models",
    "authors": [
        "Lingrui Mei",
        "Jiayu Yao",
        "Yuyao Ge",
        "Yiwei Wang",
        "Baolong Bi",
        "Yujun Cai",
        "Jiazhi Liu",
        "Mingyu Li",
        "Zhong-Zhi Li",
        "Duzhen Zhang",
        "Chenlin Zhou",
        "Jiayi Mao",
        "Tianze Xia",
        "Jiafeng Guo",
        "Shenghua Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI."
        },
        {
            "title": "Start",
            "content": "Lingrui Mei1,6, Jiayu Yao1,6, Yuyao Ge1,6, Yiwei Wang2 Baolong Bi1,6, Yujun Cai3 Chenlin Zhou Jiazhi Liu1 Mingyu Li1 Tianze Xia6 Jiayi Mao5 Zhong-Zhi Li6 Duzhen Zhang6 Jiafeng Guo1,6, Shenghua Liu1,6,, 1 Institute of Computing Technology, Chinese Academy of Sciences, 2 University of California, Merced, 3 The University of Queensland 4 Peking University, 5 Tsinghua University, 6 University of Chinese Academy of Sciences Abstract: The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present comprehensive taxonomy decomposing Context Engineering into its foundational Components and the sophisticated Implementations that integrate them into intelligent systems. We first examine the foundational Components: (1) Context Retrieval and Generation, encompassing prompt-based generation and external knowledge acquisition; (2) Context Processing, addressing long sequence processing, self-refinement, and structured information integration; and (3) Context Management, covering memory hierarchies, compression, and optimization. We then explore how these components are architecturally integrated to create sophisticated System Implementations: (1) Retrieval-Augmented Generation (RAG), including modular, agentic, and graph-enhanced architectures; (2) Memory Systems, enabling persistent interactions; (3) Tool-Integrated Reasoning, for function calling and environmental interaction; and (4) Multi-Agent Systems, coordinating communication and orchestration. Through this systematic analysis of over 1400 research papers, our survey not only establishes technical roadmap for the field but also reveals critical research gap: fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is defining priority for future research. Ultimately, this survey provides unified framework for both researchers and engineers advancing context-aware AI. Also affiliated with: (1)Key Laboratory of Network Data Science and Technology, ICT, CAS; (2)State Key Laboratory of AI Safety Corresponding Author Keywords: Context Engineering, Large Language Models, LLM Agent, Multi-Agent Systems Date: July 17, 2025 Code Repository: https://github.com/Meirtz/Awesome-Context-Engineering Contact: meilingrui23b@ict.ac.cn, liushenghua@ict.ac.cn 5 2 0 2 7 1 ] . [ 1 4 3 3 3 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 3 Why Context Engineering? 3.1 Definition of Context Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 7 8 3.2 Why Context Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2.1 Current Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2.2 Performance Enhancement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.3 Resource Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.2.4 Future Potential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4 Foundational Components 12 4.1 Context Retrieval and Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Prompt Engineering and Context Generation . . . . . . . . . . . . . . . . . . . . . . . 13 4.1.2 External Knowledge Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4.1.3 Dynamic Context Assembly . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 4.2 Context Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.2.1 Long Context Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 Contextual Self-Refinement and Adaptation . . . . . . . . . . . . . . . . . . . . . . . 18 4.2.3 Multimodal Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 4.2.4 Relational and Structured Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.3 Context Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.1 Fundamental Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4.3.2 Memory Hierarchies and Storage Architectures . . . . . . . . . . . . . . . . . . . . . 24 4.3.3 Context Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.3.4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 System Implementations 27 5.1 Retrieval-Augmented Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 5.1.1 Modular RAG Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 2 5.1.2 Agentic RAG Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.3 Graph-Enhanced RAG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 5.1.4 Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 5.2 Memory Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 5.2.1 Memory Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 5.2.2 Memory-Enhanced Agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 5.2.3 Evaluation and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Tool-Integrated Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.3.1 Function Calling Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.3.2 Tool-Integrated Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 5.3.3 Agent-Environment Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 5.4 Multi-Agent Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 5.4.1 Communication Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.2 Orchestration Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 5.4.3 Coordination Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 6 Evaluation 45 6.1 Evaluation Frameworks and Methodologies . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 6.1.1 Component-Level Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 6.1.2 System-Level Integration Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 6.2 Benchmark Datasets and Evaluation Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.2.1 Foundational Component Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . 6.2.2 System Implementation Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.3 Evaluation Challenges and Emerging Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . 48 6.3.1 Methodological Limitations and Biases . . . . . . . . . . . . . . . . . . . . . . . . . . 49 6.3.2 Emerging Evaluation Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 6.3.3 Safety and Robustness Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Future Directions and Open Challenges 50 7.1 Foundational Research Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 7.1.1 Theoretical Foundations and Unified Frameworks . . . . . . . . . . . . . . . . . . . . 51 7.1.2 Scaling Laws and Computational Efficiency . . . . . . . . . . . . . . . . . . . . . . . 3 7.1.3 Multi-Modal Integration and Representation . . . . . . . . . . . . . . . . . . . . . . . 52 7.2 Technical Innovation Opportunities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 7.2.1 Next-Generation Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 7.2.2 Advanced Reasoning and Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 7.2.3 Complex Context Organization and Solving Graph Problems . . . . . . . . . . . . . . 7.2.4 Intelligent Context Assembly and Optimization . . . . . . . . . . . . . . . . . . . . . 54 7.3 Application-Driven Research Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 7.3.1 Domain Specialization and Adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . 55 7.3.2 Large-Scale Multi-Agent Coordination . . . . . . . . . . . . . . . . . . . . . . . . . . 55 7.3.3 Human-AI Collaboration and Integration . . . . . . . . . . . . . . . . . . . . . . . . . 56 7.4 Deployment and Societal Impact Considerations . . . . . . . . . . . . . . . . . . . . . . . . . 7.4.1 Scalability and Production Deployment . . . . . . . . . . . . . . . . . . . . . . . . . . 56 7.4.2 Safety, Security, and Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 7.4.3 Ethical Considerations and Responsible Development . . . . . . . . . . . . . . . . . . 57 8 Conclusion 1. Introduction The advent of LLMs has marked paradigm shift in artificial intelligence, demonstrating unprecedented capabilities in natural language understanding, generation, and reasoning [103, 1059, 453]. However, the performance and efficacy of these models are fundamentally governed by the context they receive. This contextranging from simple instructional prompts to sophisticated external knowledge basesserves as the primary mechanism through which their behavior is steered, their knowledge is augmented, and their capabilities are unleashed. As LLMs have evolved from basic instruction-following systems into the core reasoning engines of complex applications, the methods for designing and managing their informational payloads have correspondingly evolved into the formal discipline of Context Engineering [25, 1256, 1060]. The landscape of context engineering has expanded at an explosive rate, resulting in proliferation of specialized yet fragmented research domains. We conceptualize this landscape as being composed of foundational components and their subsequent implementations. The foundational components represent the systematic pipeline of context engineering through three critical phases: Context Retrieval and Generation, encompassing prompt-based generation and external knowledge acquisition [25, 591, 48]; Context Processing, involving long sequence processing, self-refinement mechanisms, and structured information integration [196, 735, 489]; and Context Management, addressing memory hierarchies, compression techniques, and optimization strategies [1362, 1074, 813]. These foundational components serve as the building blocks for more complex, application-oriented implementations that bridge LLMs to external realities. These systems include Advanced Retrieval-Augmented Generation (RAG), which has evolved into modular and agentic architectures for dynamic knowledge 4 injection [591, 312, 965, 311]; explicit Memory Systems that mimic human cognitive faculties for persistent information retention [1182, 935, 1362]; and the entire ecosystem of Intelligent Agent Systems. This latter category represents the pinnacle of context engineering, where agents leverage Function Calling and Tool-Integrated Reasoning to interact with the world [931, 858, 663], and rely on sophisticated Agent Communication protocols and Context Orchestration to achieve complex goals in multi-agent configurations [356, 246, 894, 128]. While each of these domains has generated substantial innovation, they are predominantly studied in isolation. This fragmented development obscures the fundamental connections between techniques and creates significant barriers for researchers seeking to understand the broader landscape and practitioners aiming to leverage these methods effectively. The field urgently requires unified framework that systematically organizes these diverse techniques, clarifies their underlying principles, and illuminates their interdependencies. To address this critical gap, this survey provides the first comprehensive and systematic review of Context Engineering for LLMs. Our primary contribution is novel, structured taxonomy that classifies the multifaceted techniques used to design, manage, and optimize context. This taxonomy organizes the field into coherent categories, distinguishing between foundational Components and their integration into sophisticated System Implementations. Through this framework, we: (1) provide clear and structured overview of the state-of-the-art across each domain; (2) analyze the core mechanisms, strengths, and limitations of different approaches; and (3) identify overarching challenges and chart promising directions for future research. This work serves as both technical roadmap for navigating the complex landscape of context engineering and foundation for fostering deeper understanding and catalyzing future innovation. The remainder of this paper is organized as follows. After discussing related work and formally defining Context Engineering, we first examine the Foundational Components of the field, covering Context Retrieval and Generation, Context Processing, and Context Management. We then explore their System Implementations, including Retrieval-Augmented Generation, Memory Systems, Tool-Integrated Reasoning, and Multi-Agent Systems. Finally, we discuss evaluation methodologies, future research directions, and conclude the survey. Figure 1 provides comprehensive overview of our taxonomy, illustrating the hierarchical organization of techniques and their relationships within the Context Engineering landscape. 2. Related Work The rapid maturation of LLMs has spurred significant body of survey literature aiming to map its multifaceted landscape. This existing work, while valuable, has largely focused on specific vertical domains within the broader field of what we define as Context Engineering. Our survey seeks to complement these efforts by providing horizontal, unifying taxonomy that distinguishes between foundational components and their integration into complex systems, thereby bridging these specialized areas. Foundational Components Numerous surveys have addressed the foundational Components of context engineering that form the core technical capabilities for effective context manipulation. The challenge of Context Retrieval and Generation encompasses both prompt engineering methodologies and external knowledge acquisition techniques. Surveys on prompt engineering have cataloged the vast array of techniques for guiding LLM behavior, from basic few-shot methods to advanced, structured reasoning frameworks [25, 253, 1313]. External knowledge retrieval and integration techniques, particularly through knowledge graphs and structured data sources, are reviewed in works that survey representation techniques, integration i n E t l i n ) 4 ( e m ) 5 ( i n l I ) 6 ( t a n c D t ) 7 ( n a & Context Generation Retrieval & (4.1) e.g., Chain-of-Thought [1138], Zero-shot CoT [553], ToT [1246], GoT [69], Self-consistency [1114], ReAct [1245], Auto-CoT [1099], Automatic Prompt [307] , CLEAR Framework [702], RAG [591], Cognitive Prompting [558], KAPING [48], Dynamic Assembly [307], etc. Context Processing (4.2) e.g., Mamba [1258], LongNet [216], FlashAttention [196], Ring Attention [676], YaRN [833], Infini-attention [792], StreamingLLM [1176], Inf LLM [1175], Self-Refine [735], Reflexion [956], StructGPT [489], GraphFormers [1221], KG Integration [1321], Long CoT [147], MLLMs [49], etc. Context Management (4.3) e.g., Context Compression [317], StreamingLLM [1176], KV Cache Management [1389], Heavy Hitter Oracle [1333], Hierarchical Memory [499], Recurrent Context Compression [441], Activation Refilling [859], Context Window Management [1074], etc. Retrieval-Augmented Generation (5.1) e.g., FlashRAG [500], KRAGEN [749], ComposeRAG [1159], Self-RAG [41], CDF-RAG [531], GraphRAG [374], LightRAG [360], HippoRAG [366], RAPTOR [928], RAG-Gym [1183], Agentic RAG Systems [965], Graph-Enhanced RAG [832], Modular RAG Architectures [312], etc. Memory Systems (5.2) e.g., MemoryBank [1362], MemLLM [779], Self-Controlled Memory [649], REMEMBERER [1299], MemOS [637], Charlie Mnemonic [578], RecMind [1115], Sandbox [455], LongMemEval [1171], MADail-Bench [386], MEMENTO [566], A-MEM [1202], CAMELoT [393], Architectures [1182], Short-term & Long-term Memory [935], MemGPT [813], Memory-Enhanced Agents [571], etc. Tool-Integrated Reasoning (5.3) e.g., Toolformer [931], ReAct [1245], Gorilla [828], ToolLLM [867], Granite-FunctionCalling [5], Program-Aided Language Models [305], ToRA [341], ReTool [270], Chameleon [709], a1 [760], API-Bank [615], MCP-RADAR [310], GTA benchmark [1090], PLAY2PROMPT [259], etc. Multi-Agent Systems (5.4) e.g., KQML [280], FIPA ACL [1146], MCP protocols [37], A2A [1007], ACP [462], ANP [1], AutoGen [1158], MetaGPT [408], CAMEL [600], CrewAI [184], Swarm Agent [808], 3S orchestrator [893], SagaLLM [128], Communication Protocols [1210], Orchestration [894], Coordination Strategies [625], Agent Communication Languages [356], CoA [1327], etc. Evaluation Frameworks (6.1) e.g., Component-Level Assessment [835], System-Level Integration [1132], Self-Refinement [735], MCP-RADAR [310], LongMemEval [1171], BFCL Tool Evaluation [829], SagaLLM [128], Brittleness Assessment [1259], Contextual Calibration [380], Multi-dimensional Feedback [284], etc. Benchmark Datasets (6.2) e.g., GAIA [772], GTA [1090], WebArena [1368], VideoWebArena [476], Deep Research Bench [87], StableToolBench [359], NesTools [373], ToolHop [1255], T-Eval [157], BFCL [829], NarrativeQA [550], MEMENTO [566], API-Bank [615], Mind2Web [202], SWE-Bench [494], etc. Evaluation Challenges (6.3) Foundational Research (7.1) Technical Innovation (7.2) Application-Driven Research (7.3) e.g., Performance Gap Assessment [772, 1090], Memory System Isolation Problems [1330, 1171], O(n2) Scaling Limitations [731, 295], Transactional Integrity [128], Multi-Tool Coordination [310], Self-Validation Dependencies [390], Context Handling Failures [210], Attribution Challenges [1113], Safety-oriented Evaluation [87], Agent Assessment [965], Orchestration Evaluation [893], etc. e.g., Theoretical Foundations [1132], Scaling Laws [731], O(n2) Computational Challenges [295], Multi-modal Integration [476], Compositional Understanding [835], Context Optimization [663], Frameworks for Multi-agent Coordination [128], Information-theoretic Analysis [310], etc. e.g., LongMamba [1258], Sliding Attention [295], Memory-Augmented Architectures [1362], Modular RAG [312], GraphRAG [374], Context Assembly Optimization [1132], Tool-Integrated Reasoning [310], Agentic Systems [965],Self-Refinement Mechanisms [735], etc. e.g., Domain Specialization [87], Healthcare Applications [386], Protocol Standardization [246], MCP/A2A/ACP/ANP Protocols [616], Human-AI Collaboration [1368], Security Issues [926], Production Deployment Scalability [1227], Safety [965] and Ethical Considerations [835], etc. Figure 1: The taxonomy of Context Engineering in Large Language Models is categorized into foundational components, system implementations, evaluation methodologies, and future directions. Each area encompasses specific techniques and frameworks that collectively advance the systematic optimization of information payloads for LLMs. paradigms, and applications in enhancing the factual grounding of LLMs [483, 428, 817, 889]. The domain of Context Processing addresses the technical challenges of handling long sequences, self-refinement mechanisms, and structured information integration. Long context processing is addressed in surveys analyzing techniques for extending context windows, optimizing attention mechanisms, and managing memory efficiently [831, 645, 1289, 268]. The internal cognitive processes of LLMs are increasingly 6 surveyed, with works on self-contextualizing techniques and self-improvement paradigms gaining prominence [1329, 227, 1167, 935]. Finally, Context Management literature focuses on memory hierarchies, compression techniques, and optimization strategies that enable effective information organization and retrieval within computational constraints. While comprehensive surveys specifically dedicated to context management as unified domain remain limited, related work on memory systems and context compression techniques provides foundational insights into these critical capabilities. System Implementation In parallel, the literature has extensively covered the System Implementations that integrate foundational components into sophisticated architectures addressing real-world application requirements. The domain of RAG has received substantial attention, with foundational surveys tracing its development and impact on mitigating hallucinations [311, 253, 1131]. More recent work has surveyed the evolution towards modular, agentic, and graph-enhanced RAG architectures [162, 622, 120, 312, 1391]. Memory Systems that enable persistent interactions and cognitive architectures have been explored through surveys focusing on memory-enhanced agents and their applications. The broader category of LLM-based Agents serves as foundational area, with comprehensive overviews of autonomous agents, their architecture, planning, and methodologies [1091, 719, 277, 843, 1340, 498, 1272]. Tool-Integrated Reasoning encompassing function calling mechanisms and agent-environment interaction are well-documented, exploring the evolution from single-tool systems to complex orchestration frameworks [663, 858, 771, 867]. The evolution towards Multi-Agent Systems (MAS) represents another focal point, with surveys detailing MAS workflows, infrastructure, communication protocols, and coordination mechanisms [625, 356, 246, 1235, 38, 503, 187, 458]. Evaluation The critical aspect of evaluating these complex systems has been thoroughly reviewed, with works analyzing benchmarks and methodologies for assessing component-level and system-level capabilities and performance [1259, 380, 835, 310]. This evaluation literature spans both foundational component assessment and integrated system evaluation paradigms. Our Contribution While these surveys provide indispensable, in-depth analyses of their respective domains, they inherently present fragmented view of the field. The connections between RAG as form of external memory, tool use as method for context acquisition, and prompt engineering as the language for orchestrating these components are often left implicit. Our work distinguishes itself by proposing Context Engineering as unifying abstraction that explicitly separates foundational components from their integration in complex implementations. By organizing these disparate fields into single, coherent taxonomy, this survey aims to elucidate the fundamental relationships between them, providing holistic map of how context is generated, processed, managed, and utilized to steer the next generation of intelligent systems. 3. Why Context Engineering? As Large Language Models (LLMs) evolve from simple instruction-following systems into the core reasoning engines of complex, multi-faceted applications, the methods used to interact with them must also evolve. The term prompt engineering, while foundational, is no longer sufficient to capture the full scope of designing, managing, and optimizing the information payloads required by modern AI systems. These systems do not Figure 2: Context Engineering Evolution Timeline: comprehensive visualization of the development trajectory of Context Engineering implementations from 2020 to 2025, showing the evolution from foundational RAG systems to sophisticated multi-agent architectures and tool-integrated reasoning systems. operate on single, static string of text; they leverage dynamic, structured, and multifaceted information stream. To address this, we introduce and formalize the discipline of Context Engineering. 3.1. Definition of Context Engineering To formally define Context Engineering, we begin with the standard probabilistic model of an autoregressive LLM. The model, parameterized by θ, generates an output sequence = (y1, . . . , yT) given an input context by maximizing the conditional probability: Pθ(YC) = Pθ(yty<t, C) (1) t=1 Historically, in the paradigm of prompt engineering, the context was treated as monolithic, static string of text, i.e., = prompt. This view is insufficient for modern systems. Context Engineering re-conceptualizes the context as dynamically structured set of informational components, c1, c2, . . . , cn. These components are sourced, filtered, and formatted by set of functions, and finally orchestrated by high-level assembly function, A: The components ci are not arbitrary; they map directly to the core technical domains of this survey: = A(c1, c2, . . . , cn) (2) 8 cinstr: System instructions and rules (Context Retrieval and Generation, Sec. 4.1). cknow: External knowledge, retrieved via functions like RAG or from integrated knowledge graphs (RAG, Sec. 5.1; Context Processing, Sec. 4.2). ctools: Definitions and signatures of available external tools (Function Calling & Tool-Integrated Reasoning, Sec. 5.3). cmem: Persistent information from prior interactions (Memory Systems, Sec. 5.2; Context Management, Sec. 4.3). cstate: The dynamic state of the user, world, or multi-agent system (Multi-Agent Systems & Orchestration, Sec. 5.4). cquery: The users immediate request. The Optimization Problem of Context Engineering. From this perspective, Context Engineering is the formal optimization problem of finding the ideal set of context-generating functions (which we denote collectively as = {A, Retrieve, Select, . . . }) that maximizes the expected quality of the LLMs output. Given distribution of tasks , the objective is: = arg max τT [Reward(Pθ(YCF (τ)), τ )] (3) where τ is specific task instance, CF (τ) is the context generated by the functions in for that task, and is the ground-truth or ideal output. This optimization is subject to hard constraints, most notably the τ models context length limit, Lmax. Mathematical Principles and Theoretical Frameworks. This formalization reveals deeper mathematical principles. The assembly function is form of Dynamic Context Orchestration, pipeline of formatting 1, . . . , Formatn), where each function must be optimized and concatenation operations, = Concat (Format for the LLMs architectural biases (e.g., attention patterns). The retrieval of knowledge, cknow = Retrieve(. . . ), can be framed as an Information-Theoretic Optimality problem. The goal is to select knowledge that maximizes the mutual information with the target answer Y, given the query cquery: Retrieve = arg max Retrieve This ensures that the retrieved context is not just semantically similar, but maximally informative for solving the task. I(Y; cknowcquery) (4) Furthermore, the entire process can be viewed through the lens of Bayesian Context Inference. Instead of deterministically constructing the context, we infer the optimal context posterior P(Ccquery, History, World). Using Bayes theorem, this posterior is proportional to the likelihood of the query given the context and the prior probability of the contexts relevance: P(Ccquery, . . . ) P(cqueryC) P(CHistory, World) The decision-theoretic objective is then to find the context that maximizes the expected reward over the distribution of possible answers: (5) = arg max P(YC, cquery) Reward(Y, Y) dY P(Ccquery, . . . ) (6) This Bayesian formulation provides principled way to handle uncertainty, perform adaptive retrieval by updating priors, and maintain belief states over context in multi-step reasoning tasks. 9 Dimension Model Target Complexity Information State Prompt Engineering = prompt (static string) arg maxprompt Pθ(Yprompt) Manual or automated search over string space. Information content is fixed within the prompt. Primarily stateless. Context Engineering = A(c1, c2, . . . , cn) (dynamic, structured assembly) = arg maxF System-level optimization of = {A, Retrieve, Select, . . . }. Aims to maximize task-relevant information under constraint Lmax. Inherently stateful, with explicit components for cmem and cstate. τT [Reward(Pθ(YCF (τ)), τ )] Scalability Error Analysis Manual inspection and iterative refinement. Brittleness increases with length and complexity. Manages complexity through modular composition. Systematic evaluation and debugging of individual context functions. Table 1: Comparison of Prompt Engineering and Context Engineering Paradigms. Comparison of Paradigms The formalization of Context Engineering highlights its fundamental distinctions from traditional prompt engineering. The following table summarizes the key differences. In summary, Context Engineering provides the formal, systematic framework required to build, understand, and optimize the sophisticated, context-aware AI systems that are coming to define the future of the field. It shifts the focus from the art of prompt design to the science of information logistics and system optimization. Context Scaling Context scaling encompasses two fundamental dimensions that collectively define the scope and sophistication of contextual information processing. The first dimension, length scaling, addresses the computational and architectural challenges of processing ultra-long sequences, extending context windows from thousands to millions of tokens while maintaining coherent understanding across extended narratives, documents, and interactions. This involves sophisticated attention mechanisms, memory management techniques, and architectural innovations that enable models to maintain contextual coherence over vastly extended input sequences. The second, equally critical dimension is multi-modal and structural scaling, which expands context beyond simple text to encompass multi-dimensional, dynamic, cross-modal information structures. This includes temporal context (understanding time-dependent relationships and sequences), spatial context (interpreting location-based and geometric relationships), participant states (tracking multiple entities and their evolving conditions), intentional context (understanding goals, motivations, and implicit objectives), and cultural context (interpreting communication within specific social and cultural frameworks). Modern context engineering must address both dimensions simultaneously, as real-world applications require models to process not only lengthy textual information but also diverse data types including structured knowledge graphs, multimodal inputs (text, images, audio, video), temporal sequences, and implicit contextual cues that humans naturally understand. This multi-dimensional approach to context scaling represents fundamental shift from parameter scaling toward developing systems capable of understanding complex, ambiguous contexts that mirror the nuanced nature of human intelligence in facing complex world [1036]. 10 3.2. Why Context Engineering 3.2.1. Current Limitations Large Language Models face critical technical barriers necessitating sophisticated context engineering approaches. The self-attention mechanism imposes quadratic computational and memory overhead as sequence length increases, creating substantial obstacles to processing extended contexts and significantly impacting real-world applications such as chatbots and code comprehension models [1017, 977]. Commercial deployment compounds these challenges through repeated context processing that introduces additional latency and token-based pricing costs [1017]. Beyond computational constraints, LLMs demonstrate concerning reliability issues including frequent hallucinations, unfaithfulness to input context, problematic sensitivity to input variations, and responses that appear syntactically correct while lacking semantic depth or coherence [951, 1279, 523]. The prompt engineering process presents methodological challenges through approximation-driven and subjective approaches that focus narrowly on task-specific optimization while neglecting individual LLM behavior [800]. Despite these challenges, prompt engineering remains critical for effective LLM utilization through precise and contextually rich prompts that reduce ambiguity and enhance response consistency [964]. 3.2.2. Performance Enhancement Context engineering delivers substantial performance improvements through techniques like retrievalaugmented generation and superposition prompting, achieving documented improvements including 18-fold enhancement in text navigation accuracy, 94% success rates, and significant gains from careful prompt construction and automatic optimization across specialized domains [267, 768, 681]. Structured prompting techniques, particularly chain-of-thought approaches, enable complex reasoning through intermediate steps while enhancing element-aware summarization capabilities that integrate finegrained details from source documents [1138, 750, 1120]. Few-shot learning implementations through carefully selected demonstration examples yield substantial performance gains, including 9.90% improvements in BLEU-4 scores for code summarization and 175.96% in exact match metrics for bug fixing [306]. Domain-specific context engineering proves especially valuable in specialized applications, with executionaware debugging frameworks achieving up to 9.8% performance improvements on code generation benchmarks and hardware design applications benefiting from specialized testbench generation and security property verification [1360, 873, 44]. These targeted approaches bridge the gap between general-purpose model training and specialized domain requirements. 3.2.3. Resource Optimization Context engineering provides efficient alternatives to resource-intensive traditional approaches by enabling intelligent content filtering and direct knowledge transmission through carefully crafted prompts [630, 670]. LLMs can generate expected responses even when relevant information is deleted from input context, leveraging contextual clues and prior knowledge to optimize context length usage while maintaining response quality, particularly valuable in domains with significant data acquisition challenges [630, 670]. Specialized optimization techniques further enhance efficiency gains through context awareness and responsibility tuning that significantly reduce token consumption, dynamic context optimization employing 11 precise token-level content selection, and attention steering mechanisms for long-context inference [426, 944, 350]. These approaches maximize information density while reducing processing overhead and maintaining performance quality [944, 350]. 3.2.4. Future Potential Context engineering enables flexible adaptation mechanisms through in-context learning that allows models to adapt to new tasks without explicit retraining, with context window size directly influencing available examples for task adaptation [617]. Advanced techniques integrate compression and selection mechanisms for efficient model editing while maintaining contextual coherence [619]. This adaptability proves especially valuable in low-resource scenarios, enabling effective utilization across various prompt engineering techniques including zero-shot approaches, few-shot examples, and role context without requiring domain-specific fine-tuning [924, 129, 1075]. Sophisticated context engineering techniques including in-context learning, chain-of-thought, tree-ofthought, and planning approaches establish foundations for nuanced language understanding and generation capabilities while optimizing retrieval and generation processes for robust, context-aware AI applications [797, 974]. Future research directions indicate substantial potential for advancing context-sensitive applications through chain-of-thought augmentation with logit contrast mechanisms [953], better leveraging different context types across domains, particularly in code intelligence tasks combining syntax, semantics, execution flow, and documentation [1094], and understanding optimal context utilization strategies as advanced language models continue demonstrating prompt engineerings persistent value [1079]. Evolution toward sophisticated filtering and selection mechanisms represents critical pathway for addressing transformer architectures scaling limitations while maintaining performance quality. 4. Foundational Components Context Engineering is built upon three fundamental components that collectively address the core challenges of information management in large language models: Context Retrieval and Generation sources appropriate contextual information through prompt engineering, external knowledge retrieval, and dynamic context assembly; Context Processing transforms and optimizes acquired information through long sequence processing, self-refinement mechanisms, and structured data integration; and Context Management tackles efficient organization and utilization of contextual information through addressing fundamental constraints, implementing sophisticated memory hierarchies, and developing compression techniques. These foundational components establish the theoretical and practical basis for all context engineering implementations, forming comprehensive framework where each component addresses distinct aspects of the context engineering pipeline while maintaining synergistic relationships that enable comprehensive contextual optimization and effective context engineering strategies. 4.1. Context Retrieval and Generation Context Retrieval and Generation forms the foundational layer of context engineering, encompassing the systematic retrieval and construction of relevant information for LLMs. This component addresses the critical challenge of sourcing appropriate contextual information through three primary mechanisms: prompt-based generation that crafts effective instructions and reasoning frameworks, external knowledge retrieval that 12 Figure 3: Context Engineering Framework: comprehensive taxonomy of Context Engineering components including Context Retrieval and Generation, Context Processing, and Context Management, integrated into System Implementations such as RAG systems, memory architectures, tool-integrated reasoning, and multi-agent coordination mechanisms. accesses dynamic information sources, and dynamic context assembly that orchestrates acquired components into coherent, task-optimized contexts. 4.1.1. Prompt Engineering and Context Generation Prompt engineering and context generation forms the foundational layer of context retrieval, encompassing strategic input design that combines art and science to craft effective instructions for LLMs. The CLEAR Frameworkconciseness, logic, explicitness, adaptability, and reflectivenessgoverns effective prompt construction, while core architecture integrates task instructions, contextual information, input data, and output indicators [702, 1133, 569, 209, 25]. Zero-Shot and Few-Shot Learning Paradigms Zero-shot prompting enables task performance without prior examples, relying exclusively on instruction clarity and pre-trained knowledge [1361, 336, 553, 67, 1046]. Few-shot prompting extends this capability by incorporating limited exemplars to guide model responses, demonstrating task execution through strategic example selection [1361, 401, 103, 546, 788, 1371]. Incontext learning facilitates adaptation to novel tasks without parameter updates by leveraging demonstration examples within prompts, with performance significantly influenced by example selection and ordering strategies [365, 103, 1287, 1016, 920, 846, 1139, 348, 576]. Chain-of-Thought Foundations Chain-of-Thought (CoT) prompting decomposes complex problems into intermediate reasoning steps, mirroring human cognition [1138, 401, 336, 939, 603]. Zero-shot CoT uses trigger phrases like Lets think step by step, improving MultiArith accuracy from 17.7% to 78.7% [553, 1099, 472, 662], with Automatic Prompt Engineer refinements yielding additional gains [1215, 526]. Tree-of-Thoughts (ToT) organizes reasoning as hierarchical structures with exploration, lookahead, and backtracking capabilities, increasing Game of 24 success rates from 4% to 74% [1246, 217, 557, 598]. Graph-of-Thoughts (GoT) models reasoning as arbitrary graphs with thoughts as vertices and dependencies as edges, improving quality by 62% and reducing costs by 31% compared to ToT [69, 826, 1366]. 13 Cognitive Architecture Integration Cognitive prompting implements structured human-like operations including goal clarification, decomposition, filtering, abstraction, and pattern recognition, enabling systematic multi-step task resolution through deterministic, self-adaptive, and hybrid variants [558, 557, 1205, 1164]. Guilfords Structure of Intellect model provides psychological foundations for categorizing cognitive operations such as pattern recognition, memory retrieval, and evaluation, enhancing reasoning clarity, coherence, and adaptability [556, 191]. Advanced implementations incorporate cognitive tools as modular reasoning operations, with GPT-4.1 performance on AIME2024 increasing from 26.7% to 43.3% through structured cognitive operation sequences [243, 1030]. Method Self-Refine [735, 916] Description Enables LLMs to improve outputs through iterative feedback and refinement cycles using the same model as the generator, feedback provider, and refiner, without supervised training. Multi-Aspect Feedback [799] Integrates multiple feedback modules (frozen LMs and external tools), each focusing on specific error categories to enable more comprehensive, independent evaluation. N-CRITICS [789] ISR-LLM [1373] SELF [704] ProMiSe [884] A2R [577] Implements an ensemble of critics that evaluate an initial output. Compiled feedback from the generating LLM and other models guides refinement until stopping criterion is met. Improves LLM-based planning by translating natural language to formal specifications, creating an initial plan, and then systematically refining it with validator. Teaches LLMs meta-skills (self-feedback, self-refinement) with limited examples, then has the model continuously selfevolve by generating and filtering its own training data. Addresses self-refinement in smaller LMs using principle-guided iterative refinement, combining proxy metric thresholds with few-shot refinement and rejection sampling. Augments LLMs through Metric-based Iterative Feedback Learning, using explicit evaluation across multiple dimensions (e.g., correctness) to generate feedback and refine outputs. Experience Refinement [857] Enables LLM agents to refine experiences during task execution by learning from recent (successive) or all previous (cumulative) experiences, prioritizing high-quality ones. I-SHEEP [654] CaP [1271] Agent-R [1277] GenDiE [610] Allows LLMs to continuously self-align from scratch by generating, assessing, filtering, and training on high-quality synthetic datasets without external guidance. Uses external tools to refine chain-of-thought (CoT) responses, addressing the limitation of models that get stuck in non-correcting reasoning loops. Enables language agents to reflect on the fly through iterative self-training, using Monte Carlo Tree Search (MCTS) to construct training data that corrects erroneous paths. Enhances context faithfulness with sentence-level optimization, combining generative and discriminative training to give LLMs self-generation and self-scoring capabilities. Self-Developing [466] Enables LLMs to autonomously discover, implement, and refine their own improvement algorithms by generating them as code, evaluating them, and using DPO to recursively improve. SR-NLE [1121] Improves the faithfulness of post-hoc natural language explanations via an iterative critique and refinement process using self-feedback and feature attribution. Table 2: Self-refinement methods in large language models and their key characteristics. 4.1.2. External Knowledge Retrieval External knowledge retrieval represents critical component of context retrieval, addressing fundamental limitations of parametric knowledge through dynamic access to external information sources including databases, knowledge graphs, and document collections. Retrieval-Augmented Generation Fundamentals RAG combines parametric knowledge stored in model parameters with non-parametric information retrieved from external sources, enabling access to current, domain-specific knowledge while maintaining parameter efficiency [591, 311, 253]. FlashRAG provides comprehensive evaluation and modular implementation of RAG systems, while frameworks like KRAGEN 14 and ComposeRAG demonstrate advanced retrieval strategies with substantial performance improvements across diverse benchmarks [500, 749, 1159]. Self-RAG introduces adaptive retrieval mechanisms where models dynamically decide when to retrieve information and generate special tokens to control retrieval timing and quality assessment [41]. Advanced implementations include RAPTOR for hierarchical document processing, HippoRAG for memory-inspired retrieval architectures, and Graph-Enhanced RAG systems that leverage structured knowledge representations for improved information access [928, 366, 360]. Knowledge Graph Integration and Structured Retrieval Knowledge graph integration addresses structured information retrieval through frameworks like KAPING, which retrieves relevant facts based on semantic similarities and prepends them to prompts without requiring model training [48, 673]. KARPA provides training-free knowledge graph adaptation through pre-planning, semantic matching, and relation path reasoning, achieving state-of-the-art performance on knowledge graph question answering tasks [258]. Think-on-Graph enables sequential reasoning over knowledge graphs to locate relevant triples, conducting exploration to retrieve related information from external databases while generating multiple reasoning pathways [1000, 720]. StructGPT implements iterative reading-then-reasoning approaches that construct specialized functions to collect relevant evidence from structured data sources [489]. Agentic and Modular Retrieval Systems Agentic RAG systems treat retrieval as dynamic operations where agents function as intelligent investigators analyzing content and cross-referencing information [648, 162, 965]. These systems incorporate sophisticated planning and reflection mechanisms requiring integration of task decomposition, multi-plan selection, and iterative refinement capabilities [438, 1183]. Modular RAG architectures enable flexible composition of retrieval components through standardized interfaces and plug-and-play designs. Graph-Enhanced RAG systems leverage structured knowledge representations for improved information access, while Real-time RAG implementations address dynamic information requirements in streaming applications [312, 1391]. 4.1.3. Dynamic Context Assembly Dynamic context assembly represents the sophisticated orchestration of acquired information components into coherent, task-optimized contexts that maximize language model performance while respecting computational constraints. Assembly Functions and Orchestration Mechanisms The assembly function encompasses templatebased formatting, priority-based selection, and adaptive composition strategies that must adapt to varying task requirements, model capabilities, and resource constraints [702, 1133, 569]. Contemporary orchestration mechanisms manage agent selection, context distribution, and interaction flow control in multi-agent systems, enabling effective cooperation through user input processing, contextual distribution, and optimal agent selection based on capability assessment [894, 53, 171]. Advanced orchestration frameworks incorporate intent recognition, contextual memory maintenance, and task dispatching components for intelligent coordination across domain-specific agents. The Swarm Agent framework utilizes real-time outputs to direct tool invocations while addressing limitations in static tool registries and bespoke communication frameworks [808, 263, 246]. 15 Multi-Component Integration Strategies Context assembly must address cross-modal integration challenges, incorporating diverse data types including text, structured knowledge, temporal sequences, and external tool interfaces while maintaining coherent semantic relationships [529, 1221, 496]. Verbalization techniques convert structured data including knowledge graph triples, table rows, and database records into natural language sentences, enabling seamless integration with existing language systems without architectural modifications [12, 782, 1064, 13]. Programming language representations of structured data, particularly Python implementations for knowledge graphs and SQL for databases, outperform traditional natural language representations in complex reasoning tasks by leveraging inherent structural properties [1166]. Multi-level structurization approaches reorganize input text into layered structures based on linguistic relationships, while structured data representations leverage existing LLMs to extract structured information and represent key elements as graphs, tables, or relational schemas [681, 1125, 1324]. Automated Assembly Optimization Automated prompt engineering addresses manual optimization limitations through systematic prompt generation and refinement algorithms. Automatic Prompt Engineer (APE) employs search algorithms for optimal prompt discovery, while LM-BFF introduces automated pipelines combining prompt-based fine-tuning with dynamic demonstration incorporation, achieving up to 30% absolute improvement across NLP tasks [307, 417, 590]. Promptbreeder implements self-referential evolutionary systems where LLMs improve both task-prompts and mutation-prompts governing these improvements through natural selection analogies [275, 508]. Self-refine enables iterative output improvement through self-critique and revision across multiple iterations, with GPT-4 achieving approximately 20% absolute performance improvement through this methodology [735, 670]. Multi-agent collaborative frameworks simulate specialized team dynamics with agents assuming distinct roles (analysts, coders, testers), resulting in 29.9-47.1% relative improvement in Pass@1 metrics compared to single-agent approaches [434, 1257]. Tool integration frameworks combine Chain-of-Thought reasoning with external tool execution, automating intermediate reasoning step generation as executable programs strategically incorporating external data. LangChain provides comprehensive framework support for sequential processing chains, agent development, and web browsing capabilities, while specialized frameworks like Auto-GPT and Microsofts AutoGen facilitate complex AI agent development through user-friendly interfaces [963, 1087, 25, 867]. 4.2. Context Processing Context Processing focuses on transforming and optimizing acquired contextual information to maximize its utility for LLMs. This component addresses challenges in handling ultra-long sequence contexts, enables iterative self-refinement and adaptation mechanisms, and facilitates integration of multimodal, relational and structured information into coherent contextual representations. 4.2.1. Long Context Processing Ultra-long sequence context processing addresses fundamental computational challenges arising from transformer self-attentions O(n2) complexity, which creates significant bottlenecks as sequence lengths increase and substantially impacts real-world applications [1059, 731, 295, 268, 416]. Increasing Mistral-7B input from 4K to 128K tokens requires 122-fold computational increase, while memory constraints during prefilling and decoding stages create substantial resource demands, with Llama 3.1 8B requiring up to 16GB per 128K-token request [1032, 1227, 425]. Architectural Innovations for Long Context State Space Models (SSMs) maintain linear computational complexity and constant memory requirements through fixed-size hidden states, with models like Mamba offering efficient recurrent computation mechanisms that scale more effectively than traditional transformers [1258, 347, 346]. Dilated attention approaches like LongNet employ exponentially expanding attentive fields as token distance grows, achieving linear computational complexity while maintaining logarithmic dependency between tokens, enabling processing of sequences exceeding one billion tokens [216]. Toeplitz Neural Networks (TNNs) model sequences with relative position encoded Toeplitz matrices, reducing space-time complexity to log-linear and enabling extrapolation from 512 training tokens to 14,000 inference tokens [868, 869]. Linear attention mechanisms reduce complexity from O(N2) to O(N) by expressing self-attention as linear dot-products of kernel feature maps, achieving up to 4000 speedup when processing very long sequences [522]. Alternative approaches like non-attention LLMs break quadratic barriers by employing recursive memory transformers and other architectural innovations [547]. Position Interpolation and Context Extension Position interpolation techniques enable models to process sequences beyond original context window limitations by intelligently rescaling position indices rather than extrapolating to unseen positions [150]. Neural Tangent Kernel (NTK) approaches provide mathematically grounded frameworks for context extension, with YaRN combining NTK interpolation with linear interpolation and attention distribution correction [833, 471, 1021]. LongRoPE achieves 2048K token context windows through two-stage approaches: first fine-tuning models to 256K length, then conducting positional interpolation to reach maximum context length [218]. Position Sequence Tuning (PoSE) demonstrates impressive sequence length extensions up to 128K tokens by combining multiple positional interpolation strategies [1377]. Self-Extend techniques enable LLMs to process long contexts without fine-tuning by employing bi-level attention strategiesgrouped attention and neighbor attentionto capture dependencies among distant and adjacent tokens [499]. Optimization Techniques for Efficient Processing Grouped-Query Attention (GQA) partitions query heads into groups that share key and value heads, striking balance between multi-query attention and multihead attention while reducing memory requirements during decoding [16, 1341]. FlashAttention exploits asymmetric GPU memory hierarchy to achieve linear memory scaling instead of quadratic requirements, with FlashAttention-2 providing approximately twice the speed through reduced non-matrix multiplication operations and optimized work distribution [196, 195]. Ring Attention with Blockwise Transformers enables handling extremely long sequences by distributing computation across multiple devices, leveraging blockwise computation while overlapping communication with attention computation [676]. Sparse attention techniques include Shifted sparse attention (S2-Attn) in LongLoRA and SinkLoRA with SF-Attn, which achieve 92% of full attention perplexity improvement with significant computation savings [1304, 1217]. Efficient Selective Attention (ESA) proposes token-level selection of critical information through query and key vector compression into lower-dimensional representations, enabling processing of sequences up to 256K tokens [1084]. BigBird combines local attention with global tokens that attend to entire sequences, 17 plus random connections, enabling efficient processing of sequences up to 8 longer than previously possible [1285]. Memory Management and Context Compression Memory management strategies include Rolling Buffer Cache techniques that maintain fixed attention spans, reducing cache memory usage by approximately 8 on 32K token sequences [1341]. StreamingLLM enables processing infinitely long sequences without fine-tuning by retaining critical attention sink tokens together with recent KV cache entries, demonstrating up to 22.2 speedup over sliding window recomputation with sequences up to 4 million tokens [1176]. Infini-attention incorporates compressive memory into vanilla attention, combining masked local attention with long-term linear attention in single Transformer blocks, enabling processing of infinitely long inputs with bounded memory and computation [792]. Heavy Hitter Oracle (H2O) presents efficient KV cache eviction policies based on observations that small token portions contribute most attention value, improving throughput by up to 29 while reducing latency by up to 1.9 [1333]. Context compression techniques like QwenLong-CPRS implement dynamic context optimization mechanisms enabling multi-granularity compression guided by natural language instructions [944]. Inf LLM stores distant contexts in additional memory units and employs efficient mechanisms to retrieve token-relevant units for attention computation, allowing models pre-trained on sequences of few thousand tokens to effectively process sequences up to 1,024K tokens [1175]. 4.2.2. Contextual Self-Refinement and Adaptation Self-refinement enables LLMs to improve outputs through cyclical feedback mechanisms mirroring human revision processes, leveraging self-evaluation through conversational self-interaction via prompt engineering distinct from reinforcement learning approaches [735, 916, 25, 1211]. Foundational Self-Refinement Frameworks The Self-Refine framework uses the same model as generator, feedback provider, and refiner, demonstrating that identifying and fixing errors is often easier than producing perfect initial solutions [735, 1313, 227]. Reflexion maintains reflective text in episodic memory buffers for future decision-making through linguistic feedback [956], while structured guidance proves essential as simplistic prompting often fails to enable reliable self-correction [672, 587]. Multi-Aspect Feedback integrates frozen language models and external tools focusing on specific error categories to enable more comprehensive, independent evaluation [799]. The N-CRITICS framework implements ensemble-based evaluation where initial outputs are assessed by both generating LLMs and other models, with compiled feedback guiding refinement until task-specific stopping criteria are fulfilled [789]. The A2R framework adopts explicit evaluation across multiple dimensions including correctness and citation quality, formulating natural language feedback for each aspect and iteratively refining outputs [577]. ISR-LLM improves LLM-based planning by translating natural language to formal specifications, creating an initial plan, and then systematically refining it with validator [1373]. Meta-Learning and Autonomous Evolution SELF teaches LLMs meta-skills (self-feedback, self-refinement) with limited examples, then has the model continuously self-evolve by generating and filtering its own training data [704]. Self-rewarding mechanisms enable models to improve autonomously through iterative 18 self-judgment, where single model adopts dual roles as performer and judge, maximizing rewards it assigns itself [1163, 1278]. The Creator framework extends this paradigm by enabling LLMs to create and use their own tools through four-module process encompassing creation, decision-making, execution, and recognition [946, 856]. The Self-Developing framework represents the most autonomous approach, enabling LLMs to discover, implement, and refine their own improvement algorithms through iterative cycles generating algorithmic candidates as executable code [466]. In-context learning fundamentally represents form of meta-learning where models learn optimization strategies during pre-training that generalize across diverse tasks, enabling rapid adaptation to novel challenges during inference [179, 1165]. Meta-in-context learning demonstrates that in-context learning abilities can be recursively improved through in-context learning itself, adaptively reshaping model priors over expected tasks and modifying in-context learning strategies [177]. Memory-Augmented Adaptation Frameworks Memory augmentation represents powerful approach for implementing meta-learning through frameworks like Memory of Amortized Contexts, which uses feature extraction and memory-augmentation to compress information from new documents into compact modulations stored in memory banks [1011]. Context-aware Meta-learned Loss Scaling addresses outdated knowledge challenges by meta-training small autoregressive models to dynamically reweight language modeling loss for each token during online fine-tuning [430]. Decision-Pretrained Transformers demonstrate how transformers can be trained to perform in-context reinforcement learning, solving previously unseen RL problems by generalizing beyond pretraining distribution [1013, 582]. Context-based meta-reinforcement learning methods enhance performance through direct supervision of context encoders, improving sample efficiency compared to end-to-end training approaches [1072]. Long Chain-of-Thought and Advanced Reasoning Long Chain-of-Thought has emerged as significant evolution characterized by substantially longer reasoning traces enabling thorough problem exploration, as implemented in advanced models including OpenAI-o1, DeepSeek-R1, QwQ, and Gemini 2.0 Flash Thinking [147, 718, 1214]. LongCoT effectiveness appears linked to context window capacity, with empirical evidence suggesting larger context windows often lead to stronger reasoning performance [1229]. Extended reasoning enables self-reflection and error correction mechanisms allowing models to identify and rectify mistakes during problem-solving processes [1334]. The effectiveness of increasing reasoning step length, even without adding new information, considerably enhances reasoning abilities across multiple datasets through test-time scaling [1345]. Optimization strategies address computational inefficiencies due to verbose reasoning traces through self-generated shorter reasoning paths via best-of-N sampling, adaptive reasoning modes including ZeroThinking and Less-Thinking approaches, and explicit compact CoT methods reducing token usage while maintaining reasoning quality [791, 1348, 697]. Auto Long-Short Reasoning enables dynamic adjustment of reasoning path length according to question complexity, helping models decide when longer chains are necessary [715]. 19 4.2.3. Multimodal Context Multimodal Large Language Models (MLLMs) extend context engineering beyond text by integrating diverse data modalities including vision, audio, and 3D environments into unified contextual representations. This expansion introduces new challenges in modality fusion, cross-modal reasoning, and long-context processing while enabling sophisticated applications that leverage rich multimodal contextual understanding. Multimodal Context Integration Foundational Techniques Multimodal MLLMs expand upon traditional LLMs by integrating data from diverse modalities like vision, audio, and 3D environments [105, 49, 957]. primary integration method converts visual inputs into discrete tokens concatenated with text tokens, conditioning the LLMs generative process on combined representation [1286]. This is often facilitated by Visual Prompt Generators (VPGs) trained on image-caption pairs to map visual features into the LLMs embedding space [607]. The dominant architectural paradigm connects specialized, external multimodal encoderssuch as CLIP for vision or CLAP for audioto the LLM backbone via alignment modules like Q-Former or simple MLPs [19, 86, 609, 1130], modular design that allows for independent encoder updates without retraining the entire model [618]. Advanced Integration Strategies More sophisticated approaches enable deeper modality fusion. Crossmodal attention mechanisms learn fine-grained dependencies between textual and visual tokens directly within the LLMs embedding space, enhancing semantic understanding for tasks like image editing [564, 901, 102]. To manage lengthy inputs, hierarchical designs process modalities in stages to ensure scalability [155], while the browse-and-concentrate paradigm fuses the contexts of multiple images before LLM ingestion to overcome the limitations of isolated processing [1134]. Some research bypasses the adaptation of text-only LLMs, opting for unified training paradigms that jointly pre-train models on multimodal data and text corpora from the start to mitigate alignment challenges [1381, 1224]. Other methods leverage text as universal semantic space, using LLM in-context learning to improve generalization across diverse modality combinations [1050]. For video, context integration techniques range from prompt tuning to adapter-based methods that transform video content into sequence for reasoning [1080]. The development of these models is often constrained by the need for vast, high-quality multimodal data and significant computational resources [1295, 609, 211]. Core Challenges in Multimodal Context Processing Modality Bias and Reasoning Deficiencies primary obstacle in MLLM development is modality bias, where models favor textual inputs, generating plausible but multimodally ungrounded responses by relying on learned linguistic patterns rather than integrated visual or auditory information [1358, 24, 315, 1325]. This issue is exacerbated by training methodologies; for instance, VPGs trained on simple image-captioning tasks learn to extract only salient features for captions, neglecting other visual details crucial for more complex, instruction-based tasks, which fundamentally limits deep multimodal understanding [607, 504]. Consequently, MLLMs frequently struggle with fine-grained spatial or temporal reasoning, such as precise object localization or understanding detailed event sequences in videos [1031, 957], particularly in complex domains like social media where interpreting the interplay of text and images to understand misinformation or sarcasm is difficult [505]. Effective multimodal reasoning requires not just comprehending each modality 20 but also inferring their combined holistic meaning [385]. Compounding these issues is our limited mechanistic understanding of MLLMs themselves; their internal workings are largely black box, hindering the development of better architectures [1274]. Advanced Contextual Capabilities and Future Directions In-Context and Long-Context Learning key capability of MLLMs is in-context learning, where models adapt to new tasks from multimodal examples in the prompt without weight updates [1397, 1398, 551]. Link-context learning (LCL) enhances this by providing demonstrations with explicit causal links, improving generalization [1012]. However, in-context learning is constrained by fixed context windows, as image tokens consume significant space, limiting many-shot learning [437]. Performance is also sensitive to input order and the relative importance of each modality varies by task [1020, 1197]. Processing long multimodal contexts, crucial for applications like video analysis, remains major research frontier [1086]. Innovations include adaptive hierarchical token compression for video [1119], variable visual position encoding (V2PE) [1381], specialized modules like ContextQFormer for conversational memory [589], and dynamic, queryaware frame selection for video [581]. MLLMs also show emergent communication efficiency over extended interactions, phenomenon still under investigation [436]. Emerging Applications The ability to process rich multimodal context is unlocking new applications. MLLMs are used for predictive reasoning, such as forecasting human activity from visual scenes [1382], and have demonstrated impressive perception and cognitive capabilities across various multimodal benchmarks [290]. In VQA, context is leveraged for more precise answers, for instance, by prompting the MLLM to generate its own descriptive text context of an image [1346] or by integrating external knowledge via RAG [993, 105]. Other applications include planning digital actions based on sensory inputs [605], enhancing surgical decision support through memory-augmented context comprehension [418], and enabling nuanced video understanding by integrating visual information with speech and audio cues [642, 1193, 7]. Researchers have also extended MLLMs to emerging modalities like tactile information, event data, and graph structures [1358, 1023, 1213]. The growing importance of these real-world use cases has spurred the development of comprehensive evaluation frameworks to assess contextual comprehension [1109]. These advancements enable applications previously impossible with text-only models, such as image captioning and sophisticated multimodal reasoning [1173, 677, 139]. 4.2.4. Relational and Structured Context Large language models face fundamental constraints processing relational and structured data including tables, databases, and knowledge graphs due to text-based input requirements and sequential architecture limitations [489, 47, 1136]. Linearization often fails to preserve complex relationships and structural properties, with performance degrading when information is dispersed throughout contexts [586, 585, 938]. Knowledge Graph Embeddings and Neural Integration Advanced encoding strategies address structural limitations through knowledge graph embeddings that transform entities and relationships into numerical vectors, enabling efficient processing within language model architectures [12, 1250, 930, 1194]. Graph neural networks capture complex relationships between entities, facilitating multi-hop reasoning across 21 knowledge graph structures through specialized architectures like GraphFormers that nest GNN components alongside transformer blocks [974, 404, 1221, 483]. GraphToken demonstrates substantial improvements by explicitly representing structural information, achieving up to 73 percentage points enhancement on graph reasoning tasks through parameter-efficient encoding functions [836]. Heterformer and other hybrid GNN-LM architectures perform contextualized text encoding and heterogeneous structure encoding in unified models, addressing the computational challenges of scaling these integrated systems [496, 465, 751]. Method ODA [1001] RAG-KG [1206] KARPA [258] Faithful Reasoning [720] Approach Performance Key Innovation Observation-driven agent framework 12.87% and 8.9% improvements Recursive observation with action-reflection Historical issue KG construction 77.6% MRR, 0.32 BLEU improvement Query parsing and sub-graph retrieval Training-free KG adaptation State-of-the-art KGQA performance Pre-planning relation paths Planning-retrieval-reasoning framework N/A LLM-KG synergy with relation paths Table 3: Knowledge graph integration methods for enhanced reasoning in large language models. Verbalization and Structured Data Representations Verbalization techniques convert structured data including knowledge graph triples, table rows, and database records into natural language sentences, enabling seamless integration with existing language systems without architectural modifications [12, 782, 1064, 13]. Multi-level structurization approaches reorganize input text into layered structures based on linguistic relationships, while structured data representations leverage existing LLMs to extract structured information and represent key elements as graphs, tables, or relational schemas [681, 1125, 1324, 1035, 602]. Programming language representations of structured data, particularly Python implementations for knowledge graphs and SQL for databases, outperform traditional natural language representations in complex reasoning tasks by leveraging inherent structural properties [1166]. Resource-efficient approaches using structured matrix representations offer promising directions for reducing parameter counts while maintaining performance on structured data tasks [343]. Integration Frameworks and Synergized Approaches The integration of knowledge graphs with language models follows distinct paradigms characterized by different implementation strategies and performance trade-offs [817, 1140]. Pre-training integration methods like K-BERT inject knowledge graph triples during training to internalize factual knowledge, while inference-time approaches enable real-time knowledge access without requiring complete model retraining [690, 1237, 712]. KG-enhanced LLMs incorporate structured knowledge to improve factual grounding through retrievalbased augmentation methods like KAPING, which retrieves relevant facts based on semantic similarities and prepends them to prompts without requiring model training [48, 673, 591]. More sophisticated implementations embed KG-derived representations directly into model latent spaces through adapter modules and cross-attention mechanisms, with Text2Graph mappers providing linking between input text and KG embedding spaces [132, 1066, 428]. Synergized approaches create unified systems where both technologies play equally important roles, addressing fundamental limitations through bidirectional reasoning driven by data and knowledge [817, 853, 1111]. GreaseLM facilitates deep interaction across all model layers, allowing language context representations to be grounded by structured world knowledge while linguistic nuances inform graph 22 representations [1321]. QA-GNN implements bidirectional attention mechanisms connecting questionanswering contexts and knowledge graphs through joint graph formation and mutual representation updates via graph-based message passing [1250, 974]. Applications and Performance Enhancement Structured data integration significantly enhances LLM capabilities across multiple dimensions, with knowledge graphs providing structured information that reduces hallucinations by grounding responses in verifiable facts and improving factual accuracy through clearly defined information sources [1002, 1342, 200, 565]. Knowledge graphs enhance reasoning capabilities by providing structured entity relationships that enable complex multi-hop reasoning and logical inferences, with their rich repository of hierarchical knowledge significantly improving precision and reliability of inferences [1166, 208, 1018]. Real-world applications demonstrate substantial improvements across specialized domains. Healthcare systems combine structured medical knowledge with contextual understanding through Retrieval-Augmented Generation frameworks to improve disease progression modeling and clinical decision-making [842, 583]. Scientific research platforms organize findings into structured knowledge supporting hypothesis generation and research gap identification, while business analytics systems balance rule-based precision with AI pattern recognition for more actionable insights [1326, 1062]. Question answering systems benefit from natural language interfaces over structured data sources, with integration creating more robust systems capable of handling multimodal queries and providing personalized responses that overcome static knowledge base limitations [1317, 1116, 914, 1206]. Research demonstrates that structured knowledge representations can improve summarization performance by 40% and 14% across public datasets compared to unstructured memory approaches, with Chain-of-Key strategies providing additional performance gains through dynamic structured memory updates [459]. Data Type Knowledge graphs Method K-LAMP [48] Pan et al. [817] StructLM [1392] Shao et al. [938] Tables, databases, KGs Knowledge graphs Tables, graphs, databases Integration Method Key Innovation Retrieval-based augmentation KAPING framework Task Scope Zero-shot QA Pre-training & inference integration Synergized LLMs + KGs Multi-domain reasoning Instruction tuning 1.1M example dataset 18 datasets, 8 SKG tasks Linearization methods Schema linking & syntax prediction Text-to-SQL tasks Table 4: Representative approaches for structured data integration in large language models. 4.3. Context Management Context Management addresses the efficient organization, storage, and utilization of contextual information within LLMs. This component tackles fundamental constraints imposed by finite context windows, develops sophisticated memory hierarchies and storage architectures, and implements compression techniques to maximize information density while maintaining accessibility and coherence. 4.3.1. Fundamental Constraints LLMs face fundamental constraints in context management stemming from finite context window sizes inherent in most architectures, which significantly reduce model efficacy on tasks requiring deep understanding of lengthy documents while imposing substantial computational demands that hinder applications requiring quick responses and high throughput [1074]. Although extending context windows enables models to handle entire documents and capture longer-range dependencies, traditional transformer architectures experience quadratic computational complexity growth as sequence length increases, making processing extremely long texts prohibitively expensive [999]. While innovative approaches like LongNet have reduced this complexity to linear, balancing window size and generalization capabilities remains challenging [999, 216]. Empirical evidence reveals the lost-in-the-middle phenomenon, where LLMs struggle to access information positioned in middle sections of long contexts, performing significantly better when relevant information appears at the beginning or end of inputs [128, 685, 648]. This positional bias severely impacts performance in extended chain-of-thought reasoning tasks where critical earlier results become susceptible to forgetting, with performance degrading drastically by as much as 73% compared to performance with no prior context [128, 1138, 377]. LLMs inherently process each interaction independently, lacking native mechanisms to maintain state across sequential exchanges and robust self-validation mechanisms, constraints stemming from fundamental limits identified in Gödels incompleteness theorems [128, 368]. This fundamental statelessness necessitates explicit management systems to maintain coherent operation sequences and ensure robust failure recovery mechanisms [128]. Context management faces opposing challenges of context window overflow, where models forget prior context due to exceeding window limits, and context collapse, where enlarged context windows or conversational memory cause models to fail in distinguishing between different conversational contexts [985]. Research demonstrates that claimed benefits of chain-of-thought prompting dont stem from genuine algorithmic learning but rather depend on problem-specific prompts, with benefits deteriorating as problem complexity increases [984]. The computational overhead of long-context processing creates additional challenges in managing key-value caches which grow substantially with input length, creating bottlenecks in both latency and accuracy, while multi-turn and longitudinal interaction challenges further complicate context management as limited effective context hinders longitudinal knowledge accumulation and token demands of many-shot prompts constrain space available for system and user inputs while slowing inference [911, 719, 389]. 4.3.2. Memory Hierarchies and Storage Architectures Modern LLM memory architectures employ sophisticated hierarchical designs organized into methodological approaches to overcome fixed context window limitations. OS-inspired hierarchical memory systems implement virtual memory management concepts, with MemGPT exemplifying this approach through systems that page information between limited context windows (main memory) and external storage, similar to traditional operating systems [813]. These architectures consist of main context containing system instructions, FIFO message queues, and writable scratchpads, alongside external context holding information accessible through explicit function calls, with memory management through function-calling capabilities enabling autonomous paging decisions [831]. PagedAttention, inspired by virtual memory and paging techniques in operating systems, manages key-value cache memory in LLMs [57]. Dynamic memory organizations implement innovative systems based on cognitive principles, with MemoryBank using Ebbinghaus Forgetting Curve theory to dynamically adjust memory strength according to time and significance [1202, 1362]. ReadAgent employs episode pagination to segment content, memory gisting to create concise representations, and interactive look-up for information retrieval [1202]. Compressorretriever architectures support life-long context management by using base model forward functions to compress and retrieve context, ensuring end-to-end differentiability [1236]. Architectural adaptations enhance model memory capabilities through internal modifications including augmented attention mechanisms, refined key-value cache mechanisms, and modified positional encodings 24 [160, 1352]. Knowledge-organization methods structure memory into interconnected semantic networks enabling adaptive management and flexible retrieval, while retrieval mechanism-oriented approaches integrate semantic retrieval with memory forgetting mechanisms [515, 1362, 444]. System configurations balance efficiency and scalability through organizational approaches where centralized systems coordinate tasks efficiently but struggle with scalability as topics increase, leading to context overflow, while decentralized systems reduce context overflow but increase response time due to inter-agent querying [396]. Hybrid approaches balance shared knowledge with specialized processing for semi-autonomous operation, addressing challenges in balancing computational efficiency with contextual fidelity while mitigating memory saturation where excessive storage of past interactions leads to retrieval inefficiencies [160, 396]. Context Manager Components provide fundamental capabilities for snapshot creation, restoration of intermediate generation states, and overall context window management for LLMs [757]. 4.3.3. Context Compression Context compression techniques enable LLMs to handle longer contexts efficiently by reducing computational and memory burden while preserving critical information. Autoencoder-based compression achieves significant context reduction through In-context Autoencoder (ICAE), which achieves 4 context compression by condensing long contexts into compact memory slots that LLMs can directly condition on, significantly enhancing models ability to handle extended contexts with improved latency and memory usage during inference [317]. Recurrent Context Compression (RCC) efficiently expands context window length within constrained storage space, addressing challenges of poor model responses when both instructions and context are compressed by implementing instruction reconstruction techniques [441]. Memory-augmented approaches enhance context management through kNN-based memory caches that store key-value pairs of past inputs for later lookup, improving language modeling capabilities through retrieval-based mechanisms [393]. Contrastive learning approaches enhance memory retrieval accuracy, while side networks address memory staleness without requiring LLM fine-tuning, and consolidated representation methods dynamically update past token representations, enabling arbitrarily large context windows without being limited by fixed memory slots [393]. Hierarchical caching systems implement sophisticated multi-layer approaches, with Activation Refilling (ACRE) employing Bi-layer KV Cache where layer-1 cache captures global information compactly and layer-2 cache provides detailed local information, dynamically refilling L1 cache with query-relevant entries from L2 cache to integrate broad understanding with specific details [859]. Infinite-LLM addresses dynamic context length management through DistAttention for distributing attention computation across GPU clusters, liability mechanisms for borrowing memory across instances, and global planning coordination [935]. KCache optimizes inference by storing Cache in high-bandwidth memory while keeping Cache in CPU memory, selectively copying key information based on attention calculations [935]. Multi-agent distributive processing represents an emerging approach using LLM-based multi-agent methods to handle massive inputs in distributed manner, addressing core bottlenecks in knowledge synchronization and reasoning processes when dealing with extensive external knowledge [699]. Analysis of real-world key-value cache access patterns reveals high cache reusability in workloads like RAG and agents, highlighting the need for efficient distributed caching systems with optimized metadata management to reduce redundancy and improve speed [1389]. These compression techniques can be combined with other long-context modeling approaches to further enhance LLMs capacity to process and utilize extended contexts efficiently while reducing computational overhead and preserving information integrity [317]. 25 Method Strategy Efficiency Accuracy Length Mgmt Scalability O1-Pruner [718] RL fine-tuning N/A +Acc, -Overhead Auto pruning +Efficiency InftyThink [1214] Iterative + summarization Complexity reduction +3-13% Iterative control Scalable Long-CoT Survey [147] Long CoT + reasoning +Efficiency frameworks +Complex domains Deep exploration Test-time scaling PREMISE [1273] Prune-on-Logic [721] Prompt opt + diagnostics Gradient-inspired opt Maintained/+Acc -87.5% tokens Performance maintained Structure-aware pruning Selective pruning +Accuracy Selective framework Logic-based opt Table 5: Long-chain reasoning methods and their characteristics in large language models. O1-Pruner uses reinforcement learning-style fine-tuning to shorten reasoning chains while maintaining accuracy. InftyThink employs iterative reasoning with intermediate summarization to reduce computational complexity. Long-CoT Survey explores long chain-of-thought characteristics that enhance reasoning abilities through efficiency improvements and enhanced knowledge frameworks. PREMISE optimizes prompts with trace-level diagnostics using gradient-inspired optimization, achieving 87.5% token reduction. Prune-on-Logic performs structure-aware pruning of logic graphs through selective removal of low-utility reasoning steps. 4.3.4. Applications Effective context management extends LLMs capabilities beyond simple question-answering to enable sophisticated applications leveraging comprehensive contextual understanding across multiple domains. Document processing and analysis capabilities enable LLMs to handle entire documents or comprehend full articles rather than fragments, allowing for contextually relevant responses through comprehensive understanding of input material, particularly valuable for inherently long sequential data such as gene sequences, legal documents, and technical literature where maintaining coherence across extensive content is critical [999]. Extended reasoning capabilities facilitated by context management techniques support complex reasoning requiring maintenance and building upon intermediate results across extended sequences. By capturing longer-range dependencies, these systems support multi-step problem solving where later reasoning depends on earlier calculations or deductions, enabling sophisticated applications in fields requiring extensive contextual awareness like complex decision support systems and scientific research assistance [999, 160]. Collaborative and multi-agent systems benefit from effective context management in multi-turn dialogues or sequential tasks where maintaining consistent state and synchronizing internal information between collaborating models is essential [154]. These capabilities support applications including distributed task processing, collaborative content creation, and multi-agent problem-solving where contextual coherence across multiple interactions must be maintained [154]. Enhanced conversational interfaces leverage robust context management to seamlessly handle extensive conversations without losing thread coherence, enabling more natural, persistent dialogues that closely resemble human conversations [883]. Task-oriented LLM systems benefit from structured context management approaches, with sliding window storage implementing minimal context management systems that permanently append prompts and responses to context stores, and Retrieval-Augmented Generation systems supplementing LLMs with access to external sources of dynamic information [212, 926]. These capabilities support applications like personalized virtual assistants, long-term tutoring systems, and therapeutic conversational agents that maintain continuity across extended interactions [883]. Memory-augmented applications implement strategies enabling LLMs to persistently store, manage, 26 and dynamically retrieve relevant contextual information, supporting applications requiring knowledge accumulation over time through building personalized user models via continuous interaction, implementing effective knowledge management across extended interactions, and supporting long-term planning scenarios depending on historical context [160]. Advanced memory frameworks like Contextually-Aware Intelligent Memory (CAIM) enhance long-term interactions by incorporating cognitive AI principles through modules that enable storage and retrieval of user-specific information while supporting contextual and time-based relevance filtering [1143]. Memory management for LLM agents incorporates processes analogous to human memory reconsolidation, including deduplication, merging, and conflict resolution, with approaches like Reflective Memory Management combining prospective and retrospective reflection for dynamic summarization and retrieval optimization [1167, 382]. Case-based reasoning systems provide theoretical foundations for LLM agent memory through architectural components that enable cognitive integration and persistent context storage techniques that implement caching strategies for faster provisioning of necessary context [383, 381]. The benefits extend beyond processing longer texts to fundamentally enhancing LLM interaction quality through improved comprehension, more relevant responses, and greater continuity across extended engagements, significantly expanding LLMs utility and resolving limitations imposed by restricted context windows [883]. 5. System Implementations Building upon the foundational components of Context Engineering, this section examines sophisticated system implementations that integrate these components into practical, intelligent architectures. These implementations represent the evolution from theoretical frameworks to deployable systems that leverage context engineering principles. We present four major categories of system implementations. RAG systems demonstrate external knowledge integration through modular architectures and graph-enhanced approaches. Memory Systems showcase persistent context management through sophisticated memory architectures enabling long-term learning. Tool-Integrated Reasoning transforms language models into world interactors through function calling and environment interaction. Multi-Agent Systems present coordinated approaches through communication protocols and orchestration mechanisms. Each implementation builds upon foundational components while addressing specific challenges in context utilization, demonstrating how theoretical principles translate into practical systems. 5.1. Retrieval-Augmented Generation Retrieval-Augmented Generation bridges the gap between parametric knowledge and dynamic information access by integrating external knowledge sources with language model generation. This implementation enables models to access current, domain-specific information through modular architectures, agentic frameworks, and graph-enhanced approaches that extend beyond static training data. 5.1.1. Modular RAG Architectures Modular RAG shifts from linear retrieval-generation architectures toward reconfigurable frameworks with flexible component interaction [311, 1131, 591]. Unlike Naive RAG and Advanced RAGs query rewriting, Modular RAG introduces hierarchical architectures: top-level RAG stages, middle-level sub-modules, and bottom-level operational units [312, 730]. This transcends linear structures through routing, scheduling, and fusion mechanisms enabling dynamic reconfiguration [312]. The formal representation RAG = R, operates through sophisticated module arrangements enabling 27 Figure 4: Retrieval-Augmented Generation Framework: Overview of RAG system architectures including Modular RAG, Agentic RAG Systems, and Graph-Enhanced RAG approaches for external context integration. Rewrite-Retrieve-Read models and Generate-Read approaches, incorporating adaptive search modules, RAGFusion for multi-query processing, routing modules for optimal data source selection, and hybrid retrieval strategies addressing retrieval accuracy and context relevance [311, 491, 908, 1045, 880, 95]. Contemporary frameworks demonstrate significant improvements in retrieval accuracy and trustworthiness [1372]. FlashRAG provides modular toolkit with 5 core modules and 16 subcomponents enabling independent adjustment and pipeline combination [500]. KRAGEN enhances biomedical problem-solving by integrating knowledge graphs with vector databases, utilizing biomedical knowledge graph-optimized prompt generation to address hallucination in complex reasoning [397, 749, 973]. ComposeRAG implements atomic modules for Question Decomposition and Query Rewriting, incorporating self-reflection mechanisms for iterative refinement [1159]. This modularity facilitates integration with fine-tuning and reinforcement learning, enabling customization for specific applications and comprehensive toolkits supporting diverse NLP tasks [312, 912, 4]. 5.1.2. Agentic RAG Systems Agentic RAG embeds autonomous AI agents into the RAG pipeline, enabling dynamic, context-sensitive operations guided by continuous reasoning [965, 277]. These systems leverage reflection, planning, tool use, and multi-agent collaboration to manage retrieval strategies dynamically and adapt workflows to complex task requirements [965]. RAG and agent workflows align through query rewriting corresponding to semantic comprehension, while retrieval phases correspond to planning and execution [622]. LLM-based autonomous agents extend basic language model capabilities through multimodal perception, tool utilization, and external memory integration [1160, 1091, 931, 843]. External long-term memory serves as knowledge datastore enabling agents to incorporate and access information over extended periods [1160, 382]. Unlike static approaches, Agentic RAG treats retrieval as dynamic operation where agents function as intelligent investigators analyzing content and cross-referencing information [648, 162]. Implementation paradigms encompass prompt-based methods requiring no additional training and training-based approaches optimizing models through reinforcement learning for strategic tool invocation [648, 1318, 965]. Advanced systems enable LLM agents to query vector databases, access SQL databases, or utilize APIs within single workflows, with methodological advances focusing on reasoning capabilities, tool integration, memory mechanisms, and instruction fine-tuning for autonomous decision-making [703, 6]. Core capabilities include reasoning and planning components through task decomposition, multi-plan selection, and memory-augmented planning strategies enabling agents to break down complex tasks and 28 select appropriate strategies [438, 439]. PlanRAG improves decision-making through plan-then-retrieve approaches, enabling agents to evaluate multiple information sources and optimize retrieval strategies, while SLA management frameworks address reconfigurable multi-agent architectures [162, 461]. Tool utilization enables systems to employ diverse resources including search engines, calculators, and APIs, with frameworks like ReAct and Reflexion exemplifying how interleaving reasoning with actions enhances adaptability [162, 1160, 956]. Memory mechanisms provide external long-term storage, while adaptive retrieval strategies enable autonomous analysis of complexity and context [162, 1128]. Self-reflection and adaptation mechanisms enable Agentic RAG systems to operate in dynamic environments through iterative feedback loops refining operations based on previous interaction outcomes [1183, 686]. Advanced memory systems like MemoryBank implement update mechanisms inspired by the Ebbinghaus Forgetting Curve, enhancing agents ability to retrieve and apply learnings from past interactions [1362, 165]. CDF-RAG employs closed-loop processes combining causal graph retrieval with reinforcement learning-driven query refinement and hallucination correction [531]. Self-RAG trains models that retrieve passages on demand while reflecting on retrievals and generations, using reflection tokens to control behavior during inference [239, 41]. 5.1.3. Graph-Enhanced RAG Graph-based Retrieval-Augmented Generation shifts from document-oriented approaches toward structured knowledge representations capturing entity relationships, domain hierarchies, and semantic connections [120, 1353, 360, 1391]. This enables extraction of specific reasoning paths providing relevant information to language models while supporting multi-hop reasoning through structured pathway navigation [120]. Graph structures minimize context drift and hallucinations by leveraging interconnectivity for enhanced context-aware retrieval and logical coherence [512, 806]. Knowledge graphs serve as foundational representations encapsulating entities and interrelationships in structured formats enabling efficient querying and semantic relationship capture [162, 1058]. Graphbased knowledge representations categorize into knowledge-based GraphRAG using graphs as knowledge carriers, index-based GraphRAG employing graphs as indexing tools, and hybrid GraphRAG combining both approaches [1199]. Sophisticated implementations include GraphRAGs hierarchical indexing with community detection, PIKEs multi-level heterogeneous knowledge graphs organizing documents into threelayer hierarchies, and EMG-RAGs Editable Memory Graph architecture [313]. Graph Neural Networks enhance RAG systems by addressing limitations in handling structured knowledge, with GNNs excelling at capturing entity associations and improving knowledge consistency [228, 116]. GNNRAG implementations adopt lightweight architectures for effective knowledge graph element retrieval, improving graph structure capture before interfacing with language models [1370, 162]. The integration process encompasses graph building through node and edge extraction, retrieval based on queries, and generation incorporating retrieved information [1370]. Multi-hop reasoning capabilities enable graph-based systems to synthesize information across multiple connected knowledge graph nodes, facilitating complex query resolution requiring interconnected fact integration [1058, 166]. These systems employ structured representations capturing semantic relationships between entities and domain hierarchies in ways that unstructured text cannot [1058, 166]. Advanced frameworks like Hierarchical Lexical Graph preserve statement provenance while clustering topics for flexible retrieval and linking entities for graph-based traversal [329]. Systems like GraphRAG, LightRAG, and derivatives implement dual-level retrieval, hierarchical indexing, and graph-enhanced strategies enabling robust multilevel reasoning [1174, 313]. Prominent architectures demonstrate diverse approaches to graph-enhanced retrieval, with optimization strategies showing significant improvements in retrieval effectiveness [106]. LightRAG integrates graph structures with vector representations through dual-level retrieval paradigms improving efficiency and content quality [412, 717]. HippoRAG leverages Personalized PageRank over knowledge graphs achieving notable improvements in multi-hop question answering [1088, 746, 366]. HyperGraphRAG proposes hypergraph structured representations advancing beyond binary relations [717]. RAPTOR provides hierarchical summary tree construction for recursive context generation, while PathRAG introduces pruning techniques for graphbased retrieval [1349, 928, 134]. These structured approaches enable transparent reasoning pathways with explicit entity connections, reducing noise and improving semantic understanding while overcoming traditional RAG challenges [1174, 512]. 5.1.4. Applications Real-time RAG systems address critical challenges in production environments where dynamic knowledge bases require continuous updates and low-latency responses [1339, 528]. Core challenges include efficient deployment and processing pipeline optimization, with existing frameworks lacking plug-and-play solutions necessitating system-level optimizations [1339]. Integration of streaming data introduces complications as traditional architectures demonstrate poor accuracy with frequently changing information and decreased efficiency as document volumes grow [514]. Dynamic retrieval mechanisms advance over static approaches by continuously updating strategies during generation, adjusting goals and semantic vector spaces in real-time based on generation states and identified knowledge gaps [384]. Current limitations in determining optimal retrieval timing and query formulation are addressed through Chain-of-Thought reasoning, iterative retrieval processes, decomposed prompting, and LLM-generated content for dynamic retrieval enabling adaptive information selection, with approaches extending to adaptive control mechanisms enhancing generation quality through reflective tags [992, 530, 85, 533, 1239]. Low-latency retrieval approaches leverage graph-based methods demonstrating significant promise in speed-accuracy optimization, with dense passage retrieval techniques providing foundational improvements [519]. LightRAGs dual-level retrieval system enhances information discovery while integrating graph structures with vector representations for efficient entity relationship retrieval, reducing response times while maintaining relevance [360]. Multi-stage retrieval pipelines optimize computational efficiency through techniques like graph-based reranking, enabling dynamic access to current information while reducing storage requirements [974]. Scalability solutions incorporate distributed processing architectures with efficient data partitioning, query optimization, and fault tolerance mechanisms adapting to changing stream conditions [1040, 35]. Memory optimization through transformed heavy hitters streaming algorithms intelligently filters irrelevant documents while maintaining quality, particularly valuable for frequently changing content [514]. Production frameworks demonstrate efficiency gains through modular RAG architectures supporting pre-retrieval processes like query expansion and post-retrieval refinements such as compression and selection, enabling fine-tuning of individual components [1069]. Incremental indexing and dynamic knowledge updates ensure systems adapt to new information without full retraining, particularly crucial in rapidly evolving domains like cybersecurity and climate finance applications [830, 1056]. Modern frameworks incorporate dynamic knowledge retrieval methods enabling continuous strategy adjustment based on evolving input and contextual information, enhancing interactivity and semantic understanding while increasing applicability across cross-domain integration [384]. Advanced 30 agent-based approaches demonstrate sophisticated task allocation capabilities in complex environments, such as coordinated UAV operations requiring real-time decision-making, with applications extending to grounded planning for embodied agents [1315, 975]. Dynamic Retrieval Augmented Generation frameworks like DRAGON-AI showcase specialized implementations for ontology generation, combining textual and logical components while incorporating self-memory mechanisms enabling iterative improvement [1043]. These advances represent significant evolution toward seamlessly integrating real-time knowledge with flexible retrieval capabilities in dynamic environments. 5.2. Memory Systems Memory Systems enable LLMs to transcend stateless interactions by implementing persistent information storage, retrieval, and utilization mechanisms. This implementation transforms models from pattern-matching processors into sophisticated agents capable of learning, adaptation, and long-term contextual understanding across extended interactions. Figure 5: Memory Systems Framework: Overview of memory architectures, memory-enhanced agents, and evaluation challenges for ultra-long context processing in LLMs. 5.2.1. Memory Architectures Memory distinguishes sophisticated language systems from pattern-matching models, enabling information processing, storage, and utilization across natural language tasks [1182, 1167, 296]. LLMs face considerable memory system constraints despite breakthroughs in text generation and multi-turn conversations [1182]. Neural memory mechanisms struggle with inadequate structured information storage and reliance on approximate vector similarity calculations rather than precise symbolic operations, challenging accurate storage and retrieval for multi-hop reasoning [423]. These limitations represent critical challenges for developing AI systems operating effectively in complex real-world applications [544]. Memory Classification Frameworks LLM memory systems can be organized into multiple classification frameworks. The primary temporal classification divides memory into three categories: sensory memory (input prompts), short-term memory (immediate context processing), and long-term memory (external databases or dedicated structures) [935]. From persistence perspective, short-term memory includes keyvalue caches and hidden states existing only within single sessions, while long-term memory encompasses text-based storage and knowledge embedded in model parameters, persisting across multiple interaction cycles [935, 818]. 31 Implementation-based classifications identify parametric memory (knowledge encoded in model weights), ephemeral activation memory (context-limited runtime states), and plaintext memory accessed through Retrieval-Augmented Generation methods [637]. Current implementations lack sophisticated lifecycle management and multi-modal integration, limiting long-term knowledge evolution. Feed-forward network layers serve as key-value tables storing memory, functioning as inner lexicon for word retrieval and creating mechanisms analogous to human associative memory [518, 325, 326, 764, 464]. These classification schemes reflect attempts to develop LLM memory architectures paralleling human cognitive systems [1167]. Short-Term Memory Mechanisms Short-term memory in LLMs operates through the context window, serving as working memory maintaining immediate access to previously processed tokens [1282]. This functionality is implemented through key-value caches storing token representations but disappearing when sessions terminate [891]. Architectural variations demonstrate significant differences: transformer-based models implement working memory systems flexibly retrieving individual token representations across arbitrary delays, while LSTM architectures maintain coarser, rapidly-decaying semantic representations weighted toward earliest items [40]. Modern LLM short-term memory frequently manifests as in-context learning, reflecting models ability to acquire and process information temporarily within context windows [1180, 103]. This enables fewshot learning and task adaptation without parameter updates. Research identifies three primary memory configurations: full memory (utilizing entire context history), limited memory (using context subsets), and memory-less operation (without historical context) [1044]. Despite advances expanding context windows to millions of tokens, LLMs struggle with effective reasoning over extended contexts, particularly when relevant information appears in middle positions [891, 685]. Long-Term Memory Implementations LLMs face significant challenges maintaining long-term memory due to context window limitations and catastrophic forgetting [114]. External memory-based methods address these limitations by utilizing physical storage to cache historical information, allowing relevant history retrieval without maintaining all information within constrained context windows [682, 1362]. These approaches contrast with internal memory-based methods focusing on reducing self-attention computational costs to expand sequence length [682, 287]. Long-term memory implementations categorize into knowledge-organization methods (structuring memory into interconnected semantic networks), retrieval mechanism-oriented approaches (integrating semantic retrieval with forgetting curve mechanisms), and architecture-driven methods (implementing hierarchical structures with explicit read-write operations) [515, 1362, 444]. Memory storage representations can be further divided into token-level memory (information stored as structured text for direct retrieval) and latentspace memory (utilizing high-dimensional vectors for abstract and compact information representation) [1216, 1124]. Advanced approaches incorporate psychological principles, with MemoryBank implementing Ebbinghaus Forgetting Curve theory for selective memory preservation based on temporal factors [1362], emotion-aware frameworks employing Mood-Dependent Memory theory [444], and memorization mechanisms balancing performance advantages with privacy concerns through extraction vulnerability analysis [1041, 122, 123]. Memory Access Patterns and Structures LLMs exhibit characteristic memory access patterns with notable similarities to human cognitive processes, demonstrating clear primacy and recency effects when recalling 32 information lists [477]. Memory retrieval operates through sequential access (retrieving content in consecutive order) and random access (accessing information from arbitrary points without processing preceding content) [1387]. Memory persistence studies employ recognition experiments, recall experiments, and retention experiments to quantify information accessibility duration and retrieval conditions [810], with cognitive psychology concepts like semantic and episodic memory integration improving LLM information synthesis capabilities [240]. Memory organization encompasses diverse structural approaches including textual-form storage (complete and recent agent-environment interactions, retrieved historical interactions, external knowledge), knowledge representation structures (chunks, knowledge triples, atomic facts, summaries, mixed approaches), hierarchical systems with library-enhanced reasoning components, and functional patterns organized by tasks, temporal relevance, or semantic relationships [1329, 1290, 1027]. Core memory operations include encoding (transforming textual information into latent space embeddings), retrieval (accessing relevant information based on semantic relevance, importance, and recency), reflection (extracting higher-level insights), summarization (condensing texts while highlighting critical points), utilization (integrating memory components for unified outputs), forgetting (selective information discarding), truncation (formatting within token limitations), and judgment (assessing information importance for storage prioritization) [1331]. These structures offer varying trade-offs between comprehensiveness, retrieval efficiency, and computational requirements. 5.2.2. Memory-Enhanced Agents Memory systems fundamentally transform LLMs from stateless pattern processors into sophisticated agents capable of persistent learning and adaptation across extended interactions [1259]. Memory-enhanced agents leverage both short-term memory (facilitating real-time responses and immediate context awareness) and long-term memory (supporting deeper understanding and knowledge application over extended periods) to adapt to changing environments, learn from experiences, and make informed decisions requiring persistent information access [1259]. Agent Architecture Integration Contemporary LLM agents employ memory systems analogous to computer memory hierarchies, with short-term memory functioning as primary storage for contextual understanding within context windows, while long-term memory serves as persistent storage for extended information retention [770]. From object-oriented perspectives, AI systems generate personal memories related to individual users and system memories containing intermediate task results [1167]. Structured frameworks like MemOS classify memory into Parametric Memory (knowledge encoded in model weights), Activation Memory, and Plaintext Memory, with parametric memory representing long-term knowledge embedded within feedforward and attention layers enabling zero-shot generation [637]. Memory integration frameworks have evolved to address LLM limitations through sophisticated architectures. The Self-Controlled Memory (SCM) framework enhances long-term memory through LLM-based agent backbones, memory streams, and memory controllers managing updates and utilization [649]. The REMEMBERER framework equips LLMs with experience memory exploiting past episodes across task goals, enabling success/failure learning without parameter fine-tuning through verbal reinforcement and selfreflective feedback mechanisms [1299]. Advanced systems like MemLLM implement structured read-write memory modules addressing challenges in memorizing rare events, updating information, and preventing hallucinations [779]. Autonomous agents leveraging LLMs rely on four essential componentsperception, memory, planning, and actionworking together to enable environmental perception, interaction recall, Model MemoryBank [1363] RET-LLM [778] ChatDB [423] TiM [683] Voyager [1078] MemGPT [814] RecMind [1115] Retroformer [1249] ExpeL [1337] Synapse [1357] ChatDev [855] InteRecAgent [450] TPTU [909, 554] MetaGPT [409] S3 [301] Mem0 [169] Larimar [198] EM-LLM [286] Controllable Working Memory [597] Working Memory Hub [355] LLM-based Opinion Dynamics [175] Memory Sandbox [456] A-MEM [1203] MemEngine [1331] HIAGENT [429] MemInsight [917] Memory Sharing (MS) [302] MemoRAG [860] Echo [694] Textual Form Parametric Form Complete Recent Retrieved External Fine-tuning Editing Core Memory Systems Agent-Based Systems Advanced Memory Architectures Recent and Emerging Systems Table 6: Extended from [1329]: Memory implementation patterns. = Adopted, = Not Adopted and real-time planning and execution [614, 38]. Real-World Applications Memory-enhanced LLM agents have demonstrated transformative impact across diverse application domains. In conversational AI, memory systems enable more natural, human-like interactions by recalling past experiences and user preferences to deliver personalized, context-aware responses. Commercial implementations include Charlie Mnemonic (combining Long-Term, Short-Term, and episodic memory using GPT-4), Google Gemini (leveraging long-term memory for personalized experiences across Googles ecosystem), and ChatGPT Memory (remembering conversations across sessions) [578]. User simulation applications employ LLM-powered conversational agents mimicking human behavior for cost-effective dialogue system evaluation, adapting flexibly across open-domain dialogues, task-oriented interactions, and conversational recommendation [204], with systems like Memory Sandbox enabling user control over conversational memories through data object manipulation [455]. 34 Task-oriented agents utilize memory to perform complex autonomous operations with minimal human intervention, employing LLMs as controllers extended through multimodal perception, tool utilization, and external memory [1160]. Applications span recommendation systems (RecMind providing personalized recommendations through planning and external knowledge, InteRecAgent employing LLMs with recommender models as tools), autonomous driving (DiLu instilling human-like knowledge through reasoning, reflection, and memory), scientific research (ChemCrow automating chemical synthesis design and execution), and social simulation (generative agents exhibiting believable behavior through memory storage and synthesis) [1019, 647, 92, 825]. Proactive conversational agents address challenges in strategic dialogue scenarios requiring goal-oriented conversation steering through prompt-based policy planning methods and AI feedback generation based on dialogue history [204, 203]. Personalized assistant applications leverage memory to maintain coherent long-term relationships with users, with memory components serving as structured repositories storing contextually relevant information including user preferences and historical interactions [438]. Domain-specific implementations include healthcare assistants employing memory coordination for medical interactions [1316, 1307], recommendation agents leveraging external knowledge bases [1316, 1293], educational agents providing context-aware support through memory-enabled progress tracking [647], and specialized frameworks like MARK enhancing personalized AI assistants through user preference memory [299]. Memory Technologies and Integration Methods Memory technology evolution addresses fundamental context window limitations through RAG, which combines parametric and non-parametric memory for language generation using pre-trained seq2seq models and dense vector indices [1209, 591]. This approach enables access to information beyond parameter storage without requiring retraining, significantly extending knowledge capabilities. Advanced memory mechanisms including vector databases and retrieval-augmented generation enable vast information storage with quick relevant data access, incorporating short-term contextual memory and long-term external storage [38, 367, 1184, 507]. Non-parametric approaches maintain frozen LLM parameters while leveraging external resources like RAG to enrich task contexts [934]. Systems like Reflexion implement verbal reinforcement through selfreflective feedback in episodic memory buffers, while REMEMBERER incorporates persistent experience memory enabling learning from past successes and failures. Advanced architectures like MemoryBank enable memory retrieval, continuous evolution through updates, and personality adaptation by integrating previous interaction information [1202, 1362]. Specialized memory architectures address particular agent requirements through sophisticated organization and retrieval mechanisms. While early systems required predefined storage structures and retrieval timing, newer systems like Mem0 incorporate graph databases following RAG principles for more effective memory organization and relevance-based retrieval [1202]. Commercial and open-source implementations including OpenAI ChatGPT Memory, Apple Personal Context, mem0, and MemoryScope demonstrate widespread adoption of memory systems for enhanced personalization capabilities [1167]. Tool-augmentation paradigms validate effectiveness in complex task decomposition while leveraging world interaction tools, with memory-enhanced agents becoming central to modern AI systems performing complex tasks through natural language integration of planning, tool use, memory, and multi-step reasoning [247, 356, 1091, 34]. 5.2.3. Evaluation and Challenges Memory evaluation frameworks have emerged as critical components for systematically assessing LLM agent capabilities across multiple dimensions, reflecting the multifaceted nature of memory in intelligent systems. 35 These comprehensive evaluation approaches reveal significant challenges while pointing toward promising research directions that could unlock new capabilities for memory-enhanced agents. Evaluation Frameworks and Metrics Contemporary memory evaluation employs specialized metrics extending beyond traditional NLP performance indicators to capture nuanced memory functionality aspects [1330]. Effectiveness metrics focus on factual information storage and utilization through accuracy measures (correctness of responses based on historical messages) and recall@5 indicators (percentage of relevant messages retrieved within top-5 results). Efficiency metrics examine temporal aspects through response time (duration for information retrieval and utilization) and adaptation time (period required for new information storage) [1330]. Extensive benchmarks such as LongMemEval assess five fundamental long-term memory capabilities: information extraction, temporal reasoning, multi-session reasoning, knowledge updates, and abstention through 500 carefully selected questions, demonstrating 30% accuracy degradation in commercial assistants throughout prolonged interactions, while automated memory evaluation frameworks facilitate thorough assessment extending beyond passkey search methodologies [1171]. Dedicated frameworks target episodic memory via benchmarks assessing temporally-situated experiences, with research demonstrating that cuttingedge models including GPT-4, Claude variants, and Llama 3.1 encounter difficulties with episodic memory challenges involving interconnected events or intricate spatio-temporal associations even in comparatively brief contexts [457]. Contemporary LLM benchmarks predominantly concentrate on assessing models retention of factual information and semantic relationships while substantially overlooking episodic memory assessmentthe capacity to contextualize memories with temporal and spatial occurrence details [841]. Task-specific evaluations encompass long-context passage retrieval (locating specific paragraphs within extended contexts), long-context summarization (developing comprehensive understanding for concise summaries), NarrativeQA (answering questions based on lengthy narratives), and specialized benchmarks like MADail-Bench evaluating both passive and proactive memory recall in conversational contexts with novel dimensions including memory injection, emotional support proficiency, and intimacy assessment [1329, 1380, 550, 386]. Additional task-specific frameworks include QMSum for meeting summarization, QuALITY for reading comprehension, DialSim for dialogue-based QA requiring spatiotemporal memory, and MEMENTO for personalized embodied agent evaluation using two-stage processes to assess memory utilization in physical environment tasks [1380, 566]. Current Limitations and Challenges Memory evaluation faces substantial challenges limiting effective assessment of capabilities. Fundamental limitations include absence of consistent, rigorous methodologies for assessing memory performance, particularly regarding generalization beyond training data [284]. The lack of standardized benchmarks specifically designed for long-term memory evaluation represents another significant obstacle, with existing frameworks often failing to capture the full spectrum of memory capabilities needed for human-like intelligence [1071]. Architectural constraints significantly complicate evaluation efforts, as most contemporary LLM-based agents operate in fundamentally stateless manners, treating interactions independently without truly accumulating knowledge incrementally over time [1355, 1354], despite advances in working memory through attentional tagging mechanisms enabling flexible memory representation control [864]. This limitation prevents genuine lifelong learning assessmenta cornerstone of human-level intelligence involving continuous knowledge acquisition, retention, and reuse across diverse contexts and extended time horizons. 36 Methodological issues arise when isolating memory-specific performance from other intelligence aspects, challenging determination of whether failures stem from inadequate memory mechanisms or reasoning limitations [284]. Dynamic memory usage in real-world applications poses evaluation challenges, as controlled laboratory tests inadequately capture memory system performance in complex scenarios where information relevance changes unpredictably [1071]. Optimization Strategies and Future Research Directions Memory optimization encompasses diverse techniques enhancing utilization while minimizing computational overhead and maximizing efficiency. Biologically-inspired forgetting mechanisms provide effective optimization approaches, with frameworks like MemoryBank implementing Ebbinghaus forgetting curves to selectively preserve and discard information based on temporal factors and significance [1362]. Reflection-based optimization through systems like Reflexion enables performance assessment through integrated evaluation and self-reflection, creating dual feedback systems refining memory and behavior through continuous learning [300]. Hierarchical memory structures optimize information organization through multi-level formats enabling efficient retrieval, demonstrated by Experience-based Hierarchical Control frameworks with rapid memory access modules [862], memory consolidation processes through bidirectional fast-slow variable interactions [63], and Adaptive Cross-Attention Networks dynamically ranking memories based on query relevance [406]. Future research directions encompass hybrid memory frameworks combining parametric precision with non-parametric efficiency [934], automated feedback mechanisms for scalable response evaluation [885], multi-agent memory systems enabling collaborative learning through shared external memories [302], enhanced metadata learning with knowledge graph integration [888, 382], domain-specific memory architectures for specialized applications [501], cognitive-inspired optimization incorporating memory consolidation during inactive periods [752], and parameter-efficient memory updates through techniques like Low-Rank Adaptation for efficient knowledge integration [424, 252]. These developments promise advancing memory-enhanced LLM agents toward sophisticated, human-like cognitive capabilities while addressing computational and architectural limitations, with applications extending to long-term robotic planning, real-world decision-making systems, and collaborative AI assistants through streaming learning scenarios and continuous feedback integration [1150, 1336, 1269]. 5.3. Tool-Integrated Reasoning Tool-Integrated Reasoning transforms language models from passive text generators into active world interactors capable of dynamic tool utilization and environmental manipulation. This implementation enables models to transcend their inherent limitations through function calling mechanisms, integrated reasoning frameworks, and sophisticated environment interaction capabilities. 5.3.1. Function Calling Mechanisms Function calling transforms LLMs from generative models into interactive agents through structured output generation leveraging functions abstraction mechanism, enabling external tool manipulation and access to current, domain-specific information for complex problem-solving [5, 663, 331, 874, 58, 517, 1104]. Evolution began with Toolformers self-supervised approach demonstrating autonomous API learning, inspiring ReActs thought-action-observation cycle, progressing through specialized models like Gorilla and comprehensive frameworks including ToolLLM, RestGPT, with OpenAIs JSON standardization, while 37 Figure 6: Tool-Augmented Systems Framework: Evolution from text generators to world interactors through function calling mechanisms, tool-integrated reasoning, and environment interaction capabilities. advanced systems like Chameleon enabled multimodal question answering and TaskMatrix.AI managed AI models across domains [931, 248, 648, 541, 915, 866, 867, 709, 653, 945]. Technical implementation involves fine-tuning (dominant method providing stable capabilities via extensive API training but requiring significant resources) and prompt engineering (flexible, resource-efficient but unstable), with approaches like Reverse Chain enabling API operation via prompts, addressing challenges in large tool management [388, 5, 1323, 785, 144, 250]. Core process encompasses intent recognition, function selection, parameter-value-pair mapping, function execution, and response generation, with modern implementations utilizing structured LLM outputs for external program interaction, while tools include diverse interfaces (digital systems, scratch pads, user interactions, other LLMs, developer code), requiring complex navigation of tool selection, argument formulation, and result parsing [1259, 663, 1132, 189, 952, 584, 902]. Training Methodologies and Data Systems Training methodologies evolved from basic prompt-based approaches to sophisticated multi-task learning frameworks, with fine-tuning on specialized datasets through systems like ToolLLM and Granite-20B-FunctionCalling, beginning with synthetic single-tool data followed by human annotations [388, 5, 353, 771, 1226]. Data generation strategies include Weavers GPT-4-based environment synthesis, APIGens hierarchical verification pipelines (format checking, function execution, semantic verification), generating 60,000+ high-quality entries across thousands of APIs [1104, 1177, 1259, 1156, 65, 1393, 743]. Tool selection enhancement involves irrelevance-aware data augmentation, with Hammers function masking techniques, oracle tool mixing for increased difficulty, tool intent detection synthesis for overtriggering mitigation, emphasizing high-quality data through stringent filtering and format verification [664, 10, 353, 467, 1291, 214]. Self-improvement paradigms reduce external supervision dependence through JOSH algorithms sparse reward simulation environments and TTPAs token-level optimization with error-oriented scoring, demonstrating improvements while preserving general capabilities [573, 440, 362, 1262]. Sophisticated benchmarks include API-Bank (73 APIs, 314 dialogues), StableToolBench (API instability solutions), NesTools (nested tool evaluation), ToolHop (995 queries, 3,912 tools), addressing single-tool to multi-hop scenarios [615, 359, 373, 1255, 821, 987, 1248, 979]. 5.3.2. Tool-Integrated Reasoning Tool-Integrated Reasoning (TIR) represents paradigmatic advancement in Large Language Model capabilities, addressing fundamental limitations including outdated knowledge, calculation inaccuracy, and shallow reasoning by enabling dynamic interaction with external resources during the reasoning process [858]. Unlike traditional reasoning approaches that rely exclusively on internal model knowledge, TIR establishes synergistic relationship where reasoning guides complex problem decomposition into manageable subtasks while specialized tools ensure accurate execution of each computational step [771]. This paradigm extends beyond conventional text-based reasoning by requiring models to autonomously select appropriate tools, interpret intermediate outputs, and adaptively refine their approach based on real-time feedback [858]. The evolution of TIR methodologies encompasses three primary implementation categories addressing distinct aspects of tool utilization optimization. Prompting-based methods guide models through carefully crafted instructions without additional training, exemplified by approaches that decompose mathematical problems into executable code while delegating computation to Python interpreters [152, 595]. Supervised fine-tuning approaches teach tool usage through imitation learning, with systems like ToRA focusing on mathematical problem-solving by integrating natural language reasoning with computational libraries and symbolic solvers [341]. Reinforcement learning methods optimize tool-use behavior through outcome-driven rewards, though current implementations often prioritize final correctness without considering efficiency, potentially leading to cognitive offloading phenomena where models over-rely on external tools [223]. In operational terms, TIR-based agents serve as intelligent orchestrators that systematically interweave cognitive processing with external resource engagement to achieve targeted outcomes [1087]. This mechanism requires the harmonious integration of intrinsic reasoning capabilities and extrinsic tool utilization for progressive knowledge synthesis toward objective fulfillment, where the agents execution pathway is formally characterized as structured sequence of tool activations coupled with corresponding information assimilation events [1087]. Emerging developments have established Agentic Reasoning architectures that amplify language model intelligence by incorporating autonomous tool-deploying agents, fluidly orchestrating web-based information retrieval, computational processing, and layered reasoning-memory integration to tackle sophisticated challenges necessitating comprehensive research and cascaded logical analysis [1153]. Implementation Frameworks and Paradigms Single-tool frameworks established foundational principles of tool-integrated reasoning through specialized implementations targeting specific computational domains. Program-Aided Language Models (PAL) pioneered problem decomposition strategies by generating executable code while delegating mathematical computations to Python interpreters [305]. ToolFormer demonstrated that language models could learn external API usage with minimal demonstrations, incorporating calculators, search engines, and diverse tools to enhance computational capabilities [931]. ToRA advanced mathematical reasoning by integrating natural language processing with computational libraries and symbolic solvers, while ReTool applied reinforcement learning to optimize code interpreter usage, demonstrating improvements in self-correction patterns [341, 1311, 965]. Self-Edit utilizes execution results of generated code to improve code quality for competitive programming tasks, employing fault-aware code editor to correct errors based on test case results [1309]. Multi-tool coordination systems address the complexity of orchestrating heterogeneous tools within integrated reasoning architectures. ReAct pioneered the interleaving of reasoning traces with task-specific actions, enabling models to think and act complementarily where reasoning supports plan tracking while 39 actions interface with external information sources [1245]. Chameleon introduced plug-and-play compositional reasoning by synthesizing programs combining vision models, search engines, and Python functions with an LLM-based planner core [709]. AutoTools established automated frameworks transforming raw tool documentation into executable functions, reducing manual engineering requirements in tool integration [419, 952]. Chain-of-Agents (CoA) trains models to decode reasoning chains with abstract placeholders, subsequently calling domain-specific tools to fill knowledge gaps [594, 1327]. Agent-based frameworks represent the most sophisticated evolution of TIR systems, moving beyond static prompting approaches to create autonomous and adaptive AI systems. Unlike conventional tool-use that follows rigid patterns, agent models learn to couple Chain-of-Thought (CoT) and Chain-of-Action (CoA) patterns into their core behavior, resulting in stronger logical coherence and natural transitions between reasoning and action [1328]. These systems build upon foundational agent architectures including reactive systems that map perceptions directly to actions, deliberative systems implementing Belief-Desire-Intention (BDI) models, and hybrid architectures combining multiple subsystems in hierarchical structures [728]. Method ReAct [1247] Toolformer [931] ToolkenGPT [378] ToolLLM [867] ToRA [341] PAL [303] HuggingGPT [945] GPT4Tools [1225] CRITIC [340] Chain of Code [595] TRICE [863] TP-LLaMA [149] AlignToolLLaMA [161] ReTool [270] Tool-Star [221] ARTIST [965] Ego-R1 [1038] VTool-R1 [1155] KG-Agent [487] CACTUS [755] MuMath-Code [1265] ToRL [621] MetaTool [452] ToolEyes [1253] Graph-CoT [495] ToolRL [858] LATS [1364] Search & Retrieval Computation & Code Execution Knowledge Base & QA APIs & External Services Multimodal Tools Language Processing Interactive Environments Domain-Specific Tools Tool Categories Table 7: Tool-augmented language model architectures: Comparison of multiple methods across 8 tool categories including search, computation, knowledge bases, APIs, multimodal, language tools, interactive environments, and domain-specific applications. 5.3.3. Agent-Environment Interaction Reinforcement learning approaches have emerged as superior alternatives to prompting-based methods and supervised fine-tuning for tool integration, enabling models to autonomously discover optimal tool usage strategies through exploration and outcome-driven rewards [223]. ReTool exemplifies this advancement 40 by focusing on code interpreter optimization for mathematical reasoning, achieving 67.0% accuracy on AIME2024 benchmarks after only 400 training steps, substantially outperforming text-based RL baselines reaching 40.0% accuracy with extensive training [270]. This demonstrates that explicitly modeling tool use within decision processes enhances both reasoning capabilities and training efficiency. Search-augmented reasoning systems represent innovative integrations of information retrieval directly into reasoning processes through specialized learning environments. The Search-R1 framework trains models to make dynamic decisions about when to search and what queries to generate during multi-step reasoning tasks, unlike traditional retrieval-augmented generation systems [976]. The architecture employs specialized token systems structuring reasoning and search processes, where models learn to generate reasoning steps interspersed with explicit search actions triggered through tokens that encapsulate generated queries [648]. Multi-turn and customizable tool invocation frameworks address the complexity of coordinating multiple heterogeneous tools during reasoning processes. Recent developments include frameworks like VisTA that use reinforcement learning to enable visual agents to dynamically explore, select, and combine tools from diverse libraries based on empirical performance [454]. ReVeal demonstrates self-evolving code agents via iterative generation-verification processes [506]. In multimodal domains, systems like VideoAgent employ vision-language foundation models as tools for translating and retrieving visual information, achieving impressive performance on video understanding benchmarks [1108, 254]. Evaluation and Applications Comprehensive evaluation of tool-integrated reasoning systems requires specialized benchmarks that measure tool-integrated capabilities rather than general model performance. MCP-RADAR provides standardized evaluation framework employing strictly objective metrics derived from quantifiable performance data, with extensible design spanning software engineering, mathematical reasoning, and general problem-solving domains [310]. The framework visualizes performance through radar charts highlighting model strengths and weaknesses across multiple dimensions, enabling systematic comparison of tool-integrated language models regardless of implementation mechanisms. Real-world evaluation approaches reveal significant performance gaps between current systems and human-level capabilities, providing crucial insights into practical limitations and optimization opportunities. The General Tool Agents (GTA) benchmark addresses limitations in existing evaluations by featuring real human-written queries with implicit tool-use requirements, evaluation platforms with deployed tools across perception, operation, logic, and creativity categories, and authentic multimodal inputs including images and code snippets [1090]. Results demonstrate substantial challenges for current LLMs, with GPT-4 completing less than 50 Function calling enabled sophisticated multi-agent systems where multiple LLM agents collaborate through coordinated tool use and task decomposition, with MAS leveraging collective intelligence through parallel processing, information sharing, and adaptive role assignment, while LLM integration enhanced capabilities in planning, specialization, and task decomposition through frameworks like DyLAN, MAD, and MetaGPT [239, 903, 344, 140, 625]. Advanced multi-agent function calling employs sophisticated orchestration mechanisms decomposing complex tasks into manageable subtasks, with fundamental approaches involving splitting reward machines into parallel execution units, each agent maintaining individual reward machines, local state spaces, and propositions, while adaptive orchestration enables dynamic agent selection based on context, responses, and status reports [39, 1048, 691, 117]. 41 5.4. Multi-Agent Systems Multi-Agent Systems represent the pinnacle of collaborative intelligence, enabling multiple autonomous agents to coordinate and communicate for solving complex problems beyond individual agent capabilities. This implementation focuses on sophisticated communication protocols, orchestration mechanisms, and coordination strategies that enable seamless collaboration across diverse agent architectures. Figure 7: Multi-Agent Systems Framework: Overview of communication protocols, orchestration mechanisms, and coordination strategies for collaborative AI agent systems. 5.4.1. Communication Protocols Agent communication systems originate from the Knowledge Sharing Effort of the early 1990s, establishing foundational principles for autonomous entity coordination through standardized languages addressing interoperability challenges [369, 93]. KQML emerged as the pioneering Agent Communication Language, introducing multi-layered architecture separating content, message, and communication layers while employing speech act theory [369, 82, 657, 280]. FIPA ACL enhanced this foundation through semantic frameworks based on modal logic, feasibility preconditions, and rational effects [1146, 369, 82]. Interoperability requirements necessitate semantic-level communication capabilities enabling crossplatform agent understanding without extensive pre-communication setup, addressing increasing heterogeneity through ontology-based protocol formalization and Semantic Web technologies, while incorporating security mechanisms against communication vulnerabilities [480, 66, 443, 481, 786, 1055]. Contemporary Protocol Ecosystem Contemporary standardized protocols address fragmentation challenges hindering LLM agent collaboration [1235, 1128, 408]. MCP functions as USB-C for AI, standardizing agent-environment interactions through JSON-RPC client-server interfaces, enabling hundreds of servers across diverse domains while introducing security vulnerabilities [926, 246, 616, 266, 15, 257, 922, 1094, 370, 1185, 297, 1008, 713, 269]. A2A standardizes peer-to-peer communication through capability-based Agent Cards enabling task delegation and secure collaboration via JSON-based lifecycle models [616, 246, 926]. ACP provides generalpurpose RESTful HTTP communication supporting multipart messages and synchronous/asynchronous interactions with discovery, delegation, and orchestration features [277, 246]. ANP extends interoperability to open internet through W3C decentralized identifiers and JSON-LD graphs, with emerging protocols AGNTCY and Agora diversifying standardization ecosystems [246, 679, 1128]. 42 Progressive layering strategy: MCP provides tool access, ACP enables message exchange, A2A supports peer interaction, ANP extends network interoperability [1007, 926]. LLM-Enhanced Communication Frameworks LLMs transform agent communication through sophisticated natural language processing enabling unprecedented context sensitivity across academic and industrial applications spanning social science, natural science, and engineering domains [486, 684, 498, 1091, 1170, 1127, 896, 1052, 871]. Enhanced systems demonstrate cognitive synergy through specialized knowledge bases, planning, memory, and introspection capabilities, supporting cooperative, debate-oriented, and competitive communication paradigms [486, 356]. Communication structures encompass layered hierarchical organization, decentralized peer-to-peer networks, centralized coordination, and shared message pool architectures, complemented by sequential exchanges, universal language interfaces, and message-passing strategies [356, 1240, 1210, 167, 396, 485, 537, 659, 793, 941]. Framework implementations support comprehensive ecosystems: AutoGen enables dynamic response generation, MetaGPT provides shared message pools, CAMEL offers integrated orchestration, CrewAI facilitates adaptation, with reinforcement learning integration enhancing reward redesign, action selection, and policy interpretation [184, 38, 119, 996, 224, 865, 927, 950, 1264]. Human-agent communication introduces complex interaction landscapes through flexible participation and cognitive diversity, with agents inferring communicator properties and mirroring human communicative intentions [1399, 34, 669]. 5.4.2. Orchestration Mechanisms Orchestration mechanisms constitute the critical coordination infrastructure for multi-agent systems, managing agent selection, context distribution, and interaction flow control [894], enabling effective cooperation among human and non-human actors through user input processing, contextual distribution, and optimal agent selection based on capability assessment and response evaluation [53], while managing message flow, ensuring task progression, and addressing task deviations [171]. Advanced orchestration frameworks incorporate intent recognition, contextual memory maintenance, and task dispatching components for intelligent coordination across domain-specific agents, with the Swarm Agent framework utilizing real-time outputs to direct tool invocations while addressing limitations in static tool registries and bespoke communication frameworks [808, 263, 246]. Contemporary orchestration strategies exhibit distinct operational paradigms: priori orchestration determines agent selection through pre-execution analysis of user input and agent capabilities, while posterior orchestration distributes inputs to multiple agents simultaneously, utilizing confidence metrics and response quality assessment as demonstrated by the 3S orchestrator framework [893]; function-based orchestration emphasizes agent selection from available pools, contextual information management, and conversation flow control [54]; component-based orchestration employs dynamic planning processes where orchestrators arrange components in logical sequences based on user instructions, utilizing LLMs as component orchestration tools to generate workflows with embedded orchestration logic [675]. Emergent orchestration paradigms include puppeteer-style orchestration featuring centralized orchestrators that dynamically direct agents in response to evolving task states through reinforcement learning-based adaptive sequencing and prioritization, and serialized orchestration addressing collaboration topology complexity by unfolding collaboration graphs into reasoning sequences guided by topological traversal, enabling orchestrators to select single agents at each step based on global system state and task specifications [194]. 43 Context Management and Environmental Adaptation Context serves as the foundational element guiding agent actions and interactions within orchestrated systems, supporting operational mode diversity while maintaining application individuality and task execution sequencing through global state maintenance that enables orchestration systems to track task execution progress across distributed nodes, providing agents with contextual awareness necessary for effective subtask performance within broader workflow contexts [26]. Session-based context refinement defines collaborative scope boundaries, facilitating event-driven orchestration where agents can enter and exit dynamically, create output streams, and contribute to shared session streams, with configurable sessions enabling agent inclusion based on user input or autonomous decision-making to create adaptable systems responsive to changing task requirements [513]. Well-designed interaction structures and task orchestration mechanisms underscore contexts critical role in scalable multi-agent collaboration. Systems adapt communication patterns and agent roles to contextual requirements, supporting dynamic collaboration tailored to specific task demands through complex task decomposition and suitable agent assignment for subtask execution [1128]. This contextual adaptation encompasses both organizational and operational dimensions, enabling systems to maintain coherence while accommodating environmental variability and evolving user requirements. 5.4.3. Coordination Strategies Multi-agent orchestration encounters significant challenges in maintaining transactional integrity across complex workflows, with contemporary frameworks including LangGraph, AutoGen, and CAMEL demonstrating insufficient transaction support: LangGraph provides basic state management while lacking atomicity guarantees and systematic compensation mechanisms, AutoGen prioritizes flexible agent interactions without adequate compensatory action management potentially resulting in inconsistent system states following partial failures, and validation limitations emerge as many frameworks rely exclusively on large language models inherent self-validation capabilities without implementing independent validation procedures, exposing systems to reasoning errors, hallucinations, and inter-agent inconsistencies [128]. Context handling failures compound these challenges as agents struggle with long-term context maintenance encompassing both episodic and semantic information [210, 1113], while central orchestrator topologies introduce non-deterministic, runtime-dependent execution paths that enhance adaptability while complicating anomaly detection, requiring dynamic graph reconstruction rather than simple path matching [390], and environmental misconfigurations and LLM hallucinations can distract agentic systems, with poor recovery leading to goal deviation that becomes amplified in multi-agent setups with distributed subtasks [210, 1091]. Inter-agent dependency opacity presents additional concerns as agents may operate on inconsistent assumptions or conflicting data without explicit constraints or validation layers, necessitating anomaly detection incorporating reasoning over orchestration intent and planning coherence [390], while addressing these challenges requires comprehensive solutions such as the SagaLLM framework providing transaction support, independent validation procedures, and robust context preservation mechanisms [128], and approaches like CodeAct integrating Python interpreters with LLM agents to enable code action execution and dynamic revision capabilities through multi-turn interactions [1113]. Applications and Performance Implications Agent and context orchestration demonstrates practical utility across diverse application domains: healthcare applications employ context-switching mechanisms within specialized agent-based architectures performing information retrieval, question answering, and decision support, utilizing supervisory agents to interpret input features and assign subtasks to specialized agents based on clinical query type, user background, and data modality requirements [613, 754, 1051]; network management applications leverage context-aware orchestration to address complexity challenges by equipping Points of Access with agents dedicated to unique contexts, enabling efficient network dynamics management through context-specific action sets including available service instances and network paths [958]. Business process management and simulation represent significant application areas through platforms like AgentSimulator, enabling process behavior discovery and simulation in orchestrated and autonomous settings where orchestrated behavior follows global control-flow patterns with activity selection dependent on previous activities and agent assignment based on capabilities and availability, while autonomous behavior operates through local control-flow and handover patterns acknowledging agent autonomy in collaborative work [543]. Performance implications indicate that well-designed orchestration improves system effectiveness by leveraging distinct agent capabilities, with research demonstrating that human users frequently struggle with effective agent selection from available sets while automated orchestration enhances overall performance [72], motivating frameworks that learn agent capabilities online and orchestrate multiple agents under real-world constraints including cost, capability requirements, and operational limitations, with autonomy levels varying across implementations where some systems exhibit pronounced autonomy within designated phases, demonstrating adaptability in action management corresponding to task specificity and reaching Level 2 autonomy through contextual resource utilization [460]. 6. Evaluation The evaluation of context-engineered systems presents unprecedented challenges that transcend traditional language model assessment paradigms. These systems exhibit complex, multi-component architectures with dynamic, context-dependent behaviors requiring comprehensive evaluation frameworks that assess component-level diagnostics, task-based performance, and overall system robustness [835, 1132]. The heterogeneous nature of context engineering componentsspanning retrieval mechanisms, memory systems, reasoning chains, and multi-agent coordinationdemands evaluation methodologies that can capture both individual component effectiveness and emergent system-level behaviors [310, 931]. 6.1. Evaluation Frameworks and Methodologies This subsection presents comprehensive approaches for evaluating both individual components and integrated systems in context engineering. 6.1.1. Component-Level Assessment Intrinsic evaluation focuses on the performance of individual components in isolation, providing foundational insights into system capabilities and failure modes. For prompt engineering components, evaluation encompasses prompt effectiveness measurement through semantic similarity metrics, response quality assessment, and robustness testing across diverse input variations. Current approaches reveal brittleness and robustness challenges in prompt design, necessitating more sophisticated evaluation frameworks that can assess contextual calibration and adaptive prompt optimization [1132, 663]. Long context processing evaluation requires specialized metrics addressing information retention, positional bias, and reasoning coherence across extended sequences. The needle in haystack evaluation paradigm tests models ability to retrieve specific information embedded within long contexts, while multi-document reasoning tasks assess synthesis capabilities across multiple information sources. Position interpolation techniques and ultra-long sequence processing methods face significant computational challenges that limit practical evaluation scenarios [731, 295]. Self-contextualization mechanisms undergo evaluation through meta-learning assessments, adaptation speed measurements, and consistency analysis across multiple iterations. Self-refinement frameworks including Self-Refine, Reflexion, and N-CRITICS demonstrate substantial performance improvements, with GPT-4 achieving approximately 20% improvement through iterative self-refinement processes [735, 956, 789]. Multi-dimensional feedback mechanisms and ensemble-based evaluation approaches provide comprehensive assessment of autonomous evolution capabilities [577, 704]. Structured and relational data integration evaluation examines accuracy in knowledge graph traversal, table comprehension, and database query generation. However, current evaluation frameworks face significant limitations in assessing structural reasoning capabilities, with high-quality structured training data development presenting ongoing challenges. LSTM-based models demonstrate increased errors when sequential and structural information conflict, highlighting the need for more sophisticated benchmarks testing structural understanding [763, 668, 163]. 6.1.2. System-Level Integration Assessment Extrinsic evaluation measures end-to-end performance on downstream tasks, providing holistic assessments of system utility through comprehensive benchmarks spanning question answering, reasoning, and real-world applications. System-level evaluation must capture emergent behaviors arising from component interactions, including synergistic effects where combined components exceed individual performance and potential interference patterns where component integration degrades overall effectiveness [835, 1132]. Retrieval-Augmented Generation evaluation encompasses both retrieval quality and generation effectiveness through comprehensive metrics addressing precision, recall, relevance, and factual accuracy. Agentic RAG systems introduce additional complexity requiring evaluation of task decomposition accuracy, multi-plan selection effectiveness, and memory-augmented planning capabilities. Self-reflection mechanisms demonstrate iterative improvement through feedback loops, with MemoryBank implementations incorporating Ebbinghaus Forgetting Curve principles for enhanced memory evaluation [438, 162, 1362, 1183, 41]. Memory systems evaluation encounters substantial difficulties stemming from the absence of standardized assessment frameworks and the inherently stateless characteristics of contemporary LLMs. LongMemEval offers 500 carefully curated questions that evaluate fundamental capabilities encompassing information extraction, temporal reasoning, multi-session reasoning, and knowledge updates. Commercial AI assistants exhibit 30% accuracy degradation throughout extended interactions, underscoring significant deficiencies in memory persistence and retrieval effectiveness [1330, 1171, 457, 841, 386]. Dedicated benchmarks such as NarrativeQA, QMSum, QuALITY, and MEMENTO tackle episodic memory evaluation challenges [550, 566]. Tool-integrated reasoning systems require comprehensive evaluation covering the entire interaction trajectory, including tool selection accuracy, parameter extraction precision, execution success rates, and error recovery capabilities. The MCP-RADAR framework provides standardized evaluation employing objective metrics for software engineering and mathematical reasoning domains. Real-world evaluation reveals 46 significant performance gaps, with GPT-4 completing less than 50% of tasks in the GTA benchmark, compared to human performance of 92% [310, 1090, 126, 931]. Advanced benchmarks including BFCL (2,000 testing cases), T-Eval (553 tool-use cases), API-Bank (73 APIs, 314 dialogues), and ToolHop (995 queries, 3,912 tools) address multi-turn interactions and nested tool calling scenarios [259, 359, 373, 1255, 157, 829]. Multi-agent systems evaluation captures communication effectiveness, coordination efficiency, and collective outcome quality through specialized metrics addressing protocol adherence, task decomposition accuracy, and emergent collaborative behaviors. Contemporary orchestration frameworks including LangGraph, AutoGen, and CAMEL demonstrate insufficient transaction support, with validation limitations emerging as systems rely exclusively on LLM self-validation capabilities without independent validation procedures. Context handling failures compound challenges as agents struggle with long-term context maintenance encompassing both episodic and semantic information [128, 390, 893]. 6.2. Benchmark Datasets and Evaluation Paradigms This subsection reviews specialized benchmarks and evaluation paradigms designed for assessing context engineering system performance. 6.2.1. Foundational Component Benchmarks Long context processing evaluation employs specialized benchmark suites designed to test information retention, reasoning, and synthesis across extended sequences. Current benchmarks face significant computational complexity challenges, with O(n2) scaling limitations in attention mechanisms creating substantial memory constraints for ultra-long sequences. Position interpolation and extension techniques require sophisticated evaluation frameworks that can assess both computational efficiency and reasoning quality across varying sequence lengths [731, 295, 1227]. Advanced architectures including LongMamba and specialized position encoding methods demonstrate promising directions for long context processing, though evaluation reveals persistent challenges in maintaining coherence across extended sequences. The development of sliding attention mechanisms and memory-efficient implementations requires comprehensive benchmarks that can assess both computational tractability and task performance [1258, 347]. Structured and relational data integration benchmarks encompass diverse knowledge representation formats and reasoning patterns. However, current evaluation frameworks face limitations in assessing structural reasoning capabilities, with the development of high-quality structured training data presenting ongoing challenges. Evaluation must address the fundamental tension between sequential and structural information processing, particularly in scenarios where these information types conflict [763, 668, 163]. 6.2.2. System Implementation Benchmarks Retrieval-Augmented Generation evaluation leverages comprehensive benchmark suites addressing diverse retrieval and generation challenges. Modular RAG architectures demonstrate enhanced flexibility through specialized modules for retrieval, augmentation, and generation, enabling fine-grained evaluation of individual components and their interactions. Graph-enhanced RAG systems incorporating GraphRAG and LightRAG demonstrate improved performance in complex reasoning scenarios, though evaluation frameworks must address the additional complexity of graph traversal and multi-hop reasoning assessment [312, 965, 360]. Agentic RAG systems introduce sophisticated planning and reflection mechanisms requiring evaluation 47 of task decomposition accuracy, multi-plan selection effectiveness, and iterative refinement capabilities. Real-time and streaming RAG applications present unique evaluation challenges in assessing both latency and accuracy under dynamic information conditions [438, 162, 1183]. Tool-integrated reasoning system evaluation employs comprehensive benchmarks spanning diverse tool usage scenarios and complexity levels. The Berkeley Function Calling Leaderboard (BFCL) provides 2,000 testing cases with step-by-step and end-to-end assessments measuring call accuracy, pass rates, and win rates across increasingly complex scenarios. T-Eval contributes 553 tool-use cases testing multi-turn interactions and nested tool calling capabilities [259, 1380, 829]. Advanced benchmarks including StableToolBench address API instability challenges, while NesTools evaluates nested tool scenarios and ToolHop assesses multi-hop tool usage across 995 queries and 3,912 tools [359, 373, 1255]. Web agent evaluation frameworks including WebArena and Mind2Web provide comprehensive assessment across thousands of tasks spanning 137 websites, revealing significant performance gaps in current LLM capabilities for complex web interactions. VideoWebArena extends evaluation to multimodal agents, while Deep Research Bench and DeepShop address specialized evaluation for research and shopping agents respectively [1368, 202, 87, 476]. Multi-agent system evaluation employs specialized frameworks addressing coordination, communication, and collective intelligence. However, current frameworks face significant challenges in transactional integrity across complex workflows, with many systems lacking adequate compensation mechanisms for partial failures. Orchestration evaluation must address context management, coordination strategy effectiveness, and the ability to maintain system coherence under varying operational conditions [128, 893]. Release Date Open Source Method / Model Success Rate (%) Source 2025-02 2025-01 2024-08 2024-12 2025-01 2025-01 2024-10 2024-08 2024-10 2024-09 2024-04 2025-06 2024-04 IBM CUGA OpenAI Operator Jace.AI ScribeAgent + GPT-4o AgentSymbiotic Learn-by-Interact AgentOccam-Judge WebPilot GUI-API Hybrid Agent Agent Workflow Memory SteP TTI BrowserGym + GPT-4 61.7 58.1 57.1 53.0 52.1 48.0 45.7 37.2 35.8 35.5 33.5 26.1 23.5 [747] [807] [470] [942] [1314] [990] [1222] [1322] [980] [1135] [971] [943] [234] Table 8: WebArena [1368] Leaderboard: Top performing models with their success rates and availability status. 6.3. Evaluation Challenges and Emerging Paradigms This subsection identifies current limitations in evaluation methodologies and explores emerging approaches for more effective assessment. 48 6.3.1. Methodological Limitations and Biases Traditional evaluation metrics prove fundamentally inadequate for capturing the nuanced, dynamic behaviors exhibited by context-engineered systems. Static metrics like BLEU, ROUGE, and perplexity, originally designed for simpler text generation tasks, fail to assess complex reasoning chains, multi-step interactions, and emergent system behaviors. The inherent complexity and interdependencies of multi-component systems create attribution challenges where isolating failures and identifying root causes becomes computationally and methodologically intractable. Future metrics must evolve to capture not just task success, but the quality and robustness of the underlying reasoning process, especially in scenarios requiring compositional generalization and creative problem-solving [835, 1132]. Memory system evaluation faces particular challenges due to the lack of standardized benchmarks and the stateless nature of current LLMs. Automated memory testing frameworks must address the isolation problem where different memory testing stages cannot be effectively separated, leading to unreliable assessment results. Commercial AI assistants demonstrate significant performance degradation during sustained interactions, with accuracy drops of up to 30% highlighting critical gaps in current evaluation methodologies and pointing to the need for longitudinal evaluation frameworks that track memory fidelity over time [1330, 1171, 457]. Tool-integrated reasoning system evaluation reveals substantial performance gaps between current systems and human-level capabilities. The GAIA benchmark demonstrates that while humans achieve 92% accuracy on general assistant tasks, advanced models like GPT-4 achieve only 15% accuracy, indicating fundamental limitations in current evaluation frameworks and system capabilities [772, 1090, 126]. Evaluation frameworks must address the complexity of multi-tool coordination, error recovery, and adaptive tool selection across diverse operational contexts [310, 931]. 6.3.2. Emerging Evaluation Paradigms Self-refinement evaluation paradigms leverage iterative improvement mechanisms to assess system capabilities across multiple refinement cycles. Frameworks including Self-Refine, Reflexion, and N-CRITICS demonstrate substantial performance improvements through multi-dimensional feedback and ensemblebased evaluation approaches. GPT-4 achieves approximately 20% improvement through self-refinement processes, highlighting the importance of evaluating systems across multiple iteration cycles rather than single-shot assessments. However, key future challenge lies in evaluating the meta-learning capability itselfnot just whether the system improves, but how efficiently and robustly it learns to refine its strategies over time [735, 956, 789, 577]. Multi-aspect feedback evaluation incorporates diverse feedback dimensions including correctness, relevance, clarity, and robustness, providing comprehensive assessment of system outputs. Self-rewarding mechanisms enable autonomous evolution and meta-learning assessment, allowing systems to develop increasingly sophisticated evaluation criteria through iterative refinement [704]. Criticism-guided evaluation employs specialized critic models to provide detailed feedback on system outputs, enabling fine-grained assessment of reasoning quality, factual accuracy, and logical consistency. These approaches address the limitations of traditional metrics by providing contextual, content-aware evaluation that can adapt to diverse task requirements and output formats [789, 577]. Orchestration evaluation frameworks address the unique challenges of multi-agent coordination by incorporating transactional integrity assessment, context management evaluation, and coordination strategy effectiveness measurement. Advanced frameworks including SagaLLM provide transaction support and 49 independent validation procedures to address the limitations of systems that rely exclusively on LLM selfvalidation capabilities [128, 390]. 6.3.3. Safety and Robustness Assessment Safety-oriented evaluation incorporates comprehensive robustness testing, adversarial attack resistance, and alignment assessment to ensure responsible development of context-engineered systems. Particular attention must be paid to the evaluation of agentic systems that can operate autonomously across extended periods, as these systems present unique safety challenges that traditional evaluation frameworks cannot adequately address [965, 360]. Robustness evaluation must assess system performance under distribution shifts, input perturbations, and adversarial conditions through comprehensive stress testing protocols. Multi-agent systems face additional challenges in coordination failure scenarios, where partial system failures can cascade through the entire agent network. Evaluation frameworks must address graceful degradation strategies, error recovery protocols, and the ability to maintain system functionality under adverse conditions. Beyond predefined failure modes, future evaluation must grapple with assessing resilience to unknown unknownsemergent and unpredictable failure cascades in highly complex, autonomous multi-agent systems [128, 390]. Alignment evaluation measures system adherence to intended behaviors, value consistency, and beneficial outcome optimization through specialized assessment frameworks. Context engineering systems present unique alignment challenges due to their dynamic adaptation capabilities and complex interaction patterns across multiple components. Long-term evaluation must assess whether systems maintain beneficial behaviors as they adapt and evolve through extended operational periods [893]. Looking ahead, the evaluation of context-engineered systems requires paradigm shift from static benchmarks to dynamic, holistic assessments. Future frameworks must move beyond measuring task success to evaluating compositional generalization for novel problems and tracking long-term autonomy in interactive environments. The development of living benchmarks that co-evolve with AI capabilities, alongside the integration of socio-technical and economic metrics, will be critical for ensuring these advanced systems are not only powerful but also reliable, efficient, and aligned with human values in real-world applications [310, 1368, 1330]. The evaluation landscape for context-engineered systems continues evolving rapidly as new architectures, capabilities, and applications emerge. Future evaluation paradigms must address increasing system complexity while providing reliable, comprehensive, and actionable insights for system improvement and deployment decisions. The integration of multiple evaluation approachesfrom component-level assessment to systemwide robustness testingrepresents critical research priority for ensuring the reliable deployment of context-engineered systems in real-world applications [835, 1132]. 7. Future Directions and Open Challenges Context Engineering stands at critical inflection point where foundational advances converge with emerging application demands, creating unprecedented opportunities for innovation while revealing fundamental challenges that require sustained research efforts across multiple dimensions [835, 1132]. As the field transitions from isolated component development toward integrated system architectures, the complexity of research challenges grows exponentially, demanding interdisciplinary approaches that bridge theoretical computer science, practical system engineering, and domain-specific expertise [310, 931]. 50 This section systematically examines key research directions and open challenges that will define the evolution of Context Engineering over the coming decade. 7.1. Foundational Research Challenges This subsection examines core theoretical and computational challenges that must be addressed to advance context engineering systems beyond current limitations. 7.1.1. Theoretical Foundations and Unified Frameworks Context Engineering currently operates without unified theoretical foundations that connect disparate techniques and provide principled design guidelines, representing critical research gap that limits systematic progress and optimal system development. The absence of mathematical frameworks characterizing context engineering capabilities, limitations, and optimal design principles across different architectural configurations impedes both fundamental understanding and practical optimization [1132, 663, 835, 310]. Information-theoretic analysis of context engineering systems requires comprehensive investigation into optimal context allocation strategies, information redundancy quantification, and fundamental compression limits within context windows. Current approaches lack principled methods for determining optimal context composition, leading to suboptimal resource utilization and performance degradation. Research must establish mathematical bounds on context efficiency, develop optimization algorithms for context selection, and create theoretical frameworks for predicting system behavior across varying context configurations [731, 295]. Compositional understanding of context engineering systems demands formal models describing how individual components interact, interfere, and synergize within integrated architectures. The emergence of complex behaviors from component interactions requires systematic investigation through both empirical studies and theoretical modeling approaches. Multi-agent orchestration presents particular challenges in developing mathematical frameworks for predicting coordination effectiveness and emergent collaborative behaviors [128, 893]. 7.1.2. Scaling Laws and Computational Efficiency The fundamental asymmetry between LLMs remarkable comprehension capabilities and their pronounced generation limitations represents one of the most critical challenges in Context Engineering research. This comprehension-generation gap manifests across multiple dimensions including long-form output coherence, factual consistency maintenance, and planning sophistication, requiring investigation into whether limitations stem from architectural constraints, training methodologies, or fundamental computational boundaries [835, 1132]. Long-form generation capabilities demand systematic investigation into planning mechanisms that can maintain coherence across thousands of tokens while preserving factual accuracy and logical consistency. Current systems exhibit significant performance degradation in extended generation tasks, highlighting the need for architectural innovations beyond traditional transformer paradigms. State space models including Mamba demonstrate potential for more efficient long sequence processing through linear scaling properties, though current implementations require substantial development to match transformer performance across diverse tasks [731, 1258, 347, 216]. 51 Context scaling efficiency faces fundamental computational challenges, with current attention mechanisms scaling quadratically (O(n2)) with sequence length, creating prohibitive memory and computational requirements for ultra-long sequences. Sliding attention mechanisms and memory-efficient implementations represent promising directions, though significant research is needed to address both computational tractability and reasoning quality preservation [295, 1227, 347]. Position interpolation and extension techniques require advancement to handle sequences exceeding current architectural limitations while maintaining positional understanding and coherence. 7.1.3. Multi-Modal Integration and Representation The integration of diverse modalities within context engineering systems presents fundamental challenges in representation learning, cross-modal reasoning, and unified architectural design. Current approaches typically employ modality-specific encoders with limited cross-modal interaction, failing to capture the rich interdependencies that characterize sophisticated multi-modal understanding. VideoWebArena demonstrates the complexity of multimodal agent evaluation, revealing substantial performance gaps in current systems when processing video, audio, and text simultaneously [476]. Beyond these sensory modalities, context engineering must also handle more abstract forms of information such as graphs, whose structural semantics are not directly interpretable by language models. Capturing the high-level meaning encoded in graph structures introduces unique challenges, including aligning graph representations with language model embeddings and expressing graph topology efficiently. Recent efforts like GraphGPT [1024] and GraphRAG [244] attempt to bridge this gap through cross-modal alignment strategies, while others explore converting graphs into natural language descriptions to facilitate model understanding [262, 319]. Bi et al. [75] further propose divide-and-conquer approach to encode text-attributed heterogeneous networks, addressing context length limitations and enabling effective link prediction. Graph reasoning thus emerges as core difficulty in context engineering, requiring models to navigate complex relational structures beyond raw modalities. Temporal reasoning across multi-modal contexts requires sophisticated architectures capable of tracking object persistence, causal relationships, and temporal dynamics across extended sequences. Web agent frameworks including WebArena showcase the challenges of maintaining coherent understanding across complex multi-step interactions involving diverse modalities and dynamic content. Current systems demonstrate significant limitations in coordinating multi-modal information processing with action planning and execution [1368, 202]. Cross-modal alignment and consistency present ongoing challenges in ensuring that information extracted from different modalities remains factually consistent and semantically coherent. Deep Research Bench evaluation reveals that current multi-modal agents struggle with complex research tasks requiring synthesis across textual, visual, and structured data sources, highlighting the need for more sophisticated alignment mechanisms [87]. 7.2. Technical Innovation Opportunities This subsection explores emerging technical approaches and architectural innovations that promise to enhance context engineering capabilities. 7.2.1. Next-Generation Architectures Architectural innovations beyond traditional transformer paradigms offer promising directions for addressing current limitations in context engineering systems. State space models including LongMamba demonstrate potential for more efficient long sequence processing through linear scaling properties and improved memory utilization, though current implementations require substantial development to match transformer performance across diverse tasks [1258, 731]. Specialized position encoding methods and parameter-efficient architectures present opportunities for scaling to ultra-long sequences while maintaining computational tractability [347, 295]. Memory-augmented architectures require advancement beyond current external memory mechanisms to enable more sophisticated long-term memory organization, hierarchical memory structures, and adaptive memory management strategies. MemoryBank implementations incorporating Ebbinghaus Forgetting Curve principles demonstrate promising approaches to memory persistence, though significant research is needed to address the fundamental stateless nature of current LLMs [1362, 1330, 1171, 813, 1202]. The development of episodic memory systems capable of maintaining coherent long-term context across extended interactions represents critical architectural challenge [457, 841, 393]. Modular and compositional architectures enable flexible system construction through specialized component integration while maintaining overall system coherence. Modular RAG architectures demonstrate enhanced flexibility through specialized modules for retrieval, augmentation, and generation, enabling fine-grained optimization of individual components. Graph-enhanced approaches including GraphRAG and LightRAG showcase the potential for integrating structured knowledge representation with neural processing [312, 965, 360]. 7.2.2. Advanced Reasoning and Planning Context engineering systems require enhanced reasoning capabilities spanning causal reasoning, counterfactual thinking, temporal reasoning, and analogical reasoning across extended contexts. Current systems demonstrate limited capacity for sophisticated reasoning patterns that require integration of multiple evidence sources, consideration of alternative scenarios, and maintenance of logical consistency across complex inference chains [1132, 835]. Multi-step planning and execution capabilities represent critical advancement areas enabling systems to decompose complex tasks, formulate execution strategies, monitor progress, and adapt plans based on intermediate results. Agentic RAG systems demonstrate sophisticated planning and reflection mechanisms requiring integration of task decomposition, multi-plan selection, and iterative refinement capabilities. However, current implementations face significant challenges in maintaining coherence across extended planning horizons and adapting to dynamic information conditions [438, 162, 1183]. Tool-integrated reasoning represents paradigmatic advancement requiring dynamic interaction with external resources during reasoning processes. The GAIA benchmark demonstrates substantial performance gaps, with human achievement of 92% accuracy compared to advanced models achieving only 15%, highlighting fundamental limitations in current reasoning and planning capabilities [772, 1090, 126]. Advanced tool integration must address autonomous tool selection, parameter extraction, multi-tool coordination, and error recovery across diverse operational contexts [310, 931]. 53 7.2.3. Complex Context Organization and Solving Graph Problems Graph reasoning represents fundamental challenge in context engineering, requiring systems to navigate complex structural relationships while maintaining semantic understanding across interconnected elements. Recent advances in graph-language model integration demonstrate multiple paradigms: specialized architectural approaches that incorporate graph-specific components and text-based encoding strategies that transform graph structures into natural language representations [1085, 1023]. Architectural integration approaches include GraphGPT, which employs dual-stage instruction tuning aligning graph structural information with language tokens via self-supervised graph matching [1023, 741]. This framework introduces specialized GraphTokens refined through Graph Instruction Tuning and utilizes lightweight graph-text alignment projector for transitioning between textual and structural processing modalities [1270, 274]. Building upon instruction-tuning paradigms, GraphWiz extends this approach by incorporating DPO to enhance reasoning reliability, achieving 65% average accuracy across diverse graph tasks and significantly outperforming GPT-4s 43.8% [145]. Chain-of-thought distillation mechanisms enhance step-by-step reasoning performance [1138, 1391]. RL presents another promising direction, as demonstrated by G1, which trains LLMs on synthetic graph-theoretic tasks using the Erdős dataset comprising 50 diverse tasks, achieving strong zero-shot generalization with 3B parameter model outperforming significantly larger models [357]. Text-based encoding approaches transform graph structures into natural language descriptions using few-shot prompting and chain-of-thought reasoning without architectural modifications [262, 192]. These methods introduce diverse graph description templates contextualizing structural elements through multiple semantic interpretations [936, 716]. Recent work investigates the impact of graph description ordering on LLM performance, revealing that sequential presentation significantly influences model comprehension and reasoning accuracy [319]. Benchmark evaluations have expanded with GraphArena, offering both polynomial-time tasks and NP-complete challenges with rigorous evaluation framework that classifies outputs as correct, suboptimal, hallucinatory, or missing [1025]. Combined with existing benchmarks like NLGraph and GraphDO, these evaluations reveal substantial performance disparities between simple connectivity problems and complex tasks like maximum flow computation [1085, 895, 319]. Current implementations face challenges in scaling to large structures, maintaining consistency across multi-hop relationships, and generalizing to novel topologies, with text-based approaches offering interpretability at reduced structural precision while specialized architectures provide enhanced performance through increased complexity [889, 1100]. Emerging hybrid approaches including InstructGraph and GraphAdapter attempt to bridge these paradigms through structured format verbalizers and GNN-based adapters, though limitations persist in handling dynamic structures and temporal evolution of relationships [261]. Looking forward, broad connection paradigms that organize information through associative networks rather than fragmented searches, spreading outward from central nodes to discover potential connections between entities, may represent the next generation of RAG systems for complex context organization [131]. 7.2.4. Intelligent Context Assembly and Optimization Automated context engineering systems capable of intelligently assembling contexts from available components represent critical research frontier requiring development of context optimization algorithms, adaptive selection strategies, and learned assembly functions. Current approaches rely heavily on heuristic methods and domain-specific engineering, limiting scalability and optimality across diverse applications [1132, 663]. 54 Self-refinement mechanisms demonstrate substantial potential for intelligent context optimization through iterative improvement processes. Self-Refine, Reflexion, and N-CRITICS frameworks achieve significant performance improvements, with GPT-4 demonstrating approximately 20% improvement through iterative refinement. However, these approaches require advancement in optimization strategies for autonomous evolution and meta-learning across diverse contexts [735, 956, 789, 577]. Multi-dimensional feedback mechanisms incorporating diverse feedback dimensions including correctness, relevance, clarity, and robustness provide promising directions for context optimization. Self-rewarding mechanisms enable autonomous evolution capabilities, though research must address fundamental questions about optimal adaptation rates, stability-plasticity trade-offs, and preservation of beneficial adaptations across varying operational conditions [704]. 7.3. Application-Driven Research Directions This subsection addresses research challenges arising from real-world deployment requirements and domainspecific applications. 7.3.1. Domain Specialization and Adaptation Context engineering systems require sophisticated specialization mechanisms for diverse domains including healthcare, legal analysis, scientific research, education, and engineering applications, each presenting unique requirements for knowledge integration, reasoning patterns, safety considerations, and regulatory compliance. Domain-specific optimization demands investigation into transfer learning strategies, domain adaptation techniques, and specialized training paradigms that preserve general capabilities while enhancing domain-specific performance [1132, 663]. Scientific research applications require sophisticated reasoning capabilities over complex technical content, mathematical expressions, experimental data, and theoretical frameworks while maintaining rigorous accuracy standards. Deep Research Bench evaluation reveals significant challenges in current systems ability to conduct complex research tasks requiring synthesis across multiple information sources and reasoning over technical content. Research must address integration of symbolic reasoning with neural approaches and incorporation of domain-specific knowledge bases [87]. Healthcare applications demand comprehensive safety evaluation frameworks, regulatory compliance mechanisms, privacy protection protocols, and integration with existing clinical workflows while maintaining interpretability and auditability requirements. Medical context engineering must address challenges in handling sensitive information, ensuring clinical accuracy, supporting diagnostic reasoning, and maintaining patient privacy across complex healthcare ecosystems. Current evaluation frameworks reveal substantial gaps in medical reasoning capabilities and safety assessment methodologies [386]. 7.3.2. Large-Scale Multi-Agent Coordination Scaling multi-agent context engineering systems to hundreds or thousands of participating agents requires development of distributed coordination mechanisms, efficient communication protocols, and hierarchical management structures that maintain system coherence while enabling local autonomy. Research must address fundamental challenges in distributed consensus, fault tolerance, and emergent behavior prediction in large-scale agent populations [239, 140]. Communication protocol standardization represents critical research frontier, with emerging protocols 55 including MCP (USB-C for AI), A2A (Agent-to-Agent), ACP (Agent Communication Protocol), and ANP (Agent Network Protocol) demonstrating the need for unified frameworks enabling interoperability across diverse agent ecosystems. However, current implementations face security vulnerabilities and scalability limitations that must be addressed for large-scale deployment [37, 1007, 462, 1, 246, 926, 616]. Orchestration challenges including transactional integrity, context management, and coordination strategy effectiveness represent significant obstacles to large-scale multi-agent deployment. Contemporary frameworks including LangGraph, AutoGen, and CAMEL demonstrate insufficient transaction support and validation limitations, requiring systems that rely exclusively on LLM self-validation capabilities. Advanced coordination frameworks must address compensation mechanisms for partial failures and maintain system coherence under varying operational conditions [128, 390, 893]. 7.3.3. Human-AI Collaboration and Integration Sophisticated human-AI collaboration frameworks require deep understanding of human cognitive processes, communication preferences, trust dynamics, and collaboration patterns to enable effective hybrid teams that leverage complementary strengths. Research must investigate optimal task allocation strategies, communication protocols, and shared mental model development between humans and AI systems [1132, 835]. Web agent evaluation frameworks reveal significant challenges in human-AI collaboration, particularly in complex task scenarios requiring sustained interaction and coordination. WebArena and Mind2Web demonstrate that current systems struggle with multi-step interactions across diverse websites, highlighting fundamental gaps in collaborative task execution. Advanced interfaces require investigation into contextaware adaptation and personalization mechanisms that enhance human-AI team performance [1368, 202]. Trust calibration and transparency mechanisms represent critical research areas for ensuring appropriate human reliance on AI systems while maintaining human agency and decision-making authority. Evaluation frameworks must address explanation generation, uncertainty communication, and confidence calibration to support informed human decision-making in collaborative scenarios. The substantial performance gaps revealed by benchmarks like GAIA underscore the importance of developing systems that can effectively communicate their limitations and capabilities [772, 1090]. 7.4. Deployment and Societal Impact Considerations This subsection examines critical considerations for deploying context engineering systems at scale while ensuring responsible and beneficial outcomes. 7.4.1. Scalability and Production Deployment Production deployment of context engineering systems requires addressing scalability challenges across multiple dimensions including computational resource management, latency optimization, throughput maximization, and cost efficiency while maintaining consistent performance across diverse operational conditions. The O(n2) scaling limitation of current attention mechanisms creates substantial barriers to deploying ultra-long context systems in production environments, necessitating advancement in memoryefficient architectures and sliding attention mechanisms [295, 1227]. Reliability and fault tolerance mechanisms become critical as context engineering systems assume increasingly important roles in decision-making processes across domains. Multi-agent orchestration frameworks 56 face particular challenges in maintaining transactional integrity across complex workflows, with current systems lacking adequate compensation mechanisms for partial failures. Research must address graceful degradation strategies, error recovery protocols, and redundancy mechanisms that maintain system functionality under adverse conditions [128, 390]. Maintainability and evolution challenges require investigation into system versioning, backward compatibility, continuous integration protocols, and automated testing frameworks that support ongoing system improvement without disrupting deployed services. Memory system implementations face additional challenges due to the stateless nature of current LLMs and the lack of standardized benchmarks for long-term memory persistence and retrieval efficiency [1330, 1171]. 7.4.2. Safety, Security, and Robustness Comprehensive safety evaluation requires development of assessment frameworks that can identify potential failure modes, safety violations, and unintended behaviors across the full spectrum of context engineering system capabilities. Agentic systems present particular safety challenges due to their autonomous operation capabilities and complex interaction patterns across extended operational periods [965, 360]. Security considerations encompass protection against adversarial attacks, data poisoning, prompt injection, model extraction, and privacy violations while maintaining system functionality and usability. Multi-agent communication protocols including MCP, A2A, and ACP introduce security vulnerabilities that must be addressed while preserving interoperability and functionality. Research must develop defense mechanisms and detection systems that address evolving threat landscapes across distributed agent networks [246, 926]. Alignment and value specification challenges require investigation into methods for ensuring context engineering systems behave according to intended objectives while avoiding specification gaming, reward hacking, and goal misalignment. Context engineering systems present unique alignment challenges due to their dynamic adaptation capabilities and complex interaction patterns across multiple components. The substantial performance gaps revealed by evaluation frameworks underscore the importance of developing robust alignment mechanisms that can maintain beneficial behaviors as systems evolve and adapt [772, 128]. 7.4.3. Ethical Considerations and Responsible Development Bias mitigation and fairness evaluation require comprehensive assessment frameworks that can identify and address systematic biases across different demographic groups, application domains, and use cases while maintaining system performance and utility. Research must investigate bias sources in training data, model architectures, and deployment contexts while developing mitigation strategies that address root causes rather than symptoms [1132, 835]. Privacy protection mechanisms must address challenges in handling sensitive information, preventing data leakage, and maintaining user privacy while enabling beneficial system capabilities. Memory systems face particular privacy challenges due to their persistent information storage and retrieval capabilities, requiring advanced frameworks for secure memory management and selective forgetting mechanisms [1330, 457]. Transparency and accountability frameworks require development of explanation systems, audit mechanisms, and governance structures that enable responsible oversight of context engineering systems while supporting innovation and beneficial applications. The substantial performance gaps revealed by evaluation frameworks like GAIA (human 92% vs AI 15%) highlight the importance of transparent capability communication and appropriate expectation setting for deployed systems [772, 1090]. 57 The future of Context Engineering will be shaped by our ability to address these interconnected challenges through sustained, collaborative research efforts that bridge technical innovation with societal considerations. Success will require continued investment in fundamental research, interdisciplinary collaboration, and responsible development practices that ensure context engineering systems remain beneficial, reliable, and aligned with human values as they become increasingly integrated into critical societal functions [835, 1132, 310]. 8. Conclusion This survey has presented the first comprehensive examination of Context Engineering as formal discipline that systematically designs, optimizes, and manages information payloads for LLMs. Through our analysis of over 1400 research papers, we have established Context Engineering as critical foundation for developing sophisticated AI systems that effectively integrate external knowledge, maintain persistent memory, and interact dynamically with complex environments. Our primary contribution lies in introducing unified taxonomic framework that organizes context engineering techniques into Foundational Components (Context Retrieval and Generation, Context Processing, and Context Management) and System Implementations (Retrieval-Augmented Generation, Memory Systems, Tool-Integrated Reasoning, and Multi-Agent Systems). This framework demonstrates how core technical capabilities integrate into sophisticated architectures addressing real-world requirements. Through this systematic examination, we have identified several key insights. First, we observe fundamental asymmetry between LLMs remarkable capabilities in understanding complex contexts and their limitations in generating equally sophisticated outputs. This comprehension-generation gap represents one of the most critical challenges facing the field. Second, our analysis reveals increasingly sophisticated integration patterns where multiple techniques combine synergistically, creating capabilities that exceed their individual components. Third, we observe clear trend toward modularity and compositionality, enabling flexible architectures adaptable to diverse applications while maintaining system coherence. The evaluation challenges we identified underscore the need for comprehensive assessment frameworks that capture the complex, dynamic behaviors exhibited by context-engineered systems. Traditional evaluation methodologies prove insufficient for systems that integrate multiple components, exhibit adaptive behaviors, and operate across extended time horizons. Our examination of future research directions reveals significant opportunities including developing next-generation architectures for efficient long context handling, creating intelligent context assembly systems, and advancing multi-agent coordination mechanisms. Key challenges span theoretical foundations, technical implementation, and practical deployment, including the lack of unified theoretical frameworks, scaling limitations, and safety considerations. Looking toward the future, Context Engineering stands poised to play an increasingly central role in AI development as the field moves toward complex, multi-component systems. The interdisciplinary nature of Context Engineering necessitates collaborative research approaches spanning computer science, cognitive science, linguistics, and domain-specific expertise. As LLMs continue to evolve, the fundamental insight underlying Context Engineeringthat AI system performance is fundamentally determined by contextual informationwill remain central to artificial intelligence development. This survey provides both comprehensive snapshot of the current state and roadmap for future research, establishing Context Engineering as distinct discipline with its own principles, methodologies, and challenges to foster innovation and support responsible development of context-aware AI systems."
        },
        {
            "title": "Acknowledgments",
            "content": "This survey represents an ongoing effort to comprehensively map the rapidly evolving landscape of Context Engineering for Large Language Models. Given the dynamic nature of this field, with new developments emerging continuously, we acknowledge that despite our best efforts, some recent works or emerging trends may have been inadvertently overlooked or underrepresented. We welcome feedback from the research community to help improve future iterations of this work. We are grateful to the broader research community whose foundational contributions have made this survey possible. This work would not have been achievable without the invaluable support of both the research community and the open-source community, whose collaborative efforts in developing frameworks, tools, and resources have significantly advanced the field of Context Engineering."
        },
        {
            "title": "References",
            "content": "[1] Anp-agent communication meta-protocol specification(draft). agent-network-protocol.com/specs/communication.html. July-2025]. https:// [Online; accessed 17- [2] S. A. Automating human evaluation of dialogue systems. North American Chapter of the Association for Computational Linguistics, 2022. [3] Samir Abdaljalil, Hasan Kurban, Khalid A. Qaraqe, and E. Serpedin. Theorem-of-thought: multiagent framework for abductive, deductive, and inductive reasoning in language models. arXiv preprint, 2025. [4] Abdelrahman Abdallah, Bhawna Piryani, Jamshid Mozafari, Mohammed Ali, and Adam Jatowt. Rankify: comprehensive python toolkit for retrieval, re-ranking, and retrieval-augmented generation, arXiv preprint arXiv:2502.02464, 2025. URL https://arxiv.org/abs/2502.02464v3. [5] Ibrahim Abdelaziz, Kinjal Basu, Mayank Agarwal, Sadhana Kumaravel, Matt Stallone, Rameswar Panda, Yara Rizk, G. Bhargav, M. Crouse, Chulaka Gunasekara, S. Ikbal, Sachin Joshi, Hima P. Karanam, Vineet Kumar, Asim Munawar, S. Neelam, Dinesh Raghu, Udit Sharma, Adriana Meza Soria, Dheeraj Sreedhar, P. Venkateswaran, Merve Unuvar, David Cox, S. Roukos, Luis A. Lastras, and P. Kapanipathi. Granite-function calling model: Introducing function calling abilities via multi-task learning of granular tasks. Conference on Empirical Methods in Natural Language Processing, 2024. [6] D. Acharya, Karthigeyan Kuppan, and Divya Bhaskaracharya. Agentic ai: Autonomous intelligence for complex goalsa comprehensive survey. IEEE Access, 2025. [7] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting questions. AAAI Conference on Artificial Intelligence, 2018. [8] Shantanu Acharya, Fei Jia, and Boris Ginsburg. Star attention: Efficient llm inference over long sequences, arXiv preprint arXiv:2411.17116, 2024. URL https://arxiv.org/abs/2411. 17116v3. [9] Emre Can Acikgoz, Jeremy Greer, Akul Datta, Ze Yang, William Zeng, Oussama Elachqar, Emmanouil Koukoumidis, Dilek Hakkani-Tur, and Gokhan Tur. Can single model master both multi-turn conversations and tool use? coalm: unified conversational agentic language model, arXiv preprint arXiv:2502.08820, 2025. URL https://arxiv.org/abs/2502.08820v3. [10] Emre Can Acikgoz, Cheng Qian, Hongru Wang, Vardhan Dongre, Xiusi Chen, Heng Ji, Dilek HakkaniTur, and Gokhan Tur. desideratum for conversational agents: Capabilities, challenges, and future directions, arXiv preprint arXiv:2504.16939, 2025. URL https://arxiv.org/abs/2504. 16939v1. [11] Anum Afzal, Juraj Vladika, Gentrit Fazlija, Andrei Staradubets, and Florian Matthes. Towards optimizing retrieval augmented generation using large language model on academic data. International Conference on Natural Language Processing and Information Retrieval, 2024. [12] Ankush Agarwal, Sakharam Gawade, A. Azad, and P. Bhattacharyya. Kitlm: Domain-specific knowledge integration into language models for question answering. ICON, 2023. [13] Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Large scale knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training. arXiv preprint, 2020. [14] Monica Agrawal, S. Hegselmann, Hunter Lang, Yoon Kim, and D. Sontag. Large language models are few-shot clinical information extractors. Conference on Empirical Methods in Natural Language Processing, 2022. [15] Arash Ahmadi, S. Sharif, and Yaser Mohammadi Banadaki. Mcp bridge: lightweight, llm-agnostic restful proxy for model context protocol servers, arXiv preprint arXiv:2504.08999, 2025. URL https://arxiv.org/abs/2504.08999v1. [16] J. Ainslie, J. Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit K. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. Conference on Empirical Methods in Natural Language Processing, 2023. [17] Adel Al-Jumaily. Multi-agent system concepts theory and application phases. arXiv preprint, 2006. [18] Faisal Al-Khateeb, Nolan Dey, Daria Soboleva, and Joel Hestness. Position interpolation improves alibi extrapolation. arXiv preprint, 2023. [19] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, A. Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, O. Vinyals, Andrew Zisserman, and K. Simonyan. Flamingo: visual language model for few-shot learning. Neural Information Processing Systems, 2022. [20] Stefano V. Albrecht and P. Stone. Autonomous agents modelling other agents: comprehensive survey and open problems. Artificial Intelligence, 2017. [21] Buthayna AlMulla, Maram Assi, and Safwat Hassan. Understanding the challenges and promises of developing generative ai apps: An empirical study, arXiv preprint arXiv:2506.16453, 2025. URL https://arxiv.org/abs/2506.16453v2. [22] Reem S. Alsuhaibani, Christian D. Newman, M. J. Decker, Michael L. Collard, and Jonathan I. Maletic. On the naming of methods: survey of professional developers. International Conference on Software Engineering, 2021. 60 [23] Francesco Alzetta, P. Giorgini, A. Najjar, M. Schumacher, and Davide Calvaresi. In-time explainability in multi-agent systems: Challenges, opportunities, and roadmap. EXTRAAMAS@AAMAS, 2020. [24] Kenza Amara, Lukas Klein, Carsten T. Lüth, Paul F. Jäger, Hendrik Strobelt, and Mennatallah ElAssady. Why context matters in vqa and reasoning: Semantic interventions for vlm input modalities, arXiv preprint arXiv:2410.01690v1, 2024. URL https://arxiv.org/abs/2410.01690v1. [25] Xavier Amatriain. Prompt design and engineering: Introduction and advanced methods, arXiv preprint arXiv:2401.14423, 2024. URL https://arxiv.org/abs/2401.14423v4. [26] Zahra Aminiranjbar, Jianan Tang, Qiudan Wang, Shubha Pant, and Mahesh Viswanathan. Dawn: Designing distributed agents in worldwide network, arXiv preprint arXiv:2410.22339, 2024. URL https://arxiv.org/abs/2410.22339v3. [27] Chenxin An, Jun Zhang, Ming Zhong, Lei Li, Shansan Gong, Yao Luo, Jingjing Xu, and Lingpeng Kong. Why does the effective context length of llms fall short? International Conference on Learning Representations, 2024. [28] Kaikai An, Fangkai Yang, Liqun Li, Junting Lu, Sitao Cheng, Shuzheng Si, Lu Wang, Pu Zhao, Lele Cao, Qingwei Lin, et al. Thread: logic-based data organization paradigm for how-to question answering with retrieval augmented generation. arXiv preprint arXiv:2406.13372, 2024. [29] Kaikai An, Fangkai Yang, Junting Lu, Liqun Li, Zhixing Ren, Hao Huang, Lu Wang, Pu Zhao, Yu Kang, In Hua Ding, et al. Nissist: An incident mitigation copilot based on troubleshooting guides. Proceedings of the 27th European Conference on Artificial Intelligence (ECAI 2024), pages 44714474, 2024. [30] Kaikai An, Li Sheng, Ganqu Cui, Shuzheng Si, Ning Ding, Yu Cheng, and Baobao Chang. Ultraif: Advancing instruction following from the wild. pages 79307957, 2025. [31] Sumin An, Junyoung Sung, Wonpyo Park, Chanjun Park, and Paul Hongsuck Seo. Lcirc: recurrent compression approach for efficient long-form context and query dependent modeling in llms. North American Chapter of the Association for Computational Linguistics, 2025. [32] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurélien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. Neural Information Processing Systems, 2023. [33] John R. Anderson, M. Matessa, and C. Lebiere. Act-r: theory of higher level cognition and its relation to visual attention. Hum. Comput. Interact., 1997. [34] Jacob Andreas. Language models as agent models. Conference on Empirical Methods in Natural Language Processing, 2022. [35] Leonardo Aniello, R. Baldoni, and Leonardo Querzoni. Adaptive online scheduling in storm. Distributed Event-Based Systems, 2013. [36] Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, M. Burtsev, and Evgeny Burnaev. Arigraph: Learning knowledge graph world models with episodic memory for llm agents, arXiv preprint arXiv:2407.04363, 2024. URL https://arxiv.org/abs/2407.04363v3. 61 [37] Anthropic. Introducing the model context protocol, November 2024. URL https://www. anthropic.com/news/model-context-protocol. [Online; accessed 17-July-2025]. [38] RM Aratchige and Dr. Wmks Ilmini. Llms working in harmony: survey on the technological aspects of building effective llm-based multi agent systems, arXiv preprint arXiv:2504.01963, 2025. URL https://arxiv.org/abs/2504.01963v1. [39] Leo Ardon, Daniel Furelos-Blanco, and A. Russo. Learning reward machines in cooperative multiagent tasks. AAMAS Workshops, 2023. [40] K. Armeni, C. Honey, and Tal Linzen. Characterizing verbatim short-term memory in neural language models. Conference on Computational Natural Language Learning, 2022. [41] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning International Conference on Learning to retrieve, generate, and critique through self-reflection. Representations, 2023. [42] Hikaru Asano, Tadashi Kozuno, and Yukino Baba. Self iterative label refinement via robust unlabeled learning, arXiv preprint arXiv:2502.12565, 2025. URL https://arxiv.org/abs/2502. 12565v1. [43] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Jiacheng Guo, Liangfu Chen, Parminder Bhatia, Ramesh Nallapati, Sudipta Sengupta, and Bing Xiang. Bifurcated attention: Accelerating massively parallel decoding with shared prefixes in llms, arXiv preprint arXiv:2403.08845, 2024. URL https://arxiv.org/abs/ 2403.08845v2. [44] Avinash Ayalasomayajula, Rui Guo, Jingbo Zhou, Sujan Kumar Saha, and Farimah Farahmandi. Lasp: Llm assisted security property generation for soc verification. Workshop on Machine Learning for CAD, 2024. [45] Simon A. Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint, 2025. [46] Bobby Azad, Reza Azad, Sania Eskandari, Afshin Bozorgpour, A. Kazerouni, I. Rekik, and D. Merhof. Foundational models in medical imaging: comprehensive survey and future vision, arXiv preprint arXiv:2310.18689, 2023. URL https://arxiv.org/abs/2310.18689v1. [47] Gilbert Badaro, Mohammed Saeed, and Paolo Papotti. Transformers for tabular data representation: survey of models and applications. Transactions of the Association for Computational Linguistics, 2023. [48] Jinheon Baek, N. Chandrasekaran, Silviu Cucerzan, Allen Herring, and S. Jauhar. Knowledgeaugmented large language models for personalized contextual query suggestion. The Web Conference, 2023. [49] Tianyi Bai, Hao Liang, Binwang Wan, Ling Yang, Bozhou Li, Yifan Wang, Bin Cui, Conghui He, Binhang Yuan, and Wentao Zhang. survey of multimodal large language model from data-centric perspective, arXiv preprint arXiv:2405.16640v2, 2024. URL https://arxiv.org/abs/2405. 16640v2. 62 [50] Yu Bai, Xiyuan Zou, Heyan Huang, Sanxing Chen, Marc-Antoine Rondeau, Yang Gao, and Jackie Chi Kit Cheung. Citrus: Chunked instruction-aware state eviction for long sequence modeling. Conference on Empirical Methods in Natural Language Processing, 2024. [51] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback, arXiv preprint arXiv:2212.08073, 2022. URL https://arxiv.org/abs/2212.08073. [52] Souhail Bakkali, Sanket Biswas, Zuheng Ming, Mickaël Coustaty, Marccal Rusinol, O. R. Terrades, and Josep Llados. Globaldoc: cross-modal vision-language framework for real-world document image retrieval and classification. IEEE Workshop/Winter Conference on Applications of Computer Vision, 2023. [53] Jayachandu Bandlamudi, K. Mukherjee, Prerna Agarwal, Sampath Dechu, Siyu Huo, Vatche Isahagian, Vinod Muthusamy, N. Purushothaman, and Renuka Sindhgatta. Towards hybrid automation by bootstrapping conversational interfaces for it operation tasks. AAAI Conference on Artificial Intelligence, 2023. [54] Jayachandu Bandlamudi, Kushal Mukherjee, Prerna Agarwal, Ritwik Chaudhuri, R. Pimplikar, Sampath Dechu, Alex Straley, Anbumunee Ponniah, and Renuka Sindhgatta. Building conversational artifacts to enable digital assistant for apis and rpas. AAAI Conference on Artificial Intelligence, 2024. [55] Keqin Bao, Jizhi Zhang, Xinyu Lin, Yang Zhang, Wenjie Wang, and Fuli Feng. Large language models for recommendation: Past, present, and future. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024. [56] Sara Di Bartolomeo, Giorgio Severi, V. Schetinger, and Cody Dunne. Ask and you shall receive (a graph drawing): Testing chatgpts potential to apply graph layout algorithms. Eurographics Conference on Visualization, 2023. [57] Saikat Barua. Exploring autonomous agents through the lens of large language models: review, arXiv preprint arXiv:2404.04442, 2024. URL https://arxiv.org/abs/2404.04442v1. [58] Kinjal Basu, Ibrahim Abdelaziz, Kelsey Bradford, M. Crouse, Kiran Kate, Sadhana Kumaravel, Saurabh Goyal, Asim Munawar, Yara Rizk, Xin Wang, Luis A. Lastras, and P. Kapanipathi. Nestful: benchmark for evaluating llms on nested sequences of api calls, arXiv preprint arXiv:2409.03797, 2024. URL https://arxiv.org/abs/2409.03797v3. [59] Amin Beheshti. Natural language-oriented programming (nlop): Towards democratizing software creation. 2024 IEEE International Conference on Software Services Engineering (SSE), 2024. [60] Azadeh Beiranvand and S. M. Vahidipour. Integrating structural and semantic signals in textattributed graphs with bigtex, arXiv preprint arXiv:2504.12474, 2025. URL https://arxiv.org/ abs/2504.12474v2. 63 [61] Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba. International Conference on Learning Representations, 2024. [62] Assaf Ben-Kish, Itamar Zimerman, M. J. Mirza, James R. Glass, Leonid Karlinsky, and Raja Giryes. Overflow prevention enhances long-context recurrent llms. arXiv preprint, 2025. [63] M. Benna and Stefano Fusi. Complex synapses as efficient memory systems. BMC Neuroscience, 2015. [64] M. Benna and Stefano Fusi. Computational principles of biological memory, arXiv preprint arXiv:1507.07580, 2015. URL https://arxiv.org/abs/1507.07580v1. [65] Shelly Bensal, Umar Jamil, Christopher Bryant, M. Russak, Kiran Kamble, Dmytro Mozolevskyi, Muayad Ali, and Waseem Alshikh. Reflect, retry, reward: Self-improving llms via reinforcement learning, arXiv preprint arXiv:2505.24726, 2025. URL https://arxiv.org/abs/2505.24726v1. [66] Idoia Berges, J. Bermúdez, A. Goñi, and A. Illarramendi. Semantic web technology for agent communication protocols. Extended Semantic Web Conference, 2008. [67] Gaurav Beri and Vaishnavi Srivastava. Advanced techniques in prompt engineering for large language models: comprehensive study. 2024 IEEE 4th International Conference on ICT in Business Industry & Government (ICTBIG), 2024. [68] Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R. Gormley. Unlimiformer: Long-range transformers with unlimited length input. Neural Information Processing Systems, 2023. [69] Maciej Besta, Nils Blach, Aleš Kubíček, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, H. Niewiadomski, P. Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems with large language models. AAAI Conference on Artificial Intelligence, 2023. [70] Gregor Betz and Kyle Richardson. Judgment aggregation, discursive dilemma and reflective equilibrium: Neural language models as self-improving doxastic agents. Frontiers in Artificial Intelligence, 2022. [71] L. Bezalel, Eyal Orgad, and Amir Globerson. Teaching models to improve on tape. AAAI Conference on Artificial Intelligence, 2024. [72] Umang Bhatt, Sanyam Kapoor, Mihir Upadhyay, Ilia Sucholutsky, Francesco Quinzan, Katherine M. Collins, Adrian Weller, Andrew Gordon Wilson, and Muhammad Bilal Zafar. When should we orchestrate multiple agents?, arXiv preprint arXiv:2503.13577, 2025. URL https://arxiv.org/ abs/2503.13577v1. [73] Baolong Bi, Shaohan Huang, Yiwei Wang, Tianchi Yang, Zihan Zhang, Haizhen Huang, Lingrui Mei, Junfeng Fang, Zehao Li, Furu Wei, et al. Context-dpo: Aligning language models for contextfaithfulness. ACL 2025, 2024. [74] Baolong Bi, Shenghua Liu, Lingrui Mei, Yiwei Wang, Pengliang Ji, and Xueqi Cheng. Decoding by contrasting knowledge: Enhancing llms confidence on edited facts. ACL 2025, 2024. 64 [75] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, and Xueqi Cheng. Lpnl: Scalable link prediction with large language models. ACL 2024, 2024. [76] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Junfeng Fang, and Xueqi Cheng. Struedit: Structured outputs enable the fast and accurate knowledge editing for large language models. 2024. [77] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Hongcheng Gao, Yilong Xu, and Xueqi Cheng. Adaptive token biaser: Knowledge editing via biasing key entities. EMNLP 2024, 2024. [78] Baolong Bi, Shenghua Liu, Xingzhang Ren, Dayiheng Liu, Junyang Lin, Yiwei Wang, Lingrui Mei, Junfeng Fang, Jiafeng Guo, and Xueqi Cheng. Refinex: Learning to refine pre-training data at scale from expert-guided programs. 2025. [79] Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei, Junfeng Fang, Hongcheng Gao, Shiyu Ni, and Xueqi Cheng. Is factuality enhancement free lunch for llms? better factuality can lead to worse context-faithfulness. ICLR 2025, 2025. [80] Baolong Bi, Shenghua Liu, Yiwei Wang, Yilong Xu, Junfeng Fang, Lingrui Mei, and Xueqi Cheng. Parameters vs. context: Fine-grained control of knowledge reliance in language models. 2025. [81] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation. Conference on Empirical Methods in Natural Language Processing, 2020. [82] Dinh Doan Van Bien, David Lillis, and Rem W. Collier. Call graph profiling for multi agent systems. Multi-Agent Logics, Languages, and Organisations Federated Workshops, 2009. [83] Jonas Bode, Bastian Pätzold, Raphael Memmesheimer, and Sven Behnke. comparison of prompt engineering techniques for task planning and execution in service robotics. IEEE-RAS International Conference on Humanoid Robots, 2024. [84] P. Bonzon. Grounding mental representations in virtual multi-level functional framework. Journal of Cognition, 2023. [85] Sebastian Borgeaud, A. Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, T. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, G. Irving, O. Vinyals, Simon Osindero, K. Simonyan, Jack W. Rae, Erich Elsen, and L. Sifre. Improving language models by retrieving from trillions of tokens. International Conference on Machine Learning, 2021. [86] Zalán Borsos, Raphaël Marinier, Damien Vincent, E. Kharitonov, O. Pietquin, Matthew Sharifi, Dominik Roblek, O. Teboul, David Grangier, M. Tagliasacchi, and Neil Zeghidour. Audiolm: language modeling approach to audio generation. IEEE/ACM Transactions on Audio Speech and Language Processing, 2022. [87] FutureSearch Nikos I. Bosse, Jon Evans, Robert G. Gambee, Daniel Hnyk, Peter Muhlbacher, Lawrence Phillips, Dan Schwarz, and Jack Wildman. Deep research bench: Evaluating ai web research agents, arXiv preprint arXiv:2506.06287, 2025. URL https://arxiv.org/abs/2506.06287v1. 65 [88] Vicent Botti. Agentic ai and multiagentic: Are we reinventing the wheel?, arXiv preprint arXiv:2506.01463, 2025. URL https://arxiv.org/abs/2506.01463v1. [89] William Brach, Kristián Kostál, and Michal Ries. The effectiveness of large language models in transforming unstructured text to standardized formats. IEEE Access, 2025. [90] C. Brainerd, C. F. Gomes, and K. Nakamura. Dual recollection in episodic memory. Journal of experimental psychology. General, 2015. [91] Inês Bramão, Jiefeng Jiang, A. Wagner, and M. Johansson. Encoding contexts are incidentally reinstated during competitive retrieval and track the temporal dynamics of memory interference. Cerebral Cortex, 2022. [92] Andrés Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D. White, and P. Schwaller. Augmenting large language models with chemistry tools. Nat. Mac. Intell., 2023. [93] Maricela Claudia Bravo and Martha Coronel. Aligning agent communication protocols - pragmatic approach. International Conference on Software and Data Technologies, 2008. [94] F. Brazier, B. Dunin-Keplicz, N. Jennings, and Jan Treur. Desire: Modelling multi-agent systems in compositional formal framework. International Journal of Cooperative Information Systems, 1997. [95] Lorenz Brehme, Thomas Ströhle, and Ruth Breu. Can llms be trusted for evaluating rag systems? survey of methods and datasets, arXiv preprint arXiv:2504.20119, 2025. URL https://arxiv. org/abs/2504.20119v2. [96] R. Breil, D. Delahaye, Laurent Lapasset, and E. Feron. Multi-agent systems to help managing air traffic structure. arXiv preprint, 2017. [97] Alexander Brinkmann and Christian Bizer. Self-refinement strategies for llm-based product attribute value extraction. Datenbanksysteme für Business, Technologie und Web, 2025. [98] D. Britz, M. Guan, and Minh-Thang Luong. Efficient attention using fixed-size memory representation. Conference on Empirical Methods in Natural Language Processing, 2017. [99] Adam W. Broitman and M. J. Kahana. Neural context reinstatement of recurring events. bioRxiv, 2024. [100] Ethan A. Brooks, Logan Walls, Richard L. Lewis, and Satinder Singh. Large language models can implement policy iteration. Neural Information Processing Systems, 2022. [101] Rodney A. Brooks. robust layered control system for mobile robot. IEEE J. Robotics Autom., 1986. [102] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing instructions. Computer Vision and Pattern Recognition, 2022. [103] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. Henighan, R. Child, A. Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Ma teusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, I. Sutskever, and Dario Amodei. Language models are few-shot learners. Neural Information Processing Systems, 2020. [104] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, arXiv preprint arXiv:2005.14165, 2020. URL https://arxiv.org/abs/2005.14165. [105] Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, L. Baraldi, and R. Cucchiara. Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2024. [106] Joyce Cahoon, Prerna Singh, Nick Litombe, Jonathan Larson, Ha Trinh, Yiwen Zhu, Andreas Mueller, Fotis Psallidas, and Carlo Curino. Optimizing open-domain question answering with graph-based retrieval augmented generation, arXiv preprint arXiv:2503.02922, 2025. URL https://arxiv. org/abs/2503.02922v1. [107] Hongru Cai, Yongqi Li, Wenjie Wang, Fengbin Zhu, Xiaoyu Shen, Wenjie Li, and Tat-Seng Chua. Large language models empowered personalized web agents. The Web Conference, 2024. [108] Yujun Cai, Liuhao Ge, Jianfei Cai, and Junsong Yuan. Weakly-supervised 3d hand pose estimation from monocular rgb images. In Proceedings of the European conference on computer vision (ECCV), pages 666682, 2018. [109] Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan, and Nadia Magnenat Thalmann. Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2272 2281, 2019. [110] Yujun Cai, Liuhao Ge, Jianfei Cai, Nadia Magnenat Thalmann, and Junsong Yuan. 3d hand pose estimation using synthetic data and weakly labeled rgb images. IEEE transactions on pattern analysis and machine intelligence, 43(11):37393753, 2020. [111] Yujun Cai, Lin Huang, Yiwei Wang, Tat-Jen Cham, Jianfei Cai, Junsong Yuan, Jun Liu, Xu Yang, Yiheng Zhu, Xiaohui Shen, et al. Learning progressive joint propagation for human motion prediction. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VII 16, pages 226242. Springer International Publishing, 2020. [112] Yujun Cai, Yiwei Wang, Yiheng Zhu, Tat-Jen Cham, Jianfei Cai, Junsong Yuan, Jun Liu, Chuanxia Zheng, Sijie Yan, Henghui Ding, et al. unified 3d human motion synthesis model via conditional variational auto-encoder. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1164511655, 2021. [113] V. Camos and P. Barrouillet. Attentional and non-attentional systems in the maintenance of verbal information in working memory: the executive and phonological loops. Frontiers in Human Neuroscience, 2014. [114] Boxi Cao, Qiaoyu Tang, Hongyu Lin, Xianpei Han, Jiawei Chen, Tianshu Wang, and Le Sun. Retentive or forgetful? diving into the knowledge memorizing mechanism of language models. International Conference on Language Resources and Evaluation, 2023. 67 [115] He Cao, Zhenwei An, Jiazhan Feng, Kun Xu, Liwei Chen, and Dongyan Zhao. step closer to comprehensive answers: Constrained multi-stage question decomposition with large language models, arXiv preprint arXiv:2311.07491, 2023. URL https://arxiv.org/abs/2311.07491v1. [116] Nicola De Cao, Wilker Aziz, and Ivan Titov. Question answering by reasoning across documents with graph convolutional networks. North American Chapter of the Association for Computational Linguistics, 2018. [117] Pengfei Cao, Tianyi Men, Wencan Liu, Jingwen Zhang, Xuzhao Li, Xixun Lin, Dianbo Sui, Yanan Cao, Kang Liu, and Jun Zhao. Large language models for planning: comprehensive and systematic survey, arXiv preprint arXiv:2505.19683, 2025. URL https://arxiv.org/abs/2505.19683v1. [118] Yongcan Cao, Wenwu Yu, W. Ren, and Guanrong Chen. An overview of recent progress in the study of distributed multi-agent coordination. IEEE Transactions on Industrial Informatics, 2012. [119] Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, and Yunjie Li. Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods. IEEE Transactions on Neural Networks and Learning Systems, 2024. [120] Yukun Cao, Zengyi Gao, Zhiyang Li, Xike Xie, and S. K. Zhou. Lego-graphrag: Modularizing graph-based retrieval-augmented generation for design space exploration. arXiv preprint, 2024. [121] R. C. Cardoso and Angelo Ferrando. review of agent-based programming for multi-agent systems. De Computis, 2021. [122] Nicholas Carlini, Chang Liu, Ú. Erlingsson, Jernej Kos, and D. Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. USENIX Security Symposium, 2018. [123] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, D. Song, Ú. Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. USENIX Security Symposium, 2020. [124] Daniel Casanueva-Morato, A. Ayuso-Martinez, J. P. Dominguez-Morales, A. Jiménez-Fernandez, and G. Jiménez-Moreno. bio-inspired implementation of sparse-learning spike-based hippocampus memory model. IEEE Transactions on Emerging Topics in Computing, 2022. [125] Daniel Casanueva-Morato, A. Ayuso-Martinez, J. P. Dominguez-Morales, A. Jiménez-Fernandez, and G. Jiménez-Moreno. Bio-inspired computational memory model of the hippocampus: an approach to neuromorphic spike-based content-addressable memory. Neural Networks, 2023. [126] Amartya Chakraborty, Paresh Dashore, Nadia Bathaee, Anmol Jain, Anirban Das, Shi-Xiong Zhang, Sambit Sahu, M. Naphade, and Genta Indra Winata. T1: tool-oriented conversational dataset for multi-turn agentic planning, arXiv preprint arXiv:2505.16986, 2025. URL https://arxiv.org/ abs/2505.16986v1. [127] Kranti Chalamalasetti, Jana Gotze, Sherzod Hakimov, Brielen Madureira, Philipp Sadler, and David Schlangen. clembench: Using game play to evaluate chat-optimized language models as conversational agents. Conference on Empirical Methods in Natural Language Processing, 2023. [128] Edward Y. Chang and Longling Geng. Sagallm: Context management, validation, and transaction guarantees for multi-agent llm planning, arXiv preprint arXiv:2503.11951, 2025. URL https: //arxiv.org/abs/2503.11951v2. [129] Yu-Chu Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Weirong Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qian Yang, and Xingxu Xie. survey on evaluation of large language models. ACM Transactions on Intelligent Systems and Technology, 2023. [130] Subhajit Chaudhury, Payel Das, Sarathkrishna Swaminathan, Georgios Kollias, Elliot Nelson, Khushbu Pahwa, Tejaswini Pedapati, Igor Melnyk, and Matthew Riemer. Epman: Episodic memory attention for generalizing to longer contexts, arXiv preprint arXiv:2502.14280, 2025. URL https://arxiv. org/abs/2502.14280v1. [131] Xueqi CHEGN, Shenghua Liu, and Ruqing ZHANG. Thinking on new system for big data technology. Bulletin of Chinese Academy of Sciences (Chinese Version), 37(1):6067, 2022. [132] Viktoriia Chekalina, Anton Razzigaev, Elizaveta Goncharova, and Andrey Kuznetsov. Addressing hallucinations in language models with knowledge graph embeddings as an additional modality, arXiv preprint arXiv:2411.11531, 2024. URL https://arxiv.org/abs/2411.11531v2. [133] Bo Chen, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, and Zhao Song. Hsr-enhanced sparse attention acceleration, arXiv preprint arXiv:2410.10165, 2024. URL https://arxiv.org/abs/2410. 10165v2. [134] Boyu Chen, Zirui Guo, Zidan Yang, Yuluo Chen, Junze Chen, Zhenghao Liu, Chuan Shi, and Cheng Yang. Pathrag: Pruning graph-based retrieval augmented generation with relational paths, arXiv preprint arXiv:2502.14902, 2025. URL https://arxiv.org/abs/2502.14902v1. [135] Fei-Long Chen, Du-Zhen Zhang, Ming-Lun Han, Xiu-Yi Chen, Jing Shi, Shuang Xu, and Bo Xu. Vlp: survey on vision-language pre-training. Machine Intelligence Research, 20(1):3856, 2023. [136] Feiyang Chen, Yu Cheng, Lei Wang, Yuqing Xia, Ziming Miao, Lingxiao Ma, Fan Yang, Jilong Xue, Zhi Yang, Mao Yang, and Haibo Chen. Attentionengine: versatile framework for efficient attention mechanisms on diverse hardware platforms, arXiv preprint arXiv:2502.15349, 2025. URL https://arxiv.org/abs/2502.15349v1. [137] Huajun Chen. Large knowledge model: Perspectives and challenges. Data Intelligence, 2023. [138] Jianing Chen, Zehao Li, Yujun Cai, Hao Jiang, Chengxuan Qian, Juyuan Kang, Shuqin Gao, Honglong Zhao, Tianlu Mao, and Yucheng Zhang. Haif-gs: Hierarchical and induced flow-guided gaussian splatting for dynamic scene. 2025. [139] Jiaqi Chen, Xiaoye Zhu, Yue Wang, Tianyang Liu, Xinhui Chen, Ying Chen, Chak Tou Leong, Yifei Ke, Joseph Liu, Yiwen Yuan, Julian McAuley, and Li jia Li. Symbolic representation for any-to-any generative tasks, arXiv preprint arXiv:2504.17261v1, 2025. URL https://arxiv.org/abs/ 2504.17261v1. [140] Jiayi Chen, J. Ye, and Guiling Wang. From standalone llms to integrated intelligence: survey of compound al systems, arXiv preprint arXiv:2506.04565, 2025. URL https://arxiv.org/abs/ 2506.04565v1. [141] Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen, Xingmei Wang, Defu Lian, and Enhong Chen. When large language models meet personalization: Perspectives of challenges and opportunities. World wide web (Bussum), 2023. 69 [142] Jiyu Chen, Shuang Peng, Daxiong Luo, Fan Yang, Renshou Wu, Fangyuan Li, and Xiaoxin Chen. Edgeinfinite: memory-efficient infinite-context transformer for edge devices, arXiv preprint arXiv:2503.22196, 2025. URL https://arxiv.org/abs/2503.22196v1. [143] Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. Magicore: Multi-agent, iterative, coarse-to-fine refinement for reasoning, arXiv preprint arXiv:2409.12147, 2024. URL https://arxiv.org/abs/2409.12147v1. [144] Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang, Zenan Zhou, and Weipeng Chen. Facilitating multi-turn function calling for llms via compositional instruction tuning. International Conference on Learning Representations, 2024. [145] Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. Graphwiz: An instruction-following language model for graph computational problems. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 353364, 2024. [146] Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He. Judgelrm: Large reasoning models as judge, arXiv preprint arXiv:2504.00050, 2025. URL https: //arxiv.org/abs/2504.00050v1. [147] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: survey of long chainof-thought for reasoning large language models, arXiv preprint arXiv:2503.09567, 2025. URL https://arxiv.org/abs/2503.09567v3. [148] Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, and Wanxiang Che. Ai4research: survey of artificial intelligence for scientific research, arXiv preprint arXiv:2507.01903, 2025. URL https://arxiv.org/abs/2507.01903. [149] Chen, Wang, YF Wu, Advancing tool-augmented large 2024. https://proceedings.neurips.cc/paper_files/paper/2024/hash/ language models: URL c0f7ee1901fef1da4dae2b88dfd43195-Abstract-Conference.html. Integrating insights and Chen. . . . in inference from errors trees. [150] Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation, arXiv preprint arXiv:2306.15595, 2023. URL https://arxiv.org/abs/2306.15595v2. [151] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. simple framework for contrastive learning of visual representations. International Conference on Machine Learning, 2020. [152] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Trans. Mach. Learn. Res., 2022. [153] Yanda Chen, Ruiqi Zhong, Sheng Zha, G. Karypis, and He He. Meta-learning via language model in-context tuning. Annual Meeting of the Association for Computational Linguistics, 2021. [154] Yi Chen, JiaHao Zhao, and HaoHao Han. survey on collaborative mechanisms between large and small language models, arXiv preprint arXiv:2505.07460, 2025. URL https://arxiv.org/abs/ 2505.07460v1. 70 [155] Yixin Chen, Shuai Zhang, Boran Han, Tong He, and Bo Li. Camml: Context-aware multimodal learner for large models. Annual Meeting of the Association for Computational Linguistics, 2024. [156] Chen, Zhou, Zhang, Gong, and WX Zhao. . . . Chatcot: Tool-augmented chain-of-thought reasoning on chat-based large language models. 2023. URL https://arxiv.org/abs/2305. 14323. [157] Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, et al. T-eval: Evaluating the tool utilization capability step by step. arXiv preprint arXiv:2312.14033, 2023. [158] Zehui Chen, Kuikun Liu, Qiuchen Wang, Jiangning Liu, Wenwei Zhang, Kai Chen, and Feng Zhao. Mindsearch: Mimicking human minds elicits deep ai searcher, arXiv preprint arXiv:2407.20183, 2024. URL https://arxiv.org/abs/2407.20183v1. [159] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Haifang Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, and Jiliang Tang. Exploring the potential of large language models (llms)in learning on graphs. SIGKDD Explorations, 2023. [160] Zihan Chen, Song Wang, Zhen Tan, Xingbo Fu, Zhenyu Lei, Peng Wang, Huan Liu, Cong Shen, and Jundong Li. survey of scaling in large language model reasoning, arXiv preprint arXiv:2504.02181, 2025. URL https://arxiv.org/abs/2504.02181v1. [161] ZY Chen, Shen, Shen, and Zhi. . . . Towards tool use alignment of large language models. 2024. URL https://aclanthology.org/2024.emnlp-main.82/. [162] Mingyue Cheng, Yucong Luo, Ouyang Jie, Qi Liu, Huijie Liu, Li Li, Shuo Yu, Bohou Zhang, Jiawei Cao, Jie Ma, Daoyu Wang, and Enhong Chen. survey on knowledge-oriented retrieval-augmented generation, arXiv preprint arXiv:2503.10677, 2025. URL https://arxiv.org/abs/2503.10677v2. [163] Ning Cheng, Zhaohui Yan, Ziming Wang, Zhijie Li, Jiaming Yu, Zilong Zheng, Kewei Tu, Jinan Xu, and Wenjuan Han. Potential and limitations of llms in capturing structured semantics: case study on srl. International Conference on Intelligent Computing, 2024. [164] Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting Qin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, et al. Call me when necessary: Llms can efficiently and faithfully reason over structured environments. In Association for Computational Linguistics 2024, pages 42754295, 2024. [165] Xin Cheng, Di Luo, Xiuying Chen, Lemao Liu, Dongyan Zhao, and Rui Yan. Lift yourself up: Retrieval-augmented text generation with self memory. Neural Information Processing Systems, 2023. [166] Yao Cheng, Yibo Zhao, Jiapeng Zhu, Yao Liu, Xing Sun, and Xiang Li. Human cognition inspired rag with knowledge graph for complex problem solving, arXiv preprint arXiv:2503.06567, 2025. URL https://arxiv.org/abs/2503.06567v1. [167] Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, and Xiuqiang He. Exploring large language model based intelligent agents: Definitions, methods, and prospects, arXiv preprint arXiv:2401.03428, 2024. URL https://arxiv.org/abs/2401.03428v1. 71 [168] Egor Cherepanov, Nikita Kachaev, A. Kovalev, and Aleksandr I. Panov. Memory, benchmark & robots: benchmark for solving complex tasks with reinforcement learning, arXiv preprint arXiv:2502.10550, 2025. URL https://arxiv.org/abs/2502.10550v2. [169] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building production-ready ai agents with scalable long-term memory, arXiv preprint arXiv:2504.19413, 2025. URL https://arxiv.org/abs/2504.19413. [170] Yew Ken Chia, Lidong Bing, Soujanya Poria, and Luo Si. Relationprompt: Leveraging prompts to generate synthetic data for zero-shot relation triplet extraction. Findings, 2022. [171] Jihye Choi, Nils Palumbo, P. Chalasani, Matthew M. Engelhard, Somesh Jha, Anivarya Kumar, and David Page. Malade: Orchestration of llm-powered agents with retrieval augmented generation for pharmacovigilance, arXiv preprint arXiv:2408.01869, 2024. URL https://arxiv.org/abs/ 2408.01869v1. [172] K. Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamás Sarlós, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, and Adrian Weller. Rethinking attention with performers. International Conference on Learning Representations, 2020. [173] Zhendong Chu, Shen Wang, Jian Xie, Tinghui Zhu, Yibo Yan, Jinheng Ye, Aoxiao Zhong, Xuming Hu, Jing Liang, Philip S. Yu, and Qingsong Wen. Llm agents for education: Advances and applications, arXiv preprint arXiv:2503.11733, 2025. URL https://arxiv.org/abs/2503.11733v1. [174] Zhixuan Chu, Huaiyu Guo, Xinyuan Zhou, Yijia Wang, Fei Yu, Hong Chen, Wanqing Xu, Xin Lu, Qing Cui, Longfei Li, Junqing Zhou, and Sheng Li. Data-centric financial large language models, arXiv preprint arXiv:2310.17784, 2023. URL https://arxiv.org/abs/2310.17784v2. [175] Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh, Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, and Timothy T. Rogers. Simulating opinion dynamics with networks of llm-based agents, arXiv preprint arXiv:2311.09618, 2024. URL https://arxiv.org/abs/2311. 09618. [176] Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria Lin, James Glass, Shang-Wen Li, and Wen tau Yih. Selfcite: Self-supervised alignment for context attribution in large language models, arXiv preprint arXiv:2502.09604, 2025. URL https:// arxiv.org/abs/2502.09604v3. [177] Julian Coda-Forno, Marcel Binz, Zeynep Akata, M. Botvinick, Jane X. Wang, and Eric Schulz. Meta-in-context learning in large language models. Neural Information Processing Systems, 2023. [178] Joao Coelho, Jingjie Ning, Jingyuan He, Kangrui Mao, A. Paladugu, Pranav Setlur, Jiahe Jin, James P. Callan, João Magalhães, Bruno Martins, and Chenyan Xiong. Deepresearchgym: free, transparent, and reproducible evaluation sandbox for deep research, arXiv preprint arXiv:2505.19253, 2025. URL https://arxiv.org/abs/2505.19253v2. [179] Emile Contal and Garrin McGoldrick. Ragsys: Item-cold-start recommender as rag system. IRRAG@SIGIR, 2024. [180] Erica Coppolillo. Injecting knowledge graphs into large language models, arXiv preprint arXiv:2505.07554, 2025. URL https://arxiv.org/abs/2505.07554v1. [181] R. P. Costa, R. Froemke, P. J. Sjöström, and Mark C. W. van Rossum. Unified preand postsynaptic long-term plasticity enables reliable and flexible learning. eLife, 2015. [182] Caia Costello, Simon Guo, Anna Goldie, and Azalia Mirhoseini. Think, prune, train, improve: Scaling reasoning without scaling models, arXiv preprint arXiv:2504.18116, 2025. URL https: //arxiv.org/abs/2504.18116v1. [183] Michael Craig, Karla Butterworth, Jonna Nilsson, Colin Hamilton, P. Gallagher, and T. Smulders. How does intentionality of encoding affect memory for episodic information? Learning & memory (Cold Spring Harbor, N.Y.), 2016. [184] crewAI Inc. crewai: Framework for orchestrating role-playing, autonomous ai agents. https: //github.com/crewAIInc/crewAI, 2024. [Online; accessed 17-July-2025]. [185] A. Cruz, André V. dos Santos, R. Santiago, and B. Bedregal. fuzzy semantic for bdi logic. Fuzzy Information and Engineering, 2021. [186] Florin Cuconasu, Giovanni Trappolini, F. Siciliano, Simone Filice, Cesare Campagnano, Y. Maarek, Nicola Tonellotto, and Fabrizio Silvestri. The power of noise: Redefining retrieval for rag systems. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024. [187] Kai Cui, Anam Tahir, Gizem Ekinci, Ahmed Elshamanhory, Yannick Eich, Mengguang Li, and H. Koeppl. survey on large-population systems and scalable multi-agent reinforcement learning, arXiv preprint arXiv:2209.03859, 2022. URL https://arxiv.org/abs/2209.03859v1. [188] Yuanning Cui, Zequn Sun, and Wei Hu. prompt-based knowledge graph foundation model for universal in-context reasoning. In Advances in Neural Information Processing Systems, 2024. [189] Yue Cui, Liuyi Yao, Shuchang Tao, Weijie Shi, Yaliang Li, Bolin Ding, and Xiaofang Zhou. Enhancing tool learning in large language models with hierarchical error checklists, arXiv preprint arXiv:2506.00042, 2025. URL https://arxiv.org/abs/2506.00042v1. [190] C. Curto, A. Degeratu, and V. Itskov. Flexible memory networks. Bulletin of Mathematical Biology, 2010. [191] Ruiting Dai, Yuqiao Tan, Lisi Mo, Shuang Liang, Guohao Huo, Jiayi Luo, and Yao Cheng. G-sap: Graph-based structure-aware prompt learning over heterogeneous knowledge for commonsense reasoning. International Conference on Multimedia Retrieval, 2024. [192] Xinnan Dai, Haohao Qu, Yifen Shen, Bohang Zhang, Qihao Wen, Wenqi Fan, Dongsheng Li, Jiliang Tang, and Caihua Shan. How do large language models understand graph patterns? benchmark for graph pattern comprehension, arXiv preprint arXiv:2410.05298v2, 2024. URL https://arxiv. org/abs/2410.05298v2. [193] Fatemeh Daneshfar and H. Bevrani. Multi-agent systems in control engineering: survey. arXiv preprint, 2009. [194] Yufan Dang, Cheng Qian, Xueheng Luo, Jingru Fan, Zihao Xie, Ruijie Shi, Weize Chen, Cheng Yang, Xiaoyin Che, Ye Tian, Xuantang Xiong, Lei Han, Zhiyuan Liu, and Maosong Sun. Multi-agent collaboration via evolving orchestration, arXiv preprint arXiv:2505.19591, 2025. URL https: //arxiv.org/abs/2505.19591v1. 73 [195] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. International Conference on Learning Representations, 2023. [196] Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Neural Information Processing Systems, 2022. [197] Das, Banerjee, Aditya, and Kulkarni. Mathsensei: tool-augmented large language model for mathematical reasoning. 2024. URL https://arxiv.org/abs/2402.17231. [198] Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aurélie Lozano, Georgios Kollias, Vijil Chenthamarakshan, Jiří, Navrátil, Soham Dan, and Pin-Yu Chen. Larimar: Large language models with episodic memory control, arXiv preprint arXiv:2403.11901, 2024. URL https://arxiv.org/abs/2403.11901. [199] Adrian de Wynter, Xun Wang, Qilong Gu, and Si-Qing Chen. On meta-prompting, arXiv preprint arXiv:2312.06562, 2023. URL https://arxiv.org/abs/2312.06562v3. [200] Ramandeep Singh Dehal, Mehak Sharma, and Enayat Rajabi. Knowledge graphs and their reciprocal relationship with large language models. Machine Learning and Knowledge Extraction, 2025. [201] Mauricio R. Delgado, V. Stenger, and J. Fiez. Motivation-dependent responses in the human caudate nucleus. Cerebral Cortex, 2004. [202] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. Neural Information Processing Systems, 2023. [203] Yang Deng, Wenqiang Lei, Hongru Wang, and Tat seng Chua. Prompting and evaluating large language models for proactive dialogues: Clarification, target-guided, and non-collaboration. Conference on Empirical Methods in Natural Language Processing, 2023. [204] Yang Deng, An Zhang, Yankai Lin, Xu Chen, Ji-Rong Wen, and Tat-Seng Chua. Large language model powered agents in the web. The Web Conference, 2024. [205] Yang Deng, Xuan Zhang, Wenxuan Zhang, Yifei Yuan, See-Kiong Ng, and Tat-Seng Chua. On the multi-turn instruction following for conversational web agents. Annual Meeting of the Association for Computational Linguistics, 2024. [206] Brouillet Denis and Versace Rémy. The nature of the traces and the dynamics of memory. Psychology and Behavioral Sciences, 2019. [207] Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees G. M. Snoek, M. Worring, and Yuki Asano. Self-supervised open-ended classification with small visual language models, arXiv preprint arXiv:2310.00500, 2023. URL https://arxiv.org/abs/2310.00500v2. [208] Stefan Dernbach, Khushbu Agarwal, Alejandro Zuniga, Michael Henry, and Sutanay Choudhury. Glam: Fine-tuning large language models for domain knowledge graph alignment via neighborhood partitioning and generative subgraph encoding. AAAI Spring Symposia, 2024. [209] Rushali Deshmukh, Rutuj Raut, Mayur Bhavsar, Sanika Gurav, and Y. Patil. Optimizing human-ai interaction: Innovations in prompt engineering. 2025 3rd International Conference on Intelligent Data Communication Technologies and Internet of Things (IDCIoT), 2025. 74 [210] Darshan Deshpande, Varun Gangal, Hersh Mehta, Jitin Krishnan, Anand Kannappan, and Rebecca Qian. Trail: Trace reasoning and agentic issue localization, arXiv preprint arXiv:2505.08638, 2025. URL https://arxiv.org/abs/2505.08638v3. [211] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. North American Chapter of the Association for Computational Linguistics, 2019. [212] Dhruv Dhamani and Mary Lou Maher. The tyranny of possibilities in the design of task-oriented llm systems: scoping survey, arXiv preprint arXiv:2312.17601, 2023. URL https://arxiv.org/ abs/2312.17601v1. [213] Frederick Dillon, Gregor Halvorsen, Simon Tattershall, Magnus Rowntree, and Gareth Vanderpool. Contextual memory reweaving in large language models using layered latent state reconstruction, arXiv preprint arXiv:2502.02046, 2025. URL https://arxiv.org/abs/2502.02046v2. [214] Hanxing Ding, Shuchang Tao, Liang Pang, Zihao Wei, Jinyang Gao, Bolin Ding, Huawei Shen, and Xueqi Chen. Toolcoder: systematic code-empowered tool learning framework for large language models, arXiv preprint arXiv:2502.11404, 2025. URL https://arxiv.org/abs/2502. 11404v2. [215] Hongxin Ding, Yue Fang, Runchuan Zhu, Xinke Jiang, Jinyang Zhang, Yongxin Xu, Xu Chu, Junfeng Zhao, and Yasha Wang. 3ds: Decomposed difficulty data selections case study on llm medical domain adaptation. 2024. [216] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei. Longnet: Scaling transformers to 1, 000, 000, 000 tokens. arXiv preprint, 2023. [217] Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, and Luming Liang. The efficiency spectrum of large language models: An algorithmic survey, arXiv preprint arXiv:2312.00678, 2023. URL https://arxiv.org/abs/ 2312.00678v2. [218] Yiran Ding, L. Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. International Conference on Machine Learning, 2024. [219] Yiwen Ding, Zhiheng Xi, Wei He, Zhuoyuan Li, Yitao Zhai, Xiaowei Shi, Xunliang Cai, Tao Gui, Qi Zhang, and Xuanjing Huang. Mitigating tail narrowing in llm self-improvement via socratic-guided sampling. North American Chapter of the Association for Computational Linguistics, 2024. [220] Christian Djeffal. Reflexive prompt engineering: framework for responsible prompt engineering and ai interaction design. Conference on Fairness, Accountability and Transparency, 2025. [221] Dong, Chen, Li, Jin, Qian, and Zhu. . . . Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. 2025. URL https://arxiv.org/abs/2505.16410. [222] Guanting Dong, Jinxu Zhao, Tingfeng Hui, Daichi Guo, Wenlong Wan, Boqi Feng, Yueyan Qiu, Zhuoma Gongque, Keqing He, Zechen Wang, and Weiran Xu. Revisit input perturbation problems for llms: unified robustness evaluation framework for noisy slot filling task. Natural Language Processing and Chinese Computing, 2023. 75 [223] Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, and Ji-Rong Wen. Tool-star: Empowering llm-brained multi-tool reasoner via reinforcement learning. arXiv preprint, 2025. [224] Kaiwen Dong. Large language model applied in multi-agent systema survey. Applied and Computational Engineering, 2024. [225] Peijie Dong, Zhenheng Tang, Xiang-Hong Liu, Lujun Li, Xiaowen Chu, and Bo Li. Can compressed llms truly act? an empirical evaluation of agentic capabilities in llm compression, arXiv preprint arXiv:2505.19433, 2025. URL https://arxiv.org/abs/2505.19433v2. [226] Vicky Dong, Hao Yu, and Yao Chen. Graph-augmented relation extraction model with llms-generated support document, arXiv preprint arXiv:2410.23452, 2024. URL https://arxiv.org/abs/ 2410.23452v1. [227] Xiangjue Dong, Maria Teleki, and James Caverlee. survey on llm inference-time self-improvement, arXiv preprint arXiv:2412.14352, 2024. URL https://arxiv.org/abs/2412.14352v1. [228] Yuxin Dong, Shuo Wang, Hongye Zheng, Jiajing Chen, Zhenhong Zhang, and Chihang Wang. Advanced rag models with graph structures: Optimizing complex knowledge reasoning and text generation. 2024 5th International Symposium on Computer Engineering and Intelligent Communications (ISCEIC), 2024. [229] Zican Dong, Junyi Li, Xin Men, Wayne Xin Zhao, Bingbing Wang, Zhen Tian, Weipeng Chen, and Ji-Rong Wen. Exploring context window of large language models via decomposed positional vectors. Neural Information Processing Systems, 2024. [230] Ehsan Doostmohammadi and Marco Kuhlmann. Studying the role of input-neighbor overlap in retrieval-augmented language models training efficiency, arXiv preprint arXiv:2505.14309, 2025. URL https://arxiv.org/abs/2505.14309v1. [231] Mohammadreza Doostmohammadian, Alireza Aghasi, Mohammad Pirani, Ehsan Nekouei, H. Zarrabi, Reza Keypour, Apostolos I. Rikos, and K. H. Johansson. Survey of distributed algorithms for resource allocation over multi-agent systems, arXiv preprint arXiv:2401.15607, 2024. URL https://arxiv. org/abs/2401.15607v1. [232] A. Dorri, S. Kanhere, and R. Jurdak. Multi-agent systems: survey. IEEE Access, 2018. [233] Mauro Dragone. Component & service-based agent systems: Self-osgi. International Conference on Agents and Artificial Intelligence, 2012. [234] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom Marty, David Vazquez, Nicolas Chapados, and Alexandre Lacoste. WorkArena: How capable are web agents at solving common knowledge work tasks? In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 1164211662. PMLR, 2127 Jul 2024. URL https://proceedings. mlr.press/v235/drouin24a.html. [235] Hung Du, Srikanth Thudumu, Rajesh Vasa, and K. Mouzakis. survey on context-aware multi-agent systems: Techniques, challenges and future directions, arXiv preprint arXiv:2402.01968, 2024. URL https://arxiv.org/abs/2402.01968v2. 76 [236] Jingfei Du, Edouard Grave, Beliz Gunel, Vishrav Chaudhary, Onur Çelebi, Michael Auli, Ves Stoyanov, and Alexis Conneau. Self-training improves pre-training for natural language understanding. North American Chapter of the Association for Computational Linguistics, 2020. [237] Jusen Du, Weigao Sun, Disen Lan, Jiaxi Hu, and Yu Cheng. Mom: Linear sequence modeling with mixture-of-memories, arXiv preprint arXiv:2502.13685, 2025. URL https://arxiv.org/abs/ 2502.13685v2. [238] Mingxuan Du, Benfeng Xu, Chiwei Zhu, Xiaorui Wang, and Zhendong Mao. Deepresearch bench: comprehensive benchmark for deep research agents, arXiv preprint arXiv:2506.11763, 2025. URL https://arxiv.org/abs/2506.11763v1. [239] Shangheng Du, Jiabao Zhao, Jinxin Shi, Zhentao Xie, Xin Jiang, Yanhong Bai, and Liang He. survey on the optimization of large language model-based agents, arXiv preprint arXiv:2503.12434, 2025. URL https://arxiv.org/abs/2503.12434v1. [240] Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, and Kam-Fai Wong. Perltqa: personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering, arXiv preprint arXiv:2402.16288, 2024. URL https: //arxiv.org/abs/2402.16288v1. [241] Hanqi Duan, Yao Cheng, Jianxiang Yu, and Xiang Li. Can large language models act as ensembler for multi-gnns?, arXiv preprint arXiv:2410.16822, 2024. URL https://arxiv.org/abs/2410. 16822v2. [242] Peitong Duan, Chin yi Chen, Bjoern Hartmann, and Yang Li. Visual prompting with iterative refinement for design critique generation, arXiv preprint arXiv:2412.16829, 2024. URL https: //arxiv.org/abs/2412.16829v2. [243] Brown Ebouky, A. Bartezzaghi, and Mattia Rigotti. Eliciting reasoning in language models with cognitive tools, arXiv preprint arXiv:2506.12115, 2025. URL https://arxiv.org/abs/2506. 12115v1. [244] Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. From local to global: graph rag approach to query-focused summarization. arXiv preprint arXiv:2404.16130, 2024. [245] Candace Edwards. Hybrid context retrieval augmented generation pipeline: Llm-augmented knowledge graphs and vector database for accreditation reporting assistance, arXiv preprint arXiv:2405.15436, 2024. URL https://arxiv.org/abs/2405.15436v1. [246] Abul Ehtesham, Aditi Singh, Gaurav Kumar Gupta, and Saket Kumar. survey of agent interoperability protocols: Model context protocol (mcp), agent communication protocol (acp), agent-to-agent protocol (a2a), and agent network protocol (anp), arXiv preprint arXiv:2505.02279, 2025. URL https://arxiv.org/abs/2505.02279v2. [247] Will Epperson, Gagan Bansal, Victor Dibia, Adam Fourney, Jack Gerrits, Erkang Zhu, and Saleema Amershi. Interactive debugging and steering of multi-agent ai systems. International Conference on Human Factors in Computing Systems, 2025. 77 [248] Lutfi Eren Erdogan, Nicholas Lee, Siddharth Jha, Sehoon Kim, Ryan Tabrizi, Suhong Moon, Coleman Hooper, G. Anumanchipalli, Kurt Keutzer, and A. Gholami. Tinyagent: Function calling at the edge. Conference on Empirical Methods in Natural Language Processing, 2024. [249] Oluwole Fagbohun, Rachel M. Harrison, and Anton Dereventsov. An empirical categorization of prompting techniques for large language models: practitioners guide. arXiv preprint, 2024. [250] Kazem Faghih, Wenxiao Wang, Yize Cheng, Siddhant Bharti, Gaurang Sriramanan, S. Balasubramanian, Parsa Hosseini, and S. Feizi. Gaming tool preferences in agentic llms, arXiv preprint arXiv:2505.18135, 2025. URL https://arxiv.org/abs/2505.18135v1. [251] Linxi (Jim) Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Neural Information Processing Systems, 2022. [252] Siqi Fan, Xiusheng Huang, Yiqun Yao, Xuezhi Fang, Kang Liu, Peng Han, Shuo Shang, Aixin Sun, and Yequan Wang. If an llm were character, would it know its own story? evaluating lifelong learning in llms, arXiv preprint arXiv:2503.23514, 2025. URL https://arxiv.org/abs/2503.23514v1. [253] Wenqi Fan, Yujuan Ding, Liang bo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. survey on rag meeting llms: Towards retrieval-augmented large language models. Knowledge Discovery and Data Mining, 2024. [254] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memoryaugmented multimodal agent for video understanding, arXiv preprint arXiv:2403.11481, 2024. URL https://arxiv.org/abs/2403.11481v2. [255] Hongchao Fang and Pengtao Xie. An end-to-end contrastive self-supervised learning framework for language understanding. Transactions of the Association for Computational Linguistics, 2022. [256] Hongchao Fang, Sicheng Wang, Meng Zhou, Jiayuan Ding, and Pengtao Xie. Cert: Contrastive self-supervised learning for language understanding, arXiv preprint arXiv:2005.12766, 2020. URL https://arxiv.org/abs/2005.12766v2. [257] Junfeng Fang, Zijun Yao, Ruipeng Wang, Haokai Ma, Xiang Wang, and Tat-Seng Chua. We should identify and mitigate third-party safety risks in mcp-powered agent systems, arXiv preprint arXiv:2506.13666, 2025. URL https://arxiv.org/abs/2506.13666v1. [258] Siyuan Fang, Kaijing Ma, Tianyu Zheng, Xinrun Du, Ningxuan Lu, Ge Zhang, and Qingkun Tang. Karpa: training-free method of adapting knowledge graph as references for large language models reasoning path aggregation. arXiv preprint, 2024. [259] Wei-Wen Fang, Yang Zhang, Kaizhi Qian, James Glass, and Yada Zhu. Play2prompt: Zero-shot tool instruction optimization for llm agents via tool play, arXiv preprint arXiv:2503.14432, 2025. URL https://arxiv.org/abs/2503.14432v2. [260] Yi Fang, Dongzhe Fan, D. Zha, and Qiaoyu Tan. Gaugllm: Improving graph contrastive learning for text-attributed graphs with large language models. Knowledge Discovery and Data Mining, 2024. [261] Yi Fang, Bowen Jin, Jiacheng Shen, Sirui Ding, Qiaoyu Tan, and Jiawei Han. Graphgpt-o: Synergistic multimodal comprehension and generation on graphs. arXiv preprint, 2025. 78 [262] Bahare Fatemi, Jonathan J. Halcrow, and Bryan Perozzi. Talk like graph: Encoding graphs for large language models. International Conference on Learning Representations, 2023. [263] George Fatouros, Georgios Makridis, George Kousiouris, John Soldatos, A. Tsadimas, and D. Kyriazis. Towards conversational ai for human-machine collaborative mlops, arXiv preprint arXiv:2504.12477, 2025. URL https://arxiv.org/abs/2504.12477v1. [264] M. Fauth, F. Wörgötter, and Christian Tetzlaff. Formation and maintenance of robust long-term information storage in the presence of synaptic turnover. bioRxiv, 2015. [265] Zahra Fayyaz, Aya Altamimi, Sen Cheng, and Laurenz Wiskott. model of semantic completion in generative episodic memory. Neural Computation, 2021. [266] Xiang Fei, Xiawu Zheng, and Hao Feng. Mcp-zero: Proactive toolchain construction for llm agents from scratch. arXiv preprint, 2025. [267] Philip Feldman, James R. Foulds, and Shimei Pan. Ragged edges: The double-edged sword of retrieval-augmented chatbots, arXiv preprint arXiv:2403.01193, 2024. URL https://arxiv. org/abs/2403.01193v3. [268] Aosong Feng, Rex Ying, and L. Tassiulas. Long sequence modeling with attention tensorization: From sequence to tensor learning. Conference on Empirical Methods in Natural Language Processing, 2024. [269] Erhu Feng, Wenbo Zhou, Zibin Liu, Le Chen, Yunpeng Dong, Cheng Zhang, Yisheng Zhao, Dong Du, Zhi-Hua Zhou, Yubin Xia, and Haibo Chen. Get experience from practice: Llm agents with record & replay, arXiv preprint arXiv:2505.17716, 2025. URL https://arxiv.org/abs/2505.17716v1. [270] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms, arXiv preprint arXiv:2504.11536, 2025. URL https://arxiv.org/abs/2504.11536v2. [271] Kaituo Feng, Changsheng Li, Xiaolu Zhang, Jun Zhou, Ye Yuan, and Guoren Wang. Keypoint-based progressive chain-of-thought distillation for llms. International Conference on Machine Learning, 2024. [272] Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Y. Bengio, and M. O. Ahmed. Constant memory attention block, arXiv preprint arXiv:2306.12599, 2023. URL https://arxiv.org/abs/2306. 12599v1. [273] Yanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng Wang, Jun Yan, and Xiang Ren. Scalable multi-hop relational reasoning for knowledge-aware question answering. Conference on Empirical Methods in Natural Language Processing, 2020. [274] Yifan Feng, Shiquan Liu, Xiangmin Han, Shaoyi Du, Zongze Wu, Han Hu, and Yue Gao. Hypergraph foundation model, arXiv preprint arXiv:2503.01203v1, 2025. URL https://arxiv.org/abs/ 2503.01203v1. [275] Chrisantha Fernando, Dylan Banarse, H. Michalewski, Simon Osindero, and Tim Rocktäschel. Promptbreeder: Self-referential self-improvement via prompt evolution. International Conference on Machine Learning, 2023. 79 [276] Tharindu Fernando, Simon Denman, A. Mcfadyen, S. Sridharan, and C. Fookes. Tree memory networks for modelling long-term temporal dependencies. Neurocomputing, 2017. [277] M. Ferrag, Norbert Tihanyi, and M. Debbah. From llm reasoning to autonomous ai agents: comprehensive review, arXiv preprint arXiv:2504.19678, 2025. URL https://arxiv.org/abs/ 2504.19678v1. [278] M. Ferrag, Norbert Tihanyi, and M. Debbah. Reasoning beyond limits: Advances and open problems for llms, arXiv preprint arXiv:2503.22732, 2025. URL https://arxiv.org/abs/2503. 22732v1. [279] Christopher Fifty, Dennis Duan, Ronald G. Junkins, Ehsan Amid, Jurij Leskovec, Christopher Re, and Sebastian Thrun. Context-aware meta-learning. International Conference on Learning Representations, 2023. [280] Tim Finin, Richard Fritzson, Donald McKay, Robin McEntire, et al. Kqml-a language and protocol for knowledge and information exchange. In 13th Int. Distributed Artificial Intelligence Workshop, pages 93103, 1994. [281] Chelsea Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks. International Conference on Machine Learning, 2017. [282] Paolo Finotelli and Francis Eustache. Mathematical modeling of human memory. Frontiers in Psychology, 2023. [283] Ferdinando Fioretto, Enrico Pontelli, and W. Yeoh. Distributed constraint optimization problems and applications: survey. Journal of Artificial Intelligence Research, 2016. [284] Meire Fortunato, Melissa Tan, Ryan Faulkner, S. Hansen, Adrià Puigdomènech Badia, Gavin Buttimore, Charlie Deck, Joel Z. Leibo, and C. Blundell. Generalization of reinforcement learners with working and episodic memory. Neural Information Processing Systems, 2019. [285] Samy Foudil, Claire Pleche, and E. Macaluso. Memory for spatio-temporal contextual details during the retrieval of naturalistic episodes. Scientific Reports, 2021. [286] Zafeirios Fountas, Martin Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, and Jun Wang. Human-like episodic memory for infinite context llms, arXiv preprint arXiv:2407.09450, 2024. URL https://arxiv.org/abs/2407.09450. [287] Quentin Fournier, G. Caron, and D. Aloise. practical survey on faster and lighter transformers. ACM Computing Surveys, 2021. [288] Luca Franceschi, P. Frasconi, Saverio Salzo, Riccardo Grazzi, and M. Pontil. Bilevel programming for hyperparameter optimization and meta-learning. International Conference on Machine Learning, 2018. [289] Eduard Frankford, Daniel Crazzolara, Clemens Sauerwein, Michael Vierhauser, and Ruth Breu. Requirements for an online integrated development environment for automated programming assessment systems. International Conference on Computer Supported Education, 2024. 80 [290] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint, 2024. [291] Honghao Fu, Yilang Shen, Yuxuan Liu, Jingzhong Li, and Xiang Zhang. Sgcn: multi-order neighborhood feature fusion landform classification method based on superpixel and graph convolutional network. International Journal of Applied Earth Observation and Geoinformation, 122:103441, 2023. [292] Honghao Fu, Yufei Wang, Wenhan Yang, Alex Kot, and Bihan Wen. Dp-iqa: Utilizing diffusion prior for blind image quality assessment in the wild. 2024. [293] Honghao Fu, Hao Wang, Jing Jih Chin, and Zhiqi Shen. Brainvis: Exploring the bridge between brain and visual signals via image reconstruction. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. [294] Yuchuan Fu, Xiaohan Yuan, and Dongxia Wang. Ras-eval: comprehensive benchmark for security evaluation of llm agents in real-world environments. arXiv preprint, 2025. [295] Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, and Xiangyu Zhao. Sliding window attention training for efficient large language models, arXiv preprint arXiv:2502.18845, 2025. URL https://arxiv.org/abs/2502. 18845v2. [296] Stefano Fusi. Memory capacity of neural network models, arXiv preprint arXiv:2108.07839, 2021. URL https://arxiv.org/abs/2108.07839v2. [297] Tiantian Gan and Qiyao Sun. Rag-mcp: Mitigating prompt bloat in llm tool selection via retrievalaugmented generation. arXiv preprint, 2025. [298] Kanishk Gandhi, Gala Stojnic, B. Lake, and M. Dillon. Baby intuitions benchmark (bib): Discerning the goals, preferences, and actions of others. Neural Information Processing Systems, 2021. [299] Anish Ganguli, Prabal Deb, and Debleena Banerjee. Mark: Memory augmented refinement of knowledge, arXiv preprint arXiv:2505.05177, 2025. URL https://arxiv.org/abs/2505.05177v1. [300] Chen Gao, Xiaochong Lan, Nian Li, Yuan Yuan, Jingtao Ding, Zhilun Zhou, Fengli Xu, and Yong Li. Large language models empowered agent-based modeling and simulation: survey and perspectives. Humanities and Social Sciences Communications, 2023. [301] Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, and Yong Li. S3: Social-network simulation system with large language model-empowered agents, arXiv preprint arXiv:2307.14984, 2025. URL https://arxiv.org/abs/2307.14984. [302] Hang Gao and Yongfeng Zhang. Memory sharing for large language model based agents, arXiv preprint arXiv:2404.09982, 2024. URL https://arxiv.org/abs/2404.09982v2. [303] Gao, Madaan, Zhou, and Alon. . . . Pal: Program-aided language models. 2023. URL https://proceedings.mlr.press/v202/gao23f. [304] Luyu Gao, Xueguang Ma, Jimmy J. Lin, and Jamie Callan. Precise zero-shot dense retrieval without relevance labels. Annual Meeting of the Association for Computational Linguistics, 2022. 81 [305] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. International Conference on Machine Learning, 2022. [306] Shuzheng Gao, Xinjie Wen, Cuiyun Gao, Wenxuan Wang, and Michael R. Lyu. What makes good incontext demonstrations for code intelligence tasks with llms? International Conference on Automated Software Engineering, 2023. [307] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. Annual Meeting of the Association for Computational Linguistics, 2021. [308] Weiguo Gao. Mep: Multiple kernel learning enhancing relative positional encoding length extrapolation, arXiv preprint arXiv:2403.17698, 2024. URL https://arxiv.org/abs/2403.17698v1. [309] Xian Gao, Zongyun Zhang, Mingye Xie, Ting Liu, and Yuzhuo Fu. Graph of ai ideas: Leveraging knowledge graphs and llms for ai research idea generation, arXiv preprint arXiv:2503.08549, 2025. URL https://arxiv.org/abs/2503.08549v1. [310] Xuanqi Gao, Siyi Xie, Juan Zhai, Shqing Ma, and Chao Shen. Mcp-radar: multi-dimensional benchmark for evaluating tool use capabilities in large language models. arXiv preprint, 2025. [311] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang. Retrieval-augmented generation for large language models: survey, arXiv preprint arXiv:2312.10997, 2023. URL https://arxiv.org/abs/2312.10997v5. [312] Yunfan Gao, Yun Xiong, Meng Wang, and Haofen Wang. Modular rag: Transforming rag systems into lego-like reconfigurable frameworks, arXiv preprint arXiv:2407.21059, 2024. URL https: //arxiv.org/abs/2407.21059v1. [313] Yunfan Gao, Yun Xiong, Yijie Zhong, Yuxi Bi, Ming Xue, and Haofen Wang. Synergizing rag and reasoning: systematic review, arXiv preprint arXiv:2504.15909, 2025. URL https://arxiv. org/abs/2504.15909v2. [314] Zhangyang Gao, Daize Dong, Cheng Tan, Jun Xia, Bozhen Hu, and Stan Z. Li. graph is worth words: Euclideanizing graph using pure transformer. International Conference on Machine Learning, 2024. [315] Itai Gat, Idan Schwartz, and A. Schwing. Perceptual score: What data modalities does your model perceive? Neural Information Processing Systems, 2021. [316] Itai Gat, Felix Kreuk, Tu Nguyen, Ann Lee, Jade Copet, Gabriel Synnaeve, Emmanuel Dupoux, and Yossi Adi. Augmentation invariant discrete representation for generative spoken language modeling. International Workshop on Spoken Language Translation, 2022. [317] Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in large language model. International Conference on Learning Representations, 2023. [318] Yuyao Ge, Zhongguo Yang, Lizhe Chen, Yiming Wang, and Chengyang Li. Attack based on data: novel perspective to attack sensitive points directly. Cybersecurity, 6(1):43, 2023. [319] Yuyao Ge, Shenghua Liu, Baolong Bi, Yiwei Wang, Lingrui Mei, Wenjie Feng, Lizhe Chen, and Xueqi Cheng. Can graph descriptive order affect solving graph problems with llms? ACL 2025, 2024. 82 [320] Yuyao Ge, Shenghua Liu, Yiwei Wang, Lingrui Mei, Lizhe Chen, Baolong Bi, and Xueqi Cheng. Innate reasoning is not enough: In-context learning enhances reasoning large language models with less overthinking. 2025. [321] Binzong Geng, Zhaoxin Huan, Xiaolu Zhang, Yong He, Liang Zhang, Fajie Yuan, Jun Zhou, and Linjian Mo. Breaking the length barrier: Llm-enhanced ctr prediction in long textual user behaviors. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024. [322] Hejia Geng, Boxun Xu, and Peng Li. Upar: kantian-inspired prompting framework for enhancing large language model capabilities, arXiv preprint arXiv:2310.01441, 2023. URL https://arxiv. org/abs/2310.01441v2. [323] Antonios Georgiou, M. Katkov, and M. Tsodyks. Retroactive interference model of forgetting. Journal of Mathematical Neuroscience, 2021. [324] S. Gershman, A. Schapiro, A. Hupbach, and K. Norman. Neural context reinstatement predicts memory misattribution. Journal of Neuroscience, 2013. [325] Mor Geva, R. Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. Conference on Empirical Methods in Natural Language Processing, 2020. [326] Mor Geva, Avi Caciularu, Ke Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. Conference on Empirical Methods in Natural Language Processing, 2022. [327] Arda Gezdur and J. Bhattacharjya. Innovators and transformers: enhancing supply chain employee training with an innovative application of large language model. International Journal of Physical Distribution & Logistics Management, 2025. [328] Alireza Ghafarollahi and Markus J. Buehler. Protagents: protein discovery via large language model multi-agent collaborations combining physics and machine learning. Digital Discovery, 2024. [329] Abdellah Ghassel, Ian Robinson, Gabriel Tanase, Hal Cooper, Bryan Thompson, Zhen Han, V. Ioannidis, Soji Adeshina, and H. Rangwala. Hierarchical lexical graph for enhanced multi-hop retrieval, arXiv preprint arXiv:2506.08074, 2025. URL https://arxiv.org/abs/2506.08074v1. [330] S. Ghetti and S. Bunge. Neural changes underlying the development of episodic memory during middle childhood. Developmental Cognitive Neuroscience, 2012. [331] D. Ghica. Function interface models for hardware compilation: Types, signatures, protocols, arXiv preprint arXiv:0907.0749, 2009. URL https://arxiv.org/abs/0907.0749v1. [332] Tyler Giallanza, Declan Campbell, and Jonathan D. Cohen. Toward the emergence of intelligent control: Episodic generalization and optimization. Open Mind, 2024. [333] In Gim, Seung seob Lee, and Lin Zhong. Asynchronous llm function calling, arXiv preprint arXiv:2412.07017, 2024. URL https://arxiv.org/abs/2412.07017v1. [334] Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory 83 Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements, arXiv preprint arXiv:2209.14375, 2022. URL https://arxiv.org/abs/2209.14375. [335] D. Godden and A. Baddeley. Context-dependent memory in two natural environments: on land and underwater. arXiv preprint, 1975. [336] Arda Goknil, Femke B. Gelderblom, Simeon Tverdal, Shukun Tokas, and Hui Song. Privacy policy analysis through prompt engineering for llms, arXiv preprint arXiv:2409.14879, 2024. URL https: //arxiv.org/abs/2409.14879v1. [337] Yaroslav Golubev, Zarina Kurbatova, E. Alomar, T. Bryksin, and Mohamed Wiem Mkaouer. One thousand and one stories: large-scale survey of software refactoring. ESEC/SIGSOFT FSE, 2021. [338] Alan Gordon, Jesse Rissman, Roozbeh Kiani, and Anthony Wagner. Cortical reinstatement mediates the relationship between content-specific encoding activity and subsequent recollection decisions. Cerebral Cortex, 2014. [339] E. Gordon and B. Logan. Managing goals and resources in dynamic environments. arXiv preprint, 2005. [340] Gou, Shao, Gong, Shen, and Yang. . . . Critic: Large language models can self-correct with tool-interactive critiquing. 2023. URL https://arxiv.org/abs/2305.11738. [341] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. International Conference on Learning Representations, 2023. [342] Alex Graves, Abdel rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep recurrent neural networks. IEEE International Conference on Acoustics, Speech, and Signal Processing, 2013. [343] Ekaterina Grishina, Mikhail Gorbunov, and Maxim Rakhuba. Procrustesgpt: Compressing llms with structured matrices and orthogonal transformations, arXiv preprint arXiv:2506.02818, 2025. URL https://arxiv.org/abs/2506.02818v1. [344] Sven Gronauer and K. Diepold. Multi-agent deep reinforcement learning: survey. Artificial Intelligence Review, 2021. [345] C. Gros. Complex and adaptive dynamical systems, arXiv preprint arXiv:0807.4838, 2008. URL https://arxiv.org/abs/0807.4838v3. [346] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. International Conference on Learning Representations, 2021. [347] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Ré. On the parameterization and initialization of diagonal state space models. Neural Information Processing Systems, 2022. [348] Jian Gu, Chunyang Chen, and A. Aleti. Vocabulary-defined semantics: Latent space clustering for improving in-context learning, arXiv preprint arXiv:2401.16184, 2024. URL https://arxiv. org/abs/2401.16184v6. 84 [349] Yongli Gu, Xiang Yan, Hanlin Qin, Naveed Akhtar, Shuai Yuan, Honghao Fu, Shuowen Yang, and Ajmal Mian. Hdtcnet: hybrid-dimensional convolutional network for multivariate time series classification. Pattern Recognition, page 111837, 2025. [350] Zhuohan Gu, Jiayi Yao, Kuntai Du, and Junchen Jiang. Llmsteer: Improving long-context llm inference by steering attention on reused contexts, arXiv preprint arXiv:2411.13009, 2024. URL https://arxiv.org/abs/2411.13009v2. [351] Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian guang Lou. Evaluating llm-based agents for multi-turn conversations: survey, arXiv preprint arXiv:2503.22458, 2025. URL https://arxiv.org/abs/2503.22458v1. [352] Zhong Guan, Hongke Zhao, Likang Wu, Ming He, and Jianpin Fan. Langtopo: Aligning language descriptions of graphs with tokenized topological modeling, arXiv preprint arXiv:2406.13250, 2024. URL https://arxiv.org/abs/2406.13250v1. [353] Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, Deepak Gopinath, Dian Ang Yap, Dong Yin, Feng Nan, Floris Weers, Guoli Yin, Haoshuo Huang, Jianyu Wang, Jiarui Lu, John Peebles, Kewei Ye, Mark Lee, Nan Du, Qibin Chen, Quentin Keunebroek, Sam Wiseman, Syd Evans, Tao Lei, Vivek Rathod, Xiang Kong, Xianzhi Du, Yanghao Li, Yongqiang Wang, Yuan Gao, Zaid Ahmed, Zhaoyang Xu, Zhiyun Lu, Al Rashid, Albin Madappally Jose, Alec Doane, Alfredo Bencomo, Allison Vanderby, Andrew Hansen, Ankur Jain, A. Anupama, Areeba Kamal, Bugu Wu, Carolina Brum, Charlie Maalouf, Chinguun Erdenebileg, Chris Dulhanty, Dominik Moritz, Doug Kang, Eduardo Jimenez, Evan Ladd, Fang Shi, Felix Bai, Frank Chu, Fred Hohman, Hadas Kotek, Hannah Gillis Coleman, Jane Li, Jeffrey P. Bigham, Jeffery Cao, Jeff Lai, Jessica Cheung, Jiulong Shan, Joe Zhou, John Li, Jun Qin, Karanjeet Singh, Karla Vega, Kelvin Zou, Laura Heckman, Lauren Gardiner, Margit Bowler, Maria Cordell, Meng Cao, Nicole Hay, Nilesh Shahdadpuri, Otto Godwin, Pranay Dighe, Pushyami Rachapudi, Ramsey Tantawi, Roman Frigg, Sam Davarnia, Sanskruti Shah, Saptarshi Guha, Sasha Sirovica, Shen Ma, Shuang Ma, Simon Wang, Sulgi Kim, Suma Jayaram, Vaishaal Shankar, Varsha Paidi, Vivek Kumar, Xin Wang, Xin Zheng, Walker Cheng, Y. Shrager, Yang Ye, Yasu Tanaka, Yihao Guo, Yun Meng, Zhaoping Luo, Ouyang Zhi, Alp Aygar, Alvin Wan, Andrew D. Walkingshaw, Tzu-Hsiang Lin, Arsalan Farooq, Brent Ramerth, Colorado Reed, Chris Bartels, Chris Chaney, David Riazati, Eric Liang Yang, Erin Feldman, Gabriel Hochstrasser, Guillaume Seguin, Irina Belousova, J. Pelemans, Karen Yang, Keivan A. Vahid, Liangliang Cao, Mahyar Najibi, Marco Zuliani, Max Horton, Minsik Cho, Nikhil Bhendawade, Patrick Dong, Piotr Maj, Pulkit Agrawal, Qi Shan, Qichen Fu, R. Poston, Sam Xu, Shuangning Liu, Sushma Rao, Tashweena Heeramun, Thomas Merth, Uday Rayala, Victor Cui, Vivek Rangarajan Sridhar, Wencong Zhang, Wenqi Zhang, Wentao Wu, Xingyu Zhou, Xinwen Liu, Yang Zhao, Yin Xia, Zhile Ren, and Zhongzheng Ren. Apple intelligence foundation language models, arXiv preprint arXiv:2407.21075, 2024. URL https://arxiv.org/abs/2407.21075v1. [354] Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large language models understand graph structured data ? an empirical evaluation and benchmarking, arXiv preprint arXiv:2305.15066, 2023. URL https://arxiv.org/abs/2305.15066v2. [355] Jing Guo, Nan Li, Jianchuan Qi, Hang Yang, Ruiqiao Li, Yuzhen Feng, Si Zhang, and Ming Xu. Empowering working memory for large language model agents, arXiv preprint arXiv:2312.17259, 2024. URL https://arxiv.org/abs/2312.17259. 85 [356] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, N. Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges. International Joint Conference on Artificial Intelligence, 2024. [357] Xiaojun Guo, Ang Li, Yifei Wang, Stefanie Jegelka, and Yisen Wang. G1: Teaching llms to reason on graphs with reinforcement learning. arXiv preprint arXiv:2505.18499, 2025. [358] Yuan Guo, Tingjia Miao, Zheng Wu, Pengzhou Cheng, Ming Zhou, and Zhuosheng Zhang. Atomicto-compositional generalization for mobile agents with new benchmark and scheduling system, arXiv preprint arXiv:2506.08972, 2025. URL https://arxiv.org/abs/2506.08972v1. [359] Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. Annual Meeting of the Association for Computational Linguistics, 2024. [360] Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. Lightrag: Simple and fast retrievalaugmented generation, arXiv preprint arXiv:2410.05779, 2024. URL https://arxiv.org/abs/ 2410.05779v3. [361] Sharut Gupta, Chenyu Wang, Yifei Wang, T. Jaakkola, and Stefanie Jegelka. In-context symmetries: Self-supervised learning through contextual world models. Neural Information Processing Systems, 2024. [362] Tanmay Gupta, Luca Weihs, and Aniruddha Kembhavi. Codenav: Beyond tool-use to using real-world codebases with llm agents, arXiv preprint arXiv:2406.12276, 2024. URL https://arxiv.org/ abs/2406.12276v1. [363] Gur, Furuta, Huang, Safdari, and Matsuo. . . . real-world webagent with planning, long context understanding, and program synthesis. 2023. URL https://arxiv.org/abs/2307. 12856. [364] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, D. Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. International Conference on Learning Representations, 2023. [365] Ashwin Kumar Gururajan, Enrique Lopez-Cuena, Jordi Bayarri-Planas, Adrián Tormos, Daniel Hinjos, Pablo Bernabeu Perez, Anna Arias-Duart, Pablo A. Martin-Torres, Lucia Urcelay-Ganzabal, Marta Gonzalez-Mallo, S. Álvarez Napagao, Eduard Ayguade-Parra, and Ulises Cortés Dario Garcia-Gasulla. Aloe: family of fine-tuned open healthcare llms, arXiv preprint arXiv:2405.01886, 2024. URL https://arxiv.org/abs/2405.01886v1. [366] Bernal Jimenez Gutierrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, and Yu Su. Hipporag: Neurobiologically inspired long-term memory for large language models. Neural Information Processing Systems, 2024. [367] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrievalaugmented language model pre-training. International Conference on Machine Learning, 2020. [368] K. Gödel, B. Meltzer, and R. Schlegel. On formally undecidable propositions of principia mathematica and related systems. arXiv preprint, 1966. 86 [369] Idan Habler, Ken Huang, Vineeth Sai Narajala, and Prashant Kulkarni. Building secure agentic ai application leveraging a2a protocol, arXiv preprint arXiv:2504.16902, 2025. URL https:// arxiv.org/abs/2504.16902v2. [370] John Halloran. Mcp safety training: Learning to refuse falsely benign mcp exploits using improved preference alignment, arXiv preprint arXiv:2505.23634, 2025. URL https://arxiv.org/abs/ 2505.23634v1. [371] Tae Jun Ham, Yejin Lee, Seong Hoon Seo, Soo-Uck Kim, Hyunji Choi, Sungjun Jung, and Jae W. Lee. Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism in neural networks. International Symposium on Computer Architecture, 2021. [372] Feijiang Han, Licheng Guo, Hengtao Cui, and Zhiyuan Lyu. Question tokens deserve more attention: Enhancing large language models without training through step-by-step reading and question attention recalibration, arXiv preprint arXiv:2504.09402, 2025. URL https://arxiv.org/abs/ 2504.09402v1. [373] Han Han, Tong Zhu, Xiang Zhang, Mengsong Wu, Hao Xiong, and Wenliang Chen. Nestools: dataset for evaluating nested tool learning abilities of large language models. International Conference on Computational Linguistics, 2024. [374] Haoyu Han, Yu Wang, Harry Shomer, Kai Guo, Jiayuan Ding, Yongjia Lei, Mahantesh Halappanavar, Ryan A. Rossi, Subhabrata Mukherjee, Xianfeng Tang, Qi He, Zhigang Hua, Bo Long, Tong Zhao, Neil Shah, Amin Javari, Yinglong Xia, and Jiliang Tang. Retrieval-augmented generation with graphs (graphrag), arXiv preprint arXiv:2501.00309, 2025. URL https://arxiv.org/abs/ 2501.00309. [375] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyun Zhao, Shiqing Ma, and Zhenyu Chen. Tokenbudget-aware llm reasoning, arXiv preprint arXiv:2412.18547, 2024. URL https://arxiv.org/ abs/2412.18547v5. [376] Yuanning Han, Ziyi Qiu, Jiale Cheng, and Ray Lc. When teams embrace ai: Human collaboration strategies in generative prompting in creative design task. International Conference on Human Factors in Computing Systems, 2024. [377] R. Hankache, Kingsley Nketia Acheampong, Liang Song, Marek Brynda, Raad Khraishi, and Greig A. Cowan. Evaluating the sensitivity of llms to prior context, arXiv preprint arXiv:2506.00069, 2025. URL https://arxiv.org/abs/2506.00069v1. [378] Hao, Liu, Wang, and Hu. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. 2023. URL https://proceedings.neurips.cc/paper_files/ paper/2023/hash/8fd1a81c882cd45f64958da6284f4a3f-Abstract-Conference. html. [379] Mohanakrishnan Hariharan. Semantic mastery: Enhancing llms with advanced natural language understanding, arXiv preprint arXiv:2504.00409, 2025. URL https://arxiv.org/abs/2504. 00409v1. [380] Mareike Hartmann and Alexander Koller. survey on complex tasks for goal-directed interactive agents, arXiv preprint arXiv:2409.18538, 2024. URL https://arxiv.org/abs/2409. 18538v1. 87 [381] A. Hassani, A. Medvedev, P. D. Haghighi, Sea Ling, A. Zaslavsky, and P. Jayaraman. Context definition and query language: Conceptual specification, implementation, and evaluation. Italian National Conference on Sensors, 2019. [382] Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, and Dustin Dannenhauer. Memory matters: The need to improve long-term memory in llm-agents. Proceedings of the AAAI Symposium Series, 2024. [383] Kostas Hatalis, Despina Christou, and Vyshnavi Kondapalli. Review of case-based reasoning for llm agents: Theoretical foundations, architectural components, and cognitive integration, arXiv preprint arXiv:2504.06943, 2025. URL https://arxiv.org/abs/2504.06943v2. [384] Jacky He, Guiran Liu, Binrong Zhu, Hanlu Zhang, Hongye Zheng, and Xiaokai Wang. Context-guided dynamic retrieval for improving generation quality in rag models, arXiv preprint arXiv:2504.19436, 2025. URL https://arxiv.org/abs/2504.19436v1. [385] Jianben He, Xingbo Wang, Shiyi Liu, Guande Wu, Claudio Silva, and Huamin Qu. Poem: Interactive prompt optimization for enhancing multimodal reasoning of large language models. IEEE Pacific Visualization Symposium, 2024. [386] Junqing He, Liang Zhu, Rui Wang, Xi Wang, Gholamreza Haffari, and Jiaxing Zhang. Madial-bench: Towards real-world evaluation of memory-augmented dialogue generation. North American Chapter of the Association for Computational Linguistics, 2024. [387] Shawn He, Surangika Ranathunga, Stephen Cranefield, and B. Savarimuthu. Norm violation detection in multi-agent systems using large language models: pilot study. COINE, 2024. [388] Shengtao He. Achieving tool calling functionality in llms using only prompt engineering without fine-tuning, arXiv preprint arXiv:2407.04997, 2024. URL https://arxiv.org/abs/2407. 04997v1. [389] Wenchong He, Liqian Peng, Zhe Jiang, and Alex Go. You only fine-tune once: Many-shot incontext fine-tuning for large language model, arXiv preprint arXiv:2506.11103, 2025. URL https: //arxiv.org/abs/2506.11103v1. [390] Xu He, Di Wu, Yan Zhai, and Kun Sun. Sentinelagent: Graph-based anomaly detection in multiagent systems, arXiv preprint arXiv:2505.24201, 2025. URL https://arxiv.org/abs/2505. 24201v1. [391] Yang He, Xiao Ding, Bibo Cai, Yufei Zhang, Kai Xiong, Zhouhao Sun, Bing Qin, and Ting Liu. Selfroute: Automatic mode switching via capability estimation for efficient reasoning. arXiv preprint, 2025. [392] Yu He, Yingxi Li, Colin White, and Ellen Vitercik. Dsr-bench: Evaluating the structural reasoning abilities of llms via data structures. arXiv preprint, 2025. [393] Zexue He, Leonid Karlinsky, Donghyun Kim, Julian McAuley, Dmitry Krotov, and Rogério Feris. Camelot: Towards large language models with training-free consolidated associative memory, arXiv preprint arXiv:2402.13449, 2024. URL https://arxiv.org/abs/2402.13449v1. [394] James B. Heald, M. Lengyel, and D. Wolpert. Contextual inference in learning and memory. Trends in Cognitive Sciences, 2022. 88 [395] Shekoofeh Hedayati, Ryan E. ODonnell, and Brad Wyble. model of working memory for latent representations. Nature Human Behaviour, 2021. [396] Tooraj Helmi. Modeling response consistency in multi-agent llm systems: comparative analysis of shared and separate context approaches, arXiv preprint arXiv:2504.07303, 2025. URL https: //arxiv.org/abs/2504.07303v1. [397] Arshia Hemmat, Kianoosh Vadaei, Mohammad Hassan Heydari, and Afsaneh Fatemi. Leveraging retrieval-augmented generation for persian university knowledge retrieval. Conference on Information and Knowledge Technology, 2024. [398] M. Herrera, Marco Pérez-Hernández, A. Kumar Parlikad, and J. Izquierdo. Multi-agent systems and complex networks: Review and applications in systems engineering. Processes, 2020. [399] Nora A. Herweg, A. Sharan, M. Sperling, A. Brandt, A. Schulze-Bonhage, and M. Kahana. Reactivated spatial context guides episodic recall. Journal of Neuroscience, 2018. [400] Thomas F. Heston and Charya Khun. Prompt engineering in medical education. International Medical Education, 2023. [401] Dollaya Hirunyasiri, Danielle R. Thomas, Jionghao Lin, K. Koedinger, and Vincent Aleven. Comparative analysis of gpt-4 and human graders in evaluating human tutors giving praise to students. Human-AI Math Tutoring@AIED, 2023. [402] Thomas Hoang. Gnn: Graph neural network and large language model for data discovery, arXiv preprint arXiv:2408.13609, 2024. URL https://arxiv.org/abs/2408.13609v2. [403] W. Hoek and M. Wooldridge. Towards logic of rational agency. Logic Journal of the IGPL, 2003. [404] Aidan Hogan, E. Blomqvist, Michael Cochez, C. dAmato, Gerard de Melo, C. Gutierrez, J. E. L. Gayo, S. Kirrane, S. Neumaier, A. Polleres, Roberto Navigli, A. Ngomo, S. M. Rashid, Anisa Rula, Lukas Schmelzeisen, Juan Sequeda, Steffen Staab, and Antoine Zimmermann. Knowledge graphs. ACM Computing Surveys, 2020. [405] Nithin Holla, Pushkar Mishra, H. Yannakoudakis, and Ekaterina Shutova. Meta-learning with sparse experience replay for lifelong language learning, arXiv preprint arXiv:2009.04891, 2020. URL https://arxiv.org/abs/2009.04891v2. [406] Chuanyang Hong and Qingyun He. Enhancing memory retrieval in generative agents through llm-trained cross attention networks. Frontiers in Psychology, 2025. [407] M. Hong, Sean M. Polyn, and Lisa K. Fazio. Examining the episodic context account: does retrieval practice enhance memory for context? Cognitive Research, 2019. [408] Sirui Hong, Xiawu Zheng, Jonathan P. Chen, Yuheng Cheng, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Z. Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, and Chenglin Wu. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint, 2023. [409] Sirui Hong, Mingchen Zhuge, Jiaqi Chen, Xiawu Zheng, Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber. Metagpt: Meta programming for multi-agent collaborative framework, arXiv preprint arXiv:2308.00352, 2024. URL https://arxiv.org/abs/2308.00352. 89 [410] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: visual language model for gui agents. Computer Vision and Pattern Recognition, 2023. [411] Xiangyu Hong, Che Jiang, Biqing Qi, Fandong Meng, Mo Yu, Bowen Zhou, and Jie Zhou. On the token distance modeling ability of higher rope attention dimension. Conference on Empirical Methods in Natural Language Processing, 2024. [412] Yubin Hong, Chaofan Li, Jingyi Zhang, and Yingxia Shao. Fg-rag: Enhancing query-focused summarization with context-aware fine-grained graph rag. arXiv preprint, 2025. [413] Thanapapas Horsuwan, Piyawat Lertvittayakumjorn, Kasidis Kanwatchara, B. Kijsirikul, and P. Vateekul. Meta lifelong-learning with selective and task-aware adaptation. IEEE Access, 2024. [414] A. N. Hoskin, A. Bornstein, K. Norman, and J. Cohen. Refresh my memory: Episodic memory reinstatements intrude on working memory maintenance. Cognitive, Affective, & Behavioral Neuroscience, 2017. [415] Timothy M. Hospedales, Antreas Antoniou, P. Micaelli, and A. Storkey. Meta-learning in neural networks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020. [416] Peyman Hosseini, Ignacio Castro, Iacopo Ghinassi, and Matthew Purver. Efficient solutions for an intriguing failure of llms: Long context window does not mean llms can analyze long sequences flawlessly. International Conference on Computational Linguistics, 2024. [417] Haowen Hou, Fei Ma, Binwen Bai, Xinxin Zhu, and F. Yu. Enhancing and accelerating large language models via instruction-aware contextual compression, arXiv preprint arXiv:2408.15491, 2024. URL https://arxiv.org/abs/2408.15491v1. [418] Wenjun Hou, Yi Cheng, Kaishuai Xu, Yan Hu, Wenjie Li, and Jiangming Liu. Memory-augmented multimodal llms for surgical vqa via self-contained inquiry, arXiv preprint arXiv:2411.10937v1, 2024. URL https://arxiv.org/abs/2411.10937v1. [419] Xinyi Hou, Yanjie Zhao, Shenao Wang, and Haoyu Wang. Model context protocol (mcp): Landscape, security threats, and future research directions, arXiv preprint arXiv:2503.23278, 2025. URL https://arxiv.org/abs/2503.23278v2. [420] Zejiang Hou, Julian Salazar, and George Polovets. Meta-learning the difference: Preparing large language models for efficient adaptation. Transactions of the Association for Computational Linguistics, 2022. [421] N. Houlsby, A. Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and S. Gelly. Parameter-efficient transfer learning for nlp. International Conference on Machine Learning, 2019. [422] Marc Howard and M. Kahana. distributed representation of temporal context. arXiv preprint, 2002. [423] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, J. Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory, arXiv preprint arXiv:2306.03901, 2023. URL https: //arxiv.org/abs/2306.03901v2. [424] J. E. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. International Conference on Learning Representations, 2021. [425] Junhao Hu, Wenrui Huang, Weidong Wang, Zhenwen Li, Tiancheng Hu, Zhixia Liu, XuSheng Chen, Tao Xie, and Yizhou Shan. Raas: Reasoning-aware attention sparsity for efficient llm reasoning, arXiv preprint arXiv:2502.11147, 2025. URL https://arxiv.org/abs/2502.11147v2. [426] Junwei Hu, Weicheng Zheng, Yan Liu, and Yihan Liu. Optimizing token consumption in llms: nano surge approach for code reasoning efficiency, arXiv preprint arXiv:2504.15989, 2025. URL https://arxiv.org/abs/2504.15989v2. [427] Junyan Hu, Hanlin Niu, J. Carrasco, B. Lennox, and F. Arvin. Voronoi-based multi-robot autonomous IEEE Transactions on exploration in unknown environments via deep reinforcement learning. Vehicular Technology, 2020. [428] Linmei Hu, Zeyi Liu, Ziwang Zhao, Lei Hou, Liqiang Nie, and Juanzi Li. survey of knowledge enhanced pre-trained language models. IEEE Transactions on Knowledge and Data Engineering, 2022. [429] Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, and Ping Luo. Hiagent: Hierarchical working memory management for solving long-horizon agent tasks with large language model, arXiv preprint arXiv:2408.09559, 2024. URL https://arxiv.org/abs/2408.09559. [430] Nathan J. Hu, E. Mitchell, Christopher D. Manning, and Chelsea Finn. Meta-learning online adaptation of language models. Conference on Empirical Methods in Natural Language Processing, 2023. [431] Shengxiang Hu, Guobing Zou, Song Yang, Yanglan Gan, Bofeng Zhang, and Yixin Chen. Large language model meets graph neural network in knowledge distillation. AAAI Conference on Artificial Intelligence, 2024. [432] Siyuan Hu, Mingyu Ouyang, Difei Gao, and Mike Zheng Shou. The dawn of gui agent: preliminary case study with claude 3.5 computer use. arXiv preprint, 2024. [433] Ting Hu, Christoph Meinel, and Haojin Yang. Scaled prompt-tuning for few-shot natural language generation, arXiv preprint arXiv:2309.06759, 2023. URL https://arxiv.org/abs/ 2309.06759v1. [434] Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, and Zhenzhong Lan. Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas, arXiv preprint arXiv:2410.14255, 2024. URL https://arxiv. org/abs/2410.14255v2. [435] Yuntong Hu, Zhengwu Zhang, and Liang Zhao. Beyond text: deep dive into large language models ability on understanding graph data, arXiv preprint arXiv:2310.04944, 2023. URL https: //arxiv.org/abs/2310.04944v1. [436] Yilun Hua and Yoav Artzi. Talk less, interact better: Evaluating in-context conversational adaptation in multimodal llms, arXiv preprint arXiv:2408.01417v1, 2024. URL https://arxiv.org/abs/ 2408.01417v1. 91 [437] Brandon Huang, Chancharik Mitra, Assaf Arbelle, Leonid Karlinsky, Trevor Darrell, and Roei Herzig. Multimodal task vectors enable many-shot multimodal in-context learning. Neural Information Processing Systems, 2024. [438] Chengkai Huang, Hongtao Huang, Tong Yu, Kaige Xie, Junda Wu, Shuai Zhang, Julian J. McAuley, Dietmar Jannach, and Lina Yao. survey of foundation model-powered recommender systems: From feature-based, generative to agentic paradigms, arXiv preprint arXiv:2504.16420, 2025. URL https://arxiv.org/abs/2504.16420v1. [439] Chengkai Huang, Junda Wu, Yu Xia, Zixu Yu, Ruhan Wang, Tong Yu, Ruiyi Zhang, Ryan A. Rossi, B. Kveton, Dongruo Zhou, Julian J. McAuley, and Lina Yao. Towards agentic recommender systems in the era of multimodal large language models, arXiv preprint arXiv:2503.16734, 2025. URL https://arxiv.org/abs/2503.16734v1. [440] Chengrui Huang, Shen Gao, Zhengliang Shi, Dongsheng Wang, and Shuo Shang. Ttpa: Tokenlevel tool-use preference alignment training framework with fine-grained evaluation, arXiv preprint arXiv:2505.20016, 2025. URL https://arxiv.org/abs/2505.20016v1. [441] Chensen Huang, Guibo Zhu, Xuepeng Wang, Yifei Luo, Guojing Ge, Haoran Chen, Dong Yi, and Jinqiao Wang. Recurrent context compression: Efficiently expanding the context window of llm, arXiv preprint arXiv:2406.06110, 2024. URL https://arxiv.org/abs/2406.06110v1. [442] Jing Huang, X. Ruan, Naigong Yu, Qingwu Fan, Jiaming Li, and Jianxian Cai. cognitive model based on neuromodulated plasticity. Computational Intelligence and Neuroscience, 2016. [443] Ken Huang, Akram Sheriff, Vineeth Sai Narajala, and Idan Habler. Agent capability negotiation and binding protocol (acnbp), arXiv preprint arXiv:2506.13590, 2025. URL https://arxiv.org/ abs/2506.13590v1. [444] Le Huang, Hengzhi Lan, Zijun Sun, Chuan Shi, and Ting Bai. Emotional rag: Enhancing role-playing agents through emotional retrieval. 2024 IEEE International Conference on Knowledge Graph (ICKG), 2024. [445] Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, and Wayne Xin Zhao. Manusearch: Democratizing deep search in large language models with transparent and open multi-agent framework, arXiv preprint arXiv:2505.18105, 2025. URL https://arxiv.org/ abs/2505.18105v1. [446] Shiting Huang, Zhen Fang, Zehui Chen, Siyu Yuan, Junjie Ye, Yu Zeng, Lin Chen, Qi Mao, and Feng Zhao. Critictool: Evaluating self-critique capabilities of large language models in tool-calling error scenarios, arXiv preprint arXiv:2506.13977, 2025. URL https://arxiv.org/abs/2506. 13977v1. [447] Sirui Huang, Yanggan Gu, Xuming Hu, Zhonghao Li, Qing Li, and Guandong Xu. Reasoning factual knowledge in structured data with large language models, arXiv preprint arXiv:2408.12188, 2024. URL https://arxiv.org/abs/2408.12188v1. [448] Sirui Huang, Hanqian Li, Yanggan Gu, Xuming Hu, Qing Li, and Guandong Xu. Hyperg: Hypergraphenhanced llms for structured knowledge, arXiv preprint arXiv:2502.18125, 2025. URL https: //arxiv.org/abs/2502.18125v1. [449] Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. International Conference on Machine Learning, 2022. [450] Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. Recommender ai agent: Integrating large language models for interactive recommendations, arXiv preprint arXiv:2308.16505, 2024. URL https://arxiv.org/abs/2308.16505. [451] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. Understanding the planning of llm agents: survey, arXiv preprint arXiv:2402.02716, 2024. URL https://arxiv.org/abs/2402.02716v1. [452] Huang, Shi, Li, Fan, Wu, and Zhang. . . . Metatool benchmark for large language models: Deciding whether to use tools and which to use. 2023. URL https://arxiv.org/abs/2310. 03128. [453] Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, and Xiaoxing Ma. Advancing transformer architecture in long-context large language models: comprehensive survey, arXiv preprint arXiv:2311.12351, 2023. URL https: //arxiv.org/abs/2311.12351v2. [454] Zeyi Huang, Yuyang Ji, Anirudh Sundara Rajan, Zefan Cai, Wen Xiao, Junjie Hu, and Yong Jae Lee. Visualtoolagent (vista): reinforcement learning framework for visual tool selection, arXiv preprint arXiv:2505.20289, 2025. URL https://arxiv.org/abs/2505.20289v1. [455] Ziheng Huang, S. Gutierrez, Hemanth Kamana, and S. Macneil. Memory sandbox: Transparent and interactive memory management for conversational agents. ACM Symposium on User Interface Software and Technology, 2023. [456] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. Memory sandbox: Transparent and interactive memory management for conversational agents, arXiv preprint arXiv:2308.01542, 2023. URL https://arxiv.org/abs/2308.01542. [457] Alexis Huet, Zied Ben-Houidi, and Dario Rossi. Episodic memories generation and evaluation benchmark for large language models. International Conference on Learning Representations, 2025. [458] Dom Huh and Prasant Mohapatra. Multi-agent reinforcement learning: comprehensive survey, arXiv preprint arXiv:2312.10256, 2023. URL https://arxiv.org/abs/2312.10256v2. [459] Eunjeong Hwang, Yichao Zhou, James Bradley Wendt, Beliz Gunel, Nguyen Vo, Jing Xie, and Sandeep Tata. Enhancing incremental summarization with structured representations. Conference on Empirical Methods in Natural Language Processing, 2024. [460] Thorsten Händler. Balancing autonomy and alignment: multi-dimensional taxonomy for autonomous llm-powered multi-agent architectures. arXiv preprint, 2023. [461] Michael Iannelli, Sneha Kuchipudi, and Vera Dvorak. Sla management in reconfigurable multi-agent rag: systems approach to question answering, arXiv preprint arXiv:2412.06832, 2024. URL https://arxiv.org/abs/2412.06832v2. [462] IBM. What is agent communication protocol (acp)? https://www.ibm.com/think/topics/ agent-communication-protocol, 2025. [Online; accessed 17-July-2025]. 93 [463] Inaba, Kiyomaru, Cheng, and Kurohashi. Multitool-cot: Gpt-3 can use multiple external tools with chain of thought prompting. 2023. URL https://arxiv.org/abs/2305.16896. [464] G. Indiveri and Shih-Chii Liu. Memory and information processing in neuromorphic systems. Proceedings of the IEEE, 2015. [465] V. Ioannidis, Xiang Song, Da Zheng, Houyu Zhang, Jun Ma, Yi Xu, Belinda Zeng, Trishul M. Chilimbi, and G. Karypis. Efficient and effective training of language and graph neural network models, arXiv preprint arXiv:2206.10781, 2022. URL https://arxiv.org/abs/2206.10781v1. [466] Yoichi Ishibashi, Taro Yano, and M. Oyamada. Can large language models invent algorithms to improve themselves?: Algorithm discovery for recursive self-improvement through reinforcement learning, arXiv preprint arXiv:2410.15639, 2024. URL https://arxiv.org/abs/2410.15639v5. [467] Shadi Iskander, Nachshon Cohen, Zohar S. Karnin, Ori Shapira, and Sofia Tolmach. Quality matters: Evaluating synthetic data for tool-using llms. Conference on Empirical Methods in Natural Language Processing, 2024. [468] Z. Ismail and N. Sariff. survey and analysis of cooperative multi-agent robot systems: Challenges and directions. Applications of Mobile Robots, 2018. [469] Yusuf Izmirlioglu, Loc Pham, Tran Cao Son, and Enrico Pontelli. survey of multi-agent systems for smartgrids. Energies, 2024. [470] Jace.AI. Jace.ai web agent, 2024. URL https://www.jace.ai/. Accessed: 2025-07-14. [471] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Neural Information Processing Systems, 2018. [472] Tejas Jade and Alex Yartsev. Chatgpt for automated grading of short answer questions in mechanical ventilation, arXiv preprint arXiv:2505.04645, 2025. URL https://arxiv.org/abs/2505. 04645v1. [473] A. Jafarpour, L. Fuentemilla, A. Horner, W. Penny, and E. Duzel. Replay of very early encoding representations during recollection. Journal of Neuroscience, 2014. [474] A. Jaiswal, Nurendra Choudhary, Ravinarayana Adkathimar, M. P. Alagappan, G. Hiranandani, Ying Ding, Zhangyang Wang, E-Wen Huang, and Karthik Subbian. All against some: Efficient integration of large language models for message passing in graph neural networks, arXiv preprint arXiv:2407.14996, 2024. URL https://arxiv.org/abs/2407.14996v1. [475] H. Jaleel, Jane J. Stephan, and Sinan Naji. Multi-agent systems: review study. Ibn ALHaitham Journal For Pure and Applied Sciences, 2020. [476] Lawrence Jang, Yinheng Li, Charles Ding, Justin Lin, Paul Pu Liang, Dan Zhao, Rogerio Bonatti, and K. Koishida. Videowebarena: Evaluating long context multimodal agents with video understanding web tasks. International Conference on Learning Representations, 2024. [477] R. Janik. Aspects of human memory and large language models, arXiv preprint arXiv:2311.03839, 2023. URL https://arxiv.org/abs/2311.03839v3. [478] Shumaila Javaid, Hamza Fahim, Bin He, and Nasir Saeed. Large language models for uavs: Current state and pathways to the future. IEEE Open Journal of Vehicular Technology, 2024. 94 [479] T. S. Jayram, Younes Bouhadjar, Ryan L. McAvoy, Tomasz Kornuta, Alexis Asseman, K. Rocki, and A. Ozcan. Learning to remember, forget and ignore using attention control in memory, arXiv preprint arXiv:1809.11087, 2018. URL https://arxiv.org/abs/1809.11087v1. [480] Cheonsu Jeong. study on the mcp a2a framework for enhancing interoperability of llm-based autonomous agents, arXiv preprint arXiv:2506.01804, 2025. URL https://arxiv.org/abs/ 2506.01804v2. [481] Gang Ji and J. Bilmes. Multi-speaker language modeling. North American Chapter of the Association for Computational Linguistics, 2004. [482] Ke Ji, Junying Chen, Anningzhe Gao, Wenya Xie, Xiang Wan, and Benyou Wang. Llms could autonomously learn without external supervision, arXiv preprint arXiv:2406.00606, 2024. URL https://arxiv.org/abs/2406.00606v2. [483] Shaoxiong Ji, Shirui Pan, E. Cambria, P. Marttinen, and Philip S. Yu. survey on knowledge graphs: Representation, acquisition, and applications. IEEE Transactions on Neural Networks and Learning Systems, 2020. [484] Bowen Jiang, Runchuan Zhu, Jiang Wu, Zinco Jiang, Yifan He, Junyuan Gao, Jia Yu, Rui Min, Yinfan Wang, Haote Yang, et al. Evaluating large language model with knowledge oriented language specific simple question answering. 2025. [485] Caigao Jiang, Siqiao Xue, James Zhang, Lingyue Liu, Zhibo Zhu, and Hongyan Hao. Learning largescale universal user representation with sparse mixture of experts, arXiv preprint arXiv:2207.04648, 2022. URL https://arxiv.org/abs/2207.04648v1. [486] Feibo Jiang, Li Dong, Yubo Peng, Kezhi Wang, Kun Yang, Cunhua Pan, D. Niyato, and O. Dobre. Large language model enhanced multi-agent systems for 6g communications. IEEE wireless communications, 2023. [487] Jiang, Zhou, WX Zhao, Song, and Zhu. . . . Kg-agent: An efficient autonomous agent framework for complex reasoning over knowledge graph. 2024. URL https://arxiv.org/abs/ 2402.11163. [488] Jinhao Jiang, Kun Zhou, Wayne Xin Zhao, and Ji rong Wen. Unikgqa: Unified retrieval and reasoning for solving multi-hop question answering over knowledge graph. International Conference on Learning Representations, 2022. [489] Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, and Ji rong Wen. Structgpt: general framework for large language model to reason over structured data. Conference on Empirical Methods in Natural Language Processing, 2023. [490] Song Jiang, Zahra Shakeri, Aaron Chan, Maziar Sanjabi, Hamed Firooz, Yinglong Xia, Bugra Akyildiz, Yizhou Sun, Jinchao Li, Qifan Wang, and Asli Celikyilmaz. Resprompt: Residual connection prompting advances multi-step reasoning in large language models. North American Chapter of the Association for Computational Linguistics, 2023. [491] Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. Active retrieval augmented generation. Conference on Empirical Methods in Natural Language Processing, 2023. 95 [492] Zhonglin Jiang, Qian Tang, and Zequn Wang. Generative reliability-based design optimization using in-context learning capabilities of large language models, arXiv preprint arXiv:2503.22401, 2025. URL https://arxiv.org/abs/2503.22401v1. [493] Zhuoxuan Jiang, Haoyuan Peng, Shanshan Feng, Fan Li, and Dongsheng Li. Llms can find mathematical reasoning mistakes by pedagogical chain-of-thought. International Joint Conference on Artificial Intelligence, 2024. [494] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues?, arXiv preprint arXiv:2310.06770, 2024. URL https://arxiv.org/abs/2310.06770. [495] Jin, Xie, Zhang, KK Roy, Zhang, and Li. . . . Graph chain-of-thought: Augmenting large language models by reasoning on graphs. 2024. URL https://arxiv.org/abs/2404.07103. [496] Bowen Jin, Yu Zhang, Qi Zhu, and Jiawei Han. Heterformer: Transformer-based deep node representation learning on heterogeneous text-rich networks. Knowledge Discovery and Data Mining, 2022. [497] Feihu Jin, Jiajun Zhang, and Chengqing Zong. Parameter-efficient tuning for large language model without calculating its gradients. Conference on Empirical Methods in Natural Language Processing, 2023. [498] Haolin Jin, Linghan Huang, Haipeng Cai, Jun Yan, Bo Li, and Huaming Chen. From llms to llmbased agents for software engineering: survey of current, challenges and future, arXiv preprint arXiv:2408.02479, 2024. URL https://arxiv.org/abs/2408.02479v2. [499] Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia yuan Chang, Huiyuan Chen, and Xia Hu. Llm maybe longlm: Self-extend llm context window without tuning. International Conference on Machine Learning, 2024. [500] Jiajie Jin, Yutao Zhu, Xinyu Yang, Chenghao Zhang, and Zhicheng Dou. Flashrag: modular toolkit for efficient retrieval-augmented generation research. The Web Conference, 2024. [501] Qiao Jin, Yifan Yang, Qingyu Chen, and Zhiyong Lu. Genegpt: Augmenting large language models with domain tools for improved access to biomedical information. Bioinform., 2023. [502] Tian Jin, W. Yazar, Zifei Xu, Sayeh Sharify, and Xin Wang. Self-selected attention span for accelerating large language model inference, arXiv preprint arXiv:2404.09336, 2024. URL https://arxiv. org/abs/2404.09336v1. [503] Weiqiang Jin, Hongyang Du, Biao Zhao, Xingwu Tian, Bohang Shi, and Guang Yang. comprehensive survey on multi-agent cooperative decision-making: Scenarios, approaches, challenges and perspectives. arXiv preprint, 2025. [504] Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, Di Zhang, Wenwu Ou, Kun Gai, and Yadong Mu. Unified language-vision pretraining in llm with dynamic discrete visual tokenization. International Conference on Learning Representations, 2023. 96 [505] Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, and Srijan Kumar. Mm-soc: Benchmarking multimodal large language models in social media platforms. Annual Meeting of the Association for Computational Linguistics, 2024. [506] Yiyang Jin, Kunzhao Xu, Hang Li, Xueting Han, Yanmin Zhou, Cheng Li, and Jing Bai. Reveal: Self-evolving code agents via iterative generation-verification, arXiv preprint arXiv:2506.11442, 2025. URL https://arxiv.org/abs/2506.11442v1. [507] Jeff Johnson, Matthijs Douze, and H. Jégou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 2017. [508] Jeff A. Johnson and Daniel H. Bullock. Fragility in ais using artificial neural networks. Communications of the ACM, 2023. [509] Zhao Kaiya, Michelangelo Naim, J. Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert Yang, and Andrew Ahn. Lyfe agents: Generative agents for low-cost real-time social interactions, arXiv preprint arXiv:2310.02172, 2023. URL https://arxiv.org/abs/2310.02172v1. [510] Kurmanbek Kaiyrbekov, Nic Dobbins, and Sean D. Mooney. Automated survey collection with llmbased conversational agents, arXiv preprint arXiv:2504.02891, 2025. URL https://arxiv.org/ abs/2504.02891v1. [511] A. Kakas, P. Mancarella, F. Sadri, Kostas Stathis, and Francesca Toni. Computational logic foundations of kgp agents. Journal of Artificial Intelligence Research, 2008. [512] Vikas Kamra, Lakshya Gupta, Dhruv Arora, and Ashwin Kumar Yadav. Enhancing document retrieval using ai and graph-based rag techniques. 2024 5th International Conference on Communication, Computing & Industry 6.0 (C2I6), 2024. [513] Eser Kandogan, Nikita Bhutani, Dan Zhang, Rafael Li Chen, Sairam Gurajada, and Estevam R. Hruschka. Orchestrating agents and data for enterprise: blueprint architecture for compound ai, arXiv preprint arXiv:2504.08148, 2025. URL https://arxiv.org/abs/2504.08148v1. [514] Haoyu Kang, Yuzhou Zhu, Yukun Zhong, Ke Wang Central South University, Dalian University of Technology, Nanjing University, and Xidian University. Sakr: Enhancing retrieval-augmented generation via streaming algorithm and k-means clustering. arXiv preprint, 2024. [515] Jiazheng Kang, Mingming Ji, Zhe Zhao, and Ting Bai. Memory os of ai agent, arXiv preprint arXiv:2506.06326, 2025. URL https://arxiv.org/abs/2506.06326v1. [516] Jikun Kang, Wenqi Wu, Filippos Christianos, Alex J. Chan, Fraser Greenlee, George Thomas, Marvin Purtorab, and Andy Toulis. Lm2: Large memory models, arXiv preprint arXiv:2502.06049, 2025. URL https://arxiv.org/abs/2502.06049v1. [517] Sungmin Kang, Gabin An, and S. Yoo. quantitative and qualitative evaluation of llm-based explainable fault localization. Proc. ACM Softw. Eng., 2023. [518] Guy Kaplan, Matanel Oren, Yuval Reif, and Roy Schwartz. From tokens to words: On the inner lexicon of llms. International Conference on Learning Representations, 2024. [519] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Yu Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. Dense passage retrieval for open-domain question answering. Conference on Empirical Methods in Natural Language Processing, 2020. [520] Zdeněk Kasner and Ondrej Dusek. Beyond traditional benchmarks: Analyzing behaviors of open llms on data-to-text generation. Annual Meeting of the Association for Computational Linguistics, 2024. [521] Kiran Kate, Tejaswini Pedapati, Kinjal Basu, Yara Rizk, Vijil Chenthamarakshan, Subhajit Chaudhury, Mayank Agarwal, and Ibrahim Abdelaziz. Longfunceval: Measuring the effectiveness of long context models for function calling, arXiv preprint arXiv:2505.10570, 2025. URL https://arxiv.org/ abs/2505.10570v1. [522] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franccois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. International Conference on Machine Learning, 2020. [523] Richard Katrix, Quentin Carroway, Rowan Hawkesbury, and Matthias Heathfield. Context-aware semantic recomposition mechanism for large language models, arXiv preprint arXiv:2501.17386, 2025. URL https://arxiv.org/abs/2501.17386v2. [524] Amirhossein Kazemnejad, Inkit Padhi, K. Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Neural Information Processing Systems, 2023. [525] T. Kelley, R. Thomson, and Jonathan Milton. Standard model of mind: Episodic memory. Biologically Inspired Cognitive Architectures, 2018. [526] Daan Kepel and Konstantina Valogianni. Autonomous prompt engineering in large language models, arXiv preprint arXiv:2407.11000, 2024. URL https://arxiv.org/abs/2407.11000v1. [527] R. Kesner. Neurobiological foundations of an attribute model of memory. arXiv preprint, 2013. [528] A. Khan, Md Toufique Hasan, Kai-Kristian Kemell, Jussi Rasku, and Pekka Abrahamsson. Developing retrieval augmented generation (rag) based llm systems from pdfs: An experience report, arXiv preprint arXiv:2410.15944, 2024. URL https://arxiv.org/abs/2410.15944v1. [529] Muhammad Tayyab Khan, Lequn Chen, Ye Han Ng, Wenhe Feng, Nicholas Yew Jin Tan, and Seung Ki Moon. Leveraging vision-language models for manufacturing feature recognition in cad designs, arXiv preprint arXiv:2411.02810, 2024. URL https://arxiv.org/abs/2411.02810v1. [530] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and M. Lewis. Generalization through memorization: Nearest neighbor language models. International Conference on Learning Representations, 2019. [531] Elahe Khatibi, Ziyu Wang, and Amir M. Rahmani. Cdf-rag: Causal dynamic feedback for adaptive retrieval-augmented generation. arXiv preprint, 2025. [532] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Neural Information Processing Systems, 2020. [533] Tushar Khot, H. Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Decomposed prompting: modular approach for solving complex tasks. International Conference on Learning Representations, 2022. 98 [534] Sambhav Khurana, Xiner Li, Shurui Gui, and Shuiwang Ji. hierarchical language model for interpretable graph reasoning, arXiv preprint arXiv:2410.22372, 2024. URL https://arxiv. org/abs/2410.22372v1. [535] Daehee Kim, Deokhyung Kang, Sangwon Ryu, and Gary Geunbae Lee. Ontology-free general-domain knowledge graph-to-text generation dataset synthesis using large language model, arXiv preprint arXiv:2409.07088, 2024. URL https://arxiv.org/abs/2409.07088v1. [536] Geunwoo Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks. Neural Information Processing Systems, 2023. [537] Jaeyeon Kim, Injune Hwang, and Kyogu Lee. Learning semantic information from raw audio signal using both contextual and phonetic representations. IEEE International Conference on Acoustics, Speech, and Signal Processing, 2024. [538] Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, and Hyun Oh Song. Compressed context memory for online language model interaction. International Conference on Learning Representations, 2023. [539] Jiho Kim, Yeonsu Kwon, Yohan Jo, and Edward Choi. Kg-gpt: general framework for reasoning on knowledge graphs using large language models. Conference on Empirical Methods in Natural Language Processing, 2023. [540] Jiin Kim, Byeongjun Shin, Jinha Chung, and Minsoo Rhu. The cost of dynamic reasoning: Demystifying ai agents and test-time scaling from an ai infrastructure perspective, arXiv preprint arXiv:2506.04301, 2025. URL https://arxiv.org/abs/2506.04301v1. [541] Sehoon Kim, Suhong Moon, Ryan Tabrizi, Nicholas Lee, Michael W. Mahoney, Kurt Keutzer, and A. Gholami. An llm compiler for parallel function calling. International Conference on Machine Learning, 2023. [542] Taewoon Kim, Michael Cochez, Vincent Francois-Lavet, Mark Neerincx, and Piek Vossen. machine with short-term, episodic, and semantic memory systems. Proceedings of the AAAI Conference on Artificial Intelligence, 37(1):4856, 2023. ISSN 2159-5399. doi: 10.1609/aaai.v37i1.25075. URL http://dx.doi.org/10.1609/aaai.v37i1.25075. [543] Lukas Kirchdorfer, Robert Blümel, T. Kampik, Han van der Aa, and Heiner Stuckenschmidt. Discovering multi-agent systems for resource-centric business process simulation. Process Science, 2025. [544] J. Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, J. Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, and R. Hadsell. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences of the United States of America, 2016. [545] Louis Kirsch, James Harrison, Jascha Narain Sohl-Dickstein, and Luke Metz. General-purpose in-context learning by meta-learning transformers, arXiv preprint arXiv:2212.04458, 2022. URL https://arxiv.org/abs/2212.04458v2. [546] Yuval Kirstain, Patrick Lewis, Sebastian Riedel, and Omer Levy. few more examples may be worth billions of parameters. Conference on Empirical Methods in Natural Language Processing, 2021. 99 [547] Andrew Kiruluta, Preethi Raju, and Priscilla Burity. Breaking quadratic barriers: non-attention llm for ultra-long context horizons, arXiv preprint arXiv:2506.01963, 2025. URL https://arxiv. org/abs/2506.01963v1. [548] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. International Conference on Learning Representations, 2020. [549] Vincent Koc, Jacques Verre, Douglas Blank, and Abigail Morgan. Mind the metrics: Patterns for telemetry-aware in-ide ai application development using the model context protocol (mcp), arXiv preprint arXiv:2506.11019, 2025. URL https://arxiv.org/abs/2506.11019v1. [550] Tomás Kociský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 2017. [551] Jing Yu Koh, R. Salakhutdinov, and Daniel Fried. Grounding language models to images for multimodal inputs and outputs. International Conference on Machine Learning, 2023. [552] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks, arXiv preprint arXiv:2401.13649, 2024. URL https://arxiv.org/abs/2401.13649v2. [553] Takeshi Kojima, S. Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Neural Information Processing Systems, 2022. [554] Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. Tptu-v2: Boosting task planning and tool usage of large language model-based agents in real-world systems, arXiv preprint arXiv:2311.11315, 2023. URL https://arxiv.org/abs/2311.11315. [555] P. Korzyński, G. Mazurek, Pamela Krzypkowska, and Artur Kurasiński. Artificial intelligence prompt engineering as new digital competence: Analysis of generative ai technologies such as chatgpt. Entrepreneurial Business and Economics Review, 2023. [556] Oliver Kramer. Cognitive prompts using guilfords structure of intellect model. arXiv preprint, 2025. [557] Oliver Kramer. Conceptual metaphor theory as prompting paradigm for large language models, arXiv preprint arXiv:2502.01901, 2025. URL https://arxiv.org/abs/2502.01901v1. [558] Oliver Kramer and Jill Baumann. Unlocking structured thinking in language models with cognitive prompting. ESANN 2025 proceesdings, 2024. [559] K. Kravari and Nick Bassiliades. survey of agent platforms. Journal of Artificial Societies and Social Simulation, 2015. [560] Prashant Krishnan, Zilong Wang, Yangkun Wang, and Jingbo Shang. Towards few-shot entity recognition in document images: graph neural network approach robust to image manipulation. International Conference on Language Resources and Evaluation, 2023. [561] W. Kruijne, S. Bohté, P. Roelfsema, and C. Olivers. Flexible working memory through selective gating and attentional tagging. bioRxiv, 2019. [562] L. Krupp, Daniel Geissler, P. Lukowicz, and Jakob Karolus. Towards sustainable web agents: plea for transparency and dedicated metrics for energy consumption, arXiv preprint arXiv:2502.17903, 2025. URL https://arxiv.org/abs/2502.17903v1. [563] M. Kuhail, Jose Berengueres, Fatma Taher, Sana Z. Khan, and Ansah Siddiqui. Designing haptic boot for space with prompt engineering: Process, insights, and implications. IEEE Access, 2024. [564] Amandeep Kumar, Muzammal Naseer, Sanath Narayan, R. Anwer, Salman H. Khan, and Hisham Cholakkal. Multi-modal generation via cross-modal in-context learning, arXiv preprint arXiv:2405.18304v1, 2024. URL https://arxiv.org/abs/2405.18304v1. [565] Rajeev Kumar, Harishankar Kumar, and Kumari Shalini. Detecting and mitigating bias in llms through knowledge graph-augmented training. 2025 International Conference on Artificial Intelligence and Data Engineering (AIDE), 2025. [566] Taeyoon Kwon, Dongwook Choi, Sunghwan Kim, Hyojun Kim, Seungjun Moon, Beong woo Kwak, Kuan-Hao Huang, and Jinyoung Yeo. Embodied agents meet personalization: Exploring memory utilization for personalized assistance, arXiv preprint arXiv:2505.16348, 2025. URL https:// arxiv.org/abs/2505.16348v1. [567] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Haotong Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. Symposium on Operating Systems Principles, 2023. [568] T. Lai, Quan Hung Tran, Trung Bui, and D. Kihara. gated self-attention memory network for answer selection. Conference on Empirical Methods in Natural Language Processing, 2019. [569] Divya Lamba. The role of prompt engineering in improving language understanding and generation. International Journal For Multidisciplinary Research, 2024. [570] Xiaochong Lan, Jie Feng, Jia Lei, Xinlei Shi, and Yong Li. Benchmarking and advancing large language models for local life services, arXiv preprint arXiv:2506.02720, 2025. URL https: //arxiv.org/abs/2506.02720v1. [571] LangChain Team. Memory in langgraph. https://langchain-ai.github.io/langgraph/ concepts/memory/, 2025. Accessed: 2025-07-17. [572] Samuel T. Langlois, Oghenetekevwe Akoroda, Estefany Carrillo, J. Herrmann, S. Azarm, Huan Xu, and Michael W. Otte. Metareasoning structures, problems, and modes for multiagent systems: survey. IEEE Access, 2020. [573] B. Lattimer, Varun Gangal, Ryan McDonald, and Yi Yang. Sparse rewards can self-train dialogue agents, arXiv preprint arXiv:2409.04617, 2024. URL https://arxiv.org/abs/2409. 04617v2. [574] Pak Kin Lau and Stuart Michael McManus. Mining asymmetric intertextuality, arXiv preprint arXiv:2410.15145, 2024. URL https://arxiv.org/abs/2410.15145v1. [575] Hung Le, T. Tran, and S. Venkatesh. Self-attentive associative memory. International Conference on Machine Learning, 2020. 101 [576] Dohyun Lee, Seungil Chad Lee, Chanwoo Yang, Yujin Baek, and Jaegul Choo. Exploring in-context example generation for machine translation, arXiv preprint arXiv:2506.00507, 2025. URL https: //arxiv.org/abs/2506.00507v1. [577] Dongyub Lee, Eunhwan Park, Hodong Lee, and Heuiseok Lim. Ask, assess, and refine: Rectifying factual consistency and hallucination in llms with metric-guided feedback learning. Conference of the European Chapter of the Association for Computational Linguistics, 2024. [578] Eunhae Lee. Towards ethical personal ai applications: Practical considerations for ai assistants with long-term memory, arXiv preprint arXiv:2409.11192, 2024. URL https://arxiv.org/abs/ 2409.11192v1. [579] Gibbeum Lee, Volker Hartmann, Jongho Park, Dimitris Papailiopoulos, and Kangwook Lee. Prompted llms as chatbot modules for long open-domain conversation. In Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics, 2023. doi: 10.18653/ v1/2023.findings-acl.277. URL http://dx.doi.org/10.18653/v1/2023.findings-acl. 277. [580] Heejun Lee, Geon Park, Youngwan Lee, Jina Kim, Wonyoung Jeong, Myeongjae Jeon, and Sung Ju Hwang. training-free sub-quadratic cost transformer model serving framework with hierarchically pruned attention, arXiv preprint arXiv:2406.09827, 2024. URL https://arxiv.org/abs/2406. 09827v3. [581] Ho-Jun Lee, Junho Kim, Hyunjun Kim, and Yonghyun Ro. Refocus: Reinforcement-guided frame optimization for contextual understanding, arXiv preprint arXiv:2506.01274v1, 2025. URL https: //arxiv.org/abs/2506.01274v1. [582] Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, and E. Brunskill. Supervised pretraining can learn in-context reinforcement learning. Neural Information Processing Systems, 2023. [583] Namkyeong Lee, E. Brouwer, Ehsan Hajiramezanali, Chanyoung Park, and Gabriele Scalia. Ragenhanced collaborative llm agents for drug discovery, arXiv preprint arXiv:2502.17506, 2025. URL https://arxiv.org/abs/2502.17506v2. [584] Shinbok Lee, Gaeun Seo, Daniel Lee, Byeongil Ko, Sunghee Jung, and M. Shin. Functionchat-bench: Comprehensive evaluation of language models generative capabilities in korean tool-use dialogs. arXiv preprint, 2024. [585] Younghun Lee, Sungchul Kim, Ryan A. Rossi, Tong Yu, and Xiang Chen. Learning to reduce: Towards improving performance of large language models on structured data, arXiv preprint arXiv:2407.02750, 2024. URL https://arxiv.org/abs/2407.02750v1. [586] Younghun Lee, Sungchul Kim, Tong Yu, Ryan A. Rossi, and Xiang Chen. Learning to reduce: Optimal representations of structured data in prompting large language models, arXiv preprint arXiv:2402.14195, 2024. URL https://arxiv.org/abs/2402.14195v1. [587] Yu-Ting Lee, Hui-Ying Shih, Fu-Chieh Chang, and Pei-Yuan Wu. An explanation of intrinsic selfcorrection via linear representations and latent concepts, arXiv preprint arXiv:2505.11924, 2025. URL https://arxiv.org/abs/2505.11924v1. 102 [588] Melissa Lehman and Kenneth J. Malmberg. buffer model of memory encoding and temporal correlations in retrieval. Psychology Review, 2013. [589] Yiming Lei, Zhizheng Yang, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, and Yunhong Wang. Contextqformer: new context modeling method for multi-turn multi-modal conversations, arXiv preprint arXiv:2505.23121v1, 2025. URL https://arxiv.org/abs/2505. 23121v1. [590] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. Conference on Empirical Methods in Natural Language Processing, 2021. [591] Patrick Lewis, Ethan Perez, Aleksandara Piktus, F. Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, M. Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. Neural Information Processing Systems, 2020. [592] Bohan Li, Yutai Hou, and Wanxiang Che. Data augmentation approaches in natural language processing: survey. AI Open, 2021. [593] Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie, Tianqi Yang, Yanling Cui, Liangjie Zhang, and Qi Zhang. Adsgnn: Behavior-graph augmented relevance modeling in sponsored search. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021. [594] Chengpeng Li, Zhengyang Tang, Ziniu Li, Mingfeng Xue, Keqin Bao, Tian Ding, Ruoyu Sun, Benyou Wang, Xiang Wang, Junyang Lin, and Dayiheng Liu. Cort: Code-integrated reasoning within thinking, arXiv preprint arXiv:2506.09820, 2025. URL https://arxiv.org/abs/2506.09820v2. [595] Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey Levine, Fei-Fei Li, Fei Xia, and Brian Ichter. Chain of code: Reasoning with language model-augmented code emulator. International Conference on Machine Learning, 2023. [596] Chuanhao Li, Runhan Yang, Tiankai Li, Milad Bafarassat, Kourosh Sharifi, Dirk Bergemann, and Zhuoran Yang. Stride: tool-assisted llm agent framework for strategic and interactive decision-making, arXiv preprint arXiv:2405.16376, 2024. URL https://arxiv.org/abs/2405.16376v2. [597] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. Large language models with controllable working memory, arXiv preprint arXiv:2211.05110, 2022. URL https://arxiv.org/abs/2211.05110. [598] Daniel Li and Lincoln Murr. Humaneval on latest gpt models - 2024. arXiv preprint, 2024. [599] Fu Li, Xueying Wang, Bin Li, Yunlong Wu, Yanzhen Wang, and Xiaodong Yi. study on training and developing large language models for behavior tree generation, arXiv preprint arXiv:2401.08089, 2024. URL https://arxiv.org/abs/2401.08089v1. [600] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for \"mind\" exploration of large language model society. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. 103 [601] Guozheng Li, Peng Wang, Jiajun Liu, Yikai Guo, Ke Ji, Ziyu Shang, and Zijie Xu. Meta in-context learning makes large language models better zero and few-shot relation extractors. International Joint Conference on Artificial Intelligence, 2024. [602] Haoyang Li, Jing Zhang, Cuiping Li, and Hong Chen. Resdsql: Decoupling schema linking and skeleton parsing for text-to-sql. AAAI Conference on Artificial Intelligence, 2023. [603] Jia Li, Ge Li, Yongming Li, and Zhi Jin. Structured chain-of-thought prompting for code generation. ACM Transactions on Software Engineering and Methodology, 2023. [604] Jia Li, Xiangguo Sun, Yuhan Li, Zhixun Li, Hong Cheng, and Jeffrey Xu Yu. Graph intelligence with large language models and prompt learning. Knowledge Discovery and Data Mining, 2024. [605] Jiahao Nick Li, Yan Xu, Tovi Grossman, Stephanie Santosa, and Michelle Li. Omniactions: Predicting digital actions in response to real-world multimodal sensory inputs with llms. International Conference on Human Factors in Computing Systems, 2024. [606] Jiarui Li, Ye Yuan, and Zehua Zhang. Enhancing llm factual accuracy with rag to counter hallucinations: case study on domain-specific queries in private knowledge-bases, arXiv preprint arXiv:2403.10446, 2024. URL https://arxiv.org/abs/2403.10446v1. [607] Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Hanwang Zhang, Wei Ji, Wenqiao Zhang, Tat seng Chua, Siliang Tang, and Yueting Zhuang. Fine-tuning multimodal llms to follow zero-shot demonstrative instructions. International Conference on Learning Representations, 2023. [608] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq R. Joty, Caiming Xiong, and S. Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Neural Information Processing Systems, 2021. [609] Junnan Li, Dongxu Li, S. Savarese, and Steven C. H. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. International Conference on Machine Learning, 2023. [610] Kun Li, Tianhua Zhang, Yunxiang Li, Hongyin Luo, Abdalla Moustafa, Xixin Wu, James Glass, and Helen M. Meng. Generate, discriminate, evolve: Enhancing context faithfulness via fine-grained sentence-level self-evolution, arXiv preprint arXiv:2503.01695, 2025. URL https://arxiv.org/ abs/2503.01695v1. [611] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding, arXiv preprint arXiv:2305.06355v2, 2023. URL https://arxiv.org/abs/2305.06355v2. [612] Li, Zhao, Yu, Song, Li, Yu, and Li. . . . Api-bank: comprehensive benchmark for tool-augmented llms. 2023. URL https://arxiv.org/abs/2304.08244. [613] Michelle M. Li, Ben Y. Reis, Adam Rodman, Tianxi Cai, Noa Dagan, Ran D. Balicer, Joseph Loscalzo, Isaac S. Kohane, and M. Zitnik. One patient, many contexts: Scaling medical ai through contextual intelligence, arXiv preprint arXiv:2506.10157, 2025. URL https://arxiv.org/abs/2506. 10157v1. 104 [614] Ming Li, Keyu Chen, Ziqian Bi, Ming Liu, Benji Peng, Qian Niu, Junyu Liu, Jinlang Wang, Sen Zhang, Xuanhe Pan, Jiawei Xu, and Pohsun Feng. Surveying the mllm landscape: meta-review of current surveys, arXiv preprint arXiv:2409.18991, 2024. URL https://arxiv.org/abs/2409. 18991v1. [615] Minghao Li, Feifan Song, Yu Bowen, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: comprehensive benchmark for tool-augmented llms. Conference on Empirical Methods in Natural Language Processing, 2023. [616] Qiaomu Li and Ying Xie. From glue-code to protocols: critical analysis of a2a and mcp integration for scalable agent systems, arXiv preprint arXiv:2505.03864, 2025. URL https://arxiv.org/ abs/2505.03864v1. [617] Rongsheng Li, Jin Xu, Zhixiong Cao, Hai-Tao Zheng, and Hong-Gee Kim. Extending context window in large language models with segmented base adjustment for rotary position embeddings. Applied Sciences, 2024. [618] Shuaike Li, Kai Zhang, Qi Liu, and Enhong Chen. Mindbridge: Scalable and cross-model knowledge editing via memory-augmented modality, arXiv preprint arXiv:2503.02701v1, 2025. URL https: //arxiv.org/abs/2503.02701v1. [619] Shuaiyi Li, Zhisong Zhang, Yang Deng, Chenlong Deng, Tianqing Fang, Hongming Zhang, Haitao Mi, Dong Yu, and Wai Lam. Incomes: Integrating compression and selection mechanisms into llms for efficient model editing, arXiv preprint arXiv:2505.22156, 2025. URL https://arxiv.org/ abs/2505.22156v1. [620] Siheng Li, Cheng Yang, Zesen Cheng, Lemao Liu, Mo Yu, Yujiu Yang, and Wai Lam. Large language models can self-improve in long-context reasoning, arXiv preprint arXiv:2411.08147, 2024. URL https://arxiv.org/abs/2411.08147v1. [621] Li, Zou, and Liu. Torl: Scaling tool-integrated rl. 2025. URL https://arxiv.org/abs/ 2503.23383. [622] Xiaopeng Li, Pengyue Jia, Derong Xu, Yi Wen, Yingyi Zhang, Wenlin Zhang, Wanyu Wang, Yichao Wang, Zhaochen Du, Xiangyang Li, Yong Liu, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao. survey of personalization: From rag to agent, arXiv preprint arXiv:2504.10147, 2025. URL https://arxiv.org/abs/2504.10147v1. [623] Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability, arXiv preprint arXiv:2504.21776, 2025. URL https://arxiv.org/abs/2504.21776v1. [624] Xin Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Cheng Qian, Chuan Shi, and Cheng Yang. Graphteam: Facilitating large language model-based graph analysis via multi-agent collaboration, arXiv preprint arXiv:2410.18032v4, 2024. URL https://arxiv.org/abs/2410. 18032v4. [625] Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth, 2024. 105 [626] Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, and Ge Yu. Structure-aware language model pretraining improves dense retrieval on structured data. Annual Meeting of the Association for Computational Linguistics, 2023. [627] Yang Li, Jiacong He, Xiaoxia Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile ui action sequences. Annual Meeting of the Association for Computational Linguistics, 2020. [628] Yinghao Li, R. Ramprasad, and Chao Zhang. simple but effective approach to improve structured language model output for information extraction. Conference on Empirical Methods in Natural Language Processing, 2024. [629] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents, arXiv preprint arXiv:2310.06500, 2023. URL https://arxiv.org/abs/2310.06500. [630] Yucheng Li. Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering, arXiv preprint arXiv:2304.12102, 2023. URL https: //arxiv.org/abs/2304.12102v1. [631] Yucheng Li, Bo Dong, Chenghua Lin, and Frank Guerin. Compressing context to enhance inference efficiency of large language models. Conference on Empirical Methods in Natural Language Processing, 2023. [632] Zhaoxin Li, Xiaoming Zhang, Haifeng Zhang, and Chengxiang Liu. Refining interactions: Enhancing anisotropy in graph neural networks with language semantics, arXiv preprint arXiv:2504.01429, 2025. URL https://arxiv.org/abs/2504.01429v1. [633] Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Naifan Cheung, Nanyun Peng, and Kai-Wei Chang. Think carefully and check again! meta-generation unlocking llms for low-resource cross-lingual summarization. 2024. [634] Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Nanyun Peng, and Kai-Wei Chang. Drs: Deep question reformulation with structured output. In Association for Computational Linguistics ACL, 2025., 2024. [635] Zhecheng Li, Yiwei Wang, Bryan Hooi, Yujun Cai, Zhen Xiong, Nanyun Peng, and Kai-Wei Chang. In Association for Computational Vulnerability of llms to vertically aligned text manipulations. Linguistics ACL, 2025., 2024. [636] Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, and Yiwei Wang. Texture or semantics? vision-language models get lost in font recognition. In Conference on Language Modeling COLM, 2025., 2025. [637] Zhiyu Li, Shichao Song, Hanyu Wang, Simin Niu, Ding Chen, Jiawei Yang, Chenyang Xi, Huayi Lai, Jihao Zhao, Yezhaohui Wang, Junpeng Ren, Zehao Lin, Jiahao Huo, Tianyi Chen, Kai Chen, Ke-Rong Li, Zhiqiang Yin, Qingchen Yu, Bo Tang, Hongkang Yang, Zhiyang Xu, and Feiyu Xiong. Memos: An operating system for memory-augmented generation (mag) in large language models, arXiv preprint arXiv:2505.22101, 2025. URL https://arxiv.org/abs/2505.22101v1. [638] Zhong-Zhi Li, Ming-Liang Zhang, Fei Yin, and Cheng-Lin Liu. Lans: layout-aware neural solver for plane geometry problem. 2023. 106 [639] Zhong-Zhi Li, Ming-Liang Zhang, Fei Yin, Zhi-Long Ji, Jin-Feng Bai, Zhen-Ru Pan, Fan-Hu Zeng, Jian Xu, Jia-Xin Zhang, and Cheng-Lin Liu. Cmmath: chinese multi-modal math skill evaluation benchmark for foundation models. 2024. [640] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. 2025. [641] Zhuoqun Li, Xuanang Chen, Haiyang Yu, Hongyu Lin, Yaojie Lu, Qiaoyu Tang, Fei Huang, Xianpei Han, Le Sun, and Yongbin Li. Structrag: Boosting knowledge intensive reasoning of llms via inferencetime hybrid information structurization. International Conference on Learning Representations, 2024. [642] Zinuo Li, Xian Zhang, Yongxin Guo, Mohammed Bennamoun, F. Boussaid, Girish Dwivedi, Luqi Gong, and Qiuhong Ke. Watch and listen: Understanding audio-visual-speech moments with multimodal llm, arXiv preprint arXiv:2505.18110v2, 2025. URL https://arxiv.org/abs/2505.18110v2. [643] Zixuan Li, Jing Xiong, Fanghua Ye, Chuanyang Zheng, Xun Wu, Jianqiao Lu, Zhongwei Wan, Xiaodan Liang, Chengming Li, Zhenan Sun, Lingpeng Kong, and Ngai Wong. Uncertaintyrag: Span-level uncertainty enhanced long-context modeling for retrieval-augmented generation, arXiv preprint arXiv:2410.02719, 2024. URL https://arxiv.org/abs/2410.02719v1. [644] Zonglin Li, Ruiqi Guo, and Surinder Kumar. Decoupled context processing for context augmented language modeling. Neural Information Processing Systems, 2022. [645] Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier. Prompt compression for large language models: survey. North American Chapter of the Association for Computational Linguistics, 2024. [646] Wen li Yu and Junfeng Zhao. Quantum multi-agent reinforcement learning as an emerging ai technology: survey and future directions. International Conferences on Computing Advancements, 2023. [647] Guannan Liang and Qianqian Tong. Llm-powered ai agent systems and their applications in industry, arXiv preprint arXiv:2505.16120, 2025. URL https://arxiv.org/abs/2505.16120v1. [648] Jintao Liang, Gang Su, Huifeng Lin, You Wu, Rui Zhao, and Ziyue Li. Reasoning rag via system 1 or system 2: survey on reasoning agentic retrieval-augmented generation for industry challenges, arXiv preprint arXiv:2506.10408, 2025. URL https://arxiv.org/abs/2506.10408v1. [649] Xinnian Liang, Bing Wang, Huijia Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Scm: Enhancing large language model with self-controlled memory framework, arXiv preprint arXiv:2304.13343, 2023. URL https://arxiv.org/abs/2304.13343v4. [650] Xuechen Liang, Meiling Tao, Yinghui Xia, Tianyu Shi, Jun Wang, and Jingsong Yang. Self-evolving agents with reflective and memory-augmented abilities, arXiv preprint arXiv:2409.00872, 2024. URL https://arxiv.org/abs/2409.00872v2. [651] Xuechen Liang, Meiling Tao, Yinghui Xia, Jianhui Wang, Kun Li, Yijin Wang, Jingsong Yang, Tianyu Shi, Yuantao Wang, Miao Zhang, and Xueqian Wang. Mars: Memory-enhanced agents with reflective self-improvement, arXiv preprint arXiv:2503.19271, 2025. URL https://arxiv.org/abs/2503. 19271v2. [652] Yanbiao Liang, Huihong Shi, Haikuo Shao, and Zhongfeng Wang. Accllm: Accelerating long-context llm inference via algorithm-hardware co-design, arXiv preprint arXiv:2505.03745, 2025. URL https://arxiv.org/abs/2505.03745v1. [653] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yangyiwen Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan. Taskmatrix.ai: Completing tasks by connecting foundation models with millions of apis. Intelligent Computing, 2023. [654] Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun Du, Zhenzhu Yang, Jiaheng Liu, Chenghua Lin, Lei Ma, Wenhao Huang, and Jiajun Zhang. I-sheep: Self-alignment of llm from scratch through an iterative self-enhancement paradigm. arXiv preprint, 2024. [655] Bingli Liao and Danilo Vasconcellos Vargas. Beyond kv caching: Shared attention for efficient llms. Neurocomputing, 2024. [656] Xiaoxuan Liao, Binrong Zhu, Jacky He, Guiran Liu, Hongye Zheng, and Jia Gao. fine-tuning approach for t5 using knowledge graphs to address complex tasks, arXiv preprint arXiv:2502.16484, 2025. URL https://arxiv.org/abs/2502.16484v1. [657] David Lillis. Internalising interaction protocols as first-class programming elements in multi agent systems, arXiv preprint arXiv:1711.02634, 2017. URL https://arxiv.org/abs/1711.02634v1. [658] Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. Kagnet: Knowledge-aware graph networks for commonsense reasoning. Conference on Empirical Methods in Natural Language Processing, 2019. [659] Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xiang Ren. Common sense beyond english: Evaluating and improving multilingual language models for commonsense reasoning. Annual Meeting of the Association for Computational Linguistics, 2021. [660] Bin Lin, Chen Zhang, Tao Peng, Hanyu Zhao, Wencong Xiao, Minmin Sun, Anmin Liu, Zhipeng Zhang, Lanbo Li, Xiafei Qiu, Shen Li, Zhigang Ji, Tao Xie, Yong Li, and Wei Lin. Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache, arXiv preprint arXiv:2401.02669, 2024. URL https://arxiv.org/abs/2401.02669. [661] Jianhao Lin, Lexuan Sun, and Yixin Yan. Simulating macroeconomic expectations using llm agents, arXiv preprint arXiv:2505.17648, 2025. URL https://arxiv.org/abs/2505.17648v2. [662] Lei Lin, Jiayi Fu, Pengli Liu, Qingyang Li, Yan Gong, Junchen Wan, Fuzheng Zhang, Zhongyuan Wang, Di Zhang, and Kun Gai. Just ask one more time! self-agreement improves reasoning of language models in (almost) all scenarios. Annual Meeting of the Association for Computational Linguistics, 2023. [663] Matthieu Lin, Jenny Sheng, Andrew Zhao, Shenzhi Wang, Yang Yue, Yiran Wu, Huan Liu, Jun Liu, Gao Huang, and Yong-Jin Liu. Training of scaffolded language models with language supervision: survey, arXiv preprint arXiv:2410.16392, 2024. URL https://arxiv.org/abs/2410.16392v2. [664] Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Jun Wang, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, and Weinan Zhang. Hammer: Robust function-calling for ondevice language models via function masking, arXiv preprint arXiv:2410.04587, 2024. URL https: //arxiv.org/abs/2410.04587v2. 108 [665] Yu-Chen Lin, Akhilesh Kumar, Norman Chang, Wen-Liang Zhang, Muhammad Zakir, Rucha Apte, Haiyang He, Chao Wang, and Jyh-Shing Roger Jang. Novel preprocessing technique for data embedding in engineering code generation using large language model. 2024 IEEE LLM Aided Design Workshop (LAD), 2023. [666] Yu-Hsuan Lin, Qian-Hui Chen, Yi-Jie Cheng, Jia-Ren Zhang, Yi-Hung Liu, Liang-Yu Hsia, and Llm inference enhanced by external knowledge: survey, arXiv preprint Yun-Nung Chen. arXiv:2505.24377, 2025. URL https://arxiv.org/abs/2505.24377v1. [667] Jack Lindsey and Ashok Litwin-Kumar. Selective consolidation of learning and memory via recall-gated plasticity. bioRxiv, 2024. [668] Tal Linzen, Emmanuel Dupoux, and Yoav Goldberg. Assessing the ability of lstms to learn syntaxsensitive dependencies. Transactions of the Association for Computational Linguistics, 2016. [669] Gili Lior, Yuval Shalev, Gabriel Stanovsky, and Ariel Goldstein. Computation or weight adaptation? rethinking the role of plasticity in learning. bioRxiv, 2024. [670] Bingyang Liu, Haoyi Zhang, Xiaohan Gao, Zichen Kong, Xiyuan Tang, Yibo Lin, Runsheng Wang, and Ru Huang. Layoutcopilot: An llm-powered multi-agent collaborative framework for interactive analog layout design. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [671] E. Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. International Conference on Learning Representations, 2018. [672] Guang-Da Liu, Haitao Mao, Bochuan Cao, Zhiyu Xue, K. Johnson, Jiliang Tang, and Rongrong Wang. On the intrinsic self-correction capability of llms: Uncertainty and latent concept, arXiv preprint arXiv:2406.02378, 2024. URL https://arxiv.org/abs/2406.02378v2. [673] Guangyi Liu, Yongqi Zhang, Yong Li, and Quanming Yao. Dual reasoning: gnn-llm collaborative framework for knowledge graph question answering, arXiv preprint arXiv:2406.01145, 2024. URL https://arxiv.org/abs/2406.01145v2. [674] Guangyi Liu, Pengxiang Zhao, Liang Liu, Yaxuan Guo, Han Xiao, Weifeng Lin, Yuxiang Chai, Yue Han, Shuai Ren, Hao Wang, Xiaoyu Liang, Wenhao Wang, Tianze Wu, Linghao Li, Guanjing Xiong, Yong Liu, and Hongsheng Li. Llm-powered gui agents in phone automation: Surveying progress and prospects, arXiv preprint arXiv:2504.19838, 2025. URL https://arxiv.org/abs/2504.19838v2. [675] Hanchao Liu, Rong-Zhi Li, Weimin Xiong, Ziyu Zhou, and Wei Peng. Workteam: Constructing workflows from natural language with multi-agents. North American Chapter of the Association for Computational Linguistics, 2025. [676] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context. International Conference on Learning Representations, 2023. [677] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Neural Information Processing Systems, 2023. 109 [678] Jie Liu, Pan Zhou, Yingjun Du, Ah-Hwee Tan, Cees G. M. Snoek, J. Sonke, and E. Gavves. Capo: Cooperative plan optimization for efficient embodied multi-agent cooperation. International Conference on Learning Representations, 2024. [679] Jun Liu, Ke Yu, Keliang Chen, Ke Li, Yuxinyue Qian, Xiaolian Guo, Haozhe Song, and Yinming Li. Acps: Agent collaboration protocols for the internet of agents, arXiv preprint arXiv:2505.13523, 2025. URL https://arxiv.org/abs/2505.13523v1. [680] Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, and Yiling Lou. Large language model-based agents for software engineering: survey, arXiv preprint arXiv:2409.02977, 2024. URL https://arxiv.org/abs/2409.02977v1. [681] Kai Liu, Zhihang Fu, Chao Chen, Wei Zhang, Rongxin Jiang, Fan Zhou, Yao-Shen Chen, Yue Wu, and Jieping Ye. Enhancing llms cognition via structurization. Neural Information Processing Systems, 2024. [682] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. Think-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint, 2023. [683] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. Think-in-memory: Recalling and post-thinking enable llms with long-term memory, arXiv preprint arXiv:2311.08719, 2023. URL https://arxiv.org/abs/2311.08719. [684] Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, and Ming Cui. From llm to conversational agent: memory enhanced architecture with fine-tuning of large language models, arXiv preprint arXiv:2401.02777, 2024. URL https://arxiv.org/abs/2401.02777v2. [685] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, F. Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 2023. [686] Pei Liu, Xin Liu, Ruoyu Yao, Junming Liu, Siyuan Meng, Ding Wang, and Jun Ma. Hm-rag: Hierarchical multi-agent multimodal retrieval augmented generation. arXiv preprint, 2025. [687] Shicheng Liu, Jialiang Xu, Wesley Tjangnaka, Sina J. Semnani, Chen Jie Yu, Gui David, and Monica S. Lam. Suql: Conversational search over structured and unstructured data with large language models. NAACL-HLT, 2023. [688] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. 2025. [689] Liu, Huang, Zeng, Hao, Yu, and Li. . . . Toolace: Winning the points of llm function calling. 2024. URL https://arxiv.org/abs/2409.00920. [690] Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. K-bert: Enabling language representation with knowledge graph. AAAI Conference on Artificial Intelligence, 2019. 110 [691] Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, Zezhong Wang, Yuxian Wang, Wu Ning, Yutai Hou, Bin Wang, Chuhan Wu, Xinzhi Wang, Yong Liu, Yasheng Wang, Duyu Tang, Dandan Tu, Lifeng Shang, Xin Jiang, Ruiming Tang, Defu Lian, Qun Liu, and Enhong Chen. Toolace: Winning the points of llm function calling, arXiv preprint arXiv:2409.00920, 2024. URL https://arxiv.org/abs/2409.00920v1. [692] Weiwen Liu, Jiarui Qin, Xu Huang, Xingshan Zeng, Yunjia Xi, Jianghao Lin, Chuhan Wu, Yasheng Wang, Lifeng Shang, Ruiming Tang, Defu Lian, Yong Yu, and Weinan Zhang. The real barrier to llm agent usability is agentic roi, arXiv preprint arXiv:2505.17767, 2025. URL https://arxiv.org/ abs/2505.17767v1. [693] Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen, Bo Jiang, Aimin Zhou, and Liang He. Mathematical language models: survey, arXiv preprint arXiv:2312.07622, 2023. URL https://arxiv.org/abs/2312.07622v4. [694] Wentao Liu, Ruohua Zhang, Aimin Zhou, Feng Gao, and JiaLi Liu. Echo: large language model with temporal episodic memory, arXiv preprint arXiv:2502.16090, 2025. URL https://arxiv. org/abs/2502.16090v1. [695] Xu Liu, S. Ramirez, Petti T. Pang, C. Puryear, A. Govindarajan, K. Deisseroth, and S. Tonegawa. Optogenetic stimulation of hippocampal engram activates fear memory recall. Nature, 2012. [696] Yang Liu, Xiaobin Tian, Zequn Sun, and Wei Hu. Finetuning generative large language models with discrimination instructions for knowledge graph completion. In International Semantic Web Conference, 2024. [697] Yue Liu, Jiaying Wu, Yufei He, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, Zhiqi Huang, and Bryan Hooi. Efficient inference for large reasoning models: survey, arXiv preprint arXiv:2503.23077, 2025. URL https://arxiv.org/abs/2503.23077v2. [698] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. dynamic llm-powered agent network for task-oriented agent collaboration, arXiv preprint arXiv:2310.02170, 2023. URL https://arxiv. org/abs/2310.02170v2. [699] Zijun Liu, Zhennan Wan, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. Scaling external knowledge input beyond context windows of llms via multi-agent collaboration, arXiv preprint arXiv:2505.21471, 2025. URL https://arxiv.org/abs/2505.21471v1. [700] Zinan Liu, Haoran Li, Jingyi Lu, Gaoyuan Ma, Xu Hong, Giovanni Iacca, Arvind Kumar, Shaojun Tang, and Lin Wang. Natures insight: novel framework and comprehensive analysis of agentic reasoning through the lens of neuroscience. arXiv preprint, 2025. [701] Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh Murthy, Liangwei Yang, Silvio Savarese, Juan Carlos Niebles, Huan Wang, Shelby Heinecke, and Caiming Xiong. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Neural Information Processing Systems, 2024. [702] Leo S. Lo. The art and science of prompt engineering: new literacy in the information age. Internet Reference Services Quarterly, 2023. 111 [703] Joseph R. Loffredo and Suyeol Yun. Agent-enhanced large language models for researching political institutions, arXiv preprint arXiv:2503.13524, 2025. URL https://arxiv.org/abs/2503. 13524v1. [704] Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Fei Mi, Baojun Wang, Weichao Wang, Lifeng Shang, and Qun Liu. Self: Self-evolution with language feedback, arXiv preprint arXiv:2310.00533, 2023. URL https://arxiv.org/abs/2310.00533v4. [705] Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu. Memochat: Tuning llms to use memos for consistent long-range open-domain conversation, arXiv preprint arXiv:2308.08239, 2023. URL https://arxiv.org/abs/2308.08239. [706] Junting Lu, Zhiyang Zhang, Fangkai Yang, Jue Zhang, Lu Wang, Chao Du, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. Axis: Efficient human-agent-computer interaction with api-first llm-based agents, arXiv preprint arXiv:2409.17140, 2025. URL https://arxiv.org/ abs/2409.17140. [707] Keer Lu, Xiaonan Nie, Zheng Liang, Da Pan, Shusen Zhang, Keshi Zhao, Weipeng Chen, Zenan Zhou, Guosheng Dong, Bin Cui, and Wentao Zhang. Datasculpt: Crafting data landscapes for long-context llms through multi-objective partitioning, arXiv preprint arXiv:2409.00997, 2024. URL https://arxiv.org/abs/2409.00997v2. [708] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: co-design framework for enabling sparse attention using reconfigurable architecture. Micro, 2021. [709] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Y. Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug-and-play compositional reasoning with large language models. Neural Information Processing Systems, 2023. [710] Lu, Yu, and Khashabi. Gear: Augmenting language models with generalizable and efficient tool resolution. 2023. URL https://arxiv.org/abs/2307.08775. [711] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Annual Meeting of the Association for Computational Linguistics, 2021. [712] Yinquan Lu, H. Lu, Guirong Fu, and Qun Liu. Kelm: Knowledge enhanced pre-trained language representations with message passing on hierarchical relational graphs, arXiv preprint arXiv:2109.04223, 2021. URL https://arxiv.org/abs/2109.04223v2. [713] Elias Lumer, Anmol Gulati, V. K. Subbiah, Pradeep Honaganahalli Basavaraju, and James A. Burke. Scalemcp: Dynamic and auto-synchronizing model context protocol tools for llm agents, arXiv preprint arXiv:2505.06416, 2025. URL https://arxiv.org/abs/2505.06416v1. [714] Cheng Luo, Jiawei Zhao, Zhuoming Chen, Beidi Chen, and Anima Anandkumar. Mini-sequence transformer: Optimizing intermediate memory for long sequences training, arXiv preprint arXiv:2407.15892, 2024. URL https://arxiv.org/abs/2407.15892v4. [715] Feng Luo, Yu-Neng Chuang, Guanchu Wang, Hoang Anh Duy Le, Shaochen Zhong, Hongyi Liu, Jiayi Yuan, Yang Sui, Vladimir Braverman, Vipin Chaudhary, and Xia Hu. Autol2s: Auto longshort reasoning for efficient large language models, arXiv preprint arXiv:2505.22662, 2025. URL https://arxiv.org/abs/2505.22662v1. 112 [716] Haitong Luo, Xuying Meng, Suhang Wang, Tianxiang Zhao, Fali Wang, Hanyun Cao, and Yujun Zhang. Enhance graph alignment for large language models, arXiv preprint arXiv:2410.11370v1, 2024. URL https://arxiv.org/abs/2410.11370v1. [717] Haoran Luo, E. Haihong, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, Qika Lin, Yu Feng, Zemin Kuang, Meina Song, Yifan Zhu, and Anh Tuan Luu. Hypergraphrag: Retrieval-augmented generation with hypergraph-structured knowledge representation. arXiv preprint, 2025. [718] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint, 2025. [719] Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiaoming Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Mengxue Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xianhong Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, and Ming Zhang. Large language model agent: survey on methodology, applications and challenges, arXiv preprint arXiv:2503.21460, 2025. URL https://arxiv.org/abs/2503.21460v1. [720] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Reasoning on graphs: Faithful and interpretable large language model reasoning. International Conference on Learning Representations, 2023. [721] Renjie Luo, Jiaxi Li, Chen Huang, and Wei Lu. Through the valley: Path to effective long cot training for small language models, arXiv preprint arXiv:2506.07712, 2025. URL https://arxiv.org/ abs/2506.07712v1. [722] Xindi Luo, Zequn Sun, Jing Zhao, Zhe Zhao, and Wei Hu. Knowla: Enhancing parameter-efficient In Proceedings of the 2024 Conference of the North finetuning with knowledgeable adaptation. American Chapter of the Association for Computational Linguistics, 2024. [723] Yifan Luo, Yiming Tang, Chengfeng Shen, Zhennan Zhou, and Bin Dong. Prompt engineering through the lens of optimal control. Journal of Machine Learning, 2023. [724] Panagiotis Lymperopoulos and Vasanth Sarathy. Tools in the loop: Quantifying uncertainty of llm question answering systems that use tools, arXiv preprint arXiv:2505.16113, 2025. URL https: //arxiv.org/abs/2505.16113v1. [725] Yougang Lyu, Xiaoyu Zhang, Lingyong Yan, M. D. Rijke, Zhaochun Ren, and Xiuying Chen. Deepshop: benchmark for deep research shopping agents, arXiv preprint arXiv:2506.02839, 2025. URL https://arxiv.org/abs/2506.02839v1. [726] Jianxiang Ma. Research on the role of llm in multi-agent systems: survey. Applied and Computational Engineering, 2024. [727] Jie Ma, Zhitao Gao, Qianyi Chai, Wangchun Sun, Pinghui Wang, Hongbin Pei, Jing Tao, Lingyun Song, Jun Liu, Chen Zhang, and Li zhen Cui. Debate on graph: flexible and reliable reasoning framework for large language models, arXiv preprint arXiv:2409.03155, 2024. URL https:// arxiv.org/abs/2409.03155v1. 113 [728] Qun Ma, Xiao Xue, Deyu Zhou, Xiangning Yu, Donghua Liu, Xuwen Zhang, Zihan Zhao, Yifan Shen, Peilin Ji, Juanjuan Li, Gang Wang, and Wanpeng Ma. Computational experiments meet large language model based agents: survey and perspective, arXiv preprint arXiv:2402.00262, 2024. URL https://arxiv.org/abs/2402.00262v1. [729] Xin Ma, Yang Liu, Jingjing Liu, and Xiaoxu Ma. Mesa-extrapolation: weave position encoding method for enhanced extrapolation in llms. Neural Information Processing Systems, 2024. [730] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. Query rewriting for retrievalaugmented large language models, arXiv preprint arXiv:2305.14283, 2023. URL https://arxiv. org/abs/2305.14283v3. [731] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke S. Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length. Neural Information Processing Systems, 2024. [732] Ma, Gou, Hao, Xu, Wang, and Pan. . . . Sciagent: Tool-augmented language models for scientific reasoning. 2024. URL https://arxiv.org/abs/2402.11451. [733] Zhiyuan Ma, Zhenya Huang, Jiayu Liu, Minmao Wang, Hongke Zhao, and Xin Li. Automated creation of reusable and diverse toolsets for enhancing llm reasoning. AAAI Conference on Artificial Intelligence, 2025. [734] Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve gpt-3 after deployment. Conference on Empirical Methods in Natural Language Processing, 2022. [735] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. Neural Information Processing Systems, 2023. [736] Xinji Mai, Haotian Xu, W. Xing, Weinong Wang, Yingying Zhang, and Wenqiang Zhang. Agent rl scaling law: Agent rl with spontaneous code execution for mathematical problem solving, arXiv preprint arXiv:2505.07773, 2025. URL https://arxiv.org/abs/2505.07773v2. [737] Amjad Yousef Majid, Serge Saaybi, Tomas van Rietbergen, Vincent François-Lavet, R. V. Prasad, and Chris Verhoeven. Deep reinforcement learning versus evolution strategies: comparative survey. IEEE Transactions on Neural Networks and Learning Systems, 2021. [738] D. Maldonado, Edison Cruz, Jackeline Abad Torres, P. Cruz, and Silvana del Pilar Gamboa Benitez. Multi-agent systems: survey about its components, framework and workflow. IEEE Access, 2024. [739] Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic multi-robot collaboration with large language models. IEEE International Conference on Robotics and Automation, 2023. [740] Jeremy R. Manning, Sean M. Polyn, G. Baltuch, B. Litt, and M. Kahana. Oscillatory patterns in temporal lobe reveal context reinstatement during memory search. Proceedings of the National Academy of Sciences of the United States of America, 2011. 114 [741] Qiheng Mao, Zemin Liu, Chenghao Liu, Zhuo Li, and Jianling Sun. Advancing graph representation learning with large language models: comprehensive survey of techniques, arXiv preprint arXiv:2402.05952v1, 2024. URL https://arxiv.org/abs/2402.05952v1. [742] Amin Hosseiny Marani, Ulie Schnaithmann, Youngseo Son, Akil Iyer, Manas Paldhe, and Arushi Raghuvanshi. Graph integrated language transformers for next action prediction in complex phone calls. North American Chapter of the Association for Computational Linguistics, 2024. [743] Sophia Maria. Compass-v2 technical report, arXiv preprint arXiv:2504.15527, 2025. URL https: //arxiv.org/abs/2504.15527v1. [744] Viorica Marian and U. Neisser. Language-dependent recall of autobiographical memories. Journal of experimental psychology. General, 2000. [745] S. Mariani and Andrea Omicini. Special issue multi-agent systems: Editorial. Applied Sciences, 2019. [746] Vasilije Markovic, Lazar Obradović, Laszlo Hajdu, and Jovan Pavlović. Optimizing the interface between knowledge graphs and llms for complex reasoning, arXiv preprint arXiv:2505.24478, 2025. URL https://arxiv.org/abs/2505.24478v1. [747] Sami Marreed, Alon Oved, Avi Yaeli, Segev Shlomov, Ido Levy, Offer Akrabi, Aviad Sela, Asaf Adi, and Nir Mashkif. Towards enterprise-ready computer using generalist agent. 2025. [748] Tula Masterman, Sandi Besen, Mason Sawtell, and Alex Chao. The landscape of emerging ai agent architectures for reasoning, planning, and tool calling: survey, arXiv preprint arXiv:2404.11584, 2024. URL https://arxiv.org/abs/2404.11584v1. [749] Nicholas Matsumoto, Jay Moran, Hyunjun Choi, Miguel E. Hernandez, Mythreye Venkatesan, Paul Wang, and Jason H. Moore. Kragen: knowledge graph-enhanced rag framework for biomedical problem solving using large language models. Bioinformatics, 2024. [750] Ryoga Matsuo, Stefan Uhlich, Arun Venkitaraman, Andrea Bonetti, Chia-Yu Hsieh, Ali Momeni, Lukas Mauch, Augusto Capone, Eisaku Ohbuchi, and Lorenzo Servadei. Schemato - an llm for netlist-to-schematic conversion. arXiv preprint, 2024. [751] Costas Mavromatis, V. Ioannidis, Shen Wang, Da Zheng, Soji Adeshina, Jun Ma, Han Zhao, C. Faloutsos, and G. Karypis. Train your own gnn teacher: Graph-aware distillation on textual graphs. ECML/PKDD, 2023. [752] James L. McClelland, B. McNaughton, and R. OReilly. Why there are complementary learning systems in the hippocampus and neocortex: insights from the successes and failures of connectionist models of learning and memory. Psychology Review, 1995. [753] R. Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. Annual Meeting of the Association for Computational Linguistics, 2019. [754] Daniel McDuff, M. Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, Le Hou, Yong Cheng, Yun Liu, S. Mahdavi, Sushant Prakash, Anupam Pathak, Christopher Semturs, Shwetak N. Patel, D. Webster, Ewa Dominowska, 115 Juraj Gottweis, Joelle Barral, Katherine Chou, G. Corrado, Yossi Matias, Jacob Sunshine, A. Karthikesalingam, and Vivek Natarajan. Towards accurate differential diagnosis with large language models. Nature, 2023. [755] AD McNaughton, Ramalaxmi, Kruel, and CR Knutson. . . . Cactus: Chemistry agent connecting tool-usage to science, arxiv, 2024. [756] Sushant Mehta, R. Dandekar, R. Dandekar, and S. Panat. Latent multi-head attention for small language models, arXiv preprint arXiv:2506.09342, 2025. URL https://arxiv.org/abs/2506. 09342v2. [757] Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent operating system, arXiv preprint arXiv:2403.16971, 2024. URL https://arxiv.org/abs/2403. 16971v4. [758] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, and Xueqi Cheng. Slang: New concept comprehension of large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, page 1255812575. Association for Computational Linguistics, 2024. doi: 10.18653/v1/2024.emnlp-main.698. URL http://dx.doi.org/10.18653/v1/ 2024.emnlp-main.698. [759] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Ruibin Yuan, and Xueqi Cheng. Hiddenguard: Fine-grained safe generation with specialized representation router, arXiv preprint arXiv:2410.02684, 2024. URL https://arxiv.org/abs/2410.02684. [760] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Yuyao Ge, Jun Wan, Yurong Wu, and Xueqi Cheng. a1: Steep test-time scaling law via environment augmented generation, arXiv preprint arXiv:2504.14597, 2025. URL https://arxiv.org/abs/2504.14597. [761] Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Jiayi Mao, and Xueqi Cheng. \"not aligned\" is not \"malicious\": Being careful about hallucinations of large language models jailbreak, arXiv preprint arXiv:2406.11668, 2025. URL https://arxiv.org/abs/2406.11668. [762] T. Meiser and A. Bröder. Memory for multidimensional source information. Journal of Experimental Psychology. Learning, Memory and Cognition, 2002. [763] Gábor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language models. International Conference on Learning Representations, 2017. [764] Kevin Meng, David Bau, A. Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Neural Information Processing Systems, 2022. [765] Yuxian Meng, Shi Zong, Xiaoya Li, Xiaofei Sun, Tianwei Zhang, Fei Wu, and Jiwei Li. Gnn-lm: Language modeling based on global contexts via gnn. International Conference on Learning Representations, 2021. [766] Agnieszka Mensfelt, Kostas Stathis, and Vince Trencsenyi. Towards logically sound natural language reasoning with logic-enhanced language model agents, arXiv preprint arXiv:2408.16081, 2024. URL https://arxiv.org/abs/2408.16081v2. [767] G. M. Mensink and J. Raaijmakers. model for interference and forgetting. arXiv preprint, 1988. 116 [768] Thomas Merth, Qichen Fu, Mohammad Rastegari, and Mahyar Najibi. Superposition prompting: Improving and accelerating retrieval-augmented generation. International Conference on Machine Learning, 2024. [769] B. Meskó. Prompt engineering as an important emerging skill for medical professionals: Tutorial. Journal of Medical Internet Research, 2023. [770] Yapeng Mi, Zhi Gao, Xiaojian Ma, and Qing Li. Building llm agents by incorporating insights from computer systems, arXiv preprint arXiv:2504.04485, 2025. URL https://arxiv.org/abs/ 2504.04485v1. [771] G. Mialon, Roberto Dessì, M. Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, R. Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. Augmented language models: survey. Trans. Mach. Learn. Res., 2023. [772] G. Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants, arXiv preprint arXiv:2311.12983, 2023. URL https: //arxiv.org/abs/2311.12983v1. [773] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, and Zhihao Jia. Towards efficient generative large language model serving: survey from algorithms to systems, arXiv preprint arXiv:2312.15234, 2023. URL https://arxiv.org/abs/2312.15234v1. [774] Jacob Miller, Guillaume Rabusseau, and John Terilla. Tensor networks for language modeling. arXiv preprint, 2020. [775] Xing ming Guo, Darioush Keivan, U. Syed, Lianhui Qin, Huan Zhang, G. Dullerud, Peter J. Seiler, and Bin Hu. Controlagent: Automating control system design via novel integration of llm agents and domain expertise, arXiv preprint arXiv:2410.19811, 2024. URL https://arxiv.org/abs/ 2410.19811v1. [776] Soroush Mirjalili, Patrick S. Powell, Jonathan Strunk, Taylor James, and Audrey Duarte. Context memory encoding and retrieval temporal dynamics are modulated by attention across the adult lifespan. eNeuro, 2021. [777] Ishan Misra and L. Maaten. Self-supervised learning of pretext-invariant representations. Computer Vision and Pattern Recognition, 2019. [778] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards general read-write memory for large language models, arXiv preprint arXiv:2305.14322, 2024. URL https: //arxiv.org/abs/2305.14322. [779] Ali Modarressi, Abdullatif Köksal, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schutze. Memllm: Finetuning llms to use an explicit read-write memory. Trans. Mach. Learn. Res., 2024. [780] Behnam Mohammadi. Pel, programming language for orchestrating ai agents, arXiv preprint arXiv:2505.13453, 2025. URL https://arxiv.org/abs/2505.13453v2. [781] Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context length for transformers. Neural Information Processing Systems, 2023. 117 [782] Fedor Moiseev, Zhe Dong, Enrique Alfonseca, and Martin Jaggi. Skill: Structured knowledge infusion for large language models. North American Chapter of the Association for Computational Linguistics, 2022. [783] Dimitri Coelho Mollo and Raphael Milliere. The vector grounding problem, arXiv preprint arXiv:2304.01481, 2023. URL https://arxiv.org/abs/2304.01481v2. [784] Nieves Montes, N. Osman, and C. Sierra. Combining theory of mind and abduction for cooperation under imperfect information. European Workshop on Multi-Agent Systems, 2022. [785] Suhong Moon, Siddharth Jha, Lutfi Eren Erdogan, Sehoon Kim, Woosang Lim, Kurt Keutzer, and A. Gholami. Efficient and scalable estimation of tool representations in vector space, arXiv preprint arXiv:2409.02141, 2024. URL https://arxiv.org/abs/2409.02141v1. [786] Shinsuke Mori. stochastic parser based on an slm with arboreal context trees."
        },
        {
            "title": "International",
            "content": "Conference on Computational Linguistics, 2002. [787] Meredith Ringel Morris. Prompting considered harmful. Communications of the ACM, 2024. [788] Marius Mosbach, Tiago Pimentel, Shauli Ravfogel, D. Klakow, and Yanai Elazar. Few-shot fine-tuning vs. in-context learning: fair comparison and evaluation. Annual Meeting of the Association for Computational Linguistics, 2023. [789] Sajad Mousavi, Ricardo Luna Gutierrez, Desik Rengarajan, Vineet Gundecha, Ashwin Ramesh Babu, Avisek Naug, Antonio Guillen-Perez, and S. Sarkar. N-critics: Self-refinement of large language models with ensemble of critics. arXiv preprint, 2023. [790] Manisha Mukherjee, Sungchul Kim, Xiang Chen, Dan Luo, Tong Yu, and Tung Mai. From documents to dialogue: Building kg-rag enhanced ai assistants, arXiv preprint arXiv:2502.15237, 2025. URL https://arxiv.org/abs/2502.15237v1. [791] Tergel Munkhbat, Namgyu Ho, Seohyun Kim, Yongjin Yang, Yujin Kim, and Se young Yun. Selftraining elicits concise reasoning in large language models, arXiv preprint arXiv:2502.20122, 2025. URL https://arxiv.org/abs/2502.20122v3. [792] Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth Gopal. Leave no context behind: Efficient infinite context transformers with infini-attention, arXiv preprint arXiv:2404.07143, 2024. URL https://arxiv.org/abs/2404.07143v2. [793] Eliya Nachmani, Alon Levkovitch, Julián Salazar, Chulayutsh Asawaroengchai, Soroosh Mariooryad, R. Skerry-Ryan, and Michelle Tadmor Ramanovich. Spoken question answering and speech continuation using spectrogram-powered llm. International Conference on Learning Representations, 2023. [794] L. Nadel, Jessica D. Payne, and W. J. Jacobs. The relationship between episodic memory and context: clues from memory errors made while under stress. Physiological Research, 2002. [795] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human feedback, arXiv preprint arXiv:2112.09332, 2022. URL https://arxiv.org/abs/2112.09332. [796] Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, and Seung Wook Kim. Emerdiff: Emerging pixel-level semantic knowledge in diffusion models, arXiv preprint arXiv:2401.11739, 2024. URL https://arxiv.org/abs/2401.11739. [797] Sundaraparipurnan Narayanan and Sandeep Vishwakarma. Guard-d-llm: An llm-based risk assessment engine for the downstream uses of llms. arXiv preprint, 2024. [798] Usman Naseem, Surendrabikram Thapa, Qi Zhang, Liang Hu, Anum Masood, and Mehwish Nasim. Reducing knowledge noise for improved semantic analysis in biomedical natural language processing applications. Clinical Natural Language Processing Workshop, 2023. [799] Deepak Nathani, David Wang, Liangming Pan, and W. Wang. Maf: Multi-aspect feedback for improving reasoning in large language models. Conference on Empirical Methods in Natural Language Processing, 2023. [800] Aashutosh Nema, Samaksh Gulati, Evangelos Giakoumakis, and Bipana Thapaliya. Modp: Multi objective directional prompting, arXiv preprint arXiv:2504.18722, 2025. URL https://arxiv. org/abs/2504.18722v1. [801] Christian D. Newman, Anthony Peruma, and Reem S. Alsuhaibani. Modeling the relationship between identifier name and behavior. IEEE International Conference on Software Maintenance and Evolution, 2019. [802] M. Nieznański, Michał Obidziński, Emilia Zyskowska, and Daria Niedziałkowska. Executive resources and item-context binding: Exploring the influence of concurrent inhibition, updating, and shifting tasks on context memory. Advances in Cognitive Psychology, 2015. [803] M. Nieznański, Michał Obidziński, and Daria Ford. Does context recollection depend on the base-rate of contextual features? Cognitive Processing, 2023. [804] C. Nourani and P. Eklund. Concepts ontology algebras and role descriptions. Conference on Computer Science and Information Systems, 2017. [805] Felix Ocker, Daniel Tanneberg, Julian Eggert, and Michael Gienger. Tulip agent - enabling llm-based agents to solve tasks using large tool libraries. arXiv preprint, 2024. [806] Felix Ocker, J. Deigmöller, Pavel Smirnov, and Julian Eggert. grounded memory system for smart personal assistants, arXiv preprint arXiv:2505.06328, 2025. URL https://arxiv.org/abs/ 2505.06328v1. [807] OpenAI. Computer-using agent, 2025. URL https://openai.com/index/ computer-using-agent/. OpenAI Technical Report. [808] OpenAI. Swarm: Educational framework exploring ergonomic, lightweight multi-agent orchestration. https://github.com/openai/swarm, 2025. [Online; accessed 17-July-2025]. [809] Jonas Oppenlaender. Dangermaps: Personalized safety advice for travel in urban environments using retrieval-augmented language model, arXiv preprint arXiv:2503.14103, 2025. URL https: //arxiv.org/abs/2503.14103v3. [810] A. Orhan. Recognition, recall, and retention of few-shot memories in large language models, arXiv preprint arXiv:2303.17557, 2023. URL https://arxiv.org/abs/2303.17557v1. 119 [811] Gustavo Ortiz-Hernández, Alejandro Guerra-Hernández, J. Hübner, and W. A. Luna-Ramírez. Modularization in belief-desire-intention agent programming and artifact-based environments. PeerJ Computer Science, 2022. [812] Wendkûuni C. Ouédraogo, A. Kaboré, Haoye Tian, Yewei Song, Anil Koyuncu, Jacques Klein, David Lo, and Tegawende F. Bissyande. Large-scale, independent and comprehensive study of the power of llms for test case generation. arXiv preprint, 2024. [813] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders, and Joseph Gonzalez. Memgpt: Towards llms as operating systems, arXiv preprint arXiv:2310.08560, 2023. URL https: //arxiv.org/abs/2310.08560v2. [814] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Memgpt: Towards llms as operating systems, arXiv preprint arXiv:2310.08560, 2024. URL https://arxiv.org/abs/2310.08560. [815] Constantin-Valentin Pal, F. Leon, M. Paprzycki, and M. Ganzha. review of platforms for the development of agent systems. Inf., 2020. [816] Qianjun Pan, Wenkai Ji, Yuyang Ding, Junsong Li, Shilian Chen, Junyi Wang, Jie Zhou, Qin Chen, Min Zhang, Yulan Wu, and Liang He. survey of slow thinking-based reasoning llms using reinforced learning and inference-time scaling law, arXiv preprint arXiv:2505.02665, 2025. URL https: //arxiv.org/abs/2505.02665v2. [817] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. Unifying large language models and knowledge graphs: roadmap. IEEE Transactions on Knowledge and Data Engineering, 2023. [818] Xu Pan, Ely Hahami, Zechen Zhang, and H. Sompolinsky. Memorization and knowledge injection in gated llms, arXiv preprint arXiv:2504.21239, 2025. URL https://arxiv.org/abs/2504. 21239v1. [819] Bo Pang, Hanze Dong, Jiacheng Xu, Silvio Savarese, Yingbo Zhou, and Caiming Xiong. Bolt: Bootstrap long chain-of-thought in language models without distillation, arXiv preprint arXiv:2502.03860, 2025. URL https://arxiv.org/abs/2502.03860v1. [820] Jianhui Pang, Fanghua Ye, Derek F. Wong, and Longyue Wang. Anchor-based large language models. Annual Meeting of the Association for Computational Linguistics, 2024. [821] Bhargavi Paranjape, Scott M. Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models, arXiv preprint arXiv:2303.09014, 2023. URL https://arxiv.org/abs/2303.09014v1. [822] Parisi, Zhao, and Fiedel. Talm: Tool augmented language models. 2022. URL https: //arxiv.org/abs/2205.12255. [823] Dongju Park and Chang Wook Ahn. Self-supervised contextual data augmentation for natural language processing. Symmetry, 2019. [824] J. Park, Lindsay Popowski, Carrie J. Cai, M. Morris, Percy Liang, and Michael S. Bernstein. Social simulacra: Creating populated prototypes for social computing systems. ACM Symposium on User Interface Software and Technology, 2022. 120 [825] J. Park, Joseph C. OBrien, Carrie J. Cai, M. Morris, Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra of human behavior. ACM Symposium on User Interface Software and Technology, 2023. [826] Soya Park, J. Zamfirescu-Pereira, and Chinmay Kulkarni. Model behavior specification by leveraging llm self-playing and self-improving, arXiv preprint arXiv:2503.03967, 2025. URL https://arxiv. org/abs/2503.03967v1. [827] Rajvardhan Patil and Venkat Gudivada. review of current trends, techniques, and challenges in large language models (llms). Applied Sciences, 2024. [828] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis, arXiv preprint arXiv:2305.15334, 2023. URL https://arxiv.org/ abs/2305.15334. [829] Shishir G. Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and Joseph E. Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. [830] Shuva Paul, Farhad Alemi, and Richard Macwan. Llm-assisted proactive threat intelligence for automated reasoning, arXiv preprint arXiv:2504.00428, 2025. URL https://arxiv.org/abs/ 2504.00428v1. [831] Saurav Pawar, S. Tonmoy, S. M. M. Zaman, Vinija Jain, Aman Chadha, and Amitava Das. The what, why, and how of context length extension techniques in large language models - detailed survey. arXiv preprint, 2024. [832] Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang. Graph retrieval-augmented generation: survey. ArXiv, abs/2408.08921, 2024. URL https://api.semanticscholar.org/CorpusID:271903170. [833] Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. International Conference on Learning Representations, 2023. [834] Hao Peng, Tianyu Gao, Xu Han, Yankai Lin, Peng Li, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Learning from context or names? an empirical study on neural relation extraction. Conference on Empirical Methods in Natural Language Processing, 2020. [835] Ji-Lun Peng, Sijia Cheng, Egil Diau, Yung-Yu Shih, Po-Heng Chen, Yen-Ting Lin, and Yun-Nung Chen. survey of useful llm evaluation, arXiv preprint arXiv:2406.00936, 2024. URL https: //arxiv.org/abs/2406.00936v1. [836] Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, and Jonathan J. Halcrow. Let your graph do the talking: Encoding structured data for llms, arXiv preprint arXiv:2402.05862, 2024. URL https://arxiv.org/abs/2402.05862v1. [837] E. Pesce and G. Montana. Improving coordination in small-scale multi-agent deep reinforcement learning through memory-driven communication. Machine-mediated learning, 2019. 121 [838] F. Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models factual predictions. Conference on Automated Knowledge Base Construction, 2020. [839] Yue Pi, Wang Zhang, Yong Zhang, Hairong Huang, Baoquan Rao, Yulong Ding, and Shuanghua Yang. Applications of multi-agent deep reinforcement learning communication in network management: survey, arXiv preprint arXiv:2407.17030, 2024. URL https://arxiv.org/abs/2407. 17030v1. [840] Nancirose Piazza and Vahid Behzadan. theory of mind approach as test-time mitigation against emergent adversarial communication. Adaptive Agents and Multi-Agent Systems, 2023. [841] Mathis Pink, Vy A. Vo, Qinyuan Wu, Jianing Mu, Javier S. Turek, Uri Hasson, Kenneth A. Norman, Sebastian Michelmann, Alexander Huth, and Mariya Toneva. Assessing episodic memory in llms with sequence order recall tasks, arXiv preprint arXiv:2410.08133, 2024. URL https://arxiv. org/abs/2410.08133v1. [842] Fahmida Liza Piya and Rahmatollah Beheshti. Advancing feature extraction in healthcare through the integration of knowledge graphs and large language models. AAAI Conference on Artificial Intelligence, 2025. [843] A. Plaat, M. V. Duijn, N. V. Stein, Mike Preuss, P. V. D. Putten, and K. Batenburg. Agentic large language models, survey, arXiv preprint arXiv:2503.23037, 2025. URL https://arxiv.org/ abs/2503.23037v2. [844] Moritz Plenz and Anette Frank. Graph language models. Annual Meeting of the Association for Computational Linguistics, 2024. [845] Sean M. Polyn, K. Norman, and M. Kahana. context maintenance and retrieval model of organizational processes in free recall. Psychology Review, 2009. [846] Liam Pond and Ichiro Fujinaga. Teaching llms music theory with in-context learning and chainof-thought prompting: Pedagogical strategies for machines. International Conference on Computer Supported Education, 2025. [847] Porcu. The role of memory in llms: Persistent context for smarter conversations. Int. J. Sci. Res. Manag.(IJSRM), 12:16731691, 2024. [848] Ofir Press, Noah A. Smith, and M. Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. International Conference on Learning Representations, 2021. [849] Xavier Puig, K. Ra, Marko Boben, Jiaman Li, Tingwu Wang, S. Fidler, and A. Torralba. Virtualhome: Simulating household activities via programs. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018. [850] Pranav Putta, Edmund Mills, Naman Garg, S. Motwani, Chelsea Finn, Divyansh Garg, and Rafael Rafailov. Agent q: Advanced reasoning and learning for autonomous ai agents, arXiv preprint arXiv:2408.07199, 2024. URL https://arxiv.org/abs/2408.07199v1. [851] S. Qasim, Hassan Mahmood, and F. Shafait. Rethinking table recognition using graph neural networks. IEEE International Conference on Document Analysis and Recognition, 2019. [852] Peng Qi, Haejun Lee, OghenetegiriTGSido, and Christopher D. Manning. Answering open-domain questions of varying reasoning steps from text. Conference on Empirical Methods in Natural Language Processing, 2020. [853] Yong Qi, Gabriel Kyebambo, Siyuan Xie, Wei Shen, Shenghui Wang, Bitao Xie, Bin He, Zhipeng Wang, and Shuo Jiang. Safety control of service robots with llms and embodied knowledge graphs, arXiv preprint arXiv:2405.17846, 2024. URL https://arxiv.org/abs/2405.17846v1. [854] Zehan Qi, Xiao Liu, Iat Long Iong, Hanyu Lai, Xueqiao Sun, Xinyue Yang, Jiadai Sun, Yu Yang, Shuntian Yao, Tianjie Zhang, Wei Xu, Jie Tang, and Yuxiao Dong. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning, arXiv preprint arXiv:2411.02337, 2024. URL https://arxiv.org/abs/2411.02337v3. [855] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative agents for software development, arXiv preprint arXiv:2307.07924, 2024. URL https://arxiv. org/abs/2307.07924. [856] Cheng Qian, Chi Han, Y. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Tool creation for disentangling abstract and concrete reasoning of large language models. Conference on Empirical Methods in Natural Language Processing, 2023. [857] Cheng Qian, Jiahao Li, Yufan Dang, Wei Liu, Yifei Wang, Zihao Xie, Weize Chen, Cheng Yang, Iterative experience refinement of softwareYingli Zhang, Zhiyuan Liu, and Maosong Sun. developing agents, arXiv preprint arXiv:2405.04219, 2024. URL https://arxiv.org/abs/ 2405.04219v1. [858] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tur, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs, arXiv preprint arXiv:2504.13958, 2025. URL https://arxiv.org/abs/2504.13958v1. [859] Hongjin Qian, Zheng Liu, Peitian Zhang, Zhicheng Dou, and Defu Lian. Boosting long-context management via query-guided activation refilling, arXiv preprint arXiv:2412.12486, 2024. URL https://arxiv.org/abs/2412.12486v3. [860] Hongjin Qian, Zheng Liu, Peitian Zhang, Kelong Mao, Defu Lian, Zhicheng Dou, and Tiejun Huang. Memorag: Boosting long context processing with global memory-enhanced retrieval augmentation, arXiv preprint arXiv:2409.05591, 2025. URL https://arxiv.org/abs/2409.05591. [861] Kangan Qian, Sicong Jiang, Yang Zhong, Ziang Luo, Zilin Huang, Tianze Zhu, Kun Jiang, Mengmeng Yang, Zheng Fu, Jinyu Miao, Yining Shi, He Zhe Lim, Li Liu, Tianbao Zhou, Hongyi Wang, Huang Yu, Yifei Hu, Guang Li, Guangyao Chen, Hao Ye, Lijun Sun, and Diange Yang. Agentthink: unified framework for tool-augmented chain-of-thought reasoning in vision-language models for autonomous driving, arXiv preprint arXiv:2505.15298, 2025. URL https://arxiv.org/abs/ 2505.15298v3. [862] Changze Qiao and Mingming Lu. Efficiently enhancing general agents with hierarchical-categorical memory, arXiv preprint arXiv:2505.22006, 2025. URL https://arxiv.org/abs/2505. 22006v1. [863] Qiao, Gui, Lv, Jia, Chen, and Zhang. Making language models better tool learners with execution feedback. 2023. URL https://arxiv.org/abs/2305.13068. [864] Binjie Qin, Haohao Mao, Ruipeng Zhang, Y. Zhu, Song Ding, and Xu Chen. Working memory inspired hierarchical video decomposition with transformative representations, arXiv preprint arXiv:2204.10105, 2022. URL https://arxiv.org/abs/2204.10105v3. [865] Bowen Qin, Binyuan Hui, Lihan Wang, Min Yang, Jinyang Li, Binhua Li, Ruiying Geng, Rongyu Cao, Jian Sun, Luo Si, Fei Huang, and Yongbin Li. survey on text-to-sql parsing: Concepts, methods, and future directions, arXiv preprint arXiv:2208.13629, 2022. URL https://arxiv.org/abs/ 2208.13629v1. [866] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Y. Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shi Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bo Li, Ziwei Tang, Jing Yi, Yu Zhu, Zhenning Dai, Lan Yan, Xin Cong, Ya-Ting Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. ACM Computing Surveys, 2023. [867] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+ real-world apis. International Conference on Learning Representations, 2023. [868] Zhen Qin and Yiran Zhong. Accelerating toeplitz neural network with constant-time inference complexity. Conference on Empirical Methods in Natural Language Processing, 2023. [869] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. International Conference on Learning Representations, 2023. [870] Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, and Yiran Zhong. Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models. arXiv preprint, 2024. [871] Jiahao Qiu, Xinzhe Juan, Yiming Wang, Ling Yang, Xuan Qi, Tongcheng Zhang, Jiacheng Guo, Yifu Lu, Zixin Yao, Hongru Wang, Shilong Liu, Xun Jiang, Liu Leqi, and Mengdi Wang. Agentdistill: Training-free agent distillation with generalizable mcp boxes, arXiv preprint arXiv:2506.14728, 2025. URL https://arxiv.org/abs/2506.14728v1. [872] Jiahao Qiu, Fulian Xiao, Yiming Wang, Yuchen Mao, Yijia Chen, Xinzhe Juan, Siran Wang, Xuan Qi, Tongcheng Zhang, Zixin Yao, Jiacheng Guo, Yifu Lu, Charles Argon, Jundi Cui, Daixin Chen, Junran Zhou, Shuyao Zhou, Zhanpeng Zhou, Ling Yang, Shilong Liu, Hongru Wang, Kaixuan Huang, Xun Jiang, Yuming Cao, Yue Chen, Yunfei Chen, Zhengyi Chen, Ruowei Dai, Mengqiu Deng, Jiye Fu, Yu Gu, Zijie Guan, Zirui Huang, Xiaoyan Ji, Yumeng Jiang, Delong Kong, Haolong Li, Jiaqi Li, Ruipeng Li, Tianze Li, Zhuo-Yang Li, Haixia Lian, Meng Lin, Xudong Liu, Jiayi Lu, Jinghan Lu, Wanyu Luo, Ziyue Luo, Zihao Pu, Zhi Qiao, Rui-Fang Ren, Liang Wan, Ruixiang Wang, Tianhui Wang, Yang Wang, Zeyu Wang, Zihua Wang, Yujia Wu, Zhaoyi Wu, Hao Xin, Weiao Xing, Ruojun Xiong, Weijie Xu, Yao Shu, Xiao Yao, Xiaorui Yang, Yuchen Yang, Nan Yi, Jiadong Yu, Yang Yu, Huiting Zeng, Danni Zhang, Yunjie Zhang, Zhaoyu Zhang, Zhiheng Zhang, Xiaofeng Zheng, Peirong Zhou, Li-Ying Zhong, Xiaoyin Zong, Ying Zhao, Zhen Chen, Lin Ding, Xiaoyu Gao, Bingbing 124 Gong, Yichao Li, Yang Liao, Guang Ma, Tianyuan Ma, Xinrui Sun, Tianyi Wang, Han Xia, Ruobing Xian, Gen Ye, Tengfei Yu, Wentao Zhang, Yuxi Wang, Xi Gao, and Mengdi Wang. On path to multimodal historical reasoning: Histbench and histagent, arXiv preprint arXiv:2505.20246, 2025. URL https://arxiv.org/abs/2505.20246v3. [873] Ruidi Qiu, Grace Li Zhang, Rolf Drechsler, Ulf Schlichtmann, and Bing Li. Autobench: Automatic testbench generation and evaluation using llms for hdl design. Workshop on Machine Learning for CAD, 2024. [874] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Jirong Wen. Tool learning with large language models: survey. Frontiers Comput. Sci., 2024. [875] Xiaoye Qu, Yafu Li, Zhao yu Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, Peng Li, Wei Wei, Jing Shao, Chaochao Lu, Yue Zhang, XianSheng Hua, Bowen Zhou, and Yu Cheng. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond, arXiv preprint arXiv:2503.21614, 2025. URL https://arxiv.org/abs/2503.21614v1. [876] Victor Quintanar-Zilinskas. neuromimetic approach to the serial acquisition, long-term storage, and selective utilization of overlapping memory engrams. bioRxiv, 2019. [877] Stephan Raaijmakers, Roos Bakker, Anita Cremers, Roy de Kleijn, Tom Kouwenhoven, and Tessa Verhoef. Memory-augmented generative adversarial transformers, arXiv preprint arXiv:2402.19218, 2024. URL https://arxiv.org/abs/2402.19218. [878] Ella Rabinovich and Ateret Anaby-Tavor. On the robustness of agentic function calling. Proceedings of the 5th Workshop on Trustworthy NLP (TrustNLP 2025), 2025. [879] Neil C. Rabinowitz, Frank Perbet, H. F. Song, Chiyuan Zhang, S. Eslami, and M. Botvinick. Machine theory of mind. International Conference on Machine Learning, 2018. [880] Zackary Rackauckas. Rag-fusion: new take on retrieval-augmented generation. International Journal on Natural Language Computing, 2024. [881] Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. International Conference on Machine Learning, 2021. [882] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 2019. [883] Mohaimenul Azam Khan Raiaan, Md. Saddam Hossain Mukta, Kaniz Fatema, Nur Mohammad Fahad, S. Sakib, Most. Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami Azam. review on large language models: Architectures, applications, taxonomies, open issues and challenges. IEEE Access, 2024. [884] Keshav Ramji, Young-Suk Lee, R. Astudillo, M. Sultan, Tahira Naseem, Asim Munawar, Radu Florian, and S. Roukos. Self-refinement of language models from external proxy metrics feedback, arXiv preprint arXiv:2403.00827, 2024. URL https://arxiv.org/abs/2403.00827v1. 125 [885] Sumedh Rasal. An artificial neuron for enhanced problem solving in large language models, arXiv preprint arXiv:2404.14222, 2024. URL https://arxiv.org/abs/2404.14222v1. [886] Shaina Raza, Ranjan Sapkota, Manoj Karkee, and Christos Emmanouilidis. Trism for agentic ai: review of trust, risk, and security management in llm-based agentic multi-agent systems, arXiv preprint arXiv:2506.04133, 2025. URL https://arxiv.org/abs/2506.04133v2. [887] Jing Ren and Feng Xia. Brain-inspired artificial intelligence: comprehensive review, arXiv preprint arXiv:2408.14811, 2024. URL https://arxiv.org/abs/2408.14811v1. [888] Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, and Jiajun Zhang. Towards scientific intelligence: survey of llm-based scientific agents, arXiv preprint arXiv:2503.24047, 2025. URL https://arxiv.org/abs/2503.24047v2. [889] Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla, and Chao Huang. survey of large language models for graphs. Knowledge Discovery and Data Mining, 2024. [890] Yi Ren, Shangmin Guo, Linlu Qiu, Bailin Wang, and Danica J. Sutherland. Bias amplification in language model evolution: An iterated learning perspective. Neural Information Processing Systems, 2024. [891] Alireza Rezazadeh, Zichao Li, Wei Wei, and Yujia Bao. From isolated conversations to hierarchical International Conference on Learning schemas: Dynamic tree memory representation for llms. Representations, 2024. [892] Marco Tulio Ribeiro, Carlos Guestrin, and Sameer Singh. Are red roses red? evaluating consistency of question-answering models. Annual Meeting of the Association for Computational Linguistics, 2019. [893] Yara Rizk, Abhishek Bhandwalder, S. Boag, Tathagata Chakraborti, Vatche Isahagian, Y. Khazaeni, Falk Pollock, and Merve Unuvar. unified conversational assistant framework for business process automation, arXiv preprint arXiv:2001.03543, 2020. URL https://arxiv.org/abs/2001. 03543v1. [894] Yara Rizk, Vatche Isahagian, S. Boag, Y. Khazaeni, Merve Unuvar, Vinod Muthusamy, and Rania Y. Khalaf. conversational digital assistant for intelligent process automation. International Conference on Business Process Management, 2020. [895] S. Rizvi, Nazreen Pallikkavaliyaveetil, David Zhang, Zhuoyang Lyu, Nhi Nguyen, Haoran Lyu, B. Christensen, J. O. Caro, Antonio H. O. Fonseca, E. Zappala, Maryam Bagherian, Christopher Averill, C. Abdallah, Amin Karbasi, Rex Ying, M. Brbic, R. M. Dhodapkar, and David van Dijk. Fimp: Foundation model-informed message passing for graph neural networks, arXiv preprint arXiv:2210.09475v5, 2022. URL https://arxiv.org/abs/2210.09475v5. [896] Joshua Robinson, Christopher Rytting, and D. Wingate. Leveraging large language models for multiple choice question answering. International Conference on Learning Representations, 2022. [897] Juri Di Rocco, D. D. Ruscio, Claudio Di Sipio, P. T. Nguyen, and Riccardo Rubei. On the use of large language models in model-driven engineering. Journal of Software and Systems Modeling, 2024. [898] Juan David Salazar Rodriguez, Sam Conrad Joyce, and Julfendi Julfendi. Using customized gpt to develop prompting proficiency in architectural ai-generated images, arXiv preprint arXiv:2504.13948, 2025. URL https://arxiv.org/abs/2504.13948v2. 126 [899] Albert Roethel, M. Ganzha, and Anna Wroblewska. Enriching language models with graph-based context information to better understand textual data. Electronics, 2023. [900] Reudismam Rolim, Gustavo Soares, Rohit Gheyi, and Loris Dantoni. Learning quick fixes from code repositories. Brazilian Symposium on Software Engineering, 2018. [901] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. Computer Vision and Pattern Recognition, 2021. [902] Hayley Ross, A. Mahabaleshwarkar, and Yoshi Suhara. When2call: When (not) to call tools. North American Chapter of the Association for Computational Linguistics, 2025. [903] J. Rosser and Jakob N. Foerster. Agentbreeder: Mitigating the ai safety impact of multi-agent scaffolds, arXiv preprint arXiv:2502.00757, 2025. URL https://arxiv.org/abs/2502.00757v3. [904] Federico Rossi, Saptarshi Bandyopadhyay, Michael T. Wolf, and M. Pavone. Review of multi-agent algorithms for collective behavior: structural taxonomy, arXiv preprint arXiv:1803.05464, 2018. URL https://arxiv.org/abs/1803.05464v1. [905] Federico Rossi, Saptarshi Bandyopadhyay, Michael T. Wolf, and M. Pavone. Multi-agent algorithms for collective behavior: structural and application-focused atlas, arXiv preprint arXiv:2103.11067, 2021. URL https://arxiv.org/abs/2103.11067v1. [906] Alex Roxin and Stefano Fusi. Efficient partitioning of memory systems and its importance for memory consolidation. PLoS Comput. Biol., 2013. [907] Kaushik Roy, Yuxin Zi, Vignesh Narayanan, Manas Gaur, and Amit P. Sheth. Knowledge-infused self attention transformers, arXiv preprint arXiv:2306.13501, 2023. URL https://arxiv.org/abs/ 2306.13501v1. [908] Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Jiayang Cheng, Cunxiang Wang, Shichao Sun, Huanyu Li, Zizhao Zhang, Binjie Wang, Jiarong Jiang, Tong He, Zhiguo Wang, Pengfei Liu, Yue Zhang, and Zheng Zhang. Ragchecker: fine-grained framework for diagnosing retrieval-augmented generation. Neural Information Processing Systems, 2024. [909] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. Tptu: Large language model-based ai agents for task planning and tool usage, arXiv preprint arXiv:2308.03427, 2023. URL https://arxiv.org/ abs/2308.03427. [910] M. Russak, Umar Jamil, Christopher Bryant, Kiran Kamble, Axel Magnuson, Mateusz Russak, and Waseem Alshikh. Writing in the margins: Better inference pattern for long context retrieval, arXiv preprint arXiv:2408.14906, 2024. URL https://arxiv.org/abs/2408.14906v1. [911] Hyun Ryu and Eric Kim. Closer look at efficient inference methods: survey of speculative decoding, arXiv preprint arXiv:2411.13157, 2024. URL https://arxiv.org/abs/2411.13157v2. [912] Iman Saberi and Fatemeh Fard. Context-augmented code generation using programming knowledge graphs, arXiv preprint arXiv:2410.18251, 2024. URL https://arxiv.org/abs/2410. 18251v2. 127 [913] Abdulfattah Safa and Gözde Gül Sahin. zero-shot open-vocabulary pipeline for dialogue understanding. North American Chapter of the Association for Computational Linguistics, 2024. [914] Alireza Akhavan Safaei, Pegah Saboori, Reza Ramezani, and Mohammadali Nematbakhsh. Kglm-qa: novel approach for knowledge graph-enhanced large language models for question answering. Conference on Information and Knowledge Technology, 2024. [915] Avirup Saha, Lakshmi Mandal, Balaji Ganesan, Sambit Ghosh, Renuka Sindhgatta, Carlos Eberhardt, Dan Debrunner, and Sameep Mehta. Sequential api function calling using graphql schema. Conference on Empirical Methods in Natural Language Processing, 2024. [916] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, S. Mondal, and Aman Chadha. systematic survey of prompt engineering in large language models: Techniques and applications, arXiv preprint arXiv:2402.07927, 2024. URL https://arxiv.org/abs/2402.07927v2. [917] Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, and Yassine Benajiba. Meminsight: Autonomous memory augmentation for llm agents, arXiv preprint arXiv:2503.21760, 2025. URL https://arxiv.org/abs/2503.21760. [918] Jefferson Salan, Devyn Smith, Erica Shafer, and Rachel Diana. Variation in encoding context benefits item recognition. Memory & Cognition, 2024. [919] Alaa Saleh, Sasu Tarkoma, Praveen Kumar Donta, Naser Hossein Motlagh, S. Dustdar, Susanna Pirttikangas, and Lauri Loven. Usercentrix: An agentic memory-augmented ai framework for smart spaces, arXiv preprint arXiv:2505.00472, 2025. URL https://arxiv.org/abs/2505. 00472v1. [920] Leonard Salewski, Stephan Alaniz, Isabel Rio-Torto, Eric Schulz, and Zeynep Akata. In-context impersonation reveals large language models strengths and biases. Neural Information Processing Systems, 2023. [921] A. Samsonovich. Toward unified catalog of implemented cognitive architectures. Biologically Inspired Cognitive Architectures, 2010. [922] Narendra Reddy Sanikommu. Model context protocol: Enhancing llm performance for observability and analytics. European journal of computer science and information technology, 2025. [923] S. Santhanam. Context based text-generation using lstm networks, arXiv preprint arXiv:2005.00048, 2020. URL https://arxiv.org/abs/2005.00048v1. [924] G. Santos, Rita Maria Silva Julia, and Marcelo Zanchetta do Nascimento. Diverse prompts: Illuminating the prompt space of large language models with map-elites. IEEE Congress on Evolutionary Computation, 2025. [925] Ranjan Sapkota, Konstantinos I. Roumeliotis, and Manoj Karkee. Ai agents vs. agentic ai: conceptual taxonomy, applications and challenges, arXiv preprint arXiv:2505.10468, 2025. URL https://arxiv.org/abs/2505.10468v4. [926] Anjana Sarkar and Soumyendu Sarkar. Survey of llm agent communication with mcp: software design pattern centric review, arXiv preprint arXiv:2506.05364, 2025. URL https://arxiv.org/ abs/2506.05364v1. 128 [927] Soumajyoti Sarkar and Leonard Lausen. Testing the limits of unified sequence to sequence llm pretraining on diverse table data tasks, arXiv preprint arXiv:2310.00789, 2023. URL https: //arxiv.org/abs/2310.00789v1. [928] Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. Raptor: Recursive abstractive processing for tree-organized retrieval. International Conference on Learning Representations, 2024. [929] Gabriele Sarti. Umberto-mtsa @ accompl-it: Improving complexity and acceptability prediction with multi-task learning on self-supervised annotations (short paper). International Workshop on Evaluation of Natural Language and Speech Tools for Italian, 2020. [930] Apoorv Saxena, Adrian Kochsiek, and Rainer Gemulla. Sequence-to-sequence knowledge graph completion and question answering. Annual Meeting of the Association for Computational Linguistics, 2022. [931] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, R. Raileanu, M. Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Neural Information Processing Systems, 2023. [932] Guido Schillaci, Uwe Schmidt, and Luis Miranda. Prediction error-driven memory consolidation for continual learning: On the case of adaptive greenhouse models. KI - Künstliche Intelligenz, 35(1): 7180, 2021. ISSN 1610-1987. doi: 10.1007/s13218-020-00700-8. URL http://dx.doi.org/ 10.1007/s13218-020-00700-8. [933] Florian Schneider, Narges Baba Ahmadi, Niloufar Baba Ahmadi, Iris Vogel, Martin Semmann, and Christian Biemann. Collex - multimodal agentic rag system enabling interactive exploration of scientific collections. arXiv preprint, 2025. [934] Sheila Schoepp, Masoud Jafaripour, Yingyue Cao, Tianpei Yang, Fatemeh Abdollahi, Shadan Golestan, Zahin Sufiyan, Osmar R. Zaiane, and Matthew E. Taylor. The evolving landscape of llmand vlmintegrated reinforcement learning. arXiv preprint, 2025. [935] Lianlei Shan, Shixian Luo, Zezhou Zhu, Yu Yuan, and Yong Wu. Cognitive memory in large language models, arXiv preprint arXiv:2504.02441, 2025. URL https://arxiv.org/abs/2504. 02441v2. [936] Wenbo Shang and Xin Huang. survey of large language models on generative graph analytics: Query, learning, and applications, arXiv preprint arXiv:2404.14809v2, 2024. URL https://arxiv. org/abs/2404.14809v2. [937] Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-llm: trainable agent for role-playing, arXiv preprint arXiv:2310.10158, 2023. URL https://arxiv.org/abs/2310.10158. [938] Yutong Shao and N. Nakashole. On linearizing structured data in encoder-decoder language models: Insights from text-to-sql. North American Chapter of the Association for Computational Linguistics, 2024. [939] Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. Synthetic prompting: Generating chain-of-thought demonstrations for large language models. International Conference on Machine Learning, 2023. 129 [940] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant, Panupong Pasupat, Hexiang Hu, Urvashi Khandelwal, Kenton Lee, and Kristina Toutanova. From pixels to ui actions: Learning to follow instructions via graphical user interfaces. Neural Information Processing Systems, 2023. [941] Jonathan Shen, Ruoming Pang, Ron J. Weiss, M. Schuster, N. Jaitly, Zongheng Yang, Z. Chen, Yu Zhang, Yuxuan Wang, R. Skerry-Ryan, R. Saurous, Yannis Agiomyrgiannakis, and Yonghui Wu. Natural tts synthesis by conditioning wavenet on mel spectrogram predictions. IEEE International Conference on Acoustics, Speech, and Signal Processing, 2017. [942] Junhong Shen, Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny, and Ameet Talwalkar. Scribeagent: Towards specialized web agents using production-scale workflow data. 2024. [943] Junhong Shen, Hao Bai, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar, and Aviral Kumar. Thinking vs. doing: Agents that reason by scaling test-time interaction, arXiv preprint arXiv:2506.07976, 2025. URL https://arxiv. org/abs/2506.07976. [944] Weizhou Shen, Chenliang Li, Fanqi Wan, Shengyi Liao, Shaopeng Lai, Bo Zhang, Yingcheng Shi, Yuning Wu, Gang Fu, Zhansheng Li, Bin Yang, Ji Zhang, Fei Huang, Jingren Zhou, and Ming Yan. Qwenlong-cprs: Towards -llms with dynamic context optimization. arXiv preprint, 2025. [945] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Y. Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Neural Information Processing Systems, 2023. [946] Zhuocheng Shen. Llm with tools: survey, arXiv preprint arXiv:2409.18807, 2024. URL https: //arxiv.org/abs/2409.18807v1. [947] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark W. Barrett, Joseph Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. High-throughput generative inference of large language models with single gpu. International Conference on Machine Learning, 2023. [948] Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong, Tianrui Qin, King Zhu, Minghao Liu, Jian Yang, Ge Zhang, Jiaheng Liu, Changwang Zhang, Jun Wang, Y. Jiang, and Wangchunshu Zhou. Taskcraft: Automated generation of agentic tasks, arXiv preprint arXiv:2506.10055, 2025. URL https://arxiv.org/abs/2506.10055v2. [949] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and J. Kwok. Sparsebert: Rethinking the importance analysis in self-attention. International Conference on Machine Learning, 2021. [950] Peng Shi, Patrick Ng, Zhiguo Wang, Henghui Zhu, Alexander Hanbo Li, Jun Wang, C. D. Santos, and Bing Xiang. Learning contextual representations for semantic parsing with generation-augmented pre-training. AAAI Conference on Artificial Intelligence, 2020. [951] Weijia Shi, Xiaochuang Han, M. Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and S. Yih. Trusting your evidence: Hallucinate less with context-aware decoding. North American Chapter of the Association for Computational Linguistics, 2023. 130 [952] Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Zhumin Chen, Suzan Verberne, and Zhaochun Ren. Tool learning in the wild: Empowering language models as automatic tool agents. The Web Conference, 2024. [953] Jay Shim, Grant Kruttschnitt, Alyssa Ma, Daniel Kim, Benjamin Chek, Athul Anand, Kevin Zhu, and Sean OBrien. Chain-of-thought augmentation with logit contrast for enhanced reasoning in language models, arXiv preprint arXiv:2407.03600, 2024. URL https://arxiv.org/abs/2407. 03600v2. [954] Jiho Shin, Reem Aleithan, Hadi Hemmati, and Song Wang. Retrieval-augmented test generation: How far are we?, arXiv preprint arXiv:2409.12682, 2024. URL https://arxiv.org/abs/2409. 12682v1. [955] Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, Hyoungseok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, W. Park, Jung-Woo Ha, and Nako Sung. On the effect of pretraining corpora on in-context learning by large-scale language model. North American Chapter of the Association for Computational Linguistics, 2022. [956] Noah Shinn, Federico Cassano, Beck Labash, A. Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. Neural Information Processing Systems, 2023. [957] Fatemeh Shiri, Xiao-Yu Guo, Mona Far, Xin Yu, Reza Haf, and Yuan-Fang Li. An empirical analysis on spatial reasoning capabilities of large multimodal models. Conference on Empirical Methods in Natural Language Processing, 2024. [958] Masoud Shokrnezhad, Hao Yu, T. Taleb, Renwei Li, Kyunghan Lee, Jaeseung Song, and Cedric Westphal. Toward dynamic future with adaptable computing and network convergence (acnc). IEEE Network, 2024. [959] Connor Shorten, T. Khoshgoftaar, and B. Furht. Text data augmentation for deep learning. Journal of Big Data, 2021. [960] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and D. Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. Computer Vision and Pattern Recognition, 2019. [961] Altun Shukurlu. Improving deep knowledge tracing via gated architectures and adaptive optimization, arXiv preprint arXiv:2504.20070, 2025. URL https://arxiv.org/abs/2504.20070v1. [962] Lynn Siegel and M. Kahana. retrieved context account of spacing and repetition effects in free recall. Journal of Experimental Psychology. Learning, Memory and Cognition, 2014. [963] Aditi Singh, Abul Ehtesham, Gaurav Kumar Gupta, Nikhil Kumar Chatta, Saket Kumar, and T. T. Khoei. Exploring prompt engineering: systematic review with swot analysis, arXiv preprint arXiv:2410.12843, 2024. URL https://arxiv.org/abs/2410.12843v1. [964] Anmolika Singh and Yuhang Diao. Leveraging large language models for optimized item categorization using unspsc taxonomy. International Journal on Cybernetics & Informatics, 2024. [965] Joykirat Singh, Raghav Magazine, Yash Pandya, and A. Nambi. Agentic reasoning and tool integration for llms via reinforcement learning, arXiv preprint arXiv:2505.01441, 2025. URL https://arxiv. org/abs/2505.01441v1. [966] Krishnakant Singh, Thanush Navaratnam, Jannik Holmer, Simone Schaub-Meyer, and Stefan Roth. Is synthetic data all we need? benchmarking the robustness of models trained with synthetic images. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2024. [967] Ramneet Singh, Sathvik Joel, Abhav Mehrotra, Nalin Wadhwa, Ramakrishna Bairi, Aditya Kanade, and Nagarajan Natarajan. Code researcher: Deep research agent for large systems code and commit history, arXiv preprint arXiv:2506.11060, 2025. URL https://arxiv.org/abs/2506. 11060v1. [968] Aarush Sinha and CU Omkumar. Gmlm: Bridging graph neural networks and language models for heterophilic node classification, arXiv preprint arXiv:2503.05763, 2025. URL https://arxiv. org/abs/2503.05763v3. [969] Sanchit Sinha, Yuguang Yue, Victor Soto, Mayank Kulkarni, Jianhua Lu, and Aidong Zhang. Mamlen-llm: Model agnostic meta-training of llms for improved in-context learning. Knowledge Discovery and Data Mining, 2024. [970] Colin Sisate, Alistair Goldfinch, Vincent Waterstone, Sebastian Kingsley, and Mariana Blackthorn. Contextually entangled gradient mapping for optimized llm comprehension, arXiv preprint arXiv:2502.00048, 2025. URL https://arxiv.org/abs/2502.00048v1. [971] Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, and Ryan McDonald. Step: Stacked llm policies for web actions, arXiv preprint arXiv:2310.03720, 2024. URL https://arxiv.org/abs/2310.03720. [972] Manthankumar Solanki. Efficient document retrieval with g-retriever. arXiv preprint, 2025. [973] Karthik Soman, Peter Rose, John Morris, Rabia Akbas, Brett Smith, Braian Peetoom, Catalina Villouta-Reyes, G. Cerono, Yongmei Shi, Angela Rizk-Jackson, Sharat Israni, Charlotte A. Nelson, Sui Huang, and Sergio Baranzini. Biomedical knowledge graph-optimized prompt generation for large language models. Bioinformatics, 2023. [974] Lilian Some, Wenli Yang, Michael Bain, and Byeong Kang. comprehensive survey on integrating large language models with knowledge-based methods. Knowledge-Based Systems, 2025. [975] Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. Llmplanner: Few-shot grounded planning for embodied agents with large language models. IEEE International Conference on Computer Vision, 2022. [976] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint, 2025. [977] Woomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, and Jinwoo Shin. Hierarchical context merging: Better long context understanding for pre-trained llms. International Conference on Learning Representations, 2024. 132 [978] Woomin Song, Sai Muralidhar Jayanthi, S. Ronanki, Kanthashree Mysore Sathyendra, Jinwoo Shin, A. Galstyan, Shubham Katiyar, and S. Bodapati. Compress, gather, and recompute: Reforming long-context processing in transformers, arXiv preprint arXiv:2506.01215, 2025. URL https: //arxiv.org/abs/2506.01215v1. [979] Yewei Song, Xunzhu Tang, Cedric Lothritz, Saad Ezzini, Jacques Klein, Tegawende F. Bissyande, A. Boytsov, Ulrick Ble, and Anne Goujon. Callnavi, challenge and empirical study on llm function calling and routing, arXiv preprint arXiv:2501.05255, 2025. URL https://arxiv.org/abs/ 2501.05255v2. [980] Yueqi Song, Frank Xu, Shuyan Zhou, and Graham Neubig. Beyond browsing: Api-based web agents, arXiv preprint arXiv:2410.16464, 2025. URL https://arxiv.org/abs/2410.16464. [981] S. Srinivasa and Jayati Deshmukh. Paradigms of computational agency. Novel Approaches to Information Systems Design, 2021. [982] B. Staresina, R. Henson, N. Kriegeskorte, and Arjen Alink. Episodic reinstatement in the medial temporal lobe. Journal of Neuroscience, 2012. [983] T. Staudigl, C. Vollmar, S. Noachtar, and S. Hanslmayr. Temporal-pattern similarity analysis reveals the beneficial and detrimental effects of context reinstatement on human memory. Journal of Neuroscience, 2015. [984] Kaya Stechly, Karthik Valmeekam, and Subbarao Kambhampati. Chain of thoughtlessness? an analysis of cot in planning. Neural Information Processing Systems, 2024. [985] R. Sterken and James Ravi Kirkpatrick. Conversational alignment with artificial intelligence in context. Philosophical Perspectives, 2025. [986] Paul Stoewer, Achim Schilling, Andreas K. Maier, and Patrick Krauss. Multi-modal cognitive maps based on neural networks trained on successor representations, arXiv preprint arXiv:2401.01364, 2023. URL https://arxiv.org/abs/2401.01364v1. [987] Olly Styles, Sam Miller, Patricio Cerda-Mardini, T. Guha, Victor Sanchez, and Bertie Vidgen. Workbench: benchmark dataset for agents in realistic workplace setting, arXiv preprint arXiv:2405.00823, 2024. URL https://arxiv.org/abs/2405.00823v2. [988] Guangxin Su, Yifan Zhu, Wenjie Zhang, Hanchen Wang, and Ying Zhang. Bridging large language models and graph structure learning models for robust representation learning, arXiv preprint arXiv:2410.12096, 2024. URL https://arxiv.org/abs/2410.12096v1. [989] Hong Su, Elke A. Rundensteiner, and Murali Mani. Automaton in or out: run-time plan optimization for xml stream processing. International Symposium on Signal Processing Systems, 2008. [990] Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan Ö. Arık. Learn-by-interact: data-centric framework for self-adaptive agents in realistic environments. 2025. [991] Jinyan Su, Jennifer Healey, Preslav Nakov, and Claire Cardie. Between underthinking and overthinking: An empirical study of reasoning length and correctness in llms, arXiv preprint arXiv:2505.00127, 2025. URL https://arxiv.org/abs/2505.00127v1. [992] Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, and Yiqun Liu. Dragin: Dynamic retrieval augmented generation based on the real-time information needs of large language models. Annual Meeting of the Association for Computational Linguistics, 2024. [993] Xin Su, Man Luo, Kris Pan, Tien Pei Chou, Vasudev Lal, and Phillip Howard. Sk-vqa: Synthetic knowledge generation at scale for training context-augmented multimodal llms. arXiv preprint, 2024. [994] Budhitama Subagdja and A. Tan. Neural modeling of sequential inferences and learning over episodic memory. Neurocomputing, 2015. [995] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Hu. Stop overthinking: survey on efficient reasoning for large language models, arXiv preprint arXiv:2503.16419, 2025. URL https: //arxiv.org/abs/2503.16419v3. [996] Chuanneng Sun, Songjun Huang, and D. Pompili. Llm-based multi-agent reinforcement learning: Current and future directions, arXiv preprint arXiv:2405.11106, 2024. URL https://arxiv.org/ abs/2405.11106v1. [997] Hanshi Sun, Zhuoming Chen, Xinyu Yang, Yuandong Tian, and Beidi Chen. Triforce: Lossless acceleration of long sequence generation with hierarchical speculative decoding, arXiv preprint arXiv:2404.11912, 2024. URL https://arxiv.org/abs/2404.11912v3. [998] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. Neural Information Processing Systems, 2023. [999] Jiankai Sun, Chuanyang Zheng, E. Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, P. Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Jingwei Wen, Xipeng Qiu, Yi-Chen Guo, Hui Xiong, Qun Liu, and Zhenguo Li. survey of reasoning with foundation models: Concepts, methodologies, and outlook. ACM Computing Surveys, 2023. [1000] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Sai Wang, Chen Lin, Yeyun Gong, H. Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. arXiv preprint, 2023. [1001] Lei Sun, Zhengwei Tao, Youdi Li, and Hiroshi Arakawa. Oda: Observation-driven agent for integrating llms and knowledge graphs, arXiv preprint arXiv:2404.07677, 2024. URL https://arxiv.org/ abs/2404.07677. [1002] Lei Sun, Xinchen Wang, and Youdi Li. Pyramid-driven alignment: Pyramid principle guided integration of large language models and knowledge graphs, arXiv preprint arXiv:2410.12298, 2024. URL https://arxiv.org/abs/2410.12298v2. [1003] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui: Towards multimodal conversational agents on mobile gui. Conference on Empirical Methods in Natural Language Processing, 2022. 134 [1004] Lijun Sun, Yijun Yang, Qiqi Duan, Yuhui Shi, Chao Lyu, Yu-Cheng Chang, Chin-Teng Lin, and Yang Shen. Multi-agent coordination across diverse applications: survey, arXiv preprint arXiv:2502.14743, 2025. URL https://arxiv.org/abs/2502.14743v2. [1005] Qianru Sun, Yaoyao Liu, Tat-Seng Chua, and B. Schiele. Meta-transfer learning for few-shot learning. Computer Vision and Pattern Recognition, 2018. [1006] Yu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. Ernie 2.0: continual pre-training framework for language understanding. AAAI Conference on Artificial Intelligence, 2019. [1007] Rao Surapaneni, Miku Jha, Michael Vakoc, protocol agent2agent a2a-a-new-era-of-agent-interoperability/, April 2025. July-2025]. and Todd Segal. Announcing the https://developers.googleblog.com/en/ [Online; accessed 17- (a2a). [1008] Stefan Szeider. Mcp-solver: Integrating language models with constraint programming systems. arXiv preprint, 2024. [1009] Daniel Szelogowski. Engram memory encoding and retrieval: neurocomputational perspective, arXiv preprint arXiv:2506.01659, 2025. URL https://arxiv.org/abs/2506.01659v1. [1010] N. Taatgen, David Huss, D. Dickison, and John R. Anderson. The acquisition of robust and flexible cognitive skills. Journal of experimental psychology. General, 2008. [1011] Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan Richard Schwarz. Online adaptation of language models with memory of amortized contexts. Neural Information Processing Systems, 2024. [1012] Yan Tai, Weichen Fan, Zhao Zhang, Feng Zhu, Rui Zhao, and Ziwei Liu. Link-context learning for multimodal llms. Computer Vision and Pattern Recognition, 2023. [1013] Fahim Tajwar, Yiding Jiang, Abitha Thankaraj, Sumaita Sadia Rahman, J. Z. Kolter, Jeff Schneider, and Ruslan Salakhutdinov. Training generally curious agent, arXiv preprint arXiv:2502.17543, 2025. URL https://arxiv.org/abs/2502.17543v3. [1014] K. Tallam. From autonomous agents to integrated systems, new paradigm: Orchestrated distributed intelligence, arXiv preprint arXiv:2503.13754, 2025. URL https://arxiv.org/abs/2503. 13754v2. [1015] A. Tan, Budhitama Subagdja, Di Wang, and Lei Meng. Self-organizing neural networks for universal learning and multimodal memory encoding. Neural Networks, 2019. [1016] Chuanyuan Tan, Yuehe Chen, Wenbiao Shao, and Wenliang Chen. Make choice! knowledge base question answering with in-context learning, arXiv preprint arXiv:2305.13972, 2023. URL https://arxiv.org/abs/2305.13972v1. [1017] Sijun Tan, Xiuyu Li, Shishir G. Patil, Ziyang Wu, Tianjun Zhang, Kurt Keutzer, Joseph E. Gonzalez, and Raluca A. Popa. Lloco: Learning long contexts offline. Conference on Empirical Methods in Natural Language Processing, 2024. 135 [1018] Xiaoyu Tan, Haoyu Wang, Xihe Qiu, Yuan Cheng, Yinghui Xu, Wei Chu, and Yuan Qi. Struct-x: Enhancing large language models reasoning with structured data. arXiv preprint, 2024. [1019] Zhaoxuan Tan and Meng Jiang. User modeling in the era of large language models: Current research and future directions. IEEE Data Engineering Bulletin, 2023. [1020] Zhijie Tan, Xu Chu, Weiping Li, and Tong Mo. Order matters: Exploring order sensitivity in multimodal large language models, arXiv preprint arXiv:2410.16983v1, 2024. URL https:// arxiv.org/abs/2410.16983v1. [1021] Matthew Tancik, Pratul P. Srinivasan, B. Mildenhall, Sara Fridovich-Keil, N. Raghavan, Utkarsh Singhal, R. Ramamoorthi, J. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Neural Information Processing Systems, 2020. [1022] Fei Tang, Haolei Xu, Hang Zhang, Siqi Chen, Xingyu Wu, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Zeqi Tan, Yuchen Yan, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, and Yueting Zhuang. survey on (m)llm-based gui agents, arXiv preprint arXiv:2504.13865, 2025. URL https://arxiv. org/abs/2504.13865v2. [1023] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt: Graph instruction tuning for large language models. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2023. [1024] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. Graphgpt: Graph instruction tuning for large language models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 491500, 2024. [1025] Jianheng Tang, Qifan Zhang, Yuhan Li, Nuo Chen, and Jia Li. Grapharena: Evaluating and exploring large language models on graph computation. arXiv preprint arXiv:2407.00379, 2024. [1026] Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark Gerstein. Struc-bench: Are large language models good at generating complex structured tabular data? North American Chapter of the Association for Computational Linguistics, 2024. [1027] Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, Arman Cohan, and Mark Gerstein. Chemagent: Self-updating library in large language models improves chemical reasoning, arXiv preprint arXiv:2501.06590, 2025. URL https://arxiv.org/abs/2501.06590v1. [1028] Xuemei Tang, Jun Wang, and Q. Su. Chinese word segmentation with heterogeneous graph neural network, arXiv preprint arXiv:2201.08975, 2022. URL https://arxiv.org/abs/2201.08975v1. [1029] Yiqing Tang, Xingyuan Dai, Chengchong Zhao, Qi Cheng, and Yisheng Lv. Large language modeldriven urban traffic signal control. Australian and New Zealand Control Conference, 2024. [1030] Yongjian Tang, Rakebul Hasan, and Thomas Runkler. Fsponer: Few-shot prompt optimization for named entity recognition in domain-specific scenarios. European Conference on Artificial Intelligence, 2024. [1031] Yunlong Tang, Daiki Shimada, Jing Bi, Hang Hua, and Chenliang Xu. Empowering llms with pseudountrimmed videos for audio-visual temporal understanding. AAAI Conference on Artificial Intelligence, 2024. 136 [1032] Yao Tao, Yehui Tang, Yun Wang, Mingjian Zhu, Hailin Hu, and Yunhe Wang. Saliency-driven dynamic token pruning for large language models, arXiv preprint arXiv:2504.04514, 2025. URL https://arxiv.org/abs/2504.04514v2. [1033] Denis Tarasov and Kumar Shridhar. Distilling llms decomposition abilities into compact language models, arXiv preprint arXiv:2402.01812, 2024. URL https://arxiv.org/abs/2402.01812v1. [1034] Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai, and Kunat Pipatanakul. Typhoon t1: An open thai reasoning model, arXiv preprint arXiv:2502.09042, 2025. URL https: //arxiv.org/abs/2502.09042v2. [1035] Yi Tay, Anh Tuan Luu, Minh C. Phan, and S. Hui. Multi-task neural network for non-discrete attribute prediction in knowledge graphs. International Conference on Information and Knowledge Management, 2017. [1036] 36Kr Editorial Team. The future of ai: From parameter scaling to context scaling. Online, 2025. URL https://36kr.com/p/3337269379328264. Chinese business and technology media publication discussing context scaling in large language models. [1037] Junfeng Tian, Da Zheng, Yang Cheng, Rui Wang, Colin Zhang, and Debing Zhang. Untie the knots: An efficient data augmentation strategy for long-context pre-training in language models, arXiv preprint arXiv:2409.04774, 2024. URL https://arxiv.org/abs/2409.04774v1. [1038] Tian, Wang, Guo, Wu, Dong, and Wang. . . . Ego-r1: Chain-of-tool-thought for ultra-long egocentric video reasoning. 2025. URL https://arxiv.org/abs/2506.13654. [1039] Shulin Tian, Ruiqi Wang, Hongming Guo, Penghao Wu, Yuhao Dong, Xiuying Wang, Jingkang Yang, Hao Zhang, Hongyuan Zhu, and Ziwei Liu. Ego-r1: Chain-of-tool-thought for ultra-long egocentric video reasoning. arXiv preprint, 2025. [1040] Ramine Tinati, Xin Wang, Ian C. Brown, T. Tiropanis, and W. Hall. streaming real-time web observatory architecture for monitoring the health of social machines. The Web Conference, 2015. [1041] Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. Neural Information Processing Systems, 2022. [1042] Despina Tomkou, George Fatouros, Andreas Andreou, Georgios Makridis, F. Liarokapis, Dimitrios Dardanis, Athanasios Kiourtis, John Soldatos, and D. Kyriazis. Bridging industrial expertise and xr with llm-powered conversational agents, arXiv preprint arXiv:2504.05527, 2025. URL https: //arxiv.org/abs/2504.05527v1. [1043] Sabrina Toro, A. V. Anagnostopoulos, Sue Bello, Kai Blumberg, Rhiannon Cameron, Leigh Carmody, A. Diehl, Damion M. Dooley, William Duncan, P. Fey, Pascale Gaudet, Nomi L. Harris, marcin p. joachimiak, Leila Kiani, Tiago Lubiana, M. Munoz-Torres, Shawn T. ONeil, David Osumi-Sutherland, Aleix Puig, Justin Reese, L. Reiser, Sofia Robb, Troy Ruemping, James Seager, Eric Sid, Ray Stefancsik, Magalie Weber, Valerie Wood, M. Haendel, and Christopher J. Mungall. Dynamic retrieval augmented generation of ontologies using artificial intelligence (dragon-ai). Journal of Biomedical Semantics, 2023. [1044] Fernanda De La Torre, Cathy Mengying Fang, Han Huang, Andrzej Banburski-Fahey, Judith Amores Fernandez, and Jaron Lanier. Llmr: Real-time prompting of interactive worlds using large language models. International Conference on Human Factors in Computing Systems, 2023. [1045] Martina Toshevska and Sonja Gievska. Llm-based text style transfer: Have we taken step forward? IEEE Access, 2025. [1046] Fouad Trad and Ali Chehab. Evaluating the efficacy of prompt-engineered large multimodal models versus fine-tuned vision transformers in image-based security applications. ACM Transactions on Intelligent Systems and Technology, 2024. [1047] Khanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry OSullivan, and Hoang D. Nguyen. Multi-agent collaboration mechanisms: survey of llms, arXiv preprint arXiv:2501.06322, 2025. URL https://arxiv.org/abs/2501.06322v1. [1048] Harold Triedman, Rishi Jha, and Vitaly Shmatikov. Multi-agent systems execute arbitrary malicious code, arXiv preprint arXiv:2503.12188, 2025. URL https://arxiv.org/abs/2503.12188v1. [1049] H. Trivedi, Tushar Khot, Mareike Hartmann, R. Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. Appworld: controllable world of apps and people for benchmarking interactive coding agents. Annual Meeting of the Association for Computational Linguistics, 2024. [1050] Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, and Shou-De Lin. Text-centric alignment for multi-modality learning, arXiv preprint arXiv:2402.08086v2, 2024. URL https://arxiv.org/ abs/2402.08086v2. [1051] Tao Tu, M. Schaekermann, Anil Palepu, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Yong Cheng, Elahe Vedadi, Nenad Tomašev, Shekoofeh Azizi, Karan Singhal, Le Hou, Albert Webson, Kavita Kulkarni, S. Mahdavi, Christopher Semturs, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S. Corrado, Yossi Matias, A. Karthikesalingam, and Vivek Natarajan. Towards conversational diagnostic artificial intelligence. Nature, 2025. [1052] Eduard Tulchinskii, Laida Kushnareva, Kristian Kuznetsov, Anastasia Voznyuk, Andrei Andriiainen, Irina Piontkovskaya, Evgeny Burnaev, and Serguei Barannikov. Listening to the wise few: Selectand-copy attention heads for multiple-choice qa, arXiv preprint arXiv:2410.02343, 2024. URL https://arxiv.org/abs/2410.02343v1. [1053] Meet Udeshi, Minghao Shao, Haoran Xi, Nanda Rani, Kimberly Milner, Venkata Sai Charan Putrevu, Brendan Dolan-Gavitt, S. K. Shukla, P. Krishnamurthy, F. Khorrami, Ramesh Karri, and Muhammad Shafique. D-cipher: Dynamic collaborative intelligent multi-agent system with planner and heterogeneous executors for offensive security. arXiv preprint, 2025. [1054] M. Ursino, Nicole Cesaretti, and G. Pirazzini. model of working memory for encoding multiple items and ordered sequences exploiting the theta-gamma code. Cognitive Neurodynamics, 2022. [1055] D. H. V. Uytsel, Filip Van Aelten, and Dirk Van Compernolle. structured language model based on context-sensitive probabilistic left-corner parsing. North American Chapter of the Association for Computational Linguistics, 2001. 138 [1056] Saeid Ario Vaghefi, Aymane Hachcham, Veronica Grasso, Jiska Manicus, Nakiete Msemo, C. Senni, and Markus Leippold. Ai for climate finance: Agentic retrieval and multi-step reasoning for early warning system investments, arXiv preprint arXiv:2504.05104, 2025. URL https://arxiv.org/ abs/2504.05104v2. [1057] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. CHI Extended Abstracts, 2022. [1058] Phuc Phan Van, Dat Nguyen Minh, An Dinh Ngoc, and Huy-Phan Thanh. Rx strategist: Prescription verification using llm agents system, arXiv preprint arXiv:2409.03440, 2024. URL https://arxiv. org/abs/2409.03440v1. [1059] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and I. Polosukhin. Attention is all you need. Neural Information Processing Systems, 2017. [1060] J. D. Velásquez-Henao, Carlos Jaime Franco-Cardona, and Lorena Cadavid-Higuita. Prompt engineering: methodology for optimizing interactions with ai-language models in the field of engineering. DYNA, 2023. [1061] Gaurav Verma, Rachneet Kaur, Nishan Srishankar, Zhen Zeng, T. Balch, and Manuela Veloso. Adaptagent: Adapting multimodal web agents with few-shot learning from human demonstrations, arXiv preprint arXiv:2411.13451, 2024. URL https://arxiv.org/abs/2411.13451v1. [1062] Aliaksei Vertsel and Mikhail Rumiantsau. Hybrid llm/rule-based approaches to business insights generation from structured data, arXiv preprint arXiv:2404.15604, 2024. URL https://arxiv. org/abs/2404.15604v1. [1063] Aishwarya Vijayan. prompt engineering approach for structured data extraction from unstructured text using conversational llms. International Conference on Advances in Computing and Artificial Intelligence, 2023. [1064] Juraj Vladika, Alexander Fichtl, and Florian Matthes. Diversifying knowledge enhancement of biomedical language models using adapter modules and knowledge graphs. International Conference on Agents and Artificial Intelligence, 2023. [1065] James Vo. Sparseaccelerate: Efficient long-context inference for mid-range gpus, arXiv preprint arXiv:2412.06198, 2024. URL https://arxiv.org/abs/2412.06198v1. [1066] Blavz vSkrlj, Boshko Koloski, S. Pollak, and Nada Lavravc. From symbolic to neural and back: Exploring knowledge graph-large language model synergies. arXiv preprint, 2025. [1067] Tom Völker, Jan Pfister, Tobias Koopmann, and Andreas Hotho. From chat to publication management: Organizing your related work using bibsonomy & llms. Conference on Human Information Interaction and Retrieval, 2024. [1068] D. Walton. Using argumentation schemes to find motives and intentions of rational agent. Argument Comput., 2020. [1069] Hanlong Wan, Jian Zhang, Yan Chen, Weili Xu, and Fan Feng. Generative ai application for building industry. Building Simulation, 2024. 139 [1070] Jun Wan and Lingrui Mei. Large language models as computable approximations to solomonoff induction, arXiv preprint arXiv:2505.15784, 2025. URL https://arxiv.org/abs/2505.15784. [1071] Luanbo Wan and Weizhi Ma. Storybench: dynamic benchmark for evaluating long-term memory with multi turns, arXiv preprint arXiv:2506.13356, 2025. URL https://arxiv.org/abs/2506. 13356v1. [1072] Bernie Wang, Si ting Xu, K. Keutzer, Yang Gao, and Bichen Wu. Improving context-based meta-reinforcement learning with self-supervised trajectory contrastive learning, arXiv preprint arXiv:2103.06386, 2021. URL https://arxiv.org/abs/2103.06386v1. [1073] Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Scm: Enhancing large language model with self-controlled memory framework, arXiv preprint arXiv:2304.13343, 2025. URL https://arxiv.org/abs/2304.13343. [1074] Cangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, and Chengqian Fu. Adapting llms for efficient context processing through soft prompt compression. Proceedings of the International Conference on Modeling, Natural Language Processing and Machine Learning, 2024. [1075] Chaozheng Wang, Yuanhang Yang, Cuiyun Gao, Yun Peng, Hongyu Zhang, and Michael R. Lyu. Prompt tuning in code intelligence: An experimental evaluation. IEEE Transactions on Software Engineering, 2023. [1076] Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Kang Gu, and Sameena Shah. Docgraphlm: Documental graph language model for information extraction. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2023. [1077] Fan Wang, Chuan Lin, Yang Cao, and Yu Kang. Benchmarking general purpose in-context learning, arXiv preprint arXiv:2405.17234, 2024. URL https://arxiv.org/abs/2405.17234v6. [1078] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models, arXiv preprint arXiv:2305.16291, 2023. URL https://arxiv.org/abs/2305.16291. [1079] Guoqing Wang, Zeyu Sun, Zhihao Gong, Sixiang Ye, Yizhou Chen, Yifan Zhao, Qing-Lin Liang, and Dan Hao. Do advanced language models eliminate the need for prompt engineering in software engineering?, arXiv preprint arXiv:2411.02093, 2024. URL https://arxiv.org/abs/2411. 02093v1. [1080] Hanlin Wang, Zhan Tong, Kecheng Zheng, Yujun Shen, and Limin Wang. Contextual ad narration with interleaved multimodal sequence. Computer Vision and Pattern Recognition, 2024. [1081] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning. International Symposium on High-Performance Computer Architecture, 2020. [1082] Haochen Wang, Xiangtai Li, Zilong Huang, Anran Wang, Jiacong Wang, Tao Zhang, Jiani Zheng, Sule Bai, Zijian Kang, Jiashi Feng, et al. Traceable evidence enhanced visual grounded reasoning: Evaluation and methodology. arXiv preprint arXiv:2507.07999, 2025. 140 [1083] Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. Huatuo: Tuning llama model with chinese medical knowledge, arXiv preprint arXiv:2304.06975, 2023. URL https://arxiv.org/abs/2304.06975. [1084] Haoyu Wang, Tong Teng, Tianyu Guo, An Xiao, Duyu Tang, Hanting Chen, and Yunhe Wang. Unshackling context length: An efficient selective attention approach through query-key compression, arXiv preprint arXiv:2502.14477, 2025. URL https://arxiv.org/abs/2502.14477v1. [1085] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language? Neural Information Processing Systems, 2023. [1086] Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, A. Nambi, T. Ganu, and Hao Wang. Multimodal needle in haystack: Benchmarking long-context capability of multimodal large language models. North American Chapter of the Association for Computational Linguistics, 2024. [1087] Hongru Wang, Cheng Qian, Manling Li, Jiahao Qiu, Boyang Xue, Mengdi Wang, Heng Ji, and Kam-Fai Wong. Toward theory of agents as tool-use decision-makers, arXiv preprint arXiv:2506.00886, 2025. URL https://arxiv.org/abs/2506.00886v1. [1088] Jingjin Wang. Proprag: Guiding retrieval with beam search over proposition paths, arXiv preprint arXiv:2504.18070, 2025. URL https://arxiv.org/abs/2504.18070v1. [1089] Jingyu Wang, Lu Zhang, Xueqing Li, Huazhong Yang, and Yongpan Liu. Ulseq-ta: Ultra-long sequence attention fusion transformer accelerator supporting grouped sparse softmax and dual-path sparse layernorm. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2024. [1090] Jize Wang, Zerun Ma, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. Gta: benchmark for general tool agents. Neural Information Processing Systems, 2024. [1091] Lei Wang, Chengbang Ma, Xueyang Feng, Zeyu Zhang, Hao ran Yang, Jingsen Zhang, Zhi-Yang Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji rong Wen. survey on large language model based autonomous agents. Frontiers Comput. Sci., 2023. [1092] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Planand-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. Annual Meeting of the Association for Computational Linguistics, 2023. [1093] Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, et al. When large language model based agent meets user behavior analysis: novel user simulation paradigm. 2023. [1094] Libo Wang. Towards humanoid robot autonomy: dynamic architecture integrating continuous thought machines (ctm) and model context protocol (mcp), arXiv preprint arXiv:2505.19339, 2025. URL https://arxiv.org/abs/2505.19339v1. [1095] Liya Wang, Jason Chou, Xin Zhou, A. Tien, and Diane M. Baumgartner. Aviationgpt: large language model for the aviation domain, arXiv preprint arXiv:2311.17686, 2023. URL https: //arxiv.org/abs/2311.17686v1. 141 [1096] Liyuan Wang, Bo Lei, Qian Li, Hang Su, Jun Zhu, and Yi Zhong. Triple-memory networks: brain-inspired method for continual learning. IEEE Transactions on Neural Networks and Learning Systems, 2020. [1097] Lu Wang, Fangkai Yang, Chaoyun Zhang, Junting Lu, Jiaxu Qian, Shilin He, Pu Zhao, Bo Qiao, Ray Huang, Si Qin, Qisheng Su, Jiayi Ye, Yudi Zhang, Jian-Guang Lou, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. Large action models: From inception to implementation, arXiv preprint arXiv:2412.10047, 2025. URL https://arxiv.org/abs/2412.10047. [1098] Peijie Wang, Zhong-Zhi Li, Fei Yin, Xin Yang, Dekang Ran, and Cheng-Lin Liu. Mv-math: Evaluating multimodal math reasoning in multi-visual contexts. 2025. [1099] Qineng Wang, Zihao Wang, Ying Su, and Yangqiu Song. On the discussion of large language models: Symmetry of agents and interplay with prompts, arXiv preprint arXiv:2311.07076, 2023. URL https://arxiv.org/abs/2311.07076v1. [1100] Rongzheng Wang, Shuang Liang, Qizhi Chen, Jiasheng Zhang, and Ke Qin. Graphtool-instruction: Revolutionizing graph reasoning in llms through decomposed subtask instruction. Knowledge Discovery and Data Mining, 2024. [1101] Shengnan Wang, Youhui Bai, Lin Zhang, Pingyi Zhou, Shixiong Zhao, Gong Zhang, Sen Wang, Renhai Chen, Hua Xu, and Hongwei Sun. Xl3m: training-free framework for llm length extension based on segment-wise inference, arXiv preprint arXiv:2405.17755, 2024. URL https://arxiv. org/abs/2405.17755v1. [1102] Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge editing for large language models: survey. ACM Computing Surveys, 2023. [1103] Song Wang, Junhong Lin, Xiaojie Guo, Julian Shun, Jundong Li, and Yada Zhu. Reasoning of large language models over knowledge graphs with super-relations. International Conference on Learning Representations, 2025. [1104] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu, Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han Xiao, Z. Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yichen Xiao, Jing Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamujiang Tayier, Zhenyu Hu, Yuan Gao, Chengfeng Zheng, Yu-Jie Ye, Yihan Li, Lei Wan, Xinyue Jiang, Yujie Wang, Siyuan Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang, Huajun Chen, Y. Jiang, and Wangchunshu Zhou. Weaver: Foundation models for creative writing, arXiv preprint arXiv:2401.17268, 2024. URL https://arxiv.org/abs/2401.17268v1. [1105] Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. Visually-augmented language modeling. International Conference on Learning Representations, 2022. [1106] Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang Liu, Xiangnan He, and Tat seng Chua. Learning intents behind interactions with knowledge graph for recommendation. The Web Conference, 2021. [1107] Xiao Wang, Isaac Lyngaas, A. Tsaris, Peng Chen, Sajal Dash, Mayanka Chandra Shekar, Tao Luo, Hong-Jun Yoon, M. Wahib, and J. Gounley. Ultra-long sequence distributed transformer, arXiv preprint arXiv:2311.02382, 2023. URL https://arxiv.org/abs/2311.02382v2. 142 [1108] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and S. Yeung-Levy. Videoagent: Long-form video understanding with large language model as agent. European Conference on Computer Vision, 2024. [1109] Xiaolong Wang, Zhaolu Kang, Wangyuxuan Zhai, Xinyue Lou, Yunghwei Lai, Ziyue Wang, Yawen Wang, Kaiyu Huang, Yile Wang, Peng Li, and Yang Liu. Mucar: Benchmarking multilingual cross-modal ambiguity resolution for multimodal large language models, arXiv preprint arXiv:2506.17046v1, 2025. URL https://arxiv.org/abs/2506.17046v1. [1110] Xiaoqiang Wang, Suyuchen Wang, Yun Zhu, and Bang Liu. R3mem: Bridging memory retention and retrieval via reversible compression. arXiv preprint, 2025. [1111] Xiaoyang Wang, Pavan Kapanipathi, Ryan Musa, Mo Yu, Kartik Talamadupula, I. Abdelaziz, Maria Chang, Achille Fokoue, B. Makni, Nicholas Mattei, and M. Witbrock. Improving natural language inference using external knowledge in the science questions domain. AAAI Conference on Artificial Intelligence, 2018. [1112] Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, and A. Eshaghi. Beyond the limits: survey of techniques to extend the context length in large language models. International Joint Conference on Artificial Intelligence, 2024. [1113] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. International Conference on Machine Learning, 2024. [1114] Xuezhi Wang, Jason Wei, D. Schuurmans, Quoc Le, Ed H. Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. International Conference on Learning Representations, 2022. [1115] Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, and Yingzhen Yang. Recmind: Large language model powered agent for recommendation, arXiv preprint arXiv:2308.14296, 2024. URL https://arxiv.org/abs/2308. 14296. [1116] Yani Wang. Application of large language models based on knowledge graphs in question-answering systems: review. Applied and Computational Engineering, 2024. [1117] Yanlin Wang, Wanjun Zhong, Yanxian Huang, Ensheng Shi, Min Yang, Jiachi Chen, Hui Li, Yuchi Ma, Qianxiang Wang, and Zibin Zheng. Agents in software engineering: Survey, landscape, and vision, arXiv preprint arXiv:2409.09030, 2024. URL https://arxiv.org/abs/2409.09030v2. [1118] Yaqi Wang and Haipei Xu. Srsa: cost-efficient strategy-router search agent for real-world humanmachine interactions. 2024 IEEE International Conference on Data Mining Workshops (ICDMW), 2024. [1119] Yi Wang, Xinhao Li, Ziang Yan, Yinan He, Jiashuo Yu, Xiangyun Zeng, Chenting Wang, Changlian Ma, Haian Huang, Jianfei Gao, Min Dou, Kaiming Chen, Wenhai Wang, Yu Qiao, Yali Wang, and Limin Wang. Internvideo2.5: Empowering video mllms with long and rich context modeling. arXiv preprint, 2025. [1120] Yiming Wang, Zhuosheng Zhang, and Rui Wang. Element-aware summarization with large language models: Expert-aligned evaluation and chain-of-thought method. Annual Meeting of the Association for Computational Linguistics, 2023. 143 [1121] Yingming Wang and Pepa Atanasova. Self-critique and refinement for faithful natural language explanations, arXiv preprint arXiv:2505.22823, 2025. URL https://arxiv.org/abs/2505. 22823v1. [1122] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph classification. In Proceedings of the Web Conference 2021, pages 36633674, 2021. [1123] Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, and Julian McAuley. Memoryllm: Towards self-updatable large language models, arXiv preprint arXiv:2402.04624, 2024. URL https://arxiv.org/abs/2402. 04624. [1124] Yu Wang, Dmitry Krotov, Yuanzhe Hu, Yifan Gao, Wangchunshu Zhou, Julian McAuley, Dan Gutfreund, Rogério Feris, and Zexue He. M+: Extending memoryllm with scalable long-term memory. arXiv preprint, 2025. [1125] Yubin Wang, Xinyang Jiang, De Cheng, Wenli Sun, Dongsheng Li, and Cairong Zhao. Hpt++: Hierarchically prompting vision-language models with multi-granularity knowledge generation and improved structure modeling. arXiv preprint, 2024. [1126] Yujie Wang, Shiju Wang, Shenhan Zhu, Fangcheng Fu, Xinyi Liu, Xuefeng Xiao, Huixia Li, Jiashi Li, Faming Wu, and Bin Cui. Flexsp: Accelerating large language model training via flexible sequence parallelism. International Conference on Architectural Support for Programming Languages and Operating Systems, 2024. [1127] Yuntao Wang, Yanghe Pan, Zhou Su, Yi Deng, Quan Zhao, L. Du, Tom H. Luan, Jiawen Kang, and D. Niyato. Large model based agents: State-of-the-art, cooperation paradigms, security and privacy, and future trends. IEEE Communications Surveys & Tutorials, 2024. [1128] Yuntao Wang, Shaolong Guo, Yanghe Pan, Zhou Su, Fahao Chen, Tom H. Luan, Peng Li, Jiawen Kang, and Dusit Niyato. Internet of agents: Fundamentals, applications, and challenges, arXiv preprint arXiv:2505.07176, 2025. URL https://arxiv.org/abs/2505.07176v1. [1129] Yuxiang Wang, Xinnan Dai, Wenqi Fan, and Yao Ma. Exploring graph tasks with pure llms: comprehensive benchmark and investigation, arXiv preprint arXiv:2502.18771v1, 2025. URL https://arxiv.org/abs/2502.18771v1. [1130] Z. Wang, King Zhu, Chunpu Xu, Wangchunshu Zhou, Jiaheng Liu, Yibo Zhang, Jiashuo Wang, Ning Shi, Siyu Li, Yizhi Li, Haoran Que, Zhaoxiang Zhang, Yuanxing Zhang, Ge Zhang, Ke Xu, Jie Fu, and Wenhao Huang. Mio: foundation model on multimodal tokens, arXiv preprint arXiv:2409.17692v3, 2024. URL https://arxiv.org/abs/2409.17692v3. [1131] Zheng Wang, Shu Xian Teo, Jieer Ouyang, Yongjun Xu, and Wei Shi. M-rag: Reinforcing large language model performance through retrieval-augmented generation with multiple partitions. Annual Meeting of the Association for Computational Linguistics, 2024. [1132] Zhiruo Wang, Zhoujun Cheng, Hao Zhu, Daniel Fried, and Graham Neubig. What are tools anyway? survey from the language model perspective, arXiv preprint arXiv:2403.15452, 2024. URL https://arxiv.org/abs/2403.15452v1. 144 [1133] Ziyang Wang, Jianzhou You, Haining Wang, Tianwei Yuan, Shichao Lv, Yang Wang, and Limin Sun. Honeygpt: Breaking the trilemma in terminal honeypots with large language model, arXiv preprint arXiv:2406.01882, 2024. URL https://arxiv.org/abs/2406.01882v2. [1134] Ziyue Wang, Chi Chen, Yiqi Zhu, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, and Yang Liu. Browse and concentrate: Comprehending multimodal content via prior-llm context fusion. Annual Meeting of the Association for Computational Linguistics, 2024. [1135] Zora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory, arXiv preprint arXiv:2409.07429, 2024. URL https://arxiv.org/abs/2409.07429. [1136] Irene Weber. Large language models are pattern matchers: Editing semi-structured and structured documents with chatgpt. AKWI Jahrestagung, 2024. [1137] Hui Wei, Chenyue Feng, and Jianning Zhang. Modeling of memory mechanisms in cerebral cortex and simulation of storage performance, arXiv preprint arXiv:2401.00381, 2023. URL https: //arxiv.org/abs/2401.00381v2. [1138] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. Neural Information Processing Systems, 2022. [1139] Jerry W. Wei, Le Hou, Andrew Kyle Lampinen, Xiangning Chen, Da Huang, Yi Tay, Xinyun Chen, Yifeng Lu, Denny Zhou, Tengyu Ma, and Quoc V. Le. Symbol tuning improves in-context learning in language models. Conference on Empirical Methods in Natural Language Processing, 2023. [1140] Shaopeng Wei, Yu Zhao, Xingyan Chen, Qing Li, Fuzhen Zhuang, Ji Liu, and Gang Kou. Graph learning and its advancements on large language models: holistic survey, arXiv preprint arXiv:2212.08966, 2022. URL https://arxiv.org/abs/2212.08966v5. [1141] Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, and Lihong Li. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning. arXiv preprint, 2025. [1142] Zhiyuan Wei, Jing Sun, Zijian Zhang, and Xianhao Zhang. Llm-smartaudit: Advanced smart contract vulnerability detection. arXiv preprint, 2024. [1143] Rebecca Westhäußer, Frederik Berenz, Wolfgang Minker, and Sebastian Zepf. Caim: Development and evaluation of cognitive ai memory framework for long-term interaction with intelligent agents. arXiv preprint, 2025. [1144] Danny Weyns and F. Oquendo. An architectural style for self-adaptive multi-agent systems, arXiv preprint arXiv:1909.03475, 2019. URL https://arxiv.org/abs/1909.03475v1. [1145] Erik Wijmans, Brody Huval, Alexander Hertzberg, V. Koltun, and Philipp Krähenbühl. Cut your losses in large-vocabulary language models. International Conference on Learning Representations, 2024. [1146] Wikipedia contributors. Agent communications language Wikipedia, the free encyclopedia, 2025. URL https://en.wikipedia.org/wiki/Agent_Communications_Language. [Online; accessed 17-July-2025]. [1147] Genta Indra Winata, Lingjue Xie, Karthik Radhakrishnan, Shijie Wu, Xisen Jin, Pengxiang Cheng, Mayank Kulkarni, and Daniel Preotiuc-Pietro. Overcoming catastrophic forgetting in massively multilingual continual learning, arXiv preprint arXiv:2305.16252, 2023. URL https://arxiv. org/abs/2305.16252. [1148] Beong woo Kwak, Minju Kim, Dongha Lim, Hyungjoo Chae, Dongjin Kang, Sunghwan Kim, Dongil Yang, and Jinyoung Yeo. Toolhaystack: Stress-testing tool-augmented language models in realistic long-term interactions, arXiv preprint arXiv:2505.23662, 2025. URL https://arxiv.org/abs/ 2505.23662v1. [1149] Biao Wu, Yanda Li, Meng Fang, Zirui Song, Zhiwei Zhang, Yunchao Wei, and Ling Chen. Foundations and recent trends in multimodal mobile agents: survey, arXiv preprint arXiv:2411.02006, 2024. URL https://arxiv.org/abs/2411.02006v2. [1150] Cheng-Kuang Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, and Hung yi Lee. Streambench: Towards benchmarking continuous improvement of language agents. Neural Information Processing Systems, 2024. [1151] Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webdancer: Towards autonomous information seeking agency, arXiv preprint arXiv:2505.22648, 2025. URL https://arxiv.org/ abs/2505.22648v2. [1152] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal, arXiv preprint arXiv:2501.07572, 2025. URL https://arxiv.org/abs/2501.07572v2. [1153] Junde Wu, Jiayuan Zhu, and Yuyuan Liu. Agentic reasoning: Reasoning llms with tools for the deep research, arXiv preprint arXiv:2502.04644, 2025. URL https://arxiv.org/abs/2502. 04644v1. [1154] Likang Wu, Zhilan Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen. survey on large language models for recommendation. World wide web (Bussum), 2023. [1155] Wu, Yang, Jiang, Li, Yan, and Yu. . . . Vtool-r1: Vlms learn to think with images via reinforcement learning on multimodal tool use. 2025. URL https://arxiv.org/abs/2505. 19255. [1156] Mengsong Wu, Tong Zhu, Han Han, Chuanyuan Tan, Xiang Zhang, and Wenliang Chen. Seal-tools: Self-instruct tool learning dataset for agent tuning and detailed benchmark. Natural Language Processing and Chinese Computing, 2024. [1157] Panlong Wu, Ting Wang, Yifei Zhong, Haoqi Zhang, Zitong Wang, and Fangxin Wang. Deepform: Reasoning large language model for communication system formulation, arXiv preprint arXiv:2506.08551, 2025. URL https://arxiv.org/abs/2506.08551v2. [1158] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, A. Awadallah, Ryen W. White, Doug Burger, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation, arXiv preprint arXiv:2308.08155, 2023. URL https://arxiv.org/abs/2308.08155v2. 146 [1159] Ruofan Wu, Youngwon Lee, Fan Shu, Danmei Xu, Seung won Hwang, Zhewei Yao, Yuxiong He, and Feng Yan. Composerag: modular and composable rag for corpus-grounded multi-hop question answering, arXiv preprint arXiv:2506.00232, 2025. URL https://arxiv.org/abs/2506. 00232v1. [1160] Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, and C. Xue. Retrieval-augmented generation for natural language processing: survey, arXiv preprint arXiv:2407.13193, 2024. URL https://arxiv.org/abs/2407. 13193v3. [1161] Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, V. Ioannidis, Karthik Subbian, J. Leskovec, and James Zou. Avatar: Optimizing llm agents for tool usage via contrastive reasoning. Neural Information Processing Systems, 2024. [1162] Suhang Wu, Minlong Peng, Yue Chen, Jinsong Su, and Mingming Sun. Eva-kellm: new benchmark for evaluating knowledge editing of llms, arXiv preprint arXiv:2308.09954, 2023. URL https: //arxiv.org/abs/2308.09954. [1163] Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint, 2024. [1164] Tong Wu, Chong Xiang, Jiachen T. Wang, and Prateek Mittal. Effectively controlling reasoning models through thinking intervention, arXiv preprint arXiv:2503.24370, 2025. URL https:// arxiv.org/abs/2503.24370v3. [1165] Xinbo Wu and L. Varshney. meta-learning perspective on transformers for causal language modeling. Annual Meeting of the Association for Computational Linguistics, 2023. [1166] Xue Wu and Kostas Tsioutsiouliklis. Thinking with knowledge graphs: Enhancing llm reasoning through structured data, arXiv preprint arXiv:2412.10654, 2024. URL https://arxiv.org/ abs/2412.10654v1. [1167] Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang, and Yong Liu. From human memory to ai memory: survey on memory mechanisms in the era of llms, arXiv preprint arXiv:2504.15965, 2025. URL https://arxiv.org/abs/2504.15965v2. [1168] Zengqing Wu and Takayuki Ito. The hidden strength of disagreement: Unraveling the consensusdiversity tradeoff in adaptive multi-agent systems, arXiv preprint arXiv:2502.16565, 2025. URL https://arxiv.org/abs/2502.16565v2. [1169] Zihao Wu, Lu Zhang, Chao-Yang Cao, Xiao-Xing Yu, Haixing Dai, Chong-Yi Ma, Zheng Liu, Lin Zhao, Gang Li, Wei Liu, Quanzheng Li, Dinggang Shen, Xiang Li, Dajiang Zhu, and Tianming Liu. Exploring the trade-offs: Unified large language models vs local fine-tuned models for highly-specific radiology nli task. IEEE Transactions on Big Data, 2023. [1170] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Qin Liu, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model based agents: survey, arXiv preprint arXiv:2309.07864, 2023. URL https://arxiv.org/abs/2309.07864v3. 147 [1171] Menglin Xia, Victor Ruehle, Saravan Rajmohan, and Reza Shokri. Minerva: programmable memory test benchmark for language models, arXiv preprint arXiv:2502.03358, 2025. URL https: //arxiv.org/abs/2502.03358v2. [1172] Yuchen Xia, Manthan Shenoy, N. Jazdi, and M. Weyrich. Towards autonomous system: flexible modular production system enhanced with large language model agents. IEEE International Conference on Emerging Technologies and Factory Automation, 2023. [1173] Yutong Xia, Ao Qu, Yunhan Zheng, Yihong Tang, Dingyi Zhuang, Yuxuan Liang, Cathy Wu, Roger Zimmermann, and Jinhua Zhao. Reimagining urban science: Scaling causal inference with large language models, arXiv preprint arXiv:2504.12345v3, 2025. URL https://arxiv.org/abs/ 2504.12345v3. [1174] Zhishang Xiang, Chuanjie Wu, Qinggang Zhang, Shengyuan Chen, Zijin Hong, Xiao Huang, and Jinsong Su. When to use graphs in rag: comprehensive analysis for graph retrieval-augmented generation, arXiv preprint arXiv:2506.05690, 2025. URL https://arxiv.org/abs/2506.05690v1. [1175] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. Neural Information Processing Systems, 2024. [1176] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. International Conference on Learning Representations, 2023. [1177] MiniCPM Team Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengda Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yu-Xuan Han, Bingxiang He, Yuxian Huang, Cunliang Kong, Qiu-Tong Li, Siyuan Li, Wenhao Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Ya-Ting Lu, Pei Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Zekai Qu, Qundong Shi, Zijun Song, Jiayu Su, Zhou Su, Ao Sun, Xiang ping Sun, Peijun Tang, Fang-Ming Wang, Feng Wang, Shuo Wang, Yudong Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zi-Kang Xie, Yukun Yan, Jia-Li Yuan, Kai Zhang, Lei Zhang, Linyu Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhijun Zheng, Ge Zhou, Jie Zhou, Wei Zhou, Zihan Zhou, Zi-An Zhou, Zhiyuan Liu, Guoyang Zeng, Chaochao Jia, Dahai Li, and Maosong Sun. Minicpm4: Ultra-efficient llms on end devices, arXiv preprint arXiv:2506.07900, 2025. URL https://arxiv.org/abs/2506.07900v1. [1178] Yang Xiao, Jiashuo Wang, Ruifeng Yuan, Chunpu Xu, Kaishuai Xu, Wenjie Li, and Pengfei Liu. Limopro: Reasoning refinement for efficient and effective test-time scaling, arXiv preprint arXiv:2505.19187, 2025. URL https://arxiv.org/abs/2505.19187v1. [1179] Yilin Xiao, Chuang Zhou, Qinggang Zhang, Bo Li, Qing Li, and Xiao Huang. Reliable reasoning path: Distilling effective guidance for llm reasoning with knowledge graphs, arXiv preprint arXiv:2506.10508, 2025. URL https://arxiv.org/abs/2506.10508v1. [1180] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and Yu Su. Travelplanner: benchmark for real-world planning with language agents. International Conference on Machine Learning, 2024. [1181] Yuxi Xie, Anirudh Goyal, Xiaobao Wu, Xunjian Yin, Xiao Xu, Min-Yen Kan, Liangming Pan, and William Yang Wang. Coral: Order-agnostic language modeling for efficient iterative refinement, arXiv preprint arXiv:2410.09675, 2024. URL https://arxiv.org/abs/2410.09675v1. 148 [1182] Yue Xing, Tao Yang, Yijiashun Qi, Minggu Wei, Yu Cheng, and Honghui Xin. Structured memory mechanisms for stable context representation in large language models, arXiv preprint arXiv:2505.22921, 2025. URL https://arxiv.org/abs/2505.22921v1. [1183] Guangzhi Xiong, Qiao Jin, Xiao Wang, Yin Fang, Haolin Liu, Yifan Yang, Fangyuan Chen, Zhixing Song, Dengyu Wang, Minjia Zhang, Zhiyong Lu, and Aidong Zhang. Rag-gym: Systematic optimization of language agents for retrieval-augmented generation. arXiv preprint, 2025. [1184] Haoyi Xiong, Zhiyuan Wang, Xuhong Li, Jiang Bian, Zeke Xie, Shahid Mumtaz, and Laura E. Barnes. Converging paradigms: The synergy of symbolic and connectionist ai in llm-empowered autonomous agents, arXiv preprint arXiv:2407.08516, 2024. URL https://arxiv.org/abs/ 2407.08516v5. [1185] Junjie Xiong, Changjia Zhu, Shuhang Lin, Chong Zhang, Yongfeng Zhang, Yao Liu, and Lingyao Li. Invisible prompts, visible threats: Malicious font injection in external resources for large language models, arXiv preprint arXiv:2505.16957, 2025. URL https://arxiv.org/abs/2505.16957v1. [1186] Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Zhecheng Li, and Yiwei Wang. Enhancing llm character-level manipulation via divide and conquer. 2025. [1187] Zhen Xiong, Yujun Cai, Zhecheng Li, and Yiwei Wang. Mapping the minds of llms: graph-based analysis of reasoning llm. 2025. [1188] Zhen Xiong, Yujun Cai, Zhecheng Li, and Yiwei Wang. Unveiling the potential of diffusion large language model in controllable generation. 2025. [1189] Zidi Xiong, Yuping Lin, Wenya Xie, Pengfei He, Jiliang Tang, Himabindu Lakkaraju, and Zhen Xiang. How memory management impacts llm agents: An empirical study of experience-following behavior, arXiv preprint arXiv:2505.16067, 2025. URL https://arxiv.org/abs/2505.16067v1. [1190] Chunmei Xu, Shengheng Liu, Cheng Zhang, Yongming Huang, Zhaohua Lu, and Luxi Yang. Multiagent reinforcement learning based distributed transmission in collaborative cloud-edge systems. IEEE Transactions on Vehicular Technology, 2021. [1191] Haotian Xu, Xing Wu, Weinong Wang, Zhongzhi Li, Da Zheng, Boyuan Chen, Yi Hu, Shijia Kang, Jiaming Ji, Yingying Zhang, et al. Redstar: Does scaling long-cot data unlock better slow-reasoning systems? 2025. [1192] Hongshen Xu, Su Zhu, Zihan Wang, Hang Zheng, Da Ma, Ruisheng Cao, Shuai Fan, Lu Chen, and Kai Yu. Reducing tool hallucination via reliability alignment, arXiv preprint arXiv:2412.04141, 2024. URL https://arxiv.org/abs/2412.04141v3. [1193] Hu Xu, Gargi Ghosh, Po-Yao (Bernie) Huang, Dmytro Okhonko, Armen Aghajanyan, and Florian Metze Luke Zettlemoyer Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. Conference on Empirical Methods in Natural Language Processing, 2021. [1194] Mengjia Xu. Understanding graph embedding methods and their applications. SIAM Review, 2020. [1195] Minrui Xu, Hongyang Du, Dusist Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, A. Jamalipour, Dong In Kim, X. Shen, Victor C. M. Leung, and H. Poor. Unleashing the power of edge-cloud generative ai in mobile networks: survey of aigc services. IEEE Communications Surveys and Tutorials, 2023. 149 [1196] Minrui Xu, D. Niyato, and Christopher G. Brinton. Serving long-context llms at the mobile edge: Test-time reinforcement learning-based model caching and inference offloading, arXiv preprint arXiv:2501.14205, 2025. URL https://arxiv.org/abs/2501.14205v1. [1197] Nan Xu, Fei Wang, Sheng Zhang, Hoifung Poon, and Muhao Chen. From introspection to best practices: Principled analysis of demonstrations in multimodal in-context learning. North American Chapter of the Association for Computational Linguistics, 2024. [1198] Shuhang Xu and Fangwei Zhong. Comet: Metaphor-driven covert communication for multi-agent language games, arXiv preprint arXiv:2505.18218, 2025. URL https://arxiv.org/abs/2505. 18218v1. [1199] Tianyang Xu, Haojie Zheng, Chengze Li, Haoxiang Chen, Yixin Liu, Ruoxi Chen, and Lichao Sun. Noderag: Structuring graph-based rag with heterogeneous nodes, arXiv preprint arXiv:2504.11544, 2025. URL https://arxiv.org/abs/2504.11544v1. [1200] Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and W. Wang. Pride and prejudice: Llm amplifies self-bias in self-refinement. Annual Meeting of the Association for Computational Linguistics, 2024. [1201] Wenrui Xu and Keshab K. Parhi. survey of attacks on large language models, arXiv preprint arXiv:2505.12567, 2025. URL https://arxiv.org/abs/2505.12567v1. [1202] Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-mem: Agentic memory for llm agents. arXiv preprint, 2025. [1203] Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, and Yongfeng Zhang. A-mem: Agentic memory for llm agents, arXiv preprint arXiv:2502.12110, 2025. URL https://arxiv.org/abs/ 2502.12110. [1204] Yifei Xu, Jingqiao Zhang, Ru He, Liangzhu Ge, Chao Yang, Cheng Yang, and Ying Wu. Sas: Selfaugmentation strategy for language model pre-training. AAAI Conference on Artificial Intelligence, 2021. [1205] Zhe Xu, Daoyuan Chen, Zhenqing Ling, Yaliang Li, and Ying Shen. Mindgym: What matters in question synthesis for thinking-centric fine-tuning?, arXiv preprint arXiv:2503.09499, 2025. URL https://arxiv.org/abs/2503.09499v2. [1206] Zhentao Xu, Mark Jerome Cruz, Matthew Guevara, Tie Wang, Manasi Deshpande, Xiaofeng Wang, and Zheng Li. Retrieval-augmented generation with knowledge graphs for customer service question answering. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024. [1207] Eric Xue, Ke Chen, Zeyi Huang, Yuyang Ji, Yong Jae Lee, and Haohan Wang. Improve: Iterative model pipeline refinement and optimization leveraging llm experts, arXiv preprint arXiv:2502.18530, 2025. URL https://arxiv.org/abs/2502.18530v2. [1208] Huiyin Xue and Nikolaos Aletras. Pit one against many: Leveraging attention-head embeddings for parameter-efficient multi-head attention. Conference on Empirical Methods in Natural Language Processing, 2023. 150 [1209] Xiangyuan Xue, Zeyu Lu, Di Huang, Zidong Wang, Wanli Ouyang, and Lei Bai. Comfybench: Benchmarking llm-based agents in comfyui for autonomously designing collaborative ai systems, arXiv preprint arXiv:2409.01392, 2024. URL https://arxiv.org/abs/2409.01392v2. [1210] Bingyu Yan, Xiaoming Zhang, Litian Zhang, Lian Zhang, Ziyi Zhou, Dezhuang Miao, and Chaozhuo Li. Beyond self-talk: communication-centric survey of llm-based multi-agent systems. arXiv preprint, 2025. [1211] Tianqiang Yan and Tiansheng Xu. Refining the responses of llms by themselves, arXiv preprint arXiv:2305.04039, 2023. URL https://arxiv.org/abs/2305.04039v1. [1212] Xu Yan, Junliang Du, Lun Wang, Yingbin Liang, Jiacheng Hu, and Bingxing Wang. The synergistic role of deep learning and neural architecture search in advancing artificial intelligence. 2024 International Conference on Electronics and Devices, Computational Science (ICEDCS), 2024. [1213] Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, Haodong Chen, Qingsong Wen, Roger Zimmermann, and Yuxuan Liang. Urbanclip: Learning text-enhanced urban region profiling with contrastive language-image pretraining from the web. The Web Conference, 2023. [1214] Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian Shao, and Yueting Zhuang. Inftythink: Breaking the length limits of long-context reasoning in large language models, arXiv preprint arXiv:2503.06692, 2025. URL https://arxiv.org/abs/2503.06692v3. [1215] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. Large language models as optimizers. International Conference on Learning Representations, 2023. [1216] Hongkang Yang, Zehao Lin, Wenjin Wang, Hao Wu, Zhiyu Li, Bo Tang, Wenqiang Wei, Jinbo Wang, Zeyun Tang, Shichao Song, Chenyang Xi, Yu Yu, Kai Chen, Feiyu Xiong, Linpeng Tang, and E. Weinan. Memory3: Language modeling with explicit memory. Journal of Machine Learning, 2024. [1217] Jianxin Yang. Longqlora: Efficient and effective method to extend context length of large language models, arXiv preprint arXiv:2311.04879, 2023. URL https://arxiv.org/abs/2311. 04879v2. [1218] Jinghan Yang, Shuming Ma, and Furu Wei. Auto-icl: In-context learning without human supervision. arXiv preprint, 2023. [1219] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Octopus: Embodied vision-language programmer from environmental feedback. European Conference on Computer Vision, 2023. [1220] John Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Adriano Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Neural Information Processing Systems, 2024. [1221] Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian, Sanjay Agrawal, Amit Singh, Guangzhong Sun, and Xing Xie. Graphformers: Gnn-nested transformers for representation learning on textual graph. Neural Information Processing Systems, 2021. [1222] Ke Yang, Yao Liu, Sapana Chaudhary, Rasool Fakoor, Pratik Chaudhari, George Karypis, and Huzefa Rangwala. Agentoccam: simple yet strong baseline for llm-based web agents. 2024. 151 [1223] Lin F. Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. Give us the facts: Enhancing large language models with knowledge graphs for fact-aware language modeling. IEEE Transactions on Knowledge and Data Engineering, 2023. [1224] Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models, arXiv preprint arXiv:2505.15809v1, 2025. URL https://arxiv.org/abs/2505.15809v1. [1225] Yang, Song, Li, Zhao, and Ge. . . . Gpt4tools: Teaching large language model to use tools via self-instruction. 2023. URL https://proceedings.neurips.cc/paper_files/ paper/2023/hash/e393677793767624f2821cec8bdd02f1-Abstract-Conference. html?utm_campaign=Artificial%2BIntelligence%2BWeekly&utm_medium=email& utm_source=Artificial_Intelligence_Weekly_411. [1226] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language model to use tools via self-instruction. Neural Information Processing Systems, 2023. [1227] Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, and Song Han. Lserve: Efficient long-sequence llm serving with unified sparse attention, arXiv preprint arXiv:2502.14866, 2025. URL https://arxiv.org/abs/2502. 14866v2. [1228] Shanglong Yang, Zhipeng Yuan, Shunbao Li, Ruoling Peng, Kang Liu, and Po Yang. Gpt-4 as evaluator: Evaluating large language models on pest management in agriculture. arXiv preprint, 2024. [1229] Wang Yang, Zirui Liu, Hongye Jin, Qingyu Yin, Vipin Chaudhary, and Xiaotian Han. Longer context, deeper thinking: Uncovering the role of long-context ability in reasoning, arXiv preprint arXiv:2505.17315, 2025. URL https://arxiv.org/abs/2505.17315v1. [1230] Wen Yang, Kai Fan, and Minpeng Liao. Markov chain of thought for efficient mathematical reasoning. North American Chapter of the Association for Computational Linguistics, 2024. [1231] Yaodong Yang, Chengdong Ma, Zihan Ding, S. McAleer, Chi Jin, and Jun Wang. Game-theoretic multiagent reinforcement learning, arXiv preprint arXiv:2011.00583, 2020. URL https://arxiv. org/abs/2011.00583v4. [1232] Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, and Qi Liu. Unleashing the potential of large language models for predictive tabular tasks in data science, arXiv preprint arXiv:2403.20208, 2024. URL https://arxiv.org/abs/2403.20208v7. [1233] Yi Yang, Yixuan Tang, and Kar Yan Tam. Investlm: large language model for investment using financial domain instruction tuning, arXiv preprint arXiv:2309.13064, 2023. URL https://arxiv. org/abs/2309.13064. [1234] Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. G-daug: Generative data augmentation for commonsense reasoning. Findings, 2020. [1235] Yingxuan Yang, Huacan Chai, Yuanyi Song, Siyuan Qi, Muning Wen, Ning Li, Junwei Liao, Haoyi Hu, Jianghao Lin, Gaowei Chang, Weiwen Liu, Ying Wen, Yong Yu, and Weinan Zhang. survey of ai agent protocols, arXiv preprint arXiv:2504.16736, 2025. URL https://arxiv.org/abs/ 2504.16736v3. 152 [1236] Yuan Yang, Siheng Xiong, Ehsan Shareghi, and F. Fekri. The compressor-retriever architecture for language model os, arXiv preprint arXiv:2409.01495, 2024. URL https://arxiv.org/abs/ 2409.01495v1. [1237] Yuxin Yang, Haoyang Wu, Tao Wang, Jia Yang, Hao Ma, and Guojie Luo. Pseudo-knowledge graph: Meta-path guided retrieval and in-graph text for rag-equipped llm, arXiv preprint arXiv:2503.00309, 2025. URL https://arxiv.org/abs/2503.00309v1. [1238] Zhen Yang, Fang Liu, Zhongxing Yu, J. Keung, Jia Li, Shuo Liu, Yifan Hong, Xiaoxue Ma, Zhi Jin, and Ge Li. Exploring and unleashing the power of large language models in automated code translation. Proc. ACM Softw. Eng., 2024. [1239] Chengyuan Yao and Satoshi Fujita. Adaptive control of retrieval-augmented generation for large language models through reflective tags. Electronics, 2024. [1240] Huaiyuan Yao, Longchao Da, Vishnu Nandam, J. Turnau, Zhiwei Liu, Linsey Pang, and Hua Wei. Comal: Collaborative multi-agent large language models for mixed-autonomy traffic, arXiv preprint arXiv:2410.14368, 2024. URL https://arxiv.org/abs/2410.14368v2. [1241] Jiayu Yao, Shenghua Liu, Yiwei Wang, Lingrui Mei, Baolong Bi, Yuyao Ge, Zhecheng Li, and Xueqi Cheng. Who is in the spotlight: The hidden bias undermining multimodal retrieval-augmented generation. 2025. [1242] Jinghan Yao, Sam Ade Jacobs, Masahiro Tanaka, Olatunji Ruwase, A. Shafi, H. Subramoni, and Dhabaleswar K. Panda. Training ultra long context language model with fully pipelined distributed transformer, arXiv preprint arXiv:2408.16978, 2024. URL https://arxiv.org/abs/2408. 16978v2. [1243] Liang Yao, Chengsheng Mao, and Yuan Luo. Graph convolutional networks for text classification. AAAI Conference on Artificial Intelligence, 2018. [1244] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Neural Information Processing Systems, 2022. [1245] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. International Conference on Learning Representations, 2022. [1246] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, T. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Neural Information Processing Systems, 2023. [1247] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, arXiv preprint arXiv:2210.03629, 2023. URL https://arxiv.org/abs/2210.03629. [1248] Shunyu Yao, Noah Shinn, P. Razavi, and Karthik Narasimhan. τ-bench: benchmark for tool-agentuser interaction in real-world domains. arXiv preprint, 2024. 153 [1249] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, and Silvio Savarese. Retroformer: Retrospective large language agents with policy gradient optimization, arXiv preprint arXiv:2308.02151, 2024. URL https://arxiv.org/abs/2308.02151. [1250] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and J. Leskovec. Qa-gnn: Reasoning with language models and knowledge graphs for question answering. North American Chapter of the Association for Computational Linguistics, 2021. [1251] Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D. Manning, Percy Liang, and J. Leskovec. Deep bidirectional language-knowledge graph pretraining. Neural Information Processing Systems, 2022. [1252] Fahd Yazin, Moumita Das, A. Banerjee, and Dipanjan Roy. Contextual prediction errors reorganize naturalistic episodic memories in time. Scientific Reports, 2021. [1253] Ye, Li, Gao, Huang, Wu, Li, and Fan. . . . Tooleyes: Fine-grained evaluation for tool learning capabilities of large language models in real-world scenarios. 2024. URL https: //arxiv.org/abs/2401.00741. [1254] Ye, Li, Li, Huang, Gao, and Wu. . . . Toolsword: Unveiling safety issues of large language models in tool learning across three stages. 2024. URL https://arxiv.org/abs/2402.10753. [1255] Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, and Jiechao Chen. Toolhop: query-driven benchmark for evaluating large language models in multi-hop tool use, arXiv preprint arXiv:2501.02506, 2025. URL https://arxiv.org/abs/2501.02506v4. [1256] Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. Prompt engineering prompt engineer. Annual Meeting of the Association for Computational Linguistics, 2023. [1257] Sixiang Ye, Zeyu Sun, Guoqing Wang, Liwei Guo, Qing-Lin Liang, Zheng Li, and Yong Liu. Prompt alchemy: Automatic prompt refinement for enhancing code generation, arXiv preprint arXiv:2503.11085, 2025. URL https://arxiv.org/abs/2503.11085v1. [1258] Zhifan Ye, Kejing Xia, Yonggan Fu, Xin Dong, Jihoon Hong, Xiangchi Yuan, Shizhe Diao, Jan Kautz, Pavlo Molchanov, and Y. Lin. Longmamba: Enhancing mambas long-context capabilities via training-free receptive field enlargement. International Conference on Learning Representations, 2025. [1259] Asaf Yehudai, Lilach Eden, Alan Li, Guy Uziel, Yilun Zhao, Roy Bar-Haim, Arman Cohan, and Michal Shmueli-Scheuer. Survey on evaluation of llm-based agents, arXiv preprint arXiv:2503.16416, 2025. URL https://arxiv.org/abs/2503.16416v1. [1260] Peiling Yi and Yuhan Xia. Irony detection, reasoning and understanding in zero-shot learning. IEEE Transactions on Artificial Intelligence, 2025. [1261] Da Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Raghavi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. Agent lumos: Unified and modular training for open-source language agents. Annual Meeting of the Association for Computational Linguistics, 2023. 154 [1262] Fan Yin, Zifeng Wang, I-Hung Hsu, Jun Yan, Ke Jiang, Yanfei Chen, Jindong Gu, Long T. Le, Kai-Wei Chang, Chen-Yu Lee, Hamid Palangi, and Tomas Pfister. Magnet: Multi-turn tool-use data synthesis and distillation via graph translation, arXiv preprint arXiv:2503.07826, 2025. URL https://arxiv.org/abs/2503.07826v1. [1263] Guoli Yin, Haoping Bai, Shuang Ma, Feng Nan, Yanchao Sun, Zhaoyang Xu, Shen Ma, Jiarui Lu, Xiang Kong, Aonan Zhang, Dian Ang Yap, Yizhe Zhang, K. Ahnert, Vik Kamath, Mathias Berglund, Dominic Walsh, Tobias Gindele, Juergen Wiest, Zhengfeng Lai, Xiaoming Wang, Jiulong Shan, Meng Cao, Ruoming Pang, and Zirui Wang. Mmau: holistic benchmark of agent capabilities across diverse domains. North American Chapter of the Association for Computational Linguistics, 2024. [1264] Pengcheng Yin, Graham Neubig, Wen tau Yih, and Sebastian Riedel. Tabert: Pretraining for joint understanding of textual and tabular data. Annual Meeting of the Association for Computational Linguistics, 2020. [1265] Yin, You, Ji, Zhong, and Bai. Mumath-code: Combining tool-use large language models with multi-perspective data augmentation for mathematical reasoning. 2024. URL https:// arxiv.org/abs/2405.07551. [1266] Gunwoo Yong, Kahyun Jeon, Daeyoung Gil, and Ghang Lee. Prompt engineering for zero-shot and few-shot defect detection and classification using visual-language pretrained model. Comput. Aided Civ. Infrastructure Eng., 2022. [1267] Aspen H. Yoo and A. Collins. How working memory and reinforcement learning are intertwined: cognitive, neural, and computational perspective. Journal of Cognitive Neuroscience, 2021. [1268] Chanwoong Yoon, Taewhoo Lee, Hyeon Hwang, Minbyul Jeong, and Jaewoo Kang. Compact: Compressing retrieved documents actively for question answering. Conference on Empirical Methods in Natural Language Processing, 2024. [1269] Jiaxuan You, Mingjie Liu, Shrimai Prabhumoye, M. Patwary, M. Shoeybi, and Bryan Catanzaro. Llm-evolve: Evaluation for llms evolving capability on benchmarks. Conference on Empirical Methods in Natural Language Processing, 2024. [1270] Yuxin You, Zhen Liu, Xiangchao Wen, Yongtao Zhang, and Wei Ai. Large language models meet graph neural networks: perspective of graph mining. Mathematics, 2024. [1271] Dian Yu, Yuheng Zhang, Jiahao Xu, Tian Liang, Linfeng Song, Zhaopeng Tu, Haitao Mi, and Dong Yu. Teaching llms to refine with tools, arXiv preprint arXiv:2412.16871, 2024. URL https: //arxiv.org/abs/2412.16871v1. [1272] Miao Yu, Fanci Meng, Xinyun Zhou, Shilong Wang, Junyuan Mao, Linsey Pang, Tianlong Chen, Kun Wang, Xinfeng Li, Yongfeng Zhang, Bo An, and Qingsong Wen. survey on trustworthy llm agents: Threats and countermeasures, arXiv preprint arXiv:2503.09648, 2025. URL https: //arxiv.org/abs/2503.09648v1. [1273] Ye Yu, Yaoning Yu, and Haohan Wang. Premise: Scalable and strategic prompt optimization for efficient mathematical reasoning in large models, arXiv preprint arXiv:2506.10716, 2025. URL https://arxiv.org/abs/2506.10716v1. 155 [1274] Zeping Yu and Sophia Ananiadou. Understanding multimodal llms: the mechanistic interpretability of llava in visual question answering, arXiv preprint arXiv:2411.10950v2, 2024. URL https: //arxiv.org/abs/2411.10950v2. [1275] Zishun Yu, Tengyu Xu, Di Jin, Karthik Abinav Sankararaman, Yun He, Wenxuan Zhou, Zhouhao Zeng, Eryk Helenowski, Chen Zhu, Si-Yuan Wang, Hao Ma, and Han Fang. Think smarter not harder: Adaptive reasoning with inference aware optimization, arXiv preprint arXiv:2501.17974, 2025. URL https://arxiv.org/abs/2501.17974v2. [1276] Zhao yu Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, and Yu Cheng. Openthinkimg: Learning to think with images via visual tool reinforcement learning, arXiv preprint arXiv:2505.08617, 2025. URL https://arxiv. org/abs/2505.08617v1. [1277] Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-r: Training language model agents to reflect via iterative self-training. arXiv preprint, 2025. [1278] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. International Conference on Machine Learning, 2024. [1279] Xiaowei Yuan, Zhao Yang, Ziyang Huang, Yequan Wang, Siqi Fan, Yiming Ju, Jun Zhao, and Kang Liu. Exploiting contextual knowledge in llms through v-usable information based layer enhancement. arXiv preprint, 2025. [1280] Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, and Bo Li. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning, arXiv preprint arXiv:2505.12370, 2025. URL https: //arxiv.org/abs/2505.12370v2. [1281] Murong Yue. survey of large language model agents for question answering, arXiv preprint arXiv:2503.19213, 2025. URL https://arxiv.org/abs/2503.19213v1. [1282] Xihang Yue, Linchao Zhu, and Yi Yang. Fragrel: Exploiting fragment-level relations in the external memory of large language models. Annual Meeting of the Association for Computational Linguistics, 2024. [1283] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim. Graph transformer networks. Neural Information Processing Systems, 2019. [1284] Ge Yuyao, Cheng Yiting, Wang Jia, Zhou Hanlin, and Chen Lizhe. Vision transformer based on knowledge distillation in tcm image classification. In 2022 IEEE 5th International Conference on Computer and Communication Engineering Technology (CCET), pages 120125. IEEE, 2022. [1285] M. Zaheer, Guru Guruganesh, Kumar Avinava Dubey, J. Ainslie, Chris Alberti, Santiago Ontañón, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers for longer sequences. Neural Information Processing Systems, 2020. [1286] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models. International Journal of Computer Vision, 2023. [1287] E. Zelikman, Yuhuai Wu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning, arXiv preprint arXiv:2203.14465, 2022. URL https://arxiv.org/abs/2203.14465v2. [1288] E. Zelikman, Eliana Lorch, Lester Mackey, and A. Kalai. Self-taught optimizer (stop): Recursively self-improving code generation, arXiv preprint arXiv:2310.02304, 2023. URL https://arxiv. org/abs/2310.02304v3. [1289] Pai Zeng, Zhenyu Ning, Jieru Zhao, Weihao Cui, Mengwei Xu, Liwei Guo, XuSheng Chen, and Yizhou Shan. The cap principle for llm serving: survey of long-context large language model serving, arXiv preprint arXiv:2405.11299, 2024. URL https://arxiv.org/abs/2405.11299v2. [1290] Ruihong Zeng, Jinyuan Fang, Siwei Liu, and Zaiqiao Meng. On the structural memory of llm agents, arXiv preprint arXiv:2412.15266, 2024. URL https://arxiv.org/abs/2412.15266v1. [1291] Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, Xu Huang, Bing Qin, and Ting Liu. itool: Reinforced fine-tuning with dynamic deficiency calibration for advanced tool use, arXiv preprint arXiv:2501.09766, 2025. URL https://arxiv.org/abs/2501.09766v4. [1292] Yongcheng Zeng, Xinyu Cui, Xuanfa Jin, Guoqing Liu, Zexu Sun, Dong Li, Ning Yang, Jianye Hao, Haifeng Zhang, and Jun Wang. Evolving llms self-refinement capability via iterative preference optimization, arXiv preprint arXiv:2502.05605, 2025. URL https://arxiv.org/abs/2502. 05605v3. [1293] An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua. On generative agents in recommendation. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2023. [1294] Zhang, Zhou, Wei, and Zhao. . . . Evaluating and improving tool-augmented computationintensive math reasoning. 2023. URL https://proceedings.neurips.cc/paper_files/ paper/2023/hash/4a47dd69242d5af908cdd5d51c971cbf-Abstract-Datasets_and_ Benchmarks.html. [1295] Chao Zhang, Haoxin Zhang, Shiwei Wu, Di Wu, Tong Xu, Xiangyu Zhao, Yan Gao, Yao Hu, and Enhong Chen. Notellm-2: Multimodal large representation models for recommendation. Knowledge Discovery and Data Mining, 2024. [1296] Chaoyun Zhang, He Huang, Chiming Ni, Jian Mu, Si Qin, Shilin He, Lu Wang, Fangkai Yang, Pu Zhao, Chao Du, et al. Ufo2: The desktop agentos. arXiv preprint arXiv:2504.14603, 2025. [1297] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen. Deepemd: Few-shot image classification with differentiable earth movers distance and structured classifiers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1220312213, 2020. [1298] Dan Zhang, G. Feng, Yang Shi, and D. Srinivasan. Physical safety and cyber security analysis of multi-agent systems: survey of recent advances. IEEE/CAA Journal of Automatica Sinica, 2021. [1299] Danyang Zhang, Lu Chen, Situo Zhang, Hongshen Xu, Zihan Zhao, and Kai Yu. Large language models are semi-parametric reinforcement learning agents. Neural Information Processing Systems, 2023. [1300] Daoan Zhang, Weitong Zhang, Bing He, Jiang Zhang, Chenchen Qin, and Jianhua Yao. Dnagpt: generalized pre-trained tool for multiple dna sequence analysis tasks. bioRxiv, 2024. [1301] Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. MmIn Findings of the Association for llms: Recent advances in multimodal large language models. Computational Linguistics ACL 2024, pages 1240112430, 2024. [1302] Duzhen Zhang, Yong Ren, Zhong-Zhi Li, Yahan Yu, Jiahua Dong, Chenxing Li, Zhilong Ji, and Jinfeng Bai. Enhancing multimodal continual instruction tuning with branchlora. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025. [1303] Han Zhang, Langshi Zhou, and Hanfang Yang. Learning to retrieve and reason on knowledge graph through active self-reflection, arXiv preprint arXiv:2502.14932, 2025. URL https://arxiv.org/ abs/2502.14932v1. [1304] Hengyu Zhang. Sinklora: Enhanced efficiency and chat capabilities for long-context large language models, arXiv preprint arXiv:2406.05678, 2024. URL https://arxiv.org/abs/2406. 05678v1. [1305] Jiaxin Zhang, Zhongzhi Li, Mingliang Zhang, Fei Yin, Chenglin Liu, and Yashar Moshfeghi. Geoeval: benchmark for evaluating llms and multi-modal models on geometry problem-solving. 2024. [1306] Jing Zhang, Xiaokang Zhang, Jifan Yu, Jian Tang, Jie Tang, Cuiping Li, and Hong Chen. Subgraph retrieval enhanced model for multi-hop knowledge base question answering. Annual Meeting of the Association for Computational Linguistics, 2022. [1307] Kai Zhang, Fubang Zhao, Yangyang Kang, and Xiaozhong Liu. Llm-based medical assistant personalization with shortand long-term memory coordination. North American Chapter of the Association for Computational Linguistics, 2023. [1308] Kai Zhang, Yejin Kim, and Xiaozhong Liu. Personalized llm response generation with parameterized memory injection, arXiv preprint arXiv:2404.03565, 2025. URL https://arxiv.org/abs/2404. 03565. [1309] Kechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. Self-edit: Fault-aware code editor for code generation. Annual Meeting of the Association for Computational Linguistics, 2023. [1310] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. Annual Meeting of the Association for Computational Linguistics, 2024. [1311] Kechi Zhang, Ge Li, Jia Li, Huangzhao Zhang, Jingjing Xu, Hao Zhu, Lecheng Wang, Yihong Dong, Jing Mai, Bin Gu, and Zhi Jin. Computational thinking reasoning in large language models, arXiv preprint arXiv:2506.02658, 2025. URL https://arxiv.org/abs/2506.02658v2. [1312] Ming-Liang Zhang, Zhong-Zhi Li, Fei Yin, Liang Lin, and Cheng-Lin Liu. Fuse, reason and verify: Geometry problem solving with parsed clauses from diagram. 2024. [1313] Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Irwin King, Xue Liu, and Chen Ma. survey on test-time scaling in large language models: What, how, where, and how well?, arXiv preprint arXiv:2503.24235, 2025. URL https://arxiv.org/abs/ 2503.24235v3. [1314] Ruichen Zhang, Mufan Qiu, Zhen Tan, Mohan Zhang, Vincent Lu, Jie Peng, Kaidi Xu, Leandro Z. Agudelo, Peter Qian, and Tianlong Chen. Symbiotic cooperation for web agents: Harnessing complementary strengths of large and small llms, arXiv preprint arXiv:2502.07942, 2025. URL https://arxiv.org/abs/2502.07942v2. [1315] Tengchao Zhang, Yonglin Tian, Fei Lin, Jun Huang, Patrik P. Süli, Rui Qin, and Fei-Yue Wang. Coordfield: Coordination field for agentic uav task allocation in low-altitude urban scenarios, arXiv preprint arXiv:2505.00091, 2025. URL https://arxiv.org/abs/2505.00091v3. [1316] Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo Liu, Philip S. Yu, and Xian Li. Personaagent: When large language model agents meet personalization at test time, arXiv preprint arXiv:2506.06254, 2025. URL https://arxiv.org/abs/2506. 06254v1. [1317] Wen Zhang, Long Jin, Yushan Zhu, Jiaoyan Chen, Zhiwei Huang, Junjie Wang, Yin Hua, Lei Liang, and Hua zeng Chen. Trustuqa: trustful framework for unified structured data question answering. AAAI Conference on Artificial Intelligence, 2024. [1318] Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaochen Du, Huifeng Guo, Ruiming Tang, and Xiangyu Zhao. Process vs. outcome reward: Which is better for agentic rag reinforcement learning, arXiv preprint arXiv:2505.14069, 2025. URL https://arxiv.org/abs/2505.14069v2. [1319] Wentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: hierarchical multi-agent framework for general-purpose task solving, arXiv preprint arXiv:2506.12508, 2025. URL https://arxiv.org/abs/2506.12508v2. [1320] Xiaoyi Zhang, Zhaoyang Jia, Zongyu Guo, Jiahao Li, Bin Li, Houqiang Li, and Yan Lu. Deep video discovery: Agentic search with tool use for long-form video understanding, arXiv preprint arXiv:2505.18079, 2025. URL https://arxiv.org/abs/2505.18079v2. [1321] Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D. Manning, and J. Leskovec. Greaselm: Graph reasoning enhanced language models for question answering. International Conference on Learning Representations, 2022. [1322] Yao Zhang, Zijian Ma, Yunpu Ma, Zhen Han, Yu Wu, and Volker Tresp. Webpilot: versatile and autonomous multi-agent system for web task execution with strategic exploration, arXiv preprint arXiv:2408.15978, 2024. URL https://arxiv.org/abs/2408.15978. [1323] Yinger Zhang, Hui Cai, Yicheng Chen, Rui Sun, and Jing Zheng. Reverse chain: generic-rule for llms to master multi-api planning, arXiv preprint arXiv:2310.04474, 2023. URL https://arxiv. org/abs/2310.04474v3. [1324] Youjia Zhang, Jin Wang, Liang-Chih Yu, and Xuejie Zhang. Ma-bert: Learning representation by incorporating multi-attribute knowledge in transformers. Findings, 2021. [1325] Yu Zhang, Jinlong Ma, Yongshuai Hou, Xuefeng Bai, Kehai Chen, Yang Xiang, Jun Yu, and Min Zhang. Evaluating and steering modality preferences in multimodal large language model, arXiv preprint arXiv:2505.20977v1, 2025. URL https://arxiv.org/abs/2505.20977v1. 159 [1326] Yunyi Zhang, Ming Zhong, Siru Ouyang, Yizhu Jiao, Sizhe Zhou, Linyi Ding, and Jiawei Han. Automated mining of structured knowledge from text in the era of large language models. Knowledge Discovery and Data Mining, 2024. [1327] Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Ö. Arik. Chain of agents: Large language models collaborating on long-context tasks. Neural Information Processing Systems, 2024. [1328] Yuxiang Zhang, Yuqi Yang, Jiangming Shu, Xinyan Wen, and Jitao Sang. Agent models: Internalizing chain-of-action generation into reasoning models, arXiv preprint arXiv:2503.06580, 2025. URL https://arxiv.org/abs/2503.06580v1. [1329] Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. survey on the memory mechanism of large language model based agents, arXiv preprint arXiv:2404.13501, 2024. URL https://arxiv.org/abs/2404.13501v1. [1330] Zeyu Zhang, Quanyu Dai, Luyu Chen, Zeren Jiang, Rui Li, Jieming Zhu, Xu Chen, Yi Xie, Zhenhua Dong, and Ji-Rong Wen. Memsim: bayesian simulator for evaluating memory of llm-based personal assistants, arXiv preprint arXiv:2409.20163, 2024. URL https://arxiv.org/abs/ 2409.20163v1. [1331] Zeyu Zhang, Quanyu Dai, Xu Chen, Rui Li, Zhongyang Li, and Zhenhua Dong. Memengine: unified and modular library for developing advanced memory of llm-based agents. The Web Conference, 2025. [1332] Zheng Zhang, Liang Ding, Dazhao Cheng, Xuebo Liu, Min Zhang, and Dacheng Tao. Bliss: Robust sequence-to-sequence learning via self-supervised input representation, arXiv preprint arXiv:2204.07837, 2022. URL https://arxiv.org/abs/2204.07837v2. [1333] Zhenyu (Allen) Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark W. Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Neural Information Processing Systems, 2023. [1334] Zhihan Zhang, Zhenwen Liang, Wenhao Yu, Dian Yu, Mengzhao Jia, Dong Yu, and Meng Jiang. Learn beyond the answer: Training language models with reflection for mathematical reasoning. Conference on Empirical Methods in Natural Language Processing, 2024. [1335] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action agents. Annual Meeting of the Association for Computational Linguistics, 2023. [1336] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Y. Liu, and Gao Huang. Expel: Llm agents are experiential learners. AAAI Conference on Artificial Intelligence, 2023. [1337] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners, arXiv preprint arXiv:2308.10144, 2024. URL https://arxiv. org/abs/2308.10144. [1338] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and Jian Tang. Learning on large-scale text-attributed graphs via variational inference. International Conference on Learning Representations, 2022. [1339] Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, and Bin Cui. Retrieval-augmented generation for ai-generated content: survey, arXiv preprint arXiv:2402.19473, 2024. URL https://arxiv.org/abs/2402.19473v6. [1340] Pengyu Zhao, Zijian Jin, and Ning Cheng. An in-depth survey of large language model-based artificial intelligence agents, arXiv preprint arXiv:2309.14365, 2023. URL https://arxiv.org/abs/ 2309.14365v1. [1341] Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Xingchen Xu, Yu Huang, Wei Wang, Yue Chen, Yongchun He, and Yanzhi Wang. 7b fully open source moxin-llm/vlm from pretraining to grpo-based reinforcement learning enhancement. arXiv preprint, 2024. [1342] Qi Zhao, Hongyu Yang, Qi Song, Xinwei Yao, and Xiangyang Li. Knowpath: Knowledge-enhanced reasoning via llm-generated inference paths over knowledge graphs, arXiv preprint arXiv:2502.12029, 2025. URL https://arxiv.org/abs/2502.12029v3. [1343] Qifang Zhao, Weidong Ren, Tianyu Li, Xiaoxiao Xu, and Hong Liu. Graphgpt: Generative pre-trained graph eulerian transformer, arXiv preprint arXiv:2401.00529v3, 2023. URL https://arxiv.org/ abs/2401.00529v3. [1344] Ruilin Zhao, Feng Zhao, Long Wang, Xianzhi Wang, and Guandong Xu. Kg-cot: Chain-of-thought prompting of large language models over knowledge graphs for knowledge-aware question answering. International Joint Conference on Artificial Intelligence, 2024. [1345] Shangziqi Zhao, Jiahao Yuan, Guisong Yang, and Usman Naseem. Can pruning improve reasoning? revisiting long-cot compression with capability in mind for better reasoning, arXiv preprint arXiv:2505.14582, 2025. URL https://arxiv.org/abs/2505.14582v1. [1346] Shitian Zhao, Zhuowan Li, Yadong Lu, Alan L. Yuille, and Yan Wang. Causal-cog: causal-effect look at context generation for boosting multi-modal language models. Computer Vision and Pattern Recognition, 2023. [1347] Tony Zhao, Eric Wallace, Shi Feng, D. Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. International Conference on Machine Learning, 2021. [1348] Weixiang Zhao, Xingyu Sui, Jiahe Guo, Yulin Hu, Yang Deng, Yanyan Zhao, Bing Qin, Wanxiang Che, Tat-Seng Chua, and Ting Liu. Trade-offs in large reasoning models: An empirical analysis of deliberative and adaptive reasoning over foundational capabilities, arXiv preprint arXiv:2503.17979, 2025. URL https://arxiv.org/abs/2503.17979v1. [1349] Yibo Zhao, Jiapeng Zhu, Ye Guo, Kangkang He, and Xiang Li. E2graphrag: Streamlining graph-based rag for high efficiency and effectiveness. arXiv preprint, 2025. [1350] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is generalist web agent, if grounded. International Conference on Machine Learning, 2024. [1351] Changmeng Zheng, Dayong Liang, Wengyu Zhang, Xiao Wei, Tat seng Chua, and Qing Li. picture is worth graph: blueprint debate paradigm for multimodal reasoning. ACM Multimedia, 2024. 161 [1352] Chuanyang Zheng, Yihang Gao, Han Shi, Minbin Huang, Jingyao Li, Jing Xiong, Xiaozhe Ren, Michael Ng, Xin Jiang, Zhenguo Li, and Yu Li. Dape: Data-adaptive positional encoding for length extrapolation. Neural Information Processing Systems, 2024. [1353] Chunmo Zheng, Saika Wong, Xing Su, Yinqiu Tang, Ahsan Nawaz, and Mohamad Kassem. Automating construction contract review using knowledge graph-enhanced large language models. Automation in Construction, 2023. [1354] Junhao Zheng, Shengjie Qiu, Chengming Shi, and Qianli Ma. Towards lifelong learning of large language models: survey. ACM Computing Surveys, 2024. [1355] Junhao Zheng, Xidi Cai, Qiuke Li, Duzhen Zhang, Zhongzhi Li, Yingying Zhang, Le Song, and Qianli Ma. Lifelongagentbench: Evaluating llm agents as lifelong learners, arXiv preprint arXiv:2505.11942, 2025. URL https://arxiv.org/abs/2505.11942v3. [1356] Longtao Zheng, R. Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control. International Conference on Learning Representations, 2023. [1357] Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control, arXiv preprint arXiv:2306.07863, 2024. URL https://arxiv.org/abs/2306.07863. [1358] Xu Zheng, Chenfei Liao, Yuqian Fu, Kaiyu Lei, Yuanhuiyi Lyu, Lutao Jiang, Bin Ren, Jialei Chen, Jiawen Wang, Chengxin Li, Linfeng Zhang, D. Paudel, Xuanjing Huang, Yu-Gang Jiang, N. Sebe, Dacheng Tao, L. V. Gool, and Xuming Hu. Mllms are deeply affected by modality bias, arXiv preprint arXiv:2505.18657v1, 2025. URL https://arxiv.org/abs/2505.18657v1. [1359] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments, arXiv preprint arXiv:2504.03160, 2025. URL https://arxiv.org/abs/2504.03160v4. [1360] Li Zhong, Zilong Wang, and Jingbo Shang. Debug like human: large language model debugger via verifying runtime execution step by step. Annual Meeting of the Association for Computational Linguistics, 2024. [1361] Rui Zhong, Yang Cao, Jun Yu, and M. Munetomo. Large language model assisted adversarial robustness neural architecture search. 2024 6th International Conference on Data-driven Optimization of Complex Systems (DOCS), 2024. [1362] Wanjun Zhong, Lianghong Guo, Qi-Fei Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. AAAI Conference on Artificial Intelligence, 2023. [1363] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory, arXiv preprint arXiv:2305.10250, 2023. URL https://arxiv.org/abs/2305.10250. [1364] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. International Conference on Machine Learning, 2023. 162 [1365] Bin Zhou, Xingwang Shen, Yuqian Lu, Xinyu Li, B. Hua, Tianyuan Liu, and Jinsong Bao. Semanticaware event link reasoning over industrial knowledge graph embedding time series data. International Journal of Production Research, 2022. [1366] Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, D. Schuurmans, O. Bousquet, Quoc Le, and Ed H. Chi. Least-to-most prompting enables complex reasoning in large language models. International Conference on Learning Representations, 2022. [1367] Huichi Zhou, Kin-Hei Lee, Zhonghao Zhan, Yue Chen, Zhenhao Li, Zhaoyang Wang, Hamed Haddadi, and Emine Yilmaz. Trustrag: Enhancing robustness and trustworthiness in rag, arXiv preprint arXiv:2501.00879, 2025. URL https://arxiv.org/abs/2501.00879. [1368] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents. International Conference on Learning Representations, 2023. [1369] Wangchunshu Zhou, Y. Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Xiangru Tang, Ningyu Zhang, Huajun Chen, Peng Cui, and Mrinmaya Sachan. Agents: An open-source framework for autonomous language agents, arXiv preprint arXiv:2309.07870, 2023. URL https://arxiv.org/abs/2309. 07870v3. [1370] Yingli Zhou, Yaodong Su, Youran Sun, Shu Wang, Taotao Wang, Runyuan He, Yongwei Zhang, Sicong Liang, Xilin Liu, Yuchi Ma, and Yixiang Fang. In-depth analysis of graph-based rag in unified framework, arXiv preprint arXiv:2503.04338, 2025. URL https://arxiv.org/abs/ 2503.04338v1. [1371] Yuhang Zhou and Wei Ai. Teaching-assistant-in-the-loop: Improving knowledge distillation from imperfect teacher models in low-budget scenarios. Annual Meeting of the Association for Computational Linguistics, 2024. [1372] Yujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Zheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi Ho, and Philip S. Yu. Trustworthiness in retrieval-augmented generation systems: survey, arXiv preprint arXiv:2409.10102, 2024. URL https://arxiv.org/abs/2409.10102v1. [1373] Zhehua Zhou, Jiayang Song, Kunpeng Yao, Zhan Shu, and Lei Ma. Isr-llm: Iterative self-refined large language model for long-horizon sequential task planning. IEEE International Conference on Robotics and Automation, 2023. [1374] Zihan Zhou, Chong Li, Xinyi Chen, Shuo Wang, Yu Chao, Zhili Li, Haoyu Wang, Rongqiao An, Qi Shi, Zhixing Tan, Xu Han, Xiaodong Shi, Zhiyuan Liu, and Maosong Sun. Llmmapreduce: Simplified long-sequence processing using large language models, arXiv preprint arXiv:2410.09342, 2024. URL https://arxiv.org/abs/2410.09342v1. [1375] Zijian Zhou, Ao Qu, Zhaoxuan Wu, Sunghwan Kim, Alok Prakash, Daniela Rus, Jinhua Zhao, B. Low, and P. Liang. Mem1: Learning to synergize memory and reasoning for efficient long-horizon agents, arXiv preprint arXiv:2506.15841, 2025. URL https://arxiv.org/abs/2506.15841v1. [1376] Andrew Zhu, Liam Dugan, and Christopher Callison-Burch. Redel: toolkit for llm-powered recursive multi-agent systems. Conference on Empirical Methods in Natural Language Processing, 2024. 163 [1377] Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li. Pose: Efficient context window extension of llms via positional skip-wise training. International Conference on Learning Representations, 2023. [1378] Hongyin Zhu. Metaaid 2.5: secure framework for developing metaverse applications via large language models. arXiv preprint, 2023. [1379] Jason Zhu, Yanling Cui, Yuming Liu, Hao Sun, Xue Li, Markus Pelger, Liangjie Zhang, Tianqi Yan, Ruofei Zhang, and Huasha Zhao. Textgnn: Improving text encoder via graph neural network in sponsored search. The Web Conference, 2021. [1380] Jiachen Zhu, Menghui Zhu, Renting Rui, Rong Shan, Congmin Zheng, Bo Chen, Yunjia Xi, Jianghao Lin, Weiwen Liu, Ruiming Tang, Yong Yu, and Weinan Zhang. Evolutionary perspectives on the evaluation of llm-based ai agents: comprehensive survey, arXiv preprint arXiv:2506.11102, 2025. URL https://arxiv.org/abs/2506.11102v1. [1381] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Yue Cao, Yangzhou Liu, Haomin Wang, Weiye Xu, Hao Li, Jiahao Wang, Han Lv, Dengnian Chen, Songze Li, Yinan He, Tan Jiang, Jiapeng Luo, Yi Wang, Cong He, Botian Shi, Xingcheng Zhang, Wenqi Shao, Junjun He, Ying Xiong, Wenwen Qu, Peng Sun, Penglong Jiao, Lijun Wu, Kai Zhang, Hui Deng, Jiaye Ge, Kaiming Chen, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models, arXiv preprint arXiv:2504.10479v3, 2025. URL https://arxiv.org/abs/2504.10479v3. [1382] Mingwei Zhu, Leigang Sha, Yu Shu, Kangjia Zhao, Tiancheng Zhao, and Jianwei Yin. Benchmarking sequential visual input reasoning and prediction in multimodal large language models, arXiv preprint arXiv:2310.13473v1, 2023. URL https://arxiv.org/abs/2310.13473v1. [1383] Rongzhi Zhu, Xiangyu Liu, Zequn Sun, Yiwei Wang, and Wei Hu. Mitigating lost-in-retrieval problems in retrieval augmented multi-hop question answering. 2025. [1384] Rongzhi Zhu, Yi Liu, Zequn Sun, Yiwei Wang, and Wei Hu. When can large reasoning models save thinking? mechanistic analysis of behavioral divergence in reasoning. 2025. [1385] Runchuan Zhu, Zinco Jiang, Jiang Wu, Zhipeng Ma, Jiahe Song, Fengshuo Bai, Dahua Lin, Lijun Wu, and Conghui He. Grait: Gradient-driven refusal-aware instruction tuning for effective hallucination mitigation. 2025. [1386] Runchuan Zhu, Zhipeng Ma, Jiang Wu, Junyuan Gao, Jiaqi Wang, Dahua Lin, and Conghui He. Utilize the flow before stepping into the same river twice: Certainty represented knowledge flow for refusal-aware instruction tuning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2615726165, 2025. [1387] Tongyao Zhu, Qian Liu, L. Pang, Zhengbao Jiang, Min-Yen Kan, and Min Lin. Beyond memorization: The challenge of random memory access in language models. Annual Meeting of the Association for Computational Linguistics, 2024. [1388] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based 164 knowledge and memory, arXiv preprint arXiv:2305.17144, 2023. URL https://arxiv.org/ abs/2305.17144. [1389] Yue Zhu, Hao Yu, Chen Wang, Zhuoran Liu, and Eun Kyung Lee. Towards efficient key-value cache management for prefix prefilling in llm inference, arXiv preprint arXiv:2505.21919, 2025. URL https://arxiv.org/abs/2505.21919v1. [1390] Zhengqiu Zhu, Yong Zhao, Bin Chen, S. Qiu, Kai Xu, Quanjun Yin, Jin-Yu Huang, Zhong Liu, and Fei Wang. Conversational crowdsensing: parallel intelligence powered novel sensing approach, arXiv preprint arXiv:2402.06654, 2024. URL https://arxiv.org/abs/2402.06654v1. [1391] Zulun Zhu, Tiancheng Huang, Kai Wang, Junda Ye, Xinghe Chen, and Siqiang Luo. Graph-based approaches and functionalities in retrieval-augmented generation: comprehensive survey, arXiv preprint arXiv:2504.10499, 2025. URL https://arxiv.org/abs/2504.10499v1. [1392] Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming Ren, Stephen W. Huang, Jie Fu, Xiang Yue, and Wenhu Chen. Structlm: Towards building generalist models for structured knowledge grounding, arXiv preprint arXiv:2402.16671, 2024. URL https://arxiv.org/abs/ 2402.16671v7. [1393] Yuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and Chao Zhang. Toolqa: dataset for llm question answering with external tools. Neural Information Processing Systems, 2023. [1394] Zhixiong Zhuang, Maria-Irina Nicolae, Hui-Po Wang, and Mario Fritz. Proxyprompt: Securing system prompts against prompt extraction attacks, arXiv preprint arXiv:2505.11459, 2025. URL https://arxiv.org/abs/2505.11459v1. [1395] Ziyuan Zhuang, Zhiyang Zhang, Sitao Cheng, Fangkai Yang, Jia Liu, Shujian Huang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. Efficientrag: Efficient retriever for multi-hop question answering. In In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 33923411, 2024. [1396] Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, and Y. Zhuang. Triad: framework leveraging multi-role llm-based agent to solve knowledge base question answering. Conference on Empirical Methods in Natural Language Processing, 2024. [1397] Yongshuo Zong, Ondrej Bohdal, and Timothy M. Hospedales. Vl-icl bench: The devil in the details of benchmarking multimodal in-context learning. arXiv preprint, 2024. [1398] Yongshuo Zong, Ondrej Bohdal, and Timothy M. Hospedales. Vl-icl bench: The devil in the details of multimodal in-context learning. International Conference on Learning Representations, 2024. [1399] Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Yuwei Cao, Dongyuan Li, Renhe Jiang, and Philip S. Yu. survey on large language model based human-agent systems. arXiv preprint, 2025. [1400] Tao Zou, Le Yu, Yifei Huang, Leilei Sun, and Bo Du. Pretraining language models with text-attributed heterogeneous graphs. Conference on Empirical Methods in Natural Language Processing, 2023. [1401] Adam Zweiger, Jyothish Pari, Han Guo, Ekin Akyürek, Yoon Kim, and Pulkit Agrawal. Self-adapting language models, arXiv preprint arXiv:2506.10943, 2025. URL https://arxiv.org/abs/2506. 10943v1."
        }
    ],
    "affiliations": [
        "Institute of Computing Technology, Chinese Academy of Sciences",
        "Peking University",
        "The University of Queensland",
        "Tsinghua University",
        "University of California, Merced",
        "University of Chinese Academy of Sciences"
    ]
}