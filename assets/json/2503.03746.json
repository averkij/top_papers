{
    "paper_title": "Process-based Self-Rewarding Language Models",
    "authors": [
        "Shimao Zhang",
        "Xiao Liu",
        "Xin Zhang",
        "Junxiao Liu",
        "Zheheng Luo",
        "Shujian Huang",
        "Yeyun Gong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs' performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to a decline in performance. In this work, we propose the Process-based Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference optimization within the self-rewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based Self-Rewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 6 4 7 3 0 . 3 0 5 2 : r Process-based Self-Rewarding Language Models Shimao Zhang* Xiao Liu Xin Zhang Junxiao Liu Zheheng Luo(cid:51) Shujian Huang Yeyun Gong National Key Laboratory for Novel Software Technology, Nanjing University (cid:51)The University of Manchester Microsoft Research Asia smzhang@smail.nju.edu.cn, huangsj@nju.edu.cn, xiao.liu.msrasia@microsoft.com"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models have demonstrated outstanding performance across various downstream tasks and have been widely applied in multiple scenarios. Human-annotated preference data is used for training to further improve LLMs performance, which is constrained by the upper limit of human performance. Therefore, Self-Rewarding method has been proposed, where LLMs generate training data by rewarding their own outputs. However, the existing self-rewarding paradigm is not effective in mathematical reasoning scenarios and may even lead to decline in performance. In this work, we propose the Processbased Self-Rewarding pipeline for language models, which introduces long-thought reasoning, step-wise LLM-as-a-Judge, and stepwise preference optimization within the selfrewarding paradigm. Our new paradigm successfully enhances the performance of LLMs on multiple mathematical reasoning benchmarks through iterative Process-based SelfRewarding, demonstrating the immense potential of self-rewarding to achieve LLM reasoning that may surpass human capabilities."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) acquire powerful multi-task language capabilities through pretraining on extensive corpus (Radford et al., 2019; Brown et al., 2020). Additionally, supervised finetuning (SFT) can further effectively improve the models performance on end-tasks. However, it is found that models after SFT are prone to hallucinations (Lai et al., 2024) due to the simultaneous increasing of the probabilities of both preferred and undesirable outputs (Hong et al., 2024). Therefore, to further enhance the language capabilities of *Work done during his internship at MSRA. Corresponding authors. 1Our code and data will be available at: https://github. com/Shimao-Zhang/Process-Self-Rewarding. 1 LLMs to align with human-level performance effectively, researchers often utilize human-annotated preference data for training. representative approach is Reinforcement Learning from Human Feedback (RLHF) (Christiano et al., 2017), which utilizes RL algorithms and external reward signals to help LLMs learn specific preferences. However, most reward signals rely on human annotations or reward models, which is expensive and bottlenecked by human capability and reward model quality. So the Self-Rewarding Language Models paradigm (Yuan et al., 2024) is proposed to overcome the above limitations, which integrates the reward model and the policy model within the same model. In this framework, single model possesses the ability to both perform the target task and provide reward feedback. The model can execute different tasks based on the scenario and conduct iterative updates. This paradigm is effective in instruction-following scenarios, where the model achieves performance improvement solely through self-rewarding and iterative updates. Although the self-rewarding algorithm performs well in the instruction-following tasks, it is also demonstrated that LLMs perform poorly on the mathematical domain data based on the existing self-rewarding algorithm. In fact, models performance may even degrade as the number of iterations increases (Yuan et al., 2024). We notice two main limitations in the self-rewarding framework: (a) Existing self-rewarding algorithm is not able to provide fine-grained and accurate reward signals for complex reasoning tasks involving longthought chains; (b) For complex mathematical solution, its hard to design the criterion for generating specific scores. It means that assigning scores to complex long-thought multi-step reasoning for LLMs is more challenging than performing pairwise comparisons, with lower consistency and agreement with humans, which is proven by the results in Appendix B. In this work, we propose the paradigm of Process-based Self-Rewarding Language Models, where we introduce the step-wise LLM-as-a-Judge and step-wise preference optimization into the In nuttraditional self-rewarding framework. shell, we enable the LLMs to simultaneously conduct step-by-step complex reasoning and perform LLM-as-a-Judge for individual intermediate steps. For the limitation (a) above, to get finer-grained and more accurate rewards, Process-based SelfRewarding paradigm allows LLMs to perform stepwise LLM-as-a-Judge for the individual reasoning step. Since producing the correct final answer does not imply that LLMs can generate correct intermediate reasoning steps, it is crucial to train the model to learn not only to produce the correct final answer but also to generate correct intermediate reasoning steps. By using model itself as reward model to generate step preference pairs data, we further perform step-wise preference optimization. For the limitation (b) above, we design LLM-asa-Judge prompt for step-wise pairwise comparison rather than directly assigning scores to the answer for more proper and steadier judgments based on the observations in Appendix B. We conduct the experiments on models in different parameter sizes (7B and 72B) and test across wide range of mathematical reasoning benchmarks. Our results show that Process-based SelfRewarding can effectively enhance the mathematical reasoning capabilities of LLMs, which indicates that LLMs are able to perform effective selfrewarding at the step level. Our models that iteratively trained based on the Process-based SelfRewarding paradigm demonstrate an increasing trend in both mathematical and LLM-as-a-judge capabilities. These results suggest this frameworks immense potential for achieving intelligence that may surpass human performance."
        },
        {
            "title": "Feedback",
            "content": "Supervised Fine-tuning is an effective method to improve LLMs performance across many different downstream tasks. But it has been evidenced that SFT potentially exacerbates LLMs hallucination (Hong et al., 2024). So RLHF is further utilized to align LLMs with human preference. In the RLHF paradigm, the model is trained based on reward signals provided by external reward models and humans by reinforcement learning algorithms, such as PPO (Schulman et al., 2017), DPO (Rafailov et al., 2024), SimPO (Meng et al., 2024), and so on. Direct Preference Optimization (DPO) is preference learning algorithm which directly uses pairwise preference data, including chosen and rejected answers for optimization. Furthermore, the step-wise preference optimization has also been investigated for long-chain reasoning and has shown great performance (Lai et al., 2024; Chen et al., 2024). In our work, we introduce the step-wise preference optimization into our Process-based Self-Rewarding paradigm for more fine-grained learning."
        },
        {
            "title": "2.2 LLM-as-a-Judge",
            "content": "LLM-as-a-Judge technique has been widely used for evaluation tasks because of LLMs scalability, adaptability, and cost-effectiveness (Gu et al., 2024). In the LLM-as-a-Judge scenarios, LLMs are prompted to mimic human reasoning and evaluate specific inputs against set of predefined rules. To improve the performance of LLM-as-a-Judge, the LLM acting as the evaluator is trained to align with human preferences. When conducting LLMas-a-Judge, LLMs can play many different roles depending on the given prompt. Typical applications include tasks where LLMs are prompted to generate scores (Li et al., 2023; Xiong et al., 2024), perform pairwise comparisons (Liu et al., 2024; Liusie et al., 2023), rank multiple candidates (Yuan et al., 2023), and so on. However, the LLM-as-aJudge for individual mathematical reasoning steps has not been widely investigated. In our experiment, we design the step-wise LLM-as-a-Judge for rewarding and analyze its performance."
        },
        {
            "title": "2.3 Self-Rewarding Language Models",
            "content": "Although RLHF has been widely utilized to align LLMs with human-level performance and has achieved impressive performance, the existing methods heavily rely on high-quality reward models or human feedback, which bottlenecks these approaches. To avoid this bottleneck, Yuan et al. (2024) propose the Self-Rewarding Language Models paradigm, which uses single model as both instruction-following model and reward model simultaneously. The iterative self-rewarding algorithm operates by having the model generate responses and reward the generated response candidates, then selecting preference pairs for training. Based on this, Wu et al. (2024) further improve 2 Figure 1: Illustration of our Process-based Self-Rewarding paradigm. (1) We get EFT data by tree-search, initial data filtering and data annotation. And we get IFT data by step segmentation. (2) The model is initialized on EFT and IFT data. (3) The model conducts step-by-step search-based reasoning and performs step-wise LLM-as-a-Judge to select the chosen step and generate the step-wise preference pair at each step. (4) We perform step-wise preference optimization on the model. (5) The model enters the next iteration cycle. the judgment agreement by adding the LLM-as-aMeta-Judge action into the self-rewarding pipeline, which allows the model to evaluate its own judgments. But the existing self-rewarding methods mainly focus on the instruction-following tasks and perform poorly in the mathematical domain data (Yuan et al., 2024). And evaluating the entire response makes it difficult for the model to learn fine-grained preference information. For some long-thought reasoning tasks, it is important to enable LLMs to focus on and learn the fine-grained reasoning step preference information."
        },
        {
            "title": "Language Models",
            "content": "In this section, we propose our new Processbased Self-Rewarding Language Models pipeline. We first review the existing self-rewarding algorithm and our motivation as preliminary study in 3.1. Then we introduce our novel paradigm for more fine-grained step-wise self-rewarding and self-evolution. The entire pipeline consists of sequential stages: model initialization (3.2), reasoning and preference data generation (3.3), and model preference optimization (3.4). Finally, we provide summarized overview of our algorithm (3.5). We illustrate the entire pipeline in Figure 1."
        },
        {
            "title": "3.1 Preliminary Study",
            "content": "Complex reasoning tasks are still great challenges for LLMs now. Chain-of-Thought (Wei et al., 2022) methods prompt LLMs to solve the complex problems by reasoning step by step rather than generating the answer directly, which leads to significant improvements across many reasoning tasks (Yoran et al., 2023; Fu et al., 2022; Zhang et al., 2022). Furthermore, recent studies investigate the test-time scaling paradigm which allows the LLMs to use more resources and time for inference to achieve better performance (Lightman et al., 2023) typically based on search and step selecting (Yao et al., 2024; Wang et al., 2024b). These results highlight the importance of conducting high-quality long-thought step-by-step reasoning for LLMs in solving complex reasoning problems. Most existing preference optimization algorithms rely on reward signals from external reward models or human-annotated data. However, deploying an external reward model or getting ground truth gold reward signals from human annotators is expensive (Gao et al., 2023). Moreover, due to the inherent limitations and implicit biases of both humans and reward models, these model optimization strategies are bottlenecked (Lambert et al., 2024; Yuan et al., 2024). Thus, Self-Rewarding algorithm is proposed to mitigate this limitation by enabling the model to provide reward signals for its own outputs and perform self-improvement, showing the feasibility of achieving models that surpass human performance (Yuan et al., 2024). There are still many aspects waiting for further research and improvement in the self-rewarding 3 framework. The original method is primarily designed for instruction-following tasks and performs poorly on mathematical reasoning data. Step-bystep long-chain reasoning is widely used for complex mathematical reasoning, which allows the models to conduct more detailed thinking and finegrained verification of the reasoning steps (Lightman et al., 2023; Wang et al., 2024b; Lai et al., 2024). Given the effectiveness of step-by-step reasoning, we further propose Process-based SelfRewarding, introducing LLM-as-a-judge and preference optimization for individual steps."
        },
        {
            "title": "3.2 Model Initialization",
            "content": "To perform Process-based Self-Rewarding, models need to possess two key abilities: Step-by-step mathematical reasoning: When faced with complex reasoning problem, the model needs to think and reason step by step, outputting the reasoning process in specified format. (Each step is prefixed with Step n: , where indeicates the step number.) LLM-as-a-Judge for individual steps: The model should be able to assess the quality of the given next reasoning steps based on the existing problem and partial reasoning steps and provide detailed explanation. We construct data separately for the two tasks to perform cold start. Following Yuan et al. (2024), we refer to them as Instruction Fine-Tuning (IFT) data and Evaluation Fine-Tuning (EFT) data. For IFT data, we divide the given solution steps into individual steps logically without altering any information in the original solution by using OpenAI o1 (Jaech et al., 2024). For EFT data, since there is no available step-wise LLM-as-a-Judge dataset, we first train Qwen2.5-72B (Yang et al., 2024a) on PRM800k (Lightman et al., 2023) following Wang et al. (2024a). After getting Process Reward Model (PRM) by this, which can output single label + or - for reasoning step based on the question and the previous steps, we conduct Monte Carlo Tree Search (MCTS) on policy model. We use the probability of label + of the above PRM to compare the relative quality of all candidate steps at the same layer, and choose the best and the worst step as data pair. After the initial data filtering process, we use GPT-o1 to generate judgments and detailed explanations for the obtained data pairs. The pairs whose judgments align with the previous PRM assessments are selected as the final EFT data. Additionally, to enhance consistency, we evaluate each pair twice using GPT with different input orders and select only the pairs that have consistent results."
        },
        {
            "title": "Preference Data Generation",
            "content": "After the EFT + IFT initialization stage, the model is able to conduct both step-wise LLM-asa-Judge and step-by-step mathematical reasoning in the specified formats. Because we conduct pairwise comparison rather than single answer grading, we utilize the following search strategy: Sl = {sl,1, sl,2, sl,3, ..., sl,w1, sl,w} (1) where Sl is all candidates for the next step, is the step number starting from 1, is hyperparameter to specify the search width for each step. Scorel,i = (cid:88) 1jw, j=i O(sl,i, sl,j x, s1, s2, ..., sl1) (2) where is the next step number, sl,i indicates the i-th candidate for the next l-th step, is the prompt, and is function that takes 1 when sl,i is considered better than sl,j and 0 otherwise. sbest = Sl[max(Scorel)] sworst = Sl[min(Scorel)] sl = sbest (3) (4) (5) where max(Scorel) is the index of the candidate with the highest score and min(Scorel) corresponds to the lowest score. sl is the final chosen l-th step. (sbest ) will be chosen as chosenl rejected preference pair. , sworst This process will be repeated continuously until generation is complete. It is important to note that to enhance the effectiveness of preference data, if max(Scorel) is equal to min(Scorel), we will discard the existing sl1 and (sbest l1 ) and roll back to the previous step. l1, sworst"
        },
        {
            "title": "3.4 Step-wise Model Preference Optimization",
            "content": "With preference data collected in the Section 3.3, we conduct preference optimization training on the model. We choose Direct Preference Optimization (DPO) as the training algorithm (Rafailov et al., 2024). The difference is that we conduct more 4 fine-grained step-wise DPO in our work. The similar method has also been investigated by Lai et al. (2024). We can calculate the training loss as: = β log πθ(sb πref (sb x, s1, ..., sl1) x, s1, ..., sl1) = β log πθ(sw πref (sw x, s1, ..., sl1) x, s1, ..., sl1) )D[log σ(A B)] ,sw L(πθ; πref ) = (x,s1,...,sb (6) (7) (8) and sw where is the prompt, s1, ..., sl1 is the previous steps, sb are the best and worst steps respectively for the l-th step, β is hyperparameter controlling the deviation from the base reference policy, πθ and πref are the policies to be optimized and the reference policy respectively. After the preference optimization stage, we have the model for the next cycle. In the next iteration, we sequentially repeat the steps in 3.3 and 3.4. 3."
        },
        {
            "title": "Iteration Pipeline",
            "content": "We show the entire pipeline of our algorithm. Following Yuan et al. (2024), we refer to the model after iterations as Mn. And we refer to the Pair-wise Preference Data generated by Mn as PPD(Mn). Then the sequence in our work can be defined as: M0: The base model. M1: The model obtained by supervised finetuning (SFT) M0 on EFT + IFT data. M2: The model obtained by training M1 on PPD(M1) using step-wise DPO. Mn: The model obtained by training Mn1 on PPD(Mn1) using step-wise DPO. In summary, we initialize the base model using well-selected step-wise LLM-as-a-Judge data (EFT) and step-by-step long-thought reasoning data (IFT). Once the model possesses the corresponding two abilities, we select preference pairs through search and reward signals provided by the model itself, and train the model using step-wise DPO. Then we iterate the model by repeatedly performing the above operations."
        },
        {
            "title": "4 Experimental Setup",
            "content": "We conduct our experiments on models in different parameter sizes and several representative mathematical reasoning benchmarks. In this section, we introduce our experimental settings in detail. Models We choose the base model from Qwen2.5-Math series (Yang et al., 2024b) in our experiments, which is one of the most popular open-source LLM series. Specifically, we choose Qwen2.5-Math-7B and Qwen2.5-Math-72B. Additionally, we choose OpenAI GPT-o1 (Jaech et al., 2024) for our initialization data processing (3.2). Datasets two capabilities of the model: In our experiments, we mainly focus on Step-by-step Mathematical Reasoning: We choose subset of NuminaMath (LI et al., 2024) for IFT data construction, whose solutions have been formatted in Chain of Thought (CoT) manner. We extract subset of 28,889 samples and prompt GPT-o1 (Jaech et al., 2024) to logically segment the solutions into step-by-step format without altering any original content. The corresponding prompt is presented in Figure 3. And the instruction format for step-by-step long-thought reasoning is presented in Figure 4. Step-wise LLM-as-a-Judge: As described in the Section 3.2, we first filtrate some preference pairs using the trained PRM. Then we utilize GPT-o1 and get total of 4,679 EFT data with judgments and detailed explanations. Finally we split the whole dataset into 4,167 samples as the training set and 500 samples as the test set. The instruction format for stepwise pairwise LLM-as-a-Judge is presented in Figure 5, which is following the basic format of Zheng et al. (2023). task evaluation,"
        },
        {
            "title": "And for mathematical",
            "content": "following Yang et al. (2024b), we evaluate the LLMs mathematical capabilities across some representative benchmarks. We choose the widely used benchmarks GSM8k (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021). We also choose some complex and challenging competition benchmarks, including Gaokao2023En (Liao et al., 2024), Olympiadbench (He et al., 2024), AIME20242, and AMC20233. Evaluation Metrics We use accuracy as the evaluation metric for both the mathematical performance and LLM-as-a-Judge quality. For accuracy 2https://huggingface.co/datasets/AI-MO/ aimo-validation-aime 3https://huggingface.co/datasets/AI-MO/ aimo-validation-amc"
        },
        {
            "title": "Model",
            "content": "GPT-4o 7B Base Model M0 SRLM - M1 M2 M3 M4 PSRLM - M1 M2 M3 M4 72B Base Model M0 SRLM - M1 M2 M3 PSRLM - M1 M2 M3 M4 GSM8k MATH Gaokao2023En OlympiadBench AIME2024 AMC2023 Avg. 92.9 76.6 67.5 70.1 88.2 87.6 88.5 88.3 88.5 88.8 88.5 88. 87.5 92.9 92.1 92.5 92.8 92.6 92.6 93.7 93.7 51.7 69.0 69.4 70.0 70.2 69.5 69.7 72.2 73.3 69.7 76.4 76.1 75.8 76.1 75.6 76.4 76.4 76.6 51.2 61.6 63.9 61.3 63.9 61.8 63.9 64.7 65.2 55.3 67.3 66.8 67.5 66.2 67.3 67.8 67.3 68.1 43. 21.3 37.6 37.2 36.7 37.6 36.0 36.3 39.9 38.7 28.9 41.8 42.1 42.5 44.0 41.8 41.8 42.7 44.1 10.0 47.5 56.3 0.0 10.0 3.3 10.0 13.3 6.7 16.7 10.0 13. 10.0 16.7 20.0 20.0 13.3 13.3 20.0 23.3 23.3 22.5 45.0 40.0 40.0 45.0 45.0 47.5 50.0 55.0 40.0 47.5 55.0 52.5 42.5 45.0 57.5 52.5 57.5 36.1 51.9 50.2 51.1 53.1 51.3 53.8 54.2 55.7 48.6 57.1 58.7 58.5 55.8 55.9 59.4 59.3 60.6 Table 1: Accuracy of Process-based Self-Rewarding based on 7B and 72B base models. SRLM is the self-rewarding language model algorithm as the baseline. We bold the best results for each parameter size in each benchmark. calculation on mathematical benchmarks, we follow the implementation of Yang et al. (2024b). Implementations For initial PRM training, we fine-tune full parameters on 128 NVIDIA A100 GPUs for 1 epoch with learning_rate=1e 5 and batch_size=128. For preliminary preference pairs selection, we set simulation_depth=3, num_iterations=100, T=0.7, and top_p=0.95. When training M0 to M1, we utilize 28,889 IFT and 4,179 EFT samples. We fine-tune LLMs full parameters on 32 NVIDIA H100 GPUs for 3 epochs with learning_rate=1e 6 and batch_size=32. During the reasoning and preference data generation stage, we utilize temperature sampling which trade-off generation quality and diversity (Zhang et al., 2024). We set T=0.5, top_p=0.95. The search width for each step is set to 6, and the max iteration number is set to 20. Finally, in the step-wise preference optimization, we train LLMs full parameters on 32 NVIDIA H100 GPUs for 1 epoch with learning_rate=5e 7 and batch_size=32. To get models from M2 to M4, we use 400, 800, and 1, 200 math questions for preference pairs generation respectively, which are all sampled from the train subset of NuminaMath. For all solution-scoring judgment strategy experiments, we use the same prompt template of Yuan et al. (2024). We use greedy search in evaluations."
        },
        {
            "title": "5 Results",
            "content": "In this section, we report our main results on different mathematical benchmarks and conduct some discussions and analyses based on the results."
        },
        {
            "title": "5.1 Main Results",
            "content": "We report the performance of M0 to M4 based on Qwen2.5-Math-7B and Qwen2.5-Math-72B respectively in Table 1. Our findings are as follows: As the number of iterations increases, the overall performance of the model improves. Traditionally, external reward signals and training data are utilized for improving LLMs performance. Our results indicate that models overall performance on mathematical tasks significantly improves from M1 to M4 solely through Processbased Self-Rewarding and step-wise preference optimization without any additional guidance. This leverages the potential of LLMs for both mathematical reasoning and as evaluators. Our fine-grained algorithm outperforms the tranditional method. After three iterations, our approach achieves superior performance compared to method that applies rewards and conducts training on the entire response. Given that the initialization with different EFT data lead to different M1 fiducial performance in the two methods, we also report the performance changes from M1 to 6 7B GSM8k MATH Gaokao2023En OlympiadBench AIME2024 AMC2023 SRLM Process-based (Ours) +0.1 +0.3 +1.2 +3.8 +2.3 +3.4 0.0 +2.7 +3.3 +6.6 0.0 +10. 72B GSM8k MATH Gaokao2023En OlympiadBench AIME2024 AMC2023 SRLM Process-based (Ours) -0.1 +1.1 -0.3 +1.0 -1.1 +0. +2.2 +2.3 -3.4 +10.0 -5.0 +12.5 Table 2: The results of LLMs mathematical performance changes after all iterations from M1 to M4. M4 after multiple iterations in Table 2, which reflects the algorithms effectiveness and stability in improving the models mathematical capabilities. Our method achieves more stable and effective improvements across all benchmarks. On one hand, using step-wise preference data enables the model to focus on more fine-grained information; on the other hand, conducting LLM-as-a-Judge on individual steps helps the model more easily detect subtle differences and errors. The models show noticeable improvements on some complex tasks. For some complex and highly challenging benchmarks, such as MATH, AIME2024, and AMC2023, LLMs performance show significant improvement. Complex problems require multi-step, long-thought reasoning. Our method effectively leverages the models existing knowledge to optimize the individual intermediate reasoning steps, achieving favorable results. Our method remains effective across models of different parameter sizes. We validate our method on both 7B and 72B LLMs to strengthen our conclusions. We find performance improvements across models of different parameter sizes on multiple mathematical tasks through Processbased Self-Rewarding. We also find that the 72B model gains more stable improvements compared to the 7B model, whose mathematical reasoning and LLM-as-a-Judge capabilities are stronger. Overall, we can find that the models iterating based on the Process-based Self-Rewarding paradigm achieve significant improvements across multiple mathematical tasks, outperforming the traditional self-rewarding method."
        },
        {
            "title": "5.2 Further Analysis",
            "content": "Based on the above results, we conduct more analysis and observations of the pipeline. Step-wise LLM-as-a-Judge Capability. We evaluate the LLMs ability to accurately assess reasoning steps as reward model during the iterative process. We test the model on the test set including"
        },
        {
            "title": "Model",
            "content": "7B 72B M0 (3-shot) M1 M2 M3 M4 57.2 92.8 () 91.6 () 92.0 () 92.2 () 73.4 95.6 () 95.8 () 95.2 () 95.6 () Table 3: Judgment accuracy in step-wise LLM-as-aJudge. We report the results of models with different parameter sizes. Additionally, we use arrows to indicate the changes in accuracy during the iterations. 500 samples (3.2). We report the results in Table 3. As shown in the table, LLMs achieve strong reward model performance after initialization with small amount of EFT data, which indicates the immense potential of LLMs for step-wise LLM-asa-Judge with CoT reasoning. Additionally, we can observe that, under the same conditions, the larger model exhibits stronger capabilities as reward model than the smaller one. Additionally, although we mix EFT data and IFT data for initialization and introduce no additional LLM-as-a-Judge data during subsequent iterations, the LLMs capabilities to perform LLM-as-a-Judge as reward model are still good. Furthermore, consistent pattern is observed across different models where evaluation accuracy initially increases, then decreases, and finally rises again. Based on the analysis above, initially, LLMs gain strong evaluation capabilities through training on EFT data. And there is temporary decline (but very slight) due to training on mathematical data. Ultimately, as the models mathematical abilities improve, its ability to evaluate mathematical reasoning steps also increases. Data Distribution Analysis. Following Yuan et al. (2024), we also analyze the distribution of different data. We utilize Bert (Devlin, 2018) for embedding and t-SNE (Van der Maaten and Hinton, 2008) based on the implementation of Poliˇcar et al. (2024) for visualization. We present the results in Figure 2. For prompts, the distributions of 7 (a) Prompt Distributions (b) Response Distributions Figure 2: The data distribution of prompts and responses in EFT (red), IFT (blue) and PPD (grey) data."
        },
        {
            "title": "Iterations",
            "content": "M1 M2 M3 M"
        },
        {
            "title": "Strategy Greedy Search",
            "content": "Test-time Scaling GSM8k MATH GSM8k MATH 5.89 5.55 5.10 4.87 8.41 7.64 6.30 5.54 47.79 51.19 57.75 62.86 61.00 67.17 80.46 96. M1 M4 55.9 60.6 58.2 62.4 Table 5: The average results of 72B model on all benchmarks using greedy search or test-time scaling. The full results are reported in Table 9. Table 4: Statistics of step number and step length on GSM8k and MATH benchmarks based on 72B models. The full results are reported in Appendix A. EFT data and IFT data do not overlap, allowing the model to distinctly learn two different task patterns. For models responses, we can find the similar phenomenon that the distribution of PPD and IFT responses is distinct from EFTs, which reduces the mutual interference between LLMs two capabilities during iteration. This allows the models ability to perform LLM-as-a-Judge to improve alongside its mathematical ability finally, without being overly influenced by the training data itself. Step Number and Length of Responses. Stepby-step reasoning is important for LLMs to solve complex reasoning tasks. Therefore, we conduct statistical analysis on the reasoning steps during iterations. As shown in Table 4, for the same model, more difficult problems require more reasoning steps and longer step lengths. As the iterations progress, the step number across different tasks decreases, while the length of each step increases. This indicates that performing Processbased Self-Rewarding encourages the model to generate longer and higher-quality single reasoning steps, which helps to reach final answers with fewer steps. Additionally, this behavior is also related to LLMs preferences when performing LLM-as-a8 Judge evaluations. More results are in Appendix A. Test-time Scaling with Process-based SelfRewarding Language Models. In the test-time scaling, LLMs conduct step search and select based on the rewards from PRM. Although we dont primarily focus on the test-time scaling performance in our work, LLMs in the Process-based SelfRewarding paradigm naturally have the ability to perform test-time scaling based on self-rewarding. We perform 6 generations for each step with the temperature of 0.5 and select the best one. The results we report in Table 5 indicate that the model achieves better performance through test-time scaling compared to generating directly. Additionally, the models performance with test-time scaling improves after iterations from M1 to M4, which corresponds to the uptrend of the models mathematical abilities and LLM-as-a-Judge capabilities."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose novel paradigm, Process-based SelfRewarding Language Models, that enables LLMs to perform step-by-step long-thought mathematical reasoning and step-wise LLM-as-a-Judge simultaneously. Given the characteristics of complex math reasoning tasks, we introduce the step-by-step reasoning, step-wise LLM-as-a-Judge and step-wise preference optimization technique into the framework. Our results indicate that Process-based SelfRewarding algorithm outperforms the original SelfRewarding on variety of complex mathematical reasoning tasks, showing potential of stronger reasoning ability better than human in the future."
        },
        {
            "title": "7 Limitations",
            "content": "We aim to draw more attention to the study of adapting the self-rewarding paradigm to the complex mathematical reasoning tasks, which allows for the possibility of continual improvement beyond the human preferences. Although our new Processbased Self-Rewarding algorithm has shown effective improvements across different mathematical reasoning tasks, there are still some limitations waiting for further research. Although we successfully enable the model to perform effective stepwise LLM-as-a-Judge with small amount of EFT data, the basic capabilities of initialized M1 model directly influence the effectiveness of subsequent process-based self-rewarding. Utilizing more highquality data to initialize LLMs more adequately may lead to stronger performance. Additionally, due to the limited resources, we only conduct the process-based self-rewarding experiments from M1 to M4. Building on this, conducting experiments with more iterations to explore the impact of iteration count on LLMs performance can help us better understand and utilize the process-based self-rewarding method."
        },
        {
            "title": "References",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. 2024. Step-level value preference optimization for mathematical reasoning. arXiv preprint arXiv:2406.10858. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Jacob Devlin. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2022. Complexity-based prompting for multi-step reasoning. In The Eleventh International Conference on Learning Representations. Leo Gao, John Schulman, and Jacob Hilton. 2023. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR. Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. 2024. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. Jiwoo Hong, Noah Lee, and James Thorne. 2024. Orpo: Monolithic preference optimization without reference model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1117011189. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. arXiv preprint 2024. Openai o1 system card. arXiv:2412.16720. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. 2024. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. 2024. Rewardbench: Evaluating reward arXiv preprint models for language modeling. arXiv:2403.13787. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann 9 Fleureau, Guillaume Lample, and Stanislas Polu. 2024. Numinamath. [https://huggingface.co/ AI-MO/NuminaMath-CoT](https://github.com/ project-numina/aimo-progress-prize/blob/ main/report/numina_dataset.pdf). Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023. Generative arXiv preprint judge for evaluating alignment. arXiv:2310.05470. Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan. 2024. Mario: Math reasoning with code interpreter outputa reproducible pipeline. arXiv preprint arXiv:2401.08190. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel Collier. 2024. Aligning with human judgement: The role of pairwise preference in large language model evaluators. arXiv preprint arXiv:2403.16950. source framework for advanced reasoning with large language models. arXiv preprint arXiv:2410.09671. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. 2024b. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. 2024. Meta-rewarding language models: Self-improving alignment with llm-as-ameta-judge. arXiv preprint arXiv:2407.19594. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. 2024. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712. Adian Liusie, Potsawee Manakul, and Mark JF Gales. 2023. Zero-shot nlg evaluation through pairware comparisons with llms. arXiv preprint arXiv:2307.07889. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. 2024a. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization arXiv preprint 2024. with reference-free reward. arXiv:2405.14734. Pavlin G. Poliˇcar, Martin Stražar, and Blaž Zupan. 2024. opentsne: modular python library for t-sne dimensionality reduction and embedding. Journal of Statistical Software, 109(3):130. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(11). Jun Wang, Meng Fang, Ziyu Wan, Muning Wen, Jiachen Zhu, Anjie Liu, Ziqin Gong, Yan Song, Lei Chen, Lionel Ni, et al. 2024a. Openr: An open 10 An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. 2024b. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36. Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel Deutch, and Jonathan Berant. 2023. Answering questions by meta-reasoning over multiple chains of thought. arXiv preprint arXiv:2304.13007. Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. arXiv 2024. Self-rewarding language models. preprint arXiv:2401.10020. Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: Rank responses to align language models with arXiv preprint human feedback without tears. arXiv:2304.05302. Shimao Zhang, Yu Bao, and Shujian Huang. 2024. Edt: Improving large language models generation by entropy-based dynamic temperature sampling. arXiv preprint arXiv:2403.14541. Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought promptarXiv preprint ing in large language models. arXiv:2210.03493. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623."
        },
        {
            "title": "Statistics",
            "content": "We report the full results of step number and step length across all benchmarks on the 7B and 72B models here. The 7B results are reported in Table 7. And the 72B results are reported in Table 8. Solution Scoring v.s. Step-wise Pairwise"
        },
        {
            "title": "Comparison",
            "content": "We evaluate the GPT-4os (Hurst et al., 2024) consistency and agreement with humans on two different LLM-as-a-Judge strategies for complex mathematical reasoning tasks, including assigning scores to the answers and performing pairwise comparison between two individual reasoning steps. We report the results in Table 6. Our results indicate that for the complex mathematical reasoning task, stepwise pairwise comparison has better consistency and agreement with humans than solution scoring. It is highly challenging for LLMs to assign proper and steady score to complex long-thought multistep solution."
        },
        {
            "title": "C Prompt Templates",
            "content": "We list the prompt templates we used in our work here. The prompt we use for constructing step-bystep formatted reasoning is shown in Figure 3. And the prompts we used for step-by-step long-thought mathematical reasoning and step-wise LLM-as-aJudge are shown in Figure 4 and Figure 5 respectively."
        },
        {
            "title": "Consistency Agreement",
            "content": "Step-wise Pairwise Comparison Solution Scoring 0.84 0.72 0.88 0.32 Table 6: The consistency and agreement with human evaluation of step-wise pairwise comparison and solution scoring."
        },
        {
            "title": "Step Num",
            "content": "GSM8k MATH Gaokao2023En OlympiadBench AIME2024 AMC2023 M1 M2 M3 M4 5.91 5.24 4.50 4.09 9.35 8.03 6.43 5.21 8.68 7.43 5.84 5.11 11.75 9.54 7.36 6. 7.97 7.03 7.13 6.4 11.18 9.85 6.9 5.53 Step Length GSM8k MATH Gaokao2023En OlympiadBench AIME2024 AMC2023 M1 M2 M3 M4 48.59 54.02 63.36 73.64 61.61 70.04 89.68 113. 69.74 85.26 99.59 118.02 103.95 108.26 127.97 142.69 100.43 114.27 118.67 138.18 76.13 115.29 109.45 127.18 Table 7: Statistics of step number and step length on different methematical benchmarks based on 7B models."
        },
        {
            "title": "Step Num",
            "content": "GSM8k MATH Gaokao2023En OlympiadBench AIME2024 AMC2023 M1 M2 M3 M4 5.89 5.55 5.10 4.87 8.41 7.64 6.30 5.54 8.34 7.34 5.99 5.36 10.21 9.05 6.54 5. 8.23 7.37 7.07 6.33 9.95 9.75 6.55 6.1 Step Length GSM8k MATH Gaokao2023En OlympiadBench AIME2024 AMC2023 M1 M2 M3 M4 47.79 51.19 57.75 62.86 61.00 67.17 80.46 96. 69.72 78.00 91.21 106.28 95.38 101.93 122.53 134.62 104.97 118.08 118.61 133.66 79.36 86.88 108.95 113.60 Table 8: Statistics of step number and step length on different methematical benchmarks based on 72B models."
        },
        {
            "title": "Setting",
            "content": "GSM8k MATH Gaokao2023En OlympiadBench AIME2024 AMC2023 M1 Greedy Search M4 Greedy Search M1 Test-time Scaling M4 Test-time Scaling 92.6 93.7 94.5 94.5 76.0 76.6 79.1 79.3 66.2 68.1 64.9 68.3 41.8 44.1 41.6 43. 13.3 23.3 16.7 23.3 45.0 57.5 52.5 65.0 Table 9: The full results of greedy search and test-time scaling on 72B model. 12 Figure 3: The prompt for converting the the given solution into step-by-step format logically without altering any information in the original solution. Figure 4: The prompt for LLMs conducting step-by-step long-thought reasoning. 13 Figure 5: The prompt for LLMs conducting step-wise LLM-as-a-Judge. We create this prompt template following the basic pattern of Zheng et al. (2023)."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "National Key Laboratory for Novel Software Technology, Nanjing University",
        "The University of Manchester"
    ]
}