{
    "paper_title": "Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion",
    "authors": [
        "Shengyuan Zhang",
        "An Zhao",
        "Ling Yang",
        "Zejian Li",
        "Chenye Meng",
        "Haoran Xu",
        "Tianrun Chen",
        "AnYang Wei",
        "Perry Pengyun GU",
        "Lingyun Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed $\\textbf{ScoreLiDAR}$, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel $\\textbf{Structural Loss}$, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame ($>$5$\\times$) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 5 1 5 3 0 . 2 1 4 2 : r Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion Shengyuan Zhang1 Haoran Xu An Zhao1 Tianrun Chen1 Ling Yang2 Zejian Li1 Chenye Meng1 AnYang Wei3 Perry Pengyun GU Lingyun Sun1 1 Zhejiang University 2 Peking University 3 Zhejiang Green Zhixing Technology co., ltd 1 {zhangshengyuan,zhaoan040113,zejianlee,mengcy,tianrun.chen,sunly}@zju.edu.cn 2 {yangling0818}@163.com 3 {Haoran.Xu5,weianyang,gupengyun}@geely.com Figure 1. demonstration of the LiDAR scene completion examples. Given sparse LiDAR scan in (a), the model aims to recover the ground-truth dense scene as in (b). In these examples, scans are from SemanticKITTI [1] and KITTI360 [17] dataset. In both cases, LiDiff [24], SOTA LiDAR scene completion method, requires about 30 seconds as in (c). In comparison, our proposed ScoreLiDAR takes only about 5 seconds in (d), achieving over 5x speedup with improved completion quality indicated by lower Chamfer Distance (CD)."
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes novel distillation method tailored for 3D LiDAR scene completion models, dubbed ScoreLiDAR, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce novel Structural Loss, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains scenewise term constraining the holistic structure and pointwise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame (>5) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available on https://github.com/ happyw1nd/ScoreLiDAR. acceleration methods may lead to degraded local details and reduced realism in the completed scene. In this work, we propose ScoreLiDAR, novel distillation method tailored for 3D LiDAR scene completion diffusion models, which enables efficient and high-quality scene completion. Variational Score Distillation (VSD) [37] uses pre-trained diffusion model to calculate distribution matching loss for training student model, which has achieved impressive results. Inspired by this, ScoreLiDAR adapts and expands VSD for effective distillation of the pre-trained 3D LiDAR scene completion diffusion model. Moreover, we introduce Structural Loss to ensure the stability of training and improve the final performance. The structural loss contains scene-wise term constraining the holistic structure and point-wise term constraining the key landmark points and their relative configuration, which helps the student model to capture the geometric structure information of 3D LiDAR scenes and achieve high-quality completion. We compared the proposed ScoreLiDAR with the state-of-the-art (SOTA) LiDAR scene completion models. Extensive experiments demonstrate that ScoreLiDAR can effectively accelerate the sampling speed of LiDAR scene completion diffusion models while achieving optimal scene completion quality, as shown in Fig. 1 and Fig. 2. Our contribution can be summarized as follows: (1) We propose ScoreLiDAR, novel distillation method tailored for diffusion-based 3D LiDAR scene completion models, which achieves efficient scene completion. (2) We introduce Structural Loss to effectively capture the geometric structure information of 3D point clouds during the distillation process, which ensures high-quality scene completion. (3) Extensive experiments show that ScoreLiDAR enables fast and efficient scene completion while achieving optimal generation quality compared to the existing models. 2. Related work 3D LiDAR Scene completion 3D LiDAR scene completion refers to recovering complete scene from sparse, incomplete LiDAR scan in applications such as autonomous driving [34, 41]. Current mainstream LiDAR scene completion methods include depth completion-based and Signed Distance Field (SDF)-based approaches. Depth completion-based methods aim to recover dense depth maps from sparse depth measurements [8, 38, 42]. These methods typically leverage deep learning techniques [4, 7] and can also incorporate guidance from RGB images to achieve higher-quality completion results [28, 47, 49]. SDF-based methods represent scenes as voxel grids, with the core idea of using signed distance fields to complete sparse LiDAR scene [14, 34]. These methods are constrained by voxel resolution, making them prone to losing details within the scene [6, 24]. In addition, some methods introduce semantic information to enhance LiDAR scene completion [27, 39]. Figure 2. visualization of LiDAR scene completion performances with different models on SemanticKITTI [1] dataset. Our proposed ScoreLiDAR with 8 sampling steps performs better than LiDiff [24] with 50 steps, as shown by lower Chamfer Distance yet with less time cost. Generally, ScoreLiDAR achieves better scene completion performance and speed trade-off. 1. Introduction Recognizing the surrounding environment accurately and efficiently using onboard sensors is crucial for the safe operation of autonomous vehicles [15, 16]. Among different types of sensors, 3D LiDAR has become one of the most widely adopted sensors due to its broader detection range and higher detection precision [21, 24]. However, driving scenarios are often complex, and the 3D point clouds collected by LiDAR are typically sparse, particularly in occluded areas [14, 34]. This sparsity causes decline in the ability to understand 3D scenes [3, 24]. Thus, inferring and completing sparse 3D LiDAR scenes is necessary to provide dense and more comprehensive scene representation. Due to the advantages of strong training stability and high-generation quality, existing works utilize diffusion models to complete the 3D LiDAR scenes and achieve outstanding results [23, 24]. However, the diffusion model often requires multiple network iterations to obtain dense, complete, and high-quality LiDAR scene, which is timeconsuming [12, 50]. Autonomous vehicles require fast and efficient perception and recognition of surrounding environments, so the slow sampling speed limits the practical application of diffusion models. Although existing works have proposed acceleration methods for diffusion models [19, 29, 33, 36, 46], 3D LiDAR scenes contain complex geometric structure information, directly applying existing These methods can generate dense and complete scenes while providing semantic labels for each point, leading to broader application potential [35, 43]. Diffusino-based 3D LiDAR scene completion Due to the strong training stability and high generation quality of diffusion models, many methods leverage diffusion models for LiDAR scene completion tasks [3, 13, 2325]. The work of Lee et al. [13] is the first to apply diffusion models at the scene scale for LiDAR scene completion, enabling the generation of realistic scenes conditioned on partial observations from sparse point clouds. Similarly, R2DM [23] utilizes diffusion models based on distance and reflectance intensity image representations to generate various highfidelity 3D LiDAR scenes. LiDiff [24] indicates that adding noise to point cloud data at the scene scale leads to loss of detail. Therefore, LiDiff proposes operating directly on individual points and redefines the noise schedule and denoising processes to generate scenes with richer detail. Based on LiDiff, DiffSSC [3] further performs semantic scene completion tasks by implementing denoising and segmentation separately in both the point and semantic spaces. Moreover, LiDMs [25] constructs the pipeline from the perspectives of modal realism, geometric realism, and object realism, achieving generation under different conditions. However, due to the inherently slow sampling process of diffusion models, the inference of these diffusion-based 3D LiDAR scene completion models is relatively slow. This limitation makes it challenging to achieve fast and efficient perception as required in autonomous vehicle applications. 3. Preliminary 3.1. Brief introduction of diffusion models The diffusion models have two processes: forward diffusion and reverse denoising process [9, 30]. In the forward diffusion process, given the data x0 q(x) from the training distribution, the diffusion model adds different scales of noise to x0 according to different timesteps [1, ] to obtain noisy data {x1, x2, . . . , xT }. When is large enough, xT approaches to standard Gaussian distribution, namely, q(xT ) (0, I). This process is parameterized by series of predefined noise factors βt. By defining αt = 1 βt, the diffusion process is expressed as [9]: xt = αtx0 + 1 αtϵt (1) Here αt = (cid:81)T t=1 αt, p(xt x0) = ( αt, (1 αt)I). During the training, the diffusion model tries to predict the added noise at different timesteps t. Given the input x0 and the condition (optional), the noisy data xt can be calculated by Eq. (1). The diffusion model ϵθ predicts the noise according to xt, c, and is then optimized by calculating the ℓ2 loss between the predicted and the real noise. LDM = Et,ϵ (cid:2)ϵt ϵθ(xt, c, t)2(cid:3) (2) Here θ is the trainable parameter of ϵθ. In the reverse denoising process, the diffusion model starts from the timestep and progressively removes the predicted noise until generated sample is obtained. The process of denoising xt to obtain xt1 can be written as: xt1 = (cid:18) xt 1 αt 1 αt 1 αt (cid:19) (cid:0)xt, c, t(cid:1) ϵθ + σtz (3) Here (0, I). In this process, the number of required inference steps varies depending on different sampling methods. For instance, DDPM [9] requires 1000 timesteps, while DDIM [30] and DPM solver [18] can reduce this to 100 timesteps and 20 timesteps, respectively. 3.2. 3D LiDAR scene completion diffusion models The 3D LiDAR scene completion diffusion models take the incomplete scan = {p1, p2, ..., pN } and try to generate the complete scene G0 = {p0 1, p0 }. Given the input LiDAR scan and ground truth G, diffusion model can be trained to perform 3D LiDAR scene completion. The noisy scene Gt at timestep is calculated from the ground truth at point level [3, 24], 2, ..., p0 pt = αtpm + 1 αtϵt, pm (4) 1, pt 2, ..., pt Here Gt = {pt }. Because the LiDAR point cloud is sparse, the noisy data retains very little information about the original data. To generate more realistic point cloud scenes, the LiDAR scan can be used as condition of the diffusion model [24]. In this case, the training loss of the diffusion model is given by: LDM = Et,ϵ (cid:104)(cid:13) (cid:13)ϵ ϵθ (cid:0)Gt, P, t(cid:1)(cid:13) (cid:13) 2(cid:105) (5) Then, as described in Sec. 3.1, the completed scene G0 can be generated by progressive denoising from GT . Because the scale of the LiDAR scene is large and the data range is different across different point cloud axes, directly normalizing the entire dataset compresses the data into smaller range, which potentially leads to the loss of critical details [3, 24]. To solve this issue, LiDiff [24] modifies the diffusion process by adding local noise offset to each point pm, gradually perturbing the point cloud at each timestep. For Eq. (1), x0 is set to 0, and xt is added to each point pm, αt0 + = pm + (cid:0) pt 1 αtϵt(cid:1) = pm + 1 αtϵt (6) Due to this special case, GT cannot directly start from Inthe LiDAR scan is used to obtain GT [24]. standard Gaussian noise in the sampling process. stead, Firstly, given the initial incomplete scan P, the number of the point clouds is increased by duplicating the original points times and getting the pseudo dense scan = {p }, where we assume = KN . Then, we calculate the noisy point cloud by Eq. (6). As is noisy enough, it can be regarded as GT during the training. After that, step-by-step denoising process is applied to obtain the completed scene G0. 2, . . . , 1, 4. Method Our goal is to distill pre-trained 3D LiDAR scene completion diffusion model into student model with significantly fewer sampling steps, enabling efficient and highquality scene completion. Firstly, we introduce the distillation method tailored for 3D LiDAR scene completion diffusion models in Sec. 4.1. Then, we introduce the structural loss to improve the distillation process with both scene-wise loss and point-wise loss in Sec. 4.2. Finally, we describe the optimization procedure of ScoreLiDAR in Sec. 4.3. 4.1. Distillation for 3D LiDAR scene completion Ideally, the final student model would achieve completion results comparable to, or even better than, that of the teacher model at faster speed. In the 3D LiDAR scene completion scenario, let q0 be the distribution of the ground truth G, and ϵθ be the pre-trained scene completion diffusion model whose multi-step generated distribution approximates q0. Let Gstu be the student model that can perform efficient LiDAR scene completion with the generated distribution p0 G. Inspired by VSD [37], ScoreLiDAR minimizes the KL divergence between the distribution of the teacher model and the generated distribution of the student model [37, 48]. min η DKL (cid:0)p0 (cid:0)G0; η(cid:1) q0 (cid:0)G0(cid:1)(cid:1) (7) Here G0 is the completed scene generated by the student model Gstu condition on P, for simplicity, we omit when representing the distribution, and η is the trainable parameter of Gstu . However, the high-density regions of q0 are sparse in the data space, so it is hard to directly solve Eq. (7). Wang et al. [37] expand the optimization problems in Eq. (7) by minimizing the KL divergence between two distributions at different noise levels as: (cid:0)Gt(cid:1) qt (cid:0)Gt(cid:1)(cid:1) LKL = Et,ϵDKL (cid:0)pt (8) min η 1, pt Here is the timestep controlling the noise level, ϵ is random 2, . . . , pt noise, and Gt = {pt } is the noisy version of the completed scene G0 at timestep t. The gradient of Gstu in Eq. (8) is approximated by (cid:0)pt (cid:0)Gt(cid:1) qt (cid:0)Gt(cid:1)(cid:1) ηDKL We use the pre-trained diffusion model ϵθ to approximate Gt log qt (Gt) with Gt log qt (Gt) ˆϵ 1 αt [32]. Similarly, the score Gt log pt (Gt) can be approximated by another auxiliary diffusion model ϵϕ. Then, with the simplification as in [9], the LKL is estimated by LKL Et,ϵ (cid:2)ϵθ (cid:0)Gt, P, t(cid:1) ϵϕ (cid:0)Gt, P, t(cid:1) 2 2 (cid:3) (10) Thus, the gradient in Eq. (9) is approximated by ηDKL (cid:0)pt (cid:0)Gt(cid:1) qt (cid:0)Gt(cid:1)(cid:1) Et,ϵ (cid:2)ϵθ (cid:0)Gt, P, t(cid:1) ϵϕ (cid:0)Gt, P, t(cid:1)(cid:3) Gt η (11) The detailed derivation is in Appendix E.1. We parameterize ϵϕ by either small U-Net or low-rank adapter [10] of the teacher model ϵθ. During the distillation, the student model Gstu and ϵϕ are optimized alternately. The auxiliary diffusion model ϵϕ is independently trained with the denoising loss Eq. (5) but the training samples are replaced with generated samples G0. 4.2. Structural loss Although the distillation process in Sec. 4.1 is highly effective in training models [19, 50], we found that directly applying it to LiDAR scene completion diffusion models leads to loss of local details and reduced realism in the completed scene. This is because the point cloud in LiDAR scenes often includes complex geometric information that is not explicitly captured by diffusion models. Thus, we introduce structural loss to further refine the distillation process and improve the completion quality. This structural loss includes scene-wise loss and point-wise loss and can help the student model effectively capture geometric structure information of the 3D point clouds. Scene-wise loss. In the distillation process mentioned in Sec. 4.1, the gradient ηDKL in Eq. (11) is well-defined when 0, i.e. the generated samples are totally disturbed by Gaussian noise. However, ηDKL becomes unreliable when is small [48, 50]. This is because the student model often generates subpar results at the early stage due to the complexity of the point cloud data. It is easy for the noisy generated samples to lie outside the training distribution of the teacher model, causing the unreliable network prediction of the teacher model [48, 50]. To solve this issue, we introduce the scene-wise loss, which minimizes the distance between the ground truth scene and the completed scene G0. Concretely, we calculate the scene-wise loss by Lscene = Et,ϵ (cid:2)G G02 2 (cid:3) (12) = Et,ϵ (cid:2)Gt log pt (cid:0)Gt(cid:1) Gt log qt (cid:0)Gt(cid:1)(cid:3) Gt η (9) This loss calculates the mean squared error between each point in the generated scene and its closest corresponding Figure 3. The overall structure of ScoreLiDAR. (1) The student model generates the completed scene based on the sparse scan. (2) The sparse scan and noisy completed scene are input to ϵθ and ϵϕ. (3) The predicted score of ϵθ and ϵϕ are used to calculated the KL divergence. (4) Structural loss is calculated based on the completed scene and the ground truth. (5) The total loss is calculated with KL divergence and structural loss. (6) The student model is optimized according to the total loss. (7) The diffusion model ϵϕ is updated with the completed scene. point in the ground truth. It helps the student model capture the holistic structure, which prevents the optimization direction from deviating in the early stages and enhances training stability. The scene-wise loss also enables the generated scenes to be closer to the ground truth globally, thereby enhancing the completion quality and fidelity. Point-wise loss. As seen in Eq. (11), the distillation process only constrains the overall distribution of the completed scene, ignoring the relative positions between different points. Directly using the gradient in Eq. (11) to optimize the student model may lead to loss of local details. Thus, we introduce the point-wise loss to capture the relative structural information between different points in the 3D LiDAR scene. The point-wise loss calculates the difference between the inter-point distance matrices of the completed scene and the ground truth. Due to the large number of points in the scene, calculating the distance matrix for all points is computationally intensive. Therefore, we select key points to compute the distance matrix with . Based on the local geometric features of each point, we choose key points that are critical for representing the structure of the 3D LiDAR scene. For each point p0 in the completed scene G0, we find its K-nearest neighbor, denoted as the set Ki. Then we select the key points by calculating their curvature κi. The specific steps are as follows: Calculate the centroid p0 of the neighborhood Ki i = 1 (cid:88) p0 p0 Ki (13) Calculate the neighborhood covariance matrix Ci for p0 Ci = 1 (cid:88) j Ki (p0 p0 )(p0 p0 )T (14) Perform eigen-decomposition on the covariance matrix Ci to obtain the eigenvalues λ1 < λ2 < < λm. Curvature κi can be calculated using the eigenvalues κi = λ1 (cid:80)m j=1 λj (15) larger curvature κi indicates greater local shape variation. Those points with great local variation are typically located at corners, edges, or endpoints, which tend to shape the main structure of the scene. Therefore, the top points with the highest curvature values are selected as key points. Given the ground truth = {p1, p2, ..., pM }, we select key points (n ) from the point cloud to construct the distance matrix D. The dij represents the Euclidean distance between the point and j. Then, for completed scene G0, we select points that are closest to the key points in as the corresponding key points to obtain the distance matrix DG. Thus, the point-wise loss is calculated by Lpoint = Et,ϵ (cid:2)D DG2 2 (cid:3) (16) The point-wise loss can help the student model capture the relative configuration of key points and further enhance the geometric accuracy and detail retention of the completed scene. This ensures that key objects like cars, traffic cones, and walls are better completed, which is crucial for autonomous vehicles to recognize surroundings accurately. CD JSD Times (s) Model SemanticKITTI JSD CD KITTI360 CD JSD Model LMSCNet [26] LODE [14] MID [34] PVD [51] LiDiff [24] LiDiff (Refined) [24] ScoreLiDAR ScoreLiDAR (Refined) 0.641 1.029 0.503 1.256 0.434 0. 0.406 0.342 0.431 0.451 0.470 0.498 0.444 0.416 0.425 0.399 0.40 1.76 6.42 - 30.38 30.55 5.16 5.37 Table 1. The completion performance on SemanticKITTI dataset. Colors denote the 1st , 2nd , and 3rd best-performing model. indicates that the sampling time is estimated based on the official code and the provided checkpoints."
        },
        {
            "title": "Model",
            "content": "CD JSD Times (s) LMSCNet [26] LODE [14] MID [34] LiDiff [24] LiDiff (Refined) [24] ScoreLiDAR ScoreLiDAR (Refined) 0.979 1.565 0.637 0.564 0.517 0.472 0. 0.496 0.483 0.476 0.459 0.446 0.444 0.437 - - - 29.18 29.43 4.98 5.14 Table 2. The completion performance on the KITTI-360 dataset. The meaning of notations is the same as those in Tab. 1. ScoreLiDAR (Refined) w/o Structural Loss 0.342 0.419 0.399 0.430 0.452 0.549 0.437 0.445 Table 3. Ablation study of the structural loss. Model CD JSD Time (s) LiDiff (50 steps) [24] LiDiff (50 steps Refined) [24] LiDiff (8 steps) [24] LiDiff (8 steps Refined) [24] ScoreLiDAR (8 Steps Refined) ScoreLiDAR (4 Steps Refined) ScoreLiDAR (2 Steps Refined) ScoreLiDAR (1 Steps Refined) 0.434 0.375 0.447 0.411 0.342 0.326 0.403 0. 0.444 0.416 0.432 0.406 0.399 0.386 0.379 0.384 30.38 30.55 5.69 5.92 5.37 3.23 1.85 1.10 Table 4. Ablation study of different sampling steps on the SemanticKITTI dataset. pare ScoreLiDAR with advanced models including LMSCNet [26], LODE [14], MID [34], PVD [51] and LiDiff [24]. We first evaluate the performance of ScoreLiDAR in scene completion tasks (Sec. 5.1). Secondly, we present the results of ablation studies showing the effectiveness of the structural loss and the performances of ScoreLiDAR given different sampling steps (Sec. 5.2). Finally, we further evaluate ScoreLiDAR with the qualitative analysis ( Sec. 5.3). Structural loss. The final structural loss Gstu is 5.1. Scene completion Lstructural = λsceneLscene + λpointLpoint (17) Here λscene and λpoint are the weight of scene-wise loss and point-wise loss. 4.3. Optimization procedure During the training, the student model Gstu and ϵϕ are optimized alternately. The auxiliary diffusion model ϵϕ is trained on the completed scene of the student model with Eq. (5). As for Gstu , we follow the proposed method to select 1/10 of the points from the entire point cloud as key points for calculating the point distance matrix. Then, Gstu is optimized with the following objective Lstu = LKL + Lstructural (18) We set λscene = 0.5 and λpoint = 0.01 unless otherwise specified. The implementation details are provided in Appendix A.3. 5. Experiment In this part, we conduct series of experiments to evaluate the effectiveness of the proposed ScoreLiDAR. We comWe validate ScoreLiDAR on SemanticKITTI [1] and KITTI-360 [17] datasets. The existing SOTA LiDAR scene completion model LiDiff [24] is chosen as the teacher model. The student model shares the network architecture with the teacher model and is initialized by the teacher model. Moreover, we also use the refinement network in LiDiff [24] to refine the completed scene generated by the student model. We calculate the Chamfer Distance (CD) and the Jensen-Shannon Divergence (JSD) to evaluate the similarity between the completed scene and the ground truth. The smaller the value of CD and JSD, the closer the completed scene is to the ground truth. Tab. 1 shows the quantitative results on SemanticKITTI. ScoreLiDAR achieves the optimal performance compared to the existing models on both metrics. Compared to the SOTA method LiDiff [24] with refinement, which takes 30.55 seconds to complete scene, our proposed ScoreLiDAR completes scene in just 5.47 seconds (fivefold speedup) yet with 8% improvement in CD and 4% in JSD. Although LMSCNet [26] and LODE [14] have faster completion speeds, their completion quality is significantly lower. Notably, the performance of ScoreLiDAR outperFigure 4. Qualitative results on KITTI-360. ScoreLiDAR achieves better completion than LiDiff [25] with fewer sampling steps. forms the teacher model LiDiff [24]. This is because ScoreLiDAR introduces structural loss with scene-wise term and point-wise term, enabling the student model to effectively capture geometric structure information within LiDAR point cloud data during training. Results on KITTI360 are shown in Tab. 2. ScoreLiDAR also achieves optimal performance compared to existing models. ScoreLiDAR also boasts fivefold speedup with 12% improvement in CD and 2% in JSD compared to LiDiff [24]. 5.2. Ablation study In this part, we conduct the ablation study to verify the effectiveness of the structural loss in the training of the proposed ScoreLiDAR. We compared the scene completion performances of the proposed ScoreLiDAR with variant that does not incorporate structural loss. The results are shown in Tab. 3. The results show that the variant without structural loss exhibits lower performance in scene completion on both datasets. However, after considering the structural loss, the performance of ScoreLiDAR improves significantly, which achieves better performance on both metrics. This supports our discussion in Sec. 4.3, incorporating structural loss enables the student model to capture the geometric structure feature of 3D point clouds, thereby facilitating the effective distillation of the student model. Furthermore, we compared the scene completion performance of ScoreLiDAR with different sampling steps, and the results are shown in Tab. 4. It can be observed that as the sampling steps decrease from 8 to 1, the time required for ScoreLiDAR to complete scene also decreases, with single-step sampling allowing scene to be completed in only 1.1 seconds. With 8-step and 4-step sampling, ScoreLiDAR performs better on both metrics than LiDiff. Both metrics decay at 2-step and 1-step sampling, but in JSD ours still performs better than LiDiff. In summary, although the quality of scene completion decreases as the sampling steps are reduced, it still maintains performance comparable to or better than the existing model, achieving better performance and speed trade-off as in Fig. 2. 5.3. Qualitative analysis Fig. 4 shows the completed scenes by our proposed ScoreLiDAR and LiDiff [24] on KITTI-360. ScoreLiDAR achieves completion results with higher quality and greater fidelity. We can see that LiDiff [24] nearly fails at 8 steps. Although LiDiff [24] achieves decent completion at 50 steps, there are some missing areas on the lower and right In contrast, ScoreLiDAR reaches optimal complesides. tion including clearer objects such as cars, traffic cones, and other scene elements with only 8 steps, which is closer to Figure 5. The qualitative analysis of structural loss. The bar chart shows the distribution of distances between corresponding points in the completed and ground truth scenes. higher number of points with smaller distances demonstrates that the completed scene is closer to the ground truth. The heatmap represents the difference in distance matrices between the completed scene and the ground truth scene. Smaller values on the heatmap indicate that the completed scene is closer to the ground truth. the ground truth, especially after refinement. To further demonstrate the effectiveness of ScoreLiDAR and the structural loss, we calculate the distance between the points in the completed scene and their corresponding points in the ground truth to evaluate the overall difference. We display the calculated results in bar chat in Fig. 5. ScoreLiDAR has the highest number of points with smaller distances to their corresponding points in the ground truth. The results show that the scenes completed by ScoreLiDAR are closer to the ground truth overall, demonstrating higher fidelity. Moreover, we selected 36 corresponding key points from the ground truth and the completed scene using the method described in Sec. 4.2 and calculated the point distance matrices and DG. We then visualized the difference between and DG as heatmap. As shown in Fig. 5, on both datasets, the difference of point distance matrix between the completed scene of LiDiff [24] and the ground truth is the largest, followed by the ScoreLiDAR variant without the structural loss and the smallest difference is achieved by ScoreLiDAR. This also indicates that the scene completed by ScoreLiDAR is closer to the ground truth. We also conduct user study to evaluate the performance of ScoreLiDAR. We used ScoreLiDAR and LiDiff [24] to complete scenes based on the same input scans and asked users to choose the scene they believed was closer to the ground truth. ScoreLiDAR received 65% user preference over LiDiff [24]. This indicates that the detail and fidelity of the scenes completed by ScoreLiDAR more closely resemble the ground truth for most users. The details of the user study are shown in Appendix D.4. 6. Conclusion Summary. This paper proposes ScoreLiDAR, novel distillation method tailored for 3D LiDAR scene completion diffusion models. The distilled model enables efficient LiDAR scene completion. By introducing the structural loss with scene-wise term and point-wise term, ScoreLiDAR trains the student model to effectively capture the holistic structure and the relative configuration of key points and achieve efficient and high-quality scene completion. Limitations. While ScoreLiDAR achieves efficient, highquality LiDAR scene completion, its performance is constrained by the teacher model. As the performance of the teacher model improves, so does the capability of the student model. Moreover, the ability to perform semantic scene completion is also determined by the teacher model. If the teacher model can complete the semantic scene, the distilled student model will also be able to follow the teacher. Thus, further exploration is required to find more effective method to improve the training process [44, 45] of ScoreLiDAR and avoid the limitations of the teacher model, achieving more efficient semantic LiDAR scene completion."
        },
        {
            "title": "References",
            "content": "[1] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Semantickitti: Dataset For Semantic Scene Understanding Of Lidar Sequences. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9297 9307, 2019. 1, 2, 6, 12 [2] Akmal Butt and Petros Maragos. Optimum Design of Chamfer Distance Transforms. IEEE Transactions on Image Processing, 7(10):14771484, 1998. 12 [3] Helin Cao and Sven Behnke. DiffSSC: Semantic LiDAR Scan Completion Using Denoising Diffusion Probabilistic Models. arXiv preprint arXiv:2409.18092, 2024. 2, 3 [4] Nathaniel Chodosh, Chaoyang Wang, and Simon Lucey. Deep Convolutional Compressed Sensing For Lidar Depth Completion. In Computer VisionACCV 2018: 14th Asian Conference on Computer Vision, Perth, Australia, December 26, 2018, Revised Selected Papers, Part 14, pages 499 513. Springer, 2019. 2 [5] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3075 3084, 2019. 17 [6] Angela Dai, Daniel Ritchie, Martin Bokeloh, Scott Reed, Jurgen Sturm, and Matthias Nießner. Scancomplete: LargeScale Scene Completion And Semantic Segmentation For In Proceedings of the IEEE/CVF Conference 3d Scans. on Computer Vision and Pattern Recognition, pages 4578 4587, 2018. 2 [7] Abdelrahman Eldesokey, Michael Felsberg, and Fahad Shahbaz Khan. Confidence Propagation Through CNNs For IEEE Transactions on Guided Sparse Depth Regression. Pattern Analysis and Machine Intelligence, 42(10):2423 2436, 2019. [8] Chen Fu, Christoph Mertz, and John Dolan. Lidar and Monocular Camera Fusion: On-road Depth CompleIn 2019 IEEE Intelligent tion For Autonomous Driving. Transportation Systems Conference (ITSC), pages 273278. IEEE, 2019. 2 [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 3, 4, 13 In Low-Rank Adaptation Of Large Language Models. International Conference on Learning Representations, 2021. 4, 17 [11] Jaehoon Jung, Michael Olsen, David Hurwitz, Alireza Kashani, and Kamilah Buker. 3D Virtual Intersection Sight Distance Analysis Using LiDAR Data. Transportation research part C: emerging technologies, 86:563579, 2018. 12 [12] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. In International Conference on Learning Representations, 2024. [13] Jumin Lee, Woobin Im, Sebin Lee, and Sung-Eui Yoon. Diffusion Probabilistic Models For Scene-Scale 3d Categorical Data. arXiv preprint arXiv:2301.00527, 2023. 3, 13 [14] Pengfei Li, Ruowen Zhao, Yongliang Shi, Hao Zhao, Jirui Yuan, Guyue Zhou, and Ya-Qin Zhang. Lode: Locally Conditioned Eikonal Implicit Scene Completion From Sparse Lidar. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 82698276. IEEE, 2023. 2, 6, 13, 15 [15] You Li, Julien Moreau, and Javier Ibanez-Guzman. EmerIEEE gent Visual Sensors For Autonomous Vehicles. Transactions on Intelligent Transportation Systems, 24(5): 47164737, 2023. 2, 12 [16] Yiming Li, Zhiding Yu, Christopher Choy, Chaowei Xiao, Jose Alvarez, Sanja Fidler, Chen Feng, and Anima Anandkumar. Voxformer: Sparse Voxel Transformer For CameraIn Proceedings of based 3d Semantic Scene Completion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90879098, 2023. 2, 12 [17] Yiyi Liao, Jun Xie, and Andreas Geiger. Kitti-360: Novel Dataset And Benchmarks For Urban Scene understanding in 2d And 3d. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):32923310, 2022. 1, 6, 12 [18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: Fast Ode Solver For Diffusion Probabilistic Model Sampling In Around 10 Steps. Advances in Neural Information Processing Systems, 35: 57755787, 2022. 3 [19] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-Instruct: Universal Approach for Transferring Knowledge From Pre-trained In Advances in Neural Information Diffusion Models. Processing Systems, page 7652576546, 2023. 2, [20] Marıa Luisa Menendez, JA Pardo, Pardo, and MC Pardo. Journal of the Franklin The Jensen-Shannon Divergence. Institute, 334(2):307318, 1997. 12 [21] Amir Meydani. State-of-the-Art Analysis Of The Performance Of The Sensors Utilized In Autonomous Vehicles In Extreme Conditions. In International Conference on Artificial Intelligence and Smart Vehicles, pages 137166. Springer, 2023. 2 [10] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: [22] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [23] Kazuto Nakashima and Ryo Kurazume. Lidar Data Synthesis With Denoising Diffusion Probabilistic Models. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1472414731. IEEE, 2024. 2, 3 [24] Lucas Nunes, Rodrigo Marcuzzi, Benedikt Mersch, Jens Behley, and Cyrill Stachniss. Scaling Diffusion Models To Real-World 3D LiDAR Scene Completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1477014780, 2024. 1, 2, 3, 6, 7, 8, 12, 13, 14, 15 [25] Haoxi Ran, Vitor Guizilini, and Yue Wang. Towards Realistic Scene Generation With LiDAR Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1473814748, 2024. 3, 7, 13 [26] Luis Roldao, Raoul de Charette, and Anne Verroust-Blondet. Lmscnet: Lightweight Multiscale 3d Semantic Completion. In 2020 International Conference on 3D Vision (3DV), pages 111119. IEEE, 2020. 6, 15 [27] Luis Roldao, Raoul De Charette, and Anne Verroust3D Semantic Scene Completion: Survey. Blondet. International Journal of Computer Vision, 130(8):1978 2005, 2022. 2 [28] Kwonyoung Ryu, Kang-il Lee, Jegyeong Cho, and Kuk-Jin Yoon. Scanline Resolution-Invariant Depth Completion Using Single Image And Sparse LiDAR Point Cloud. IEEE Robotics and Automation Letters, 6(4):69616968, 2021. 2 [29] Tim Salimans and Jonathan Ho. Progressive Distillation In International for Fast Sampling of Diffusion Models. Conference on Learning Representations, 2021. 2 [30] Jiaming Song, Chenlin Meng, and Stefano Ermon. DearXiv preprint noising Diffusion Implicit Models. arXiv:2010.02502, 2020. 3 [31] Shuran Song, Fisher Yu, Andy Zeng, Angel Chang, Manolis Savva, and Thomas Funkhouser. Semantic Scene ComIn Proceedings of pletion From Single Depth Image. the IEEE/CVF conference on computer vision and pattern recognition, pages 17461754, 2017. 16 [32] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. ScoreBased Generative Modeling through Stochastic Differential Equations. In International Conference on Learning Representations, 2021. 4 [33] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency Models. In International Conference on Machine Learning, pages 3221132252, 2023. [34] Ignacio Vizzo, Benedikt Mersch, Rodrigo Marcuzzi, Louis Wiesmann, Jens Behley, and Cyrill Stachniss. Make It Dense: Self-Supervised Geometric Scan Completion of Sparse 3d Lidar Scans In Large Outdoor Environments. IEEE Robotics and Automation Letters, 7(3):85348541, 2022. 2, 6, 13, 15 [35] Fengyun Wang, Dong Zhang, Hanwang Zhang, Jinhui Tang, and Qianru Sun. Semantic Scene Completion With Cleaner In Proceedings of the IEEE/CVF Conference on Self. Computer Vision and Pattern Recognition, pages 867877, 2023. 3 [36] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. 2 [37] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. ProlificDreamer: High-Fidelity And Diverse Text-to-3D Generation With Variational Score Distillation. In Advances in Neural Information Processing Systems, page 84068441, 2023. 2, 4, 16, 17 [38] Cho-Ying Wu and Ulrich Neumann. Scene CompletenessAware Lidar Depth Completion For Driving Scenario. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 24902494. IEEE, 2021. [39] Zhaoyang Xia, Youquan Liu, Xin Li, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, and Yu Qiao. Scpnet: SemanIn Proceedings of tic Scene Completion On Point Cloud. the IEEE/CVF conference on computer vision and pattern recognition, pages 1764217651, 2023. 2 [40] Yuwen Xiong, Wei-Chiu Ma, Jingkang Wang, and Raquel Urtasun. Learning Compact Representations for Lidar Completion and Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10741083, 2023. 12 [41] Yuwen Xiong, Wei-Chiu Ma, Jingkang Wang, and Raquel Urtasun. Learning Compact Representations For Lidar Completion and Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10741083, 2023. 2 [42] Yan Xu, Xinge Zhu, Jianping Shi, Guofeng Zhang, Hujun Bao, and Hongsheng Li. Depth Completion from Sparse LiIn Proceedings dar Data With Depth-Normal Constraints. of the IEEE/CVF International Conference on Computer Vision, pages 28112820, 2019. 2 [43] Yujie Xue, Ruihui Li, Fan Wu, Zhuo Tang, Kenli Li, and Mingxing Duan. Bi-SSC: Geometric-Semantic Bidirectional Fusion for Camera-based 3D Semantic Scene Completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2012420134, 2024. 3 [44] Ling Yang, Haotian Qian, Zhilong Zhang, Jingwei Liu, and Bin Cui. Structure-guided adversarial training of diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7256 7266, 2024. [45] Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, and CUI Bin. Cross-modal contextualized diffusion models for text-guided visual generation and editing. In The Twelfth International Conference on Learning Representations, 2024. 9 [46] Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024. 2 [47] Yanchao Yang, Alex Wong, and Stefano Soatto. Dense Depth Posterior (DDP) From Single Image And Sparse Range. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3353 3362, 2019. 2 [48] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step Diffusion With Distribution Matching Distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66136623, 2024. 4 [49] Qingyang Yu, Lei Chu, Qi Wu, and Ling Pei. Grayscale And Normal Guided Depth Completion With Low-cost LiDAR. In IEEE International Conference on Image Processing, pages 979983. IEEE, 2021. 2 [50] Shengyuan Zhang, Ling Yang, Zejian Li, An Zhao, Chenye Meng, Changyuan Yang, Guang Yang, Zhiyuan Yang, and Lingyun Sun. Distribution Backtracking Builds Faster Convergence Trajectory for One-step Diffusion Distillation. arXiv preprint arXiv:2408.15991, 2024. 2, 4 [51] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d Shape Generation And Completion Through Point-voxel Diffusion. In Proceedings of the IEEE/CVF international conference on computer vision, pages 58265835, 2021. 6, A. Experiment protocol A.1. Dataset setup SemanticKITTI [1] dataset is large-scale benchmark for 3D semantic segmentation in autonomous driving, extending the KITTI Odometry dataset with dense semantic annotations for over 43,000 LiDAR scans. It provides labels for 25 classes, such as car, road, and building, capturing diverse urban and rural scenes. The SemanticKITTI dataset consists of 22 sequences, where sequences 00-10 are densely annotated for each scan, enabling tasks such as semantic segmentation and semantic scene completion using sequential scans. Sequences 11-21 serve as the test set, showcasing diverse and challenging traffic situations and environment types to evaluate model performance in real-world autonomous driving scenarios. SemanticKITTI is widely used in research and serves as critical resource for advancing LiDAR-based perception systems. KITTI-360 [17] dataset is comprehensive benchmark for 3D scene understanding in autonomous driving, capturing 360-degree panoramic imagery and 3D point clouds across diverse urban environments. It includes over 73 km of driving data with dense semantic annotations for both 2D (images) and 3D (point clouds), covering categories like vehicles, buildings, and vegetation. KITTI-360 including LiDAR, provides high-resolution sensor data, GPS/IMU, and stereo camera recordings, making it ideal for tasks such as 3D semantic segmentation, panoptic segmentation, and mapping in real-world driving scenarios. A.2. Evaluation metrics Chamfer Distance (CD) [2] is metric used to measure the similarity between two sets of points, often employed for evaluating the quality of generated point clouds or geometric shapes. For two point sets and Q, the Chamfer Distance is defined as: CD(P, Q) = 1 (cid:88) pP min qQ pq2+ 1 (cid:88) qQ qp2 min pP (19) The first term calculates the average squared distance from each point in to its nearest neighbour in Q. The second term calculates the average squared distance from each point in to its nearest neighbour in . Chamfer Distance evaluates how well two point sets approximate each other by considering their nearest neighbour distances in both directions. CD effectively captures local geometric features and exhibits strong robustness in local shape matching, which is commonly used in evaluating the matching and reconstruction of 3D point clouds. Jensen-Shannon Divergence (JSD) [20] is symmetric measure of similarity between two probability distributions. It is variation of the Kullback-Leibler (KL) divergence and is widely used in information theory, statistics, and machine learning. Given two probability distributions and over the same domain, JSD is defined as: JSD(P Q) = 1 2 KL(P ) + 1 KL(QM ) (20) Here = 1 KL(P ) is the Kullback-Leibler divergence. 2 (P + Q) is the average distribution, and JSD measures how much and diverge from their average distribution . It is symmetric (JSD(P Q) = JSD(QP )) and always produces finite value in the range [0, 1] when using base-2 logarithms. Unlike KL divergence, JSD avoids issues with undefined values when probabilities are zero in one of the distributions. JSD is an efficient metric to evaluate the similarity between two distributions. The calculation of JSD in this paper is followed by Xiong et al. [40]. A.3. Implementation details We choose the pre-trained LiDiff [24] model as the teacher model ϵθ, the student model Gstu and the auxiliary diffusion model ϵϕ shares the same network architecture as the teacher model and are initialized by the teacher model. The ScoreLiDAR is trained on SemanticKITTI dataset. The pretrained diffusion model is provided by the official release of LiDiff [24]. For optimization, we use the Stochastic Gradient Descent (SGD) optimizer with the default parameters. The learning rate is set to 3e 5 and the batch size is set to 1. The training ratio between the student model and the auxiliary diffusion model is maintained at 1 : 1. To reduce computational costs, when calculating the point-wise loss, we first randomly select 1 10 of the points from the ground truth scene. Then, following the proposed method, we select the top 1 3 points with the highest curvature from these points as the key points to calculate the distance matrix. That is, the final number of key points is 1 30 of the total number of points in the ground truth scene. When calculating the K-nearest neighbours, we set = 180. The weights of scene-wise loss λscene and the point-wise loss λpoint are set to 0.5 and 0.01, respectively. ScoreLiDAR requires only 50 iterations to achieve convergence, taking approximately 10 minutes on single A40 GPU, which is highly efficient. Our code will be publicly available soon. B. Discussion Firstly, we discuss the significance of this study. For autonomous vehicles, accurately recognizing and perceiving their surrounding environment during operation is critical [15, 16]. This is particularly important for identifying objects that may affect the vehicles movement, such as other vehicles, pedestrians, traffic cones, and signposts [11]. Figure 6. Comparison of scene completion details between ScoreLiDAR and the SOTA model LiDiff [24]. The magnified images are enlarged views of the regions corresponding to the boxes in the completed scene images. Compared to LiDiff, ScoreLiDAR better completes the details of vehicles, making it closer to the ground truth scene. Accurate and efficient recognition of these objects is essential for the safe operation of autonomous vehicles. However, the scan data obtained by onboard LiDAR as shown in Fig. 6 (a) is sparse [14, 34], and it is difficult to identify key objects such as vehicles from the magnified regions of the sparse scan. However, from the ground truth scene in Fig. 6 (b), it is evident that these regions contain moving vehicles. Autonomous vehicles cannot obtain sufficient information about the driving environment from these sparse LiDAR scans [13, 25]. Therefore, it is necessary to use appropriate methods to complete the sparse LiDAR scans. LiDiff [24] uses DDPM [9] models to complete 3D LiDAR scenes, achieving impressive results. However, due to the inherent characteristics of diffusion models, LiDiff [24] requires approximately 30 seconds to complete single scene, limiting its applicability in autonomous vehicles. In contrast, the proposed ScoreLiDAR can complete scene in almost 5 seconds, more than 5 times faster than LiDiff [24], while achieving higher completion quality. As shown in Fig. 6 (c), in the magnified region, although LiDModel SemanticKITTI JSD CD KITTI360 CD JSD"
        },
        {
            "title": "Model",
            "content": "SemanticKITTI JSD CD KITTI360 CD JSD ScoreLiDAR w/o Point-wise loss w/o Scene-wise loss w/o Structural Loss 0.342 0.351 0.367 0. 0.399 0.414 0.422 0.430 0.452 0.485 0.477 0.549 0.437 0.486 0.451 0.445 LiDiff (Refined) w/ Structural loss 0.375 0.399 0.416 0. 0.517 0.535 0.446 0.450 Table 7. Ablation study of training LiDiff [24] with structural loss. Table 5. Ablation study of the scene-wise and point-wise loss. The metrics refer to the performance with refinement. Colors denote the 1st , 2nd , and 3rd best-performing model. ScoreLiDAR SemanticKITTI JSD CD KITTI360 CD JSD λscene = 0.5, λpoint = 0.01 λscene = 0.05, λpoint = 0.01 λscene = 0.5, λpoint = 0.1 0.342 0.354 0.358 0.399 0.409 0. 0.452 0.494 0.539 0.437 0.457 0.474 Table 6. Ablation study of the different weights of the scene-wise and point-wise loss. The first row refers to the default configuration of the ScoreLiDAR. The metrics refer to the performance with refinement. iff [24] completes some vehicle shapes, they are not fully reconstructed. For example, in the left region, there should be two vehicles, but the scene completed by LiDiff only contains one. In contrast, the scene completed by ScoreLiDAR features clearer and more complete vehicle structures (Fig. 6 (d)), making it closer to the ground truth. Thus, with the scenes completed by the proposed ScoreLiDAR, autonomous vehicles can more easily recognize critical objects in their driving environment, enabling safer and more effective navigation. Moreover, ScoreLiDAR allows for different sampling steps during distillation to achieve varying completion speeds. However, due to the limitations of the teacher model, ScoreLiDARs completion quality decreases at smaller sampling steps. With further improvements or by replacing the teacher model for better distillation, ScoreLiDAR could complete scene within one second, achieving real-time scene completion. This is crucial for autonomous vehicles to scan and recognize their driving environment effectively. Therefore, future work will focus on exploring this aspect. C. Additional completed scenes Fig. 8 and Fig. 9 show additional completed scenes by the proposed ScoreLiDAR and compare them with the scenes completed by LiDiff [24]."
        },
        {
            "title": "ScoreLiDAR",
            "content": "SemanticKITTI JSD CD KITTI360 CD JSD = 1/30 = 1/60 0.342 0. 0.399 0.409 0.452 0.452 0.437 0.471 Table 8. Ablation study of different key points number. The first row refers to the default configuration of the ScoreLiDAR. The metrics refer to the performance with refinement. D. Additional experiment results D.1. More ablation study of structural loss To further validate the effectiveness of the structural loss, we evaluated the performance of variants trained with only point-wise loss or scene-wise loss and compared them with default ScoreLiDAR. As shown in Tab. 5, compared to the default ScoreLiDAR, the performance of variants trained with only scene-wise loss or point-wise loss decreased. However, compared to the variants without structural loss, the variants using only one type of loss still showed improved completion performance. These results confirm the effectiveness of the structural loss in the distillation process. Additionally, we investigated the impact of different weights of scene-wise and point-wise loss on the completion quality. The results are shown in Tab. 6. It can be observed that reducing λscene or increasing λpoint leads to decline in the performance of ScoreLiDAR but still achieves comparable performance. This verifies the effectiveness of the proposed structural loss in improving the completion performance of the student model. Finally, we trained LiDiff using structural loss to investigate whether structural loss can enhance the performance of LiDiff. The results are shown in Tab. 7. Training LiDiff [24] with structural loss does not result in performance improvement. This may be because structural loss is not suitable for direct addition to the training loss of LiDiff [24], i.e. the traditional diffusion model training loss. D.2. Ablation study of different key point number As mentioned in Appendix A.3, the optimal number of the key point is set to the 1 30 of the total number of points in the ground truth. To investigate the impact of different numbers of key points on the completion performance of ScoreLiDAR, we decreased the number of key points for Model CD JSD Time (s)"
        },
        {
            "title": "Model",
            "content": "User preference LiDiff (50 steps) [24] LiDiff (50 steps Refined) [24] LiDiff (8 steps) [24] LiDiff (8 steps Refined) [24] ScoreLiDAR (8 Steps) ScoreLiDAR (4 Steps) ScoreLiDAR (2 Steps) ScoreLiDAR (1 Steps) 0.564 0.517 0.619 0.550 0.452 0.482 0.525 0.750 0.549 0.446 0.471 0. 0.437 0.461 0.457 0.478 29.18 29.43 5.46 5.77 5.14 3.16 1.69 1.03 Table 9. Ablation study of different sampling steps on the KITTI360 dataset. The metrics of ScoreLiDAR refer to the performance with refinement. model training and evaluated the completion performance. As shown in Tab. 8, when the number of key points decreases, the performance of ScoreLiDAR declines. This is because an insufficient number of key points causes the point-wise loss to fail in effectively capturing the relative positional information between key points, preventing the student model from learning the local geometric structure, and thereby reducing the completion quality. D.3. Ablation study of different sampling steps on KITTI-360 We also conduct the ablation study of different sampling steps on the KITTI-360 dataset. The results are shown in Tab. 9. Similar to the results on the SemanticKITTI dataset, as the number of sampling steps decreases, the time required for ScoreLiDAR to complete scene is reduced. Although the completion performance declines slightly, it remains comparable to that of existing SOTA models. D.4. User study The user study is conducted to verify the completion performance of ScoreLiDAR further. We first used ScoreLiDAR and the current SOTA method LiDiff [24] to complete the same 30 input LiDAR scans, resulting in 30 pairs of completed scenes. We then randomly recruited seven volunteers and guided each to evaluate the detail and fidelity of these 30 pairs of scene images, selecting the one they believed to be closer to the ground truth. The seven volunteers included five men and two women, aged 2430, with five participants having research backgrounds related to autonomous driving or LiDAR scene completion and the remaining two participants having backgrounds related to artificial intelligence. They were given unlimited time for the evaluation, but the average completion time for all volunteers was 30 minutes. The result of the user study is shown in Tab. 10. Compared to LiDiff, ScoreLiDAR received 65% user preference, surpassing the majority threshold. This indicates that, in the eyes of most users, the detail and fidelity of LiDiff [24] ScoreLiDAR 35% 65% Table 10. Results of user study. Our ScoreLiDAR outperforms the existing SOTA model."
        },
        {
            "title": "Model",
            "content": "SemanticKITTI (IoU) % 0.5m 0.2m 0.1m LMSCNet [26] LODE [14] MID [34] PVD [51] LiDiff [24] LiDiff (Refined) [24] ScoreLiDAR ScoreLiDAR (Refined) 32.23 43.56 45.02 21.20 42.49 40.71 38.43 37. 23.05 47.88 41.01 7.96 33.12 38.92 25.75 29.57 3.48 6.06 16.98 1.44 11.02 24.75 8.34 15.63 Table 11. The IoU evaluation results on the SemanticKITTI dataset. Model LMSCNet [26] LODE [14] MID [34] LiDiff [24] LiDiff (Refined) [24] ScoreLiDAR ScoreLiDAR (Refined) KITTI-360 (IoU) % 0.5m 0.2m 0.1m 25.46 42.08 44.11 42.22 40.82 36.82 33.29 16.35 42.63 36.38 32.25 36. 25.49 28.60 2.99 5.85 15.84 10.80 21.34 9.70 15.95 Table 12. The IoU evaluation results on the KITTI-360 dataset. the scenes completed by ScoreLiDAR more closely resemble the ground truth. The results of the user study further demonstrate the effectiveness of ScoreLiDAR in LiDAR scene completion. D.5. Visualization of key points To validate the feasibility of our proposed key point selection method, we visualized the selected key points in the ground truth scene. As shown in Fig. 7, the red key points are mostly distributed on walls, traffic cones, cars, and corners, while smooth areas such as the road surface have no key points. These key points are crucial for expressing the details of 3D LiDAR scenes. Selecting these points to compute the point-wise loss allows the student model to more easily capture the relative configuration information between key points, thereby better completing key objects in the scene. Figure 7. The visualization of the selected key points. Red points refer to the key points selected by the proposed method. D.6. Experiments on scene occupancy We calculate the Intersection-Over-Union (IoU) [31] to evaluate the occupancy of the completed scene compared with the ground truth scene. IoU represents the degree of overlap between the voxels in the completed scene and those in the ground truth scene. higher IoU value indicates higher completeness of the completed scene. During the evaluation, we considered three different voxel resolutions: 0.5m, 0.2m, and 0.1m. The smaller the voxel resolution, the more fine-grained details are considered in the evaluation metrics, and vice versa. Tab. 11 and Tab. 12 show the IoU of ScoreLiDAR and existing models. Under low voxel resolutions, ScoreLiDAR achieves comparable IoU values, meaning ScoreLiDAR generates dense and accurate point clouds. When the voxel resolutions become higher, the performance of ScoreLiDAR declines. This is because the existing method is mainly based on signed distance fields, which implement the scene completion using voxel representation. ScoreLiDAR is point-level scene completion with the input of point clouds obtained from LiDAR scans, which works better at smaller voxel resolutions. The results in Tab. 11 and Tab. 12 do not align with our experimental results and user study findings. Therefore, these results are provided for reference only. E. Introduction on utilized methods E.1. Variational score distillation Variational Score Distillation (VSD), proposed by ProlificDreamer [37], is designed to leverage pre-trained diffusion model to train NeRF [22], enabling the rendering of high-quality 3D objects. Given text prompt y, the probabilistic distribution of all possible 3D representations can be modeled as probabilistic density µ(θy) by NeRF model parameterized by θ. Let qµ 0 (x0c, y) as the distribution of the rendered image x0 of NeRF given the camera c, and p0(x0y) as the distribution of the pre-trained text-to-image diffusion model at = 0. To generate high-quality 3D objects, ProlificDreamer [37] optimizes the distribution of µ by minimizing the following KL divergence min µ DKL (qµ 0 (x0 y) p0 (x0 y)) (21) However, directly solving this variational inference problem is challenging because p0 is complex, and its high-density regions may be extremely sparse in highdimensional spaces. Therefore, ProlificDreamer reformulates it as an optimization problem at different time steps t, referring to these problems as Variational Score Distillation (VSD), min µ Et,c [(σt/αt) ω(t)DKL (qµ (xt c, y) pt (xt y))] (22) Theorem 1 in [37] proves that introducing the additional does not affect the global optimum of Eq. (21). Theorem 2 in [37] provides the method for optimizing the problem in Eq. (22). dθτ dτ = Et,ϵ,c[ω(t)(σtxt log pt (xt y) (cid:125) (cid:123)(cid:122) score of noisy real images (cid:124) (σtxt log qµτ (cid:123)(cid:122) score of noisy rendered images (xt c, y)) (cid:125) (cid:124) ) (θτ , c) θτ ] (23) Here the score of noisy real images is approximated by the pre-trained diffusion model ϵpretrain(xt, t, y) and the score of noisy rendered images is approximated by another diffusion model ϵϕ(xt, t, c, y), which is trained on the rendered images with the standard diffusion objective. (cid:21) (cid:88) (cid:20)(cid:13) (cid:13) (cid:13)ϵϕ (cid:17) (cid:17) (cid:16) (cid:16) i=1 ϵ αtg min ϕ θ(i), Et,ϵ,c + σtϵ, t, c, (cid:13) 2 (cid:13) (cid:13) 2 (24) In practice, ϵϕ(xt, t, c, y) is parameterized by small UNet or the Low-rank adaptation (LoRA) [10] of the teacher model. With the alternating training of NeRF and ϵϕ(xt, t, c, y), ProlificDreamer [37] is ultimately able to generate high-quality 3D objects. E.2. MinkowskiEngine Sparse tensor computation plays critical role in fields such as 3D point cloud processing, computer vision, and physical simulations. Unlike dense tensors, sparse tensors contain high proportion of zero values and directly applying traditional tensor operations can lead to inefficient use of computational resources. Minkowski Engine [5] addresses these challenges by providing high-performance framework tailored for sparse tensor computation, enabling efficient operations on high-dimensional sparse data. In this paper, we used the Minkowski Engine to process sparse point cloud data. Minkowski Engine introduces several innovative approaches to sparse tensor processing. Efficient Sparse Tensor Representation. Sparse tensors are represented using coordinate-value pairs, eliminating the need to store zeros. This representation reduces both memory usage and computational overhead. Sparse Convolution Operations The framework supports high-dimensional sparse convolutions, with kernels designed to adapt to varying sparsity patterns. Optimized memory access patterns and parallel computation strategies ensure high efficiency. Fast Coordinate Mapping Minkowski Engine employs hash tables for rapid coordinate mapping, which accelerates tensor indexing and sparse pattern matching. Automatic Differentiation Support The framework includes built-in support for automatic differentiation, facilitating the training of machine learning models based on sparse tensors. Multi-Dimensional Capability Minkowski Engine can handle sparse tensors of arbitrary dimensions, making it suitable for wide range of applications, from 2D image processing to 5D simulations. Minkowski Engine has been widely adopted in various domains including 3D point cloud processing, physical simulations and medical imaging. By significantly improving computational efficiency and scalability, the Minkowski Engine has become preferred choice for handling sparse tensor computations in both research and industrial applications. F. Ethical statement The potential ethical impact of our work is about fairness. As human is included as kind of object in the LiDAR scene, when performing scene completion, it may be necessary to complete human figures. Human-related objects may have data bias related to fairness issues, such as the bias to gender or skin colour. Such bias can be captured by the student model in the training. F.1. Notification to human subjects In our user study, we present the notification to subjects to inform the collection and use of data before the experiments. Dear volunteers, we would like to thank you for supporting our study. We propose ScoreLiDAR, novel distillation method tailored for 3D LiDAR scene completion, which introduces structural loss to help the student model capture the geometric structure information. All information about your participation in the study will appear in the study record. All information will be processed and stored according to the local law and policy on privacy. Your name will not appear in the final report. Only an individual number assigned to you is mentioned when referring to the data you provided. We respect your decision whether you want to be If you decide to para volunteer for the study. ticipate in the study, you can sign this informed consent form."
        },
        {
            "title": "The Institutional Review Board approved the use of",
            "content": "users data of the main authors affiliation. G. Failure examples Fig. 10 presents some failure cases of ScoreLiDAR. From these examples, it can be observed that ScoreLiDAR exhibits over-completion to some extent, where regions that do not exist are completed. Before the completion, as mentioned in Sec.3 in the main paper, the number of points of the input sparse scan is increased by concatenating its points times and the dense input is obtained. If the number of points of exceeds the actual number of points in the ground truth, it can lead to redundant points in the completed scene. These redundant points may be distributed in areas that do not require completion, resulting in the situations observed in the failure cases. Figure 8. Completed samples of ScoreLiDAR from KITTI-360 dataset. Figure 9. Completed samples of ScoreLiDAR from SemanticKITTI dataset. Figure 10. Failure examples of ScoreLiDAR."
        }
    ],
    "affiliations": [
        "Peking University",
        "Zhejiang Green Zhixing Technology co., ltd",
        "Zhejiang University"
    ]
}