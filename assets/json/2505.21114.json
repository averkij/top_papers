{
    "paper_title": "Differentiable Solver Search for Fast Diffusion Sampling",
    "authors": [
        "Shuai Wang",
        "Zexian Li",
        "Qipeng zhang",
        "Tianhui Song",
        "Xubin Li",
        "Tiezheng Ge",
        "Bo Zheng",
        "Limin Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have demonstrated remarkable generation quality but at the cost of numerous function evaluations. Recently, advanced ODE-based solvers have been developed to mitigate the substantial computational demands of reverse-diffusion solving under limited sampling steps. However, these solvers, heavily inspired by Adams-like multistep methods, rely solely on t-related Lagrange interpolation. We show that t-related Lagrange interpolation is suboptimal for diffusion model and reveal a compact search space comprised of time steps and solver coefficients. Building on our analysis, we propose a novel differentiable solver search algorithm to identify more optimal solver. Equipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256 with only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of 2.33 with only 10 steps. Notably, our searched solver outperforms traditional solvers by a significant margin. Moreover, our searched solver demonstrates generality across various model architectures, resolutions, and model sizes."
        },
        {
            "title": "Start",
            "content": "Shuai Wang 1 Zexian Li 2 Qipeng Zhang 2 Tianhui Song 1 Xubin Li 2 Tiezheng Ge 2 Bo Zheng 2 Limin Wang 1 3 5 2 0 2 7 2 ] . [ 1 4 1 1 1 2 . 5 0 5 2 : r Figure 1: Visualization of searched Solver Parameters of DDPM/VP and Rectified Flow. We limited the order of solver coefficients of the last two steps for 5/6 NFE. The left images show the absolute value of searched coefficients {cj }. The right image shows the searched timesteps of different NFE and fitted curves."
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have demonstrated remarkable generation quality, but at the cost of numerous function evaluations. Advanced ODE-based solvers have recently been developed to mitigate the substantial computational demands of reverse-diffusion solving under limited sampling steps. However, these solvers, heavily inspired by Adams-like multistep methods, rely solely on t-related Lagrange interpolation. We show that t-related Lagrange interpolation is suboptimal for diffusion models and reveal compact search space comprised of time steps and solver coefficients. Building on our analysis, we propose novel differentiable solver search algorithm to identify more optimal solver. Equipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and FlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet-256 256 with only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches FID 1State Key Lab of Novel Software Technology, Nanjing Uni2Taobao & Tmall Group of Alibaba, versity, Nanjing, China. Hangzhou, China. 3Shanghai AI Lab, Shanghai, China.. Correspondence to: Limin Wang <lmwang@nju.edu.cn>. Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025. Copyright 2025 by the author(s). 1 score of 2.33 with only 10 steps. Notably, our searched solver significantly outperforms traditional solvers(even some distillation methods). Moreover, our searched solver demonstrates generality across various model architectures, resolutions, and model sizes. 1. Introduction Image generation is fundamental task in computer vision research, which aims at capturing the inherent data distribution of original image datasets and generating high-quality synthetic images through distribution sampling. Diffusion models (Ho et al., 2020; Song et al., 2020b; Karras et al., 2022; Liu et al., 2022; Lipman et al., 2022; Wang et al., 2025) have recently emerged as highly promising solutions to learn the underlying data distribution in image generation, outperforming GAN-based models (Brock et al., 2018; Sauer et al., 2022) and Auto-Regressive models (Chang et al., 2022) by significant margin. However, diffusion models necessitate numerous denoising steps during inference, which incur substantial computational cost, thereby limiting the widespread deployment of pre-trained diffusion models. To achieve fast diffusion sampling, the existing studies have explored two distinct approaches. Training-based techniques by distilling the fast ODE trajectory into the model parameters, thereby circumIn addition, solverventing redundant refinement steps. Differentiable Solver Search for Fast Diffusion Sampling based methods (Lu et al., 2023; Zhang & Chen, 2023; Song et al., 2020a) tackle the fast sampling problem by designing high-order numerical ODE solvers. For training-based acceleration, (Salimans & Ho, 2022) aligns the single-step student denoiser with the multi-step teacher output, thereby reducing inference burdens. The consistency model concept, introduced by (Song et al., 2023), directly teaches the model to produce consistent predictions at any arbitrary timesteps. Building upon (Song et al., 2023), subsequent works (Zheng et al., 2024; Kim et al., 2023; Wang et al., 2024a; Song et al., 2025) propose improved techniques to mitigate discreet errors in LCM training. Furthermore, (Lin et al., 2024; Kang et al., 2024; Yin et al., 2024; Zhou et al., 2024; Wang et al., 2023) leverage adversarial training and distribution matching to enhance the quality of generated samples. To improve the training efficiency of distribution matching. However, training-based methods introduce changes to the model parameters, resulting in an inability to fully exploit the pre-training performance. Solver-based methods rely heavily on the ODE formulation in the reverse-diffusion dynamics and hand-crafted multi-step solvers. (Lu et al., 2023; 2022) and (Zhang & Chen, 2023) point out the semi-linear structure of the diffusion ODE and propose an exponential integrator to tackle faster sampling in diffusion models. (Zhao et al., 2023) further enhances the sampling quality by borrowing the predictor-corrector structure. Thanks to the multistep-based ODE solver methods, high-quality samples can be generated within as few as 10 steps. To further improve efficiency, (Gao et al., 2023) tracks the backward error and determines the adaptive step. Moreover, (Karras et al., 2022; Lu et al., 2022) propose handcrafted timesteps scheduler to sample respaced timesteps. (Xue et al., 2024) argues that timesteps sampled in (Karras et al., 2022; Lu et al., 2022) are suboptimal, thus proposing an online optimization algorithm to find the optimal sampling timesteps for generation. Apart from timesteps optimization, (Shaul et al., 2023) learns specific path transition to improve the sampling efficiency. In contrast to training-based acceleration methods, solverbased approaches do not necessitate parameter adjustments and preserve the optimal performance of the pre-trained model. Moreover, solvers can be seamlessly applied to any arbitrary diffusion model trained with similar noise scheduler, offering high degree of flexibility and adaptability. This motivates us to investigate the generative capabilities of pre-trained diffusion models within limited steps from diffusion solver perspective. Current state-of-the-art diffusion solvers (Lu et al., 2023; Zhao et al., 2023) adopt Adams-like multi-step methods that use the Lagrange interpolation function to minimize integral errors. We argue that an optimal solver should be tailored to specific pre-trained denoising functions and their corresponding noise schedulers. In this paper, we explore solver-based methods for fast diffusion sampling by improving diffusion solvers using data-driven approaches without destroying the pre-training internality in diffusion models. Inspired by (Xue et al., 2024), we investigate the sources of error in the diffusion ODE and discover that the interpolation function form is inconsequential and can be reduced to coefficients. Furthermore, we define compact search space related to the timesteps and solver coefficients. Therefore, we propose differentiable solver search method to identify the optimal parameters in the compact search space. Based on our novel differentiable solver search algorithm, we investigate the upper bound performance of pre-trained diffusion models under limited steps. Our searched solver significantly improves the performance of pre-trained diffusion models, and outperforms traditional solvers with large gap. On ImageNet-256 256, armed with our solver, rectified-flow models of SiT-XL/2 and FlowDCNXL/2 achieve 2.40 and 2.35 FID respectively under 10 steps, while DDPM model of DiT-XL/2 achieves 2.33 FID. Surprisingly, our findings reveal that when equipped with an optimized high-order solver, the DDPM can achieve comparable or even surpass the performance of rectified flow models under similar NFE constraints. To summarize, our contributions are We reveal that the interpolation function choice is not important and can be reduced to coefficients through the pre-integral technique. We demonstrate that the upper bound of discretization error in reverse-diffusion ODE is related to both timesteps and solver coefficients and define compact solver search space. Based on our analysis, we propose novel differentiable solver search algorithm to find the optimal solver parameter for given diffusion models. For DDPM/VP time scheduling, armed with our searched solver, DiT-XL/2 achieves 2.33 FID under 10 steps, beating DPMSolver++/UniPC by significant margin. For Rectified-flow models, armed with our searched solver, SiT-XL/2 and FlowDCN-XL/2 achieve 2.40 and 2.35 FID respectively under 10 steps on ImageNet256 256. For Text-to-Image diffusion models like FLUX, SD3, PixArt-Σ, our solver searched on ImageNet-256 256 consistently yields better images compared to traditional solvers with the same CFG scale. 2 Differentiable Solver Search for Fast Diffusion Sampling 2. Related Work Diffusion Model gradually adds x0 with Gaussian noise ϵ to perturb the corresponding known data distribution p(x0) into simple Gaussian distribution. The discrete perturbation function of each satisfies (xtαtx0, σ2 I), where αt, σt > 0. It can also be written as Equation (1). xt = αtxreal + σtϵ (1) dt dt σ dt log αt Moreover, as shown in Equation (2), Equation (1) has forward continuous-SDE description, where (t) = log αt and g(t) = dσ2 . (Anderson, 1982) establishes pivotal theorem that the forward SDE has an equivalent reverse-time diffusion process as in Equation (3), so the generating process is equivalent to solving the diffusion SDE. Typically, diffusion models employ neural networks and distinct prediction parametrization to estimate the score function logx pxt(xt) along the sampling trajectory (Song et al., 2020b; Karras et al., 2022; Ho et al., 2020). dxt = (t)xtdt + g(t)dw (2) dxt = [f (t)xt g(t)2x log p(xt)]dt + g(t)dw (3) (Song et al., 2020b) also shows that there exists corresponding deterministic process Equation (4) whose trajectories share the same marginal probability densities of Equation (3). dxt = [f (t)xt 1 g(t)2xt log p(xt)]dt (4) Rectified Flow Model simplifies diffusion model under the framework of Equation (2) and Equation (3). Different from (Ho et al., 2020) introduces non-linear transition scheduling, the rectified-flow model adopts linear function to transform data to standard Gaussian noise. 2023) discovered the semi-linear structure in DDPM/VP reverse ODEs. Furthermore, (Zhao et al., 2023) enhanced the sampling quality by borrowing the predictor-corrector structure. Thanks to the multi-step ODE solvers, high-quality samples can be generated within as few as 10 steps. To further improve efficiency, (Gao et al., 2023) tracks the backward error and determines the adaptive step. Moreover, (Karras et al., 2022; Lu et al., 2022) proposed handcrafted timestep scheduler to sample respaced timesteps. However, (Xue et al., 2024; Sabour et al., 2024; Chen et al., 2024a) argued that the handcrafted timesteps are suboptimal, and thus proposed an online optimization algorithm to find the optimal sampling timestep for generation. Apart from timestep optimization, (Shaul et al., 2023) learned specific path transition to improve the sampling efficiency. 3. Problem Definition As rectified-flow constitutes simple yet elegant formulation within the diffusion family, we choose rectified-flow as the primary subject of discussion in this paper to enhance readability. Importantly, our proposed algorithm is not constrained to rectified-flow models. We explore its applicability to other diffusion models such as DDPM/VP in Section 6. Recall the continuous integration of reverse-diffusion in Equation (7) with the pre-defined interval {t0, t1, ...tN }. Given the pre-trained diffusion models and their corresponding ODE defined in Equation (4), before we tackle the integration of interval [ti, ti+1], we have already obtained the sampled velocity field of previous timestep {(xj, tj, vj = vθ(xj, tj)}i j=0. Here, we directly denote xti as xi for presentation clarity: (cid:90) ti+1 xi+1 = xi + vθ(xt, t)dt (7) ti xt = txreal + (1 t)ϵ (5) Instead of estimating the score function xt log pt(xt), rectified-flow models directly learn neural network vθ(xt, t) to predict the velocity field vt = dxt = (xreal ϵ). As shown in Equation (8), we strive to develop more optimal solver that minimizes the integral error while enhancing image quality under limited sampling steps (NFE) without requiring any parameter adjustments for the pretrained model. (cid:90) 1 L(θ) = E[ vθ(xt, t) vt2dt] (6) Φ = arg min E[Φ(ϵ, vθ) (ϵ + (cid:90) 1 0 vθ(xt, t)dt)]. (8) Solver-based Fast Sampling Method does not necessitate parameter adjustments and preserves the optimal performance of the pre-trained model. It can be seamlessly applied to an arbitrary diffusion model trained with similar noise scheduler, offering high degree of flexibility and adaptability. Solvers heavily rely on the reverse diffusion ODE in Equation (4). Current solvers are mainly focused on DDPM/VP noise schedules. (Lu et al., 2022; Zhang & Chen, 4. Analysis of reverse-diffusion ODE Sampling Initially, we revisit the multi-step methods commonly used by (Zhao et al., 2023; Zhang & Chen, 2023; Lu et al., 2023) and identify potential limitations. Specifically, we argue that the Lagrange interpolation function used in AdamsBashforth methods is suboptimal for diffusion models. Moreover, we show that the specific form of the interpolation function is inconsequential, as pre-integration and 3 Differentiable Solver Search for Fast Diffusion Sampling Figure 2: Generated images from Flux.1-dev with Guidance=2.0 and our solver (searched on SiT-XL/2). Euler-Shift3 is the default solver provided by diffusers and Flux community. Our solver(DS-Solver) achieves better visual quality from 5 to 10 steps(NFE). expectation estimation ultimately reduce it to set of coefficients. Inspired by (Xue et al., 2024), we prove that timesteps and these coefficients effectively constitute our search space. 4.1. Recap the multi-step methods As shown in Equation (9), the Euler method employs vi as an estimate of Equation (9) throughout the interval [ti, ti+1]. Higher-order multistep solvers further improve the estimation quality of the integral by incorporating interpolation functions and leveraging previously sampled values. xi+1 = xi + (ti+1 ti)vθ(xi, ti). (9) The most classic multi-step solver AdamsBashforth method (Bashforth & Adams, 1883)(deemed as Adams for brevity) incorporates the Lagrange polynomial to improve the estimation accuracy within given interval. solvers employ the Lagrange interpolation function or difference formula to estimate the value in the given interval. However, the Lagrange interpolation function and other similar methods only take into account while the v(x, t) also needs as inputs. Using first-order Taylor expansion of at xi and higher-order expansion of at ti, we can readily derive the error bound of the estimation. 4.2. Focus on Solver coefficients instead of the interpolation function In contrast to typical problems of solving ordinary differential equations, when considering reverse-diffusion ODEs along with prerained models, compact searching space is present. We define universal interpolation function P, which has no explicit form. measures the distance of (xt, t) between previous sampled points {(xj, tj)}i j=0 to determine the interpolation weight for {vj}i j=0. (cid:90) ti+1 xi+1 xi + xi+1 xi + ti (cid:88) j= (cid:88) j=0 (cid:89) ( k=0,k=j tk tj tk )vjdt (10) xi+1 xi + (cid:90) ti+1 vj ti (cid:89) ( k=0,k=j tk tj tk )dt (11) xi + (cid:90) ti+ ti (cid:88) j=0 vj (cid:88) j= P(xt, t, xj, tj)vjdt. (12) (cid:90) ti+1 ti P(xt, t, xj, tj)dt. (13) ti k=0,k=j ((cid:81)i As Equation (11) states, (cid:82) ti+1 )dt of the Lagrange polynomial can be pre-integrated into constant coefficient, resulting in only naive summation being required for ODE solving. Current SoTA multi-step solvers (Lu et al., 2023; Zhao et al., 2023) are heavily inspired by AdamsBashforth-like multi-step solvers. These ttk tj tk Assumption 4.1. We assume that the remainder term of the universal interpolation function (cid:80)i j=0 P(xt, t, xj, tj)vj for v(x, t) is bound as O(dxm) + O(dtn), where O(dxm) is the mth-order infinitesimal for dx, O(dtm) is the nth-order infinitesimal for dt. Equation (13) has recurrent dependency, as xt also relies 4 Differentiable Solver Search for Fast Diffusion Sampling on (cid:80)i j=0 P(xt, t, xj, tj)vjdt. To eliminate the recurrent dependency, shown in Equation (14), we simply use the first order Taylor expansion of x(t) at xi to replace the original form. Recall that vi is already determined by xi and ti, thus the partial integral of Equation (14) can be formulated as Equation (15). Unlike naive Lagrange interpolation, Cj(xi) is function of the current xi instead of constant scalar. Learning Cj(xi) function will cause the generalization to be lost. This limits the actual usage in diffusion model sampling. xi+1 xi + (cid:88) j=0 vj (cid:90) ti+1 ti P(xi + vi(t ti), t, xj, tj)dt xi+1 xi + (cid:88) j=0 vjCj(xi)(ti+1 ti) (14) (15) Theorem 4.2. Given sampling time interval [ti, ti+1] and suppose Cj(xi) = gj(xi) + bj , Adams-like linear multi-step methods have an error expectation of (ti+1 ti)Exi (cid:80)i j=0 vjgj(xi). replacing Cj(x) with Exi[Cj(xi)] is the optimal choice and owns an error expectation of (ti+1 ti)Exi (cid:80)i j=0 vj[gj(xi) Exi gj(xi). We place the proof in Appendix H. According to Theorem 4.2, we opt to replace Cj(xi) with its expectation Exi[Cj(xi)], thus we obtain diffusionscheduler-related coefficients while maintaining generalization ability. Finally, given the predefined time intervals, we obtain the optimization target Equation (16), where cj = Exi[Cj(xi)]. The expectation can be deemed as optimized through massive data and gradient descent. xi+1 xi + (cid:88) j=0 vjcj (ti+1 ti) (16) 4.3. Optimal search space for solver Assumption 4.3. As shown in Equation (17), the pre-trained velocity model vθ is not perfect and the error between vθ and ideal velocity field ˆv is L1-bounded, where η is constant scalar. ˆv vθ η ˆv (17) Previous discussions assume we have perfect velocity function. However, the ideal velocity is hard to obtain, we only have pre-trained velocity models. Following Equation (16), we can expand Equation (16) from ti=0 to ti=N to obtain the error bound caused by non-ideal velocity estimation. Theorem 4.4. The error caused by the non-ideal velocity estimation model can be formulated in the following 5 equation. We can employ triangle inequalities to obtain the error-bound(L1) of xN ˆxN , the proof can be found in the Appendix I. xN ˆxN η 1 (cid:88) (cid:88) i=0 j=0 cj (ti+1 ti) Based on Theorem 4.4, since the error bound is related to timesteps and solver coefficients, we can define much more compact search space consisting of {cj and {ti}N Theorem 4.5. Based on Theorem 4.4 and Theorem 4.2. We can derive the total upper error bound(L1) of our solver search method and other counterparts. j<i,j=0,i= }N i=0. The total upper error bound of Our solver search is: 1 (cid:88) (ti+1 ti)( i= (cid:88) j=0 ηExigj(xi) + bj +Exi (cid:88) j= vjgj(xi) Exigj(xi)) Compared to Adams-like linear multi-step methods. Our searched solver has small upper error bound. The proof can be found in the Appendix I. Through Theorem 4.5, our searched solvers own relatively small upper error bound. Thus we can theoretically guarantee optimal compared to Adams-like methods. 5. Differentiable solver search. }N j<i,j=0,i=1 and {ti}N Through previous discussion and analysis, we identify {cj i=0 as the target search items. To this end, we propose differentiable data-driven solver search approach to determine these searchable items. Timestep Parametrization. As shown in Algorithm 1, we employ unbounded parameters {ri, }N 1 i=0 as the optimization subject, as the integral interval is from 0 to 1, we convert ri into time-space deltas ti with softmax normalization function to force their summation to 1. We can access timestep ti+1 through ti+1 = ti + ti. We initialize {ri}N 1 i=0 with 1.0 to obtain uniform timestep distribution. Coefficients Parametrization. Inspired by (Xue et al., 2024). Given Equation (16) and Equation (7), when the velocity field vθ(x, t) yields constant value, an implicit constraint (cid:80)i = 1 emerges. This observation motivates us to re-parameterize the diagonal value of as {1 (cid:80)i1 i=0 . We initialize {ck , } with zeros to mimic the behavior of the Euler solver. , }N 1 j=0 cj k=0 ci Differentiable Solver Search for Fast Diffusion Sampling Algorithm 1 Solver Parametrization Algorithm 2 Differentiable Solver Search , } Requires: {ri, } and {cj TimeDeltas: t0, t1, ..., tn1. SolverCoefficients: RN {ti, }=Softmax({ri}) 1 c0 1 ... 1 c0 1 ... = c0 n1 c1 n1 ... . . . 1 (cid:80)n1 k=0 ck n1 i=0 , M, buffer Q. l=0 = Euler(ϵ, vθ) . Require: vθ model, {ti, }N 1 Compute { xl, }L for = 0 to 1 do buffer vθ(xti , ti) Compute = (cid:80)i ti+1 = ti + ti xti+1 = xti + vti j=0 MijQj. end for return: xtn1 , L({ xl}L l=0, {xi}N i=0) Mono-alignment Supervision. We take the L-step Euler solvers ODE trajectory { x}L l=0 as reference. We minimize the gap between the target and source trajectories with the MSE loss. We also adopt Huber loss as auxiliary supervision for xtN . 6. Extending to DDPM/VP framework Applying our differentiable solver search to DDPM is infeasible. However, (Song et al., 2020b) suggests that there exists continuous SDE process with {f (t) = 1 2 βt; g(t) = βt} corresponding to discrete DDPM. This motivates us to transform the search space from the infeasible discrete space to its continuous SDE counterpart. (Lu et al., 2022) and (Zhang & Chen, 2023) discover the semi-linear structure of diffusion and propose exponential integral with ϵ parametrization to tackle the fast sampling prob2 βsds, σt = lem of DDPM models, where αt = (cid:112) (cid:82) 0 βsds and λt = log αt . (Lu et al., 2023) furσt ther discovers that parametrization is more powerful for diffusion sampling under limited steps, where = xtσϵ . (cid:82) 0 1 1 αt xt = σt σs xs + σt (cid:90) λt λs eλ xθ(xt(λ), t(λ))dλ (18) We opt to follow the parametrization as DPM-Solver++. However, we find directly interpolating eλxθ(xt, t) as whole part is hard for searching, and yields worse results. To avoid conflating the interpolation coefficients with exponential integral, we employ ωt = αt and transform Equaσt tion (18) into Equation (19) with similar interpolation format as Equation (15), where t(ω) maps ω to timestep. xt σt σs xs + σt(ωt ωs) (cid:88) k=1 ck xθ( xk, tk) (19) 7. Experiment We demonstrate the efficiency of our differentiable solver search by conducting experiments on publicly available diffusion models. Specifically, we utilize DiT-XL/2 (Peebles 6 (a) FID of Search Model (b) FID of RefTraj Steps Figure 3: Ablations studies of Differentiable Solver Search. We evaluate the searched solver on SiT-XL/2, and report the FID performance curve of searched solvers. & Xie, 2023) trained with DDPM scheduling and rectifiedflow models SiT-XL/2 (Ma et al., 2024) and FlowDCNXL/2 (Wang et al., 2024b). Our default training setting employs the Lion optimizer (Chen et al., 2024c) with constant learning rate of 0.01 and no weight decay. We sample 50,000 images for the entire search process. Notably, searching with 50,000 samples using FlowDCN-B/2 requires approximately 30 minutes on 8 H20 computation cards. During the search, we deliberately avoid using CFG to construct reference and source trajectories, thereby preventing misalignment. 7.1. Rectified Flow Models We search solver with FlowDCN-B/2, FlowDCN-S/2 and SiT-XL/2. We compare the search solvers performance with the second-order and fourth-order Adam multistep method on SiT-XL/2, FlowDCN-XL/2 trained on ImageNet 256 256 and FlowDCN-XL/2 trained on ImageNet 512 512. Search Model. We tried different search models among different size and architecture. We report the FID performance of SiT-XL/2 in Figure 3a. Surprisingly, we find that the FID performance of SiT-XL/2 equipped with the solver searched using FlowDCN-B/2 outperforms the solver searched on SiT-XL/2 itself. The reconstruction error(in Appendix) between the sampled result produced by Euler-250 steps is as expected. These findings suggest that there exists minor discrepancy between FID and the pursuit of minimal error Differentiable Solver Search for Fast Diffusion Sampling (a) SiT-XL/2-R256 (b) FlowDCN-XL/2-R256 (c) FlowDCN-XL/2-R512 Figure 4: The same searched solver on different Rectified-Flow Models. R256 and R512 indicate the generation resolution of given model. We search solver with FlowDCN-B/2 on ImageNet-256 256 and evaluate it with SiT-XL/2 and FlowDCN-XL/2 on different resolution datasets. Our searched solver outperforms traditional solvers by significant margin. More metrics(sFID, IS, Precision, Recall) are places at Appendix in the current solver design. SiT-XL-R256 NFE-CFG Params FID IS Step of Reference Trajectory. We provide reference trajectory { x}L l=0 of different sampling step for differentiable solver search. We take FlowDCN-B/2 as the search model and report the FID measured on SiT-XL/2 in Figure 3b. As the sampling step of reference trajectory increases, the FID of SiT-XL/2 further improves and becomes better. However, the performance improvement is not significant when the number of steps is 5 or 6, which suggests that there is limit to the improvement achievable with an extremely small number of steps. ImageNet 256 256. We validate the searched solver on SiT-XL/2 and FlowDCN-XL/2. We arm the pre-trained model with CFG of 1.375. As shown in Figure 4a, our searched solver improves FID performance significantly and achieves 2.40 FID under 10 steps. As shown in Figure 4b, our searched solver achieves 2.35 FID under 10 steps, beating traditional solvers by large margins. We also provide the comparison with recent solver-based distillation (Zhao et al., 2024) to demonstrate the efficiency of our searched solver in Table 1. Our searched solver achieves better metric performance under similar NFE with much fewer parameters. ImageNet 512 512. Since (Ma et al., 2024) has not released SiT-XL/2 trained on 512512 resolution, we directly report the performance collected from FlowDCN-XL/2. We arm FlowDCN-XL/2 with CFG of 1.375 and four channels. Our searched solver achieves 2.77 FID under 10 steps, beating traditional solver by large margin, even slightly outperforming the Euler solver with 50 steps(2.81FID). Text to Image. Shown in Figure 2, we apply our solver search on FlowDCN-B/2 and SiT-XL/2 to the most advanced Rectified-Flow model Flux.1-dev and SD3 (Esser et al., 2024). We find Flux.1-Dev would produce grid points in Heun Heun Heun Adam2 Adam2 Adam4 Adam4 16x2 22x2 30x2 15x2 16x2 15x2 16x2 FlowTurbo FlowTurbo FlowTurbo FlowTurbo (7+3)x2 (8+2)x2 (12+2)x2 (17+3)x2 ours ours ours ours ours 6x2 7x2 8x2 10x2 15x 0 0 0 / 0 / 0 2.9 107 2.9 107 2.9 107 2.9 107 21 28 36 55 55 3.68 2.79 2.42 2.49 2.42 2.33 2.27 3.93 3.63 2.69 2.22 3.57 2.78 2.65 2.40 2.24 / / / 236 237 242 224 / / 248 214 229 234 238 244 Table 1: Comparsion with Distillation methods. Our searched solver achieves much better result under the same NFE with much fewer parameters. generation. To alleviate the grid pattern, we decouple the velocity field into mean and direction, only apply our solver to the direction, and replace the mean with an exponential decayed mean. The details can be found in the appendix. We also provide result of distillation on SD1.5 and SDXL with solver search in Appendix F. 7.2. DDPM/VP Models We choose the open-source DiT-XL/2(Peebles & Xie, 2023) model trained on ImageNet 256 256 as the search model to carry out the experiments. We compare the performance of the searched solver with DPM-Solver++ and UniPC on 7 Differentiable Solver Search for Fast Diffusion Sampling Figure 5: The images generated from PixArt-Σ with CFG=2.0 equipped with Our DS-Solver ( searched on DiT-XL/2-R256 ).In comparison to DPM-Solver++ and UniPC, our results consistently exhibit greater clarity and possess more details. Our solver achieves better quality from 5 to 10 steps(NFE). Methods NFEs 6 7 8 9 10 DPM-Solver++ with uniform-λ (Lu et al., 2023) 38.04 20.96 14.69 11.09 8.32 6. DPM-Solver++ with uniform-t (Lu et al., 2023) 31.32 14.36 DPM-Solver++ with uniform-λ-opt (Xue et al., 2024) 12.53 DPM-Solver++ with uniform-t-opt (Xue et al., 2024) 12.53 5.44 5.44 7.62 3.58 3. 4.93 7.54 3.81 3.77 3.23 5. 4.12 3.13 2.79 UniPC with uniform-λ (Zhao et al., 2023) 41.89 30. 19.72 12.94 8.49 6.13 UniPC with uniform-t (Zhao et al., 2023) 23. 10.31 UniPC with uniform-λ-opt (Xue et al., 2024) UniPC with uniform-t-opt (Xue et al., 2024) Searched-Solver 8.66 8. 7.40 4.46 4.46 3.94 5.73 3. 3.74 2.79 4.06 3.72 3.29 2. 3.39 3.04 3.40 3.01 3.01 2. 2.37 2.33 Table 2: FID () of different NFEs on DiT-XL/2-R256 . -opt indicates online optimization of the timesteps scheduler. Methods NFEs 5 7 8 9 10 UniPC with uniform-λ (Zhao et al., 2023) 41. 19.81 13.01 9.83 8.31 7.01 UniPC with uniform-t (Zhao et al., 2023) 20.28 10.47 UniPC with uniform-λ-opt (Xue et al., 2024) UniPC with uniform-t-opt (Xue et al., 2024) 11.40 11. Searched-solver(searched on DiT-XL/2-R256) 10.28 5.95 5.95 6.02 6. 4.82 4.64 4.31 5.13 4.46 4. 4.68 6.93 6.01 4.36 4.05 3. 3.74 3.54 3.64 Table 3: FID () of different NFEs on DiT-XL/2-R512. -opt indicates online optimization of the timesteps scheduler. ImageNet 256 256 and ImageNet 512 512. ImageNet 256 256. Following (Peebles & Xie, 2023) and (Xue et al., 2024), We arm pre-trained DiT-XL/2 with 8 Differentiable Solver Search for Fast Diffusion Sampling"
        },
        {
            "title": "Solver Steps CFG",
            "content": "GenEval Metrics Color.Attr Two.Obj Pos"
        },
        {
            "title": "Overall",
            "content": "DPM++"
        },
        {
            "title": "Ours",
            "content": "5 8 5 8 5 8 2.0 2.0 2.0 2.0 2.0 2. 6.50 5.25 6.50 6.72 5.25 7.25 33.08 39.65 34.85 40.66 37.37 42. 4.75 0.40519 5.75 0.43074 5.25 0.41387 6.00 0.44134 4.75 0.41933 7.50 0.45064 Table 4: Results on GenEval Benchmark for PixArt at 512 Resolution.Our searched solver achieves better performance compared with UniPC/DPM++ on PixArt-512 512."
        },
        {
            "title": "FID sFID",
            "content": "IS DPM++ DPM++ DPM++ UniPC UniPC UniPC Ours Ours Ours 5 8 10 5 8 5 8 10 60.0 38.4 35.6 57.9 37.6 35.3 46.4 33.6 33.4 209 116.9 114.7 206.4 115.3 113. 204 115.2 114.7 25.59 33.0 33.7 25.88 33.3 33.6 28.0 32.6 32.5 PR 0.36 0.50 0. 0.38 0.51 0.54 0.46 0.54 0."
        },
        {
            "title": "Recall",
            "content": "0.20 0.36 0.37 0.20 0.36 0.36 0.23 0.39 0.39 Table 5: Metrics of different NFEs on PixArt-α (Our Solver are searched on ImageNet 256x256). CFG of 1.5 and apply CFG only on the first three channels. As shown in Table 2, our searched solver improves FID performance significantly and achieves 2.33 FID under 10 steps. ImageNet 512 512. We directly apply the solver searched on 256 256 resolution to ImageNet 512 512. The result is also great to some extent, DiT-XL/2(512 512) achieves 3.64 FID under 10 steps, outperforming DPM-Solver++ and UniPC with large gap. Text to Image. As we search solver with DiT and its corresponding noise scheduler, so it is infeasible to apply our solver to other DDPM models with different βmin and βmax. Fortunately, we find (Chen et al., 2024b) and (Chen et al., 2023) also employ the same βmin and βmax as DiT. So we can provide the visualization results of our searched solver on PixArt-Σ and PixArt-α. Our visualization result is produced with CFG of 2. We take PixArt-alpha as the text-to-image model. We follow the evaluation pipeline of ADM and take COCO17-Val as the reference batch. We generate 5k images using DPM-Solver++, UniPC and our solver searched on DiT-XL/2-R256. Also, we provided the performance results on GenEval Benchmark (Ghosh et al., 2023) in Section 7.2. 7.3. Visualization Of Solver Parameters Searched Coefficients are visualized in Figure 1. The absolute value of searched coefficients corresponding to DDPM/VP shares different pattern, coefficients in DDPM/VP are more concentrated on the diagonal while rectified-flow demonstrates more flattened distribution. This indicates there exists more curved sampling path in DDPM/VP compared to rectified-flow. Searched Timesteps are visualized in Figure 1. Compared to DDPM/VP, rectified-flow models more focus on the more noisy region, exhibiting small time deltas at the beginning. We fit the searched timestep of different NFE with polynomials and provide the respacing curves in Equation (20) and Equation (21). [0, 1], and = 0 indicates the most noisy timestep. ReFlow : 1.96t4 + 3.51t3 0.97t2 + 0.43t DDPM/VP : 2.73t4 + 6.30t3 4.744t2 + 2.17t (20) (21) 8. Conclusion We have found compact solver search space and proposed novel differentiable solver search algorithm to identify the optimal solver. Our searched solver outperforms traditional solvers by significant margin. Equipped with the searched solver, DDPM/VP and Rectified Flow models significantly improve under limited sampling steps. However, our proposed solver still has several limitations which we plan to address in future work. 9. Limitations In the main paper, we demonstrate text-to-image visualization with small CFG value. However, it is intuitive that using larger CFG would result in superior image quality. We attribute the inferior performance of large CFGs in our solver to the limitations of current naive solver structures and searching techniques. We hypothesize that incorporating predictor-corrector solver structures would enhance numerical stability and yield better images. Additionally, training with CFGs may also be beneficial."
        },
        {
            "title": "Impact Statement",
            "content": "This paper proposes search-based solver for fast diffusion sampling. We acknowledge that it could lower the barrier for creating diffusion-based AIGC contents. Acknowledgements. This work is supported by National Key R&D Program of China (No. 2022ZD0160900), Jiangsu Frontier Technology Research and Development Program (No. BF2024076), and the Collaborative Innovation Center of Novel Software Technology and Industrializa9 Differentiable Solver Search for Fast Diffusion Sampling tion, Alibaba Group through Alibaba Innovative Research Program."
        },
        {
            "title": "References",
            "content": "Anderson, B. D. Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313 326, 1982. Bashforth, F. and Adams, J. C. An attempt to test the theories of capillary action by comparing the theoretical and measured forms of drops of fluid. University Press, 1883. Brock, A., Donahue, J., and Simonyan, K. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Chen, D., Zhou, Z., Wang, C., Shen, C., and Lyu, S. On the trajectory regularity of ode-based diffusion sampling. arXiv preprint arXiv:2405.11326, 2024a. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic textto-image synthesis. arXiv preprint arXiv:2310.00426, 2023. Chen, J., Ge, C., Xie, E., Wu, Y., Yao, L., Ren, X., Wang, Z., Luo, P., Lu, H., and Li, Z. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024b. Chen, X., Liang, C., Huang, D., Real, E., Wang, K., Pham, H., Dong, X., Luong, T., Hsieh, C.-J., Lu, Y., et al. Symbolic discovery of optimization algorithms. Advances in neural information processing systems, 36, 2024c. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. Gao, Y., Pan, Z., Zhou, X., Kang, L., and Chaudhari, P. Fast diffusion probabilistic model sampling through the lens of backward error analysis. arXiv preprint arXiv:2304.11446, 2023. Ghosh, D., Hajishirzi, H., and Schmidt, L. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Kang, M., Zhang, R., Barnes, C., Paris, S., Kwak, S., Park, J., Shechtman, E., Zhu, J.-Y., and Park, T. Distilling diffusion models into conditional gans. arXiv preprint arXiv:2405.05967, 2024. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35: 2656526577, 2022. Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., Uesaka, T., He, Y., Mitsufuji, Y., and Ermon, S. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Lin, S., Wang, A., and Yang, X. Sdxl-lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Lu, C., Zhou, Y., Bao, F., Chen, J., LI, C., and Zhu, J. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 57755787, 2022. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpmsolver++: Fast solver for guided sampling of diffusion probabilistic models, 2023. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Sabour, A., Fidler, S., and Kreis, K. Align your steps: Optimizing sampling schedules in diffusion models. arXiv preprint arXiv:2404.14507, 2024. Differentiable Solver Search for Fast Diffusion Sampling Zhang, Q. and Chen, Y. Fast sampling of diffusion models with exponential integrator. In The Eleventh International Conference on Learning Representations, 2023. Zhao, W., Bai, L., Rao, Y., Zhou, J., and Lu, J. Unipc: unified predictor-corrector framework for fast sampling of diffusion models. arXiv preprint arXiv:2302.04867, 2023. Zhao, W., Shi, M., Yu, X., Zhou, J., and Lu, J. Flowturbo: Towards real-time flow-based image generation with velocity refiner. arXiv preprint arXiv:2409.18128, 2024. Zheng, J., Hu, M., Fan, Z., Wang, C., Ding, C., Tao, D., and Cham, T.-J. Trajectory consistency distillation. arXiv preprint arXiv:2402.19159, 2024. Zhou, M., Zheng, H., Wang, Z., Yin, M., and Huang, H. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024. Salimans, T. and Ho, J. fast sampling of diffusion models. arXiv:2202.00512, 2022."
        },
        {
            "title": "Progressive distillation for\narXiv preprint",
            "content": "Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. Shaul, N., Perez, J., Chen, R. T., Thabet, A., Pumarola, A., and Lipman, Y. Bespoke solvers for generative flow models. arXiv preprint arXiv:2310.19075, 2023. Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv:2010.02502, October 2020a. URL https://arxiv.org/abs/2010.02502. Song, T., Feng, W., Wang, S., Li, X., Ge, T., Zheng, B., and Wang, L. Dmm: Building versatile image generation model via distillation-based model merging. arXiv preprint arXiv:2504.12364, 2025. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Wang, F.-Y., Huang, Z., Bergman, A. W., Shen, D., Gao, P., Lingelbach, M., Sun, K., Bian, W., Song, G., Liu, Y., et al. Phased consistency model. arXiv preprint arXiv:2405.18407, 2024a. Wang, S., Teng, Y., and Wang, L. Deep equilibrium object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 62966306, 2023. Wang, S., Li, Z., Song, T., Li, X., Ge, T., Zheng, B., and Wang, L. Flowdcn: Exploring dcn-like architectures for fast image generation with arbitrary resolution. arXiv preprint arXiv:2410.22655, 2024b. Wang, S., Tian, Z., Huang, W., and Wang, L. Ddt: arXiv preprint Decoupled diffusion transformer. arXiv:2504.05741, 2025. Xue, S., Liu, Z., Chen, F., Zhang, S., Hu, T., Xie, E., and Li, Z. Accelerating diffusion sampling with optimized time steps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8292 8301, 2024. Yin, T., Gharbi, M., Zhang, R., Shechtman, E., Durand, F., Freeman, W. T., and Park, T. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66136623, 2024. 11 A. More Metrics of Searched Solver Differentiable Solver Search for Fast Diffusion Sampling We adhere to the evaluation guidelines provided by ADM and DM-nonuniform, reporting only the FID as the standard metric in Figure 4a. To clarify, we do not report selective results on rectified flow models; we present sFID, IS, PR, and Recall metrics for SiT-XL(R256), FlowDCN-XL/2(R256), and FlowDCN-B/2(R256). Our solver searched on FlowDCN-B/2, consistently outperforms the handcrafted solvers across FID, sFID, IS, and Recall metrics. B. Computational complexity compared to other methods. For sampling. When performing sampling over time steps, our solver caches all pre-sampled predictions, resulting in memory complexity of O(n). The model function evaluation also has complexity of O(n) (O(2 n) for CFG enabled). It is important to note that the memory required for caching predictions is negligible compared to that used by model weights and activations. Besides classic methods, we have also included comparison with the latest Flowturbo published on NeurIPS24. Adam2 Adam4 heun DPM-Solver++ UniPC FlowTurbo our Steps NFE NFE-CFG Cache Pred Order n 2n 2n 4n 2n 2n >2n 2n 2n >n 2 4 2 2 3 2 2 4 2 2 3 2 search samples / / / / / 540000(Real) 50000(Generated) For searching. Solver-based algorithms, limited by their searchable parameter sizes, demonstrate significantly lower performance in few-step settings compared to distillation-based algorithms(5/6steps), making direct comparisons inappropriate. Consequently, we selected algorithms that are both acceleratable on ImageNet and comparable in performance, including popular methods such as DPM-Solver++, UniPC, and classic Adams-like linear multi-step methods. Since our experiments primarily utilize SiT, DiT, and FlowDCN that trained on the ImageNet dataset. We also provide fair comparisons by incorporating the latest acceleration method, FlowTurbo. Additionally, we have included results from the heun method as reported in FlowTurbo. C. Ablation on Search Samples We ablate the number of search samples on the 10-step and 8-step solver settings. Samples means the total training samples the searched solver has seen. Unique Samples means the total distinct samples the searched solver has seen. Our searched solver converges fast and gets saturated near 30000 samples. iters(10-step-solver) 313 626 939 1252 1565 626 939 1252 samples 10000 20000 30000 40000 50000 20000 30000 40000 50000 unique samples 10000 10000 10000 10000 10000 20000 30000 40000 50000 FID IS 2.54 2.38 2.49 2.29 2.41 2.47 2.40 2.48 2.41 239 239 240 239 240 237 238 237 239 PR 0.79 0.79 0.79 0.80 0.80 0.78 0.79 0.80 0.80 Recall 0.59 0.60 0.59 0.60 0.59 0.60 0.60 0.59 0. D. Solver Across different variance schedules Since our solvers are searched on specific noise scheduler and its corresponding pre-trained models, applying the searched coefficients and timesteps to other noise schedulers yields meaningless results. We have tried applied searched solver on 12 Differentiable Solver Search for Fast Diffusion Sampling (a) SiT-XL/2-R256 (b) FlowDCN-XL/2-R (c) FlowDCN-XL/2-R512 (d) SiT-XL/2-R256 (e) FlowDCN-XL/2-R256 (f) FlowDCN-XL/2-R512 (g) SiT-XL/2-R256 (h) FlowDCN-XL/2-R (i) FlowDCN-XL/2-R512 (j) SiT-XL/2-R256 (k) FlowDCN-XL/2-R256 (l) FlowDCN-XL/2-R512 Figure 6: The same searched solver on different Rectified-Flow Models. R256 and R512 indicate the generation resolution of given model. We search solver with FlowDCN-B/2 on ImageNet-256 256 and evaluate it with SiT-XL/2 and FlowDCN-XL/2 on different resolution datasets. Our searched solver outperforms traditional solvers by significant margin. Differentiable Solver Search for Fast Diffusion Sampling iters(8-step-solver) 313 626 939 1252 1565 626 939 1252 1565 samples 10000 20000 30000 40000 50000 20000 30000 40000 50000 unique samples 10000 10000 10000 10000 10000 20000 30000 40000 50000 FID IS 2.99 2.78 2.72 2.67 2.69 2.70 2.82 2.79 2.65 228 229 235 228 235 231 232 231 PR 0.78 0.79 0.79 0.79 0.79 0.79 0.79 0.79 0.79 Recall 0.59 0.60 0.60 0.60 0.59 0.59 0.59 0.60 0.60 SiT(Rectified flow) and DiT(DDPM with βmin = 0.1, βmax = 20) to SD1.5(DDPM with βmin = 0.085, βmax = 12), but the results were inconclusive. Notably, despite sharing the DDPM name, DiT and SD1.5 employ distinct βmin, βmax values, thereby featuring different noise schedulers. more in-depth discussion of these experiments can be found in Section(Extend to DDPM/VP). E. Solver for different variance schedules As every DDPM has corresponding continuous VP scheduler, so we can transform the discreet DDPM into continuous VP, thus we successfully searched better solver compared to DPM-Solvers. The details can be found in Section 6. To put it simply, under the empowerment of our high-order solver, the performance of DDPM and FM does not differ significantly (8, 9, 10 steps), which contradicts the common belief that FM is stronger at limited sampling steps. F. Text to image Distillation Experiments We unify distillation and solver search to obtain high-quality multi-step generative models. We adopt adversarial training and trajectory supervision. We will open source the training code of unified training techniques. Table 6: Performance comparison on validation set of COCO-2017. Method Res. Time () # Steps # Param. FID () SDv1-5+DPMSolver (Upper-Bound) (Lu et al., 2022) Rectified Flow Rectified Diffusion Rectified Flow PeRFlow Rectified Diffusion Ours(Distillation+solver search) PeRFlow-SDXL Rectified Diffusion-SDXL Ours(LORA+Distillation+solver search) 512 512 512 512 512 512 1024 1024 1024 0.88s 0.88s 0.88s 0.21s 0.21s 0.21s 0.21s 0.71s 0.71s 0.71s 25 25 4 4 4 4 4 4 4 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 0.9B 3B 3B 3B 20. 21.65 21.28 103.48 22.97 20.64 18.99 27.06 25.81 21.3 G. Limitations. We place the limitation at the appendix, in order to provide more discussion space and obtain more insights from reviews. We copy the original limitation content and add more. Misalignd Reconstrucion loss and Performance. Our proposed methods are specifically designed to minimize integral error within limited number of steps. However, ablation studies reveal mismatch between FID performance and Reconstruction error. To address this issue, we plan to enhance our searched solver by incorporating distribution matching supervision, thereby better aligning sampling quality. Larger CFG Inference. In the main paper, we demonstrate text-to-image visualization with small CFG value. However, it is intuitive that utilizing larger CFG would result in superior image quality. We attribute the inferior performance of 14 Differentiable Solver Search for Fast Diffusion Sampling Table 7: Performance comparison on COCO-2014."
        },
        {
            "title": "Method",
            "content": "Res. Time () # Steps # Param. FID () SDXL-Turbo SDXL-Lightning DMDv2 LCM Phased Consistency Model PeRFlow-XL Rectified Diffusion-XL (Phased) Ours(LORA+Distillation+solver search) Stable Diffusion XL (3B) and its accelerated or distilled versions 3B 3B 3B 3B 3B 3B 3B 3B 0.34s 0.71s 0.71s 0.71s 0.71s 0.71s 0.71s 0.71s 512 1024 1024 1024 1024 1024 1024 1024 4 4 4 4 4 4 4 4 23.19 24.56 19.32 22.16 21.04 20.99 19.71 11.4 large CFGs on our solver to the limitations of current naive solver structures and searching techniques. We hypothesize that incorporating predictor-corrector solver structures would enhance numerical stability and yield better images. Additionally, training with CFGs may also be beneficial. Resource Consumption We can hard code the searched coefficients and timesteps into the program files. However, Compared to hand-crafted solvers, our solver still needs searching process. H. Proof of pre-integral error expectation Theorem H.1. Given sampling time interval [ti, ti+1] and suppose Cj(x) = gj(x) + bj methods will introduce an upper error bound of (ti+1 ti)Exi (cid:80)i Our solver search(replacing Cj(x) with Exi[Cj(xi)]) owns an upper error bound of (ti+1 ti)Exi (cid:80)i Exigj(xi) j=0 vjgj(xi). , Adams-like linear multi-step j=0 vj[gj(xi) Proof. Suppose Cj(xi) = gj(xi) + bj thus pre-integral coefficients of Adams-like linear multi-step methods will only reduce into b. . Adams-like linear multi-step methods would not consider x-related interpolation. We obtain the error expectation of the pre-integral of Adams-like linear multi-step methods: Exi =Exi (cid:88) j= (cid:88) j=0 vj[Cj(xi)](ti+1 ti) (cid:88) j=0 vjbj (ti+1 ti) vj(ti+1 ti)[Cj(xi) bj =(ti+1 ti)Exi (cid:88) j=0 vjgj(xi) We obtain the error expectation of the pre-integral of our solver search methods: Exi =Exi (cid:88) j=0 (cid:88) j=0 vj[Cj(xi)](ti+1 ti) (cid:88) j=0 vjExi[Cj(xi)](ti+1 ti) vj(ti+1 ti)[Cj(xi) ExiCj(xi) =(ti+1 ti)Exi (cid:88) j=0 vj[gj(xi) Exigj(xi) 15 (22) (23) (24) (25) (26) (27) Differentiable Solver Search for Fast Diffusion Sampling Next, define the optimization problem: = Exi (cid:88) j=0 vj[gj(xi) aj]2 2. We suppose different vj are orthogonal and vj2 this choice is optimal. 2 = 1. As we leave ci as the expectation of Cj(xi), we will demonstrate aj = 2Exi (vj2 2(gj(xi) aj)) (28) Let aj = 0, we obtain: aj = Exi gi(xi)vj 2 Exi vj 2 2 2 = Exigj(xi) = ExiCj(xi) bj . So our searched solver has lower and optimal error expectation: (ti+1 ti)Exi (cid:88) j=0 vj[gj(xi) Exigj(xi)] (ti+1 ti)Exi (cid:88) j=0 vjgj(xi) (29) Recall Assumption 4.1, the integral upper error bound of universal interpolation will be: v(xt, t)dt v(xt, t)dt (cid:88) j=0 vj (cid:90) ti+1 ti (cid:90) ti+ P(xt, t, xj, tj)dt. ti (cid:88) j=0 P(xt, t, xj, tj)vjdt. (cid:90) ti+ ti (cid:90) ti+1 ti (cid:90) ti+1 = = [v(xt, t) ti (cid:90) ti+1 ti < v(xt, t) (cid:88) j=0 (cid:88) j=0 P(xt, t, xj, tj)vj]dt. P(xt, t, xj, tj)vjdt. <(ti+1 ti)[O(dxm) + O(dtn)] (30) (31) (32) (33) (34) Combining Equation (34) and the error expectation of the pre-integral part, we will get the total error bound of the solver 16 Differentiable Solver Search for Fast Diffusion Sampling search. (cid:90) ti+ ti (cid:90) ti+1 = v(xt, t)dt v(xt, t)dt (cid:88) j=0 (cid:88) j=0 vjExi[Cj(xi)](ti+1 ti). (cid:90) ti+ vj ti P(xt, t, xj, tj)dt+ ti (cid:88) j= vj (cid:90) ti+1 ti P(xt, t, xj, tj)dt (cid:88) j= vjExi[Cj(xi)](ti+1 ti). (cid:90) ti+1 < ti (cid:88) j= vj (cid:90) ti+1 = v(xt, t)dt (cid:88) j=0 vj (cid:90) ti+1 ti P(xt, t, xj, tj)dt+ (cid:90) ti+ ti P(xt, t, xj, tj)dt (cid:88) j=0 vjExi[Cj(xi)](ti+1 ti). v(xt, t)dt (cid:88) j=0 vj (cid:90) ti+1 ti P(xt, t, xj, tj)dt+ vj[Cj(xi)](ti+1 ti) (cid:88) j=0 vjExi[Cj(xi)](ti+1 ti). ti (cid:88) j=0 <(ti+1 ti)[O(dxm) + O(dtn)] + (ti+1 ti)Exi (cid:88) j=0 vj[gj(xi) Exigj(xi)] <(ti+1 ti)([O(dxm) + O(dtn)] + Exi (cid:88) j=0 vj[gj(xi) Exigj(xi)]) (35) (36) (37) (38) (39) (40) (41) (42) (43) Since ((O(dxm) + O(dtn)) is much smaller than Exi (cid:80)i O(dtn)) term. j=0 vj[gj(xi) Exigj(xi)]. We can omit the ((O(dxm) + I. Proof of total upper error bound Theorem I.1. Compared to Adams-like linear multi-step methods. Our Solver search has small upper error bound. The total upper error bound of Adams-like linear multi-step methods is: 1 (cid:88) i=0 ( 1 ) (cid:88) j=0 ηbj + Exi (cid:88) j=0 vj[gj(xi)]) The total upper error bound of Our solver search is: 1 (cid:88) (ti+1 ti) i=0 (cid:88) j= ηExi gj(xi) + bj + Exi (cid:88) j=0 vjgj(xi) Exigj(xi)) Proof. We donate the continuous integral result of the ideal velocity field ˆv as ˆx, the solved integral result of the ideal velocity field ˆv as ˆxN , the continuous integral result of the pre-trained velocity model vθ as ˆx, the solved integral result of the pre-trained velocity model vθ as xN . xN = ϵ + 1 (cid:88) (cid:88) i=0 j=0 vjcj (ti+1 ti) (44) Differentiable Solver Search for Fast Diffusion Sampling The error caused by the non-ideal velocity estimation model can be formulated in the following equation. we can employ triangular inequalities to obtain the error-bound xN ˆxN , which is related to solver coefficients and timestep choices. xN ˆxN = 1 (cid:88) (cid:88) (vj ˆvj)cj (ti+1 ti) i=0 j= 1 (cid:88) (cid:88) i=0 j= 1 (cid:88) (cid:88) i=0 j=0 (vj ˆvj)cj (ti+1 ti) vj ˆvj) cj (ti+1 ti) η 1 (cid:88) (cid:88) i= j=0 cj (ti+1 ti) The total error of our searched solver is: xN ˆx =xN ˆxN + ˆxN ˆx xN ˆxN + ˆxN ˆx η 1 (cid:88) (cid:88) i=0 j=0 cj (ti+1 ti)+ 1 (cid:88) i=0 1 (cid:88) (ti+1 ti)(O(dxm) + O(dtn) + Exi (cid:88) η cj (ti+1 ti) + (ti+1 ti)Exi (cid:88) j=0 (cid:88) j=0 vj[gj(xi) Exigj(xi)]) vj[gj(xi) Exigj(xi)]) ηExigj(xi) + bj + Exi (cid:88) j=0 vj[gj(xi) Exi gj(xi)]) = i=0 j= 1 (cid:88) (ti+1 ti) i=0 (cid:88) j=0 The total error of Adams-like linear multi-step method is: 1 (cid:88) i=0 ( 1 ) (cid:88) j=0 ηbj + Exi (cid:88) j=0 vj[gj(xi)]) j=0 ηbj +Exi (cid:80)i Obviously, as ((cid:80)i owns smaller upper error bound than uniform timesteps. Recall that η vj, the error is mainly determined by Exi (cid:80)i Recall that Exi (cid:80)i error bound because we search coefficients and timesteps simultaneously. j=0 vj[gj(xi) Exigj(xi)] Exi (cid:80)i j=0 vj[gj(xi)]. j=0 vj[gj(xi)]) is not equal between different timestep intervals, Optimized timesteps j=0 vj[gj(xi)], thus our solver search has minimal upper 18 J. Searched Parameters Differentiable Solver Search for Fast Diffusion Sampling We provide the searched parameters and cj . Note cj needs to be converted into follwing Algorithm 1. J.1. Solver Searched on SiT-XL/2 NFE TimeDeltas 5 7 8 9 10 0.0424 0.1225 0.2144 0.3073 0.3135 0.0389 0.0976 0.161 0.2046 0.2762 0. 0.0299 0.0735 0.1119 0.1451 0.1959 0.2698 0.1738 0.0303 0.0702 0.0716 0.1112 0.1501 0.1833 0.2475 0.1358 0.028 0.0624 0.0717 0.0894 0.1092 0.1307 0.1729 0.2198 0.1159 0.0279 0.0479 0.0646 0.0659 0.1045 0.1066 0.1355 0.1622 0.1942 0."
        },
        {
            "title": "Coeffcients cj\ni",
            "content": "0.0 0.0 0.0 1.17 1.07 1.83 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.93 0. 0.0 0.0 0.0 0.0 0.71 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.04 1.62 2.98 1.32 0.0 0.0 2.52 2.04 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.76 0. 0.0 0.0 0.0 0.0 0.0 0.66 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 1.53 2.09 0.0 0.0 0.0 0.93 1.23 2.31 0.59 0.09 0.07 0.05 0.21 0.05 0.99 1.91 0.09 0.19 0. 0.55 1.47 0.37 0.67 1.79 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.92 0.78 0.06 0.02 0.16 0.02 0.12 0.1 0.16 0.0 0.0 0.0 0.0 0.0 1.7 0.0 0.52 1.76 1.8 0.98 0.24 1.36 0.22 0.06 0.02 0.18 0.14 0.02 0.02 0.0 0.0 0.0 0.0 0.0 0.0 0.12 1.1 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.32 1.72 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.93 0.63 1.29 0.39 0.11 1.41 0.83 1.59 0.07 0.05 0.27 1.53 0.27 0.07 0.11 0.03 0.05 0.15 0.01 0.27 0.07 0.03 0.21 0.03 0.09 0.15 0.15 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.17 1.15 0.19 0. 0.09 0.99 0.25 0.21 1.71 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.95 0.59 1.17 0.35 0.11 1.45 0.75 1.49 0.13 0.01 0.05 0.05 0.31 0.05 0.03 0.09 0.03 0.15 0. 0.29 1.59 0.23 0.07 0.09 0.03 0.03 0.09 0.17 0.03 0.15 0.11 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.17 1.19 0.27 0.03 0.91 0.09 0.05 0.25 0.05 0.05 0.79 0.05 0.07 1. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 19 Differentiable Solver Search for Fast Diffusion Sampling J.2. Solver Searched on FlowDCN-B/2 NFE TimeDeltas 0.0 1.26 1.38 2.26 0.0 0. 0.0 0.0 Coeffcients cj 0.0 0.0 0.0 0.0 0.0 0.92 0.0 0.0 0.0 0.0 0.0 0.7 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.22 0.0 1.12 2.0 0.3 0.0 0.0 0.9 1.56 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.74 0.0 0.0 0.0 0.0 0.0 0.0 0.62 0.0 0.0 0.0 0.0 0.0 0. 5 6 7 8 9 0.0521 0.1475 0.2114 0.2797 0.3092 0.0391 0.0924 0.165 0.2015 0.2511 0.2511 0.0387 0.0748 0.103 0.1537 0.184 0.234 0.2117 0.0071 0.0613 0.078 0.1163 0.1421 0.188 0.2077 0.1996 0.0017 0.051 0.0636 0.0911 0.1007 0.1443 0.1694 0.191 0.1872 0.0016 0.0538 0.0347 0.0853 0.0853 0.1198 0.1351 0.165 0.1788 0.1406 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.43 1.57 1.53 2.29 0.0 0.0 0.0 1.11 1.03 1.99 0.07 0.21 0.15 0.05 0. 0.07 0.23 0.31 0.41 0.61 1.33 0.17 0.59 1.31 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.43 0.61 1.55 0.99 0.11 2.07 0.05 0.49 0.05 0.33 0.09 0. 1.33 1.93 0.73 1.71 0.23 0.25 0.29 0.05 0.21 0.01 0.25 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.61 1.45 0.25 0.41 1.25 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 6.19 0.11 0.81 0.73 0.17 1.37 0.19 1.45 0.31 0.05 0.29 0.03 0.23 0.05 0.19 0.01 0.21 0.13 0.23 0.15 0.17 0.35 1.35 0.21 0.17 0.11 0.19 0.0 0.0 0.0 0.0 0.0 0.25 1.23 0.09 0. 0.09 1.09 0.23 0.17 1.21 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 7.8801 0.4 0.48 0.26 0.0 0.1 0.18 0.12 0.16 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.74 0.18 0.86 0.04 0.04 1.28 0.26 0.06 0.08 0.06 0.14 0.08 0.1 0.16 0.14 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.26 1.42 0.2 0.1 0.04 0.0 0.22 1.24 0.08 0.08 0. 0.14 1.06 0.06 0.08 0.08 1.02 0.14 0.34 1.38 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Differentiable Solver Search for Fast Diffusion Sampling J.3. Solver Searched on DiT-XL/2 NFE TimeDeltas 5 7 8 9 10 0.2582 0.1766 0.1766 0.2156 0.1731 0.2483 0.1506 0.1476 0.1568 0.1733 0. 0.2241 0.1415 0.1205 0.1158 0.1443 0.1627 0.0911 0.2033 0.1476 0.1094 0.099 0.1116 0.1233 0.131 0.0748 0.1959 0.1313 0.1142 0.0863 0.0898 0.0916 0.1119 0.1054 0.0735 0.2174 0.1123 0.1037 0.0724 0.0681 0.0816 0.0938 0.0977 0.0849 0."
        },
        {
            "title": "Coeffcients cj\ni",
            "content": "0.0 0.0 0.0 1.43 0.93 1.55 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.69 0. 0.0 0.0 0.0 0.0 0.59 0.0 0.0 0.0 0.0 0.0 0.0 1.36 0.9 0.08 0.0 0.0 0.0 0.0 1.84 0.5 0.0 0.0 0.0 0.0 0.0 1.08 0.0 0.0 0.0 0.0 0.0 0.0 0.56 0.0 0.0 0.0 0.0 0.0 0.0 0.56 0.0 1.38 1.08 0.28 0.0 0.0 0.0 0.0 0.0 2.02 0.78 1. 0.0 0.0 0.0 0.0 0.64 1.5 0.0 0.0 0.0 0.0 0.0 0.26 1.0 0.2 1.4901e 08 0.1 0.06 0.0 0.06 0.06 0.02 0.1 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.14 1.76 0.8 0.48 1.62 0.02 0.62 1.42 0.06 0.12 0.04 0.12 0.1 0.06 0.04 0.06 0.02 0.04 0.04 0.0 0.0 0.0 0.0 0.0 0.26 1.12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.04 0. 0.16 1.04 0.08 0.08 0.56 0.14 0.12 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.28 0.78 1.62 0.44 1.48 0.02 0.36 0.1 0.16 0.06 0.04 0.22 0.08 0.1 0.04 0.04 0.04 0.04 0.0 0.04 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.3 0.12 1.08 0.24 0.06 0.86 0.0 0.02 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.06 0.08 0.5 0.02 0.14 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.74 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.17 0.35 0.99 0.25 0.11 0.99 0.05 0.07 0.85 0.03 0.03 0.03 0.21 0.11 0.67 0.01 0.03 0.01 0.01 0.03 0.03 0.07 0.03 0.03 0.03 0.03 0.01 0.01 0.01 0.01 0.25 0.09 0.93 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.09 0.03 0.81 0.05 0.03 0.01 0.11 0.27 0.07 0.01 0.05 0.57 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0."
        }
    ],
    "affiliations": [
        "Shanghai AI Lab, Shanghai, China",
        "State Key Lab of Novel Software Technology, Nanjing University, Nanjing, China",
        "Taobao & Tmall Group of Alibaba, Hangzhou, China"
    ]
}