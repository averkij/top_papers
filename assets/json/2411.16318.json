{
    "paper_title": "One Diffusion to Generate Them All",
    "authors": [
        "Duong H. Le",
        "Tuan Pham",
        "Sangho Lee",
        "Christopher Clark",
        "Aniruddha Kembhavi",
        "Stephan Mandt",
        "Ranjay Krishna",
        "Jiasen Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce OneDiffusion, a versatile, large-scale diffusion model that seamlessly supports bidirectional image synthesis and understanding across diverse tasks. It enables conditional generation from inputs such as text, depth, pose, layout, and semantic maps, while also handling tasks like image deblurring, upscaling, and reverse processes such as depth estimation and segmentation. Additionally, OneDiffusion allows for multi-view generation, camera pose estimation, and instant personalization using sequential image inputs. Our model takes a straightforward yet effective approach by treating all tasks as frame sequences with varying noise scales during training, allowing any frame to act as a conditioning image at inference time. Our unified training framework removes the need for specialized architectures, supports scalable multi-task training, and adapts smoothly to any resolution, enhancing both generalization and scalability. Experimental results demonstrate competitive performance across tasks in both generation and prediction such as text-to-image, multiview generation, ID preservation, depth estimation and camera pose estimation despite relatively small training dataset. Our code and checkpoint are freely available at https://github.com/lehduong/OneDiffusion"
        },
        {
            "title": "Start",
            "content": "Duong H. Le1, Aniruddha Kembhavi1 1AI2 Tuan Pham2, Sangho Lee1 Christopher Clark1 Stephan Mandt2 Ranjay Krishna1,3 Jiasen Lu1 2 University of California, Irvine 3 University of Washington * Equal contribution 4 2 0 2 5 2 ] . [ 1 8 1 3 6 1 . 1 1 4 2 : r Figure 1. OneDiffusion is unified diffusion model designed for both image synthesis and understanding across diverse tasks. It supports text-to-image generation (red box), conditional image generation from input images (orange box) and its reverse task Image understanding (orange box). It can also perform ID customization (blue box), and multi-view generation (purple box) with arbitrary number of input and output images."
        },
        {
            "title": "Abstract",
            "content": "We introduce OneDiffusion, versatile, large-scale diffusion model that seamlessly supports bidirectional image It ensynthesis and understanding across diverse tasks. ables conditional generation from inputs such as text, depth, pose, layout, and semantic maps, while also handling tasks like image deblurring, upscaling, and reverse processes such as depth estimation and segmentation. Additionally, OneDiffusion allows for multi-view generation, camera pose estimation, and instant personalization using sequential image inputs. Our model takes straightforward yet effective approach by treating all tasks as frame sequences with varying noise scales during training, allowing any frame to act as conditioning image at inference time. Our unified training framework removes the need for specialized architectures, supports scalable multi-task training, and adapts smoothly to any resolution, enhancing both generalization and scalability. Experimental results demonstrate competitive performance across tasks in both generation and prediction such as text-to-image, multiview generation, ID preservation, depth estimation and camera pose estimation despite relatively small training dataset. Our code and checkpoint are freely available at https: //github.com/lehduong/OneDiffusion. 1. Introduction Diffusion models, particularly in text-to-image (T2I) generation, have recently shown remarkable results. Models such as DALL-E [43], Imagen [43], and Stable Diffusion [15, 41, 47] have established new benchmarks for generating high-quality, photorealistic images from text prompts. Additionally, recent studies have demonstrated the effectiveness of diffusion models in various other computer vision tasks, such as depth estimation [22] or optical flow estimation [37, 49], etc. However, despite these advancements, diffusion models are typically trained individually for either T2I generation or specific tasks. In contrast, large language models (LLMs) (e.g. GPT4 [1]) have demonstrated their ability to function as universal models. They can perform wide range of tasks across different domains without the need for task-specific modules, and can effectively handle tasks they havent been explicitly trained on zero-shot. This universality has been immensely valuable; it has dramatically simplified using, training and scaling these models, and ultimately lead to better performance. This incentivizes us to ask whether diffusion models can become universal in similar way. Designing unified architecture for diverse image synthesis tasks presents significant challenges. Current methods often depend on external add-ons to handle new tasks. For example, ControlNet [68] or T2I-Adapter [39] require specialized module to encode the conditional inputs, and personalization models typically require encoding the identity through pretrained facial recognition network and adding auxiliary losses to preserve identity [21, 58, 63]. Additionally, tasks vary widely in input requirements. For instance, multi-view generation alone requires handling arbitrary input-output view combinations, posed or unposed images, and camera pose conditioning [18, 24, 34, 50, 57], while image understanding tasks require diverse outputs such as depth, pose, or segmentation. Finally, existing training recipes are often tightly tuned to particular tasks and therefore cannot be relied on to generalize between tasks. In this work, we present OneDiffusion unified diffusion model that seamlessly supports bidirectional image synthesis and understanding across diverse tasks. Our approach enables single model to perform multiple tasks without the need for external losses and add-ons. Inspired by recent advances in diffusion models for sequential data [7, 48, 69], we model all conditions and target images as sequence of views with varying noise levels during training. At inference, any of the views can be used as conditional input, or set to noise and then used to generate an output image. Conditioning text can also be changed to define the task, and specify additional conditioning details (e.g. camera pose). The simple, but flexible, framework allows our model to support many kinds image generation and image understanding tasks with unified architecture and training objective. To train our model we create the One-Gen dataset, which integrates high-quality data across variety of sources. This includes standard T2I data along with synthetic outputs from state-of-the-art models to support range of tasks such as depth estimation, segmentation, pose estimation, etc. Our dataset also incorporates data for ID customization and multiview generation, providing diverse conditioning setups. The One-Gen dataset enables scalable joint training across multiple tasks, flexible conditioning options, improved generalization, and serves as solid foundation for our model. To demonstrate how general-purpose our training algorithm is, we train OneDiffusion completely from scratch. First, we train on text-to-image to equip the model with general image synthesis abilities, then on One-Gen to learn the full set of tasks. Our final model has 2.8 billion parameters and is equipped with diverse set of skill, shown in Figure 1. The model also adapts naturally to various resolutions, enabling zero-shot high-resolution generation for sequence tasks even when such resolutions were not encountered during training. We evaluate OneDiffusion on diverse set of tasks for both generative and predictive tasks. On T2I, OneDiffusion efficiently generates high-quality imIn ages while utilizing fewer number of parameters. the multiview generation task, OneDiffusion demonstrates performance comparable to state-of-the-art methods that are specifically designed and exclusively trained for this purpose. On depth estimation, OneDiffusion is competitive with SoTA exhibits performance that diffusion-based depth estimation baselines. We also show OneDiffusion supports novel conditioning setups, such as text-to-multi-view and image-to-multi-view. For highvariability tasks like face identification from single image, the model is capable of generating multiple consistent images featuring flexible expressions and poses, demonstrating strong generalization to unseen domains. Unified diffusion models. Several attempts have been made to unify diffusion model for different type of controls [42, 60, 70]. However, they are limited to utilization of multiple image conditions. These models usually requires to design complicated adapters for different conditions. [35, 36, 55, 71] propose unified models for language and images. Concurrently, [59] propose finetuning multimodal large language model with diffusion objective on diverse tasks like text-to-image, editing, and subject-driven generation etc. In contrast, our model distinguishes itself by leveraging bidirectional capabilities of diffusion models and addressing wide range of diverse tasks. 2. Related work 3. Methodology Diffusion models for generative task. Recent advancements in diffusion models have greatly improved image generation capabilities, with models like Stable Diffusion [3, 4, 15, 41, 47, 72] setting new standards in text-to-image synthesis. Building on this foundation, controllable diffusion models such as ControlNet [68] and T2I-Adapter [39] extend functionality by enabling fine-grained control through auxiliary inputs like edge maps, depth maps, or human poses. Meanwhile, instruct-Pix2Pix [5] introduces natural language-guided image editing, making these tools more user-friendly. For personalized applications, identityfocused models, including IP-Adapter [63], InstantID [58], PhotoMaker [27], and PuLiD [21], personalize generation by conditioning on reference images. Meanwhile, in multiview generation, recent methods [18, 34, 50, 57], employ camera ray embeddings or 3D geometry to achieve consistent viewpoints. Together, these innovations showcase the versatility of diffusion models in delivering controllable, personalized, and multi-perspective image synthesis. Diffusion models for predictive tasks. Beyond image generation and manipulation, diffusion models have also proven effective for predictive tasks within computer vision. Marigold [22] fine-tunes the Stable Diffusion model [47] to perform monocular depth estimation, demonstrating the adaptability of diffusion models for prediction-based applications. Furthermore, diffusion models have been utilized for optical flow estimation, as shown in the works of Saxena et al. [49] and Luo et al. [37], where the models predict pixel-level motion between consecutive frames. Additionally, Li et al. [26] trained diffusion model for openvocabulary semantic segmentation, showcasing the potential of these models for more complex vision tasks. Prior works have attempt to unify diffusion model for predictive tasks [17, 19]. These studies show that diffusion models are not only useful for generating images but also highly effective for various predictive tasks in computer vision. 3 3.1. Flow matching for generative modeling Flow matching [2, 30, 33] is framework for training continuous-time generative models by learning timedependent vector field that transports between two probability distributions. More specifically, time-dependent vector field ut : [0, 1] Rd Rd governs the transformation from base distribution p0 to the target distribution p1 through an ODE dx = ut(x)dt. The solution of this ODE is flow ϕt : [0, 1] Rd Rd with initial condition ϕ0(x) = x, and this flow characterizes push-forward operation pt = [ϕt]#p0, in which pt is the density of samples p0 transported by from time 0 to time t. The goal is approximate this ODE using learned time-dependent vector field parameterized as neural network vθ(t, x). Due to the intractable nature of ut, [30] proposed to learn vθ(t, x) using the conditional flow matching (CFM) objective: LCFM(θ) := Et,q(z),pt(xz) vθ(t, x) ut(xz)2 (1) This objective is equivalent to the original flow matching objective, and only requires the samples from the target distribution and suitable conditional probability path. 3.2. Proposed Approach Objective. We cast the problem of image generation with Inspired multimodal conditions as sequential modeling. by previous work on diffusion model for sequential data [7, 48, 69], we jointly model all conditions and target images as sequence of views. Note that the number of views is determined by tasks. Particularly, = 1 for text-to-image tasks, = 2 for image-to-image translation such depth/pose/image editing etc, > 2 for multiview generation or ID customization. Mathematically, i=1 RHW let views {xi}N be sampled from training dataset q(x1, ..., xN ). Given time variables ti, our goal to learn function [0, 1]N RN HW vθ(t1, ..., tN , x1, ..., xN ) is : Figure 2. Illustration of training and inference pipeline for OneDiffusion. We encode the desired task for each sample via special task token. During training we independently sample different diffusion timesteps for each view and add noise to them accordingly. In inference, we replace input image(s) with Gaussian noises while setting timesteps of conditions to 0. RHW D. Intuitively, vθ serves as generalized timedependent vector field where each input xi paired with its respective time variable ti. Learning vθ enables arbitrary conditional generation, where any subset of views can be selected as conditions to generate the remaining views, as explained below. This setup allows us to dynamically configure the generation process, supporting flexible applications across range of generative tasks. Training. Our training pipeline is visualized on the left side of Figure 2. At each training step, we independently sample ti LogNorm(0, 1) [15] and Gaussian noise ϵi (0, I). This results in different noise levels for each views. We apply an interpolation-based forward process: This flow matching objective [2, 30, 33] guides the model to learn the optimal velocity field vθ by minimizing the difference from the target velocity field u. Inference. Our framework allows for both joint sampling and conditional sampling across any chosen set of views. In details, we define the target views we want to sample as xK = {xi}iK, and the set of conditional views as xK = {xi}i /K. To perform conditional sampling, we start by initializing the target views xK as Gaussian noise. At each timestep t, we compute the corresponding timedependent vector field vK θ (t, xxK) by fixing the conditional views to their known values xK = and setting their time variables to zero tK = 0; while keeping the time variables of the target views as tK = t: xti = αti xi + βtiϵi (2) vK θ (t, xxK = x) = vθ(tK = t, tK = 0, xK = x, xK = x) where αt and βt satisfy the boundary conditions α0 = 0, α1 = 1 and β0 = 1, β1 = 0. Similar to [72], we adopt the linear interpolation schedule: xti = tixi + (1 ti)ϵi the corresponding velocity field ui for each view xi is: ui(ti, xi) = xi ϵi (3) (4) with the aggregated target as = (x1 ϵ1, ..., xN ϵN ) RN HW D, our training loss is the joint flow-matching objective: L(θ) = (cid:2)vθ(t1, ..., tN , x1, ..., xN ) u2(cid:3) (5) (6) Note that unlike vθ, vK is valid time dependent vector θ field, as all the views in now has the same t. Thus, by integrating this vector field using an ordinary differential equation (ODE) solver, we can generate the conditional samples we are interested in. We illustration the inference on the right side of Figure 2. 3.3. Implementation Details Model architecture. We adopt the Next-DiT architecture [72] in our model. By leveraging full transformer-based architecture, our model can work with different numbers of views . We independently encode each frame i.e. images and conditions as latent RN HW with VAE tokenizer [15] and concatenate them in dimension. With 4 Figure 3. High-resolution samples from text of our OneDiffusion model, showcasing its capabilities in precise prompt adherence, attention to fine details, and high image quality across wide variety of styles. flexible , our approach establishes universal framework which supports diverse input-modality with variable length. Following [72], we also apply 3D RoPE [53] for positional encoding, enabling generalization to different resolutions and aspect ratios. Text-to-Image (1 views). With only single view, training and inference follow the same process as standard text-to-image diffusion models. We prepend the task label [[text2image]] to the caption to specify the task. Image-to-Image (2 views). We set the first view as the target image and the second as the conditioning input. During inference, we can use one or both views for generation, and the model is trained to produce the target image. For tasks like bounding box or semantic map generation, we add the hexadecimal color code and class label to the prompt. For instance, to segment mouse with yellow mask, is: [[semantic2image]] <#FFFF00 the prompt yellow mask: mouse> photo of ... Further details are provided in the appendix. ID Customization (2-4 views). We sample images of the same individual across views, concatenating captions for each input image and using token [[imgX]] to denote each image. We also prepend the task label [[faceid]] to the captions. At inference, we can condition on an arbitrary number of images and generate multiple outputs, leading to more consistent results. Multiview Generation (4-12 views). Inspired by [18], we use Plucker ray embeddings to represent camera poses. For each image patch, we calculate Plucker coordinates as = (o d, d) using its ray origin and direction d. The result embedding has dimensions H/8W/86, matching the spatial size of the latent, and is replicated across channels to form 16 channel embedding. Unlike [18], we treat ray embedding as independent view following image latents as unified sequence rather than concatenating by channels. This design allows flexible denoising, enabling multi-view image generation conditioned on camera poses or sampling ray embeddings to predict poses from image conditions, similar to RayDiffusion [67]. We scale the ray embeddings to have unit variance, as in [47]."
        },
        {
            "title": "As with other",
            "content": "tasks, we prepend the task label [[multiview]] to the caption. During inference, we can substitute images or Plucker ray embeddings with Gaussian noise for multi-view generation and camera pose estimation, respectively. Training details Our model is trained from scratch using flow-matching objective. Similar to prior works [8, 15], we use three stage training recipe. In the first stage, we pretrained the text-to-image model with resolution of 2562 (500K steps) and 5122 (500K steps). In the second stage, we continue training on mixed of tasks, using 5122 for T2I and 2562 for other tasks, for total of 1M steps. Finally, in the last stage, we finetune the model at high resolution of (1024) for T2I. For ID customization fine-tuning, we use 2-5 views. For fewer views (2-3), we apply resolution of 5122, while for more views, we use 2562 resolution. 5 Figure 4. Illustration of our model capability to generate HED, depth, human pose, semantic mask, and bounding box from input image. For semantic segmentation, we segment the sword (highlighted in yellow) and the moon (highlighted in cyan) the first example, while segmenting road (yellow), sky (cyan) in the second. For object detection, We localize the head and moon (both highlighted in cyan). Leveraging these conditions, we can reverse the process to recreate variant of the input image based on the same caption. Additionally, we can edit the image by modifying specific elements, such as replacing the moon with Saturn (last example). During training, we use an in-batch sampling strategy at each stage, sampling tasks (T2I, Image-to-Image, ID customization, and multiview generation) with equal probability. The noise schedulers shift value is set to 3, as suggested in [15]. We use AdamW optimizer with learning rate η = 0.0005. Training is performed on TPU v3-256 pod with global batch size of 256 in the first two phases, and the final fine-tuning stage is completed on 64 H100 GPUs using the same configuration. 4. One-Gen Datasets Text-to-Image We leverage both public and internal (synthetic) datasets. The public datasets including: PixelProse [52], Unsplash, Coyo [6], JourneyDB [40]. Additionally, we use 10M internal synthetic dataset consisting of images re-captioned with LLaVA-NeXT [31] and Molmo [11]. The length of the text description for each image varies from 100 to 150 words. When an original prompt is available, we use both the LLM-generated caption and the original caption. Image-to-Image For simpler tasks like deblurring, inpainting, image generation from canny edge, or upscaling, we use 1M -sample subset of our synthetic data and apply the relevant pre-processor for each image to create an input condition for it. For more complex tasks, we create synthetic dataset from outputs generated by Midjourney, Stable Diffusion [15], and Flux-dev following the process outlined below: Semantic Map and Detection For each image, we use the LLaVA-NeXT [31] model to identify entities or subjects (e.g., person, shirt, dog, building), with maximum of 10 entities per image. Based on these subject names from LLaVA-Next, we perform semantic segmentation using SAM [23] and extract bounding boxes. Each class is assigned random color from predefined list. This dataset contains 350K triplets consisting of semantic map, bounding box, and the original image. Depth Map We generate the depth dataset by applying DepthAnything-v2 [62] to 500K images sampled from various datasets, including both real and synthetic images. Additionally, we caption 40K images from Hypersim dataset [46] with LLaVA-NeXT and incorporate these into the training set. Human Poses We collect different subset with 50K images, primarily featuring human for pose conditioning. We use YOLOv5 to detect the bounding boxes for region of interests and apply ViTPose [61] for pose estimation. ID Customization We collect dataset of celebrities and characters from games and movies by from publicly available images. After filtering to ensure each subject has at least four images and removing NSFW content, the dataset includes approximately 60K subjects and total of 1.3M images. We caption these images using the LLaVA-NeXT. Multiview Generation We use the DL3DV-10K dataset [29], Objaverse [10], CO3D [45]. For Objaverse dataset, we utilize the 80K filtered split from LGM [54] and caption provided by Cap3D [38]. In the DL3DV dataset, we sample an image from each scene and caption it using LLaVA-Next. For CO3D, we exclude captions and include only the task token in the text input. 6 Figure 5. Illustration of the multiview generation with single input image. We equally slice the azimuth in range of [45, 60] and elevation in range of [15, 45] for the left scenes. For the right scene, the azimuth range is set to [0; 360] and elevation range is set to [15; 15]. Methods LUMINA-Next [72] PixArt-Σ [9] SDXL [41] PlayGroundv2.5 [25] IF-XL SD3-medium [15] Hunyuan-DiT [28] DALLE3 FLUX-dev FLUX-schnell OneDiffusion Params (B) 2.0 0.6 2.6 2.6 5.5 2.0 1.5 12.0 12.0 2.8 # Data (M) GenEval 14 33 1200 1000 75 0.46 0.54 0.55 0.56 0.61 0.62 0.63 0.67 0.67 0.71 0.65 Table 1. GenEval benchmark at resolution of 1024 1024. Comparison of text-to-image performance on the 5. Experiments In this section, we evaluate our OneDiffusion model on broad range of image generation and understanding tasks. We do not perform task-specific finetuning in any results. Details about additional qualitive exampels are in Appendix. 5.1. Text-to-Image Qualitative results of OneDiffusion for text-to-image task is illustrated in Figure 3. Thanks to the diversity of our One-Gen dataset, the model can handle various art styles, spanning both artistic and photorealistic designs. Following previous works [15], we evaluated the textto-image capabilities of our model on GenEval benchmark [20]. For each prompt, we generate 4 images using Euler solver with 100 steps and guidance scale of 5. The results for OneDiffusion, along with those of baseline models, are presented in Table 1. Our model demonstrates strong performance compared to similarly sized baselines, excelling in multitasking capabilities despite being trained on relatively smaller dataset.This performance is largely attributed to the diversity of the dataset and the comprehensive captions provided for each sample. 5.2. Controllable Image generation We show the experiment with image-to-image translation using various source domains, including HED, depth map, Model Zero123 [32] Zero123-XL [12] EscherNet [24]"
        },
        {
            "title": "OneDiffusion",
            "content": "Condition 1-view 1-view 1-view 2-view 3-view 1-view 2-view (unknown poses) 2-view (known poses) 3-view (unknown poses) 3-view (known poses) PSNR 18.51 18.93 20.24 22.91 24.09 19.01 19.83 20.22 20.64 21.79 Table 2. Comparison of NVS metrics across different number of condition view settings. Increasing the number of condition views improves the reconstruction quality. human pose, semantic map, bounding boxes. We report the qualitative results in Figure 4 and Figure 19 in appendix. The results demonstrate that OneDiffusion effectively aligns with the conditioning image across wide range of input conditions during the generation process, leveraging pure attention mechanisms and supplementary information from captions. 5.3. Multiview Generation We evaluate the multiview generation capabilities of our method using the Google Scanned Object dataset [13]. Table 2 presents comparison of our approach (OneDiffusion) with state-of-the-art methods, including Zero123 [32], Zero123-XL [12], and EscherNet [15]. It is important to note that these models are specifically designed and exclusively trained for the multiview generation task. In contrast, our model supports variable number of conditional inputs and, due to its flexible denoising framework, can incorporate additional conditional views even when their camera poses are unknown. As shown in Table 2, OneDiffusion consistently outperforms Zero123 and Zero123-XL in the 1-view condition. Additionally, OneDiffusion demonstrates strong performance even when camera poses are unknown. For in the 2-view condition with unknown poses, instance, OneDiffusion achieves PSNR of 19.83, which is only marginally lower than the PSNR of 20.22 obtained when 7 Figure 6. Illustration of ID customization using reference images. Unlike prior methods that rely on face embeddings and often fail to generalize, our model demonstrates superior generalization. It effectively adjusts facial expressions and gaze directions (first row), changes viewpoints (second row), and even customizes non-human IDs (third row). All results in the third row are generated from single reference image, while InstantID fails as its face detector cannot detect faces in the input. camera poses are known. Similarly, in the 3-view condition, OneDiffusion achieves PSNR values of 20.64 and 21.79 for unknown and known poses, respectively. These results highlight the adaptability and effectiveness of our method across varying input conditions, demonstrating its potential for practical applications in multiview generation. Figure 5 presents two qualitative examples of our multiview generation from single front-view image. Our model successfully generates consistent views across varying azimuths and elevations. For additional visualizations, please refer to Figures 10 and 11 in Appendix. As result of the flexibility of our framework, we can also perform text-tomultiview directly by masking all images and only inputing the camera poses as in Figure 12 in Appendix. 5.4. ID Customization We further evaluate OneDiffusion on ID customization tasks, which involve using one or multiple ID images as inputs for personalized generation. To assess performance, we compare with STOA methods, including InstantID [58], PuLID [21], and PhotoMaker [27], using both qualitative and quantitative analyses. Our evaluation extends beyond the standard benchmark (unsplash-50 [16]) to test generalization on ID customization tasks, such as varying expressions, viewpoints, and even non-human images. Figure 6 illustrates examples of altering facial expressions and gaze directions (first row), changing viewpoints (second row), and customizing non-human IDs (third row). Method ID CLIP-T PhotoMaker [27] InstantID [58] PuLID [21] Ours 0.193 0.648 0.654 0. 27.38 26.41 31.23 26.80 Table 3. Quantitative results on Unsplash-50. Our method achieves success in these tasks, where all other methods fail. Unlike previous approaches that rely on face embeddings and primarily replicate the original face, OneDiffusion employs attention mechanisms between images and text conditions. This enables flexible end-toend training and generates more expressive outputs, making our method suitable for wider range of applications. Intuitively, the mechanism that ensures consistent multiview generation also proves effective for manipulating camera angles in ID customization, highlighting its adaptability across related applications. Additional visualizations are provided in Figure 13 and 14. We also present the quantitative results on the Unsplash50 [16] benchmark in Table 3. This benchmark focuses solely on style changes and re-contextualization, where PuLID [21] demonstrates strong performance by leveraging embeddings from ID encoder networks trained on human faces for discrimination tasks. While this approach effectively preserves the identity traits of input images, it faces significant limitations when handling more complex face manipulations. 8 Figure 7. Qualitative comparison between diffusion-based depth estimation - Marigold [22] and our methods."
        },
        {
            "title": "Method",
            "content": "DiverseDepth [64] MiDaS [44] DPT [44] LeReS [65] Omnidata [14] HDN [66] Marigold [22] DepthAnything-2 [62] Ours NYUv"
        },
        {
            "title": "DIODE",
            "content": "AbsRel 11.7 11.1 9.8 9.0 7.4 6.9 6.0 4.6 6.8 δ1 AbsRel 87.5 88.5 90.3 91.6 94.5 94.8 95.9 97.7 95.2 37.6 33.2 18.2 27.1 33.9 24.6 31.0 27.1 29.4 δ1 63.1 71.5 75.8 76.6 74.2 78.0 77.2 74.8 75.2 Table 4. Comparison of depth estimation methods on NYUv2 and DIODE datasets. 5.5. Depth Estimation For image understanding tasks, we evaluate our models performance on monocular depth estimation using standard benchmarks: NYUv2 [51] and DIODE [56]. We report the quantitative results in Table 4.Quantitative results are presented in Table 4, where our model achieves competitive performance compared to baselines that leverage pretrained text-to-image diffusion models, such as Marigold [22]. However, as shown in Figure 7, our model demonstrates greater robustness than diffusion-based depth estimators like Marigold. Specifically, it excels in handling open-world images, including paintings, hazy weather, and unconventional textures. For further qualitative comparisons, refer to Figures 15 and 16. 5.6. Camera Pose Estimation We evaluate our model on camera pose estimation using the Google Scanned Object dataset [13]. For this task, we use six rendered images of each synthetic object and estimate the camera poses by denoising the corresponding ray embeddings. Following RayDiffusion [67], we apply least squares optimization to estimate the camera centers and rotations. The camera center accuracy, measured with threshold of 0.3, is reported in Table 5. Figure 8. Qualitative comparison between RayDiffusion and OneDiffusion on GSO dataset. OneDiffusion yields better prediction. Method RayDiffusion [67] OneDiffusion Accuracy 0.20 0. Table 5. Comparison of zero-shot camera pose estimation methods on the GSO dataset, evaluated by Camera Center Accuracy at threshold of 0.3. Figure 8 provides qualitative comparison between our model and RayDiffusion. RayDiffusion consistently predicts camera poses in the upper hemisphere due to the bias in its training data, such as CO3D, which predominantly features upper-hemisphere views. In contrast, thanks to the diversity of our large-scale training dataset, OneDiffusion achieves higher accuracy and avoids this limitation. 5.7. Other Tasks Since directly extracting masks, bounding boxes, and keypoints from raw output images is not straightforward, we provide several qualitative results for human pose estimation and semantic segmentation on COCO dataset in Figures 17 and 18 in the appendix, respectively. Since our model does not distinguish tasks of conditions and images its performance on understanding tasks during training, serves as an additional indicator of the model. We leave further exploration of this aspect for future work. 9 6. Conclusion experiments demonstrate that OneDiffusion Our achieves impressive results across variety of tasks, including conditional T2I generation, depth estimation, open vocabulary semantic segmentation, pose estimation, multi-view generation, ID customization and camera pose estimation. We believe this work advances the capabilities of diffusion models, providing versatile and scalable solution comparable to the flexibility offered by large language models. This represents significant step toward developing general-purpose vision model that can serve as the backbone for wide variety of applications."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Michael Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. arXiv preprint arXiv:2209.15571, 2022. 3, 4 [3] Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, Zach Eaton-Rosen, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. 3 [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 3 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 3 [6] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: https : / / github . com / Image-text pair dataset. kakaobrain/coyo-dataset, 2022. [7] Boyuan Chen, Diego Marti Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. arXiv preprint arXiv:2407.01392, 2024. 2, 3 [8] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. 5 [9] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. arXiv preprint arXiv:2403.04692, 2024. 7 Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: In CVF universe of annotated 3d objects. 2023 ieee. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1314213153, 2022. 6 [11] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Christopher Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Christopher Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jennifer Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Marie Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hanna Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 6, 1 [12] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. Advances in Neural Information Processing Systems, 36, 2024. [13] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: highquality dataset of 3d scanned household items. In 2022 International Conference on Robotics and Automation (ICRA), pages 25532560. IEEE, 2022. 7, 9 [14] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: scalable pipeline for making multitask mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1078610796, 2021. 9 [15] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 3, 4, 5, 6, 7 [16] Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Lcmlookahead for encoder-based text-to-image personalization. arXiv preprint arXiv:2404.03620, 2024. 8 [17] Yulu Gan, Sungwoo Park, Alexander Schubert, Anthony Philippakis, and Ahmed Alaa. Instructcv: Instructiontuned text-to-image diffusion models as vision generalists. arXiv preprint arXiv:2310.00390, 2023. 3 [18] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan Barron, and Ben Poole. Cat3d: Create anything arXiv preprint in 3d with multi-view diffusion models. arXiv:2405.10314, 2024. 2, 3, [10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana [19] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, 10 Han Hu, et al. Instructdiffusion: generalist modeling interface for vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1270912720, 2024. 3 [20] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. Advances in Neural Information Processing Systems, 36, 2024. 7 [21] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, and Qian He. Pulid: Pure and lightning id customization via contrastive alignment. arXiv preprint arXiv:2404.16022, 2024. 2, 3, 8, [22] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth In Proceedings of the IEEE/CVF Conference estimation. on Computer Vision and Pattern Recognition, pages 9492 9502, 2024. 2, 3, 9, 1, 7, 8 [23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 6 [24] Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, and Andrew Davison. Eschernet: generaIn Proceedings of tive model for scalable view synthesis. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95039513, 2024. 2, 7 [25] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. 7 [26] Ziyi Li, Qinye Zhou, Xiaoyun Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Open-vocabulary object segmentation with diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7667 7676, 2023. 3 [27] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing reIn CVPR, alistic human photos via stacked id embedding. 2024. 3, 8, 1 [28] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, et al. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. arXiv preprint arXiv:2405.08748, 2024. 7 [29] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. [30] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In The Eleventh International Conference on Learning Representations, 2023. 3, 4 [31] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 6 [32] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-toIn Proceedings of 3: Zero-shot one image to 3d object. the IEEE/CVF international conference on computer vision, pages 92989309, 2023. 7 [33] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3, 4 [34] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 2, 3 [35] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations, 2022. 3 [36] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal arXiv models with vision, language, audio, and action. preprint arXiv:2312.17172, 2023. [37] Ao Luo, Xin Li, Fan Yang, Jiangyu Liu, Haoqiang Fan, Flowdiffuser: Advancing optical and Shuaicheng Liu. In Proceedings of flow estimation with diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1916719176, 2024. 2, 3 [38] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pretrained models. Advances in Neural Information Processing Systems, 36, 2024. 6 [39] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 42964304, 2024. 2, 3 [40] Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, JourYi Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. neydb: benchmark for generative image understanding, 2023. 6 [41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 3, 7 [42] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, et al. Unicontrol: unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023. 3, [43] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. 2 11 [44] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. 9 [45] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation In Proceedings of of real-life 3d category reconstruction. the IEEE/CVF international conference on computer vision, pages 1090110911, 2021. 6 [46] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synIn thetic dataset for holistic indoor scene understanding. Proceedings of the IEEE/CVF international conference on computer vision, pages 1091210922, 2021. 6 [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, [48] David Ruhe, Jonathan Heek, Tim Salimans, and Emiel arXiv preprint Hoogeboom. Rolling diffusion models. arXiv:2402.09470, 2024. 2, 3 [49] Saurabh Saxena, Charles Herrmann, Junhwa Hur, Abhishek Kar, Mohammad Norouzi, Deqing Sun, and David Fleet. The surprising effectiveness of diffusion models for optical flow and monocular depth estimation. Advances in Neural Information Processing Systems, 36, 2024. 2, 3 [50] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. 2, 3 [51] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Indoor segmentation and support inference from Fergus. In Computer VisionECCV 2012: 12th Eurgbd images. ropean Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part 12, pages 746760. Springer, 2012. 9 [52] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang, Abhinav Bhatele, Gowthami Somepalli, and Tom Goldstein. From pixels to prose: large dataset of dense image captions. arXiv preprint arXiv:2406.10328, 2024. 6 [53] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [54] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054, 2024. 6 [55] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 3 [56] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo, Haochen Wang, Falcon Dai, Andrea Daniele, Mohammadreza Mostajabi, Steven Basart, Matthew Walter, et al. Diode: dense indoor and outdoor depth dataset. arXiv preprint arXiv:1908.00463, 2019. 9 [57] Peng Wang and Yichun Shi. multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. 2, Imagedream: Image-prompt arXiv preprint [58] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 2, 3, 8, 1 [59] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 3 [60] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 77547765, 2023. 3 [61] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. Advances in Neural Information Processing Systems, 35:3857138584, 2022. 6 [62] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. 6, 9, 1, 7, 8 [63] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, [64] Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu, Changming Sun, and Dou Renyin. Diversedepth: Affine-invariant depth prediction using diverse data. arXiv preprint arXiv:2002.00569, 2020. 9 [65] Wei Yin, Jianming Zhang, Oliver Wang, Simon Niklaus, Long Mai, Simon Chen, and Chunhua Shen. Learning to In Proceedrecover 3d scene shape from single image. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 204213, 2021. 9 [66] Chi Zhang, Wei Yin, Billzb Wang, Gang Yu, Bin Fu, and Chunhua Shen. Hierarchical normalization for robust monocular depth estimation. Advances in Neural Information Processing Systems, 35:1412814139, 2022. 9 [67] Jason Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan, and Shubham Tulsiani. Cameras as rays: Pose estimation via ray diffusion. arXiv preprint arXiv:2402.14817, 2024. 5, 9 [68] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2, 3 [69] Zihan Zhang, Richard Liu, Rana Hanocka, and Kfir Aberman. Tedi: Temporally-entangled diffusion for long-term motion synthesis. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, [70] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-Yee Wong. 12 Uni-controlnet: All-in-one control to text-to-image diffusion models.(may 2023), 2023. 3, 1 [71] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 3 [72] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv preprint arXiv:2406.18583, 2024. 3, 4, 5,"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Additional Qualitative results ID Customization. We report ours results for ID Customization tasks in Figure 13 and Figure 14. It can be observed from Figure 13 that OneDiffusion well preserve the identity of person with single input and highly manipulatable. It can re-contextualize the image (as in first prompt), change the style from realistic photo to Pixar style (second prompt) and modify the medium to watercolor painting (third prompt). Moreover, our approach does not relying on face embedding as in previous works [21, 27, 58] making it highly versatile. As illustrated in Figure 14, our model can preserver highly details, intricate structure as armor of the person in 4th row. OneDiffusion can also work with non-human subject as the Gundam robot in 3rd row. The model performs well with other style than photorealistic input such as anime (1st row), 3D figure (2nd row), cartoon (5th row). Our model is highly editable where we can control the style, human pose, camera angle, expression. Multiview generation We report additional results for multiview generation in Figure 10 and Figure 11. The generation process is as follow: we set the azimuth ranges to [0.45, 0.6] and elevation ranges to [15, 45], except for the last row of Figure 11. Then we equally slice these ranges to 80 views. We first generate 3 anchor views from the input image and independently synthesize subsequent images based on input image and the nearest anchor. For each generation batch, we generate 3 novels view and condition on 2 images. We report views with index in [0, 10, 20, , 70] in below figures. OneDiffusion is capable of generating photorealistic results of arbitrary objects or scenes from any number of input views either realistic captured (2nd, 3rd row of Figure 11) or synthesized images (Figure 10). Our model works best for camera trajectory covering front views of scene. As mentioned ealier, OneDiffusion can also generate consistent multiview images from pure text and without any input images. Specifically, we simply input the azimuth and elevation as input for camera poses and generate all images from Gaussian noises as in Figure 12. Depth estimation We provide additional qualitative results of OneDiffusion and compare it with MarigoldLCM [22] and DepthAnything-v2 [62] in Figure 15 and 16. We can see that our model estimator is more robust than Marigold on open-world test suits and is highly correlated with output of DepthAnything model. 1 Figure 9. Distribution of training datasets for all tasks. Segments proportional to sampling rates. The inner section shows the supercategory of target tasks, it can be observed that we train the model with equal budget for text-to-image, image-to-image and multiview generation. The outer section shows datasets used for each super-category. Human Pose estimation We report additional results for pose estimation on COCO dataset in Figure 17. It can be observed that our model can predict multiple people in an image without relying on object detector models. Semantic Segmentation We report qualitative results of semantic segmentation on COCO dataset in Figure 18. Unlike previous models [42, 70], our semantic-to-image and vice verse does not enforce hard association between colors and the target classes. We provide the color masks and class name as additional input in caption. 8. Summary Datasets We train the model on multiple datasets reported in Section 4 and illustrated in Figure 9. The pie-chart segment each dataset proportional to the sampling rate of it in third stage of the training process. We train the model with equal budget for text-to-image, image-to-image translation (2 frames), and multiview generations (2 6 frames). Note that we filter and only use subset of COYO with 11M images in our training. Due to the missing samples during download process, the LAION-aesthetic dataset only has 6M images. We recaption the LAION-aesthetic dataset with Molmo [11]. Figure 10. Qualitative results of image-to-multiview generation. The left most images are input. We equally slice the azimuth in range of [45, 60] and elevation in range of [15, 45] for all scenes. 2 Figure 11. Qualitative results of image-to-multiview generation. We equally slice the azimuth in range of [45, 60] and elevation in range of [15, 45] for the first 3 scenes. For the last scene, the azimuth range is set to [0; 360] and elevation range is set to [15; 15]. 3 4 Figure 12. Qualitative results of text-to-multiview generation. The azimuth and elevation of left to right columns are [0, 30, 60, 90] and [0, 10, 20, 30], respectively. We use following prefix for all prompts to improve the quality and realism of generated images: photorealistic, masterpiece, highly detail, score 9, score 8 up. Figure 13. Qualitative results of OneDiffusion for (single reference) ID Customization task with photo of human faces. The left most images are input, target prompts for left to right columns are: 1) Photo of man/woman wearing suit at Shibuya at night. He/She is looking at the camera, 2) pixarstyle, cartoon, person in pixar style sitting on crowded street, 3) watercolor drawing of man/woman with Space Needle in background 5 Figure 14. Qualitative results of OneDiffusion for (single reference) ID Customization task with photo of of non-human subjects or cartoon style input. OneDiffusion is highly versatile and can produce good results for all kind of input and not limited to photorealistic human images. Since we rely on attention, the model can attend to the condition view and preserve intricate details and is not limited by any bottleneck e.g. latent representation. 6 Figure 15. Qualitative comparison for depth estimation between OneDiffusion, Marigold [22] and DepthAnything-v2 [62] Figure 16. Qualitative comparison for depth estimation between OneDiffusion, Marigold [22] and DepthAnything-v2 [62] 8 9 Figure 17. Qualitative examples of human pose estimation on COCO datasets. Figure 18. Qualitative examples of semantic segmentation on COCO datasets. The target class for each image (from left to right, from top to bottom) are (sheep, grass, mountain, sky), (apple, person, building), (vase, flower, ), (dog, book, sheet), (umbrella, person, building, gate), (boat, dock, drum). Figure 19. Illustration of our model capability to generate semantic mask, detection, human pose, depth, and canny edge from input image. For semantic segmentation, we segment the flower (highlighted in yellow) and the rock (highlighted in green). For object detection, We localize the backpack (highlighted in yellow) and butterfly (highlighted in cyan). Leveraging these conditions, we can reverse the process to recreate variant of the input image based on the same caption."
        }
    ],
    "affiliations": [
        "AI2",
        "University of California, Irvine",
        "University of Washington"
    ]
}