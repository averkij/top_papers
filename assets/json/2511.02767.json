{
    "paper_title": "Dynamic Reflections: Probing Video Representations with Text Alignment",
    "authors": [
        "Tyler Zhu",
        "Tengda Han",
        "Leonidas Guibas",
        "Viorica Pătrăucean",
        "Maks Ovsjanikov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at https://video-prh.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 7 6 7 2 0 . 1 1 5 2 : r Preprint. Under review. DYNAMIC REFLECTIONS: PROBING VIDEO REPRESENTATIONS WITH TEXT ALIGNMENT Tyler Zhu Tengda Han Leonidas Guibas Viorica Patraucean Maks Ovsjanikov Princeton University Google DeepMind"
        },
        {
            "title": "ABSTRACT",
            "content": "The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. collection) data provided at test time, especially when using state-of-theart video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to generalpurpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data."
        },
        {
            "title": "INTRODUCTION",
            "content": "A hallmark of general intelligence is the ability to reason about the world in its different manifestations and to integrate different sensory data into unified mental representations (Ernst & Bulthoff, 2004; Kudithipudi et al., 2022). The pursuit of such unified representations is also fundamental to the development of capable multimodal AI systems and for the advancement of embodied agents that must operate within complex world (Xie et al., 2024; Fung et al., 2025). In recent years, significant progress has been made towards this goal. The success of multimodal learning approaches has highlighted the potential of joint training paradigms (Radford et al., 2021; Yin et al., 2024; Liu et al., 2024b; Dubey et al., 2024; Team et al., 2023; 2025), while parallel line of research has provided evidence that even unimodal models, when trained at scale, develop representations with strong structural similarities (Huh et al., 2024; Maniparambil et al., 2024a; Tjandrasuwita et al., 2025). This observation has given rise to the Platonic Representation Hypothesis (PRH), which posits that neural networks converge towards shared statistical model of reality in their latent spaces (Huh et al., 2024). Building on this, several recent works have established wide range of techniques to evaluate and exploit cross-modal alignment, in both zero-shot and few-shot scenarios (Maniparambil et al., 2024a; Jha et al., 2025; Schnaus et al., 2025; Hadgi et al., 2025). However, prior work has focused almost exclusively on the alignment of static modalities: primarily images and text. In other words, although the Platonic Representation Hypothesis has been stated in full generality, so far, its validity has only been evaluated on static data, leaving its applicability to dynamic, temporal domains an open question. Consequently, the rich information contained in video: motion, causality and temporal dependencies, exhibited, e.g., in intricate human interactions Work performed while the author was at Google DeepMind. 1 Preprint. Under review. (Gu et al., 2018; Wang et al., 2023), has been largely overlooked in the context of representation alignment. notable limitation of previous studies focusing on static modalities was raised in Huh et al. (2024), where the authors pointed out: the maximum theoretical value for the alignment metric is 1. Is score of 0.16 indicative of strong alignment [...] or does it signify poor alignment with major differences left to explain? We leave this as an open question. In this work, we provide partial answer to this open question by showing that the limited alignment observed previously is, in large part, due to the impoverished data given at test time. Specifically, we demonstrate that by considering multiple video frames instead of single image, as well as diverse set of captions instead of single annotation, the alignment score can be improved significantly, achieving close to 0.4 in some cases, without modifying the underlying trained models. This result highlights, for the first time, that large improvements in alignment can be achieved through efforts at test time, which is complementary to the training-time resources (model size, amount of training data, etc.) considered in prior work. It also suggests that multi-frame approaches can capture different aspects of scene better, and points to the possibility of strong zero-shot evaluation metric capable of probing the representation power of both image and video encoders. More broadly, in this work we extend previous cross-modal alignment studies into the temporal domain by conducting the first comprehensive investigation of video-text representation similarity. We introduce robust evaluation framework to probe and compare the capabilities of 121 modern video and language models. Our findings reveal that the rich, temporal nature of video provides powerful signal for semantic grounding. Specifically, we demonstrate that (1) state-of-the-art selfsupervised video encoders, e.g., VideoMAEv2 (Wang et al., 2023), achieve competitive alignment with text compared to top-performing image encoders, e.g., DINOv2 (Oquab et al., 2023); (2) we observe that alignment quality is highly sensitive to the richness of the provided visual and textual data, and (3) there is non-trivial relation between the video-text alignment and the performance of video models on downstream tasks. Finally, we highlight challenging situations with hard negative examples, in which current video and text models struggle, leaving room for future improvement."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Our work is situated at the intersection of three domains: the study of emergent representation alignment, self-supervised video understanding, and multimodal learning. We first review the recent work in representation convergence, then discuss the most prominent paradigms for learning from video data, and finally, contextualize our work by highlighting the specific gap in probing the alignment of unimodal trained video models. The Platonic Representation Hypothesis and Emergent Alignment in Static Modalities foundational concept for our investigation is the Platonic Representation Hypothesis (PRH), introduced by Huh et al. (2024). The PRH posits that as neural networks are scaled in terms of capacity, data diversity, and task variety, their learned internal representations converge toward shared, universal statistical model of reality. This converged latent structure is termed the Platonic representation, drawing an analogy to Platos notion of an ideal reality that underlies our sensory observations. This hypothesis has been empirically tested by measuring the geometric similarity of representation spaces, obtained using wide range of encoders. The primary method for this is comparing the representational kernels, which characterize how models measure dissimilarity between data points (Kornblith et al., 2019; Maniparambil et al., 2024b; Huh et al., 2024). The key finding supporting the PRH is that as unimodal vision and language models become more capable, the structure of their latent spaces becomes increasingly similar, even without explicit cross-modal supervision. This convergence has been most robustly demonstrated in the alignment of static modalities, primarily between images and text (Maniparambil et al., 2024b; Tjandrasuwita et al., 2025). For example, the work of Maniparambil et al. (2024b) showed that powerful, independently trained vision and language encoders develop representations with surprisingly high degree of semantic similarity. Furthermore, following prior work (Merullo et al., 2022), the authors of Maniparambil et al. (2024b) showed that these disparate latent spaces may differ only by simple, learnable linear transformation. more fundamental principle behind these and related results was offered in Huh et al. (2024), 1Our project page with code is at https://video-prh.github.io. 2 Preprint. Under review. which posited that since all modalities are projections of the same physical world, models trained at sufficient scale will inevitably converge on similar latent structures that reflect this shared reality (Huh et al., 2024; Tjandrasuwita et al., 2025). The vision encoders used to validate these claims are typically state-of-the-art self-supervised models like DINOv2 (Oquab et al., 2023). Recent work has also examined the conditions and methods related to this emergent alignment (Tjandrasuwita et al., 2025). Cross-modal alignment has also been assessed via alignment probing, which correlates emergent alignment potential with the representations clustering quality (k-NN performance) over linear separability (Zhang et al., 2025). The underlying universal structure has even been shown to be sufficiently strong to enable both unsupervised and weakly supervised translation between disparate embedding spaces (Jha et al., 2025; Zhang et al., 2025; Schnaus et al., 2025). While these studies investigate and exploit emergent alignment, they focus almost exclusively on static modalities. Our work complements these findings by conducting the first systematic probe of emergent alignment in the temporal domain, and shedding light on the dependence of alignment on test-time data in both text and visual domains. Learning Representations from Video Learning from video data presents unique challenges and opportunities due to the temporal dimension. Research in this area is divided into unimodal self-supervised learning, which focuses on spatiotemporal structure, and joint video-language pretraining, which focuses on explicit semantic alignment, e.g., Xu et al. (2021); Zhao et al. (2024). In the former category, masked autoencoding has proven highly effective (Tong et al., 2022; Bardes et al., 2024; Wang et al., 2023). This paradigm, inspired by its success in language and image domains, involves masking portions of the input video and training model to reconstruct the missing content in either pixel or feature spaces. Although initial approaches in masked video representation learning have focused on modest model sizes, recent efforts have also been made to scale such models to the billion parameter regime (Wang et al., 2023; Carreira et al., 2024; Assran et al., 2025). Critically, such models are pre-trained on vast, unlabeled video datasets with self-supervised (e.g., reconstruction) objectives. For example, VideoMAEv2 has no exposure to textual supervision, yet it learns powerful spatiotemporal features that achieve top performance on downstream action understanding tasks (Wang et al., 2023). This makes it the ideal subject for probing the existence of emergent semantic alignment. Furthermore, there is recent evidence that even unimodal video models, when trained at scale, tend to learn features that are useful in broad range of tasks (Wu et al., 2025; Carreira et al., 2024). We also note that evaluating self-supervised video representations is challenging, and current approaches rely on expensive task-specific training (Wang et al., 2023; Carreira et al., 2024; Hasson et al., 2025). There is thus strong need for zero-shot metrics with strong predictive power for video models. Bridging Unimodal Video and Text: Probing for Emergent Alignment Given the success of powerful unimodal encoders in both static (Oquab et al., 2023) and temporal (Wang et al., 2023) domains, and the evidence for emergent alignment in static data (Maniparambil et al., 2024b), natural question arises: does the Platonic Representation Hypothesis extend to the temporal domain? While our work is the first to conduct systematic probe of this question, we acknowledge related efforts that bridge video and text representations, e.g., (Kim et al., 2023; Liu et al., 2024a; Li et al., 2025). However, existing works in this domain typically rely on an existing source of alignment rather than investigating the intrinsic properties of video encoders."
        },
        {
            "title": "3 OUR APPROACH",
            "content": "At high-level, our approach follows the methodology introduced in Huh et al. (2024), which uses mutual k-NN metric to measure the similarity across different modalities. We adapt this approach to our setting, extending it to measure the multi-frame (or multi-clip) and multi-caption similarity of video-text pairs. Our setting is illustrated in Figure 1. At its core, the Platonic representation alignment is calculated in the following stages. Dataset. We assume test set of video-caption pairs: = (V, C) = {(v1, c1), . . . , (vN , cN )}. Here vi is video, and ci is corresponding set of text captions for this video. Each caption cij in the set ci provides textual description of the given video vi. Encoding. We embed each video using video encoder into some video embedding space Evid(vi) = vi Rp. Similarly, we encode each set of captions ci into text embedding space using text encoder: Etext(ci) = ci Rq. Importantly, the dimensionality of the embedding 3 Preprint. Under review. Figure 1: Scaling both the number of video frames and text captions at test time improves alignment. Given paired video vi and set of captions ci, leveraging rich multi-frame and multicaption information at test time leads to improved alignment, measured in terms of mutual k-NN. spaces, and are typically different, making it difficult to directly compare distances. We define RN and RN to be the matrices obtained by stacking all of the video and text embeddings respectively. For simplicity we denote = Evid(V ), and = Etext(C). Alignment metric. The Mutual k-NN (MkNN) metric introduced in Huh et al. (2024) measures the agreement between the nearest neighbor structure in two different embedding spaces. Given two feature sets RN p, RN q, we first construct two binary indicator matrices MX and MY, both of size , s.t., (MX)ij = 1 if element (row) is among the k-nearest neighbors of element (row) in space X, and 0 otherwise. The Mutual k-NN alignment is then calculated as: AMkNN(X, Y) = 1 kN (cid:88) (cid:88) (MX MY)ij. i=1 j=1 (1) Here, denotes the Hadamard product (element-wise multiplication) of the two indicator matrices. Observe that the operation in Eq. (1) represents simply the mean overlap (alignment) in the sets of nearest neighbors, normalized by hyperparameter (typically set to = 10 for dataset of 1024 examples). Additionally, we also follow previous work and optimize over the choice of intermediate layers for both encoders, and pick the pair of layers that maximizes the alignment score. Incorporating multi-instance data. As mentioned above, our main goal is to extend previous approaches, which focused on static instances to datasets of videos (treated as multi-frame sequences) paired with potentially multiple diverse captions; see Sec. 4 for detailed description of our data sources. For video vi and given video encoder Evid, which natively processes clips with no frames, we extract the indices of nf frames through uniform linear interpolation to study the effect of using an increasing number of frames nf at test-time. Using one frame (nf = 1) corresponds to the setup used in prior works that focus on image-text alignment. For videos longer than no, we use nearest neighbor interpolation to extract increasing numbers of frames that are multiple of no, e.g. for no = 16, we consider nf {16, 32, 64, 80}. We pass the no-length sub-clips through the encoder and then average the representations over sub-clips. Similarly, for set of captions cij associated with given video vi, we can use anywhere from one caption to the full set of captions. We concatenate the set of selected captions into single string and use text-based encoders (including LLM-based ones as discussed below) to extract their intermediate features. When an encoder produces per-token embedding, we average the features along the token dimension to obtain [layer, hidden dimension]-shaped feature, associated with each video."
        },
        {
            "title": "4 DATA AND MODELS",
            "content": "Data. We evaluate video-text alignment on several datasets that contain paired video/text data. Our main investigations use VATEX and the Perception Encoder Video Datasets (PVD) (Wang et al., 2019; Bolya et al., 2025). We construct two test sets using 1024 videos randomly sampled from VATEX and PVD. Each video in VATEX lasts around 10 seconds and is taken from unique YouTube video. They are captioned by 10 different annotators in English and Chinese (we only use the English captions). In other words, each video contains 10 different captions (each caption is produced by different annotator and is typically sentence with around 15 words on average). This provides rich source of diversity which we can use to vary the amount of textual information used for 4 Preprint. Under review. Figure 2: Video representations are strongly aligned with text. We measure alignment between modern vision encoders and Gemma 2 9B-it (subset of all vision models shown for clarity) on the VATEX video dataset using single caption for each video. In addition to the points, we plot linear regression of alignment scores to representation strength for three different LLMs. Our takeaways are threefold: (1) The strongest models for both alignment and retrieval are large video models ( ) (2) Averaging over frames is simple yet effective baseline for image models on video input ( ), while image-only models are limited to scores below 22% ( ), in line with Huh et al. (2024). (3) Recent text models have improved alignment with vision models, shown by the regression lines. alignment. To achieve similar effect with PVD, we use Gemini-2.5 Pro to split each fine-grained caption into 10 separate captions in similar style to VATEX; see Sec A.3 for more details. Models. We experimented with wide range of both video and image encoders, as well as aggregation strategies. For image models, we consider two variants: first, we simply use single (first) frame ( ); second, to introduce naive temporal dynamics, we averaged the image features across 8 frames in the temporal dimension. We denote the resulting method as image model on video ( ). For video models ( ), we consider range of encoders, including self-supervised ones like VideoMAE and VideoMAEv2 (Tong et al., 2022; Wang et al., 2023), as well as partially text-supervised ones like VideoPrism (Zhao et al., 2024). We also evaluate recent vision models, such as the Perception Encoder and DINOv3 (Bolya et al., 2025; Simeoni et al., 2025). We make distinction between text-aligned and self-supervised models and use the former as an upper bound for our alignment evaluation. For image models which are trained on video data, i.e., Perception Encoder (Bolya et al., 2025), we denote them as video models when tested on video input. In total, we test 85 different vision models and variants; see full list in the Appendix, Section A.2. For language models, we experimented with broad range of encoders, starting with the ones considered in Huh et al. (2024). We also consider recent unimodal language models from the Gemma 2 series (Team et al., 2024b) and evaluate their potential as text encoders. Given caption, we encode it with text encoder and then average the representation over the token sequence. As mentioned above, we store intermediate representations across all layers of the encoder and select the best pair of layers for each pair of vision and text models when measuring alignment."
        },
        {
            "title": "5 VIDEO-TEXT ALIGNMENT RESULTS",
            "content": "Our first results on the VATEX dataset (Wang et al., 2019) are summarized in Figure 2. These results are obtained by using randomly selected single (out of 10) caption for each video. We evaluate video understanding performance using nearest neighbour video retrieval on 10,000-video subsets of the Kinetics-400 and SSv2 test sets, respectively (Kay et al., 2017; Goyal et al., 2017). We use weighted k-NN accuracy as retrieval metric following NPID and DINO (Wu et al., 2018; Caron et al., 2021); note that this is different from our Mutual k-NN alignment metric. We sweep over the number of nearest neighbours and find 8 to give good results for majority of the models for 5 Preprint. Under review. Figure 3: Vision-text alignment scales strongly with the amount of visual and textual data available at test time. (left) Providing more frames increases vision-text alignment for both image and video models, with the latter being able to take advantage of more frames more effectively. (right) Providing more captions to the text model also significantly boosts alignment with vision models, across all frame counts. the retrieval task. We also report the alignment of the same vision models against wide range of language encoders on the VATEX dataset in the Appendix, Figure 11. We make several observations: first, when considering pure image and text models that were studied in prior work Huh et al. (2024), the alignment scores that we obtain closely follow previously reported values on other datasets (e.g., the alignment score between the best image model DINOv2 and the best text encoder outside of the Gemma family is 0.18). Secondly, we note that more recent text models in the Gemma-2 family lead to better alignment with video models, even if trained purely for text generation, highlighting the utility of these models as text encoders. Simply using powerful text encoder already increases the best image-text alignment score to approximately 0.206. This is consistent with, e.g., Zhang et al. (2025) which has highlighted the importance of the language model for multimodal alignment. Third, we note that basic temporal averaging across multiple frames with powerful image models exhibits remarkably high video-text alignment reaching alignment of approximately 0.223. Finally, there is very wide range of text-alignment scores across video models. Nevertheless, the highest alignment is achieved with self-supervised VideoMAEv2 model. This last result is notable, since it shows, both that natively-trained video models can outperform extremely strong image-based models (DINOv2) in text alignment, and that temporal dynamics play an important role in semantic grounding. Overall, these results above point to the potential, first, of using multi-frame video data for text alignment, and second, of using language models for computing informative embeddings. Since in all of our experiments, Gemma-based encoders achieved the best alignment, our subsequent experiments focus on that particular text encoder and investigate the role of vision encoders in text alignment."
        },
        {
            "title": "6 VIDEO-TEXT ALIGNMENT AND DATA DEPENDENCE",
            "content": "A key observation of our work is that the quality of the vision-text alignment depends strongly on the amount of visual and caption data given at test time. This is in direct complement to the focus of previous works that analyzed the dependence of the alignment on the size of the pre-training datasets or number of trainable parameters in different models. Figure 3 illustrates the dependence of vision-text alignment scores (against the Gemma 2-9b-it text encoder) on the number of frames from video and the number of associated captions in the VATEX dataset (Wang et al., 2019). Figure 3 (left) demonstrates how the average alignment score changes with an increasing number of frames for different caption counts and two vision models: VideoMAEv2 (Wang et al., 2023) and DINOv2 (Oquab et al., 2023). Observe that across all settings, the alignment scores generally increases as more frames are incorporated, suggesting that richer temporal context from the video leads to better alignment with textual descriptions. Figure 3 (right) complements the first by showing the alignment scores dependency on the number of captions used, for the same vision models and selection of frame counts. Increasing the number of captions leads to significant improvement in alignment scores, particularly when starting with 6 Preprint. Under review. small number of captions. We note that since in the VATEX dataset each caption corresponds to description of the same video provided by different user, increasing the number of captions increases both the coverage of the captured visual concepts, as well as the diversity of perspectives on the same concept and event. In Section A.1, we show alignment scores against multiple Gemma models when using single vs all captions in VATEX, reinforcing this same observation (Figure 12). These plots collectively underscore the importance of visual (frames) and textual (captions) data quantity in achieving high vision-text alignment, with distinct performance characteristics associated with different vision models. We emphasize that our results strongly complement the preliminary experiments in Sec. 6 of Huh et al. (2024), relating information density to alignment. We investigate, for the first time, using both multi-frame video data and multiple complementary captions, as opposed to single short description distilled from longer one as done in that work. We observe that having diverse captions, possibly describing the same visual concept in different ways, can boost alignment significantly. This effect is quantified in Figure 7, where linear fit finds that going from 1 to 10 captions improves alignment by 60% on average. To validate this behavior further, we demonstrate that even artificially synthesized text descriptions from single long video caption can obtain alignment that is higher than that of the source caption. Namely, in Section A.3, we find that our test time scaling approach improves alignment on the PVD dataset (Bolya et al., 2025), which does not naturally come with multiple diverse captions (Figure 8). Test-time Scaling Laws Building on the empirical observations in Figure 3, we also quantify these dependencies using predictive parametric model. We tested several formulations, but found that saturation-based model (Eq. 2) provided the best fit by significant margin: score(nf , nc) = (Cf nα + Ccnβ ). (2) Here nf and nc are the number of video frames and text captions given to specific pair of vision and text encoders (as in Fig. 1), represents the theoretical saturation score corresponding to ideal alignment of this vision/text model pair, whereas Cf , Cc, α and β are fitted scalar parameters. This formulation achieves remarkably high coefficients of determination for both VideoMAEv2 (R2 = 0.9791) and DINOv2 (R2 = 0.9964), with strong predictive power for test-time data scaling against the Gemma-2 text model. The fitted parameters are also highly informative: VideoMAEv2 (S 0.41, Cf = 0.15, Cc = 0.13, α = 0.75, β = 1.30) versus DINOv2 (S 0.37, Cf = 0.05, Cc = 0.13, α = 1.76, β = 1.4). Notably, the frame coefficient (Cf ) for VideoMAEv2 is nearly triple that of DINOv2, while the caption coefficients (Cc) remain comparable, highlighting the video models stronger ability to leverage temporal information from additional frames to improve alignment. This parametric analysis resembles compute-optimal scaling laws, such as those identified for pretraining, e.g., in Hoffmann et al. (2022), which model the dependency of loss function (e.g., Negative Log-Likelihood) on compute or data size. Whereas those laws are formulated in terms of loss metric (where lower is better), our metric is an alignment score (where higher is better), making our additive saturation model conceptually equivalent. The saturation score (e.g., 0.41 for VideoMAEv2) can be interpreted as the base accuracy of an ideal alignment process, while the subtracted terms represent the respective error penalties incurred by the approximation of providing only finite data at inference time. We note that such predictive models for test-time scaling can be useful to design strategies for multimodal data acquisition (e.g., when collecting multiple high-quality annotations of video data, which can be costly), as well as to compare the ability of different encoders to incorporate diverse data modalities. We include test-time scaling laws for other vision models in the Appendix, Section A.4."
        },
        {
            "title": "7 CROSS-MODAL ALIGNMENT AND DOWNSTREAM PERFORMANCE",
            "content": "Previous works, including Huh et al. (2024) and Maniparambil et al. (2024a), have highlighted that multimodal alignment between images and text is correlated with performance on unimodal tasks, i.e., image models that perform well on pure vision tasks (e.g., depth estimation) tend to also align well with language. Our goal is to evaluate whether similar claim holds for video encoders. 7 Preprint. Under review. Figure 4: Correlation between video/text alignment and downstream video perception task performance, for SSL methods trained without text supervision. We take advantage of recent work that trained large-scale unimodal video encoders and evaluate their performance on various semantic and non-semantic downstream tasks. We focus on video models trained without explicit text supervision, in order to better analyze the emergent alignment between representations, as opposed to models explicitly trained for video-language alignment (e.g. through video-text contrastive losses). In particular, we consider the best performing video models reported in Carreira et al. (2024) and correlate their vision-text feature alignment against Gemma 2-9b-it on the VATEX dataset to their accuracy on video analysis tasks: point tracking on the Perception Test dataset (Patraucean et al., 2023), box tracking on the Waymo Open dataset (Sun et al., 2020), camera pose estimation on RealEstate10k (Zhou et al., 2018b), ScanNet depth estimation (Dai et al., 2017), and action classification on the SSv2 (Goyal et al., 2017) and Kinetics-700-2020 (Kay et al., 2017; Smaira et al., 2020) datasets. Downstream accuracy is computed by training separate learnable attention-based decoder on top of the frozen video features for each task. Figure 4 summarizes our key results. Observe that generally for self-supervised video models, there is strong positive correlation between the cross-modal alignment scores and semantic tasks performance such as action classification on SSv2 and Kinetics. Interestingly, there is also significant correlation between the alignment score and the accuracy on non-semantic perception tasks such as camera pose estimation, depth prediction, and object tracking. The point tracking task represents notable exception, with weak correlation between text alignment and downstream performance. This can potentially be due to the highly local nature of the point tracking task, and also suggests room for improvement in terms of truly general purpose video encoders. Overall, these results suggest that video-text alignment could potentially be used as powerful zeroshot metric for probing the quality of video representations as an alternative or complementary to more expensive evaluation techniques that require repeatedly training multiple cross-modal decoders during self-supervised video model development."
        },
        {
            "title": "8 TEMPORAL ANALYSIS & CROSS-MODEL ALIGNMENT",
            "content": "Given that both modalities that we focus on follow the arrow of time, we can study how their representations encode temporal ordering via cross-modal alignment. We analyze temporal alignment on two datasets: the simple, synthetic Test of Time (Bagad et al., 2023) and the challenging long-form VideoComp (Kim et al., 2025). Test of Time. The synthetic dataset in Test of Time was specifically designed to probe the temporal abilities of video models. It contains 180 synthetic (video, caption) pairs, with captions of the form c1 circle appears {after, before} c2 circle, where c1, c2 are selected colors, and the videos are 30 frame clips enacting the caption with the shapes appearing in the same region of black square (upper left, upper right, bottom left, bottom right). For each choice of shape and {c1, c2}, there are four pairs of related captions: two for each choice of preposition and color order. Within these, there are two sets of logically equivalent captions (e.g., c1 before c2 and c2 after c1), for which their videos may be identical or different depending on the location of the shapes. 8 Preprint. Under review. Figure 5: Video and text eventually are information aligned, but encode temporal differently. While the video text alignment is largely perfect for all of the models once = 3, they differ heavily before that when looking at = 1, 2. Text models are often bag-of-words, while video models differ. Figure 6: Video embeddings are sensitive to temporal caption reordering. Video-text alignment drops against the temporal reorder negatives in VideoComp, with the most aligned video representations having the largest such drop (models towards the right). We use video-text alignment on this dataset to understand how sensitive text and vision models are to this temporal reordering. We find that when we take = 3 neighbors, many models get near perfect alignment as expected, since each example has 3 distinctly closer neighbors (Figure 5). However, the order in which each modality ranks these three neighbors differs significantly, as shown by the differing = 1, 2 alignments. We find that the language models tend to rank the neighbor which has the same words but ordered differently first, i.e. for c1 square appears after c2 square, the closest neighbor will be c2 square appears after c1 square. The other two related captions, c1 before c2 and c2 before c1, the logically equivalent statement, are equally close to the original caption. Thus, in this case, LLMs measure closeness more akin to bag-of-words than being temporally sensitive, at least at the shallower layers from where we are extracting the features. VideoComp. We also test multimodal alignment on VideoComp (Kim et al., 2025). We use VideoComps test set, sourced from YouTube and based on ActivityNet Captions and YouCook2 (Krishna et al., 2017; Zhou et al., 2018a). We use the temporal reorder examples which compare caption of video against negative caption describing the same events in different order. This results in 512 video-caption pairs, so we use lower = 5 for these experiments. To test the sensitivity to such negatives, we first compute the standard alignment using the text embeddings of positive captions. Then, we recompute neighbors in the text space, by considering the negative of each caption, say ci, and computing its neighbors to the positive captions of other videos. Since negative captions are still from the same overall video, just temporally shuffled, this tests if the model is still aligned with the content in temporally-agnostic manner. As shown in Figure 6, the alignment is lower with the negative captions than the positive captions, but not significantly. Notably, models with larger alignment suffer more of drop, suggesting that these models may be learning temporally-aware structures which are somewhat perturbed by the negative. However, there is still room for further improvement in their temporal awareness. Cross-model alignment. We also investigated the alignment of features across different video models, and report the results in the Appendix Section A.5. We observe that text-supervised and selfsupervised models, predictably, form clusters in pairwise alignment. However, interestingly, we note that some models (such as DINOv2) are able to span multiple clusters, and we hypothesize that such alignment against multiple models provides strong indicator of the versatility of given vision model. We leave complete investigation of this phenomenon as interesting future work."
        },
        {
            "title": "9 CONCLUSION, LIMITATIONS, AND FUTURE WORK",
            "content": "In this work, we conduct the first comprehensive study extending the Platonic Representation Hypothesis into the temporal domain. We demonstrate that alignment scores dramatically improve doubling in some cases simply by utilizing multiple video frames and diverse caption sets at inference, without any retraining. We quantify this phenomenon with novel, highly accurate (R2 > 0.98) saturation-based scaling law, which quantitatively confirms that native video mod9 Preprint. Under review. els (like VideoMAEv2) are fundamentally more able to leverage temporal information than static encoders (like DINOv2). Through an extensive analysis involving 121 vision and language models, we show that alignment against text encoders strongly correlates with downstream performance on both semantic and nonsemantic tasks, suggesting that vision-text alignment can be used as an informative zero-shot metric to guide video model development. At the same time, our analysis sheds light on some limitations of existing pure video foundation models, showing that many video models are outperformed by image models applied frame-by-frame. Finally, while generative models are promising direction for video models, it remains an open question of how we can best harness their latent representations for understanding, as currently their alignment to text is quite weak."
        },
        {
            "title": "10 ACKNOWLEDGEMENTS",
            "content": "The authors would like to thank Joe Heyward, Dilara Gokay, Yana Hasson, Morgane Riviere, and Pauline Luc (Google DeepMind) for many useful comments and discussions, as well as for help in running experiments presented in this work, and David Fan for cross referencing the WebSSL results."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "All our results can be reproduced using models and datasets hosted on HuggingFace or open-sourced by their authors; see Section A.2 in the Appendix for full list of models."
        },
        {
            "title": "REFERENCES",
            "content": "Kelsey Allen, Carl Doersch, Guangyao Zhou, Mohammed Suhail, Danny Driess, Ignacio Rocco, Yulia Rubanova, Thomas Kipf, Mehdi SM Sajjadi, Kevin Murphy, et al. Direct motion models for assessing generated videos. arXiv preprint arXiv:2505.00209, 2025. 18 Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇcic, and Cordelia Schmid. Vivit: video vision transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 68366846, 2021. 16 Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, et al. V-jepa 2: Self-supervised video models enable understanding, prediction and planning. arXiv preprint arXiv:2506.09985, 2025. 3, 16 Piyush Bagad, Makarand Tapaswi, and Cees G. M. Snoek. Test of Time: Instilling Video-Language Models with Sense of Time. In CVPR, 2023. 8 Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting feature prediction for learning visual representations from video. arXiv preprint arXiv:2404.08471, 2024. Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In International Conference on Machine Learning, pp. 813824. PMLR, 2021. 16 Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181, 2025. 4, 5, 7, 16 Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. 5 10 Preprint. Under review. Joao Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, et al. Scaling 4d representations. arXiv preprint arXiv:2412.15212, 2024. 3, 8, Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 58285839, 2017. 8 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. 1 Marc Ernst and Heinrich Bulthoff. Merging the senses into robust percept. Trends in cognitive sciences, 8(4):162169, 2004. 1 David Fan, Shengbang Tong, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, et al. Scaling language-free visual representation learning. arXiv preprint arXiv:2504.01017, 2025. 16 Pascale Fung, Yoram Bachrach, Asli Celikyilmaz, Kamalika Chaudhuri, Delong Chen, Willy Chung, Emmanuel Dupoux, Hongyu Gong, Herve Jegou, Alessandro Lazaric, et al. Embodied ai agents: Modeling the world. arXiv preprint arXiv:2506.22355, 2025. Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama. arXiv preprint arXiv:2307.12928, 2023. 16 Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The something something video database for learning and evaluating visual common sense. In Proceedings of the IEEE international conference on computer vision, pp. 58425850, 2017. 5, 8 Chunhui Gu, Chen Sun, David Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: video dataset of spatio-temporally localized atomic visual actions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 60476056, 2018. 2 Souhail Hadgi, Luca Moschella, Andrea Santilli, Diego Gomez, Qixing Huang, Emanuele Rodol`a, Simone Melzi, and Maks Ovsjanikov. Escaping platos cave: Towards the alignment of 3d and text latent spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1982519835, 2025. Yana Hasson, Pauline Luc, Liliane Momeni, Maks Ovsjanikov, Guillaume Le Moing, Alina Kuznetsova, Ira Ktena, Jennifer Sun, Skanda Koppula, Dilara Gokay, et al. Scivid: Crossdomain evaluation of video models in scientific applications. arXiv preprint arXiv:2507.03578, 2025. 3 Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. 16 Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. 7 Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. Position: The platonic representation hypothesis. In Hal Daume III and Aarti Singh (eds.), Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pp. 2061720642. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/ v235/huh24a.html. 1, 2, 3, 4, 5, 6, 7, 21 Rishi Jha, Collin Zhang, Vitaly Shmatikov, and John Morris. Harnessing the universal geometry of embeddings. arXiv preprint arXiv:2505.12540, 2025. 1, 11 Preprint. Under review. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset. arXiv preprint arXiv:1705.06950, 2017. 5, 8 Dahun Kim, AJ Piergiovanni, Ganesh Mallya, and Anelia Angelova. Videocomp: Advancing finegrained compositional and temporal alignment in video-text models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2906029070, 2025. 8, 9 Dahye Kim, Jungin Park, Jiyoung Lee, Seongheon Park, and Kwanghoon Sohn. Language-free training for zero-shot video grounding. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 11141123, 2023. 3 Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural network representations revisited. In International conference on machine learning, pp. 3519 3529. PMlR, 2019. Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In International Conference on Computer Vision (ICCV), 2017. 9 Dhireesha Kudithipudi, Mario Aguilar-Simon, Jonathan Babb, Maxim Bazhenov, Douglas Blackiston, Josh Bongard, Andrew Brna, Suraj Chakravarthi Raja, Nick Cheney, Jeff Clune, et al. Biological underpinnings for lifelong learning machines. Nature Machine Intelligence, 4(3):196 210, 2022. 1 Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022. 16 Chau-Yi Li, Chia-Hao Li, Pin-Yu Chen, and Chen-Kuo Chiang. ELIOT: zero-shot generative video-to-text retrieval framework, 2025. Han Liu, Yinwei Wei, and Liqiang Nie. Dreaming user multimodal representation for micro-video recommendation, 2024a. 3 Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pp. 216233. Springer, 2024b. 1 Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Mohamed El Amine Seddik, Sanath Narayan, Karttikeya Mangalam, and Noel OConnor. Do vision and language encoders represent the world similarly? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1433414343, 2024a. 1, 7 Mayug Maniparambil, Raiymbek Akshulakov, Yasser Abdelaziz Dahou Djilali, Mohamed El Amine Seddik, Sanath Narayan, Karttikeya Mangalam, and Noel OConnor. Do vision and language encoders represent the world similarly? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1433414343, 2024b. 2, 3 Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie Pavlick. Linearly mapping from image to text space. arXiv preprint arXiv:2209.15162, 2022. 2 Meta. Introducing llama 3.1: Our most capable models to date, 2024. URL https://ai.meta. com/blog/llama-3-1/. 16 Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision, 2023. 2, 3, 6, 16 12 Preprint. Under review. Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adria Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Mateusz Malinowski, Yi Yang, Carl Doersch, et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:4274842761, 2023. 8 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. 1, 15 Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. 16 Dominik Schnaus, Nikita Araslanov, and Daniel Cremers. Its (blind) match! towards visionlanguage correspondence without parallel data. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2498324992, 2025. 1, 3 Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, and Maxime et al. Oquab. Dinov3. arXiv preprint arXiv:2508.10104, 3(4), 2025. 5, 16 Lucas Smaira, Joao Carreira, Eric Noland, Ellen Clancy, Amy Wu, and Andrew Zisserman. short note on the kinetics-700-2020 human action dataset. arXiv preprint arXiv:2010.10864, 2020. 8 Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. arXiv preprint arXiv:2106.10270, 2021. 15 Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 24462454, 2020. 8 Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1 Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024a. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024b. 5, 16 Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. 1, 16 Megan Tjandrasuwita, Chanakya Ekbote, Liu Ziyin, and Paul Pu Liang. Understanding the emergence of multimodal representation alignment. arXiv preprint arXiv:2502.16282, 2025. 1, 2, 3 Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are dataefficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. 3, 5, 16, 18 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 16 Preprint. Under review. Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. VideoMAE v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1454914560, 2023. 2, 3, 5, 6, 16, 18 Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 45814591, 2019. 4, 5, 6 Yao-Chih-Lee Wu, Yi-Ting Chen, Yi-Hsuan Tsai, and Wen-Hsiang Liao. VideoREPA: videobased representation alignment framework for physics-enhanced video generation, 2025. 3 Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Unsupervised feature learning via nonparametric instance discrimination. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 37333742, 2018. Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. Large multimodal agents: survey. arXiv preprint arXiv:2402.15116, 2024. 1 Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. VideoCLIP: Contrastive pre-training for zero-shot video-text understanding. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 67876800. Association for Computational Linguistics, 2021. URL https://aclanthology.org/2021.emnlp-main.544. 3 Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12):nwae403, 2024. 1 Le Zhang, Qian Yang, and Aishwarya Agrawal. Assessing and learning alignment of unimodal In Proceedings of the Computer Vision and Pattern Recognition vision and language models. Conference, pp. 1460414614, 2025. 3, 6 Long Zhao, Nitesh Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, et al. Videoprism: foundational visual encoder for video understanding. arXiv preprint arXiv:2402.13217, 2024. 3, 5, 18 Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018a. 9 Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018b."
        },
        {
            "title": "A APPENDIX",
            "content": "The supplementary materials below provide additional results and illustrations that support and extend the claims made in the main paper. APPENDIX OVERVIEW The Appendix is structured as follows: Section A.1: Additional Results. We provide additional results on the VATEX dataset, visualizing alignment between range of language and vision models. We show alignment when using both one caption and all 10 captions from the VATEX dataset, highlighting the improvement across all vision and language models. Section A.2: List of models. We give full list of models that we used in our study, including details on both vision and language models. Section A.3: Details on caption synthesis and PVD alignment scores. We provide the details and qualitative examples of our approach for creating synthetic captions on the PVD dataset, and our resulting alignment scores. 14 Preprint. Under review. Section A.4: Additional test-time scaling laws. We provide additional examples of our test-time scaling laws for other models and datasets. Section A.5: Video-video model alignment. We illustrate alignment within different video models. A.1 ADDITIONAL RESULTS We include in Figure 11 the alignment scores of all the vision and text encoders considered in our analysis on the VATEX dataset; see the full list in Section A.2. The rows correspond to vision encoders and the columns correspond to language models. The label for each model has the format: <model name> (<max alignment score>) <num layers>. Each ij entry in the matrix contains the alignment score (also colour-coded: lighter shades correspond to higher alignment scores) and tuple (layeri, layerj) indicating from which layers in vision model and language model j, respectively, the representations were extracted to get the best alignment for that pair; we recommend zooming in for better readability. We used single caption for each video (out of 10) sampled randomly. As mentioned in Section 6, the alignment scores improve significantly when considering more video frames and/or more text captions. In Figure 12, we show additional results that confirm this observation. We calculate alignment scores between visual representations produced by subset of the vision encoders considered above and language representations given by different variants of text models when processing all of the captions from the VATEX dataset. As before, the rows correspond to vision encoders and the columns correspond to language models. For vision models, we consider: (1) image models applied on single frame of each video, (2) image models applied on every frame of the video and the representations averaged over time, and (3) video models. When using single caption, we observe that the best alignment scores are generally obtained by video models, which are able to encode not only features about the scene appearance, but also details about the scene dynamics, leading to better alignment with caption representations. notable exception is the Perception Encoder Core Giant image model applied on all the frames of the video, which obtains the best alignment score, but is also contrastively trained on images, videos, and text. Figure 7: Multiple captions provides consistent improvement in alignment. Going from 1 to 10 captions improves alignment by 60%, based on the best fit estimate. However, when we consider all the ten captions available for each video, the alignment scores improve significantly overall, and several image models applied on the full videos or even on single frames outperform many video models. This is further reinforced when we plot the alignment with one vs. all ten captions in Figure 7. The line of best fit is = 1.6x 0.018 with = 0.998, indicating that multi-caption data consistently improves alignment. A.2 FULL LIST OF MODELS In total, we analyze 121 models. We use the following 85 open-sourced visual models and their variants for our main analysis. All the models are available on HuggingFace or from the GitHub repositories of the authors. We use the class token when available, and fall back to averaging over all tokens as backup. This performed best experimentally. Key results are shown in Figure 2. AugReg [ , ] (Steiner et al., 2021): 8 variants of the image model across Tiny, Small, Base, and Large sizes, evaluated on image and video settings. CLIP [ , ] (Radford et al., 2021): 12 variants of the image model across Base, Large, and Huge sizes (including IN12k pre-training), evaluated on image and video settings. 15 Preprint. Under review. MAE [ , ] (He et al., 2022): 6 variants of the image model across Base, Large, and Huge sizes, evaluated on image and video settings. Timesformer [ ] (Bertasius et al., 2021): 2 video variants of ViT-Base, finetuned on either K400 or SSv2. ViViT [ ] (Arnab et al., 2021): 1 video variant (ViViT-B-16x2) finetuned on Kinetics-400. VideoMAE [ ] (Tong et al., 2022): 3 video variants, including Base (on SSv2 and Kinetics) and Huge (on Kinetics). VideoMAEv2 [ ] (Wang et al., 2023): 3 video variants across Base, Large, and Huge sizes. DINOv2 [ , ] (Oquab et al., 2023): 8 variants of the image model across Small, Base, Large, and Giant sizes, evaluated on image and video settings. WebSSL [ , ] (Fan et al., 2025): 6 variants of the image model across 1B, 5B, and 7B parameter sizes, evaluated on image and video settings. PE (Perception Encoder) [ , ] (Bolya et al., 2025): 24 variants of the image model across Core, Language, and Spatial types with sizes from Tiny to Giant, evaluated on image and video settings. V-JEPA 2 [ ] (Assran et al., 2025): 4 video variants covering Large, Huge, and Giant sizes with 384384 resolution Giant model as well. DINOv3 [ , ] (Simeoni et al., 2025): 8 variants of the image model across Base, Large, Huge, and 7B sizes, evaluated on image and video settings. We also look into 6 additional video models for their video-video alignment with each other: 4DS-e, TRAJAN, VideoMAEv2 k710 finetuned, V-JEPA v1, WALT, and VideoPrism (see Section A.5). For language models, we experiment with the following 30 open-sourced models and their variants: T5 (Raffel et al., 2020): 5 variants including Small, Base, Large, 3B, and 11B. BLOOM (Le Scao et al., 2022): 5 variants including 560M, 1.1B, 1.7B, 3B, and 7.1B. Llama (Touvron et al., 2023): 4 variants including 7B, 13B, 30B, and 65B. OpenLlama (Geng & Liu, 2023): 3 variants of an open-source reproduction of Llama including 3B, 7B, and 13B. Llama 3 (Meta, 2024): 4 variants in total: Llama 3.2-1B, Llama 3.2-3B, Llama 3.1-8B, and Llama 3.3-70B. Gemma (Team et al., 2024a): 2 variants including 2B and 7B. Gemma 2 (Team et al., 2024b): 3 instruction-tuned variants including 2B-it, 9B-it, and 27B-it. Gemma 3 (Team et al., 2025): 4 instruction-tuned variants including 1B-it, 4B-it, 12B-it, and 27B-it. A.3 SYNTHETIZING CAPTIONS FOR PVD DATASET USING LLMS To study the effect of including an increasing number of captions on the alignment score between video representations and the language representation of the associated captions, we synthesized multiple captions for videos in the PVD dataset using LLMs, starting from the detailed caption provided in the original dataset. Importantly, we encouraged the model to only use details present in the original caption, and not hallucinate new possible descriptions. We relied on Gemini 2.5 Pro. For reproducibility purposes, we provide below the prompt used to generate these captions, which was obtained from the prompt given to the VLM model when collecting the annotations for the original PVD dataset. We also include qualitative example in Figure 9. 16 Preprint. Under review. Prompt for Caption Generation Each The goal is not to summarize all of Review the caption for general context. Extract the most relevant and concise information. Dont include all of the key information in every caption. Pick some information Create ten concise captions of video using the provided detailed captions. caption should mention different details present. the information 10 different ways, but to get 10 different descriptions all potentially containing different information. TASK: Extract key information from the captions and combine it into set of ten captions, each of which is single phrase or set of phrases that includes some subset of the relevant details in alt text format. Steps to Follow: 1. 2. 3. to leave out between captions. 4. phrases with approximately 120 tokens, considering special characters like comma as part of the token count. 5. 6. What to Avoid: - Avoid adding or inferring information not present in the original metadata and captions. - Avoid using complex sentence structures or prioritizing sentence flow. Return list separated only by new lines, with only the captions, nothing else. Minimize the use of special characters and including everything in each caption. Pick only few things to include in each caption, its okay to leave details out. Combine extracted information into alt text format using short phrase or set of We show the effect of adding more frames or using multiple captions at test time in Figure 8. We re-iterate that Perception Encoder is trained on this dataset and sees text and videos together during training, but we include it as reference point regardless. On the left, we plot the alignment scores as we add more frames and average the resulting embeddings. We find that image-based models benefit slightly from this, but plateau quickly. Video models improve strongly with frames however, demonstrating their ability to utilize more visual information. On the right, we plot the alignment scores as we add more synthesized captions. The dashed lines indicate the baseline alignment score of each vision model when considering the original detailed caption. We can observe that for all models, using even just 3 captions leads to better alignment compared to the baseline score. This saturates around 6 captions for most models, pointing to potential redundancy between the synthesized captions beyond this number. Figure 8: Test time scaling is effective for other datasets. We apply our test time scaling for PVD. (left) Scaling with more frames is generally helpful, but is especially important for video models to improve. Here, frames are sampled linearly spaced. (right) Scaling the number of captions at test time, even synthesized ones, outperforms using the original provided caption (dotted lines). A.4 TEST-TIME SCALING LAWS FOR OTHER MODELS AND DATASETS We include test time scaling laws for other models on VaTeX and PVD in Table 1, extending the results from Section 6. Overall, most of the models had high R2. 17 Preprint. Under review. Human caption The video shows person making burger. The person is adding mayonnaise to one bun and then placing different vegetables and patty on top of it after that applies ketchup on another bun. The person is also using wooden boards to keep the ingredients and the burger. The person is wearing red and white checkered shirt. Synthesized captions 1. Person making burger. 2. Adding mayonnaise and vegetables to burger. 3. Burger assembled on wooden board. 4. Tutorial showing how to make burger. 5. Person in red and white checkered shirt making burger. 6. Well-lit video with vibrant colors showing burger preparation. 7. person makes burger using wooden board. 8. Tutorial on adding mayonnaise and vegetables to burger. 9. Person in red and white checkered shirt preparing burger. 10. Great tutorial for learning how to make burger. Figure 9: Example of video frames, the original human-provided caption, and LLM-synthesized captions for video in the PVD dataset. A.5 CROSS-MODEL (PAIRWISE VIDEO-VIDEO) ALIGNMENT) Figure 10 shows the pairwise feature alignment between selection of video models, as well as the Gemma2 text encoder. We include both MAE-based models (Tong et al., 2022), Kinetics-finetuned VideoMAEv2-G (Wang et al., 2023), self-supervised 4DS-e model (Carreira et al., 2024) as well as language-supervised model (Zhao et al., 2024), and recent pure motion encoder TRAJAN (Allen et al., 2025). We observe that there are several clusters appearing across video models, which seem to reflect language-alignment and spatio-temporal awareness (especially among MAE-based models). We hypothesize that although language alignment along is correlated with downstream performance, cross-model alignment might hold more predictive power. Specifically, strong, general-purpose models must align across multiple clusters to be useful in broad range of downstream (including pixel or patch-level) applications. 18 Preprint. Under review. Table 1: Comparison of Scaling Law Parameters for VaTeX and PVD Model dinov2 base dinov2 giant dinov2 large pe core giant R2 Type VaTeX 0.9928 0.287 0.9950 0.311 PVD VaTeX 0.9964 0.365 0.9979 0.343 PVD VaTeX 0.9955 0.339 0.9970 0.329 PVD VaTeX 0.9965 0.405 0.9976 0.417 PVD pe core large VaTeX 0.9710 0.109 timesformer base VaTeX 0.9442 0.297 0.9871 0.293 PVD videomaev2 base VaTeX 0.9788 0.334 0.9856 0.410 PVD videomaev2 huge VaTeX 0.9791 0.410 0.9878 0.296 PVD videomaev2 large PVD videoprism VaTeX 0.9832 0.400 0.9850 0. Cf 0.043 0.019 0.046 0.017 0.046 0.013 0.029 0.015 α 2.302 0. 1.762 1.415 1.852 3.704 1.853 2.238 Cc 0.095 0.101 0.132 0.114 0.120 0. 0.150 0.148 β 1.426 1.939 1.400 1.931 1.354 2.046 1.316 1. 0.014 1.695 0.027 1.396 0.118 0.063 0.156 0. 0.146 0.092 1.792 1.838 0.451 0.075 0.748 0.479 0.093 0.092 0.088 0. 0.128 0.079 1.357 1.867 1.221 1.619 1.302 1.939 0.145 0. 0.058 1.673 0.090 0.740 0.140 1. vivit 16x2 VaTeX 0.9789 0.209 0.9845 0.243 PVD 0.094 0.067 1.163 1.157 0.060 0.070 1.391 2. Figure 10: Video encoders cluster into semantic and geometric specialities. We find that videovideo alignment exhibits two clusters of models: those which are closer aligned to language and semantic tasks (bottom right), and those which are closer to geometric abilities (top left). However, some models are able to span both axes, notably DINOv2 and VideoMAEv2 K710 finetuned. 19 Preprint. Under review. Figure 11: We measure alignment between large number of vision and text encoders on the VATEX dataset, using only one caption for alignment. 20 Preprint. Under review. Figure 12: We measure alignment between subset of our highest scoring vision and text encoders in Figure 11 on the VATEX dataset, using all ten captions for alignment. We observe significant boost when integrating information across multiple captions. Notably, our best results significantly exceed those reported in Huh et al. (2024), highlighting that the paucity of annotations in both visual space (images vs. videos) and text space (single caption vs. multiple descriptions) can help to explain the limited alignment observed in prior work."
        }
    ],
    "affiliations": [
        "Google DeepMind",
        "Princeton University"
    ]
}