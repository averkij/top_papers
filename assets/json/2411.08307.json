{
    "paper_title": "PerceiverS: A Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation",
    "authors": [
        "Yungang Yi",
        "Weihua Li",
        "Matthew Kuo",
        "Quan Bai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Music generation has progressed significantly, especially in the domain of audio generation. However, generating symbolic music that is both long-structured and expressive remains a significant challenge. In this paper, we propose PerceiverS (Segmentation and Scale), a novel architecture designed to address this issue by leveraging both Effective Segmentation and Multi-Scale attention mechanisms. Our approach enhances symbolic music generation by simultaneously learning long-term structural dependencies and short-term expressive details. By combining cross-attention and self-attention in a Multi-Scale setting, PerceiverS captures long-range musical structure while preserving performance nuances. The proposed model, evaluated on datasets like Maestro, demonstrates improvements in generating coherent and diverse music with both structural consistency and expressive variation. The project demos and the generated music samples can be accessed through the link: https://perceivers.github.io."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 1 ] . [ 1 7 0 3 8 0 . 1 1 4 2 : r PerceiverS: Multi-Scale Perceiver with Effective Segmentation for Long-Term Expressive Symbolic Music Generation Yungang Yi, Weihua Li, Matthew Kuo, Quan Bai Auckland University of Technology, Auckland, New Zealand Email: pgv0915@autuni.ac.nz, weihua.li@aut.ac.nz, matthew.kuo@aut.ac.nz University of Tasmania, Tasmania, Australia Email: quan.bai@utas.edu.au AbstractMusic generation has progressed significantly, especially in the domain of audio generation. However, generating symbolic music that is both long-structured and expressive remains significant challenge. In this paper, we propose PerceiverS (Segmentation and Scale), novel architecture designed to address this issue by leveraging both Effective Segmentation and Multi-Scale attention mechanisms. Our approach enhances symbolic music generation by simultaneously learning longterm structural dependencies and short-term expressive details. By combining cross-attention and self-attention in MultiScale setting, PerceiverS captures long-range musical structure while preserving performance nuances. The proposed model, evaluated on datasets like Maestro, demonstrates improvements in generating coherent and diverse music with both structural consistency and expressive variation. The project demos and the generated music samples can be accessed through the link: https://perceivers.github.io. Index TermsSymbolic Music Generation, Long-Term Structure, Transformer Models, Cross-Attention, Self-Attention, PerceiverS, Effective Segmentation, Multi-Scale Attention I. INTRODUCTION Recent advancements in music generation, especially in audio generation models such as AudioLDM [1], MusicGen [2], and Jen-1 [3], have demonstrated significant progress, with these models capable of generating highly natural-sounding music. However, symbolic music generation, an area where models can generate and manipulate music in symbolic form, plays crucial role in the field of music generation, particularly due to its editable nature. This allows for operations such as cutting and rearranging different sections or substituting instrument timbres, enabling human involvement in high-quality music production during the post-processing stage. Compared to audio generation, symbolic music offers further level of abstraction, making it easier for machine learning models to capture deeper musical characteristics and understanding. However, the symbolic music generation still faces two key challenges. First, despite the advancements, many of the most expressive datasets, recorded from live performances and recording studios, are seldom used compared to manually created MIDI-file datasets. The primary reason is that they lack detailed annotations, making it harder for models to learn complex structures. Furthermore, due to computational limitations, these models cannot fully capture the context of an entire piece of music. Techniques, such as chunking and quantization, are often employed to reduce computational leading to the loss of crucial musical details complexity, and making it difficult for models to grasp the full structure of composition. Second, the waterfall-like approaches that use abstract structural representations as conditions for music generation tasks have enabled the generation of structured music. However, such methods rely heavily on handcrafted feature engineering. Our objective is to design and develop model that is capable of learning the long-range dependencies in music without relying on explicit structural annotations. The emergence of Transformer Attention technologies, such as Perceiver AR [4], has made it possible to access much longer contextual dependencies. It allows for the simultaneous learning of musical structure and the generation of expressive performances. Perceiver AR has demonstrated the ability to length of up to 32,768 tokens using attend to context the Maestro dataset [5], where the query in cross-attention attends to significantly longer key/value pairs [4]. However, this approach has also introduced challenges. Specifically, the causal mask, when applied with the default input sequence segmentation, does not fully conceal tokens that should not be visible during autoregressive training and generation, which ultimately degrades the quality of the generated music. Additionally, when using ultra-long context as condition, the model tends to generate identical or similar repetitive segments as the sequence length increases due to issues with high similarity in the context of neighboring tokens, which leads to high token autocorrelation [6] tendency. To address the challenges mentioned above, in this paper, we propose PerceiverS, novel model that addresses the causal masking issue by incorporating Effective Segmentation. Additionally, PerceiverS employs Multi-Scale attention to mitigate the high token autocorrelation problem that arises from relying solely on long-range dependencies. Specifically, by adjusting the input sequence segmentation to start from the head segment with an effective causal mask and aggressively increasing the segment length up to the maximum input sequence length, we resolve the learning limitations caused by the causal mask in Perceiver AR [4]. Additionally, by incorporating Multi-Scale masks across multiple layers of cross-attention, the model considers both ultra-long and shortrange attention simultaneously. This approach addresses the limitation in Perceiver AR, which focuses solely on longrange attention [4]. Different from Perceiver AR, our approach enhances symbolic music generation by effectively capturing both long-term structural dependencies and short-term expressive details. Through improved segmentation and multi-scale attention mechanisms, PerceiverS generates coherent, diverse music without relying on extensive structural annotations. Extensive experiments have been conducted to evaluate the performance of the proposed PerceiverS. The experimental results demonstrate an average 40% improvement in Overlap Area when measured against training dataset, highlighting substantial advantage of our approach over Perceiver AR [4] in generating high-quality symbolic music. the original II. RELATED WORKS A. Capturing Music Expressiveness The most significant early work in this area came from Googles team, which introduced Music Transformer [7], model capable of generating expressive piano music using the Piano-e-Competition dataset, later known as the Maestro dataset [5]. Since this model was trained using MIDI files recorded from live piano performances, it utilized the attention mechanism to focus on the context of all previously generated tokens when predicting the next one, allowing it to generate highly detailed and expressive music. However, due to the quadratic complexity O(n2) of the Transformer attention mechanism, it could only generate music lasting for tens of seconds, but not for several minutes. Unlike Music Transformer [7], few other models utilize performance datasets, primarily due to the lack of annotations associated with these types of datasets and the computational limitations involved in processing them. in factor 1) Dataset Selection: key generating production-quality music lies in the selection of datasets. The Maestro dataset [5], consisting of real human performances, offers dynamic and expressive recording music. Datasets there are recorded from live performances are rare, but quite few Automatic Music Transcription (ATM) datasets, including GiantMIDI [8], ATEPP [9], PiJAMA [10], and others. Advanced models have been developed, from Hawthorne [11] to Kong [8], that convert audio into MIDI files. These advances have made it possible to use vast amount of recorded audio music to train models, as the development of symbolic music models has long been constrained by the limited availability of datasets. Manually created MIDI datasets, like LAKH [12], provide valuable human-annotated information, e.g., beats, bars, and phrases, which allows for flexible segmentation and richer feature extraction but lack expressive nuances found in live performances, such as dynamics, tempo variations, and subtle timing shifts. Other attempts, such as ASAP [13] with humanassisted beat correction, and advancements in automatic beat tracking, such as Beat This! [14], aim to bring annotation to live-recorded datasets, though accuracy limitations still present challenges. 2) Computational Limitations: Although using MIDI datasets recorded from live performances and music studios allows for the generation of music with rich and expressive details, generating such music over long durations remains challenge. This is mainly due to the substantial increase in computational resources required for processing long-range contextual dependencies. Almost all current models, e.g., Music Transformer, employ strategies such as chunking and quantization to reduce token sequence length and vocabulary size [7]. While these approaches help to reduce computational burdens, they also limit the models ability to capture ultralong dependencies and compromise expressive performance details. As stated on the Music Transformer webpage,1 Some failure modes include too much repetition, sparse sections, and jarring jumps. In our experiments, we also observed that chunking introduces similar issues. This trade-off prevents the effective generation of long-term coherent symbolic music. B. Capturing Long-term Coherence In the efforts to learn musical structures and generate symbolic music with long-distance coherence, the approaches can generally be categorized into two main types, i.e., those that utilize explicit structural features and those that encourage the model to learn implicit structural features. Each approach is introduced in the following subsections. 1) Explicit Structural Features: The explicit use of structural features often relies on handcrafted feature engineering and external analyzing tools. common method in these models is waterfall-like approach. Typically, the process begins by generating lead sheet and then using the lead sheet as condition for the subsequent generation tasks. MuseMorphose [15] explicitly controls rhythmic intensity and polyphony density on bar basis by training on datasets with bar annotations, specifically the LPD-17-cleansed and Pop1K7 datasets. Compose & Embellish [16] leverages thirdparty tools such as the skyline algorithm, edit similarity, and A* search measures to extract melody and identify structural patterns, enabling the model to produce music with enhanced structural organization. However, it still relies on bar-annotated datasets for training. Rule-Guided Diffusion [17] uses note density and chord to condition generation, resulting in structured musical segments. Its training does not rely on an annotated dataset but rather uses the performance dataset, Maestro. Still, it only produces short musical pieces instead of fulllength segments. Whole-Song Hierarchical Generation [18] is capable of generating fully structured, complete pieces of music. It employs multi-stage approach, using annotations from the POP909 dataset [19], including chord information and separate tracks for melody, bridge, and accompaniment, to produce structured elements such as form, lead sheet, and accompaniment, ultimately generating complete, full-length piece. 1https://magenta.tensorflow.org/music-transformer 2) Implicit Structural Features: Another important approach involves enabling the model to learn the structural information of music implicitly. The method has the advantage of being more generalizable, as it does not rely on handcrafted feature engineering. However, its downside lies in the increased difficulty for the model to capture complex structural features of music. MusicVAE [20] uses bidirectional RNN and Conductor RNN to generate per-bar latent vectors that are decoded into individual notes, focusing on bar-level structure through training on datasets containing bar annotations, which may not be applicable to freely performed music. Museformer [21] applies sparse Transformer attention by fully attending to all tokens in selected bars and the summarized vectors of all preceding bars, allowing it to capture long-range context with limited computational resources. However, it still leverages the Lakh MIDI dataset [12]s bar annotations, which cannot be used with unannotated performance datasets. C. SOTA Solutions with Long-Term Dependency On Performance Datasets The Perceiver AR model [4] from DeepMind has been significant source of inspiration. It combines cross-attention and self-attention mechanisms, enabling the model to attend to sequences with up to tens of thousands of tokens. Like Perceiver [22] and Perceiver IO [23], Perceiver AR [4] uses shorter query in its cross-attention mechanism to attend to much longer sequences, thereby minimizing computational costs. As noted in the paper, this approach allows the model to attend to up to 32,768 tokens when trained on the Maestro dataset [5], offering significantly longer context compared to models like Transformer-XL [24]. This ability to efficiently process long-range contextual data is crucial for learning the structure of entire musical pieces. However, Perceiver AR leverages the last tokens as the Query with limited causal mask, and training with teacherforcing on long sequences led to lower quality generation. Furthermore, we observed that relying solely on long, especially ultra-long, context resulted in repetitive segments in the latter part of generated content. III. PRELIMINARIES In this section, we introduce the fundamental concepts and key challenges necessary to understand our proposed model. We review the operational mechanism of cross-attention in Perceiver AR, the role of its causal mask, and the key considerations when using ultra-long sequences as context for token generation. A. Input Sequence Preprocessing Let the complete sequence be = {x1, x2, . . . , xl}, where is the total length of the entire music sequence, and is the maximum input length, representing the longest sequence that the model can attend to in one pass. The query length is denoted by n, which represents the number of tokens the model uses to query the context, and typically, m. In typical Transformer approach, segment of length is extracted from the sequence at random. Specifically, we select starting index [1, + 1], and the segment used for training is: ˆX = {xs, xs+1, . . . , xs+m1} This approach produces overlapping fixed-length windows for training, such as: {x1, x2, . . . , xm}, {x2, x3, . . . , xm+1}, . . . These sequences will be handled using causal mask. 1) Causal Masking in Typical Transformers: In typical transformer with causal masking, the goal is to ensure that when generating token i, the model does not attend to tokens where > i. The causal mask for this is typically represented by: Mij = (cid:40) 0 if if < This matrix is added to the attention scores QK so that all positions > (i.e., future tokens) are masked out by setting their attention scores to , ensuring they dont influence the generation of the current token. For example, consider case where the query has length of = 5 and the key/value has length of = 5. The expected causal mask (Vanilla Transformer) is as follows: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 This mask ensures that the third query token only attends to the first three tokens. 2) Perceiver ARs Causal Mask Issue: In Perceiver AR [4], the situation is different because the query token length is much smaller than the key and value lengths m. Specifically, the causal mask only works on the final part of the context sequence, equivalent to the length of the query n, but does not mask tokens that occur before that. This results in some tokens before the query length being visible during training, which is not an issue for generation except that the segment from x1 to xmn is not properly learned by the model. Lets denote the sequence of keys and values as and , respectively, and the query length as n, while the context length (keys/values) is m, where > n. The attention mask matrix in Perceiver AR can be represented as follows: Mij = if 0 if > and 0 if > Below shows an example of the causal mask used in Perceiver AR with = 5 (query length) and = 10 (context length): 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Thus, the model can peek at tokens before the query length because they are not fully masked, allowing it to attend to tokens before the intended context during training. This partial masking leads to mismatch between training and autoregressive generation, degrading the quality of the generated music. Note that this issue primarily affects unconditional generation, where the model generates music without any specific prompt or primer, making it more sensitive to inconsistencies between training and generation contexts. In conditional generation, where the model starts with an initial prompt or primer sequence, using primer with length close to the unmasked portion during training can help preserve generation quality and prevent degradation. 3) Calculation for Attention with Mask: The attention mechanism with causal mask is computed as follows: Attention(Q, K, ) = softmax (cid:18) QK + dk (cid:19) B. Ultra-Long Context in Autoregressive Generation We observed that relying only on the ultra-long context in autoregressive models can lead to generated sequences containing excessively long repetitive short segments, especially as the sequence length increases. Given that the probabilities of generating tokens xt and xtk (where is small integer) are, respectively: p(xtx1, x2, . . . , xt1) p(xtkx1, x2, . . . , xtk1) As increases, the contexts [x1, x2, . . . , xtk1] and [x1, x2, . . . , xt1] become nearly identical due to the ultrathe conditional distributions long context. Consequently, p(xtx1, x2, . . . , xt1) and p(xtkx1, x2, . . . , xtk1) are almost indistinguishable, resulting in KL divergence close to zero: DKL(p(xtx1, x2, . . . , xt1)p(xtkx1, x2, . . . , xtk1)) 0 This similarity in conditional distributions means that the model is likely to generate similar tokens in nearby steps, as the probability distributions governing xt and xtk become nearly identical. Thus, the probability of xt = xtk increases, leading to repetitive short segments. When such repetitive tokens are generated across multiple time steps, the sequence exhibits high token autocorrelation. Mathematically, the token autocorrelation at lag for the sequence = [x1, x2, . . . , xt] can be expressed as: ρk(X) = (cid:80)T t=k+1(xt x)(xtk x) t=1(xt x)2 (cid:80)T where represents the mean of the sequence X. When values at nearby steps exhibit high similarity, as suggested by nearly identical conditional distributions, the term (xt x)(xtk x) becomes large, leading to high token autocorrelation at lag k. During generation, identical values are not produced at nearby steps to avoid training penalties. In fact, the similar context of neighboring tokens causes the generation process to produce identical or similar tokens at nearby steps, leading to higher probability of generating repetitive short segments as the sequence grows longer. IV. PROPOSED APPROACH Our proposed model, PerceiverS, introduces dual approach of Effective Segmentation and Multi-Scale attention to address the limitations in symbolic music generation. Building on the strengths of Perceiver AR [4] and introducing mechanisms to handle both short and ultra-long dependencies, PerceiverS (Segmentation and Scale) achieves greater coherence and expressiveness in generated music. Since Perceiver AR provides the possibility of accessing extremely long contexts, we attempt to use Perceiver AR as baseline model to learn entire musical pieces and evaluate the quality of its generation. Our goal for symbolic music generation is to achieve long-term coherence while maintaining diversity within shorter segments. Furthermore, we expect the model to learn patterns similar to human composition, with repetition and development. The detailed steps of our approach and improvements are elaborated in the following subsections. A. Improving the Model to Effectively Learn Ultra-Long Sequences This section outlines an improved dataset preprocessing strategy designed to enhance token generation quality in autoregressive models. As discussed in the previous section, Perceiver ARs causal mask has limitations in its coverage of the entire input sequence, making it necessary to implement preprocessing adjustments before feeding data into the model. We set the maximum context length to 32,768 tokens. Based on the Perceiver ARs original implementation2, we randomly cropped the dataset. In this approach, random starting point is selected between 0 and (the sequence length - the maximum input length + 1), from which segment of length equal to the maximum input length is taken. The upper part of Figure 1 demonstrates the original input sequence pre-processing of the baseline model. Specifically, the baseline model segments the input sequence using the maximum input length as the window size, leaving the beginning of the sequence uncovered by the causal mask. This results in these initial tokens not being progressively used as validation tokens in teacher forcing, preventing the model from learning 2https://github.com/google-research/perceiver-ar with traditional segmentation causes mismatch between the training phase, where teacher forcing is used, and the autoregressive generation process. This approach allows the model to peek at unintended tokens during training, which degrades generation quality during inference when such tokens are unavailable for reference. While this modification does not address the model itself, the experimental results (in the Experiments and Results subsection) will show that this improvement significantly impacts performance. B. Further Improving the Model for Generating Music with Both Coherence and Diversity After applying ultra-long contexts in the autoregressive generation, we aim to combine the strengths of both consistency and diversity, allowing the proposed PerceiverS to generate music without tendency toward repeated segments caused by attending solely to long-distance contexts. The Multi-Scale causal cross-attention mechanism adopted in the proposed PerceiverS, while somewhat similar to the concept of Museformer [21], is fundamentally different. It introduces multiple layers of attention with varying scales of attending length, designed to balance focus on both long and short contexts, thereby enhancing diversity and reducing repetitive tendencies. Figure 2 illustrates the Multi-Scale Causal Cross-Attention Mechanism of PerceiverS. Specifically, within the multi-layer cross-attention, tokens from earlier in the sequence are masked by different scales, and the resulting outputs are combined before being fed into the self-attention layer. This approach enables the model to incorporate cross-attention at multiple scales simultaneously. Two layers of causal cross-attention are implemented. One layer operates without scale mask, while the second layer applies scale mask that masks out all tokens before the last 1,024 tokens. The output of cross-attention layers is combined using the cascade approach. The cascade method feeds the output from the first layer directly as input to the second layer, allowing each layer to refine and build upon the preceding layers output. This method contrasts with alternative approaches such as adding (Sum), concatenating followed by linear projection (Concat), or concatenating and processing through two-layer MLP. V. TECHNICAL DETAILS In this section, we provide the mathematical formulation and technical explanation of our proposed approach, PerceiverS, for symbolic music generation, based on improvements to Perceiver AR [4], including our proposed Effective Segmentation for input sequences and Multi-Scale cross-attention. A. Improved Segmenting To address the mismatch between training and autoregressive generation, we propose segmenting method aligned with the causal mask mechanism, whether using random or sequential sampling, that emphasizes shorter context sequences, gradually building up to the maximum context length. Fig. 1. Comparison of Data Preprocessing Methods and Causal Masks Between the Baseline and PerceiverS to generate tokens at the start of the sequence that fall outside the mask. In contrast, we propose an alternative method that does not rely on cropping the dataset based on the longest input sequence. Instead, we randomly select an endpoint for cropping between (the query length + 1) and (the sequence length + 1). segment of up to the maximum input length is then taken, leading up to this endpoint (or shorter for tokens near the beginning). Padding is applied as in the baseline model. The lower part of Figure 1 illustrates the effective segmentation input sequence pre-processing of the proposed PerceiverS. It begins learning token generation from the initial part of the sequence with effective causal mask coverage. This approach ensures that tokens from 1 to (m n) in the sequence are effectively covered by the causal mask, enabling the model to learn token generation at the beginning of the sequence more effectively. The rationale for this improvement is that Perceiver AR [4]s causal mask operates in final block mechanism, where it provides context for the length of the input tokens but only partially masks within the query token length. Training maximum input length for each sequence, yielding segments such as: ˆX2:m+1 = {x2, x3, . . . , xm+1}, ˆX3:m+2 = {x3, x4, . . . , xm+2}, ... In practice, dataset segments are randomly selected from these defined segments. For sequential segment retrieval, the method begins with progressively longer prefixes as: {x1, x2, . . . , xn}, {x1, x2, . . . , x2n}, . . . , {x1, x2, . . . , xkn} where kn Here, is an integer ranging from 1 to K, where represents the maximum number of segments possible within the full sequence length l. When kn > m, subsequent segments shift forward along the sequence in fixed-length windows of size m, such as: {xknm, xknm+1, . . . , xkn} This approach ensures that the model learns the structure of the sequence progressively, closely resembling the autoregressive generation process where tokens are predicted one at time. As result, is better aligned with the model the autoregressive nature of music generation, reducing the peeking problem and improving generation quality. Thus, the model can smoothly learn the generation of sequences {x1, x2, . . . , xmn} with an effective causal mask. Key Advantages Progressive learning: The model learns long-range dependencies progressively by starting with shorter sequences and gradually increasing the context length. Consistency: This method ensures the training process mimics autoregressive generation, reducing quality degradation during generation. Causal alignment: This approach aligns with the causal mask, preventing the model from peeking at future tokens during training. B. Multi-Scale Cross-Attention Mechanism Our Multi-Scale cross-attention mechanism allows the model to handle different levels of context simultaneously. We experiment with strategy applied to both cross-attention layers on top of the causal mask: one without any scale mask (all tokens are unmasked), and the other masking the tokens from the 1st to the nth token. Below are the formulas for these two settings: 1. Attention mask without scale mask: In this case, all tokens are visible to the model. The scale mask matrix ˆM is defined as: ˆMij = 0, i, Fig. 2. Multi-Scale Causal Cross-Attention Mechanism of PerceiverS Let the complete sequence be = {x1, x2, . . . , xl}, where is the total length of the entire sequence, is the maximum input length the model can attend to in one pass, and is the query length within that input. The input sequences used for training, denoted as ˆX, are generated as follows: ˆXi:j = {xi, xi+1, . . . , xj}, where + Here, is starting position chosen within the sequence, and is the end position such that the length + 1 does not exceed the maximum input length m. When accessing data from the dataset, instead of using fixed-length sequences with the maximum input length, we progressively extract sequences by increasing the context length from up to m. This way, the training set includes progressively larger prefixes of the sequence: {x1, x2, . . . , xn}, {x1, x2, . . . , xn+1}, . . . , {x1, x2, . . . , xm} After reaching the full context length m, we continue segmenting from various starting positions while preserving the This means that no tokens are masked by the scale mask, and all key/value pairs can be attended to. 2. Attention mask with scale mask (masking the 1st to the nth tokens): In this case, tokens from the 1st to the nth positions are masked by the scale mask, preventing the model from attending to these tokens. The scale mask matrix ˆM is defined as: (cid:40) ˆMij = 0 if > if This ensures that only tokens starting from the + 1th position are visible, while the model cannot attend to tokens before this position. The combined causal mask and scale mask ˆM are added directly to the attention score calculation, modifying the softmax as follows: Attention(Q, K, ) = softmax (cid:32) QK + + ˆM dk (cid:33) Here, is the causal mask that ensures the model doesnt attend to future tokens, and ˆM is the scale mask applied to limit attention to certain parts of the sequence. Both masks work together to control which tokens the model can attend to during training. In the Cascade mode, the output from the first attention layer is fed as input to the next layer, enabling each layer to refine and build upon the previous layers output. VI. EXPERIMENT AND RESULTS A. Experimental Setup Our experimental setup included five key components: 1) Dataset Selection: In our experiments, we used three datasets: Maestro [5], GiantMIDI [8], and ATEPP [9]. The Maestro dataset [5] was the primary dataset used, containing 1,251 sequences, with validation set of 240 sequences. Additionally, we combined the three classical piano datasets (Maestro [5], GiantMIDI [8], and ATEPP [9]), resulting in training set of 25,662 sequences and validation set of 4,824 sequences. This combined dataset was only used for training the cascade Multi-Scale attention model to assess the models generalization abilities. 2) MIDI Preprocessing: For MIDI preprocessing, we set the Note On and Note Off events within the range of 0 to 127. Time Shift events were discretized into 100 time steps per second, where each step represents 10 milliseconds. Volume events were quantized into 32 bins, and pedal events were mapped to the duration of relevant notes, discarding the pedal events afterward. Each song ends with token_end marker. No data augmentation, such as key or tempo changes, was applied, though this is planned for future experiments. 3) Hyperparameter Selection: The hidden dimension was set to 1,024 with 24 self-attention layers. Each attention layer had 16 heads, and the head dimension was 64. Adam optimization was used, with an initial learning rate set to 0.03125. Each generated sequence length was set to 8,192 tokens, resulting in approximately 2-10 minutes of music. The music generation in this setup was unconditional, meaning that no external conditions or prompts were used to guide the generation process. The resulting MIDI files were rendered into audio using the Vintage Piano sound from Logic Pro. 4) Hardware Setup: All experiments were conducted on an NVIDIA RTX 4080 GPU with 16GB of memory. The batch size was set to 1, and training on the Maestro dataset [5] took approximately 9 minutes and 30 seconds per epoch. Each experiment was run for 100 epochs. 5) Evaluation Metrics: We adopted the evaluation method outlined in the paper On the Evaluation of Generative Models in Music [25]. For evaluation, we constructed reference dataset by separating set of pieces from the training dataset (Maestro [5]) before model training. Then, we generated an equal number of files using the model to form the generated dataset. We calculated the distances within each dataset and between the generated and reference datasets for the following metrics: Total Used Pitch (PC): Measures the overall pitch diversity by counting distinct pitch classes used throughout the entire piece. Total Used Note (NC): Counts the distinct notes (pitch and octave combinations) used across the entire piece, indicating the variety in pitch and register. Total Pitch Class Histogram (PCH): histogram representing the frequency distribution of pitch classes across the entire piece, offering insights into pitch class preference. Pitch Class Transition Matrix (PCTM): matrix representing the probabilities of transitioning from one pitch class to another. This metric captures melodic and harmonic movement patterns. Pitch Range (PR): Measures the difference between the highest and lowest pitches used in the piece, indicating the overall range of pitches. Average Pitch Interval (PI): The average interval between consecutive pitches, which reflects the tendency towards stepwise or leapwise motion in melodies. Average Inter-Onset Interval (IOI): Measures the average time interval between note onsets, capturing the rhythmic density across the entire piece. Note Length Histogram (NLH): histogram representing the distribution of note lengths, giving insights into note duration diversity. Note Length Transition Matrix (NLTM): matrix representing the transition probabilities between different note lengths, capturing rhythmic patterns and variations in note duration. The original paper [25] includes metrics analyzed on bar-by-bar basis. We introduced four metrics based on time segments, as bar annotations are unavailable in performance datasets, making time-based segmentation essential for evaluating long-term coherence and local diversity. Segment Used Pitch (PC/seg): Similar to Total Used Pitch, but calculated within segments. This metric helps capture pitch diversity across shorter sections. Segment Used Note (NC/seg): Counts distinct notes within fixed-length segments, allowing us to analyze the variety of notes used across different parts of the piece. Segment Pitch Class Histogram (PCH/seg): Represents the pitch class distribution within each segment, providing insights into the consistency of pitch class usage across different sections. Segment Average Inter-Onset Interval (IOI/seg): Measures the average inter-onset interval within each segment, helping to analyze rhythmic density and tempo consistency within shorter sections. To calculate these additional segment-based metrics, each evaluation piece was divided into 64 segments of equal duration. All of these metrics are calculated based on the intra-set distances within the generated dataset and the baseline dataset, as well as the inter-set distances between the generated dataset and the baseline dataset. We then compute the KL Divergence (KLD) and overlap area (OA) for these values. B. Experiments and Results We conducted three sets of experiments to evaluate the models performance in various scenarios. 1) Input Sequence Segmentation: The purpose of this experiment is to demonstrate that Effective Segmentation is essential for the model to fully leverage ultra-long-distance context. We compared two types of input sequence segmentations, with maximum sequence length set to 32,786 tokens: Baseline Model: random starting position is selected within the range [0, sequence length - max input length + 1], then segment of max input length tokens is taken from this start. Improved Model: random end position is selected within the range [query length + 1, sequence length + 1], then segment of tokens, up to the max input length, is taken backward from this end. Segments shorter than the max input length are padded at the beginning. 2) Multi-Scale Attention: While an ultra-long context provides long-term consistency, it tends to generate repetitive segments in the latter part of the sequence (see Figure 3). To address this issue, we added different scale masks to the multi-layer cross-attention, allowing different layers to focus on distinct context lengths and merging these results. The aim of this experiment is to evaluate whether adding attention outputs for shorter-range dependencies can improve generation quality. Here, we define two scale masks for the maximum sequence length of 32,786 tokens: No Scale Mask: All tokens are unmasked. Masked Scale: Only the last 1,024 tokens are visible, with all previous tokens masked. The cross-attention layer without mask is fed into the cross-attention layer with context mask as its input. We refer to this approach as cascade Multi-Scale cross attention. Fig. 3. Piano roll showing repetitive segments over time in the long context model. Fig. 4. Piano roll showing diverse phrases over time in the long context model after applying multi-scale attention. The segmentation methods produced very different results in autoregressive training and generation, with the baseline model exhibiting poor quality and the improved model showing excellent quality, as shown in the data within the red dashed box in Table I. The music generated using the improved input sequence processing approach is closer to the ground truth in terms of lower KLD metrics, including PC, PC/seg, NC/seg, PCH/seg, PCTM, PR, PI, IOI, and IOI/seg, as well as higher OA metrics, including PC, PC/seg, NC, NC/seg, PCTM, PR, PI, IOI, IOI/seg, and NLTM. This result indicates that the improvement effectively enables the model to utilize ultralong-distance context for music generation. Results, as shown in the data within the solid blue box in Table I, indicate that this approach significantly improved the models generation quality. By examining the overlap values (OA) of the four parameters IOI, IOI/seg, NLH, and NLTMwhich are related to rhythm characteristicsit is evident that the models generation aligns more closely with the reference set. Additionally, the OA values for PCH and PCH/seg indicate that the harmonic characteristics also better resemble those of the reference dataset. Furthermore, unusual repetitive segments in the generated music are now rare, as shown in Figure 4, with the generated music even exhibiting rich diversity of phrases. TABLE EVALUATION METRICS FOR BASELINE, EFFECTIVE SEGMENTATION, AND MULTI-SCALE MODELS Baseline Effective Segmentation Multi-Scale KLD OA Comparison KLD OA Comparison KLD OA Comparison PC PC/seg NC NC/seg PCH PCH/seg PCTM PR PI IOI IOI/seg NLH NLTM 0.036 0.308 0.110 0.578 0.017 0.122 0.393 0.058 0.026 0.327 0.128 0.057 0.122 0.614 0.235 0.027 0.109 0.948 0.817 0.302 0.696 0.521 0.670 0.713 0.841 0.029 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0.007 0.055 0.209 0.519 0.098 0.091 0.253 0.004 0.018 0.056 0.064 0.088 0.187 0.827 0.369 0.040 0.169 0.779 0.699 0.655 0.888 0.883 0.877 0.722 0.655 0.040 +35% +57% +49% +54% 18% 14% +117% +28% +70% +31% +1% 22% +41% 0.017 0.055 0.165 0.626 0.060 0.082 0.177 0.013 0.071 0.004 0.038 0.092 0.139 0.759 0.704 0.040 0.151 0.907 0.871 0.424 0.774 0.729 0.922 0.860 0.748 0.048 +24% +200% +49% +38% 4% +7% +41% +11% +40% +38% +21% 11% +69% The Comparison column shows the percentage difference between the current OA and the baseline OA. In our experiments, in addition to using the cascade method to combine cross-attention outputs from layers with different context masks, we also explored other methods for computing the combined results, including: Sum: The results of layers and are summed. Concatenate: The results of layers and are concatenated, then linearly projected to the appropriate shape. MLP: The results of layers and are concatenated and fed through two-layer MLP. Among these, the Cascade approach achieved the highest generation quality. 3) Fine-Tuning with Combined Datasets on Cascade MultiScale Cross-Attention Model: Finally, we conducted an additional experiment using the model with the cascade method to combine two layers of cross-attention with different scale masks. After training on the Maestro dataset [5], this model was further fine-tuned on the combined dataset of Maestro [5], GiantMIDI [8], and ATEPP [9], as mentioned earlier. Generated music samples from the fine-tuned model are available on our project website: https://perceivers.github.io. VII. DISCUSSION Perceiver AR [4] represents significant advancement in the realm of autoregressive (AR) Transformers, as it directly addresses the challenge of handling long context attention, limitation that has constrained almost all previous Transformer architectures. By breaking free from traditional context length constraints, Perceiver AR [4] has made possible many tasks that were previously unachievable for AI models. distinctive feature of Perceiver AR [4]s design is its use of differing key-value (KV) and query (Q) lengths within the attention mechanism. This unique approach allows for an unrestricted context it requires carefully length, yet considered application of causal masking. The causal mask only applies effectively to the query length, meaning that both training and generation must account for this query-specific length. To maximize learning, it is crucial for the model to begin training with query-length sequences starting from the beginning of each dataset sample, rather than focusing solely on the context-length segment. Without this adjustment, the model would fail to develop the capability for generating tokens beyond the masked region. Our proposed Effective Segmentation technique, which optimizes the input sequence processing for Perceiver AR [4], has shown that, with appropriate data segmentation, Perceiver AR [4] can effectively learn long contexts and produce coherent, contextually rich content. The ultra-long context attention indeed provides the model with remarkable benefits in maintaining content consistency over extended generation. However, potential drawback of solely relying on longrange dependencies emerged: as sequences get longer, the model increasingly tends to generate repetitive short segments. This occurs because, in the later stages of generation, the long context windows are nearly identical, differing by only few tokens. Consequently, the probability of generating identical or similar segments increases, further amplified by the high token autocorrelation [6] tendency due to the high similarity of context as the sequence lengthens.. To mitigate this issue, we introduce Multi-Scale crossattention mechanism that combines both long and short conlengths into text windows. By integrating varying context multi-layer cross-attention, our approach successfully reduces the tendency for repetitive sequences while maintaining the models long-term consistency. This combination strategy enables the generation of high-quality symbolic music over extended sequences. Our enhanced model, PerceiverS (Segmentation and Scale), leverages Effective Segmentation aligned with Perceiver AR [4]s operation mode. By applying Multi-Scale masking strategies within multi-layer cross-attention, PerceiverS effectively generates cohesive, consistent music over extended temporal contexts, preserving intricate musical patterns and expressive details across long sequences. Additionally, through the use of performance music datasets, PerceiverS is capable of producing high-quality symbolic music that captures the nuances of human performance. Importantly, because the model does not rely on annotated datasets, PerceiverS can be trained on MIDI data derived from audio using any automatic music transcription (AMT) technique. This capability suggests future in which symbolic music generation can leverage vast historical recordings, unbounded by the limitations of manually annotated data. In essence, Perceiver AR [4] and, by extension, PerceiverS, are general-purpose models adaptable to wide range of AI tasks. The Effective Segmentation and Multi-Scale innovations introduced here open up avenues for future applications across domains such as text, image, and video. Future research could thus extend the potential of PerceiverS, exploring its capabilities across diverse modalities and expanding its utility within the broader landscape of AI tasks. VIII. CONCLUSION In this work, we introduced novel model, PerceiverS, which builds on the Perceiver AR [4] architecture by incorporating Effective Segmentation and Multi-Scale attention mechanism. The Effective Segmentation approach progressively expands the context segment during training, aligning more closely with autoregressive generation and enabling smooth, coherent generation across ultra-long symbolic music sequences. The Multi-Scale attention mechanism further enhances the models ability to capture both long-term structural dependencies and short-term expressive details. By addressing limitations in existing models, particularly the issue of causal masking in autoregressive generation and the high token autocorrelation problem in ultralong sequences, PerceiverS enables the effective handling of ultra-long token sequences without compromising the quality of generated music. Through our proposed Effective Segmentation in dataset preprocessing and Multi-Scale attention modifications, we demonstrated significant improvements in generating coherent and diverse musical pieces. Our approach to symbolic music generation provides new balance between structural coherence and expressive diversity, setting foundation for future advancements in symbolic music generation models. REFERENCES [1] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: TextarXiv preprint to-audio generation with latent diffusion models. arXiv:2301.12503, 2023. [2] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Simple and controllable music generation. Advances in Neural Information Processing Systems, 36, 2024. [3] Peike Patrick Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, and Alex Wang. Jen-1: Text-guided universal music generation with omnidirectional diffusion models. In 2024 IEEE Conference on Artificial Intelligence (CAI), pages 762769. IEEE, 2024. [4] Curtis Hawthorne, Andrew Jaegle, Catalina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz Malinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, et al. General-purpose, long-context autoregressive modeling with perceiver ar. In International Conference on Machine Learning, pages 85358558. PMLR, 2022. [5] Curtis Hawthorne, Andriy Stasyuk, Adam Roberts, Ian Simon, ChengZhi Anna Huang, Sander Dieleman, Erich Elsen, Jesse Engel, and Douglas Eck. Enabling factorized piano music modeling and generation with the maestro dataset. arXiv preprint arXiv:1810.12247, 2018. [6] Minhyeok Lee. mathematical interpretation of autoregressive generative pre-trained transformer and self-supervised learning. Mathematics, 11(11):2451, 2023. [7] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew Dai, Matthew Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer. arXiv preprint arXiv:1809.04281, 2018. [8] Qiuqiang Kong, Bochen Li, Jitong Chen, and Yuxuan Wang. GiantmidiarXiv piano: large-scale midi dataset for classical piano music. preprint arXiv:2010.07061, 2020. [9] Huan Zhang, Jingjing Tang, Syed RM Rafee, Simon Dixon, George Fazekas, and Geraint Wiggins. Atepp: dataset of automatically transcribed expressive piano performance. In Ismir 2022 Hybrid Conference, 2022. [10] Drew Edwards, Simon Dixon, and Emmanouil Benetos. Pijama: Piano jazz with automatic midi annotations. Transactions of the International Society for Music Information Retrieval, 2023. [11] Curtis Hawthorne, Erich Elsen, Jialin Song, Adam Roberts, Ian Simon, Colin Raffel, Jesse Engel, Sageev Oore, and Douglas Eck. OnarXiv preprint sets and frames: Dual-objective piano transcription. arXiv:1710.11153, 2017. [12] Colin Raffel. Learning-based methods for comparing sequences, with Columbia applications to audio-to-midi alignment and matching. University, 2016. [13] Francesco Foscarin, Andrew Mcleod, Philippe Rigaux, Florent Jacquemard, and Masahiko Sakai. Asap: dataset of aligned scores and performances for piano transcription. In International Society for Music Information Retrieval Conference, pages 534541, 2020. [14] Francesco Foscarin, Jan Schluter, and Gerhard Widmer. Beat this! arXiv preprint tracking without dbn postprocessing. accurate beat arXiv:2407.21658, 2024. [15] Shih-Lun Wu and Yi-Hsuan Yang. Musemorphose: Full-song and finegrained piano music style transfer with one transformer vae. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:1953 1967, 2023. [16] Shih-Lun Wu and Yi-Hsuan Yang. Compose & embellish: Wellstructured piano performance generation via two-stage approach. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. [17] Yujia Huang, Adishree Ghatare, Yuanzhe Liu, Ziniu Hu, Qinsheng Zhang, Chandramouli Sastry, Siddharth Gururani, Sageev Oore, and Yisong Yue. Symbolic music generation with non-differentiable rule guided diffusion. arXiv preprint arXiv:2402.14285, 2024. [18] Ziyu Wang, Lejun Min, and Gus Xia. Whole-song hierarchical generation of symbolic music using cascaded diffusion models. arXiv preprint arXiv:2405.09901, 2024. [19] Ziyu Wang, Ke Chen, Junyan Jiang, Yiyi Zhang, Maoran Xu, Shuqi Dai, Xianbin Gu, and Gus Xia. Pop909: pop-song dataset for music arrangement generation. arXiv preprint arXiv:2008.07142, 2020. [20] Adam Roberts, Jesse Engel, and Douglas Eck. Hierarchical variational autoencoders for music. In NIPS Workshop on Machine Learning for Creativity and Design, volume 3, 2017. [21] Botao Yu, Peiling Lu, Rui Wang, Wei Hu, Xu Tan, Wei Ye, Shikun Zhang, Tao Qin, and Tie-Yan Liu. Museformer: Transformer with fineand coarse-grained attention for music generation. Advances in Neural Information Processing Systems, 35:13761388, 2022. [22] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Perceiver: General perception with In International conference on machine learning, Zisserman, and Joao Carreira. iterative attention. pages 46514664. PMLR, 2021. [23] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021. [24] Zihang Dai. Transformer-xl: Attentive language models beyond fixedlength context. arXiv preprint arXiv:1901.02860, 2019. [25] Li-Chia Yang and Alexander Lerch. On the evaluation of generative models in music. Neural Computing and Applications, 32(9):47734784, 2020."
        }
    ],
    "affiliations": [
        "Auckland University of Technology, Auckland, New Zealand",
        "University of Tasmania, Tasmania, Australia"
    ]
}