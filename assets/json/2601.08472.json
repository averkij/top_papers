{
    "paper_title": "sui-1: Grounded and Verifiable Long-Form Summarization",
    "authors": [
        "Benedikt Droste",
        "Jan Philipp Harries",
        "Maximilian Idahl",
        "Björn Plüster"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, a critical limitation in compliance-sensitive domains such as government and legal analysis. We present sui-1, a 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. Our synthetic data pipeline combines chain-of-thought prompting with multi-stage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3x more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. Model weights and an interactive demo are publicly available."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 1 2 7 4 8 0 . 1 0 6 2 : r sui-1: Grounded and Verifiable Long-Form Summarization Benedikt Droste* Jan Philipp Harries Maximilian Idahl Björn Plüster ellamind"
        },
        {
            "title": "Abstract",
            "content": "Large language models frequently generate plausible but unfaithful summaries that users cannot verify against source text, critical limitation in compliance-sensitive domains such as government and legal analysis. We present sui-1, 24B parameter model that produces abstractive summaries with inline citations, enabling users to trace each claim to its source sentence. The model processes documents up to 100K tokens in single pass and supports iterative processing for texts exceeding 2 million tokens. Our synthetic data pipeline combines chain-of-thought prompting with multistage verification, generating over 22,000 high-quality training examples across five languages from diverse sources including parliamentary documents, web text, and Wikipedia. Evaluation shows sui-1 significantly outperforms all tested open-weight baselines, including models with 3 more parameters. These results demonstrate that task-specific training substantially outperforms scale alone for citation-grounded summarization. Model weights and an interactive demo are publicly available."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models excel at summarization but frequently generate unfaithful content, including fabricated facts or misattributed claims that users cannot trust without laboriously verifying against source documents. Citation-grounded summarization addresses this by requiring models to explicitly attribute claims to source text, enabling users to quickly assess reliability and trace claims to their origins. Training citation-grounded models presents challenges: standard datasets lack citation annotations, manual annotation is prohibitively expensive, and the task requires precise coordination between content generation and source reference. We address these challenges through synthetic data generation using capable teacher model, with automated verification ensuring citation accuracy before training. We introduce sui-1, 24B parameter model achieving 84% overall accuracy on LLM-as-a-judge evaluation compared to 4356% for baselines (Figure 1). The model notably outperforms all tested open-weight baselines, demonstrating that task-specific training provides greater benefit than scale alone. sui-1 reliably processes documents exceeding 100 pages, with iterative chunking enabling summarization of texts up to 2 million tokens."
        },
        {
            "title": "2 Related Work",
            "content": "Long-document summarization remains challenging even for modern large language models, which suffer from factual hallucinations despite improved context windows (Ji et al., 2023). Early architectural *Corresponding author: benedikt@ellamind.com Figure 1: Overall performance comparison. sui-1 (84.2%) significantly outperforms open-weight baselines and approaches the reference model (89.1%). innovations like PEGASUS (Zhang et al., 2020) and Longformer (Beltagy et al., 2020) addressed length constraints through sparse attention or hierarchical processing, but they lack the semantic reasoning capabilities of current LLMs and do not produce verifiable citations. Attribution has emerged as essential for trustworthy language model outputs. The ALCE benchmark (Gao et al., 2023) established evaluation standards for attributed text generation. LongCite (Zhang et al., 2024) addresses sentence-level citation generation for long-context question answering through coarseto-fine pipeline, training models to produce fine-grained citations from retrieved chunks. While retrievalaugmented generation (RAG) approaches like Self-RAG (Asai et al., 2024) or WebGPT (Nakano et al., 2022) cite sources by retrieving external documents, they rely on complex inference-time infrastructure. In contrast, this work focuses on internal grounding: training models to generate verifiable inline citations from the provided context window alone, enabling self-contained generation without external retrieval. Synthetic data generation has proven effective for instruction tuning (Wang et al., 2023; Taori et al., 2023), with Orca (Mukherjee et al., 2023) demonstrating that smaller models can learn complex reasoning from larger teachers. We apply similar principles to citation-grounded summarization, using capable teacher model with automated verification to ensure training data quality."
        },
        {
            "title": "3 Approach",
            "content": "Data and Preprocessing. The source corpus draws from three primary sources: the German Parliamentary Documentation System (DIP)1 providing legislative proposals, committee reports, and ministerial responses; long-form German texts from Common Crawl via OSCAR; and multilingual Wikipedia extracts. Documents range from short announcements to reports exceeding 50,000 words. We introduce multilingual diversity with German comprising 74% of training data, English 10%, and French, Italian, 1https://dip.bundestag.de 2 Source Documents Sentence Tagging LLM Generation Citation Verification Training Data XML tags Teacher LLM 95.2% pass 22K examples Figure 2: Synthetic data generation pipeline: sentences are tagged with unique XML identifiers, processed by teacher LLM with chain-of-thought prompting, verified for citation accuracy, and filtered for quality. and Spanish each approximately 5%. Each sentence receives unique 8-character hexadecimal identifier derived from MD5 hashing, wrapped in XML format (e.g., <a3f5e823>text</a3f5e823>). This tagging scheme enables precise citation tracking without verbatim quotes. The identifiers are deterministic, language-agnostic, and prevent positional shortcuts. Generation Pipeline. Figure 2 illustrates the five-stage pipeline: sentence tagging, prompt construction, LLM generation (using frontier closed-source model), citation verification, and quality filtering. We employ XML-style tags in brackets immediately following claims: The budget increased [<a3f5e823>]. Documents under 30K tokens are processed in single pass; longer documents are chunked (15K tokens each), summarized independently, then merged while preserving all citations (see Appendix A). The generation prompt comprises 1618 rules with chain-of-thought reasoning (Wei et al., 2022), requiring each tag to appear exactly once and citations to immediately follow supported claims. Custom Instructions. Custom instructions are generated via LLM analysis of each document, producing positive instructions (focus areas, formatting), adversarial instructions (requests for absent information), and format instructions (bullet points, length limits). The training distribution allocates 30% of examples without instructions, 40% with positive instructions, 10% adversarial, 10% bullet point formats, and 10% short summary requests. Examples include positive instructions (Create summary for legal experts explaining the CETA agreements Annex structure and ratchet clauses) and adversarial instructions that request absent information (Explain CETAs impact on automotive CO2 emissions, topic not in the source). To ensure inference-time flexibility, we strip explicit constraints from training examples. For instance, generation prompts specify Summarize in exactly {n} bullet points. Only bullets, no introduction or conclusion, but training data is relaxed to simply Summarize in {n} bullet points. This prevents the model from requiring overly rigid instruction formats during deployment. Quality Control. Multi-stage filtering ensures data quality: tag verification confirms all citations exist in the source (95.2% pass rate), quality annotation evaluates reasoning coherence and citation distribution, and specificity checks penalize generic filler. Citation distribution is quantified through an evenness score measuring tag spacing uniformity; examples in the bottom 15th percentile by evenness or with excessive uncited gaps are filtered to ensure citations are spread throughout the summary rather than clustered. second LLM pass rewrites chain-of-thought reasoning from third-person (The summary should be structured...) to first-person perspective (I will structure the summary...), creating more natural patterns for supervised fine-tuning. Table 1 summarizes the final dataset of 22,152 training and 225 test examples, totaling over 357 million tokens."
        },
        {
            "title": "4 Training",
            "content": "We select Mistral-Small-3.2-24B-Instruct (Mistral AI, 2024) as our base model for its strong multilingual capabilities, particularly in German and other European languages. We fine-tune using LoRA (Hu et al., 3 Attribute Total Examples Avg. Tokens Generation Mode Iterative Oneshot Train Test 22,152 16,158 225 15,892 60.6% 58.2% 27.5% 29.8% With Custom Instruction 68.1% 67.6% Table 1: Dataset statistics. Figure 3: Token distribution of training documents. The long-tail distribution reflects diverse document lengths (truncated at 50K for visibility; maximum is 179K tokens). 2022) with rank 16, training for 2 epochs on sequences up to 100K tokens. Training employs four NVIDIA H100 GPUs with context parallelism to handle the extended sequence lengths, accommodating full source documents with XML tags plus generated summaries. Flash Attention and gradient checkpointing enable memory-efficient processing. We produce both full-precision and FP8 quantized variants, with quantization reducing memory requirements by 50% while maintaining generation quality. Full hyperparameters are provided in Appendix C."
        },
        {
            "title": "5 Results",
            "content": "We evaluate on 225 held-out examples using LLM-as-a-judge (Liu et al., 2023) via the elluminate platform2 across five criteria: factual accuracy, coverage, specificity, format compliance, and instruction following. We compare against reference model (frontier closed-source model with extended reasoning), MistralSmall-3.2-24B-Instruct, Gemma-3-27B-Instruct, Qwen3-32B-Instruct, and Llama-3.3-70B-Instruct. Evaluation Metrics. We employ five specific criteria to assess summary quality. Scores represent the percentage of passed checks across the test set, where each question is scored binary (yes/no) per sample: 2https://elluminate.ai 4 Criterion Description Factual Accuracy (Fact) Does the summary avoid introducing new facts, entities, numbers, or claims not supported by the source content? Coverage (Cov.) Specificity (Spec.) Does the summary cover the documents main points and key takeaways at appropriate granularity? Are claims specific and informative rather than generic filler (e.g., \"there are several points\")? Format Compliance (Fmt.) Is the output compliant with formatting instructions including language consistency, semantic-aware planning, and paragraph structure? Instruction Following (Instr.) If custom instruction is provided, is it followed appropriately? Table 2: Evaluation criteria definitions. Each metric is scored binary per sample. Model Reference Model3 sui-1 (Ours) sui-1 FP8 Gemma-3-27B-Instruct Qwen3-32B-Instruct Mistral-Small-3.2-24B-Instruct Llama-3.3-70B-Instruct Fact Cov.1 Spec. Fmt. Instr. .958 .905 .884 .916 .905 .905 .811 .705 .600 .516 .368 .221 .126 . .989 .979 .979 .916 .916 .937 .674 .926 .895 .926 .147 .179 .137 . .874 .832 .747 .432 .379 .389 .200 All .891 .842 . .556 .520 .499 .427 1 Coverage scores are lower when samples require constrained formats (bullet points, short summaries). 2 Custom instruction adherence; tests whether the model follows user-specified formats. 3 Frontier closed-source model with extended reasoning capabilities. Table 3: Results across five evaluation criteria. sui-1 achieves 0.842 overall, with dramatic improvements in format compliance (0.895 vs 0.1370.411 for baselines). Table 3 shows sui-1 achieves 0.842 overall, substantially outperforming baselines (0.4270.556) and approaching the reference model (0.891). Format compliance improves most dramatically: 0.895 vs 0.1370.411 for baselines. FP8 quantization reduces overall score to 0.811 (3.7% degradation) while maintaining format compliance. Notably, Llama-3.3-70B-Instruct scores lowest (0.427) despite 3x parameters, confirming task-specific training outweighs scale. Coverage Trade-off. sui-1 achieves the highest coverage score (0.600) among all open-weight models, second only to the reference model (0.705). Baselines range from 0.042 to 0.368, demonstrating that task-specific training improves not just format compliance but also content coverage. The remaining gap to the reference reflects intentional design: when users request constrained formats such as bullet points or short summaries, the model correctly trades breadth for precision while maintaining high factual accuracy (0.905) and specificity (0.979). Format Compliance Gap. The dramatic improvement in format compliance (0.895 vs 0.1370.411) represents our key finding. Generating citation-grounded summaries requires following 1618 fixed rules for citation placement and structure while simultaneously adapting to custom instructions like bullet points or specific focus areas. Baseline models struggle to combine these layered requirements, often satisfying some constraints while violating others. sui-1 learns to balance rigid format rules with flexible user instructions through synthetic training, enabling the reliable compliance that real-world applications demand. 5 Baseline (Mistral-Small-24B) sui-1 The document discusses various aspects of the Federal Ministrys budget proposals. There are several important points regarding fiscal policy and government spending priorities. The ministry has outlined plans for infrastructure and energy investments. Regional development is also addressed. [No citations, generic filler] revisions to the budget The Federal Ministry of Finance has announced significant framework [<a3f5e823>]. The proposed changes include 12% increase in infrastructure spending [<b7d2c941>]. Regional development receives additional transfers [<d2a8b3c1>]... [Verifiable inline citations] Figure 4: Comparison of baseline vs sui-1 outputs on the same input. Baselines produce generic summaries without citations; sui-1 generates specific claims with verifiable XML tags. Model Reasoning will begin with brief opening overview that introduces the budget revision and its primary goal. plan to include supporting reference here to anchor the main concept. Next, will explain the specific changes, including the 12% infrastructure spending increase, using reference to highlight this key figure. Finally, will describe the ministers statements on sustainable development, ensuring supporting reference is included to validate the policy direction. Generated Summary The Federal Ministry of Finance has announced significant revisions to the budget framework [<a3f5e823>]. The proposed changes include 12% increase in infrastructure spending [<b7d2c941>]. Minister Schmidt emphasized commitment to sustainable development [<e1a5b829>]... View Source Citations: <a3f5e823> The Federal Government has submitted the draft budget... <b7d2c941> Infrastructure investments will increase by 12%... <e1a5b829> Minister Schmidt stated that sustainability remains... Figure 5: Example sui-1 output showing first-person reasoning about summary structure, followed by the summary with inline citations. Interactive demo: https://huggingface.co/spaces/ellamind/sui-demo Qualitative Analysis. Figure 5 shows representative model output. The structured reasoning step identifies key document themes before generation, enabling coherent organization. XML tag citations immediately follow their supported claims, allowing mechanical verification. Baseline models typically either omit citations entirely or produce malformed tags that do not match source sentences."
        },
        {
            "title": "6 Release",
            "content": "To facilitate reproducibility and enable practical applications, we release our trained model weights and associated resources. 6.1 Released Artifacts The primary release comprises the fine-tuned sui-1 model weights in two variants: 6 sui-1-24B. The full-precision model weights are available at ellamind/sui-1-24b on the HuggingFace Hub.3 sui-1-24B-FP8. The FP8 quantized variant is available at ellamind/sui-1-24b-fp8, providing deployment-ready model with reduced memory requirements. This variant is recommended for production deployments where memory efficiency is prioritized. Both model variants are released under the Apache 2.0 license, enabling commercial use. 6.2 Dataset Availability The training dataset is available at ellamind/summarizer_dataset_v1 on the HuggingFace Hub. The dataset includes all columns described in this paper: source documents with XML tags, generated summaries, reasoning traces, custom instructions, and quality annotations. Researchers can use this dataset to reproduce our training procedure or develop alternative approaches to citation-grounded summarization. 6.3 Usage Recommendations We recommend temperature 0 for deterministic, reproducible outputs. The model uses the standard Mistral-Small-3.2 chat template with system prompt establishing the summarizer role, followed by the tagged source document and optional custom instruction. The XML tagging preprocessing can be implemented using standard sentence segmentation libraries (spaCy recommended for German) with MD5 hashing for tag generation. The tag format must match the training data: 8-character lowercase hexadecimal identifiers wrapped in angle brackets."
        },
        {
            "title": "7 Conclusion",
            "content": "We presented sui-1, 24B parameter model trained to produce verifiable inline citations through synthetic data generation. Our synthetic data pipeline with multi-stage quality filtering produces 22K verified training examples across five languages. Evaluation demonstrates that task-specific training dramatically improves format compliance and instruction following (0.895 versus 0.137 to 0.411 for baselines), with the 24B model approaching the performance of the reference model (0.842 vs 0.891). Limitations. Training data derives primarily from German-language sources (parliamentary documents, web text, and Wikipedia), which may limit generalization to other languages and specialized domains. The LLM-as-a-judge evaluation, while efficient and scalable, may exhibit systematic biases compared to human evaluation. Model Release. Weights are available on HuggingFace: ellamind/sui-1-24b (full precision) and ellamind/sui-1-24b-fp8 (FP8 quantized)."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Torsten Fassbender and Florentin Rauscher (PwC) for the initial idea of using XML tagging for inline citations and for sharing an example using the 8-character hash format. Additional computational resources were provided through the AI service center KISSKI (grant no. 01IS22093C), funded by the German Federal Ministry of Education and Research (BMBF). 3The repository includes an end-to-end example script demonstrating document tagging, inference, and citation extraction."
        },
        {
            "title": "References",
            "content": "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-RAG: Learning to retrieve, generate, and critique through self-reflection. arXiv preprint arXiv:2310.11511, 2024. URL https://arxiv.org/abs/2310.11511. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. Enabling large language models to generate text with citations. arXiv preprint arXiv:2305.14627, 2023. URL https://arxiv.org/abs/2305. 14627. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2022. URL https://arxiv.org/abs/2106.09685. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):138, 2023. URL https://dl.acm.org/doi/10.1145/3571730. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-Eval: NLG evaluation using GPT-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023. URL https://arxiv.org/abs/2303.16634. Mistral AI. Mistral small 3.2 24b instruct. https://huggingface.co/mistralai/ Mistral-Small-3.2-24B-Instruct-2506, 2024. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Sahoo, Harshit Mahajan, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of GPT-4. arXiv preprint arXiv:2306.02707, 2023. URL https://arxiv.org/abs/2306.02707. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted questionanswering with human feedback. arXiv preprint arXiv:2112.09332, 2022. URL https://arxiv. org/abs/2112.09332. OpenAccess AI Collective. Axolotl: tool for fine-tuning large language models. https://github. com/OpenAccess-AI-Collective/axolotl, 2024. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models, 2023. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2023. URL https://arxiv.org/abs/2212.10560. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. URL https://arxiv.org/abs/2201.11903. Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, and Juanzi Li. LongCite: Enabling LLMs to generate fine-grained citations in long-context QA. arXiv preprint arXiv:2409.02897, 2024. URL https://arxiv.org/abs/ 2409.02897. Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. PEGASUS: Pre-training with extracted gap-sentences for abstractive summarization. arXiv preprint arXiv:1912.08777, 2020."
        },
        {
            "title": "A Preprocessing Details",
            "content": "This appendix describes the sentence segmentation and XML tag generation process used to prepare source documents for citation-grounded summarization. A.1 Sentence Segmentation We employ language-specific spaCy models for sentence boundary detection: de_core_news_lg for German, en_core_web_sm for English, fr_core_news_sm for French, it_core_news_sm for Italian, and es_core_news_sm for Spanish. Special handling addresses common segmentation errors: Abbreviations: Common abbreviations (e.g., Dr., Nr., bzw.) are protected from incorrect sentence splits Enumeration markers: Patterns like 1., a) at sentence starts are preserved Whitespace normalization: Multiple spaces and irregular line breaks are standardized A.2 XML Tag Generation Each sentence receives unique identifier through the following process: 1. Compute MD5 hash of the sentence text (UTF-8 encoded) 2. Extract first 8 characters of the hexadecimal digest 3. Wrap sentence with opening and closing XML tags: <[hash]>sentence</[hash]> For example, the sentence The budget was approved. might receive the tag a3f5e823, resulting in: <a3f5e823>The budget was approved.</a3f5e823> This scheme offers several advantages over alternatives: Determinism: The same sentence always produces the same tag Language-agnostic: Works across all languages without modification Collision resistance: 8 hex characters provide 168 4.3 billion unique tags Compactness: Short tags minimize token overhead compared to verbatim quotes The verification algorithm distinguishes citation tags (8-character hex patterns) from legitimate HTML formatting tags (e.g., <b>, <i>) by checking the tag content against the set of valid source document tags."
        },
        {
            "title": "B Prompt Templates",
            "content": "This appendix provides key excerpts of the prompt templates used in our synthetic data generation pipeline. B.1 Oneshot Summarization Prompt The oneshot prompt is used for documents that fit within single context window. Key elements include the reasoning instruction, XML tag selection guidelines, citation placement rules, and custom instruction handling. # Instructions 1. Start by thinking about how to structure {word_count}-word-long professional summary of the whole text. What are the most important sections of the source text? Which details should the summary contain without exceeding the requested word count? From which specific sections should the {number_of_xml_tags} XML tags be taken to ensure quotes support all important details and references will be well-distributed throughout the summary? The reasoning should be in {language}. 2. The input text is sentence-tagged using XML tags. Create list of {number_of_xml_tags} XML tags from the given text, capturing the most significant data and facts. Ensure that the sentences within the XML tags capture the most important information and are well-distributed throughout all important sections of the text. 3. Make sure that all key paragraphs in the summary will be supported by XML tags and references! Use only XML tags from the sentence-tagged input text! 4. Each XML tag from the <xml_tags> list must appear exactly once in the summary. Do not include or paraphrase the original sentence text--only reference its tag. 5. Each reference must appear individually, never combine multiple references at once! Always insert the corresponding citation immediately after the statement it supports. [Rules 6-16 continue with formatting, structure, and custom instruction handling guidelines...] B.2 Iterative Chunk Summarization Prompt The iterative prompt is used for individual chunks of longer documents. It differs from the oneshot prompt in acknowledging the partial nature of the content and targeting consistent 300-600 word output regardless of chunk size. B.3 Iterative Final Merge Prompt The merge prompt synthesizes partial summaries into coherent final summary. It includes instructions for eliminating redundancy across partial summaries while preserving all unique information and maintaining proper citation distribution. B.4 Custom Instruction Generation Prompt Custom instructions are generated using separate prompt that analyzes the source document and produces contextually appropriate requests: 10 Based on the following text excerpt, generate 6 custom instructions in {language}: Generate 3 POSITIVE instructions that CAN be fulfilled based on this texts content. These should be varied and cover different aspects: - Audience level: From general public to domain experts - Format preferences: Bullet points, paragraphs, news article style - Focus areas: Financial aspects, policy recommendations, ethical considerations, technical details - Tone & Style: Academic, critical, neutral, conversational - Information density: High-level overview vs. comprehensive detail Generate 3 ADVERSARIAL/NEGATIVE instructions that CANNOT be meaningfully fulfilled because they ask about topics NOT present in this text."
        },
        {
            "title": "C Training Configuration",
            "content": "Parameter Value Base Model LoRA Rank / Alpha LoRA Dropout Target Modules Mistral-Small-3.2-24B-Instruct 16 / 32 0.05 q, k, v, o, gate, up, down Learning Rate Warmup Ratio Epochs Effective Batch Size 1.5e-4 (cosine schedule) 0.02 2 Sequence Length Precision 99,968 tokens bfloat16 Table 4: Complete LoRA fine-tuning hyperparameters. Training uses the Axolotl framework (OpenAccess AI Collective, 2024) with Flash Attention 2 and gradient checkpointing on four NVIDIA H100 GPUs with context parallelism to handle the extended sequence lengths. Examples follow ChatML format with system prompt, user message (tagged document + optional instruction), and assistant response (reasoning + summary with citations). The FP8 quantization uses standard post-training quantization, reducing model size from approximately 48GB to 24GB while maintaining generation quality on most metrics."
        },
        {
            "title": "D Example Outputs",
            "content": "This appendix provides two representative examples demonstrating sui-1s output with different custom instructions. D.1 Example 1: Bullet Point Format (English) Custom Instruction: Summarize the text in 5 bullet points. Source: German parliamentary document on Environmental Liability Act amendments. 11 The proposed amendments to the Environmental Liability Act establish clear framework for the financial responsibility of operators, stipulating that they must bear the costs of prevention, damage limitation, and remediation measures. However, this obligation is not absolute; the operator is relieved of these costs if the environmental damage or immediate danger was caused by third party despite the operator having taken appropriate safety precautions, or if the damage occurred while strictly adhering to official permits and regulations and the operator can prove they did not act with intent or negligence [<43901bb4>]. significant exemption is introduced for the agricultural and forestry sectors, where the cost liability for the responsible party is waived if environmental damage results from emissions, activities, or the use of products within the scope of proper land and forest management, provided the damage was not caused by intentional or negligent actions [<c50fa73d>]. The text clarifies the enforcement powers of the competent authorities, specifying that they can act both upon application and ex officio to ensure the implementation of remediation obligations [<49d7b274>]. To streamline the legal framework, the summary suggests removing the requirement for authorities to justify their administrative acts within the specific Environmental Liability Act, as this duty is already sufficiently covered by the general Administrative Procedure Act. The summary highlights contentious debate regarding the introduction of financial security regulation, with the Federal Council arguing that the federal government should immediately utilize its authorization to create such regulation [<4402efdb>]. D.2 Example 2: Expert Focus (German) Custom Instruction: Fasse die im Text genannten finanziellen Auswirkungen auf den Bundeshaushalt sowie die erwarteten Bürokratiekosten für die Wirtschaft detailliert zusammen. (Summarize the financial impacts on the federal budget and expected bureaucracy costs for businesses in detail.) Source: Budget Committee report on the Family Care Leave Act. Der vorliegende Bericht des Haushaltsausschusses befasst sich mit den finanziellen Auswirkungen und dem Vollzugsaufwand des Gesetzentwurfs zur Familienpflegezeit. Ein wesentlicher Aspekt ist die Bereitstellung einer Kreditlinie zur Refinanzierung der Arbeitgeber, wobei dem Bund langfristig jährliche Aufwendungen von bis zu 8 Millionen Euro entstehen, die im Einzelplan 17 ausgeglichen werden müssen [<6dee22a0>]. Diese Kosten decken die durchschnittlichen Arbeitsund Zinskosten sowie das Risiko von Kreditausfällen durch Privatinsolvenzen ab. In der Sozialversicherung ist mit einem Saldo aus Beitragsmindereinnahmen und -mehreinnahmen zu rechnen, der jedoch schwer abschätzbar ist. Die Länder und Gemeinden werden durch das Gesetz nicht mit zusätzlichen Kosten belastet [<063a42b4>]. Der Vollzugsaufwand wird durch das Bundesamt für Familie und zivilgesellschaftliche Aufgaben bewältigt. Hierbei wird betont, dass der notwendige Personalbedarf durch die Kapazitäten gedeckt werden kann, die infolge des Wegfalls des Zivildienstes frei werden [<221ca355>]. Ein zentraler Punkt der Analyse sind die Bürokratiekosten, die durch neue Informationspflichten entstehen. Für die Wirtschaft ergeben sich mittelfristig Kosten von durchschnittlich rund 32,50 Euro pro Fall bei einer Fallzahl von bis zu 44.000 pro Jahr für Arbeitgeber sowie rund 10 Euro pro Fall für Versicherer [<91b1f9b2>]. Die betroffenen Bürgerinnen und Bürger müssen mit einem Zeitaufwand von insgesamt rund 20 Minuten pro Person rechnen. Both examples demonstrate sui-1s ability to follow custom instructions while maintaining citation accuracy. The XML tags (e.g., <43901bb4>) link directly to specific sentences in the source documents. 12 D.3 Evaluation Platform Figures 6 and 7 show the elluminate evaluation interface used for LLM-as-a-judge scoring. The platform provides per-criterion pass rates, aggregated overall scores, and detailed per-sample explanations. Figure 6: Elluminate evaluation dashboard showing sui-1 results: 84.2% overall score with per-criterion breakdown. Figure 7: Per-sample evaluation detail showing individual criterion scores and explanations for single test example."
        }
    ],
    "affiliations": [
        "ellamind"
    ]
}