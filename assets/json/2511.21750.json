{
    "paper_title": "SO-Bench: A Structural Output Evaluation of Multimodal LLMs",
    "authors": [
        "Di Feng",
        "Kaixin Ma",
        "Feng Nan",
        "Haofeng Chen",
        "Bohan Zhai",
        "David Griffiths",
        "Mingfei Gao",
        "Zhe Gan",
        "Eshan Verma",
        "Yinfei Yang",
        "Zhifeng Chen",
        "Afshin Dehghan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct a comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated image-schema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the model's structured output capability. We plan to make the benchmark available to the community."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 2 0 5 7 1 2 . 1 1 5 2 : r SO-Bench: Structural Output Evaluation of Multimodal LLMs Di Feng, Kaixin Ma, Feng Nan, Haofeng Chen, Bohan Zhai, David Griffiths, Mingfei Gao, Zhe Gan, Eshan Verma, Yinfei Yang, Zhifeng Chen, Afshin Dehghan Apple First authors, Core authors, Project lead Multimodal large language models (MLLMs) are increasingly deployed in real-world, agentic settings where outputs must not only be correct, but also conform to predefined data schemas. Despite recent progress in structured generation in textual domain, there is still no benchmark that systematically evaluates schema-grounded information extraction and reasoning over visual inputs. In this work, we conduct comprehensive study of visual structural output capabilities for MLLMs with our carefully designed SO-Bench benchmark. Covering four visual domains, including UI screens, natural images, documents, and charts, SO-Bench is built from over 6.5K diverse JSON schemas and 1.8K curated imageschema pairs with human-verified quality. Benchmarking experiments on open-sourced and frontier proprietary models reveal persistent gaps in predicting accurate, schema compliant outputs, highlighting the need for better multimodal structured reasoning. Beyond benchmarking, we further conduct training experiments to largely improve the models structured output capability. We plan to make the benchmark available to the community. Date: December 5,"
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement in large language models (LLMs) and multimodal large language models (MLLMs) (McKinzie et al., 2024; Zhang et al., 2025; Tong et al., 2024; Li et al., 2024; Team et al., 2025a; Bai et al., 2025; Team et al., 2025b; Wang et al., 2025b) has expanded their role beyond natural language or image understanding and generation. Increasingly, these models are leveraged in agentic applications, such as web automation (Yang et al., 2025d; Wang et al., 2025a; Zhangheng et al., 2025), data extraction (Mixtral, 2025; Niu et al., 2025; Cui et al., 2025), and tool use (Wang et al., 2024; Chen et al., 2023; Lu et al., 2025), where outputs are consumed not by humans but by downstream systems, controllers, or APIs. In this context, model must produce outputs that are not only semantically correct, but also structurally valid, conforming exactly to predefined customized input schema, typically in JSON formats. This structured outputs formulation enables LLMs or MLLMs to interface reliably Figure 1 An example of visual structured output task. Given customized JSON schema often specified by the downstream applications, e.g. MenuReader, model is tasked to extract information from input image, following the schema definition and user instruction. 1 with external systems, perform programmatic reasoning, and execute grounded actions. Due to its importance, several industrial API extensions have incorporated structured output modes (e.g., OpenAI (OpenAI, 2024), Google Gemini (Gemini, 2025), Anthropic (Claude, 2025).) While structured output from text-only inputs (Yang et al., 2025b; Tang et al., 2023b; Gu et al., 2024; Geng et al., 2025) has received growing attention, the multimodal domain, especially the visual structured output capability of MLLMs, remains largely underexplored. Given visual inputs (e.g., natural images, screens, charts) and customized schemas (often specified by downstream applications), visual structured output involves extracting information from images, and aligning them with hierarchical schemas and user instructions. For instance, adding the product labels shown on UI screen to the shopping cart, or saving the information from the menu to the notes apps, illustrated in Figure 1. Prior visual-language works such as Pix2Struct (Lee et al., 2023), Image2Struct (Roberts et al., 2024), and IR3D-Bench (Liu et al., 2025) have made progress toward structured visual understanding, but they typically emphasize text captioning or semantic parsing with limited domains rather than rigorous customized schema-level adherence with diverse real-world scenarios. Most relevant to visual structured output is key information extraction (KIE) in document parsing and OCR understanding (Yang et al., 2024; Ouyang et al., 2025), where MLLM is tasked to extract information for pre-defined keywords. However, KIE focuses on semantic parsing with simple input fields, ignoring the diversity of real-world schemas with nested, complex structure. As result, there is no study that systematically quantifies how well MLLMs can produce structured, schema-compliant outputs grounded in visual evidence. To address these limitations, we conduct comprehensive study of visual structured output capability for MLLMs. To this end, we build SO-Bench, visual structured output benchmark with nearly 1.8K diverse, high-quality samples, each image paired with unique JSON schema and user instruction, sourced from pool of over 112K diverse images in four domains (natural images, UI screens, documents, and charts), 6.5K real-world and synthetic schemas, and 60K user profiles. When constructing the benchmark dataset, we carefully design multi-stage auto-labeling pipeline, including (1) accurate image-schema association through multimodal embedding search, (2) synthetic schema generation with multi-image grouping, and (3) human-in-the-loop progressive response generation and refinement with critic model. In addition, human domain experts conduct inspection at each stage to ensure data quality. To quantify models structured output capability, we develop an evaluation pipeline that measures both models ability to follow schema instructions and its behavior under constrained decoding. The evaluation decomposes performance into schema adherence, structural fidelity, and value-level accuracy (with exact match or fuzzy match), enabling fair comparisons of different schema responses. Based on SO-Bench, we conduct comprehensive benchmarking experiments across wide range of MLLMs of different scales, including open-sourced and proprietary models. Our results reveal that small models (e.g., with 3B and 7B scales) exhibit substantial gaps in structured generation and schema compliance compared to their proprietary counterparts. Though the strongest frontier models (e.g., GPT-5 and Gemini 2.5-Pro) show good performance in schema following with over 95% accuracy, there is large room of improvements for predicting fully correct structured outputs, with only up to nearly 19% accuracy (fuzzy match). Ablation studies further illustrate that models visual structured output ability highly correlates with its competence in visual instruction following, agentic tool invocation and parameter filling, and general visual knowledge. To study how training impacts models visual structured output capabilities, we further construct large-scale training set at the post-training stage using the same data labeling pipeline, and train MLLMs with supervised finetuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR). Experimental results show that both SFT and RLVR enhances schema validation and field matching accuracy up to 20% and 13%, respectively, underscoring the importance of targeted supervision for improving structured reasoning. In summary, our contributions are three-folds: diverse, high-quality benchmark, SO-Bench, to quantify structured output capabilities of MLLMs; Comprehensive experiments to show the limitation of existing MLLMs in visual structured outputs; Model training experiments to largely enhance the structured output capabilities. We plan to release our benchmark as well as the evaluation pipeline."
        },
        {
            "title": "2 Related Works",
            "content": "Structured Output Benchmarks. Several benchmarks have been recently proposed to quantify LLMs capability to generate structured output with text-only inputs. For example, StructEval Yang et al. (2025b) benchmarks text-to-structure generation across formats like JSON, YAML, and HTML, emphasizing format fidelity. JSONSchemaBench Geng et al. (2025) extends this by testing constrained decoding on thousands of real-world JSON schemas, measuring coverage and quality across diverse constrained decoding frameworks. StructBench Tang et al. (2023b) auto-generates structure-rich text tasks such as table-to-JSON or LaTeX conversion. In contrast to text-only applications, visual structured output focuses on extracting image information following the pre-defined schemas. Pix2Struct Lee et al. (2023) pretrains an image-to-text model that parses screenshots to HTML, improving layout understanding. Image2Struct Roberts et al. (2024) targets structured extraction from rendered data (webpages, formulas, music scores), yet only focuses on digital images with limited domains. IR3D-Bench Liu et al. (2025) tests structured scene reconstruction from pure synthetic 3D renderings. Overall, existing studies either omit image inputs or cover narrow visual domains, leaving the broader capability of MLLMs in generating diverse, schema-grounded outputs under-explored. Agentic Tool Use Benchmarks. SO-Bench also relates to agentic tool-use evaluations that quantify structured reasoning and function-call accuracy. The Berkeley Function Calling Leaderboard (BFCL) Patil et al. (2025) and Tau-Bench Yao et al. (2024) assess agents ability to produce executable structured calls and manage multi-step tool interactions, while ToolVQA Yin et al. (2025) extends this concept to visual question answering with external tools. Unlike these benchmarks, which focus on textual tool APIs or with limited number of tools (e.g., 14 tools in GTA (Wang et al., 2024)), SO-Bench evaluates multimodal structured generation grounded in visual inputs with large-scale diverse JSON schemas, bridging perception and structured reasoning. OCR Understanding. Visual structured output connects to document understanding and OCR-rich tasks, as it requires model to correctly parse textual information from images. Benchmarks such as OCRBenchV2 (Fu et al., 2024), CC-OCR (Yang et al., 2024), and OmniDocBench (Ouyang et al., 2025) focus on extracting text and layout from scanned or synthetic documents, often formulating key information extraction (KIE) tasks with simple predefined templates in one-layer dictionary. In contrast, SO-Bench focuses on distinct, realistic JSON schemas with nested structures, aligned with application-driven agentic tool-use requirements."
        },
        {
            "title": "3.1 Overview",
            "content": "We formulate the visual structured output problem as follows: given an input image I, pre-defined JSON schema S, and user instruction X, model is expected to generate structured output that both (1) conforms syntactically to and (2) semantically reflects the information extracted from and X. For multimodal LLM, the structured output is generated auto-regressively with the predicted probability p(Y I, X, S). JSON schema is nested dictionary specifying keys, data types, and object hierarchy following the standard (org, 2025). user instruction provides guidance on the information to be extracted from the input images, which can be precise and descriptive, or concise and ambiguous, for example, Help me save this poster. The benchmark should be diverse in order to test the general visual structured output capabilities of MLLMs. The diversity originates from images (domain coverage and visual representations) and JSON schemas (e.g., schema complexity, field type variety), which will be introduced in Section 3.2. The main challenge for building such benchmark is how to associate an image with the representative JSON schema, and generate its structured output response accurately and efficiently. To tackle this challenge, we develop multi-stage auto-labeling pipeline with human in-the-loop detailed in Section 3.3."
        },
        {
            "title": "3.2 Data Collection",
            "content": "To ensure broad visual coverage, SO-Bench aggregates image data from ten recent public datasets spanning four domains with different image resolutions and aspect ratios: (1) UI (mobile, web, and desktop screens), 3 Figure 2 Overview of the multi-stage data generation pipeline for SO-Bench, including schema generation, user intent generation, and response generation stages. At each stage, we leverage proprietary frontier models (e.g., GPT-5 and Gemini-2.5-Pro) as generators with careful prompt design. Data from each stage is checked with human domain experts before passing to the next stage. Before the schema generation stage, input images and JSON schemas are embedded through CLIP model for embedding search. Details of the pipeline is introduced in Section 3.3. (2) natural images, (3) documents, and (4) charts. These domains jointly cover wide range of real-world structured extraction scenarios, such as menu parsing, receipt understanding, poster collection, UI layout analysis, document parsing, and chart interpretation. We prioritize English-language data and selectively down-sample test sets to maximize visual and semantic diversity. Figure 3a summarizes the data composition and domain distribution. Regarding JSON schema collection, we curated large and diverse schema repository covering both real-world and synthetic cases. The repository includes subset of the real-world JSON schemas from JsonSchemaBench (Geng et al., 2025), which spans domains such as function signatures, service APIs, and system configurations. Out of 11K full schema repository, we keep 6K unique schemas related to visual applications. We additionally generate 500 synthetic JSON schemas by prompting proprietary model for specific use cases (e.g., reading receipt, parsing nutrition labels). This collection serves as JSON schema databank from which imageschema pairs are constructed. Embedding. To enable efficient schemaimage retrieval and matching (detailed in Section 3.3), we embed both images and JSON schemas using an off-the-shelf CLIP model (Radford et al., 2021). For an Image I, we first use proprietary model (e.g., GPT-5 or Gemini-2.5-Pro) to generate its caption , and then employ the CLIPs image and text encoders to extract visual and textual embeddings, denoted by EI and ET , respectively. As for JSON schema S, we prompt with another proprietary model to summarize its fields, purpose, and example use cases with canonical name (e.g., ReceiptExtractor, MenuReader). These textual descriptions are embedded with the CLIP text encoder, denoted by ES."
        },
        {
            "title": "3.3 Data Curation",
            "content": "Figure 2 illustrates the the full data curation workflow, which can be divided into three stages. In the schema generation stage, JSON schema is linked to the input image, either selected from the schema repository (Section 3.3.1) or generated on-the-fly with multi-image grouping (Section 3.3.2). In the second stage, user instructions are added to the image-schema pairs to simulate realistic human-agent interaction patterns. Inspired by the persona-based prompt generation (Ge et al., 2024), we first synthesize 60K diverse user profiles varying in age, occupation, and locale. For each imageschema pair, user profile and the chat style are sampled randomly as context information for the user intent, leading to rich spectrum of instruction styles, including conversational, direct, ambiguous, or even dialectal to reflect natural user diversity. At the response generation stage, the structured output is generated and iteratively refined, based on critic model and human annotators feedback. To ensure data quality, group of eight human domain experts check and filter the intermediate outputs, before proceeding to the next stages. 3.3.1 Image-Schema Association Given an image embedding, we retrieve the top-k most relevant JSON schemas from the schema databank based on the visual-textual multimodal nearest neighbor search. Given an image and schema S, its similarity 4 (a) Image category composition. (b) t-SNE Plot for image features. Figure 3 (a). SO-Bench is collected from four categories: (1) UI: RICO (Deka et al., 2017), WebUI (Wu et al., 2023), ScreenSpot Pro (Li et al., 2025)); (2) Documents: OmniDocBench (Ouyang et al., 2025), DocVQA (Mathew et al., 2021), InfographicVQA (Mathew et al., 2022); (3) Charts: ChartQA-Pro (Masry et al., 2025), ChartMuseum (Tang et al., 2025); and (4) Natural images: HierText (Long et al., 2022). (b). Image feature distribution. We use CLIP (Radford et al., 2021) image embedder to embed all images, and show their t-SNE features. The distribution from each image category is different, indicating the visual diversity. score is computed as the weighted sum of cosine similarities between image/caption embeddings and schema embeddings, written as: sim(I, S) = w1 cos(EI , ES) + w2 cos(ET , ES). From these candidates, large multimodal model is prompted to select the best-matching schema, ensuring semantic alignment and structural appropriateness. Typically, we set = 20, and interleave schema selection from the model or random selection to increase data diversity. (3.1) Note that the semantic search approach described above could still end up with less-relevant image-schema associations. We performed human quality check and filter out those samples before proceeding to the next data generation stage. 3.3.2 Schema Generation with Multi-image Grouping To further enhance schema diversity and structural depth, we introduce multi-image schema generation pipeline. For each query image, we identify its top-m nearest neighbor images in the embedding space (based on both image and caption similarity). The selected image cluster (in our experiment = 3) is jointly passed to schema generator that proposes unified nested schema capturing the shared structure across the images, e.g., multi-item menus or multi-section forms. This approach effectively generalizes synthetic schema generation beyond single-image templates. Formerly, given images and j, their similarity score is computed as: sim(Ii, Ij) = w1 cos(EIi, EIj ) + w2 cos(EIi , ETj ) + w3 cos(ETi, EIj ) + w4 cos(ETi, ETj ). (3.2) 3.3.3 Progressive Response Generation and Refinement For each triplet of image, schema, and user intent, we generate the structured output using our response generation pipeline. When available, auxiliary signals such as OCR texts, layout metadata, or HTML structures (for UI images) provided by the original benchmark datasets are used as guidance to improve fidelity. The generated JSON outputs are validated against schema constraints to ensure syntactic correctness and logical completeness. Besides, we introduce hybrid automated and manual review loop. criticrefiner workflow, powered by LLM-based validators, examines each structured output for schema validity and semantic consistency and summarizes suggestions of improvements. Invalid or sub-optimal outputs are regenerated up to three times considering the feedback from the critic model. During this process, human experts conduct manual inspections and provide feedback for the output refinement. 5 (a) Structure depth distribution. (b) Field count distribution. (c) Median Structure depth. Figure 4 JSON schema data statistics in SO-Bench. The input JSON schemas are more complex than output formats in terms of the number of nested structure depths and the number of fields. Regarding image category, chart data shows the most complex schema structure. Figure 5 Top 100 frequent feature counts in the input JSON schemas (best view with magnificence)."
        },
        {
            "title": "3.4 Dataset Statistics",
            "content": "To characterize structural complexity, we compute both the depth of nested hierarchies and the number of fields in each JSON schema and corresponding structured output. Figure 4a and Figure 4b present the distributions of structure depth and field count, respectively. In general, input JSON schemas exhibit higher complexity than the generated structured outputs, ranging from single-level schema with one field to as deep as 22 levels and over 2K fields, reflecting the rich diversity of schema structures in SO-Bench. Figure 4c further compares the median structure depth across four image domains, showing that charts tend to involve slightly deeper hierarchies than other categories. Figure 3b visualizes the t-SNE projection of image embeddings obtained from the CLIP image encoder. Images from different domains form distinct clusters, confirming the visual diversity of the dataset. Finally, we plot the top 100 most frequent feature names appearing in the JSON schema repository in Figure 5."
        },
        {
            "title": "3.5 Evaluation Metrics",
            "content": "We used abstract syntax tree (AST) evaluation method following BFCL (Patil et al., 2025). In particular, given the structured output from the model and ground truth in dict format, we iteratively compare all keys in the dict and their corresponding values, and recursively compare all the nested dictionaries. For all primitive types (int, float, str, list), we used exact match by default. Since not all information in the ground truth is directly relevant to the user intent, and some visual information can be hard to extract exactly (e.g., blurry, truncated images), we further define two other matching strategies for primitive types: 1) we allow fuzzy match if the ground truth information does not include explicit texts shown in the image (e.g., exact data value in line plot). Here, we adopt normalized edit distance to compare strings and relative error to compare int and float. 2) We also label ignore for certain fields, if the key is not required field in the schema definition and it is not directly related to the user query (e.g., the style of an UI screenshot when asking about content). Here, we simply assign matching score of 1, if the fuzzy match score is above pre-defined threshold and skip comparisons for ignored fields. To enable such fine-grained evaluation, we also 6 Models Schema Val. Field Match (Exact) Full Match (Exact) Field Match (Fuzzy) Full Match (Fuzzy) Qwen3-VL (2B) (Yang et al., 2025a) Qwen2.5-VL (3B) (Bai et al., 2025) Qwen3-VL (4B) (Yang et al., 2025a) Gemma-3 (4B) (Team et al., 2025a) Intern3.5-VL (4B) (Wang et al., 2025b) Phi-4-Vision (5.6B) (Abouelenin et al., 2025) Qwen3-VL (8B) (Yang et al., 2025a) Pixtral-2409 (12B) (Agrawal et al., 2024) LLama-4-Scout (17B-16E) (Meta, 2025) Gemma-3 (27B) (Team et al., 2025a) Qwen2.5-VL (32B) (Bai et al., 2025) Qwen3-VL (32B) (Yang et al., 2025a) Intern3.5-VL (38B) (Wang et al., 2025b) Qwen2.5-VL (72B) (Bai et al., 2025) Claude-4.5-Haiku (Antrophic, 2025a) Claude-4.5-Sonnet (Antrophic, 2025b) GPT-4o-mini (Hurst et al., 2024) GPT-4o (Hurst et al., 2024) GPT-5-mini (OpenAI, 2025) GPT-5 (OpenAI, 2025) Gemini-2.5-flash (Comanici et al., 2025) Gemini-2.5-pro (Comanici et al., 2025) 16.28 60.71 57.32 61.47 71.91 22.04 54.72 77.16 83.78 82.93 87.16 47.71 89.32 87.39 95.70 96.50 83.64 76.94 98.70 96.38 91.69 97. 36.42 40.58 53.00 30.40 50.55 27.27 53.24 43.19 52.80 47.06 57.28 58.38 57.68 57.86 61.79 62.32 50.91 59.73 58.59 61.17 64.93 71.46 2.50 1.12 3.76 0.77 1.68 0.46 5.24 1.12 2. 1.68 3.84 5.74 3.08 4.22 3.63 5.15 2.67 4.96 5.09 5.22 6.74 11.38 39.53 41.59 53.49 31.10 51.53 27.78 55.06 45.64 54.16 47.33 58.84 59.35 58.01 58.91 62.44 62.67 52.74 60.91 60.29 62.74 66.32 73. 6.36 2.68 6.78 1.43 2.96 0.72 8.25 3.30 5.54 3.01 6.91 9.57 5.35 9.25 6.93 8.74 5.82 10.39 10.07 11.60 11.31 18.91 Table 1 comparison of structured output performance among different models. We cluster models based on their model size (active parameters), as well as open-sourced or proprietary models. add an evaluation label generation step in our pipeline, where we provide the image, schema, user intent and ground-truth response (after final refinement), and prompt proprietary MLLM to generate matching type for each primitive field (e.g., one of {exact, fuzzy, ignore}). The pseudo code of our evaluation function is presented in Algorithm 1. We report the following three metrics in our experiments. (1) Schema Validation Accuracy: the percentage of outputs that are valid w.r.t the given schema definition. (2) Field Matching Accuracy (FMA): Let F(D) denote all fields in nested dictionary (including both intermediate nested structures and leaf fields). For ground truth G(k) and output O(k) across examples: FMA = (cid:80)N k=1 {f F(G(k)) : F(O(k)), Match(f, )} k=1 F(G(k)) (cid:80)N , (3.3) where Match(f, ) returns true for exact/fuzzy matches, with nested structures matching when all their subfields match. (3) Full Structure Matching Accuracy (FSMA): FSMA ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) k=1 (cid:105) (cid:104) F(G(k)), F(O(k)) : Match(f, ) . 1 (3.4)"
        },
        {
            "title": "4 Experiments and Results",
            "content": "In this section, we first report our benchmarking results in Section 4.1 and Section 4.2, and then introduce our training experiments in Section 4.3. Unless mentioned otherwise, we evaluate models visual structured output capability with detailed system prompt illustrating the task instruction. To do that, we serialize the input JSON schema, append it with the user intent to prompt the models for response. Detailed prompt is provided in Appendix E."
        },
        {
            "title": "4.1 Main Results",
            "content": "Table 1 compares the structured output performance across several state-of-the-art open-source and large proprietary models. We report three metrics, namely, schema validation accuracy, field matching accuracy (exact and fuzzy), and full structure matching accuracy (exact and fuzzy), as introduced in Section 3.5. Among 7 Benchmarks IFEval (Zhou et al., 2023) LiveBench-250425 (IF) (White et al., 2024) LiveBench-250425 (Coding) (White et al., 2024) BFCL (Patil et al., 2025) OCRBenchV2 (Fu et al., 2024) CC-OCR (Yang et al., 2024) DocVQA (Mathew et al., 2021) RefCOCO (Yu et al., 2016) MMMU (val) (Yue et al., 2024) MathVista (Lu et al., 2023) MIABench (Qian et al., 2024) Schema Val. Field Match -0.261 -0.257 0.024 0.600 -0.062 -0.429 -0.163 -0.199 0.555 0.024 0.685 0.498 0.579 0.960 0.030 0.840 0.289 0.548 0.637 0.014 0.939 0.090 -0.308 0.539 0.754 0.789 0.469 -0.119 0.464 -0.438 0.791 0.480 0.878 0.420 0.212 0.050 0.001 0.106 0.779 0.070 0.278 0.000 0.097 0. Table 2 Pearson correlation (r ) and the p-value for testing non-correlation (p ) between structural output metrics (Schema Validation, Field Match) and external benchmarks. IFEval, LiveBench-250425 (IF) for text-only instruction following; BFCL for agentic tool use; OCRBenchV2, CC-OCR, DocVQA for text-rich image understanding; RefCOCO for image referring; MMMU for general knowledge; MathVista for math; MIABench for visual instruction following. High correlations with high confidence (low values) are highlighted in blue. all models, Gemini-2.5-Pro achieves top results across all metrics, with schema validation accuracy reaching nearly 98%, demonstrating strong schema adherence capabilities. GPT-5 also shows clear improvement over its predecessor GPT-4o with over 15% higher schema validation accuracy. However, all models exhibit relatively lower performance in field matching. For field match (fuzzy), only Gemini-2.5-Pro achieves accuracy higher than 70%, and no model achieves above 20% on Full Match (fuzzy), indicating substantial room for improvement in producing fully compliant structured outputs. Upon further inspection of errors, we also found that the large gap between field match and full match accuracies are often due to the model predicting semantically correct answers for certain fields, which is beyond what fuzzy match can capture. We leave semantic matching methods out from our metrics for simplicity and reproducibility, and we think developing more flexible matching functions could be an interesting future work direction."
        },
        {
            "title": "4.2 Ablation Studies",
            "content": "4.2.1 Metrics Correlation Analysis To better understand how SO-Bench relates to existing benchmarks, we analyze its correlation with several public datasets that capture complementary aspects of multimodal reasoning. To do that, we compute Pearson correlation coefficients between the SO-Bench metrics and representative benchmark metrics among several models in Table 1. The results are summarized in Table 2 (Appendix provides more details). Strong and statistically significant correlations with low values are highlighted in blue. From the results, SO-Bench shows the strongest alignment with BFCL (Patil et al., 2025), LiveBench (Coding) (White et al., 2024), MMMU (Yue et al., 2024), and MIABench (Qian et al., 2024), indicating that structured output performance is closely linked to models agentic reasoning and tool use, general vision knowledge, and visual instruction-following abilities, respectively. Though the correlation between schema compliance and OCR understanding (OCRBenchV2 (Fu et al., 2024) and DocVQA (Mathew et al., 2021)) appears weaker with high values, the Field Match metric exhibits moderate correlation with approx. = 0.5, suggesting that accurate value extraction partially benefits from text recognition capabilities. Interestingly, we do not observe strong correlations with IFEval (Zhou et al., 2023) and RefCOCO (Yu et al., 2016) benchmarks. 4.2.2 Schema complexity We further analyze results by schema depth to gain finer understanding of structured output performance. Figure 6 reports schema validation accuracy and fuzzy field-match accuracy across different input schema 8 Figure 6 Model performance across different schema depths. The left panel reports schema validation accuracy, while the right shows fuzzy field-match accuracy. Number of examples per schema depth 4 with 204 frames, 5 with 746 frames, 6 with 557 frames and 6 with 262 frames. Models Structured output API Instruction Following Schema Val. Field Match (Fuzzy) Schema Val. Field Match (Fuzzy) GPT-4o-mini (Hurst et al., 2024) GPT-4o (Hurst et al., 2024) GPT-5-mini (OpenAI, 2025) GPT-5 (OpenAI, 2025) Gemini-2.5-flash (Comanici et al., 2025) Gemini-2.5-pro (Comanici et al., 2025) 99.75 99.67 94.52 95.02 95.44 96.93 51.88 60.51 60.11 62.67 67.99 71.83 95.19 92.36 99.92 99.67 98.01 98.92 56.94 63.01 61.78 64.52 71.41 76.17 Table 3 Results from structured output APIs and instruction following prompts on subset of SO-Bench. depth levels. As schema depth increases, both metrics consistently degrade for all models, suggesting that greater structural complexity poses significant challenge to visual structured output generation. In particular, Figure 6 (left) shows that GPT-5, GPT-5-mini, and Gemini-2.5-Pro sustain high schema validation accuracy above 95% even at depths larger than six, whereas smaller models, such as Intern3.5-VL (4B), exhibit substantial performance drops by around 40%. The result highlights that small models struggle to maintain schema-compliant outputs under deeper, more nested structures, with limited complex instruction-following abilities. 4.2.3 Inference with Structured Output API In this section, we evaluate models using either their public structured output APIs or instruction-following prompts. To enable this comparison, we curate subset of SO-Bench containing approximately 1.2K samples, restricted to JSON schemas supported by both OpenAI (OpenAI, 2024) and Gemini (Gemini, 2025) APIs. The results are summarized in Table 3. Interestingly, while the GPT-4o and GPT-4o-mini models achieve slightly higher Schema Validation Accuracy when using the structured output API, their Field Match Accuracy (Fuzzy) tends to decline. In contrast, for the GPT-5 series and Gemini models, instruction-following prompts generally outperform API-based structured generation. These results suggest that although structured output APIs enforce schema compliance more reliably, they may sometimes constrain content generation, leading to less accurate field-level value predictions. 9 Figure 7 Performance of models trained with different scales of data. Field Match and Full Structure Match are fuzzy version."
        },
        {
            "title": "4.3 Training Experiments",
            "content": "To study how training impacts the performance of models on our SO-Bench, we further constructed large-scale training set using the same pipeline as described in Section 3.3, except for the human verification part. In particular, we source images from the training set of HierText (Long et al., 2022), AriaUI (Yang et al., 2025c) and COYO (Byeon et al., 2022), and paired them with either our JSON schema repository or synthetic JSON schemas . We then proceed with persona-based user intent generation and response generation followed by final model-based refinement. In total, we collected 114K examples for training. We then proceed with SFT and RL experiments on this data to understand its impact. More details on training experiments are in Appendix B. 4.3.1 Supervised Fine-tuning (SFT) We conduct our experiments on an internal 3B dense model, which is pre-trained on mixture of image-text paired and text-only instruction following dataset. The model adopts ViTDet-L (Li et al., 2022) as vision encoder and uses the AnyRes strategy (Liu et al., 2024) to handle image inputs. As starting point, we conduct supervised fine-tuning on training subsets of different sizes to understand the effect of data scaling. We present the baseline model and SFT models results in Figure 7. We see that training leads to salient gains on SO-Bench. When trained on the entire training set, our 3B model even achieves comparable performance as models that are 10x larger (in Table 1). We also see that as we increase the data scale, the models performance continues to improve and there is no clear sign of plateau, so we believe that larger scale of data might further improve the model performance. How does the model generalize to unseen schema types? For this setting, we use the subset of training data that has synthetically generated schemas to train the model, which is about 35K examples. We compare against the models trained with either 35K examples with only real-world schemas or 35K randomly sampled data (covering both types). We present the results in Figure 8. Here we further break down the results by the schema type. We see that on the subset of SO-Bench with synthetic schemas, the gaps across models are relatively smaller with the random subset achieving the best performance and real-schema subset lags behind. However, on the subset of SO-Bench with real schemas, the model trained on synthetic schema performs much worse than the other two models. Thus we think that the synthetically generated schemas still exhibit distribution shift compared to real-world schemas, and learning from diverse set covering both types is necessary to achieve the best performance and generalization. How does the model generalize to unseen image domains? For this setting, we use only the AriaUI subset of the data to train the model, which has about 18K examples, and we compare this model with the counterpart that is trained on 20K randomly sampled data. We present the results in Figure 9. Overall the model trained on AriaUI subset performances worse, and when we break down the results by image categories, we see the gap is largely explained by the Charts domain. Upon further inspection, we found that AriaUI Figure 8 Performance of models on two subsets of SO-Bench. Field Match and Full Structure Match are fuzzy version. Models Schema Val. Field Match (Fuzzy) Full Match (Fuzzy) Baseline 3B +RLVR +SFT (14K) +SFT (50K) +SFT (50K) + RLVR 58.7 72.0 81.3 85.8 86.6 45.6 47.1 54.9 56.5 56. 4.4 4.9 6.5 7.1 6.9 Table 4 Results from baseline model and after RL/SFT training using subset of SO-Bench training data. training subset has average schema depth of 2.7, which is less than the average schema depth of the randomly sampled 20K data, which is 2.9, thus the model did not learn to handle deep schema well. As we see in Figure 4c, the natural images subset has the lowest schema depth whereas charts subset has the highest schema depth, which corresponds to the results that the model trained on AriaUI subset actually performs best on natural images while lags behind the most on charts. We also find that the model often extract some correct information but fail to obey the input schema by generating shallow or even flat structure. This behavior again suggests that training on large and diverse set of schemas is critical to achieve strong performance. 4.3.2 Reinforcement Learning (RL) Recently, reinforcement learning with verifiable rewards (RLVR) has gained popularity within the community due to its effectiveness in improving models performance (DeepSeek-AI et al., 2025; Yue et al., 2025). Verifiable rewards enjoy the benefit of clean and deterministic training signals and not needing separate reward model. In our case, the correctness of structured output can also be evaluated programmatically, thus it offers natural testbed for RLVR. In our experiments, we define the following reward function to encourage both syntactic validity and semantic correctness in generated outputs. Given prediction and ground truth G, the reward R(O, G) is defined as: (cid:40) R(O, G) = 0.1 α FMA(O, G)2 if is invalid JSON otherwise , (4.1) where FMA(O, G) is the field match accuracy defined in Section 3.5, and α is schema compliance multiplier: α = (cid:40) 1.0 0.8 if is valid w.r.t the schema otherwise (4.2) This formulation imposes small penalty for syntactically invalid outputs, while differentiating between schema-compliant and non-compliant valid JSON through the multiplier α. The quadratic transformation of the match score amplifies the reward signal for high-quality outputs, encouraging the policy to focus on near-correct predictions during training. We train models using Mirrow Descent Policy Optimization (MDPO) (Tomar et al., 2020) method on 14K subset of training data in RL experiments, and we experimented with 11 training from baseline checkpoint directly, as well as checkpoint that is SFT trained on 50K non-overlapping data. To understand the performance of SFT vs RLVR, we also train model on the same 14K data with SFT for comparison. The results are shown in Table 4. We see that RL training shows positive signal and it significantly boosts schema validation accuracy while also brings small gain on field match accuracies compared to the baseline. However, compared to the SFT model counterpart the performance is still much worse. When training on top of the SFT checkpoint, we see that the gains are very small. We conjecture that baseline 3B models capability is capped at certain level so learning from its own sampled outputs is less effective than learning from frontier models outputs directly. We leave further exploration on RL such as reasoning augmented training or better reward function design to future work."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced SO-Bench, new benchmark for evaluating the structured output capabilities of multimodal LLMs. By combining diverse visual domains with over 6.5K JSON schemas and unified evaluation protocol, SO-Bench provides the first systematic framework for assessing schema-grounded visual reasoning. Our experiments reveal that even the strongest frontier models still struggle with schema adherence and structural fidelity, underscoring the gap between visual understanding and reliable structured generation. Further training experiments with SFT and RLVR showcase the importance of targeted supervision for improving structured outputs."
        },
        {
            "title": "Discussions and Limitations",
            "content": "We identify several limitations of our work here. First, due to the high complexity of generating structured output, we rely on frontier models to automatically generate the initial benchmark labels. We applied several strategies to improve the label quality, including 1). multi-stage auto-labeling pipeline design, 2). per-stage correction from human annotators and critic models, 3). shuffling among proprietary models (e.g. GPT-5 or Gemini-2.5-Pro) at each generation stage to reduce model biases. However, there could still be some label errors which cap models performance. Secondly, as we discussed in the section 4.1, there could exist multiple correct ground truths for some examples and our current annotation only captures one of them. For example, semantically similar sentences for string fields, or certain flexible schema that allow different realization. Thus the models performance might be underestimated for those cases. Finally, in our evaluation metrics design, FMA and FSMA (Equation 3.5 and Equation 3.5) mainly capture the recall rates of prediction. model which predicts many additional fields not existed in ground truth labels could still achieve high FMA or FSMA scores. We leave better metrics design as future work."
        },
        {
            "title": "6 Acknowledgment",
            "content": "We thank Suzie Petryk, Dongxu Li, Guoli Yin, Zi-Yi Dou, Alexander Metz, and Jiarui Lu for invaluable suggestions and discussions. Apple and the Apple logo are trademarks of Apple Inc., registered in the U.S. and other countries and regions."
        },
        {
            "title": "References",
            "content": "Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024. URL https://arxiv.org/abs/2402.14740. Antrophic. Introducing claude haiku 4.5, 2025a. URL https://www.anthropic.com/news/claude-haiku-4-5. Antrophic. Introducing claude sonnet 4.5, 2025b. URL https://www.anthropic.com/news/claude-sonnet-4-5. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023. Claude. Increase output consistency (json mode), 2025. URL https://docs.claude.com/en/docs/test-and-evaluate/ strengthen-guardrails/increase-consistency. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, et al. PaddleOCR-VL: Boosting multilingual document parsing via 0.9 ultra-compact vision-language model. arXiv preprint arXiv:2510.14528, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/pdf/2501.12948. Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha 13 Kumar. Rico: mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pages 845854, 2017. Ling Fu, Zhebin Kuang, Jiajun Song, Mingxin Huang, Biao Yang, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, et al. OCRBench V2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning. arXiv preprint arXiv:2501.00321, 2024. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024. Gemini. Structured output, 2025. URL https://ai.google.dev/gemini-api/docs/structured-output/. Saibo Geng, Hudson Cooper, Michał Moskal, Samuel Jenkins, Julian Berman, Nathan Ranchin, Robert West, Eric Horvitz, and Harsha Nori. JsonSchemaBench: rigorous benchmark of structured outputs for language models. arXiv preprint arXiv:2501.10868, 2025. Zhouhong Gu, Haoning Ye, Xingzhou Chen, Zeyang Zhou, Hongwei Feng, and Yanghua Xiao. StrucText-Eval: Evaluating large language models reasoning ability in structure-rich text. arXiv preprint arXiv:2406.10621, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o system card. arXiv preprint arXiv:2410.21276, 2024. Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexiang Hu, Fangyu Liu, Julian Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: Screenshot parsing as pretraining for visual language understanding. In International Conference on Machine Learning, pages 1889318912. PMLR, 2023. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. LLaVA-OneVision: Easy visual task transfer. Transactions on Machine Learning Research, 2024. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. Yanghao Li, Hanzi Mao, Ross B. Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. ArXiv, abs/2203.16527, 2022. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. URL https://llava-vl.github.io/blog/2024-01-30-llava-next/. Parker Liu, Chenxin Li, Zhengxin Li, Yipeng Wu, Wuyang Li, Zhiqin Yang, Zhenyuan Zhang, Yunlong Lin, Sirui Han, and Brandon Feng. IR3D-Bench: Evaluating vision-language model scene understanding as agentic inverse rendering. arXiv preprint arXiv:2506.23329, 2025. Shangbang Long, Siyang Qin, Dmitry Panteleev, Alessandro Bissacco, Yasuhisa Fujii, and Michalis Raptis. Towards In Proceedings of the IEEE/CVF Conference on end-to-end unified scene text detection and layout analysis. Computer Vision and Pattern Recognition, pages 10491059, 2022. Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Haoping Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, et al. Toolsandbox: stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 11601183, 2025. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Ahmed Masry, Mohammed Saidul Islam, Mahir Ahmed, Aayush Bajaj, Firoz Kabir, Aaryaman Kartha, Md Tahmid Rahman Laskar, Mizanur Rahman, Shadikur Rahman, Mehrad Shahmohammadi, et al. Chartqapro: more diverse and challenging benchmark for chart question answering. arXiv preprint arXiv:2504.05506, 2025. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. DocVQA: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. Minesh Mathew, Viraj Bagal, Rubèn Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Anton Belyi, et al. MM1: methods, analysis and insights from multimodal llm pre-training. In European Conference on Computer Vision, pages 304323. Springer, 2024. 14 Meta. Llama 4: Leading intelligence. unrivaled speed and efficiency, 2025. URL https://www.llama.com/. Mixtral. Mistral ocr, 2025. URL https://mistral.ai/news/mistral-ocr. Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, et al. MinerU2.5: decoupled vision-language model for efficient high-resolution document parsing. arXiv preprint arXiv:2509.22186, 2025. OpenAI. Introducing structured outputs in the api, 2024. URL https://openai.com/index/ introducing-structured-outputs-in-the-api/. OpenAI. Introducing gpt-5, 2025. URL https://openai.com/index/introducing-gpt-5/. JSON Schema org. Json schema, 2025. URL https://json-schema.org. Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, et al. OmniDocBench: Benchmarking diverse pdf document parsing with comprehensive annotations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2483824848, 2025. Shishir Patil, Huanzhi Mao, Fanjia Yan, Charlie Cheng-Jie Ji, Vishnu Suresh, Ion Stoica, and Joseph Gonzalez. The berkeley function calling leaderboard (bfcl): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. MIA-Bench: Towards better instruction following evaluation of multimodal llms. arXiv preprint arXiv:2407.01509, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. Josselin Roberts, Tony Lee, Chi Wong, Michihiro Yasunaga, Yifan Mai, and Percy Liang. Image2struct: Benchmarking structure extraction for vision-language models. Advances in Neural Information Processing Systems, 37:115058115097, 2024. Benny Tang, Angie Boggust, and Arvind Satyanarayan. Vistext: benchmark for semantically rich chart captioning. arXiv preprint arXiv:2307.05356, 2023a. Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, et al. Chartmuseum: Testing visual reasoning capabilities of large vision-language models. arXiv preprint arXiv:2505.13444, 2025. Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark Gerstein. StrucBbench: Are large language models really good at generating complex structured data? arXiv preprint arXiv:2309.08963, 2023b. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Louis Rouillard, Thomas Mesnard, et al. Gemma 3 technical report. arXiv e-prints, pages arXiv2503, 2025a. Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-VL technical report. arXiv preprint arXiv:2504.07491, 2025b. Manan Tomar, Lior Shani, Yonathan Efroni, and Mohammad Ghavamzadeh. Mirror descent policy optimization. arXiv preprint arXiv:2005.09814, 2020. Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. Haoming Wang, Haoyang Zou, Huatong Song, Jiazhan Feng, Junjie Fang, Junting Lu, Longxiang Liu, Qinyu Luo, Shihao Liang, Shijue Huang, et al. UI-Tars 2 technical report: Advancing gui agent with multi-turn reinforcement learning. arXiv preprint arXiv:2509.02544, 2025a. Jize Wang, Ma Zerun, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le. GTA: benchmark for general tool agents. Advances in Neural Information Processing Systems, 37:7574975790, 2024. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. InternVL3.5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025b. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 4, 2024. Jason Wu, Siyan Wang, Siman Shen, Yi-Hao Peng, Jeffrey Nichols, and Jeffrey Bigham. Webui: dataset for enhancing visual ui understanding with web semantics. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages 114, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen-3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Jialin Yang, Dongfu Jiang, Lipeng He, Sherman Siu, Yuxuan Zhang, Disen Liao, Zhuofeng Li, Huaye Zeng, Yiming Jia, Haozhe Wang, et al. StructEval: Benchmarking llms capabilities to generate structural outputs. arXiv preprint arXiv:2505.20139, 2025b. Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. Aria-ui: Visual grounding for gui instructions. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2241822433, 2025c. Zhen Yang, Zi-Yi Dou, Di Feng, Forrest Huang, Anh Nguyen, Keen You, Omar Attia, Yuhao Yang, Michael Feng, Haotian Zhang, et al. Ferret-UI Lite: Lessons from building small on-device gui agents. arXiv preprint arXiv:2509.26539, 2025d. Zhibo Yang, Jun Tang, Zhaohai Li, Pengfei Wang, Jianqiang Wan, Humen Zhong, Xuejing Liu, Mingkun Yang, Peng Wang, Shuai Bai, et al. CC-OCR: comprehensive and challenging ocr benchmark for evaluating large multimodal models in literacy. arXiv preprint arXiv:2412.02210, 2024. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. tau-bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. Shaofeng Yin, Ting Lei, and Yang Liu. Toolvqa: dataset for multi-step reasoning vqa with external tools. arXiv preprint arXiv:2508.03284, 2025. Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In European conference on computer vision, pages 6985. Springer, 2016. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Yang Yue, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in LLMs beyond the base model? In 2nd AI for Math Workshop @ ICML 2025, 2025. Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. MM1.5: Methods, analysis & insights from multimodal llm fine-tuning. In The Thirteenth International Conference on Learning Representations, 2025. LI Zhangheng, Keen You, Haotian Zhang, Di Feng, Harsh Agrawal, Xiujun Li, Mohana Prasad Sathya Moorthy, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-UI 2: Mastering universal user interface understanding across platforms. In The Thirteenth International Conference on Learning Representations, 2025. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023."
        },
        {
            "title": "A Dataset",
            "content": "The original Vistext (Tang et al., 2023a) eval dataset contains only single chart images that only have single data series, make the task less challenging. In our work, we first cluster charts into groups of size 2-6 using original chart titles bag-of-words as features and TF-IDF method to find similar charts. Then we render the chart group into one image treating each chart as sub-plot. In total 383 charts are converted into 99 images, which are used in our data generation pipeline."
        },
        {
            "title": "B Training Details",
            "content": "B.1 Training Dataset This section provides additional details on the training dataset used in our study (Section 4.3). For supervised fine-tuning, we construct training corpus sourced from the training sets of HierText (Long et al., 2022), AriaUI (Yang et al., 2025c), and COYO (Byeon et al., 2022). We balance both in-domain vs. out-of-domain data and real vs. synthetic JSON schemas. HierText serves as the in-domain dataset because its test split is used to build part of the SO-Bench evaluation set. COYO and AriaUI, in contrast, provide out-of-domain training data. For the full HierText training set (10K images), we generate one version with synthetic schemas and another with real JSON schemas. For AriaUI, we downsample to 10K images, drawn from desktop, mobile, and web UI screens. Similar to HierText split, we generate one example with synthetic schema and another with real JSON schema for every image. Since COYO is extremely large (over 747M imagetext pairs), we first categorize and then randomly downsample it to 95K images, consisting of: 20K infographics, 10K handwritten images, 40K scene-text images, 5K maps, and 20K tables. Among these, 80% of the images are paired with real JSON schemas, while 20% use synthetic schemas. Finally, we filter out noisy samples with low label quality using the critic model, ending up with 114K training samples. B.2 Training hyperparameters In all our of SFT experiments, we train the model for around 3 epochs and we adjust the batch size (among 64, 128, 256) and training steps accordingly based on the training data size. We used learning rate of 2e-5 and max sequence length of 16k. For RL experiments, we used learning rate of 3e-7, batch size of 256. For each sample, we generate 32 rollouts and compute the advantage using the REINFORCE Leave One Out (RLOO) (Ahmadian et al., 2024). Again we train the model for around 3 epochs."
        },
        {
            "title": "C Error Examples",
            "content": "Figure 11 shows an example output from Gemini-2.5-pro. We see that the model is already very good at interpreting chart data and its predicted data values are very close to the ground truth, which are considered to be correct with fuzzy match. However, there is no explicit unit annotation on the chart, so the model has to infer the unit based on the text. Although the predicted answer also make sense, the edit distance w.r.t the ground truth already exceeds the threshold, thus it could not score full match on this example. One potential to improve our evaluation metrics is to introduce VLM-as-a-judge for fields with fuzzy match. As we discussed in subsection 4.3, due to overall shallower schema depth in AriaUI subset of training data, the model trained on it could not learn to generalize well to deep schemas in chart domain. Figure 12 shows such an error example from the model. In this case, the extracted values are correct, but the model just hallucinated shallow structured that is not compliant with the input schema, leading to 0 scores on all metrics. In Figure 13, Figure 14, Figure 15, we show 3 more error examples made by the SFT model trained on the entire training set. In Figure 13, we see that the model is partially correct on the generated outputs, but it mistakenly treated the title of the schema as one of the target fields, and produced an extra field which leads to 0 score on schema validation. This suggests that the models instruction following ability could be further improved. In Figure 14, we see that the models output is fully compliant with the schema, but all values are off compared to the ground truth. We see that the text in this image is very dense, and even for humans we 1 Figure 9 The results breakdown by the data categories for the model trained with 20K randomly sampled data and model trained with Aria-UI subset of the data. In the left figure, we show the schema validation accuracy and in the right figure, we show the field match accuracy (fuzzy). Figure 10 Metrics correlation analysis. have to zoom in on the first section of the page to see the details. This suggest that the visual perception ability of high-resolution images is still relatively weak. Finally, in Figure 15, we see that the model is correct in terms of schema validation and also get partial score on field match. However, it could not capture all available discounts on the page, because the information is actually scatter at different places. This hints that the general visual understanding ability that require reasoning over entire image is also direction for improvement."
        },
        {
            "title": "D Ablation Studies",
            "content": "Figure 10 studies the metrics correlations between SO-Bench and other benchmarks from multiple opensourced and proprietary models presented in Table 1. Their Pearson correlation coefficients are summarized in Table 2."
        },
        {
            "title": "E Instruction Following Prompt",
            "content": "Table presents the instruction-following prompt used for structured output generation. Notably, the prompt includes explicit guidance on handling default values, particularly for fields labeled as required but not directly parsable from the input images. The same rules are consistently applied throughout both the data 2 Figure 11 An example error (Chart image) from Gemini-2.5-Pro predictions. The box with Robot icon contains model predictions and the box with start icon contains ground truth. The blue font indicate that our fuzzy match metric is able to count those fields as correct, and the red font indicate mismatched field. Figure 12 An example error (Chart image) from the model trained on AriaUI subset of data. Although the extracted values themselves are correct, the output does not follow the schema structure at all, leading to 0 scores on all metrics in this case. 3 Figure 13 An example error (Natural image) from the SFT model trained on full data. This example gets partial score on field match but is invalid w.r.t. the schema Figure 14 An example error (Document) from the SFT model trained on full data. This example gets 0 score on field match while being valid w.r.t the schema generation pipeline and the inference process, ensuring alignment between training and evaluation settings. 4 Figure 15 An example error (UI image) from the SFT model trained on full data. This example gets partial score on field match while also being valid w.r.t the schema"
        },
        {
            "title": "Instruction Following Prompt for Structured Output",
            "content": "SYSTEM PROMPT Visual Structured Information Extractor You are an expert at extracting structured information from images according to provided schemas. Given the image and user intent, extract information that matches the provided JSON schema. Constraint-Aware Default Rules (for missing/unextractable required fields) General precedence P0 Use explicit schema defaults: If provided in schema, prefer these. P1 Respect enumerations/const: Use the declared const or the first enum value (in stable order). P2 Apply type-specific rules while respecting schema constraints. Strings Default placeholder: #. Respect minLength/maxLength: If minLength = 1, return exactly hash characters. If minLength = maxLength = n, return hash characters. If pattern forbids #, construct the shortest valid alternative using allowed characters (\"A\", \"0\", etc.). Only output null if \"null\" is explicitly allowed. Integers & Numbers Default to 0. If minimum/exclusiveMinimum is present, choose the lowest valid value. If maximum/exclusiveMaximum is present and 0 violates it, choose the highest valid value under the bound. If multipleOf is present, adjust to the nearest valid multiple. Booleans Default to false unless otherwise constrained. Dates / Times Use RFC 3339-valid placeholders: \"1970-01-01\" for date \"1970-01-01T00:00:00Z\" for date-time \"00:00:00Z\" for time If stricter patterns exist, choose the simplest valid match. Arrays If minItems = k, return exactly items, recursively filled with these same rules. Ensure uniqueness if uniqueItems:true. Objects Populate all required fields recursively. Never add undeclared fields. Edge-Case Notes Never hallucinate values. Required fields must be filled with constraint-compliant placeholders. Optional fields may be omitted or set to null if allowed. If constraints are contradictory, return the closest minimally violating placeholder. Stable tie-breaking: when multiple placeholder choices are possible, use the first in schema order. 6 Output Format Return only valid JSON, wrapped in: json { ... } All required fields must appear. Placeholders must respect schema constraints. No comments, no extra text, no extra fields. USER PROMPT JSON Schema: json {schema_json} User Intent: {user_intent} Please analyze the image and extract structured information according to the schema above, honoring the user intent. Return only valid JSON in fenced json code block. Algorithm 1 Field Matching with Fuzzy Evaluation Require: Prediction pred, ground truth gt, labels Ensure: Metrics = {mtotal, mstring, mnumber, mlist, mdict} 1: procedure Match(pred, gt, ℓ) if ℓ = ignore then 2: 3: 4: mtotal mtotal + 1 if both are strings then 5: 6: 7: 8: 9: match (cid:40)EditSim(pred, gt) > τstr Norm(pred) = Norm(gt) if ℓ = fuzzy otherwise if match then mstring mstring + 1 end if else if both are numbers then (cid:40) match pred gt/ max(gt, ϵ) < τnum if ℓ = fuzzy pred = gt otherwise end if mlist mlist + 1 else if both are dicts then else if both are lists of length then if [1, n] : Match(pred[i], gt[i], ℓ[i]) then if match then mnumber mnumber + 1 end if 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: end if 22: 23: end procedure 24: 25: Parameters: τstr = 0.8 (string similarity), τnum = 0.05 (relative error), ϵ = 10 {k gt : ℓ[k] = ignore} if > 0 and : pred and Match(pred[k], gt[k], ℓ[k]) then mdict mdict + 1 end if end if Meaningful keys"
        }
    ],
    "affiliations": [
        "Apple"
    ]
}