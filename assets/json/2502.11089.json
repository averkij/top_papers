{
    "paper_title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
    "authors": [
        "Jingyang Yuan",
        "Huazuo Gao",
        "Damai Dai",
        "Junyu Luo",
        "Liang Zhao",
        "Zhengyan Zhang",
        "Zhenda Xie",
        "Y. X. Wei",
        "Lean Wang",
        "Zhiping Xiao",
        "Yuqing Wang",
        "Chong Ruan",
        "Ming Zhang",
        "Wenfeng Liang",
        "Wangding Zeng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers a promising direction for improving efficiency while maintaining model capabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 9 8 0 1 1 . 2 0 5 2 : r Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention Jingyang Yuan1,2, Huazuo Gao1, Damai Dai1, Junyu Luo2, Liang Zhao1, Zhengyan Zhang1, Zhenda Xie1, Y. X. Wei1, Lean Wang1, Zhiping Xiao3, Yuqing Wang1, Chong Ruan1, Ming Zhang2, Wenfeng Liang1, Wangding Zeng 1DeepSeek-AI 2Key Laboratory for Multimedia Information Processing, Peking University, PKU-Anker LLM Lab 3University of Washington {yuanjy, mzhang_cs}@pku.edu.cn, {zengwangding, wenfeng.liang}@deepseek.com"
        },
        {
            "title": "Abstract",
            "content": "Long-context modeling is crucial for next-generation language models, yet the high computational cost of standard attention mechanisms poses significant computational challenges. Sparse attention offers promising direction for improving efficiency while maintaining model capabilities. We present NSA, Natively trainable Sparse Attention mechanism that integrates algorithmic innovations with hardware-aligned optimizations to achieve efficient long-context modeling. NSA employs dynamic hierarchical sparse strategy, combining coarse-grained token compression with fine-grained token selection to preserve both global context awareness and local precision. Our approach advances sparse attention design with two key innovations: (1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design, with implementation optimizations for modern hardware. (2) We enable end-to-end training, reducing pretraining computation without sacrificing model performance. As shown in Figure 1, experiments show the model pretrained with NSA maintains or exceeds Full Attention models across general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile, NSA achieves substantial speedups over Full Attention on 64k-length sequences across decoding, forward propagation, and backward propagation, validating its efficiency throughout the model lifecycle. 1. Introduction The research community increasingly recognizes long-context modeling as crucial capability for next-generation large language models, driven by diverse real-world applications ranging from in-depth reasoning (DeepSeek-AI, 2025; Zelikman et al., 2022), repository-level code generation (Zhang et al., 2023a; Zhang et al.) and multi-turn autonomous agent systems (Park et al., 2023). Recent breakthroughs, including OpenAIs o-series models, DeepSeek-R1 (DeepSeek-AI, 2025), and Gemini 1.5 Pro (Google et al., 2024), enabling models to process entire codebases, lengthy documents, maintain coherent multi-turn conversations over thousands of tokens, and perform complex reasoning across long-range dependencies. However, the high complexity (Zaheer et al., 2020) of vanilla Attention (Vaswani et al., 2017) mechanisms emerges as critical latency bottleneck as sequence length increases. Theoretical estimates indicate that attention *Contribution during internship at DeepSeek-AI. Figure 1 Comparison of performance and efficiency between Full Attention model and our NSA. Left: Despite being sparse, NSA surpasses Full Attention baseline on average across general benchmarks, long-context tasks, and reasoning evaluation. Right: For 64k-length sequence processing, NSA achieves substantial computational speedup compared to Full Attention in all stages: decoding, forward propagation, and backward propagation. computation with softmax architectures accounts for 7080% of total latency when decoding 64k-length contexts, underscoring the urgent need for more efficient attention mechanisms. natural approach to efficient long-context modeling is to take advantage of the inherent sparsity of softmax attention (Ge et al., 2023; Jiang et al., 2023), where selectively computing critical query-key pairs can significantly reduce computational overhead while preserving performance. Recent advances demonstrate this potential through diverse strategies: KV-cache eviction methods (Li et al., 2024; Zhang et al., 2023b; Zhou et al., 2024), blockwise KV-cache selection methods (Tang et al., 2024; Xiao et al., 2024), and sampling, clustering or hashing-based selection methods (Chen et al., 2024; Desai et al., 2024; Liu et al., 2024). Despite these promising strategies, existing sparse attention methods often fall short in practical deployments. Many approaches fail to achieve speedups comparable to their theoretical gains; also, most methods mainly focus on inference stage, lacking effective training-time support to fully exploit the sparsity patterns of attention. To address these limitations, the deployment of effective sparse attention must tackle two key challenges: (1) Hardware-aligned inference speedup: Converting theoretical computation reductions into actual speed improvements requires hardware-friendly algorithm design during both prefilling and decoding stages to mitigate memory access and hardware scheduling bottlenecks; (2) Training-aware algorithm design: Enabling end-to-end computation with trainable operators to reduce training costs while maintaining model performance. These requirements are crucial for real-world applications to achieve fast long-context inference or training. When considering both aspects, existing methods still exhibit noticeable gap. To achieve more effective and efficient sparse attention, we present NSA, Natively trainable Sparse Attention architecture that integrates hierarchical token modeling. As shown in Figure 2, NSA reduces per-query computation by organizing keys and values into temporal blocks and processing them through three attention paths: compressed coarse-grained tokens, selectively retained fine-grained tokens, and sliding windows for local contextual information. Then we implement specialized kernels to maximize its practical efficiency. NSA introduces two 2 Figure 2 Overview of NSAs architecture. Left: The framework processes input sequences through three parallel attention branches: For given query, preceding keys and values are processed into compressed attention for coarse-grained patterns, selected attention for important token blocks, and sliding attention for local context. Right: Visualization of different attention patterns produced by each branch. Green areas indicate regions where attention scores need to be computed, while white areas represent regions that can be skipped. core innovations corresponding to the key requirements above: (1) Hardware-aligned system: Optimize blockwise sparse attention for Tensor Core utilization and memory access, ensuring balanced arithmetic intensity. (2) Training-aware design: Enable stable end-to-end training through efficient algorithms and backward operators. This optimization enables NSA to support both efficient deployment and end-to-end training. We evaluate NSA through comprehensive experiments on real-world language corpora. Pretraining on 27B-parameter transformer backbone with 260B tokens, we assess NSAs performance across general language evaluations, long-context evaluations, and chain-of-thought reasoning evaluation. We further compare the kernel speed on A100 GPUs with optimized Triton (Tillet et al., 2019) implementations. Experimental results demonstrate that NSA achieves comparable or superior performance to full attention baseline, while outperforming existing sparse attention approaches. Additionally, NSA delivers substantial speedups across decoding, forward, and backward stages compared to Full Attention, with the speedup ratio increasing for longer sequences. These results validate that our hierarchical sparse attention design effectively balances model capability and computational efficiency. 2. Rethinking Sparse Attention Methods Modern sparse attention methods have made significant strides in reducing the theoretical computational complexity of transformer models. However, most approaches predominantly apply sparsity during inference while retaining pretrained Full Attention backbone, potentially introducing architectural bias that limits their ability to fully exploit sparse attentions advantages. Before introducing our native sparse architecture, we systematically analyze these limitations through two critical lenses. 2.1. The Illusion of Efficient Inference Despite achieving sparsity in attention computation, many methods fail to achieve corresponding reductions in inference latency, primarily due to two challenges: Phase-Restricted Sparsity. Methods such as H2O (Zhang et al., 2023b) apply sparsity 3 during autoregressive decoding while requiring computationally intensive pre-processing (e.g. attention map calculation, index building) during prefilling. In contrast, approaches like MInference (Jiang et al., 2024) focus solely on prefilling sparsity. These methods fail to achieve acceleration across all inference stages, as at least one phase remains computational costs comparable to Full Attention. The phase specialization reduces the speedup ability of these methods in prefilling-dominated workloads like book summarization and code completion, or decoding-dominated workloads like long chain-of-thought (Wei et al., 2022) reasoning. Incompatibility with Advanced Attention Architecture. Some sparse attention methods fail to adapt to modern decoding efficient architectures like Mulitiple-Query Attention (MQA) (Shazeer, 2019) and Grouped-Query Attention (GQA) (Ainslie et al., 2023), which significantly reduced the memory access bottleneck during decoding by sharing KV across multiple query heads. For instance, in approaches like Quest (Tang et al., 2024), each attention head independently selects its KV-cache subset. Although it demonstrates consistent computation sparsity and memory access sparsity in Multi-Head Attention (MHA) models, it presents different scenario in models based on architectures like GQA, where the memory access volume of KV-cache corresponds to the union of selections from all query heads within the same GQA group. This architectural characteristic means that while these methods can reduce computation operations, the required KV-cache memory access remains relatively high. This limitation forces critical choice: while some sparse attention methods reduce computation, their scattered memory access pattern conflicts with efficient memory access design from advanced architectures. These limitations arise because many existing sparse attention methods focus on KV-cache reduction or theoretical computation reduction, but struggle to achieve significant latency reduction in advanced frameworks or backends. This motivates us to develop algorithms that combine both advanced architectural and hardware-efficient implementation to fully leverage sparsity for improving model efficiency. 2.2. The Myth of Trainable Sparsity Our pursuit of native trainable sparse attention is motivated by two key insights from analyzing inference-only approaches: (1) Performance Degradation: Applying sparsity post-hoc forces models to deviate from their pretrained optimization trajectory. As demonstrated by Chen et al. (2024), top 20% attention can only cover 70% of the total attention scores, rendering structures like retrieval heads in pretrained models vulnerable to pruning during inference. (2) Training Efficiency Demands: Efficient handling of long-sequence training is crucial for modern LLM development. This includes both pretraining on longer documents to enhance model capacity, and subsequent adaptation phases such as long-context fine-tuning and reinforcement learning. However, existing sparse attention methods primarily target inference, leaving the computational challenges in training largely unaddressed. This limitation hinders the development of more capable long-context models through efficient training. Additionally, efforts to adapt existing sparse attention for training also expose challenges: Non-Trainable Components. Discrete operations in methods like ClusterKV (Liu et al., 2024) (includes k-means clustering) and MagicPIG (Chen et al., 2024) (includes SimHash-based selecting) create discontinuities in the computational graph. These non-trainable components prevent gradient flow through the token selection process, limiting the models ability to learn optimal sparse patterns. Inefficient Back-propagation. Some theoretically trainable sparse attention methods suffer from practical training inefficiencies. Token-granular selection strategy used in approaches 4 like HashAttention (Desai et al., 2024) leads to the need to load large number of individual tokens from the KV cache during attention computation. This non-contiguous memory access prevents efficient adaptation of fast attention techniques like FlashAttention, which rely on contiguous memory access and blockwise computation to achieve high throughput. As result, implementations are forced to fall back to low hardware utilization, significantly degrading training efficiency. 2.3. Native Sparsity as an Imperative These limitations in inference efficiency and training viability motivate our fundamental redesign of sparse attention mechanisms. We propose NSA, natively sparse attention framework that addresses both computational efficiency and training requirements. In the following sections, we detail the algorithmic design and operator implementation of NSA. 3. Methodology Our technical approach spans algorithm design and kernel optimization. In the following subsections, we first introduce the background of our methodology. Then we present the overall framework of NSA, followed by its key algorithmic components. Finally, we detail our hardware-optimized kernel design that maximizes practical efficiency. 3.1. Background Attention Mechanism is widely used in language modeling where each query token q𝑡 computes relevance scores against all preceding keys k:𝑡 to generate weighted sum of values v:𝑡. Formally, for an input sequence of length 𝑡, the attention operation is defined as: where Attn denotes the attention function: o𝑡 = Attn (cid:0)q𝑡, k:𝑡, v:𝑡(cid:1) Attn (cid:0)q𝑡, k:𝑡, v:𝑡(cid:1) = 𝑡 𝑖=1 𝛼𝑡,𝑖v𝑖 (cid:205)𝑡 𝑗= 𝛼𝑡, 𝑗 , 𝛼𝑡,𝑖 = 𝑒 𝑡 k𝑖 𝑑𝑘 . (1) (2) Here, 𝛼𝑡,𝑖 represents the attention weight between q𝑡 and k𝑖, and 𝑑𝑘 is the feature dimension of keys. As sequence length increases, attention computation becomes increasingly dominant in the overall computational cost, presenting significant challenges for long-context processing. Arithmetic Intensity is the ratio of compute operations to memory accesses. It intrinsically shapes algorithm optimization on hardware. Each GPU has critical arithmetic intensity determined by its peak compute capability and memory bandwidth, calculated as the ratio of these two hardware limits. For computation tasks, arithmetic intensity above this critical threshold becomes compute-bound (limited by GPU FLOPS), while below it becomes memorybound (limited by memory bandwidth). Specifically for causal self-attention mechanism, during training and prefilling phases, batched matrix multiplications and attention computations exhibit high arithmetic intensity, making these stages compute-bound on modern accelerators. In contrast, auto-regressive decoding becomes memory-bandwidth constrained because it generates one token per forward pass while requiring loading the entire key-value cache, resulting in low arithmetic intensity. This leads to different optimization goals reducing computation cost during training and prefilling, while reducing memory access during decoding. 5 3.2. Overall Framework To leverage the potential of attention with natural sparse pattern, we propose replacing the original key-value pairs k:𝑡, v:𝑡 in Equation (1) with more compact and information-dense set of representation key-value pairs 𝐾𝑡, 𝑉𝑡 given each query q𝑡. Specifically, we formally define the optimized attention output as follows: 𝐾𝑡 = 𝑓𝐾 (q𝑡, k:𝑡, v:𝑡), 𝑉𝑡 = 𝑓𝑉 (q𝑡, k:𝑡, v:𝑡) 𝑡 = Attn (cid:0)q𝑡, 𝐾𝑡, 𝑉𝑡(cid:1) (3) (4) where 𝐾𝑡, 𝑉𝑡 are dynamically constructed based on the current query q𝑡 and the contextual 𝑡 , 𝑉 𝑐 𝑡 , memory k:𝑡, v:𝑡. We can design various mapping strategies to get different categories of 𝐾𝑐 and combine them as follows: 𝑡 = 𝑐 𝑔𝑐 𝑡 Attn(q𝑡, 𝐾𝑐 𝑡 , 𝑉 𝑐 𝑡 ). (5) As illustrated in Figure 2, NSA have three mapping strategies = {cmp, slc, win}, representing compression, selection, and sliding window for keys and values. 𝑔𝑐 𝑡 [0, 1] is the gate score for corresponding strategy 𝑐, derived from input features via an MLP and sigmoid activation. Let 𝑁𝑡 denote the total number of remapped keys/values: 𝑁𝑡 = 𝑐 size[ 𝐾𝑐 𝑡 ]. (6) We maintain high sparsity ratio by ensuring𝑁𝑡 𝑡. 3.3. Algorithm Design In this subsection, we introduce the design of our remapping strategies 𝑓𝐾 and 𝑓𝑉: token compression, token selection, and sliding window. 3.3.1. Token Compression By aggregating sequential blocks of keys or values into block-level representations, we obtain compressed keys and values that capture the information of the entire block. Formally, the compressed key representation is defined as: 𝐾cmp 𝑡 = 𝑓 cmp 𝐾 (k:𝑡) = (cid:26) 𝜑(k𝑖𝑑+1:𝑖𝑑+𝑙) (cid:12) (cid:12) (cid:12) (cid:12) 1 𝑖 (cid:23) (cid:27) (cid:22) 𝑡 𝑙 𝑑 (7) R𝑑𝑘 𝑡𝑙 where 𝑙 is the block length, 𝑑 is the sliding stride between adjacent blocks, and 𝜑 is learnable MLP with intra-block position encoding to map keys in block to single compressed key. 𝑑 is tensor composed by compresion keys. Usually, we adopt 𝑑 < 𝑙 to mitigate 𝐾cmp 𝑡 information fragmentation. An analogous formulation holds for the compressed value representation 𝑉 cmp . Compressed representations capture coarser-grained higher-level semantic information and reduce computational burden of attention. 𝑡 6 3.3.2. Token Selection Using only compressed keys, values might lose important fine-grained information, motivating us to selectively preserve individual keys, values. Below we describe our efficient token selection mechanism that identifies and preserves the most relevant tokens with low computational overhead. Blockwise Selection. Our selection strategy processes key and value sequences in spacial continuous blocks, motivated by two key factors: hardware efficiency considerations and inherent distribution patterns of attention scores. Blockwise selection is crucial to achieve efficient computation on modern GPUs. That is because modern GPU architectures exhibit significantly higher throughput for continuous block accesses compared to random index-based reads. Also, blockwise computation enables optimal utilization of Tensor Cores. This architectural characteristic has established blockwise memory access and computation as fundamental principle in high-performance attention implementations, as exemplified by FlashAttentions block-based design. Blockwise selection follows the inherent distribution patterns of attention scores. Prior works (Jiang et al., 2024) have shown that attention scores often exhibit spatial continuity, suggesting that neighboring keys tend to share similar importance levels. Our visualization in Section 6.2 also shows this spatial continuous pattern. To implement blockwise selection, we first divide key, value sequences into selection blocks. To identify the most important blocks for attention computation, we need to assign importance scores to each block. Below we present our method for computing these block-level importance scores. Importance Score Computation. Computing block importance scores could introduce significant overhead. Fortunately, the attention computation of compression tokens produces intermediate attention scores that we can leverage to induce selection block importance scores, formulated as: pcmp 𝑡 = Softmax (cid:16) 𝑡 𝐾cmp 𝑇 𝑡 (cid:17) , (8) 𝑡 𝑡𝑙 𝑑 is the attention scores between 𝑞𝑡 and compression keys 𝐾cmp where pcmp . Let 𝑙 denote the selection block size. When compression blocks and selection blocks share the same blocking scheme, i.e., 𝑙 = 𝑙 = 𝑑, we can directly obtain the selection block importance scores pslc 𝑡 by pslc straightforwardly. For cases where the blocking schemes differ, we derive the importance scores for selection blocks according to their spatial relationship. Given 𝑑 𝑙 and 𝑑 𝑙, we have: 𝑡 = pcmp 𝑡 𝑡 pslc 𝑡 [ 𝑗] = 𝑙 𝑑 1 𝑙 𝑑 1 𝑚=0 𝑛= pcmp 𝑡 (cid:20) 𝑙 𝑑 𝑗 + 𝑚 + 𝑛 (cid:21) , (9) where[] denotes the indexing operator for accessing vector element. For models employing GQA or MQA where key-value caches are shared across query heads, consistent block selection across these heads has to be ensured to minimize KV cache loading during decoding. The shared importance scores across heads in group are formally defined as: 𝐻 pslc 𝑡 = pslc,(ℎ) 𝑡 , (10) ℎ=1 where (ℎ) in the superscript denotes the head index, and 𝐻 is the number of query heads in each group. This aggregation ensures consistent block selection across heads within the same group. Top-𝑛𝑛𝑛 Block Selection. After obtaining the selection block importance scores, We retain tokens within the top-𝑛 sparse blocks ranked by block importance scores, formulated as: 𝑡 [𝑖]) 𝑛} I𝑡 = {𝑖 rank(pslc 𝑡 = Cat (cid:2){k𝑖𝑙+1:(𝑖+1) 𝑙 𝑖 I𝑡}(cid:3) , 𝐾slc where rank() denotes the ranking position in descending order, with rank = 1 corresponding to the highest score, I𝑡 is the set of selected blocks indices, Cat denotes the concatenation 𝑡 R𝑑𝑘 𝑛𝑙 operation. 𝐾slc is tensor composed by compresion keys. An analogous formulation applies to the fine-grained value 𝑉 slc . The selected keys and values then participate in the attention computation with q𝑡 as defined in Equation (5). (12) (11) 𝑡 3.3.3. Sliding Window In attention mechanisms, local patterns typically adapt faster and can dominate the learning process, potentially preventing the model from effectively learning from compression and selection tokens. To address this issue, we introduce dedicated sliding window branch that explicitly handles local context, allowing other branches (compression and selection) to focus on learning their respective features without being shortcutted by local patterns. Specifically, = v𝑡𝑤:𝑡 in window 𝑤, and isolate attention we maintain recent tokens 𝐾win computations of different information sources (compression tokens, and selected tokens, sliding window) into separate branches. These branch outputs are then aggregated through learned gating mechanism. To further prevent shortcut learning across attention branches with marginal computational overhead, we provide independent keys and values for three branches. This architectural design enables stable learning by preventing gradient interference between local and long-range pattern recognition, while introducing minimal overhead. = k𝑡𝑤:𝑡, 𝑉 win 𝑡 𝑡 After obtaining all three categories of keys and values ( 𝐾cmp ), we compute the final attention output following Equation (5). Together with the compression, selection, and sliding window mechanisms described above, this forms the complete algorithmic framework of NSA. ; and 𝐾win , 𝑉 cmp , 𝑉 win 𝑡 ; 𝐾slc 𝑡 , 𝑉 slc 𝑡 𝑡 𝑡 𝑡 3.4. Kernel Design To achieve FlashAttention-level speedup during the training and prefilling, we implement hardware-aligned sparse attention kernels upon Triton. Given MHA is memory-intensive and inefficient for decoding, we focus on architectures with shared KV caches like GQA and MQA following the current state-of-the-art LLMs. While compression and sliding window attention computations are readily compatible with existing FlashAttention-2 kernels, we introduce the specialized kernel design for sparse selection attention. If we were to follow FlashAttentions strategy of loading temporally continuous query blocks into SRAM, it would result in inefficient memory access since queries within block may require disjoint KV blocks. To address this, our key optimization lies in different query grouping strategy: for each position on the query sequence, we load all query heads within GQA group (they share the same sparse KV blocks) into SRAM. Figure 3 illustrates our forward pass implementation. The proposed kernel architecture is characterized by the following key features: 1. Group-Centric Data Loading. For each inner loop, load all heads queries 𝑄 R[ℎ,𝑑𝑘 ] in the group at position 𝑡 and their shared sparse key/value block indices I𝑡. 8 2. Shared KV Fetching. In the inner loop, Sequentially load continuous key/value blocks indexed by I𝑡 into SRAM as 𝐾 R[ 𝐵𝑘,𝑑𝑘 ], 𝑉 R[ 𝐵𝑘,𝑑𝑣 ] to minimize memory loading, where 𝐵𝑘 is the kernel block size satisfying 𝐵𝑘𝑙. 3. Outer Loop on Grid. Since the inner-loop length (proportional to the selected block count 𝑛) remains nearly identical for different query blocks, we put query/output loops in Tritons grid scheduler to simplify and optimize the kernel. This design achieves near-optimal arithmetic intensity by (1) eliminating redundant KV transfers through group-wise sharing, and (2) balancing compute workloads across GPU streaming multiprocessors. Figure 3 Kernel design for NSA. The kernel loads queries by GQA groups (Grid Loop), fetches corresponding sparse KV blocks (Inner Loop), and performs attention computation on SRAM. Green blocks indicate data on SRAM, while blue indicates data on HBM. 4. Experiments We evaluate NSA through three lenses: (1) general benchmarks performance, (2) long-context benchmarks performance, and (3) chain-of-thought reasoning performance, comparing against Full Attention baseline and state-of-the-art sparse attention methods. We defer the efficiency analysis of our sparse computation paradigm to Section 5, where we provide detailed discussions on training and inference speed. 4.1. Pretraining Setup Following the common practice in state-of-the-art LLMs, our experiments adopt backbone combining Grouped-Query Attention (GQA) and Mixture-of-Experts (MoE), featuring 27B total parameters with 3B active parameters. The model consists of 30 layers with hidden dimension of 2560. For GQA, we set the number of groups to 4, with total of 64 attention heads. For each head, the hidden dimensions of the query, key, and value are configured as 𝑑𝑞 = 𝑑𝑘 = 192 and 𝑑𝑣 = 128, respectively. For MoE, we utilize the DeepSeekMoE (Dai et al., 2024; DeepSeek-AI, 2024) structure, with 72 routed experts and 2 shared experts, and set the top-k experts to 6. To ensure training stability, the MoE in the first layer is replaced by an MLP in the form of SwiGLU. 9 Figure 4 Pretraining loss comparison between Full Attention and our NSA on 27B-parameter model. Both models exhibit stable convergence, with NSA achieving lower loss values. Model MMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEval Pass@1 0-shot Acc. 5-shot Acc. 5-shot Acc. 3-shot Acc. 8-shot Acc. 4-shot F1 1-shot Pass@1 3-shot Acc. 5-shot Full Attn NSA 0.567 0.565 0.279 0.286 0.576 0.587 0.497 0. 0.486 0.520 0.263 0.264 0.503 0.545 0.482 0.466 0.335 0.348 Avg. 0.443 0.456 Table 1 Pretraining performance comparison between the full attention baseline and NSA on general benchmarks, across knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH, GSM8K, MATH, DROP), and coding (MBPP, HumanEval) tasks. NSA achieves superior average performance on most benchmarks despite high sparsity. The proposed architecture achieves an effective trade-off between computation cost and model performance. For NSA, we set compression block size 𝑙 = 32, sliding stride 𝑑 = 16, selected block size 𝑙 = 64, selected block count 𝑛 = 16 (including fixed activating the 1 initial block and 2 local blocks), and sliding window size 𝑤 = 512. Both Full Attention and sparse attention models are pretrained on 270B tokens of 8k-length texts, followed by continued training and supervised fine-tuning on 32k-length texts with YaRN (Peng et al., 2024) to achieve long-context adaptation. Both models are trained to full convergence to ensure fair comparison. As shown in Figure 4, the pretraining loss curve of our NSA and Full Attention baseline demonstrates stable and smooth decline, with NSA consistently outperforming the Full Attention model. 4.2. Baselines Methods In addition to comparing with Full Attention, we evaluate several state-of-the-art inference-stage sparse attention methods: H2O (Zhang et al., 2023b), infLLM (Xiao et al., 2024), Quest (Tang et al., 2024), and Exact-Top, which first computes full attention score and select the top-𝑛 scores keys corresponding to each query and then calculates attention on these positions. These methods Model H2O InfLLM Quest Exact-Top Full Attn NSA SQA MQA Synthetic Code Avg. MFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC 0.428 0.474 0.495 0.502 0.512 0.503 0.429 0.517 0.561 0.605 0. 0.624 0.308 0.356 0.365 0.397 0.409 0.112 0.101 0.306 0.250 0.295 0.245 0.321 0.288 0.350 0.305 0.231 0.277 0.293 0.316 0.324 0.208 0.257 0.257 0.291 0.294 0. 0.437 0.356 0.307 0.341 0.704 0.766 0.792 0.810 0.830 0.905 0.421 0.486 0.478 0.548 0. 0.550 0.092 0.303 0.143 0.383 0.135 0.392 0.156 0.423 0.163 0.437 0.232 0.469 Table 2 Performance comparison between our NSA and baselines on LongBench, including subsets in single document QA, multi-document QA, synthetic and code task categories. NSA outperformed most of the baselines including Full Attention. span diverse sparse attention paradigms, including KV-cache eviction, query-aware selection, and exact top-𝑛 sparse selection. For general evaluation, where most samples have lengths within the local context window of sparse attention baselines, these methods are effectively equivalent to Full Attention. Therefore, we present only the comparison results between NSA and Full Attention baseline in this setting. In the long-context evaluation, we conduct comparisons across all baseline methods, with the sparsity of all sparse attention methods set to the same to ensure fair comparison. For chainof-thought reasoning evaluation, which requires long-text supervised fine-tuning, we limit our comparison to Full Attention, as sparse attention baselines do not support training. 4.3. Performance Comparison General Evaluation. We evaluated the pretrained NSA and Full Attention baseline, on comprehensive suite of benchmarks spanning knowledge, reasoning, and coding capabilities, including MMLU (Hendrycks et al., 2020), MMLU-PRO (Wang et al., 2024), CMMLU (Li et al., 2023), BBH (Suzgun et al., 2022), GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2020), DROP (Dua et al., 2019), MBPP (Austin et al., 2021), and HumanEval (Chen et al., 2021). The results are shown in Table 1. Despite its sparsity, NSA achieves superior overall performance, outperforming all baselines including Full Attention on 7 out of 9 metrics. This indicates that although NSA may not fully leverage its efficiency advantages on shorter sequences, it shows strong performance. Notably, NSA demonstrates significant gains in reasoning-related benchmarks (DROP: +0.042, GSM8K: +0.034), suggesting that our pretraining helps models to develop specialized attention mechanisms. This sparse attention pretraining mechanism forces model to focus on the most important information, potentially enhancing performance by filtering out noise from irrelevant attention pathways. The consistent performance across diverse evaluations also validates NSAs robustness as general-purpose architecture. Long-Context Evaluation. As shown in Figure 5, NSA achieves perfect retrieval accuracy across all positions in 64k-context needle-in-a-haystack (Kamradt, 2023) test. This performance stems from our hierarchical sparse attention design, which combines compression tokens for efficient global context scanning, and selection tokens for precise local information retrieval. The coarse-grained compression identifies relevant context blocks at low computational cost, while the token-level attention on selected tokens ensures the preservation of critical fine-grained information. This design enables NSA to maintain both global awareness and local precision. We also evaluate NSA on LongBench (Bai et al., 2023) against state-of-the-art sparse attention 11 Figure 5 Needle-in-a-Haystack retrieval accuracy across context positions with 64k context length. NSA achieves perfect accuracy through its hierarchical sparse attention design. methods and Full Attention baseline. To ensure consistent sparsity, we set the token activated by each query in all sparse attention baselines to 2560 tokens, which corresponds to the average number of tokens activated in NSA when handling 32k sequence lengths. Following StreamLLM (Xiao et al., 2023), this token budget includes the leading 128 tokens and 512 local tokens. We exclude certain subsets from LongBench due to their low scores across all models, which may not provide meaningful comparisons. As shown in Table 2, NSA achieves the highest average score 0.469, outperforming all baselines (+0.032 over Full Attention and +0.046 over Exact-Top). This improvement arises from two key innovations: (1) our native sparse attention design, which enables end-to-end optimization of sparse patterns during pretraining, facilitates synchronized adaptation between the sparse attention module and other model components; and (2) the hierarchical sparse attention mechanism achieves balance between local and global information processing. Notably, NSA demonstrates exceptional performance on tasks requiring complex reasoning over long contexts, achieving +0.087 and +0.051 improvements over Full Attention on multi-hop QA tasks (HPQ and 2Wiki), exceeding the performance of baselines on code understanding (LCC: +0.069), and outperforming other methods on passage retrieval (PassR-en: +0.075). These results validate NSAs capability to handle diverse long-context challenges, with its natively pretrained sparse attention providing additional benefits in learning task-optimal patterns. Chain-of-Thought Reasoning Evaluation. To evaluate NSAs compatibility with advanced downstream training paradigms, we investigate its capacity to acquire chain-of-thought mathematical reasoning abilities via post-training. Given the limited effectiveness of reinforcement learning on smaller-scale models, we employ knowledge distillation from DeepSeek-R1, conducting supervised fine-tuning (SFT) with 10B tokens of 32k-length mathematical reasoning traces. This produces two comparable models: Full Attention-R (Full Attention baseline) and NSA-R (our sparse variant). We assess both models on the challenging American Invitational Mathematics Examination (AIME 24) benchmark. We use sampling temperature of 0.7 and top-𝑝 value of 0.95 to generate 16 responses for each question and obtain the average score. To validate the impact of reasoning depth, we conduct experiments with two generation context limits: 8k and 16k tokens, measuring whether extended reasoning chains improve accuracy. Example comparisons of model predictions are provided in Appendix A."
        },
        {
            "title": "Generation Token Limit",
            "content": "Full Attention-R NSA-R 8192 0.046 0.121 16384 0.092 0.146 Table 3 AIME Instruction-based Evaluating after supervised fine-tuning. Our NSA-R demonstrates better performance than Full Attention-R at both 8k and 16k sequence lengths Figure 6 Comparison of Triton-based NSA kernel with Triton-based FlashAttention-2 kernel. Our implementation significantly reduces latency across all context lengths, with the improvement becoming more pronounced as input length increases. As shown in Table 3, NSA-R achieves significantly higher accuracy than Full Attention-R under the 8k context setting (+0.075), with this advantage persisting at 16k contexts (+0.054). These results validate two key benefits of native sparse attention: (1) The pretrained sparse attention patterns enable efficient capture of long-range logical dependencies critical for complex mathematical derivations; (2) Our architectures hardware-aligned design maintains sufficient context density to support growing reasoning depth without catastrophic forgetting. The consistent outperformance across context lengths confirms sparse attentions viability for advanced reasoning tasks when natively integrated into the training pipeline. 5. Efficiency Analysis We evaluate the computational efficiency of NSA against Full Attention on an 8-GPU A100 system. In efficiency analysis, we also configure the model with GQA group 𝑔 = 4, heads per group ℎ = 16, query/key dimension 𝑑𝑘 = 192, and value dimension 𝑑𝑣 = 128. Following the same settings in Section 4, we set NSA compression block size 𝑙 = 32, sliding stride 𝑑 = 16, selected block size 𝑙 = 64, selected block count 𝑛 = 16, and sliding window size 𝑤 = 512. 5.1. Training Speed We compare the Triton-based implementations of our NSA attention and Full Attention with Triton-based FlashAttention-2 to ensure fair speed comparison across the same backend. As shown in Figure 6, our NSA achieves progressively greater speedups as context length increases, up to 9.0 forward and 6.0 backward speedup at 64k context-length. Notably, the speed advantage becomes more pronounced with longer sequences. This speedup stems from our"
        },
        {
            "title": "Expected Speedup",
            "content": "8192 8192 2048 4 16384 32768 16384 2560 6.4 32768 3584 9.1 65536 5632 11.6 Table 4 Memory access volume (in equivalent number of tokens) per attention operation during decoding. Due to the low arithmetic intensity and memory-bound nature of decoding, the expected speedup is approximately linear with the volume of memory access. hardware-aligned algorithm design to maximize the efficiency of sparse attention architecture: (1) The Blockwise memory access pattern maximizes Tensor Core utilization through coalesced loads, (2) The delicate loop scheduling in the kernel eliminates redundant KV transfers. 5.2. Decoding Speed The decoding speed of Attention is primarily determined by the memory access bottleneck, which is closely tied to the amount of KV cache loading. In each decoding step, Our NSA just needs to load at most (cid:4) 𝑠𝑙 (cid:5) compression tokens, 𝑛𝑙 selected tokens, and 𝑤 neighbor tokens, where 𝑠 is the cached sequence length. As shown in Table 4, our method exhibits significant reduction in latency as the decoding length increases, achieving up to 11.6 speedup at 64k context-length. This advantage in memory access efficiency also amplifies with longer sequences. 𝑑 6. Discussion In this section, we reflect on the development process of NSA and discuss key insights gained from our exploration of different sparse attention strategies. While our approach demonstrates promising results, understanding the challenges encountered with alternative strategies and analyzing attention patterns provides valuable context for future research directions. We first examine challenges with alternative token selection strategies that motivated our design choices, followed by visualizations that offer insights into attention distribution patterns. 6.1. Challenges with Alternative Token Selection Strategies Before designing NSA, we explored adapting existing sparse attention methods to the training stage. However, these attempts encountered various challenges, prompting us to design different sparse attention architecture: Key-Clustering Based Strategies. We examined clustering-based strategies like ClusterKV (Liu et al., 2024). These methods store Keys and Values from the same cluster in contiguous memory regions. While theoretically feasible for training and inference, they face three significant challenges: (1) Non-trivial computational overhead introduced by dynamic clustering mechanisms; (2) Operator optimization difficulties exacerbated by inter-cluster imbalances, especially in Mixture-of-Experts (MoE) systems, where skewed Expert Parallelism (EP) group execution times lead to persistent load imbalances; (3) Implementation constraints arising from the need for mandatory periodic reclustering and chunk-sequential training protocols. These combined factors create substantial bottlenecks, significantly limiting their effectiveness for real-world deployment. 14 Figure 7 Compare training loss on 3Bparameter model with Full Attention and different token selection strategies and. Our NSA achieves better performance. Figure 8 Visualization of Attention Map on Full Attention transformer. Lightcolored regions indicate higher attention values. As shown in the figure, attention scores exhibit blockwise clustering distribution. Other Blockwise Selection Strategies. We also considered blockwise key, value selection strategies different from NSA, such as Quest (Tang et al., 2024) and InfLLM (Xiao et al., 2024). These methods rely on computing an importance score for each block and selecting the top-𝑛 blocks based on their similarity with 𝑞𝑡. However, existing methods face two critical issues: (1) Since the selection operation is non-differentiable, importance score computation based on neural networks relies on auxiliary loss, which increases operator overhead and often degrades model performance; (2) Heuristic parameter-free importance score computation strategy suffer from low recall rates, leading to suboptimal performance. We evaluate both approaches on 3B-parameter model with similar architecture and compare their loss curve with NSA and Full Attention. For the auxiliary loss-based selection method, we introduce additional queries and representative keys for each block to estimate the block importance scores. These scores are supervised by the mean attention scores between the original queries and keys within each block. For the heuristic parameter-free selection method, following the strategy of Quest, we implement direct selection using the product between queries and coordinate-wise min-max of the key chunks, without introducing additional parameters. We also explore cold-start training approach where Full Attention is applied for the initial 1000 steps before transitioning to the heuristic blockwise selection. As shown in Figure 7, both methods exhibited inferior loss. 6.2. Visualization To explore potential patterns in transformer attention distributions and seek inspiration for our design, we visualize the attention map from our pretrained 27B Full Attention model in Figure 8. The visualization reveals interesting patterns where attention scores tend to exhibit blockwise clustering characteristics, with nearby keys often showing similar attention scores. This observation inspired our design of NSA, suggesting that selecting key blocks based on spatial continuity might be promising approach. The blockwise clustering phenomenon indicates that tokens adjacent in the sequence may share certain semantic relationships with query tokens, though the exact nature of these relationships requires further investigation. This observation motivated us to explore sparse attention mechanism that operates on continuous token blocks rather than individual tokens, aiming to enhance computational efficiency and preserve high-attention patterns. 15 7. Related Works We review existing approaches that improve the efficiency of attention computation through sparse attention. These methods can be broadly categorized into three groups based on their core strategies: (1) fixed sparse pattern, (2) dynamic token pruning, and (3) query-aware selection. We introduce several representative works from each category. 7.1. Fixed Sparse Pattern SlidingWindow is commonly used approach that allows the query to compute attention only within fixed window. StreamingLLM (Xiao et al., 2023) addresses the challenges of processing long text streams by maintaining two critical portions of the context: an attention sink (early tokens) and local context window. While these approaches effectively reduce memory and computation costs, their rigid pattern of ignoring contexts limits their performance on tasks requiring full context understanding. 7.2. Dynamic Token Pruning H2O (Zhang et al., 2023b) implements an adaptive approach to reduce KV-cache memory usage during decoding. This method dynamically evicts tokens deemed less important for future predictions based on their recent utility according to attention score. SnapKV (Li et al., 2024) also introduces token pruning strategy that reduces the KV cache by selectively retaining only the most crucial features, enabling efficient memory usage. SnapKV identifies important features through attention weight analysis and voting during prefilling, then updates KV cache by combining selected compressed features with recent context to maintain prompt consistency. 7.3. Query-Aware Selection Quest (Tang et al., 2024) employs blockwise selection strategy where each chunks importance is estimated by product between query and coordinate-wise min-max of the key chunks. The results scores help to select top𝑛 important key-value chunks for attention. InfLLM (Xiao et al., 2024) combines fixed patterns with retrieval by maintaining attention sinks, local context, and retrievable chunks. This method selects representative keys from each chunk to estimate chunk importance. HashAttention (Desai et al., 2024) formulates pivotal token identification as recommendation problem by mapping queries and keys to Hamming space using learned functions. ClusterKV (Liu et al., 2024) achieves sparsity by firstly clustering keys and then selecting the most relevant clusters for attention computation based on query-cluster similarity. 8. Conclusion We present NSA, hardware-aligned sparse attention architecture for efficient long-context modeling. By integrating hierarchical token compression with blockwise token selection within trainable architecture, our architecture achieves accelerated training and inference while maintaining Full Attention performance. NSA advances the state-of-the-art by demonstrating general benchmark performance matches full-attention baselines, exceeding modeling capability in long-context evaluations, and enhanced reasoning ability, all accompanied by measurable reductions in computational latency and achieving significant speedup."
        },
        {
            "title": "References",
            "content": "J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry, Q. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Y. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Z. Chen, R. Sadhukhan, Z. Ye, Y. Zhou, J. Zhang, N. Nolte, Y. Tian, M. Douze, L. Bottou, Z. Jia, et al. Magicpig: Lsh sampling for efficient llm generation. arXiv preprint arXiv:2410.16179, 2024. K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, et al. Training verifiers to solve math word problems, 2021. URL https://arxiv. org/abs/2110.14168, 2021. D. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. 2024. URL https://arxiv.org/abs/2405.04434. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. A. Desai, S. Yang, A. Cuadron, A. Klimovic, M. Zaharia, J. E. Gonzalez, and I. Stoica. Hashattention: Semantic sparsity for faster inference. arXiv preprint arXiv:2412.14468, 2024. D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019. S. Ge, Y. Zhang, L. Liu, M. Zhang, J. Han, and J. Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. G. T. Google, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. H. Jiang, Q. Wu, C.-Y. Lin, Y. Yang, and L. Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736, 2023. 17 H. Jiang, Y. Li, C. Zhang, Q. Wu, X. Luo, S. Ahn, Z. Han, A. H. Abdi, D. Li, C.-Y. Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. arXiv preprint arXiv:2407.02490, 2024. G. Kamradt. LLMTest NeedleInAHaystack. GitHub repository, 2023. URL https://github.c om/gkamradt/LLMTest_NeedleInAHaystack. Accessed: [Insert Access Date Here]. H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023. Y. Li, Y. Huang, B. Yang, B. Venkitesh, A. Locatelli, H. Ye, T. Cai, P. Lewis, and D. Chen. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469, 2024. G. Liu, C. Li, J. Zhao, C. Zhang, and M. Guo. Clusterkv: Manipulating llm kv cache in semantic space for recallable compression. arXiv preprint arXiv:2412.03213, 2024. J. S. Park, J. C. OBrien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents: Interactive simulacra of human behavior. In S. Follmer, J. Han, J. Steimle, and N. H. Riche, editors, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 2023, San Francisco, CA, USA, 29 October 2023 1 November 2023, pages 2:12:22. ACM, 2023. B. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large language models. In ICLR. OpenReview.net, 2024. N. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR, abs/1911.02150, 2019. M. Suzgun, N. Scales, N. Schärli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V. Le, E. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. J. Tang, Y. Zhao, K. Zhu, G. Xiao, B. Kasikci, and S. Han. Quest: Query-aware sparsity for efficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024. P. Tillet, H.-T. Kung, and D. Cox. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, pages 1019, 2019. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. Chainof-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. C. Xiao, P. Zhang, X. Han, G. Xiao, Y. Lin, Z. Zhang, Z. Liu, and M. Sun. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 18 G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. E. Zelikman, Y. Wu, J. Mu, and N. D. Goodman. Star: Bootstrapping reasoning with reasoning. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 December 9, 2022, 2022. F. Zhang, B. Chen, Y. Zhang, J. Keung, J. Liu, D. Zan, Y. Mao, J. Lou, and W. Chen. Repocoder: Repository-level code completion through iterative retrieval and generation. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 610, 2023, pages 2471 2484. Association for Computational Linguistics, 2023a. K. Zhang, J. Li, G. Li, X. Shi, and Z. Jin. Codeagent: Enhancing code generation with toolintegrated agent systems for real-world repo-level coding challenges. In L. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 1116, 2024, pages 1364313658. Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y. Tian, C. Ré, C. Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023b. Z. Zhou, C. Li, X. Chen, S. Wang, Y. Chao, Z. Li, H. Wang, R. An, Q. Shi, Z. Tan, et al. Llm mapreduce: Simplified long-sequence processing using large language models. arXiv preprint arXiv:2410.09342, 2024. 19 A. Example of AIME Results Prompt: UserLet 𝑥, 𝑦 and 𝑧 be positive real numbers that satisfy the following system of equations: log2 log2 log2 (cid:19) (cid:18) 𝑥 𝑦𝑧 (cid:17) (cid:19) (cid:16) 𝑦 𝑥𝑧 (cid:18) 𝑧 𝑥 𝑦 = = = 1 2 1 3 1 4 Then the value of Find 𝑚 + 𝑛. Please reason step by step, and put your final answer within .Assistant 𝑚 𝑛 where 𝑚 and 𝑛 are relatively prime positive integers. (cid:12) is (cid:12) (cid:12)log (𝑥4 𝑦3𝑧2)(cid:12) Our Result: think.../think (2275 tokens) Given the system of equations: (cid:1) = 1 = 1 (cid:17) (cid:16) 𝑥 𝑦𝑧 (cid:0) 𝑦 𝑥𝑧 1. log2 3 3. log2 We convert these logarithmic equations to their exponential forms: 2 2. log2 (cid:17) (cid:16) 𝑧 𝑥 𝑦 = 1 4 𝑥 𝑦𝑧 = 𝑦 𝑥𝑧 = 21/3 3. 𝑧 𝑥 𝑦 = 21/ 2 2. 1. Expressing each variable in terms of 𝑥, 𝑦, and 𝑧: 2 𝑦𝑧 2. 𝑦 = 21/3 𝑥𝑧 3. 𝑧 = 21/4 𝑥 𝑦 1. 𝑥 = 2 𝑦𝑧 and 𝑦 = 21/3 𝑥𝑧, we get 𝑥 = Substituting these expressions into each other, we find: 2 (21/3 𝑥𝑧) 𝑧, leading to - From 𝑥 = 𝑧 = 25/12. - Substituting 𝑧 into the equation for 𝑦, we get 𝑦 = 23/8. - Substituting 𝑧 and 𝑦 into the equation for 𝑥, we get 𝑥 = 27/24. Next, we compute 𝑥4 𝑦3𝑧2: - 𝑥4 = (27/24)4 = 27/6 - 𝑦3 = (23/8)3 = 29/8 - 𝑧2 = (25/12)2 = 25/ Multiplying these together: 𝑥4 𝑦3𝑧2 = 27/6 29/8 25/6 = 275/24 = 225/8 Taking the logarithm base 2: The absolute value is: log (𝑥4 𝑦3𝑧2) = 25/8 log2 (𝑥4 𝑦3𝑧2) = 25/8 20 Thus, 𝑚 = 25 and 𝑛 = 8, giving 𝑚 + 𝑛 = 33. Baseline Result: think.../think (9392 tokens) Given the system of logarithmic equations: log2 log2 log (cid:19) (cid:18) 𝑥 𝑦𝑧 (cid:17) (cid:19) (cid:16) 𝑦 𝑥𝑧 (cid:18) 𝑧 𝑥 𝑦 = = = 1 2 1 3 1 4 We convert these equations to exponential form: 2 = 1 3 = 2 1 4 = 2 𝑥 𝑦𝑧 𝑦 𝑥𝑧 𝑧 𝑥 𝑦 = 𝑥 = 2 𝑦𝑧 = 𝑦 = 2 1 3 𝑥𝑧 = 𝑧 = 1 4 𝑥 𝑦 Substituting these into each other, we find: 1 1. 𝑥 = 2 𝑦𝑧 2. 𝑦 = 2 3 𝑥𝑧 3. 𝑧 = 1 4 𝑥 𝑦 By solving these equations step-by-step, we find: - 𝑧 = 25/12 - 𝑥 𝑦 = 22/3 - 𝑦 = 21/24 - 𝑥 = 21/24 However, upon verifying the third equation, we realize the system might not have solution. Therefore, we use matrix inversion to solve the system: (𝑥), 𝐵 = log2 Let 𝐴 = log2 log2 log2 log (𝑥) log2 ( 𝑦) log2 (𝑧) log2 ( 𝑦) log2 (𝑥) log2 (𝑥) log2 (𝑧) = 1 2 (𝑧) = 1 3 ( 𝑦) = 1 4 ( 𝑦), 𝐶 = log2 (𝑧). The system becomes: 𝐴 𝐵 𝐶 = 1 2 𝐵 𝐴 𝐶 = 1 3 𝐶 𝐴 𝐵 = 1 21 We solve this system using Cramers rule and matrix inversion. The determinant of the matrix is -2, and the solutions are: 𝐴 = 61 48 , 𝐵 = 13 16 , 𝐶 = 1 6 Thus, we have: 𝑥 = 261/48, 𝑦 = 213/16, 𝑧 = 21/"
        },
        {
            "title": "We need to find",
            "content": "(cid:12) (cid:12)log2 (𝑥4 𝑦3𝑧2)(cid:12) (cid:12): 4 log2 𝑥 + 3 log2 𝑦 + 2 log2 𝑧 = 4 61 48 + 3 13 16 + 2 (cid:18) (cid:19) 1 6 Converting to common denominator (48): 4 61 48 = 244 48 , 3 13 16 = 117 , 2 (cid:18) (cid:19) 1 = 16 48 Summing these: 244 48 + 117 16 48 = 345 48 = 115 The fraction 115 115 + 16 = 131. 16 is in simplest form, so 𝑚 = 115 and 𝑛 = 16. Therefore, 𝑚 + 𝑛 = 131 Prompt: User Real numbers 𝑥 and 𝑦 with 𝑥, 𝑦 > 1 satisfy log𝑥 ( 𝑦 𝑥) = log𝑦 (𝑥4𝑦) = 10. What is the value of 𝑥 𝑦? Please reason step by step, and put your final answer within . Assistant Our Result: think.../think (15147 tokens) Given the equations: 22 1. log𝑥 ( 𝑦 𝑥) = 10 2. log𝑦 (𝑥4𝑦) = 10 We start by applying the logarithm power rule to both equations. For the first equation: Let 𝑧 = log𝑦 (𝑥). Then, log𝑥 ( 𝑦) = 1 𝑧 . Substituting this into the equation, we get: log𝑥 ( 𝑦 𝑥) = 𝑥 log𝑥 ( 𝑦) = 10 For the second equation: 𝑥 1 𝑧 = 10 = 𝑥 = 10𝑧 log𝑦 (𝑥4𝑦) = 𝑦 log𝑦 (𝑥4) = 10 Using the power rule, we get: 𝑦 4 log𝑦 (𝑥) = 10 = 4𝑦 log𝑦 (𝑥) = 10 Since log𝑦 (𝑥) = 𝑧, this simplifies to: 4𝑦𝑧 = 10 = 𝑦𝑧 = 5 2 We now have two expressions: 1. 𝑥 = 10𝑧 2. 𝑦 = 5/2 𝑧 Substituting 𝑥 = 10𝑧 and 𝑦 = 5/2 𝑧 into the product 𝑥 𝑦, we get: 𝑥 𝑦 = (10𝑧) (cid:19) (cid:18) 5/2 𝑧 = 10𝑧 5 2𝑧 = 10 5 2 = 25 Thus, the value of 𝑥 𝑦 is 25 . Baseline Result: think.../think (16223 tokens) Given the problem where real numbers 𝑥 and 𝑦 with 𝑥, 𝑦 > 1 satisfy the equations log𝑥 ( 𝑦 𝑥) = 10 and log𝑦 (𝑥4 𝑦) = 10, we need to find the value of 𝑥 𝑦. First, we convert the logarithmic equations to their exponential forms using the change of base formula. 23 1. log𝑥 ( 𝑦 𝑥) = 10 𝑥 log𝑥 ( 𝑦) = 10 Using the change of base formula, log𝑥 ( 𝑦) = ln 𝑦 log𝑥 𝑦 = 4 9 . Thus, ln 𝑥 . Since log𝑦 𝑥 = 9 4 , we have 𝑥 4 9 = 10 = 𝑥 = 10 9 = 90 4 = 45 2 2. log𝑦 (𝑥4 𝑦) = 10 4 log𝑦 𝑥 + 1 = 10 = 4 log𝑦 𝑥 = 9 = log𝑦 𝑥 = 9 4 Again, using the change of base formula, log𝑥 𝑦 = 4 9 . Next, we express 𝑥 and 𝑦 in terms of each other: 𝑥 = 45 2 and 𝑦 = (cid:19) 4/9 (cid:18) 45 2 We then compute the product 𝑥 𝑦: 𝑥 𝑦 = (cid:19) 4/9 (cid:19) (cid:18) 45 2 (cid:18) 45 2 = (cid:18) 45 2 (cid:19) 1+4/ (cid:19) 13/9 = (cid:18) 45 2 However, upon re-evaluating and using another approach by setting 𝑥 = 2𝑎 and 𝑦 = 2𝑏, we find: log𝑥 ( 𝑦 𝑥) = 10 = 𝑥 log𝑥 𝑦 log𝑥 𝑦 = 10 = 𝑥 4 9 = 10 = 𝑥 = 45 2 log𝑦 (𝑥4 𝑦) = 10 = 4 log𝑦 𝑥 + 1 log𝑦 𝑥 = 10 = 4 9 4 + 1 = 10 = 9 + 1 = 10 consistent Finally, we find that the product 𝑥 𝑦 simplifies to: 𝑥 𝑦 = (cid:19) (cid:18) 45"
        }
    ],
    "affiliations": [
        "DeepSeek-AI",
        "Key Laboratory for Multimedia Information Processing, Peking University, PKU-Anker LLM Lab",
        "University of Washington"
    ]
}