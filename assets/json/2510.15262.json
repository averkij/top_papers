{
    "paper_title": "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning",
    "authors": [
        "Zhiyuan Fan",
        "Yifeng Liu",
        "Qingyue Zhao",
        "Angela Yuan",
        "Quanquan Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Empirical scaling laws prescribe how to allocate parameters, data, and compute, while maximal-update parameterization ($\\mu$P) enables learning-rate transfer across widths by equalizing early-time update magnitudes. However, in modern scale-invariant architectures, training quickly enters an optimizer-governed steady state where normalization layers create backward scale sensitivity and the effective learning rate becomes width dependent, degrading $\\mu$P transfer. We address this by introducing a weight-decay scaling rule for AdamW that preserves sublayer gain across widths. Empirically, the singular-value spectrum of each matrix parameter scales in norm as $\\sqrt{\\eta/\\lambda}$ with an approximately invariant shape; under width scaling $d$, we observe that the top singular value scales approximately as $\\sqrt{\\eta/\\lambda}\\cdot d^{0.75}$. Combining this observation with the $\\mu$P learning-rate rule $\\eta_2\\propto d^{-1}$ for matrix-like parameters implies an empirical weight-decay scaling rule $\\lambda_2\\propto \\sqrt{d}$ that approximately keeps sublayer gains width invariant. Together with vector-like parameters trained at $\\eta_1=\\Theta_d(1)$ and $\\lambda_1=0$, this yields \\emph{zero-shot} transfer of both learning rate and weight decay from proxy to target widths, removing per-width sweeps. We validate the rule on LLaMA-style Transformers and in a minimal synthetic setting, and we provide a simple diagnostic, matching top singular values, to check sublayer-gain invariance. Our results extend $\\mu$P beyond the near-init regime by explicitly controlling steady-state scales set by the optimizer, offering a practical recipe for width-robust hyperparameter transfer under AdamW."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 2 6 2 5 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Robust Layerwise Scaling Rules by Proper Weight\nDecay Tuning",
            "content": "Zhiyuan Fan Yifeng Liu Qingyue Zhao Angela Yuan Quanquan Gu Abstract Empirical scaling laws prescribe how to allocate parameters, data, and compute, while maximalupdate parameterization (µP) enables learning-rate transfer across widths by equalizing early-time update magnitudes. However, in modern scale-invariant architectures, training quickly enters an optimizergoverned steady state where normalization layers create backward scale sensitivity and the effective learning rate becomes width dependent, degrading µP transfer. We address this by introducing weightdecay scaling rule for AdamW that preserves sublayer gain across widths. Empirically, the singular-value spectrum of each matrix parameter scales in norm as (cid:112)η/λ with an approximately invariant shape; under width scaling d, we observe that the top singular value scales approximately as (cid:112)η/λ d0.75. Combining this observation with the µP learning-rate rule η2 d1 for matrix-like parameters implies an empirical weight-decay scaling rule λ2 that approximately keeps sublayer gains width invariant. Together with vector-like parameters trained at η1 = Θd(1) and λ1 = 0, this yields zero-shot transfer of both learning rate and weight decay from proxy to target widths, removing per-width sweeps. We validate the rule on LLaMA-style Transformers and in minimal synthetic setting, and we provide simple diagnostic, matching top singular values, to check sublayer-gain invariance. Our results extend µP beyond the near-init regime by explicitly controlling steady-state scales set by the optimizer, offering practical recipe for width-robust hyperparameter transfer under AdamW."
        },
        {
            "title": "1 Introduction",
            "content": "Over the past few years, empirical scaling laws have emerged as guiding principle for developing everlarger language models. growing body of work demonstrates that test loss often follows simple power-law relationships with respect to model size, dataset size, and compute budget. These regularities provide nearly closed-form prescriptions for distributing resources: how many parameters to allocate, how much data to train on, and even which learning-rate schedule to adopt for compute-efficient training (Hoffmann et al., 2022; Kaplan et al., 2020). Initially derived for GPT-style Transformers and later refined under compute-optimal training regimes, these laws now serve as foundation for many large-scale model design practices. Maximal-update Parameterization (µP) (Yang et al., 2021) complements such global scaling insights by analyzing the dynamics of individual updates. Its central principle is that as model widens, the rate of change of each parameter tensor should remain invariant. Specifically, under suitable initialization, vectorlike parameters (embeddings, LayerNorm gains, biases) should maintain constant learning rate, while matrix parameters in attention and feed-forward layers should have learning rates that scale inversely with width. This formulation makes the optimal learning rate largely independent of model dimension, enabling one to tune it on smaller proxy models and reuse it directly for much larger counterparts. Nevertheless, the analysis of µP primarily captures the early-time behavior, when parameters stay close to initialization and their magnitudes are determined largely by it. In modern scale-invariant architectures, training dynamics soon reach steady state where weight directions continue to evolve, but norms remain Department of Electrical Engineering and Computer Science, MIT, Cambridge, MA, USA; email: fanzy@mit.edu Department of Computer Science, UCLA, CA, USA; email: liuyifeng@g.ucla.edu Department of Computer Science, UCLA, CA, USA; email: zhaoqy24@g.ucla.edu Department of Computer Science, UCLA, CA, USA; email: hzyuan@cs.ucla.edu Corresponding author: Department of Computer Science, UCLA, CA, USA; email: qgu@cs.ucla.edu 1 roughly stable due to implicit or explicit regularization. In this regime, the governing scales are dictated more by the optimizer than the initialization (Defazio, 2025; Kosson et al., 2023), extending beyond the tensor-program assumptions underlying µP. Because these steady states depend on model width, so too do the layerwise output scales. Although normalization layers enforce approximate forward scale-invariance, they introduce backward scale-sensitivity. For homogeneous normalizers such as LayerNorm or BatchNorm (without bias terms), scaling the prenormalized activations by factor α leaves the normalized output unchanged, yet the gradient with respect to the input scales inversely as 1/α by the chain rule (Santurkar et al., 2018; Xu et al., 2019). Consequently, width-dependent activation magnitudes yield width-dependent gradient magnitudes, rendering the effective learning rate scale-dependent. As result, even if µP achieves perfect early-time matching, transfer from proxy to target models can degrade once optimization enters this steady regime. As result, we need to tune weight decay to make sublayer gain invariant across different model width. In this paper, we resolve this problem by introducing proper weight decay scaling rule for µP. Our contributions are: We inspect the singular value spectrum of weight matrices under the steady state of AdamW training. It is observed that the singular value spectrum of each weight matrix grows in proportion to (cid:112)η/λ, while the shape of the spectrum remains nearly unchanged. The proportional scaling of the learning rate and weight decay preserves the sublayer gain. When scaling up the model width d, we observe that the top singular value magnitude approximately scales with (cid:112)η/λ d0.75. Together with the learning rate scaling η2 d1 for matrix-like parameters, maintaining sublayer gain invariance requires scaling the weight decay of matrix-like parameters as λ2 d. Combined with fixing the vector-like sublayers (i.e., embedding layers and RMSNorm blocks) to learning rate of η1 = Θd(1) and weight decay λ1 = 0, we show that the new hyperparameter scheme achieves both optimal learning rate and optimal weight decay transfer at the same time. Finally, we present an illustrative model using synthetic data in which the weight decay scaling rule can be observed. Since the synthetic data is purely random, this suggests that the weight decay scaling rule is an inherent property of the model architecture, shedding light on potentially more fine-grained layerwise scaling rules for future work."
        },
        {
            "title": "2.1 Hyperparameter Transfer",
            "content": "Empirical scaling rules provide global prescriptions for allocating hyperparameters, data, and compute, and have repeated shown near power-law regularities across modalities and have repeated shown near power-law regularities across modalities and architectures (Bjorck et al., 2024; Henighan et al., 2020; Hestness et al., 2017; Hoffmann et al., 2022; Kaplan et al., 2020; Li et al., 2025). While these results guide what to scale, they say less about how to transport tuned hyperparameters across model widths. In practice, this gap has been bridged either by black-box search (e.g., Hyperband/ASHA, BOHB, and modern HPO frameworks) (Akiba et al., 2019; Falkner et al., 2018; Horvath et al., 2021; Jamieson and Talwalkar, 2016; Li et al., 2018; Perrone et al., 2018) or by phenomenological heuristics. principled alternative is the Tensor Program view and Maximal-update Parameterization (µP), which unify Standard Parameterization (Glorot and Bengio, 2010; He et al., 2015), Neural Tangent style scalings (Jacot et al., 2018), and Mean-Field limits (Chizat and Bach, 2018; Mei et al., 2018; Rotskoff and Vanden-Eijnden, 2022; Sirignano and Spiliopoulos, 2020). The core prescription of µP is that to set hyperparameters such that the update magnitudes of each tensor family should be width-invariant, yielding learning-rate split: vector-like parameters keep constant learning rates, while matrix-like parameters scale their learning rate by inversely to the model width d, enabling µTransfer of tuned hyperparameters from the 2 proxy to target model of larger model width (Yang et al., 2021; Yang and Hu, 2020). Empirically, µTransfer has been observed across architectures and optimizers (SGD/Adam) and at LLM scale (Dey et al., 2024; Haas et al., 2024; Lingle, 2024; Meta AI, 2024). Subsequent works extended the framework (e.g., depth-wise transfer and spectral perspectives), further clarifying when early-time dynamics align across widths (Yang and Littwin, 2023; Yang et al., 2023, 2024). However, despite its success, the current Tensor Program theory and the analyses of µP primarily characterize the near-initialization regime, where the total number of effective update steps is small comparing to the model width and the initial magnitudes dominate (Chen et al., 2025; Golikov and Yang, 2022; Yang and Hu, 2020; Yang and Littwin, 2023; Yang et al., 2023). In modern pretraining, however, optimization typically runs for longer timestep much larger than the model width (Achiam et al., 2023; Liu et al., 2024). In this long-horizon regime, even for linear activations the existing theory remains insufficient to describe the terminal implicit bias or generalization (Bordelon and Pehlevan, 2022, 2025; Chizat et al., 2024). Furthermore, scale-invariant architectures interact with normalization in way that creates width-dependent effective learning rates: non-affine normalizers (BatchNorm/LayerNorm/RMSNorm without bias) preserve forward-scale invariance but introduce backward scale sensitivity, since gradients through the normalizer scale inversely with the pre-normalized activation magnitude (Santurkar et al., 2018; Xu et al., 2019). As training leaves the near-init regime, norms stabilize and optimizer dynamics set the effective scale (Defazio, 2025; Kosson et al., 2023), so layerwise output scales (and hence gradients) become width-dependent even if early-time µP matching is perfect. This motivates width-aware weight decay design to preserve sublayer gain across widths, the central focus of our work."
        },
        {
            "title": "2.2 Weight Decay Scaling Rule",
            "content": "The original analysis of maximal-update (Yang and Littwin, 2023) produces identical predictions for any weight-decay scaling rule with λ = O(d). Recently, Wang and Aitchison (2024) advocated linear rule λ = Θ(d) for AdamW, arguing that the effective shrinkage (1 ηλ) should not vary with d. Empirical learning-rate transfer studies partially corroborate this intuition: fixed λ deteriorates transfer as width grows, while larger λ can recover it (Lingle, 2024; Wang and Aitchison, 2024). Variants of linear scaling have also appeared in low-precision or variant-µP settings when measuring LR transfer via train/val loss (Blake et al., 2024; Narayan et al., 2025), and heuristic layerwise arguments have been used to justify linear scaling for hidden matrices (Dey et al., 2025). Beyond scaling, there is active line of work on weight decay and its role in generalization. Decoupled weight decay was introduced to separate L2 regularization from the adaptive update (Loshchilov and Hutter, 2019). Subsequent analyses and measurements have examined norm dynamics, effective learning rates, and rotational equilibria in scale-invariant networks trained with SignGD-like methods (a family that includes Adam/AdamW) (DAngelo et al., 2024; Kobayashi et al., 2024; Kosson et al., 2023; Xiao, 2024; Zhou et al., 2024). Particularly relevant to us, DAngelo et al. (2024) observed SGD generalization optima along curves approximately satisfying η 1/λ in under-training regimes. Kosson et al. (2023) further argued that in steady state the scale-invariant parameter norms track (cid:112)η/λ and per-step directional rotation scales like ηλ, picture consistent with optimizer-governed steady-state dynamics rather than initialization-dictated ones. Orthogonal lines explore weight-decay schedules over time (Jacobs et al., 2025; Xie et al., 2023) and extrapolate decay rules to non-AdamW optimizers and their analyses (Li and Arora, 2020; Pethick et al., 2025a,b; Sun et al., 2025; Wen et al., 2025). In contrast, we target the specific question raised by the tension above: what width-dependent λ makes sublayer gains invariant under AdamW and preserves µTransfer? Our empirical and synthetic analyses indicate that, when combined with the standard η2 d1 for matrix-like parameters and η1 = Θd(1) with λ1 = 0 for vector-like parameters, setting the matrix-parameter decay as keeps singular-value scales (and thus sublayer gains) width-invariant in the optimizer-determined λ2 steady state."
        },
        {
            "title": "3.1 General Notations",
            "content": "We use lowercase boldface letters such as to denote vectors, and uppercase boldface letters such as to denote matrices. For vector Rd, the root-mean-square (RMS) norm is defined as vrms v2/ d. Similarly, the RMS norm of matrix is defined as the RMS norm of its vectorization. The operator norm of matrix is defined as Wop supxRd yrms/xrms.The notation indicates the element-wise absolute value, while and represent element-wise multiplication and division of tensors and y, x1/2. respectively. The notation xk stands for element-wise exponentiation with exponent k, and {1, 2, . . . , k} denotes an index set, and represents the empty set. The logarithm The shorthand log is taken in base 2, while ln refers to the natural logarithm. For non-negative sequences {an} and {bn}, the notation an O(bn) (equivalently, bn Ω(an)) indicates the existence of constant > 0 such that an Cbn for all > 0, whereas an = Θ(bn) signifies the existence of constants C1, C2 > 0 satisfying C1bn an C2bn for all > 0. The term Θd(1) serves as variant of Θ(1), emphasizing that both C1 and C2 are independent of d. We use to denote = Θ(b), and an = o(bn) for limn an/bn = 0. (cid:74) (cid:75)"
        },
        {
            "title": "3.2 AdamW Optimizer",
            "content": "Let denote parameter tensor, and let be the loss function evaluated on sampled mini-batch at the current step. In this paper, we analyze the model at fixed point in time, in static fashion. Therefore, we do not include the timestep throughout the analysis. We denote by WL the gradient of the loss with respect to the parameter tensor W. AdamW (Loshchilov and Hutter, 2019), variant of Adam (Kingma and Ba, 2015) with decoupled weight decay, maintains the bias-corrected, accumulated firstand second-order moments, and V, of the gradient using exponential moving-average coefficients β1 and β2, and updates (cid:98)G (cid:0) η(cid:0) (cid:98)G + λW(cid:1), + ε(cid:1). Throughout this paper, we neglect the stabilizer ε > 0, as it is typically set to an insignificantly small value."
        },
        {
            "title": "3.3 Parameter Classes",
            "content": "Denote as the model width (embedding dimension). We study the scaling rule with respect to throughout this work while keeping other architectural parameters fixed (e.g., model depth, number of MHA (multihead attention) heads, and FFN (feed-forward network) expansion coefficient). Based on how the parameter shapes scale with d, we divide the parameters into two groups: Vector-like. This group contains parameters whose number of entries scales linearly with the model width. Examples include the embedding layer (shape with fixed vocabulary size E), RMSNorm gains (shape d), and other one-dimensional parameters. We denote representative vector parameter by R1d. Matrix-like. This group contains parameters whose number of entries scales quadratically with the model width. These include all dense projections in MHA and FFN blocks. In LLaMA-style models, this includes the attention projections WQ, WK, WV , WO Rdd in MHA, and the linear projections Wgating, Win Rmd and Wout Rdm in FFN, with fixed expansion ratio m/d. When matrix is rectangular with dimensions (κd) for constant κ, we treat it as κ = Θ(1) square blocks. We denote representative square block by Rdd. The two groups exhibit different behaviors as the model width scales. Thus, ideal transferable parameterization schemes should assign distinct hyperparameter scalings to each group."
        },
        {
            "title": "3.4 Maximal-update Parameterization",
            "content": "Maximal-update Parameterization (µP) principally suggests that per-step functional changes should be width-invariant. Specifically, Yang et al. (2021) suggest scaling hyperparameters such that the features and their updates after single step of gradient descent remain invariant to the model width d: Condition C1 (Desideratum of µP). Let = (x; W) be sublayer with input x, output y, and weight matrix W. Denote by the change in the output after one gradient update step. The goal is to ensure that yrms xrms = Θd(1) and yrms xrms = Θd(1). By analyzing the model dynamics around parameter initialization, where parameter magnitudes are dominated by the initial variance, Yang et al. (2021) suggest that one should scale the two types of parameters differently based on how their shapes scale with model width. Their prescription yields: Class Initial variance Learning rate Vector-like Matrix-like σ2 1 = Θd(1) 2 = Θ(d1) σ η1 = Θd(1) η2 = Θ(d1) Table 1: µP scaling rules by parameter class. We remark that for matrix-like sublayers, the initial variance follows fan-in scaling (Glorot and Bengio, 2010; He et al., 2015), and the learning rate η2 d1 enforces width-invariant per-step functional changes (Yang et al., 2021). The scaling rule further suggests scaling the attention temperature by 1/dk, inversely proportional to the head dimension dk, which itself scales with model width d. It also recommends scaling the vocabulary readout as zi = ei, y/d to keep the logit scale invariant. In this paper, we complement prior research by studying the scaling rule for the weight decay coefficient λ with respect to the model width d."
        },
        {
            "title": "4 Robust Scaling Rule by Propoer Weight Decay Scaling",
            "content": "In this section, we present the weight decay scaling rule for robust hyperparameter tuning. The key idea is based on how weight decay interacts with the framework of µP. Recent works (Defazio, 2025; Kosson et al., 2023) show that, in the presence of weight decay and for homogeneous models, the training dynamics of AdamW enter stable regime governed not by initialization or training timestep, but by the learning rate and weight decay. In this regime, when the hyperparameters of AdamW are held fixed, the weight norm of the weight matrix stabilizes and lies on sphere, where each training update behaves like rotation. Specifically, when training model parameter in homogeneous sublayer using AdamW with learning rate η and weight decay λ > 0, the root-mean-square norm of the weight matrix quickly converges to Wrms = Θd (cid:18)(cid:114) η λ (cid:19) , independent of the initialization. This suggests that, instead of tuning the initial variance, we should tune the weight decay λ to ensure that the sublayer gain yrms/xrms remains invariant across model width. 5 Figure 1: Statistics of the FFN weight matrices under AdamW with various learning rates η and weight decay values λ. The plotted lines are averaged across matrix-like sublayers from all blocks. Left: Sublayer gain yrms/xrms during training. Right: Singular value spectrum σi(W) of the final weight matrices. The singular values are sorted in descending order; the horizontal axis shows the spectral index, and the vertical axis shows the corresponding singular value. Consider linear layer in which = (x; W) Wx. The sublayer gain can be written as yrms xrms = Wrms ρ(d), (4.1) where ρ(d) is the alignment factor, which is influenced by both the distribution of the singular value spectra of and the alignment between the spectra and the input vector x. To make the sublayer gain invariant across different model widths, it is sufficient to determine the power law of ρ(d)."
        },
        {
            "title": "4.1 Matching Weight Matrix Spectra under Fixed Model Width",
            "content": "Although the alignment factor ρ(d) cannot be directly determined beforehand, our key observation is that the ratio between learning rate and weight decay, (cid:112)η/λ, predominantly controls the overall magnitude of the singular value spectra, whereas their shape remains nearly unchanged. Consequently, sublayer-gain invariance can be achieved by matching the spectra, which in turn can be realized through an appropriate scaling rule for the weight decay. In our experiment, we train LLaMA models (Touvron et al., 2023) of width = 512 on the FineWeb dataset (Penedo et al., 2024) using the AdamW optimizer, without learning-rate annealing but with short warm-up period. Under these conditions, the models reach the steady state characterized by rotational equilibrium (Defazio, 2025; Kosson et al., 2023). The complete training configuration is summarized in Table 3, where smaller baseline learning rate, ηbase = 5.0 103, is adopted to prevent divergence when scaling η upward. Every 1000 steps, we sample batch from the training data and run forward pass. For each linear sublayer = Wx, we record the input scale xrms and output scale yrms of the data batch, and report their ratio yrms/xrms as the sublayer gain. In addition, we compute the singular-value spectra of all matrix-like parameters at the end of training. The results, shown in Figure 1, indicate that proportional scaling of the learning rate η and weight decay λ leaves both the sublayer gains and singular values nearly invariant. In contrast, doubling the weight decay 6 Figure 2: Statistics of the FFN weight matrices under AdamW with various matrix-like weight decay scalings λ2, and learning rate scaling specified in Table 1, where η1 = Θd(1) is used for vector-like parameters and η2 1/d for matrix-like parameters. The plotted lines are averaged across matrix-like sublayers from all blocks. Left: Sublayer gain yrms/xrms during training. Right: Singular value spectrum σi(W) of the final weight matrices. Alignment is observed when the weight decay scaling follows λ2 d. λ leads to uniform down-scaling of both quantities by approximately spectra grows proportionally to (cid:112)η/λ, while their shape remains stable across runs. 2. Overall, the magnitude of the Fact F1. In the steady state of AdamW training, fixing the model width d, the singular values satisfy σi(W) (cid:114) η λ , implying that proportional scaling of the learning rate and weight decay preserves the sublayer gain. We remark that numerically estimating the scaling rule of spectrum is difficult because no single scalar captures spectrum magnitude well. The operator norm Wop reflects only the largest singular value, ignoring the rest. The Frobenius norm WF Wrms aggregates all singular values but collapses both the decay profile and potential sparsity into one number. Moreover, the spectral tail has little impact on signal amplification and is less relevant for satisfying width-invariant gain. In practice, direct visualization of the spectrum remains the most reliable diagnostic."
        },
        {
            "title": "4.2 Matching Spectra Induces Sublayer Gain Invariance",
            "content": "Next, we sweep the scaling rules of weight decay to examine how the singular value spectra evolve with model width d. In the LLaMA training configuration under study, we find that the alignment factor ρ(d) approximately follows ρ(d) d0.75, as the singular value spectra align across different widths within matrixlike parameters when (cid:112)η2/λ2 d0.75. As shown in Figure 2, this scaling results in consistent spectral alignment and preserves amplification behavior across model widths. In contrast, alternative weight decay rules lead to noticeable deviations in the spectra. These results suggest that gain invariance is effectively maintained under the proposed parameterization scheme. To summarize the empirical trend, we state the following observation: 7 Figure 3: Transfer of the optimal base learning rate ηbase (Left) and weight decay λbase (Right) across model widths. Each curve shows the loss landscape for specific width d, with minima aligned after scaling according to Table 2. For visualization, all curves are vertically shifted by constant offsets so that the losses are directly comparable across widths. The alignment of minima indicates that the proposed parameterization enables consistent hyperparameter transfer across scales. Fact F2. Consider matrix-like linear sublayer = (x; W) Wx in Transformer, where denotes the input, the output, and the weight matrix. When the sublayer is trained using AdamW with learning rate η, weight decay λ, and all other hyperparameters fixed, scaling the model width while maintaining (cid:114) η λ d0.75 aligns the magnitude of the weight matrix top singular values σi(W) across widths and keeps the sublayer gain yrms/xrms approximately invariant. By combining this rule with the learning-rate scaling of µP, the initialization scaling rule that ensures proper behavior at initialization, and the common practice of disabling weight decay for vector-like parameters, we obtain the following parameterization scheme: Class Initial variance Learning rate Weight decay Vector-like Matrix-like σ2 1 = Θd(1) 2 = Θ(d1) σ2 η1 = Θd(1) η2 = Θ(d1) λ1 = 0 λ2 = Θ( d) Table 2: Our proposed layerwise scaling rules by parameter classes."
        },
        {
            "title": "4.3 Tuning Weight Decay Enables Hyperparameter Transfer",
            "content": "We evaluate the new scaling rule by sweeping the base learning rate ηbase and base weight decay λbase across models of varying widths. We train LLaMA-style models on FineWeb with widths ranging from = 256 (approximately 19M parameters) up to = 2048 (approximately 500M parameters). The hyperparameters are scaled such that η1 = η2 = ηbase and λ2 = λbase at the base model width dbase 256, and the remaining values are scaled according to Table 2. Each model is trained for 20,000 steps using cosine learning rate annealing, which decays to 0.01 the peak value after linear warm-up of 1,000 steps. The configuration 8 Figure 4: Validation loss differences across learning-rate and weight-decay pairs. Loss values are measured relative to the optimal configuration (ηbase = 0.02, λbase = 0.075). clear diagonal ridge of near-optimal points reveals that increasing λ requires reducing η, confirming their strong correlation. Both axes are spaced approximately logarithmically. is summarized in Table 3. As shown in Figure 3, the proposed parameterization scheme enables consistent transfer of the optimal base learning rate ηbase and weight decay λbase across model widths. This consistency supports two practical scaling strategies: Base-to-Target Transfer. Following the µP parameterization view, select small base width dbase, perform local hyperparameter sweep, and compute the corresponding hyperparameters for larger target width dtarget using the derived scaling rules in Table 2. This approach will lead to learning rates of matrix-like parameters that are comparably smaller than those of vector-like ones. Proxy-to-Target Scaling. Set the base width dbase dtarget to the target model width, and perform hyperparameter search at small width dproxy with layerwise-tuned learning rates according to the new scaling rule in Table 2. In this way, we can use standard parameterization for the target model while benefiting from zero-shot hyperparameters obtained via proper hyperparameter scaling. We remark that changing the base width dbase can be viewed as redefining the reference point for layerwise scaling, which in turn adjusts the learning-rate ratio η1/η2 between vector-like and matrix-like parameters."
        },
        {
            "title": "4.4 Tradeoff between Learning Rate and Weight Decay",
            "content": "Figure 4 shows that the optimal learning rate decreases as the weight decay increases, forming an approximately diagonal ridge of near-optimal configurations. This ridge indicates that the learning rate and weight decay are not independent hyperparameters but are closely correlated. There is no single learning rate that works uniformly across all weight decays, and vice versa. Similar correlations have been reported by DAngelo et al. (2024), who also observe that tuning one parameter requires adjusting the other. More importantly, we find that the models located along this ridge achieve almost identical final losses 9 Figure 5: Statistics of the weight matrix Win under AdamW in the synthetic run with learning rate scaling η 1/d and various scalings of weight decay λ. Left: Singular value spectrum σi(W) of the final weight matrices. When the weight decay follows (cid:112)η/λ d0.75, the top singular values are approximately aligned, matching our realistic training results in Section 4.2. Right: During training, the root-mean-square of the weight matrix Wrms converges to stable value, approximately proportional to (cid:112)η/λ. despite having different weight decays. This suggests that the precise choice of weight decay is less critical once the corresponding optimal learning rate is used. Consequently, full two-dimensional hyperparameter search could often be unnecessary. In practice, one can heuristically select weight decay, perform onedimensional sweep over the learning rate, and then transfer the resulting configuration to larger models using the µP scaling rules summarized in Table 2. All models in this experiment are trained on 5B tokens with 500 warmup steps."
        },
        {
            "title": "Illustrative Example of Sqrt Weight Decay Scaling",
            "content": "In this section, we present an illustrative model using synthetic data, where the square-root weight decay scaling rule can be observed. Although, to our knowledge, this rule has not been previously reported in the literature, we note that it arises naturally from the model architecture rather than the data distribution. Consider two-layer FFN = (x; W) Wout φ(Winx) with weight matrices Win, Wout Rdd and ReLU activation function φ. We train the model on synthetic data, where both the input vector (0, Id) and the upstream gradient yL (0, Id) are independently drawn from standard normal distributions. In this setup, by the chain rule, the gradients of the sublayers are given by WoutL = yL φ(Winx), Win = (cid:0)(W outyL) φ(Winx)(cid:1)x. We train the model across varying widths d, from 256 to 2048, using AdamW with learning rate scaled as η 1/d, following the µP scaling rule for matrix-like sublayers and the proposed weight decay scaling rule λ d. The model is trained for 20,000 steps without learning rate annealing, reaching the steady state characterized by rotational equilibrium (Defazio, 2025; Kosson et al., 2023). The AdamW hyperparameters match those used in our main LLaMA experiment in Figure 2, specifically for matrix-like layers. As shown in Figures 5 and 6, the top singular value spectra of the weight matrices align under the weight decay scaling rule λ d, consistent with the experimental results on LLaMA in Figure 2. This further suggests that the observed weight decay scaling rule arises naturally in training regimes with very high variance, such as Transformers trained for next-token prediction."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we study the weight decay scaling rule of µP in light of the steady-state dynamics characterized by rotational equilibrium (Kosson et al., 2023). This result complements the original µP framework, which guarantees early-stage sublayer gain by scaling the initialization variance to match the sublayer gain during the steady stage of training. We find that for matrix-like parameters in linear layers, setting the ratio between learning rate and weight decay as (cid:112)η/λ d0.75, where denotes model width, enables the top singular value to align across model widths. This, in turn, implies that sublayer gain remains approximately invariant across different widths. The observation leads to layerwise scaling rule: assign vector-like parameters learning rate of η1 = Θd(1) and weight decay of λ1 = 0, independent of model width. For matrix-like parameters, use learning rate scaling as η2 d1 and weight decay scaling as λ2 d. This layerwise scaling rule enables zero-shot hyperparameter transfer, in contrast to traditional scaling rules that require an optimal learning rate sweep at each model width. An illustrative example is also provided to demonstrate that square-root weight decay scaling can be observed in minimal models. The success of this approach may hint at mean-field-like behavior in large models, where training dynamics are well captured by random matrix theory. We remark that the concurrent work (Filatov et al., 2025) empirically observes that (near-)optimal training hyperparameters equalize operator norms across widths, mirroring our finding that maintaining widthinvariant sublayer gain yields robust transfer. Their mechanism differs from ours: rather than tuning weight decay, they adjust the batch size to match sublayer gain. Since batch size primarily controls the gradientnoise scale and thus acts as an implicit regularizer (Mandt et al., 2017; Welling and Teh, 2011), while weight decay constitutes explicit regularization , the two observations are closely related. Our explicit-regularization view leads to cleaner, layerwise scaling prescriptions (e.g., vectorvs. matrix-like parameters) and yields more directly interpretable rule for extrapolating across widths than batch-size tuning alone. Scope. Our results apply to AdamW and LLaMA architectures with fixed number of heads and FFN ratio. It is not obvious whether the observed scaling rule λ2 is universal across all architectures: mixture-of-experts architectures, alternatives to self-attention, or other architectural choices might alter the scaling factor, and this would be interesting to study. However, we believe that inspecting sublayer gain using singular value spectra and attempting to match the top singular value spectra is transferable procedure that may be adopted for extended research. Outlook. Future work includes extending the research to other optimizers (e.g., SGD with momentum, Adafactor), mixture-of-experts and structured-sparse models, and regimes where batch size or training tokens grow with width. It would also be promising direction to study how to scale hyperparameters when increasing model depth. Developing predictive link between data distribution, optimizer statistics, and spectral shape could turn the empirical law for (cid:112)η/λ into principled theory for steady-state transfer. Our illustrative example of two-layer feed-forward network can be seen as step in this direction. Perspective. We advocate studying LLM training as dynamical physical system, using tools from dynamical systems and statistical physics. In this view, theory plays role analogous to fluid mechanics: not exact in every detail, but predictive at the right scales. In practice, this means privileging models that explain and forecast observed phenomena, e.g., training at the edge of stability (Arora et al., 2022; Cohen et al., 2021), rotational steady states under decoupled weight decay (Kosson et al., 2023; Loshchilov and Hutter, 2019), and noise-driven effects captured by stochastic-thermodynamic views of SGD (Mandt et al., 2017; Welling and Teh, 2011), over exact-but-fragile formalisms. Echoing Boxs dictum that all models are wrong, but some are useful (Box, 1976), our aim is to develop useful, testable models of steady-state training dynamics that complement mathematical analysis. This work is step in that direction."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. Optuna: nextgeneration hyperparameter optimization framework. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 26232631, 2019. Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on the edge of stability in deep learning. In International Conference on Machine Learning, pages 9481024. PMLR, 2022. Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu Wei, and Xia Song. Scaling optimal lr across token horizons. arXiv preprint arXiv:2409.19913, 2024. Charlie Blake, Constantin Eichenberg, Josef Dean, Lukas Balles, Luke Prince, Bjorn Deiseroth, Andres Felipe Cruz-Salinas, Carlo Luschi, Samuel Weinbach, and Douglas Orr. u-µp: The unit-scaled maximal update parametrization. arXiv preprint arXiv:2407.17465, 2024. Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evolution in wide neural networks. Advances in Neural Information Processing Systems, 35:3224032256, 2022. Blake Bordelon and Cengiz Pehlevan. Deep linear network training dynamics from random initialization: Data, width, depth, and hyperparameter transfer. arXiv preprint arXiv:2502.02531, 2025. George E. P. Box. Science and statistics. Journal of the American Statistical Association, 71(356):791799, 1976. Zixiang Chen, Greg Yang, Qingyue Zhao, and Quanquan Gu. Global convergence and rich feature learning in l-layer infinite-width neural networks under µp parametrization. arXiv preprint arXiv:2503.09565, 2025. Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. Advances in neural information processing systems, 31, 2018. Lenaıc Chizat, Maria Colombo, Xavier Fernandez-Real, and Alessio Figalli. Infinite-width limit of deep linear neural networks. Communications on Pure and Applied Mathematics, 77(10):39584007, 2024. Jeremy Cohen, Simran Kaur, Yuanzhi Li, Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. arXiv preprint arXiv:2103.00065, 2021. Francesco DAngelo, Maksym Andriushchenko, Aditya Vardhan Varre, and Nicolas Flammarion. Why do we need weight decay in modern deep learning? Advances in Neural Information Processing Systems, 37: 2319123223, 2024. Aaron Defazio. Why gradients rapidly increase near the end of training. arXiv preprint arXiv:2506.02285, 2025. Nolan Dey, Shane Bergsma, and Joel Hestness. Sparse maximal update parameterization: holistic approach to sparse training dynamics. Advances in Neural Information Processing Systems, 37:3383633862, 2024. Nolan Dey, Bin Claire Zhang, Lorenzo Noci, Mufan Li, Blake Bordelon, Shane Bergsma, Cengiz Pehlevan, Boris Hanin, and Joel Hestness. Dont be lazy: Completep enables compute-efficient deep transformers. arXiv preprint arXiv:2505.01618, 2025. Stefan Falkner, Aaron Klein, and Frank Hutter. Bohb: Robust and efficient hyperparameter optimization at scale. In International conference on machine learning, pages 14371446. PMLR, 2018. Oleg Filatov, Jiangtao Wang, Jan Ebert, and Stefan Kesselheim. Optimal scaling needs optimal norm. arXiv preprint arXiv:2510.03871, 2025. 12 Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249 256. JMLR Workshop and Conference Proceedings, 2010. Eugene Golikov and Greg Yang. Non-gaussian tensor programs. Advances in Neural Information Processing Systems, 35:2152121533, 2022. Moritz Haas, Jin Xu, Volkan Cevher, and Leena Chennuru Vankadara. Effective sharpness aware minIn High-dimensional Learning Dynamics 2024: The imization requires layerwise perturbation scaling. Emergence of Structure and Reasoning, 2024. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing humanIn Proceedings of the IEEE international conference on level performance on imagenet classification. computer vision, pages 10261034, 2015. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Samuel Horvath, Aaron Klein, Peter Richtarik, and Cedric Archambeau. Hyperparameter transfer learning with adaptive complexity. In International conference on artificial intelligence and statistics, pages 1378 1386. PMLR, 2021. Tom Jacobs, Chao Zhou, and Rebekka Burkholz. Mirror, mirror of the flow: How does regularization shape implicit bias? arXiv preprint arXiv:2504.12883, 2025. Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. Kevin Jamieson and Ameet Talwalkar. Non-stochastic best arm identification and hyperparameter optimization. In Artificial intelligence and statistics, pages 240248. PMLR, 2016. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. Seijin Kobayashi, Yassir Akram, and Johannes Von Oswald. Weight decay induces low-rank attention layers. Advances in Neural Information Processing Systems, 37:44814510, 2024. Atli Kosson, Bettina Messmer, and Martin Jaggi. Rotational equilibrium: How weight decay balances learning across neural networks. arXiv preprint arXiv:2305.17212, 2023. Houyi Li, Wenzhen Zheng, Qiufeng Wang, Hanshan Zhang, Zili Wang, Shijie Xuyang, Yuantao Fan, Zhenyu Ding, Haoying Wang, Ning Ding, Shuigeng Zhou, Xiangyu Zhang, and Daxin Jiang. Predictable scale: Part optimal hyperparameter scaling law in large language model pretraining, 2025. URL https: //arxiv.org/abs/2503.04715. 13 Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hyperband: novel bandit-based approach to hyperparameter optimization. Journal of Machine Learning Research, 18(185): 152, 2018. Zhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. In International Conference on Learning Representations, 2020. arXiv:1910.07454. Lucas Lingle. large-scale exploration of µ-transfer. arXiv preprint arXiv:2404.05728, page 1, 2024. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. Stephan Mandt, Matthew Hoffman, and David Blei. Stochastic gradient descent as approximate bayesian inference. Journal of Machine Learning Research, 18(134):135, 2017. Song Mei, Andrea Montanari, and Phan-Minh Nguyen. mean field view of the landscape of two-layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665E7671, 2018. Meta AI. Llama 4: Advancing multimodal intelligence. llama-4-multimodal-intelligence/, llama-4-multimodal-intelligence/. Accessed: 2025-04-10. 2024. apr URL https://ai.meta.com/blog/ https://ai.meta.com/blog/ Saaketh Narayan, Abhay Gupta, Mansheej Paul, and Davis Blalock. µnit scaling: Simple and scalable fp llm training. arXiv preprint arXiv:2502.05967, 2025. Guilherme Penedo, Hynek Kydlıˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. Valerio Perrone, Rodolphe Jenatton, Matthias Seeger, and Cedric Archambeau. Scalable hyperparameter transfer learning. Advances in neural information processing systems, 31, 2018. Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, and Volkan Cevher. Training deep learning models with norm-constrained lmos. arXiv preprint arXiv:2502.07529, 2025a. Thomas Pethick, Wanyun Xie, Mete Erdogan, Kimon Antonakopoulos, Tony Silveti-Falls, and Volkan arXiv preprint Cevher. Generalized gradient norm clipping & non-euclidean (l 0, 1)-smoothness. arXiv:2506.01913, 2025b. Grant Rotskoff and Eric Vanden-Eijnden. Trainability and accuracy of artificial neural networks: An interacting particle system approach. Communications on Pure and Applied Mathematics, 75(9):18891935, 2022. Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization? Advances in neural information processing systems, 31, 2018. Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: law of large numbers. SIAM Journal on Applied Mathematics, 80(2):725752, 2020. Tao Sun, Yuhao Huang, Li Shen, Kele Xu, and Bao Wang. Investigating the role of weight decay in enhancing nonconvex sgd. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 15287 15296, 2025. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 14 Xi Wang and Laurence Aitchison. How to set adamws weight decay as you scale model and dataset size. arXiv preprint arXiv:2405.13698, 2024. Max Welling and Yee Teh. Bayesian learning via stochastic gradient langevin dynamics. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 681688, 2011. Kaiyue Wen, David Hall, Tengyu Ma, and Percy Liang. Fantastic pretraining optimizers and where to find them. arXiv preprint arXiv:2509.02046, 2025. Lechao Xiao. Rethinking conventional wisdom in machine learning: From generalization to scaling. arXiv preprint arXiv:2409.15156, 2024. Zeke Xie, Zhiqiang Xu, Jingzhao Zhang, Issei Sato, and Masashi Sugiyama. On the overlooked pitfalls of weight decay and how to mitigate them: gradient-norm perspective. Advances in Neural Information Processing Systems, 36:12081228, 2023. Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. Advances in neural information processing systems, 32, 2019. Ge Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zeroshot hyperparameter transfer. Advances in Neural Information Processing Systems, 34:1708417097, 2021. Greg Yang and Edward Hu. Feature learning in infinite-width neural networks. arXiv preprint arXiv:2011.14522, 2020. Greg Yang and Etai Littwin. Tensor programs ivb: Adaptive optimization in the infinite-width limit. arXiv preprint arXiv:2308.01814, 2023. Greg Yang, James Simon, and Jeremy Bernstein. spectral condition for feature learning. arXiv preprint arXiv:2310.17813, 2023. Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs VI: feature learning in infinite In The Twelfth International Conference on Learning Representations, ICLR depth neural networks. 2024, Vienna, Austria, May 7-11, 2024, 2024. Pan Zhou, Xingyu Xie, Zhouchen Lin, and Shuicheng Yan. Towards understanding convergence and generalization of adamw. IEEE transactions on pattern analysis and machine intelligence, 46(9):64866493, 2024. 15 Value / Configuration"
        },
        {
            "title": "A Hyperparameter Table",
            "content": "Setting Model & Data Architecture Dataset Compute Epochs Architecture LLaMA FineWeb H200 GPU 1 Model width Model depth MHA heads Count FFN expansion ratio Context length Attention scaling Various 8 16 8/3 1024 d1/2 base m1 AdamW hyperparameters Beta values Epsilon Schedule Training setup Grad-norm clip Dropout Batch size Steps Precision Parameterization Setting Initialization std Learning rate Weight decay (β1, β2) = (0.9, 0.95) ε = 108 1000 steps linear learning rate warmup cosine anneal to 0.01 the peak learning rate, if annealed 1.0 0 480 20,000 BF Base Type (vector) Type II (matrix) σbase = 2.0 102 Default ηbase = 1.0 103 Default λbase = 1.0 101 σ1 = σbase η1 = ηbase λ1 = 0 σ2 = σbase m1/2 η2 = ηbase m1 λ2 = λbase m1/2 d Table 3: Default experimental setup of experiments. Here md d/dbase with dbase = 256."
        },
        {
            "title": "B Other Figures",
            "content": "Figure 6: Statistics of the weight matrix Wout in the synthetic run, complementary to Figure 5."
        }
    ],
    "affiliations": [
        "Department of Computer Science, UCLA, CA, USA",
        "Department of Electrical Engineering and Computer Science, MIT, Cambridge, MA, USA"
    ]
}