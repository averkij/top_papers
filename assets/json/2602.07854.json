{
    "paper_title": "Geometry-Aware Rotary Position Embedding for Consistent Video World Model",
    "authors": [
        "Chendong Xiang",
        "Jiajun Liu",
        "Jintao Zhang",
        "Xiao Yang",
        "Zhengwei Fang",
        "Shizun Wang",
        "Zijun Wang",
        "Yingtian Zou",
        "Hang Su",
        "Jun Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \\textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \\textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \\textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs."
        },
        {
            "title": "Start",
            "content": "Geometry-Aware Rotary Position Embedding for Consistent Video World Model Chendong Xiang * 1 Jiajun Liu * 2 Jintao Zhang * 1 Xiao Yang 1 Zhengwei Fang 1 Shizun Wang 3 Zijun Wang 4 Yingtian Zou 5 Hang Su 1 Jun Zhu 1 6 2 0 2 7 1 ] . [ 2 4 5 8 7 0 . 2 0 6 2 : r Figure 1. Camera-controlled video generation with ViewRope. Up: generated video with camera trajectories with loop closure (rotate-away-rotate-back). Down: generated videos with high motion minecraft gaming. ViewRope maintains consistent scene appearance when the camera revisits previously observed viewpoints."
        },
        {
            "title": "Abstract",
            "content": "Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce ViewRope, geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides modelnative inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose Geometry-Aware Frame-Sparse Attention, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present ViewBench, diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs. of Comp. Sci. *Equal contribution 1Dept. and Tech., Institute for AI, BNRist Center, THBI Lab, Tsinghua-Bosch Joint ML Center, Tsinghua University 2Gaoling School of Artificial Intelligence, Renmin University of China 3National University of Singapore 4Peking University 5School of Artificial Intelligence, Shanghai Jiao Tong University. Correspondence to: Hang Su <suhangss@tsinghua.edu.cn>, Jun Zhu <dcszj@tsinghua.edu.cn>. Preprint. February 18, 2026. 1. Introduction Developing pose-conditioned visual world models predictive simulators that generate future observations under an explicit viewpoint trajectoryis fundamental to interactive AI systems (Ha & Schmidhuber, 2018; Hafner et al., 2023; Yang et al., 2025b; Zhang et al., 2025c). Despite remarkable progress in open-domain video diffusion models 1 Submission and Formatting Instructions for ICML 2026 (Wan et al., 2025; Kong et al., 2025; Google DeepMind, 2025; Bao et al., 2024), current generators fail to maintain long-term geometric consistency: as viewpoints evolve, they do not preserve stable scene structures that can be revisited. This deficiency becomes most apparent in loop-closure trajectories (e.g., rotate-away-rotate-back), where the camera returns to previously observed viewpoint after traversing elsewhere. consistent world model should reconstruct identical structures and appearances upon revisiting. Instead, existing generators frequently hallucinate new details or drift, revealing the absence of reliable mechanism for retaining and retrieving 3D-consistent content over time. Existing approaches typically address this challenge through two strategies. The first enlarges context via external retrieval or memoryselecting historical frames based on field-of-view overlap or maintaining explicit 3D spatial structures (Yu et al., 2025a; Wu et al., 2025; Huang et al., 2025a; Oshima et al., 2025). However, these mechanisms rely on pixel-level concatenation or external data structures rather than being integrated into the models internal representation natively; this often incurs substantial compute and can become brittle when histories are long or camera motion is complex. The second employs geometry-first pipelines such as 3D Gaussian Splatting (Kerbl et al., 2023) and specialized novel-view synthesis transformers (Jin et al., 2024; Gao et al., 2024a; Li et al., 2025b)which enforce strict 3D consistency but typically sacrifice open-domain flexibility. We trace this failure to bottleneck in positional modeling. Most video transformers parameterize spacetime structure in screen coordinates (x, y, t) via learned absolute/relative embeddings (Su et al., 2023). However, under camera rotation and translation, correspondence is dictated by projective geometry: the same 3D point can map to widely separated image locations across time, and conversely, nearby pixels need not be co-visible. As result, screen-space positional bias is misaligned with the invariances required for view-consistent generation, inducing geometric drift that compounds over long trajectories and becomes most evident at loop closure when the camera revisits previously observed viewpoints (Sec. 4). The central challenge is thus to equip transformers with mechanism to identify and reuse the same physical content across temporally distant tokens whose image-plane coordinates are decorrelated by camera motionwithout resorting to explicit 3D scene reconstruction or compromising open-domain generative flexibility. Our key insight is that long-horizon view consistency is governed by angular correspondence of viewing directions, rather than locality in the image planeand that this geometric prior can be embedded directly into the attention mechanism without external memory structures. Under calibrated camera motion, two tokens are likely to be informative to each other when their associated camera rays are co-visible (i.e., intersect the same physical content), even if they are separated by long temporal gaps and occupy unrelated pixel coordinates. Motivated by this principle, we propose ViewRope, geometry-aware positional encoding that injects patch-level viewing-ray directions into self-attention through ray-based rotary transformations of the query/key features. In contrast to standard 2D/3D RoPE, which encodes pixel-space offsets, ViewRope parameterizes attention as function of relative ray geometry, yielding modelnative inductive bias for retrieving and reusing consistent 3D content from long histories. Unlike explicit memory approaches that maintain external data structures, ViewRope realizes geometric correspondence implicitly through attention itself, offering lightweight and complementary mechanism. To further support long-context generation, we introduce Geometry-Aware Frame-Sparse Attention, which leverages geometry-conditioned relevance to select small set of co-visible historical frames, replacing quadratic dense attention with geometry-driven sparsity while preserving loop-closure fidelity. To evaluate view consistency directly, we introduce ViewBench, diagnostic benchmark tailored to cameraconditioned long-horizon generation. Unlike generic perceptual metrics (e.g., FVD/IS) that primarily measure frame quality, ViewBench targets loop-closure trajectories such as rotate-awayrotate-back and quantifies revisit fidelity and geometric drift. Experiments show that our approach substantially improves view-consistency on ViewBench while remaining efficient, narrowing the gap between geometryrigid 3D pipelines and open-domain diffusion generators. Our contributions are summarized as follows: ViewRope: geometric positional encoding that injects patch-level camera-ray directions into attention, yielding model-native inductive bias for long-term geometric consistency. Geometry-Aware Frame-Sparse Attention: an efficient, geometry-conditioned retrieval mechanism that selects covisible historical frames, enabling consistent long-video generation with low latency. ViewBench: targeted evaluation suite for quantifying view-consistency and loop-closure behavior in cameraconditioned video generation models. 2. Related Work 2.1. Conditioning Transformers on Camera Geometry Camera conditioning is essential for binding geometric viewpoint information to visual tokens in multiview and video transformers. dominant approach is to encode camera parameters into raymapsper-pixel 6D embeddings containing ray origins and directions or Plucker coordi2 Submission and Formatting Instructions for ICML nates (Mildenhall et al., 2020; Zhang et al., 2024; Gao et al., 2024b; Jin et al., 2024; Weber et al., 2025). Concatenating these raymaps to input tokens allows models to condition on both intrinsics and extrinsics (Zhang et al., 2024). However, raymaps typically rely on global reference frame, which can be arbitrary and hinder generalization (Mildenhall et al., 2019; Guizilini et al., 2024). To mitigate this, recent methods employ relative attentionlevel encodings that leverage the geometric relationship between views. CaPE and GTA embed relative SE(3) pose by applying transformations directly to attention mechanisms (Safin et al., 2023; Miyato et al., 2024; Kong et al., 2024). PRoPE (Li et al., 2025b) models the complete relative projective transformation, encoding both camera intrinsics and extrinsics within the attention mechanism to better ground visual tokens in 3D space. While these methods improve view synthesis and avoid global frame dependency, they assume shared camera poses for all pixels within view, lacking fine-grained geometric modeling. 2.2. Interactive World Models Recent interactive world models enable controllable simulation of physical environments by conditioning video generation on user actions and historical context (Che et al., 2024; Zhang et al., 2025c; Li et al., 2025a). To support realtime interaction, the field has seen paradigm shift from bidirectional diffusion to causal or autoregressive architectures (Valevski et al., 2024; Yin et al., 2025; Yang et al., 2025a; Huang et al., 2025b), often utilizing KV-caching and distillation to accelerate inference. While scaling training on massive gameplay datasets enables foundation models like Matrix-Game (Zhang et al., 2025c) and HunyuanGameCraft (Li et al., 2025a) to achieve high-dynamic controllability, maintaining long-term spatial consistency particularly during scene revisitsremains critical bottleneck (Lian et al., 2025). To address this limitation, recent works introduce explicit memory mechanisms. Context-as-Memory (Yu et al., 2025a) retrieves historical frames based on field-of-view overlap and concatenates them into the generation context. Memory Forcing (Huang et al., 2025a) incorporates geometry-indexed spatial memory to enforce coherence across extended horizons. More ambitiously, Wu et al. (Wu et al., 2025) propose augmenting world models with explicit 3D point-cloud memory inspired by human spatial cognition, while WorldPack (Oshima et al., 2025) compresses trajectory history via packing and selective retrieval. These memory-based approaches demonstrate improved loop-closure consistency but rely on external data structures that are not native to the attention mechanism. Our work instead embeds geometric correspondence directly into positional encoding, enabling implicit memory retrieval through attention without auxiliary modules. 2.3. Sparse Attention with Long Sequence The quadratic complexity of the self-attention mechanism with respect to sequence length poses significant challenge for modeling long sequences. To address this bottleneck, recent works have explored sparse attention mechanisms that reduce computational cost by attending to only subset of tokens. In both language and vision domains, various sparse attention methods have been proposed. These approaches typically rely on learnable schemes (Gao et al., 2025; DeepSeek-AI et al., 2025), pattern-based selection (Lai et al., 2025; Xi et al., 2025), or low-cost dynamic estimation (Zhang et al., 2025b;a; Zhu et al., 2025b;a). However, in the setting of Autoregressive Diffusion for video generation, sparse attention remains relatively underexplored. Current state-of-the-art approaches typically rely on Sliding Window Attention to handle long temporal sequences. For instance, LongLive (Yang et al., 2025a) utilizes short window attention combined with frame sinks to maintain efficiency and consistency during real-time interactive long video generation. 3. Method 3.1. Problem Formulation We study pose-conditioned video generation as visual world model. Given an initial observation x0 (or short context x0) and target camera trajectory, the model generates future video x1:T that is consistent with the specified viewpoint evolution. Let C1:T := {(Rt, Pt, Kt)}T t=1 denote the camera trajectory, where Rt SO(3), Pt R3, and Kt R33 represent camera rotation, translation, and intrinsics at time t, respectively. Optionally, we map the trajectory to low-level action prompt (e.g., turn, move) and provide global text description of the scene. We denote the resulting text/action conditioning by Y. Our goal is to learn the conditional distribution pθ(x1:T x0, C1:T , Y). (1) Standard video generators mainly enforce local temporal coherence, which only constrains adjacent frames. For generic photometric/perceptual distance d(, ), this is captured by Ltemp(θ) := Expθ (cid:35) d(xt, xt1) . (2) (cid:34) (cid:88) t=2 Such objectives do not prevent long-horizon geometric drift under camera motion, because screen-space proximity is not aligned with physical correspondence. 3 Submission and Formatting Instructions for ICML 2026 In contrast, pose-conditioned world model must satisfy loop-closure requirement: if the camera at time revisits (approximately) viewpoint observed at some t, then the rendered observations should agree up to projective geometry on their co-visible region. Concretely, define the revisit indicator wt,k := I((Ct, Ck) ε) , < t, (3) where (, ) measures pose similarity (e.g., rotation/ translation discrepancy under calibrated intrinsics) and ε is tolerance threshold. When wt,k = 1, the generated frames xt and xk should be photometrically consistent on their covisible region Ωt,k. Let Wkt denote the projective warp from to (see Appendix for details). We formalize this via loop-closure loss: with the camera extrinsic rotation Rcam aligned view rotation: , we obtain worldRi,u,v = Rcam Rlocal i,u,v. (6) Rotating query/key feature subvectors. Let Rd and Rd be query/key vectors for token. We reserve subset of channels that can be grouped into 3D subvectors (so 3m d), and rotate each subvector by Ri,u,v: VR(q, Ri,u,v) = q, (7) where 3ℓ:3ℓ+3 = Ri,u,vq3ℓ:3ℓ+3, ℓ = 0, . . . , 1. We apply the same transformation to keys. Intuitively, this aligns portion of the feature space with the physical viewing direction of each patch in world coordinates. Llc(θ) := Expθ (cid:20) (cid:88) (cid:88) wt,k t=1 k<t ρ(xt(u) xk(Wkt(u))) (4) (cid:21) , (cid:88) uΩt,k where ρ() is robust penalty. The difficulty is that this loop-closure objective couples frames across arbitrarily long temporal gaps: achieving loop closure requires retrieving geometrically corresponding content from long history under causal (streaming) generation. Rather than explicitly optimizing pixel-level consistency loss, we parameterize pθ with Diffusion Transformer (DiT) and inject 3D view geometry into its attention mechanism, so that (i) attention scores become sensitive to relative viewing rays, and (ii) the model can efficiently select and attend to the most geometrically relevant historical frames while generating online. 3.2. ViewRope: View-Centric Positional Encoding in Attention We introduce View-centric Position Encoding (ViewRope), encoding each tokens 3D viewing direction directly into the attention mechanism. Unlike 2D/3D positional embeddings or global pose tokens, ViewRope assigns per-patch rotation derived from camera intrinsics/extrinsics, so attention can operate on relative view geometry at patch granularity. Per-patch ray construction. For patch centered at pixel coordinates (u, v) in camera/view i, we compute its normalized viewing ray ri,u,v S2 (the unit sphere) in the camera coordinate system using intrinsics Ki: ri,u,v = K1 K1 [u, v, 1] [u, v, 1]2 . (5) We then build local rotation Rlocal i,u,v SO(3) that maps the canonical optical axis = [0, 0, 1] to ri,u,v. Combining 4 Geometry-aware attention scores. Consider query token from view at (ui, vi) and key token from view at (uj, vj). Their rotated dot product becomes (cid:10)VR(q, Ri,ui,vi ), VR(k, Rj,uj ,vj )(cid:11) Rj,uj ,vj = (cid:0)R1 = qR Rj,uj ,vj (cid:1) k. (8) i,ui,vi i,ui,vi The relative rotation R1 Rj,uj ,vj captures the angular relationship between the two viewing rays. This makes attention naturally sensitive to 3D view similarity, improving long-range recall and loop closure consistency. i,ui,vi 3.3. Geometry-Aware Frame Sparse Attention Long-context generation with dense attention scales quadratically with sequence length. To support streaming worldmodeling over many frames, we adopt frame-aligned block-sparse attention scheme inspired by SampleAttention (Zhu et al., 2025a). The key design is to set the block size to exactly match one latent frame, so blocks correspond to time steps. As ViewRope encodes 3D viewing geometry in the attention space, we can directly estimate frame-level geometric relevance. Let Q, K, RLD be token sequences, partitioned into blocks of size (L = B). Denote block as Qi RBD and block as Kj, Vj RBD. Block relevance estimation (stochastic). Instead of computing full block-to-block attention, we sample small set of token indices {1, . . . , B} of size Ks (e.g., Ks = 10) and estimate head-averaged affinity: Sij = 1 HKs (cid:88) (cid:88) h=1 sS i,s )k(h) (q(h) j,s . (9) We then apply causal constraint via block mask Mcausal (disallowing > in the streaming setting) by setting Sij = when causal = 0. ij Submission and Formatting Instructions for ICML 2026 Figure 2. Method overview. (a) ViewRope computes per-patch viewing rays from intrinsics, constructs local rotations, and rotates query/key feature subvectors in attention. The resulting dot product encodes relative angular relationships between viewing rays. (b) Geometry-Aware Frame Sparse Attention estimates block (frame) relevance and selects top-k geometrically relevant historical frames, replacing quadratic dense attention with geometry-driven sparsity. Top-k block selection. For each query block i, we select the top-k key blocks under Sij among valid past blocks: Ti = TopK(cid:0){ Sij}j: causal ij = (cid:1). (10) We always include = to preserve local context. The final sparsity mask is Mij = (cid:40) 1 if (j Ti or = i) causal 0 otherwise. ij = 1, (11) Sparse attention computation. We compute attention for block only over selected blocks: (cid:32) Oi = softmax Qi (K{j Mij =1}) (cid:33) V{j Mij =1}. (12) With fixed k, the attention cost scales linearly with the number of frames, enabling efficient long-horizon generation. We implement the sparse attention kernel with TileLang (Wang et al., 2025), following the optimization principles of FlashAttention (Dao, 2023). Training vs. inference (streaming with cache). We utilize teacher-forcing (Arriola et al., 2025) to enbale ARgeneration. During training, historical frames are taken from ground-truth latents, forming clean KV cache; the current denoising step queries this cache with ViewRope-rotated features and top-k frame mask. During autoregressive inference, we maintain KV cache of previously generated latent frames and apply the same relevance estimation and top-k selection at each denoising step, preserving causality while retrieving geometrically relevant history. The detailed training and inference procedures are shown in Algorithm 1. 3.4. Progressive Training Pipeline To stabilize adaptation to autoregressive streaming generation and long contexts, we employ progressive schedule: Stage I: Short-clip teacher forcing. Train on short clips (e.g., 17 frames) with teacher forcing to align the model with the autoregressive generation interface and caching behavior. Stage II: Introduce ViewRope. Enable ViewRope while keeping clips short, allowing the model to learn viewconditioned correspondence without the confound of very long contexts. Stage III: Enable Frame Sparse Attention. Activate frame-aligned block sparsity to adapt the model to long-context retrieval efficiently, while keeping sequence lengths moderate. Stage IV: Scale context length. Increase training sequence length substantially under sparse attention, endowing the model with long-horizon video generation and improved loop-closure consistency. 4. Experiments We conduct experiments to validate the effectiveness of ViewRope. All experiments use the same backbone, training budget, and data to ensure fair comparison. 4.1. Experimental Setup Datasets and Benchmarks. We evaluate on ViewBench, diagnostic benchmark we construct to systematically evaluate view-consistency under camera motion. Existing datasets hold limitations for expected and unified evaluation: Context-as-Memory (Yu et al., 2025a) constrains camera motion to yaw-only rotation with pitch fixed at zero, preventing evaluation of general 3D rotations; GF-Minecraft (Yu et al., 2025b) focuses on action controllability without GT Submission and Formatting Instructions for ICML 2026 Table 1. Comparison with existing datasets. ViewBench fills gaps in evaluating view-consistency for interactive world models. Property CaM GF-MC ViewBench Yaw Pitch Roll Loop-closure traj. Controlled angles geometric overlap annotations required for attention analysis. Table 1 summarizes these differences. ViewBench addresses these gaps with: (i) complete 3-axis rotation coverage (yaw, pitch, roll) with systematic angle sampling; (ii) round-trip loop closure (Lian et al., 2025) trajectories where the camera returns to previously visited viewpoints; (iii) 10 photorealistic UE5 environments spanning indoor, outdoor, urban, and natural settings. The training set contains 1k+ video sequences (500k frames), and the evaluation set consists of 600 separately collected samples with non-overlapping trajectories. See Appendix for details. Metrics. We report metrics across three categories: 1)Visual Quality: PSNR, SSIM, LPIPSstandard metrics for frame-level reconstruction quality; 2) Loop Closure Error (LCE): LPIPS between the starting frame x0 and the generated frame xt upon returning to the start pose, directly measuring persistent spatial memory. Lower is better. Baselines. We compare against two categories of methods: 1) 3D RoPE (Su et al., 2023): Standard temporal-spatial RoPE without camera geometry, serving as no-geometry baseline; 2) GTA (Miyato et al., 2024): Relative SE(3) transformation applied to attention, encoding extrinsics only. Furthermore, we investigate the integration of ViewRope with various sparse attention mechanisms, including our proposed Geometry-Aware Sparse Attention and sliding window attention, to validate the effectiveness of our algorithm and demonstrate the efficiency improvements achieved through sparse attention patterns. Implementation Details. We build upon WAN 2.2 TI2V5B (Wan et al., 2025), text-and-image-to-video diffusion transformer, and adapt it for streaming video generation via teacher-forcing training. The training data combines Context-as-Memory (Yu et al., 2025a), GF-Minecraft (Yu et al., 2025b), and ViewBench at 1:1:1 sampling ratio. All RoPE variants are applied to the same channels for fair comparison. The ViewBench evaluation set is separately collected with non-overlapping trajectories. See Appendix A.1 for detailed training configurations. 4.2. View Consistency Comparison Table 2 presents the quantitative comparison on ViewBench. We make three key observations: (1) ViewRope achieves the best loop closure performance. ViewRope reduces 6 LCE by 4% compared to GTA, the strongest baseline. This demonstrates that per-patch ray encoding provides more precise geometric alignment than per-camera projective encoding when revisiting previous viewpoints. (2) Geometryaware encoding consistently outperforms absolute encoding. Both GTA and ViewRope outperform 3D RoPE, confirming the findings of prior work (Li et al., 2025b; Miyato et al., 2024) that relative geometric relationships are more effective than absolute coordinates. (3) ViewRope maintains competitive visual quality. Despite focusing on geometric consistency, ViewRope achieves comparable or better PSNR/SSIM than baselines, indicating that the geometric inductive bias does not sacrifice generation quality. Comparison with State-of-the-Art Interactive World Models. We further compare ViewRope with two leading interactive world model systems: Matrix-Game-2 (Zhang et al., 2025c) and HY-WorldPlay (Li et al., 2025a). ViewRope consistently outperforms both baselines across all evaluated rotation magnitudes (3075). The improvement is most pronounced in loop-closure error (LCE): ViewRope reduces LCE by 6.5% compared to HY-WorldPlay at 30, 7.9% at 45, and 11.4% at 75, demonstrating that per-patch geometric encoding provides stronger spatial memory than action-conditioned approaches. Notably, the performance gap widens with increasing rotation angle, suggesting that ViewRopes ray-based attention becomes more beneficial for larger camera excursions where geometric correspondence is critical. Qualitative comparisons and extended results including large-angle (90180) trajectories are provided in Appendix C.7. 4.3. Efficiency of Geometry-Aware Sparse Attention Experiments setup. To validate sparse attention, we continue training the models from Section 4.2 on 61-frame sequences with various sparse attention mechanisms (topk=5) for 6k steps, then on 201-frame sequences for 2k steps to enable longer sequence generation. We evaluate on 90 and 180 scenarios which require longer sequences. Table 3 shows the results of sparse attention comparison on ViewBench. We make two key observations: (1) ViewRope w/ Sparse consistently outperforms other methods. ViewRope w/ Sparse outperforms all baselines, reducing LCE by 16% compared to sliding window attention. This demonstrates that our geometry-aware sparse attention mechanism is more effective for long-sequence view synthesis. (2) ViewRope stabilizes sparse training. We found that both naıve sparse attention (without geometric encoding) and GTA w/ Sparse suffer from loss divergence during training, whereas ViewRope w/ Sparse maintains stable convergence throughout. We attribute this stability to ViewRopes ray-based rotations, which impose geometrically meaningful structure on the Q/K dot products used for Submission and Formatting Instructions for ICML 2026 Table 2. Position encoding comparison on ViewBench. We report visual quality (PSNR, SSIM, LPIPS) and geometric consistency (LCE) for 30 and 75 view synthesis. Best in bold. Method 30 deg 75 deg PSNR SSIM LPIPS LCE PSNR SSIM LPIPS LCE 3D RoPE GTA ViewRope (Ours) 17.09 17.33 17.53 0.4133 0.4325 0.4378 0.4219 0.4165 0.4080 0.4929 0.4707 0.4497 14.78 15.12 15. 0.3634 0.3784 0.3916 0.5501 0.5403 0.5398 0.4831 0.4723 0.4562 Table 3. Sparse attention comparison on ViewBench. We report visual quality (PSNR, SSIM, LPIPS) and geometric consistency (LCE) for 90 and 180 view synthesis. Best in bold. Method 90 deg 180 deg PSNR SSIM LPIPS LCE PSNR SSIM LPIPS LCE Sparse w/o ViewRope GTA w/ Sparse ViewRope w/ Sliding Window ViewRope w/ Sparse (Ours) 10.97 8.603 15.20 15.61 0.080 0.0755 0.3701 0.4001 0.8887 0.8316 0.5513 0.5382 0.8932 0.8020 0.6543 0.5445 9.937 9.275 14.44 14.35 0.0618 0.078 0.3406 0. 0.9286 0.8062 0.6139 0.6043 0.9243 0.7924 0.6598 0.5609 relevance scoring (Eq. 9). This structure yields more reliable frame selection and, consequently, more stable gradient signals during sparse training. Counterfactual Validation. To verify that our sparse selection is causally meaningful (not just random sparsity that happens to work), we conduct counterfactual experiments where we alter the selection strategy: 1) Random Selection: Select frames randomly from the entire history; 2)Exclude Selected: Explicitly exclude the top-k frames identified by ViewRope, and randomly select frames from the remaining history. Table 4 shows that random selection causes 25.2% LCE degradation. Crucially, when we explicitly exclude the frames identified as important by ViewRope (Exclude Selected), performance degrades even further by 38.1%. This confirms that our geometry-aware selection identifies the specific frames that are causally necessary for consistency. Table 4. Counterfactual validation. Explicitly excluding the frames selected by ViewRope causes the largest performance drop, confirming their causal importance. Condition 180 deg LCE LCE Normal (Geo-Sparse) Random Selection Exclude Selected 0.5609 0.7027 0.7744 0.0% +25.2% +38.1% Figure 3. Visualization of attention specialization. Left: standard temporal head focuses on recent or temporally periodic frames. Middle: geometry-aware head captures long-range spatial overlap (evident in the antidiagonal activation during loop closure). Right: The aggregated attention map illustrates how geometric cues guide sparse block selection. 7 Visualizing Geometric Relationships. To further substantiate that the model learns meaningful geometric relationships, we visualize the attention maps for loop closure sequence in Figure 3. We observe that different attention heads specialize in distinct patterns: while common heads (left) focus on temporal locality, specific geometry-sensitive heads (middle) emerge to capture spatial overlap. Notably, these geometry heads exhibit high activation for temporally distant but spatially aligned frames. This geometric signal is successfully integrated into the final estimated attention map (right), thereby guiding the block selection mechanism to retrieve the correct historical context. Efficiency Comparison. We evaluate computational efficiency. Sparse attention (top-k = 5) reduces training time from 27.66 s/iter to 22.01 s/iter on 201-frame sequences, achieving 25% acceleration compared to dense attention. Case Study. Figure 4 shows the cases where viewrope w/ sparse outperforms other methods. In Figure 4(a), the green column on the right side disappears after the turn under the sliding window method, while ViewRope maintains it well. In Figure 4(b), the blue wall on the right side becomes blurry and exhibits noticeable drift and hallucinated details under the sliding window method, whereas ViewRope accurately recovers the original scene structure. This demonstrates that ViewRopes geometry-aware sparse attention mechanism is more effective for long-sequence geometry-aware retrieval. 4.4. Ablation Studies Channel Allocation for ViewRope. Efficiently integrating ViewRope into the existing 3D RoPE architecture is key design challenge. The original model partitions RoPE into Temporal (T), Height (H), and Width (W) components, occupying 44, 42, and 42 dimensions respectively, totaling 128 dimensions. We investigate four strategies for embedding ViewRope: (1) Embedding in the lowest frequency bands of the dimension (channels 3244); (2) Embedding in the lowest frequency bands of and dimenSubmission and Formatting Instructions for ICML 2026 Figure 4. Case study. Upper and lower sequences show ViewRope with Sliding Window and Sparse attention, respectively. Table 5. Ablation: ViewRope embedding strategies. We compare different strategies for integrating ViewRope into the feature channels. Embedding in T-dimension low frequencies yields the best performance. Embedding Strategy Training Loss generally improves visual quality (PSNR, SSIM) by accessing more texture details, but geometric consistency (LCE) peaks at the training setting (k = 5), suggesting trade-off between texture richness and geometric precision. T-dim low-freq (ch 3244) H/W-dim low-freq (ch 7486, 116128) H/W-dim low-freq (replace 3D RoPE) All dimensions (ch 1128) 0.0859 0.0861 0.0874 0.0894 sions (channels 7486 and 116128); (3) Embedding in the lowest-frequency bands of and W, while disabling the corresponding 3D RoPE components to prevent interference; (4) Distributing ViewRope across all dimensions (1128). We evaluate these strategies using training loss as the metric. Table 5 presents the ablation results for these embedding schemes. Embedding in the lowest frequency bands of the dimension yields the best performance (0.0859), suggesting that the temporal dimension offers the most suitable capacity for encoding view information without disrupting critical spatial feature representations governed by the and dimensions. Interestingly, simply replacing the original 3D RoPE components with ViewRope leads to performance degradation (0.0874 vs. 0.0861), indicating that the original positional information remains complementary to our relative geometric encoding. Finally, distributing ViewRope across all dimensions results in the highest training loss (0.0894), likely due to the excessive interference with the backbones pre-trained frequency structure. Number of Retrieved Frames. Due to space constraints, we provide the ablation study on the number of retrieved frames (k) in Appendix D.1. We observe that increasing 5. Conclusion and Future Work In this work, we introduced ViewRope, novel geometric positional encoding that effectively bridges the gap between 3D consistency and generative flexibility in video diffusion models. By treating view as position and embedding camera ray directions directly into the attention mechanism, our method enables the model to maintain long-term spatial persistence and geometric consistency, particularly in challenging loop-closure scenarios. Furthermore, our proposed Geometry-Aware Sparse Attention significantly improves computational efficiency by selectively attending to geometrically relevant historical frames, making longvideo generation feasible without sacrificing performance. Experimental results on our newly proposed ViewBench demonstrate that our approach achieves state-of-the-art consistency while significantly reducing computational costs compared to dense attention baselines. Despite these advancements, our method has limitations. First, it may struggle with drastic scene transitions, such as moving from one room to another, where geometric correspondences between views are weak or nonexistent. Integrating explicit 3D modeling with generative approaches and combining them with implicit representations remains to be explored. Further post-training with distillation and RL to support more dynamic scenarios and longer sequence generation remains as future research. 8 Submission and Formatting Instructions for ICML 2026 6. Impact Statement This paper advances machine learning for interactive world models. Our view-consistent video generation could enhance VR/AR, entertainment, education, and training applications. However, high-quality video generation raises concerns about deepfake misuse and requires careful consideration of copyright and privacy protection in virtual content creation. We acknowledge the need for responsible development of such technologies."
        },
        {
            "title": "References",
            "content": "Arriola, M., Gokaslan, A., Chiu, J. T., Yang, Z., Qi, Z., Han, J., Sahoo, S. S., and Kuleshov, V. Block diffusion: Interpolating between autoregressive and diffusion language models, 2025. URL https://arxiv.org/ abs/2503.09573. Bao, F., Xiang, C., Yue, G., He, G., Zhu, H., Zheng, K., Zhao, M., Liu, S., Wang, Y., and Zhu, J. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models, 2024. URL https://arxiv. org/abs/2405.04233. Che, H., He, X., Liu, Q., Jin, C., and Chen, H. Gamegenx: Interactive open-world game video generation, 2024. URL https://arxiv.org/abs/2411.00769. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning, 2023. DeepSeek-AI, Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., Lu, C., Zhao, C., Deng, C., Xu, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Li, E., Zhou, F., Lin, F., Dai, F., Hao, G., Chen, G., Li, G., Zhang, H., Xu, H., Li, H., Liang, H., Wei, H., Zhang, H., Luo, H., Ji, H., Ding, H., Tang, H., Cao, H., Gao, H., Qu, H., Zeng, H., Huang, J., Li, J., Xu, J., Hu, J., Chen, J., Xiang, J., Yuan, J., Cheng, J., Zhu, J., Ran, J., Jiang, J., Qiu, J., Li, J., Song, J., Dong, K., Gao, K., Guan, K., Huang, K., Zhou, K., Huang, K., Yu, K., Wang, L., Zhang, L., Wang, L., Zhao, L., Yin, L., Guo, L., Luo, L., Ma, L., Wang, L., Zhang, L., Di, M. S., Xu, M. Y., Zhang, M., Zhang, M., Tang, M., Zhou, M., Huang, P., Cong, P., Wang, P., Wang, Q., Zhu, Q., Li, Q., Chen, Q., Du, Q., Xu, R., Ge, R., Zhang, R., Pan, R., Wang, R., Yin, R., Xu, R., Shen, R., Zhang, R., Liu, S. H., Lu, S., Zhou, S., Chen, S., Cai, S., Chen, S., Hu, S., Liu, S., Hu, S., Ma, S., Wang, S., Yu, S., Zhou, S., Pan, S., Zhou, S., Ni, T., Yun, T., Pei, T., Ye, T., Yue, T., Zeng, W., Liu, W., Liang, W., Pang, W., Luo, W., Gao, W., Zhang, W., Gao, X., Wang, X., Bi, X., Liu, X., Wang, X., Chen, X., Zhang, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yu, X., Li, X., Yang, X., Li, X., Chen, X., Su, X., Pan, X., Lin, X., Fu, X., Wang, Y. Q., Zhang, Y., Xu, Y., Ma, Y., Li, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Qian, Y., Yu, Y., Zhang, Y., Ding, Y., Shi, Y., Xiong, Y., He, Y., Zhou, Y., Zhong, Y., Piao, Y., Wang, Y., Chen, Y., Tan, Y., Wei, Y., Ma, Y., Liu, Y., Yang, Y., Guo, Y., Wu, Y., Wu, Y., Cheng, Y., Ou, Y., Xu, Y., Wang, Y., Gong, Y., Wu, Y., Zou, Y., Li, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Wu, Z. F., Ren, Z. Z., Zhao, Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Gou, Z., Ma, Z., Yan, Z., Shao, Z., Huang, Z., Wu, Z., Li, Z., Zhang, Z., Xu, Z., Wang, Z., Gu, Z., Zhu, Z., Li, Z., Zhang, Z., Xie, Z., Gao, Z., Pan, Z., Yao, Z., Feng, B., Li, H., Cai, J. L., Ni, J., Xu, L., Li, M., Tian, N., Chen, R. J., Jin, R. L., Li, S. S., Zhou, S., Sun, T., Li, X. Q., Jin, X., Shen, X., Chen, X., Song, X., Zhou, X., Zhu, Y. X., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Huang, Z., Xu, Z., Zhang, Z., Ji, D., Liang, J., Guo, J., Chen, J., Xia, L., Wang, M., Li, M., Zhang, P., Chen, R., Sun, S., Wu, S., Ye, S., Wang, T., Xiao, W. L., An, W., Wang, X., Sun, X., Wang, X., Tang, Y., Zha, Y., Zhang, Z., Ju, Z., Zhang, Z., and Qu, Z. Deepseek-v3.2: Pushing the frontier of open large language models, 2025. URL https://arxiv.org/abs/2512.02556. Gao, R., Holynski, A., Henzler, P., Brussee, A., MartinBrualla, R., Srinivasan, P., Barron, J. T., and Poole, B. Cat3d: Create anything in 3d with multi-view diffusion models. arXiv preprint arXiv:2405.10314, 2024a. Gao, R., Holynski, A., Henzler, P., Brussee, A., MartinBrualla, R., Srinivasan, P., Barron, J. T., and Poole, B. Cat3d: Create anything in 3d with multi-view diffusion models, 2024b. URL https://arxiv.org/abs/ 2405.10314. Gao, Y., Zeng, Z., Du, D., Cao, S., Zhou, P., Qi, J., Lai, J., So, H. K.-H., Cao, T., Yang, F., and Yang, M. Seerattention: Learning intrinsic sparse attention in your llms, 2025. URL https://arxiv.org/abs/ 2410.13276. Google DeepMind. Veo 3 technical report. Technical report, Google DeepMind, 2025. URL https://storage. googleapis.com/deepmind-media/veo/ Veo-3-Tech-Report.pdf. Guizilini, V., Vasiljevic, J., Ambrus, R., Shakhnarovich, G., Johnson-Roberson, M., and Gaidon, A. Zero-shot novel view synthesis with large-scale diffusion models, 2024. I., Fang, Ha, D. and Schmidhuber, J. World models. arXiv preprint arXiv:1803.10122, 2(3), 2018. Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. Submission and Formatting Instructions for ICML 2026 Huang, J., Hu, X., Han, B., Shi, S., Tian, Z., He, T., and Jiang, L. Memory forcing: Spatio-temporal memory for consistent scene generation on minecraft, 2025a. URL https://arxiv.org/abs/2510.03198. Mildenhall, B., Srinivasan, P. P., Ortiz-Cayon, R., Kalantari, N. K., Ramamoorthi, R., Ng, R., and Kar, A. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines, 2019. Huang, X., Li, Z., He, G., Zhou, M., and Shechtman, E. Self forcing: Bridging the train-test gap in autoregressive video diffusion, 2025b. URL https://arxiv.org/ abs/2506.08009. Jin, H., Lee, D., Gu, J., Zou, J., Feng, Y., Wu, Y., Li, W., Jiang, X., Chen, A., Zhang, H., and Liang, X. Lvsm: Large video-to-3d synthesis model, 2024. Kerbl, B., Kopanas, G., Leimkuhler, T., and Drettakis, G. 3d gaussian splatting for real-time radiance field rendering, 2023. URL https://arxiv.org/abs/2308. 04079. Kong, W., Tian, Q., Zhang, Z., Min, R., Dai, Z., Zhou, J., Xiong, J., Li, X., Wu, B., Zhang, J., Wu, K., Lin, Q., Yuan, J., Long, Y., Wang, A., Wang, A., Li, C., Huang, D., Yang, F., Tan, H., Wang, H., Song, J., Bai, J., Wu, J., Xue, J., Wang, J., Wang, K., Liu, M., Li, P., Li, S., Wang, W., Yu, W., Deng, X., Li, Y., Chen, Y., Cui, Y., Peng, Y., Yu, Z., He, Z., Xu, Z., Zhou, Z., Xu, Z., Tao, Y., Lu, Q., Liu, S., Zhou, D., Wang, H., Yang, Y., Wang, D., Liu, Y., Jiang, J., and Zhong, C. Hunyuanvideo: systematic framework for large video generative models, 2025. URL https://arxiv.org/abs/2412.03603. Kong, X., Liu, S., Lyu, X., Taher, M., Qi, X., and Davison, A. J. Eschernet: generative model for scalable view synthesis, 2024. URL https://arxiv.org/abs/ 2402.03908. Lai, X., Lu, J., Luo, Y., Ma, Y., and Zhou, X. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference, 2025. URL https: //arxiv.org/abs/2502.20766. Li, J., Tang, J., Xu, Z., Wu, L., Zhou, Y., Shao, S., Yu, T., Cao, Z., and Lu, Q. Hunyuan-gamecraft: High-dynamic interactive game video generation with hybrid history condition, 2025a. URL https://arxiv.org/abs/ 2506.17201. Li, R., Yi, B., Liu, J., Gao, H., Ma, Y., and Kanazawa, A. Cameras as relative positional encoding, 2025b. URL https://arxiv.org/abs/2507.10496. Lian, K., Cai, S., Du, Y., and Liang, Y. Toward memoryaided world models: Benchmarking via spatial consistency, 2025. URL https://arxiv.org/abs/ 2505.22976. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural radiance fields for view synthesis, 2020. Miyato, T., Jaeger, B., Welling, M., and Geiger, A. Gta: geometry-aware attention mechanism for multi-view transformers, 2024. URL https://arxiv.org/ abs/2310.10375. Oshima, Y., Iwasawa, Y., Suzuki, M., Matsuo, Y., and Furuta, H. Worldpack: Compressed memory improves spatial consistency in video world modeling. arXiv preprint arXiv:2512.02473, 2025. Safin, A., Cremers, D., and Leal-Taixe, L. Repast: Relative pose attention scene representation transformer, 2023. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/ 2104.09864. Valevski, D., Leviathan, Y., Arar, M., and Fruchter, S. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. Wan, T., Wang, A., Ai, B., Wen, B., Mao, C., Xie, C.-W., Chen, D., Yu, F., Zhao, H., Yang, J., Zeng, J., Wang, J., Zhang, J., Zhou, J., Wang, J., Chen, J., Zhu, K., Zhao, K., Yan, K., Huang, L., Feng, M., Zhang, N., Li, P., Wu, P., Chu, R., Feng, R., Zhang, S., Sun, S., Fang, T., Wang, T., Gui, T., Weng, T., Shen, T., Lin, W., Wang, W., Wang, W., Zhou, W., Wang, W., Shen, W., Yu, W., Shi, X., Huang, X., Xu, X., Kou, Y., Lv, Y., Li, Y., Liu, Y., Wang, Y., Zhang, Y., Huang, Y., Li, Y., Wu, Y., Liu, Y., Pan, Y., Zheng, Y., Hong, Y., Shi, Y., Feng, Y., Jiang, Z., Han, Z., Wu, Z.-F., and Liu, Z. Wan: Open and advanced large-scale video generative models, 2025. URL https: //arxiv.org/abs/2503.20314. Wang, L., Cheng, Y., Shi, Y., Tang, Z., Mo, Z., Xie, W., Ma, L., Xia, Y., Xue, J., Yang, F., and Yang, Z. Tilelang: composable tiled programming model for ai systems, 2025. URL https://arxiv.org/abs/ 2504.17577. Weber, E., Holynski, A., Jampani, V., Gupta, S., and Snavely, N. Fillerbuster: consistent video generation model with explicit background layout control, 2025. Wu, T., Yang, S., Po, R., Xu, Y., Liu, Z., Lin, D., and Wetzstein, G. Video world models with long-term spatial memory. arXiv preprint arXiv:2506.05284, 2025. 10 Submission and Formatting Instructions for ICML Zhu, Z., Tang, H., Li, Y., Liu, D., Xu, H., Lan, K., Zhang, D., Jiang, Y., Zhou, H., Wang, C., Zhang, S., Sun, L., Wang, Y., Sun, Y., Chen, L., and Yu, K. Moba: Multifaceted memory-enhanced adaptive planning for In Proceedings of efficient mobile task automation. the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations), pp. 535549. Association for Computational Linguistics, 2025b. doi: 10.18653/v1/2025.naacl-demo. 43. URL http://dx.doi.org/10.18653/v1/ 2025.naacl-demo.43. Xi, H., Yang, S., Zhao, Y., Xu, C., Li, M., Li, X., Lin, Y., Cai, H., Zhang, J., Li, D., Chen, J., Stoica, I., Keutzer, K., and Han, S. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity, 2025. URL https://arxiv.org/abs/2502.01776. Yang, S., Huang, W., Chu, R., Xiao, Y., Zhao, Y., Wang, X., Li, M., Xie, E., Chen, Y., Lu, Y., Han, S., and Chen, Y. Longlive: Real-time interactive long video generation, 2025a. URL https://arxiv.org/abs/ 2509.22622. Yang, S., Huang, W., Chu, R., Xiao, Y., Zhao, Y., Wang, X., Li, M., Xie, E., Chen, Y., Lu, Y., et al. Longlive: Realtime interactive long video generation. arXiv preprint arXiv:2509.22622, 2025b. Yin, T., Zhang, Q., Zhang, R., Freeman, W. T., Durand, F., Shechtman, E., and Huang, X. From slow bidirectional to fast autoregressive video diffusion models, 2025. URL https://arxiv.org/abs/2412.07772. Yu, J., Bai, J., Qin, Y., Liu, Q., Wang, X., Wan, P., Zhang, D., and Liu, X. Context as memory: Scene-consistent interactive long video generation with memory retrieval, 2025a. URL https://arxiv.org/abs/2506.03141. Yu, J., Qin, Y., Wang, X., Wan, P., Zhang, D., and Liu, X. Gamefactory: Creating new games with generative interactive videos, 2025b. URL https://arxiv.org/ abs/2501.08325. Zhang, J., Li, R., Tancik, M., Gao, H., and Kanazawa, A. Cameras as rays: Pose-conditioned transformers for freeview synthesis, 2024. Zhang, J., Xiang, C., Huang, H., Wei, J., Xi, H., Zhu, J., and Chen, J. Spargeattention: Accurate and training-free sparse attention accelerating any model inference, 2025a. URL https://arxiv.org/abs/2502.18137. Zhang, P., Chen, Y., Huang, H., Lin, W., Liu, Z., Stoica, I., Xing, E., and Zhang, H. Vsa: Faster video diffusion with trainable sparse attention, 2025b. URL https: //arxiv.org/abs/2505.13389. Zhang, Y., Peng, C., Wang, B., Wang, P., Zhu, Q., Kang, F., Jiang, B., Gao, Z., Li, E., Liu, Y., and Zhou, Y. Matrixgame: Interactive world foundation model, 2025c. URL https://arxiv.org/abs/2506.18701. Zhu, Q., Duan, J., Chen, C., Liu, S., Feng, G., Lv, X., Chuanfu, X., Lin, D., and Yang, C. Sampleattention: Near-lossless acceleration of long context llm inference with adaptive structured sparse attention, 2025a. URL https://arxiv.org/abs/2406.15486. 11 A. Detailed Training And Inference Pipeline Submission and Formatting Instructions for ICML 2026 We provide the detailed training and inference pipeline with Frame Sparse Attention under Teacher Forcing as shown in Algorithm 1. Input: Clean Q0, K0, V0, Camera sequence C, Input: Noise Qt, Kt, Vt. Algorithm 1 Training and Inference with Frame Sparse Attention under Teacher Forcing 1: Training Procedure: 2: Input: Clean sequence Z0, Camera sequence C, Text 3: Noising: Add noise to Z0 to obtain Zt 4: pred Model(Z0, Zt, C, Y) 5: FlowMatching loss(pred, Z0, N) 6: Inside Attention: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: Q0, Qt VR(Q0, C), VR(Qt, C) K0, Kt VR(K0, C), VR(Kt, C) Q, K, [Q0; Qt], [K0; Kt], [V0; Vt] Sample indices randomly q[I] k[I] ApplyTeacherForcingMask(s) TopK(softmax(s)) FrameSparseAttention(q, k, v, M) 1: Inference Procedure: 2: Input: First frame x0, Camera sequence C, Text 3: for in do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: Model(n0, c, cache=True) {Cache current frame} 18: end for Given qt, k0, v0 (Clean KV Cache) Sample indices randomly qc, kc qt[I], k0[I] qc M TopK(softmax(s)) FrameSparseAttention(qt, k0, v0, M) Initialize noise nt for step in DenoiseSteps do pred Model(nt, c, Y) nt ODE Update(nt, pred) end for n0 nt Inside Attention: A.1. Training Configuration We provide detailed training configurations for reproducibility. Model and Resolution. We build upon WAN 2.2 TI2V-5B (Wan et al., 2025), text-and-image-to-video diffusion transformer with 5 billion parameters. The training resolution is 480 832 with 61 frames per clip. To convert the model into streaming video generator, we use teacher-forcing training to align the model with the autoregressive generation interface and KV-caching behavior. Optimization. We train with batch size of 64 for 6k steps, using the AdamW optimizer with learning rate 5 105 and linear warmup for 50 iterations. The training process takes approximately 2 days on 16 NVIDIA A100 GPUs. Training Data. The training data consists of three sources: Context-as-Memory (Yu et al., 2025a): 760k frames of yaw-only camera motion sequences. GF-Minecraft (Yu et al., 2025b): 4M frames of gameplay videos with diverse actions. ViewBench: 500k frames of synthetic sequences with complete 3-axis rotation coverage. To obtain balanced distribution of camera poses and scene geometries, we adjust the sampling rate of each dataset to achieve 1:1:1 ratio during training. Fair Comparison. All RoPE variants (3D RoPE, GTA, ViewRope) are applied to the same channels of the latent representation to ensure fair comparison. The channel allocation follows the ablation study in Section 4.4, where ViewRope is embedded in the lowest frequency bands of the temporal dimension (channels 3244). B. Loop Closure Formulation This section provides the formal definition of loop-closure consistency referenced in Section 3.1. Standard video generators enforce local Expθ the form Ltemp(θ) (cid:105) , which only constrain adjacent frames and do not prevent long-horizon geometric drift. temporal coherence via objectives of t=2 d(xt, xt1) (cid:104)(cid:80)T := 12 Submission and Formatting Instructions for ICML 2026 pose-conditioned world model must additionally satisfy loop closure. Define the revisit indicator wt,k := I((Ct, Ck) ε) , < t, (13) where (, ) measures pose similarity and ε is tolerance threshold. Let Wkt denote the image warp induced by the relative camera motion and scene geometry. For pixel = (u, v) with homogeneous coordinate = [u, v, 1], and depth field Dt at time t: Xt(u) := K1 Xk(u) := RkR1 Dt(u), Xt(u) + (cid:0)Pk RkR1 Pt Wkt(u) := π(cid:0)Kk Xk(u)(cid:1), (cid:1), (14) (15) (16) where π() denotes perspective division. The loop closure loss Llc(θ) enforces xt(u) xk(Wkt(u)) for Ωt,k (the mutually visible region). We formalize this as: (cid:88) (cid:88) wt,k (cid:88) ρ(xt(u) xk(Wkt(u))) , (17) t=1 k<t uΩt,k Llc(θ) := Expθ where ρ() is robust penalty (e.g., Huber). C. ViewBench Benchmark This appendix provides additional details on ViewBench. For the main comparison with existing datasets, see Table 1 in Section 4.1. For baseline comparisons with Matrix-Game-2 and HY-WorldPlay, see Table 8 and the accompanying analysis. C.1. Extended Dataset Comparison Table 6 provides an extended comparison with additional properties beyond those shown in the main text. Table 6. Extended comparison with existing datasets. Property Yaw Pitch Roll Loop-closure trajectories Controlled angle magnitudes Per-frame SE(3) c2w Overlap annotations Engine CaM partial FOV-based UE5 GF-MC ViewBench MC depth-based UE5 C.2. Scene Environments ViewBench comprises 10 photorealistic UE5 environments spanning indoor, outdoor, urban, and natural settings  (Table 7)  . The diversity in geometry, lighting, and texture ensures evaluation results generalize across visual conditions. C.3. Camera Rotation Ranges ViewBench trajectories are organized into two parts. Part 1: Pure rotation. The camera is stationary and performs rotation-only motion covering all 7 axis combinations (3 single-axis + 3 dual-axis + 1 triple-axis). Each trajectory follows rotate-awayrotate-back loop-closure design, with rotation magnitudes sampled from {30, 75, 90, 180}. For each of 10 scenes 7 axis combinations, we generate 100 samples, yielding 7,000 clips. 13 Submission and Formatting Instructions for ICML 2026 Table 7. ViewBench scenes. Scene Type Description Abandoned HongKong Outdoor/Urban Abandoned Mall DeadCity PostApocalypticCity FPS Template Container Yard Rome SuburbsCityPack ChineseAlley UrbanDistrict Gate Indoor Outdoor/Urban Outdoor/Urban Outdoor/Desert Outdoor/Industrial Outdoor/Historical Roman-style architecture Outdoor/Suburban Outdoor/Cultural Indoor-like Mid-scale urban ruins Two-floor shopping mall Derelict city, dark lighting Large-scale post-apocalyptic Middle-Eastern battlefield Container stacking yard Suburban street scene Chinese-style alleyway Narrow shantytown alleys Part 2: Rotation + translation. The camera moves within compact exploration radius while simultaneously rotating, mimicking interactive navigation. Each sequence is composed of actions randomly drawn from four types: RotateOnly (inplace camera rotation), MoveOnly (pure WASD translation without rotation), MoveAndRotate (simultaneous translation and rotation), and Orbit (circular motion around point of interest). All action types may include roll rotation with type-specific probabilities and ranges. C.4. Data Format Each frame is annotated with 4 4 camera-to-world (c2w) SE(3) matrix, Euler angles (pitch, roll, yaw), position in centimeters, FOV (including the vertical one and the horizontal one), and binary WASD key states. The rotation matrix follows ZYX convention (R = Rz(yaw) Ry(pitch) Rx(roll)) in UE5s left-handed coordinate system (X-Forward, Y-Right, and Z-Up). post-processing pipeline converts ViewBench data into the action formats used by CaM and GF-Minecraft, enabling unified evaluation. Depth-based frame overlap annotations are also provided for attention recall analysis. C.5. Training and Evaluation Splits Training. The training set combines Part 1 (pure rotation) and Part 2 (rotation + translation) data, totaling 1,059 video sequences (500k frames at 30 fps) across 10 scenes. During training, ViewBench data is mixed with CaM and GF-Minecraft at 1:1:1 sampling ratio. The evaluation set is separately collected from the training set with non-overlapping trajectories, ensuring no data leakage between training and evaluation. Evaluation. The evaluation set consists of separately collected Part 1 pure-rotation loop-closure trajectories from the same 10 scenes, totaling 600 samples. Each sample is downsampled to 16 fps and contains 61 frames, providing first frame, the full camera trajectory with per-frame SE(3) poses, and ground-truth UE5-rendered video. We evaluate frame-level PSNR, SSIM, and LPIPS against the ground truth, as well as Loop Closure Error (LCE): where x0 is the ground-truth first frame and ˆxT is the generated frame at the return pose. LCE isolates the challenge of remembering previously seen content after an extended camera excursion. LCE = LPIPS(x0, ˆxT ), (18) C.6. Complete Baseline Results on ViewBench Table 8 in the main text presents results for small-to-medium angles (30, 45, 75) where ViewRope consistently outperforms all baselines. Here we provide the complete results including large-angle (90, 180) trajectories. We evaluate representative baselines that accept first frame and per-frame action sequence. For fair comparison, baseline evaluation uses only yaw/pitch trajectories (no roll). ViewRope is additionally evaluated on the full set including roll. The unified evaluation script automatically converts ViewBench SE(3) poses to each models expected action format (e.g., mouse deltas for Matrix-Game-2, yaw-only deltas for CaM). Extended analysis of large-angle performance. At large angles (90180), ViewRope shows lower performance than HY-WorldPlay. We identify two factors orthogonal to our positional encoding contribution: 14 Submission and Formatting Instructions for ICML 2026 Table 8. Complete baseline results on ViewBench across all rotation magnitudes. Best in bold. See Section 4.2 for analysis. Angle Model PSNR SSIM LPIPS LCE 30 45 75 90 180 Matrix-Game-2 HY-WorldPlay ViewRope (ours) Matrix-Game-2 HY-WorldPlay ViewRope (ours) Matrix-Game-2 HY-WorldPlay ViewRope (ours) Matrix-Game-2 HY-WorldPlay ViewRope (ours) Matrix-Game-2 HY-WorldPlay ViewRope (ours) 14.27 17.04 17.53 13.55 16.38 16.74 13.46 15.19 15. 12.41 16.56 15.61 12.74 14.82 14.35 0.2806 0.4238 0.4378 0.2535 0.4085 0.4130 0.2625 0.3847 0.3916 0.1878 0.4174 0. 0.2033 0.3403 0.3458 0.5723 0.4697 0.4080 0.6071 0.5096 0.4527 0.6084 0.5643 0.5398 0.6684 0.4970 0.5382 0.6310 0.5978 0. 0.5553 0.4811 0.4497 0.6175 0.4944 0.4545 0.6288 0.5151 0.4562 0.7121 0.4169 0.5445 0.6732 0.4413 0.5609 (1) Evaluation frame-rate mismatch. To fit large-angle round-trip trajectories into fixed length of 161 frames, the evaluation sequences are uniformly resampled with endpoints held fixed, resulting in per-frame angular steps that substantially exceed the constant angular velocity seen during training. As consequence, our model under-rotates (e.g., achieving 80 when 180 is requested), producing large LCE at the return pose. HY-WorldPlay is trained on data with variable frame rates and employs Dual Action conditioning with RL-based post-training (WorldCompass) to explicitly optimize action following, making it more robust to such speed variation. (2) Error accumulation in teacher-forcing models. Our model is trained with teacher forcing, where the ground-truth context is provided during training. At inference time, each frame is autoregressively conditioned on previously generated frames, so errors compound over the longer sequences required by large-angle trajectories. HY-WorldPlay mitigates this through Context Forcing distillation and 4-step denoising with self-correction, substantially reducing error accumulation. These limitations are system-level and can be addressed independently of ViewRope. Combining ViewRope with advanced training strategies such as self-forcing (Huang et al., 2025b) and RL-based action post-training is promising direction for future work. C.7. Qualitative Comparison We present qualitative comparisons between Matrix-Game-2.0 (M-G 2.0), HY-WorldPlay (HY-World), and ViewRope (Ours) on ViewBench loop-closure trajectories. Each case shows the input first frame (left) followed by keyframes sampled from the generated video. Arrow icons indicate the camera rotation direction at each keyframe. The camera first rotates away from the starting viewpoint and then returns, forming closed loop. Case 1: Yaw + Pitch in an urban street scene (Figure 5). The camera rotates upward and rightward, then reverses back to the starting view. M-G 2.0 produces severe brightness collapse mid-trajectory, losing nearly all scene content in the dark frames. HY-WorldPlay maintains plausible appearance but exhibits geometric driftbuilding structures shift position upon return. ViewRope preserves both the scene structure and lighting conditions throughout the trajectory, yielding return frame closely matching the ground truth. Case 2: Pure yaw in an Asian street scene (Figure 6). The camera pans left and then reverses rightward to return. M-G 2.0 generates quite big hallucination, introducing new scene elements (e.g., yellow trees) that are absent in the ground truth upon return. In this case, both HY-WorldPlay and ViewRope accurately recover the original storefronts and street layout, demonstrating strong long-term spatial memory. Case 3: Pure pitch in Roman architecture scene (Figure 7). The camera tilts downward toward the ground and then pitches back up to the starting view. This tests vertical rotation consistency. M-G 2.0 fails catastrophically on the returnthe final frame shows completely different indoor scene with wooden structures, indicating total loss of scene 15 Submission and Formatting Instructions for ICML 2026 identity. HY-WorldPlay maintains the general scene category but produces blurry architecture and seemingly fails to return. Comparatively, ViewRope faithfully reproduces the arched stone structures visible in the starting frame. Figure 5. Case 1: Yaw + Pitch loop closure in an urban street. M-G 2.0 suffers from brightness collapse. HY-WorldPlay exhibits geometric drift. ViewRope maintains structural and lighting consistency. Figure 6. Case 2: Pure yaw loop closure in an Asian street. M-G 2.0 hallucinates entirely different content on return. HY-WorldPlay introduces nonexistent elements. ViewRope recovers the original scene faithfully. 16 Submission and Formatting Instructions for ICML 2026 Figure 7. Case 3: Pure pitch loop closure in Roman architecture. M-G 2.0 generates completely different scene upon return. HY-WorldPlay produces blurry, inconsistent structures. ViewRope accurately restores the original arched architecture. D. Additional Results D.1. Ablation of Number of Topk Frames As shown in Table 9, we report the ablation results of the number of topk frames on ViewBench. We use the model trained with topk=5 and adjust the number of retrieved frames at inference time to 1, 3, 10, and 20. Increasing the number of retrieved frames generally improves visual quality metrics (PSNR, SSIM, and LPIPS), suggesting that accessing more reference frames provides richer texture details for generation. However, we observe that the Loop Closure Error (LCE) achieves its optimum at topk=5 and degrades as the number of frames increases further. This indicates that while more context helps visual quality, the model, having been trained with topk=5, may be distracted by the additional retrieved frames or struggle to effectively utilize the expanded context for long-term geometric consistency. Table 9. Ablation of number of topk frames on ViewBench. We report visual quality (PSNR, SSIM, LPIPS) and geometric consistency (LCE) for 90 and 180 view synthesis. Best in bold. 90 deg 180 deg PSNR SSIM LPIPS LCE PSNR SSIM LPIPS LCE Top 1 Top 3 Top 5 Top 10 Top 20 13.01 15.11 15.71 15.53 16.17 0.3328 0.3792 0.3929 0.3951 0.4087 0.8230 0.5470 0.5267 0.5386 0.5240 0.8416 0.5777 0.5591 0.5804 0. 13.00 14.54 14.92 14.99 14.92 0.3337 0.3559 0.3574 0.3548 0.3576 0.8357 0.6096 0.5991 0.5997 0.5976 0.8109 0.5413 0.5385 0.5956 0."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Peking University",
        "Tsinghua University"
    ]
}