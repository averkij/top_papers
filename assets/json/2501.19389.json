{
    "paper_title": "Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models",
    "authors": [
        "Wenzhi Fang",
        "Dong-Jun Han",
        "Liangqi Yuan",
        "Seyyedali Hosseinalipour",
        "Christopher G. Brinton"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains a critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRA's feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving a wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages a sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide a rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRA's superior performance compared to various baselines."
        },
        {
            "title": "Start",
            "content": "Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Wenzhi Fang 1 Dong-Jun Han 2 Liangqi Yuan 1 Seyyedali Hosseinalipour 3 Christopher G. Brinton"
        },
        {
            "title": "Abstract",
            "content": "Fine-tuning large language models (LLMs) on devices is attracting increasing interest. Recent works have fused low-rank adaptation (LoRA) techniques with federated fine-tuning to mitigate challenges associated with device model sizes and data scarcity. Still, the heterogeneity of computational resources remains critical bottleneck: while higher-rank modules generally enhance performance, varying device capabilities constrain LoRAs feasible rank range. Existing approaches attempting to resolve this issue either lack analytical justification or impose additional computational overhead, leaving wide gap for an efficient and theoretically-grounded solution. To address these challenges, we propose federated sketching LoRA (FSLoRA), which leverages sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on the devices, FSLoRA flexibly adapts to device-specific communication and computational constraints. We provide rigorous convergence analysis of FSLoRA that characterizes how the sketching ratios affect the convergence rate. Through comprehensive experiments on multiple datasets and LLM models, we demonstrate FSLoRAs superior performance compared to various baselines. 5 2 0 2 1 3 ] . [ 1 9 8 3 9 1 . 1 0 5 2 : r 1. Introduction On-device LLMs have recently gained significant attention as promising complement to cloud-based large language models (LLMs) (Fan et al., 2024). They align with the 1Department of Electrical and Computer Engineering, Purdue University 2Department of Computer Science and Engineering, Yonsei University 3Department of Electrical Engineering, University at Buffalo-SUNY. Correspondence to: Wenzhi Fang <fang375@purdue.edu>. 1 typical paradigm of LLMs: they are initialized with base model pre-trained on extensive datasets to capture linguistic patterns, semantics, and contextual nuances, and then finetuned on specific datasets to achieve better performance in specialized or domain-specific tasks. However, an LLM fine-tuned on single device often achieves unsatisfactory performance due to the limited data available on each device. Fortunately, federated learning (McMahan et al., 2017; Chen et al., 2023) offers an effective solution here, enabling the model to be fine-tuned across distributed group of clients within the same task domain, without any data sharing. However, federated learning imposes significant computational and memory costs, as each device must fine-tune the LLM using its local dataset and send updates to the server for model aggregation. Recently, many parameterefficient fine-tuning methods have been proposed (Lester et al., 2021; Li & Liang, 2021; Hu et al., 2021) to reduce the cost associated with model adaptation. Among them, low-rank adaptation (LoRA) (Hu et al., 2021) stands out as particularly effective approach due to its flexibility. LoRA enables efficient fine-tuning by approximating weight updates through low-rank decomposition = BA, where matrices and contain significantly fewer trainable parameters than the original weight matrix. To support distributed on-device LLM, Zhang et al. (2024); Ye et al. (2024) incorporated LoRA with FedAvg (McMahan et al., 2017), significantly reducing the fine-tuning cost by cutting down the number of parameters that need to be synchronized across distributed devices. Challenges. While integrating federated learning with LoRA reduces the number of trainable parameters through matrix decomposition, communication costs still increase linearly with the rank of the decomposition. This poses challenges when complex tasks demand higher-rank LoRA modules, particularly on resource-constrained mobile devices. Furthermore, the heterogeneity in computational and communication capabilities across distributed devices makes uniform rank inefficient: fixed rank may be too large for some devices while being too small for more powerful ones, resulting in underutilized resources. Consequently, an approach that reduces communication overhead while adapting LoRA ranks to heterogeneous device capaFederated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models bilities is highly desirable for collaborative fine-tuning of LLMs. Although some existing approaches have attempted to provide solution here (Cho et al., 2024; Bai et al., 2024; Wang et al., 2024), they either lack theoretical justification or impose additional computational overhead, leaving gap for an efficient and theoretically-grounded solution. As discussed in Section 2.2, comprehensive approach that retains LoRAs advantages while addressing heterogeneous on-device fine-tuning under tight resource constraints, with theoretical guarantees, has remained elusive. 1.1. Contributions Motivated by these limitations, this work aims to develop methodology for collaborative on-device LLM fine-tuning that (i) retains the flexibility of LoRA, (ii) provides theoretical convergence guarantees, and (iii) addresses the challenges posed by system heterogeneity and resource constraints across distributed devices. As depicted in Figure 1, our key idea is to introduce sketching-based LoRA update to the fine-tuning process, which allows devices to selectively update subset of columns and rows of the LoRA modules during each round, reducing the computation and communication consumption through the system. Additionally, our method customizes the fine-tuning process by adjusting the sparsity level of the sketching matrix, i.e., the size of the updated submatrices for each device in each iteration, as illustrated in Figure 1. As we will see, the impact of the introduced sketching mechanism on the overall optimization landscape requires careful consideration, posing additional challenges for the theoretical analysis which we investigate. Overall, we make the following contributions: We propose federated sketching LoRA (FSLoRA), which leverages sketching mechanism to enable devices to selectively update submatrices of global LoRA modules maintained by the server. By adjusting the sketching ratios, which determine the ranks of the submatrices on devices, FSLoRA effectively adapts to device-specific communication and computational constraints. We present rigorous convergence analysis of FSLoRA under non-uniform submatrix update scenarios (i.e., heterogeneous LoRA configurations) across devices, revealing how the sketching ratios affect the convergence rate. In particular, we show how increasing the sketching ratios improves convergence theoretically but raises communication and computation costs, highlighting delicate trade-off in selecting the sketching ratios. We conduct extensive experiments across multiple datasets and LLM models with diverse parameter settings, demonstrating FSLoRAs superior performance compared to various baselines in terms of testing accuracy, training time, and resource utilization, validating the effectiveness Figure 1: An illustration of our proposed methodology where the server maintains pair of global LoRA modules while the devices adaptively update submatrices of the global LoRA modules through sketching during each round. of the sketching mechanism. 1.2. Related Works LoRA-based parameter-efficient fine-tuning. LoRA was first introduced in (Hu et al., 2021) as parameter-efficient alternative to full model fine-tuning by utilizing low-rank matrix approximation. Subsequently, Kalajdzievski (2023) proposed rank-stabilized LoRA (rsLoRA), an approach that enhances LoRAs performance in high-rank scenarios by modifying the scaling factor. Shuttleworth et al. (2024) demonstrated that with this novel scaling factor design, rsLoRA could approach the performance of full model finetuning as the rank of LoRA modules increases. Malinovsky et al. (2024) investigated an alternating update of LoRA modules and provided theoretical analysis. Han et al. (2024) introduced sparse matrix in parallel with LoRA modules to improve the fitting capability of LoRA. Lialin et al. (2023); Xia et al. (2024) proposed ReLoRA and Chain of LoRA, which periodically merge learned LoRA modules into the full model to achieve high-rank fine-tuning. However, the works mentioned above consider centralized scenarios, assuming that the data required for fine-tuning is available at the central server. Collaborative Fine-tuning via Federated LoRA. Leveraging the efficiency of LoRA, federated LoRA is recently gaining recognition as promising approach for collaborative fine-tuning of LLMs across distributed devices (Chen et al., 2023). Sun et al. (2024) examined the performance of federated LoRA with the incorporation of differential privacy. To address communication overhead, Kuo et al. (2024) proposed integrating communication compression with federated LoRA. Meanwhile, Bai et al. (2024); Cho 2 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models et al. (2024); Byun & Lee (2024); Wang et al. (2024); Koo et al. (2024) explored the challenges of resource heterogeneity among distributed devices and introduced heterogeneous LoRA as solution. However, the approaches proposed in (Cho et al., 2024; Koo et al., 2024; Byun & Lee, 2024) lack theoretical justification. FlexLoRA, introduced in (Bai et al., 2024), incurs additional computational overhead due to its reliance on singular value decomposition (SVD). Furthermore, the stack LoRA method proposed in (Bai et al., 2024; Wang et al., 2024) requires the devices to integrate the LoRA modules into the base model, thereby compromising the inherent flexibility of LoRA. Overall, there is still lack of systematic and theoretically grounded solution that can effectively tackle the challenges of heterogeneity in collaborative on-device LLM fine-tuning systems. 2. Problem Background 2.1. LoRA-based Federated LLM Fine-tuning Following the decomposition of LoRA, the federated LoRA fine-tuning problem can be formulated as (B, A) := min B,A (cid:88) fi(B, A) 1 i=1 fi(B, A) := EξDi [ℓ(W0 + BA, ξ)] , (1) where W0 denotes the frozen base model, Rmr, Rrn are LoRA modules, denotes the number of devices, ξ denotes data sample, and Di is the local dataset on device i. ℓ, fi, and are the sample loss function, the local loss for device i, and the global loss, respectively. Problem (1) aligns with the conventional federated optimization formulation, which thus can be solved using the FedAvg algorithm. Based on the FedAvg framework, Zhang et al. (2024) developed the federated LoRA algorithm, which applies uniform rank across all devices, overlooking system heterogeneity. This one-size-fits-all approach leads to resource mismatches, where computationally constrained devices may struggle, while more powerful devices remain underutilized with fixed rank. 2.2. Arent the Existing Solutions Good Enough? To address this issue, researchers have proposed heterogeneous federated LoRA approaches, where devices maintain non-uniform LoRA modules with varying ranks. They also introduce mechanisms to overcome the challenges of directly aggregating matrices with different dimensions. However, these methods often lack theoretical justification or introduce significant computational overhead, as outlined below. HeteroLoRA (Cho et al., 2024) lets the server pad the updates from the devices with smaller ranks to match the size 3 of the largest rank during aggregation. During model dissemination, devices receive truncated version of the global LoRA modules from the server. HeteroLoRA, while easy to implement, lacks solid theoretical foundation. Additionally, its dependence on zero-padding diminishes optimization efficiency, potentially limiting overall performance. FlexLoRA (Bai et al., 2024) requires the server to collect the individual LoRA matrices Bi and Ai from the devices and then computes their product BiAi. To support the initialization of non-uniform LoRA modules, the server applies truncated SVD to the averaged product 1 i=1 BiAi. However, this approach introduces extra computational and memory overhead due to truncated SVD, and the associated error may limit the performance. (cid:80)N FedStackLoRA (Wang et al., 2024) introduces stacking mechanism where the server concatenates LoRA modules from the devices. The concatenated matrices are then sent back to the devices, which compute their product and merge it into the base model before initializing new LoRA modules for the next fine-tuning round. However, this approach increases communication complexity linearly with the number of devices, imposes higher computation and memory demands on the devices, and compromises LoRAs flexibility to support multiple adapters for different tasks. In summary, theoretically-grounded solution that preserves LoRAs benefits while effectively addressing system heterogeneity and the constraints of resource-limited devices remains lacking. 3. Methodology Driven by the limitations of existing methods, we propose new federated LoRA reformulation. Building on this foundation, we develop FSLoRA, resource-adaptive algorithm that preserves LoRAs flexibility while accounting for system heterogeneity and resource constraints. 3.1. Our Formulation We propose sketching-based LoRA formulation for collaborative LLM fine-tuning as follows: (B, A) := min B,A 1 (cid:88) i=1 (B, A) (2) (B, A) := ESSi;ξDi [ℓ(W0 + BSA, ξ)] , where Rmr, Rrn are LoRA modules, is the local loss function at device with sketching, and denotes sketching matrix randomly sampled from the diagonal matrix set Si = S(r, ki). The set S(r, ki) comprises diagonal matrices of size with exactly ki non-zero entries. The formal definition of S(r, k) is provided below: Definition 3.1 (Random-k sketching). random-k diagoFederated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models nal matrix set is defined as: (cid:40) S(r, k) = = (cid:88) eje , {1, . . . , r}, = jI (cid:41) , where Bℓ(W0 + BSA, ξ), Aℓ(W0 + BSA, ξ), and ℓ(W0 + BSA, ξ) represent the gradients of ℓ(W0 + BSA, ξ) with respect to B, A, and W0 + BSA, respectively. where e1, . . . , er Rr are standard unit basis vectors and index set is random subset of [r] := {1, 2, . . . , r} sampled uniformly from all subsets of [r] with cardinality k. With being matrix sampled from Si, we have BSA = ki (cid:88) jIi Beje A, where Ii corresponds to the index set of non-zero entries of S. Bej extracts the j-th column of while A extracts the j-th row of A. In other words, only ki columns and rows in the LoRA modules and are activated by the sketching matrix in the loss ℓ(W0 + BSA, ξ) at device i. On the other hand, the sketching matrix satisfies ESSi [S] = Ir where Ir is r-dimensional identity matrix. Based upon this property, W0 + BSA can be treated as an unbiased estimate of W0 + BA. Intuition. larger rank allows LoRA modules to be more expressive, leading to better performance. However, resource-constrained devices cannot afford the computational or communication demands of large-rank modules. Our formulation (2) leverages the sketching matrix to balance the expressiveness of high-rank LoRA modules with the resource constraints of different devices. With the sketching mechanism introduced, the local gradients with respect to the LoRA modules on the devices will exhibit structured sparsity. By adjusting the sketching ratio ki/r, we can tailor the sparsity of the gradient to match the capabilities of each device, ensuring affordable training while maintaining performance across heterogeneous systems, as elaborated in the following subsection. Overall, compared to the original objective in (1), our formulation offers more resourceadaptive and flexible framework, tailored to address the diverse capabilities of heterogeneous devices. 3.2. Sparsity in the Gradients In this subsection, we analyze the gradient structure of LoRA modules and highlight the gradients sparsity properties under given sketching matrix. To begin, we present the gradient expressions for the LoRA modules and in the following lemma. The proof is provided in Appendix D.2. Lemma 3.2 (Gradient Formulation). For given sketching matrix S, the gradients of ℓ(W0 + BSA, ξ) with respect to and take the following form In particular, random-k diagonal sketching matrix selectively samples rows or columns of matrix through left product or right product, respectively. With being random-k diagonal matrix, the gradients of ℓ(W0 + BSA, ξ) with respect to LoRA modules and A, as shown in (3), naturally become structurally sparse matrices. This sparsity reduces the computational and memory overhead during training, allowing for faster gradient computation and parameter updates. Additionally, sparse training enables better scalability across distributed devices by reducing communication costs, as only the non-zero elements need to be transmitted. The sparsity level of these gradients at each device is determined by the corresponding sketching matrix set Si. Remark 3.3 (Sparsity Level Control). key advantage of our formulation is its flexible control over the sparsity level of local gradients, achieved by configuring the parameter ki of the sketching matrix set Si = S(r, ki). This mechanism allows each device to tailor its local updates according to its communication and computation resource constraints, ensuring efficient and scalable fine-tuning in heterogeneous federated systems. Lowering ki helps resource-constrained devices reduce computation and communication overhead, while more capable devices can increase ki to conduct more informative local updates. 3.3. Algorithm Based on the formulation in (2), we propose resourceadaptive algorithm termed FSLoRA for collaborative ondevice fine-tuning. FSLoRA allows each device to update submatrices of the original modules and in each round. The server maintains pair of global LoRA modules and and periodically updates them by aggregating sparse local updates received from distributed devices. Specifically, the procedure of FSLoRA at each round is detailed below. The server begins by generating sketching matrices {St Si}N i=1 for all devices, where Si represents the set of possible sketching matrices for device i. These sketches are then sent to the corresponding devices. Additionally, the server broadcasts the current global LoRA modules [Bt; At] to all devices. Devices perform local fine-tuning using sketch St ically, guided by sketching matrix St during the h-th iteration of the t-th round is given by: i. Specifi, the update at device Bℓ(W0 + BSA, ξ) = ℓ(W0 + BSA, ξ)AS Aℓ(W0 + BSA, ξ) = SBℓ(W0 + BSA, ξ), (3) (cid:21) (cid:20)Bt,h+1 At,h+1 = (cid:21) (cid:20)Bt,h At,h γ (cid:20)Bt,h (St (St i) i)At,h (cid:21) , (4) 4 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Algorithm 1 Federated Sketching LoRA (FSLoRA) Require: Base model W0, initial LoRA modules B0 and A0, learning rate γ, and sketching matrix set {Si}N i=1 1: for = 0, 1, . . . , 1 do 2: 3: 4: 5: 6: 7: Server generates sketching matrices {St and sends them back to the devices Server broadcasts the current global LoRA modules to the devices for = 0, 1, . . . , 1 do Si}N i=1 Devices update local LoRA modules via (4) end for Bt,0 Devices upload non-zero columns of (Bt,H ) and non-zero rows of (At,H ) to the server Server updates the global LoRA modules via (5) At,0 8: 9: end for i . (cid:21) (cid:21) = St iAt,h )(At,h ) , ξt,h ) ; At,h ] is , ξt,h St (cid:20)Bt,h At,h where γ denotes the learning rate and [Bt,h shorthand representation for: (cid:20)ℓ(W0 + Bt,h (Bt,h iAt,h )ℓ(W0 + Bt,h The update direction in (4) corresponds to the negative stochastic gradient of ℓ(W0 + BSA, ξ) with respect to [B; A] for given sketch St i, as established in Lemma 3.2. The total update for device during one round of training, consisting of local steps, can be expressed as follows: (cid:16)(cid:80)H1 h=0 Bt,h i) (cid:16)(cid:80)H1 i) (St (cid:17) h=0 At,h Bt,0 At, (cid:20)Bt,H At,H γ(St . = (cid:17) (cid:21) γ i From the above equation, we see that only the columns of and the rows of corresponding to the nonzero entries of St are updated during the t-th round at device i. In essence, St selectively activates specific columns of and rows of for each round. Afterward, devices transmit these nonzero columns and rows of the sparse model updates to the server. Using the sketch information, the server reconstructs the corresponding sparse matrices from the received updates and aggregates them to update the global model: (cid:21) (cid:20)Bt+1 At+1 = (cid:21) (cid:20)Bt At + 1 N (cid:88) i=1 (cid:20)Bt,H At,H Bt,0 At,0 (cid:21) . (5) The above procedure is repeated for = 0, 1, . . . , 1 across global rounds. Algorithm 1 summarizes the overall process of FSLoRA. 3.4. Comparison with Communication Compression Although both the sketching approach in FSLoRA and communication compression (Kuo et al., 2024) reduce communication overhead, the sketching approach fundamentally differs from traditional compression techniques. Notably, these two methods are orthogonal and can be combined to achieve greater efficiency. Specifically, the compression can be applied to the transmission of non-zero columns of and the non-zero rows of in FSLoRA to further enhance communication efficiency. We demonstrate the effectiveness of this combination in Appendix C.2. Additionally, the compression focuses solely on reducing the transmission load, leaving the gradient computation and model updates unchanged from the vanilla federated LoRA, FSLoRA goes beyond communication savings by also reducing gradient computation and model update overhead through sparse training. 4. Analysis In this section, we analyze the convergence of the proposed FSLoRA algorithm. We will show that the iterate sequence generated by FSLoRA algorithm converges to the stationary point of function (2). In our analysis, we will use the following notations. Notations: We define ℓ(B, A, ξ; S) = ℓ(W0 + BSA, ξ) and fi(B, A; S) = EξDi [ℓ(W0 + BSA, ξ)] for given (B, A) = ESSi[ fi(B, A; S)]. For simplicity, and we denote = [B; A] and rewrite (B, A), fi(B, A), (B, A), fi(B, A; S), and ℓ(B, A, ξ; S) as (B, A), (X), fi(X; S), and ℓ(X, ξ; S) ref (X), fi(X), (X), spectively. Additionally, we use to denote the Frobenius norm in our analysis. We conduct analysis based on the following assumptions. Assumption 4.1. fi(X) is differentiable and L-smooth, i.e., there exists positive constant such that X, Y, fi(X) fi(Y) LX Y, i. Assumption 4.2. The variance of the gradient from the sketching matrix Si can be bounded as fi(X; S) ESSi fi(X; S) Xf (X)2 σ2 , i, (X) = ESSi[ fi(X; S)]. In addition, for given ℓ(X, ξ; S) due where S, the variance of the stochastic gradient to data sampling ξ Di can be bounded as EξDiX ℓ(X, ξ; S) fi(X; S)2 σ2 g, i, where fi(X; S) = EξDi[ℓ(X, ξ; S)]. Assumption 4.3. The gradient dissimilarity between the global loss (X) and each local loss (X) satisfies (cid:13) (cid:13)Xf (X)Xf (X)(cid:13) 2 (cid:13) chXf (X)2 +δ2 h, i, where ch 0 and (X) = 1 5 (cid:80)N i=1 (X). Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Assumptions 4.1 and 4.2 are standard in stochastic optimization (Demidovich et al., 2023; Fang et al., 2024), while Assumption 4.3 is commonly used in distributed optimization (Fang et al., 2022; Yi et al., 2022) to characterize data heterogeneity. Building on these assumptions, we analyze the convergence behavior of FSLoRA. Our main result is summarized in the following theorem. for FSLoRA. However, on the other hand, as ki increases, the communication and computation costs at device will increase. In other words, there is trade-off in the selection of the sketching ratios. Additionally, Corollary 4.5 suggests that FSLoRA achieves similar linear speedup in the number of local updates and the number of devices as FedAvg (Yu et al., 2019; Khaled et al., 2020). Theorem 4.4. Suppose that Assumptions 4.1-4.3 hold and 1 2L }. the learning rate satisfies γ min{ (ch+1)(cid:101)LLH t=0 generated by FSLoRA 6 Then the iterate sequence {Xt}T 1 satisfies 1 , 1 (cid:88) (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13)"
        },
        {
            "title": "1\nT",
            "content": "f (X0) γT +6 (σ2 +σ2 )+12 γ (cid:101)L (σ2 + σ2 + 3σ2 h), t=0 γ (cid:16) 1 (cid:16) 1 where = denotes the lower bound of (X). L, (cid:101)L = (cid:80)N ki i=1 (cid:17) (cid:17) (cid:80)N i=1 r2 k2 L, and novel step in the proof of Theorem 4.4 is the characterization of how the introduced sketching mechanism impacts the optimization landscape. This analysis delves into the way the sketching operation modifies the smoothness properties of the objective function, particularly how it introduces scaled smoothness constants, L, that impacts ki the overall optimization. Further details are presented in Appendix D.3. and r2 k2 Based on the results in Theorem 4.4, we have the following corollary by applying an appropriate learning rate γ to Algorithm 1. (cid:112) Corollary 4.5. Under the same assumptions of Theorem 4.4, let F0 = (X0) and the learning rate γ = (cid:101)L which satisfies the condition outlined in Theorem 4.4 when is large enough. Then the iterate sequence {Xt}T 1 t=0 generated by FSLoRA satisfies / (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) (cid:32) 1 1 (cid:88) t= +O (σ2 + σ2 + σ2 h) HN F0 (cid:112) (cid:101)L HN (cid:112) (cid:101)L (cid:33) . (7) Discussion: The results obtained in Corollary 4.5 show the impacts of the variances associated with both the sketching matrix (i.e., σ2 g), as well as the data heterogeneity (i.e., σ2 h), on FSLoRAs convergence. Furthermore, the parameter (cid:101)L provides insight into how the sketching operation influences the convergence rate. Increasing ki would lead to faster convergence rate ) and the data sample ξ (i.e., σ2 6 5. Experiments In this section, we present experiments to evaluate the performance of the proposed FSLoRA. All the experiments are conducted on cluster equipped with 4 NVIDIA A100 GPUs, each with 40 GB of memory. The number of devices is set to 20 in our experiments. Other hyperparameter configurations can be found in Appendix A. (6) 5.1. Models and Datasets Our experiments are based on RoBERTa (125M) (Liu, 2019) and LLaMA-3.2-3B (3.21B) (Dubey et al., 2024). For RoBERTa, we fine-tune and evaluate it on the GLUE benchmark (Wang, 2018), which includes QNLI, MRPC, CoLA, MNLI, RTE, SST-2, and QQP tasks. For LLaMA-3.2-3B, we fine-tune and evaluate it on the Commonsense170K dataset (Hu et al., 2023), covering eight commonsense reasoning question-answering tasks: ARC-c, ARC-e, BoolQ, HellaSwag, OBQA, PIQA, SIQA, and WinoGrande. Further details on these datasets are provided in Appendix B. 5.2. Main Results Baselines for Heterogeneous LoRA Settings: We consider the following state-of-the-art baselines that integrate LoRA with FL: HeteroLoRA (Cho et al., 2024), FlexLoRA (Bai et al., 2024), and FedStackLoRA (Wang et al., 2024). In our approach, the rank of the global LoRA modules is fixed as = 64, while the sketching ratio for device is sampled from the set {0.125, 0.25, 0.5, 0.75}. For fair comparison, we apply the same rank configuration to all other baselines as in FSLoRA. RoBERTa Model: Table 5.1 presents performance comparison between the proposed FSLoRA and baseline methods in the heterogeneous LoRA scenario on the GLUE benchmark with the RoBERTa model. As shown in Table 5.1, compared with the baselines, our approach boosts the average performance across these seven tasks by noticeable margin. Concretely, FSLoRA consistently outperforms HeteroLoRA across all the considered tasks. In addition, our approach significantly outperforms FlexLoRA in most tasks, with marginal performance differences only in QNLI and QQP. Similarly, it outperforms FedStackLoRA on all tasks with the exception of the QQP task. It is worth noting that FSLoRA and HeteroLoRA maintain similar simplicity Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Table 5.1: Testing accuracy over 3 independent runs for fine-tuning the RoBERTa model on the GLUE benchmark. FSLoRA achieves notable improvement in average performance compared to the baselines. Method GPU hours QNLI MRPC CoLA MNLI RTE SST-2 QQP HeteroLoRA FlexLoRA FedStackLoRA FSLoRA 10.7h 12.6h 12.3h 10.9h 87.5 0.5 88.5 0.2 87.2 0.3 84.4 0.9 81.2 0.4 78.1 0. 75.3 1.2 77.5 1.2 77.4 1.7 66.3 0.8 63.0 0.5 74.6 0.5 69.0 1.7 62.2 1.9 54.4 2.1 89.5 0.0 92.8 0.4 93.4 0.1 85.3 0.1 87.4 0.1 87.1 0.3 88.0 0. 87.3 0.2 82.2 0.5 76.4 0.2 69.8 1.2 93.5 0.1 85.8 0. Avg. 79.6 78.9 78.9 83.3 Table 5.2: Testing accuracy over 3 independent runs for fine-tuning the LLaMA-3.2-3B model on the commonsense reasoning benchmark. FSLoRA demonstrates consistent performance improvement across these tasks compared to baselines. Method GPU hours ARC-c ARC-e BoolQ HellaSwag OBQA PIQA SIQA WinoGrande Avg. HeteroLoRA FlexLoRA FedStackLoRA FSLoRA 44.2h 60.8h 56.2h 44.5h 69.2 0.2 69.9 0.3 67.5 0. 84.6 0.2 84.7 0.4 83.1 0.5 68.4 0.5 66.9 0.2 65.8 0.9 80.0 0.7 80.5 0.4 78.4 0.5 69.9 0.0 72.30.1 69.20.7 77.3 0.0 78.10.2 75.5 0.6 68.7 0.3 70.4 0.4 67.1 0. 72.0 0.3 73.3 0.5 71.5 0.5 73.8 0.6 86.2 0.1 68.5 0.1 83.1 1.1 78.7 0. 82.0 0.2 75.8 0.0 74.8 0.6 73.8 74.5 72.3 77.9 in computation and communication complexity, whereas FlexLoRA and FedStackLoRA incur additional overhead, as detailed in Section 2.2. This increased computational complexity is also reflected in GPU hours, which represent the total computational time accumulated across all devices. As reported in Table 5.1, FSLoRA and HeteroLoRA demonstrate comparable computational efficiency, requiring similar GPU hours, whereas FlexLoRA and FedStackLoRA incur higher computational costs. In Figure 2, we present the averaged testing accuracy of the proposed algorithm alongside the baseline methods across seven tasks described in Section 5.1, plotted against the communication rounds, showcasing the convergence behavior of the proposed algorithm. FlexLoRA achieves fast initial convergence but falls behind in final accuracy due to the approximation errors introduced by its use of truncated SVD, which is effective early on but limits long-term performance. Similarly, HeteroLoRAs reliance on zero-padding reduces optimization efficiency, preventing it from achieving higher accuracy. FedStackLoRA also underperforms due to the random reinitialization of LoRA modules at each communication round, disrupting tuning continuity. In contrast, FSLoRA overcomes these limitations, achieving the highest final accuracy among all methods. LLaMA-3.2-3B Model: In Table 5.2, we scale up to LLaMA-3.2-3B, which contains 3.21 billion parameters, to evaluate the effectiveness of our proposed algorithm with increasing model complexity. The experimental results demonstrate that FSLoRA consistently achieves superior performance compared to baseline methods across all benchmark tasks. This advantage is particularly significant because fine-tuning LLaMA-3.2-3B, with its substantially larger parameter count, introduces greater challenges for deployment on heterogeneous distributed devices with limited Figure 2: Convergence behavior of FSLoRA and baselines on the GLUE benchmark with the RoBERTa model. Testing accuracy is averaged over seven tasks. computational and communication resources. This further demonstrates the superiority of the proposed FSLoRA. 5.3. Ablation Study Impact of Sketching: In Figures 3 and 4(a), we compare the performance of FSLoRA with and without sketching on finetuning the RoBERTa model and the LLaMA-3.2-3B model, respectively. For FSLoRA with sketching, we apply uniform sketching ratio of ki/r = 0.5 across all distributed devices. Notably, FSLoRA without sketching is equivalent to the vanilla federated LoRA. The upload budget for each device is set to 100 and 400 times the size of the full global LoRA modules at the corresponding rank for the RoBERTa and the LLaMA-3.2-3B models, respectively. As shown in Figures 3 and 4(a), both FSLoRA with and without sketching achieve higher accuracy when the rank increases, due to the availability of more tunable parameters. In addition, FSLoRA consistently outperforms its non-sketched counterpart across all the ranks and datasets. The use of sketching 7 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Figure 3: Comparison between FSLoRA with and without sketching, where the upload budget for devices is set to 100 the full global LoRA modules at the corresponding rank. The experiment is performed on the GLUE benchmark and the RoBERTa model. FSLoRA with sketching obtains better performance, validating the effectiveness of sketching. (a) Comparison of FSLoRA with and without sketching, with an upload budget 400 the global LoRA module size at each rank. (b) Impact of the rank of global LoRA modules on FSLoRA, given fixed rank for the updated submatrices at the devices. (c) Impact of the sketching ratio on FSLoRA under fixed rank = 64 for the global LoRA modules. Figure 4: Fine-tuning the LLaMA-3.2-3B model on the commonsense reasoning benchmark. The results are averaged over eight tasks as described in Section 5.1. increases the communication frequency for devices within the same communication budget, thereby facilitating the optimization process and enhancing fine-tuning efficiency. Impact of the Global Rank: In Figure 4(b), we investigate the impact of the rank of the global LoRA modules on FSLoRAs performance. We vary the rank of the global LoRA modules while keeping the rank of submatrices updated by the devices to be consistent (i.e., ki = 8). This ensures that the communication and computational resources on the client side remain unchanged. As illustrated in Figure 4(b), FSLoRA demonstrates improved performance as the global rank increases within the range considered. This observation validates that using large rank for the global LoRA modules results in more expressive model. Moreover, the proposed sketching mechanism enables resource-constrained systems to effectively benefit from large rank. Impact of Sketching Ratio: Finally, we investigate the impact of the sketching ratio on FSLoRAs performance by maintaining constant global LoRA rank = 64 while varying the sketching ratio ki/r in the range {0.125, 0.25, 0.5, 1}. From Figure 4(c), we see that there is slight performance degradation as the sketching ratio decreases, which aligns with our theoretical analysis. This observation reflects the inherent tradeoff: while larger sketching ratio enables better convergence, smaller sketching ratio reduces both computational and communication overhead. Notably, the slight performance degradation observed demonstrates FSLoRAs ability to effectively balance efficiency and accuracy, underscoring its advantage in scenarios with limited resources. Further Experiments: Additional results, including detailed comparisons on each task in the commonsense reasoning benchmark corresponding to Figures 4(a) and 4(b), the integration of communication compression and sketching, and the experiments with more devices, are provided in Appendix C. 6. Conclusion and Future Work We have proposed FSLoRA, novel on-device collaborative LLM fine-tuning framework that introduces sketching mechanism to enhance both performance and efficiency in resource-constrained systems. By maintaining large-rank LoRA modules on the server and allowing devices to selectively update submatrices based on the sketching ratios, FSLoRA effectively adapts to heterogeneous communication and computational constraints. We provide rigorous convergence analysis of FSLoRA that characterizes how 8 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models the sketching ratios affect the convergence rate. Finally, we confirmed the effectiveness of FSLoRA through extensive experiments across multiple datasets and models. direction for future work is to extend FSLoRA beyond LLM finetuning and explore its performance in pretraining, which remains an open area for further investigation."
        },
        {
            "title": "Impact Statement",
            "content": "This paper makes important contributions to on-device LLM fine-tuning by developing resource-adaptive algorithm for collaborative fine-tuning in resource-constrained systems. The focus of this work is on the technical advancement of LLM fine-tuning algorithms. While this research has potential societal impacts, it primarily addresses technical challenges and does not necessitate specific discussion on societal consequences."
        },
        {
            "title": "References",
            "content": "Bai, J., Chen, D., Qian, B., Yao, L., and Li, Y. Federated fine-tuning of large language models under heterogeneous tasks and client resources, 2024. URL https://arxiv.org/abs/2402.11505. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Byun, Y. and Lee, J. Towards federated low-rank adaptation of language models with rank heterogeneity. arXiv preprint arXiv:2406.17477, 2024. Chen, C., Feng, X., Zhou, J., Yin, J., and Zheng, X. Federated large language model: position paper. arXiv preprint arXiv:2307.08925, 2023. Cho, Y. J., Liu, L., Xu, Z., Fahrezi, A., and Joshi, G. Heterogeneous lora for federated fine-tuning of on-device foundation models. arXiv preprint arXiv:2401.06432, 2024. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fan, D., Messmer, B., and Jaggi, M. On-device collaborative language modeling via mixture of generalists and specialists. arXiv preprint arXiv:2409.13931, 2024. Fang, W., Yu, Z., Jiang, Y., Shi, Y., Jones, C. N., and Zhou, Y. Communication-efficient stochastic zeroth-order optimization for federated learning. IEEE Transactions on Signal Processing, 70:50585073, 2022. Fang, W., Han, D.-J., Chen, E., Wang, S., and Brinton, C. G. Hierarchical federated learning with multi-timescale gradient correction. arXiv preprint arXiv:2409.18448, 2024. Han, A., Li, J., Huang, W., Hong, M., Takeda, A., Jawanpuria, P., and Mishra, B. Sltrain: sparse plus low-rank approach for parameter and memory efficient pretraining. arXiv preprint arXiv:2406.02214, 2024. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. Hu, Z., Wang, L., Lan, Y., Xu, W., Lim, E.-P., Bing, L., Xu, X., Poria, S., and Lee, R. K.-W. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933, 2023. Kalajdzievski, D. rank stabilization scaling factor for fine-tuning with lora, 2023. URL https://arxiv. org/abs/2312.03732. Khaled, A., Mishchenko, K., and Richtarik, P. Tighter theory for local sgd on identical and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pp. 45194529. PMLR, 2020. Koo, J., Jang, M., and Ok, J. Towards robust and efficient federated low-rank adaptation with heterogeneous clients, 2024. URL https://arxiv.org/abs/ 2410.22815. Kuo, K., Raje, A., Rajesh, K., and Smith, V. Federated lora with sparse communication, 2024. Lester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021. Demidovich, Y., Malinovsky, G., Shulgin, E., and Richtarik, P. Mast: Model-agnostic sparsified training. arXiv preprint arXiv:2311.16086, 2023. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. 9 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Ye, R., Wang, W., Chai, J., Li, D., Li, Z., Xu, Y., Du, Y., Wang, Y., and Chen, S. Openfedllm: Training large language models on decentralized private data via federated learning. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 61376147, 2024. Yi, X., Zhang, S., Yang, T., and Johansson, K. H. Zerothorder algorithms for stochastic distributed nonconvex optimization. Automatica, 142:110353, 2022. Yu, H., Yang, S., and Zhu, S. Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pp. 56935700, 2019. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Zhang, J., Vahidian, S., Kuo, M., Li, C., Zhang, R., Yu, T., Wang, G., and Chen, Y. Towards building the fedIn ICASSP eratedgpt: Federated instruction tuning. 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 69156919. IEEE, 2024. Lialin, V., Muckatira, S., Shivagunde, N., and Rumshisky, A. Relora: High-rank training through low-rank updates. In The Twelfth International Conference on Learning Representations, 2023. Liu, Y. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 364, 2019. Malinovsky, G., Michieli, U., Hammoud, H. A. A. K., Ceritli, T., Elesedy, H., Ozay, M., and Richtarik, P. Randomized asymmetric chain of lora: The first meaningful theoretical framework for low-rank adaptation. arXiv preprint arXiv:2410.08305, 2024. McMahan, B., Moore, E., Ramage, D., Hampson, S., and Arcas, B. A. Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics, pp. 12731282. PMLR, 2017. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can new dataset arXiv preprint suit of armor conduct electricity? for open book question answering. arXiv:1809.02789, 2018. Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Sap, M., Rashkin, H., Chen, D., LeBras, R., and Choi, Y. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. Shuttleworth, R., Andreas, J., Torralba, A., and Sharma, P. Lora vs full fine-tuning: An illusion of equivalence. arXiv preprint arXiv:2410.21228, 2024. Sun, Y., Li, Z., Li, Y., and Ding, B. Improving lora in privacy-preserving federated learning. arXiv preprint arXiv:2403.12313, 2024. Wang, A. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. Wang, J., Liu, Q., Liang, H., Joshi, G., and Poor, H. V. novel framework for the analysis and design of heterogeneous federated learning. IEEE Transactions on Signal Processing, 69:52345249, 2021. Wang, Z., Shen, Z., He, Y., Sun, G., Wang, H., Lyu, L., and Li, A. Flora: Federated fine-tuning large language models with heterogeneous low-rank adaptations. arXiv preprint arXiv:2409.05976, 2024. Xia, W., Qin, C., and Hazan, E. Chain of lora: Efficient finetuning of language models via residual learning. arXiv preprint arXiv:2401.04151, 2024. 10 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models A. Details of Hyperparameters Table A.1: The hyperparameters for RoBERTa & GLUE and LLaMA-3.2-3B & Commensense Reasoning benchmarks. Hyperparameter Batch size LoRA dropout rate Learning rate, γ Communication round, Local iteration number, Number of edge devices, RoBERTa & GLUE LLaMA-3.2-3B & Commensense Reasoning 16 0.1 5e200 50 20 16 0.1 3e750 20 20 Target module [query, value, classification head] [q proj, proj, proj, up proj, down proj] 11 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models B. Details of Datasets B.1. GLUE Benchmark GLUE is widely recognized benchmark designed to assess the natural language understanding capabilities of language models (Wang, 2018). CoLA focuses on whether given sentence is acceptable according to linguistic rules. It evaluates models ability to recognize well-formed sentences. Input: single sentence. Output: label indicating whether the sentence is acceptable or unacceptable. SST-2 is designed for sentiment classification on movie reviews or short texts. It tests whether model can correctly identify positive or negative sentiment in given sentence. Input: single sentence. Output: label indicating positive or negative sentiment. MRPC checks if two sentences are paraphrases of each other, i.e., if they mean the same thing. Input: Two sentences (sentence1 and sentence2). Output: label indicating either equivalent or not equivalent. QQP tests models ability to determine if two questions ask the same thing. Input: Two questions. Output: label indicating duplicate or not duplicate. MNLI tests whether given hypothesis is entailed, contradicted, or neutral with respect to premise. Input: premise (first sentence) and hypothesis (second sentence). Output: label indicating entailment, contradiction, or neutral. QNLI aims to determine if context sentence correctly answers given question. Input: question and sentence. Output: label indicating the sentence answers the question or it does not. RTE provides pairs of sentences to see if one implies the other. Input: Two sentences (sentence1 and sentence2) Output: label indicating whether the meaning of one sentence is entailed from the other one. 12 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models B.2. Commonsense Reasoning Benchmark Table B.1: The prompt template of the Commonsense170K dataset (Hu et al., 2023)."
        },
        {
            "title": "Input Template",
            "content": "ARC-c/e Please choose the correct answer to the question: [QUESTION] Answer1: [ANSWER 1] Answer2: [ANSWER 2] Answer3: [ANSWER 3] Answer4: [ANSWER 4] Answer format: answer1/answer2/answer3/answer4 the correct answer is [ANSWER]"
        },
        {
            "title": "BoolQ",
            "content": "Please answer the following question with true or false, question: [QUESTION] Answer format: true/false the correct answer is [ANSWER] HellaSwag Please choose the correct ending to complete the given sentence: [ACTIVITY LABEL]: [CONTEXT] Ending1: [ENDING 1] Ending2: [ENDING 2] Ending3: [ENDING 3] Ending4: [ENDING 4] Answer format: ending1/ending2/ending3/ending4 the correct answer is [ANSWER] OBQA PIQA SIQA Please choose the correct answer to the question: [QUESTION] Answer1: [ANSWER 1] Answer2: [ANSWER 2] Answer3: [ANSWER 3] Answer4: [ANSWER 4] Answer format: answer1/answer2/answer3/answer4 the correct answer is [ANSWER] Please choose the correct solution to the question: [QUESTION] Solution1: [SOLUTION 1] Solution2: [SOLUTION 2] Answer format: solution1/solution2 the correct answer is [ANSWER] Please choose the correct answer to the question: [QUESTION] Answer1: [ANSWER 1] Answer2: [ANSWER 2] Answer3: [ANSWER 3] Answer format: answer1/answer2/answer3 the correct answer is [ANSWER] WinoGrande Please choose the correct answer to fill in the blank to complete the given sentence: [SENTENCE] Option1: [OPTION 1] Option2: [OPTION 2] the correct answer is [ANSWER] Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models The Commonsense170K dataset is mixture of multiple datasets including about 170K training samples from ARC-c/e (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), OBQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), and WinoGrande (Sakaguchi et al., 2021) datasets. ARC-c/e contains the challenge and easy question set from the ARC dataset of genuine grade-school level, multiplechoice science questions. BoolQ is question-answering dataset with yes/no questions derived from natural, real-world scenarios. HellaSwag includes questions for commonsense natural language inference, where context and multiple endings are given, requiring the most coherent ending to be selected. OBQA involves multi-step problem-solving that combines commonsense knowledge, reasoning, and comprehension of accompanying textual information. PIQA focuses on questions requiring physical commonsense to solve. Each question offers two answer choices. SIQA targets reasoning about human actions and their social implication. WinoGrande is designed as binary-choice fill-in-the-blank task, this dataset evaluates the ability to resolve ambiguous sentences through commonsense reasoning. The input template, i.e., prompt format for these datasets is detailed in Table B.1. 14 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models C. Further Experiments In this section, we provide additional results, including detailed per-task comparisons from the commonsense reasoning benchmark corresponding to Figures 4(a) and 4(b), the investigation of the integration of communication compression and sketching, and the experiments with more devices. C.1. Further Details of Ablation Study Impact of Sketching: In Figure 5, we compare the performance of FSLoRA with and without sketching on eight tasks from the commonsense reasoning benchmark using the LLaMA-3.2-3B model. For FSLoRA with sketching, we apply uniform sketching ratio of ki/r = 0.5 across all distributed devices. The uploading budget for each device is set to 400 times the size of the full global LoRA modules at the corresponding rank. It is clear that FSLoRA with sketching consistently outperforms its non-sketched counterpart across these eight tasks, demonstrating the effectiveness of sketching in improving performance. Figure 5: Comparison of FSLoRA with and without sketching, with an upload budget 400 the global LoRA module size at each rank. This is based on the commonsense reasoning benchmark and the LLaMA-3.2-3B model. We observe that the sketching mechanism improves performance across all considered tasks. The average accuracy of the eight tasks is shown in Figure 4(a). Impact of the Global Rank: In Figure 6, we present the impact of the rank of the global LoRA modules on FSLoRAs performance across eight tasks from the commonsense reasoning benchmark. We consider four configurations: 1) = 8, ki/r = 1, 2) = 16, ki/r = 0.5, 3) = 32, ki/r = 0.25, and 4) = 64, ki/r = 0.125. The rank of submatrices updated by the devices at each iteration remains consistent across all configurations (i.e., ki = 8), ensuring that the communication and computational resources on the client side are kept fixed for all cases. We see that the third subfigure exhibits oscillations as the sketching ratio increases. One potential explanation for this behavior is that the BoolQ task may be more sensitive to variations in the sketching ratio. Overall, FSLoRA demonstrates improved performance as the global rank increases. C.2. Integration of Sketching and Top-k Compression In Figure 7, we investigate the integration of sketching and communication compression, two orthogonal techniques. We employ top-k compression, widely used compression method that Kuo et al. (2024) introduced to LoRA, at each device to 15 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Figure 6: Impact of the rank of global LoRA modules on FSLoRA, given fixed rank for the updated submatrices at the devices. This is based on the commonsense reasoning benchmark and the LLaMA-3.2-3B model. Overall, FSLoRA demonstrates improved performance as the global rank increases. The average accuracy of the eight tasks is shown in Figure 4(b). reduce communication overhead before uploading model updates. The compression ratio is fixed at 0.5 for all considered methods, while the sketching ratio ki/r varies in the range {0.125, 0.25, 0.5, 1}. Notably, FSLoRA with sketching ratio ki/r = 1 is equivalent to the vanilla federated LoRA, meaning no sketching is applied. Figure 7 plots testing accuracy against communication overhead, where the x-axis represents the per-device upload communication load (MB). The results demonstrate that incorporating sketching further enhances efficiency, with lower sketching ratios leading to higher testing accuracy for the same communication cost, highlighting the benefits of combining these two orthogonal techniques. Figure 7: Comparison of top-k compression and its integration with sketching, evaluated on the commonsense reasoning benchmark using the LLaMA-3.2-3B model. The results show that combining these two orthogonal techniques significantly enhances performance, demonstrating the benefits of integrating sketching with top-k compression. Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models C.3. Experiments with More Devices Finally, we increase the number of devices to 50 and repeat the comparison of FSLoRA with and without sketching. Figure 8 shows the detailed per-task comparison while Figure 9 shows the averaged performance comparison across these eight tasks from the commonsense reasoning benchmark. We vary the rank of the global LoRA modules in the rage {4, 16, 64}. The sketching ratio is set to 0.5 for FSLoRA. This experiment is also based on the LLaMA-3.2-3B model. As shown in Figures 8 and 9, the advantages of FSLoRA are preserved as the number of devices increases. Figure 8: Comparison of FSLoRA with and without sketching, with an upload budget 400 the global LoRA module size at each rank, evaluated on the commonsense reasoning benchmark and the LLaMA-3.2-3B model. The number of devices is set to 50. We observe that the sketching mechanism improves performance across all considered tasks. The average accuracy of the eight tasks is shown in Figure 9. Figure 9: Comparison of FSLoRA with and without sketching, with an upload budget 400 the global LoRA module size at each rank, evaluated on the LLaMA-3.2-3B model. The number of devices is set to 50. The results are averaged over eight tasks from the commonsense reasoning benchmark. Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models D. Proof of the Theoretical Results D.1. Preliminaries Before presenting the proof of the main results, we first introduce some preliminary facts that will be used later. Lemma D.1. Suppose sequence of independent random matrices {Pi}N i=1 satisfy E[Pi] = 0, i. Then, (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) Pi ="
        },
        {
            "title": "1\nN 2",
            "content": "N (cid:88) i=1 Pi2 . Lemma D.2. (Wang et al., 2021) Suppose sequence of random matrices {Pi}N 0, i. Then, i=1 satisfy [Pi Pi1,Pi2, . . . , P1] = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) i=1 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) Pi = Pi2(cid:105) (cid:104) . (cid:88) i=1 Lemma D.3 (Random sketching bounds). Let be random diagonal sketching matrix of the form = (cid:88) jI ej , where e1, . . . , er Rr are standard unit basis vectors and {1, . . . , r} is chosen uniformly at random with = k. Then any matrix we have and in expectation we have (cid:104) ES S2(cid:105) X2. S2 r2 k2 X2, (8) (9) Proof. Since is diagonal with exactly diagonal entries equal to gives and the rest zero, its largest eigenvalue is . Squaring = S2 r2 k2 I, Equivalently, x(cid:0)S S(cid:1)x r2 k2 x2, x. Setting = xj to be the j-th column of and summing over implies S2 = xj2 = (cid:88) (cid:88) (S S) xj r2 (cid:88) xj2 = r2 k2 X2, which proves (8). For the expected bound (9), note that each diagonal index {1, . . . , r} is included in with probability expectation of S2 satisfies . Hence the E(cid:2)S2(cid:3) = r2 k2 (cid:104)(cid:88) (cid:105) ej = jI r2 k2 = I. Thus for any vector x, Sx2(cid:105) (cid:104) ES (cid:104) = ES S (cid:105) = x(cid:16) (cid:17) E[S2] = x2. Summing over columns of again establishes ES (cid:2)X S2(cid:3) = (cid:88) This completes the proof of Lemma D.3. ES (cid:2)Sxj2(cid:3) = (cid:88) (cid:16) (cid:17) E[S2] xj = 18 X2. Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models D.2. Proof of Lemma 3.2 From the chain rule for matrix calculus, we know that: Yg(XY) = Xg(XY), Xg(XY) = g(XY)Y, where g(XY) denotes the gradient of to XY. Applying this to ℓ(W0 + BSA, ξ), we proceed as follows: To compute the gradient with respect to B, set = and = SA: Bℓ(W0 + BSA, ξ) = ℓ(W0 + BSA, ξ)(SA). Similarly, to compute the gradient with respect to A, set = BS and = A: Aℓ(W0 + BSA, ξ) = SBℓ(W0 + BSA, ξ). D.3. Proof of Theorem 4.4 The proof of Theorem 4.4 relies on the following proposition. Proposition D.4. Under Assumption 4.1, fi(X; S) = fi(BS, A), Si, 1 (X) are smooth with parameters r2 k2 i=1 , ki (cid:16) 1 , and (cid:80)N (cid:80)N ki i=1 (cid:17) L, respectively. (X) = ESSi[ fi(X; S)], and (X) = The proof of Proposition D.4 is deferred to Appendix D.4. With this proposition, we are ready to prove Theorem 4.4. In FSLoRA, the update direction in (4) corresponds to the negative stochastic gradient of ℓ(W0 + BSA, ξ) with respect i. We have defined ℓ(X, ξ; S) = ℓ(W0 + BSA, ξ). The iterative equation for the proposed to [B; A] for given sketch St FSLoRA algorithm thus can be written as Xt+1 = Xt γ 1 (cid:88) H1 (cid:88) i= h=0 ℓ(Xt,h , ξt,h ; St i), (10) where gt,h we have denotes the stochastic gradient ℓ(Xt,h , ξt,h ; St i). Based on the smoothness of (X), i.e., Proposition D.4, E[f (Xt+1)] E[f (Xt)] Xf (Xt), γ (cid:42) (cid:124) (cid:123)(cid:122) T1 1 (cid:88) H1 (cid:88) i=1 h= gt,h + γ2Ls 2 (cid:43) (cid:125) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 (cid:124) (cid:88) H1 (cid:88) i= h=0 (cid:123)(cid:122) T2 gt,h , (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:125) (11) where Ls = (cid:16) 1 (cid:80)N i=1 ki (cid:17) L. 19 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models For T1, we have (cid:42) T1 = HE Xf (Xt), γ"
        },
        {
            "title": "1\nN H",
            "content": "N (cid:88) H1 (cid:88) i=1 h=0 (cid:43) gt,h (cid:42) = HE Xf (Xt), γ"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) H1 (cid:88) i=1 h=0 (cid:43) fi(Xt,h ; St i) (cid:42) = HE Xf (Xt), γ"
        },
        {
            "title": "1\nN H",
            "content": "N (cid:88) H1 (cid:88) (cid:43) Xf (Xt,h ) i= h=0 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) E"
        },
        {
            "title": "1\nN H",
            "content": "N (cid:88) H1 (cid:88) = γH 2 (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) γH 2 (cid:88) H1 (cid:88) i=1 h= Xf (cid:13) 2 (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) + γH 2 (cid:13) (cid:13) Xf (Xt) (cid:13) (cid:13) (cid:13)"
        },
        {
            "title": "1\nN H",
            "content": "Xf (cid:13) 2 (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) γH 2 (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) γH 2 i=1 h=0 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 N (cid:88) H1 (cid:88) i=1 h=0 Xf (cid:13) 2 (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) + γ 2 H1 (cid:88) h=0 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 (cid:88) i=1 Xf (Xt) 1 N (cid:88) i=1 Xf (cid:13) 2 (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) γH (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) γ 2H (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 (cid:88) H1 (cid:88) i=1 h=0 Xf (cid:13) 2 (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) + γHL2 2 1 (cid:88) i= r2 k2 H1 (cid:88) h=0 (cid:13) (cid:13)Xt,h (cid:13) Xt(cid:13) 2 (cid:13) (cid:13) , (12) where the last inequalities follow Jensens inequality and Proposition D.4. For T2, we have (cid:88) H1 (cid:88) i= h=0 (cid:88) H1 (cid:88) i=1 h=0 gt,h (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) gt,h 1 (cid:88) H1 (cid:88) i= h=0 fi(Xt,h ; St i) 1 (cid:88) H1 (cid:88) i=1 h=0 Xf i (Xt,h ) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 3E 1 (cid:88) H1 (cid:88) i=1 h=0 gt,h 1 (cid:88) H1 (cid:88) i=1 h=0 fi(Xt,h (cid:13) 2 (cid:13) ; St (cid:13) i) (cid:13) (cid:13) =E 1 1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) + 3E (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 = 3 2 (cid:88) i=1 Xf (cid:13) 2 (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) + 3E (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 N (cid:88) H1 (cid:88) i=1 h=0 Xf (cid:13) 2 (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) (cid:88) H1 (cid:88) fi(Xt,h ; St i) gt,h H1 (cid:88) h=0 fi(Xt,h (cid:88) H1 (cid:88) 1 i=1 h=0 (cid:13) 2 (cid:13) ; St (cid:13) i) (cid:13) (cid:13) h= H1 (cid:88) i=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) h=0 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) + 3 (cid:88) i=1 H1 (cid:88) h=0 fi(Xt,h ; St i) H1 (cid:88) h=0 Xf (cid:13) 2 (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) + 3E (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 (cid:88) H1 (cid:88) i=1 h=0 Xf (cid:13) 2 (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) 3H + σ2 σ2 + 3E (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 (cid:88) H1 (cid:88) i= h=0 Xf (cid:13) 2 (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) , (13) where the last equality comes from Lemma D.1 while the last inequality follows Lemma D.2 and Assumption 4.2. 20 Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Combining (11), (12), and (13) gives rise to E[f (Xt+1)] E[f (Xt)] γH 2 (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) γ 2H (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) H1 (cid:88) i=1 h=0 Xf (cid:13) 2 (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) + γHL2 2 + 3γ2Ls 2 (cid:88) H1 (cid:88) i=1 h=0 + σ2 σ2 N"
        },
        {
            "title": "1\nN H\n\n",
            "content": "H (cid:13) (cid:13)Xt,h (cid:13) Xt(cid:13) 2 (cid:13) (cid:13) + (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) H1 (cid:88) Xf E[f (Xt)] γH 2 (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) 2 (cid:13) (cid:13) (Xt,h (cid:13) ) (cid:13) (cid:13) + σ2 σ2 i=1 + h=0 3γ2Ls 2 Xt(cid:13) 2 (cid:13) (cid:13) (cid:125) , (cid:13) (cid:13)Xt,h (cid:13) (14) + γHL2 2 (cid:88) i=1 r2 k2 i"
        },
        {
            "title": "1\nN H\n(cid:124)",
            "content": "H1 (cid:88) h=0 (cid:123)(cid:122) T3 where the second inequality follows the condition γ 1 3HLs . For T3, we have T3 = 1 = 1 N (cid:88) i=1 (cid:88) i=1 r2 k2 r2 k2 H1 (cid:88) h=0 H1 (cid:88) h=0 (cid:13) (cid:13)Xt,h (cid:13) Xt(cid:13) 2 (cid:13) (cid:13) γ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) h1 (cid:88) gt,h 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) =γ2 1 (cid:88) i=1 r2 k2 3γ2H(σ + σ2 ) h1 (cid:88) τ =0 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:88) τ =0 H1 (cid:88) h=0 (cid:32) 1 (cid:33) (cid:33) (cid:33) r2 k2 r2 k2 r2 k2 i=1 (cid:88) i=1 (cid:88) i=1 =3γ2H(σ2 + σ2 ) 3γ2H(σ2 + σ2 ) (cid:32) (cid:32) 1 1 (cid:33) r2 k2 (cid:33) (cid:32) + 9 (cid:32) 1 3 1 (cid:88) i=1 (cid:88) i=1 r2 k2 gt,h h1 (cid:88) τ =0 Xfi(Xt,τ , St i) h1 (cid:88) τ = Xf 2 (cid:13) (cid:13) (Xt,τ (cid:13) ) (cid:13) (cid:13) + 3γ2 1 (cid:88) i=1 r2 k2 H1 (cid:88) h1 (cid:88) h=0 τ =0 (cid:13) (cid:13)Xf (Xt,τ )(cid:13) 2 (cid:13) + γ2 3 (cid:88) H1 (cid:88) h1 (cid:88) i=1 h=0 τ =0 (cid:13) (cid:13)Xf (Xt,τ ) Xf (Xt) Xf (Xt)(cid:13) 2 (cid:13) + 9γ2L2 1 (cid:88) i=1 r2 k2 (cid:32) chγ2H 2E (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) + 9 H1 (cid:88) h1 (cid:88) (cid:13) (cid:13)Xt,τ Xt(cid:13) 2 (cid:13) h= 1 (cid:88) i=1 τ =0 (cid:33) r2 k2 γ2H 2σ + 9 (cid:33) (cid:32) 1 (cid:88) i= r2 k2 γ2H 2E (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) γ2H(σ2 + σ2 ) + (cid:33) (cid:32) 1 (cid:88) i=1 r2 k2 γ2H 2σ2 + 9γ2L2H 2 1 (cid:124) (cid:88) i=1 r2 k2 H1 (cid:88) (cid:13) (cid:13)Xt,τ Xt(cid:13) 2 (cid:13) τ =0 (cid:123)(cid:122) T3 (cid:125) (cid:32) +9 (cid:33) 1 (cid:88) i= r2 k2 (ch + 1)γ2H 2E (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) , (15) where the first inequality equality comes from Lemma D.2 and Assumption 4.2 and the second inequality is due to Assumption 4.3. Therefore, it follows that (1 9γ2L2H 2)T3 3 (cid:33) (cid:32) 1 (cid:88) i= r2 k2 γ2H 2(σ2 + σ2 + 3σ2 h) + 9 (cid:33) (cid:32) 1 (cid:88) i=1 r2 k2 (ch + 1)γ2H 2E (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) . 21 3γ2Ls 2 (cid:17) Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Based on the condition on γ outlined in Theorem 4.4, we have γ 1 that 18HL . We thus obtain 1 9γ2L2H 2 1 2 . It follows T3 6 (cid:33) (cid:32)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 r2 k2 γ2H 2(σ2 + σ2 + 3σ h) + 18 (cid:33) (cid:32)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 r2 k2 (ch + 1)γ2H 2E (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) . (16) Plugging (16) into (14) gives rise to E[f (Xt+1)] E[f (Xt)] (cid:32) γH 2 (cid:32) 9 (cid:33)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 r2 k2 (ch + 1)γ3H 3L2 (cid:33) (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) + 3 (cid:32)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 E[f (Xt)] (cid:33) r2 k2 γH 4 (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) γ3H 3L2(σ2 + σ2 + 3σ2 h) + 3γ2Ls + σ2 σ2 + 3γ3H 3L2 s(σ2 + σ2 + 3σ h) + (σ2 + σ2 ). (17) Due to γ (cid:115)(cid:18) 6 1 inequality, we have (cid:19) 1 r2 k2 (cid:80)N i=1 (ch+1)HL , we have γH 2 9 (cid:16) 1 (cid:80)N i=1 r2 k2 (ch + 1)γ3H 3L2 γH 4 . Reorganizing the above (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) 4 E[f (Xt)] E[f (Xt+1)] γH + 12 (cid:33) (cid:32) 1 N (cid:88) i=1 r2 k2 γ2H 2L2(σ2 + σ2 + 3σ h) + 6γLs + σ2 σ2 . Telescoping the above inequality from = 0 to 1 and utilizing γ 1 2L , we have 1 T 1 (cid:88) t=0 (cid:13) (cid:13)Xf (Xt)(cid:13) 2 (cid:13) 4 (X0)E[f (XT )] γT +12 (cid:32) 1 (cid:88) i=1 r2 k2 (cid:33) γL This completes the proof of Theorem 4.4. (σ2 +σ2 +3σ h)+6 (cid:33) (cid:32) 1 (cid:88) i= ki γL +σ2 σ2 . D.4. Proof of Proposition D.4 i) For illustration, we need to recover to [B; A] in this proof. According to the definition of fi(X; S) and fi(B, A), we have fi(X; S) = fi(B, A; S) =EξDi [ℓ(W0 + BSA, ξ)] =fi(BS, A). As fi(B, A) is L-smooth, we have fi(BS + BS, + A) fi(BS, A) + (cid:21) (cid:28)(cid:20)BSfi(BS, A) Afi(BS, A) , (cid:21)(cid:29) (cid:20)BS + 2 (cid:13) (cid:20)BS (cid:13) (cid:13) (cid:13) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) . (18) (19) (20) According to (18) and (19), we have fi(B + B, + A; S) = fi(BS + BS, + A) and fi(B, A; S) = fi(BS, A). Combining these with (20) gives rise to fi(B + B, + A; S) fi(B, A; S) + (cid:28)(cid:20)BSfi(BS, A) Afi(BS, A) (cid:21) , (cid:21)(cid:29) (cid:20)BS + (cid:13) (cid:20)BS (cid:13) (cid:13) (cid:13) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) . We denote L(W0 + BSA) = fi(B, A; S) = EξDi [ℓ(W0 + BSA, ξ)] . (21) (22) Federated Sketching LoRA: On-Device Collaborative Fine-Tuning of Large Language Models Note that BSfi(BS, A) = L(W0 + BSA)A and Afi(BS, A) = SBL(W0 + BSA). We thus have (cid:28)(cid:20)BSfi(BS; A) (cid:21) Afi(BS; A) , (cid:21)(cid:29) (cid:20)BS = = = (cid:21) (cid:28)(cid:20) L(W0 + BSA)A SBL(W0 + BSA) (cid:21) (cid:28)(cid:20)L(W0 + BSA)AS SBL(W0 + BSA) (cid:20)B (cid:21) fi(B, A; S) fi(B, A; S) (cid:28)(cid:20)B , (cid:21)(cid:29) , , , (cid:21)(cid:29) (cid:20)BS (cid:20)B (cid:21)(cid:29) (23) where the last equality follows the fact that fi(B, A; S) = L(W0 + BSA) defined in (22) and (cid:20)B (cid:21) fi(B, A; S) fi(B, A; S) = (cid:20)L(W0 + BSA)AS SBL(W0 + BSA) (cid:21) . Plugging (23) into (21) gives rise to fi(B + B, + A; S) fi(B, A; S) + (cid:28)(cid:20)B (cid:21) fi(B, A; S) fi(B, A; S) (cid:20)B , (cid:21)(cid:29) + 2 (cid:13) (cid:20)BS (cid:13) (cid:13) (cid:13) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) . (24) In particular, (cid:13) (cid:20)BS (cid:13) (cid:13) (cid:13) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:21)(cid:13) (cid:20)BS 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:20)B (cid:13) (cid:13) (cid:13) = r2 k2 (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) = BS2 + A2. From (8), we know BS2 r2 k2 B2. Therefore, we have . As result, fi(B, A; S) (i.e., fi(X, S)) is r2 k2 -smooth. ii) Note that (X) = ESSi[ fi(X, S)]. Therefore, we further take expectation for (24) over Si, leading to (B + B, + A) S (B, A) + (cid:28)(cid:20)Bf Af (cid:21) (B, A) (B, A) (cid:20)B , (cid:21)(cid:29) + 2 ESSi (cid:13) (cid:20)BS (cid:13) (cid:13) (cid:13) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) . In particular, ESSi words, ESSi (cid:13) (cid:20)BS (cid:13) (cid:13) (cid:13) (cid:21)(cid:13) (cid:13) (cid:20)BS 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) = ki = ESSiBS2 + A2. From (9), we know ESSiBS2 ki (cid:13) (cid:20)B (cid:13) (cid:13) (cid:13) . We thus claim that (B, A) (i.e., (X)) is ki (cid:21)(cid:13) 2 (cid:13) (cid:13) (cid:13) -smooth. B2. In other iii) Finally, for (X) = 1 (cid:80)N i=1 (X), we have Since i (X) is ki -smooth, we thus have (X) = 1 (cid:88) i= i (X). i (X) i (Y) ki Y, X, Y. To find the Lipschitz constant of (X), we analyze the difference between the gradients at two points and Y: (X) (Y) = (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 1 N (cid:88) i=1 (cid:0)f (X) i (Y)(cid:1) 1 (cid:32) Therefore, (X) is (cid:16) 1 (cid:80)N i=1 ki (cid:17) -smooth. (cid:88) i=1 (cid:13) (cid:13)f (X) i (Y)(cid:13) (cid:13) (cid:33) X . 1 (cid:88) i=1 ki (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (25)"
        }
    ],
    "affiliations": [
        "Department of Computer Science and Engineering, Yonsei University",
        "Department of Electrical Engineering, University at Buffalo-SUNY",
        "Department of Electrical and Computer Engineering, Purdue University"
    ]
}