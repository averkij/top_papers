{
    "paper_title": "PoseLess: Depth-Free Vision-to-Joint Control via Direct Image Mapping with VLM",
    "authors": [
        "Alan Dao",
        "Dinh Bach Vu",
        "Tuan Le Duc Anh",
        "Bui Quang Huy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper introduces PoseLess, a novel framework for robot hand control that eliminates the need for explicit pose estimation by directly mapping 2D images to joint angles using projected representations. Our approach leverages synthetic training data generated through randomized joint configurations, enabling zero-shot generalization to real-world scenarios and cross-morphology transfer from robotic to human hands. By projecting visual inputs and employing a transformer-based decoder, PoseLess achieves robust, low-latency control while addressing challenges such as depth ambiguity and data scarcity. Experimental results demonstrate competitive performance in joint angle prediction accuracy without relying on any human-labelled dataset."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 1 1 1 7 0 . 3 0 5 2 : r PoseLess: Depth-Free Vision-to-Joint Control via Direct Image Mapping with VLM Alan Dao (Gia Tuan Dao)1, Dinh Bach Vu1 ,Tuan Le Duc Anh1, Bui Quang Huy1 Menlo Research alan@menlo.ai, bach@menlo.ai, charles@menlo.ai, yuuki@menlo.ai 1Equal contribution. February 20,"
        },
        {
            "title": "Abstract",
            "content": "This paper introduces PoseLess, novel framework for robot hand control that eliminates the need for explicit pose estimation by directly mapping 2D images to joint angles using projected representations. Our approach leverages synthetic training data generated through randomized joint configurations, enabling zero-shot generalization to real-world scenarios and cross-morphology transfer from robotic to human hands. By projecting visual inputs and employing transformer-based decoder, PoseLess achieves robust, low-latency control while addressing challenges such as depth ambiguity and data scarcity. Experimental results demonstrate competitive performance in joint angle prediction accuracy without relying on any human-labelled dataset. Figure 1: How PoseLess works"
        },
        {
            "title": "1 Introduction",
            "content": "Robotic hand control has traditionally relied on explicit pose estimation to bridge the gap between visual perception and motor execution. This process typically involves extracting keypoints or 3D skeletal representations from images before translating them into joint commands. While effective, such approaches suffer from inherent limitations, including accumulated errors from multi-stage processing pipelines, sensitivity to occlusions, and dependency on high-quality labeled datasets. Furthermore, conventional methods often require careful calibration between the vision and actuation systems to maintain accuracy. Recent advances in computer vision and deep learning have opened new possibilities for direct perception-tocontrol models that bypass intermediate pose estimation. By leveraging large-scale vision-language models (VLMs) with robust image projection strategies, it is now feasible to map high-dimensional image inputs directly to control outputs without requiring an explicit representation of pose. This paradigm shift enables more flexible and efficient control policies that are less susceptible to the limitations of traditional approaches. 1.1 Motivation The motivation for PoseLess stems from the need for more robust, scalable, and data-efficient approach to robotic hand control. Traditional pose-based methods struggle in scenarios where depth estimation is unreliable, such as monocular vision setups, or where the diversity of hand morphologies introduces additional complexity. PoseLess offers novel alternative by leveraging vision-language model (VLM) to project visual inputs and decode them directly into joint angles. This approach not only simplifies the control pipeline but also enhances generalization across different hand morphologies, including human-to-robot transfer. By training on synthetic data with extensive domain randomization, our method eliminates the dependency on manually labeled datasets and ensures adaptability to real-world variations. 1.2 Contributions Our key contributions are as follows: We introduce novel framework that leverages VLM (e.g., Qwen 2.5 3B Instruct) to directly map monocular images to robot joint angles, bypassing pose estimation entirely. The VLMs ability to see 1 and project images enables robust, morphologyagnostic feature extraction, reducing error propagation inherent in two-stage pipelines. We introduce synthetic data pipeline generates infinite training examples by randomizing joint angles and domain-randomizing visual features (e.g., lighting, textures). This eliminates reliance on costly labeled datasets while ensuring robustness to realworld variations. We provide evidence of the models cross-morphology generalization, demonstrating its ability to mimic human hand movements despite being trained solely on robot hand data. These findings mark significant step toward understanding and leveraging such generalization for broader applications. We provide evidence that depth-free control is possible paving way for later adoption with camera that is not supporting depth estimation capability that is frequently used in robotics research."
        },
        {
            "title": "2 Related Work",
            "content": "Our work draws inspiration from and builds upon several key areas in robotics and computer vision: traditional pose-based control, direct mapping approaches, image tokenization in robotics, poseless control paradigms, synthetic data generation, and cross-morphology transfer learning. [2022]. Our work aligns with this trend by directly mapping images to joint angles; however, we distinguish ourselves by leveraging the power of VLMs for feature extraction and by relying entirely on synthetic training data. The work in Fu et al. [2022] explores direct joint angle estimation from multi-view images for full-body human kinematics, highlighting the potential of deep learning for bypassing pose estimation in broader contexts. Similarly, Sepahvand et al. [2024] demonstrates direct imageto-joint mapping for specific continuum arm, but it does not address the challenges of generalizing across diverse hand morphologies."
        },
        {
            "title": "Models",
            "content": "The use of image projection for robotic control is inspired by recent advancements in vision-language models (VLMs) Brohan et al. [2023], Pertsch et al. [2025]. VLMs, such as those used in Palo and Johns [2024], Kim et al. [2024], Black et al. [2024], have demonstrated remarkable capabilities in understanding and reasoning about visual scenes by converting images into discrete tokens that can be processed by transformer architectures. Our approach leverages the inherent ability of VLMs Bai et al. [2025] to extract rich, contextualized visual features through tokenization, enabling more robust and generalizable mapping from images to joint angles compared to methods relying on hand-crafted features or convolutional neural networks. 2.4 Poseless Control and Cross-Morphology 2.1 Pose-Based Control Generalization Traditional robotic hand control heavily relies on explicit pose estimation as an intermediary step Wen et al. [2020]. Methods in this category typically involve detecting hand keypoints Zimmermann et al. [2019] or reconstructing 3D hand meshes Hasson et al. [2019] from images, followed by mapping these estimated poses to joint configurations. While demonstrating effectiveness in controlled environments, these techniques often suffer from compounding errors in the multi-stage pipeline, are sensitive to occlusions, and require large amounts of labeled data for training. Moreover, accurate depth estimation, often crucial for robust pose recovery, can be challenging in monocular vision setups commonly used in robotics. 2.2 Direct Mapping Approaches In contrast to pose-based methods, direct mapping approaches seek to establish direct correspondence between visual input and control outputs, bypassing explicit pose estimation. Early work in this area includes end-to-end learning for visuomotor control Levine et al. [2016], where deep neural networks are trained to map raw pixel data to motor commands. More recent studies have explored direct mapping for robotic manipulation tasks Kalashnikov et al. [2018], often using demonstrations or teleoperation data for training Sivakumar et al. The concept of poseless control, where robot actions are guided without explicit pose reconstruction, has been explored in various contexts. Research in tactile-based manipulation Yin et al. [2023] demonstrates that complex tasks can be achieved using only touch sensing, eliminating the need for visual pose estimation. Similarly, direct gesture recognition systems Pascher et al. [2024] map hand movements to robot commands without explicitly extracting pose information. Our work extends the notion of poseless control to the realm of dexterous manipulation by directly mapping visual inputs to joint angles. Furthermore, we demonstrate the potential for cross-morphology generalization, transferring control policies learned from synthetic robot hand data to real human hands, capability rarely explored in prior work Ying et al. [2024], Zare et al. [2023]. 2.5 Synthetic Data and Domain Randomization Training robotic control policies in simulation using synthetic data offers numerous advantages, including costeffectiveness, scalability, and safety. Domain randomization techniques, which involve randomizing visual and physical properties of the simulated environment Tobin et al. [2017], Peng et al. [2018], have proven effective in bridging the gap between simulation and reality. Our 2 approach relies entirely on synthetic data generated by randomizing joint angles and applying domain randomization to visual features, enabling robust performance in real-world scenarios without requiring any real-world training data. The study in Meattini et al. [2022] highlights the importance of careful simulative evaluation for robot hand control, reinforcing the validity of our approach."
        },
        {
            "title": "3 Methodology",
            "content": "This section details the innovative methodology for generating large-scale synthetic dataset, crucial for training our Poseless image-to-joint angle mapping model. Addressing the limitations of traditional pose-based control and the data dependencies of direct mapping approaches highlighted in Section 2, our approach uniquely relies on fully synthetic data, generated through randomized joint angle configurations within controlled rendering environment. This eliminates the need for real-world labeled data, significant departure from conventional methods and key enabler of our cross-morphology generalization capabilities. 3.1 Articulated Hand Model and Joint Space Definition We utilized detailed 3D model of shadow-hand as the basis for our synthetic data generation. Following the joint structure reflected in our target output format, we defined joint space encompassing 25 degrees of freedom. These joints, denoted as lh WRJ2, lh WRJ1, lh FFJ4 through lh THJ1, capture the essential articulation of human-like hand. For each joint j, we established physiologically plausible angle ranges [min anglej, max anglej], informed by biomechanical studies of human hand dexterity and the operational limits of typical robot hand actuators An et al. [1979]. These ranges were carefully selected to ensure the generation of realistic and diverse hand poses, while avoiding physically impossible configurations. 3.2 Randomized Joint Angle Sampling and Pose Generation To create diverse dataset of hand poses, we implemented randomized joint angle sampling procedure. For each synthetic data instance, we independently sampled each of the 25 joint angles from uniform distribution within its pre-defined range: θj U(min anglej, max anglej) {1, 2, ..., 25} (1) This uniform sampling strategy ensures broad exploration of the joint configuration space, generating wide variety of hand postures. This contrasts with datasets derived from human demonstrations or pose estimation pipelines, which may exhibit biases towards common or easily trackable poses, as discussed in Section 2 regarding limitations of pose-based methods Wen et al. [2020], Zimmermann et al. [2019]. The sampled joint angles Θ = {θ1, θ2, ..., θ25} were then applied to the 3D hand model within the MuJoCo rendering environment, generating corresponding 3D hand pose."
        },
        {
            "title": "3.3 Controlled Synthetic Image Rendering",
            "content": "To create consistent and controlled visual environment for our synthetic data, we employed fixed rendering parameters, focusing on the variation introduced solely through joint angle randomization. Specifically, we used: Fixed Lighting: single, fixed light source was positioned at the center of the scene, providing consistent illumination across all rendered images. Fixed Camera Angle: The camera position and orientation were fixed throughout the data generation process, ensuring consistent viewpoint. White Background: All images were rendered with plain white background, eliminating background clutter and focusing the visual input on the hand itself. While we maintain controlled visual environment with fixed lighting, camera angle, and background, the randomization of hand textures and materials still introduces visual diversity within the dataset, ensuring the model learns features robust to variations in hand appearance. This approach allows us to isolate the effect of joint angle variation on the visual input, simplifying the learning task while still providing sufficient visual variability for robust training. 3.4 Synthetic Data Pair Generation and Dataset Creation For each randomized joint angle configuration Θ and controlled rendering, we generated synthetic data pair. The input component was the rendered monocular image, saved as PNG file. The output component was the corresponding set of 25 joint angles, formatted as an XML-like string: <lh_WRJ2>angle</lh_WRJ2><lh_WRJ1>angle</lh_WRJ1> ...<lh_THJ1>angle</lh_THJ1> This specific format was chosen to align with the input requirements of the Vision Language Model (VLM) architecture, Qwen 2.5 3B Instruct Bai et al. [2025], which is designed to process text-based instructions and structured outputs. By iteratively repeating the randomized sampling and rendering process, we generated synthetic dataset of 100,000 image-joint angle pairs. This large-scale, diverse dataset is crucial for effectively training the VLM to learn the complex, non-linear mapping from raw pixel inputs directly to joint angles, bypassing the need for explicit pose estimation and contributing to the novelty of our Poseless approach as highlighted in Section 1.1."
        },
        {
            "title": "3.5 Advantages of Synthetic Data with Con-",
            "content": "trolled Rendering Our synthetic data generation methodology, employing randomized joint angles within controlled rendering environment, offers significant advantages: generated using the randomized joint angle methodology and controlled rendering environment detailed in Section 3. The training objective was to minimize the Mean Squared Error (MSE) between the predicted joint angle values and the ground truth joint angles from the synthetic dataset. The MSE loss was calculated as: Elimination of Data Bottleneck: By generating data synthetically, we overcome the limitations of costly and time-consuming real-world data collection and annotation, major hurdle for pose-based methods and direct mapping techniques relying on real-world demonstrations, as discussed in Section 2 Wen et al. [2020], Sivakumar et al. [2022]. Focused Data Diversity on Joint Angles: By isolating joint angle variations and controlling other visual factors, we ensure that the model primarily learns the relationship between joint configurations and visual appearance, potentially simplifying the learning task and improving performance for joint angle estimation. Perfect Ground Truth: Synthetic data provides noise-free and perfectly accurate ground truth joint angle values, eliminating the inaccuracies and ambiguities inherent in pose estimation pipelines and real-world labeling processes. Scalability and Reproducibility: The data generation process is fully automated, scalable, and reproducible, facilitating future research and allowing for the creation of even larger datasets if needed. MSE ="
        },
        {
            "title": "1\nN × J",
            "content": "N (cid:88) (cid:88) i=1 j=1 (ˆθij θij)2 (2) where is the number of samples, = 25 is the number of joints, ˆθij is the predicted angle for joint in sample i, and θij is the corresponding ground truth angle. The model was trained for 4500 steps and evaluated on held-out validation set (also synthetically generated using the same procedure). Performance was assessed based on the MSE of the predicted joint angles."
        },
        {
            "title": "4.2 Evaluation Metrics and Results",
            "content": "To evaluate the performance of our fine-tuned model across different training checkpoints, we measured the following metrics on the validation set: Average MSE, Standard Deviation of MSE, Minimum MSE, and Maximum MSE across all 25 joint angles and validation samples. To visualize the trend of average MSE over training checkpoints, we present line chart in Figure 2. Average MSE vs. Steps 102 1. Avg MSE This synthetic data-driven training approach, with its controlled rendering environment and focus on joint angle randomization, is central to our Poseless frameIt enables the development of robust, datawork. efficient, and generalizable image-to-joint angle mapping model that breaks away from the limitations of traditional pose-based control and opens new avenues for cross-morphology robotic manipulation. The subsequent sections will detail the architecture of our VLM-based Poseless model and the training procedure employed using this synthetic dataset. g 1 0.8 0. 0.4 1,500 2,000 2,500 3,000 3,500 4,000 4,500 Checkpoint"
        },
        {
            "title": "4 Experiments and Results",
            "content": "To validate the effectiveness of our Poseless approach, we conducted fine-tuning experiments using the synthetically generated dataset described in Section 3. This section details the experimental setup, evaluation metrics, and results obtained. 4.1 Experimental Setup We employed the pre-trained Vision Language Model Qwen2.5-VL-3B-Instruct Bai et al. [2025] as the foundation for our Poseless model. This VLM was fine-tuned on dataset of 100,000 synthetic image-joint angle pairs, Figure 2: Line chart depicting the Average MSE for different training checkpoints. Figure 2 visually confirms the trend in MSE across checkpoints. The average MSE generally decreases as training progresses up to checkpoint cp-3500, after which it starts to slightly increase, suggesting point of diminishing returns or potential overfitting beyond this checkpoint. The relatively low average MSE achieved, particularly at checkpoint cp-3500, demonstrates the feasibility of our Poseless approach for learning direct image-tojoint angle mappings using synthetic data and VLM architecture."
        },
        {
            "title": "5 Discussion",
            "content": "The experimental results underscore the potential of the PoseLess framework as robust alternative to traditional pose-based control. By directly mapping monocular images to joint angles through projected representations, our approach avoids the compounded errors typically associated with multi-stage pipelines. The observed reduction in mean squared error (MSE) up to checkpoint cp3500 illustrates that the model effectively learns the underlying mapping between visual features and joint configurations, even when trained solely on synthetic data Levine et al. [2016]. One notable insight from our experiments is the balance between training progress and overfitting. While performance steadily improves with additional training, the slight increase in MSE past checkpoint cp-3500 indicates point of diminishing returns. This suggests that the benefits of extended training may be offset by the risk of overfitting, especially in scenarios where synthetic data may not fully capture the variability present in real-world conditions Tobin et al. [2017]. The reliance on synthetic data generated with controlled rendering parameters offers clear advantages. The use of randomized joint configurations and domain randomization minimizes dependency on laborintensive, manually labeled datasets. This approach not only provides noise-free ground truth for each sample but also ensures scalability and reproducibility. However, the controlled nature of the synthetic environmentcharacterized by fixed lighting, camera angle, and uniform backgroundmay limit the exposure of the model to the diverse conditions encountered in real-world settings. Future work should explore incorporating additional visual variability to further enhance the models robustness Peng et al. [2018]. Another compelling aspect of PoseLess is its demonstrated ability for cross-morphology generalization. Although the model was trained exclusively on robotic hand data, preliminary results suggest that it can effectively transfer learned control policies to human hand configurations. This finding opens up promising avenues for applications in prosthetics and human-robot interaction, where adaptability across different hand morphologies is critical Ying et al. [2024]. Nevertheless, comprehensive evaluation across wider range of hand types and dynamic environments is essential to fully validate this capability. Finally, while the current implementation has achieved competitive performance, several challenges remain. Integrating real-world data into the training regime, refining the image projection process, and extending the framework to accommodate multi-view or temporal data could provide further performance improvements. Addressing these limitations will be key to realizing the full potential of direct image-to-joint mapping in diverse and uncontrolled settings. In summary, PoseLess represents significant step toward more efficient and adaptable robotic hand control. The frameworks ability to bypass explicit pose estimation and operate effectively using synthetic data not only simplifies the control pipeline but also paves the way for future innovations in cross-morphology generalization and depth-free control."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduced PoseLess, novel framework for depth-free, vision-to-joint control of robotic and human hands. By leveraging the power of projected representations and Vision Language Models (VLMs), PoseLess successfully circumvents the need for explicit pose estimation, directly mapping monocular images to joint angles. Our experiments demonstrated that training solely on synthetic data generated through randomized joint configurations and domain randomization techniques yields competitive performance in joint angle prediction accuracy, specifically achieving reduced mean squared error up to checkpoint cp-3500. This validates the efficacy of the poseless control paradigm in addressing challenges like depth ambiguity and data scarcity. The results further suggest the potential for crossenabling control policies morphology generalization, learned from robotic hand data to be effectively transferred to human hands. This capability, coupled with the inherent advantages of using noise-free and scalable synthetic dataset, positions PoseLess as promising solution for future robotic applications, including prosthetics and human-robot interaction. The elimination of depth dependency further simplifies hardware requirements, broadening the accessibility and potential applications of this technology. While the controlled synthetic environment offers benefits, it also presents limitation in terms of exposure to real-world variability. Future research will focus on mitigating this limitation by incorporating real-world data and enriching the synthetic dataset with increased visual variability. Further enhancements will involve exploring the integration of multi-view or temporal data to enhance robustness in diverse and unconstrained settings, and rigorously evaluating the cross-morphology capabilities in more dynamic environments. In conclusion, PoseLess marks significant step toward more efficient, adaptable, and dataefficient robotic hand control, opening new avenues for research and application in both robotics and beyond. The frameworks ability to learn complex visuomotor mappings directly from images, without relying on explicit pose or depth information, represents paradigm shift with the potential to revolutionize robotic manipulation and human-robot interaction."
        },
        {
            "title": "References",
            "content": "Kai-Nan An, Edmund Chao, William Cooney III, and Ronald Linscheid. Normative model of human hand for biomechanical analysis. Journal of biomechanics, 12(10):775788, 1979. 5 Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: vision-language-action flow model for general robot control, 2024. URL https://arxiv.org/ abs/2410.24164. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale, 2023. URL https://arxiv.org/abs/2212.06817. Zipeng Fu, Xuxin Cheng, and Deepak Pathak. Deep whole-body control: Learning unified policy for manipulation and locomotion, 2022. URL https: //arxiv.org/abs/2210.10044. Yana Hasson, Gul Varol, Dimitrios Tzionas, Igor Kalevatykh, Michael J. Black, Ivan Laptev, and Cordelia Schmid. Learning joint reconstruction of hands and manipulated objects, 2019. URL https://arxiv.org/ abs/1904.05767. Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, and Sergey Levine. Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation, 2018. URL https://arxiv.org/abs/1806.10293. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model, 2024. URL https:// arxiv.org/abs/2406.09246. Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies, 2016. URL https://arxiv.org/abs/1504. 00702. R. Meattini, D. Chiaravalli, G. Palli, and C. Melchiorri. Simulative evaluation of joint-cartesian hybrid motion mapping for robot hands based on spatial in-hand information. Frontiers in Robotics and AI, 9:878364, June 2022. doi: 10.3389/frobt.2022.878364. Norman Di Palo and Edward Johns. Keypoint action tokens enable in-context imitation learning in robotics, 2024. URL https://arxiv.org/abs/2403.19578. Hands-on robotics: Max Pascher, Alia Saad, Jonathan Liebers, Roman Heger, Jens Gerken, Stefan Schneegass, and Uwe Enabling comGruenefeld. In munication through direct gesture control. Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction, HRI 24, page 822827. ACM, March 2024. 10. 1145/3610978.3640635. URL http://dx.doi.org/10. 1145/3610978.3640635. doi: Xue Bin Peng, Marcin Andrychowicz, Wojciech Zaremba, Sim-to-real transfer of robotic and Pieter Abbeel. control with dynamics randomization. In 2018 IEEE International Conference on Robotics and Automation (ICRA), page 38033810. IEEE, May 2018. doi: 10.1109/icra.2018.8460528. URL http://dx.doi.org/ 10.1109/ICRA.2018.8460528. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models, 2025. URL https://arxiv.org/abs/2501.09747. Shayan Sepahvand, Guanghui Wang, and Farrokh JanabiSharifi. Image-to-joint inverse kinematic of supportive continuum arm using deep learning. Proceedings of the Conference on Robots and Vision, May 2024. doi: 10.21428/d82e957c.d8706a7c. URL http: //dx.doi.org/10.21428/d82e957c.d8706a7c. Aravind Sivakumar, Kenneth Shaw, and Deepak Pathak. Robotic telekinesis: Learning robotic hand imitator by watching humans on youtube, 2022. URL https: //arxiv.org/abs/2202.10448. Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world, 2017. URL https://arxiv. org/abs/1703.06907. Bowen Wen, Chaitanya Mitash, Sruthi Soorian, Andrew Kimmel, Avishai Sintov, and Kostas E. Bekris. Robust, occlusion-aware pose estimation for objects grasped by 6 adaptive hands. In 2020 IEEE International Conference on Robotics and Automation (ICRA), page 62106217. IEEE, May 2020. doi: 10.1109/icra40945. 2020.9197350. URL http://dx.doi.org/10.1109/ ICRA40945.2020.9197350. Zhao-Heng Yin, Binghao Huang, Yuzhe Qin, Qifeng Chen, and Xiaolong Wang. Rotating without seeing: Towards in-hand dexterity through touch, 2023. URL https://arxiv.org/abs/2303.10880. Chengyang Ying, Zhongkai Hao, Xinning Zhou, Xuezhou Xu, Hang Su, Xingxing Zhang, and Jun Zhu. Peac: Unsupervised pre-training for cross-embodiment reinforcement learning, 2024. URL https://arxiv.org/ abs/2405.14073. Maryam Zare, Parham M. Kebria, Abbas Khosravi, and Saeid Nahavandi. survey of imitation learning: Algorithms, recent developments, and challenges, 2023. URL https://arxiv.org/abs/2309.02473. Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max Argus, and Thomas Brox. Freihand: dataset for markerless capture of hand pose and shape from single rgb images, 2019. URL https: //arxiv.org/abs/1909.04349."
        }
    ],
    "affiliations": [
        "Menlo Research"
    ]
}