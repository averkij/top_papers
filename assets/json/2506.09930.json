{
    "paper_title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models",
    "authors": [
        "Irving Fang",
        "Juexiao Zhang",
        "Shengbang Tong",
        "Chen Feng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, \"generalist\" robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates a barrier for reproducibility and accessibility. To address this gap, we introduce a unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, finetuning on action data can erode the original VLM's generalist reasoning abilities. We release our task suite and evaluation code to serve as a standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 0 3 9 9 0 . 6 0 5 2 : r From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models Irving Fang Juexiao Zhang Shengbang Tong Chen Feng"
        },
        {
            "title": "Abstract",
            "content": "One promise that Vision-Language-Action (VLA) models hold over traditional imitation learning for robotics is to leverage the broad generalization capabilities of large Vision-Language Models (VLMs) to produce versatile, generalist robot policies. However, current evaluations of VLAs remain insufficient. Traditional imitation learning benchmarks are unsuitable due to the lack of language instructions. Emerging benchmarks for VLAs that incorporate language often come with limited evaluation tasks and do not intend to investigate how much VLM pretraining truly contributes to the generalization capabilities of the downstream robotic policy. Meanwhile, much research relies on real-world robot setups designed in isolation by different institutions, which creates barrier for reproducibility and accessibility. To address this gap, we introduce unified probing suite of 50 simulation-based tasks across 10 subcategories spanning language instruction, vision, and objects. We systematically evaluate several state-of-the-art VLA architectures on this suite to understand their generalization capability. Our results show that while VLM backbones endow VLAs with robust perceptual understanding and high-level planning, which we refer to as good intentions, this does not reliably translate into precise motor execution: when faced with out-of-distribution observations, policies often exhibit coherent intentions, but falter in action execution. Moreover, fine-tuning on action data can erode the original VLMs generalist reasoning abilities. We release our task suite and evaluation code to serve as standardized benchmark for future VLAs and to drive research on closing the perception-to-action gap. More information, including the source code, can be found at https://ai4ce.github.io/INT-ACT/"
        },
        {
            "title": "Introduction",
            "content": "Building on the significant progress of large Vision-Language Models (VLMs) [7, 13, 23, 30] in computer vision and multi-modal tasks, Vision-Language-Action (VLA) [2, 16, 19] models are emerging to transfer these generalist skills to embodied agents and robotics. The ultimate goal is to create robots that can interpret natural language instructions, perceive and plan under complex and diverse environments, and execute versatile behaviors, all under fewor zero-shot scenarios. This goal calls for strong generalization capability beyond the training and fine-tuning of robotics data. Early results are encouraging: VLAs can natively follow high-level language instructions in cluttered scenes and perform highly dexterous manipulation on challenging objects [2, 16, 19]. Moreover, since VLAs are typically pre-trained on large-scale cross-embodiment dataset [9], they can be fine-tuned on new tasks with novel configurations for quick adaptation, partially alleviating the data scarcity problem plaguing robotics research. Equal contribution Project Lead Preprint. Under review. Despite this progress, there is no consensus on how to systematically measure the generalization capability of VLAs, which is supposedly strong point of leveraging the modern VLMs over previous perceptual systems. Notably, the current evaluation procedure for VLA generally relies on: 1. Recent simulation benchmarks such as CALVIN [27], LIBERO [22] and SimplerEnv [20] are equipped with natural language instructions, enabling the evaluation of vision-language-action (VLA) models instruction-following capabilities, which differentiate them from the more traditional benchmarks such as RLBench [17] or RoboMimic [26] that focus primarily on low-level control and imitation, with limited or no support for diverse language-conditioned tasks. However, the variety of tasks is relatively limited, and VLAs generalization capability is not targeted. 2. Real-world robot setup as seen in [2, 16, 19]. While this type of evaluation offers endless flexibility and opportunities to test the generalization of VLA policy, it has considerably bigger barriers than any vision language benchmark due to the time and monetary commitment required. As result, it remains unclear to many how well the state-of-the-art VLA models can generalize beyond the robotics datasets they train on by leveraging the remarkable generalization capability of their built-in VLM. To fill this void, we propose unified probing suite comprising 50 simulationbased tasks organized into 10 categories that vary systematically along three categories: Object diversity: out-of-distribution (OOD) object, appearance, and affordances unseen during embodiment-finetuning. Language complexity: from templated commands (e.g., Put on B) to compositional, knowledge and reasoning-intensive instructions. Vision-language thinking: parsing various distractors that are challenging at the perception or planning level. In this work, we make two primary contributions: We introduce and open-source INT-ACT, comprehensive VLA generalization probing suite comprising 50 tasks across 3 major categories and 10 subcategories, substantially extending the scope of existing VLA benchmarks. Through extensive benchmarking, we uncover two key failure modes in current state-of-the-art VLA models: persistent and pronounced Intention-Action Gap, where strong semantic understanding under distribution shift fails to translate into reliable execution. Fragile multimodal generalization, particularly under language variations and compounding visual-language distribution shifts."
        },
        {
            "title": "2.1 Vision-Language-Action Models as Robotic Foundation Models\nThe paradigm of foundation models—large, pre-trained architectures fine-tunable for diverse down-\nstream tasks, has transformed natural language processing (NLP) and computer vision (CV). Notable\nexamples include BERT [10] and GPT [5] for NLP, and CLIP [36] and DINOv2 [32] for vision,\nall showcasing strong zero- and few-shot generalization. This success has shifted research from\nspecialized models towards unified architectures leveraging web-scale pre-training data.",
            "content": "In robotics, Robotic Foundation Models aim to equip embodied agents with similar generalist abilities by training on diverse robotics datasets like Open-X Embodiment [9]. Early works, such as RT-1 [3], Octo [29], and RDT [25], typically use separate pretrained encoders (e.g., T5 [37] for language, SigLIP [44] for vision) to encode multimodal inputs, and subsequently train imitation learning policies (e.g., diffusion policies [8]) to map encoded features to robot actions. Following the success of VLMs [1, 6, 7, 18, 30, 39], Vision-Language-Action (VLA) models aim to integrate the powerful pretrained vision-language models (VLMs) directly, instead of leveraging only pretrained encoders. For instance, RT-2 [4] fine-tunes PaLM-E [11] and PaLI-X [6] on combined vision-language and robotics data via autoregressive token prediction, treating discretized robot actions as tokens. OpenVLA [19] finetunes Prismatic VLM [18] on open robotics datasets exclusively, while SpatialVLA [35] and FAST [34] further refine action tokenization strategies. Conversely, models like π0 [2] circumvent tokenization by coupling specialized transformers for actions with VLMs (e.g., PaliGemma [1]) and applying flow-matching training [21, 24, 45]. 2 These models can achieve zero-shot inference through retrieval if the tasks closely resemble training scenarios (e.g., robot, action representation, camera pose, etc). However, fine-tuning on domainspecific demonstration data is necessary for practical deployment. Our study examines VLAs, with the aim of determining whether the pretrained vision-language alignment inherent in VLM contributes to enhanced out-of-distribution generalization after domain-specific fine-tuning."
        },
        {
            "title": "2.2 Benchmarks for Vision-Language-Action Models\nBenchmarking in robotics and embodied AI has traditionally relied on standardized simulation\nenvironments to systematically evaluate model performance. Early policy learning benchmarks such\nas RLBench [17], RoboMimic [26], Factor World [42], and RoboManip [28] have predominantly\nfocused on robotic manipulation tasks without explicit language instructions, thus limiting their\nsuitability for evaluating Vision-Language-Action (VLA) models.",
            "content": "Recent benchmarks have begun integrating language instructions. Notable examples include CALVIN [27] and LIBERO [22]. However, these benchmarks suffer from simplistic rendering or highly unimodal trajectories, which exacerbates the sim2real gap and undermines their practical applicability. SimplerEnv [20] represents significant advancement by combining system identification, greenscreen compositing, and rigorous statistical validation. This methodology effectively ranks policies in manner closely aligned with their real-world performance. Nevertheless, SimplerEnvs effectiveness in assessing the broader generalization capabilities essential for VLAs is limited by its restricted set of tasks (fewer than 10 tasks across only two robotic platforms) Alternatively, real-world benchmarks, recently exemplified by VLA studies [2, 16, 19], offer increased validity through physically diverse and complex test environments. However, these benchmarks entail substantial logistical, financial, and temporal investments, significantly hindering the feasibility and reproducibility of systematic evaluations for the broader research community. To reconcile these limitations, our work extends SimplerEnv by introducing comprehensive and unified evaluation suite specifically engineered to probe generalization across three critical dimensions: language complexity, object diversity, and visual variation. This structured approach enables precise assessment of how effectively state-of-the-art VLAs leverage their underlying Vision-Language Model (VLM) capabilities to generalize beyond immediate training and fine-tuning conditions. 3 INT-ACT: VLA Probing Suite In this section, we describe our methodology for crafting the INT-ACT Suite and the rationale behind our major design choices. Testbed choice To minimize barriers in deployment, we choose to build our suite entirely in simulation. Our probing suite extends the SimplerEnv benchmark [20], which is built on the Maniskill2 simulator [14]. It stands out among other VLA benchmarks [22, 27] by design to closely mathch models performance in the real world, making it an ideal testbed for our needs. However, the original SimplerEnv benchmark only includes simplistic tasks. To cater to our need to probe generalization, we significantly expand the original SimplerEnv suite from 4 tasks per dataset to 50 tasks tailored for the BridgeV2 dataset. We also include an additional metric that can keep track of the policys intention. These additions greatly enhance SimplerEnvs breadth and generalization testing potential. Design Principles generalizable VLA policy should inherit VLMs strengths in vision-language understanding and broad generalization. We therefore organize our probing tasks into three categories: object diversity, language complexity, and vision-language thinking. We elaborate on each below."
        },
        {
            "title": "3.1 Object Diversity\nTruly generalist robot policies require robust perceptual capabilities that extend beyond the specific\nobject distributions encountered during training or fine-tuning. VLMs have demonstrated strong\nopen-vocabulary recognition capabilities [46], enabling reasoning over novel and uncommon vi-\nsual categories. To assess whether state-of-the-art VLAs can leverage this attribute of VLMs, we\nsignificantly expand the object set used in SimplerEnv.",
            "content": "Since its impossible to audit whether an object appears in the proprietary dataset that π0 is pretrained on, we focus on the fine-tune dataset. We argue that this choice also resonates with VLAs real-world deployment: In practice, we care more about whether the VLA fine-tuned on my deployment dataset can generalize beyond the objects shown in my dataset, since additional fine-tuning incurs extra cost. 3 Figure 1: Left: Examples of tasks with out-of-distribution objects. Right: Examples of tasks with commonsense reasoning, distractors, and commonsense reasoning + distractors. We begin by auditing all objects present in the BridgeV2 dataset. To introduce out-of-distribution (OOD) diversity, we first include additional household items inspired by other robotic benchmarks such as LIBERO [22] and Fractal [3], which feature similar affordances but never appear in the BridgeV2 dataset. Beyond that, we incorporate objects from entirely different domains, such as industrial tools. These additions are deliberately chosen to induce visual mismatch with the pretrained representations learned on domestic data, testing the generalization limits of the VLA policies. The gathered objects are placed into simple PUT {Source} ON {Target} tasks by replacing either the source or the target objects with an OOD object. Then, we expand the combinatorial complexity of the tasks by making both the source object and the target object OOD. Additionally, to probe whether the VLA policy falls to memorizing spurious correlations from training data, we introduce OOD Relations between objects seen in the BridgeV2 dataset, such as putting carrot on sponge."
        },
        {
            "title": "3.2 Language Complexity",
            "content": "Recent advancements in Vision-Language Models (VLMs) [31, 40] demonstrate impressive capabilities in understanding diverse and semantically rich instructions such as those that require commonsense and reasoning. For instance, when prompted with Create an image of the flower celebrated in spring festivals in the country where sushi originated, modern VLMs correctly infer Japan as the country and subsequently cherry blossom as the flower. In contrast, recent robotics datasets and benchmarks generally rely on short, templated commands without ambiguity, such as PUT {X} ON {Y}. To probe whether VLAs inherit the adavanced generalization abilities of their underlying VLMs, we sweep through all instructions in the BridgeV2 dataset and augment the original SimplerEnv instructions with more complex linguistic variations, including changes in action verbs, semantic negation, referential appearance, and commonsense cues. Examples of such variations are provided in Table 2b. The variation logic are as follows: Language Action: paraphrasing verbs to be compositional and less frequent in BridgeV2. Language Negation: adding negation such as not, dont to irrelevant objects. Language Appearance: replacing object with descriptive words, such as purple object, instead of an eggplant."
        },
        {
            "title": "3.3 Vision-Language Thinking",
            "content": "Robust real-world deployment of VLA models requires resilience to the complex and cluttered visual environments that typify human environments. However, the original SimplerEnv benchmark focuses on minimalist scenes that include only the source and target objects relevant to given task. To address this limitation, we augment the benchmark with visually richer scenes by introducing additional objects that are irrelevant to the specified task. These objects are chosen from broader pool of distractors and placed throughout the scene to simulate clutter and occlusion. Beyond generic clutter, we also include semantically challenging distractors that require commonsense reasoning to disambiguate. For example, in task where the agent is instructed to pick up an orange juice box, we 4 introduce an orange nearby. This setup probes whether the VLA model can resolve ambiguity based on language context and semantic cues. Prior research [20, 42] has shown that visual factors such as lighting changes, background variation, and camera pose shifts can degrade model performance. Unfortunately, SimplerEnvs reliance on green-screen compositing complicates the simulation of lighting, texture, and camera pose variation. However, we note that recent studies have found that state-of-the-art imitation learning and VLA models are relatively robust to lighting and background perturbations, especially when pretrained with diverse visual datasets [42]. That said, variations in camera pose remain critical challenge. We refer our readers to previous research [20, 42] on the impact of these visual factors on large-scale robot learning policy."
        },
        {
            "title": "4.1 Model Selection and Training Protocol\nWe evaluate four prominent models and their variants in INT-ACT suite: π0 [2], SpatialVLA [35],\nMagma [43], and Octo [29]. They are selected to represent a diverse set of architectural paradigms\nand training methodologies in the current VLA landscape. All finetuning experiments are conducted\non BridgeV2, a popular large robot learning dataset consistent with SimplerEnv. We adhere to the\ntraining and fine-tuning protocols in the respective papers to ensure fair and consistent comparisons.",
            "content": "Pi-Zero (π0), is chosen for its consistently strong performance across standard VLA benchmarks and serves as representative of the diffusion/flow-based modeling family. π0 is evaluated in two variants: (1) fine-tuned from the released checkpoint on BridgeV2 (denoted π0-finetune), which mirrors the intended usage, and (2) trained from scratch using their pretrained VLM (i.e., Paligemma) and only BridgeV2 as the robot data source (denoted π0-scratch), which serves as strong baseline for multi-task imitation learning. SpatialVLA is included as leading autoregressive VLA model, frequently reported to outperform other autoregressive counterparts such as OpenVLA in various experimental settings [12, 35]. SpatialVLA is used directly from an official BridgeV2-finetuned checkpoint. Magma, while built on similar autoregressive models, leverages co-training regime with visionlanguage data, strategy that retains model the vision-language understanding capability for VQA and agentic tasks. We are curious to know whether this co-training can also help its generalization in action, particularly under our proposed out-of-distribution evaluations. Magma is evaluated in zero-shot configuration, following the protocol in its paper. Octo is trained on cross-embodiment datasets conditioned on both visual and language inputs. Though not VLA model in the strictest definition discussed in Sec. 2.1, it has served as an exemplar large multi-task imitation learning baseline in the VLA literature. We evaluate both Octo Small (27M parameters) and Octo Base (93M parameters) variants. Octo is run in zero-shot mode. Although fine-tuning is recommended for novel scenes, our preliminary trials, as well as independent research [35], suggest that zero-shot Octo actually performs slightly better in BridgeV2 scenes. Therefore, we stick to the zero-shot setting adhere to the literature. Our benchmark is organized into three major evaluation categories of object, language and visionlanguage, each with more subcategories as described in Sec. 3. Together with the original SimplerEnv tasks, our suite consists of 50 tasks designed to provide comprehensive and fine-grained evaluation. breakdown of the task distribution is provided in Fig 2a. Evaluation Procedure For each task, we evaluate each model across 24 episodes, corresponding to all possible scene and object configurations predefined by ManiSkill2. Each configuration is repeated across 3 random seeds, following standard practice in prior works [19, 29]. All reported metrics are averaged across episodes and seeds. Metrics We report three primary metrics: Grasp Success Rate: Whether the robots gripper successfully grasps the correct source object at any point during the episode. This metric is natively supported by SimplerEnv. Intention Correct Rate: new metric introduced in our benchmark, defined by whether the gripper moves to within small radius of the correct source object at any frame. This captures the policys intention to grasp the correct object, even if the grasping fails afterwards. Task Success Rate: If the task is successfully completed, natively supported by SimplerEnv. (a) The distribution of our tasks compared to SimplerEnvs original tasks (b) Sample of language variations Figure 2: Illustration of the INT-ACT probing suite. Left: suite category breakdown. Right: illustraion of language variations. These metrics jointly capture both motor-level competence (grasping) and semantic grounding (correct object targeting), enabling finer evaluation of policy behavior, decomposing the perception and action stages."
        },
        {
            "title": "4.2 Performance analysis\nIntention-Action Gap Table 1 summarizes average performance across generalization categories,\nwith radar plots in Figure 3 visualizing the Intention Correctness and Task Success Rate. A pro-\nnounced gap emerges: while most VLM-initialized VLAs achieve near-perfect Intention Correctness\n(80 − 100%) across all categories—even weaker Octo variants show 50%+, their Task Success Rates\ndrop drastically. The π0 scratch model leads performance but still falls far short of matching its\nintention correctness, highlighting a stark intention-to-execution disparity. This observation con-\nfirms that VLMs endow VLA policies with a generalizable notion of “what to do,” yet executing\nthat intent—especially under distributional shifts, remains challenging. This Intention-Action Gap\npersists even when comparing VLM-based VLAs against non-VLM baselines like Octo, suggesting\nthe limitation lies not in semantic grounding, but in translating it into robust low-level control. To\ndeepen understanding, we analyze model behaviors using INT-ACT with three major questions. The\nfull results are attached in the Appendix C.",
            "content": "Figure 3: The Intention-Action Gap illustrated by comparing the two radar maps. Task Success Radar on the left, Intention Correctness Radar on the left. Best viewed in color. Question 1: How do VLAs generalize to OOD objects? VLAs show robust intention to OOD objects, but execution falls short. Changing the target can affect grasping of the source. We break down per-object OOD performance in Figure 4. Specifically, Figure 4a presents metrics for putting two source objects, carrot and coke can (OOD), on four target objects, plate, ramekin, keyboard and wheel, respectively. All targets except plate are OOD targets. 6 Table 1: Results of the INT-ACT probing suite by category. u O t a O i O + D o l O i A . L t N . L . p . L r D . . m . i + . m C"
        },
        {
            "title": "Metric",
            "content": "Avg."
        },
        {
            "title": "Language",
            "content": "Vision + Lang. π0 fintune [2] π0 scratch [2] Magma [43] SpatialVLA [35] Octo Small [29] Octo Base [29] intention grasp success intention grasp success intention grasp success intention grasp success intention grasp success intention grasp success 84.5 54.4 30.4 89.5 66.7 48.9 85.4 46.5 21.6 69.6 41.3 21.5 31.5 13.9 1. 41.5 14.1 2.4 99.0 77.8 47.6 100 87.2 68.1 96.5 57.6 29.2 100 59.4 39.6 56.6 26.7 5. 74.7 27.8 8.3 93.8 64.2 49.3 91.3 77.4 66.3 96.5 58.3 24.7 100 54.2 29.2 28.5 14.6 1. 35.1 13.2 2.4 100 66.1 24.4 99.2 71.1 44.2 90.3 46.9 27.8 42.5 26.7 6.7 22.5 10.3 0. 51.4 15.0 0.3 84.7 56.9 21.5 91.3 88.2 75.7 94.1 54.5 23.3 17.7 12.5 3.1 23.3 13.2 0. 43.7 17.0 0.3 93.4 61.1 39.6 99.0 81.6 60.8 83.0 50.3 21.2 93.8 43.8 20.8 9.0 3.1 0. 21.5 6.2 0.7 88.9 63.9 38.5 99.3 84.7 58.0 90.6 56.6 27.1 52.1 38.5 29.2 58.0 30.6 3. 53.1 21.2 5.2 71.9 44.4 22.2 79.9 59.4 39.2 79.2 37.5 14.2 71.9 40.6 22.9 29.2 12.2 0. 30.6 7.3 1.0 95.5 56.3 43.1 99.0 69.1 61.5 91.7 45.8 25.0 70.8 58.3 39.6 43.4 16.0 5. 44.4 17.7 9.7 82.9 61.1 25.2 84.7 61.8 25.2 83.6 41.2 18.3 61.8 43.1 18.7 41.7 19.4 1. 58.1 19.0 1.2 89.5 45.4 26.4 93.7 59.5 41.3 87.1 45.0 17.5 90.5 48.2 23.2 26.6 9.1 0. 34.9 10.7 0.0 38.9 17.2 10.6 55.0 26.9 14.7 55.6 25.8 13.1 50.8 25.0 7.5 10.3 1.9 0. 16.1 4.4 0.0 Consistent with the overall performance, all VLA models demonstrate strong intention correctness across every target. By contrast, grasping success rate varies significantly. Interestingly, even when the source object is kept unchanged and only the target object varies, the grasping success rate can still swing up to 40% in some cases. Since the underlying grasp primitive should be identical, this suggests the brittle coupling of high-level perception and planning with low-level action of these end-to-end policies. For comparisons on OOD source objects in Figure 4b, we see that SpatialVLA, Magma, and π0 models all demonstrate robust intentions. Meanwhile, grasping and task success rates vary more by object identity than by their OOD status. This is also reflected in the heatmaps in Figure 4a. The coke can, though unseen in BridgeV2, generally yields higher grasping success rates than the carrot, common object in BridgeV2. hese per-object OOD analyses highlight an Intention-Action gap: the generalized perception capabilities of VLMs do not directly translate to generalized action performance. This observation suggests need for improved architectural designs for integrating VLMs into robotic systems, possibly by incorporating insights from grounding or modularized approaches such as [15, 38, 41]. Question 2: How well do VLAs preserve the linguistic capability? VLAs do not preserve the linguistic capability of the underlying VLM when facing complex language instruction. We list performance delta in three categories of language variations in Table 2. Both Lang. Action and Lang. Negation emphasizes the linguistic robustness while Lang. Appearance additionally involves simple visual understanding. Results show all models suffer performance drops in all categories. To verify if simply paraphrasing task instructions with more diverse vocabulary will mitigate the challenge, we additionally finetuned π0 checkpoint with task paraphrasing, denoted as π0 finetune+rephrase. Results show that although paraphrasing improves over π0 finetune on various tasks in general, it still degrades severely in the face of language variation. One exception is the intention in Lang. Action, which gets restored by finetuning with paraphrasing, but this does not translate into more robust action execution. We conclude that linguistic capabilities from VLMs are not fully preserved after end-to-end VLA training; execution still breaks under simple language perturbations. This comes as surprise since the backbone VLMs, such as Paligemma and Llama3, are popular choices in vision-language tasks, so one 7 (a) OOD target objects. (b) OOD source objects. Figure 4: OOD generalization results. Out-of-Distribution objects are painted in orange . Table 2: Impact of language variation on VLA generalization. Average performance on tasks before applying the language variations is listed in light gray . ( perf.) Method"
        },
        {
            "title": "Magma",
            "content": "π0 scratch π0 finetune π0 finetune +rephrase + Lang. Action + Lang. Appearance + Lang. Negation"
        },
        {
            "title": "Success",
            "content": "before 100 before 100 before 96.5 100 59.3 59. after 90.6 (-5.9) 39.6 after 52.1 (-47.9) 38.5 (-20.8) 29.2 (-10.4) 70.8 (-29.2) 58.3 (-1.0) 29.2 27.1 (-2.1) 68.1 58.0 (-10.1) 99.0 (-1.0) 47.6 57.6 56.6 (-1.0) 87.2 84.7 (-2.4) 77.8 after 88.9 (-10.1) 63.9 (-13.9) 38.5 (-9.0) 77.1 74.3 (-2.8) 56.6 45.1 (-11.5) 90.6 (-8.0) 99.0 95.5 (-3.5) 98.6 98.0 91.7 (-6.3) after 98.6 (+0.0) after 99.3 (-0.7) before 99.0 before 98.6 43.8 39.6 (-4.2) 35.1 43. 100 71.9 (-28.1) 40.6 (-3.1) 96.6 55.6 22.9 22.9 (+0.0) 27.7 79.9 64.2 45.8 (-18.4) 25.0 (-10.1) 79.2 (-17.4) 37.5 (-18.1) 14.2 (-13.5) 87.5 69.1 (-18.4) 61.5 (-18.4) 79.9 (-11.5) 59.4 (-12.2) 39.2 (-24.0) 77.5 56.3 (-21.2) 43.1 (-16.7) 71.9 (-24.0) 44.4 (-20.8) 22.2 (-24.3) 81.9 51.0 (-30.9) 41.3 (-29.2) 78.1 (-18.8) 53.1 (-18.8) 33.0 (-22.2) 59. 71.9 91.4 71.6 95.9 65.2 70. 96.9 63.2 46.5 55.2 would expect their generalizability in language can be easily absorbed by the VLAs. Magma, possibly thanks to its joint vision-language co-training, shows best robustness to action words variations as seen in the Lang. Action category. Nonetheless, the loss of linguistic generalizability after VLA training revealed by INT-ACT is significant. Question 3: How well do VLAs preserve the vision-language thinking? Lastly, we further test the robustness of VLAs in scenarios involving commonsense reasoning and visual distractions. To this end, we analyze the performance trend in three progressively complex categories: Distraction, Language Commonsense, and Language Commonsense with Distraction. We take two specific tasks for case studies: Put carrot on plate and Put orange juice on plate, and extend them by adding distraction objects and replacing the source object with commonsense paraphrasing as illustrated in Figure 5. In addition to Intention Correctness and Task Success Rate, we additionally monitor if any non-source object is moved during the episode and report it as the Wrong Object Attempt Rate. For the carrot case, all models maintain high intention correctness with either distractions or commonsense paraphrases alone, despite the decreased success rate. Wrong object attempts stay near zero in these single-factor settings. However, when the toy bunny is added to the scene, together with the language commonsense factor \"rabbits favorite vegetables\", the wrong object attempt surges, especially for π0 finetune, indicating breakdown in correctly binding linguistic intent to the visual scene. For the orange juice case, we take advantage of the name and add an orange as the distraction, which causes language distraction too. This completely breaks the models, yielding high wrong object attempt rates across all of them. In conclusion, the probing suggests that while VLAs have some 8 (a) Carrot case study. (b) Orange juice case study. Figure 5: Case studies showing the impact of visual distractions and language commonsense variations. Task illustrations are on the left. Success rate, Intention Correctness and Wrong Object Attemp Rate are grouped by models on the right. robustness to isolated commonsense and visual distractions, they are brittle under multimodal ambiguity, where linguistic priors override visual grounding, leading to systematic misbehavior. To further support our observations in Questions 2 and 3, we prompted the pretrained PaliGemma by with instructions and objects for simple VQA, as shown in Figures 6 and 7 in Appendix B. The results indicate that even as smaller language model, and not specifically optimized for instruction following, PaliGemma demonstrates considerable understanding before being fine-tuned as Visual Language Action (VLA) model. This once again suggests that theres significant room to improve VLA architecture design to better leverage the inherent capabilities of VLMs, rather than disrupting them. Insights from paradigms like freezing VLMs in image generation, as explored in [33], could prove beneficial here."
        },
        {
            "title": "5 Conclusion and Limitation",
            "content": "In this work, we proposed VLA generalization probing suite INT-ACT with 50 tasks across 10 categories in object, language, and vision, aiming to understand the generalization boundary of 9 state-of-the-art VLAs. Our benchmarking effort using INT-ACT reveals two critical challenges in the current generation of VLA models. First is persistent and wide Intention-Action Gap. While VLM backbones confer impressive semantic understanding, enabling VLAs to generalize high-level goals across distribution shifts, this competence rarely transfers to reliable execution. Grasp success and task completion suffer dramatically, even in tasks with correct intent. Further probing shows that VLA models degrade under simple language variations, failing to preserve the generalization capabilities of the underlying language backbones. Multimodal reasoning during action execution, especially when vision and language out-of-distribution compounds, remains significant weakness. Even models with joint vision-language pretraining and language augmentation fine-tuning struggle with commonsense disambiguation in the presence of distractions. Our work is not without limitations. The current INT-ACT is based on the BridgeV2 dataset and its associated robot and sensor configurations. Expanding it to more robot embodiments would make the results more compelling. At the same time, since the possible objects and language distribution shift is theoretically infinite, it would be great if we could incorporate technology like LLMs or 3D generative models to automate the task creation process to increase the number of significantly out-of-distribution tasks, thus providing more compelling statistical signals. In addition, INT-ACT is developed from SimplerEnv so it is fully in simulation. This makes it extremely accessible but also inherits the same sim2real gap. Although SimplerEnv is built to match real-world performance, extending the INT-ACT to real-world is still worthwhile future step. Acknowledgement. Irving Fang and Juexiao Zhang share the first authorship. Their collaboration initiated the project. They both conducted experiments and wrote the paper. Juexiao Zhang led the project, designed and implemented the evaluation metric and tasks. Irving Fang suggested emphasize the project on evaluation and implemented the code infrastructure. Shengbang Tong engaged in conceptualization, advised on benchmarking, VLM and VLA training, and paper writing. Chen Feng oversaw and advised the project. We thank Saining Xie and Yiming Li for their helpful discussions, SimplerEnv authors and Allen Ren for opensourcing their code as our important reference. The work was supported in part through NSF grants 2238968, 2322242 and 2026479, and the NYU IT High Performance Computing resources, services, and staff expertise."
        },
        {
            "title": "References",
            "content": "[1] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. Paligemma: versatile 3b vlm for transfer, 2024. URL https://arxiv.org/abs/2407.07726. 2 [2] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: vision-language-action flow model for general robot control, 2024. URL https://arxiv. org/abs/2410.24164. 1, 2, 3, 5, 7 [3] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale. In arXiv preprint arXiv:2212.06817, 2022. 2, 4 10 [4] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In arXiv preprint arXiv:2307.15818, 2023. 2 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. [6] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu, Keran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali-x: On scaling up multilingual vision and language model, 2023. URL https://arxiv.org/abs/2305.18565. 2 [7] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: jointly-scaled multilingual language-image model. In ICLR, 2023. 1, 2 [8] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 2024. 2 [9] Open X-Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben BurgessLimerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi 11 Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi \"Jim\" Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil Joshi, Niko Suenderhauf, Ning Liu, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag Sanketi, Patrick \"Tree\" Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martin-Martin, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vikash Kumar, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiangyu Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yongqiang Dou, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open X-Embodiment: Robotic learning datasets and RT-X models. https://arxiv.org/abs/2310.08864, 2023. 1, [10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology. org/N19-1423/. 2 [11] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In arXiv preprint arXiv:2303.03378, 2023. 2 [12] Github Issue from user: abadithela. Poor performance of openvla on bridge. https://github. com/simpler-env/SimplerEnv/issues/78, March 2025. URL https://github.com/ simpler-env/SimplerEnv/issues/78. GitHub Issue #78 in simpler-env/SimplerEnv repository. 5 [13] Google. Gemini, google-gemini-ai/. 1 2023. URL https://blog.google/technology/ai/ [14] Jiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone Tao, Xinyue Wei, Yunchao Yao, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui Chen, and Hao Su. Maniskill2: unified benchmark for generalizable manipulation skills. In International Conference on Learning Representations, 2023. 3 [15] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatiotemporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. 7 12 [16] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. π0.5: vision-language-action model with open-world generalization, 2025. URL https://arxiv.org/abs/2504.16054. 1, 2, 3 [17] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J. Davison. Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters, 2020. 2, [18] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In International Conference on Machine Learning (ICML), 2024. 2 [19] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard (eds.), Proceedings of The 8th Conference on Robot Learning, volume 270 of Proceedings of Machine Learning Research, pp. 26792713. PMLR, 0609 Nov 2025. URL https://proceedings.mlr.press/v270/kim25c.html. 1, 2, 3, 5 [20] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, Sergey Levine, Jiajun Wu, Chelsea Finn, Hao Su, Quan Vuong, and Ted Xiao. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024. 2, 3, 5 [21] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling, 2023. URL https://arxiv.org/abs/2210.02747. 2 [22] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023. 2, 3, [23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1 [24] Qiang Liu. Rectified flow: marginal preserving approach to optimal transport, 2022. URL https://arxiv.org/abs/2209.14577. 2 [25] Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. 2 [26] Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto Martín-Martín. What matters in learning from offline human demonstrations for robot manipulation. In arXiv preprint arXiv:2108.03298, 2021. 2, [27] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters (RA-L), 7(3):73277334, 2022. 2, 3 [28] Masaki Murooka, Tomohiro Motoda, and Ryoichi Nakajo. RoboManipBaselines, December 2024. URL https://github.com/isri-aist/RoboManipBaselines. 3 [29] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. In Proceedings of Robotics: Science and Systems, Delft, Netherlands, 2024. 2, 5, 7 13 [30] OpenAI. gpt4o, 2024. URL https://openai.com/index/hello-gpt-4o/. 1, [31] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi 14 Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. 4 [32] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=a68SUt6zFt. Featured Certification. 2 [33] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. 9 [34] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models, 2025. URL https://arxiv.org/abs/2501.09747. [35] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, and Xuelong Li. Spatialvla: Exploring spatial representations for visual-language-action model, 2025. URL https://arxiv.org/abs/2501.15830. 2, 5, 7 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. URL https://api.semanticscholar.org/ CorpusID:231591445. 2 [37] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. URL http://jmlr.org/papers/v21/20-074.html. 2 [38] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. 7 [39] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. 2 [40] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning, 2024. URL https://arxiv.org/abs/2412. 14164. [41] Beichen Wang, Juexiao Zhang, Shuwen Dong, Irving Fang, and Chen Feng. Vlm see, robot do: Human demo video to robot action plan via vision language model. arXiv preprint arXiv:2410.08792, 2024. 7 15 [42] Annie Xie, Lisa Lee, Ted Xiao, and Chelsea Finn. Decomposing the generalization gap in imitation learning for visual robotic manipulation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 31533160. IEEE, 2024. 3, 5 [43] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. arXiv preprint arXiv:2502.13130, 2025. 5, 7 [44] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 1194111952. IEEE, 2023. doi: 10.1109/ ICCV51070.2023.01100. URL https://doi.org/10.1109/ICCV51070.2023.01100. 2 [45] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [46] Orr Zohar, Kuan-Chieh Wang, and Serena Yeung. Prob: Probabilistic objectness for open world object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1144411453, June 2023."
        },
        {
            "title": "A Experiment details",
            "content": "Training π0 on BridgeV2. When fine-tuning and training π0 on the BridgeV2 dataset, we employ 4 H100 80GB GPUs from Nvidia and the following parameters. The parameters are entirely the same for both training and fine-tuning. It takes about 17 hours to finish the training loop for 15 epochs on BridgeV2. Table 3: Hyperparameters for training and fine-tuning π"
        },
        {
            "title": "Global Batch Size\nEpochs\nSingle Card Batch Size\nOptimizer LR\nOptimizer Betas\nOptimizer Eps\nOptimizer Weight Decay\nScheduler Warmup Steps\nChunk Size\nFreeze Vision Encoder\nLanguage Tokenizer Max Length\nMLP Projector Width",
            "content": "1024 15 32 5e 5 [ 0.9, 0.999 ] 1e-8 0 200 4 False 72 1024 Notably, the max state and action dimensions for the MLP projectors are set differently for finetuning and training from scratch. Specifically, the fine-tuning version uses 32 because the official π0 checkpoint uses 32 to accommodate various embodiments in the dataset. However, for training from scratch, we discover that setting the max state and action dimension to exactly the datasets state and action dimension (7 in this case), thus avoiding 0-padding, does improve performance. This configuration follows another third-party open-source implementation of π0 3. Evaluation π0 variants. When evaluating π0 finetune and π0 scratch, we choose checkpoints at epoch 1, 2, 3, 4, 5, 10, and 15 and evaluate them on the 4 original tasks from SimplerEnv. Then we pick the best performing checkpoint for each method and roll out the full-scale evaluation in INTACT. Better performance could be possible if we sweep all checkpoints in INT-ACT, but since our evaluation aims to probe the models generalization capabilities, we believe picking the checkpoint by the original 4 tasks is more reasonable choice. Threshold For Intention Correctness The specific threshold used in the calculation of Intention Correct Rate is set to 5 centimeters. This is empirically determined but is set to be close to other thresholds already exists in the original SimplerEnv."
        },
        {
            "title": "B Prompt PaliGemma for Task Variations",
            "content": "Since π0 is fine-tuned from PaliGemma, we are interested in knowing whether the PaliGemma VLM itself is able to sort out the task variations before being tuned to action data. We take the carrot on plate, the spoon on towel and their variation tasks as two sets of example, and the results are displayed in Figure 6. Specifically, we give the varied task instructions and the first frame of visual observation, and ask PaliGemma to answer in short sentence what the task actually is doing. As expected, PaliGemma is able to understand many of the task variations, but it also has strong tendency to repeat the instructions without exactly following the prompts. This causes complication for targeted measurement as to whether the model does not understand the language variations or it actually understands but tends to literally repeat the previous instruction. Therefore, we additionally prompt the PaliGemma to specifically answer questions about the variation just like visual question answering. Results on the same carrot and spoon examples are in Figure 7. Results indicate that PaliGemma shows pretty good understanding to the appearance descriptions and commonsense. For example, for the difficult case of carrot on plate with commonsense and distraction task, the pi0 finetune model make high rate of Wrong Object Attempt as shown in Figure 5a, but the PaliGemma can figure it out, as shown in Figure 7b, providing another piece of evidence that VLA 3https://github.com/allenzren/open-pi-zero 17 (a) Carrot on plate and its variations. (b) Spoon on towel and its variations. Figure 6: Prompting PaliGemma with the task and the first frame of visual observations. Prompts are in gray boxes where the task variations are indicated. PaliGemma answers are in green boxes and the wrong answers are in red. loses VLMs vision-language thinking capability after finetuning. Of course, PaliGemma makes some mistakes itself, so maybe changing it to newer and stronger VLM, especially ones with better visual grounding, could mitigate the problem. We leave this investigation as future work. 18 (a) VQA: appearance. Carrot case on the left, spoon case on the right. (b) VQA: commonsense. Carrot case on the left, spoon case on the right. Figure 7: Prompting PaliGemma to do VQA on the task instructions and the first frame of visual observations. Prompts are in gray boxes where the task variations are indicated. PaliGemma answers are in green boxes and the wrong answers are in red."
        },
        {
            "title": "C Full Benchmarking Results",
            "content": "In this part we attach the full results. In addition to the evaluation metrics, we also compute some performance deltas. map of how the deltas are computed is included in Appendix D. We will release plot to better visualize these results in the future. C.1 Original Tasks"
        },
        {
            "title": "Model",
            "content": "Grasp Correct Delta Delta(%) Intention Correct Delta Delta(%) Success Rate Delta Delta(%) Wrong Obj Attempt Delta Delta(%) Table 4: Full benchmark results for the original Simpler tasks π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase 0.611 0.611 0.417 0.444 0.167 0.222 0.722 0.97 0.99 1.00 0.83 0.47 0.57 1.00 0.86 0.93 0.58 0.57 0.07 0.17 0.67 0.67 0.96 0.38 0.46 0.36 0.15 0. 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 widowx_carrot_on_plate 1.000 1.000 1.000 1.000 0.389 0.750 1.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0. 0.361 0.542 0.125 0.250 0.014 0.014 0.500 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 widowx_put_eggplant_in_basket 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 1.00 1.00 1.00 0.67 0.92 1.00 1.00 1.00 1.00 0.94 0.54 0.61 1. 0.96 1.00 1.00 0.92 0.67 0.71 0.94 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 widowx_stack_cube 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. widowx_spoon_on_towel 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.82 0.90 0.96 0.61 0.10 0.31 0.94 0.26 0.40 0.29 0.10 0.00 0.00 0.22 0.46 0.87 0.21 0.21 0.10 0.01 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 N/A N/A 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.02 0.00 0.00 0.08 0.03 0.00 0.00 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.00 0.00 0.00 0.00 0.00 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A C.2 Out-of-Distribution Objects C.2.1 Out-of-Distribution Sources Model Grasp Correct Delta Delta(%) Intention Correct Delta Delta(%) Success Rate Delta Delta(%) Wrong Obj Attempt Delta Delta(%) Table 5: Full benchmark results for the OOD Source Tasks π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase 0.639 0.875 0.875 0.514 0.236 0.194 0.847 0.903 0.875 0.792 0.694 0.292 0.278 0. 0.431 0.417 0.167 0.625 0.000 0.028 0.542 0.597 0.931 0.333 0.500 0.056 0.028 0.583 0.028 0.264 0.458 0.069 0.069 -0.028 0.125 0.292 0.264 0.375 0.250 0.125 0.056 0.194 -0.181 -0.194 -0.250 0.181 -0.167 -0.194 -0.181 -0.014 0.319 -0.083 0.056 -0.111 -0.194 -0. 4.549 43.190 109.983 15.622 41.680 -12.481 17.307 47.734 43.190 89.993 56.247 75.000 25.008 26.926 -29.548 -31.817 -59.995 40.618 -100.000 -87.504 -25.001 -2.275 52.272 -20.014 12.494 -66.660 -87.504 -19.232 widowx_pepsi_on_plate_clean 0.875 1.000 1.000 0.917 0.417 0.611 0. -0.125 0.000 0.000 -0.083 0.028 -0.139 -0.028 -12.497 0.000 0.000 -8.333 7.140 -18.516 -2.780 0.444 0.722 0.333 0.292 0.014 0.042 0.653 widowx_coke_can_on_plate_clean 1.000 1.000 1.000 0.944 0.556 0.667 1.000 0.000 0.000 0.000 -0.056 0.167 -0.083 0. 0.000 0.000 0.000 -5.557 42.856 -11.107 0.000 0.736 0.806 0.458 0.361 0.056 0.042 0.708 widowx_orange_juice_on_plate_clean 0.875 0.653 1.000 1.000 0.000 0.069 0.931 1.000 1.000 1.000 1.000 0.167 0.056 1.000 -0.125 -0.347 0.000 0.000 -0.389 -0.680 -0. -12.500 -34.720 0.000 0.000 -100.000 -90.737 -6.943 widowx_nut_on_plate_clean 0.000 0.000 0.000 0.000 -0.222 -0.694 0.000 0.000 0.000 0.000 0.000 -57.144 -92.591 0.000 0.306 0.306 0.125 0.292 0.000 0.014 0.403 0.486 0.819 0.250 0.042 0.000 0.000 0. 0.083 0.181 0.208 0.042 0.000 0.028 0.153 0.375 0.264 0.333 0.111 0.042 0.028 0.208 -0.056 -0.236 0.000 0.042 -0.014 0.000 -0.097 0.125 0.278 0.125 -0.208 -0.014 -0.014 -0.111 23.087 33.335 166.640 16.680 0.000 199.760 30.553 103.849 48.714 266.640 44.440 299.760 199.760 41. -15.379 -43.588 0.000 16.680 -100.000 0.000 -19.440 34.616 51.280 100.000 -83.320 -100.000 -100.000 -22.220 0.000 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0. 0.000 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 20 C.2.2 Out-of-Distribution Target Model Grasp Correct Delta Delta(%) Intention Correct Delta Delta(%) Success Rate Delta Delta(%) Wrong Obj Attempt Delta Delta(%) Table 6: Full benchmark results for the OOD Target tasks π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase 0.597 0.556 0.167 0.417 0.083 0.167 0.528 0.903 0.861 0.333 0.792 0.028 0.056 0.931 0.542 0.569 0.333 0.250 0.097 0.153 0.486 0.486 0.736 0.333 0.458 0.194 0.167 0.417 -0.014 -0.056 -0.250 -0.028 -0.083 -0.056 -0. -0.069 -0.125 -0.667 -0.042 -0.444 -0.514 -0.069 -0.069 -0.042 -0.083 -0.194 -0.069 -0.069 -0.236 -0.125 0.125 -0.083 0.014 0.028 -0.056 -0.306 -2.275 -9.087 -59.995 -6.262 -50.000 -24.992 -26.926 -7.142 -12.676 -66.670 -5.000 -94.120 -90.248 -6.943 -11.367 -6.818 -20.014 -43.753 -41.660 -31.248 -32. -20.455 20.455 -20.014 3.112 16.660 -24.992 -42.309 widowx_carrot_on_keyboard_clean 1.000 1.000 0.208 0.792 0.181 0.583 1.000 0.000 0.000 -0.792 -0.208 -0.208 -0.167 0.000 0.000 0.000 -79.170 -20.830 -53.570 -22.219 0.000 widowx_eggplant_on_keyboard 1.000 0.958 0.375 0.875 0.028 0.250 1.000 0.000 -0.042 -0.625 -0.125 -0.639 -0.667 0.000 0.000 -4.170 -62.500 -12.500 -95.835 -72.727 0.000 widowx_carrot_on_wheel_clean 1.000 1.000 0.625 0.875 0.194 0.458 1.000 0.000 0.000 -0.375 -0.125 -0.194 -0.292 0. 0.000 0.000 -37.500 -12.500 -50.004 -38.882 0.000 0.250 0.306 0.042 0.208 0.000 0.000 0.194 0.278 0.514 0.042 0.542 0.000 0.000 0.222 0.194 0.514 0.000 0.208 0.000 0.000 0.167 widowx_carrot_on_ramekin_clean 1.000 1.000 0.750 1.000 0.556 0.611 0. 0.000 0.000 -0.250 0.000 0.167 -0.139 -0.056 0.000 0.000 -25.000 0.000 42.847 -18.516 -5.557 0.111 0.639 0.167 0.292 0.000 0.014 0.125 -0.111 -0.236 -0.083 -0.042 -0.014 -0.014 -0.306 -0.542 -0.389 -0.917 -0.069 -0.097 -0.306 -0.722 -0.167 -0.028 -0.125 -0.042 -0.014 -0.014 -0. -0.250 0.097 0.042 0.042 -0.014 0.000 -0.375 -30.767 -43.588 -66.640 -16.667 -100.000 -100.000 -61.113 -66.099 -43.081 -95.649 -11.361 -100.000 -100.000 -76.469 -46.146 -5.126 -100.000 -16.680 -100.000 -100.000 -66.667 -69.233 17.951 33.360 16.680 -100.000 0.000 -75.000 0.014 0.000 0.000 0.000 0.000 0.000 0. 0.014 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.014 0.000 0.042 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.028 0.028 0.028 0.014 0.000 0.000 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.014 0.000 0.042 0.000 0.000 0. 0.000 0.014 0.000 0.000 0.028 0.028 0.028 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A C.2.3 Out-of-Distribution Source and Target Model π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase Table 7: Full benchmark results for the OOD Source and Target Tasks Grasp Correct Delta Delta(%) Intention Correct Delta Delta(%) Success Rate Delta Delta(%) Wrong Obj Attempt Delta Delta(%) 0.778 0.833 0.167 0.431 0.111 0.208 0.778 0.569 0.847 0.333 0.833 0.264 0.306 0.292 0.736 0.722 0.000 0.625 0.139 0.167 0.417 0.194 0.625 0.000 0.292 0.014 0.000 0. -0.125 -0.042 -0.625 -0.264 -0.181 -0.069 -0.139 -0.333 -0.028 -0.458 0.139 -0.028 0.028 -0.625 -0.167 -0.153 -0.792 -0.069 -0.153 -0.111 -0.500 -0.403 -0.306 -0.333 -0.208 -0.042 -0.028 -0.181 -13.850 -4.766 -78.944 -38.000 -61.909 -24.997 -15.156 -36.926 -3.177 -57.901 19.991 -9.531 9.996 -68. -18.465 -17.466 -100.000 -10.003 -52.377 -39.998 -54.551 -67.443 -32.834 -100.000 -41.660 -74.985 -100.000 -30.954 widowx_coke_can_on_keyboard_clean 1.000 1.000 0.167 0.972 0.167 0.667 1.000 0.000 0.000 -0.833 0.028 -0.389 0.000 0.000 0.000 0.000 -83.330 2.944 -70.001 0.000 0. 0.389 0.236 0.083 0.139 0.000 0.000 0.306 widowx_coke_can_on_ramekin_clean 0.778 1.000 0.417 1.000 0.472 0.653 0.403 -0.222 0.000 -0.583 0.056 -0.083 -0.014 -0.597 -22.220 0.000 -58.330 5.884 -15.006 -2.085 -59.720 0.153 0.847 0.042 0.500 0.028 0.014 0. widowx_coke_can_on_wheel_clean 0.903 0.986 0.125 1.000 0.264 0.375 0.722 0.708 0.667 0.000 0.792 0.028 0.056 0.819 -0.097 -0.014 -0.875 0.056 -0.292 -0.292 -0.278 -9.720 -1.390 -87.500 5.884 -52.499 -43.750 -27.777 widowx_nut_on_wheel_clean -0.292 -0.333 -1.000 -0.208 -0.139 0.000 -0.181 -29.167 -33.330 -100.000 -20.830 -83.340 -0.060 -18.057 0.250 0.583 0.000 0.292 0.000 0.000 0.167 0.069 0.569 0.000 0.000 0.000 0.000 0.181 -0.347 -0.569 -0.375 -0.222 -0.056 -0.042 -0.403 -0.583 0.042 -0.417 0.139 -0.028 -0.028 -0. -0.486 -0.222 -0.458 -0.069 -0.056 -0.042 -0.542 -0.417 -0.250 -0.250 -0.042 0.000 0.000 -0.208 -47.168 -70.686 -81.824 -61.534 -100.000 -100.000 -56.861 -79.242 5.177 -90.901 38.466 -50.030 -66.640 -82.353 -66.037 -27.584 -100.000 -19.219 -100.000 -100.000 -76.471 -85.716 -30.509 -100.000 -100.000 N/A N/A -53. 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.000 0.014 0.014 0.014 0.000 0.000 0.000 0.000 0.014 0.014 0.000 0.069 0.000 0.000 0.000 0.028 0.000 0.014 0.000 0.000 0.000 -0.014 0.000 0.000 0.000 0.014 0.000 0.000 -0.014 0.014 0.014 0. 0.000 0.000 0.000 -0.014 0.014 0.014 0.000 0.069 0.000 0.000 0.000 0.028 0.000 0.014 N/A N/A N/A -100.000 N/A N/A N/A N/A N/A N/A -100.000 N/A N/A N/A N/A N/A N/A -100.000 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 21 C.2.4 Out-of-Distribution Objects Relationship Model Grasp Correct Delta Delta(%) Intention Correct Delta Delta(%) Success Rate Delta Delta(%) Wrong Obj Attempt Delta Delta(%) widowx_carrot_on_sponge_clean Table 8: Full benchmark results for the OOD Relation Tasks π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase 0.65 0.67 0.38 0.17 0.06 0.19 0.60 0.14 0.72 0.00 0.35 0.04 0.00 0.15 0.81 0.93 0.79 0.67 0.01 0.04 0.97 0.847 0.944 0.583 0.833 0.014 0.014 0.861 0.04 0.06 -0.04 -0.28 -0.11 -0.03 -0.13 -0.71 -0.22 -0.58 -0.49 0.03 -0.01 -0. -0.17 -0.06 -0.21 -0.17 -0.46 -0.53 -0.03 0.236 0.333 0.167 0.389 -0.153 -0.208 0.139 6.82 9.09 -10.01 -62.50 -66.66 -12.50 -17.31 -83.61 -23.53 -100.00 -58.33 199.76 -100.00 -82.26 -17.14 -5.63 -20.83 -20.01 -97.06 -92.68 -2.78 38.641 54.546 39.981 87.483 -91.660 -93.744 19. C.3 Language Complexity C.3.1 Language Action 0.00 0.00 -0.04 -0.43 -0.24 -0.10 0.00 0.00 0.00 -4.17 -43.06 -60.71 -12.96 0.00 0.28 1.00 0.60 1.00 0.17 0.96 0.11 0.57 0.00 0.15 0.00 0.65 1.00 0.32 widowx_small_plate_on_green_cube_clean 0.00 0.89 0.14 1.00 0.00 0.79 0.10 0.86 0.00 0.11 0.00 0.03 0.01 0.99 -11.11 0.00 -20.83 -13.89 100.12 -60.03 -1.39 -0.11 0.00 -0.21 -0.14 0.06 -0.04 -0. widowx_eggplant_on_sponge_clean 0.85 0.96 1.00 0.89 0.04 0.11 1.00 1.000 1.000 1.000 1.000 0.056 0.069 1.000 -0.15 -0.04 0.00 -0.11 -0.63 -0.81 0.00 -15.28 -4.17 0.00 -11.11 -93.75 -87.88 0.00 widowx_cube_on_plate_clean 0.000 0.000 0.000 0.000 -0.333 -0.680 0.000 0.000 0.000 0.000 0.000 -85.720 -90.737 0.000 0.56 0.82 0.21 0.31 0.00 0.00 0.86 0.750 0.875 0.458 0.333 0.000 0.014 0.778 -0.08 0.06 0.04 -0.14 -0.01 -0.01 -0.18 -0.75 -0.74 -0.46 -0.24 0.00 -0.01 -0. -0.26 -0.08 -0.75 -0.31 -0.10 -0.31 -0.08 -23.08 10.25 33.36 -55.55 -100.00 -100.00 -36.11 -100.00 -84.13 -100.00 -70.83 N/A -100.00 -98.21 -32.20 -9.23 -78.26 -50.00 -100.00 -100.00 -8.82 0.389 0.333 0.333 0.083 -0.014 0.000 0.278 107.699 61.538 266.640 33.320 -100.000 0.000 55. 0.01 0.01 0.00 0.03 0.00 0.01 0.00 0.06 0.07 0.00 0.03 0.01 0.00 0.06 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.000 0.000 0.000 0.000 0.014 0.014 0.000 0.01 0.01 0.00 0.03 0.00 0.01 0.00 0.06 0.07 0.00 0.03 0.00 -0.01 0. 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.000 0.000 0.000 0.000 0.014 0.014 0.000 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 0.00 -100.00 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A"
        },
        {
            "title": "Model",
            "content": "Grasp Correct Delta Delta(%) Intention Correct Delta Delta(%) Success Rate Delta Delta(%) Wrong Obj Attempt Delta Delta(%) Table 9: Full benchmark results for the Language Action tasks π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase 0.36 0.60 0.00 0.31 0.17 0.03 0.58 0.86 0.92 0.50 0.75 0.24 0.21 0.78 0.36 0.90 0.04 0.33 0.28 0.08 0.65 0.97 0.97 1.00 0.88 0.54 0.53 0. -0.25 -0.01 -0.42 -0.14 0.00 -0.19 -0.14 0.00 -0.01 -0.08 0.18 0.17 0.04 0.11 -0.31 -0.06 -0.33 -0.13 -0.08 -0.07 -0.04 0.00 -0.01 0.00 0.04 0.07 -0.04 -0.04 -40.91 -2.27 -100.00 -31.25 0.00 -87.50 -19.24 0.00 -1.49 -14.28 31.71 240.04 25.00 16. -45.83 -5.79 -88.88 -27.27 -23.08 -45.46 -6.00 0.00 -1.41 0.00 5.00 14.70 -7.31 -4.17 widowx_carrot_on_plate_lang_action 0.81 1.00 0.04 0.89 0.46 0.35 0.99 -19.44 0.00 -95.83 -11.11 17.86 -53.70 -1.39 -0.19 0.00 -0.96 -0.11 0.07 -0.40 -0.01 0.11 0.54 0.00 0.14 0.00 0.00 0. widowx_stack_cube_lang_action 0.26 0.36 0.21 0.13 0.00 0.00 0.28 -0.07 0.00 -0.13 -0.07 0.04 -0.07 -0.03 -6.94 0.00 -12.50 -7.35 7.69 -11.36 -2.78 0.93 1.00 0.88 0.88 0.58 0.54 0.97 widowx_spoon_on_towel_lang_action 0.29 0.82 0.68 0.97 0.00 0.17 0.11 0.86 0.04 0.61 0.01 0.36 0.99 0.26 widowx_eggplant_in_basket_lang_action 0.88 1.00 0.74 1.00 0.96 1.00 0.71 1.00 0.08 0.67 0.19 0.88 0.93 1.00 -14.49 -2.78 -83.33 -6.06 -8.33 -49.02 4. -0.14 -0.03 -0.83 -0.06 -0.06 -0.35 0.04 0.00 0.00 0.00 0.00 0.00 -0.04 0.00 0.00 0.00 0.00 0.00 0.00 -4.55 0.00 -0.25 0.00 -0.13 -0.11 -0.01 -0.01 -0.17 0.00 -0.04 -0.08 0.03 0.00 0.00 0.06 -0.17 -0.19 -0.21 -0.10 -0.06 0.00 -0. 0.06 -0.17 0.00 0.10 -0.01 -0.11 -0.01 -69.23 0.00 -100.00 -44.45 -100.00 -100.00 -33.33 0.00 -10.34 -28.59 28.56 N/A N/A 24.99 -36.37 -22.22 -100.00 -46.66 -57.13 0.00 -55.81 6.79 -18.46 0.00 15.90 -14.30 -36.37 -1.47 0.00 0.00 0.00 0.03 0.01 0.01 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 -0.04 -0.02 0.00 0.00 -0.08 -0.03 0.00 0. 0.00 0.00 0.00 0.00 0.00 0.00 0.00 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A -100.00 -100.00 N/A N/A -100.00 -100.00 N/A N/A N/A N/A N/A N/A N/A N/A N/A C.3.2 Language Negation"
        },
        {
            "title": "Model",
            "content": "Grasp Correct Delta Delta(%) Intention Correct Delta Delta(%) Success Rate Delta Delta(%) Wrong Obj Attempt Delta Delta(%) Table 10: Full benchmark results for the Language Negation tasks π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase 0.54 0.72 0.42 0.37 0.17 0.18 0.65 0.58 0.78 0.33 0.21 0.32 0.06 0.81 0.65 0.75 0.88 0.67 0.00 0.06 0.61 0.00 0.13 0.00 0.25 0.00 0.00 0. -0.07 0.11 0.00 -0.07 0.00 -0.04 -0.07 -0.08 -0.18 -0.04 -0.25 -0.04 -0.10 0.11 -0.25 -0.13 0.08 -0.03 -0.29 -0.22 -0.31 -0.43 -0.29 -0.17 -0.38 0.00 -0.03 -0.49 -11.37 18.19 0.00 -15.64 0.00 -18.74 -9.62 -12.50 -18.84 -11.12 -54.55 -11.54 -63.63 15. -27.70 -14.29 10.52 -4.00 -100.00 -80.00 -33.34 -100.00 -70.00 -100.00 -60.00 N/A -100.00 -89.74 C.3.3 Language Appearance widowx_carrot_on_plate_lang_neg 1.00 1.00 0.92 0.83 0.54 0.65 1.00 0.00 0.00 -0.08 -0.17 0.15 -0.10 0. 0.00 0.00 -8.33 -16.67 39.28 -12.96 0.00 0.11 0.51 0.38 0.07 0.01 0.01 0.18 widowx_spoon_on_towel_lang_neg 0.44 0.69 0.17 0.17 0.01 0.03 0.68 0.04 0.00 -0.08 -0.04 -0.10 -0.25 0.06 4.35 0.00 -8.33 -4.55 -14.58 -35.29 5. 1.00 1.00 0.92 0.88 0.57 0.46 1.00 widowx_coke_can_on_plate_lang_neg 0.33 0.85 0.29 1.00 0.38 0.96 0.21 1.00 0.00 0.03 0.00 0.08 0.89 0.40 widowx_orange_juice_on_plate_lang_neg 0.00 0.03 0.07 0.19 0.00 0.08 0.13 0.46 0.00 0.03 0.00 0.03 0.06 0.24 -15.28 0.00 -4.17 5.88 -95.00 -87.50 -11.11 -96.82 -70.22 -91.67 -54.17 N/A -60.03 -74.63 -0.15 0.00 -0.04 0.06 -0.53 -0.58 -0.11 -0.85 -0.46 -0.92 -0.54 0.03 -0.04 -0.69 -0.25 -0.03 0.25 -0.18 0.00 0.00 -0. -0.01 -0.18 -0.04 -0.04 -0.08 0.01 0.08 -0.40 -0.51 -0.08 -0.15 -0.06 -0.04 -0.31 -0.31 -0.24 -0.13 -0.17 0.00 -0.01 -0.35 -69.22 -5.13 200.00 -72.21 0.00 0.00 -63.89 -3.04 -20.63 -19.97 -19.97 -85.70 99.76 13.95 -54.72 -63.79 -18.18 -42.32 -100.00 -100.00 -43. -100.00 -77.28 -100.00 -57.15 N/A -100.00 -86.20 0.00 0.00 0.00 0.00 0.06 0.01 0.00 0.03 0.00 0.00 0.04 0.01 0.00 0.01 0.07 0.00 0.04 0.00 0.01 0.03 0.04 0.78 0.60 0.63 0.21 0.14 0.14 0.74 0.00 0.00 0.00 0.00 0.06 0.01 0. 0.01 0.00 0.00 -0.04 -0.01 0.00 0.01 0.07 0.00 0.04 -0.01 0.01 0.03 0.04 0.78 0.60 0.63 0.21 0.14 0.14 0.74 N/A N/A N/A N/A N/A N/A N/A"
        },
        {
            "title": "33.33\nN/A\nN/A\n-49.94\n-49.94\nN/A\nN/A",
            "content": "N/A N/A N/A -100.00 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A"
        },
        {
            "title": "Model",
            "content": "π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase Table 11: Full benchmark results for the Language Appearance tasks Grasp Correct Delta Delta(%) Intention Correct Delta Delta(%) Success Rate Delta Delta(%) Wrong Obj Attempt Delta Delta(%) 0.36 0.58 0.25 0.25 0.01 0.04 0.21 0.74 0.81 0.88 0.33 0.08 0.00 0.72 0.15 0.72 0.21 0.38 0.22 0.04 0.11 1.00 0.65 1.00 0.88 0.32 0.63 1.00 -0.25 -0.03 -0.17 -0.19 -0.15 -0.18 -0. -0.11 -0.14 0.29 -0.50 0.07 -0.01 -0.14 -0.51 -0.24 -0.17 -0.08 -0.14 -0.11 -0.58 0.03 -0.33 0.00 0.04 -0.15 0.06 0.00 -40.91 -4.54 -40.00 -43.75 -91.66 -81.25 -71.15 -13.11 -14.70 50.01 -60.00 499.52 -100.00 -16.13 -77.08 -24.64 -44.45 -18.18 -38.46 -72.73 -84. 2.86 -33.80 0.00 5.00 -32.36 9.76 0.00 widowx_carrot_on_plate_lang_color 0.99 1.00 0.42 0.88 0.14 0.35 0.71 -0.01 0.00 -0.58 -0.13 -0.25 -0.40 -0.29 -1.39 0.00 -58.33 -12.50 -64.28 -53.70 -29.17 0.28 0.56 0.04 0.08 0.00 0.00 0. widowx_cube_on_plate_lang_shape 0.92 0.96 1.00 0.92 0.31 0.07 0.99 -0.08 -0.04 0.00 -0.08 0.25 0.00 -0.01 -8.33 -4.17 0.00 -8.33 450.24 -0.05 -1.39 0.44 0.71 0.46 0.04 0.00 0.00 0.47 widowx_spoon_on_towel_lang_color -0.04 0.00 -0.58 -0.04 -0.04 -0.31 -0.01 -4.34 0.00 -58.33 -4.55 -6.25 -43.13 -1.47 0.06 0.92 0.61 1.00 0.08 0.42 0.08 0.88 0.11 0.63 0.03 0.40 0.93 0.04 widowx_eggplant_in_basket_lang_color 0.94 1.00 0.58 1.00 1.00 1.00 0.79 1.00 0.13 0.67 0.36 0.96 0.97 1.00 0.00 0.00 0.00 0.00 0.00 4.55 0.00 0.00 0.00 0.00 0.00 0.00 0.04 0.00 -0.08 0.01 -0.08 -0.17 -0.01 -0.01 -0. -0.31 -0.17 0.00 -0.29 0.00 -0.01 -0.31 -0.40 -0.26 -0.13 -0.12 0.01 0.01 -0.56 0.13 -0.32 0.04 0.18 0.03 0.06 0.03 -23.08 2.57 -66.64 -66.68 -100.00 -100.00 -66.66 -40.74 -19.04 0.00 -87.49 N/A -100.00 -39.28 -87.88 -30.16 -60.01 -60.01 14.30 99.76 -93. 15.25 -35.39 4.35 29.55 28.56 18.18 2.94 0.01 0.00 0.00 0.00 0.00 0.04 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.03 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.00 0.04 0. 0.00 0.00 0.00 0.00 -0.01 -0.01 0.00 -0.02 0.01 0.00 -0.08 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A -100.00 -100.00 N/A -100.00 N/A N/A -100.00 0.00 N/A N/A N/A N/A N/A N/A N/A N/A N/A 23 C.4 Vision-Language Thinking C.4.1 Object Distraction Model Grasp Correct Delta Delta(%) Intention Correct Delta Delta(%) Success Rate Delta Delta(%) Wrong Obj Attempt Delta Delta(%) Table 12: Full benchmark results for the Object Distraction tasks π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase 0.708 0.639 0.458 0.583 0.153 0.181 0.708 0.64 0.56 0.33 0.37 0.11 0.28 0.61 0.57 0.90 0.67 0.31 0.36 0.22 0.74 0.90 0.72 0.25 0.54 0.22 0.25 0. 0.85 0.83 0.88 0.54 0.32 0.21 0.89 0.000 0.056 0.000 0.125 0.000 0.000 0.069 0.097 0.028 0.042 0.139 -0.014 -0.042 -0.014 15.911 4.549 9.983 31.236 -8.340 -18.737 -1.925 0.04 0.00 0.17 -0.04 0.03 0.11 0.08 -0.10 -0.06 0.29 -0.15 0.00 0.07 0. 0.13 -0.11 0.08 0.11 0.11 0.04 0.06 -0.06 -0.04 0.08 -0.15 0.03 -0.07 -0.03 6.98 -0.01 99.94 -10.00 33.32 66.66 15.79 -14.58 -5.80 77.79 -33.33 0.00 45.44 6.00 16.08 -13.33 49.97 25.81 100.03 20.00 7.14 -6.15 -4.77 10.52 -22.00 9.53 -25.00 -3. -0.431 -0.361 -0.167 -0.500 0.000 -0.028 -0.472 -100.000 -86.664 -100.000 -80.000 N/A -100.000 -87.175 widowx_carrot_on_plate_distract 1.000 1.000 0.958 0.944 0.583 0.736 1.000 0.000 0.000 -0.042 -0.056 0.194 -0.014 0.000 0.000 0.000 -4.170 -5.557 49.996 -1.845 0. 0.222 0.542 0.208 0.236 0.014 0.014 0.306 widowx_carrot_on_keyboard_distract 1.00 1.00 0.50 0.88 0.35 0.82 1.00 0.00 0.00 0.29 0.08 0.17 0.24 0.00 0.00 0.00 140.04 10.53 92.30 40.47 0.00 0.17 0.33 0.13 0.17 0.00 0.03 0. widowx_spoon_on_towel_distract 0.01 0.00 -0.04 0.03 0.00 -0.04 0.03 1.45 0.00 -4.17 3.03 0.00 -5.88 2.94 0.17 0.97 0.51 1.00 0.33 0.96 0.11 0.94 0.00 0.67 0.01 0.67 0.97 0.26 widowx_coke_can_on_keyboard_distract 0.28 1.00 0.40 0.96 0.08 0.29 0.21 1.00 0.00 0.31 0.00 0.68 0.13 0.96 0.00 -4.17 74.99 2.86 83.34 2.08 -4.17 0.00 -0.04 0.13 0.03 0.14 0.01 -0. widowx_coke_can_on_plate_distract -0.01 0.00 0.00 0.06 0.04 -0.11 0.00 -1.39 0.00 0.00 5.88 7.50 -16.66 0.00 0.68 0.99 0.61 1.00 0.38 1.00 0.29 1.00 0.10 0.60 0.01 0.56 1.00 0.71 widowx_orange_juice_on_plate_distract 0.014 0.125 0.000 0.250 0.000 0.028 0.153 -98.411 -80.852 -100.000 -75.000 N/A -60.029 -83.583 -0.861 -0.528 -1.000 -0.750 0.000 -0.042 -0. 0.000 0.042 0.000 0.083 0.000 0.000 0.028 -0.139 0.000 0.083 -0.014 0.000 0.000 -0.194 -0.08 0.03 0.08 -0.04 0.00 0.03 -0.11 -0.29 -0.36 0.12 -0.10 -0.10 0.00 -0.33 -0.11 0.17 0.00 0.07 0.00 0.00 -0.18 -0.06 -0.19 -0.08 -0.07 0.04 -0.03 0. -38.457 0.000 66.640 -5.547 0.000 0.000 -38.887 -33.33 9.10 199.76 -20.00 N/A N/A -57.12 -63.64 -41.27 60.01 -46.66 -100.00 0.00 -55.82 -28.58 70.57 0.00 49.96 N/A N/A -59.09 -7.55 -24.13 -18.18 -19.22 74.93 -66.64 0.00 -0.306 -0.264 -0.125 -0.208 0.000 -0.014 -0. -100.000 -86.364 -100.000 -71.443 N/A -100.000 -93.107 0.000 0.028 0.000 0.000 0.014 0.014 0.000 0.00 0.03 0.00 0.03 0.01 0.03 0.00 0.03 0.00 0.04 0.01 0.03 0.04 0.00 0.00 0.00 0.00 0.04 0.00 0.03 0.00 0.00 0.01 0.00 0.00 0.01 0.03 0. 0.778 0.736 0.500 0.500 0.083 0.292 0.625 0.000 0.028 0.000 0.000 0.014 0.014 0.000 -0.01 0.03 0.00 0.03 0.01 0.03 0.00 0.01 0.00 0.04 -0.07 0.00 0.04 0.00 0.00 0.00 0.00 0.04 0.00 0.03 0.00 0.00 0.01 0.00 -0.01 0.01 0.03 0. 0.778 0.736 0.500 0.500 0.083 0.292 0.625 N/A N/A N/A N/A N/A N/A N/A -100.00 N/A N/A N/A N/A N/A N/A 33.17 N/A N/A -83.31 0.00 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A -100.00 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 24 C.4.2 Commonsense Model Grasp Correct Delta Delta(%) Intention Correct Delta Delta(%) Success Rate Delta Delta(%) Wrong Obj Attempt Delta Delta(%) widowx_carrot_on_plate_lang_common Table 13: Full benchmark results for the Commonsense tasks π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase 0.153 0.431 0.167 0.611 0.028 0.000 0.333 0.486 0.486 0.375 0.292 0.111 0.278 0. 0.139 0.681 0.417 0.292 0.056 0.042 0.431 1.000 0.764 0.958 0.875 0.403 0.278 0.986 0.736 0.625 0.875 0.583 0.042 0.153 0.583 0.431 0.597 0.125 0.167 0.000 0.000 0.361 0.236 0.583 0.458 0.333 0.000 0.000 0.361 -0.458 -0.181 -0.250 0.167 -0.139 -0.222 -0. -0.111 -0.069 0.208 -0.125 0.028 0.111 0.042 -0.528 -0.278 0.042 -0.167 -0.306 -0.111 -0.264 0.028 -0.222 -0.042 0.042 -0.069 -0.292 -0.014 -0.167 -0.250 0.083 -0.111 -0.250 -0.125 -0.333 -0.167 -0.333 -0.208 -0.333 -0.056 -0.028 -0.222 -0.194 0.167 0.292 -0.292 0.000 -0.028 -0. -74.996 -29.542 -59.995 37.491 -83.340 -100.000 -53.847 -18.603 -12.504 124.955 -29.986 33.320 66.680 7.895 -79.164 -28.984 11.120 -36.366 -84.613 -72.731 -38.000 2.859 -22.533 -4.170 4.996 -14.710 -51.221 -1.390 -18.465 -28.574 10.522 -16.007 -85.714 -44.990 -36.366 -27.902 -35.818 -62.496 -66.660 -100.000 -100.000 -38. -45.153 40.000 174.925 -46.672 N/A -100.000 -33.335 -0.278 -0.250 -0.083 0.028 -0.014 -0.014 -0.333 -0.181 -0.250 -0.042 -0.208 0.000 0.000 -0.153 -0.389 -0.319 0.042 -0.097 -0.083 -0.014 -0.333 -0.153 -0.194 -0.083 -0.111 -0.097 -0.306 -0.264 -0.194 -0.292 -0.208 -0.194 -0.056 -0.042 -0. -0.236 -0.417 -0.208 -0.042 0.000 0.000 -0.208 -0.139 0.056 0.042 -0.125 0.000 -0.014 -0.139 -76.922 -46.148 -66.640 11.107 -100.000 -100.000 -66.667 -72.227 -81.815 -100.000 -100.000 N/A N/A -78.553 -84.851 -36.504 20.019 -46.663 -85.700 -100.000 -55.813 -18.647 -21.540 -8.692 -18.185 -100.000 -100.000 -27. -26.414 -36.204 -45.451 -53.836 -100.000 -100.000 -64.706 -48.570 -50.848 -83.320 -100.000 N/A N/A -53.570 -45.467 18.185 33.360 -42.852 N/A -100.000 -34.492 0.028 0.000 0.000 0.000 0.000 0.000 0.000 0.014 0.014 0.000 0.000 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.000 0.014 0. 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.000 0.000 0.014 0.028 0.000 0.000 0.000 0.000 0.000 0.000 0.028 0.000 0.028 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.014 0.000 0.000 0.000 0.000 0. -0.021 0.014 0.000 -0.083 -0.028 0.014 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 -0.014 0.000 0.000 0.014 0.000 0.000 0.000 0.000 0.014 0.028 0.000 0.000 0.000 0.000 0.000 0.000 0.028 0.000 N/A N/A N/A N/A N/A N/A N/A 0.000 N/A N/A N/A N/A N/A N/A -100.000 N/A N/A -100.000 -100.000 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A -100.000 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A -0.125 -0.014 0.000 0.000 -0.278 -0.611 -0.125 -12.500 -1.390 0.000 0.000 -71.432 -81.479 -12.500 0.083 0.875 0.292 0.986 0.042 1.000 0.278 1.000 0.000 0.111 0.000 0.139 0.875 0.167 widowx_carrot_on_keyboard_lang_common 0.069 1.000 0.056 1.000 0.000 0.708 0.000 0.833 0.000 0.431 0.000 0.653 0.042 1.000 0.000 0.000 240.038 5.255 138.453 11.909 0.000 0.000 0.000 0.500 0.042 0.250 0.069 0.000 widowx_spoon_on_towel_lang_common -0.444 0.000 -0.042 0.014 -0.333 -0.514 -0.042 -46.374 0.000 -4.170 1.513 -50.000 -72.546 -4.412 0.069 0.514 0.556 1.000 0.250 0.958 0.111 0.931 0.014 0.333 0.000 0.194 0.903 0.264 widowx_eggplant_in_basket_lang_common 0.667 1.000 0.708 1.000 0.875 1.000 0.500 1.000 0.000 0.667 0.000 0.778 1.000 0.681 widowx_coke_can_on_plate_lang_common 0.542 0.986 0.514 0.986 0.250 1.000 0.167 0.917 0.000 0.181 0.000 0.569 0.250 0.958 0.000 0.000 0.000 0.000 0.000 -15.153 0.000 -1.390 -1.390 0.000 -2.937 -67.499 -14.585 -4.170 0.000 0.000 0.000 0.000 0.000 -0.139 0. -0.014 -0.014 0.000 -0.028 -0.375 -0.097 -0.042 widowx_nut_on_plate_lang_common 0.903 0.667 0.667 0.417 0.069 0.083 1.000 -0.097 -0.333 -0.333 -0.583 -0.097 0.028 0.000 -9.720 -33.330 -33.330 -58.330 -58.340 49.970 0.000 0.250 0.403 0.042 0.000 0.000 0.000 0. widowx_orange_juice_on_plate_lang_common 0.986 0.917 1.000 1.000 0.069 0.028 0.931 0.111 0.264 0.000 0.000 0.069 -0.042 0.000 12.697 40.426 0.000 0.000 N/A -60.029 0.000 0.167 0.361 0.167 0.167 0.000 0.000 0.264 C.4.3 Commonsense and Distraction Table 14: Full benchmark results for the Commonsense and Distraction tasks Model Grasp Correct Delta Delta(%) Intention Correct Delta Delta(%) Success Rate Delta Delta(%) Wrong Obj Attempt Delta Delta(%) π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase π0 finetune π0 baseline Spatial VLA Magma Octo Small Octo Base π0 finetune + rephrase 0.208 0.569 0.250 0.222 0.042 0.042 0. 0.014 0.333 0.167 0.486 0.014 0.014 0.069 0.611 0.361 0.792 0.500 0.028 0.153 0.319 0.028 0.069 0.042 0.083 0.000 0.000 0.056 0.000 0.014 0.000 0.000 0.014 0.014 0.083 0.069 -0.111 -0.167 -0.069 -0.014 0.000 -0.319 -0.139 -0.097 0.000 -0.125 -0.014 0.014 -0. -0.125 -0.264 -0.083 -0.083 -0.014 0.000 -0.264 -0.208 -0.514 -0.417 -0.250 0.000 0.000 -0.306 -0.236 -0.569 -0.458 -0.333 0.014 0.014 -0.278 49.988 -16.330 -40.005 -23.797 -25.015 0.000 -74.197 -90.903 -22.583 0.000 -20.455 -49.940 N/A -79.170 -16.981 -42.224 -9.520 -14.281 -33.360 0.000 -45. -88.227 -88.091 -90.901 -75.008 N/A N/A -84.621 -100.000 -97.617 -100.000 -100.000 N/A N/A -76.922 widowx_spoon_on_towel_lang_common_distract 0.514 1.000 0.625 0.889 0.319 0.278 0.556 0.000 0.000 -0.333 -0.042 -0.014 0.083 -0.347 -0.006 0.000 -34.780 -4.478 -4.160 42.852 -38. 0.069 0.167 0.042 0.069 0.000 0.000 0.014 widowx_carrot_on_plate_lang_common_distract 0.333 0.750 0.833 0.889 0.097 0.125 0.375 -0.542 -0.236 -0.167 -0.111 -0.014 -0.014 -0.500 -61.901 -23.943 -16.670 -11.113 -12.511 -10.007 -57.143 0.000 0.250 0.083 0.333 0.000 0.000 0. widowx_coke_can_on_plate_lang_common_distract 0.819 0.653 0.875 0.833 0.056 0.361 0.750 -0.167 -0.333 -0.125 -0.083 -0.125 -0.208 -0.208 -16.902 -33.800 -12.500 -9.098 -69.227 -36.586 -21.736 0.431 0.292 0.250 0.208 0.000 0.000 0.181 widowx_coke_can_on_plate_lang_common_distract 0.208 0.181 0.083 0.125 0.000 0.014 0.319 -0.778 -0.736 -0.917 -0.875 -0.069 -0.014 -0.611 -78.873 -80.303 -91.670 -87.500 -100.000 -49.940 -65.673 0.028 0.028 0.000 0.042 0.000 0.000 0.028 0.000 -0.389 -0.208 -0.042 -0.014 0.000 -0.250 -0.083 -0.042 0.042 0.056 0.000 0.000 -0. -0.111 -0.222 0.000 0.042 0.000 0.000 -0.069 -0.139 -0.333 -0.167 -0.125 0.000 0.000 -0.236 widowx_orange_juice_on_plate_lang_common_distractv2 0.069 0.167 0.125 0.042 0.042 0.028 0.264 -0.917 -0.750 -0.875 -0.958 -0.028 0.000 -0.667 -92.959 -81.819 -87.500 -95.830 -39.990 0.000 -71. 0.000 0.000 0.000 0.000 0.000 0.000 0.042 -0.167 -0.361 -0.167 -0.167 0.000 0.000 -0.222 0.000 -70.007 -83.320 -37.474 -100.000 N/A -94.733 -100.000 -14.307 99.760 20.017 N/A N/A -66.660 -20.517 -43.244 0.000 24.955 N/A N/A -27.787 -83.317 -92.311 -100.000 -74.985 N/A N/A -89. -100.000 -100.000 -100.000 -100.000 N/A N/A -84.197 0.042 0.014 0.250 0.014 0.014 0.111 0.236 0.514 0.181 0.042 0.000 0.014 0.083 0.556 0.083 0.236 0.083 0.000 0.000 0.056 0.111 0.444 0.667 0.667 0.708 0.111 0.097 0.292 0.444 0.792 0.833 0.417 0.000 0.056 0. 0.042 0.000 0.250 0.014 0.014 0.097 0.236 0.486 0.181 0.042 0.000 0.014 0.083 0.556 0.083 0.236 0.083 0.000 0.000 0.056 0.097 0.444 0.667 0.667 0.708 0.111 0.069 0.292 0.444 0.792 0.833 0.417 0.000 0.028 0.653 N/A 0.000 N/A N/A N/A 699.281 N/A 1748.441 N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A N/A 699.281 N/A N/A N/A N/A N/A 250.060 N/A N/A N/A N/A N/A N/A 100.120 N/A"
        },
        {
            "title": "D How to Calculate Delta",
            "content": "The deltas in all the metrics are calculated by subtracting the performance in the tasks listed in the left column of Table 15 from the performance in the corresponding task listed in the right column. Table 15: Full mapping from variant task names to canonical task names. Canonical Task Name Variant Task Name widowx_spoon_on_towel widowx_spoon_on_towel widowx_carrot_on_plate widowx_carrot_on_plate widowx_stack_cube widowx_stack_cube widowx_put_eggplant_in_basket widowx_put_eggplant_in_basket widowx_carrot_on_plate widowx_cube_on_plate_clean widowx_cube_on_plate_clean widowx_small_plate_on_green_cube_clean widowx_carrot_on_plate widowx_coke_can_on_plate_clean widowx_carrot_on_plate widowx_pepsi_on_plate_clean widowx_carrot_on_plate widowx_carrot_on_sponge_clean widowx_put_eggplant_in_basket widowx_eggplant_on_sponge_clean widowx_carrot_on_plate widowx_carrot_on_keyboard_clean widowx_coke_can_on_plate_clean widowx_coke_can_on_keyboard_clean widowx_spoon_on_towel widowx_spoon_on_towel_distract widowx_carrot_on_plate widowx_carrot_on_plate_distract widowx_carrot_on_keyboard_clean widowx_carrot_on_keyboard_distract widowx_coke_can_on_plate_clean widowx_coke_can_on_plate_distract widowx_coke_can_on_keyboard_clean widowx_coke_can_on_keyboard_distract widowx_carrot_on_plate widowx_carrot_on_plate_lang_common widowx_carrot_on_plate widowx_carrot_on_plate_lang_action widowx_carrot_on_plate widowx_carrot_on_plate_lang_neg widowx_carrot_on_plate_distract widowx_carrot_on_plate_lang_neg_action widowx_carrot_on_plate_lang_common widowx_carrot_on_plate_lang_common_distract widowx_spoon_on_towel widowx_spoon_on_towel_lang_action widowx_spoon_on_towel widowx_spoon_on_towel_lang_common widowx_spoon_on_towel_lang_common widowx_spoon_on_towel_lang_common_distract widowx_stack_cube widowx_stack_cube_lang_action widowx_put_eggplant_in_basket widowx_eggplant_in_basket_lang_action widowx_put_eggplant_in_basket widowx_eggplant_in_basket_lang_color widowx_put_eggplant_in_basket widowx_eggplant_in_basket_lang_common widowx_carrot_on_keyboard_clean widowx_carrot_on_keyboard_lang_common widowx_coke_can_on_plate_clean widowx_coke_can_on_plate_lang_common widowx_coke_can_on_plate_clean widowx_coke_can_on_plate_lang_neg widowx_coke_can_on_plate_lang_common widowx_coke_can_on_plate_lang_common_distract widowx_carrot_on_plate widowx_orange_juice_on_plate_clean widowx_orange_juice_on_plate_clean widowx_orange_juice_on_plate_distract widowx_orange_juice_on_plate_clean widowx_orange_juice_on_plate_lang_neg widowx_orange_juice_on_plate_clean widowx_orange_juice_on_plate_lang_common widowx_orange_juice_on_plate_lang_common_distract widowx_orange_juice_on_plate_lang_common widowx_orange_juice_on_plate_lang_common_distractv2 widowx_orange_juice_on_plate_lang_common widowx_nut_on_plate_clean widowx_nut_on_plate_lang_common widowx_eggplant_on_keyboard_clean widowx_carrot_on_ramekin_clean widowx_carrot_on_wheel_clean widowx_coke_can_on_ramekin_clean widowx_coke_can_on_wheel_clean widowx_nut_on_wheel_clean widowx_cube_on_plate_lang_shape widowx_spoon_on_towel_lang_neg widowx_spoon_on_towel_lang_color widowx_carrot_on_plate_lang_color widowx_carrot_on_plate widowx_nut_on_plate_clean widowx_put_eggplant_in_basket widowx_carrot_on_plate widowx_carrot_on_plate widowx_coke_can_on_plate_clean widowx_coke_can_on_plate_clean widowx_nut_on_plate_clean widowx_cube_on_plate_clean widowx_spoon_on_towel widowx_spoon_on_towel widowx_carrot_on_plate"
        }
    ],
    "affiliations": []
}