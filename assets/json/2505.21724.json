{
    "paper_title": "OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions",
    "authors": [
        "Cheng Luo",
        "Jianghui Wang",
        "Bing Li",
        "Siyang Song",
        "Bernard Ghanem"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), a novel task that aims to online generate synchronized verbal and non-verbal listener feedback, conditioned on the speaker's multimodal input. OMCRG reflects natural dyadic interactions and poses new challenges in achieving synchronization between the generated audio and facial responses of the listener. To address these challenges, we innovatively introduce text as an intermediate modality to bridge the audio and facial responses. We hence propose OmniResponse, a Multimodal Large Language Model (MLLM) that autoregressively generates high-quality multi-modal listener responses. OmniResponse leverages a pretrained LLM enhanced with two novel components: Chrono-Text, which temporally anchors generated text tokens, and TempoVoice, a controllable online TTS module that produces speech synchronized with facial reactions. To support further OMCRG research, we present ResponseNet, a new dataset comprising 696 high-quality dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and facial behavior annotations. Comprehensive evaluations conducted on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 4 2 7 1 2 . 5 0 5 2 : r OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions Cheng Luo1, Jianghui Wang1, Bing Li1, Siyang Song2, Bernard Ghanem1 1King Abdullah University of Science and Technology, 2University of Exeter Project Page: https://omniresponse.github.io/"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we introduce Online Multimodal Conversational Response Generation (OMCRG), novel task that aims to online generate synchronized verbal and non-verbal listener feedback, conditioned on the speakers multimodal input. OMCRG reflects natural dyadic interactions and poses new challenges in achieving synchronization between the generated audio and facial responses of the listener. To address these challenges, we innovatively introduce text as an intermediate modality to bridge the audio and facial responses. We hence propose OmniResponse, Multimodal Large Language Model (MLLM) that autoregressively generates highquality multi-modal listener responses. OmniResponse leverages pretrained LLM enhanced with two novel components: Chrono-Text, which temporally anchors generated text tokens, and TempoVoice, controllable online TTS module that produces speech synchronized with facial reactions. To support further OMCRG research, we present ResponseNet, new dataset comprising 696 high-quality dyadic interactions featuring synchronized split-screen videos, multichannel audio, transcripts, and facial behavior annotations. Comprehensive evaluations conducted on ResponseNet demonstrate that OmniResponse significantly outperforms baseline models in terms of semantic speech content, audio-visual synchronization, and generation quality."
        },
        {
            "title": "Introduction",
            "content": "Generating realistic human conversational responses has substantial potential across numerous applications, spanning from human-computer interactions [33], immersive metaverse experiences [23], to mental health interventions [24]. However, human communication is inherently multimodal and complex. In face-to-face interactions, speakers convey their messages not only through spoken language but also through non-verbal cues, such as lip movements and facial expressions. Correspondingly, listeners provide multimodal responses consisting of verbal (e.g., audible affirmations or disapprovals) and non-verbal responses (e.g., subtle head nods). While considerable efforts [8, 60] have been dedicated to modeling text dialogue, particularly in language-based interfaces [29], modeling multimodal conversational interactions has been much underexplored. In this paper, we explore new task: learning to simultaneously generate verbal and non-verbal listener 1 responses in an online dyadic conversation setting, conditioned on the speakers verbal and non-verbal inputs (see Figure 1). We refer to this task as Online Multimodal Conversational Response Generation. Although various audio-to-video generation methods (e.g. talking head generation [74]) have shown impressive performance, these methods focus on synthesizing visual content aligned Corresponding author. 1Previous studies [6, 17] defined speakerlistener framework for dyadic interactions, in which the listener both attends to the speakers utterances and provides verbal and nonverbal feedback. Preprint. Under review. Figure 1: Illustration of the new OMCRG task. (a) In offline tasks, the generation model generates the listeners full response only after receiving the entire input sequence from the speaker. (b) Differently, OMCRG task requires sequentially processing the speakers incoming input and generating multi-modal responses for the listener on the fly. with input audio signals, which ignores explicitly modeling multimodal conversational interactions. Recent studies [37, 43, 54] propose to generate facial reactions for listener; however, these methods overlook verbal responses, which are essential to engage in dialogue fully. The OMCRG task is complex and poses major challenges in three aspects. First, it is non-trivial to directly achieve synchronization between the generated audio and facial reactions of the listener for OMCRG task. As revealed in existing talking-head works [74, 59], achieving precise alignment between facial motion and audio is already challenging, even when the entire audio signal is given. In contrast, OMCRG is to generate both audio and facial reactions simultaneously and incrementally. Such online and multimodal generation settings make face-audio synchronization much more difficult, due to the high variability and semantic ambiguity of audio modality. Second, due to the online setting, the model has to reason over partial speaker input and generate audio-visual responses on the fly, which requires both powerful audio-visual understanding and generation abilities. While powerful pre-trained models have been developed for language and vision, audio modeling remains comparatively underdeveloped, making it more challenging to generate expressive and appropriate audio and facial reactions. Third, the lack of high-quality datasets for dyadic multimodal interaction significantly hinders the development of OMCRG. We address the above challenges by proposing unified framework, OmniResponse, which autoregressively generates high-quality multimodal listener responses. Rather than directly synchronizing generated audio and facial reactions, our key insight is to introduce text as an intermediate modality for the OMCRG task. Compared with audio, text offers clearer semantics and reduces uncertainty, making it more tractable for learning multimodal reaction generation. However, text is static modality without inherent temporal information, posing challenges for synchronizing spoken words with visual frames in an autoregressive generation setting. To overcome this, we introduce Multimodal Large Language Model augmented with two innovative modules: Chrono-Text and TempoVoice. The Chrono-Text module temporally anchors generated textual tokens by incorporating additional tokens (markers) that explicitly encode time, ensuring alignment between words and visual frames. TempoVoice is controllable, online text-to-speech module designed to produce synchronized audio from these temporally annotated textual embeddings, ensuring accurate synchronization between audio and facial reactions. In addition, we construct high-quality dataset named ResponseNet, comprising 696 dyadic conversation pairs. Each pair includes synchronized split-screen video streams of both speaker and listener, multichannel audio recordings, verbatim text transcriptions, and detailed facial-behavior annotations (i.e., facial expressions and head movements). Through extensive retrieval for scarce dyadic video data, rigorous content filtering, meticulous camera-shift alignment, and manual annotation, ResponseNet delivers unique and valuable resource for benchmarking OMCRG. Our contributions are summarized as follows: (1) we present OmniResponse, the first online model to jointly process and generate synchronized streams of conversational human behavior, establishing foundation for future work in humanagent interaction; (2) we introduce ResponseNet, an annotated dyadic conversation dataset and benchmark, enabling standardized evaluation of OMCRG models."
        },
        {
            "title": "2 Related Work",
            "content": "Facial Reaction Generation. Facial Reaction Generation (FRG) [56, 75] is particularly challenge new task as it requires to predict the non-deterministic human facial reactions under different contexts. The early approaches to FRG [21, 22] relied on Generative Adversarial Networks (GANs) [39, 16] typically conditioned the generation process on the speaker visual-speech behaviors. Since FRG is non-deterministic process (i.e., different facial reactions can be triggered by the same speaker behavior [56]), recent advances have shifted towards more sophisticated generative frameworks. For example, Ng et al. [43] introduces non-deterministic approach based on Variational Autoencoders (VAEs) [25], which enabled sampling diverse human facial motions. This work was complemented by novel dataset containing paired recordings of active speakers and silent listeners, providing essential training data for modeling natural reactions. Zhou et al. [75] developed specialized speaker-listener video dataset for head motion generation, which is somewhat limited by its relatively short clip durations (median length of 9.0 sec) and modest dataset scale (1.58 hours total), and thus constraining their models ability to learn long-term temporal dependencies. More recent works have attempted to address these limitations through innovative architectural choices or larger-scale datasets [54, 55]. Luo et al. [37] and Zhu et al. [76] proposed transformer-based [63] VAE and diffusion models [57, 18], respectively, training them on hybrid collection of videos from three different human-human dyadic interaction datasets [10, 52, 46]. While such FRG approaches achieved temporal alignment and more diverse facial reactions, they fail to produce multi-modal human behavior responses such as voice and spoken words. Furthermore, the datasets they used are multilingual without listenrs audio stream, comprehensive multimodal annotations or precise timestamps, limiting their utility for training online models for multimodal human response generation. Spoken Dialogue Models. Spoken dialogue models generate natural speech responses in real-time, requiring systems to process both verbal content and paralinguistic elements of communication. Early approaches, including AudioPALM [53], Spectron [42], and SpeechGPT [69] adopted pipelines that combine automatic speech recognition (ASR), text generation, and text-to-speech (TTS) synthesis. However, their requirement to complete the entire response before speech generation makes them unsuitable for live human-computer interactions. Recent developments [40, 14, 45] have shifted towards end-to-end approaches that directly model speech-to-speech generation. Representative examples include Moshi et al. [14] and dGSLM [45], which operate as full-duplex speech dialogue systems capable of processing continuous speaker input while generating appropriate vocal responses. Although these advances are significant, they focus exclusively on speech and text modalities, overlooking the crucial visual aspects of human communication. Even recent work by Park et al. [47] that includes visual-speech data is limited to intermittent speaker-listener interactions. Autoregressive Generative Model. Transformer-based autoregressive models [63] have revolutionized numerous domains in AI, demonstrating remarkable success in language modeling [8, 60], multi-modal processing [34, 3, 28, 41], and generative tasks [51, 73, 67, 66, 65, 58]. Their success can be attributed to their inherent scalability and ability to unify multi-modal training under single autoregressive objective, enabling seamless integration of different data modalities. The adaptation of transformers to visual tasks was pioneered by approaches such as VQVAE [62] and VQGAN [15], which introduced effective methods for quantizing visual information into discrete tokens. They align visual generation with the successful paradigm of language modeling by employing decoderonly transformers to predict sequences of image tokens. Subsequent research [11] has focused on enhancing both the efficiency of tokenization processes [38, 31] and sampling procedures [68], while simultaneously scaling up model architectures to handle increasingly complex tasks. Recent trends have further pushed towards unified architectures capable of handling multiple modalities [41, 71] and diverse tasks [73, 67] within single autoregressive framework."
        },
        {
            "title": "3 Methodology",
            "content": "Problem Definition. Let Fs Given the speakers steaming facial sequence Fs goal of OMCRG is to online generate facial reactions Fl be the speakers facial and audio cues at time t, respectively. 1:t from time 1 to t, the at time step t. 1:t and audio sequence As and audio feedback Al and As Figure 2: Overview of the proposed OmniResponse. The model takes textual conversational history and newly coming multimodal information (e.g., facial cues) from the speaker and listener as input, and generates temporally synchronized facial and textual responses for the listener by leveraging pre-trained LLM enhanced with our proposed Chrono-Text Markup. The generated text embeddings are converted into audio synchronized with the facial response by the proposed TempoVoice module. Such multi-modal generation has been much less underexplored, different from recent works [75, 37, 14, 69] mainly focusing on single-modal response generation. To provide natural responses, it is crucial to ensure that the generated facial reactions and audio are temporally synchronized and react appropriately to the speaker. However, this is significantly challenging due to the inherent difficulty of online audio-visual understanding and generation. Instead of direct audio-visual generation, we introduce text as an intermediate modality and decompose OMCRG into two subproblems: (1) text-face-response generation: generating temporally aligned facial reactions Fl t; (2) synchronous text-to-speech synthesis: converting the textual response to the audio waveform segments Al aligned with the facial reactions. However, the absence of temporal information in text prevents temporal alignment with facial reactions and audio, challenging the two subproblems. We address this issue by two novel modules. and textual response Wl Overview. We present OmniResponse, novel framework for the OMCRG task (see Figure 2), where OmniResponse is new MLLM enhanced by two proposed key components: Chrono-Text Markup and TempoVoice. In particular, our OmniResponse leverages the capability of pretrained LLM to understand and interpret the speakers multimodal inputs and autoregressively generate meaningful responses in terms of text and facial reactions. To address the lack of temporal information in text, the proposed Chrono-Text Markup embeds explicit temporal marks between text tokens, endowing the input and output text with time-aware embeddings and ensuring precise alignment with the generated facial reactions. Furthermore, the proposed TempoVoice generates audio responses temporally synchronized with both the generated textual response and the listeners facial movements. 3.1 OmniResponse Model Architecture. As shown in Figure 2, OmniResponse processes multiple modalities from the speaker and the listener, temporally aligns different modalities, and outputs synchronous multimodal responses to the speaker. In particular, at each time step t, OmniResponse consumes: (1) Static text inputs: task-specific instruction prompt Winstruct and the conversation history prior to time τ (τ < t), denoted Whistory,<τ ; and (2) Temporal inputs: the previously generated facial features of the listener ˆFl τ :t1 and the accumfd udfsalated text sequences from both participants (Ws τ :t1) over the interval [τ, 1]. Using these inputs, OmniResponse predicts the next facial features ˆFl t, and the corresponding τ :t1, the facial features of the speaker Fs t, the verbal response ˆWl τ :t1, ˆWl 4 µ in the current frame, ensuring precise temporal alignment in all modalities. speech segment ˆAl Formally, we defined this process as: µ, ˆWl t, ˆAl { ˆFl t} = M(cid:0)Winstruct, Whistory,<τ , Fs τ :t1, ˆFl τ :t1, Ws τ :t1, ˆWl τ :t1 (cid:1). Vision Projection. We introduce the vision projection layer to enable the pretrained LLM (Phi-3.5 mini-instruct with 3.8B parameters [1]) to process visual facial features. The layer is implemented as multilayer perceptron (MLP) that maps the the listeners and speakers past facial features ˆFl 1:t1 and Fs 1:t1 into embedding features V1:t1 aligned with the LLM token space. During autoregressive generation, the MLLM employs causal self-attention to model temporal dependencies between the next token and previous one, and outputs the next listener vision embedding ˆVl t. Vision Decoder. learnable vision decoder, comprising transformer layers, converts ˆVl back into the original coefficient space to produce the predicted listener facial coefficients ˆFl t. Subsequently, pre-trained visual renderer maps these visual coefficients to 2D frames, using given portrait image. Please refer to the appendix for additional details. Chrono-Text Markup. Visual frames inherently encode temporal information, whereas text tokens are static and lack any temporal dimension. Additionally, visual frames and textual tokens typically differ in length due to their fundamentally different modalities, making unified autoregressive prediction challenging. To resolve this mismatch, we propose Chrono-Text Markup, novel yet straightforward approach that explicitly embeds temporal information into textual data, aligning the textual sequence precisely with the visual frame sequence. Unlike prior approaches such as TimeMarker [12], which inserts timestamps only between visual frames or the method by Ng et al. [44], which integrates timestamp embeddings into textual tokens, our method employs only two special markers, ensuring that the textual and visual sequences have identical lengths. Specifically, we insert two special tokens into the transcript: [PAUSE] to denote silent intervals between utterances, and [LASTING] to indicate that the previous textual word continues speaking to the current time. Each text token is placed between pause and lasting tokens. Multimodal Context Modeling. Our synchronous Multimodal LLM integrates both static and dynamic inputs: Static inputs: the instruction prompt and the accumulated conversation history. Dynamic inputs: frame-aligned visual embeddings and timestamped textual tokens for both speaker and listener. All tokens are jointly processed by an omni-attention mechanism that enforces causal, cross-modal interactions. Under this operation, each visual token attends to preceding visual tokens and to text tokens marked by chrono-text markers at earlier timestamps; similarly, each dynamic text token attends to past visual and textual tokens. However, this omni-attention prevents dynamic tokens from looking at future tokens. This ensures the generation adheres to temporal dynamics and cross-modal interactions. Meanwhile, static tokens remain globally accessible, ensuring that every dynamic update remains guided by the overarching instructions. TempoVoice. Generating natural speech that is precisely synchronized with text and facial frames poses significant challenge. To address this, we introduce dedicated synthesis pipeline, TempoVoice. Our framework begins by combining the listeners voiceprint, extracted via the Spark-tts global tokenizer [64] to capture speaker identity, with the hidden states of the generated text (see Figure 3). We then apply sinusoidal positional encodings to the merged embeddings. Since audio-token sequences typically differ in length from visual frames and textual tokens, we prepend series of zero-initialized placeholder tokens, each endowed with positional information. These placeholders serve as queries in cross-attention module within Transformer decoder, attending over the fused textvoice representations. This mechanism enables fully synchronous, autoregressive generation of audio tokens in lockstep with visual frames and text tokens. Finally, linear projection layer maps the decoder outputs to logits over the discrete audiocodec vocabulary. 5 Figure 3: Architecture of TempoVoice. TempoVoice transforms textual hiddenstate embeddings into audio segments. The decoder logits are then quantized into discrete audio semantic tokens ˆAµ, as defined by the Spark-tts audio tokenizer [64]. Conditioned on these semantics and the global speaker-identity embeddings, the tokenizer reconstructs the continuous waveform segment. 3.2 Training Objectives To train OmniResponse, the training objective is weighted combination of text generation loss Ltext, vision reconstruction Lvision, and audio generation loss Laudio: = Ltext + λvisionLvision + λaudioLaudio, (1) where λvision and λaudio are the scaling factors balancing text, vision, and audio loss terms. Text Generation Loss. The text loss encourages accurate next-token prediction conditioned on both speaker context and past listener states: Ltext = (cid:88) log pθ (cid:0)Wl (cid:12) (cid:12) Winstruct, Whistory,<τ , Fs τ :t1, ˆFl τ :t1, Ws τ :t1, ˆWl τ :t1 (cid:1). (2) Vision Reconstruction Loss. To align predicted and ground-truth facial dynamics, we apply an ℓ2 reconstruction loss on the listeners feature embeddings: Lvision = (cid:88) (cid:13) (cid:13) ˆFl Fl (cid:13) 2 2. (cid:13) (3) Audio Generation Loss. The audio loss operates over discrete semantic tokens Al µ, indexed by µ, which correspond to frame indices = µk (k reconciles the higher audio sampling rate with the frame-based rate of visual/text tokens). We maximize the likelihood of each token conditioned on previous audio semantics and the listeners hidden states: Laudio = (cid:88) log pθ (cid:0)Al µ (cid:12) (cid:12) Al <µ, Htk+1:t (cid:1), µ (4) where Htk+1:t denotes the models hidden representations for the corresponding listener text tokens ˆWl tk+1:t. This formulation ensures coherent alignment across modalities throughout generation."
        },
        {
            "title": "4 Dataset Construction",
            "content": "Existing publicly available dyadic video datasets do not satisfy the requirements of the OMCRG task (Figure 1). For example, mono-view talking-head datasets and offline dialogue corpora (e.g., MultiDialog [47]) do not offer split-screen recordings that capture speaker and listener simultaneously. Others, such as IEMOCAP [9], feature predominantly side profile views recorded in noisy environments and provide only mixed audio channels, thus preventing separate analysis of each participants speech. Furthermore, datasets such as ViCo [74], ICD [43], and REACT2024 [54] lack comprehensive textual annotations, suffer from low video resolution [74, 9, 54], or exhibit inconsistent spoken languages [54]. To fill the dataset gap, we introduce ResponseNet that comprises 696 temporally synchronized dyadic video pairs, totaling over 14 hours of natural conversational exchanges. Each pair provides high-resolution (1024 1024) frontal-face streams for both speaker and listener, along with separated audio channels to support fine-grained analysis of verbal and nonverbal behavior. Figure 1 shows ResponseNet is the only dataset that satisfies the key requirements: (1) online video streaming, (2) separate audio channels, and (3) textual word-level annotations for both participants. The construction of ResponseNet follows rigorous workflow that integrates automated tools with extensive human-in-the-loop curation. (1) Initially, split-screen videos featuring simultaneous appearances of speaker and listener are sourced from YouTube according to predefined topic and quality criteria. These clips are then filtered to remove low-resolution, noisy, or frequently camera transitions. (2) Human annotators perform thorough review to correct camera-view mis-alignments and ensure precise temporal synchronization between streams. (3) Next, mixed-channel audio tracks are automatically separated into discrete speaker and listener channels using speaker separation tools such as MossFormer2 [72] and subsequently verified and refined by experts. Finally, word-level transcripts are generated via automatic speech recognition [50] and meticulously proofread to guarantee accuracy. 6 Table 1: Comparison of conversation datasets. (cid:160) and (cid:160) denote speaker and listener data respectively. ResponseNet provides complete multimodal data (speaker+listener) with their separated audios. Dataset Video Audio Text Online Separated Audios # Dialogues Total Duration MultiDialog [47] (cid:160)+(cid:160) (cid:160)+(cid:160) (cid:160)+(cid:160) ICD [43] ViCo [75] REACT2024 [55] (cid:160)+(cid:160) (cid:160)+(cid:160) IEMOCAP [9] ResponseNet (cid:160)+(cid:160) (cid:160)+(cid:160) (cid:160)+(cid:160) (cid:160)+(cid:160) (cid:160)+(cid:160) (cid:160)+(cid:160) (cid:160)+(cid:160) (cid:160)+(cid:160) (cid:160)+(cid:160) (cid:160) 8,733 182,132 483 5,919 151 339.7h 72h 1.6h 71.8h 11.5h 14.2h Figure 4: Statistics of ResponseNet. (a) Distribution of video clip durations. (b) Distribution of dyadic conversation topics. (c) Word cloud of spoken words in dyadic conversations. By combining automation with meticulous manual oversight across data sourcing, preprocessing, alignment, audio separation, and annotation, this pipeline yields high-quality, richly annotated dyadic video corpus ideally suited for multimodal conversational response generation. The statistics of ResponseNet are shown in Figure 4. The durations of speaker-listener video clips range from 27.13 seconds (short conversations) to 863.13 seconds (long conversations) in ResponseNet. Figure 4.(a) shows that the average clip duration in ResponseNet is 73.39 seconds, significantly longer than that of other dyadic datasets such as REACT2024 (30 seconds), and Vico (9 seconds). This extended duration ensures that each clip captures sufficient conversational exchanges. Figure 4.(b) illustrates that the conversations span diverse range of topics, including professional discussions (e.g., economic interviews, news commentaries), emotionally driven interactions (e.g., intimate conversations), educational settings (e.g., teaching interviews), and interdisciplinary expert discussions. Figure 4.(c) presents word cloud highlighting the most frequent words in the conversations. Such diversity shows that ResponseNet captures rich and varied human-human interactions rather than being restricted to narrow or monotonic conversation patterns. Words related to personal relationships (e.g., \"love,\" \"family,\" \"friends\") and broader real-world topics (e.g., \"world,\" \"market,\" \"history,\" \"school\") are prominent."
        },
        {
            "title": "5 Experiments",
            "content": "Implementation Details. Our framework was implemented using PyTorch [48] and trained on four NVIDIA Tesla A100 GPUs. The model optimization was performed using the AdamW optimizer [26] with learning rate of 2 105, β1 = 0.9, β2 = 0.999, and weight decay of 104, accompanied by cosine learning rate scheduler. Training was executed with batch size of one for 2,000 epochs. Additionally, we fine-tuned the LLM using the LoRA [20] technique. More implementation details are provided in the supplementary material. Evaluation Metrics. Quantitatively evaluating the quality of multimodal response generation remains non-trivial. We thereby employ comprehensive metrics to evaluate generation results across text, audio, and visual modalities. For text response, we use METEOR [7], BERTScoreF 1 [70], and ROUGE-L [32] to measure how appropriate and natural the generated responses are, based on reference responses from the ResponseNet test set. We also adopt Distinct-2 [30] to evaluate diversity through the ratio of unique bi-grams. For audio response, we adopt UTMOSv2 [5], neural MOS predictor that estimates the perceptual naturalness, and employ LSE-D (LipSpeech Error 7 Table 2: Quantitative Results on ResponseNet test set. Model METEOR BERTScoreF 1 ROUGE-L Distinct-2 LSE-D UTMOSv2 FD FVD Text Audio Video 0.835 8.96 1.56 Ground-Truth Offline Text Dialogue Generation System GPT-4o [2] GPT-4 [2] GPT-o1 [2] 0.167 0.163 0.189 0.805 0.822 0.822 Online Auditory Dialogue Generation System Moshi [14] 0.120 0. 0.079 0.082 0.113 0.928 0.960 0.948 0.078 0.499 Facial Reaction Generation System ReactFace [37] ViCo [75] Online Multimodal Conversational Response Generation Baseline LSTM [19] Audio-visual LLM OmniResponse (Ours) 0.042 0.030 0.141 0.716 0.662 0.806 0.000 0.020 0. 0.000 0.155 0.882 9.72 10.03 9. 2.21 1.21 1.32 1.41 32.72 57.13 340.28 325. 6.51 580.86 15.46 320.92 681.55 314.94 Distance) [49, 13] to evaluate synchronization between generated speech and lip movements. For facial response, we compute Fréchet Distance (FD) [4] between real and generated facial-feature distributions, and Fréchet Video Distance (FVD) [61] to assess the spatialtemporal visual quality of generated video sequences. 5.1 Quantitative Results To the best of our knowledge, few works have explored the OMCRG task before. We build two baselines and compare them in Table 2: (1) LSTM-based method employing recurrent neural network [19] for temporal sequence modeling; (2) Audio-visual LLM taking speakerlistener audio and visual inputs and leveraging pre-trained LLM to generate audiovisual frames autoregressively. Table 2 additionally lists the generation performance of representative single-modality generation methods, including offline text-only dialogue models (e.g., GPT variants), online audio-only generation models (e.g., Moshi), and facial reaction generation approaches. Different from these methods focusing on generating single modality, our method enables online, synchronized generation across audio, visual, and textual modalities for modeling human conversation. Table 2 shows that our OmniResponse achieves the best performance in dialogue speech content (METEOR, BERTScoreF 1, ROUGE-L, Distinct-2), audio quality (UTMOSv2), audiovisual synchronization (LSE-D), as well as temporal consistency and visual quality (FVD). Although the LSTM baseline achieves lower FD owing to its tendency to produce repetitive static visual output, it fails to generate rich, synchronized multimodal responses. Audio-Visual LLM achieves much lower speech content quality (METEOR and BertScoreF 1) and struggles with audiovisual synchronization (LSE-D). Although Audio-Visual LLM leverages powerful LLM, it is still challenging to directly synchronize generated audio with facial reactions, especially in the absence of strong audio foundation model. Instead, we introduce novel framework that effectively adapts pre-trained LLMs for audiovisual generation with the proposed Chrono-Text Markup and Tempo Voice. 5.2 Qualitative Results Figure 5 presents qualitative result. The synthesized listener remains silent while the speaker is speaking, but then produces an immediate or delayed response at the end of each speaker turn. This behavior demonstrates that OmniResponse effectively captures the temporal dynamics of online dyadic conversation and generates responses at appropriate timestamps. For example, between 100.97 and 132.05 s, the listener interjects briefly between 120.13 and 121.57 in response to the speakers ongoing content, reflecting natural human conversational interaction. In contrast, conventional pipeline that integrates ASR, dialogue generation, TTS, and talking-head components waits for predefined silence threshold before producing an offline multimodal response, thus diminishing conversational behaviors such as interruptions, backchannels, questions, and immediate feedback. In contrast, OmniResponse maintains the continuous flow of dyadic conversation by continuously modeling and generating synchronized time series streams of textual, visual, and audio outputs. 8 Figure 5: Qualitative Results. Given the speakers audio and video streams and corresponding utterances (left), OmniResponse autoregressively generates synchronized visual, audio, and textual response streams (right). For clarity, [LASTING] tokens are removed from the generated dialogue. 5.3 Ablation Studies Effectiveness of Chrono-Text Markup. We construct baselines removing the proposed Chrono-Text Markup from our OmniResponse. In the baselines, each predicted word is assigned timestamp indicating when it emerges; if this timestamp falls within temporal window around the current time, the word is retained and appended to the spoken output; otherwise, it is discarded. As shown in the last rows of Table 3, incorporating Chrono-Text Markup significantly improves audio-visual synchronization, reducing the LSE-D score from 11.51 to 9.56. In addition, it enhances the semantic alignment of speech with conversational context, increasing METEOR from 0.122 to 0.141 and BERTScoreF 1 from 0.766 to 0.806. Improvements in FD and UTMOSv2 further indicate that ChronoText Markup boosts the quality of the generated audio and facial responses. These results demonstrate the effectiveness of Chrono-Text Markup in generating high-quality multimodal responses. Table 3: Ablation study on the effects of the proposed ChronoText Markup and TempoVoice. Tempo Voice Effectiveness of TempoVoice. To study the effect of our TempoVoice, we remove it from our framework and instead directly feed the hidden states, which are trimmed or padded to match the target audio length, into multilayer perceptron to predict audio token logits. As shown in Table 3, removing TempoVoice degrades audiovisual synchronization and reduces the quality of generated audio responses, where UTMOSv2 drops from 1.41 to 1.23, and LSE-D increases from 9.56 to 11.91. These results highlight the importance of TempoVoice in temporally aligning audio with the other modalities and enhancing the quality of the generated audio. Chrono-Text Markup METEOR BERTScoreF 1 596.27 19.58 23.42 15.46 13.64 11.91 11.51 9. 0.755 0.778 0.766 0.806 0.090 0.128 0.122 0.141 LSE-D UTMOSv2 1.21 1.23 1.39 1.41 FD"
        },
        {
            "title": "6 Conclusion",
            "content": "We have presented OmniResponse, an online multimodal generation model that produces online verbal and non-verbal listener responses to the multimodal behaviors of speaker. OmniResponse integrates techniques for processing multimodal inputs, synchronizing across modalities, and aligning responses with the speakers content. To enable evaluation of this task, Online Multimodal Conversational Response Generation in Dyadic Interactions, we introduce ResponseNet, dataset containing parallel recordings of speaker and listener streams. Our model and dataset lay the foundation for future research in this emerging field. Experimental results demonstrate that OmniResponse significantly increases speech semantic content, audio-visual synchronization, audio, and visual quality. Acknowledgments. This work is supported by the KAUST Center of Excellence for Generative AI under award number 5940. The computational resources are provided by IBEX, which is managed by the KAUST Supercomputing Core Laboratory."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao, Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu, Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song, Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang, Yunan Zhang, and Xiren Zhou. Phi-3 technical report: highly capable language model locally on your phone. https://arxiv.org/abs/2404.14219, 2024. [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. [4] Helmut Alt and Michael Godau. Computing the fréchet distance between two polygonal curves. International Journal of Computational Geometry & Applications, 5(01n02):7591, 1995. [5] Kaito Baba, Wataru Nakata, Yuki Saito, and Hiroshi Saruwatari. The t05 system for the voicemos challenge 2024: Transfer learning from deep image classifier to naturalness mos prediction of high-quality synthetic speech. In IEEE Spoken Language Technology Workshop, pages 818824, 2024. [6] Mikhail Bakhtin. The problem of speech genres. In Modern genre theory, pages 8297. Routledge, 2014. 10 [7] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. [8] Tom Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. [9] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette Chang, Sungbok Lee, and Shrikanth Narayanan. Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation, 42:335359, 2008. [10] Angelo Cafaro, Johannes Wagner, Tobias Baur, Soumia Dermouche, Mercedes Torres Torres, Catherine Pelachaud, Elisabeth André, and Michel Valstar. The noxi database: multimodal In Proceedings of ACM International recordings of mediated novice-expert interactions. Conference on Multimodal Interaction, pages 350359, 2017. [11] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. [12] Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Timemarker: versatile video-llm for long and short video understanding with superior temporal localization ability. arXiv preprint arXiv:2411.18211, 2024. [13] Joon Son Chung and Andrew Zisserman. Out of time: automated lip sync in the wild. In Computer VisionACCV 2016 Workshops: ACCV 2016 International Workshops, pages 251263. Springer, 2017. [14] Alexandre Défossez, Laurent Mazaré, Manu Orsini, Amélie Royer, Patrick Pérez, Hervé Jégou, Edouard Grave, and Neil Zeghidour. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. [15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [17] Dirk KJ Heylen. Understanding speaker-listener interaction. In Annual Conference of the International Speech Communication Association, pages 21512154, 2009. [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [19] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997. [20] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. [21] Yuchi Huang and Saad Khan. Dyadgan: Generating facial expressions in dyadic interactions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1118, 2017. [22] Yuchi Huang and Saad Khan. Generating photorealistic facial expressions in dyadic interactions. In British Machine Vision Conference, page 201, 2018. [23] Do Yuon Kim, Ha Kyung Lee, and Kyunghwa Chung. Avatar-mediated experience in the metaverse: The impact of avatar realism on user-avatar relationship. Journal of Retailing and Consumer Services, 73:103382, 2023. 11 [24] Everlyne Kimani, Timothy Bickmore, Ha Trinh, and Paola Pedrelli. Youll be great: virtual agent-based cognitive restructuring to reduce public speaking anxiety. In International Conference on Affective Computing and Intelligent Interaction, pages 641647, 2019. [25] Diederik Kingma. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [26] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [27] Raynard Kington, Stacey Arnesen, Wen-Ying Sylvia Chou, Susan Curry, David Lazer, and Antonia Villarruel. Identifying credible sources of health information in social media: principles and attributes. NAM perspectives, 2021:1031478, 2021. [28] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild. https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/, May 2024. [29] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. [30] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055, 2015. [31] Xiang Li, Hao Chen, Kai Qiu, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024. [32] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, 2004. [33] Christine Lisetti, Reza Amini, Ugan Yasavur, and Naphtali Rishe. can help you change! an empathic virtual agent delivers behavior change health interventions. ACM Transactions on Management Information Systems, 4(4):128, 2013. [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36, 2024. [35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [36] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun Lee, et al. Mediapipe: framework for building perception pipelines. arXiv preprint arXiv:1906.08172, 2019. [37] Cheng Luo, Siyang Song, Weicheng Xie, Micol Spitale, Zongyuan Ge, Linlin Shen, and Hatice Gunes. Reactface: Online multiple appropriate facial reaction generation in dyadic interactions. IEEE Transactions on Visualization and Computer Graphics, pages 118, 2024. [38] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple. arXiv preprint arXiv:2309.15505, 2023. [39] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. [40] Kentaro Mitsui, Koh Mitsuda, Toshiaki Wakatsuki, Yukiya Hono, and Kei Sawada. Pslm: Parallel generation of text and speech with llms for low-latency spoken dialogue systems. arXiv preprint arXiv:2406.12428, 2024. [41] David Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 4m: Massively multimodal masked modeling. Advances in Neural Information Processing Systems, 36, 2024. 12 [42] Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, and Michelle Tadmor Ramanovich. Spoken question answering and speech continuation using spectrogram-powered llm. arXiv preprint arXiv:2305.15255, 2023. [43] Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, and Shiry Ginosar. Learning to listen: Modeling non-deterministic dyadic facial motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2039520405, 2022. [44] Evonne Ng, Sanjay Subramanian, Dan Klein, Angjoo Kanazawa, Trevor Darrell, and Shiry Ginosar. Can language models learn to listen? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1008310093, 2023. [45] Tu Anh Nguyen, Benjamin Muller, Bokai Yu, Marta Costa-Jussa, Maha Elbayad, Sravya Popuri, Paul-Ambroise Duquenne, Robin Algayres, Ruslan Mavlyutov, Itai Gat, et al. Spirit-lm: Interleaved spoken and written language model. arXiv preprint arXiv:2402.05755, 2024. [46] Cristina Palmero, Javier Selva, Sorina Smeureanu, Julio Junior, CS Jacques, Albert Clapés, Alexa Moseguí, Zejian Zhang, David Gallardo, Georgina Guilera, et al. Context-aware personality inference in dyadic scenarios: Introducing the udiva dataset. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 112, 2021. [47] Se Jin Park, Chae Won Kim, Hyeongseop Rha, Minsu Kim, Joanna Hong, Jeong Hun Yeo, and Yong Man Ro. Lets go real talk: Spoken dialogue model for face-to-face conversation. arXiv preprint arXiv:2406.07867, 2024. [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32, 2019. [49] KR Prajwal, Rudrabha Mukhopadhyay, Vinay Namboodiri, and CV Jawahar. lip sync expert is all you need for speech to lip generation in the wild. In Proceedings of the ACM International Conference on Multimedia, pages 484492, 2020. [50] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya In International Sutskever. Robust speech recognition via large-scale weak supervision. Conference on Machine Learning, pages 2849228518, 2023. [51] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [52] Fabien Ringeval, Andreas Sonderegger, Juergen Sauer, and Denis Lalanne. Introducing the recola multimodal corpus of remote collaborative and affective interactions. In IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, pages 18, 2013. [53] Paul Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al. Audiopalm: large language model that can speak and listen. arXiv preprint arXiv:2306.12925, 2023. [54] Siyang Song, Micol Spitale, Cheng Luo, German Barquero, Cristina Palmero, Sergio Escalera, Michel Valstar, Tobias Baur, Fabien Ringeval, Elisabeth Andre, et al. React2023: The first multiple appropriate facial reaction generation challenge. In Proceedings of the ACM International Conference on Multimedia, pages 96209624, 2023. [55] Siyang Song, Micol Spitale, Cheng Luo, Cristina Palmero, German Barquero, Hengde Zhu, Sergio Escalera, Michel Valstar, Tobias Baur, Fabien Ringeval, Elisabeth André, and Hatice Gunes. React 2024: the second multiple appropriate facial reaction generation challenge. In IEEE International Conference on Automatic Face and Gesture Recognition, 2024. [56] Siyang Song, Micol Spitale, Yiming Luo, Batuhan Bal, and Hatice Gunes. Multiple appropriate facial reaction generation in dyadic interaction settings: What, why and how? arXiv preprint arXiv:2302.06514, 2023. [57] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020. [58] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [59] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive generating expressive portrait videos with audio2video diffusion model under weak conditions. In European Conference on Computer Vision, pages 244260. Springer, 2024. [60] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [61] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [62] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in Neural Information Processing Systems, 30, 2017. [63] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. [64] Xinsheng Wang, Mingqi Jiang, Ziyang Ma, Ziyu Zhang, Songxiang Liu, Linqin Li, Zheng Liang, Qixi Zheng, Rui Wang, Xiaoqin Feng, et al. Spark-tts: An efficient llm-based text-to-speech model with single-stream decoupled speech tokens. arXiv preprint arXiv:2503.01710, 2025. [65] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [66] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [67] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [68] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [69] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. arXiv preprint arXiv:2305.11000, 2023. [70] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675, 2019. [71] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. Meta-transformer: unified framework for multimodal learning. arXiv preprint arXiv:2307.10802, 2023. [72] Zhao, Ma, Ni, Zhang, Wang, TH Nguyen, Zhou, Yip, Ng, and Ma. Mossformer2: Combining transformer and rnn-free recurrent network for enhanced timedomain monaural speech separation. arxiv 2024. arXiv preprint arXiv:2312.11825, 2025. 14 [73] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [74] Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang Wang. Talking face generation by adversarially disentangled audio-visual representation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 92999306, 2019. [75] Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun Zhao, and Tao Mei. Responsive listening head generation: benchmark dataset and baseline. In European Conference on Computer Vision, pages 124142, 2022. [76] Hengde Zhu, Xiangyu Kong, Weicheng Xie, Xin Huang, Linlin Shen, Lu Liu, Hatice Gunes, and Siyang Song. Perfrdiff: Personalised weight editing for multiple appropriate facial reaction generation. In Proceedings of the ACM International Conference on Multimedia, 2024."
        },
        {
            "title": "A Implementation Details and Hyperparameters",
            "content": "Table 4: Implementation details and hyperparameters. Setup Batch Size Training Epoch for the Unified Stage Training Epoch for A/V finetuning Warmup Epoch Large Language Model Text Tokenizer Audio Tokenizer Facial Coefficients Lora Rank Lora Alpha Optimizer Learning Rate Model Parameters Nparam β1 β2 λvision λaudio 1 1500 500 100 Phi-3.5 Mini-Instruct [1] (3.8B) Phi-3.5 Mini-Instruct [1] Tokenizer Spark-tts [64] BiCodec MediaPipe [36] facial blendshapes + transformation matrix 64 16 AdamW 2.0 105 4.5B 0.9 0.999 1.0 100 Table 4 summarizes the key hyperparameters used in our experiments. For the core architecture, we employ the Phi-3.5 Mini-Instruct large language model [1] for multimodal fusion and dialogue reasoning. Input modalities are processed as follows: the audio waveform is tokenized into discrete representations using the BiCodec of Spark-tts [64], while text is tokenized using the Phi-3.5 Mini-Instruct tokenizer, augmented with special tokens such as [PAUSE] and [LASTING]. Visual features are extracted using the widely adopted MediaPipe toolkit [36], yielding 52-dimensional facial blendshape coefficients to capture local facial movements and 12-dimensional transformation matrix representing head pose dynamics. Model optimization is performed using the AdamW optimizer [26], with an initial learning rate of 2.0 105, β1 = 0.9, β2 = 0.999, and weight decay of 1.0 104. The batch size is set to 1, and cosine learning rate scheduler is applied throughout training. The model is first trained end-to-end, including all components (i.e., , LLM, vision projection, decoder, and TempoVoice) for 1,500 epochs, with 100-epoch warmup phase. To enable efficient adaptation of the large language model, we employ the LoRA fine-tuning strategy [20] (rank 64, alpha 16), while all other parameters of OmniResponse are jointly optimized. Subsequently, dedicated fine-tuning stage is performed for the audio and visual components (i.e., , vision projection, decoder, and TempoVoice) over an additional 500 epochs."
        },
        {
            "title": "B Methodological Details",
            "content": "In this section, we provide comprehensive overview of OmniResponse, highlighting its architectural design and the key technical innovations, namely, Chrono-Text Markup and TempoVoice. B.1 Network Architecture OmniResponse is composed of several interconnected modules: vision projection layer for encoding the visual frames of both the speaker and listener, large language model [1] for fusing visual features, textual instructions, conversational history, and dynamic texts, and Chrono-Text Markup module for temporal alignment of text tokens. The model jointly predicts the next visual token, text token, and audio response token. vision decoder layer reconstructs the listeners visual frame from the predicted visual token, while the TempoVoice module converts textual embeddings into audio waveforms. Vision Projection Layer. The Vision Projection Layer, denoted as Mvis-proj(), encodes the previously predicted visual frames of the listener ˆFl τ :t1, and projects them into sequence of embedding features Vτ :t1 over the temporal interval [τ, 1]. τ :t1 together with the speakers visual frames Fs Figure 6: Illustration of Prompt Construction. The final prompt (messgae) is composed of system prompt, the conversation history from the speaker (user) and listener (assistant), and dynamic speaker/listener text processed by our Chrono-Text Markup module. Here, τ is the starting index of the considered time window, which limits the number of temporal visual tokens and reduces computational overhead. The process is formulated as follows: Vτ :t1 = Mvis-proj (cid:0) ˆFl τ :t1, Fs τ :t1 (cid:1) (5) The projection module Mvis-proj can be instantiated either as multilayer perceptron that processes the concatenated visual features of the speaker and listener: (cid:2) ˆFl (cid:3) (where [] denotes concatenation), or as transformer-based layer, where the listeners visual features serve as queries, and the speakers visual features act as keys and values within cross-attention mechanism. τ :t1, Fs τ :t This architecture enables effective temporal fusion of visual information from both conversational participants, providing context for subsequent response generation. Vision Decoder. The vision decoder consists of two-layer Transformer Decoder that processes the predicted embeddings ˆVl τ +1:t generated by the large language model for the first τ positions, and maps them to the facial coefficient space ˆFl τ +1:t. Subsequently, pre-trained visual renderer converts these facial coefficients into 2D facial frames, conditioned on given portrait image. The renderer is trained on large-scale web video dataset and is utilized as tool to synthesize photorealistic images by mapping the predicted facial expression and head pose coefficients to high-quality 2D visuals. Static Text. The large language model accepts both visual and textual inputs. The textual inputs include static text. Specifically, static text contains the instruction prompt Winstruct and the conversation history Whistory,<τ . The construction process for the instruction prompt is illustrated in Figure 6. The final prompt comprises the static system message (serving as the assistants instruction) and the conversation history between the speaker (user) and the listener (assistant) up to time τ . This static text is provided to the LLM following the visual coefficients. B.2 Chrono-Text Markup In addition to the static instruction and conversation history, we also supply the model with dynamic text annotated with precise timing information. Figure 6 illustrates how we interleave static and dynamic text when constructing each prompt: the static text preserves long-term context, while the dynamic text encodes exactly when each word occurs and how long silences last. To achieve this, Chrono-Text Markup introduces two special tokens, [PAUSE] and [LASTING], into the token stream according to the timestamps in our dataset. At each frame timestamp: If neither 17 Figure 7: Example of Dynamic Text. the speaker nor listener is uttering word, we insert [PAUSE] token; When speech is present, we emit the actual word tokens (e.g., do, think) and then append one or more [LASTING] tokens to occupy the remainder of that words duration in the timeline. Here is an example shown in Figure 7. The large language model also generates dynamic text predictions. By encoding precise timing information into these text embeddings, the subsequent audio synthesis produces segments that are more tightly synchronized with the spoken content. 18 Figure 8: Illustration of Multimodal Context Modeling. Each visual token attends to all preceding visual tokens and static and dynamic text tokens annotated by Chrono-Text markers at earlier timestamps. Similarly, each dynamic text token attends to all past visual and textual tokens, enabling rich cross-modal context integration. B.3 Multimodal Context Modeling We partition the input sequence into two streams, static and dynamic, and fuse them with single causal omni-attention layer (Figure 8). Static stream. The instruction prompt and the complete conversation history are encoded as global tokens. These tokens are never masked and therefore remain visible to every other token throughout the sequence. Dynamic stream. Visual. Frame-aligned visual embeddings. Text. Timestamped speakerand listener-side tokens annotated with Chrono-Markup. Causal omni-attention. All tokens enter shared attention block that enforces strict chronology both within and across modalities: Visual tokens attend to earlier visual tokens, to text tokens whose timestamps precede the current frame, and to all static tokens. Dynamic text tokens attend to past visual and text tokens, plus all static tokens. Future dynamic tokens are masked to preserve temporal integrity. The static tokens remain unmasked, providing global semantic guidance at each step. This design yields temporally coherent cross-modal interactions while maintaining consistent global context, key qualities for online conversational generation. 19 B.4 TempoVoice TempoVoice is designed to transform generated textual tokens into temporally synchronized audio waveforms. Given the hidden representations corresponding to the listeners text tokens, Hτ :t, TempoVoice generates audio tokens τ :µ, which are then converted into continuous audio waveforms using an audio tokenizer. The process is defined as follows: τ :µ, [Avoiceprint, Hτ :t](cid:1) :µ = TempoVoice(cid:0)P τ :µ denotes the positional encodings for positions (cid:4) τ (cid:5) to µ, Avoiceprint represents the where τ voiceprint embeddings, and Hτ :t are the generated textual embeddings over the interval [τ, t]. Here, [] indicates concatenation along the temporal axis. The resulting audio tokens τ BiCodec module from Spark-TTS. :µ are subsequently transformed into audio waveforms using the (6) k"
        },
        {
            "title": "C ResponseNet Dataset",
            "content": "Figure 9: Illustration of Dataset Construction Pipeline. C.1 Construction Pipeline To build the ResponseNet dataset, we design three-stage pipeline encompassing data collection, data processing, and data refinement, as illustrated in Figure 9. This structured process ensures highquality, temporally aligned multimodal data suitable for online conversational response generation tasks. Step 1: Data Collection. We begin by sourcing dyadic conversational videos from diverse public domains, including interviews, podcasts, and online discussions. Candidate videos are selected through combination of automatic filtering tools and human curation to ensure conversational structure and speaker clarity. These videos include one speaker and one listener. The data is manually labeled for speaker turns, and high-resolution videos are retained for downstream visual analysis. Step 2: Data Processing. This step extracts synchronized multimodal data from the raw videos. First, Video Segmentation and Filtering isolates segments with clear speaker-listener interaction using face detection and quality heuristics. We apply Camera-view Alignment to standardize the perspective, especially in multi-camera recordings. Next, we conduct Face Cropping and Separation to isolate individual speaker and listener views. In parallel, the audio track is separated and segmented using speaker diarization and voice activity detection. However, automatic tools could lead to bad cases, we correct these separated audio tracks manually. We then apply an ASR system (whisper [50]) for Transcription to obtain timestamped word-level alignments. Subsequently, we extract facial behavior features using MediaPipe [36], yielding per-frame ARKit blendshape coefficients and 3D head pose transformation matrices for both speaker and listener tracks. Step 3: Data Refinement. To ensure privacy and label accuracy, we conduct multi-level cleaning. First, in the De-identification and Sanitization stage, we mask personally identifiable information (PII) and redact sensitive content from transcripts and audio. Then, Human Validation and Correction is performed to manually inspect and correct transcription errors, alignment mismatches, and feature inconsistencies. Finally, Quality Control and Filtering phase discards corrupted or ambiguous segments, yielding clean, high-quality dataset with tightly aligned audio, visual, and textual modalities. Overall, the pipeline enables reliable construction of multimodal dialogue samples with rich facial dynamics and accurate verbal content, supporting the development of real-time response generation models. C.2 Dataset Statistics The dataset is partitioned into training, validation, and test splits following the standard ratio of 6:2:2. Specifically, we ensure that the distributions of conversation topics, speaker identities, and recording conditions are balanced across each subset to avoid potential biases and to facilitate robust evaluation. The detailed statistics of the video stream pairs in each split are summarized in Table 5. Table 5: Data split of video stream pairs in our dataset. Split Number of Video Stream Pairs Proportion (%) Train Validation Test Total 417 139 140 59.9 20.0 20.1 100.0 Each data sample consists of synchronized pair of video streams representing dyadic conversational interaction. The train, validation, and test splits are disjoint with respect to participant pairs to ensure fair evaluation and to prevent data leakage. This stratified partitioning enables comprehensive benchmarking of model performance across diverse conversational scenarios. C.3 Privacy Considerations The YouTube platform enforces strict content moderation policies to prevent the dissemination of violent or harmful material [27]. In addition, according to YouTubes copyright guidelines2, the use of copyrighted material for research purposes typically qualifies as fair use, permitting reuse without the need for explicit permission from the copyright holder. Together, these factors ensure that our dataset collection and usage align with established privacy and ethical standards."
        },
        {
            "title": "D Evaluation Protocol",
            "content": "D.1 Evaluation Metrics Quantitative evaluation of multimodal response generation is inherently challenging due to the need to assess multiple aspects of quality across different modalities. To provide comprehensive assessment, we employ suite of metrics spanning text, audio, and visual outputs. 2https://www.youtube.com/howyoutubeworks/policies/copyright/#fair-use 21 Text Metrics. METEOR [7]: Measures the alignment between generated and reference responses by considering synonymy, stemming, and word order, providing nuanced evaluation of semantic adequacy. BERTScoreF 1 [70]: Calculates the F1 score of semantic similarity between generated and reference texts by comparing their contextual embeddings from pretrained RoBERTa model [35], providing robust measure of semantic fidelity. ROUGE-L [32]: Evaluates the longest common subsequence between generated and reference responses, reflecting fluency and content overlap. Distinct-2 [30]: Calculates the proportion of unique bi-grams in the generated responses, serving as an indicator of output diversity and lexical richness. Audio Metrics. UTMOSv2 [5]: two-stage neural MOS predictor for high-quality synthetic speech that fuses EfficientNetV2 spectrogram embeddings with SSL-based speech representations; the resulting model achieves state-of-the-art correlation with human ratings of naturalness and intelligibility. LSE-D (LipSpeech Error Distance) [49, 13]: Measures the temporal alignment and synchronization between generated audio and corresponding lip movements, reflecting audio-visual coherence. Visual Metrics. Fréchet Distance (FD) [4]: Computes the distributional distance between real and generated facial feature embeddings, assessing the realism of static visual features. Fréchet Video Distance (FVD) [61]: Quantifies the spatial-temporal quality of generated video sequences by comparing their feature distributions to those of real videos, thus evaluating overall video realism and consistency. By leveraging these complementary metrics, we are able to rigorously assess the naturalness, diversity, and synchronization of generated responses across modalities, enabling thorough benchmarking of model performance on the ResponseNet test set. D.2 Baseline Methods As this is the first work addressing OMCRG tasK, we compare OmniResponse with diverse set of prior methods that target single-modality generation, as well as several multimodal baselines. Specifically, we include the following baselines: Offline Text Dialogue Generation Systems: State-of-the-art large language models, including GPT-4o, GPT-4, and GPT-o1 [2], are evaluated for their ability to generate text responses in offline settings. These models only produce text outputs, without audio or visual generation. Online Auditory Dialogue Generation System: Moshi [14] is adopted as representative model for generating spoken responses in real time, focusing exclusively on audio outputs. Facial Reaction Generation Systems: ReactFace [37] and ViCo [75] serve as facial reaction generation baselines, producing only visual (facial) responses based on the conversational context. Online Multimodal Conversational Baselines: To provide direct comparison for OMCRG, we construct two multimodal baselines: 1. LSTM-based model [19] employing recurrent neural network for temporal sequence modeling across modalities. The LSTM takes visual-audio-text modalities of speaker as inputs, and outputs listeners visual and audio modalities. 22 2. An Audio-visual LLM baseline that takes both speaker and listener audiovisual inputs and autoregressively generates audio-visual responses of the listener via pre-trained large language model [1]. While previous approaches focus primarily on generating single modality, OmniResponse is designed to produce synchronized and coherent responses across text, audio, and visual channels in an online setting."
        },
        {
            "title": "E Risks and Potential Misuse",
            "content": "This system is developed for multi-modal conversational AI, but certain risks should be acknowledged. For instance, realistic synthetic contents could be misused for impersonation or misleading information. During real-time human-user interactions, users may also develop misunderstandings or excessive reliance on the system without proper contents control. To avoid these risks, we recommend clear labeling of the generated contents, appropriate usage monitoring, and the inclusion of protective measures against potential misuse."
        },
        {
            "title": "F Limitations",
            "content": "While our approach performed well on the evaluated datasets, the remaining challenges include the proposed approach (e.g., its results) may largely depend on the quality and diversity of training data, replying on accurate speakerlistener segmentation and can be negatively affected in noisy or overlapping conversations. Additionally, generating well-aligned multi-modal responses remains difficult in fast-changing or emotionally rich interactions, while our paper lacks fairness analysis. Future work will focus on improving these aspects."
        },
        {
            "title": "G Broader Impacts",
            "content": "Our work contributes to the development of more intuitive and responsive multi-modal dialogue systems, with potential applications in education, healthcare, assistive communication, and companion. These technologies may improve access to information, support inclusive interaction, and enhance user experience across diverse contexts and scenerios. We encourage responsible research practices that prioritize transparency, user safety, and alignment with social values to ensure that such systems serve the public good."
        },
        {
            "title": "H Responsibility and License",
            "content": "We acknowledge full responsibility in the event of any rights infringement. The dataset is distributed under the Creative Commons CC BY-NC-SA license, permitting use with attribution for non-commercial purposes and requiring derivative works to be shared alike."
        }
    ],
    "affiliations": [
        "King Abdullah University of Science and Technology",
        "University of Exeter"
    ]
}