{
    "paper_title": "Scaling RL to Long Videos",
    "authors": [
        "Yukang Chen",
        "Wei Huang",
        "Baifeng Shi",
        "Qinghao Hu",
        "Hanrong Ye",
        "Ligeng Zhu",
        "Zhijian Liu",
        "Pavlo Molchanov",
        "Jan Kautz",
        "Xiaojuan Qi",
        "Sifei Liu",
        "Hongxu Yin",
        "Yao Lu",
        "Song Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce a full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) a large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) a two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and a vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks a firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On a single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens)."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 6 6 9 7 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Scaling RL to Long Videos",
            "content": "Yukang Chen1* Wei Huang1,3* Baifeng Shi1,4 Qinghao Hu2 Hanrong Ye1 Ligeng Zhu1 Zhijian Liu1 Pavlo Molchanov1 Jan Kautz1 Xiaojuan Qi3 Sifei Liu1 Hongxu Yin1 Yao Lu1 Song Han1,2 2025-7-11 1NVIDIA 2MIT 3HKU 4UC Berkeley *Equal contribution Core contribution Github.com/NVlabs/Long-RL Abstract: We introduce full-stack framework that scales up reasoning in vision-language models (VLMs) to long videos, leveraging reinforcement learning. We address the unique challenges of long video reasoning by integrating three critical components: (1) large-scale dataset, LongVideo-Reason, comprising 52K long video QA pairs with high-quality reasoning annotations across diverse domains such as sports, games, and vlogs; (2) two-stage training pipeline that extends VLMs with chain-of-thought supervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) training infrastructure for long video RL, named Multi-modal Reinforcement Sequence Parallelism (MR-SP), which incorporates sequence parallelism and vLLM-based engine tailored for long video, using cached video embeddings for efficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves strong performance on long video QA benchmarks such as VideoMME. It also outperforms Video-R1-7B and even matches Gemini-1.5Pro across temporal reasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on our LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to 2.1 speedup on long video RL training. LongVILA-R1 demonstrates consistent performance gains as the number of input video frames scales. LongVILA-R1 marks firm step towards long video reasoning in VLMs. In addition, we release our training system for public availability that supports RL training on various modalities (video, text, and audio), various models (VILA and Qwen series), and even image and video generation models. On single A100 node (8 GPUs), it supports RL training on hour-long videos (e.g., 3,600 frames / around 256k tokens). Code and models are available at https://github.com/NVlabs/Long-RL. 1. Introduction Understanding long videos requires more than simple recognitionit demands reasoning from temporal, spatial, goal-oriented, and narrative perspectives [1]. As illustrated in Figure 2, answering high-level questions often hinges on models ability to integrate clues distributed across time, infer hidden goals or strategies, track entities spatially, and comprehend the evolving plot. For instance, predicting the winner of football penalty shootout involves assessing emotional cues and tactical behavior (temporal and goal reasoning), while determining the final location of hidden ball requires precise spatial tracking. Likewise, evaluating poker players decision demands interpreting implicit strategies beyond surface actions (goal reasoning) and understanding characters development or match trajectory reflects the need for plot reasoning. These examples underscore that reasoning is indispensable for long video understanding that goes beyond recognition alone. Despite the clear importance of reasoning in long video understanding, enabling such capabilities in long-video VLMs poses significant challenges [2, 3, 4, 5, 6]. First, the collection of high-quality long video reasoning datasets is inherently difficult. Unlike domains such as math or code reasoning, where structured supervision and benchmarks are readily available [7, 8], long video reasoning requires annotating complex temporal dynamics, goals, spatial re2025 NVIDIA. All rights reserved. Figure 1 Training efficiency comparison with MR-SP (SP degree=4) on Qwen2.5-VL-7B and LongVILA-R1-7B and single node 8 A100 GPUs. It achieves 2.1 speed-up and avoids GPU OOM issue on long frames. Scaling RL to Long Videos Figure 2 Examples of LongVILA-R1. The illustration demonstrates sample tasks and their reasoning. From left to right, the examples include predicting the results of football match, decision-making reasoning in Texas Holdem Poker, and trajectory for spatial dynamics of objects. Notably, the spatial tracking video involves relatively complex dynamic moving, for which the model fails to achieve accurate reasoning until the number of input video frames increases to 128. lations, and narrative elementsoften across minutes or hours of footage [9]. This process is labor-intensive and subjective, making large-scale dataset construction slow and costly. Second, the RL training framework for long videos is challenging. Reinforcement learning, common strategy for aligning models with complex reasoning objectives, is computationally expensive and sampleinefficient [10]. When applied to long videos, RL becomes even more burdensome due to the extended video frames, requiring more memory and longer rollout runtime. These challenges jointly hinder the development of effective long video VLMs with strong reasoning capabilities. In this work, we introduce LongVILA-R1, comprehensive framework exploring the reasoning capabilities for long-video understandings. Firstly, we strategically construct high-quality dataset with CoT annotations for long video reasoning, named LongVideo-Reason. Leveraging powerful VLM (NVILA-8B) [11] and leading opensource reasoning LLM, we develop dataset comprising 52K high-quality Question-Reasoning-Answer pairs for long videos. We use 18K high-quality samples for Long-CoT-SFT to initialize the models reasoning and instruction-following abilities, and 33K samples with an additional 110K video data [2] for reinforcement learning. This two-stage training combines high-quality reasoning annotations with reinforcement learning, enabling LongVILA-R1 to achieve superior and generalized video reasoning. We also manually curate balanced set of 1K long-video samples to build new benchmark, LongVideoReason-eval, that evaluates performance from four perspectives: Temporal, Goal and Purpose, Spatial, and Plot and Narrative, for comprehensive assessment. 2 Scaling RL to Long Videos Figure 3 Data Distribution of LongVideo-Reason and total data in the LongVILA-R1 training framework. LongVideoReason comprises total of 18,077 videos, from which 52K QAs with reasoning annotations. Additionally, we also include additional 110K QAs from existing works [12, 13, 14, 15, 16] for RL training. Secondly, we propose training framework for VLMs to advance long video reasoning. As illustrated in Figure 5, this framework incorporates two stages, i.e., Stage-1: Long CoT-SFT, and Stage-2: RL for long video reasoning. To address the unique challenges of long video RL, including massive visual embeddings, heavy rollouts, and long-context LLM prefilling, we develop an efficient and scalable solution, referred to as Multi-modal Reinforcement Sequence Parallelism (MR-SP). It incorporates vLLM engine [17] tailored for LongVILA and caching scheme for video embeddings. The MR-SP system alleviates the problem of intensive memory and facilitates RL training of long video VLMs. As shown in Figure 1, our MR-SP system achieves up to 2.1 speedup on 512frame video RL training on 7B models and enables longer training frames without out-of-memory (OOM). LongVILA-R1-7B shows strong performance on general video benchmark, e.g., 68.4% on VideoMME [18](with subtitle). On our LongVideo-Reason-eval benchmark, LongVILA-R1-7B achieves an average accuracy of 67.9% across four reasoning categories, surpassing open-source models including Video-R1-7B [2] and proprietary models such as GPT-4o [19] by large margin, and matches In addition, the performance of Gemini-1.5-Pro [20]. LongVILA-R1 exhibits progressive performance improvements as the number of input frames increased. 2. Related Work Multi-modal reasoning models. The field of multimodal reasoning has advanced significantly, particularly in Vision-Language Models (VLMs). GPT-4o [19] improves visual understanding through enhanced reasoning, while Gemini-1.5-Pro [20] extends context length to 1 million tokens, achieving state-of-the-art performance in VideoMME [18]. Following the substantial progress of architecture and training algorithms in open-source VLMs [11, 21, 22, 23, 24], multi-modal reasoning has been further explored in works including LMM-R1 [25] which employs two-stage training strategy, VisionR1 [26] that addresses post-cold-start overthinking, and Video-R1 [2] which enhances RL for video via T-GRPO in 16 frames. However, these approaches primarily focus on single images or short videos, and long video reasoning still poses great challenges. Sequence parallelism. Training with long contexts often exceeds the memory capacity of single device [27], necessitating efficient distribution strategies. Sequence parallelism (SP) has become widely adopted solution [28, 29, 30, 31, 32]. For example, ring-based systems like LightSeq [33] and Ring Attention [28] use point-to-point (P2P) communication, while DeepSpeed-Ulysses [29] employs all-to-all (A2A) primitives to optimize attention computations. Additionally, USP and LoongTrain [31, 32] were introduced to integrate Ring-style SP and Ulysses SP. LongVILA [4] further proposed multi-modal SP (MM-SP), enabling vision-language models to handle long-context inputs. However, multi-modal reinforcement learning introduces additional challenges, as it requires extensive sampling from long, mixed-token sequences [10], particularly in complex group optimization tasks [7]. RL frameworks for LLMs/VLMs. Reinforcement Learning (RL) has become key strategy for enhancing Large Language Models (LLMs), particularly through Reinforcement Learning with Human Feedback (RLHF) [34] or Direct Preference Optimization [35], which aligns model outputs with human preferences. Recent advancements demonstrate that RL significantly improves LLM reasoning abilities. For example, DeepSeek-R1 [36] utilizes the Group Relative Policy Optimization (GRPO) algorithm [37], integrating group-based sampling and rulebased rewards. On the other hand, RL [38] poses unique challenge of heavy computational cost, especially in multimodal settings. To address this problem, HybridFlow [10] is introduced, leveraging Ray [39] for efficient data flow and vLLM [17] for faster sampling. Nevertheless, this remains bottleneck when processing long video sequences, with group-based sampling constrained by the high computational cost of long-context sampling and visual encoding. In this work, we propose MR-SP that provides up to 2.1 speedup for long video RL training. Scaling RL to Long Videos Figure 4 Data generation process for the LongVideo-Reason dataset. This process begins with segmenting videos into 10-second clips and generating captions for each clip using NVILA-8B. Then based on the captions of all clips in video, we generate question-answer pairs that involve reasoning across the content of the whole video, along with the reasoning annotations using leading open-source reasoning LLM. Reasoning questions are categorized into Temporal, Goal and Purpose, Spatial, and Plot and Narrative. Finally, the reasoning annotations are reformatted for conciseness and alignment with video details. We present more detailed figure of data generation process in Figure 9 in the appendix. 3. LongVideo-Reason Data Construction High-quality annotated datasets for long videos are critical for VLMs. Existing publicly available long-video datasets lack high-quality reasoning annotations. In this section, we detail the data construction process (Figure 4) of 52K long-video QAs with reasons. 3.1. Overview of Data Curation We first curate 18K long videos from the Shot2Story dataset [40] (Figure 3, left). We then apply high-quality automated annotation pipeline for CoT as detailed in Section 3.2, and end up with total of 52K QuestionReasoning-Answer pairs where each sample, based on the type of question it is reasoning about, can be categorized into Temporal Reasoning, Goal and Purpose Reasoning, Spatial Reasoning, or Plot and Narrative Reasoning (Figure 3, middle). This dataset is designed to support various types of long-video reasoning tasks comprehensively. Given the sensitivity of GRPO to batch sampling [25, 41], data filtering approach is adopted. Specifically, we employ test-scaling method where LongVILA [4] performs inference 10 times on the original datasets. Questions consistently answered correctly or incorrectly are labeled as \"easy\" or \"hard\" while those inducing diverse predictions are labeled as \"medium\". We use both the \"easy\" and \"hard\" samples for Stage-1 COT-SFT and use the \"medium\" samples for Stage-2 RL training. The reason is that GRPO expects different rollouts of each sample to be diverse in order to have meaningful advantages, and the gradient vanishes if all the rollouts predict correct or incorrect answers. The COT-SFT subset (18K) features high-quality CoT reasoning processes formatted in standard <think></think><answer></answer> structure, providing abundant resources for warm-up training during Stage 1 of the models reasoning capabilities. Meanwhile, the RL subset contains 33K challenging longvideo Q&A, which is leveraged in Stage 2 for scaling reasoning through reinforcement learning. To further enhance RL scaling, we incorporate an additional 110K highquality open-source videos (Figure 3, right) from other datasets [12, 13, 14, 15, 16]. This combination improves the models generalization. 3.2. Long-Video Reasoning Generation We introduce an automated annotation pipeline (Figure 4) that generates high-quality Question-Reasoning-Answer pairs from long videos. This process begins by segmenting videos into short clips (10 seconds each), each of which is annotated using the NVILA-8B [11] model to provide descriptive captions. Leveraging the breakthrough in textbased reasoning, we then deploy leading open-source reasoning LLM, provide the captions of all the clips in each video, and prompt it to generate diverse types of Question-Reasoning-Answer pairs that involve reasoning over the content across the whole video. Specifically, we design four types of prompts to encourage the LLM to generate Question-Reasoning-Answer pair that focuses on one of the four types of reasoning: Temporal Reasoning, Goal and Purpose Reasoning, Spatial Reasoning, or Plot and Narrative Reasoning. To ensure VLMs focus on visual details, we also craft the prompts with phrases such as checking the video and analyzing the scene, which guide iterative examination of visual content. Finally, an LLM is then used to refine and streamline the reasoning steps. We also manually curated 1,000 high-quality complex reasoning questions across four reasoning categories to serve as new benchmark (LongVideo-Reason-eval) for evaluating VLMs in reasoning abilities. This entire data procedure consumes about 40,000 GPU hours on H100s. 4. LongVILA Training Pipeline As shown in Figure 5, there are two extended training stages in LongVILA-R1, i.e., (1) warm-up for long video reasoning, utilizing 18K data with high-quality CoT for SFT on the MM-SP system [4]; (2) reinforcement learning with dense frames from long videos. 4 Scaling RL to Long Videos Figure 5 The LongVILA-R1 training pipeline. LongVILA-R1 builds upon the base training pipeline for LongVILA. MM-SP is further employed for SFT on long video understanding tasks with long CoT. Then, reinforcement scaling learning is conducted through Multi-modal Reinforcement Sequential Parallelism (MR-SP). Figure 6 LongVILA-R1 RL training framework. The framework integrates multi-modal reinforcement sequence parallelism (MR-SP) for scalable video frame encoding and LLM prefilling. RL utilizes vLLM-based engine with cached video embeddings, tailored for LongVILA rollout. Rewards for accuracy and format guide policy optimization. 4.1. Long Video CoT Supervised Fine-Tuning Utilizing 52K high-quality question-reasoning-answer pairs, we apply the data filtering method described in Section 3.1 to select 18K examples for long CoT-SFT, serving as warm-up phase for subsequent RL. This stage equips the model with fundamental reasoning abilities and instruction-following skills for long video scenarios. To efficiently perform SFT on hundreds of frames, we adopt the MM-SP [4] training system from LongVILA. As demonstrated in Section 6.2, SFT solely on our LongVideoReason dataset also effectively improves the model with foundational reasoning capabilities. 4.2. GRPO for Long Video Building on the advancements of the GRPO [7] algorithm and prior explorations of multi-modal reasoning training [2, 25], we adhere to the standard GRPO framework to train our model. For each given question 𝑞, the policy model generates group of candidate responses {𝑜1, 𝑜2, ..., 𝑜𝐺} from the old policy 𝜋𝜃𝑜𝑙𝑑, accompanied by their corresponding rewards {𝑟1, 𝑟2, ..., 𝑟𝐺}, which are computed based on rule-based reward functions (format/accuracy). The model 𝜋𝜃 is subsequently optimized by maximizing the following objective function: 𝒥 (𝜃) = E𝑞,{𝑜𝑖}[ 1 𝐺 𝐺 (min( 𝑖=1 𝜋𝜃(𝑜𝑖𝑞) 𝜋𝜃𝑜𝑙𝑑 (𝑜𝑖𝑞) 𝐴𝑖, clip( 𝜋𝜃(𝑜𝑖𝑞) 𝜋𝜃𝑜𝑙𝑑 (𝑜𝑖𝑞) , 1 𝜖, 1 + 𝜖)𝐴𝑖) 𝛽D𝐾𝐿(𝜋𝜃𝜋𝑟𝑒𝑓 ))] (1) where 𝜖 and 𝛽 are hyper-parameters, 𝐺 is set as 8 in our experiments, and the sampled rewards above are normalized to get the advantages (𝐴𝑖) for updating the model: 𝐴𝑖 = 𝑟𝑖 mean({𝑟1, 𝑟2, ..., 𝑟𝐺}) std({𝑟1, 𝑟2, ..., 𝑟𝐺}) (2) However, RL for long videos presents significant challenges due to the high computational demands of processing hundreds to thousands of frames. Existing RL frameworks struggle with such long-context training in rollout and LLM prefilling. To address this, we develop the MR-SP framework (Section 5), which efficiently scales reinforcement learning for long-context video reasoning. Considering the sensitivity of GRPO to sampling during training [41], we use the 33K filtered data for reinforcement learning as described in Section 3.1. Additionally, an extra 110K samples from Video-R1 [2] are incorporated to scale up the RL. This approach aims to guide the model in freely exploring and developing more effective and generalized reasoning strategies. 5. Multi-modal Reinforcement SP Existing RL frameworks for VLMs, such as R1-V [38] and EasyR1 [42], are not designed for long videos which present unique challenges due to their high token volume. To address this, we introduce Multi-modal Reinforcement Sequence Parallelism (MR-SP), framework for efficient RL training on long videos. MR-SP leverages sequence parallelism in both rollout and pre-filling stages, enabling long videos in RL, with reduced overhead. 5 Scaling RL to Long Videos Figure 7 The workflow of multi-modal reinforcement sequential parallel (MR-SP). To accommodate multi-modal input in reinforcement learning, we develop custom sharding strategy to ensure balanced workload distribution and compatibility with SP communication. Efficient video embedding reuse and vLLM rollout acceleration strategies are implemented, while meeting the demands of policy model prefilling for dense video frames. 5.1. Stage 1 - Rollout with Paralleled Encoding To support long-video reinforcement learning efficiently, we adopt sequence parallelism (SP) for the video encoding stage. As shown in Figure 7, the input video frames are first evenly divided across multiple GPUs (e.g., GPU 1 to GPU 3), each equipped with its own vision tower. Each GPU independently processes slice of the video, encoding only subset of the frames. The resulting video embeddings are then aggregated with text embeddings via an all-gather operation as indicated by the \"All-Gather\" arrow in the figure. This strategy distributes the encoding workload, allowing the system to handle significantly longer videos by leveraging more GPUs, while avoiding the risk of GPU memory overflow. The parallel encoding scheme ensures balanced utilization of the vision towers and enables scalable long-video processing that would otherwise be infeasible on single device. After the video embeddings are globally gathered, they are reused for downstream usage throughout the RL pipeline. As illustrated in Figure 7, the gathered embeddings are reused during multiple rollouts without recomputation. For instance, in each training step, we typically perform 8 to 16 rollouts. Without recycling, the same video would need to be re-encoded dozens of times per step, severely impacting training speed. By caching and reusing the gathered embeddings, MR-SP eliminates this redundancy and significantly accelerates training. 5.2. Stage 2 - Prefilling with Sequence Parallelism For each rollout, both the reference and policy models require prefilling compute-intensively in RL for long videos. With the gathered embedding from Stage 1 reused, we parallelize the inference stage across devices using sequence parallelism. As illustrated in Figure 7, we globally gathered input embeddings are first padded to uniform length (Padding Sequence) and then evenly partitioned across GPUs (Sharding to Local GPU). This allows each GPU to handle only portion of the input sequence during prefilling. This parallelism is applied to both policy and reference model prefilling. Then, each GPU locally computes logits for its token slice, prefilling in parallel. 6. Experimental Results 6.1. Main Results Table 1 shows the performance comparison on 9 video benchmarks [13, 14, 18, 43, 44, 45, 46, 47, 48]. LongVILA-R1-7B consistently outperforms LongVILA7B across all benchmarks, with performance gaps that vary according to the complexity of the reasoning tasks. Table 3 presents the general performance of LongVILAR1, comparing to existing advanced models [1, 2, 50, 52, 53, 54, 57, 58, 59, 60] under comparable model sizes on the Video-MME [18] benchmark. The LongVILA-R1-7B achieves the leading scores under different video lengths. LongVILA-R1-7B achieves leading scores across different video lengths, obtaining scores of 60.3 and 65.9 in the settings without subtitles and with subtitles, respectively. Table 2 compares the results of our LongVideo-Reasoneval benchmark. We also show some qualitative results in the appendix. LongVILA-R1-7B model achieves strong performance with an average score of 67.9, significantly surpassing Video-R1-7B [2] and GPT-4o [19] and slightly outperforms Gemini-1.5-Pro [20]. While Gemini-1.5-Pro demonstrates strong performance in Temporal Reasoning, Goal and Purpose Reasoning, and Plot and Narrative Reasoning, the LongVILA-R1-7B reaches scores that are generally comparable. Notably, in the Spatial Reasoning category, LongVILA-R1-7B achieves score of 70.0. 6 Scaling RL to Long Videos Table 1 Performance on 9 video benchmarks [13, 14, 18, 43, 44, 45, 46, 47, 48]. LongVILA-R1-7B outperforms LongVILA-7B across all benchmarks, with varying margins that reflect different levels of reasoning difficulty. - t test 57.0 - 57.5 45.3 51.9 50.8 50.2 53.0 - 56.3 56.7 59.5 59.6 h g test - - 72.2 38.4 - - 51.7 53.1 62.7 - 60.1 67.7 68.1 e e val 32.6 53.3 43.2 5.9 - - 6.9 - - 28.2 - 58.0 58.2 e d val 61.3 66.7 64.0 37.6 - 41.8 - - 54.8 39.2 56.4 57.1 57.6 T r val - - - - - - 51.4 54.9 - - 57.1 58.1 59.3 e test 43.5 - - 43.5 - 51.2 54.6 57.3 61.1 46.6 56. 67.1 67.6 - N mc - - - - 61.6 - - - - - 79.4 80.7 80.8 e val - 64.4 66.7 12.4 - - 4.5 - - - 51.8 63.0 66.8 VideoMME w/o sub. w/ sub. 59.9 71.9 75.0 39.9 - 39.9 47.9 54.9 56.0 - 58. 60.1 62.4 63.3 77.2 81.3 41.6 - 43.6 50.3 56.4 57.6 - 61.5 65.1 68.4 Model GPT-4V [49] GPT-4o [19] Gemini-1.5-Pro [20] Video-LLaVA-7B [50] Flash-VStream-7B [51] ShareGPT4Video-8B [52] VideoLLaMA2-7B [53] VideoLLaMA2.1-7B [53] Kangaroo-8B [54] PLLaVA-7B [55] LLaVA-OV-7B [56] LongVILA-7B [4] LongVILA-R1-7B Table 2 Performance on LongVideo-Reason-eval. LongVILA-R1-7B achieves the highest overall score. Model Temporal GPT-4o [19] Gemini-1.5-Pro [20] Video-R1-7B [2] LongVILA-R1-7B 64.4 73.2 66.4 71.6 Goal 62.4 66.4 62.8 66. Plot 60.4 66.4 63.6 63.6 Spatial Overall 55.6 63.2 58. 70.0 60.7 67.3 62.7 67.9 6.2. Ablation Study Scaling video frames. The reasoning capability of LongVILA-R1 scales consistently with the number of input video frames. Specifically, Figure 8 illustrates the performance of LongVILA-1.5B (grey line) and LongVILA1.5B-R1 (red line) on the long-video reasoning benchmark under varying frame inputs. With only 16 input frames, LongVILA-R1-1.5B is close to LongVILA-1.5B. However, as the number of frames increases to 512, LongVILAR1-1.5B consistently outperforms and eventually achieves score of 64.3. Notably, LongVILA-R1-1.5B demonstrates steady performance improvements throughout the scaling process. In contrast, LongVILA-1.5B hits performance bottleneck with 256 frames, and got degradation on 512 frames, due to its inability to effectively process such dense visual information. The enhanced reasoning capabilities of LongVILA-R1-1.5B allow it to effectively integrate and infer information from long videos. Ablation on pipeline and datasets. As shown in Table 4, we ablate the effectiveness of training stages and datasets, using LongVILA-1.5B as starting point. The accuracies are evaluated on LongVideo-Reason-eval. means skipping this stage, means training this stage with our datasets, and means training this stage with other datasets [12, 13, 14, 15, 16]. Our CoT-SFT dataset results in better performance than that of other datasets. In addition, incorporating RL on top of the warm-up phase (CoT-SFT) yields additional improvements compared to using only SFT. We show that if we skip CoT-SFT and train our models directly with RL, the accuracy drops. If we apply Video-R1 datasets in both CoT-SFT and RL stages, the performance is inferior to using ours. Training efficiency on MR-SP. We conduct the training efficiency comparison for our MR-SP system, one A100 node, i.e., 8xA100 (80GB) GPUs. We measure the forward time for each training step. The results are obtained after 7 Scaling RL to Long Videos Table 3 Performance comparison on VideoMME [18] benchmark in details. Model Video-LLaVA-7B [50] SliME-8B [57] ShareGPT4Video-8B [52] VideoChat2-7B [58] VideoLLaMA2-7B [53] Chat-Univi-v1.5-7B [59] Kangaroo-8B [54] ShareGemini-7B [60] LongVA-7B [1] Video-R1-7B [2] LongVILA-7B [4] LongVILA-R1-7B w/o subtitle subtitle Overall Short Medium Long Overall Short Medium Long 39.9 45.3 39.9 39.5 47.9 40.6 56.0 43.2 52.6 59.3 60.1 62.4 45.3 53.3 48.3 48.3 56.0 45.7 66.1 49.1 61.1 - 69.0 71.9 38.0 55.4 36.3 37.0 45.4 40.3 55.3 41.3 50.4 - 58.3 62. 36.2 39.8 35.0 33.2 42.1 35.8 46.7 39.1 46.2 - 53.0 53.3 41.6 47.2 43.6 43.8 50.3 45.9 57.6 47.9 54.3 - 65.1 68.4 46.1 55.4 53.6 52.8 59.4 51.2 68.0 49.1 61.1 - 72.9 76. 40.7 44.4 39.3 39.4 47.6 44.6 55.4 47.3 53.6 - 64.9 69.8 38.1 41.7 37.9 39.2 43.8 41.8 49.3 43.4 47.6 - 57.4 59.1 Figure 8 Scaling video frames improves long video reasoning performance against non-reasoning baselines. 10 warming up iterations and averaged over 5 iterations to minimize variance. We use LongVILA-7B-R1 model with training batch size as 1 per GPU and rollout number as 5. Figure 1 presents the training efficiency comparison across different numbers of frames. The figure plots the runtime per step (in seconds) for three settings: the plain RL system without MR-SP, MR-SP Stage 1 only, and the full MR-SP system (Stage 1 & 2). The baseline runtime increases steeply as the frame count grows. Using only Stage 1 of MR-SP significantly improves efficiency up to 512 frames but encounters GPU out-of-memory (OOM) issues beyond that point. In contrast, the full MR-SP system consistently reduces runtime, achieving up to 2.1 speedup at 512 frames and scaling efficiently to 1024 frames without OOM, highlighting the benefit of combining sequence reuse and sequence parallelism for long video RL training. Table 4 Ablations on training pipeline and datasets. CoT-SFT RL O Accuracy 58.1 60. 52.4 59.1 59.4 61.9 7. Conclusion We present comprehensive framework designed to fully scale VLMs for reasoning over long videos. LongVILAR1 encompasses meticulously constructed large-scale dataset, LongVideo-Reason, and parallelized training framework, MR-SP. Leveraging our curated dataset of 52K long video question-reasoning-answer pairs, combined with other open-source video datasets, we adopt two-stage training process that integrates CoT-SFT and RL. LongVILA-R1-7B demonstrates outstanding performance on mainstream video benchmarks, achieving 68.4% on VideoMME with subtitles, and attaining an average accuracy of 67.9% across four reasoning categories in the LongVideo-Reason testing set. LongVILA-R1 maintains consistent performance improvements, extending the feasible frame count from 16 to 512. Notably, our MR-SP leads to speed up of 2.1 for long video RL training, supporting hour-level (3,600 frames / 256k tokens) RL training on single node of 8 A100 GPUs. In addition, we release our training system to the public, which supports RL training across multiple modalities (video, text, and audio), various models (including the VILA and Qwen series), and even image and video generation models. In the appendix, we discuss limitations, border impacts, detailed data generation process, and provide more examples. 8 A. Limitations Scaling RL to Long Videos Despite our best efforts to create high-quality long video reasoning dataset, the definition of reasoning still requires further refinement and more comprehensive conclusion. Moreover, while our proposed efficient reasoning RL training framework is the first to support video training with hundreds to thousands of frames, real-world scenarios often involve videos with far greater frame counts. Thus, there remains significant potential for improvement in the extraction and learning of ultra-dense visual information. B. Border Impacts The development of LongVILA-R1 represents significant advancement in long video reasoning, enabling real world systems to perform sophisticated temporal and compositional understanding across diverse and extended visual contexts. This progress holds transformative potential across multiple domains, including embodied AI, robotics, autonomous systems, and AR/VR applications. By equipping vision-language models (VLMs) with the ability to process and reason over long-duration video data, LongVILA-R1 lays the foundation for AI systems capable of understanding event sequences, tracking object persistence and transformations, and inferring causal and physical relationships over extended frames. Long video reasoning technologies powered by LongVILA-R1 can significantly enhance embodied AI and robotics by enabling agents to sustain coherent, long-term understanding of their environment. Robots equipped with such capabilities would excel in performing complex, multi-stage tasks, adapting to dynamic contexts, and building richer world models for planning and decision-making. These advancements also promise to unlock new opportunities in education, healthcare, and entertainment. For instance, long video understanding could enable AI tutors to analyze and summarize extended instructional videos, or assist healthcare professionals in reviewing lengthy procedural recordings. Furthermore, such systems could enhance sports analytics, and other areas requiring nuanced temporal reasoning. In conclusion, LongVILA-R1 demonstrates the potential of long video reasoning to drive progress across wide range of applications, from robotics to immersive virtual environments. However, unlocking the full promise of this technology requires steadfast commitment to ethical principles, privacy protection, and the broader goal of benefiting humanity. By addressing these challenges, the AI community can ensure that advancements in long video reasoning contribute positively to society while mitigating associated risks. C. More Demos in Different Reasoning Subsets In this section, we selected examples of four types of reasoning tasks from the benchmark to evaluate and compare the reasoning processes and answers provided by Gemini-1.5-Pro, Video-R1-7B, and LongVILA-R1-7B. On Figure 12, 20-minute StarCraft match is depicted, where the models analyze the players unit compositions, strategies, and play styles to predict the potential developments on the battlefield. While Gemini-1.5-Pro produced correct prediction of the outcome, its reasoning process contained factual inaccuracies. In contrast, Video-R1-7B, influenced by the characteristics of its training data, tended to summarize answers based on options, neglecting critical video details and resulting in incorrect reasoning. LongVILA-R1-7B, however, is able to accurately analyze the players operational styles and specific moments marked in the video, leading to comprehensive and accurate prediction of the matchs trajectory. On Figure 13, another example demonstrates the models abilities in narrative reasoning and visual information analysis. Gemini-1.5-Pro failed to correctly infer why the man appearing for the second time in the video is not the husband. In contrast, both Video-R1-7B and LongVILA-R1-7B successfully reasoned that the mans habit of wearing ring on his left hand is key indicator, providing accurate answers. Figure 14 illustrates the models spatial perception and reasoning abilities as the camera moves through room. Gemini-1.5-Pro effectively identified the key information within the video and provided the correct answer through straightforward reasoning. In contrast, Video-R1-7B experienced significant localization errors during the reasoning process, leading to critical issue for reasoning models: mismatch between the reasoning analysis and the final answer. LongVILA-R1-7B demonstrated superior performance by leveraging dense frame analysis to accurately infer the spatial relationships between rooms and furniture across different levels, ultimately delivering coherent reasoning process and the correct answer. On Figure 15, the focus shifts to temporal analysis in Lego video featuring diverse events and interactions. All three models successfully reasoned through the sequence of events and provided correct answers, showcasing their proficiency in temporal reasoning tasks. As supplement to Figure 2, Figure 10 provides more comprehensive comparison of two examples: \"2022 FIFA Argentina vs. Netherlands\" and \"Moving the Cup and Guessing Where the Ball Is.\" In the football match example, while Gemini-1.5-Pro produced the correct answer, its 9 Scaling RL to Long Videos Figure 9 Detailed data generation process for the LongVideo-Reason dataset, supplementing Figure 4. output contained hallucinatory content influenced by biases in its pre-learned knowledge. Video-R1 not only failed to provide accurate video analysis reasoning but also made incorrect predictions. In contrast, LongVILA-R1 successfully analyzed the players performance and emotions during the match, integrating these factors through its robust reasoning capabilities to make accurate predictions about the outcome. For the more challenging task of tracking the ball, Gemini1.5-Pros reasoning is inconsistent with the spatial content throughout, while Video-R1 failed to deduce the balls final position accurately. Remarkably, LongVILA-R1 precisely analyzed the spatial transformations following the movement of the box, demonstrating superior interpretative and reasoning abilities. 10 Scaling RL to Long Videos Figure 10 Detailed comparisons in the football game example. The video is available at Link. Scaling RL to Long Videos Figure 11 Detailed comparisons in the moving cup example. The video is available at Link. 12 Scaling RL to Long Videos Figure 12 LongVILA-R1 reasoning example in the \"Goal and Purpose\" category. The video is available at Link. Scaling RL to Long Videos Figure 13 LongVILA-R1 reasoning example in the \"Plot and Narrative\" category. The video is available at Link. 14 Scaling RL to Long Videos Figure 14 LongVILA-R1 reasoning example in the \"Spatial\" category. The video is available at Link. Scaling RL to Long Videos Figure 15 LongVILA-R1 reasoning example in the \"Temporal\" category. The video is available at Link. 16 Scaling RL to Long Videos Figure 16 LongVILA-R1 reasoning example in the taboo game. The video is available at Link."
        },
        {
            "title": "References",
            "content": "Scaling RL to Long Videos [1] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. CoRR, abs/2406.16852, 2024. [2] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. CoRR, abs/2503.21776, 2025. [3] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In ECCV, pages 453470. Springer, 2024. [4] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos. In ICLR, 2025. [5] Yanan Guo, Wenhui Dong, Jun Song, Shiding Zhu, Xuan Zhang, Hanqing Yang, Yingbo Wang, Yang Du, Xianing Chen, and Bo Zheng. Fila-video: Spatio-temporal compression for fine-grained long video understanding. CoRR, abs/2504.20384, 2025. [6] Yunhang Shen, Chaoyou Fu, Shaoqi Dong, Xiong Wang, Peixian Chen, Mengdan Zhang, Haoyu Cao, Ke Li, Xiawu Zheng, Yan Zhang, et al. Long-vita: Scaling large multi-modal models to 1 million tokens with leading short-context accuray. CoRR, abs/2502.05177, 2025. [7] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. [8] Changshu Liu, Shizhuo Dylan Zhang, and Reyhaneh Jabbarvand. Codemind: framework to challenge large language models for code reasoning. CoRR, abs/2402.09664, 2024. [9] Songhao Han, Wei Huang, Hairong Shi, Le Zhuo, Xiu Su, Shifeng Zhang, Xu Zhou, Xiaojuan Qi, Yue Liao, and Si Liu. Videoespresso: large-scale chain-of-thought dataset for fine-grained video reasoning via core frame selection. CoRR, abs/2411.14794, 2024. [10] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient RLHF framework. In EuroSys, pages 12791297. ACM, 2025. [11] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. NVILA: efficient frontier visual language models. CoRR, abs/2412.04468, 2024. [12] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. CoRR, abs/2410.02713, 2024. [13] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In CVPR, pages 97779786, 2021. [14] Viorica Patraucean, Lucas Smaira, Ankush Gupta, Adrià Recasens, Larisa Markeeva, Dylan Banarse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Matejovicova, Yury Sulsky, Antoine Miech, Alexandre Fréchette, Hanna Klimczak, Raphael Koster, Junlin Zhang, Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and João Carreira. Perception test: diagnostic benchmark for multimodal video models. In NeurIPS, 2023. [15] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pages 19881997, 2017. [16] Bo Wu, Shoubin Yu, Zhenfang Chen, Josh Tenenbaum, and Chuang Gan. STAR: benchmark for situated reasoning in real-world videos. In NeurIPS - Datasets and Benchmarks, 2021. [17] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [18] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. CoRR, abs/2405.21075, 2024. [19] OpenAI. Gpt-4o. 2025. 18 Scaling RL to Long Videos [20] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530, 2024. [21] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, Zhangwei Gao, Erfei Cui, Xuehui Wang, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. CoRR, abs/2504.10479, 2025. [22] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. [23] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In CVPR, pages 2668926699, 2024. [24] Baifeng Shi, Boyi Li, Han Cai, Yao Lu, Sifei Liu, Marco Pavone, Jan Kautz, Song Han, Trevor Darrell, Pavlo Molchanov, and Hongxu Yin. Scaling vision pre-training to 4k resolution. CoRR, abs/2503.19903, 2025. [25] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. LMM-R1: empowering 3b lmms with strong reasoning abilities through two-stage rule-based RL. CoRR, abs/2503.07536, 2025. [26] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. CoRR, abs/2503.06749, 2025. [27] Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. In ICLR, 2024. [28] Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. CoRR, abs/2310.01889, 2023. [29] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. CoRR, abs/2309.14509, 2023. [30] Qiaoling Chen, Diandian Gu, Guoteng Wang, Xun Chen, YingTong Xiong, Ting Huang, Qinghao Hu, Xin Jin, Yonggang Wen, Tianwei Zhang, and Peng Sun. Internevo: Efficient long-sequence large language model training via hybrid parallelism and redundant sharding. CoRR, abs/2401.09149, 2024. [31] Jiarui Fang and Shangchun Zhao. USP: unified sequence parallelism approach for long context generative AI. CoRR, abs/2405.07719, 2024. [32] Diandian Gu, Peng Sun, Qinghao Hu, Ting Huang, Xun Chen, Yingtong Xiong, Guoteng Wang, Qiaoling Chen, Shangchun Zhao, Jiarui Fang, Yonggang Wen, Tianwei Zhang, Xin Jin, and Xuanzhe Liu. Loongtrain: Efficient training of long-sequence llms with head-context parallelism. CoRR, pdf/2406.18485, 2024. [33] Dacheng Li, Rulin Shao, Anze Xie, Eric Xing, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. Lightseq: Sequence level parallelism for distributed training of long context transformers. 2023. [34] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 35:27730 27744, 2022. [35] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. [36] DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. [37] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. [38] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. 19 Scaling RL to Long Videos [39] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael Jordan, et al. Ray: distributed framework for emerging {AI} applications. In OSDI, pages 561577, 2018. [40] Mingfei Han, Linjie Yang, Xiaojun Chang, and Heng Wang. Shot2story20k: new benchmark for comprehensive understanding of multi-shot videos. CoRR, abs/2312.10300, 2023. [41] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1.5: Scaling reinforcement learning with llms. CoRR, abs/2501.12599, 2025. [42] Yaowei Zheng, Junting Lu, Shenzhi Wang, Zhangchi Feng, Dongdong Kuang, and Yuwen Xiong. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/EasyR1, 2025. [43] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: dataset for understanding complex web videos via question answering. In AAAI, pages 91279134, 2019. [44] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. In NeurIPS, 2023. [45] Yifan Du, Kun Zhou, Yuqi Huo, Yifan Li, Wayne Xin Zhao, Haoyu Lu, Zijia Zhao, Bingning Wang, Weipeng Chen, and Ji-Rong Wen. Towards event-oriented long video understanding. CoRR, abs/2406.14129, 2024. [46] Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. CoRR, abs/2407.15754, 2024. [47] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Lou, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, pages 2219522206, 2024. [48] Zijia Zhao, Haoyu Lu, Yuqi Huo, Yifan Du, Tongtian Yue, Longteng Guo, Bingning Wang, Weipeng Chen, and Jing Liu. Needle in video haystack: scalable synthetic framework for benchmarking video mllms. CoRR, abs/2406.09367, 2024. [49] OpenAI. Gpt-4v(ision) technical work and authors. https://openai.com/contributions/gpt-4v, 2023. [50] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. In EMNLP, pages 59715984. ACL, 2024. [51] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memory-based real-time understanding for long video streams. CoRR, abs/2406.08085, 2024. [52] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Zhenyu Tang, Li Yuan, et al. Sharegpt4video: Improving video understanding and generation with better captions. NeurIPS, 37:1947219495, 2024. [53] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, and Lidong Bing. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. CoRR, abs/2406.07476, 2024. [54] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. CoRR, abs/2408.15542, 2024. [55] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See-Kiong Ng, and Jiashi Feng. Pllava : Parameter-free llava extension from images to videos for video dense captioning. CoRR, abs/2404.16994, 2024. [56] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. Trans. Mach. Learn. Res., 2025. [57] Yifan Zhang, Qingsong Wen, Chaoyou Fu, Xue Wang, Zhang Zhang, Liang Wang, and Rong Jin. Beyond llava-hd: Diving into high-resolution large multimodal models. CoRR, abs/2406.08487, 2024. [58] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. CoRR, abs/2305.06355, 2023. [59] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. In CVPR, pages 1370013710, 2024. [60] Xiao Wang, Qingyi Si, Jianlong Wu, Shiyu Zhu, Li Cao, and Liqiang Nie. Retake: Reducing temporal and knowledge redundancy for long video understanding. CoRR, abs/2412.20504, 2024."
        }
    ],
    "affiliations": [
        "HKU",
        "MIT",
        "NVIDIA",
        "UC Berkeley"
    ]
}