{
    "paper_title": "Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning",
    "authors": [
        "DiJia Su",
        "Hanlin Zhu",
        "Yingchen Xu",
        "Jiantao Jiao",
        "Yuandong Tian",
        "Qinqing Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose a hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce a simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks."
        },
        {
            "title": "Start",
            "content": "Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning DiJia Su 1 Hanlin Zhu * 2 Yingchen Xu * 1 3 Jiantao Jiao 2 Yuandong Tian 1 Qinqing Zheng 1 Abstract Large Language Models (LLMs) excel at reasoning and planning when trained on chainof-thought (CoT) data, where the step-by-step thought process is explicitly outlined by text tokens. However, this results in lengthy inputs where many words support textual coherence rather than core reasoning information, and processing these inputs consumes substantial computation resources. In this work, we propose hybrid representation of the reasoning process, where we partially abstract away the initial reasoning steps using latent discrete tokens generated by VQ-VAE, significantly reducing the length of reasoning traces. We explore the use of latent trace abstractions in two scenarios: 1) training the model from scratch for the Keys-Finding Maze problem, 2) fine-tuning LLMs on this hybrid data with an extended vocabulary including unseen latent tokens, for both logical and mathematical reasoning problems. To facilitate effective learning, we introduce simple training procedure that randomly mixes latent and text tokens, which enables fast adaptation to new latent tokens. Our approach consistently outperforms the baselines methods in various benchmarks, such as Math (+4.2%, Llama-3.2-1B), GSM8K (+4.1%, Llama3.2-3B), and Fresh-Gaokao-Math-2023 (+13.3%, Llama-3.1-8B) with an average reduction of 17% in reasoning traces length. 5 2 0 2 5 ] . [ 1 5 7 2 3 0 . 2 0 5 2 : r a"
        },
        {
            "title": "1 Introduction",
            "content": "Reasoning capabilities are increasingly recognized as critical component of Artificial General Intelligence (AGI) systems. Recent research has demonstrated that Large Language Models (LLMs) can exhibit sophisticated reasoning and planning abilities using chain-of-thought (CoT) method- *Equal contribution Equal advising 1Meta AI 2UC Berkeley 3UCL. Preprint. 1 ologies, including prompting LLMs with examples where complex problems are broken down into explicit reasoning steps (Wei et al., 2022b; Chen et al., 2022a; Yao et al., 2024). More recently, number of studies have further shown that when models are trained to articulate the intermediate steps of reasoning process (Nye et al., 2021b; Lehnert et al., 2024), they achieve significantly higher accuracy. The effectiveness of this approach has been demonstrated across multiple domains, including mathematical problem-solving (Yue et al., 2023; Gandhi et al., 2024; Yu et al., 2023; Tong et al., 2024), logical inference (Lin et al., 2024; Dziri et al., 2024), multistep planning tasks (Lehnert et al., 2024; Su et al., 2024), etc. However, training with explicit reasoning traces in text space comes with notable computational costs (Deng et al., 2023; 2024), as the models must process lengthy input sequences. In fact, much of the text serves primarily to maintain linguistic coherence, rather than conveying core reasoning information. Several works have attempted to mitigate this issue. For example, Hao et al. (2024) investigate reasoning in continuous latent space as means of compressing the reasoning trace, and Deng et al. (2024) explore internalizing the intermediate steps through iterative CoT eliminations, see Section 2 for more examples. Nonetheless, these approaches rely on multi-stage training procedures that resemble curriculum learning, which still incur significant computational costs, and their final performances fall behind models trained with complete reasoning traces. To tackle this challenge, we propose to use discrete latent tokens to abstract the initial steps of the reasoning traces. These latent tokens, obtained through vectorquantized variational autoencoder (VQ-VAE), provide compressed representation of the reasoning process by condensing surface-level details. More precisely, we replace the text tokens with their corresponding latent abstractions from left to right until pre-set location, leaving the remaining tokens unchanged. We then fine-tune LLMs with reasoning traces with such assorted tokens, allowing the models to learn from both abstract representations of the thinking process and detailed textual descriptions. One technical challenge posed for the fine-tuning is that the vocabulary is now extended and contains unseen latent tokens. To faToken Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning cilitate quick adaptation to those new tokens, we employ randomized replacement strategy: during training, we randomly vary the number of text tokens being substituted by latent tokens for each sample. Our experiments confirm that this simple strategy leads to straightforward accommodation of unseen latent tokens. We conduct comprehensive evaluation of our approach on diverse range of benchmarks spanning multiple domains. Specifically, we assess its performance on multistep planning tasks (Keys-Finding Maze) and logical reasoning benchmarks (ProntoQA (Saparov & He, 2022), ProsQA (Hao et al., 2024)) for training T5 or GPT-2 models from scratch. In addition, we fine-tune different sizes of LLama-3.1 and LLama-3.2 models using our approach and evaluate them on number of mathematical reasoning benchmarks, including GSM8K (Cobbe et al., 2021a), Math (Hendrycks et al., 2021), and OlympiadBenchMath (He et al., 2024), see Section 4 for more details. Across all these tasks and model architectures, our models consistently outperform baseline models trained with text-only reasoning traces, demonstrating the effectiveness of compressing the reasoning process with assorted tokens."
        },
        {
            "title": "2 Related Work",
            "content": "Explicit Chain-of-Thought Prompting. The first line of work in Chain-of-Thought (CoT) use the traditional chain of prompt in text tokens (Wei et al., 2022a; Nye et al., 2021a). Research works demonstrated that by adding few-shot examples to the input prompt or even zero-shot, the model can perform better in question answering (Chen et al., 2022b; Kojima et al., 2022; Chung et al., 2024). To further improve the model reasoning performance, there has been research effort into prompting with self-consistency (Wang et al., 2022). Here the model is prompted to generate multiple responses and select the best one based on majority voting. On the other hand, research has shown that top-k alternative tokens in the beginning of the prompt can also improve the models reasoning capability (Wang & Zhou, 2024). On top of these empirical results, there has been research on theoretical understanding of why CoT improves the models performance through the lens of expressivity (Feng et al., 2024; Li et al., 2024) or training dynamics (Zhu et al., 2024). In nutshell, CoT improves the models effective depth because the generated output is being fed back to the original input. CoT is also important for LLMs to perform multi-hop reasoning according to the analysis of training dynamics (Zhu et al., 2024). Learning with CoT Data. In addition to the success of CoT prompting, an emerging line of works have explored training LLMs on data with high-quality reasoning traces, for example, the works of Nye et al. (2021b); Azerbayev et al. (2023); Lehnert et al. (2024); Su et al. (2024); Yu et al. (2024); Yang et al. (2024); Deng et al. (2023; 2024). There is also surge of interest in synthesizing datasets with diverse intermediate steps for solving problems in various domains, see, e.g., the works of Kim et al. (2023); Tong et al. (2024); Yu et al. (2023); Yue et al. (2023); Lozhkov et al. (2024). Wen et al. (2024) also theoretically studies how training with reasoning trace can improve the sample complexity of certain tasks. LLM Reasoning in Latent Space. There has been research investigating LLM reasoning in the latent space. Hao et al. (2024) have proposed to use the last hidden state of language model as the next input embeddings, allowing the model to continue reasoning within continuous latent space. The authors show that this approach effectively captures multiple reasoning paths simultaneously, mimicking breadth-first-search strategy. Goyal et al. (2023) proposes to insert learnable pause tokens into the original text, in order to delay the generation. As result, the model can leverage additional computation before providing the final answer. Parallel to this, Pfau et al. (2024) have explored filler tokens, which are used to solve computational tasks that are otherwise unattainable without intermediate token generation. In addition, Liu et al. (2024) propose latent coprocessor method that operates on the transformers keyvalue cache to improve the LLM performance. Nevertheless, none of these methods have shown good performance when integrated into modern-sized LLMs and tested on real-world LLM datasets instead of synthetic ones. Orthogonal to these works, Pagnoni et al. (2024) proposes tokenization-free architecture that encodes input bytes into continuous patch representations, which is then used to train latent Transformer, and Barrault et al. (2024) perform autoregressive sentence prediction in an embedding space. While these two works both leverage continuous latent spaces, our work focuses on the direct use of discrete latent tokens."
        },
        {
            "title": "3 Methodology",
            "content": "In this section, we describe our methodology to enable LLMs to reason with discrete latent tokens. The notations are summarized in Appendix B. Let = denote sample input, where = (p1, p2, . . . , ptp ) are the prompt tokens, = (c1, c2, . . . , ctc) are the reasoning step (chain-of-thought) tokens, = (s1, s2, . . . , sts ) are the solution tokens, and denotes concatenation. Our training procedure consists of two stages: 1. Learning latent discrete tokens to abstract the reasoning steps, where we train model to convert into sequence of latent tokens = (z1, z2, . . . , ztz ) such that tz < tc. The compression rate = tc/tz controls the level of abstraction. 2 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning Figure 3.1: An example illustrating our replacement strategy. With chunk size = 16 and compression rate = 16, we encode 32 textual CoT tokens into 2 discrete latent tokens from left to right. The other CoT tokens will remain in their original forms. 2. Training the LLM with partial and high-level abstract of the reasoning steps, where we construct modified input (cid:101)X by replacing the first tokens of by the corresponding latent abstractions: (cid:101)X = [z1, . . . , , cm+1, . . . , ctc] S. (1) Figure 3.1 illustrates this replacement strategy. We randomize the value of during training. 3.1 Learning Latent Abstractions We employ vector-quantized variable autoencoder (VQVAE) (Van Den Oord et al., 2017) type of architecture to map CoT tokens into discrete latent tokens Z. To enhance abstraction performance, our VQ-VAE is trained on the whole input sequence X, but only applied to in the next stage. Following Jiang et al. (2022; 2023), we split into chunks of length and encode each chunk into latent codes, where is preset compression rate. More precisely, our architecture consists of the following five components: : codebook containing vectors in Rd. fenc : (cid:55) Rd text tokens to x1, . . . , that encodes sequence of latent embedding vectors = , where is the vocabulary of text tokens. : Rd (cid:55) E: the quantization operator that replaces the encoded embedding by the nearest neighbor in E: q(x) = argmineiE ei x2 2. : (cid:55) Rd that maps text tokens to ddimensional embedding vector. We use to generate continuous embedding of the prompt . fdec : Rd Rk (cid:55) that decodes latent embeddings back to text tokens, conditioned on prompt embedding. Figure 3.2: graphical illustration of our VQ-VAE. fenc encodes the text tokens into latent embeddings, which are quantized by checking the nearest neighbors in the codebook. fdec decodes those quantized embeddings back to text tokens. When applying the VQ-VAE to compress the text tokens, the discrete latent tokens are essentially the index of corresponding embeddings in the codebook. In particular, each continuous vector in the codebook has an associated latent token z, which we use to construct the latent reasoning steps 1. For simplicity, we assume the lengths of the input and the prompt are and exactly. Similar to Van Den Oord et al. (2017), we use an objective composed of 3 terms: L(X) = log p(Xfdec(q( X)g(P ))) (cid:125) (cid:123)(cid:122) reconstruction loss (cid:124) + (cid:88) i=1 sg[ Xi] q( Xi)2 2 (cid:123)(cid:122) (cid:125) (cid:124) VQ loss + β Xi sg[q( Xi)]2 , 2 (cid:125) (cid:123)(cid:122) commitment loss (cid:124) (2) 1To decode latent token z, we look up the corresponding embedding and feed it to fdec. 3 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning where = fenc(X), sg[] is the stop-gradient operator, and β is hyperparameter controlling the strength of the commitment loss. The VQ loss and the commitment loss ensure that the encoder outputs remain close to the codebook, while the reconstruction loss concerns with the decoding efficacy. As standard for VQ-VAE, we pass the gradient fdec(L) unaltered to fenc directly as the quantization operator q() is non-differentiable. Figure 3.2 illustrates our architecture. In practice, we use causal Transformer for both fenc and fdec, the model details are discussed in Appendix A. Thus far we obtain latent representation both semantically meaningful and conducive to reconstruction, setting the stage for the subsequent training phase where the LLM is trained to perform reasoning with abstractions. 3.2 Reasoning with Discrete Latent Tokens In this second stage, we apply the obtained VQ-VAE to form modifed samples (cid:101)X with latent abstractions as in Equation (1), then train an LLM to perform next token prediction. Below, we outline the major design choices that are key to our models performance, and ablate them in Section 4. Partial Replacement. Unlike previous planning works (Jiang et al., 2022; 2023) that project the whole input sequence onto compact latent space, we only replace < tc CoT tokens with their latent abstractions, leaving the remaining tokens unchanged. We delimit the latent tokens by injecting special <boLatent> and <eoLatent> tokens to encapsulate them. Left-to-Right (AR) Replacement. We replace the leftmost tokens of C, rather than subsampling tokens at different locations. Mixing Samples with Varying Values of m. For finetuning an existing LLM on the reasoning dataset with latent tokens, one remarkable challenge is to deal with the extended vocabulary. As the LLM is pretrained with trillions of tokens, it is very hard for it to quickly adapt to tokens (and corresponding embeddings) beyond the original vocabulary. Previous works that aim to replace or eliminate CoT tokens (Deng et al., 2024; Hao et al., 2024) employ multistage curriculum training approach, where those operations are gradually applied to the entire input sequence. In the context of our approach, this means we increase the values of in each stage until it reaches pre-set cap value. However, such training procedure is complex and computationally inefficient, where dedicated optimization tuning is needed. In this work, we employ simple single stage training approach where the value of is randomly set for each sample. Surprisingly, this not only makes our training more efficient, but also leads to enhanced performance."
        },
        {
            "title": "4 Experiments",
            "content": "We empirically evaluate our approach on two categories of benchmarks: (1) Synthetic datasets including the Keys-Finding Maze, ProntoQA (Saparov & He, 2022), and ProsQA (Hao et al., 2024), where we pretrain T5 or GPT-2 models from scratch using the method in Section 3; (2) Real-world mathematic reasoning problems, where we fine-tune Llama models (Dubey et al., 2024) on the MetaMathQA (Yu et al., 2023) or the DartMATH (Tong et al., 2024) dataset, and then test on indomain datasets Math and GSM-8K, along with out-ofdomain datasets including Fresh-Gaokao-Math-2023, DeepMind-Math, College-Math, OlympiaBench-Math, and TheoremQA. The detailed setup is introduced in Section 4.1. We compare our approach to the following baselines: Sol-Only: the model is trained with samples that only contains questions and solutions, without any reasoning steps; CoT: the model is trained with samples with complete CoT tokens; iCoT (Deng et al., 2024): method that utilizes curriculum learning to gradually eliminate the need of CoT tokens in reasoning; Pause Token (Goyal et al., 2023): method that injects learnable pause token into the sample during training, in order to offer extra computation before giving out the final answer. 4.1 Benchmarks 4.1.1 SYNTHETIC BENCHMARKS Keys-Finding Maze is complex navigation environment designed to evaluate an agents planning capabilities. The agent is randomly positioned within maze comprising 4 3 3 interconnected rooms, with the objective of reaching randomly placed goal destination. To successfully reach the destination, the agent must collect keys (designated with green, red, and blue colors) that correspond to matching colored doors. These keys are randomly distributed among the rooms, requiring the agent to develop sophisticated planning strategies for key acquisition and door traversal. The agent is only allowed to take one key at time. This environment poses substantial cognitive challenge, as the agent must identify which keys are necessary for reaching the destination, and optimize the order of key collection and door unlocking to establish the most efficient path to the goal. Following Lehnert et al. (2024); Su et al. (2024), we generate intermediate search traces using the nondeterministic 4 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning A* algorithm (Hart et al., 1968). The dataset contains 100k training samples. See Appendix A.2 for more information and graphical illustrations. ProntoQA (Saparov & He, 2022) is dataset consists of 9000 logical reasoning problems derived from ontologies - formal representations of relationships between concepts. Each problem in the dataset is constructed to have exactly one correct proof or reasoning path. One distinctive feature of this dataset is its consistent grammatical and logical structure, which enables researchers to systematically analyze and evaluate how LLMs approach reasoning tasks. ProsQA (Hao et al., 2024) is more difficult benchmark building on top of ProntoQA. It contains 17,886 logical problems curated by randomly generated directed acyclic graphs. It has larger size of distracting reasoning paths in the ontology, and thus require more complex reasoning and planning capabilities. 4.1.2 MATHEMATICAL REASONING We fine-tune pretrained LLMs using the MetaMathQA (Yu et al., 2023) or the Dart-MATH (Tong et al., 2024) dataset. MetaMathQA is curated dataset that augments the existing Math (Hendrycks et al.) and GSM8K (Cobbe et al., 2021b) datasets by various ways of question bootstrapping, such as (i) rephrasing the question and generating the reasoning path; (ii) generating backward questions, self-verification questions, FOBAR questions (Jiang et al., 2024), etc. This dataset contains 395k samples in total, where 155k samples are bootstrapped from Math and the remaining 240k come from GSM8K. We rerun the MetaMath data pipeline by using Llama-3.1-405B-Inst to generate the response. DartMATH (Tong et al., 2024) also synthesizes responses for questions in Math and GSM8K, with the focus on difficult questions via difficulty-aware rejection tuning. For evaluation, we test the models on the original Math and GSM8K datasets, which are in-domain, and also the following outof-domain benchmarks: College-Math (Tang et al., 2024) consists of 2818 collegelevel math problems taken from 9 textbooks. These problems cover over 7 different areas such as linear algebra, differential equations, and so on. They are designed to evaluate how well the language model can handle complicated mathematical reasoning problems in different field of study. DeepMind-Math (Saxton et al., 2019) consists of 1000 problems based on the national school math curriculum for students up to 16 years old. It examines the basic mathematics and reasoning skills across different topics. OlympiaBench-Math (He et al., 2024) is text-only English subset of Olympiad-Bench focusing on advanced level mathematical reasoning. It contains 675 highly difficult math problems from competitions. TheoremQA (Chen et al., 2023) contains 800 problems focuses on solving problems in STEM fields (such as math, physics, and engineering) using mathematical theorems. Fresh-Gaokao-Math-2023 (Tang et al., 2024) contains 30 math questions coming from Gaokao, or the National College Entrance Examination, which is national standardized test that plays crucial role in the college admissions process. 4.2 Main Results We employ consistent strategy for training VQ-VAE and replacing CoT tokens with latent discrete codes across all our experiments, as outlined below. The specific model architecture and key hyperparameters used for LLM training are presented alongside the results for each category of benchmarks. All the other details are deferred to Appendix A. VQ-VAE Training For each benchmark, we train VQVAE for 100k steps using the Adam optimizer, with learning rate 105 and batch size 32. We use codebook of size 1024 and compress every chunk of = 16 tokens into single latent token (i.e., the compression rate = 16). Randomized Latent Code Replacement We introduce stochastic procedure for partially replacing CoT tokens with latent codes. Specifically, we define set of predetermined numbers = {0, 72, 128, 160, 192, 224, 256}, which are all multipliers of = 16. For each training example, we first sample mmax then sample an integer [0, 16, 32, . . . , mmax] uniformly at random. The first CoT tokens are replaced by their corresponding latent discrete codes, while the later ones remain as raw text. This stochastic replacement mechanism exposes the model to wide range of latent-text mixtures, enabling it to effectively learn from varying degrees of latent abstraction. 4.2.1 SYNTHETIC BENCHMARKS Hyperparameters and Evaluation Metric For our experiments on the ProntoQA and ProsQA datasets, we fine-tune the pretrained GPT-2 model (Radford et al., 2019) for 16k steps, where we use learning rate of 104 with linear warmup for 100 steps, and the batch size is set to 128. To evaluate the models, we use greedy decoding and check the exact match with the ground truth. For Keys-Finding Maze, due to its specific vocabulary, we trained T5 model (Raffel et al., 2020) from scratch for 100k steps with learning rate of 7.5 104 and batch size of 1024. We evaluate the models by the 1-Feasible10 metric. Namely, for each evaluation task, we randomly sample 10 responses with top-k (k=10) decoding and check 5 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning Model Keys-Finding Maze ProntoQA ProsQA 1-Feasible-10 (%) Num. Tokens Accuracy Num. Tokens Accuracy Num. Tokens Sol-Only CoT Latent (ours) 3 43 62.8 ( +19.8) 645 1312.0 374.6 93.8 98.8 100 ( +1.2) 3.0 92.5 7. 76.7 77.5 96.2 ( +18.7) 8.2 49.4 10.9 Table 4.1: Our latent approach surpasses the other baselines on Keys-Finding Maze, ProntoQA and ProsQA with large margin . We use top-k (k = 10) decoding for Keys-Finding Maze and greedy decoding for ProntoQA and ProsQA. In terms of token efficiency, our latent approach also generates much shorter reasoning traces than the CoT baseline, closely tracking or even outperforming the Sol-Only approach. Bold: best results. Underline: second best results. ( +Performance gain compared with the second best result.) Model In-Domain Out-of-Domain Average Math GSM8K Gaokao-Math-2023 DM-Math College-Math Olympia-Math TheoremQA All Datasets Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B Sol-Only CoT iCoT Pause Token Latent (ours) Sol-Only CoT iCoT Pause Token Latent (ours) Sol-Only CoT iCoT Pause Token Latent (ours) 4.7 10.5 8.2 5.1 14.7 ( +4.2) 6.1 21.9 12.6 25.2 26.1 ( +4.2) 11.5 32.9 17.8 39.6 37.2 6.8 42.7 10.5 5.3 48.7 ( +6) 8.1 69.7 17.3 53.7 73.8 ( +4.1) 11.8 80.1 29.6 79.5 84.1 ( +4.0) 0.0 10.0 3.3 2.0 10. 3.3 16.7 3.3 4.1 23.3 ( +6.6) 3.3 16.7 16.7 6.1 30.0 ( +13.3) 10.4 3.4 11.3 1.4 14.6 ( +3.3) 5.3 17.1 7.6 0.5 20.5 ( +3.4) 14.0 27.3 16.0 7.4 27.1 17.4 39.3 20.3 25.4 41.3 ( +2) 7.0 30.9 14.2 11.8 32.9 ( +2) 13.0 41.9 21.3 25.1 44.0 ( +2.1) 1.3 1.5 2.1 0.0 1.8 1.8 2.2 4.9 0.7 4.2 3.9 9.8 10.7 0.6 11.3 ( +0.6) 6.8 11.6 13.9 1.0 13. 3.8 7.3 7.6 1.3 10.2 ( +2.6) 6.7 15.8 14.8 4.0 18.4 ( +2.6) 4.6 14.1 7.7 2.1 17.8 ( +3.7) 6.7 25.2 11.7 14.8 28.1 ( +2.9) 9.6 33.4 18.3 25.9 37.9 ( +4.5) Table 4.2: Our latent approach outperforms the baselines on various types of mathematical reasoning benchmarks. The models are fine-tuned on the MetaMathQA (Yu et al., 2023) dataset. The Math and GSM8K are in-domain datasets since they are used to generate MetaMathQA, while the others are out-of-domain. Bold: best results. Underscore: second best results. +: Performance gain compared with the second best result. if any of them is feasible and reaches the goal location. evaluation. Results As shown in Table 4.1, our latent approach performs better than the baselines for both the Keys-Finding Maze and ProntoQA tasks. Notably, the absolute improvement is 15% for the Keys-Finding Maze problem, and we reach 100% accuracy on the relatively easy ProntoQA dataset. For the more difficult ProsQA, the CoT baseline only obtains 77.5% accuracy, the latent approach achieves 17.5% performance gain. 4.2.2 MATHEMATICAL REASONING Hyperparameters and Evaluation Metrics We considered 3 different sizes of LLMs from the LLaMa herd: Llama3.2-1B, Llama-3.2-3B and Llama-3.1-8B models. For all the models, we fine-tune them on the MetaMathQA dataset for 1 epoch. To maximize training efficiency, we use batch size of 32 with sequence packing of 4096. We experiment with different learning rates 105, 2.5 105, 5 105, 104 and select the one with the lowest validation error. The final choices are 105 for the 8B model and 2.5 105 for the others. For all the experiments, we use greedy decoding for Accuracy Comparison Table 4.2 presents the results. Our latent approach consistently outperforms all the baselines across nearly all the tasks, for models of different sizes. For tasks on which we do not observe improvement, our approach is also comparable to the best performance. The gains are more pronounced in specific datasets such as Gaokao-Math-2023. On average, we are observing +5.3 points improvement for the 8B model, +2.9 points improvement for the 3B model, and +3.7 points improvement for the 1B model. Tokens Efficiency Comparison Alongside the accuracy, we also report the number of tokens contained in the generated responses in Table 4.3, which is the dominating factor of the inference efficiency. Our first observation is that for all the approaches, the model size has little influence on the length of generated responses. Overall, the CoT method outputs the longest responses, while the Sol-Only method outputs the least number of tokens, since it is trained to generate the answer directly. The iCoT method generates short responses as well (42.8% reduction compared to CoT), as 6 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning Model In-Domain (# of tokens) Out-of-Domain (# of tokens) Average Math GSM8K Gaokao-Math-2023 DM-Math College-Math Olympia-Math TheoremQA All Datasets Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B Sol-Only CoT iCoT Pause Token Latent (ours) Sol-Only CoT iCoT Pause Token Latent (ours) Sol-Only CoT iCoT Pause Token Latent (ours) 4.7 646.1 328.4 638.8 501.6 ( -22%) 6.1 649.9 344.4 307.9 516.7 ( -20%) 11.5 624.3 403.5 469.4 571.9 ( -9 %) 6.8 190.3 39.8 176.4 181.3 ( -5%) 8.1 212.1 60.7 162.3 198.8 ( -6%) 11.8 209.5 67.3 119.0 193.9 ( -8 %) 0.0 842.3 354.0 416.1 760.5 ( -11%) 3.3 823.3 564.0 108.9 618.5 ( -25%) 3.3 555.9 444.8 752.6 545.8 ( -2 %) 10.4 578.7 170.8 579.9 380.1 ( -34%) 14.0 392.8 154.3 251.5 340.0 ( -13%) 17.4 321.8 137.0 413.4 292.1 ( -10%) 5.3 505.6 278.7 193.8 387.3 ( -23%) 7.0 495.9 224.9 500.96 418.0 ( -16%) 13.0 474.3 257.1 357.3 440.3 ( -8%) 1.3 1087.0 839.4 471.9 840.0 ( -22%) 1.8 1166.7 697.6 959.5 832.8 ( -29%) 3.9 736.5 575.4 988.1 575.5 ( -22%) 6.8 759.6 363.6 212.8 670.2 ( -12%) 3.8 1103.3 797.1 648.2 913.7 ( -17 %) 6.7 760.1 430.9 600.1 637.2 ( -16 %) 4.6 655.2 369.5 495 518 ( -21%) 6.7 642.9 344.2 354.7 513.6 ( -20%) 9.6 578.5 362.5 480 513.7 ( -10%) Table 4.3: The average number of tokens in the generated responses. Compared with the CoT baseline, our latent approach achieves an 17% reduction in response length on average, while surpassing it in final performance according to Table 4.2. The iCoT method generates shorter responses than our approach, yet performs significantly worse, see Table 4.2. -: Trace length reduction rate compared with CoT. Model In-Domain Out-of-Domain Average math GSM8K Fresh-Gaokao-Math-2023 DeepMind-Mathematics College-Math Olympia-Math TheoremQA All Datasets Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B All-Replace Curriculum-Replace Poisson-Replace Latent-AR (ours) All-Replace Curriculum-Replace Poisson-Replace Latent (ours) All-Replace Curriculum-Replace Possion-Replace Latent (ours) 6.7 7.1 13.9 14.7 10.7 10.2 23.6 26.1 15.7 14.6 37.9 37. 4.2 9.8 49.5 48.7 12.8 14.9 65.9 73.8 19.9 23.1 83.6 84.1 0.0 3.3 10.0 10.0 10.0 3.3 13.3 23.3 6.7 13.3 16.6 30. 11.8 13.0 12.2 14.6 19.4 16.8 17.9 27.1 21.1 20.3 42.7 41.3 6.0 7.9 18.9 20.5 12.8 12.9 28.9 32.9 19.5 18.7 44.7 44. 2.1 2.4 2.3 1.8 5.3 3.9 2.9 4.2 5.0 3.9 9.9 10.2 8.5 10.5 9.0 11.3 11.8 14.4 11.2 13.5 17.5 16.6 19.1 18. 5.6 7.8 15.1 17.8 11.8 10.9 20.5 28.1 15.0 15.8 36.3 37.9 Table 4.4: Our latent token replacement strategy significantly outperforms the alternative choices: All-Replace (where all the textual CoT tokens are replaced by latent tokens at once), Curriculum-Replace (where we gradually replace the text tokens for the entire CoT subsequence by latent tokens over the course of training) and Poisson-Replace (where individual chunks of text tokens are replaced with probabilities 0.5). the CoT data has been iteratively eliminated in its training procedure. However, this comes at the cost of significantly degraded model performance compared with CoT, as shown in Table 4.2. Our latent approach shows an average 17% reduction in token numbers compared with CoT while surpassing it in prediction accuracy. 4.3 Ablation & Understanding Studies Replacement Strategies Our latent approach partially replaces the leftmost CoT tokens, where the value of varies for each sample. We call such replacement strategies AR-Replace. Here we consider three alternative strategies: (1) All-Replace: all the text CoT tokens are replaced by the latent tokens. (2) Curriculum-Replace: the entire CoT subsequence are gradually replaced over the course of training, similar to the training procedure used by iCoT and COCONUT (Hao et al., 2024). We train the model for 8 epochs. Starting from the original dataset, in each epoch we construct new training dataset whether we further replace the leftmost 16 textual CoT tokens by discrete latent token. (3) Poisson-Replace: instead of replacing tokens from left to right, we conduct Poisson sampling process to select CoT tokens to be replaced: we split the reasoning traces into chunks consisting of 16 consecutive text tokens, where each chunk is randomly replaced by the latent token with probability 0.5. Table 4.4 reports the results. Our AR-Replace strategy demonstrate strong performance, outperforming the other two strategies with large performance gap. Our intuition is as follows. When all the textual tokens are removed, the model struggles to align the latent tokens with the linguistic and semantic structures it learned during pretraining. 7 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning In contrast, partial replacement offers the model bridge connecting text and latent spaces: the remaining text tokens serve as anchors, helping the model interpret and integrate the latent representations more effectively. Interestingly, the curriculum learning strategy is bridging the two spaces very well, where All-Replace and Curriculum-Replace exhibit similar performance. This is similar to our observation that iCoT performs remarkably worse than CoT for mathematical reasoning problems. Poisson-Replace demonstrates performance marginally worse to our AR-Replace strategy on the 1B and 8B models, but significantly worse on the 3B model. Our intuition is that having fix pattern of replacement (starting from the beginning and left to right) is always easier for the model to learn. This might be due to the limited finetuning dataset size and model capacity. Attention Weights Analysis To understand the reason why injecting latent tokens enhanced the models reasoning performance, we randomly selected two questions from the Math and Collegue-Math dataset and generate responses, then analyze the attention weights over the input prompt tokens: (a) Prompt: between $120%$ of 30 and $130%$ of 20? What is the positive difference Mark has $50 in his bank account. (b) Prompt: He earns $10 per day at his work. If he wants to buy bike that costs $300, how many days does Mark have to save his money? 1. What is the positive difference between $120%$ of 30 and $130%$ of 20? Figure 4.1: Comparing with the CoT model, our latent approach have high attention weights on numbers and text tokens representing mathematical operations. 2. Mark has $50 in his bank account. He earns $10 per day at his work. If he wants to buy bike that costs $300, how many days does Mark have to save his money? Specifically, we take the last attention layer, compute the average attention weights over different attention heads and show its relative intensity over the prompt tokens2. We compare the averaged attention weights of our model with the CoT model in Figure 4.1. Interestingly, our model learns to grasp stronger attention to numbers and words representing mathematical operations. Both Figure 4.1a and Figure 4.1b show that the latent model focus more on the numbers, such as 120, 30, and 130 for the first question. For the second question, our latent model shows larger attention weights on numbers including 50, 10, and 300, and also tokens semantically related to mathematical operations such as earns (means addition) and cost (means subtraction). This suggests that, by partially compressing the reasoning trace into mix of latent and text tokens, we 2We first compute the average attention weights across multiple heads. This gives us single lower triangular matrix. Then, we take the column sum of this matrix to get an aggregated attention weights for each token. Last, we normalize the weights by their average to obtain the relative intensity. one line pseudocode is: column sum(avg(attention matrices)) / avg(column sum(avg(attention matrices))). 8 allow the model to effectively focus on important tokens that build the internal logical flow. See Appendix C.1 for the exact response generated by our approach and the CoT baseline. Additional Experiments We provide 4 additional example responses for questions in the Math and TheoremQA datasets in Appendix D. In Appendix E, we compare all the approaches when the model is trained on the DARTMATH (Tong et al., 2024) dataset, where similar trends are observed."
        },
        {
            "title": "5 Conclusion",
            "content": "We present novel approach to improve the reasoning capabilities of LLMs, by compressing the initial steps of the reasoning traces using discrete latent tokens obtained from VQ-VAE. By integrating both abstract representation and textual details of the reasoning process into training, our approach enables LLMs to capture essential reasoning information with improved token efficiency. Furthermore, by randomizing the number of text tokens to be compressed during training, we unlock fast adaptation to unseen latent tokens. Our comprehensive evaluation demonstrates the effectiveness across multiple domains, outperforming standard methods that rely on complete textual reasoning traces. Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents method to enhance the reasoning capability of Large Language Models (LLMs) by combining latent and text tokens in the reasoning trace. In terms of society impact, while reasoning with (opaque) latent tokens may trigger safety concerns, our approach provides VQVAE decoder that can decode the latent tokens into human readable format, mitigating such concerns."
        },
        {
            "title": "References",
            "content": "Azerbayev, Z., Schoelkopf, H., Paster, K., Santos, M. D., McAleer, S., Jiang, A. Q., Deng, J., Biderman, S., and Welleck, S. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. Barrault, L., Duquenne, P.-A., Elbayad, M., Kozhevnikov, A., Alastruey, B., Andrews, P., Coria, M., Couairon, G., Costa-juss`a, M. R., Dale, D., et al. Large concept models: Language modeling in sentence representation space. arXiv e-prints, pp. arXiv2412, 2024. Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022a. Chen, W., Ma, X., Wang, X., and Cohen, W. W. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022b. Chen, W., Yin, M., Ku, M., Lu, P., Wan, Y., Ma, X., Xu, J., Wang, X., and Xia, T. Theoremqa: theorem-driven question answering dataset. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 78897901, 2023. Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021a. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021b. Deng, Y., Choi, Y., and Shieber, S. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dziri, N., Lu, X., Sclar, M., Li, X. L., Jian, L., Lin, B. Y., West, P., Bhagavatula, C., Bras, R. L., Hwang, J. D., Sanyal, S., Welleck, S., Ren, X., Ettinger, A., Harchaoui, Z., and Choi, Y. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36, 2024. Feng, G., Zhang, B., Gu, Y., Ye, H., He, D., and Wang, L. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36, 2024. Gandhi, K., Lee, D., Grand, G., Liu, M., Cheng, W., Sharma, A., and Goodman, N. D. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024. Goyal, S., Ji, Z., Rawat, A. S., Menon, A. K., Kumar, S., and Nagarajan, V. Think before you speak: Training language models with pause tokens. arXiv preprint arXiv:2310.02226, 2023. Hao, S., Sukhbaatar, S., Su, D., Li, X., Hu, Z., Weston, J., and Tian, Y. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. Hart, P. E., Nilsson, N. J., and Raphael, B. formal basis for the heuristic determination of minimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100107, 1968. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). Deng, Y., Prasad, K., Fernandez, R., Smolensky, P., Chaudhary, V., and Shieber, S. Implicit chain of thought reasoning via knowledge distillation. arXiv preprint arXiv:2311.01460, 2023. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021. 9 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning Jiang, W., Shi, H., Yu, L., Liu, Z., Zhang, Y., Li, Z., and Kwok, J. Forward-backward reasoning in large language models for mathematical verification. In Findings of the Association for Computational Linguistics ACL 2024, pp. 66476661, 2024. Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021b. Jiang, Z., Zhang, T., Janner, M., Li, Y., Rocktaschel, T., Grefenstette, E., and Tian, Y. Efficient planning in compact latent action space. arXiv preprint arXiv:2208.10291, 2022. Jiang, Z., Xu, Y., Wagener, N., Luo, Y., Janner, M., Grefenstette, E., Rocktaschel, T., and Tian, Y. H-gap: Humanoid control with generalist planner. arXiv preprint arXiv:2312.02682, 2023. Kim, S., Joo, S. J., Kim, D., Jang, J., Ye, S., Shin, J., and Seo, M. The cot collection: Improving zero-shot and fewshot learning of language models via chain-of-thought fine-tuning. arXiv preprint arXiv:2305.14045, 2023. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 2219922213, 2022. Lehnert, L., Sukhbaatar, S., Su, D., Zheng, Q., McVay, P., Rabbat, M., and Tian, Y. Beyond a*: Better planning with transformers via search dynamics bootstrapping. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id=SGoVIC0u0f. Li, Z., Liu, H., Zhou, D., and Ma, T. Chain of thought empowers transformers to solve inherently serial problems, 2024. URL https://arxiv.org/abs/ 2402.12875. Lin, B. Y., Bras, R. L., and Choi, Y. Zebralogic: Benchmarking the logical reasoning ability of language models, 2024. URL https://huggingface.co/spaces/ allenai/ZebraLogic. Liu, L., Pfeiffer, J., Wu, J., Xie, J., and Szlam, A. Deliberation in latent space via differentiable cache augmentation. 2024. URL https://arxiv.org/abs/2412. 17747. Lozhkov, A., Ben Allal, L., Bakouch, E., von Werra, L., and Wolf, T. Finemath: the finest collection of mathematical content, 2024. URL https://huggingface.co/ datasets/HuggingFaceTB/finemath. Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021a. Pagnoni, A., Pasunuru, R., Rodriguez, P., Nguyen, J., Muller, B., Li, M., Zhou, C., Yu, L., Weston, J., Zettlemoyer, L., Ghosh, G., Lewis, M., Holtzman, A., and Iyer, S. Byte latent transformer: Patches scale better than tokens. 2024. URL https://arxiv.org/abs/ 2412.09871. Pfau, J., Merrill, W., and Bowman, S. R. Lets think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. URL http://jmlr. org/papers/v21/20-074.html. Saparov, A. and He, H. Language models are greedy reasoners: systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022. Saxton, D., Grefenstette, E., Hill, F., and Kohli, P. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019. Su, D., Sukhbaatar, S., Rabbat, M., Tian, Y., and Zheng, Q. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces. arXiv preprint arXiv:2410.09918, 2024. Tang, Z., Zhang, X., Wang, B., and Wei, F. Mathscale: Scaling instruction tuning for mathematical reasoning. arXiv preprint arXiv:2403.02884, 2024. Tong, Y., Zhang, X., Wang, R., Wu, R., and He, J. Dartmath: Difficulty-aware rejection tuning for mathematical problem-solving. arXiv preprint arXiv:2407.13690, 2024. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Wang, X. and Zhou, D. Chain-of-thought reasoning without prompting. 2024. URL https://arxiv.org/abs/ 2402.10200. 10 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022a. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022b. Wen, K., Zhang, H., Lin, H., and Zhang, J. From sparse dependence to sparse attention: Unveiling how chain-ofthought enhances transformer sample efficiency. arXiv preprint arXiv:2410.05459, 2024. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok, J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Yu, P., Xu, J., Weston, J., and Kulikov, I. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024. Yue, X., Qu, X., Zhang, G., Fu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. Zhu, H., Huang, B., Zhang, S., Jordan, M., Jiao, J., Tian, Y., and Russell, S. Towards theoretical understanding of thereversal cursevia training dynamics. arXiv preprint arXiv:2405.04669, 2024. 11 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning"
        },
        {
            "title": "A Experiment Details",
            "content": "A.1 VQ-VAE Model Details The codebook size is 64 for ProntoQA and ProsQA, 512 for the Keys-Finding Maze, and 1024 for math reasoning problems. For both encoder fenc and decoder fdec, we use 2-layer transformer with 4 heads, where the embedding size is 512 and the block size is 512. We set the max sequence to be 2048 for the synthetic dataset experiments and 256 for the math reasoning experiments. A.2 Keys-Finding Maze A.2.1 ENVIRONMENT DETAILS In this section, we introduce our synthetic keys-finding maze environment. Figure A.1 shows an example maze that consists of rooms, where the size of each room is (m = 3 and = 5). The goal of the agent (represented by the black circle) is to reach the gold diamond using the minimum number of steps. The agent cannot cross the wall. Also, there are three doors (represented by squares) of different colors (i.e., red, green, and blue) which are closed initially. The agent have to pick up keys to open the door in the same color. Note that the agent can not carry more than one key at the same time. Figure A.2 shows an example optimal trajectory of the maze in Figure A.1. The agent first picks up the blue key and opens the blue door to obtain the red key. Then the agent navigates to the red door and opens it. Finally the agent is able to reach the objective. Figure A.1: An example of the keys-finding maze environment. (a) Phase 1 (b) Phase 2 (c) Phase 3 (d) Phase 4 Figure A.2: An (optimal) trajectory of the maze in Figure A.1. Phase 1: the agent picks up the blue key; Phase 2: the agent opens the blue door to obtain the red key; Phase 3: the agent carries the red key to the red door; Phase 4: the agent opens the red door and reaches the objective. A.2.2 DATASET DETAILS Our dataset consists of 100k training data points, 500 validation data points, and 300 data points for testing. For each data point, the structure of the prompt and response is as follows: 12 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning [Prompt]: maze size: agent: (xa0, ya0), walls: (x1, y1), (x2, y2), . . . objective: (xo, yo), keys: [red key]: (xrk, yrk), . . . doors: [red door]: (xrd, yrd), . . . [Response]: create-node (xa1, ya1, fa1, ha1), create-node (xa2, ya2, fa2, ha2), . . . agent (xa1, ya1), (xa2, ya2), . . . , (xaT , yaT ), Below, we show the prompt and response for an example training data pint. Prompt initial state: maze size: 19x19 wall: (0,0), (0,1), (0,2), (0,3), (0,4), (0,5), (0,6), (0,7), (0,8), (0,9), (0,10), (0,11), (0,12), (0,13), (0,14), (0,15), (0,16), (0,17), (0,18), (1,0), (1,6), (1,12), (1,18), (2,0), (2,6), (2,12), (2,18), (3,0), (3,6), (3,12), (3,18), (4,0), (4,6), (4,12), (4,18), (5,0), (5,6), (5,12), (5,18), (6,0), (6,1), (6,3), (6,4), (6,5), (6,6), (6,7), (6,8), (6,9), (6,10), (6,11), (6,12), (6,13), (6,14), (6,15), (6,16), (6,17), (6,18), (7,0), (7,12), (7,18), (8,0), (8,6), (8,12), (8,18), (9,0), (9,6), (9,12), (9,18), (10,0), (10,6), (10,12), (10,18), (11,0), (11,6), (11,12), (11,18), (12,0), (12,1), (12,2), (12,3), (12,4), (12,6), (12,8), (12,9), (12,10), (12,11), (12,12), (12,13), (12,14), (12,15), (12,16), (12,17), (12,18), (13,0), (13,12), (13,18), (14,0), (14,6), (14,12), (14,18), (15,0), (15,6), (15,12), (15,18), (16,0), (16,6), (16,12), (16,18), (17,0), (17,6), (17,12), (17,18), (18,0), (18,1), (18,2), (18,3), (18,4), (18,5), (18,6), (18,7), (18,8), (18,9), (18,10), (18,11), (18,12), (18,13), (18,14), (18,15), (18,16), (18,17), (18,18), agent: (11,5), objective: (1,1), [red key]: (16,1), [green key]: (16,9), [blue key]: (7,2), [red door]: (12,5), [green door]: (6,2), [blue door]: (12,7) Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning Response (CoT and Answer) create-node: (11,5), heuristics: 14, cost: 0 create-node: (11,4), heuristics: 13, cost: 1 create-node: (11,3), heuristics: 12, cost: 2 create-node: (10,3), heuristics: 11, cost: 3 create-node: (9,3), heuristics: 10, cost: 4 create-node: (9,2), heuristics: 9, cost: 5 create-node: (8,2), heuristics: 8, cost: 6 create-node: (7,2), heuristics: 7, cost: 7 create-node: (7,2), heuristics: 7, cost: 8 create-node: (7,3), heuristics: 8, cost: 9 create-node: (7,4), heuristics: 9, cost: 10 create-node: (7,5), heuristics: 10, cost: 11 create-node: (7,6), heuristics: 11, cost: 12 create-node: (7,7), heuristics: 12, cost: 13 create-node: (8,7), heuristics: 13, cost: 14 create-node: (9,7), heuristics: 14, cost: 15 create-node: (10,7), heuristics: 15, cost: 16 create-node: (11,7), heuristics: 16, cost: 17 create-node: (11,7), heuristics: 16, cost: 18 create-node: (12,7), heuristics: 17, cost: 19 create-node: (13,7), heuristics: 18, cost: 20 create-node: (14,7), heuristics: 19, cost: 21 create-node: (14,8), heuristics: 20, cost: 22 create-node: (15,8), heuristics: 21, cost: 23 create-node: (15,9), heuristics: 22, cost: 24 create-node: (16,9), heuristics: 23, cost: 25 create-node: (16,9), heuristics: 23, cost: 26 create-node: (16,8), heuristics: 22, cost: 27 create-node: (16,7), heuristics: 21, cost: 28 create-node: (15,7), heuristics: 20, cost: 29 create-node: (14,7), heuristics: 19, cost: 30 create-node: (13,7), heuristics: 18, cost: 31 create-node: (12,7), heuristics: 17, cost: 32 create-node: (11,7), heuristics: 16, cost: 33 create-node: (10,7), heuristics: 15, cost: 34 create-node: (9,7), heuristics: 14, cost: 35 create-node: (8,7), heuristics: 13, cost: 36 create-node: (7,7), heuristics: 12, cost: 37 create-node: (7,6), heuristics: 11, cost: 38 create-node: (7,5), heuristics: 10, cost: 39 create-node: (7,4), heuristics: 9, cost: 40 create-node: (7,3), heuristics: 8, cost: 41 create-node: (7,2), heuristics: 7, cost: 42 create-node: (7,2), heuristics: 7, cost: 43 create-node: (6,2), heuristics: 6, cost: 44 create-node: (5,2), heuristics: 5, cost: 45 create-node: (5,1), heuristics: 4, cost: 46 create-node: (4,1), heuristics: 3, cost: 47 create-node: (3,1), heuristics: 2, cost: 48 create-node: (2,1), heuristics: 1, cost: 49 create-node: (1,1), heuristics: 0, cost: 50 agent: (11,5), current key: none, remaining key: [red key]: [green key]: [blue key]: remaining door: [red door]: [green door]: [blue door]: agent: (11,4), current key: none, remaining key: [red key]: [green key]: [blue key]: remaining door: [red door]: [green door]: [blue door]: agent: (11,3), current key: none, remaining key: [red key]: [green key]: [blue key]: remaining door: [red door]: [green door]: [blue door]: agent: (10,3), current key: none, remaining key: [red key]: [green key]: [blue key]: remaining door: [red door]: [green door]: [blue door]: agent: (9,3), current key: none, remaining key: [red key]: [green key]: [blue key]: remaining door: [red door]: [green door]: [blue door]: agent: (9,2), current key: none, remaining key: [red key]: [green key]: [blue key]: remaining door: [red door]: [green door]: [blue door]: agent: (8,2), current key: none, remaining key: [red key]: [green key]: [blue key]: remaining door: [red door]: [green door]: [blue door]: agent: (7,2), current key: none, remaining key: [red key]: [green key]: [blue key]: remaining door: [red door]: [green door]: [blue door]: agent: (7,2), current key: [blue key], remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: [blue door]: agent: (7,3), current key: [blue key], remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: [blue door]: agent: (7,4), current key: [blue key], remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: [blue door]: agent: (7,5), current key: [blue key], remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: [blue door]: agent: (7,6), current key: [blue key], remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: [blue door]: agent: (7,7), current key: [blue key], remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: [blue door]: agent: (8,7), current key: [blue key], remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: [blue door]: agent: (9,7), current key: [blue key], remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: [blue door]: agent: (10,7), current key: [blue key], remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: [blue door]: agent: (11,7), current key: [blue key], remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: [blue door]: agent: (11,7), current key: none, remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: agent: (12,7), current key: none, remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: agent: (13,7), current key: none, remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: agent: (14,7), current key: none, remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: agent: (14,8), current key: none, remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: agent: (15,8), current key: none, remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: agent: (15,9), current key: none, remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: agent: (16,9), current key: none, remaining key: [red key]: [green key]: remaining door: [red door]: [green door]: agent: (16,9), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (16,8), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (16,7), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (15,7), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (14,7), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (13,7), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (12,7), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (11,7), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (10,7), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (9,7), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (8,7), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (7,7), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (7,6), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (7,5), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (7,4), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (7,3), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (7,2), current key: [green key], remaining key: [red key]: remaining door: [red door]: [green door]: agent: (7,2), current key: none, remaining key: [red key]: remaining door: [red door]: agent: (6,2), current key: none, remaining key: [red key]: remaining door: [red door]: agent: (5,2), current key: none, remaining key: [red key]: remaining door: [red door]: agent: (5,1), current key: none, remaining key: [red key]: remaining door: [red door]: agent: (4,1), current key: none, remaining key: [red key]: remaining door: [red door]: agent: (3,1), current key: none, remaining key: [red key]: remaining door: [red door]: agent: (2,1), current key: none, remaining key: [red key]: remaining door: [red door]: agent: (1,1), current key: none, remaining key: [red key]: remaining door: [red door]: The prompt describes the maze in structured language. The maze size = m(n + 1) + 1 (e.g., in Figure A.1, the maze size = 19). The positions of walls are (x1, y1), (x2, y2), . . ., and so on. The position of the agent in time step is (xat, yat), where = 0 corresponds to the initial position The position of the objective is (xo, yo), and the position of keys and doors in color (where = r, g, b) are (xck, yck) and (xcd, ycd), respectively. The response describes an optimal path (i.e., with minimal total times steps ) for the agent to reach the objective. A.2.3 MODEL DETAILS Following Su et al. (2024); Lehnert et al. (2024), we employ similar encode-decoder transformer architecture with rotary embeddings and no drop-out. Our model consisted of 6 layers with 3 attention heads, and the embedding size is 64. A.3 ProntoQA and ProsQA We used the pretrained GPT-2 model which has the following parameters: Parameter Number of Layers (Transformer Blocks) Hidden Size (Embedding Size) Number of Attention Heads Vocabulary Size Total Number of Parameters Value 12 768 12 50,257 117 million Table A.1: Hyperparameters of the pretrained GPT-2 model used for ProntoQA and ProsQA. 14 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning A.4 LLM experiments We use the Llama Cookbook3 codebase to fine-tune the Llama models. As described in Section 4.2, we use batch size of 32 with sequence packing of 4096. We experiment with different learning rates 105, 2.5 105, 5 105, 104 and select the one with the lowest validation error. The final choices are 105 for Llama-3.2-8B and 2.5 105 for Llama-3.2-1B and Llama-3.2-3B."
        },
        {
            "title": "B Notations",
            "content": "Table B.1 summarizes the notations we used throughout the paper. = pi ci si zi = tc/tz (cid:101)X ei fenc() = x1, . . . , q() g() fdec(, ) input text sample where means concatenation prompt of length tp the i-th token of prompt (in text) reasoning trace of length tc the i-th token of trace (in text) solution of length ts the i-th token of solution (in text) the complete latent reasoning traces of length tz the i-th token of latent trace compression rate number of trace tokens to be replaced by latent tokens during training modified input with mixed text and latent tokens codebook of VQ-VAE the i-th vector in the codebook, which corresponds to the i-th latent token dimension of eis vocabulary of text tokens chunk size encodes chunk of text tokens to embedding vectors of outputted by fenc() quantization operator that replaces, e.g., x1 by its nearest neighbor in E: embedding vectors g(x1) = argmineiE ei x12 maps prompt to d-dimensional embedding vector decodes L/r quantized embedding vectors in back to text tokens, conditioning on prompt embedding generated by g() Table B.1: Mathematical notations used throughout the paper. 3https://github.com/meta-llama/llama-cookbook 15 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning"
        },
        {
            "title": "C Details of Attention Weights Analysis",
            "content": "C.1 Generated Responses Question 1 Prompt: What is the positive difference between $120%$ of 30 and $130%$ of 20? Response of Our Latent Approach: <boLatent><Latent-code-278> <Latent-code-416><eoLatent> % of 30 and $130%$ of 20. First, lets calculate $120%$ of 30: $120%$ of 30 = 1.2 30 = 36. Next, lets calculate $130%$ of 20: $130%$ of 20 = 1.3 20 = 26. Now, lets find the positive difference between 36 and 26: 36 - 26 = 10 The answer is 10. Response of Baseline (CoT): To find the positive difference between 120% of 30 and 130% of 20, we need to calculate each value first. 120% of 30 is calculated as 1.2 * 30 = 36. 130% of 20 is calculated as 1.3 * 20 = 26. Now, we need to find the positive difference between these two values. To do this, we subtract the smaller value from the larger value: 36 - 26 = 10. The final answer is 10. Question Prompt: Mark has $50 in his bank account. He earns $10 per day at his work. If he wants to buy bike that costs $300, how many days does Mark have to save his money? Response of Our Latent Approach: <boLatent> <Latent-code-123> <eoLatent> need to calculate how much more money he needs to buy the bike and then divide that amount by his daily earnings. The bike costs $300, and Mark already has $50. So, he needs: $300 (bike cost) - $50 (current savings) = $250 (more money needed) Mark earns $10 per day. To find out how many days he needs to save, we divide the amount he needs by his daily earnings: $250 (more money needed) / $10 (daily earnings) = 25 days. The answer is 25. Response of Baseline (CoT): To find out how many days Mark needs to save his money, we need to subtract the cost of the bike from the amount of money he has, then divide the result by the amount he earns per day. Mark has $50 and wants to buy bike that costs $300. First, subtract the cost of the bike from the amount of money Mark has: $50 - $300 = -$250 Since Mark cant have negative amount of money, we need to convert the negative amount to positive one by adding the cost of the bike to the amount of money Mark has: $50 + $300 = $350 Now, divide the result by the amount Mark earns per day: $350 / $10 = 35 The final answer is 35. 16 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning"
        },
        {
            "title": "Example Generation of Our Latent Approach",
            "content": "Prompt: Evaluate the expression a3 a2 if = 5 Generated Text: <boLatent><Latent-code-130><eoLatent> We can use the rule of exponents that states aman = am+n. So, a3 a2 = a3+2 = a5. Now, we substitute = 5 into the expression: 55. 55 = 5 5 5 5 5 = 3125. The answer is 3125."
        },
        {
            "title": "Example Generation of Our Latent Approach",
            "content": "Prompt: Compute (cid:19) (cid:18)15 2 . Generated Text: <boLatent> <Latent-code-888> <Latent-code-744> <Latent-code-196> <Latent-code-92> <Latent-code-557> <eoLatent> 2!13! To simplify this expression, we can cancel out the common factors in the numerator and denominator: = 1514 2 = 105. The answer is 105. 2!(152)! = 15! (cid:19) 15! (cid:18)15 2 Example Generation of Our Latent Approach Question: How many vertical asymptotes does the graph of = 2 x2+x6 have? Answer: <boLatent><Latent-code-773> <Latent-code-514> <eoLatent> the denominator, which is x2 + 6. We can factor this quadratic expression as (x + 3)(x 2). Now, we can see that the denominator will be equal to zero when either (x + 3) = 0 or (x 2) = 0. Solving for these values, we get = 3 or = 2. Therefore, the vertical asymptotes of the graph of = rac2x2 + 6 are = 3 and = 2. Since there are two distinct values of that make the denominator equal to zero, the graph has 2 vertical asymptotes. The answer is 2. Example Generation of Our Latent Approach Question: If there exists an ordered numbering of the nodes such that for each node there are no links going to lower-numbered node, then there are no directed cycles in directed graph. True or false? Answer: <boLatent><Latent-code-526> <eoLatent> directed graph. directed cycle is sequence of nodes in graph, where each node is connected to the next node by directed edge, and the last node is connected back to the first node by directed edge. Now, lets consider the given condition: If there exists an ordered numbering of the nodes such that for each node there are no links going to lower-numbered node, then there are no directed cycles in directed graph. This condition essentially means that if we number the nodes in specific way, and there are no links going to lower-numbered node, then there are no directed cycles in the graph. To see why this is true, lets consider what would happen if there were directed cycle in the graph. If there were directed cycle, then there would be sequence of nodes where each node is connected to the next node by directed edge, and the last node is connected back to the first node by directed edge. However, if we number the nodes in the way described, and there are no links going to lower-numbered node, then this directed cycle would be impossible. This is because the last node in the cycle would have to be connected to lower-numbered node, which would violate the condition. Therefore, if we number the nodes in this way, then there can be no directed cycles in the graph. The answer is True. 17 Token Assorted: Mixing Latent and Text Tokens for Improved Language Model Reasoning"
        },
        {
            "title": "E Additional Experiments",
            "content": "We present results of different approaches for fine-tuning Llama-3.1-8B model on the DART-MATH (Tong et al., 2024) dataset. The observations are similar to those we presented in Section 4. Model (Dart-Math) In-Domain Out-of-Domain Average math GSM8K Fresh-Gaokao-Math-2023 DeepMind-Mathematics College-Math Olympia-Math TheoremQA All Datasets Llama-3.1-8B Sol-only CoT iCoT Latent (Ours) 13.3 43.1 35.2 43.2 ( +0.1) 16.4 84.5 61.8 83.9 0.0 30.7 30.0 33.3 ( +2.6) 18.2 47.8 30.6 44.7 15.9 45.7 37.6 47.1 ( +1.4) 4.7 10.1 8.3 13.3 ( +3.2) 16.9 21.2 19.5 20.3 12.2 40.4 31.8 40.8 ( +0.4) Table E.1: Our approach surpasses the iCoT and Sol-Only baseline when trained on the DART-MATH dataset (Tong et al., 2024), while marginally outperforming the CoT baseline. Model (Dart-Math) In-Domain (# of tokens) Out-of-Domain (# of tokens) Average (# of tokens) math GSM8K Fresh-Gaokao-Math-2023 DeepMind-Mathematics College-Math Olympia-Math TheoremQA All Datasets Llama-3.1-8B Sol-only CoT iCoT Latent (Ours) 10.9 522.7 397.1 489.1( -6.4%) 8.1 181.0 118.6 163.5 ( -9.7%) 10.2 628.8 440.8 462.1( -26.5%) 8.4 343.2 227.9 265.6 ( -22.6%) 11.2 486.3 321.9 396.3 ( -18.5%) 16.1 893.7 614.4 801.3 ( -10.3%) 16.13 648.3 485.7 591. 11.6 529.1 372.3 452.7 ( -16%) Table E.2: The average number of tokens in the generated responses. Our approach generates shorter reasoning traces then the CoT baseline. -: Trace length reduction rate compared with CoT."
        }
    ],
    "affiliations": [
        "Meta AI",
        "UC Berkeley",
        "UCL"
    ]
}