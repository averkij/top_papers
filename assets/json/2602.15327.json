{
    "paper_title": "Prescriptive Scaling Reveals the Evolution of Language Model Capabilities",
    "authors": [
        "Hanlin Zhang",
        "Jikai Jin",
        "Vasilis Syrgkanis",
        "Sham Kakade"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given a pre training compute budget, what downstream accuracy is attainable with contemporary post training practice, and how stable is that mapping as the field evolves? Using large scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundaries, high conditional quantiles of benchmark scores as a function of log pre training FLOPs, via smoothed quantile regression with a monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits a consistently advancing boundary over time. We then extend our approach to analyze task dependent saturation and to probe contamination related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near full data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus 2k, the latest model performance evaluation dataset, and introduces a practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time."
        },
        {
            "title": "Start",
            "content": "February 18, 2026 6 2 0 2 7 1 ] . [ 1 7 2 3 5 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Prescriptive Scaling Reveals the Evolution of\nLanguage Model Capabilities",
            "content": "Hanlin Zhang1, Jikai Jin2, Vasilis Syrgkanis2, and Sham Kakade1 1Harvard University 2Stanford University Abstract For deploying foundation models, practitioners increasingly need prescriptive scaling laws: given pretraining compute budget, what downstream accuracy is attainable with contemporary post-training practice, and how stable is that mapping as the field evolves? Using large-scale observational evaluations with 5k observational and 2k newly sampled data on model performance, we estimate capability boundarieshigh conditional quantiles of benchmark scores as function of log pre-training FLOPs, via smoothed quantile regression with monotone, saturating sigmoid parameterization. We validate the temporal reliability by fitting on earlier model generations and evaluating on later releases. Across various tasks, the estimated boundaries are mostly stable, with the exception of math reasoning that exhibits consistently advancing boundary over time. We then extend our approach to analyze task-dependent saturation and to probe contamination-related shifts on math reasoning tasks. Finally, we introduce an efficient algorithm that recovers near-full-data frontiers using roughly 20% of evaluation budget. Together, our work releases the Proteus-2k, the latest model performance evaluation dataset, and introduces practical methodology for translating compute budgets into reliable performance expectations and for monitoring when capability boundaries shift across time. (cid:18) Blog ı Datasets Code"
        },
        {
            "title": "1 Introduction",
            "content": "Over the past several years, language model (LM) scaling has emerged as one of the most robust empirical laws in modern machine learning (Hestness et al., 2017). Across model families and training regimes, increasing pre-training compute has been shown to produce smooth and predictable improvements in loss, perplexity, and, to lesser extent, downstream task performance (Brown et al., 2020; Chowdhery et al., 2023; Gadre et al., 2024; Hoffmann et al., 2022). This observation has driven paradigm in which scale itself becomes primary design variable, enabling practitioners to trade off data, model size, and compute in principled way (Hoffmann et al., 2022; Kaplan et al., 2020). As language models transition from research artifacts to deployed systems, the limitations of existing scaling laws have become increasingly pronounced. Despite the success of scaling laws, they do not answer one question that practitioners can usually face: given fixed pre-training compute budget C, what downstream performance can one realistically expect to achieve with high probability after post-training? While average trends with respect to compute are sometimes stable, downstream behaviors of interest (such as reasoning performance, instruction following, or domain-specific Equal Contribution Benchmark IFEval BBH MATH Lvl 5 GPQA MUSR MMLU-PRO Acc. @ 1024 FLOPs 0.828 0.700 0.539 0.424 0.535 0. Table 1. Estimated attainable accuracies predicted by the no-split 0.98-quantile sigmoid boundaries at 1024 FLOPs. question answering) exhibit substantial heterogeneity even among models trained with similar FLOPs (Jin et al., 2025). Post-training procedures (Ziegler et al., 2019), data curation choices (Setlur et al., 2024), and temporal effects (Dominguez-Olmedo et al., 2024) further complicate the relationship between pre-training compute and deployed performance, weakening the direct applicability of standard scaling laws for real-world decision making. Recent work has highlighted this gap from multiple perspectives: downstream benchmark scaling can be noisy, benchmark-dependent, and weakly coupled to pre-training loss, in part due to heterogeneous training factors (e.g., data mixture, architectures, and evaluation artifacts) and the disconnection between loss and downstream accuracy (Chen et al., 2024; Gadre et al., 2024; Lourie et al., 2025; Qi et al., 2025; Schaeffer et al., 2024; Zhang et al., 2025a). At the same time, the rapid growth of public evaluation repositoriesespecially leaderboards that aggregate thousands of post-trained checkpointsmakes it increasingly feasible to study these relationships empirically from observational data. In this paper, we study prescriptive scaling: given base-model pre-training compute budget, what attainable post-training performance should we expect on target benchmark? Rather than modeling only mean trends, we summarize the attainable region with capability boundaries: for each task we estimate high conditional quantile of observed post-trained accuracy as function of log pretraining compute (Koenker and Bassett, 1978). This framing is robust to outliers and recipe-specific variation, and it yields an end-to-end, decision-oriented compute-to-performance map from large collections of heterogeneous checkpoints. Crucially, we treat time as first-class axis: by fitting boundaries on earlier model generations and validating on later ones, we can gain knowledge of whether compute-based boundary remains predictive as training recipes and post-training techniques evolve. We rely on three complementary data sources: (i) the Open LLM Leaderboard v1 (Beeching et al., 2023) and v2 (Fourrier et al., 2024), each containing thousands of models evaluated on six benchmarks under consistent metrics, (ii) public leaderboards for state-of-the-art frontier models (e.g., Epoch AI and LifeArchitect.AI), and (iii) newly added 2.4k open-weight models (PROTEUS-2K) focusing on releases after the Open LLM Leaderboard v2 cutoff (2025-03-13) until the end of 2025, that we evaluate ourselves (including new model families of Qwen3 (Yang et al., 2025), Gemma-3 (Team et al., 2025), GPT-OSS (Agarwal et al., 2025)), following the same Open LLM Leaderboard pipeline. Together, these sources provide both breadth (many heterogeneous post-training pipelines) and basis for assessing temporal validity (Dominguez-Olmedo et al., 2024). The main contributions are summarized below: Sigmoid capability boundaries: Compared with pre-trained model performance, we show that the attainable post-trained performance is much more predictable and is well-characterized by simple monotone, saturating sigmoid function of log-compute. Temporal validity and task-dependent ceilings: Using chronological train/validation splits, we find that capability boundaries for majority of tasks are comparatively stable over time, yielding nearly deterministic relationship between compute and attainable accuracies, while math reasoning exhibits consistently improving boundary. As an illustration, Table 1 provides estimated attainable accuracies (0.98-quantile sigmoid boundaries) at budget of 1024 FLOPs. 2 Case studies: saturation and contamination: We apply prescriptive scaling to revisit two evaluation issues. Our saturation analysis suggests two qualitatively different limits to scaling: some tasks quickly hit stable, size-determined ceiling, while others (notably math) exhibit an evolving ceiling over time. Our contamination analysis on frontier models finds no clear evidence of AIME-2025 score inflation due to contamination. Efficient prescriptive scaling via adaptive sampling: We propose sampling algorithm that accurately recovers sigmoid capability boundaries under limited computation budget (typically 20% of the full, parameter-count-weighted evaluation budget; and 5% on some tasks)."
        },
        {
            "title": "2 Estimation of Post-training Capability Boundaries",
            "content": "Recent frontier-model reports (Achiam et al., 2023) emphasize an engineering goal of predictable scaling: using compute as controllable input to forecast key training statistics and downstream benchmark behavior from smaller-scale runs, so that model development can be budgeted and planned in advance. Adopting this perspective, we use the term Prescriptive Scaling to denote the prescriptive question at the center of this paper: how pre-training FLOPs budget translates into the range of targeted downstream performance attainable after standard post-training. Definition 1 (Prescriptive Scaling). Given certain budget of FLOPs C, the goal of Prescriptive Scaling is with performance y. to train from scratch model end-to-end to exhibit targeted behaviors or properties (Here for simplicity, we treat specific benchmark scores as targets.) Prescriptive Scaling, in the sense of budgeting training compute to reach desired downstream behavior, is ultimately an engineering question: given resource constraint, what performance can one reliably attain with contemporary training and post-training practice? For language models, the most consistently reported and directly controllable resource is pre-training compute. At the same time, deployed models are rarely raw checkpoints: they are produced by heterogeneous post-training pipelines (instruction tuning, RL, domain adaptation), and their benchmark scores exhibit substantial variance even at similar compute (Jiang et al., 2025; Jin et al., 2025; Zhang et al., 2025b). To connect this broad engineering for predictability goal to measurable evidence, we narrow the problem to estimating capability boundaries: for each task, we ask how high the performance distribution of post-trained models reaches as function of the base models pre-training FLOPs. This abstraction does not claim compute is the only driver. Rather, we treat compute as practical design coordinate. The estimated boundaries constitute empirical envelopes conditioned on the prevailing post-training methodologies, data curation practices, and evaluation protocols within the observed model ecosystem, thereby enabling the translation of target accuracy level into plausible range of computational requirements in data-driven manner."
        },
        {
            "title": "2.1 Setting and Modeling Assumptions",
            "content": "For each task, we collect evaluation results for set of post-trained model. Each observation is model paired with (i) an estimated base-model pre-training compute budget Ci > 0 (FLOPs) and (ii) an observed score yi [0, 1]. Multiple models can share the same Ci when they are derived from the same base model. We work in log-compute zi = log10 Ci. When assessing temporal and fit on one period at generalization, we further group observations into chronological periods time. Motivated by the practical need for setting computational budget for targeted accuracy, we treat base-model pre-training compute as the primary conditioning variable for attainable post-training capability. We are interested in the mapping (cid:55) attainable (upper-tail) accuracy of post-trained models with log-pretraining-compute z, 3 Qτ (Y and we refer to this mapping (at fixed quantile level τ ) as capability boundary. One major challenge towards this goal is that outliers are ubiquitous across all model families.1 So, rather than studying the genuine maximal accuracy that we observe from evaluation results, we instead focus on recovering qτ (z) = z), the conditional τ -quantile of the observed ) should be read as an empirical accuracy given log-pretraining-compute = z. Note that qτ ( attainable boundary for the observed model population. As with any observational study, if an underrepresented model family or recipe class consistently achieves higher scores at fixed compute, then the true attainable boundary could lie above our estimate; conversely, the main use case of prescriptive scaling is to provide conservative, decision-oriented compute-to-performance map that can be updated as new families and recipes enter the evaluation ecosystem. To estimate qτ (z), we approximate it with parameterized estimator qτ (z; θ) where θ is learnable parameter. Define ˆyi = qτ (zi; θ) and minimize smoothed pinball loss, standard objective for quantile regression (Koenker, 2005; Narayan et al., 2024; Steinwart and Christmann, 2011): ℓτ (yi ˆyi) + λ Ω(θ), (cid:88) (θ) = ℓτ (u) = iPt κ log(1 + eκu) + (τ 1)u. We use τ = 0.98, κ = 50, λ = 103. Sensitivity analyses are performed in Section G."
        },
        {
            "title": "2.2 Capability Boundary Estimators",
            "content": "For each task and training period, we fit function qτ (z) intended to approximate the conditional τ -quantile Qτ (Y = z). We compare the following classes: Constant (no-compute) baseline: qconst Binwise constant: partition into bins with edges e0 < τ (z) = c, single scalar for all z. z-values only. Predict qbin (z) = cb for Sigmoid: monotone saturating function in z, in the form of qsig 1, and 0 0, 0 1 τ 1+et , with β y0. y0 < eB computed from the training [eb, eb+1) (last bin inclusive), with = 0, . . . , 1. τ (z; θ) = y0 + σ(a + βz), σ(t) = I-spline: strictly more general function class than sigmoid (a flexible monotone baseline), where we replace the linear predictor + βz with monotone spline and pass it through sigmoid so predictions remain saturating in [0, 1]. (Full definition and constraints are given in Appendix Section B.3.) Bin construction for the binwise model. We use group-aware equal-mass binning on the training z-values only, never splitting identical values across bins. The full boundary-placement and minimum-bin-size merging procedure is provided in Appendix Section B.2; the resulting edges e0 < < eB are used for both training and evaluation."
        },
        {
            "title": "2.3 Evaluation Metrics",
            "content": "We report two complementary error metrics: 1. Pinball loss (quantile accuracy). We evaluate the mean smoothed pinball loss on both train and OOD validation periods. This is proper scoring rule for quantiles in the unsmoothed limit and directly reflects how well qτ (z) targets the τ -quantile under asymmetric penalties (underprediction is penalized more heavily when τ is close to 1). The main limitation is that as scalar 1See Section B.1 for concrete illustrative example and brief discussion. 4 aggregate, it can hide where errors occur (e.g., at low vs high compute) and its sign (underestimate vs. overestimate), which motivates us to include an extra coverage metric. 2. Coverage error. Within each log-compute bin [eb, eb+1), let . We compute empirical coverage ˆτb = 1 τ . nb { This measures whether the fitted capability boundary achieves the intended quantile coverage locally in compute. and nb = and report the signed deviation ˆτb [eb, eb+1) iIb 1 : zi = (cid:80) yi ˆyi { } }"
        },
        {
            "title": "3 Sigmoid Scaling Laws for Post-training Performance Boundaries",
            "content": "t and (ii) the boundary fit on t+1) for = 1, 2, 3, visualizing both (i) the boundary fit on Figure 1. Sigmoid capability boundaries across time. In each subfigure, points correspond to post-trained models (x-axis: base-model pre-training compute; y-axis: benchmark score). We compare sigmoid fits across consecutive periods t, ( t+1 to illustrate boundary shift. In this section, we apply the methodology from the previous section to characterize post-training capability boundaries in several settings. Each observation is post-trained model checkpoint paired with the pre-training compute of its base model and an observed benchmark score (see Section 2.1). We begin with open-weight models from the Open LLM Leaderboard (Section 3.1). We then use additional slices of the model landscape to probe external validity and robustness of the fitted boundaries. Finally, we connect these post-training boundaries to the classical pre-training scaling-law perspective by comparing official pretrained base models against the fitted post-training envelope (Section 3.2). P"
        },
        {
            "title": "3.1 Cross-temporal Scaling of Open-weight Models",
            "content": "t, In this subsection, we study open-weight models on the Open LLM Leaderboard. To stress-test temporal generalization, we partition all models into four chronological evaluation periods P4 (date ranges and counts in Appendix Section D). We then evaluate three rolling traintest pairs : for each t, we fit the τ -capability boundary on 1, 2, 3 t+1) for ( and evaluate out-ofP } t+1, restricting evaluation to the overlap of the train and OOD ranges in to avoid distribution on extrapolation. We focus on two questions: which function class best captures the observed computeperformance boundary, and how the fitted boundaries drift over time. Prior work has explored alternative P1, . . . , { P 5 function classes primarily for pre-training scaling laws (Caballero et al., 2023; Donoway et al., 2025); here we evaluate these alternatives in the post-training regime. On the other hand, while temporal effects can inflate pretrained models benchmark scores (Dominguez-Olmedo et al., 2024), large-scale study that incorporates post-trained models is still lacking."
        },
        {
            "title": "3.1.1 The Shape of Capability Boundary",
            "content": "In Section 2.2 we discussed several candidate estimators for the τ -capability boundary. Table 2 reports the in-distribution (ID) and out-of-distribution (OOD) performance of these estimators. Among the function families considered, Sigmoid performs competitively (normalization details in Appendix Section D), matching the more flexible I-spline in ID pinball loss and achieving better OOD calibration. Given its strong generalization and simplicity, we use Sigmoid as the default boundary class in the remainder of the paper. finding 1. Post-training capability boundaries are approximately sigmoid functions of the log-compute."
        },
        {
            "title": "3.1.2 Temporal Stability of the Sigmoid Capability Boundary",
            "content": "We summarize cross-temporal transfer using two diagnostics from Section 2.3: (i) signed coverage τ ) and (ii) out-of-distribution pinball loss ρτ . Negative coverage error indicates undererror (ˆτ coverage (newer models exceed the predicted τ -boundary more than intended), while positive values indicate over-coverage. Pinball loss Calibration error Estimator ID Constant Binwise I-spline Sigmoid 5.35 4.01 4.00 4.08 103 103 103 103 OOD 6.23 5.00 4.92 4. 103 103 103 103 ID 4.12 1.66 1.83 1.84 102 102 102 10 OOD 3.60 2.81 2.41 2.21 102 102 102 102 Table 2. Results averaged over rolling splits = 1, 2, 3 and tasks. Values are absolute pinball loss/calibration error. Figure 2. Temporal drift and the stability of knowledge-intensive capabilities. Left: coverage error ˆτ loss ρτ . Both fit on and evaluate on t+1. τ . Right: pinball 6 Figure 3. Pre-training vs. post-training scaling laws. Panels (a) and (b) compare capability boundaries for pretrained and post-trained models. Panel (c) compares how frequently pretrained accuracies and post-trained capability boundaries violate monotonicity in compute. Figure 4. MATH LVL 5: evaluation on newly released open-weight models. (a) and (b): fitted sigmoid capability . (c) and (d): boundaries on leaderboard models (red) and newly evaluated models (blue) in periods { on PROTEUS-2K, fitted capability boundary on leaderboard models in period 4 (red) and on models released after the retirement of the Open LLM Leaderboard. (c) contains models from old base model families (i.e., base models that already exist in the leaderboard), while (d) contains new model families. for 3, 4 P } Figure 2 shows that for BBH, GPQA, MMLU-PRO, and MUSR, both diagnostics are stable across periods, indicating that compute-only Sigmoid boundary transfers reliably to the next generation of open-weight models. The main departures occur on MATH LVL 5 (and to lesser extent IFEVAL), where we observe under-coverage and elevated ρτ on the earliest split, consistent with nonstationarity of the effective boundary over time. Bin-wise breakdowns underlying these aggregates are deferred to Section D.2. Remark 1. qualitatively new recipe class or architecture family could shift the attainable boundary upward at fixed compute. Under our framework this would manifest as systematic under-coverage in the next-period evaluation, motivating an updated boundary fit."
        },
        {
            "title": "3.2 From Pre-training Scaling Laws to Post-training Capability Boundaries",
            "content": "Pre-training scaling laws (Hoffmann et al., 2022; Kaplan et al., 2020) relate training compute to model quality (e.g., benchmark accuracy) under controlled training recipes. In practice, deployed systems are almost always post-trained (instruction tuning, preference learning, domain adaptation), and the observed benchmark landscape reflects heterogeneous post-training pipelines and evaluation artifacts (Ouyang et al., 2022). Our τ -capability boundary instead estimates high-quantile level of performance that is attainable after post-training among models built on given amount of compute. 7 To connect this view back to the pre-training scaling-law perspective, we compare the official pretrained models (i.e., non-instruction-tuned base models) against the post-trained τ -boundary in Figure 3. Two takeaways emerge. finding 2. The pretrainpost-train gap is task dependent. On knowledge-intensive benchmarks (e.g., MMLU-PRO), pretrained models lie comparatively close to the post-trained capability boundary at the same compute. In contrast, on reasoningand instruction-following benchmarks (e.g., MATH LVL 5 and IFEVAL) pretrained models sit substantially below the post-trained boundary. This qualitative pattern aligns with prior work (Wu et al., 2025); here we quantify it at scale across diverse model families and heterogeneous post-training techniques. From Figure 3 (c) we can see that among pretrained models, performance is more often locally nonmonotone in compute: larger-compute base models can score lower than smaller ones. In contrast, for post-trained models the attainable upper envelope is much more consistently monotone in compute, aligning with our monotone boundary fits. finding 3. Compute predicts potential much more reliably than raw pretrained accuracies. Increased pre-training compute is more reliably reflected in best-achievable post-training performance than in raw pretrained performance, which depends strongly on modeling choices. Appendix Section provides additional results comparing pretrained and post-trained models downstream performance. Remark 2. Motivated by recent findings that multi-benchmark performance is often well-approximated by low-dimensional latent capability factors (Jin et al., 2025; Ruan et al., 2024), we also run our Sigmoid τ -capability-boundary estimator on PCA-derived factors. In our setting, the top three PCs explain 95% of the total variance. Surprisingly, only the first exhibits clear monotone increase with pre-training compute, suggesting that compute-driven progress concentrates primarily along single dominant latent axis. Details can be found in Appendix Section H.3."
        },
        {
            "title": "3.3 External Validity: Newly Evaluated Models on PROTEUS-2K",
            "content": "The Open LLM Leaderboard is not exhaustive: many models are never added, and new releases arrive after the leaderboards retirement. To examine external validity beyond this curated subset, we evaluate additional open-weight models that are absent from the leaderboard, including models released before and after its retirement. Across tasks, the leaderboard-fitted boundary continues to upper-bound the best observed performance in this held-out set, with the main exceptions occurring for MATH LVL 5, consistent with the temporal non-stationarity highlighted earlier. As shown in Figure 4, on MATH LVL 5, the capability boundary advances primarily at the highcompute end, with little evidence of systematic uplift at smaller compute. Appendix Section reports results for the remaining tasks."
        },
        {
            "title": "4.1 Budget-Constrained Balanced I-Optimal Design",
            "content": "Evaluating every model across all tasks would give the most accurate estimate of the performance boundary, but is often prohibitively expensive. We therefore study how to select only subset of models to evaluate under hard evaluation budget, while still reliably recovering well-calibrated boundary. In this section, we introduce an efficient approach to achieve this, motivated by the optimal experimental design literature (de Aguiar et al., 1995; Goos and Jones, 2011; Goos et al., 8 2016; Smucker et al., 2018). The I-optimal design allocates evaluations to minimize the average predictive variance of the estimated capability boundary across compute regimes (Pukelsheim, 2006). Intuitively, it concentrates budget on the most informative models so the fitted sigmoid frontier has uniformly low uncertainty rather than low error at few points. Cost and budget. For each model i, let ci be its parameter count. We assume evaluation cost denote the set of candidate models in training period grows roughly linearly with model size. Let and Ct = (cid:80) [0, 100], we allocate per-period budget Ut = α Uk that yields accurate OOD predictions for the next period. iPt ci the total cost of evaluating all of them. Given user-chosen α 100 Ct, and seek subset St with (cid:80) iSk ci Sigmoid boundary and information matrix. Recall that the τ -quantile performance boundary is modeled as sigmoid function of log-compute = log10(FLOPs), qτ (z; θ) = y0 + σ(a + bz), with parameters θ = (y0, L, a, b) and σ(t) = 1/(1 + et). Let j(z; θ) = (cid:2)1, σ, Lσ(1 σ)z(cid:3) denote the Jacobian of qτ with respect to θ as column vector, where we write σ = σ(a + bz) for brevity, and we evaluate it at nominal parameter θ0 obtained from an initial estimate. For selected set we define the local information matrix σ), Lσ(1 (S) = (cid:88) iS j(zi; θ0)j(zi; θ0). For numerical stability, we use ridge prior M0 = ηI with η = 109, and let Σθ(S) (M0 +M (S))1. We consider the bin partition introduced in Section 2.2 and let zb denote the midpoint of bin b. The delta method gives an approximate predictive variance of the boundary at zb, We then define the I-optimal predictive-variance objective vb(S) j(zb; θ0) Σθ(S) j(zb; θ0). Φinfo(S) = (cid:88) b=1 wb vb(S), (1) where we use uniform bin weights wb = 1/B. In other words, Φinfo characterizes the average predictive variance across bin midpoints. Maximizing Φinfo is therefore equivalent to minimizing this average predictive variance. Figure 5. Performance of balanced I-optimal design as function of budget parameter α, averaged over = 1, 2, 3. 9 Bin-balanced design. To explicitly encourage coverage of all compute regimes, we augment the objective with bin-balance term. Let b(i) { (cid:12) (cid:12) denote the number of selected models in bin b. We define : b(i) = } be the bin index of model and nb(S) = 1, . . . , (cid:12) (cid:12) { } Φbal(S) = (cid:88) b=1 log(cid:0)nb(S) + ε(cid:1), (2) where ε > 0 is small constant. This imposes preference for designs that distribute models more evenly across bins. Balanced I-optimal objective. Our final design criterion for period combines the predictivevariance and bin-balance terms: Φλ(St) = Φinfo(St) + λ Φbal(St), s.t. (cid:88) iSt ci Ut, 0 trades off boundary uncertainty against bin coverage. Setting λ = 0 recovers standard ) under the budget constraint using standard greedy t. This procedure only uses model metadata (zi, ci) and the where λ I-optimality. We approximately maximize Φλ( gain-per-cost heuristic over models in local Jacobian. Details can be found in Section J. Empirical behavior. Figure 5 shows average OOD performance on period + 1 for as function of the budget parameter α. Across periods and tasks, error decreases rapidly as α increases and stabilizes between 20% and 50%. In particular, for GPQA and MUSR we obtain near-identical boundary estimates to the full-data fit using as little as α = 5% of the evaluation budget. finding 4. Efficient prescriptive scaling. Our proposed balanced design can recover well-calibrated sigmoid boundary with substantial savings in evaluation cost. 1, 2, 3 } { P"
        },
        {
            "title": "5 Case Studies: Saturation and Contamination Diagnostics",
            "content": "Up to now, our prescriptive scaling framework has focused on estimating capability boundaries as function of pre-training compute. In modern evaluation pipelines, however, two additional issues are central: (i) task-dependent saturation narratives on public leaderboards, where the relationship between scale and scores evolves over time, and (ii) time-dependent evaluation artifacts, including contamination and training on the test task (Dominguez-Olmedo et al., 2024). In this section, we show how capability-boundary estimation yields quantitative diagnostics for both, and we additionally use frontier-model leaderboards to probe external validity beyond open-weight models."
        },
        {
            "title": "5.1 Task-dependent Saturation on Leaderboards",
            "content": "Benchmark saturationwhen static test sets lose headroom and discriminative power as models improveis recurring theme in evaluation: rapid ceiling effects on earlier leaderboards have motivated harder successor suites (e.g., SuperGLUE after GLUE) (Wang et al., 2019), while dynamic benchmarking frameworks explicitly anticipate and mitigate saturation by iteratively refreshing evaluation data (Kiela et al., 2021). At the ecosystem level, large-scale analyses show that nearsaturation emerges quickly for many benchmarks across vision and NLP, suggesting that tracking how the attainable envelope evolves can be as important as reporting absolute scores (Ott et al., 2022). This pressure has spurred reset\" benchmarks intended to restore headroom (e.g., MMLU-Pro (Wang et al., 2024) and Humanitys Last Exam (Phan et al., 2025)) as well as rolling evaluations designed to remain challenging under rapid capability growth (White et al., 2024). 10 (a) MMLU-Pro (knowledge): weaker saturation. (b) MATH Lvl 5 (reasoning): stronger saturation. Figure 6. Task-dependent saturation on Open LLM Leaderboard v2. Dominated large marks large (> 13B) models whose task score is below the cumulative best score achieved by small models up to that date (the small-model boundary). MMLU-Pro appears less saturated than MATH Lvl 5. In this subsection, we revisit the saturation problem with focus on its dependency on model size. Concretely, we are interested in the following question: does saturation occur simply because people are building larger models? Hooker (2025) argues that the relationship between scale and capability is becoming less predictable, and that bigger does not reliably imply better on open benchmarks. Our replication on Open LLM Leaderboard v2 produces deeper, task-dependent narrative of this phenomenon. Figure 6 contrasts mostly knowledge-based benchmark with pure reasoning benchmark. These saturation diagnostics are parameter-count centric and are intended as complementary lens to our compute-based boundaries: they summarize whether small models quickly reach their attainable boundary on benchmark and whether larger models retain persistent advantage."
        },
        {
            "title": "5.1.1 Quantifying Saturation with a Size–Time Boundary Model",
            "content": "To quantify how model size shifts the attainable performance on benchmark while accounting for time-dependent effects, we fit simple sizetime boundary model. For benchmark b, let qb τ (x, t) := Qτ (Yb = x, = t) denote the attainable τ -quantile score at size and time t. We model logit(qb τ (x, t)) as logit(cid:0)qb τ (xi, ti)(cid:1) = αb + βbxi + ϕb(ti) + δbgb(ti) + θb xigb(ti), 0, 1 { where yib [0, 1] is model is score on benchmark b, xi = log(#paramsi), and ti is release time. Here ϕb(t) captures smooth time trend, and gb(t) is late-period indicator (we use the cutoff date in Figure 6). We fix τ = 0.98. Intuitively, the quantity βb + θbgb(t) can be viewed as the marginal size effect on the capability boundary at time t, while ˆqτ (13B, g=1) characterizes the late-period attainable boundary for small (13B-parameter) models. We fit the model on Open LLM Leaderboard data together with our newer-model evaluations. Comparing MATH LVL 5 with MMLU-PRO  (Table 3)  , we find that the estimated 13B attainable 0.52), boundary is much higher on Math in the late period (ˆq0.98 consistent with small models approaching the top boundary on Math but remaining substantially below the boundary on MMLU-Pro, where larger models retain dominance. 0.94) than on MMLU-Pro ( }"
        },
        {
            "title": "Benchmark",
            "content": "ˆβ ˆβ + ˆθ ˆq0.98(13B, g=0) ˆq0.98(13B, g=1) MATH Lvl 5 MMLU-Pro 0.25 0. 0.76 0.47 0.03 0.15 0.94 0.52 Table 3. Key fitted statistics from the sizetime boundary model. Here g=0 and g=1 denote the earliest and latest times in the pooled dataset, respectively; ˆβ + ˆθ is the late-period marginal size effect on the boundary. finding 5. Task-dependent small-model ceilings. Knowledge-intensive capability, such as MMLUPro, remains scale-limited in current practice; post-training does not eliminate the advantage of larger base models. Practically, this clarifies that saturation depends on both model scale and the task."
        },
        {
            "title": "5.2 Contamination or Train-on-test-task Diagnostics on Frontier Benchmarks",
            "content": "Beyond open-weight leaderboards, we use frontier-model evaluations to probe external validity and to construct contamination-oriented diagnostics. We first test whether the sigmoid boundary remains competitive relative to the more flexible I-spline class on closed-source frontier models with known compute. We then examine simple cross-benchmark shift test designed to detect post-release score inflation consistent with benchmark-specific contamination."
        },
        {
            "title": "5.2.1 Frontier-model Scaling on GPQA",
            "content": "Epoch AI evaluates many closed-source frontier models. We fit the capability boundary of GPQA diamond (Rein et al., 2024) based on models with known compute. As in Section 3.1.1, we compare the sigmoid estimator with the more complex I-spline estimator in Figure 7a and find they are largely similar, supporting the external validity of the sigmoid scaling law on frontier models. Additional results for boundaries fitted from other publicly available frontier leaderboards are provided in Section H.1."
        },
        {
            "title": "5.2.2 A Cross-benchmark Shift Test",
            "content": "We also re-examine contamination by exploiting the hypothesis that both benchmark scores are (monotone) sigmoid functions of pretraining compute, which implies an approximately linear relationship between their logit-transformed true accuracies. Since all models post-date MATH-500, any MATH-500 inflation can affect all points, whereas AIME-2025-specific inflation should only affect models released after Feb. 6th, 2025. This motivates the regression logit(0.01yi) = α + β logit(0.01mi) + γ 1 post-AIME + εi, } { (3) Here yi is the AIME 2025 accuracy and mi is the corresponding MATH-500 accuracy for the same model. positive γ corresponds to systematically higher AIME 2025 performance than would be predicted from MATH-500. Restricting to the overlapping range of MATH-500 scores across the two release groups (n = 90), we estimate positive but not statistically significant shift (p-value = 0.15). In other words, we find no clear aggregate evidence that post-release AIME-2025 scores are unusually high relative to what MATH-500 performance would predict, though modest contamination effects cannot be ruled out. 12 (a) GPQA Diamond (b) Scaling of math accuracies Figure 7. Scaling laws for frontier models."
        },
        {
            "title": "6 Related Works\nScaling laws and downstream predictability. Classical scaling laws relate model size, data, and\ncompute to pretraining loss under controlled settings (Hoffmann et al., 2022; Kaplan et al., 2020).\nTranslating these forecasts into actionable guidance for future model development (Hernandez et al.,\n2021; Kaplan et al., 2020; McCandlish et al., 2018; Zhang et al., 2025c), however, remains an open\nchallenge. Particularly, their extension to downstream task performance has proven substantially\nnoisier and less reliable (Chen et al., 2024; Lourie et al., 2025; Schaeffer et al., 2024). Recent work\non observational scaling laws studies compute–performance relationships using heterogeneous\ntrained models (Ruan et al., 2024), and shows that post-training on test or target tasks can make\ndownstream performance appear more predictable, while also introducing confounding temporal\neffects (Dominguez-Olmedo et al., 2024). We build on this line by focusing on attainable performance\nboundaries rather than average trends.\nDownstream performance forecasting and scaling analyses. Under controlled regimes, down-\nstream accuracy often follows simple power laws in training compute, enabling reliable FLOP-to-\naccuracy extrapolation (Krajewski et al., 2025). In realistic settings, however, emergent abilities,\nmetric noise, and heterogeneous pipelines frequently violate these trends (Schaeffer et al., 2024). To\naddress this, prior work proposes two-stage regressions from compute to pretraining loss to down-\nstream metrics (Chen et al., 2024), clustering methods that isolate predictable task subsets (Xu et al.,\n2025), and rectified scaling laws that forecast fine-tuning performance via a learned data-size term\n(Lin et al., 2024). Large-scale observational analyses from Epoch AI further inform forecasting by\ntracking compute, data, and performance trajectories across thousands of models (Epoch AI, 2022).\nIn contrast to fine-tuning–centric or point-forecasting approaches, we study post-training capability\nboundaries, estimating high-quantile, prescriptive performance boundaries from heterogeneous\nmodels and analyzing their temporal stability and data efficiency. Post-training techniques such as\ninstruction tuning and preference optimization can substantially shift downstream performance\nwithout changing pretraining compute (Ouyang et al., 2022; Ziegler et al., 2019). Recent studies\nemphasize that post-training often elicits latent capabilities, leading to large variance among models\ntrained with similar FLOPs (Donoway et al., 2025; Jin et al., 2025). In parallel, targeted pretrain-\ning methods demonstrate that aligning pretraining data with downstream objectives can yield\nsignificant gains (Brandfonbrener et al., 2024; Mizrahi et al., 2025). Our work complements these\nalgorithmic advances by abstracting over heterogeneous post-training pipelines and treating models\nwith similar pretraining FLOPs as a single domain, enabling large-scale observational analysis that\nis better aligned with engineering decision-making.",
            "content": ""
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced prescriptive scaling, decision-oriented framework for mapping pre-training compute budgets to reliable, high-probability downstream performance expectations under contemporary post-training practice. By estimating high-quantile capability boundaries from large, heterogeneous model populations, we show that attainable post-training performance is well-approximated by simple, monotone sigmoid functions of log-compute and is temporally stable for most tasks. Notable exceptions, such as math reasoning, exhibit shifting boundary, highlighting where algorithmic progress continues to move the attainable envelope. Beyond forecasting, our approach enables practical diagnostics for saturation, contamination, and evaluation efficiency, allowing recovery of near-full boundaries with small fraction of evaluation budget. Together, these results position capability boundaries as practical tool for budgeting, monitoring, and interpreting progress in language models as scaling regimes evolve."
        },
        {
            "title": "Acknowledgment",
            "content": "VS is partially supported by the Schmidt Sciences Trustworthy AI Program award on AI Safety in the Inference-time Compute Paradigm; SK acknowledges the support from the National Science Foundation Grant under award IIS 2229881; HZ and SK acknowledge the Chan Zuckerberg Initiative Foundation for establishing the Kempner Institute for the Study of Natural and Artificial Intelligence."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. Open llm leaderboard. https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_ leaderboard, 2023. Aaron Blakeman, Aaron Grattafiori, Aarti Basant, Abhibha Gupta, Abhinav Khattar, Adi Renduchintala, Aditya Vavre, Akanksha Shukla, Akhiad Bercovich, Aleksander Ficek, et al. Nemotron 3 nano: Open, efficient mixture-of-experts hybrid mamba-transformer model for agentic reasoning. arXiv preprint arXiv:2512.20848, 2025. David Brandfonbrener, Hanlin Zhang, Andreas Kirsch, Jonathan Richard Schwarz, and Sham Kakade. Color-filter: Conditional loss reduction filtering for targeted language model pre-training. Advances in Neural Information Processing Systems, 37:9761897649, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=sckjveqlCZ. 14 Yangyi Chen, Binxuan Huang, Yifan Gao, Zhengyang Wang, Jingfeng Yang, and Heng Ji. Scaling laws for predicting downstream performance in llms. arXiv preprint arXiv:2410.08527, 2024. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Fernandes de Aguiar, Bourguignon, MS Khots, DL Massart, and Phan-Than-Luu. D-optimal designs. Chemometrics and intelligent laboratory systems, 30(2):199210, 1995. Ricardo Dominguez-Olmedo, Florian Dorner, and Moritz Hardt. Training on the test task confounds evaluation and emergence. arXiv preprint arXiv:2407.07890, 2024. Elizabeth Donoway, Hailey Joren, Arushi Somani, Henry Sleight, Julian Michael, Michael DeWeese, John Schulman, Ethan Perez, Fabien Roger, and Jan Leike. Quantifying elicitation of latent In The Thirty-ninth Annual Conference on Neural Information capabilities in language models. Processing Systems, 2025. Epoch AI. Parameter, compute and data trends in machine learning. https://epoch.ai/data/ ai-models, 2022. Clémentine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. Open llm leaderboard v2. https://huggingface.co/spaces/open-llm-leaderboard/open_ llm_leaderboard, 2024. Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, et al. Language models scale reliably with over-training and on downstream tasks. arXiv preprint arXiv:2403.08540, 2024. Peter Goos and Bradley Jones. Optimal design of experiments: case study approach. John Wiley & Sons, 2011. Peter Goos, Bradley Jones, and Utami Syafitri. I-optimal design of mixture experiments. Journal of the American Statistical Association, 111(514):899911, 2016. Danny Hernandez, Tom Brown, Tom Conerly, et al. Scaling laws for transfer. arXiv preprint arXiv:2102.01293, 2021. Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409, 2017. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, pages 3001630030, 2022. Sara Hooker. On the slow death of scaling. Available at SSRN 5877662, 2025. Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Alon Albalak, and Yejin Choi. Artificial hivemind: The open-ended homogeneity of language models (and beyond). arXiv preprint arXiv:2510.22954, 2025. 15 Jikai Jin, Vasilis Syrgkanis, Sham Kakade, and Hanlin Zhang. Discovering hierarchical latent capabilities of language models via causal representation learning. arXiv preprint arXiv:2506.10378, 2025. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhiyu Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking benchmarking in nlp. In Proceedings of NAACL, 2021. Roger Koenker. Quantile regression, volume 38. Cambridge university press, 2005. Roger Koenker and Gilbert Bassett. Regression quantiles. Econometrica, 46(1):3350, 1978. doi: 10.2307/1913643. Jakub Krajewski, Amitis Shidani, Dan Busbridge, Sam Wiseman, and Jason Ramapuram. Revisiting the scaling properties of downstream metrics in large language model training. arXiv preprint arXiv:2512.08894, 2025. Haowei Lin, Baizhou Huang, Haotian Ye, Qinyu Chen, Zihao Wang, Sujian Li, Jianzhu Ma, Xiaojun Wan, James Zou, and Yitao Liang. Selecting large language model to fine-tune via rectified scaling law. In International Conference on Machine Learning, pages 3008030107. PMLR, 2024. Nicholas Lourie, Michael Hu, and Kyunghyun Cho. Scaling laws are unreliable for downstream tasks: reality check. arXiv preprint arXiv:2507.00885, 2025. Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical model of large-batch training. arXiv preprint arXiv:1812.06162, 2018. David Mizrahi, Anders Boesen Lindbo Larsen, Jesse Allardice, Suzie Petryk, Yuri Gorokhov, Jeffrey Li, Alex Fang, Josh Gardner, Tom Gunter, and Afshin Dehghan. Language models improve when pretraining data matches target tasks. arXiv preprint arXiv:2507.12466, 2025. Taman Narayan, Serena Lutong Wang, Kevin Robert Canini, and Maya Gupta. Expected pinball loss for quantile regression and inverse cdf estimation. Transactions on Machine Learning Research, 2024. Team Olmo, Allyson Ettinger, Amanda Bertsch, Bailey Kuehl, David Graham, David Heineman, Dirk Groeneveld, Faeze Brahman, Finbarr Timbers, Hamish Ivison, et al. Olmo 3. arXiv preprint arXiv:2512.13961, 2025. Simon Ott, Adriano Barbosa-Silva, Kathrin Blagec, Jan Brauner, and Matthias Samwald. Mapping global dynamics of benchmark creation and saturation in artificial intelligence. Nature Communications, 13(1):6793, 2022. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. 16 Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006. Zhenting Qi, Fan Nie, Alexandre Alahi, James Zou, Himabindu Lakkaraju, Yilun Du, Eric P. Xing, Sham M. Kakade, and Hanlin Zhang. EvoLM: In search of lost language model training dynamics. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. James Ramsay. Monotone regression splines in action. Statistical science, pages 425441, 1988. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Yangjun Ruan, Chris Maddison, and Tatsunori Hashimoto. Observational scaling laws and the predictability of langauge model performance. Advances in Neural Information Processing Systems, 37:1584115892, 2024. Rylan Schaeffer, Hailey Schoelkopf, Brando Miranda, Gabriel Mukobi, Varun Madan, Adam Ibrahim, Herbie Bradley, Stella Biderman, and Sanmi Koyejo. Why has predicting downstream capabilities of frontier ai models with scale remained elusive? arXiv preprint arXiv:2406.04391, 2024. Amrith Setlur, Saurabh Garg, Xinyang Geng, Naman Garg, Virginia Smith, and Aviral Kumar. Rl on incorrect synthetic data scales the efficiency of llm math reasoning by eight-fold. Advances in Neural Information Processing Systems, 37:4300043031, 2024. Byran Smucker, Martin Krzywinski, and Naomi Altman. Optimal experimental design. Nat. Methods, 15(8):559560, 2018. Ingo Steinwart and Andreas Christmann. Estimating conditional quantiles with the help of the pinball loss. Bernoulli, 17(1):211225, 2011. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537, 2019. Boxin Wang, Chankyu Lee, Nayeon Lee, Sheng-Chieh Lin, Wenliang Dai, Yang Chen, Yangyi Chen, Zhuolin Yang, Zihan Liu, Mohammad Shoeybi, et al. Nemotron-cascade: Scaling cascaded reinforcement learning for general-purpose reasoning models. arXiv preprint arXiv:2512.13607, 2025. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid ShwartzZiv, Neel Jain, Khalid Saifullah, Sreemanti Dey, et al. Livebench: challenging, contaminationlimited llm benchmark. arXiv preprint arXiv:2406.19314, 2024. Juncheng Wu, Sheng Liu, Haoqin Tu, Hang Yu, Xiaoke Huang, James Zou, Cihang Xie, and Yuyin Zhou. Knowledge or reasoning? close look at how llms think across domains. arXiv preprint arXiv:2506.02126, 2025. 17 Chengyin Xu, Kaiyuan Chen, Xiao Li, Ke Shen, and Chenggang Li. Unveiling downstream performance scaling of llms: clustering-based perspective. arXiv preprint arXiv:2502.17262, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Guanhua Zhang, Ricardo Dominguez-Olmedo, and Moritz Hardt. Train-before-test harmonizes language model rankings. arXiv preprint arXiv:2507.05195, 2025a. Guanhua Zhang, Florian Dorner, and Moritz Hardt. How benchmark prediction from fewer data misses the mark. arXiv preprint arXiv:2506.07673, 2025b. Hanlin Zhang, Depen Morwani, Nikhil Vyas, Jingfeng Wu, Difan Zou, Udaya Ghai, Dean Foster, and Sham Kakade. How does critical batch size scale in pre-training? In The Thirteenth International Conference on Learning Representations, 2025c. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019. 20 20 21 22 22 22 22 23 24 25 26 28 32 34 34 35 35"
        },
        {
            "title": "Table of Contents",
            "content": "A Pinball Loss and Quantile Regression . A.1 Properties of Pinball Loss . . A.2 Performance Frontier via Quantile Regression . . . . . . . . . . . . . . . . . . . . . Additional Details for Section 2.12. B.1 Concrete Illustrative Outlier Example . . B.2 Full Bin Construction Algorithm for the Binwise Model . . B.3 Full I-spline Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Pre-training vs. Post-training Diagnostics Details and Additional Analyses for Section 3 . D.1 Omitted Details in Section 3 . . D.2 Bin-wise Diagnostics underlying Figure 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Scaling Laws for Model Size Newly Evaluated models Sensitivity to Smoothed-Pinball Hyperparameters Additional Results . H.1 Public Leaderboards of Frontier Models . H.2 Results for Open LLM Leaderboard v1 . H.3 Latent Capability Factors and Prescriptive Boundaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Saturation Analysis across Open LLM Leaderboard Versions and Tasks Greedy Optimization for the Balanced I-Optimal Design"
        },
        {
            "title": "A Pinball Loss and Quantile Regression",
            "content": "A.1 Properties of Pinball Loss Our sigmoid boundaries are not fitted with symmetric squared loss, but with high-quantile pinball loss (Koenker and Bassett, 1978) that explicitly targets the upper envelope of the data. For residual the true pinball loss at quantile level τ = qτ (z; θ), (0, 1) is ρτ (r) = max(cid:0)τ r, (τ 1)r(cid:1), which is piecewise linear with kink at = 0. Minimizing its expected value recovers the τ -quantile of the (conditional) response distribution (Koenker and Bassett, 1978). In practice we use smoothed variant ρτ (r) = log(cid:0)1 + eκr(cid:1) + (τ 1 κ 1) r, 50 (Figure 8a), which is numerically stable yet visually indistinguishable from the sharp with κ pinball loss away from = 0. The key property of the pinball loss is its asymmetry. As shown in Figure 8b, the gradient gτ (r) = ρτ (r) is approximately τ 1 for < 0 and τ for > 0. For high quantiles (e.g., τ = 0.98), this means that points lying above the boundary (positive residuals, > qτ ) incur much larger gradient magnitude than those below it. Intuitively, the optimizer is heavily penalized whenever the boundary lies below high-performing models, but it almost ignores points that underperform relative to the current boundary. This is precisely the behavior we want when estimating capability boundary: the boundary should track the best models at given compute budget, not the mean. (a) Smoothed vs. true pinball loss and squared loss as function of ˆy. residual = (b) Gradients of the pinball loss, highlighting asymmetric weighting of over/under prediction. (c) True boundary vs. squared-loss and pinball-loss fits on synthetic scaling-law data. Figure 8. Visualizing the pinball loss and its effect on boundary fitting. (a) The true pinball loss and its smoothed approximation for several quantile levels τ ; smoothing (softplus) affects only narrow band around = 0. (b) The corresponding gradients, showing that positive residuals (boundary below data) produce much larger gradients than negative residuals. (c) On synthetic data, the pinball boundary closely tracks the upper envelope, whereas the squared-loss boundary is pulled toward the bulk of underperforming points. Figure 8c illustrates the practical impact of this choice on synthetic scaling-law dataset. The black curve denotes the true boundary used to generate the data; most points are sampled below it, with few small above-boundary outliers. Fitting sigmoid with symmetric squared loss pulls the boundary toward the bulk of the cloud, underestimating the achievable performance at high 20 compute. In contrast, fitting with the smoothed pinball loss at τ = 0.98 yields boundary that closely matches the true upper envelope. Together, Figures 8a8c show that the pinball loss provides principled and computationally convenient way to implement high-quantile, envelope-seeking objective for capability boundaries. A.2 Performance Frontier via Quantile Regression natural question is: if we are interested in capability boundary, why not regress on the maximal observed score at each compute level rather than on high conditional quantile? In this subsection, we explain why we instead target high quantileimplemented via the pinball lossand why we view this as the more meaningful object for our purposes. In our setting, the number of models per compute bin is highly uneven and changes over time. The sample maximum in bin is an extremely high-variance statistic whose expectation increases with the number of draws, even if the underlying distribution of model performance at that compute were fixed. regression on bin-wise maxima would therefore confound two effects: genuine changes in the performance distribution as compute increases, and incidental variation in how many models happened to be trained, how hard different groups searched hyperparameters, or how aggressively they pruned weak runs. In contrast, conditional quantile qτ (z) with τ (0, 1) is defined in terms of the underlying distribution of scores at compute level z, and does not explode just because one period happens to contain more models in particular bin. Highquantile regression with the pinball loss is standard, well-understood estimator in this regime, and empirically leads to much smoother, more stable boundarys than directly regressing on bin-wise maxima. What we ultimately want is not the performance of the single luckiest model ever trained at given compute value, but statement of the form: if we train model at compute using contemporary practices, what level of performance can we expect to achieve with high probability? high conditional quantile has direct probabilistic semantics that matches this question. If = log10 and is the score on given task, then an ideal quantile boundary qτ satisfies Pr(cid:0)Y qτ (Z) (cid:12) (cid:12) b(cid:1) = τ for each compute bin b. This is exactly the coverage property we evaluate in Section 2: we can check, bin by bin and period by period, whether the fitted boundary overor under-covers the empirical distribution of models. There is no analogous notion of coverage for regression on maxima; one would simply be fitting smooth curve through extreme points, without built-in way to say what fraction of future models should lie above or below it. In realistic training processes there are occasional outlier models: unusually strong models that benefited from, for instance, severe data contamination. The maximum is by construction 0.98 behaves driven entirely by such extremes. high, but not extreme, quantile such as τ differently. In bins with reasonable number of models, the 98th percentile is already near the top of what the given compute range could produce, but it is much less sensitive to single outlier run. In our experiments, this choice yields boundaries whose in-sample and 2% across tasks and periods, and whose out-of-sample coverage errors are on the order of 1% behavior is stable under modest changes in binning and regularization. 21 Finally, there is conceptual point about what we mean by boundary. In practice, one rarely cares about the single best model that anyone, anywhere, has ever managed to train at given compute value. What matters is the level that is reliably achievable by following reasonably competitive pipeline, perhaps with some hyperparameter tuning but without assuming arbitrarily many parallel bets. high-quantile boundary formalizes this notion of reliably reachable top-tier performance: it separates out the typical high end of the distribution from the one-off lucky run. For these reasons we treat qτ ( ), rather than the conditional maximum, as the primary object of interest, and we use the pinball loss to estimate it in way that can be directly calibrated and validated. This interpretation is relative to the population of reported models we observe; substantial selection effects could shift the implied boundary. Additional Details for Section 2.12.3 B.1 Concrete Illustrative Outlier Example As concrete example, the model qingy2024/Benchmaxx-Llama-3.2-1B-Instruct achieves 0.83 and 0.48 raw accuracies on BBH and MATH LVL 5, respectively, while the second-best models using Llama-3.2-1B as their base only reach 0.36 and 0.08. Empirically, broadly applicable algorithmic improvements are typically reproduced by multiple independent derivatives (e.g., fine-tuning variants), whereas isolated spikes can reflect idiosyncratic effects such as benchmarkspecific overfitting or leakage. We include this example solely to motivate the use of high-quantile estimation (which is more robust than conditional maxima); we do not attempt to verify or attribute the underlying cause. B.2 Full Bin Construction Algorithm for the Binwise Model We use group-aware equal-mass binning on the training z-values. Let denote the number of training samples, and let the sorted training values be grouped into unique levels with counts g=1 so identical values are never split across bins. Given target bin count and (ug, ng) { } minimum bin size mmin, we set Beff = min(B, G) and target mass = max(mmin, ). We sweep the unique levels in increasing order, accumulating counts until the running total reaches (or the last group), then place bin boundary at that unique value. If any resulting bin has fewer than mmin samples, we iteratively merge it with an adjacent bin by removing boundary (merging with the smaller neighbor when there is choice) until all bins satisfy the minimum size. This yields < eB (with the final number of bins after merging), which define bins [eb1, eb] edges e0 < used for both training and evaluation. N/Beff B.3 Full I-spline Definition I-spline estimator. We use an I-spline basis to parameterize flexible monotone function of (Ramsay, 1988). Let j=1 denote the } associated nonnegative M-spline basis (order p). Define I-spline basis functions ℓ=0 be nondecreasing knot sequence and let Mj( κℓ } { { ) so each Ij(z) is nondecreasing in z. We then model Ij(z) = (cid:90) κ0 Mj(u) du, g(z) = a0 + (cid:88) j=1 wjIj(z), wj 0, 22 (4) (5) which guarantees that g(z) is nondecreasing. Predictions are qI saturating boundary in [0, 1]. τ (z) = σ(g(z)), ensuring monotone Pre-training vs. Post-training Diagnostics In this appendix, we provide additional details for the comparison between pretrained checkpoints and post-trained variants in Section 3.2. Pretrained subset and evaluation protocol. We leverage the official and pretrained labels available from the Open LLM Leaderboard to construct the set of official pretrained checkpoints. This choice is closer to existing scaling-law studies (Dominguez-Olmedo et al., 2024; Ruan et al., 2024), which are mostly based on popular open-weight pretrained model families such as Llama, Qwen, and Gemma. Quantities reported. Let qpost summarize the relationship between pretrained checkpoints and post-training capability using: (z) denote the fitted post-trained sigmoid capability boundary. We τ Gap-to-boundary: qpost τ (zi) ypre is below the post-training envelope at the same compute). for each pretrained checkpoint (how far the base checkpoint Post-training lift (paired when possible): ypost-best ypre , where ypost-best is the best post-trained score among checkpoints sharing the same base model identity/compute as i. Figure 9 visualizes these quantities across all six tasks. The results indicate that post-training provides the largest gains on IFEVAL and MATH LVL 5, with much smaller gains on MMLU-PRO, BBH, GPQA, and MUSR; this qualitative pattern holds across the observed range of pretraining compute. Figure 10 overlays the fitted pretrained capability boundaries with the corresponding post-trained boundaries across all six tasks. Figure 9. Summary diagnostics connecting pretraining compute to post-training capability. These plots quantify (i) how far pretrained checkpoints lie below the post-trained τ -capability boundary, and (ii) how much post-training can lift performance at fixed compute. (a) MMLU-PRO (b) BBH (c) GPQA (d) MATH LVL 5 (e) MUSR (f) IFEVAL Figure 10. Pretrained vs. post-trained overlays across all tasks. See Figure 3 for the legend description. Details and Additional Analyses for Section 3 (a) BBH (b) GPQA (c) IFEVAL (d) MUSR Figure 11. Sigmoid capability boundaries over time. Complementary of Figure 1. 24 D.1 Omitted Details in Section 3 This section collects few plotting and normalization conventions used in Section 3. 1, 2, 3 Rolling train/validation protocol and overlap restriction. For each temporal split , } t+1. To avoid we fit each boundary estimator on extrapolating beyond observed compute, OOD evaluation is restricted to the overlap of the training and validation ranges in = log10 C. The table below provides detailed base model information for models in all four time periods. We only include base models that are used at least ten times. One can see that it covers almost all mainstream base models during that time period. and evaluate out-of-distribution (OOD) on { P"
        },
        {
            "title": "Base model",
            "content": "gemma-1-2b gemma-2-27b gemma-2-2b gemma-2-9b llama-2-13b llama-2-70b llama-2-7b llama-3-70b llama-3-8b llama-3.1-70b llama-3.1-8b llama-3.2-1b llama-3.2-3b mistral-7b phi-3-4b phi-4-14b qwen2-0.5b qwen2-1.5b qwen2-72b qwen2-7b qwen2.5-0.5b qwen2.5-1.5b qwen2.5-14b qwen2.5-32b qwen2.5-3b qwen2.5-7b 2024-06 2024-07..2024-09 2024-10..2024-12 2025-01..2025-03 13 15 14 24 26 167 132 10 12 23 13 27 113 12 89 63 14 12 14 14 12 65 10 156 14 65 22 43 44 14 22 58 57 84 28 25 53 21 10 13 197 50 25 35 49 48 23 56 133 109 11 185 80 70 Relative improvements in Table 2. In Table 2, we report percent changes relative to the constant baseline Constant. For metric value (pinball loss or coverage error), the plotted relative change is % = mmethod mConstant . mConstant Thus, more negative values indicate better performance than Constant. Bin-wise coverage heatmaps in Figure 14. Coverage is evaluated using the bin-wise coverage metric from Section 2.3. Bins are constructed on the training period only in (never splitting identical values), and the same bin edges are reused for OOD evaluation on t+1. Within each bin, we compute empirical coverage ˆτ and report the signed deviation ˆτ τ . Negative values indicate under-coverage: more than (1 τ ) fraction of models in that bin exceed the predicted τ -boundary fit on t. D.2 Bin-wise Diagnostics underlying Figure 2 Figure 12 and 13 report the bin-wise OOD breakdowns of three benchmarks: MMLU-PRO, MATH LVL 5 and IFEVAL, that are omitted from the main paper for space. These plots use log-compute bins constructed on the training period only (see Appendix Section D) and evaluate only on the trainOOD overlap in z. Figure 12. Bin-wise coverage across periods (supplement to Figure 2 (Left)). Bin-wise signed OOD coverage error within log-compute bins when fitting on and validating on t+1, for = 1, 2, 3. Figure 13. Bin-wise pinball loss across periods (supplement to Figure 2 (Right)). Bin-wise OOD pinball loss within log-compute bins when fitting on t+1, shown for MMLU-PRO, MATH LVL 5, and IFEVAL with = 1, 2, 3. and validating on Localization of deviations. The bin-wise heatmaps show that the largest deviations in both coverage and ρτ are concentrated in small subset of mid-to-high compute bins (not uniformly across compute), rather than reflecting pervasive misfit across all scales. The complete in-distribution (ID) and out-of-distribution (OOD) coverage error and pinball loss are visualized in Figure 14 and 15. We can see that apart from the aforementioned three benchmarks, 26 Figure 14. In-distribution and out-of-distribution coverage error across log-compute bins for train ( ( t+1), = 1, 2, 3 across six Open LLM Leaderboard tasks. t) and validation Figure 15. In-distribution and out-of-distribution pinball loss across log-compute bins for train ( = 1, 2, 3 across six Open LLM Leaderboard tasks. t) and validation ( t+1), the remaining ones all have mild ID and OOD errors across different compute bins, implying that the scaling remains stable on these benchmarks."
        },
        {
            "title": "E Scaling Laws for Model Size",
            "content": "In this section, we provide complementary results for the capability boundaries as functions of the model size. This is in contrast with classical scaling laws (Kaplan et al., 2020) use either the pretraining compute or the model size plus the pretraining token size to predict downstream task performance. Using model sizes as the single predictive factor is useful, since it informs us how much capability small model can acquire with an unbounded amount of pretrained data. Figure 16 compares the scaling of pretrained and post-trained models. The findings are largely similar with the pretraining-compute-based counterparts. One novel finding is that on MUSR, while larger pretrained models do not show clear benefit over smaller ones, post-trained models have much clearer scaling in terms of model size. This suggests that for multi-step reasoning tasks, larger models could have larger potentials than smaller ones even if the base model accuracies are similar. (a) MMLU-PRO (b) MATH LVL 5 (c) BBH (d) GPQA (e) MUSR (f) IFEVAL Figure 16. Sigmoid Capability Boundaries as functions of model size. In Figure 17, we further compare the temporal changes of the sigmoid capability boundaries across different tasks. While MATH LVL 5 and IFEVAL performances initially improve under fixed model size, the curves tend to stablize in later periods. 28 (a) MMLU-PRO (b) MATH LVL (c) Big-bench Hard (BBH) (d) GPQA (e) MUSR (f) IFEVAL Figure 17. Comparison of sigmoid performance boundaries for ( t) and ( t+1), 1 3 across different tasks. 29 (a) MMLU-PRO (b) MATH LVL 5 (c) Big-bench Hard (BBH) (d) GPQA (e) MUSR (f) IFEVAL Figure 18. Comparison of sigmoid performance boundaries within each for old and held-out new models. (a) MMLU-PRO (b) MATH LVL 5 (c) Big-bench Hard (BBH) (d) GPQA (e) MUSR (f) IFEVAL Figure 19. Comparison of sigmoid performance boundaries within each for old and held-out new models"
        },
        {
            "title": "F Newly Evaluated models",
            "content": "In this section, we provide complete results for the newly evaluated open-weight models. Concretely, we evaluate the performance of two different types of models: Models available on huggingface with the most number of likes, filter by compatability with the lm-evalharness. list of base families of these models is given in Table 5. The most recent models officially released by wellknown industry labs near the end of 2025, which we manually picked. This includes Allen AIs OLMo-3 (Olmo et al., 2025), NVIDIAs Nemotron nano (Blakeman et al., 2025) and cascade (Wang et al., 2025). Base model family # models Mistral-7B-v0.3 Qwen2.5-1.5B Llama-3.2-3B gemma-2-9b Qwen3-4B-Base Qwen2.5-3B Llama-3.2-1B gemma-2-2b Qwen2.5-32B Meta-Llama-3-8B Llama-3.1-8B Mistral-7B-v0.1 Llama-2-13b-hf gemma-3-1b-pt SmolLM3-3B-Base Qwen2-7B Yi-34B Others 143 124 100 97 90 89 83 75 63 62 56 45 44 42 39 36 34 124 https: at at the results release evaluation We //huggingface.co/datasets/hlzhang109/ proteus-2k. The most up-to-date part of proteus-2k, namely those built on new base models that do not appear on the Open LLM Leaderboard, are prohttps://huggingface.co/datasets/ vided hlzhang109/proteus-selected. The results for models released before the retirement of the Open LLM Leaderboard are shown in Figure 18, while those of the later models are shown in Figure 19, where the models are additionally divided into two classes according to whether the base models are new. Overall, we find that the capability boundaries that we estimate from the Open LLM Leaderboard reliably upper-bounds the best possible accuracies attainable on various tasks, with only one caveat: on MATH LVL 5, there are several notable outliers in the right panel of Figure 19b. Table 5. Model counts by base model family among the newly evaluated models. Total Sensitivity to Smoothed-Pinball Hyperparameters Our sigmoid capability-boundary estimator in Section 2.1 minimizes smoothed pinball objective with two numerical hyperparameters: the smoothing parameter κ in ℓτ , and the ridge weight λ in λ Ω(θ). Throughout the main paper, we fix κ = 50 and λ = 103. This appendix verifies that our empirical conclusions (e.g., the relative ranking of estimator families and the cross-temporal coverage patterns) are not artifacts of these choices. Why in-sample cross-validation is not needed in our setting. We do not treat (κ, λ) as modelselection hyperparameters in the usual sense. The sigmoid family is low-dimensional with explicit monotonicity/range constraints, so the dominant difficulty is period shift (fit on t, evaluate on t+1), rather than in-period overfitting. Moreover, κ only controls how closely the smooth loss approximates the non-smooth check loss in narrow band around zero residual, and λ is included primarily for numerical conditioning rather than increased expressivity. Accordingly, cross-temporal evaluation already constitutes the intended validation; in-period cross-validation can add variance while optimizing for different objective (within-period prediction). 32 Figure 20. In-period tuning has negligible effect on cross-temporal generalization. The fixed default (κ, λ) = (50, 103) used throughout the paper is competitive with values selected by random in-period splits. Left: κ and log10 λ selected by t+1: fixed default vs. in-period tuned (κ, λ). in-period tuning (aggregated across random splits). Right: OOD metrics on As sanity check, we ran an auxiliary tuning experiment: within each t, we randomly split observations into two halves, tune (κ, λ) over small grid by minimizing validation pinball loss, t+1. Figure 20 shows that (i) selected hyperparameters concentrate in and then re-evaluate on narrow region (typically κ ), and (ii) OOD pinball loss and coverage are essentially unchanged relative to the fixed default (κ, λ) = (50, 103). Thus, cross-validation provides little marginal benefit for the cross-temporal goal emphasized in the main paper. 104, 103 and λ 20, 50 { { } } { 104, 103, 102, 10 20, 50, 100, 200, 1000 Grid sensitivity and practical recommendations. We further swept κ } and λ and measured OOD pinball loss and absolute coverage error on two representative tasks: BBH (typically well-calibrated in Section 3) and MATH LVL 5 (where temporal drift is most apparent). Figures 21 and 22 show that performance is stable across broad reasonable region, but degrades for overly large ridge (λ = 101). Within the stable region, κ has comparatively mild effect once it is moderate, while coverage can improve slightly for smaller λ. { } Figure 21. OOD pinball loss under (κ, λ) sweep. Lower is better. Results are shown for two tasks and two representative period splits (t=1 and t=3). 33 Figure 22. OOD absolute coverage error overly large λ, while κ has weaker effect once it is in moderate range. ˆτ τ under (κ, λ) sweep. Lower is better. Coverage is most sensitive to"
        },
        {
            "title": "H Additional Results",
            "content": "H.1 Public Leaderboards of Frontier Models In this subsection, we apply the same methodology to fit sigmoid scaling law using data from Epoch AI. Compared with the Open LLM Leaderboard, Epoch AI includes many closed-source models but the totoal number of evaluated models being smaller. The results are shown in Figure 23. We can see that MATH Lvl 5 and Mock ATME show no pattern of performance gain from increasing the FLOPs. GPQA diamond, on the other hand, indicates clear scaling in the FLOPs. (a) AIME 2025 (https: (b) MMLU (https: //artificialanalysis.ai/ evaluations/aime-2025) //lifearchitect.ai/ models-table/) (c) MMLU-Pro //lifearchitect.ai/ models-table/) (https: Figure 23. Sigmoid scaling law for frontier models. Evaluation data is publicly available from Artificial Analysis and lifearchitect.ai. 34 (a) ARC (b) GSM8K (c) HellaSwag (d) MMLU (e) TruthfulQA (f) Winogrande Figure 24. Comparison of sigmoid performance boundaries across different time periods Open LLM Leaderboard v1. H.2 Results for Open LLM Leaderboard v1 In Figure 24 we present the sigmoid scaling laws learned on six tasks from the Open LLM Leaderboard v1, which is an older version compared with the v2 studied in the main part of the paper. We also define four time periods and investigate how model performance changes with both compute and time. The findings are quite different from that of v2: while several benchmarks such as GSM8K and TruthfulQA induces large performance gain when moving from P2, all benchmarks are saturated by P3. Furthermore, except from MMLU where clear scaling relationship between FLOPS and capability boundary is observed, on remaining benchmarks larger FLOPs bring little or no performance gains in P3. This strongly indicates that these benchmarks were either too old or faced severe comtamination issues, even when the leaderboard was still active. P1 to H.3 Latent Capability Factors and Prescriptive Boundaries In this section, we provide more details about the PCA results mentioned in Remark 2. It is easy to see that while PC1 demonstrate clear scaling in the compute, PC2 and PC3 have capability boundaries that are almost flat. This implies that the scaling law we established for the Open LLM Leaderboard may largely be attributed to advances in single component. 35 (a) PC1. (b) PC2. (c) PC3. Figure 25. The scaling of different principal components."
        },
        {
            "title": "I Saturation Analysis across Open LLM Leaderboard Versions and Tasks",
            "content": "This appendix provides the complete set of plots used to discuss saturation effects and the slow death of scaling narrative. We reproduce the core logic of Hooker (2025, Figure 3) on the Open LLM Leaderboard v1 and v2. These plots are observational: they reflect submitted models, training recipes, post-training, and benchmark targeting over time. They should not be read as controlled parameter scaling law; rather, they summarize how easily larger models translate into higher leaderboard scores for given task. Open LLM Leaderboard v2. Saturation is highly task-dependent on v2. In our runs, knowledgeheavy or knowledge+reasoning tasks (e.g., MMLU-Pro, GPQA) exhibit materially less domination by small models than pure reasoning tasks (e.g., MATH Lvl 5). Open LLM Leaderboard v1. Hooker (2025) uses the (now-archived) v1 leaderboard suite; these plots show why conclusions about the death of scaling can be sensitive to the benchmark suite. Many v1 tasks show strong frontier convergence indicating more saturated evaluation regime. 36 (a) IFEVAL (b) BBH (c) MATH LVL 5 (d) GPQA (e) MUSR (f) MMLU-PRO Figure 26. Open LLM Leaderboard v2: saturation diagnostics by task. (a) ARC (b) HELLASWAG (c) MMLU (d) TRUTHFULQA (e) WINOGRANDE (f) GSM8K Figure 27. Open LLM Leaderboard v1: saturation diagnostics by task. Greedy Optimization for the Balanced I-Optimal Design This appendix provides implementation details for the greedy solver used to approximately maximize the balanced I-optimal design criterion in each period (Section 4.1). The goal is to select subset of candidate models St Pt under the per-period size budget (cid:80) Ut. iSt ci 37 Notation. We reuse the balanced objective Φλ(S) = Φinfo(S) + λ Φbal(S) and the definitions of Φinfo and Φbal from Section 4.1. For the greedy updates, it is convenient to collect the following quantities: Candidates and metadata. For each Pt, we assume we know (zi, ci, b(i)): log pre-training compute zi, evaluation cost ci, and log-compute bin index b(i) 1, . . . , (Section 2.3). { } Local Jacobians. Let θ0 be nominal frontier parameter obtained from an initial fit. Define Rp as the Jacobian of the high-quantile boundary qτ (z; θ) with respect to θ ji evaluated at θ0 (explicit form in Section 4.1), where = 4. j(zi; θ0) Bin midpoints. Let zb b=1 be the bin midpoints and jb } { use weights wb (uniform wb = 1/B in our experiments) and define j(zb; θ0) the associated Jacobians. We (cid:88) b=1 wb jbj Rpp. Inverse information. For current design S, let K(S) (cid:16) ηI + (cid:88) iS (cid:17)1 , jij where η > 0 is small ridge term for numerical stability. Bin counts. Let nb(S) { : b(i) = } and let ε > 0 be the balance constant in Φbal. Algorithm. Algorithm 1 summarizes the greedy gain-per-cost procedure used in our experiments. Algorithm 1: Greedy optimization for the balanced I-optimal design Require: Period-t candidates Pt with metadata Require: Bin midpoints } Require: Nominal sigmoid parameters θ0 = (y0, L, a0, b0); ridge η > 0; balance constant ε > 0; iPt; budget Ut } b=1 and weights (e.g., wb = 1/B) (zi, ci, b(i)) zb { { wb } { tradeoff λ Ut"
        },
        {
            "title": "Pt do",
            "content": "iSt ci j(zi; θ0) Ensure: Selected subset St Pt with (cid:80) 1: Precompute local geometry (at θ0). 2: for each ji 3: 4: end for 5: for = 1 to do j(zb; θ0) 6: 7: end for (cid:80)B b=1 wb jbj 8: 9: Initialize with small anchor set. 10: Choose (e.g., two extreme-z models and two near = jb (cid:16) (cid:17)1 ηI + (cid:80) { iS jij : b(i) = 11: 12: nb 13: while there exists feasible 14: 15: 16: Urem do Evaluate gain-per-cost for each feasible candidate. for each feasible ji; for = 1, . . . , Pt Pt Ku; α 1 + uv"
        },
        {
            "title": "Urem do",
            "content": "} 38 (Jacobian of qτ (z; θ) w.r.t. θ) (so Φinfo(S) = tr(AK)) a0/b0); Urem Ut (cid:80) iS ci log(nb(i) + ε) (equivalently tr(AKi) + tr(AK)) vAv α info(i) bal(i) log(nb(i) + 1 + ε) info(i) + λ bal(i) ci feasible candidate with the largest g(i) 0 then 1 + (u)v (no positive-gain addition remains) (ShermanMorrison update) Ku; α v(v) α ; Urem } Urem ci; nb(i) nb(i) + 1 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: g(i) end for if g(i) break end if ji; S 27: 28: end while 29: return St { Below, we briefly justify the key computations used in Algorithm 1 and document few implementation choices. ShermanMorrison update and closed-form gain. Let be the current design and / with Jacobian = ji. Write = (ηI + (cid:80) candidate )1 and α = 1 + uKu. The rank-one update gives jS jjj Ki (cid:16) ηI + (cid:88) (cid:17)1 jjj = jS{i} (Ku)(Ku) α . Using tr(A(Ku)(Ku)) = (Ku)A(Ku), the marginal information gain admits the closed form info(i) = tr(AKi) + tr(AK) = (Ku)A(Ku) 1 + uKu , Ku). This avoids refactoring and reduces which is what we compute in the inner loop (with each candidate evaluation to O(p2) operations. Anchor initialization. The greedy selection requires an initial set for which the local geometry is well-conditioned. In practice we initialize with small anchor set that spans the observed compute range: two models near the minimum/maximum z, and (when available) up to two additional models near the nominal sigmoid inflection point = a0/b0. We then fit an initial boundary to obtain θ0 and proceed with greedy additions. Optional 1-exchange polish. Greedy forward selection is fast but not guaranteed to reach local optimum under the knapsack constraint. Optionally, after the greedy pass we apply small number if it increases Φλ. of Fedorov-style 1-exchange moves: remove one Empirically this step yields only modest improvements, but it provides robustness check on the greedy solution. Figure 28 and 29 report how the resulting design quality varies with the budget parameter α (per-period and averaged over periods). and swap in feasible ℓ / 39 (a) = 1 (b) = 2 (c) = (d) Average over Figure 28. In-sample and out-of-sample coverage calibration error on period + 1 as function of budget parameter α when the boundary is estimated using balanced I-optimal design on period t. Curves correspond to different evaluation tasks. (a) = 1 (b) = 2 (c) = 3 (d) Average over Figure 29. In-sample and out-of-sample pinball loss on period + 1 as function of budget parameter α when the boundary is estimated using balanced I-optimal design on period t. Curves correspond to different evaluation tasks."
        }
    ],
    "affiliations": [
        "Harvard University",
        "Stanford University"
    ]
}