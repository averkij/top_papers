{
    "paper_title": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design",
    "authors": [
        "Baoyun Zhao",
        "He Wang",
        "Liang Zeng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions."
        },
        {
            "title": "Start",
            "content": "G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Baoyun Zhao 1 He Wang 2 3 Liang Zeng"
        },
        {
            "title": "Abstract",
            "content": "While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.1 6 2 0 2 9 ] . [ 1 3 5 2 8 0 . 2 0 6 2 : r 1Software College, Northeastern University, 110819, Shenyang, China 2International Centre for Theoretical Physics Asia-Pacific, University of Chinese Academy of Sciences, 100190, Beijing, China 3Taiji Laboratory for Gravitational Wave Universe, University of Chinese Academy of Sciences, 100049, Beijing, China 4Tsinghua University, 100084, Beijing, China. Correspondence to: He Wang <hewang@ucas.ac.cn>, Liang Zeng <zengliangcs@gmail.com>. 1Our code are available at https://github.com/ zboyn/G-LNS. 1 Figure 1. Comparison of G-LNS and traditional AHD methods. For combinatorial optimization problems, unlike existing AHD methods that are largely restricted to local search, G-LNS enables structural reshaping through LLM-generated LNS operators, allowing the search to escape local optima. 1. Introduction Combinatorial Optimization Problems (COPs) are ubiquitous in industrial manufacturing and logistics scheduling, where computational efficiency directly impacts operational costs (Dreo, 2006; Desale et al., 2015). Since many practical COPs are NP-hard (Garey & Johnson, 2002), hand-crafted heuristics have long been the dominant approach for solving large-scale instances (Blum & Roli, 2003; Gendreau et al., 2010). However, traditional heuristic design relies heavily on domain expertise and is often tailored to specific problem structures, which substantially limits generalization across diverse tasks (Stutzle & Lopez-Ibanez, 2018). Recent advances in Large Language Models (LLMs), particularly in logical reasoning (Wei et al., 2022; Zeng et al., 2024; Zhang & Zeng, 2024) and code generation (Chen, 2021; Zeng et al., 2025), have spurred growing interest in Automated Heuristic Design (AHD) (Burke et al., 2013). By leveraging LLMs to automatically generate and refine algorithmic code, AHD searches for high-performance heuristics within discrete algorithm space (Yang et al., 2023). Pioneering frameworks such as FunSearch (Romera-Paredes et al., 2024) and EoH (Liu et al., 2024b) introduced the evolutionary ThoughtCode paradigm and demonstrated G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design promising results on canonical tasks such as Bin Packing. Subsequent work extended this paradigm through reflective evolution (Ye et al., 2024), tree-based exploration strategies (Zheng et al., 2025), and heuristic set evolution to improve generalization (Liu et al., 2025). Despite this progress, existing AHD methods exhibit fundamental structural bottleneck (Figure 1). Most approaches instantiate AHD around either Constructive Heuristics (Glover et al., 2001), which evolve priority rules for sequential decision-making, or Guided Local Search (Voudouris & Tsang, 1996), which optimizes penalty functions under fixed neighborhood operators. Constructive heuristics follow an irreversible trajectory: early suboptimal decisions are difficult to correct through later rule adjustments(Martı & Reinelt, 2011). Conversely, while local search enables iterative refinement(Voudouris & Tsang, 1999), current AHD methods typically treat neighborhood structures (e.g., 2opt(Johnson & McGeoch, 1997)) as fixed priors, restricting the LLM to parameter tuning rather than enabling structural algorithmic innovation(Liu et al., 2024c). To overcome these limitations, we draw inspiration from Large Neighborhood Search (LNS) (Shaw, 1998), metaheuristic that achieves strong structural reshaping through alternating destroy and repair operations(Ropke & Pisinger, 2006). The effectiveness of LNS critically depends on the coupling between these two operators: the destroy phase determines the structural defects introduced into the solution, while the repair phase must be specifically adapted to reconstruct them(Pisinger & Ropke, 2018). This strong interdependence makes automated LNS design particularly challenging and has largely prevented its adoption within existing AHD frameworks(Da Ros et al., 2025). In this work, we propose G-LNS, an evolutionary framework that enables LLMs to automatically design problem-specific LNS operators. Instead of optimizing scalar heuristics or fixed templates, G-LNS prompts the LLM to generate executable code for both destroy and repair operators. To explicitly model their coupling, the framework maintains separate populations for destroy and repair operators and evaluates them jointly within an adaptive LNS process. cooperative evaluation mechanism records the performance of operator pairs, allowing G-LNS to identify complementary destroyrepair logic and guide their co-evolution through synergy-aware crossover. This design allows the search process to move beyond local adjustments and discover heuristics capable of effective structural disruption and reconstruction. Our contributions are summarized as follows: Generative LNS for AHD. We propose G-LNS, an evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators, enabling structural solution perturbation beyond constructive rules and fixed local moves. Synergy-aware Co-evolution. We introduce cooperative evaluation mechanism with synergy matrix to explicitly capture and exploit the coupling between destroy and repair operators during evolution. Empirical Effectiveness and Generalization. Extensive experiments on TSP and CVRP demonstrate that G-LNS outperforms state-of-the-art AHD methods and strong classical solvers, achieving near-optimal solutions with substantially reduced computational budgets. 2. Background 2.1. Automatic Heuristic Design Automatic Heuristic Design (AHD) aims to automatically discover high-performance heuristics for combinatorial optimization problems (COPs)(Burke et al., 2013; Stutzle & Lopez-Ibanez, 2018; Chen et al., 2025b). Let denote the instance space and the solution space. heuristic is mapping : within discrete algorithm space H. The objective of AHD is to identify heuristic that minimizes the expected objective value over target distribution D: = arg min hH EID (cid:2)f (h(I) I)(cid:3). (1) While AHD enables systematic search over algorithmic logic, the expressiveness of the search space is determined by how heuristics are parameterized. Many existing approaches restrict the search to predefined templates, limiting the ability to induce fundamental changes in the underlying search dynamics. This work focuses on expanding the design space from fixed heuristic forms to structurally adaptive search operators. 2.2. Large Neighborhood Search Large Neighborhood Search (LNS) is meta-heuristic that explores the solution space through iterative destroy-andrepair operations (Shaw, 1998). Given solution x, destroy operator removes subset of components (controlled by destruction ratio ϵ), producing partial solution xpartial = d(x). repair operator then reconstructs complete solution = r(xpartial). To balance exploration and exploitation, LNS typically employs Simulated Annealing (SA) acceptance criterion (Henderson et al., 2003), where non-improving solutions may be accepted with probability that decreases over time. The performance of LNS critically depends on the coupling between and r: effective operators must introduce targeted structural disruption while enabling efficient reconstruction. Designing such complementary operator pairs remains central challenge and motivates automation. 2 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design 2.3. Problem Formulation We formulate the automated design of LNS operators as an optimization problem over discrete code space. Let Θd and Θr denote the spaces of executable code implementing destroy and repair operators, respectively. policy π = (d, r) Θd Θr is evaluated by running an LNS algorithm with inherent stochasticity ξ on instances D. The expected performance of π is defined as J(π) = EID,ξ (cid:2)f (A(I, π; ξ) I)(cid:3), and the overall objective is to identify π = arg min πΘdΘr J(π). (2) (3) 2.4. Related Work Recent work has integrated large language models (LLMs) into evolutionary frameworks for automated heuristic design (Novikov et al., 2025; Chen et al., 2025a). Methods such as FunSearch (Romera-Paredes et al., 2024) and EoH (Liu et al., 2024b) use LLMs as variation operators to generate heuristic code conditioned on prior implementations and performance feedback, forming thoughtcode co-evolution paradigm that improves exploration efficiency in large discrete algorithm spaces. However, most existing approaches rely on fixed algorithmic templates, focusing on constructive heuristics or parameter tuning within predefined local search frameworks(Ye et al., 2024; Zheng et al., 2025), and thus largely overlook the structural design of neighborhood operators. In contrast, our work targets the automated generation of tightly coupled destroy-and-repair operators, enabling LLMs to reshape the search process at the structural level. Further discussion is provided in the appendix (Section A). 3. Methodology 3.1. Overview of the G-LNS Framework We propose G-LNS, framework for automating the discovery of high-performance destroy and repair operators in Large Neighborhood Search (LNS). G-LNS formulates heuristic design as an evolutionary process over algorithmic structures, where large language models (LLMs) act as intelligent variation operators to explore the space of algorithmic logic beyond fixed heuristic templates. This evolutionary formulation requires reliable mechanism to assess the quality of newly generated operators. To this end, we instantiate the LNS algorithm with an adaptive scoring mechanism inspired by Adaptive Large Neighborhood Search (ALNS) (Ropke & Pisinger, 2006), which provides quantitative fitness signal by measuring each operators contribution to optimization performance. As illustrated in Figure 2, the overall methodology consists of four phasesInitialization, Evaluation, Population Management, and Evolution. 3.2. Initialization To facilitate the co-evolution of these interdependent components, G-LNS establishes dual-population architecture(Potter & De Jong, 1994), maintaining distinct repositories for destroy operators (Pd) and repair operators (Pr), each with capacity of . The initialization phase begins by injecting compact set of classic domain-expert heuristics as seeds. For instance, in TSP/VRP tasks, we utilize Random Removal and Worst Removal for Pd, and Greedy Insertion for Pr. These seeds serve dual purpose: they provide foundational search capabilities and function as InContext Examples(Brown et al., 2020) to align the LLM with the specific task logic. To fully populate the pools (when the number of seeds < ), we employ the Initialization Action (denoted as i1 for destroy and i2 for repair). Through i1, i2, the LLM is prompted to conceptualize novel algorithmic logic and translate it into executable Python code, thereby ensuring the diversity of the search space from the onset. (See Appendix D.1 for specific details on prompt engineering). Simultaneously, we initialize three metric structures to guide the evolutionary process. (1) Global Fitness Scores (F = {F d, r} with d, RN ): Initialized to zeros, these vectors track the cumulative performance of each operator in Pd and Pr for population management. (2) Synergy Matrix (S RN ): Initialized to zeros, this matrix records the cooperative performance of specific pairs (di, rj) to guide joint crossover. (3) Adaptive Weights (W = {W d, r} with d, RN ): Initialized to ones, these weights are used solely within each evaluation episode to regulate the roulette wheel selection probabilities. 3.3. Evaluation Phase The objective of this phase is twofold: to quantify the individual contribution of each operator for population management, and to capture the coupling effectiveness between destroy and repair operators for synergistic evolution. Given the inherent stochasticity of LNS, we employ MultiEpisode Evaluation Mechanism. We conduct independent evaluation episodes. Each episode starts from random initial solution x0 and executes for iterations. Importantly, the Adaptive Weights are reset to 1 at the beginning of each episode to ensure independent exploration, whereas the Global Fitness and Synergy Matrix accumulate statistics across all episodes to obtain robust performance metrics that smooth out single-episode stochasticity. G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Figure 2. The overall workflow of the G-LNS framework. The framework operates in cyclic manner consisting of four phases: (1) Initialization, where the dual populations (Pd, Pr) are seeded with domain expertise and LLM-generated operators; (2) Evaluation, where operator pairs are dynamically selected and scored via an Adaptive LNS process; (3) Population Management, which ranks and prunes low-performing operators; and (4) Evolution, leveraging LLMs to perform mutation and crossover strategies to replenish the population with novel heuristics. In each iteration of an episode, we Adaptive Selection. select destroy operator di and repair operator rj based on their current weights. The selection probability (di) (and similarly for rj) follows the Roulette Wheel mechanism: 1. Adaptive Weights Update: To guide the search direction during the current episode, the weights of the selected operators are updated dynamically using smoothing factor λ: (di) = wd k=1 wd (cid:80)N (4) Scoring and Update. The selected pair (di, rj) is applied to the current solution to generate neighbor x. Its acceptance and the corresponding reward σ are determined hierarchically. If improves the global best, we update both and xcurr and assign σ1. If only improves the current solution, we update xcurr and assign σ2. For deteriorating solutions, acceptance is governed by the Metropolis criterion; if accepted, we update xcurr with reward σ3. Otherwise, the solution is discarded and assigned the lowest reward (σ4). Formally: σ = if (x) < (x) if (x) < (xcurr) if is accepted by SA σ1, σ2, σ3, σ4, otherwise Based on σ, we perform three distinct updates: (5) 4 wd λwd + (1 λ)σ, wr λwr + (1 λ)σ (6) 2. Global Fitness Accumulation: To evaluate the overall quality of an operator for the subsequent Population Management phase, we accumulate the reward into global fitness score : (di) (di) + σ, (rj) (rj) + σ (7) 3. Synergy Recording: To identify high-performing structural couplings, we update the corresponding entry in the synergy matrix: Sij Sij + σ (8) High values in Sij indicate that destroy operator and repair operator possess complementary logic, which will be exploited in the Synergistic Joint Crossover phase. 3.4. Population Management As the evolutionary process progresses, the operator pools Pd and Pr inevitably accumulate sub-optimal or redundant G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design heuristics. Retaining these inefficient operators restricts the populations capacity to accommodate new, potentially superior logic. Therefore, upon the completion of every evaluation episodes, we rank the operators in both populations based on their accumulated Global Fitness . The system then prunes the bottom operators from each pool, thereby freeing up population slots for the subsequent LLM-driven Evolution phase while preserving the high-performing elites. 3.5. LLM-Driven Evolution Mechanism This phase leverages the code reasoning capabilities of LLMs to replenish the population slots vacated during management. We frame this as heuristic search process within the algorithm space. To fill the empty slots, we randomly sample from three targeted evolutionary strategies, ensuring diverse mix of local refinements and structural recombinations. Mutation (Local Exploitation) (m1, m2). Mutation focuses on fine-tuning within the local algorithm space by modifying single parent operator sampled from the elite pool. The specific action adapts to the parents rank to balance stability and innovation: Logic Evolution (m1) is applied to lower-ranking elites to generate novel mechanisms for exploration; intermediate elites are assigned randomly; Parameter Calibration (m2) is applied to top-ranking elites to adjust hyperparameters (e.g., randomization ratios) for stability. Homogeneous Crossover (Feature Recombination) (c1). This strategy facilitates information exchange between operators of the same type. Two parents are selected via fitnessproportionate sampling based on . The LLM is prompted to fuse the logical strengths of both parents, synthesizing new operator that inherits hybrid characteristics (e.g., combining spatial clustering with random perturbation). Synergistic Joint Crossover (Structural Coupling) (c2). Addressing the inherent coupling between destroy and repair actions is core innovation of G-LNS. We select the Destroy-Repair pair with the highest accumulated synergy score from S. By conditioning the repair generation on the destroy logic, the LLM evolves this pair as unified entity, ensuring the repair operator is specifically tailored to reconstruct the structural defects introduced by the destroy operator, thereby maximizing their synergistic performance. Robustness Guarantee. To mitigate the risk of LLM hallucinations, we implement Pre-evaluation Filter(Chen, 2021). All generated operators are subjected to sanity check on small-scale instance set(Romera-Paredes et al., 2024). Only those that are error-free and satisfy time complexity constraints are admitted to the population; otherwise, the regeneration process is triggered. State Reset. Once the populations are fully replenished, the Global Fitness scores and the Synergy Matrix are reset to zero. This ensures that the newly generated operators start on equal footing with the surviving elites in the subsequent evaluation cycle, preventing historical bias from dominating the search. 4. Experiments To ensure rigorously fair comparison, our experimental design strictly follows the experimental settings established in the aforementioned LLM-based AHD baselines. We evaluate G-LNS across three distinct domains: TSP, CVRP, and OVRP. Following these conventions, we utilize randomly generated instances for the evolution phase. During the testing phase, the learned operators are evaluated on both held-out generated instances and widely-used benchmark datasets (i.e., TSPLib(Reinelt, 1991) and CVRPLib(Uchoa et al., 2017)) to verify their cross-distribution generalization. To mitigate the impact of randomness, we conduct three independent evolutionary runs for each task, capping the evolution process at Gmax = 200 generations. This configuration represents significant reduction compared to the 1,000 generations typically employed by the baselines, demonstrating that our framework achieves superior sample efficiency with substantially lower token budget. In the final testing phase, the best-found operator pair is applied to test instances with fixed budget of Ttest = 500 LNS iterations (increased from the = 100 iterations configured for the evolution phase) to rigorously measure solution quality. Settings. We employ DeepSeek-V3.2(Liu et al., 2024a) as the core LLM for operator generation. Regarding the evolutionary framework defined in Section 3, we maintain population size of = 5 for both destroy and repair operator pools, and prune the bottom = 2 operators in each management phase. For the evaluation process, we conduct = 10 independent episodes, each consisting of = 100 iterations. The hyperparameters for the inner LNS loop are configured as follows: initial temperature T0 = 100, cooling rate α = 0.97, destruction ratio ϵ = 0.2, and weight update parameter λ = 0.5. The scoring vector is set to σ = {1.5, 1.2, 0.8, 0.1}. Detailed descriptions of the evaluation datasets are provided in Appendix C.1. To ensure fair comparison, all LLM-based AHD baselines utilize the same DeepSeek-V3.2 backbone. Baseline. To verify the performance of G-LNS, we compare it against baselines from three categories: (1) Handcrafted heuristics. We employ LKH-3(Helsgaun, 2017) as the state-of-the-art baseline for TSP. For CVRP and OVRP, we utilize the widely-adopted solver OR-Tools to provide high-quality reference solutions. (2) Neural Combinatorial Optimization (NCO) methods, specifically POMO(Kwon et al., 2020), which serves as representative constructive learning baseline for routing problems. (3) LLM-based 5 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design AHD methods, including FunSearch(Romera-Paredes et al., 2024), EoH(Liu et al., 2024b), ReEvo(Ye et al., 2024), EVOMCTS(Wang & Zeng, 2025), and MCTS-AHD(Zheng et al., 2025). Regarding the latter, we also evaluate its variant applied to Ant Colony Optimization (ACO), which evolves pheromone update rules and serves as representative iterative AHD baseline. Additionally, we include Standard ALNS equipped with classic domain-specific operators as baseline to demonstrate the effectiveness of the generated operators(Ropke & Pisinger, 2006). It is worth noting that while existing AHD methods primarily focus on evolving Constructive priority rules or tuning parameters for guided search, G-LNS extends the design space to the structural destroy-and-repair logic of LNS. 4.1. Experimental Results Performance on synthetic held-out instances. As evidenced in Table 1 (Top), G-LNS (Ours) consistently achieves the lowest optimality gaps among all LLM-driven methods across all problem sizes on TSP tasks. For largescale instances such as TSP100 and TSP200, our framework significantly outperforms both Evo-MCTS and the iterative MCTS-AHD(ACO), effectively overcoming the scalability challenges that often lead existing AHD methods to exhibit gaps exceeding 10%. This superior performance is driven by the evolution of state-dependent destroy and repair logic, which allows G-LNS to outperform Standard ALNS detailed in Appendix E. Specifically, it identifies Adaptive Continuous-Segment Removal and Diversity-Aware Insertion for TSP, and Progressive Stochastic-Worst Removal paired with Context-Aware Greedy Insertion for CVRP. Unlike static rules, these operators dynamically adjust destruction magnitude and exploration noise based on real-time solution states, enabling superior escape from local optima. On CVRP tasks (Table 1, Bottom), G-LNS exhibits remarkable scalability and robustness in the face of complex capacity constraints. These constraints often pose significant challenges for traditional constructive heuristics, which tend to get trapped in local optima due to the irreversible nature of their sequential decisions. While G-LNS performs slightly below the optimal reference solutions on smallscale instances, its advantages become more pronounced as the problem complexity increases. On the largest instances (CVRP100/200), G-LNS not only outperforms all LLM-based baselineswhich typically show optimality gaps exceeding 10%but also identifies solutions superior to those provided by the OR-Tools solver. The ability to navigate this constrained solution space suggests that the learned destroy-and-repair operators can successfully correct structural defects that constructive methods are unable to address.(See Appendix F.1 for details on OVRP). G-LNS achieves superior solution quality while exhibiting significantly higher computational efficiency compared to benchmark methods. In CVRP experiments, while the ORTools baseline utilizes fixed budget of 320 seconds for the evaluation batch (5 seconds per instance for 64 instances), G-LNS requires substantially less time across all problem scales: its total inference time for 500 iterations ranges from merely 3.23s for CVRP10 to 280.91s for CVRP200. In contrast, MCTS-AHD, another iterative approach, incurs much heavier computational burden, requiring 84.16s for CVRP10 and 2407.14s for CVRP200, making G-LNS over an order of magnitude faster. These results demonstrate that G-LNS can identify higher-quality solutions than both the standard solverwhich fails to fully converge within the time limit and leaves gap of 1.27%2.09% and expensive iterative heuristics, all while consuming much smaller computational budget. Robust generalization to real-world benchmarks. To further assess the cross-distribution generalization capability of G-LNS, we extended our evaluation to widely recognized standard benchmarks, including TSPLib(Reinelt, 1991) and CVRPLib(Uchoa et al., 2017). These datasets feature diverse problem distributions and scales that differ significantly from the random instances used during evolution. As detailed in Appendix F.2, G-LNS demonstrates superior generalization performance compared to state-of-the-art AHD methods, including the strong baseline EoH-S (Liu et al., 2025). G-LNS consistently achieves the lowest optimality gaps across all evaluated datasets. Notably, on the challenging CVRPLib Set F, our method reduces the optimality gap from 40.1% (EoH-S) to 15.9%. Similarly, on TSPLib, G-LNS maintains low gap of 2.8%, significantly outperforming baselines that struggle with unseen distributions. These results confirm that the destroy-and-repair operators evolved by G-LNS capture intrinsic structural properties of combinatorial problems rather than merely overfitting to the training distribution. 4.2. Ablation Studies To validate the necessity of each evolutionary strategy and component within the G-LNS framework, we conducted ablation studies on the TSP50 and CVRP50 dataset. We established the full G-LNS as the baseline and compared it against four degenerated variants: w/o Mut. (No Mutation) excludes the local refinement strategies (i.e., Logic Evolution and Parameter Calibration). The evolution relies entirely on crossover operations, assessing the impact of fine-tuning single operators. w/o Homo. (No Homogeneous Crossover) removes the mechanism of fusing operators of the same type. New operators are generated solely through mutation or synergistic pairing, testing the benefit of recombining high-level logic features within the same operator class. w/o Syn. (No Synergistic Joint 6 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Table 1. Performance comparison on TSP and CVRP instances across five problem sizes. Top: Results for TSP, where G-LNS is evolved on TSP50 and evaluated on 64 held-out instances for each size. Optimal solutions are derived using LKH-3. Bottom: Results for CVRP (C = 50) under the same evolution and evaluation protocol. Reference solutions are obtained using OR-Tools (total 320 s; 5 per instance). For all LLM-based AHD methods, reported values represent the average of three independent runs. The overall best results are underlined, and the best results among LLM-based AHD methods are highlighted in bold. Traveling Salesman Problem (TSP) TSP10 TSP20 TSP50 TSP TSP200 Method Gap Obj. Gap Obj. Optimal (LKH-3) POMO ALNS FunSearch EoH ReEvo MCTS-AHD Evo-MCTS MCTS-AHD(ACO) Ours 0.00% 2.833 0.00% 2.833 0.25% 2.840 5.38% 2.986 4.73% 2.967 3.15% 2.922 2.85% 2.914 2.09% 2.892 0.11% 2.836 0.00% 2.833 0.00% 3.825 0.05% 3.827 3.53% 3.960 11.78% 4.276 9.07% 4.172 6.97% 4.092 7.84% 4.125 4.96% 4.015 0.27% 3.836 0.01% 3. Gap 0.00% 0.43% 5.65% 15.27% 14.73% 10.86% 11.24% 7.82% 1.21% 0.37% Obj. 5.717 5.741 6.040 6.590 6.559 6.338 6.359 6.164 5.786 5. Gap 0.00% 2.34% 4.85% 17.24% 17.28% 12.88% 12.02% 9.54% 3.45% 1.10% Obj. 7.813 7.996 8.192 9.160 9.163 8.820 8.753 8.599 8.083 7. Gap Obj. 0.00% 10.665 20.35% 12.835 5.96% 11.290 17.62% 12.544 17.84% 12.568 14.77% 12.240 13.16% 12.068 10.20% 11.753 6.22% 11.329 1.31% 10.805 Capacitated Vehicle Routing Problem (CVRP, = 50) CVRP CVRP20 CVRP50 CVRP100 CVRP200 Method OR-Tools POMO ALNS FunSearch EoH ReEvo MCTS-AHD Evo-MCTS MCTS-AHD(ACO) Ours Gap Obj. Gap Obj. 0.00% 3.096 2.42% 3.171 2.45% 3. 11.94% 3.466 11.31% 3.446 11.10% 3.440 11.10% 3.440 8.07% 3.346 2.80% 3.183 1.44% 3.141 0.00% 4.606 3.81% 4.781 4.62% 4.818 26.34% 5.819 21.38% 5.591 22.00% 5.619 20.65% 5.557 17.52% 5.412 6.85% 4.921 2.20% 4.707 Gap 0.00% 4.27% 4.26% Obj. 8.145 8.493 8.492 33.49% 11.014 26.88% 10.468 24.94% 10.308 23.88% 10.221 21.67% 10.038 9.094 11.65% 8.250 1.29% Gap Obj. Gap Obj. 2.09% 14.106 4.69% 14.466 3.43% 14.291 31.75% 18.205 27.27% 17.585 24.42% 17.192 23.77% 17.102 19.50% 16.512 12.90% 15.600 0.00% 13.817 1.27% 25.088 28.11% 31.738 3.63% 25.674 24.87% 30.936 19.50% 29.606 17.60% 29.135 16.37% 28.831 12.79% 27.942 10.52% 27.380 0.00% 24.774 Crossover) decouples the evolution of destroy and repair operators. Instead of evolving complementary pairs based on the synergy matrix S, it treats the populations independently. w/o Adapt. (No Adaptive Weights) disables the Adaptive LNS scoring mechanism during the evaluation phase, where operators are selected with uniform probability to verify the importance of dynamic resource allocation. Furthermore, to evaluate the robustness of the adaptive mechanism, we conducted sensitivity analysis on the scoring vector σ = {σ1, σ2, σ3, σ4} which governs operator weight updates. We compared our default setting against Flat configuration ({1, 1, 1, 0.1}), where rewards for different success levels are indistinguishable, and an Aggressive configuration ({10, 5, 2, 0}), amplifying the reward variance to verify the necessity of hierarchical reward system. Table 2 shows G-LNS achieves the lowest gaps, validating our evolutionary strategies. (1) Evolutionary Components: Significant drops in w/o Mut. and w/o Homo. highlight the importance of logic fine-tuning and feature recombination. The degradation in w/o Syn. confirms the structural Table 2. Ablation study of the key components in G-LNS. Each variant removes one componentMutation (Mut.), Homogeneous Crossover (Homo.), Synergistic Crossover (Syn.), or Adaptive Weights (Adapt.)to assess its impact on solution quality for TSP50 and CVRP50. The best results are highlighted in bold. G-LNS (Original) ALNS w/o Mut. w/o Homo. w/o Syn. w/o Adapt. G-LNS Flat G-LNS Aggressive TSP 0.37% 5.65% 1.55% 1.40% 1.24% 0.95% 1.69% 0.51% CVRP50 1.29% 4.26% 1.96% 2.03% 1.87% 1.68% 2.31% 1.50% coupling between destroy and repair operators; independent evolution disrupts their synergy. (2) Scoring Mechanism: The decline in w/o Adapt. verifies the feedback loops value. Notably, the Flat vector performs worse than removing adaptivity entirely, proving that indistinguishable 7 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Figure 3. Convergence and Evolutionary Analysis. (a) Evolutionary Progress: Validation score trajectory of the best operator over 200 generations; the steady decline confirms the LLMs ca- (b) Evaluation pacity to evolve high-performance heuristics. Progress: Convergence comparison on CVRP100 instances. GLNS identifies the best solution in 70s across all 64 instances, significantly outperforming both the Solver (320s) and MCTSAHD(ACO) (1110s) in terms of search efficiency. rewards mislead the search. Meanwhile, the Aggressive setting falls short of the default, confirming the necessity of balanced hierarchical reward system. 4.3. Convergence Analysis Figure 3 illustrates the dual performance characteristics of G-LNS: the progressive improvement of operator quality during evolution (Fig. 3a) and the rapid convergence of the final evolved operators during evaluation (Fig. 3b). Evolutionary Efficiency. Figure 3(a) depicts the validation performance of the elite operators across 200 generations. We observe rapid quality improvement in the initial phase (Generations 0-50), indicating that the LLM quickly grasps the fundamental logic of destroy-and-repair operations from the seed examples. Subsequently, the curve exhibits steady refinement trend (Generations 50-200), where the framework fine-tunes the operator logic to escape local optima. The narrowing variance (shaded area) suggests that the population converges towards set of robust and high-performing heuristics. Solving Convergence. Figure 3(b) compares the convergence behavior of G-LNS against representative baselines on CVRP100 instances. Several key observations can be drawn: (1) Superior Convergence Rate: Compared to Standard ALNS and MCTS-AHD(ACO), G-LNS demonstrates significantly steeper descent in the early iterations. This suggests that the LLM-generated destroy operators possess stronger structural perturbation capabilities, allowing the search to quickly identify promising regions in the solution space. (2) Beating the Solver: G-LNS surpasses the solution quality of constructive baselines within the first 50 iterations. More importantly, it eventually converges to solution (Obj 13.8) superior to that of the OR-Tools solver (Obj 14.1). (3) Computational Efficiency: GLNS achieves state-of-the-art performance in approximately 70 seconds, which is not only 4.5 faster than the 320second budget allocated to OR-Tools , but also orders of 8 (a) Before (C = 11.26) (b) Destroy (c) Repair (C = 9.96) Figure 4. Case Study on Structural Reshaping. Visualizing snapshot of the evolutionary process on CVRP50. (a-b) The generated repair operator targets the entangled region for destruction. (c) The destroy operator resolves the crossing by optimizing node-to-vehicle assignments, reducing the cost from 11.26 to 9.96. magnitude more efficient than MCTS-AHD(ACO), which requires 1110 seconds. This dramatic reduction in computational overhead highlights the practical value of our evolved heuristics for time-critical applications. 4.4. Case Study Figure 4 illustrates representative snapshot of single optimization iteration during the evolutionary process of G-LNS on CVRP50 instance. The iteration begins with solution trapped in local optimum (Cost = 11.26, Fig. 4a), characterized by crossing edges and inefficient routings. In the Destroy phase (Fig. 4b), applying destruction rate of ϵ = 0.2, the evolved PSWR operator (Appendix E.2) exhibits targeted strategy. It specifically identifies and removes nodes involved in the most entangled regions, effectively dismantling the sub-optimal structures to facilitate re-optimization. Subsequently, the ACAGI repair operator (Fig. 4c) reconstructs the solution. Crucially, this process goes beyond merely re-sequencing nodes within their original paths. As shown in the transition from (b) to (c), the operator dynamically reassigns customers to different routes, correcting sub-optimal node-to-vehicle assignments. This simultaneous optimization of clustering and sequencing successfully untangles the crossings, significantly reducing the cost to 9.96. 5. Conclusion In this paper, we introduced G-LNS, generative evolutionary framework that overcomes the structural limitations of existing LLM-based Automated Heuristic Design by co-evolving tightly coupled destroy and repair operators. Through synergy-aware evaluation mechanism and novel crossover strategies, G-LNS successfully discovers sophisticated heuristic logic that significantly outperforms stateof-the-art baselines and strong classical solvers on complex routing problems, while demonstrating superior generalization capabilities. For future work, we plan to extend G-LNS to multi-objective optimization settings and investigate its applicability to broader range of combinatorial problems beyond routing tasks. G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design 6. Acknowledge We express our gratitude to the team behind the LLM4AD platform(Liu et al., 2024d). Their open-source framework significantly facilitated the implementation of baseline methods and provided robust environment for our comparative experiments. We also thank the DeepSeek team for developing the DeepSeek-V3.2 model(Liu et al., 2024a), which served as the core LLM in our framework. This work was supported by the National Key Research and Development Program of China under Grant No. 2021YFC2203004. HW acknowledges support from the National Natural Science Foundation of China (NSFC) under Grant Nos. 12547104 and 12405076."
        },
        {
            "title": "References",
            "content": "Beck, J. C., Feng, T., and Watson, J.-P. Combining constraint programming and local search for job-shop INFORMS Journal on Computing, 23(1): scheduling. 114, 2011. Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016. Bengio, Y., Lodi, A., and Prouvost, A. Machine learning for combinatorial optimization: methodological tour dhorizon. European Journal of Operational Research, 290(2):405421, 2021. Burke, E. K., Hyde, M. R., Kendall, G., Ochoa, G., Ozcan, E., and Woodward, J. R. classification of hyperheuristic approaches: revisited. In Handbook of metaheuristics, pp. 453477. Springer, 2018. Chen, C., Zhong, M., Sun, J., Fan, Y., and Shi, J. Hifoprompt: Prompting with hindsight and foresight for llm-based automatic heuristic design. arXiv preprint arXiv:2508.13333, 2025a. Chen, H., Wang, Y., Cai, Y., Hu, H., Li, J., Huang, S., Deng, C., Liang, R., Kong, S., Ren, H., et al. Heurigym: An agentic benchmark for llm-crafted heuristics in combinatorial optimization. arXiv preprint arXiv:2506.07972, 2025b. Chen, M. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Da Ros, F., Soprano, M., Di Gaspero, L., and Roitero, K. Large language models for combinatorial optimization: systematic review. arXiv preprint arXiv:2507.03637, 2025. Desale, S., Rasool, A., Andhale, S., and Rane, P. Heuristic and meta-heuristic algorithms and their relevance to the real world: survey. Int. J. Comput. Eng. Res. Trends, 351(5):23497084, 2015. Dreo, J. Metaheuristics for hard optimization: methods and case studies. Springer Science & Business Media, 2006. Blot, A., Hoos, H. H., Jourdan, L., Kessaci-Marmion, M.- E., and Trautmann, H. Mo-paramils: multi-objective automatic algorithm configuration framework. In International Conference on Learning and Intelligent Optimization, pp. 3247. Springer, 2016. Duflo, G., Kieffer, E., Brust, M. R., Danoy, G., and Bouvry, P. gp hyper-heuristic approach for generating tsp heuristics. In 2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW), pp. 521529. IEEE, 2019. Blum, C. and Roli, A. Metaheuristics in combinatorial optimization: Overview and conceptual comparison. ACM computing surveys (CSUR), 35(3):268308, 2003. Bresson, X. and Laurent, T. The transformer network arXiv preprint for the traveling salesman problem. arXiv:2103.03012, 2021. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Fu, Z.-H., Qiu, K.-B., and Zha, H. Generalize small pre-trained model to arbitrarily large tsp instances. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 74747482, 2021. Garey, M. R. and Johnson, D. S. Computers and intractability, volume 29. wh freeman New York, 2002. Gendreau, M., Potvin, J.-Y., et al. Handbook of metaheuristics, volume 2. Springer, 2010. Glover, F., Gutin, G., Yeo, A., and Zverovich, A. Construction heuristics for the asymmetric tsp. European Journal of Operational Research, 129(3):555568, 2001. Burke, E. K., Gendreau, M., Hyde, M., Kendall, G., Ochoa, G., Ozcan, E., and Qu, R. Hyper-heuristics: survey of the state of the art. Journal of the Operational Research Society, 64(12):16951724, 2013. Guo, Q., Wang, R., Guo, J., Li, B., Song, K., Tan, X., Liu, G., Bian, J., and Yang, Y. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023. 9 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Helsgaun, K. An extension of the lin-kernighan-helsgaun tsp solver for constrained traveling salesman and vehicle routing problems. Roskilde: Roskilde University, 12: 966980, 2017. Liu, F., Yao, Y., Guo, P., Yang, Z., Lin, X., Zhao, Z., Tong, X., Mao, K., Lu, Z., Wang, Z., et al. systematic survey on large language models for algorithm design. ACM Computing Surveys, 2024c. Henderson, D., Jacobson, S. H., and Johnson, A. W. The theory and practice of simulated annealing. In Handbook of metaheuristics, pp. 287319. Springer, 2003. Huang, Z., Wu, W., Wu, K., Wang, J., and Lee, W.-B. Calm: Co-evolution of algorithms and language model for automatic heuristic design. arXiv preprint arXiv:2505.12285, 2025. Jiang, X., Wu, Y., Wang, Y., and Zhang, Y. Unco: Towards unifying neural combinatorial optimization through large language model. 2024. Johnson, D. S. and McGeoch, L. A. The traveling salesman problem: case study in local optimization. Local search in combinatorial optimization, 1(1):215310, 1997. Joshi, C. K., Cappart, Q., Rousseau, L.-M., and LauLearning the travelling salesperson probrent, T. lem requires rethinking generalization. arXiv preprint arXiv:2006.07054, 2020. Kambhampati, S., Valmeekam, K., Guan, L., Verma, M., Stechly, K., Bhambri, S., Saldyt, L., and Murthy, A. Llms cant plan, but can help planning in llm-modulo frameworks. arXiv preprint arXiv:2402.01817, 2024. Kool, W., Van Hoof, H., and Welling, M. Attention, arXiv preprint learn to solve routing problems! arXiv:1803.08475, 2018. Koza, J. R. Genetic programming as means for programming computers by natural selection. Statistics and computing, 4(2):87112, 1994. Kwon, Y.-D., Choo, J., Kim, B., Yoon, I., Gwon, Y., and Min, S. Pomo: Policy optimization with multiple optima for reinforcement learning. Advances in Neural Information Processing Systems, 33:2118821198, 2020. Li, F., Golden, B., and Wasil, E. The open vehicle routing problem: Algorithms, large-scale test problems, and computational results. Computers & operations research, 34 (10):29182930, 2007. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseekv3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Liu, F., Zhang, R., Xie, Z., Sun, R., Li, K., Lin, X., Wang, Z., Lu, Z., and Zhang, Q. Llm4ad: platform for algorithm design with large language model. arXiv preprint arXiv:2412.17287, 2024d. Liu, F., Liu, Y., Zhang, Q., Tong, X., and Yuan, M. Eohs: Evolution of heuristic set using llms for automated heuristic design. arXiv preprint arXiv:2508.03082, 2025. Lopez-Ibanez, M., Dubois-Lacoste, J., Caceres, L. P., Birattari, M., and Stutzle, T. The irace package: Iterated racing for automatic algorithm configuration. Operations Research Perspectives, 3:4358, 2016. Martı, R. and Reinelt, G. The linear ordering problem: exact and heuristic methods in combinatorial optimization, volume 175. Springer Science & Business Media, 2011. Matai, R., Singh, S. P., and Mittal, M. L. Traveling salesman problem: an overview of applications, formulations, and solution approaches. Traveling salesman problem, theory and applications, 1(1):125, 2010. Mei, Y., Chen, Q., Lensen, A., Xue, B., and Zhang, M. Explainable artificial intelligence by genetic programming: survey. IEEE Transactions on Evolutionary Computation, 27(3):621641, 2022. Nazari, M., Oroojlooy, A., Snyder, L., and Takac, M. Reinforcement learning for solving the vehicle routing problem. Advances in neural information processing systems, 31, 2018. Novikov, A., Vu, N., Eisenberger, M., Dupont, E., Huang, P.-S., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J., Mehrabian, A., et al. Alphaevolve: coding agent for scientific and algorithmic discovery. arXiv preprint arXiv:2506.13131, 2025. ONeill, M. Riccardo poli, william b. langdon, nicholas f. mcphee: field guide to genetic programming: Lulu. com, 2008, 250 pp, isbn 978-1-4092-0073-4, 2009. Pisinger, D. and Ropke, S. general heuristic for vehicle routing problems. Computers & operations research, 34 (8):24032435, 2007. Pisinger, D. and Ropke, S. Large neighborhood search. In Handbook of metaheuristics, pp. 99127. Springer, 2018. Liu, F., Tong, X., Yuan, M., Lin, X., Luo, F., Wang, Z., Lu, Z., and Zhang, Q. Evolution of heuristics: Towards efficient automatic algorithm design using large language model. arXiv preprint arXiv:2401.02051, 2024b. Potter, M. A. and De Jong, K. A. cooperative coevolutionary approach to function optimization. In International conference on parallel problem solving from nature, pp. 249257. Springer, 1994. 10 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Reinelt, G. Tspliba traveling salesman problem library. ORSA journal on computing, 3(4):376384, 1991. Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S., Wang, P., Fawzi, O., et al. Mathematical discoveries from program search with large language models. Nature, 625 (7995):468475, 2024. Ropke, S. and Pisinger, D. An adaptive large neighborhood search heuristic for the pickup and delivery problem with time windows. Transportation science, 40(4):455472, 2006. Shaw, P. Using constraint programming and local search methods to solve vehicle routing problems. In International conference on principles and practice of constraint programming, pp. 417431. Springer, 1998. Song, J., Yue, Y., Dilkina, B., et al. general large neighborhood search framework for solving integer linear programs. Advances in Neural Information Processing Systems, 33:2001220023, 2020. Stutzle, T. and Lopez-Ibanez, M. Automated design of metaheuristic algorithms. In Handbook of metaheuristics, pp. 541579. Springer, 2018. Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014. Toth, P. and Vigo, D. The vehicle routing problem. SIAM, 2002. Toth, P. and Vigo, D. Vehicle routing: problems, methods, and applications. SIAM, 2014. Uchoa, E., Pecin, D., Pessoa, A., Poggi, M., Vidal, T., and Subramanian, A. New benchmark instances for the capacitated vehicle routing problem. European Journal of Operational Research, 257(3):845858, 2017. Vinyals, O., Fortunato, M., and Jaitly, N. Pointer networks. Advances in neural information processing systems, 28, 2015. Voudouris, C. and Tsang, E. Partial constraint satisfaction problems and guided local search. Proc., Practical Application of Constraint Technology (PACT96), London, pp. 337356, 1996. Voudouris, C. and Tsang, E. Guided local search and its application to the traveling salesman problem. European journal of operational research, 113(2):469499, 1999. Wang, H. and Zeng, L. Automated algorithmic discovery for gravitational-wave detection guided by llm-informed evolutionary monte carlo tree search. 2025. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wen, M., Linde, E., Ropke, S., Mirchandani, P., and Larsen, A. An adaptive large neighborhood search heuristic for the electric vehicle scheduling problem. Computers & Operations Research, 76:7383, 2016. Wouda, N. A. and Lan, L. Alns: python implementation of the adaptive large neighbourhood search metaheuristic. Journal of Open Source Software, 8(81):5028, 2023. Wu, Z., Qi, Q., Zhuang, Z., Sun, H., and Wang, J. Pretokenization of numbers for large language models. In The Second Tiny Papers Track at ICLR 2024, 2024. Xie, Z., Liu, F., Wang, Z., and Zhang, Q. Llm-driven neighborhood search for efficient heuristic design. In 2025 IEEE Congress on Evolutionary Computation (CEC), pp. 18. IEEE, 2025. Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., and Chen, X. Large language models as optimizers. In The Twelfth International Conference on Learning Representations, 2023. Ye, H., Wang, J., Cao, Z., Berto, F., Hua, C., Kim, H., Park, J., and Song, G. Reevo: Large language models as hyperheuristics with reflective evolution. Advances in neural information processing systems, 37:4357143608, 2024. Ye, H., Xu, H., Yan, A., and Cheng, Y. Large language model-driven large neighborhood search for large-scale milp problems. In Forty-second International Conference on Machine Learning, 2025. Zeng, L., Zhong, L., Zhao, L., Wei, T., Yang, L., He, J., Cheng, C., Hu, R., Liu, Y., Yan, S., et al. Skyworkmath: Data scaling laws for mathematical reasoning in large language modelsthe story goes on. arXiv preprint arXiv:2407.08348, 2024. Zeng, L., Li, Y., Xiao, Y., Li, C., Liu, C. Y., Yan, R., Wei, T., He, J., Song, X., Liu, Y., et al. Skywork-swe: Unveiling data scaling laws for software engineering in llms. arXiv preprint arXiv:2506.19290, 2025. Zhang, L. and Zeng, L. Sat-ldm: Provably generalizable image watermarking for latent diffusion models with selfaugmented training. arXiv preprint arXiv:2501.00463, 2024. Zheng, Z., Xie, Z., Wang, Z., and Hooi, B. Monte carlo tree search for comprehensive exploration in llm-based automatic heuristic design. arXiv preprint arXiv:2501.08603, 2025. 11 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design A. More Discussion on Related Work A.1. AHD Automated Heuristic Design (AHD), frequently referred to as Hyper-heuristics (Burke et al., 2013), aims to automate the discovery of optimization algorithms. Formally, instead of searching for an optimal solution in the solution space S, AHD searches for high-quality heuristic algorithm within an algorithm space H. The objective is to identify heuristics that generalize well across target distribution of problem instances, rather than overfitting to single case. Historically, Evolutionary Computation (EC) has served as the primary search strategy for AHD. Among various EC methods(Lopez-Ibanez et al., 2016; Blot et al., 2016; Burke et al., 2018), Genetic Programming (GP)(Koza, 1994; ONeill, 2009) has long been considered the prevailing approach. In the GP paradigm, heuristics are typically represented as syntax trees, which are evolved through genetic operations such as subtree crossover and point mutation to optimize their performance on training instances(Mei et al., 2022). Despite its success, traditional GP-based AHD faces significant bottleneck: the reliance on hand-crafted genetic operators. The mutation and crossover operators often require substantial domain expertise to ensure that the modified heuristics remain syntactically valid and semantically meaningful (Duflo et al., 2019). This dependency on manual design limits the flexibility of the search process, motivating the recent shift towards more intelligent, generative approaches for algorithm discovery. A.2. Neural Combinatorial Optimization (NCO) Neural Combinatorial Optimization (NCO) has emerged as promising paradigm to address the computational prohibitiveness of traditional exact solvers. The core motivation of NCO is to learn heuristics from data offline, enabling the generation of high-quality approximate solutions in real-time inference (Bello et al., 2016; Bengio et al., 2021). Sequence-to-Sequence Modeling. Early NCO approaches formulated the construction of solutions as sequence-tosequence (Seq2Seq) prediction task, similar to neural machine translation (Sutskever et al., 2014). However, standard Seq2Seq models struggled with combinatorial problems because the output vocabulary (e.g., the specific cities to visit) varies for each input instance, unlike the fixed vocabulary in translation tasks (Vinyals et al., 2015). To overcome this limitation, Vinyals et al. (2015) introduced the Pointer Network (Ptr-Net), which employs pointer mechanism to select input elements directly as outputs using attention scores. From Supervised Learning to RL. While Ptr-Nets laid the foundation, optimizing them via Supervised Learning proved impractical due to the high cost of obtaining optimal labels for large-scale instances (Bello et al., 2016). Consequently, the field shifted towards Reinforcement Learning (RL), treating the generation process as Markov Decision Process (Nazari et al., 2018). Bello et al. (2016) proposed an Actor-Critic framework where the neural network acts as policy to minimize the tour length, using the negative tour length as the reward signal. They also introduced Active Search to refine solutions during inference by sampling multiple trajectories to find the best candidate (Bello et al., 2016). Symmetry-Aware Transformer (POMO). Modern NCO methods have evolved to leverage the Transformer architecture for better feature extraction and long-range dependency modeling (Kool et al., 2018; Bresson & Laurent, 2021). Notably, POMO (Kwon et al., 2020) addressed critical start node bias inherent in previous constructive policies. By exploiting the rotational symmetry of routing problems, POMO generates diverse trajectories in parallel (one for each starting node) and uses the average reward as low-variance shared baseline (Kwon et al., 2020). This approach significantly stabilizes training and achieves state-of-the-art performance among constructive NCO methods. Limitations and The Shift to AHD. Despite their fast inference speed, NCO models fundamentally operate as black boxes and often suffer from poor generalization (Bengio et al., 2021). Their performance typically degrades significantly when applied to problem scales or distributions unseen during training (distributional shift) (Joshi et al., 2020; Fu et al., 2021). These limitations regarding interpretability and scalability have motivated the recent surge in LLM-based Automated Heuristic Design, which aims to evolve explicit, generalizable algorithmic code instead of opaque neural weights (Romera-Paredes et al., 2024; Liu et al., 2024b). A.3. LLM for Combinatorial Optimization Recent advancements have spurred paradigm shift in applying Large Language Models (LLMs) to Combinatorial Optimization (CO). We categorize existing methodologies into two distinct streams: LLM as Solver and LLM as Designer. 12 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Table 3. Comparison of LLM-based approaches for Combinatorial Optimization. Our G-LNS is unique in targeting the structural design of LNS operators. Method LLM Role Target Heuristic Type Search Strategy Key Characteristic / Focus LLM as Solver (Direct Inference) OPRO (Yang et al., 2023) Fine-tuned LLMs (Jiang et al., 2024) Solver Solver N/A (Direct Solution) N/A (Direct Solution) Iterative Prompting Optimizes solutions via natural language feedback history. Supervised Fine-tuning Enhances LLMs domain knowledge for specific CO constraints. LLM as Designer (Automated Heuristic Design) FunSearch (Romera-Paredes et al., 2024) EoH (Liu et al., 2024b) ReEvo (Ye et al., 2024) MCTS-AHD (Zheng et al., 2025) EoH-S (Liu et al., 2025) LHNS (Xie et al., 2025) Designer Designer Designer Designer Designer Designer Constructive Constructive Constructive Constructive Heuristic Set Constructive Evolution Evolution Reflective Evolution MCTS Evolution LNS The first Thought-Code evolution for mathematical discovery. Applies AHD to standard COPs with purely constructive rules. Introduces reflexivity to guide mutations via historical analysis. Uses Tree Search to balance global exploration and exploitation. Evolves complementary set of heuristics for better generalization. Applies LNS logic to perturb code blocks (Algorithm-level LNS). G-LNS (Ours) Designer LNS (Destroy & Repair) Synergistic Evolution Designs structural operators for solution-level LNS. LLM as Solver (Direct & Iterative Optimization). This paradigm treats LLMs as black-box optimizers, prompting them to output solutions directly based on problem descriptions. Early attempts employed zero-shot or few-shot prompting to solve small-scale instances (Guo et al., 2023). To improve performance, Yang et al. (2023) introduced Optimization by PROmpting (OPRO), where the LLM iteratively refines solutions using natural language feedback and optimization trajectories as in-context information. Other works explore fine-tuning LLMs specifically for CO tasks to enhance their understanding of constraints (Jiang et al., 2024). However, the LLM as Solver paradigm faces intrinsic limitations. First, LLMs struggle with the tokenization of highprecision coordinates and numerical reasoning, often perceiving numbers as linguistic tokens rather than mathematical values (Wu et al., 2024). Second, as noted by Kambhampati et al. (2024), LLMs function better as idea generators than reliable planners; they lack the rigorous backtracking capabilities required for NP-hard problems. Consequently, these methods are prone to hallucinations and scale poorly to large instances due to limited context windows. LLM as Designer (Automated Heuristic Design). Acknowledging the limitations of direct inference, the field has gravitated towards LLM-based AHD, repurposing LLMs to generate executable code. This paradigm ensures correctness and scalability by offloading execution to standard Python interpreters. Pioneering frameworks like FunSearch (Romera-Paredes et al., 2024) and EoH (Liu et al., 2024b) established the foundational Thought-Code evolutionary paradigm, treating LLMs as mutation operators to evolve populations of constructive heuristics(Huang et al., 2025). Advanced Search Strategies. To overcome the tendency of standard population-based methods to converge into local optima, recent works have introduced more sophisticated search mechanisms. ReEvo (Ye et al., 2024) integrates reflective evolution mechanism, mimicking human thinking to retrospectively analyze historical performance and guide more effective code mutations. Furthermore, MCTS-AHD (Zheng et al., 2025) and Evo-MCTS (Wang & Zeng, 2025) introduce Monte Carlo Tree Search (MCTS) into the evolutionary process. By organizing heuristics in tree structure, these methods balance exploration and exploitation, allowing for the comprehensive development of temporarily underperforming heuristics that standard populations might prematurely discard. Heuristic Set Evolution. Addressing the generalization bottleneck where single heuristic fails to cover diverse instance distributions, EoH-S (Liu et al., 2025) proposes the concept of Automated Heuristic Set Design (AHSD). Instead of optimizing solitary algorithm, EoH-S evolves complementary set of heuristics, ensuring that each problem instance is covered by at least one effective strategy in the set, thereby achieving superior cross-distribution performance. Differentiation from LLM-driven Heuristic Neighborhood Search. It is crucial to distinguish our approach from the recently proposed LHNS (Xie et al., 2025). LHNS applies the logic of neighborhood search to the algorithm space itselfiteratively perturbing heuristic code blocks to find better functions. While LHNS uses LNS-like concepts to guide the code search process, the output remains standard heuristic function. Summary: Paradigm Shift in Framework. As summarized in Table 3, in contrast to prior arts, our G-LNS does not merely evolve better scoring function or local guidance rule; it fundamentally alters the algorithmic framework. By explicitly tasking the LLM with designing structural destroy and repair operators, G-LNS enables the solver to perform complex topological transformations on the solutions. This represents shift from optimizing parameters/rules within fixed skeleton to automating the design of the solvers structural components, capability that extends beyond the scope of previous AHD methods. 13 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design A.4. Large Neighborhood Search (LNS) Origins and Evolution. The Large Neighborhood Search (LNS) paradigm, originally introduced by Shaw (1998) for Vehicle Routing Problems, utilizes ruin and recreate principle to escape local optima. Unlike local search methods that rely on small moves (e.g., 2-opt), LNS rearranges significant portion of the solution structure (Gendreau et al., 2010). pivotal advancement was the Adaptive LNS (ALNS) (Ropke & Pisinger, 2006), which maintains portfolio of operators and dynamically adjusts their selection probabilities based on historical performance. This adaptive mechanism established LNS as robust framework capable of handling diverse constraints without extensive parameter tuning (Pisinger & Ropke, 2007). Applications and Robustness. Due to its flexibility, LNS has become dominant meta-heuristic for wide array of NP-hard combinatorial optimization problems. In the domain of logistics, it has been successfully applied to the Pickup and Delivery Problem with Time Windows (Ropke & Pisinger, 2006) and the Electric Vehicle Routing Problem (Wen et al., 2016). Beyond routing, LNS has demonstrated exceptional performance in scheduling tasks, particularly for the Job Shop Scheduling Problem (Beck et al., 2011). Data-Driven and LLM-Enhanced LNS. Recent advancements have integrated Machine Learning with LNS, particularly for Mixed Integer Linear Programming (MILP). Approaches such as the general LNS framework by Song et al. (2020) and the LLM-LNS framework by Ye et al. (2025) employ learning-based techniquesranging from imitation learning to LLM reasoningto automate the neighborhood selection process. These methods focus on learning policy to select subset of variables for optimization, typically relying on off-the-shelf solvers (e.g., Gurobi) to solve the resulting sub-problems. In contrast, our G-LNS utilizes the generative capabilities of LLMs to explicitly write code for domain-specific destroy and repair operators, thereby evolving the underlying algorithmic logic independent of external solvers. B. Details of Optimization Problem B.1. Traveling Salesman Problem The Traveling Salesman Problem (TSP)(Matai et al., 2010) is quintessential NP-hard combinatorial optimization problem that serves as standard benchmark for heuristic algorithms. Given set of cities, the objective is to find the shortest possible closed tour that visits every city exactly once and returns to the starting point. Formally, an instance of TSP can be modeled as complete undirected graph = (V, E), where = {v1, . . . , vN } is the set of nodes (cities) and represents the edges connecting every pair of nodes. Each edge (i, j) is associated with distance dij, where node is represented by coordinate vector xi R2, and the cost dij = xi xj2 corresponds to the Euclidean distance between cities and j. Let π = (π1, π2, . . . , πN ) denote permutation of the node indices {1, . . . , }, representing the sequence of visited cities. The optimization goal is to find permutation π that minimizes the total tour length: min π (π) = 1 (cid:88) i=1 dπi,πi+1 + dπN ,π1 (9) LNS Application Example. To apply the LNS framework to the TSP, the search process iterates through Destroy and Repair phase to escape local optima(Wouda & Lan, 2023). Unlike local search methods (e.g., k-opt) that perform small edge swaps, LNS structurally decomposes the solution: Destroy Phase (Ruin): Given complete tour π, destroy operator d() removes subset of cities (denoted as Vrem V), leaving partial tour πpartial. For example, Random Removal operator might stochastically select nodes to remove, while Segment Removal operator removes contiguous sequence of cities to disrupt specific sub-path. Repair Phase (Recreate): repair operator r() takes the partial tour πpartial and the set of removed cities Vrem as input, re-inserting the nodes into the tour to form new feasible solution π. classic example is the Greedy Insertion, which iteratively inserts each node Vrem into the position (i, + 1) in πpartial that minimizes the incremental cost = dπi,v + dv,πi+1 dπi,πi+1. Through this mechanism, LNS can perform large moves in the search space, effectively reshaping the tour structure to find superior global solutions. 14 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design B.2. Capacitated Vehicle Routing Problem The Capacitated Vehicle Routing Problem (CVRP)(Toth & Vigo, 2002; 2014) is generalization of the TSP and fundamental problem in logistics and transportation. Unlike TSP, CVRP involves multiple vehicles serving set of customers, subject to vehicle capacity constraints. The objective is to design set of optimal routes that minimize the total travel cost while satisfying customer demands. Formally, CVRP instance is defined on graph = (V, E), where = {v0, v1, . . . , vN }. Here, node v0 represents the central depot, and Vc = {v1, . . . , vN } represents the set of customers. Each customer vi has positive demand qi, and the depot has demand q0 = 0. We are given fleet of identical vehicles, each with maximum capacity Q. solution consists of set of routes = {R1, R2, . . . , RK}, where each route Rk starts and ends at the depot v0. Let dij denote the travel distance (cost) between node and j. The objective is to minimize the total distance of all routes: min (cid:88) k= Cost(Rk) = min (cid:88) (cid:88) dij k=1 (i,j)Rk (10) subject to the following constraints: 1. Coverage: Every customer vi Vc must be visited exactly once by exactly one vehicle. 2. Capacity: The total demand of customers served in any single route Rk must not exceed the vehicle capacity, i.e., (cid:80) viRk qi Q. B.3. Open Vehicle Routing Problem The Open Vehicle Routing Problem (OVRP)(Li et al., 2007) is distinct variant of the classical CVRP. The fundamental difference lies in the route structure: in OVRP, vehicles are not required to return to the depot after servicing the last customer on their route. This problem formulation typically arises in third-party logistics scenarios where vehicles are hired for one-way trips, or in situations where drivers use their personal vehicles and return home directly after the last delivery. Formally, the problem is defined on the same graph = (V, E) as the CVRP, with depot v0 and customer set Vc. The constraints regarding customer coverage and vehicle capacity remain identical to those in CVRP. However, route Rk = (v0, vk1 , vk2, . . . , vkm) in OVRP forms Hamiltonian path rather than cycle. The objective is to minimize the total travel distance of the open routes. Mathematically, this can be expressed as minimizing the sum of edge weights in the active routes, excluding the return arcs to the depot: (cid:88) min mk1 (cid:88) dvkj ,vkj+ (11) where vk0 = v0 is the depot, and vkm is the last customer visited by vehicle k. Unlike CVRP, the cost term dvkm ,v0 is omitted. Consequently, the OVRP seeks to find set of paths that cover all customers with minimum total length, ending at any customer node. k=1 j=0 C. Details of Experiments C.1. Dataset Generation and Benchmarks Dataset Settings. Following the experimental protocols established in MCTS-AHD(Zheng et al., 2025), we adopt consistent data generation mechanism to ensure rigorous comparison. We strictly distinguish between the datasets used for discovering operators and those used for final evaluation. Training Set (Operator Discovery). The search for high-quality LNS operators is conducted on compact Training Set consisting of 16 instances with fixed problem size of = 50. Limiting the training to specific scale and small number of instances ensures that the discovered operators capture generalizable logic rather than overfitting to massive datasets. 15 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Testing Set (Performance Evaluation). The learned heuristics are subsequently evaluated on held-out Testing Set to assess their performance and scalability. This set comprises 64 instances for each problem scale {10, 20, 50, 100, 200}, allowing us to verify whether the operators trained on = 50 can effectively generalize to both smaller and larger instances. Instance Characteristics. The specific parameters for instance generation are configured as follows: TSP: Node coordinates are sampled uniformly from the unit square [0, 1]2. CVRP: Consistent with standard NCO settings, node coordinates are sampled from [0, 1]2 with the depot fixed at (0.5, 0.5). OVRP: The instance settings are identical to those of CVRP. Customer demands are integers sampled uniformly from [1, 9], and the vehicle capacity is set to = 50. For both CVRP and OVRP, customer demands are integers sampled uniformly from {1, . . . , 9}, and the vehicle capacity is set to = 50. To further assess cross-distribution generalization, we extend our evaluation to the standard TSPLib(Reinelt, 1991) and CVRPLib(Uchoa et al., 2017) benchmarks(See F.2 for details). C.2. Implementation Details For the baseline methods, we strictly adhered to their official open-source implementations to guarantee the reliability of the results: LKH-3: We utilized the official executable(Helsgaun, 2017) with default parameters. Deep Learning Baselines (POMO): We used the pre-trained models provided by the original authors and performed inference with greedy decoding (batch size = 1) to measure the raw inference speed without augmentation. ALNS: ALNS: We implemented the Adaptive Large Neighborhood Search based on the classic framework proposed by Pisinger & Ropke (2007). The operator portfolio includes diverse set of removal (random, worst, and related removal) and insertion (greedy and regret-k insertion) heuristics. To ensure competitive baseline, we adopted the standard parameter settings for the adaptive weight adjustment and the simulated annealing acceptance criterion, as tuned in the original work. This ensures that the performance of ALNS reflects its robust general capability rather than sub-optimal implementation. LLM-based AHD: For methods like EoH and MCTS-AHD, we reproduced the evolutionary process using the same LLM backend (DeepSeek-V3.2) and prompt engineering settings as described in their respective papers, ensuring that performance differences stem from the algorithm structure rather than the language model capability. C.3. Evaluation Budget and Efficiency To demonstrate the superior sample efficiency of G-LNS, we enforced strict constraint on the computational budget compared to standard baselines. LLM Interaction Budget. Existing LLM-based AHD methods typically rely on extensive trial-and-error, requiring substantial budget of 1,000 evolutionary generations (or interactions) to ensure convergence. In contrast, G-LNS is configured with significantly reduced budget of only 200 generations. Efficiency Analysis. Despite utilizing only 20% of the interaction budget required by baseline methods, G-LNS achieves superior performance as evidenced in Table 1 and Table 4. This 5 reduction in LLM queries translates directly to significantly lower token consumption and operational costs. It indicates that evolving high-level structural operators (Destroy and Repair) allows the search process to navigate the algorithm space much more efficiently than evolving low-level constructive rules, avoiding the need for massive brute-force sampling. 16 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design D. Details of Algorithm D.1. Prompts of G-LNS Actions G-LNS employs distinct evolutionary actions to discover high-performance heuristics. Next, we describe the meaning and prompt engineering of each action. To ensure robustness and standardized outputs, these prompts strictly contain the task background, existing heuristic references as contexts, function identification, input/output specifications, and logical constraints according to the specific optimization task. We execute the LLM calls through these structured prompts to obtain both the algorithmic design idea and its executable Python implementation. The rest of this subsection provides examples for prompts, in which we highlight the functional components in distinct colors: Initial Action i1 (Destroy Initialization): Action i1 represents directly getting an idea of designing valid Destroy Operator from scratch and Python implementation to populate the heuristic pool when domain-expert seeds are insufficient. Prompt for Operator i1(Destroy Initialization) The task is to design novel Destroy Operator for Large Neighborhood Search (LNS) framework. Given complete solution sequence (a tour of cities for TSP) and target number of elements to remove (destroy cnt), the function must determine which elements to remove. The objective is to develop removal strategy that effectively perturbs the current solution. This allows the subsequent Repair operator to reconstruct the solution in way that helps escape local optima and minimizes the total cost. You are an expert in heuristic optimization algorithms, specifically Adaptive Large Neighborhood Search (ALNS). Your task is to design new Destroy Operator (removal operator) for the following problem: Problem Description: {task description} Existing Destroy Operators (Reference): {operators str} Requirements: 1. First, describe your new algorithm and main steps in one sentence. The description must be inside brace. Next, implement it in Python as function named destroy. 2. This function must accept 3 inputs: current solution, destroy cnt, distance matrix. 3. The function must return 2 outputs: removed elements, partial solution. 4. The logic should be strictly different from the existing ones provided in the reference to improve population diversity. 5. Do not give additional explanations. Initial Action i2 (Repair Initialization): Action i2 focuses on initializing the Repair Operator population (Pr). It prompts the LLM to design constructive strategy for re-inserting removed elements into partial solution, ensuring the reconstructed tour minimizes total cost. Prompt for Operator i2 (Repair Initialization) The task is to design novel Repair Operator (Insertion Operator) for Large Neighborhood Search (LNS) framework. Given partial solution partial solution (where some elements have been removed) and list of removed elements, the function must determine the best positions to re-insert these elements to restore complete solution. The objective is to reconstruct the solution in way that minimizes the total cost You are an expert in heuristic optimization algorithms, specifically Large Neighborhood Search (LNS). Your task is to design new Repair Operator (insertion operator) for the following problem: Problem Description: {task description} 17 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Existing Repair Operators (Reference): {operators str} Requirements: 1. First, describe your new algorithm and main steps in one sentence. The description must be inside brace. Next, implement it in Python as function named repair. 2. This function must accept 3 inputs: partial solution, removed elements, distance matrix. 3. The function must return 1 output: complete solution. 4. The logic should be innovative and distinct from the reference operators to ensure diverse reconstruction paths. 5. Do not give additional explanations. Mutation Actions m1 & m2 (Adaptive Refinement): Actions m1 and m2 focus on fine-tuning single parent operator. The specific mutation strategy is adaptively selected based on the operators performance rank within the population: top-tier operators trigger Parameter Calibration (m2) to consolidate gains, bottom-tier operators trigger Logic Evolution (m1) to escape local optima, while intermediate operators are assigned strategy stochastically. Prompt for Mutation Actions (m1 & m2) You are an algorithm optimizer. We have {operator type} operator for LNS. Problem Description: {task description} Strategy: {advice} (The {advice} slot is dynamically filled with one of the following strict instructions based on the rank:) m1 (Logic Evolution): Generate novel algorithmic mechanisms or formulas to replace existing logic components. m2 (Parameter Calibration): Adjust current parameter settings (e.g., the degree of randomization or greedy thresholds) to optimize operator behavior. Current Code: {operator code} Task: Refine and improve this operator code based on the strategy above. 1. Refine and improve this operator strictly following the strategy provided above. 2. If you need helper functions, define them INSIDE the main function. 3. Do not give additional explanations. Homogeneous Crossover (c1) facilitates feature recombination within the same operator type. To ensure the propagation of high-quality traits, two parent operators are selected from the population using Roulette Wheel Selection based on their historical fitness scores. The LLM is then prompted to synthesize hybrid operator by preserving the structural form of Parent 2 while integrating the high-level logical insights from Parent 1. Prompt for Action c1 (Homogeneous Crossover) You are an expert in heuristic optimization. Your task is to create NEW {operator type} operator by combining the ideas/logic of two parent operators. Problem Description: {task description} Parent 1 Code (Inspiration Source): {parent1 code} Parent 2 Code (Structural Base): {parent2 code} Task: Please create new algorithm that has similar form to Parent 2 and is inspired by Parent 1. The new 18 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design algorithm should outperform both parents. 1. First, list the common ideas in Parent 1 that may give good performances. 2. Second, based on the common idea, describe the design idea of the new algorithm and its main steps in one sentence. 3. Next, implement it in Python. Requirements: 1. The new operator MUST follow the standard LNS {operator type} signature strictly. 2. Define all helper functions INSIDE the main function. 3. Do not give additional explanations. Synergistic Joint Crossover (c2) addresses the inherent structural dependency between destroy and repair actions, representing core innovation of G-LNS. Instead of evolving operators in isolation, this strategy selects coupled Destroy-Repair pair using Roulette Wheel Selection based on Synergy Scores (accumulated during the evaluation phase). The LLM is explicitly prompted to co-evolve these operators as unified entity, ensuring the repair mechanism is tailor-made to reconstruct the specific topological defects introduced by the destroy mechanism. Prompt for Action c2 (Synergistic Joint Crossover) You are an expert in heuristic optimization. We are employing Synergistic Joint Crossover (Structural Coupling) strategy to evolve LNS operators. Problem Description: {task description} Selected High-Synergy Pair: Parent Destroy Operator: {destroy code} Parent Repair Operator: {repair code} Task: Evolve this pair as UNIFIED ENTITY to create new Destroy-Repair pair. The goal is to address the inherent coupling between destroy and repair actions. Specifically, ensure that the generated Repair operator is specifically tailored to reconstruct the structural defects introduced by the generated Destroy operator, thereby maximizing their synergistic performance. Requirements: 1. Design NEW Destroy operator and NEW Repair operator. 2. The new Destroy operator should create specific structural defects. 3. The new Repair operator must be designed to fix these specific defects efficiently. 4. Both must follow standard LNS signatures strictly. 5. Define all helper functions INSIDE the main functions. 6. Return ONE code block containing BOTH the new Destroy operator and the new Repair operator. 7. Do not give additional explanations. 19 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design D.2. Main Framework Algorithm Algorithm 1 presents the detailed pseudo-code for the proposed G-LNS framework. The procedure begins with the initialization of operator populations and global metrics (Lines 13). The core execution flow alternates between two phases: the Evaluation Phase (Lines 522), where operators are dynamically selected and scored via independent Adaptive LNS episodes, and the Evolution Phase (Lines 2338), where the LLM evolves the population topology based on accumulated fitness and synergy scores periodically. Algorithm 1 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design 1: Input: Task I; Max Generations Gmax; Population Size ; Eval Episodes K; Inner Steps = 100; Destruction Ratio ϵ = 0.2; Initial Temp T0 = 100; Cooling Rate α = 0.97; Weight Update λ = 0.5; Rewards Ψ = {σ1, . . . , σ4}; Pruning Count . else , . σ σ4 (Rejected). xcurr x; σ σ2 (Better). else if (x) < (xcurr) then xcurr x; σ σ3 (Accepted). x; xcurr x; σ σ1 (Global Best). else if exp((f (x) (xcurr))/T ) > rand(0, 1) then end if Update Metrics: Update d, r, , and Sij using σ, λ. α. // Phase 1: Evaluation (Adaptive LNS Episode) Initialize episode: xcurr RandomSolution(I); T0; d, 1. for = 1 to do Select di Pd and rj Pr via roulette wheel selection (p w). Generate rj(di(xcurr, ϵ)). // Score & Acceptance if (x) < (x) then 2: Output: Best solution and optimized operator populations 3: Initialize: Pd, Pr with seeds; Weights d, 1; Fitness 0; Synergy 0; Init(I); 1. 4: while Gmax do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: end while 41: Return and elite operators. Management: Rank Pd, Pr by fitness ; prune bottom operators. Reset fitness F, 0 for fair competition. while Pd < or Pr < do end for // Phase 2 & 3: Evolution (Triggered every generations/episodes) if (mod K) = 0 then end if Validation: Run sanity check on generated code; add operator if valid. Sample strategy {Mutation, Homo-Cross, Joint-Cross}. if = Mutation then Select (di, rj) via synergy Sij; (d, r) LLM(Promptjoint(di, rj)). Select opa, opb; opnew LLM(Prompthomo(opa, opb)). Select parent op; opnew LLM(Promptmut(op)). else if = Homo-Cross then else if = Joint-Cross then end if + 1. end while G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design E. Designed Operators In this section, we compile the most successful heuristics produced by G-LNS, spanning the entire suite of experimental settings. E.1. Discovered Operators for TSP For the TSP, G-LNS evolved sophisticated pair of operators that utilize State-Dependent Adaptation to navigate the search space. We term these Adaptive Continuous-Segment Removal (ACSR) and Diversity-Adaptive Probabilistic Insertion (DAPI). Mechanism Analysis. The ACSR operator implements magnitude-dependent strategy: for moderate perturbation, it precisely identifies and removes the single most expensive continuous segment to refine local connections; for aggressive destruction, it automatically switches to multi-segment fragmentation to prevent structural lock-in. Cooperatively, the DAPI operator transcends fixed-parameter logic by monitoring real-time solution diversity. It dynamically tunes the temperature of its Softmax selection mechanismincreasing exploration (high temperature) when the solution becomes clustered, and focusing on exploitation (low temperature) when diversity is high. def destroy(x, destroy_cnt, dist_mat=None): \"\"\" Hybrid destroy operator combining: 1. Continuous removal strategy from Parent 1 2. Adaptive distance-based selection from Parent 2 3. Edge distance optimization for selecting the best continuous segment \"\"\" def simple_random_destroy(x, cnt): new_x = copy.deepcopy(x) removed = [] for _ in range(min(cnt, len(new_x))): idx = random.randint(0, len(new_x) - 1) removed.append(new_x[idx]) new_x.pop(idx) return removed, new_x if len(x) <= destroy_cnt: return list(range(len(x))), [] if dist_mat is None or len(x) <= 1: return simple_random_destroy(x, destroy_cnt) new_x = copy.deepcopy(x) removed_cities = [] = len(new_x) if destroy_cnt >= len(new_x): removed_cities = copy.deepcopy(new_x) new_x = [] return removed_cities, new_x if destroy_cnt <= * 0.4: segment_scores = [] for start_idx in range(n): # Moderate destruction - use distance-based continuous removal segment_dist = 0 for in range(destroy_cnt - 1): idx1 = (start_idx + i) % idx2 = (start_idx + + 1) % segment_dist += dist_mat[new_x[idx1]][new_x[idx2]] if destroy_cnt < n: idx_before = (start_idx - 1) % idx_start = start_idx % segment_dist += dist_mat[new_x[idx_before]][new_x[idx_start]] # Edge after segment idx_end = (start_idx + destroy_cnt - 1) % idx_after = (start_idx + destroy_cnt) % segment_dist += dist_mat[new_x[idx_end]][new_x[idx_after]] segment_scores.append(segment_dist) if random.random() < 0.7: # 70% chance: remove worst segment (highest distance) start_index = np.argmax(segment_scores) else: # 30% chance: probabilistic selection scores_array = np.array(segment_scores) weights = scores_array / scores_array.sum() start_index = np.random.choice(range(n), p=weights) else: # Aggressive destruction - combine continuous removal with random elements segments_to_remove = [] remaining_cnt = destroy_cnt while remaining_cnt > 0 and len(segments_to_remove) < n: max_segment_size = min(remaining_cnt, max(2, int(destroy_cnt * 0.3))) segment_size = random.randint(1, max_segment_size) start_idx = random.randint(0, - 1) segments_to_remove.append((start_idx, segment_size)) 21 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design remaining_cnt -= segment_size all_indices = set() for start_idx, segment_size in segments_to_remove: for in range(segment_size): idx = (start_idx + i) % all_indices.add(idx) target_indices = list(all_indices)[:destroy_cnt] target_indices.sort(reverse=True) for idx in target_indices: if idx < len(new_x): removed_cities.append(new_x[idx]) new_x.pop(idx) return removed_cities, new_x removal_indices = [] for in range(destroy_cnt): removal_idx = (start_index + i) % # Adjust for previous removals if were in circular context if removal_idx >= len(new_x): removal_idx = removal_idx % len(new_x) removal_indices.append(removal_idx) removal_indices.sort(reverse=True) for idx in removal_indices: if idx < len(new_x): removed_cities.append(new_x[idx]) new_x.pop(idx) if len(removed_cities) < destroy_cnt and len(new_x) > 0: additional_needed = destroy_cnt - len(removed_cities) additional_removed, new_x = simple_random_destroy(new_x, additional_needed) removed_cities.extend(additional_removed) return removed_cities, new_x Listing 1. Generated Destroy Operator for TSP (ACSR) def repair_diversity_adaptive(x, removed_cities, dist_mat): \"\"\" Monitors solution diversity to dynamically adjust Softmax temperature and exploration thresholds. \"\"\" new_x = list(x) # Helper: Calculate clustered-ness (Diversity Metric) def _calculate_diversity(path): if len(path) <= 1: return 0.5 total = sum(dist_mat[path[i]][path[(i+1)%len(path)]] for in range(len(path))) avg = total / len(path) return min(avg / np.max(dist_mat), 1.0) # Helper: Softmax selection with Temperature def _select_softmax(costs, T): min_c = min(costs) # Higher -> Flatter distribution (More Exploration) # Lower -> Sharper distribution (More Exploitation) weights = [math.exp(-(c - min_c)/max(1, min_c)/T) for in costs] total = sum(weights) probs = [w/total for in weights] return random.choices(range(len(costs)), weights=probs)[0] # [Step 1] State Analysis diversity = _calculate_diversity(new_x) # [Step 2] Parameter Adaptation # Low diversity (<0.5) triggers high randomness to escape clustering random_threshold = 0.1 + 0.4 * (1.0 - diversity) # Inverse relationship: Low diversity -> High Temperature temperature = 3.0 - 2.0 * diversity # [Step 3] Insertion Loop random.shuffle(removed_cities) for city in removed_cities: = len(new_x) # Calculate insertion costs for all positions costs = [] for in range(n + 1): prev = new_x[i-1] if > 0 else new_x[-1] curr = new_x[i] if < else new_x[0] delta = dist_mat[prev][city] + dist_mat[city][curr] - dist_mat[prev][curr] costs.append(delta) # Decision: Random vs. Strategic if random.random() < random_threshold: 22 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design # Pure Exploration insert_pos = random.randint(0, n) else: # Strategic Phase if random.random() < 0.8: # Softmax Probabilistic Selection (Parent 2 Logic) insert_pos = _select_softmax(costs, temperature) else: # Pure Greedy (Argmin) insert_pos = costs.index(min(costs)) new_x.insert(insert_pos, city) # [Step 4] Probabilistic Local Search # Trigger 2-opt more frequently if randomness was high if random.random() < (0.2 + 0.5 * random_threshold): # (Fast 2-opt implementation omitted for brevity) pass return new_x Listing 2. Generated Destroy Operator for TSP (DAPI) E.2. Discovered Operators for CVRP For the CVRP, G-LNS evolved sophisticated pair of operators that exhibit Dynamic Adaptation to manage the trade-off between exploration and exploitation. We term these Progressive Stochastic-Worst Removal (PSWR) and Adaptive Context-Aware Greedy Insertion (ACAGI). Mechanism Analysis. The PSWR operator introduces time-dependent strategy: it initiates with random removal to perform global perturbation and progressively shifts towards worst-cost removal to refine local inefficiencies, controlled by dynamic ratio ρt. Cooperatively, the ACAGI operator transcends static logic by monitoring real-time insertion difficulty. It adaptively increases its search depth (Regret-k) and exploration noise when the solution space becomes constrained, while employing multi-objective scoring function (balancing distance and capacity waste) to minimize the number of vehicles used. def destroy(x, destroy_cnt, problem_data): \"\"\"Hybrid Random-Worst Removal: Combines random exploration with worst-distance exploitation\"\"\" def calculate_saving(route, node_idx, dist_mat, depot): node = route[node_idx] prev_node = route[node_idx-1] if node_idx > 0 else depot next_node = route[node_idx+1] if node_idx < len(route)-1 else depot cost_with = dist_mat[prev_node][node] + dist_mat[node][next_node] cost_without = dist_mat[prev_node][next_node] return cost_with - cost_without # Initialization new_x = [route[:] for route in x] removed_customers = [] depot = problem_data.get(depot_idx, 0) dist_mat = problem_data.get(distance_matrix) all_customers = [c for route in new_x for in route] if len(all_customers) <= destroy_cnt: return all_customers, [[]] # Initial calculation of savings node_savings = {} for r_idx, route in enumerate(new_x): for i, node in enumerate(route): saving = calculate_saving(route, i, dist_mat, depot) node_savings[node] = saving removed_count = 0 # Progressive Removal Strategy while removed_count < destroy_cnt and len(all_customers) - removed_count > 0: greedy_ratio = removed_count / destroy_cnt remaining_customers = [c for route in new_x for in route] if not remaining_customers: break # Re-evaluate savings for current partial solution accuracy if removed_count > 0: node_savings = {} for r_idx, route in enumerate(new_x): for i, node in enumerate(route): saving = calculate_saving(route, i, dist_mat, depot) node_savings[node] = saving # Probabilistic Switch based on Progress if random.random() < greedy_ratio: 23 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design candidate_nodes = [(node_savings[node], node) for node in remaining_customers] candidate_nodes.sort(key=lambda x: x[0], reverse=True) top_k = min(3, len(candidate_nodes)) selected_node = random.choice(candidate_nodes[:top_k])[1] else: selected_node = random.choice(remaining_customers) removed_customers.append(selected_node) for route in new_x: if selected_node in route: route.remove(selected_node) break removed_count += new_x = [r for in new_x if len(r) > 0] return removed_customers, new_x Listing 3. Generated Destroy Operator for CVRP (PSWR) def insert(x, removed_customers, problem_data): \"\"\" Hybrid Greedy-Regret Insertion with Cached Loads and Adaptive Exploration \"\"\" new_x = [list(route) for route in x] demands = problem_data[demands] capacity = problem_data[capacity] dist_mat = problem_data[distance_matrix] depot = problem_data.get(depot_idx, 0) def insertion_cost_delta(route, pos, customer): if not route: # Empty route return dist_mat[depot][customer] + dist_mat[customer][depot] prev_node = route[pos-1] if pos > 0 else depot next_node = route[pos] if pos < len(route) else depot added = dist_mat[prev_node][customer] + dist_mat[customer][next_node] removed = dist_mat[prev_node][next_node] return added - removed route_loads = [sum(demands[c] for in route) for route in new_x] k_regret = 2 exploration_factor = 0.3 insertion_difficulty = 0.0 difficulty_decay = 0.8 cust_demands = [demands[c] for in removed_customers] for customer_idx, (customer, cust_demand) in enumerate(zip(removed_customers, cust_demands)): feasible_insertions = [] # Phase 1: Fast feasibility check with cached loads for r_idx, (route, route_load) in enumerate(zip(new_x, route_loads)): if route_load + cust_demand > capacity: continue route_len = len(route) positions = range(route_len + 1) for pos in positions: cost_inc = insertion_cost_delta(route, pos, customer) feasible_insertions.append({ cost: cost_inc, route_idx: r_idx, position: pos, route_load: route_load, route_length: route_len }) new_route_cost = dist_mat[depot][customer] + dist_mat[customer][depot] feasible_insertions.append({ cost: new_route_cost, route_idx: len(new_x), position: 0, route_load: 0, route_length: }) if not feasible_insertions: new_x.append([customer]) route_loads.append(cust_demand) insertion_difficulty = insertion_difficulty * difficulty_decay + 1.0 continue # Phase 2: Adaptive selection strategy feasible_insertions.sort(key=lambda x: x[cost]) best_cost = feasible_insertions[0][cost] remaining_customers = len(removed_customers) - customer_idx - 1 if remaining_customers > 0 and len(feasible_insertions) < 3: insertion_difficulty = insertion_difficulty * difficulty_decay + 1.0 else: insertion_difficulty = insertion_difficulty * difficulty_decay difficulty_threshold = len(removed_customers) * 0.3 24 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design if insertion_difficulty > difficulty_threshold: k_regret = min(4, len(feasible_insertions)) exploration_factor = 0.4 # More exploration else: k_regret = min(3, max(2, len(feasible_insertions) // 3)) exploration_factor = 0.2 # More greedy if len(feasible_insertions) >= k_regret: regret_values = [] max_regret_candidates = min(k_regret, len(feasible_insertions)) for in range(max_regret_candidates): ins = feasible_insertions[i] cost_diff = ins[cost] - best_cost load_ratio = ins[route_load] / capacity if capacity > 0 else 0 load_penalty = 0.15 * load_ratio length_penalty = 0.05 * (ins[route_length] / 20) if ins[route_length] > 0 else 0 regret_score = cost_diff + load_penalty + length_penalty if random.random() < exploration_factor: noise = random.uniform(-0.1, 0.1) * best_cost if best_cost > 0 else 0 regret_score += noise regret_values.append((regret_score, i)) if regret_values: regret_values.sort(key=lambda x: x[0]) if random.random() < (0.8 - 0.2 * (insertion_difficulty / difficulty_threshold)): selected_idx = regret_values[0][1] else: top_m = min(3, len(regret_values)) weights = [1.0 / (i + 1) for in range(top_m)] total_weight = sum(weights) rand_val = random.random() * total_weight cumulative = 0 for j, in enumerate(weights): cumulative += if rand_val <= cumulative: selected_idx = regret_values[j][1] break else: selected_idx = regret_values[0][1] selected = feasible_insertions[selected_idx] else: selected = feasible_insertions[0] else: selected = feasible_insertions[0] # Phase 3: Apply insertion with cache update if selected[route_idx] == len(new_x): new_x.append([customer]) route_loads.append(cust_demand) else: route = new_x[selected[route_idx]] route.insert(selected[position], customer) route_loads[selected[route_idx]] += cust_demand final_routes = [] final_loads = [] for route, load in zip(new_x, route_loads): if route: final_routes.append(route) final_loads.append(load) if len(final_routes) > 1: consolidated_routes = [] consolidated_loads = [] used = [False] * len(final_routes) for in range(len(final_routes)): if used[i]: continue current_route = final_routes[i] current_load = final_loads[i] for in range(i + 1, len(final_routes)): if used[j]: continue if current_load + final_loads[j] <= capacity: if len(current_route) + len(final_routes[j]) < 15: current_route.extend(final_routes[j]) current_load += final_loads[j] used[j] = True consolidated_routes.append(current_route) consolidated_loads.append(current_load) used[i] = True final_routes = consolidated_routes final_loads = consolidated_loads if not final_routes: final_routes = [[]] final_loads = [0] 25 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design return final_routes F. Details of Results F.1. Details on OVRP Listing 4. Generated Repair Operator for CVRP (ACAGI) Open Vehicle Routing Problem (OVRP). It is pertinent to emphasize that OVRP often presents greater challenge for heuristic design compared to the standard CVRP, particularly for constructive approaches. In the standard CVRP, the requirement to return to the depot forces the route to form closed loop. This naturally prevents the algorithm from creating overly elongated paths, as the cost of returning to the depot effectively limits how far vehicle can wander. In contrast, OVRP removes this return requirement. Without the need to close the loop, sequential constructive heuristics (like those employed by the baselines) often fail to maintain compact route structure. They tend to greedily extend the path to the nearest neighbors without considering the global shape, resulting in loose, fragmented routes that are difficult to optimize. Following the experimental setup described in Appendix C.1, we compare G-LNS against the rigorous OR-Tools solver (configured with time limit consistent with CVRP settings) and LLM-based AHD methods. The quantitative results are summarized in Table 4. Table 4. Performance comparison on Open Vehicle Routing Problem (OVRP) across five problem sizes. The Gap is calculated relative to the best solution found among all methods. The best results are highlighted in bold. Method OR-Tools OVRP OVRP20 OVRP50 OVRP100 OVRP200 Gap Obj. Gap Obj. Gap Obj. Gap Obj. Gap Obj. 0.00% 2.2885 0.00% 3.4277 0.00% 5.6736 0.00% 8. 2.05% 15.1893 EoH ReEvo Ours (G-LNS) 15.21% 2.6366 15.15% 2.6353 0.02% 2.2890 27.21% 4.3605 31.17% 4.4960 0.27% 3.4371 34.89% 7.6530 29.30% 7.3360 0.75% 5.7163 33.30% 11.8056 31.50% 11.6463 8.8979 0.47% 36.67% 20.3424 36.33% 20.2910 0.00% 14.8841 Performance Analysis. On small to medium-scale instances (N {10, 20, 50, 100}), the exact solver logic of OR-Tools remains highly effective, achieving optimal or near-optimal solutions. In this regime, G-LNS exhibits robust competitiveness, maintaining optimality gaps consistently below 0.8%. However, the superior scalability of G-LNS becomes evident on large-scale instances (N = 200). While OR-Tools begins to struggle under the computational time limityielding suboptimal gap of 2.05%G-LNS successfully identifies significantly better solutions, reducing the objective cost from 15.19 (OR-Tools) to 14.88, thereby establishing new best-known frontier (Gap 0.00%). In stark contrast, the constructive LLM-based baselines fail to adapt to the open-route structure. As anticipated, their reliance on sequential decision-making leads to poor performance, with optimality gaps exceeding 30% on instances where 50. This significant performance disparity highlights the critical advantage of the LNS paradigm: by explicitly evolving Destroy and Repair operators to iteratively reshape existing topologies rather than constructing them step-by-step, G-LNS effectively avoids the local optima that trap constructive methods. F.2. Details on Benchmarks In this subsection, we provide the comprehensive results on the standard TSPLib and CVRPLib benchmarks. We compare G-LNS against LLM-based AHD methods, including EoH, ReEvo, and the state-of-the-art heuristic set method EoH-S. Summary of Results. Table 5 presents the average optimality gap across all instances within each respective category. G-LNS outperforms all baselines across all benchmark sets. Experimental Setup. We adopt the evaluation configuration directly from EoH-S (Liu et al., 2025). Consistent with their protocol, all baseline methods are evaluated using normalized node coordinates mapped to the range [0, 1]2. The scaling factor is derived from the maximum spatial extent of each instance: Scaling factor = max(xmax xmin, ymax ymin) (12) 26 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Table 5. Comparison of results on TSPLib and CVRPLib Benchmarks. The best values are highlighted in bold. Benchmarks EoH ReEvo EoH-S TSPLib 18.1% 21.3% 9.1% Ours 2.8% 7.9% 31.7% 31.8% 22.5% CVRPLib 8.7% 36.2% 33.9% 18.3% CVRPLib 7.9% 32.3% 29.7% 24.3% CVRPLib 53.9% 64.6% 40.1% 15.9% CVRPLib CVRPLib 44.2% 43.0% 29.1% 15.1% 8.1% 26.8% 26.0% 16.7% CVRPLib 26.8% 26.5% 19.1% 11.2% CVRPLib This normalization step is standard for constructive heuristics to ensure numerical stability. Furthermore, following the standard protocol in these baselines, the reported optimality gaps are calculated relative to the best-known solutions obtained by the LKH-3 solver. In contrast, our proposed G-LNS operates directly on the raw, unnormalized instance data. This distinction highlights that the operators evolved by G-LNS capture the intrinsic topological logic of the routing problems rather than relying on specific coordinate scales. Detailed Results. The detailed optimality gaps are presented as follows: TSPLib: Table 6 reports the results on symmetric TSPLib instances. CVRPLib: Table 7 (Sets A, B, E, F, M, P), Table 8 (Set X) report the results on the Capacitated Vehicle Routing Problem benchmarks. Table 6. Detailed optimality gaps (%) on TSPLib instances. Baselines are evaluated with coordinate normalization, while Ours operates on raw data. Instance EoH ReEvo EoH-S Ours Instance EoH ReEvo EoH-S Ours TSPLib Results a280 berlin52 bier127 ch130 ch150 d198 d493 d657 eil51 eil76 eil101 fl417 gil262 kroA100 kroB100 kroC100 kroD100 kroE100 kroA150 kroB150 kroA200 kroB200 lin105 pr76 pr124 24.7 16.9 18.2 15.8 20.2 15.6 18.4 19.5 15.8 12.2 13.3 29.4 20.9 11.9 22.5 18.2 17.0 19.3 14.9 12.0 21.3 19.7 14.7 16.2 22.9 30.3 20.6 14.7 28.0 23.2 17.3 19.0 17.6 12.2 14.4 22.4 31.0 21.6 34.7 28.4 17.9 15.4 22.0 23.5 28.0 16.1 15.6 40.3 31.3 22.1 13.7 10.3 11.4 4.9 7.3 16.9 12.2 15.5 4.3 6.6 9.7 14.3 11.7 6.1 11.6 4.4 9.1 6.7 9.1 9.0 8.3 6.6 3.7 4.0 5. 4.2 3.1 2.8 1.5 2.0 5.4 3.5 3.9 1.1 1.9 2.6 4.5 3.2 1.8 2.4 1.2 2.7 2.1 2.5 2.3 3.0 1.9 1.1 1.4 1.8 pr136 pr144 pr152 pr226 pr264 pr299 pr439 rat99 rat195 rat575 rat783 rd100 rd400 st70 ts225 tsp225 u159 u574 u724 pr1002 pcb442 p654 lin318 pr107 Average. 16.4 17.4 20.7 19.2 22.2 28.5 24.3 14.2 6.7 14.1 18.9 15.3 14.1 9.3 10.5 21.3 28.3 21.6 16.4 21.2 18.5 27.8 12.6 14.6 18.1 9.9 14.2 28.9 19.5 15.2 18.5 20.5 22.3 9.6 20.5 24.6 28.1 20.4 17.5 18.7 16.5 27.3 22.8 24.0 23.3 18.1 18.9 33.5 2.6 21.3 6.0 4.9 11.1 9.8 7.7 11.9 11.0 13.0 6.5 9.9 11.4 8.5 12.5 2.6 3.4 9.1 8.5 14.0 12.7 14.0 9.9 9.6 10.6 4.5 9.1 1.9 1.5 3.4 2.8 2.2 3.7 3.3 3.8 6.8 3.0 3.5 2.1 3.4 0.8 0.9 2.7 2.9 4.1 3.6 4.3 3.1 3.6 2.9 3.5 2. 27 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Table 7. Detailed optimality gaps (%) on CVRPLib instances (Set A, B, E, F, M, P). Results are grouped by dataset. The best results are highlighted in bold. Instance EoH ReEvo EoH-S Ours Instance EoH ReEvo EoH-S Ours A-n32-k5 A-n33-k5 A-n33-k6 A-n34-k5 A-n36-k5 A-n37-k5 A-n37-k6 A-n38-k5 A-n39-k5 A-n39-k6 A-n44-k6 A-n45-k6 A-n45-k7 A-n46-k7 B-n31-k5 B-n34-k5 B-n35-k5 B-n38-k6 B-n39-k5 B-n41-k6 B-n43-k6 B-n44-k7 B-n45-k5 B-n45-k6 B-n50-k7 B-n50-k8 E-n22-k4 E-n23-k3 E-n30-k3 E-n33-k4 E-n51-k5 E-n76-k7 F-n45-k4 F-n72-k4 33.3 29.7 24.1 21.2 39.8 40.5 35.5 37.0 27.1 24.9 19.0 28.9 22.0 50. 13.8 15.3 18.4 43.1 90.8 18.4 21.2 22.9 28.1 44.1 46.8 19.3 16.8 20.8 22.6 26.6 29.4 41.7 41.0 78.9 M-n101-k10 M-n121-k7 M-n151-k12 44.4 38.0 46.8 P-n16-k8 P-n19-k2 P-n20-k2 P-n21-k2 P-n22-k2 P-n22-k8 P-n23-k8 P-n40-k5 P-n45-k5 P-n50-k7 P-n50-k8 P-n50-k 3.3 14.3 13.5 20.8 18.4 28.7 20.5 19.7 34.0 30.0 26.7 28.0 35.1 21.4 29.9 20.2 35.7 36.4 35.2 31.2 19.7 39.2 20.0 44.4 19.7 36.5 8.8 28.1 31.2 31.8 90.7 21.7 20.3 16.6 28.8 55.5 38.5 22.9 31.1 18.8 15.9 20.8 25.1 46.6 62.7 69.7 48.6 47.6 42. 2.9 30.7 23.7 27.2 24.1 34.7 15.6 23.4 45.1 19.8 33.3 19.5 32.3 20.5 21.0 16.9 29.5 31.2 15.0 25.8 22.1 29.4 19.3 18.0 11.2 26.8 7.1 8.1 19.6 22.6 24.7 13.2 20.1 19.0 19.1 21.5 24.0 10.6 26.3 21.1 12.5 13.8 15.7 33.7 Set 7.0 2.6 7.1 5.4 8.8 8.1 4.2 8.6 11.9 8.7 10.7 3.6 4.2 6. A-n48-k7 A-n53-k7 A-n54-k7 A-n55-k9 A-n60-k9 A-n61-k9 A-n62-k8 A-n63-k9 A-n63-k10 A-n64-k9 A-n65-k9 A-n69-k9 A-n80-k10 Average. Set 2.3 6.7 2.5 6.4 20.5 8.3 8.0 8.4 18.0 13.5 5.4 8.6 B-n51-k7 B-n52-k7 B-n56-k7 B-n57-k7 B-n57-k9 B-n63-k10 B-n64-k9 B-n66-k9 B-n67-k10 B-n68-k9 B-n78-k10 Average. Set 3.3 7.9 1.9 5.2 15.0 7. E-n76-k8 E-n76-k10 E-n76-k14 E-n101-k8 E-n101-k14 Average. Set 45.9 45.6 7.0 18.6 F-n135-k7 Average. Set 30.9 22.9 20.5 34.8 38.8 48.3 33.4 28.4 30.4 30.6 37.7 34.7 31.6 31.7 28.8 50.3 76.0 46.7 16.0 29.3 60.8 22.6 39.8 27.6 52.7 36.2 36.5 31.9 34.4 42.5 51.5 32.3 39.9 53.3 22.3 22.9 33.2 3.1 10.2 6.1 12.5 9.4 20.8 8.9 21.5 25.1 21.8 18.4 20. 10.4 22.3 10.1 M-n200-k16 M-n200-k17 Average. 45.3 46.2 44.2 Set 2.9 21.7 13.0 3.6 5.9 4.4 6.0 4.0 7.1 8.2 6.9 10.7 P-n51-k10 P-n55-k7 P-n55-k10 P-n55-k15 P-n60-k10 P-n60-k15 P-n65-k10 P-n70-k10 P-n76-k4 P-n76-k5 P-n101-k4 Average. 28 36.1 29.8 28.6 28.3 32.4 38.2 28.6 29.2 43.6 26.0 36.8 26.8 31.5 38.2 34.5 25.3 30.2 45.5 33.0 18.1 38.3 31.8 47.3 28.0 32.1 31.8 33.2 73.2 46.1 38.0 16.9 35.4 41.1 19.6 38.9 18.7 23.9 33.9 31.0 28.0 24.7 37.5 47.2 29.7 61.4 64. 40.2 35.7 43.0 31.6 30.4 16.8 13.8 32.2 30.8 33.1 32.6 20.7 27.8 29.1 26.0 27.4 19.4 12.7 26.5 32.0 23.4 22.0 13.3 20.4 17.6 33.7 20.7 20.0 22.5 11.6 16.8 23.8 19.3 16.6 19.5 18.6 11.7 23.2 16.3 27.6 18.0 28.6 34.0 20.7 28.1 32.5 24.3 15.1 7.4 9.7 5.9 9.3 5.1 8.5 2.6 18.3 6.6 11.6 6.9 7.4 7. 1.1 5.7 15.2 6.2 5.6 9.2 10.2 7.6 12.8 9.1 8.2 8.7 7.8 10.3 8.0 10.7 9.1 7.9 28.9 40.1 22.0 15.9 31.6 35.4 29.1 20.5 17.2 23.7 10.3 17.0 14.3 26.8 14.3 20.1 21.8 19.9 16. 18.9 13.7 15.1 9.1 4.9 3.9 2.7 6.0 11.2 7.7 8.0 15.9 9.9 12.2 8.1 G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design Table 8. Detailed optimality gaps (%) on CVRPLib instances (Sets X). Results are grouped by dataset. The best results are highlighted in bold. Instance EoH ReEvo EoH-S Ours Instance EoH ReEvo EoH-S Ours Set X-n101-k25 48.7 X-n106-k14 7.7 X-n110-k13 20.2 X-n115-k10 50.8 X-n120-k6 20.4 X-n125-k30 16.0 X-n129-k18 20.2 X-n134-k13 61.0 X-n139-k10 23.5 X-n143-k7 53.5 X-n148-k46 30.7 X-n153-k22 44.5 6.6 X-n157-k13 X-n162-k11 12.7 X-n167-k10 27.5 X-n172-k51 43.4 X-n176-k26 35.2 7.5 X-n181-k23 X-n186-k15 22.9 X-n190-k8 17.0 X-n195-k51 27.5 X-n200-k36 18.3 40.0 10.8 24.8 46.3 19.3 21.0 14.0 50.8 24.8 43.8 17.0 39.0 21.6 24.4 23.8 40.8 37.0 15.6 20.7 14.9 37.5 16.9 24.9 6.5 16.3 39.4 18.3 14.2 15.4 24.6 19.7 35.4 18.6 28.7 6.3 17.5 19.7 35.9 27.0 6.6 17.0 18.2 29.6 10. 7.4 3.6 12.8 11.3 16.8 7.4 13.0 13.0 14.3 13.5 10.8 10.0 3.7 9.5 16.4 14.2 11.9 2.5 14.7 8.2 13.5 7.7 X-n204-k19 23.4 X-n209-k16 20.4 X-n214-k11 30.3 X-n219-k73 1.8 X-n223-k34 26.2 X-n228-k23 38.5 X-n233-k16 53.6 X-n237-k14 23.1 X-n242-k48 15.1 X-n247-k50 35.8 X-n251-k28 11.5 X-n256-k16 19.9 X-n261-k13 33.7 X-n266-k58 9.2 X-n270-k35 19.4 X-n275-k28 13.8 X-n280-k17 23.3 X-n284-k15 27.7 X-n289-k60 22.1 X-n294-k50 50.7 X-n298-k31 38.7 Average. 26.8 19.8 17.0 32.5 48.9 17.1 26.9 64.3 19.0 10.4 33.2 14.3 17.2 35.0 10.7 23.4 21.4 22.1 23.6 25.0 36.4 18.8 26.5 19.2 11.7 23.0 1.4 14.1 26.2 34.2 18.3 8.8 29.7 10.7 15.0 26.3 8.0 13.9 11.7 25.0 20.7 11.8 22.1 19.9 19.1 14.2 8.1 18.9 7.1 9.7 11.9 16.4 16.6 6.1 9.2 8.2 8.4 15.6 7.4 12.1 5.9 14.8 16.7 9.6 17.4 10.0 11."
        }
    ],
    "affiliations": [
        "International Centre for Theoretical Physics Asia-Pacific, University of Chinese Academy of Sciences",
        "Software College, Northeastern University",
        "Taiji Laboratory for Gravitational Wave Universe, University of Chinese Academy of Sciences",
        "Tsinghua University"
    ]
}