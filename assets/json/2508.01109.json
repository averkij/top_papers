{
    "paper_title": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?",
    "authors": [
        "Satiyabooshan Murugaboopathy",
        "Connor T. Jerzak",
        "Adel Daoud"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 9 0 1 1 0 . 8 0 5 2 : r Platonic Representations for Poverty Mapping: Unified VisionLanguage Codes or AgentInduced Novelty? Satiyabooshan Murugaboopathy, Fraunhofer Center Connor T. Jerzak, UT Austin Adel Daoud, Chalmers & Link√∂ping University Correspondence: connor.jerzak@austin.utexas.edu August 5, 2025 Abstract We here investigate whether socio-economic indicators, such as household wealth, leave recoverable informational imprints in both satellite imagery (capturing physical features like buildings and roads) and Internet-sourced text (reflecting historical, cultural, and economic narratives of neighborhoods). Using Demographic and Health Survey (DHS) data from African neighborhoods (clusters), we pair high-resolution Landsat images with textual descriptions generated by large language models (LLMs) conditioned on location and year, as well as text retrieved by an LLM-driven AI Search Agent from web sources. We develop multimodal framework that predicts household wealth (measured by the International Wealth Index (IWI)) through five pipelines: (i) vision model on satellite images, (ii) an LLM using only location and year, (iii) an AI agent that searches and synthesizes web text, (iv) joint image-text encoder, and (v) an ensemble of all signals. Our framework yields three contributions. First, evaluations show that fusing vision and agent/LLM-generated text outperforms vision-only baselines in wealth prediction (e.g., R2 = 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge (artificial neural memory) proving surprisingly more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision and language modalities correlate moderately (median cosine similarity across modalities of about 0.60 after alignment), suggesting shared latent code of material well-being while retaining complementary details, broadly consistent with the Platonic Representation Hypothesis. Although the superior performance of LLM-only text over agent-retrieved data challenges our AgentInduced Novelty Hypothesis, modest gains from combining agent data in some splits offer weak support for the idea that agent-gathered informationemerging from dynamic interaction with the Internet introduces degree of unique representational structures not fully captured by static LLM knowledge. Third, we release large-scale multimodal dataset comprising approximately 60,000 DHS clusters, each linked to satellite images, LLM-generated descriptions, and associated texts retrieved by AI agents."
        },
        {
            "title": "1 Introduction",
            "content": "In an era where the timely and accurate measurement of socio-economic disparities is crucial for effective policy-making, humanitarian aid, and sustainable development, traditional household surveys, such as the Demographic and Health Surveys (DHS), provide invaluable ground-truth data but face significant limitations in terms of scale, cost, and frequency (DHS et al., 2013). These surveys, conducted periodically across lowand middle-income countries, capture wealth metrics such as the International Wealth Index (IWI)a composite measure of household assets and living conditionsbut often leave significant gaps in geographic and temporal coverage, particularly in remote or rapidly changing regions (Sakamoto et al., 2025). To bridge these gaps, researchers have increasingly turned to remote sensing technologies, leveraging high-resolution Earth observation (EO) imagery from satellites, such as Landsat, to infer poverty patterns through visual 1 cues, including infrastructure density, land use, and vegetation health (Yeh et al., 2020; Pettersson et al., 2023; Kakooei et al., 2024a; Burke et al., 2021). While these vision-based approaches have shown promise, they are inherently limited by what can be seen from above, often missing nuanced socio-cultural, historical, or contextual factors that influence material well-being (OBrien, 2023; Zhu et al., 2025). The advent of large language models (LLMs) and multimodal AI systems offers complementary pathway, enabling the extraction and synthesis of textual information from vast digital repositories, including web sources and encyclopedic knowledge (Sarmadi et al., 2025). This raises two interrelated questions: (1) To what extent can useful textual information about neighborhoods in lowand middle-income countries be recovered from LLMs artificial neural memory or found by AI Search Agents on the Internet? (2) If recoverable, how well can socio-economic status be distilled into compact, latent representation across vision and language modalities, potentially converging toward shared Platonic representation of material well-being (Huh et al., 2024)? Or, do modalities complement each other to enhance estimation? Figure 1: DAG representing the imprint of the poverty/wealth index Yi on satellite imagery Mi and textual data Wi, processed by LLMs neural weights and queried by AI Search Agent. Wi is challenging to observe directly, hence colored in red. Instead, the goal is to reconstruct faithful representation via the LLMs neural memory or AI agent search capabilities. These texts are denoted WDesc for the LLM, and WTrace (summary of search result) for the search agent. Bi represents background factors. (raw search result) and WSummary Figure 1 illustrates these questions via directed acyclic graph (DAG), where the materialization of poverty/wealth index Yi in African neighborhoods causally influences both satellite imagery Mi and textual data Wi. Socio-economic conditions leave observable traces in the physical environment (e.g., building density, road networks) visible in EO data, as well as in linguistic artifacts (e.g., historical accounts, news reports) (Daoud et al., 2019; Daoud and Dubhashi, 2023). However, Wi is challenging to observe directly, necessitating LLMs to reconstruct it from neural memory as WDesc , and AI Search Agents (ASA) to query the Internet for WTrace (raw traces) and thereafter generate WSummary (summaries). i 2 Despite growing interest in multimodality in poverty mapping, systematic investigations remain scarce, hampered by the lack of aligned, large-scale datasets (Lamichhane et al., 2025; Kakooei et al., 2024b). To address this limitation, we here analyze whether household wealth on the African continent, as measured by DHS IWI scores, can be textually and visually reconstructed and encoded in joint latent space. Drawing on continent-scale corpus of more than 60,000 DHS clusters across Africa from 1990 to 2020, we pair high-resolution, cloud-free Landsat composites with LLM-generated spatiotemporal narratives and AIagent-retrieved contextual information. At its heart, our approach taps five different signals to see and describe each neighborhoods wealth. First, we use satellite images to capture physical cluesthings like roads, buildings, and vegetation. Next, we ask language model to imagine narrative for the place based only on its location and year. Then, an AI agent goes online, gathers real-world text about the area, and boils it down into concise summary. After that, we train joint encoder to blend the visual and textual cues into single shared representation. Finally, we let an ensemble weigh and combine all five perspectives before we analyze embedding spaces to investigate representational convergence and complementarity across modalities. Figure 2 provides visual overview. The remainder of the paper is organized as follows: We first review the related work on remote sensing and AI in poverty mapping. Next, we detail our agent and data curation frameworks, followed by an experimental performance and representational analysis, and conclude with implications, limitations, and future directions."
        },
        {
            "title": "2 Problem Setup & Related Work",
            "content": "Early efforts in remote sensing for poverty estimation used nighttime light imagery as proxy for economic activity (Elvidge et al., 2009). More recent advances have focused on daytime satellite imagery combined with deep learning techniques. For example, Jean et al. (2016) demonstrated the use of transfer learning from night lights to predict poverty from daytime images in African countries. Subsequent works have applied convolutional neural networks directly to satellite data for wealth prediction (Yeh et al., 2020; Pettersson et al., 2023; Kakooei et al., 2024b). Interpretability has also been focus, with methods using object detection to generate explainable poverty maps (Babenko et al., 2017). Recent reviews synthesize the state of Earth observation and ML for poverty research, highlighting applications in causal inference and smallarea estimation (Sakamoto et al., 2025). Other studies explore fairness and biases in satellite-based poverty maps (Aiken et al., 2023). The emergence of large language models (LLMs) has opened new avenues for socio-economic inference using textual data. LLMs have been employed to estimate regional socio-economic indicators directly from prompts (Han et al., 2024). Synergizing LLM agents with knowledge graphs has shown promise for socioeconomic prediction (Zhou et al., 2024). Additionally, biases in LLMs related to socioeconomic attributes have been investigated (Arzaghi et al., 2024; de Pieuchon et al., 2025). Along with LLMs, multimodal approaches that combine vision and language are increasingly being explored. Sarmadi et al. (2025) leveraged GPT 4s multimodal capabilities to rank satellite images by poverty levels. Other works combine satellite imagery with non-visual features (such as X/Twitter activity, distance from residential roads, and Internet speed) to improve poverty prediction (Jung et al., 2025). Despite these advances, systematic studies on representational convergence across modalities in poverty mapping remain limited (Lamichhane et al., 2025). Our work builds on the framework outlined in the Introduction (Figure 1), where the latent poverty/wealth index Yi causally influences both satellite imagery Mi and textual data Wi, assuming conditional independence between modalities given Yi. This structure motivates our investigation into whether vision and language encoders converge toward shared Platonic representation of material well-being, as inspired by the Platonic Representation Hypothesis as proposed by Huh et al. (2024). While this hypothesis 3 has been examined in general vision-language models (Radford et al., 2021), its application to specific scientific domains, such as economic development and poverty mappingparticularly in contexts where data modalities are gathered endogenously by AI agentsis novel. Agent-Induced Novelty Hypothesis. We also introduce in this work (to our knowledge) new hypothesis whether dynamic, agent-driven data collection can introduce unique representational structures beyond those captured by static LLM memory. In agentic systems, AI agents autonomously gather and synthesize data via LLM-guided paths, we propose the Agent-Induced Novelty Hypothesis: AI-agent-gathered data injects representational noveltyunique structures emergent from endogenous LLM reasoning and dynamic LLMdata interactionsnot fully captured by an implicit assumption in standard Platonic Representation frameworks that data are upstream models. Indeed, unlike static or LLM-internal representations, agent-induced data evolves through iterative queries, retrievals, and syntheses (Miehling et al., 2025). Does this process introduce complementarities such as real-time contextual adaptations or path-dependent representational shifts? Does this novelty enhance predictive power in tasks like poverty mapping? We will later test the novelty hypothesis by comparing embeddings from agent traces (WTrace ), evaluating whether fused models exhibit higher complementarity in way that would indicate novelty separate from Platonic convergence. ) against LLM-only descriptions (WDesc Formalization. To test the Platonic Representation Hypothesis and the Agent-Induced Novelty Hypothesis outlined above, we formalize the poverty mapping task as supervised regression problem. The objective [0, 100] for each DHS cluster i, which is aggregated at the is to predict the International Wealth Index Yi neighborhood level from household surveys. Let Mi denote the Earth observation (EO) features extracted from satellite imagery (e.g., Landsat multispectral bands, processed via pre-trained vision model to yield embeddings). , the agents synthesized summary of those traces; WDesc Textual signals, as introduced in the DAG (Figure 1) and elaborated in the hypotheses, are decomposed , the raw agent search traces (e.g., concatenated Wikipedia excerpts and web search results); , the LLM-only spatiotemporal descripand , from agent and LLM-only decoding, respectively. Embeddings from textual components (e.g., via = (Mi, EText ) into: WTrace WSummary tion (e.g., generated from location-year prompts without external search); and scalar predictions, ÀÜY Agent ÀÜY LLM language model encoder) are denoted EText for some encoding function . , while fused multimodal embeddings are EFused i i"
        },
        {
            "title": "3 Data & Methods",
            "content": "Data Curation. We create multimodal dataset centered on DHS units, comprising approximately 60,000 geolocated neighborhood clusters across Africa, with surveys conducted between 1990 and 2020. For each cluster, we extract the International Wealth Index (IWI) as ground truth, composite score (0100) reflecting household assets and conditions, which is aggregated to the cluster (neighborhood) level. Visual data consist of high-resolution, cloud-free Landsat composites (30m resolution, multispectral bands) centered on DHS coordinates. Text data are generated via two channels: (i) LLM-generated descriptions (GPT-4.1 Nano, no search or tools enabled) conditioned on year, location coordinates, and location name (reverse-geocoded from coordinates); and (ii) AI-agent-retrieved context, where an LLM-driven agent queries Wikipedia and Internet search to extract socioeconomic, historical, and contextual information about DHS place name, also given year and coordinates. 4 The AI Search Agent has GPT-4.1 Nano LLM core, and was constructed using the open-source tool LangGraph (Wang and Duan, 2024); the maximum recursion depth was set to 20, meaning that the agent could take at most 20 steps of iterative searching before completing. In practice, we observed that the majority of agent invocations terminated with three search steps or fewer. (Note that both the LLM and the search agent use the same backbone and identical place data, isolating the impact of agentic capabilities.) The resulting combined corpusimages, LLM texts, agent traces, with IWI labels, dubbed IWI-Africa-Multimodal will be released on Hugging Face.1 See Table 1 for two illustrative AI Search Agent traces. Table 1: Examples of data gathered by the AI Search Agent. Search Trace 1/10 Lat/Long Full Agent Trace Wikipedia Trace -18.85 / 47. Place Name Manazary, Madagascar / -1.63 29.36 Kanzenze, Rwanda Manazary is small rural commune in the outskirts of Antananarivo... suggesting moderate level of development typical of rural Madagascar. [concatenated with search results: AntananarivoAvaradrano is district... elevation of 1,318 metres... etc.] Rubavu District, including Kanzenze sector, experienced social and economic development... suggests medium level of wealth and infrastructure. [concatenated with search results: Gender equality on socio-economic development in Rubavu... population and housing census... etc.] Manazary is commune in Madagascar... population estimated at 37,000 in 2001... primarily engaged in agriculture with rice as main crop... 40% in fishing. Rubavu District is one of seven districts in Western Province, Rwanda... capital is Gisenyi... urban area population of 149,209 in 2012. AntananarivoAvaradrano is district of Analamanga in Madagascar. It covers smaller communes in the outskirts of Antananarivo... APPROVAL SHEET This thesis entitled \"Gender Equality on socio-economic development in Rubavu district, Rwanda\"... Prediction Framework. Figure 2 illustrates our implementation of the five pipelines for estimating IWI scores while evaluating convergence and complementarity across modalities. (i) VISION PIPELINE: Vision model (e.g., 12-layer Vision Transformer architecture with patch size 16 (Stewart et al., 2022)) is pre-trained unsupervised on Landsat images, then fine-tuned supervised on IWI labels via ridge regression on embeddings. (ii) LLM-ONLY PIPELINE: An LLM (e.g., Llama-4-Maverick, GPT-4.1 Nano) predicts IWI directly from location-year prompts, leveraging internal neural memory; outputs include prediction, justification of prediction, and confidence in prediction. (iii) AI SEARCH AGENT PIPELINE: An AI agent (GPT-4.1 Nano core) retrieves and synthesizes web text (as above), then predicts IWI; we extract traces (raw text) and justifications for embedding. (iv) ENSEMBLE PIPELINE: Pipelines (i)(iii) independently generate modality-specific embeddings, which are then concatenated and used to train ridge regression model supervised on IWI labels. Here, embeddings refer to dense vector representations of input data (e.g., images or text) learned by neural networks, which capture semantic or visual features in compact, latent space suitable for downstream tasks like regression. In our pipelines, we distinguish between (1) frozen embeddings, which are pre-computed from pre-trained modelse.g., OpenAIs text-embedding-3-small [abbreviated as 1To link with the full DHS data, users must register with the DHS Program and agree to its privacy and data use policies, as outlined at https://dhsprogram.com. 5 Figure 2: Overview of the quintuple prediction framework for estimating household wealth (IWI) from DHS clusters. The Agent-Induced Novelty Hypothesis questions whether the red arrow is actually present to the same degree as others. 6 OAI] or sentence-transformers/all-mpnet-base-v2, abbreviated as [MPNet]and not updated during training to preserve general knowledge, and (2) fine-tuned embeddings, which are adjusted on our dataset to adapt to poverty-specific patterns. This distinction allows us to evaluate the trade-offs between leveraging off-the-shelf representations and task-specific optimization. We also define modality-specific acronyms used across experiments: NMR (Neural Memory Reconstruction) denotes the LLM-only pipeline, where text is generated solely from the models internal knowledge (artificial neural memory) based on location and year prompts; ASA (AI Search Agent) refers to the pipeline using agent-retrieved Internet text, including raw traces and summaries; and CV (computer vision) indicates the satellite imagery pipeline. We conduct three distinct evaluation experiments designed to assess model robustness across spatial and temporal dimensions: RANDOM SPLIT: standard 80/20 train-test split is applied without any geographic or temporal constraints, serving as the baseline for performance comparison. OUT-OF-COUNTRY (OOC): The model is trained on clusters from subset of countries (a random 80% subset of the data) and evaluated on held-out countries not present in the training set (the remaining 20%). This test evaluates cross-border generalization and the models ability to transfer knowledge beyond national boundaries. OUT-OF-TIME (OOT): The model is trained on data from one time period (encompassing 80% of the data) and evaluated on disjoint time span (the remaining 20%). This assesses temporal generalization and the models resilience to shifts in socio-economic conditions over time. For each experiment, we ensure strict data partitioning to prevent information leakage. In the OOC split, entire countries are treated as atomic units and assigned to either training or test folds. We randomly assign countries such that the distribution of clusters across folds is balanced. In the OOT split, complete years (e.g., 19901995) are used as units of partitioning, ensuring that no overlapping time periods exist between the train and test sets. In the RANDOM SPLIT, cluster assignments are made purely at random, with no restrictions based on location or year. All models are evaluated using two distinct training strategies, depending on whether embeddings are frozen or fine-tuned: For models with frozen embeddings, we perform 100 bootstrap iterations with an 80/20 train-test split per bootstrap iteration. Final results are reported as the mean and standard error across bootstraps. For models with fine-tuned embeddings, we use 5-fold cross-validation, with 70/15/15 train/validation/test split per fold. This ensures stable and reliable performance estimates while minimizing overfitting. Uncertainties are estimated as the standard deviation of the performance metrics obtained across the five cross-validation iterations on the test fold. Each model is evaluated under the same experimental protocol, allowing for fair comparisons between single-modality and multimodal configurations. All evaluations are conducted using single-frame inputs; no temporal image or text sequences are used. This design choice allows us to isolate the impact of modality fusion from temporal dynamics. Performance is measured using the coefficient of determination (R2) and root mean squared error (RMSE) on predicting out-of-sample poverty. The R2 metric quantifies the proportion of variance in the International Wealth Index (IWI) that is explained by the model, providing clear interpretation of the models predictive power. Confidence intervals are obtained via bootstrapping (for frozen embedding approaches) or cross-validation (for fine-tuned embedding approaches). 7 This experimental setup enables an assessment of how multimodal signalsparticularly those generated through agent-driven web retrieval and LLM-based reasoningenhance poverty prediction beyond what is achievable with satellite imagery alone. It also supports our investigation into the representational convergence of vision and language, testing both the Platonic Representation Hypothesis and the Agent-Induced Novelty Hypothesis in real-world, open-ended context."
        },
        {
            "title": "4 Results",
            "content": "Figure 3: Test performance (R2) across evaluation splits (RANDOM, OOC, OOT), grouped by model procedure and data source. Each dot represents the R2 for model under specific split strategy. The dot shape encodes the split type, and the dot color encodes the embedding model. Rows are ordered by highest R2 for readability. This visualization highlights the performance of various model+embedding combinations across generalization scenarios. General patterns Our evaluation reveals significant performance gains when combining multi-modal signals for poverty prediction, as shown in Figure 3 (full results in Table A.I.1). The NMR+CV approach using Llama-4-Maverick with OpenAI embeddings achieves the highest performance across all evaluation strategies, with an R2 of 0.765 on the random split. This represents substantial improvement over the best single-modality approach (NMR alone at R2 = 0.668) and significant advancement over the CV-only baseline (R2 = 0.634). Our CV-only baseline outperforms the shallow baseline method by Pettersson et al. (R2 = 0.60 on OOC split), despite using only daytime satellite imagery (instead of both daytime and nighttime images, which is known to boost performance). This demonstrates the effectiveness of our pipeline and 8 Figure 4: Spatial distribution of the difference in prediction residuals between CV-only and NMR+CV. Blue indicates regions where the baseline CV model outperforms NMR+CV. Red indicates regions where NMR+CV outperforms CV. Hexagons summarize mean residual differences across locations. suggests that our multimodal approach can achieve comparable or better results with fewer data sources. The performance gains are consistent across evaluation strategies, with the combined model maintaining strong performance even under OOC and out-of-time (OOT) splits. It is notable that the ASA searchers alone perform below the CV-only benchmark, possibly indicating that finding relevant online information is noisy process. Notably, the OOC split (where no countries in the training set appear in the test set) produces the most significant performance degradation across all models, indicating that country-specific features play critical role in poverty prediction. This is particularly evident in the CV-only baseline, which drops from R2 = 0.634 (RANDOM) to R2 = 0.446 (OOC). In contrast, the OOT split (where no years in the training set appear in the test set) shows relatively smaller performance drop, suggesting that year-specific features are less critical than country-specific features for poverty prediction in our dataset. (agent-synthesized summaries), and WDesc To further contextualize performance, we conducted an extensive analysis of different model archi- (raw agent search tectures and data sources, building on the textual representations defined earlier: WTrace traces), WSummary refers to all agent-crawled text data (WTrace sources; JustificationOnly includes only the agents justification of IWI prediction text; and JustificationPrediction combines justification with the agents scalar prediction. The results reveal that the CleanedTraces approach consistently outperforms other subset text sources, including Wikipedia, JustificationOnly, and JustificationPrediction. This finding suggests that the full breadth of agent-generated con- ) without filtering; Wikipedia is the subset from Wikipedia (LLM-generated descriptions). Here, CleanedTraces i 9 text provides richer signals for poverty prediction. The ASA+CV approach using CleanedTraces achieves an R2 of 0.740 on the random split, which is 10.4% higher than the CV-only baseline (R2 = 0.634) and 1.8% higher than the best single-modality NMR approach (R2 = 0.668). This suggests that the AI-search agents raw text collection provides novel information that complements the vision pipeline. See Table A.I.1 for full results. Our analysis of different embedding models reveals that text-embedding-3-small (OAI) consistently outperforms the fine-tuned MPNet model (sentence-transformers/all-mpnet-base-v2) across all evaluation strategies (see Table A.I.1 and Figure 3 for detailed results). For instance, in the NMR+CV pipeline under random splits, OAI achieves an R2 of 0.765 compared to MPNets 0.747. MPNet cannot match the performance of the frozen-weights OAI model. This suggests that the pretraining and contextual understanding captured in OAI provide significant advantage for poverty prediction tasks over smaller model fine-tuned on our dataset. Best Africa-wide Spatial and Temporal Analysis of Model Performance. To quantify spatial variability in gains from multimodality, Figure 4 plots the following difference in absolute residuals, ( Yi ÀÜYi ). This value is negative (blue) when the CV baseline outperforms the best multi-modal approach; it is positive (red) otherwise. We benchmark here the best-performing joint model against the computer-vision model, as the CV approach is dominant in the ML literature on poverty prediction. Yi ÀÜY CV As visualized in Figure 4, the combined NMR+CV model demonstrates particularly strong performance in densely populated regions such as South Africa, where it achieves improved accuracy compared to the CV-only baseline. It also significantly outperforms in conflict-affected areas of Somalia and central Africa (e.g., Chad). In contrast, performance improvements are more modest along the eastern coast (e.g., Guinea), suggesting that the information provided by the NMR component may be less valuable in regions with moderate levels of development and inequality. Time-series improvements To complement this spatial analysis, we also analyzed where the temporal improvements are occurring, using the same metric as for the Africa-wide spatial evaluation. Figure 5 shows that the improvements are consistent over time, where the multi-modal model outperforms the CVonly model. Most of the residual improvements are occurring in the 1990s. Interestingly, this is the period when satellite images are most scarce, with an average pixel availability of about three-quarters across the continent, from 1990 to 2000 Jerzak et al. (2023). Model Size Analysis. The performance of different LLMs reveals clear relationship between model size and performance, with the largest model (Llama-4-Maverick, 405B parameters) achieving the best results. However, the smaller models (GPT-4.1 Nano, Grok-3-Mini) perform comparably well at significantly lower computational cost. This suggests that for practical deployment, moderately sized LLMs can provide an excellent balance between performance and cost. Agent-Induced Novelty vs. Platonic Representation. One of the most notable findings from our experiments is that the LLM-only NMR approach consistently outperforms the ASA approaches. For instance, NMR with Llama-4-Maverick achieves R2 = 0.668 on RANDOM SPLIT, while the best ASA approach (CleanedTraces) only achieves R2 = 0.606, indicating that the LLMs internal knowledge is more predictive than agent-retrieved text. This contradicts our initial expectation that agent-retrieved data would provide more valuable live context and instead aligns with findings in Gema et al. (2025), which show reduced performance in language tasks as context (e.g., via reasoning chains) becomes overly large. The marginal gain from NMR+ASA+CV (R2 = 0.772) over NMR+CV in random splits provides modest evidence for the Agent-Induced Novelty Hypothesis, as agent data adds some unique structures not in 10 Figure 5: Absolute mean residual difference across years for the best system combining NMR+ASA+CV vs CV-only baseline. static LLM knowledge; however, this gain is inconsistent (e.g., absent in OOT splits) and minimal overall, suggesting agent-induced novelty is weak here. These results more strongly support the Platonic Representation Hypothesis, as the LLMs internal model encodes sufficient shared representations of poverty, with agent data contributing only marginal beneficial novelty in specific scenarios. Future work could quantify this further through embedding distance metrics or ablation studies to isolate agent-specific contributions. 0.11 derived from random pairings). The test produced t-statistic of 312.96 and p-value < 1 Validation of Intra-Cluster Similarity Against Null Distribution To statistically validate the observed alignment in Figure 6, we performed one-sample t-test on the cosine similarities between NMR and CV embeddings for matching clusters, testing the null hypothesis that their mean is zero (with empirical 1010, œÉ decisively rejecting the null and confirming significant positive similarities. This evidence supports partial representational convergence across modalities, consistent with the Platonic Representation Hypothesis, as vision and text embeddings share meaningful semantic structures beyond random chance while contributing complementary details to poverty prediction. Regional Representational Convergence. To explore the Platonic Representation Hypothesis in geographically informed manner, we investigate whether representational alignment between vision/language modalities varies by region, potentially reflecting differences in socio-economic documentation or infrastructure visibility across Africa. To this end, we compute similarity matrix between NMR-based and CV-based embeddings using cosine similarity after alignment through canonical correlation analysis (Weenink, 2003), with rows/columns sorted by latitude for regional interpretability (Figure 7). The resulting matrix reveals localized clusters of 11 Figure 6: Histogram of cosine similarities between NMR and CV embeddings, after alignment through canonical correlation analysis (first component), for matched DHS clusters. higher similarity, suggesting that embeddings from geographically proximate regions along similar latitudes exhibit stronger alignment. This spatial coherence indicates that geographically proximate areas exhibit systematically stronger cross-modal alignment. Additional Robustness Checks. Appendix provides details on additional robustness tests. For example, it examines potential training/test leakage. There, in Figure A.I.2, we analyze whether temporal patterns in model performance might indicate leakage into future periods. As shown in Figure A.I.2, R2 scores across years for exemplary models (ASA-CleanedTraces-OAI3Small and NMR-Llama4Maverick-OAI3Small) exhibit no clear temporal trend, indicating consistent performance over time and minimal evidence of direct data leakage."
        },
        {
            "title": "5 Discussion & Conclusion",
            "content": "Overall, our findings support the hypothesis that socio-economic indicators, such as household wealth, can be encoded in compact, recoverable latent representations across visual and textual modalities: fused models outperform single-modality baselines by 21.77% in R2 across random and country-holdout evaluations. Analysis within our framework indicates moderate convergence between vision and text embeddings, which exhibit median cosine similarity of around 0.60 after alignment through canonical correlation analysis partially aligning with the Platonic Representation Hypothesis (Huh et al., 2024); evidence for agent-induced novelty is limited. By releasing large-scale multimodal DHS dataset, we enable further research in AIaugmented poverty mapping, with implications for equitable global development monitoring and policy interventions. 12 Figure 7: Latent representation similarity matrix of matching embeddings from NMR and CV, based on cosine similarity after alignment through canonical correlation analysis (first component). Embeddings are sorted by latitude to allow regional interpretability. See Figure A.I.3 for ASA/CV comparison. From practical standpoint, the NMR approach offers significant scalability advantages over the ASA approach. The NMR pipeline requires only single LLM inference per location, whereas the ASA approach requires full web search and text processing pipeline. This makes NMR more cost-effective and suitable for large-scale poverty mapping applications. Our results show that NMR alone achieves 95% of the performance of the combined NMR+CV approach, making it an excellent choice for resource-constrained deployments. Limitations of our study include reliance on DHS clusters, which may introduce sampling biases by excluding remote areas from sampling (Kakooei and Daoud, 2024). The search agent may also retrieve post-treatment information from the web, potentially biasing causal analyses. Additionally, the computational demands of agent-driven pipelines at continental scales limit scalability for global or time-series applications. Future work could integrate causal inference into multimodal socio-economic AI while addressing textspecific challenges (Grimmer et al., 2022; Daoud et al., 2022; Pieuchon et al., 2024)such as improved detection and control of post-treatment bias in agent representationsto further test the Platonic Representation and Agent-Induced Novelty Hypotheses."
        },
        {
            "title": "References",
            "content": "Emily Aiken, Esther Rolf, and Joshua Blumenstock. Fairness and representation in satellite-based poverty maps: Evidence of urban-rural disparities and their impacts on downstream policy. arXiv preprint arXiv:2305.01783, 2023. Mina Arzaghi, Florian Carichon, and Golnoosh Farnadi. Understanding intrinsic socioeconomic biases in 13 large language models, 2024. URL https://arxiv.org/abs/2405.18662. Boris Babenko, Jonathan Hersh, David Newhouse, Anusha Ramakrishnan, and Tom Swartz. Poverty mapping using convolutional neural networks trained on high and medium resolution satellite images, with an application in mexico, 2017. URL https://arxiv.org/abs/1711.06323. Marshall Burke, Anne Driscoll, David B. Lobell, and Stefano Ermon. Using satellite imagery to understand and promote sustainable development. 371(6535):eabe8628, 2021. ISSN 0036-8075, 10959203. doi: 10.1126/science.abe8628. URL https://www.sciencemag.org/lookup/doi/10. 1126/science.abe8628. Adel Daoud and Devdatt Dubhashi. Statistical Modeling: The Three Cultures. 5(1), 2023. ISSN 2644-2353, 2688-8513. doi: 10.1162/99608f92.89f6fe66. URL https://hdsr.mitpress.mit.edu/pub/ uo4hjcx6/release/1. Adel Daoud, Bernhard Reinsberg, Alexander Kentikelenis, Thomas Stubbs, and Lawrence King. The international monetary funds interventions in food and agriculture: An analysis of loans and conditions. Food Policy, 83:204218, 2019. Adel Daoud, Connor Jerzak, and Richard Johansson. Conceptualizing treatment leakage in text-based In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, causal inference. editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 56385645, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.413. URL https://aclanthology.org/2022.naacl-main.413/. Nicolas Audinet de Pieuchon, Adel Daoud, Connor T. Jerzak, Moa Johansson, and Richard Johansson. Benchmarking debiasing methods for llm-based parameter estimates, 2025. URL https://arxiv. org/abs/2506.09627. MEASURE DHS et al. Demographic and health surveys. Calverton: Measure DHS, 2013. Christopher Elvidge, Paul Sutton, Benjamin Tuttle, Tilottama Ghosh, and Kimberly Baugh. Global urban mapping based on nighttime lights. In Global mapping of human settlement, pages 157172. CRC Press, 2009. Aryo Pradipta Gema, Alexander H√§gele, Runjin Chen, Andy Arditi, Jacob Goldman-Wetzler, Kit FraserTaliente, Henry Sleight, Linda Petrini, Julian Michael, Beatrice Alex, et al. Inverse scaling in test-time compute. arXiv preprint arXiv:2507.14417, 2025. Justin Grimmer, Margaret Roberts, and Brandon Stewart. Text as data: new framework for machine learning and the social sciences. Princeton University Press, 2022. Sungwon Han, Donghyun Ahn, Seungeon Lee, Minhyuk Song, Sungwon Park, Sangyoon Park, Jihee Kim, and Meeyoung Cha. Geosee: Regional socio-economic estimation with large language model. arXiv preprint arXiv:2406.09799, 2024. Minyoung Huh, Brian Cheung, Tongzhou Wang, and Phillip Isola. Position: The platonic representation hypothesis. In Forty-first International Conference on Machine Learning, 2024. Neal Jean, Marshall Burke, Michael Xie, Matthew Alampay Davis, David Lobell, and Stefano Ermon. Combining satellite imagery and machine learning to predict poverty. Science, 353(6301):790794, 2016. 14 Connor Jerzak, Fredrik Johansson, and Adel Daoud. Integrating earth observation data into causal inference: challenges and opportunities. arXiv preprint arXiv:2301.12985, 2023. Woojin Jung, Arunesh Sinha, Andrew Kim, Vatsal Shah, Yuxiao Lu, Lami Lee, and Tawfiq Ammari. The last mile in remote sensing poverty prediction. ACM J. Comput. Sustain. Soc., 3(3), June 2025. doi: 10.1145/3724422. URL https://doi.org/10.1145/3724422. Mohammad Kakooei and Adel Daoud. Increasing the confidence of predictive uncertainty: earth observations and deep learning for poverty estimation. IEEE Transactions on Geoscience and Remote Sensing, 62:113, 2024. Mohammad Kakooei, James Bailie, Albin S√∂derberg, Albin Becevic, and Adel Daoud. Mapping africa settlements: High resolution urban and rural map by deep learning and satellite imagery, 2024a. URL https://arxiv.org/abs/2411.02935. Mohammad Kakooei, Klaudia Solska, and Adel Daoud. Analyzing poverty through intra-annual time-series: wavelet transform approach, 2024b. URL https://arxiv.org/abs/2411.02855. Badri Raj Lamichhane, Mahmud Isnan, and Teerayut Horanont. Exploring machine learning trends in poverty mapping: review and meta-analysis. Science of Remote Sensing, page 100200, 2025. Erik Miehling, Karthikeyan Natesan Ramamurthy, Kush Varshney, Matthew Riemer, Djallel Bouneffouf, John Richards, Amit Dhurandhar, Elizabeth Daly, Michael Hind, Prasanna Sattigeri, et al. Agentic ai needs systems theory. arXiv preprint arXiv:2503.00237, 2025. Joseph OBrien. Seeing what we cant: Evaluating implicit biases in deep learning satellite imagery models trained for poverty prediction. 2023. Markus Pettersson, Mohammad Kakooei, Julia Ortheden, Fredrik Johansson, and Adel Daoud. Time series of satellite imagery improve deep learning estimates of neighborhood-level poverty in africa. In IJCAI, pages 61656173, 2023. Nicolas Pieuchon, Adel Daoud, Connor Jerzak, Moa Johansson, and Richard Johansson. Can large language models (or humans) disentangle text? In Proceedings of the Sixth Workshop on Natural Language Processing and Computational Social Science (NLP+ CSS 2024), pages 5767, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/ abs/2103.00020. Kazuki Sakamoto, Connor T. Jerzak, and Adel Daoud. scoping review of earth observation and machine learning for causal inference: Implications for the geography of poverty, 2025. URL https://arxiv. org/abs/2406.02584. Hamid Sarmadi, Ola Hall, Thorsteinn R√∂gnvaldsson, and Mattias Ohlsson. Leveraging chatgpts multimodal vision capabilities to rank satellite images by poverty level: Advancing tools for social science research, 2025. URL https://arxiv.org/abs/2501.14546. Adam Stewart, Caleb Robinson, Isaac Corley, Anthony Ortiz, Juan Lavista Ferres, and Arindam Banerjee. Torchgeo: deep learning with geospatial data. In Proceedings of the 30th international conference on advances in geographic information systems, pages 112, 2022. 15 Jialin Wang and Zhihua Duan. Agent ai with langgraph: modular framework for enhancing machine translation using large language models. arXiv preprint arXiv:2412.03801, 2024. David Weenink. Canonical correlation analysis. In Proceedings of the Institute of Phonetic Sciences of the University of Amsterdam, volume 25, pages 8199. University of Amsterdam Amsterdam, 2003. Christopher Yeh, Anthony Perez, Anne Driscoll, George Azzari, Zhongyi Tang, David Lobell, Stefano Ermon, and Marshall Burke. Using publicly available satellite imagery and deep learning to understand economic well-being in africa. Nature communications, 11(1):2583, 2020. Zhilun Zhou, Jingyang Fan, Yu Liu, Fengli Xu, Depeng Jin, and Yong Li. Synergizing llm agents and knowledge graph for socioeconomic prediction in lbsn, 2024. URL https://arxiv.org/abs/ 2411.00028. Fucheng Warren Zhu, Connor Thomas Jerzak, and Adel Daoud. Optimizing multi-scale representations to detect effect heterogeneity using earth observation and computer vision: Applications to two anti-poverty In Biwei Huang and Mathias Drton, editors, Proceedings of the Fourth Conference on Causal rcts. Learning and Reasoning, volume 275 of Proceedings of Machine Learning Research, pages 894919. PMLR, 0709 May 2025. URL https://proceedings.mlr.press/v275/zhu25a.html."
        },
        {
            "title": "6 Appendix I. Additional Empirical Results",
            "content": "Table A.I.1: Test Split Performance (R2 and RMSE) Across Evaluation Strategies, Grouped by Modality and Embedding Model, Sorted by Random Split R2 Source Procedure RMSE Embedding Llama4Maverick NMR+ASA+CV Llama+Traces NMR+CV NMR+ASA+CV Llama+Traces NMR+CV ASA+CV ASA+CV NMR+ASA NMR NMR+ASA NMR CV NMR NMR ASA NMR ASA ASA ASA ASA NMR NMR ASA ASA ASA ASA ASA NMR R¬≤ RMSE Random Split 9.045 0.772 OAI+ViT 9.196 OAI+ViT 0.765 9.412 MPNet+ViT 0.754 9.533 MPNet+ViT 0.747 Llama4Maverick 9.662 0.740 OAI+ViT CleanedTraces 10.423 MPNet+ViT 0.698 CleanedTraces 10.460 0.697 OAI3Small Llama+Traces 10.945 0.668 OAI3Small Llama4Maverick 10.961 0.667 MPNet-FT Llama+Traces 11.456 0.641 MPNet-FT Llama4Maverick 11.469 0.634 ViT16Small EOData 11.680 0.622 OAI3Small GPT4.1Nano 11.984 0.607 MPNet-FT DeepSeekR1 11.921 0.606 OAI3Small CleanedTraces 12.231 0.591 MPNet-FT GPT4.1Nano 12.197 0.588 OAI3Small Top10Search 12.529 0.565 OAI3Small Wikipedia 12.645 0.557 JustificationOnly OAI3Small 12.832 0.544 JustificationPrediction OAI3Small 13.482 0.503 MPNet-FT Grok3Mini 13.532 0.493 OAI3Small Grok3Mini 13.722 0.478 MPNet-FT CleanedTraces 13.735 0.477 MPNet-FT Wikipedia 13.764 0.475 Top10Search MPNet-FT 14.092 0.450 JustificationPrediction MPNet-FT 14.131 0.447 MPNet-FT JustificationOnly 18.918 0.008 OAI3Small DeepSeekR R¬≤ OOC Split RMSE R¬≤ OOT Split 0.529 0.527 0.504 0.536 0.463 0.487 0.371 0.440 0.386 0.418 0.446 0.362 0.417 0.301 0.351 0.262 0.225 0.279 0.287 0.254 0.269 0.241 0.138 0.229 0.230 0.220 -0.143 12.169 12.132 12.288 12.085 12.893 12.808 14.062 13.285 13.871 13.700 13.194 14.192 13.792 14.739 14.543 15.148 15.618 15.057 14.969 15.562 15.111 15.717 16.677 15.865 15.826 15.910 19.055 0.694 0.700 0.701 0.698 0.659 0.628 0.610 0.609 0.605 0.558 0.569 0.557 0.523 0.504 0.504 0.483 0.455 0.463 0.458 0.394 0.438 0.382 0.377 0.381 0.374 0.371 -0. 10.165 10.075 9.947 9.998 10.735 11.093 11.516 11.527 11.478 12.135 12.080 12.288 12.611 12.973 12.886 13.248 13.605 13.514 13.585 14.261 13.819 14.344 14.402 14.345 14.465 14.504 18.911 17 Figure A.I.1: R2 scores across years for the best system combining NMR+ASA+CV benchmarking against CV-only. It is possible that IWI information from the DHS data appears within Data Leakage Considerations. the agent traces or the neural weights. This does not cause train/test leakage unless the data postdates the models training snapshot. Moreover, the DHS data are not publicly available, and to our knowledge, are not directly used in training corpora. Moreover, the MPNet model used in many of our analyses was trained in 2020, predating much of the DHS data examined here. While we cannot definitively determine whether such information was used in model training, we did perform string search of the agent traces for the terms IWI, International Wealth Index, and DHS. We found that 10.4% of agent traces contained these query terms, which could be associated with documentary evidence from both preand post-focal year. We reran the ASA results without these observations and found similar results as with (ASA-CleanedTraces-OAI3Small: R2 RANDOM = 0.585; RMSERANDOM = 12.229); see also Figure A.I.2 and Figure 5, which shows absolute improvements from multimodal data over time relative the image-only baseline. R2 Figure A.I.2: variants: ASA-CleanedTraces-OAI3Small and NMR-Llama4Maverick-OAI3Small. No clear temporal trend is observed (as we might have expected under direct data leakage, assuming that more recent DHS surveys are systematically more likely or less likely to be included in the LLM training corpora. exemplary model across scores years two for 19 Figure A.I.3: Latent representation similarity matrix of matching embeddings from ASA and CV, based on cosine similarity after alignment through canonical correlation analysis. Embeddings are sorted by latitude to allow regional interpretability across pairs."
        },
        {
            "title": "7 Appendix II. Modeling Details",
            "content": "Computing Infrastructure. All experiments presented were generated on the following hardware/software setup: 4 Tesla V100-PCIE-32GB; total VRAM: 128GB. (CUDA Version: 12.6). 80 Intel(R) Xeon(R) Gold 6148 CPUs @ 2.40GHz; total RAM: 754GB. Operating System: Ubuntu 24.04.2 LTS. Kernel: Linux 6.8.0-57-generic. Python Version: 3.10.15. Pretrained Model Details. The used LLMs are described in TableA.II.1 and information on the embedding models chosen is to be found in TableA.II.2 Table A.II.1: Details of the LLMs used in our experiments. Model size is shown if available. Model GPT-4.1 Nano Llama 4 Maverick DeepSeek R1 (0528) DeepSeek Grok 3 Mini Developer OpenAI Meta xAI Size 405B (A17B) 671B (A37B) DeepSeek API OpenAI Groq xAI Table A.II.2: Details of embedding models used. Model OpenAI (text-embed-3-small) MPNet (all-mpnet-base-v2) Context Len. (Tokens) Emb. Dim 8192 384 1536 768 To encode the satellite imagery, pretrained Vision Transformer model, ViTSmall16_Weights.LANDSAT_ETM_SR_MOCO, is deployed and pre-trained on Landsat imagery (Stewart et al., 2022). Satellite image inputs were 5-channel (RBG+{SW, LW}Infared) and 224224 resolution. Ridge regression was employed for supervised prediction from visual, textual, and fused embeddings. The only model fine-tuned beyond ridge regression (alpha: 1.0) was the MPNet embedding model (all-mpnet-base-v2), which was trained using 5-fold cross-validation. All other embeddings (e.g., OpenAI text-embed-3-small) were used in frozen mode (no fine-tuning). Joint representations were constructed by directly concatenating embeddings, and the differing dimensions (e.g., 768 for MPNet, 1536 for OpenAI embeddings) were handled without requiring projection. The AI Search Agent was built using LangGraph, with maximum recursion depth of 20. All experiments were run using 80/20 splits of data into training/test sets."
        }
    ],
    "affiliations": [
        "Chalmers & Link√∂ping University",
        "Fraunhofer Center",
        "UT Austin"
    ]
}