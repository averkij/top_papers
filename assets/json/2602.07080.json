{
    "paper_title": "CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs",
    "authors": [
        "Yicheng He",
        "Zheng Zhao",
        "Zhou Kaiyu",
        "Bryan Dai",
        "Jie Fu",
        "Yonghui Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current paradigms for code verification rely heavily on external mechanisms-such as execution-based unit tests or auxiliary LLM judges-which are often labor-intensive or limited by the judging model's own capabilities. This raises a fundamental, yet unexplored question: Can an LLM's functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the model's neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as a mechanistic diagnostic task, mapping the model's explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the model's internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as a decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit."
        },
        {
            "title": "Start",
            "content": "CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs Yicheng He * 1 Zheng Zhao * 2 Kaiyu Zhou 3 Bryan Dai 4 Jie Fu 4 Yonghui Yang"
        },
        {
            "title": "Abstract",
            "content": "Current paradigms for code verification rely heavily on external mechanismssuch as executionbased unit tests or auxiliary LLM judgeswhich are often labor-intensive or limited by the judging models own capabilities. This raises fundamental, yet unexplored question: Can an LLMs functional correctness be assessed purely from its internal computational structure? Our primary objective is to investigate whether the models neural dynamics encode internally decodable signals that are predictive of logical validity during code generation. Inspired by mechanistic interpretability, we propose to treat code verification as mechanistic diagnostic task, mapping the models explicit algorithmic trajectory into line-level attribution graphs. By decomposing complex residual flows, we aim to identify the structural signatures that distinguish sound reasoning from logical failure within the models internal circuits. Analysis across Python, C++, and Java confirms that intrinsic correctness signals are robust across diverse syntaxes. Topological features from these internal graphs predict correctness more reliably than surface heuristics and enable targeted causal interventions to fix erroneous logic. These findings establish internal introspection as decodable property for verifying generated code. Our code is at https:// github.com/bruno686/CodeCircuit. 6 2 0 2 6 ] . [ 1 0 8 0 7 0 . 2 0 6 2 : r 1. Introduction Large language models (LLMs; Liu et al., 2024a; He et al., 2025b; Rozière et al., 2023; Guo et al., 2024) have transformed code generation from simple solvers to complex agents (Wang et al., 2024a; Shinn et al., 2023; Madaan et al., *Equal contribution 1University of Illinois Urbana-Champaign 2University of Edinburgh 3Nanyang Technological University 4IQuest Research 5National University of Singapore. Correspondence to: Yicheng He <yh84@uiuc.edu>. Preprint. February 10, 2026. 1 2023). As these models integrate into mission-critical workflows, the demand for functionally correct and maintainable code has intensified (Liu et al., 2023; 2024b). However, the stochastic nature of LLMs makes reliable assessment persistent bottleneck. While execution-based verification via unit tests remains the standard (Khan et al., 2024; Dong et al., 2025; Wang et al., 2023b; Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021), it is limited by labor-intensive test design and sparse edge-case coverage. Emergent LLM-as-a-Judge frameworks offer scalable automation (He et al., 2025a; Jiang et al., 2025), yet they introduce inference overhead and suffer from recursive dependencies where reliability is bounded by the evaluators own reasoning priors. Given these limitations of external mechanisms, natural question arises: Can code correctness be assessed intrinsically, by examining the LLMs internal computational structure? Prior work has probed neuron activations and representation dynamics during generation (Bui et al., 2025; Huang et al., 2025; Patel et al., 2025), yet these analyses have not been directly linked to the correctness of the generated code. In parallel, recent advances in mechanistic interpretability (Michaud et al., 2024; Sharkey et al., 2025; Rai et al., 2024a; Gao et al., 2025; Templeton et al., 2024), specifically the use of attribution graphs to trace reasoning pathways (Dunefsky et al., 2024; Ameisen et al., 2025; Zhao et al., 2025), have shown promise in reasoning domains. We believe that bridging this gap requires deeper look at how code-specific structures are manifested within the models latent space during the generation process. Unlike natural language or abstract math, code possesses rigid, objective topology, such as control flow and variable binding. Recent research (Zhang et al., 2025) demonstrates that LLMs could form internal finite automata rather than relying on superficial statistical shortcuts. Thus, we hypothesize that reliable code generation requires the model to instantiate specific neural circuits that mirror this topology, and that errors manifest as detectable structural collapses within these circuits. By decoding the intrinsic circuits that drive code generation, we gain window into the models internally consistent computational trajectoriesbypassing both the opacity of black-box methods and the incompleteCodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs ness of external verification frameworks, ultimately providing more robust foundation for assessing model reliability. To operationalize this, we introduce CodeCircuit, whitebox verification framework that analyzes the line-level attribution graph of the generated code. We adopt the instrumentation of sparse autoencoders (transcoders) to decompose complex residual flows into causal graph of interpretable features. Using CodeCircuit, we conduct an extensive analysis across diverse programming languages (Python, C++, Java). Although line-level attribution graphs are computationally demanding, this detail is crucial for exposing failure mechanisms. CodeCircuit thus serves as scientific instrument for precise auditing and debugging, providing mechanistic insights into code generation beyond what coarser methods can reveal. Our results reveal that, within fixed model, internal attribution structures correlate with code correctness across multiple programming languages. We find that correct and incorrect code exhibit systematic topological differences in their attribution graphs (Figure 1). Consequently, CodeCircuit consistently outperforms both black-box methods (e.g., Temperature Scaling; Shih et al., 2023) and gray-box methods (e.g., Chain-ofEmbedding; Wang et al., 2024b) in verification performance. Notably, we demonstrate that these signals are not merely correlational: by performing targeted interventions on the graphs nodes (Meng et al., 2022; 2023), we can causally correct erroneous code, proving that the attribution graph captures the functional mechanism of generation. Our contributions are summarized as follows: Assessing Generated Code via Internal Computational Structure. We investigate the feasibility of evaluating LLM-generated code by examining the models internal neural dynamics rather than its surface-level output. By extracting line-level attribution graphs, we demonstrate that code correctness appears to be reflected in structured patterns of internal computation, enabling reliability assessment without external supervision or execution. Shared Signals and Causal Debugging. We provide empirical evidence that these structural correctness signals are robust across multiple programming languages, suggesting that LLMs learn shared computational circuits independent of surface syntax. Furthermore, we establish the causal nature of these features: by intervening on specific nodes within the attribution graph, we demonstrate the ability to patch the models internal logic, moving from passive verification to active mechanistic debugging. 2. Preliminary Mechanistic Interpretability via Attribution Graph. Attribution Graphs (AGs; Ameisen et al., 2025) provide causal, linear decomposition of transformer models computation for specific input prompt, enabling the tracing of information flow through interpretable features. The construction of an AG is predicated on the local replacement model, which substitutes the standard Multi-Layer Perceptrons (MLPs) with set of Per-Layer Transcoders (PLTs). PLT creates sparse feature vector (l) at layer from the residual stream x(l) via an encoder (l) enc and non-linearity σ (e.g., TopK or ReLU): (l) = σ(W (l) encx(l) + b(l) enc). (1) Crucially, these features are trained to reconstruct the local MLP output (or residual update) via decoder weights (l) dec. Unlike standard Autoencoders, PLTs disentangle the superposition of features in the dense activation space: ˆm(l) = (l) decf (l) + b(l) dec. (2) The local replacement model is constructed by replacing the true MLP output m(l) with an error-corrected PLT output ˆm(l) adj = ˆm(l) + (m(l) ˆm(l)), where the error term (m(l) ˆm(l)) is computed on the forward pass and effectively acts as bias node. This yields locally linearized computation in which downstream quantities are linear functions of feature activations and residual stream components, with attention outputs and normalization statistics treated as fixed. Linearity therefore holds with respect to the substituted MLP pathways under the frozen forward pass. The AG is directed acyclic graph = (V, E) where nodes include active PLT features, token embeddings, error terms, and output logits. An edge wij from source node (at layer l) to target node (at layer > l) represents the linear contribution of node is activation to the pre-activation of node under the locally linearized computation. Let ai denote the activation of node i, and let vout,i and vin,j denote the residual stream directions associated with the output of node and the input of node j, respectively. The contribution is defined as: wij = ai in,jJijvout,i, (3) where Jij is the Jacobian of the frozen residual stream transformation mapping the output of node to the input of node j, with stop-gradients applied to all nonlinearities. This formulation attributes influence through the residual stream rather than via direct feature-to-feature dependencies. To enhance interpretability, the graph undergoes pruning process. Rather than matrix inversion (which is intractable for large feature spaces), nodes are pruned based on their attribution to the final model output. We compute the attribution of every feature node to the correct logit (or target metric) via single backward pass. Nodes are retained only if they contribute significant portion of the total influence relative to threshold τ . Edges with negligible weights 2 CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs Figure 1. Topological fingerprints of code generation errors. Distributions of five graph features extracted from attribution graphs show differences between correct (blue) and incorrect (orange) code construction steps. These results demonstrate that the attribution topology provides structural signal for monitoring the integrity of the code generation process. are similarly pruned, resulting in sparse, mechanistically faithful circuit responsible for the models behavior. Problem Formulation. We consider the task of verifying the correctness of code snippet generated by large language model in response to natural language specification S. Formally, the code snippet is decomposed into sequence of discrete logical steps (e.g., lines of code), denoted as = {s1, s2, . . . , sN }. Our objective is to predict the correctness label yi {0, 1} for each step si, where yi = 1 indicates that the i-th step is logically sound and consistent with the specification S, and yi = 0 otherwise. Unlike traditional verification methods that rely on the execution output of the entire snippet C, we formulate code verification as mechanistic sequence labeling problem. For each step si, the LLMs internal computation is represented by high-dimensional activation trace. We map this trace onto an interpretable basis to construct step-level attribution graph Gi = (Vi, Ei), which serves as formal proxy for the models latent algorithmic execution. The problem is thus transformed into learning diagnostic mapping : Gi yi. This mapping is decomposed into two successive operations. First, structural encoding function Φ distills the complex, variable-sized topology of the attribution graph into fixed-size feature vector xi Rd: xi = Φ(Gi), (4) where xi captures the structural fingerprints of the computational trajectory, including node composition, information flow density, and topological bottlenecks. Second, discriminative classifier hϕ, parameterized by ϕ, maps the structural encoding to correctness probability: ˆyi = hϕ(xi) = (yi = 1 Gi; ϕ). (5) The learning objective is to find the optimal parameters ϕ that minimize the empirical risk over dataset of labeled code steps: ϕ = arg min ϕ 1 (cid:88) j= L(hϕ(Φ(Gj)), yj), (6) where is standard cross-entropy loss function. This formulation enables the detection of logical fallacies at granular, step-by-step level, grounded in the structural pathologies of the models internal reasoning pathways. 3. CodeCircuit: Mechanistic Code Verification The fundamental premise of our framework CodeCircuit (Figure 2) is that the correctness of generated code sequence is intrinsically encoded within the latent algorithmic trajectory of the language model. Traditional verification paradigms treat the model as an opaque function, focusing on extrinsic execution outcomes that often fail to capture the underlying causal mechanisms of logical fallacies. In contrast, building upon the mechanistic verification paradigm established by Zhao et al. (2025), we propose white-box framework that audits the structural integrity of the models internal reasoning pathways. By projecting high-dimensional neural activations onto an interpretable basis, we extend their approach to identify the specific structural fingerprints of algorithmic success or failure in code, shifting the diagnostic focus from output observation to internal process auditing. 3.1. Latent Algorithmic Tracing via Attribution Graphs To expose the hidden computational logic of code generation, we construct step-level attribution graph Gi = (Vi, Ei) for each discrete logical step i. This graph serves as formal causal map of the models mechanistic reasoning pathway. As established in the preliminaries, we utilize local replacement model where standard MLP layers are substituted by PLTs. This transformation projects the residual stream into sparse, interpretable feature space (l), where individual dimensions correspond to discrete algorithmic primitives. The resulting graph Gi captures the linear flow of information from input embeddings Vemb through these latent features Vf eat to the final logit output. To account for the portion of the computation not captured by the interpretable features, we explicitly include error nodes Verr, defined as the residual difference between the true 3 CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs Figure 2. Overview of the CodeCircuit framework. CodeCircuit maps an LLMs internal dynamics into line-level Attribution Graph to detect errors. By extracting structural features, including global, topological, and node states, the framework identifies latent structural fingerprints of validity. NL represents the next line. MLP output and its local PLT reconstruction: decf (l) + b(l) e(l) = m(l) (W (l) dec). (7) Including Verr allows for quantitative assessment of the mechanical faithfulness of the reasoning pathway, where the total influence of error nodes serves as proxy for the epistemic uncertainty inherent in the models latent algorithm. 3.2. Structural Feature Extraction and Pathological Analysis Inspired by Zhao et al. (2025), we hypothesize that logical errors in code generation manifest as structural anomalies within the attribution graphs topology. To capture these, we define comprehensive feature extraction mapping Φ : Gi xi that distills the complex topology of Gi into fixed-size diagnostic vector xi. complete enumeration of the statistics-, activation-, and topology-based features included in xi is provided in Appendix A.3. This mapping systematically analyzes the circuit across multiple hierarchical dimensions of the computational trajectory. The first dimension concerns the mechanical composition and faithfulness of the circuit. Beyond simple cardinality measures of the node set Vi and edge set Ei, we define the error-to-feature influence ratio ηi to quantify the models reliance on unexplained components. Let wuv denote the attribution weight of the edge from node to node v; the cumulative error influence is formulated as: (cid:80) wuv (cid:80) wuv (cid:80) (cid:80) ηi = uVerr vVi (8) . uVf eat vVi high value of ηi indicates computation driven by opaque heuristics rather than interpretable algorithmic steps, suggesting higher probability of logical fragility. 4 Furthermore, we analyze the global structure of the reasoning pathway. We compute the graph density ρi = Ei/(Vi(Vi 1)) and the number of connected components to evaluate whether the latent algorithm is executing unified or fragmented process. To capture the hierarchical and modular nature of the computation, we calculate the average clustering coefficient Ci, which measures the degree to which nodes in the graph tend to cluster together: 2 (cid:12) (cid:26) (u,w)Ei (cid:12) (cid:12) u,wN (v) (cid:12) (u,w)Ei kv(kv 1) (cid:27)(cid:12) (cid:12) (cid:12) (cid:12) , Ci = 1 Vi (cid:88) vVi (9) where (v) is the neighborhood of node and kv is its degree. In the context of code generation, high modularity often corresponds to the execution of independent sub-tasks, while the presence of disjoint components may indicate fragmented reasoning where the model fails to integrate necessary context into the program state. To identify critical information bottlenecks and logical hubs within the trajectory, we analyze the distribution of betweenness centrality CB(v) and degree centrality. For node Vi, the betweenness centrality represents the fraction of all shortest paths between node pairs that pass through v, formulated as: CB(v) = (cid:88) s=v=tVi σst(v) σst , (10) where σst is the total number of shortest paths from to and σst(v) is the number of those paths passing through v. We aggregate these values into statistical moments, including the mean µ(CB), standard deviation σ(CB), and maximum value max(CB), to characterize the concentration of CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs information flow. robust algorithmic execution typically exhibits stable, high-centrality features that coordinate statetracking, whereas an error-prone step may manifest as fragmented bottleneck with no clear coordination hub. Finally, we characterize attribution dynamics using total logit attribution score. Instead of relying on shallow connectivity, we aggregate the gradient-based attribution along all paths from feature node to the final output logits. We extract the statistical profile of these total influence scores S, where Sv represents the integrated gradient of the target probability with respect to feature v. This allows us to distinguish between features that merely activate and those that functionally drive the deep algorithmic processing essential for correct code generation. 3.3. Diagnostic Classification and Structural Bug Diagnosis The final stage of our framework involves the training of diagnostic classifier hϕ that maps the structural vector xi to correctness probability ˆyi = hϕ(xi). We utilize Gradient Boosting Decision Tree (GBDT; Ke et al., 2017) architecture, which is particularly effective for handling the heterogeneous, tabular nature of the extracted topological features. The objective is to minimize the cross-entropy loss over labeled dataset of correct and incorrect code generations: L(ϕ) = (cid:88) [yi log(ˆyi) + (1 yi) log(1 ˆyi)] . (11) Beyond simple binary classification, the GBDT model provides feature importance rankings that enable structural bug diagnosis. By identifying which topological properties are most predictive of failure, we can pinpoint the mechanical cause of logical error. For instance, high importance score for the error-to-feature ratio ηi combined with low betweenness centrality for state-tracking features suggests that the models failure stems from lack of interpretable state maintenance. This transforms code verification from post-hoc execution test into proactive, mechanistic audit of the models latent computational integrity. 4. Experiments In this section, we empirically validate our central hypothesis: that code correctness is intrinsic to the models latent computational topology and can be diagnosed without external execution. We structure our evaluation around five primary research questions: RQ1 (Effectiveness): Can CodeCircuit accurately distinguish between correct and incorrect code across diverse programming languages? RQ2 (Generalization): Do the identified discriminative signals generalize across different programming syntax? RQ3 (Scalability):Does predictive power of internal signals remain robust as algorithmic complexity increases? RQ4 (Visualization): Are there visually distinct topological patterns in the internal circuits of correct versus incorrect code? RQ5 (Causal Intervention): Can targeted interventions on internal failure modes rectify errors in code generation? 4.1. Evaluation Setup Datasets and Problem Domains. To ensure cross-lingual robustness, we evaluate CodeCircuit across Python, Java, and C++. We utilize the MBPP dataset (Austin et al., 2021) as the benchmark for Python and curate semantically equivalent task sets for Java and C++ by translating MBPP specifications and generating solutions via Gemma2-2B-it (Team et al., 2024). We use transcoder trained by (Lieberum et al., 2024). This alignment ensures that the model operates on identical algorithmic logic across disparate syntaxes. Ground truth labels are derived from GPT-4o (OpenAI et al., 2024), where step is defined as correct code line. We emphasize that external supervision is used only to obtain evaluation labels and is not required by CodeCircuit during inference. Any functional or logical deviation within line is treated as positive instance of failure. Detailed dataset statistics and label distributions are provided in Appendix A.1. The exact prompt templates used for code generation and line-wise labeling, as well as the attribution-graph extraction configuration, are shown in Appendix and A.2. Baselines. We compare CodeCircuit against suite of established black-box and gray-box metrics. These include Temperature Scaling (Shih et al., 2023), Maximum Softmax Probability (MaxProb), Perplexity (PPL), Energy (Liu et al., 2020), and Entropy derived from final token logits as measures of surface-level confidence. We also include Chain-of-Embedding (CoE), such as real-space CoER, complex-space CoE-C variants (Wang et al., 2024b) and CoT-Kinetics (Bi et al., 2025), as competitive gray-box baseline to evaluate the consistency of the reasoning process throughout the generation trajectory. Evaluation Metrics. Following Zhao et al. (2025), we evaluate verifier performance using AUROC, FPR@95, and AUPR. Since the task focuses on identifying coding errors, incorrect steps are treated as the positive class throughout. Taken together, these metrics characterize ranking quality, error detection strength, and false alarm behavior. 4.2. Main Performance Effectiveness. We first evaluate the fundamental efficacy of CodeCircuit in identifying code errors. This analysis ad5 CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs Table 1. Performance comparison of methods across different programming languages and baselines. Paradigm Method Black-Box Gray-Box MaxProb PPL Entropy Temp. Scaling Energy CoE-R CoE-C CoT-Kinetics White-Box Ours Python Java C++ AUROC AUPR FPR@95 AUROC AUPR FPR@95 AUROC AUPR FPR@95 51.25 51.24 51.42 50.86 49.45 50.75 51.10 51.41 79.89 29.08 29.08 29.04 28.85 25.63 25.22 42.91 29.04 54. 97.20 97.20 96.88 97.20 89.10 93.52 93.52 96.88 77.84 56.29 56.29 56.56 55.65 57.52 49.68 52.46 56.57 68. 46.00 46.00 46.34 45.44 45.38 40.07 44.76 46.34 56.83 91.20 91.20 89.96 92.08 88.38 94.46 94.64 89.96 79. 54.06 54.06 53.16 55.79 42.78 59.07 56.40 53.16 72.21 27.95 27.94 27.98 28.44 23.42 40.17 37.72 27.98 64. 93.39 93.39 93.39 91.49 97.71 92.49 92.61 93.39 80.51 Table 2. Performance comparison of different methods across Python, C++, and Java test sets. Results are evaluated using AUROC, AUPR, and FPR@95 metrics. Test Set Method (Train Set) Metrics AUROC AUPR FPR@95 Python Java C++ Baseline (MaxProb) CodeCircuit (Java) CodeCircuit (C++) CodeCircuit (Python) Baseline (MaxProb) CodeCircuit (Python) CodeCircuit (C++) CodeCircuit (Java) Baseline (MaxProb) CodeCircuit (Java) CodeCircuit (Python) CodeCircuit (C++) 51.25 55.42 53.18 79. 56.29 62.15 58.74 68.06 54.06 59.33 57.10 72.21 29.08 34.15 31.50 54. 46.00 48.33 47.12 56.83 27.95 36.80 32.55 64.55 97.20 94.10 95.82 77. 91.20 88.45 90.15 79.66 93.39 88.24 91.12 80.51 dresses whether internal circuit pathologies provide more reliable signal for correctness than surface-level heuristics (RQ1). As shown in Table 1, CodeCircuit outperforms all baseline paradigms across Python, Java, and C++, demonstrating the superior reliability of internal circuit pathologies over surface-level heuristics. Specifically, our approach achieves an AUROC of 79.89 in Python, representing substantial improvement over the best-performing black-box and gray-box methods, which hover around 51.42. This trend is consistent across other metrics and languages; for instance, we achieve an AUPR of 54.77 in Python and 56.83 in Java, nearly doubling the performance of traditional methods like MaxProb or CoT-Kinetics in many cases. Furthermore, our method markedly reduces the FPR@95, maintaining values as low as 80.51 in C++, whereas baselines frequently exceed 90. These results highlight that by leveraging internal model states, we can distinguish correct from erroneous code with far greater precision than methods relying on output probabilities or chain-of-thought consistency. Generalization. We investigate the cross-programming language invariance of these structural signatures by conducting transfer experiments across Python, Java, and C++ (RQ2). As shown in Table 2, our method exhibits robust zero-shot generalization capabilities, consistently outperforming the surface-level baseline (MaxProb) even when the probe is trained on different programming language. For instance, when testing on the Java set, the probe trained on Python achieves an AUROC of 62.15, marking 5.86% absolute improvement over the Java-specific MaxProb baseline (56.29). We observe nuanced transferability across the three languages; notably, the Java-trained probe generalizes effectively to both C++ (59.33 AUROC) and Python (55.42 AUROC), suggesting that the structural patterns captured from Java may encapsulate more fundamental logic common to imperative languages. While performance gap naturally exists between cross-domain transfer and in-domain training, the consistent superiority of our method over the baseline confirms that code errors manifest through set of universal structural pathologies. These results suggest that our white-box approach captures fundamental reasoning inconsistencies that transcend language-specific syntax. Scalability. To test the robustness of these signals, we analyze how predictive power scales as code complexity and physical length (number of lines {10, 20, 30}) increase (RQ3). As shown in the Figure 3, the performance advantage of CodeCircuit becomes significantly more pronounced as task difficulty grows, with AUROC rising from approximately 80 to nearly 92. While surface-level baselines like MaxProb remain stagnant, our method capitalizes on the richer internal activations provided by longer sequences to more accurately identify circuit pathologies. This suggests that unlike gray-box methods that struggle with consistency, our white-box approach becomes increasingly reliable in more demanding and complex computational contexts. 4.3. Visual Analysis (RQ4) To move beyond binary prediction, we conduct qualitative and fine-grained analysis of the attribution graphs to uncover the structural fingerprints of specific failure modes. Following Zhao et al. (2025), we visualize the separation between correct and erroneous computational trajectories 6 CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs Figure 3. Predictor performance on Python tasks by difficulty, showing CodeCircuits advantage as complexity increases. Figure 4. Feature distributions after PCA for correct (blue) and incorrect (red) code generation in different programming languages. using manifold projection techniques and case studies of individual circuits in Figure 4. The plots reveal that correct steps often form dense subset within the broader distribution of incorrect steps. This spatial overlap suggests many failures are structural near misses, maintaining similar graph topologies to valid code. Nevertheless, distinct zone of computational integrity exists where only correct samples reside. This observation is consistent with the findings of Zhao et al. (2025) for reasoning tasks. This consistency across languages confirms that our graph features capture universal signatures of correctness that transcend specific programming language. 4.4. Causal Intervention (RQ5) Finally, we move beyond passive verification to test the causal nature of the identified structural features. Consistent with recent findings by Zhao et al. (2025) in the domain of reasoning, we hypothesize that the anomalies detected by CodeCircuit are not merely correlational byproducts but active drivers of the models generation. To validate this, we perform mechanistic patching experimentintervening on specific nodes in the attribution graph to correct algorithmic logic in real-time. We illustrate this via case study on classic boundary condition failure. Given prompt to implement binary search algorithm, the model generated the incorrect update logic high = mid, common bug that leads to infinite loops, instead of the correct high = mid - 1  (Table 3)  . CodeCircuit flagged this step as structurally anomalous. Investigating the attribution graph revealed that the error was driven by specific transcoder feature in the middle layers, which semantically corresponded to greedy matching heuristic that overrode the precise arithmetic state maintenance required for the loop invariant. To test causality, we performed targeted intervention using activation clamping. We re-ran the generation and verified that by placing forward hook to suppress (zero out) this maladaptive feature, the models trajectory was immediately corrected. As shown in Table 3, the intervention successfully steered the model to generate high = mid - 1 and proceed to functionally correct solution. We also observed successful corrections via feature amplification, specifically by boosting under-active features associated with variable binding context. These results corroborate the causal link between latent circuit activation and reasoning correctness established by Zhao et al. (2025), while extending the phenomenon to the strict syntactic and logical constraints of programming. This serves as proof-of-concept for latent debugging: the ability to fix software bugs not by editing the source code, but by modulating the neural circuitry that generates it. 5. Related Work 5.1. Mechanistic Interpretability Mechanistic interpretability (Gao et al., 2025; Michaud et al., 2024; Rai et al., 2024b) aims to reverse-engineer 7 CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs Before Intervention (Incorrect) After Intervention (Correct) def search_insert(nums, target): low, high = 0, len(nums) - 1 while low <= high: def search_insert(nums, target): low, high = 0, len(nums) - 1 while low <= high: mid = (low + high) // 2 if nums[mid] == target: return mid elif nums[mid] < target: low = mid + 1 else: high = mid return low mid = (low + high) // 2 if nums[mid] == target: return mid elif nums[mid] < target: low = mid + 1 else: high = mid - 1 return low Trace: The model fails to decrement high, leading to potential infinite loop or incorrect insertion index. Trace: Suppressing the greedy feature allows the model to correctly update the boundary. Table 3. Side-by-side comparison of code reasoning trace before and after causal intervention. The highlight indicates the point of divergence where suppressing single greedy-matching transcoder feature corrects the models internal computational path and restores logical validity. neural networks into their underlying computational algorithms. The field was largely formalized by the circuits\" framework (Olah et al., 2020), which posits that features are the fundamental units of representation and are connected by weights to form circuits. In the context of Transformers, Elhage et al. (2021) introduced mathematical framework that identified induction heads as the mechanism for incontext learning. This paradigm has enabled the discovery of specific circuits for tasks such as indirect object identification (Wang et al., 2022) and modular arithmetic (Nanda et al., 2023). To address the challenge of polysemanticity, where single neurons represent multiple unrelated concepts (Olah et al., 2017), recent research has leveraged the Linear Representation Hypothesis (Elhage et al., 2022) and Sparse Autoencoders to decompose activations into monosemantic features (Bricken et al., 2023; Cunningham et al., 2023). Furthermore, to bridge the gap between manual inspection and large-scale models, automated discovery methods such as ACDC (Conmy et al., 2023) and LLM-based explanations (Bills et al., 2023) have been proposed. 5.2. LLM-based Code Generation LLMs have rapidly advanced from basic code completion utilities to sophisticated systems for automated software engineering (Liu et al., 2024a; He et al., 2025b; Li et al., 2023; Wang et al., 2023a; Hui et al., 2024). The initial phase of this evolution was marked by the development of foundational models, such as CodeLlama (Rozière et al., 2023) and DeepSeek-Coder (Guo et al., 2024), which established new performance baselines. These models were typically trained on massive, high-quality code corpora and leveraged longcontext windows to capture extensive code dependencies. subsequent research direction has focused on enhancing the functional correctness of generated code, moving beyond conventional open-loop generation. This involves the integration of execution-based feedback and self-debugging mechanisms (Chen et al., 2024; Jiang et al., 2024), where the model iteratively refines its proposed solution based on signals derived from compilers, linters, or test case execution. More recently, the scope of LLM-based code generation has expanded from isolated code snippets to full projectlevel development. The emergence of agentic frameworks, capable of autonomously addressing real-world software issues, has been significant development. The capabilities of these agents are rigorously evaluated using benchmarks like SWE-bench (Jimenez et al., 2023) and platforms such as OpenDevin (Wang et al., 2024a). 6. Conclusion This work asks single question: whether the correctness of LLM-generated code can be inferred from the models internal computation, without relying on execution, tests, or external judges. This question is motivated by the growing use of LLMs in settings where external evaluation is costly or incomplete, and where reliability must be assessed from the generation process itself rather than from downstream outcomes. Through systematic analysis based on line-level attribution graphs, we show that this is indeed possible. Correct and incorrect code exhibit consistent and distinguishable internal structures across programming languages, indicating that code correctness is encoded in the models internal representations. These results suggest that internal analysis provides viable and general approach to assessing code correctness, and open the door to reliability evaluation methods grounded directly in model behavior rather than external supervision. CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs"
        },
        {
            "title": "Impact Statement",
            "content": "This work contributes to the advancement of reliable and interpretable machine learning, specifically in the domain of automated software engineering. As LLMs are increasingly deployed to generate code for critical infrastructure and applications, ensuring their correctness is of paramount societal importance. Our proposed framework, CodeCircuit, advances this goal by shifting verification from opaque, blackbox outcomes to transparent, mechanistic processes. By exposing the internal circuits of code generation, this work aids in the early detection of logical failures and software vulnerabilities that standard execution-based testing might miss. Furthermore, our focus on interpretability aligns with the broader goal of developing trustworthy AI systems that are understandable by humans. However, we note that while mechanistic verification improves reliability, it should currently serve as an aid to, rather than replacement for, human review and rigorous software testing protocols."
        },
        {
            "title": "References",
            "content": "Ameisen, E., Lindsey, J., Pearce, A., Gurnee, W., Turner, N. L., Chen, B., Citro, C., Abrahams, D., Carter, S., Hosmer, B., Marcus, J., Sklar, M., Templeton, A., Bricken, T., McDougall, C., Cunningham, H., Henighan, T., Jermyn, A., Jones, A., Persic, A., Qi, Z., Ben Thompson, T., Zimmerman, S., Rivoire, K., Conerly, T., Olah, C., and Batson, J. Circuit tracing: Revealing computational graphs in language models. Transformer Circuits Thread, 2025. URL https://transformer-circuits.pub/ 2025/attribution-graphs/methods.html. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q. V., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Bi, J., Yan, D., Wang, Y., Huang, W., Chen, H., Wan, G., Ye, M., Xiao, X., Schuetze, H., Tresp, V., et al. Cot-kinetics: theoretical modeling assessing lrm reasoning process. arXiv preprint arXiv:2505.13408, 2025. and Saunders, W. Bills, S., Cammarata, N., Mossing, D., Tillman, H., I., Leike, J., Wu, Language models can https: Gao, L., Goh, G., Sutskever, J., explain neurons //openaipublic.blob.core.windows.net/ neuron-explainer/paper/index.html, 2023. in language models. Bricken, T., Templeton, A., Batson, J., Chen, B., Jermyn, A., Conerly, T., Turner, N., Anil, C., Denison, C., Askell, A., et al. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2, 2023. 9 Bui, T.-D., Vu, T. T., Nguyen, T.-T., Nguyen, S., and Vo, H. D. Correctness assessment of code generated by large language models using internal representations. arXiv preprint arXiv:2501.12934, 2025. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Chen, X., Lin, M., Schärli, N., and Zhou, D. Teaching large language models to self-debug. In International Conference on Learning Representations (ICLR), 2024. Conmy, A., Mavor-Parker, A., Lynch, A., Heimersheim, S., and Garriga-Alonso, A. Towards automated circuit discovery for mechanistic interpretability. Advances in Neural Information Processing Systems, 36:1631816352, 2023. Cunningham, H., Ewart, A., Riggs, L., Huben, R., and Sharkey, L. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. Dong, Y., Ding, J., Jiang, X., Li, G., Li, Z., and Jin, Z. Codescore: Evaluating code generation by learning code execution. ACM Transactions on Software Engineering and Methodology, 34(3):122, 2025. Dunefsky, J., Chlenski, P., and Nanda, N. Transcoders find interpretable LLM feature circuits. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/ forum?id=J6zHcScAo0. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., et al. mathematical framework for transformer circuits. Transformer Circuits Thread, 1(1):12, 2021. Elhage, N., Hume, T., Olsson, C., Nanda, N., Henighan, T., Johnston, S., ElShowk, S., Joseph, N., DasSarma, N., Mann, B., et al. Softmax linear units. transformer circuits thread, 2022. Gao, L., Rajaram, A., Coxon, J., Govande, S. V., Baker, B., and Mossing, D. Weight-sparse transformers have interpretable circuits, 2025. URL https://arxiv. org/abs/2511.13653. Guo, D., Zhu, Q., Yang, D., Xie, Z., Dong, K., Zhang, W., et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Hanna, M., Piotrowski, M., Lindsey, J., and Ameisen, https://github.com/ circuit-tracer. E. CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs safety-research/circuit-tracer, 2025. The first two authors contributed equally and are listed alphabetically. He, J., Shi, J., Zhuo, T. Y., Treude, C., Sun, J., Xing, Z., Du, X., and Lo, D. Llm-as-a-judge for software engineering: Literature review, vision, and the road ahead. arXiv preprint arXiv:2510.24367, 2025a. He, Y., Huang, C., Li, Z., Huang, J., and Yang, Y. Visplay: Self-evolving vision-language models from images. arXiv preprint arXiv:2511.15661, 2025b. Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., et al. Measuring coding challenge competence with APPS. In Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2021. Huang, Y., Ma, L., Nishikino, K., and Akazaki, T. Risk assessment framework for code llms via leveraging internal states. In Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering, pp. 432443, 2025. Hui, B., Yang, J., Cui, Z., Yang, J., Liu, D., Zhang, L., Liu, T., Zhang, J., Yu, B., Dang, K., et al. Qwen2.5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., et al. Starcoder: May the source be with you! arXiv preprint arXiv:2305.06161, 2023. Lieberum, T., Rajamanoharan, S., Conmy, A., Smith, L., Sonnerat, N., Varma, V., Kramar, J., Dragan, A., Shah, R., and Nanda, N. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2. In Belinkov, Y., Kim, N., Jumelet, J., Mohebbi, H., Mueller, A., and Chen, H. (eds.), Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pp. 278300, Miami, Florida, US, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.blackboxnlp-1. 19. URL https://aclanthology.org/2024. blackboxnlp-1.19/. Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., et al. Deepseekv3 technical report. arXiv preprint arXiv:2412.19437, 2024a. Liu, J., Xia, C. S., Wang, Y., and Zhang, L. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:21558 21572, 2023. Jiang, H., Chen, Y., Cao, Y., Lee, H.-y., and Tan, R. T. Codejudgebench: Benchmarking llm-as-a-judge for coding tasks. arXiv preprint arXiv:2507.10535, 2025. Liu, J., Xie, S., Wang, J., Wei, Y., Ding, Y., and Zhang, L. Evaluating language models for efficient code generation. arXiv preprint arXiv:2408.06450, 2024b. Jiang, X., Dong, J., Wang, Y., Zheng, Y., Shang, L., Li, G., Jin, Z., and Jiao, L. Self-planning code generation with large language models. ACM Transactions on Software Engineering and Methodology, 33(7), 2024. doi: 10. 1145/3672456. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Niranjan, O., et al. Swe-bench: Can language modarXiv preprint els resolve real-world github issues? arXiv:2310.06770, 2023. Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., and Liu, T.-Y. Lightgbm: highly efficient gradient boosting decision tree. Advances in neural information processing systems, 30, 2017. Khan, M. A. M., Bari, M. S., Do, X. L., Wang, W., Parvez, M. R., and Joty, S. Xcodeeval: An execution-based large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 67666805, 2024. Liu, W., Wang, X., Owens, J., and Li, Y. Energy-based outof-distribution detection. Advances in neural information processing systems, 33:2146421475, 2020. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, S., et al. Self-refine: Iterative refinement with selffeedback. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Locating and editing factual associations in GPT. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Meng, K., Bau, D., Andonian, A., and Belinkov, Y. Massediting memory in transformer. In International Conference on Learning Representations (ICLR), 2023. Michaud, E. J., Liao, I., Lad, V., Liu, Z., Mudide, A., Loughridge, C., Guo, Z. C., Kheirkhah, T. R., Vukelic, M., and Tegmark, M. Opening the ai black box: program synthesis via mechanistic interpretability. arXiv preprint arXiv:2402.05110, 2024. 10 CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs MacDiarmid, M., Freeman, C. D., Sumers, T. R., Rees, E., Batson, J., Jermyn, A., Carter, S., Olah, C., and Henighan, T. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. URL https: //transformer-circuits.pub/2024/ scaling-monosemanticity/index.html. Wang, K., Variengien, A., Conmy, A., Shlegeris, B., and Steinhardt, J. Interpretability in the wild: circuit for indirect object identification in gpt-2 small. arXiv preprint arXiv:2211.00593, 2022. Wang, X., Li, B., and Neubig, G. Opendevin: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024a. Wang, Y., Le, H., Gotmare, A. D., Bui, N. D., Li, J., and Hoi, S. C. Codet5+: Open code large language models for code understanding and generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 10691088, 2023a. Wang, Y., Zhang, P., Yang, B., Wong, D. F., and Wang, R. Latent space chain-of-embedding enables output-free llm self-evaluation. arXiv preprint arXiv:2410.13640, 2024b. Wang, Z., Zhou, S., Fried, D., and Neubig, G. Executionbased evaluation for open-domain code generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 12711290, 2023b. Zhang, Y., Du, W., Jin, D., Fu, J., and Jin, Z. Finite state automata inside transformers with chain-of-thought: mechanistic study on state tracking. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025. Zhao, Z., Koishekenov, Y., Yang, X., Murray, N., and Cancedda, N. Verifying chain-of-thought reasoning via its computational graph. arXiv preprint arXiv:2510.09312, 2025. Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217, 2023. Olah, C., Mordvintsev, A., and Schubert, L. Feature visualization. Distill, 2(11):e7, 2017. Olah, C., Cammarata, N., Schubert, L., Goh, G., Petrov, M., and Carter, S. Zoom in: An introduction to circuits. Distill, 5(3):e00024001, 2020. OpenAI et al. Gpt-4o system card, 2024. URL https: //arxiv.org/abs/2410.21276. Patel, S., Yadavally, A., Dhulipala, H., and Nguyen, T. Planning large language model for static detection of runtime errors in code snippets. In Proceedings of the IEEE/ACM 47th International Conference on Software Engineering (ICSE 2025), pp. 872884, 2025. doi: 10.1109/ICSE55347.2025.00102. Rai, D., Zhou, Y., Feng, S., Saparov, A., and Yao, Z. practical review of mechanistic interpretability for arXiv preprint transformer-based language models. arXiv:2407.02646, 2024a. Rai, D., Zhou, Y., Feng, S., Saparov, A., and Yao, Z. practical review of mechanistic interpretability for arXiv preprint transformer-based language models. arXiv:2407.02646, 2024b. Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Sharkey, L., Chughtai, B., Batson, J., Lindsey, J., Wu, J., Bushnaq, L., Goldowsky-Dill, N., Heimersheim, S., Ortega, A., Bloom, J., et al. Open problems in mechanistic interpretability. arXiv preprint arXiv:2501.16496, 2025. Shih, A., Sadigh, D., and Ermon, S. Long horizon temperature scaling. In International conference on machine learning, pp. 3142231434. PMLR, 2023. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Team, G., Riviere, M., Pathak, S., Sessa, P. G., Hardin, C., Bhupatiraju, S., Hussenot, L., Mesnard, T., Shaber, B., Haber, A., et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Templeton, A., Conerly, T., Marcus, J., Lindsey, J., Bricken, T., Chen, B., Pearce, A., Citro, C., Ameisen, E., Jones, A., Cunningham, H., Turner, N. L., McDougall, C., 11 CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs A. Experimental Settings A.1. Dataset Statistic We report the final statistics of our curated multilingual MBPP incremental datasets in Table 4. Following our evaluation setup in Section 4.1, we use MBPP as the Python benchmark and construct semantically equivalent C++ and Java task sets by translating the same specifications, ensuring that the three datasets share comparable underlying algorithmic logic while differing only in surface syntax. Each JSONL instance corresponds to one labeled generation record with binary correctness label (answer {0, 1}), where 1 denotes correct and 0 denotes wrong. In total, our corpus contains 1,447 Python instances, 3,423 C++ instances, and 3,126 Java instances. The label distribution exhibits noticeable language-dependent skew: Python has the highest correctness ratio (81.6% correct vs. 18.4% incorrect), while C++ and Java are more challenging with substantially higher error rates (C++: 66.8%/33.2%; Java: 61.7%/38.3%). This imbalance motivates us to evaluate CodeCircuit across languages and to study whether attribution-graph structural signatures remain predictive under varying error prevalence and programming syntax. Dataset Total Steps % Correct % Incorrect C++ Python Java 3,423 1,447 3, 66.8% 81.6% 61.7% 33.2% 18.4% 38.3% Table 4. Final statistics of our curated MBPP incremental datasets, showing the number of instances and the distribution of correct/incorrect labels. A.2. Experimental Hyperparameter For attribution-graph extraction, we use the ReplacementModel instantiated from the base LLM, Google Gemma-2-2B-IT1, together with the corresponding GemmaScope transcoder2, and run all tracing in bfloat16 precision on single GPU. We use the Circuit-Tracer (Hanna et al., 2025) library for attribution graph computation. For each input prompt, we construct an attribution graph with the following settings: we attribute at most 10 output logits and target cumulative logit probability mass of 0.95. To control graph size and memory footprint, we cap the number of retained feature nodes to 8,192 and compute attributions in mini-batches of size 64. We disable parameter offloading and verbose debugging output. Following the preprocessing in our pipeline, we skip prompts longer than 550 characters to avoid excessively long contexts during tracing. Each resulting attribution graph is serialized in PyTorch format (.pt), and we additionally append per-instance metadata record in JSONL format, including the task identifier, step index, graph path, and the corresponding binary correctness label. A.3. Detailed Features For each line-level attribution graph, we extract fixed-length feature vector from the pruned subgraph to characterize both node composition and circuit topology. Concretely, we first prune the full attribution graph using an influence-based masking rule with node-threshold of 0.8, which retains only the most influential nodes according to the cumulative influence scores returned by the pruning routine. Let the pruned node set contain (i) transcoder feature nodes, (ii) error nodes (one error node per layer-position pair), as well as (optionally) token and logit nodes if they survive pruning. We then compute three groups of features. (1) High-level statistics. We record (a) the total number of active features in the original graph (the length of the activationvalue vector), (b) the number of pruned feature nodes, and (c) the number of pruned error nodes. To summarize output uncertainty, we additionally compute (d) the top-1 logit probability and (e) the entropy of the traced logit probability vector. These quantities capture the overall circuit scale and the models output confidence for the step. (2) Aggregated node and activation statistics. Using the cumulative influence scores restricted to the pruned nodes, we compute (a) the mean influence over all pruned nodes as global saliency summary. For error nodes, we compute (b) the total error influence (sum over pruned error nodes) and (c) the mean error influence, serving as proxy for how much of the computation is routed through unexplained residual components. For pruned feature nodes, we gather their activation 1https://huggingface.co/google/gemma-2-2b-it 2https://huggingface.co/mntss/gemma-scope-transcoders 12 CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs Gemma-2-2B-IT Code Generation Prompt Template You are code assistant. You must output ONLY valid code. Do not include comments, markdown, explanations, or extra text. Output only code. Task prompt: {TASK_PROMPT} Table 5. Prompt template used for code generation. The final input is the concatenation of the fixed instruction block above and the task prompt, separated by two newline characters. Line-wise Code Correctness Checker Prompt Template You are code correctness checker. Review the provided code line by line. For each line, output score of 1 or 0 according to the following rules: 1. If the line has syntax error, output 0. 2. If the line is syntactically correct but violates the coding requirements described in the task, output 0 for that line and all subsequent lines. Once line violates the requirements, all following lines must also be 0. 3. If the line is syntactically correct and meets the coding requirements so far, output 1. The code has exactly {N_LINES} lines. Output only list of exactly {N_LINES} integers, e.g., [1, 1, 0, 1]. Do not include any explanation or extra text. 4. Focus solely on whether the substantive code contains issues, ignoring parameter variable names, function names, and so on. Task Description: {TASK_DESCRIPTION} Code to Check: {CODE_TO_CHECK} Table 6. Prompt template used for line-wise correctness scoring. {N_LINES} is computed as the number of lines in {CODE_TO_CHECK}; the model is instructed to output list of exactly {N_LINES} binary integers. values and compute (d) mean activation, (e) maximum activation, and (f) activation standard deviation. Finally, to capture where computation concentrates across depth, we construct (g) layer-wise histogram of pruned feature counts: for each transformer layer, we count how many retained feature nodes originate from that layer. This produces length-L vector, where is the number of transformer layers. (3) Topological and edge-based features of the pruned subgraph. We form directed weighted graph from the pruned adjacency matrix, where directed edges represent attributed influence and edge weights are inherited from the adjacency matrix. From edge weights, we compute (a) the sum, (b) mean, and (c) standard deviation of all edge weights, together with (d) the number of edges in the pruned subgraph and (e) the graph density. To capture structural fragmentation, we compute (f) the number of weakly connected components. We then compute centrality-based summaries: (g) mean degree centrality, (h) maximum degree centrality, (i) mean betweenness centrality (computed with edge weights), and (j) maximum betweenness centrality. To quantify global connectivity, we compute (k) the average shortest path length within the largest weakly connected component (set to 1 if undefined). Finally, we add task-relevant routing signal: (l) the shortest-path distance from any retained input-token node to any retained logit node (set to 1 if no path exists), which measures how directly input evidence can flow to the output decision within the pruned circuit. For degenerate cases where the pruned graph has fewer than two nodes, we return default feature vector with zeros and sentinel values (e.g., 1 for undefined shortest-path statistics) to preserve consistent feature dimensionality across all examples. 13 CodeCircuit: Toward Inferring LLM-Generated Code Correctness via Attribution Graphs B. Prompt Template For code generation, we use fixed instruction block that strictly constrains the model to output only valid code; this block is concatenated with the task prompt (two newline characters in between) to form the final input, as shown in Table 5. For correctness checking, we use single user prompt that instructs judge model to perform line-wise validation and return an integer list of length {N_LINES}, where any violation at given line forces that line and all subsequent lines to be scored as 0; we then parse the returned list as per-line correctness signals, as shown in Table 6. C. Limitation The primary limitation of this approach is the computational overhead required to construct attribution graphs. Unlike black-box methods, our framework involves gradient-based backpropagation and feature projections, which increase time and memory complexity relative to standard inference. For extensive code generation tasks, these requirements may limit real-time deployment. Nevertheless, this bottleneck presents significant opportunity for algorithmic optimization. Future research could focus on sparse attribution techniques that target only critical logical junctions or utilize low-rank approximations to reduce the cost of Jacobian computations. By transitioning from exhaustive circuit tracing to selective structural diagnostics, the computational burden can be substantially mitigated, paving the way for efficient, white-box verification in production environments."
        }
    ],
    "affiliations": [
        "IQuest Research",
        "Nanyang Technological University",
        "National University of Singapore",
        "University of Edinburgh",
        "University of Illinois Urbana-Champaign"
    ]
}