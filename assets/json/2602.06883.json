{
    "paper_title": "Vision Transformer Finetuning Benefits from Non-Smooth Components",
    "authors": [
        "Ambroise Odonnat",
        "Laetitia Chapel",
        "Romain Tavenard",
        "Ievgen Redko"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, a high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. A key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering a novel perspective on the functional properties of transformers. The code is available at https://github.com/ambroiseodt/vit-plasticity."
        },
        {
            "title": "Start",
            "content": "Vision Transformer Finetuning Benefits from Non-Smooth Components Ambroise Odonnat 1 2 Laetitia Chapel 3 Romain Tavenard 4 Ievgen Redko 1 6 2 0 F 6 ] . [ 1 3 8 8 6 0 . 2 0 6 2 : r Abstract The smoothness of the transformer architecture has been extensively studied in the context of generalization, training stability, and adversarial robustness. However, its role in transfer learning remains poorly understood. In this paper, we analyze the ability of vision transformer components to adapt their outputs to changes in inputs, or, in other words, their plasticity. Defined as an average rate of change, it captures the sensitivity to input perturbation; in particular, high plasticity implies low smoothness. We demonstrate through theoretical analysis and comprehensive experiments that this perspective provides principled guidance in choosing the components to prioritize during adaptation. key takeaway for practitioners is that the high plasticity of the attention modules and feedforward layers consistently leads to better finetuning performance. Our findings depart from the prevailing assumption that smoothness is desirable, offering novel perspective on the functional properties of transformers. vit-plasticity 1. Introduction Transformers (Vaswani et al., 2017) have become the default backbone of state-of-the-art models in wide range of domains, including natural language processing (Brown et al., 2020; Touvron et al., 2023), computer vision (Caron et al., 2021; Dosovitskiy et al., 2021), time series forecasting (Ilbert et al., 2024; Nie et al., 2023), and mathematical reasoning (Comanici et al., 2025; Guo et al., 2025). These foundation models are typically pretrained on large amounts of diverse data and then adapted to more specific domains (Shukor et al., 2025). In practice, the discrepancy between the training and downstream data can hurt performance (Quionero-Candela et al., 2009) and requires updating the model weights to adapt to the distribution shift. 1Noahs Ark Lab 2Inria 3Univ. Rennes 2, IRISA 4Institut Agro Rennes-Angers, IRISA. Correspondence to: Ambroise Odonnat <ambroise.odonnat@gmail.com>. Preprint. February 9, 2026. 1 Figure 1. Non-smooth components facilitate finetuning. We illustrate the benefits of high plasticity during the finetuning of ViT-Base on Cifar10 (values normalized to [0, 1]). Smooth modules like LayerNorm (top left) have low and steady rates of change, resulting in low plasticity (see Definition 1). This constrains the gradient norms during the optimization, leading to slow descent on the loss landscape (bottom left). In contrast, the rates of change of non-smooth components, such as multihead attention (top right), are large and vary lot, resulting in high plasticity and gradients of high magnitude. This allows the exploration of the loss landscape and faster descent towards (local) minima (bottom right). Finetuning foundation models. The cost of adaptation has drastically increased, with models growing larger and larger as byproduct of the scaling hypothesis (Hoffmann et al., 2022; Kaplan et al., 2020). This has led to considerable research effort toward parameter-efficient finetuning methods (PEFT, Han et al., 2024; Houlsby et al., 2019; Liu et al., 2022). It allows finetuning foundation models at fraction of the cost required for full adaptation and quickly became standard practice in research and industry (Mangrulkar et al., 2022). We focus on the popular family of selective approaches, where only subset of parameters is updated during finetuning (Guo et al., 2019; Lee et al., 2019, 2023). Recent works empirically studied the benefits of adapting one type of transformer component across the whole network: the normalization layers (Zhao et al., 2024), Vision Transformer Benefits from Non-Smooth Components FC2 σ FC1 (z1, . . . , zn) + FFN LN2 + MHA LN (x1, . . . , xn) Figure 2. Overview of our contributions. We conduct comprehensive analysis of vision transformer components (left) through the perspective of their plasticity (Definition 1). Our theoretical analysis allows us to rank modules in terms of their plasticity (Section 4). Experiments on large-scale ViTs support our theoretical insights (Section 5.1), as shown by the distribution of plasticity over all benchmarks (middle). Through large-scale finetuning runs on an 86M-parameter ViT (Section 5.2), we demonstrate the real-world benefits of plasticity. As showcased by the average relative gain, i.e, improvement over the linear probing accuracy, on diverse set of 11 classification benchmarks (right), higher plasticity yields greater finetuning benefits. the attention module (Touvron et al., 2022), or the feedforward layers (Ye et al., 2023). However, little is known from theoretical perspective about the adaptability of those modules1. This motivates us to ask: Which transformer components should be prioritized during finetuning and why? Our approach. We focus on vision transformers (ViT, Dosovitskiy et al., 2021) and aim to reconcile the intrinsic functional properties of the individual components with the empirical performance observed when adapting them. To avoid confounders and since considering all possible combinations of the modules is computationally prohibitive, we conduct systematic component-wise study where each type of module is finetuned in isolation. We build upon the intuition that promoting the smoothness of neural network, e.g., by regularizing its Lipschitz constant (Newhouse et al., 2025), reduces its sensitivity to input perturbations (Rosca et al., 2020). While this is desirable for generalization (Krogh & Hertz, 1991; Neyshabur et al., 2017; Rosca et al., 2020), training stability (Zhai et al., 2023), or adversarial robustness (Miyato et al., 2018), it limits the degree of freedom given component has to adapt its outputs to changes in the inputs, and thus its plasticity. As result, it hinders the adaptation to downstream data during finetuning. This motivates us to quantify the plasticity of transformer components as an average rate of change, where high values would indicate low smoothness (the formal definition shall come in Section 3). 1In what follows, we use the terms module and component interchangeably, always referring to normalization layers, multihead attention modules, and feedforward layers. Our contributions. We provide theoretical ranking of vision transformer components in terms of plasticity, supported by empirical evidence. We demonstrate through comprehensive experiments that high plasticity consistently leads to better finetuning performance. Our main contributions, illustrated in Fig. 2, are: 1. Intuitive measure: We formalize the plasticity of module as its average rate of change, which captures how much given component amplifies or reduces the variations in its input (Section 3). 2. Theoretical analysis: We establish theoretical ranking among transformer components by deriving upper bounds on their plasticity (Section 4). 3. Plasticity ranking: Experiments on large-scale ViTs validate our theoretical insights, showing that the attention module consistently has the highest plasticity, followed by the first and second feedforward layers, the LayerNorm preceding the feedforward, and finally the LayerNorm preceding the attention module (Section 5.1). 4. Finetuning benefits: Exhaustive finetuning runs with an 86M ViT on diverse set of classification benchmarks showcase that adapting modules with high plasticity, namely attention modules and the feedforward layers, results in higher and more stable performance across initialization and learning rates (Section 5.2). Our findings provide novel perspective on the impact of smoothness on the finetuning of vision transformers. We believe that the highlighted link, illustrated in Fig. 1, will help guide the design of more efficient adaptation methods. 2 Vision Transformer Benefits from Non-Smooth Components 2. Background Throughout the paper, we use the notation [n] to represent the set {1, . . . , n}. The Euclidean norm of Rn is denoted by and its ℓ-norm is denoted by . sequence of tokens = (x1, . . . , xn) (Rd)n can be seen as matrix2 in Rdn with Frobenius norm xF = ((cid:80)n xi2)1/2 and spectral norm x2 = σmax(x), with σmax(x) the largest singular value of x. We denote by Br Rd the closed ball centered at 0 with radius > 0. Neural network smoothness. Formally, the smoothness of function is related to the number of continuous derivatives it has on its domain. In deep learning, it can refer to several related concepts, such as differentiability, Lipschitz continuity, or robustness to input perturbations (Rosca et al., 2020). common way to quantify smoothness is through the notion of Lipschitz continuity. function : (Rd)n (Rd)n is said to be Lipschitz continuous if there exists constant 0 such that for any pair of inputs x, (Rd)n, we have (x) (y)F Kx yF. The smallest constant is called the Lipschitz constant of , denoted by Lip(f ), and writes Lip(f ) = supx=y(Rd)n . (x)f (y)F xyF Vision transformers. ViT takes as input 2D images, embedded into sequences of tokens by splitting them into patches of size , which are then flattened and linearly projected in Rd. The architecture consists of succession of transformer encoders. Akin to BERT (Devlin et al., 2019), classification token CLS is prepended to the sequence of tokens to perform classification. transformer encoder is illustrated in Fig. 2 (left), where the LayerNorms are denoted by LN1 and LN2, the attention module is denoted by MHA, and the feedforward linear layers are denoted by FC1 and FC2. After the last layer, the embedding of the CLS token is pooled to perform classification. Implementation details are given in Appendix C.1. Transformer components. We recall below how each module operates on sequence of tokens (Rd)n. LayerNorms: LayerNorm with weights γ, β Rd acts on each input token individually with the formula (cid:18) (x) = γ xj µ(xj) σ(xj) (cid:19) + β 1jn (Rd)n, where is the element-wise product and µ(xj), σ(xj) are the mean and standard deviation of the token xj. Multi-head self-attention: Let such that = is an integer. Let Qh, h, be matrices in Rkd and 2In the PyTorch implementation, all matrices are transposed because the input data is viewed as matrices in Rnd instead of the common Rdn we use. Oh Rdk. multi-head self-attention module with weights (Oh, Qh, h, h)1hH outputs (x) = (cid:88) h="
        },
        {
            "title": "Ohf h",
            "content": "att(x) (Rd)n, where the single-head self-attention Qh, h, and writes att has weights att(x) = (V hx) softmax (cid:33)"
        },
        {
            "title": "K hx",
            "content": "(cid:32) (cid:0)Qhx(cid:1) (Rk)n, with the softmax applied row-wise. Feedforward linear layers: feedforward module with weights W1 Rd4d, W2 R4dd combines two linear layers (cid:55) W1x and (cid:55) W2x with GeLU to output (x) = W2gelu(W1x) (Rd)n. 3. Vision transformer plasticity Regularizing the Lipschitz constant of model is common approach to encourage smoothness (Miyato et al., 2016; Newhouse et al., 2025). While it serves as useful inductive bias in generalization (Bartlett et al., 2017; Sokolic et al., 2017), training stability (Miyato et al., 2018; Zhai et al., 2023), and adversarial robustness (Jia et al., 2024; Tsuzuku et al., 2018), too much smoothness can constrain the models capacity and its adaptability to new tasks, as shown in Rosca et al. (2020). This motivates us to identify the components whose Lipschitz constants might be too small (see Rosca et al., 2020, Section 5), which could impact the adaptation during finetuning. This can be done by analyzing the rates of change of the components since they lower bound the Lipschitz constant via (x)f (y)F Lip(f ). xyF Plasticity measure. Building upon this intuition, we formalize below the plasticity3 of module, i.e., its ability to adapt its output in response to changes in the inputs: Definition 1 (Plasticity). Let ν be the uniform distribution over the set of distinct pairs of sequences of tokens in (Rd)n. We define the plasticity of transformer component as P(f ) = E(x,y)ν (cid:20) (x) (y)F yF (cid:21) . (1) 3Akin to the neuroplasticity of the brain defined as its ability to change its activity in response to intrinsic or extrinsic stimuli\" (Puderbaugh & Emmady, 2023). The loss of plasticity at the network level has been studied in deep reinforcement learning (Lyle et al., 2023) or continual learning (Dohare et al., 2024). 3 Vision Transformer Benefits from Non-Smooth Components Definition 1 ensures that, for any component , we have P(f ) Lip(f ). The Lipschitz constant is worst-case estimation that ensures control over each rate of change. To better capture the overall behavior of transformer components over the distribution of input sequences, we rather compute the average rate of change. This is reminiscent of the notion of average smoothness, defined for functions on metric probabilistic space in Ashlagi et al. (2021). Two regimes of plasticity can be distinguished. If P(f ) < 1, the module contracts the input discrepancy on average. If P(f ) > 1, then amplifies the change in the input on average and pushes the value of Lip(f ) from below. In the rest of this work, we will say that components in the first regime have low plasticity and are smooth, and that the components in the second regime have high plasticity and low smoothness (or are non-smooth, by abuse of language). Connection to finetuning. The plasticity measure introduced in Definition 1 captures the sensitivity of transformer components to input changes. high plasticity implies high Lipschitz constant and thus low smoothness. For given module with weights θ, we have from Federer (1969) that xf Lip(f ). Let be the finetuning loss. During gradient descent, the weights are updated following θ θ ηθL, which involves the gradient of with respect to the parameters via θL = (f L) θ , using the Vector-Jacobian product notation (Béthune et al., 2024; Dagréou et al., 2024). Since our goal is to identify the components that adapt best to downstream data during finetuning, natural question is: what is the connection between the gradient with respect to inputs and the gradient with respect to the parameters? On the theoretical side, Béthune et al. (2024) showed that these notions are two sides of the same coin. More precisely, the authors proved that regularizing the Lipschitz constant with respect to the inputs amounts to bounding the norm of the gradients with respect to the parameters (see Béthune et al., 2024, Theorem 1). As such, too much regularization on the smoothness can impact optimization; conversely, having looser Lipschitz constraints, e.g., thanks to high plasticity, might facilitate the learning process. This has been empirically observed in Newhouse et al. (2025, Section 4.4), where the authors show that reducing the Lipschitz constant negatively impacts the performance of 145M Lipschitz-constrained transformer on FineWeb (Penedo et al., 2024). In particular, matching the NanoGPT baseline (Jordan et al., 2024a) requires Lipschitz constant of up to 10264. Expected benefits of plasticity. The connection between input-output and weight-output smoothness hints at the role of plasticity in the learning process. We expect the components with high plasticity, i.e., the non-smooth ones, to allow large gradient norms during finetuning, thus leading to faster and better adaptation4. We illustrate this process in Fig. 1. This can be understood intuitively, with plastic components carrying more information about the downstream data than the smooth ones. Provided our insights are confirmed through experiments (Fig. 2 offers sneak peek for impatient readers), our perspective would depart from the conventional wisdom that promoting smoothness is beneficial to learning (Miyato et al., 2018; Neyshabur et al., 2017; Rosca et al., 2020; Zhai et al., 2023). 4. Theoretical analysis In this section, we derive upper bounds on the plasticity P(f ). It allows us to compare transformer components in terms of plasticity. The proofs are given in Appendix B. The next proposition states the results for the LayerNorms. Proposition 1 (LayerNorm). Let be LayerNorm with weights γ, β Rd. Assume that all tokens in position [n] have the same mean µi and standard deviation σi on Rd and let σ > 0 be the minimal standard deviation. Then, we have P(f ) 1 σ γ. The requirement on tokens comes from the fact that images are normalized with ImageNet statistics during preprocessing (Kolesnikov et al., 2020) and embedded into sequences of tokens with the same layer. This implies that the µi, σi depend only on the embedding layer. Having σi = 0 for some [n] would force all the tokens in position to be equal, independently of the embedded images. Since this is not the case, neither at initialization nor after pretraining, we must have σ = mini[n] σi > 0. We now proceed to bound the plasticity of the feedforward linear layers. This is reminiscent of the well-known upper bound on the Lipschitz constant of linear operators (Federer, 1969; Virmaux & Scaman, 2018). Proposition 2 (Feedforward layer). Let be feedforward linear layer with weights Rd4d (resp. R4dd). Then, we have P(f ) 2. We now proceed to the upper bound for the multi-head self-attention module. Since self-attention is not globally Lipschitz continuous (Kim et al., 2021), we need to restrict , where Br Rd ourselves to sequences (x1, . . . , xn) in Bn is the closed ball centered in 0 with radius of r. 4Note, however, that we do not expect linear relationship with downstream performance akin to unsupervised accuracy estimation methods (Deng et al., 2023; Garrido et al., 2023; Xie et al., 2024, 2025). 4 Vision Transformer Benefits from Non-Smooth Components Proposition 3 (Multi-head self-attention). Let be multi-head self-attention module with weights (Oh, Qh, h, h)1hH . Let Ah = (Qh)K h/ and > 0. Assume that sequences of tokens are in Bn . Then, we have P(f ) (cid:88) h=1 Oh2V h2 (cid:113) 3n + (12n + 3)r4Ah2 2. The setting of bounded tokens has been studied in Castin et al. (2024) and holds in practice (see Darcet et al., 2024, Fig. 4). This can be understood by the fact that images are normalized during preprocessing before being projected in Rd using layer with bounded weights. As shown in Castin et al. (2024, Proposition 3.4), the bound in Proposition 3 is tight in terms of sequence length n. In ViT, the average token norm is 20 (see Section 5.1) and the sequence length have similar order of is around 200. Hence, and magnitude. It leads to an effective growth rate of r2 in Proposition 3, since Ah2 1 in practice (see Zhai et al., 2023, Fig. 3). Recalling that the total energy of digital image is defined as the sum of its squared pixel intensities, the next corollary allows us to obtain tighter bound with growth rate in n. Proposition 4 (Tighter upper bound). Let be multi-head self-attention module with weights (Oh, Qh, h, h)1hH . Let Ah = (Qh)K h/ and let α be the spectral norm of the embedding layer. Assume that sequences of tokens are obtained from images with total energy bounded by > 0. Then, we have P(f ) (cid:88) h=1 Oh2V h2 (cid:0) + α2EAh (cid:1). The assumption on input images, discussed in detail in Appendix B.4, holds in standard signal processing setting (see, e.g., Goodman, 2005; Mallat, 2008); It allows us to bound the Frobenius norm of sequences of tokens. in Proposition 4, This is key to obtaining the growth rate further improving Proposition 3. Note that the mean-field limit with + (Castin et al., 2024; Geshkovski et al., 2023; Sander et al., 2022) is interesting from mathematical perspective. In particular, it leads to upper bounds independent of the sequence length (Castin et al., 2024; Geshkovski et al., 2023). However, this setting is not suitable for vision transformers where is usually below 103 (Dehghani et al., 2023; Dosovitskiy et al., 2021; Kolesnikov et al., 2020). Theoretical ranking. To compare the modules, we focus on the relative order of their upper bounds. Propositions 1 and 2 imply that the bound over P(f ) is tighter for the 5 normalization than for the linear layers. Indeed, for vector γ Rd and matrix Rdm with entries in similar range, γ is comparable to , which is smaller than the spectral norm of since i, j, Wij (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) i=1 Wij2 = ej 2, with ej Rm has zero entry everywhere except in j-th position, where we used the fact that the spectral norm is the operator norm induced by the Euclidean norm. similar analysis can be done for the multi-head self-attention module: since spectral norms are above 1 in practice (see Zhai et al., 2023, Fig. 3), the sum over the heads of products of spectral norms and the dependency in the sequence length of Propositions 3 and 4 imply looser control over the plasticity of the multi-head self-attention module compared to the LayerNorms and the feedforward. We validate our insight by numerically computing the upper bounds on an 86M pretrained ViT with sequence length = 197 and 12 attention heads; see Appendix D.1 for details. In Fig. 6, we can see the ranking between modules: the multi-head self-attention module has the highest upper bound, followed by the feedforward linear layers, and then the LayerNorms. The conclusion of the theoretical analysis is the following: Takeaway 1. Our analysis suggests the following plasticity ranking: MHA FC1 FC2 LN2 LN1. 5. Experiments In this section, we experimentally show that (a) the plasticity of vision transformer components follows the ranking predicted by our theory (Section 5.1) and (b) components with high plasticity lead to better and more stable finetuning accuracy across initializations and learning rates (Section 5.2). Experimental setup. We conduct most of our experiments using an 86M-parameter ViT with patch size of 16 (ViT-Base). We also experiment with larger model of 632M parameters with smaller patch size of 14 and larger sequence length (ViT-Huge). Models are pretrained on ImageNet-21k (Deng et al., 2009), see Appendix C.1 for details. We perform both the plasticity and finetuning studies on diverse set of 11 commonly used classification benchmarks: Cifar10, Cifar100 (Krizhevsky, 2009); 5 variants from Cifar10-C (Hendrycks & Dietterich, 2019): Contrast, Gaussian Noise, Motion Blur, Snow, Speckle Noise; 2 domains from DomainNet (Peng et al., 2019): Clipart, Sketch; Flowers102 (Nilsback & Zisserman, 2008) and Pets (Parkhi et al., 2012). Images are resized to 224 224 resolution and preprocessed following the protocol of Dosovitskiy et al. (2021), see Appendix C.2 for details. Vision Transformer Benefits from Non-Smooth Components Figure 3. Plasticity analysis on Sketch. The distribution of rates of change (x) (y)F/x yF on ViT-Base (left) follows the theoretical ranking of Section 4. We observe along transformer blocks of ViT-Base (middle) that the attention module has the highest plasticity P(f ), followed by the first and second linear layers of the feedforward. The LayerNorms are the most rigid, with plasticity below 1. The same pattern is obtained on ViT-Huge (right), where the higher attention plasticity further validates our theory (see Proposition 3) since the sequence length is larger than with ViT-Base. 5.1. Plasticity analysis In this section, we compute the plasticity measure introduced in Definition 1 on pretrained vision transformers. To allow for diverse discrepancies yF, the sequences are obtained by embedding 12800 pretraining images from ImageNet (Deng et al., 2009), and the sequences are obtained similarly on various downstream data. Results on Sketch are displayed in Fig. 3, and additional results on all benchmarks are given in Appendix D.1. The full experimental details are provided in Appendix C.4. Empirical ranking. In Fig. 3 (left), we display, for each module of ViT-Base, the distribution of rates of change (x) (y)F/x yF on Sketch. We can see that the ranking established in Section 4 correctly predicts the practical behavior of transformer components. In addition, we observe that the first feedforward layer has larger rates of change than the second. Despite being closer, the LayerNorms also exhibit distinct plasticity, with the LayerNorm preceding the feedforward layer being less rigid than the one preceding the attention module. Our findings are consistent across all benchmarks, as can be seen in the overall distribution of plasticity displayed in Fig. 2 (middle) and in Figs. 7 to 16. It allows us to refine the ordering established in Section 4 into: MHA FC1 FC2 LN2 LN1. In the rest of this work, this ordering will define the plasticity rank of each component. In Fig. 3 (middle), the evolution of the plasticity P(f ) over the layers of ViT-Base is displayed. The x-axis represents the layer depth, denoted as percentage of the overall depth. The ordering previously mentioned is respected. We can also see the two regimes of plasticity mentioned in Section 3: the attention module and the feedforward linear layers have high plasticity with values P(f ) > 1. In contrast, the LayerNorms have low plasticity P(f ) < 1. Following our terminology, this implies that the attention modules and feedforward layers are non-smooth, contrary to the LayerNorms. Remark 5.1. The smoothness and low plasticity of normalization layers can be explained by the fact that, by design, they limit the propagation of perturbations in the input by rescaling the features. This has notably been leveraged in prior works to mitigate the non-stationarity in time series (Kim et al., 2022). As we will see in Section 5.2, this is not desirable property to adapt to downstream data. Impact of the sequence length. We further confirm the theoretical insights of Section 4 by conducting similar plasticity analysis on ViT-Huge, which has longer sequence length = 257. The results are displayed in Fig. 3 (right). We observe similar evolution along the depth, with larger plasticity for the attention module than with ViT-Base. This can be explained by the dependency on the sequence length in the attention upper bound of Proposition 3. Our findings are consistent across all benchmarks. This showcases, as hinted by the upper bounds of Section 4, that plasticity is an intrinsic property of the components and their weights. The conclusion of the plasticity analysis is: Takeaway 2. The empirical plasticity ranking of vision transformer modules supports our theoretical insights with: MHA FC1 FC2 LN2 LN1. Vision Transformer Benefits from Non-Smooth Components Figure 4. Benefits of plasticity on Sketch. Transformer components are ordered in terms of decreasing plasticity. We can see that the performance across learning rates and seeds (left) is better and more stable for plastic components. This can be understood by looking at the evolution of the gradient norms (middle) and the validation loss (right) throughout training: we can see that the higher plasticity, the larger gradient norms, and the better the generalization. 5.2. Benefits of plasticity for finetuning In this section, each transformer component is finetuned in isolation along the depth of ViT-Base, leading to the 5 configurations in Table 3. The optimization is done with SGD following the protocol of Dosovitskiy et al. (2021) summarized in Table 5. We conduct sweep over 4 well-spaced learning rates to ensure fair comparison of modules with different numbers of trainable parameters. Each experiment is done over 3 seeds, leading to total of 800 finetuning runs. The experimental details are given in Appendix C.3. Better performance. The results on all the 11 benchmarks are displayed in Table 6, deferred to Appendix D.2 for space constraints. We observe that the attention modules and feedforward layers with high plasticity lead to enhanced finetuning performances, surpassing the LayerNorms on most benchmarks. The benefit of plasticity is even more salient on challenging datasets such as Cifar100, Clipart, and Sketch, where MHA, FC1, and FC2 surpass LN1 and LN2 by large margin. The last row of Table 6 reports the average top-1 accuracy on all the benchmarks. We can see that the performance ordering is aligned with the plasticity ranking from Section 5.1: the attention modules and feedforward layers result in higher accuracy than the LayerNorms. These results are consistent with Fig. 2 (right), where we display the relative gain, i.e., the percentage improvement of the finetuning accuracy over the linear-probing accuracy. We further notice that the ranking is also respected among components of the same size, namely the attention modules and the feedforward layers on the one hand, and the LayerNorms on the other hand. Our findings consistently showcase the superior performance of the attention modules and the first feedforward linear layer, which have the highest plasticity. In Table 1, we report the performance decrease between MHA and the other configurations. We conclude that, except for FC1, the improvement of finetuning the MHA Table 1. MHA leads to better performance (11 benchmarks). We report the decrease in performance between MHA and the other configurations, averaged over the benchmarks. Entries in bold indicate that the decrease is statistically significant according to Wilcoxon signed-rank test at confidence level of 5%. configuration decrease (%) FC1 0.07 FC2 LN1 LN2 0.44 0.8 0. is statistically significant compared to the other modules. The adaptability of the attention module to downstream data is reminiscent of Touvron et al. (2022, Section 4), where the authors found that tuning the attention module alone can be beneficial for ViTs models of varying sizes, ranging from 6M to 340M parameters; it notably surpasses the full finetuning baseline on small datasets. This showcases the relevance of our plasticity analysis in understanding the behavior of vision transformers. Robust adaptation. The absolute best performance is not the only factor to take into account: being robust to the choices of hyperparameters is also of great importance for practitioners. The main source of variability during finetuning comes from the initialization and the learning rate. In Fig. 4 (left), we report the distribution of top-1 accuracy over the grid of learning rates (see Table 5) and 3 seeds. We can see that the finetuning performance of the components with high plasticity is steadier. In particular, the attention module has the smallest variability. This pattern remains consistent overall, notably for the multi-head selfattention, as can be seen in Fig. 18. Our findings hint at the benefits of plasticity during the optimization process discussed in the next paragraph. Interplay between plasticity and optimization. In Section 1, we argued that, given the interplay between inputoutput smoothness and weight-output smoothness, high 7 Vision Transformer Benefits from Non-Smooth Components plasticity should lead to large gradient norms. In Fig. 4, we display the evolution of the gradient norms (middle) and the validation loss (right) for the finetuning run on Sketch that achieves the highest accuracy (this corresponds to learning rate η = 1e2). This confirms our intuition: the ordering of gradient norms is aligned with the plasticity ranking established in Section 5.1. In tandem, the loss descent is steeper for components with high plasticity, such as the attention modules and the feedforward layers. These patterns are consistent across benchmarks, learning rates, and seeds (see Figs. 19 to 51), which confirms the role of plasticity during the optimization process illustrated in Fig. 1. Our findings are reminiscent of the intuition that ResNet layers with larger gradient magnitudes carry more information about the target data (see Lee et al., 2023, Section 4). The benefits of plasticity on the learning process are in accordance with the empirical evidence from Fig. 2 (right). The conclusion of the finetuning analysis can be summarized as: Takeaway 2. higher plasticity facilitates the optimization and leads to better and more stable finetuning performance. Our findings indicate that the components to prioritize during finetuning should be the attention module and the first feedforward linear layer. 6. Related work Smoothness. Smoothness has been studied extensively in deep learning, e.g., in generalization (Bartlett et al., 2017; Jukic & Šnajder, 2025; Rosca et al., 2020), training stability (Zhai et al., 2023), generative modeling (Miyato et al., 2016; Szegedy et al., 2014), adversarial robustness (Hein & Andriushchenko, 2017; Jia et al., 2024; Tsuzuku et al., 2018; Weng et al., 2018), and differential privacy (Béthune et al., 2024). Common practices in deep learning, such as weight decay (Hanson & Pratt, 1988), dropout (Srivastava et al., 2014), and early stopping (Hardt et al., 2016), encourage smoothness. We extend this discussion in Appendix A. In our work, we identify the components with low smoothness and showcase the benefits of non-smooth\" components for finetuning. However, we do not promote smoothness during the learning process in any way. Lipschitz constant estimation. Estimating the Lipschitz constant of neural networks is hard problem (Virmaux & Scaman, 2018). Theoretical bounds are often loose, except for simple blocks such as linear maps and activation (Béthune et al., 2024). For transformers, the nonlinear nature of self-attention makes the estimation more involved. Notably, Kim et al. (2021) showed that vanilla attention is not globally Lipschitz. Tight upper bounds have been obtained when restricted to sequences of bounded tokens Castin et al. (2024). Imposing Lipschitz constraints is common way to promote smoothness (Newhouse et al., 2025; Rosca et al., 2020). While in our work, we need not estimate Lipschitz constants, the proof techniques to derive the upper bounds in Section 4 are akin to those used to bound the Lipschitz constant of the modules. Parameter-efficient finetuning. There exists plethora of PEFT methods (Zhang et al., 2025). Our work is in line with the selective approaches, common in vision models, where only subset of the parameters is finetuned (Guo et al., 2019; Lee et al., 2019, 2023; Liu et al., 2021; Wang et al., 2021; Xu et al., 2021). Another widely used category consists of additive methods, where small adapters, such as normalization layers, are inserted in the model (Houlsby et al., 2019; Lian et al., 2022; Pfeiffer et al., 2021). The well-known LoRa (Hu et al., 2022) method belongs to the reparameterization methods, where the weights are decomposed and reparameterized to adapt to fewer parameters. While those approaches are performance-oriented and often tune several types of modules together, we conduct component-wise analysis with the aim of theoretically understanding the adaptability of each transformer module. 7. Discussion This paper investigates the plasticity of the vision transformer components by analyzing their average smoothness. Through theory and experiments, we demonstrate the benefits of this approach to identify the transformer components to prioritize during finetuning. In particular, finetuning non-smooth components (with high plasticity), namely the attention modules and the feedforward layers, consistently results in better and more stable performance. Our findings offer novel perspective on the role played by smoothness in finetuning transformers. We hope our findings can help the design of more efficient adaptation methods and contribute to the effort towards better understanding the transformer architecture (see, e.g., Jelassi et al., 2022; Raghu et al., 2021; Von Oswald et al., 2023; Zekri et al., 2025). Limitations and future work. To extend our analysis, promising approach would be to study the effect of tailored optimization (e.g., adaptive learning rates and scheduler) on the performance of each module. In addition, the current analysis is limited to vision transformers, but it could serve as groundwork to explore the adaptability and plasticity of large language models. In particular, transformer decoders mainly differ from transformer encoders via the attention module that becomes causal. As such, the theoretical insights of Section 4 naturally extend to LLMs since Proposition 3 is still verified for masked attention (see Castin et al., 2024, Theorem 4.3). Another promising direction for future work is to combine our findings with reparameterization methods, e.g., by applying LoRa (Hu et al., 2022) only to the components with high plasticity. 8 Vision Transformer Benefits from Non-Smooth Components"
        },
        {
            "title": "Acknowledgement",
            "content": "The authors would like to thank Zehao Xiao, Abdelhakim Benechehab, Vasilii Feofanov, and Albert Thomas for insightful comments about early versions of this work, as well as Théo Moutakanni and Guillaume Carlier for fruitful discussions that led to this project."
        },
        {
            "title": "Impact statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Anil, C., Lucas, J., and Grosse, R. Sorting out Lipschitz function approximation. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 291301. PMLR, 09 15 Jun 2019. URL https://proceedings.mlr.press/ v97/anil19a.html. Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In Precup, D. and Teh, Y. W. (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214223. PMLR, 06 11 Aug 2017. URL https://proceedings.mlr.press/ v70/arjovsky17a.html. Ashlagi, Y., Gottlieb, L.-A., and Kontorovich, A. Functions with average smoothness: structure, algorithms, and learning. In Belkin, M. and Kpotufe, S. (eds.), Proceedings of Thirty Fourth Conference on Learning Theory, volume 134 of Proceedings of Machine Learning Research, pp. 186236. PMLR, 1519 Aug 2021. URL https: //proceedings.mlr.press/v134/ashlagi21a.html. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization, 2016. URL https://arxiv.org/abs/1607.06450. Bartlett, P. For valid generalization the size of the weights is more important than the size of the network. In Mozer, M., Jordan, M., and Petsche, T. (eds.), Advances in Neural Information Processing Systems, volume 9. MIT Press, 1996. URL https: //proceedings.neurips.cc/paper_files/paper/ 1996/file/fb2fcd534b0ff3bbed73cc51df620323Paper.pdf. Bartlett, P. L., Foster, D. J., and Telgarsky, M. J. SpectrallyIn normalized margin bounds for neural networks. Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https: //proceedings.neurips.cc/paper_files/paper/ 2017/file/b22b257ad0519d4500539da3c8bcf4ddPaper.pdf. Béthune, L., Massena, T., Boissin, T., Bellet, A., Mamalet, F., Prudent, Y., Friedrich, C., Serrurier, M., and Vigouroux, D. DP-SGD without clipping: The lipschitz neural network way. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=BEyEziZ4R6. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners, 2020. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. Castin, V., Ablin, P., and Peyré, G. How smooth is attention? In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. Chen, X., Hsieh, C.-J., and Gong, B. When vision transformers outperform resnets without pre-training or In International Conferstrong data augmentations. ence on Learning Representations, 2022. URL https: //openreview.net/forum?id=LtKcMgGOeLt. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A. M., Pillai, T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. URL http://jmlr.org/papers/v24/22-1144.html. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., and et al., O. R. 9 Vision Transformer Benefits from Non-Smooth Components Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/ abs/2507.06261. Dagréou, M., Ablin, P., Vaiter, S., and Moreau, T. How In The Third to compute hessian-vector products? Blogpost Track at ICLR 2024, 2024. URL https: //openreview.net/forum?id=rTgjQtGP3O. Darcet, T., Oquab, M., Mairal, J., and Bojanowski, P. Vision transformers need registers. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=2dnO3LLiJ1. Dasoulas, G., Scaman, K., and Virmaux, A. Lipschitz normalization for self-attention layers with application to graph neural networks. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 24562466. PMLR, 1824 Jul 2021. URL https://proceedings.mlr.press/v139/ dasoulas21a.html. Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A. P., Caron, M., Geirhos, R., Alabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen, M., Arnab, A., Wang, X., Riquelme Ruiz, C., Minderer, M., Puigcerver, J., Evci, U., Kumar, M., Steenkiste, S. V., Elsayed, G. F., Mahendran, A., Yu, F., Oliver, A., Huot, F., Bastings, J., Collier, M., Gritsenko, A. A., Birodkar, V., Vasconcelos, C. N., Tay, Y., Mensink, T., Kolesnikov, A., Pavetic, F., Tran, D., Kipf, T., Lucic, M., Zhai, X., Keysers, D., Harmsen, J. J., and Houlsby, N. Scaling vision transformers to 22 billion parameters. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 74807512. PMLR, 2329 Jul 2023. URL https:// proceedings.mlr.press/v202/dehghani23a.html. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248255. IEEE, 2009. URL https://ieeexplore.ieee.org/ abstract/document/5206848/. Deng, W., Suh, Y., Gould, S., and Zheng, L. Confidence and dispersity speak: Characterizing prediction matrix for unsupervised accuracy estimation. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 76587674. PMLR, 23 29 Jul 2023. URL https://proceedings.mlr.press/ v202/deng23e.html. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lanIn North American Chapter of guage understanding. the Association for Computational Linguistics, 2019. URL https://api.semanticscholar.org/CorpusID: 52967399. Dohare, S., Hernandez-Garcia, J. F., Lan, Q., Rahman, P., Mahmood, A. R., and Sutton, R. S. Loss of plasticity in deep continual learning. Nature, 632(8026):768774, 08 2024. ISSN 1476-4687. doi: 10.1038/s41586-02407711-7. URL https://doi.org/10.1038/s41586024-07711-7. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for In International Conferimage recognition at scale. ence on Learning Representations, 2021. URL https: //openreview.net/forum?id=YicbFdNTTy. Elhage, N., Nanda, N., Olsson, C., Henighan, T., Joseph, N., Mann, B., Askell, A., Bai, Y., Chen, A., Conerly, T., DasSarma, N., Drain, D., Ganguli, D., HatfieldDodds, Z., Hernandez, D., Jones, A., Kernion, J., Lovitt, L., Ndousse, K., Amodei, D., Brown, T., Clark, J., Kaplan, J., McCandlish, S., and Olah, C. mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformercircuits.pub/2021/framework/index.html. Federer, H. Geometric Measure Theory. Classics in Mathematics. Springer Berlin, Heidelberg, 1969. ISBN 9783642620102. Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B. Sharpness-aware minimization for efficiently imIn International Conference proving generalization. on Learning Representations, 2021. URL https:// openreview.net/forum?id=6Tm1mposlrM. Garrido, Q., Balestriero, R., Najman, L., and Lecun, Y. RankMe: Assessing the downstream performance of pretrained self-supervised representations by their rank. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1092910974. PMLR, 2329 Jul 2023. URL https: //proceedings.mlr.press/v202/garrido23a.html. Geshkovski, B., Letrouit, C., Polyanskiy, Y., and Rigollet, P. The emergence of clusters in self-attention dyIn Thirty-seventh Conference on Neural Innamics. formation Processing Systems, 2023. URL https:// openreview.net/forum?id=aMjaEkkXJx. Goodman, J. W. Introduction to Fourier Optics. Roberts and 10 Vision Transformer Benefits from Non-Smooth Components Company Publishers, Englewood, Colorado, 3rd edition, 2005. ISBN 978-0974707723. Gu, Y., Han, X., Liu, Z., and Huang, M. PPT: Pretrained prompt tuning for few-shot learning. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 84108423, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.acl-long.576. URL https://aclanthology.org/ 2022.acl-long.576/. Gu, Y., Wang, X., Wu, J. Z., Shi, Y., Chen, Y., Fan, Z., XIAO, W., Zhao, R., Chang, S., Wu, W., Ge, Y., Shan, Y., and Shou, M. Z. Mix-of-show: Decentralized low-rank adaptation for multi-concept customization of diffusion models. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=NnIaEaBfXD. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., and et al., Z. G. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Guo, Y., Shi, H., Kumar, A., Grauman, K., Rosing, T., and Feris, R. Spottune: Transfer learning through adaptive fine-tuning. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 48004809, 2019. doi: 10.1109/CVPR.2019.00494. Han, Z., Gao, C., Liu, J., Zhang, J., and Zhang, S. Q. Parameter-efficient fine-tuning for large models: comprehensive survey. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https: //openreview.net/forum?id=lIsCS8b6zj. Hanson, S. and Pratt, L. Comparing biases for minimal network construction with back-propagation. In Touretzky, D. (ed.), Advances in Neural Information Processing Systems, volume 1. Morgan-Kaufmann, 1988. URL https: //proceedings.neurips.cc/paper_files/paper/ 1988/file/1c9ac0159c94d8d0cbedc973445af2daPaper.pdf. Hardt, M., Recht, B., and Singer, Y. Train faster, generalize better: Stability of stochastic gradient descent. In Balcan, M. F. and Weinberger, K. Q. (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 12251234, New York, New York, USA, 2022 Jun 2016. PMLR. URL https: //proceedings.mlr.press/v48/hardt16.html. Hein, M. and Andriushchenko, M. Formal guarantees on the robustness of classifier against adversarial manipulation. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https: //proceedings.neurips.cc/paper_files/paper/ 2017/file/e077e1a544eec4f0307cf5c3c721d944Paper.pdf. Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturbations. In International Conference on Learning Representations, 2019. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Hernández-Cano, A., Hägele, A., Huang, A. H., Romanou, A., Solergibert, A.-J., Pasztor, B., Messmer, B., Garbaya, D., ˇDurech, E. F., Hakimi, I., Giraldo, J. G., Ismayilzada, M., Foroutan, N., Moalla, S., Chen, T., Sabolˇcec, V., Xu, Y., Aerni, M., AlKhamissi, B., Marinas, I. A., Amani, M. H., Ansaripour, M., Badanin, I., Benoit, H., Boros, E., Browning, N., Bösch, F., Böther, M., Canova, N., Challier, C., Charmillot, C., Coles, J., Deriu, J., Devos, A., Drescher, L., Dzenhaliou, D., Ehrmann, M., Fan, D., Fan, S., Gao, S., Gila, M., Grandury, M., Hashemi, D., Hoyle, A., Jiang, J., Klein, M., Kucharavy, A., Kucherenko, A., Lübeck, F., Machacek, R., Manitaras, T., Marfurt, A., Matoba, K., Matrenok, S., Mendoncça, H., Mohamed, F. R., Montariol, S., Mouchel, L., Najem-Meyer, S., Ni, J., Oliva, G., Pagliardini, M., Palme, E., Panferov, A., Paoletti, L., Passerini, M., Pavlov, I., Poiroux, A., Ponkshe, K., Ranchin, N., Rando, J., Sauser, M., Saydaliev, J., Sayfiddinov, M. A., Schneider, M., Schuppli, S., Scialanga, M., Semenov, A., Shridhar, K., Singhal, R., Sotnikova, A., Sternfeld, A., Tarun, A. K., Teiletche, P., Vamvas, J., Yao, X., Ilic, H. Z. A., Klimovic, A., Krause, A., Gulcehre, C., Rosenthal, D., Ash, E., Tramèr, F., VandeVondele, J., Veraldi, L., Rajman, M., Schulthess, T., Hoefler, T., Bosselut, A., Jaggi, M., and Schlag, I. Apertus: Democratizing open and compliant llms for global language environments, 2025. Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Vinyals, O., Rae, J., and Sifre, L. An empirical analysis of compute-optimal large language model training. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 30016 30030. Curran Associates, Inc., 2022. URL https: //proceedings.neurips.cc/paper_files/paper/ 2022/file/c1e2faff6f588870935f114ebe04a3e5Paper-Conference.pdf. Horn, R. A. and Johnson, C. R. Matrix Analysis. Cambridge 11 Vision Transformer Benefits from Non-Smooth Components University Press, Cambridge; New York, 2nd edition, 2012. ISBN 978-0521548236. Houliston, S., Odonnat, A., Arnal, C., and Cabannes, V. Provable benefits of in-tool learning for large language models, 2025. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for NLP. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 27902799. PMLR, 0915 Jun 2019. URL https://proceedings.mlr.press/v97/ houlsby19a.html. Transactions of the Association for Computational Linguistics, 13:264280, 2025. doi: 10.1162/tacl_a_00739. URL https://aclanthology.org/2025.tacl-1.13/. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020. URL https://arxiv.org/abs/2001.08361. Kim, H., Papamakarios, G., and Mnih, A. The lipschitz constant of self-attention. In Meila, M. and Zhang, T. (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 55625571. PMLR, 18 24 Jul 2021. URL https://proceedings.mlr.press/ v139/kim21i.html. Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. Kim, T., Kim, J., Tae, Y., Park, C., Choi, J.-H., and Choo, J. Reversible instance normalization for accurate time-series In International forecasting against distribution shift. Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=cGDAkQo1C0p. HuggingFace. Transformers. https://github.com/ huggingface/transformers, 2025. Accessed: 202509-21. Ilbert, R., Odonnat, A., Feofanov, V., Virmaux, A., Paolo, G., Palpanas, T., and Redko, I. SAMformer: Unlocking the potential of transformers in time series forecasting with sharpness-aware minimization and channel-wise attention. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/ forum?id=8kLzL5QBh2. Jelassi, S., Sander, M. E., and Li, Y. Vision transformers provably learn spatial structure. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https:// openreview.net/forum?id=eMW9AkXaREI. Jia, X., Chen, Y., Mao, X., Duan, R., Gu, J., Zhang, R., Xue, H., Liu, Y., and Cao, X. Revisiting and exploring efficient fast adversarial training via law: Lipschitz regularization and auto weight averaging. IEEE Transactions on Information Forensics and Security, 19:81258139, 2024. doi: 10.1109/TIFS.2024.3420128. Jordan, K., Bernstein, J., Rappazzo, B., @fernbear.bsky.social, Vlado, B., Jiacheng, Y., Cesista, F., Koszarsky, B., and @Grad62304977. modded-nanogpt: Speedrunning the nanogpt baseline, 2024a. URL https: //github.com/KellerJordan/modded-nanogpt. Jordan, K., Jin, Y., Boza, V., You, J., Cesista, F., Newhouse, L., and Bernstein, J. Muon: An optimizer for hidden layers in neural networks, 2024b. URL https: //kellerjordan.github.io/posts/muon/. Kimi Team, Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., Chen, Z., Cui, J., Ding, H., Dong, M., Du, A., Du, C., Du, D., Du, Y., Fan, Y., Feng, Y., Fu, K., and et al., B. G. Kimi k2: Open agentic intelligence, 2025. Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. In arXiv preprint arXiv:1412.6980, 2014. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. Big transfer (bit): General visual representation learning. In Computer Vision ECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part V, pp. 491507, Berlin, Heidelberg, 2020. Springer-Verlag. ISBN 9783-030-58557-0. doi: 10.1007/978-3-030-58558-7_29. URL https://doi.org/10.1007/978-3-030-585587_29. Krizhevsky, A. from tiny images. Learning multiple layers of feareport, tures 2009. URL https://www.cs.toronto.edu/kriz/ learning-features-2009-TR.pdf."
        },
        {
            "title": "In Technical",
            "content": "Krogh, A. and Hertz, J. simple weight decay can improve generalization. In Moody, J., Hanson, S., and Lippmann, R. (eds.), Advances in Neural Information Processing Systems, volume 4. Morgan-Kaufmann, 1991. URL https: //proceedings.neurips.cc/paper_files/paper/ 1991/file/8eefcfdf5990e441f0fb6f3fad709e21Paper.pdf. Lee, J., Tang, R., and Lin, J. What would elsa do? freezing layers during transformer fine-tuning, 2019. URL https: //arxiv.org/abs/1911.03090. Jukic, J. and Šnajder, J. From robustness to improved generalization and calibration in pre-trained language models. Lee, Y., Chen, A. S., Tajwar, F., Kumar, A., Yao, H., Liang, P., and Finn, C. Surgical fine-tuning improves adapta12 Vision Transformer Benefits from Non-Smooth Components tion to distribution shifts. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=APuPRxjHvZ. Luxburg, U. v. and Bousquet, O. Distancebased classification with lipschitz functions. J. Mach. Learn. Res., 5: 669695, December 2004. ISSN 1532-4435. Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W.-t., Rocktäschel, T., Riedel, S., and Kiela, D. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Li, X. L. and Liang, P. Prefix-tuning: Optimizing continuous prompts for generation. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 45824597, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/ 2021.acl-long.353. URL https://aclanthology.org/ 2021.acl-long.353/. Lian, D., Zhou, D., Feng, J., and Wang, X. Scaling &amp; shifting your features: new baseline for efficient model tuning. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 109123. Curran Associates, Inc., 2022. URL https: //proceedings.neurips.cc/paper_files/paper/ 2022/file/00bb4e415ef117f2dee2fc3b778d806dPaper-Conference.pdf. Liu, H., Tam, D., Mohammed, M., Mohta, J., Huang, T., Bansal, M., and Raffel, C. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=rBCvMG-JsPd. Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J. Gpt understands, too, 2023. URL https: //arxiv.org/abs/2103.10385. Liu, Y., Agarwal, S., and Venkataraman, S. Autofreeze: Automatically freezing model blocks to accelerate fine-tuning, 2021. URL https://arxiv.org/abs/ 2102.01386. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/ forum?id=Bkg6RiCqY7. Loshchilov, I., Hsieh, C.-P., Sun, S., and Ginsburg, B. nGPT: Normalized transformer with representation learning on the hypersphere. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=se4vjm7h4E. Lyle, C., Zheng, Z., Nikishin, E., Avila Pires, B., Pascanu, R., and Dabney, W. Understanding plasticity in neural networks. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 2319023211. PMLR, 2329 Jul 2023. URL https: //proceedings.mlr.press/v202/lyle23b.html. Mallat, S. Wavelet Tour of Signal Processing, Third Edition: The Sparse Way. Academic Press, Inc., USA, 3rd edition, 2008. ISBN 0123743702. Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, S., Bossan, B., and Tietz, M. PEFT: State-of-theart parameter-efficient fine-tuning methods. https:// github.com/huggingface/peft, 2022. Mao, Y., Mathias, L., Hou, R., Almahairi, A., Ma, H., Han, J., Yih, S., and Khabsa, M. UniPELT: unified framework for parameter-efficient language model tuning. In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 62536264, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.433. URL https://aclanthology.org/2022.acl-long.433/. Marin. Marin: developing foundation models. https: //marin.community/, 2025. Accessed: 2025-09-30. Miyato, T., ichi Maeda, S., Koyama, M., Nakae, K., and Ishii, S. Distributional smoothing with virtual adversarial training, 2016. URL https://arxiv.org/abs/ 1507.00677. Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spectral normalization for generative adversarial networks. In International Conference on Learning Representations, 2018. URL https://openreview.net/ forum?id=B1QRgziT-. Nair, P. Softmax is $1/2$-lipschitz: tight bound across all $ell_p$ norms. Transactions on Machine Learning Research, 2026. ISSN 2835-8856. URL https: //openreview.net/forum?id=6dowaHsa6D. Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. Progress measures for grokking via mechaIn The Eleventh International nistic interpretability. Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=9XFSbDPmdW. Newhouse, L., Hess, R. P., Cesista, F., Zahorodnii, A., Bernstein, J., and Isola, P. Training transformers with enforced 13 Vision Transformer Benefits from Non-Smooth Components lipschitz constants, 2025. URL https://arxiv.org/ abs/2507.13338. In Proceedings of the IEEE International Conference on Computer Vision, pp. 14061415, 2019. Neyshabur, B., Bhojanapalli, S., Mcallester, D., and Srebro, N. Exploring generalization in deep learning. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https: //proceedings.neurips.cc/paper_files/paper/ 2017/file/10ce03a1ed01077e3e289f3e53c72813Paper.pdf. Nie, Y., Nguyen, N. H., Sinthong, P., and Kalagnanam, J. time series is worth 64 words: Long-term forecastIn The Eleventh International ing with transformers. Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=Jbdc0vTOcol. Nilsback, M.-E. and Zisserman, A. Automated flower classification over large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pp. 722729, 2008. URL https: //api.semanticscholar.org/CorpusID:15193013. Novak, R., Bahri, Y., Abolafia, D. A., Pennington, J., and Sohl-Dickstein, J. Sensitivity and generalization in neural networks: an empirical study. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=HJC2SzZCW. Odonnat, A., Bouaziz, W., and Cabannes, V. Clustering head: visual case study of the training dynamics in transformers, 2025a. URL https://arxiv.org/abs/ 2410.24050. Odonnat, A., Bouaziz, W., and Cabannes, V. Easing optimization paths: circuit perspective, 2025b. URL https://arxiv.org/abs/2501.02362. Park, N. and Kim, S. How do vision transformers work? In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=D78Go4hVcxO. Parkhi, O. M., Vedaldi, A., Zisserman, A., and Jawahar, C. V. Cats and dogs. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 34983505, 2012. URL https://api.semanticscholar.org/CorpusID: 279027499. Penedo, G., Kydlíˇcek, H., allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., Werra, L. V., and Wolf, T. The fineweb datasets: Decanting the web for the finest text data In The Thirty-eight Conference on Neural at scale. Information Processing Systems Datasets and Benchmarks Track, 2024. URL https://openreview.net/ forum?id=n6SCkn2QaG. Pfeiffer, J., Kamath, A., Rücklé, A., Cho, K., and Gurevych, I. AdapterFusion: Non-destructive task composition for transfer learning. In Merlo, P., Tiedemann, J., and Tsarfaty, R. (eds.), Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pp. 487503, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.39. URL https://aclanthology.org/2021.eacl-main.39/. Puderbaugh, M. and Emmady, P. D. Neuroplasticity. In StatPearls. StatPearls Publishing, Treasure Island (FL), May 2023. URL https://www.ncbi.nlm.nih.gov/books/ NBK557811/. Updated May 1, 2023. Qu, C., Dai, S., Wei, X., Cai, H., Wang, S., Yin, D., Xu, J., and Wen, J.-R. Tool learning with large language models: survey. Frontiers of Computer Science, 19(8):198343, 2025. Quionero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, N. D. Dataset Shift in Machine Learning. The MIT Press, 2009. ISBN 0262170051. Raghu, M., Unterthiner, T., Kornblith, S., Zhang, C., and Dosovitskiy, A. Do vision transformers see like convolutional neural networks? In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=R-616EWWKF5. Rosca, M., Weber, T., Gretton, A., and Mohamed, S. case for new neural network smoothness constraints. In Zosa Forde, J., Ruiz, F., Pradier, M. F., and Schein, A. (eds.), Proceedings on \"I Cant Believe Its Not Better!\" at NeurIPS Workshops, volume 137 of Proceedings of Machine Learning Research, pp. 2132. PMLR, 12 Dec 2020. URL https://proceedings.mlr.press/v137/ rosca20a.html. Sander, M. E., Ablin, P., Blondel, M., and Peyré, G. Sinkformers: Transformers with doubly stochastic attention. In Camps-Valls, G., Ruiz, F. J. R., and Valera, I. (eds.), Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pp. 35153530. PMLR, 2830 Mar 2022. URL https: //proceedings.mlr.press/v151/sander22a.html. Schick, T., Dwivedi-Yu, J., Dessi, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=Yacmpz84TH. Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., and Wang, B. Moment matching for multi-source domain adaptation. Serrurier, M., Mamalet, F., González-Sanz, A., Boissin, T., Loubes, J.-M., and del Barrio, E. Achieving robustness 14 Vision Transformer Benefits from Non-Smooth Components in classification using optimal transport with hinge regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 505514, 2021. doi: 10.1109/CVPR46437.2021.00057. Serrurier, M., Mamalet, F., FEL, T., Béthune, L., and Boissin, T. On the explainable properties of 1lipschitz neural networks: An optimal transport perIn Thirty-seventh Conference on Neural Inspective. formation Processing Systems, 2023. URL https:// openreview.net/forum?id=ByDy2mlkig. Shukor, M., Bethune, L., Busbridge, D., Grangier, D., Fini, E., El-Nouby, A., and Ablin, P. Scaling laws for optimal data mixtures, 2025. URL https://arxiv.org/abs/ 2507.09404. Sokolic, J., Giryes, R., Sapiro, G., and Rodrigues, M. R. D. Robust large margin deep neural networks. IEEE Transactions on Signal Processing, 65(16):42654280, 2017. doi: 10.1109/TSP.2017.2708039. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):19291958, 2014. URL http: //jmlr.org/papers/v15/srivastava14a.html. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I., and Fergus, R. Intriguing properties of neural networks, 2014. URL https://arxiv.org/ abs/1312.6199. Thor, W. M. for How to calculate gpu vram requirehttps: an large-language model. ments //apxml.com/posts/how-to-calculate-vramrequirements-for-an-llm, 2025. Accessed: 202509-21. Touvron, H., Cord, M., El-Nouby, A., Verbeek, J., and Jégou, H. Three things everyone should know about vision transformers. In Computer Vision ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXIV, pp. 497515, Berlin, Heidelberg, 2022. Springer-Verlag. ISBN 978-3-031-200526. doi: 10.1007/978-3-031-20053-3_29. URL https: //doi.org/10.1007/978-3-031-20053-3_29. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and efficient foundation language models, 2023. URL https://arxiv.org/abs/ 2302.13971. Tsuzuku, Y., Sato, I., and Sugiyama, M. Lipschitz-margin training: scalable certification of perturbation invariance for deep neural networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS18, pp. 65426551, Red Hook, NY, USA, 2018. Curran Associates Inc. 15 Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https: //proceedings.neurips.cc/paper_files/paper/ 2017/file/3f5ee243547dee91fbd053c1c4a845aaPaper.pdf. Virmaux, A. and Scaman, K. Lipschitz regularity of deep neural networks: analysis and efficient estimation. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. URL https: //proceedings.neurips.cc/paper_files/paper/ 2018/file/d54e99a6c03704e95e6965532dec148bPaper.pdf. Von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M. Transformers learn in-context by gradient descent. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 3515135174. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/ von-oswald23a.html. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. Tent: Fully test-time adaptation by entropy minimization. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=uXl3bZLkr3c. Weng, T.-W., Zhang, H., Chen, P.-Y., Yi, J., Su, D., Gao, Y., Hsieh, C.-J., and Daniel, L. Evaluating the robustness of neural networks: An extreme value theory approach. In International Conference on Learning Representations, 2018. URL https://openreview.net/ forum?id=BkUHlMZ0b. Wortsman, M., Liu, P. J., Xiao, L., Everett, K. E., Alemi, A. A., Adlam, B., Co-Reyes, J. D., Gur, I., Kumar, A., Novak, R., Pennington, J., Sohl-Dickstein, J., Xu, K., Lee, J., Gilmer, J., and Kornblith, S. Small-scale proxies for large-scale transformer training instabilities. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=d8w0pmvXbZ. Xie, R., Odonnat, A., Feofanov, V., Deng, W., Zhang, J., and An, B. Mano: Exploiting matrix norm for unsupervised accuracy estimation under distribution shifts. In The Thirty-eighth Annual Conference on Neural InVision Transformer Benefits from Non-Smooth Components formation Processing Systems, 2024. URL https:// openreview.net/forum?id=mH1xtt2bJE. Xie, R., Odonnat, A., Feofanov, V., Redko, I., Zhang, J., and An, B. Leveraging gradients for unsupervised accuracy estimation under distribution shift. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. URL https://openreview.net/forum?id=FIWHRSuoos. Xu, R., Luo, F., Zhang, Z., Tan, C., Chang, B., Huang, S., and Huang, F. Raise child in large language model: Towards effective and generalizable fine-tuning, 2021. URL https://arxiv.org/abs/2109.05687. Ye, P., Huang, Y., Tu, C., Li, M., Chen, T., He, T., and Ouyang, W. Partial fine-tuning: successor to full finetuning for vision transformers, 2023. Zekri, O., Odonnat, A., Benechehab, A., Bleistein, L., Boullé, N., and Redko, I. Large language models as markov chains, 2025. URL https://arxiv.org/abs/ 2410.02724. Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 4077040803. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/ zhai23a.html. Zhang, D., Feng, T., Xue, L., Wang, Y., and Tang, J. Parameter-efficient fine-tuning for foundation models. arXiv preprint arXiv:2501.13787, 2025. Zhao, B., Tu, H., Wei, C., Mei, J., and Xie, C. Tuning layernorm in attention: Towards efficient multiIn The Twelfth International modal LLM finetuning. Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=YR3ETaElNK. Zi, B., Qi, X., Wang, L., Wang, J., Wong, K.-F., and Zhang, L. Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices, 2023. URL https:// arxiv.org/abs/2309.02411. 16 Vision Transformer Benefits from Non-Smooth Components"
        },
        {
            "title": "Appendix",
            "content": "Roadmap. In this appendix, we discuss additional related work in Appendix A. We detail the proofs of our theoretical results in Appendix B. We provide the full implementation details in Appendix and additional experiments in Appendix D. We display the corresponding table of contents below."
        },
        {
            "title": "Table of Contents",
            "content": "A Extended related work Proofs of Section 4 B.1 Proof of Proposition 1 . B.2 Proof of Proposition 2 . B.3 Proof of Proposition 3 . B.4 Proof of Proposition 4 . Implementation details C.1 Vision transformers . C.2 Data preprocessing . C.3 Finetuning setup . C.4 Plasticity setup . . . . . Additional experiments D.1 Plasticity analysis . . D.2 Finetuning analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 19 20 21 22 25 25 25 25 27 27 34 17 Vision Transformer Benefits from Non-Smooth Components A. Extended related work In this section, we extend the discussion of prior works related to our paper. Smoothness in neural networks. Neural networks smoothness, typically quantified via Lipschitz constants and spectral norms, has been studied in the context of in-domain generalization (Bartlett et al., 2017; Jukic & Šnajder, 2025; Luxburg & Bousquet, 2004; Neyshabur et al., 2017; Novak et al., 2018; Rosca et al., 2020; Sokolic et al., 2017), training stability (Miyato et al., 2018; Zhai et al., 2023), generative modeling (Miyato et al., 2016; Szegedy et al., 2014), adversarial robustness (Anil et al., 2019; Hein & Andriushchenko, 2017; Jia et al., 2024; Rosca et al., 2020; Tsuzuku et al., 2018; Weng et al., 2018) and differential privacy (Béthune et al., 2024). Neyshabur et al. (2017) discusses the interplay between complexity measures based on norms, margin control, Lipschitz constants, and sharpness. These works discuss the benefits of promoting smoothness by regularizing Lipschitz constants (Rosca et al., 2020) or spectral norms (Zhai et al., 2023) during training. Common practices in deep learning such as weight decay (Bartlett, 1996; Hanson & Pratt, 1988; Krogh & Hertz, 1991), dropout (Srivastava et al., 2014) and early stopping (Hardt et al., 2016) encourage neural networks smoothness. Smoothness at scale. Recently, smoothness has been studied in the context of stabilizing large models, such as LLMs (Zhai et al., 2023). During training at such large scale, many instabilities appear and notable loss spikes (Chowdhery et al., 2023; Hernández-Cano et al., 2025; Marin, 2025). They can cause the model to diverge and have been the subject of many studies. Inspired by mechanistic interpretability (Elhage et al., 2021), companion work proposed to study the gradient descent of the transformer (Odonnat et al., 2025a,b) on the sparse modular addition problem (Nanda et al., 2023). It provides simple yet sufficient testbed to observe involved optimization dynamics at small scale. In more realistic settings, methods to control the gradient norms have been proposed, such as QK-Norm (Dehghani et al., 2023; Wortsman et al., 2024), or constraining the representation space on the hypersphere (Loshchilov et al., 2025), and are used to train industry-level LLMs. These approaches are reminiscent of generalization bounds based on margins and spectral norms (Bartlett et al., 2017; Neyshabur et al., 2017). Recently, the Muon optimizer (Jordan et al., 2024b), that normalize the spectral norms of the weights, has shown tremendous benefits in solving training instabilities. In addition, Newhouse et al. (2025) that Muon allows for optimizing Lipschitz-constrained neural networks at scale. The combination of these approaches can also be beneficial: the Kimi team managed to train on over 1T tokens without any loss spikes thanks to MuonClip (Kimi Team et al., 2025) that combines Muon with QK-Norm. Lipschitz constant estimation. lot of effort has been put into estimating the Lipschitz constants of neural networks. While linear and activation layers have known tight Lipschitz constant (Béthune et al., 2024; Castin et al., 2024; Virmaux & Scaman, 2018), estimating the Lipschitz constant of feedforward networks is NP-hard (Virmaux & Scaman, 2018) with loose theoretical upper bounds (Virmaux & Scaman, 2018). The non-linear nature of the self-attention module makes the estimation of its Lipschitz constant more involved. Kim et al. (2021) showed that the vanilla attention is not globally Lipschitz, while the dot-product attention is. Tight upper bounds on the attention module restricted to sequences of bounded tokens have been provided in Castin et al. (2024). Dasoulas et al. (2021) showed the benefits of enforcing Lipschitz continuity in self-attention for graph neural networks. Imposing Lipschitz constraints on neural networks has also been done in generative modeling (Arjovsky et al., 2017) or to ensure robustness and explainability, e.g., with 1-Lipschitz neural networks (Serrurier et al., 2021, 2023). Parameter-efficient finetuning. PEFT methods can be categorized into 5 main families (Zhang et al., 2025). Selective methods, common in vision models, aim to finetune only subset of the parameters (Guo et al., 2019; Lee et al., 2019, 2023; Liu et al., 2021; Wang et al., 2021; Xu et al., 2021). Additive methods insert small adapter networks between the models layers to be trained during finetuning (Houlsby et al., 2019; Lian et al., 2022; Pfeiffer et al., 2021). Prompt methods, commonly used for large language models (Gu et al., 2022; Li & Liang, 2021; Liu et al., 2023), involve learning soft commands to guide the model. Reparameterization methods, such as LoRa (Hu et al., 2022) and its companions (Gu et al., 2023; Zi et al., 2023), decompose and reparameterize the models weights to adapt fewer parameters. These methods can be combined, leading to the last family of hybrid approaches (Mao et al., 2022). We note that the recent shift of large language models from static predictors towards dynamic, context-aware agents redefines finetuning methods. The benefits of learning to use tools instead of incorporating the knowledge in the model weights have been demonstrated in (Houliston et al., 2025). This explain the superiority and scalability of approaches such as Toolformer (Schick et al., 2023), Retrieval-Augmented Generation (RAG, Lewis et al., 2020) and plethora of other approaches (Qu et al., 2025). Vision Transformer Benefits from Non-Smooth Components B. Proofs of Section 4 In this section, we detail the proofs of our theoretical results, which involve simple manipulations of matrix norms. Notations. Throughout the paper, we use the notation [n] to represent the set {1, . . . , n}. The Euclidean norm of Rn is denoted by and its ℓ norm is denoted by . Entries of matrix Rnm write Aij, its rows write Ai and its columns write A,j. The Frobenius norm of matrix Rnm writes AF = and its spectral norm writes A2 = σmax(A), defined as the largest singular value of A. We denote by Br Rd the closed ball centered at 0 with radius > 0. j=1 A2 ij (cid:16)(cid:80)n (cid:17)1/2 (cid:80)m i=1 Useful properties. The next lemma recalls some well-known properties of the Frobenius norm and its connection with the spectral norms (see Horn & Johnson, 2012, p.364, Section 5.6.P20), which will be used in our proofs. Lemma 1. For any matrices Rnm and Rmp, we have ABF AFBF ABF A2BF ABF AFB2, where the first property is referred to as the submultiplicativity of the Frobenius norm. Proof. Let = AB. The entries of writes Cij = (cid:80)m k=1 AikBkj = B,j. Applying Cauchy-Schwartz leads to: Cij2 = B,j2 Ai2B,j2. Hence, the Frobenius norm of AB verifies AB2 = C2 = = = = (cid:88) (cid:88) i=1 (cid:88) j=1 (cid:88) i=1 j=1 Cij Ai2B,j2 (cid:88) Ai2 B,j2 (cid:88) i= (cid:88) (cid:88) i=1 k=1 (cid:88) (cid:88) j=1 (cid:88) A2 ik (cid:88) B2 lj j=1 (cid:88) l=1 (cid:88) B2 lj A2 ik i=1 = A2 k=1 FB2 F. l= j=1 Taking the square root concludes the proof by monotonicity. For the second result, using the same notations, we recall that the columns of write C,j = A(B,j). 19 Vision Transformer Benefits from Non-Smooth Components Recalling that the spectral norm 2 is the operator norm induced by on Rn, it leads to ABF = CF = (cid:88) (cid:88) 2 ij j=1 C,j A(B, j)2 A2 2(B, j)2 i=1 (cid:88) j=1 (cid:88) j=1 (cid:88) j=1 = = = A2 2 = A2 = A2 2 (cid:88) j=1 (cid:88) B,j2 (cid:88) B2 lj j=1 (cid:88) l=1 (cid:88) B2 lj = A2 j=1 l=1 2B2 F. (operator norm property of 2) Taking the square root concludes the proof by monotonicity. For the last result, we use the previous one by using the fact that the Frobenius norm is the sum of singular values, which are invariant by transposition, and that the spectral norm is the maximal singular value. This leads to the Frobenius and the spectral norm to remain invariant under transposition. As such, we have ABF = (AB)F = BAF B2AF = AFB2, which concludes the proof. B.1. Proof of Proposition 1 Proof. We start by upper-bounding the plasticity of LayerNorms. Let be LayerNorm with weights γ, β Rd and let ν be the uniform distribution over the set of distinct pairs of sequences of tokens in (Rd)n. Let (x, y) be pair of two distinct sequences of tokens sampled according to ν. By assumption, we have for any [n] that 1 n, µ(xi) = µ(yi) = µi and σ(xi) = σ(yi) = σi > 0, with µ(xi), σ(xi) (respectively µ(yi), σ(yi)) the mean and standard deviation of the token xi Rd (respectively of yi). From the definition of LayerNorm (see Section 2), it leads to (x) (y) = γ (cid:18) x1 µ(x1) σ(x1) + β, . . . , γ xn µ(xn) σ(xn) (cid:19) + β (cid:19) + β (cid:18) γ y1 µ(y1) σ(y1) + β, . . . , γ (cid:18) = γ x1 µ1 σ1 + β, . . . , γ xn µn σn yn µ(yn) σ(yn) (cid:19) + β (cid:18) γ y1 µ1 σ1 + β, . . . , γ yn µn σn (cid:19) + β (cid:18) = γ x1 y1 σ1 , . . . , γ xn yn σn (cid:19) . 20 Vision Transformer Benefits from Non-Smooth Components Denoting x, respectively y, the sequence with entries (cid:16) xi σi (cid:17)n i=1 , respectively (cid:16) yi σi (cid:17)n i=1 , we have (x) (y)F = γ (x y)F = Γ(x y)F Γ2x yF, where the second line comes from defining Γ Rdd as diagonal matrix with values the entries of γ Rd and replacing the element-wise product by matrix product, and the last line comes from using Lemma 1. We recall that y2 = (cid:88) i=1 (xi yi)/σi2 (cid:18) 1 minn i=1 σi (cid:19)2 (cid:88) i=1 (xi yi)2 = 1 σ2 y2 F, where σ = minn matrix. We then obtain i=1 σi > 0, and that Γ2 = maxd i=1 γi = γ by the definition of the spectral norm of diagonal (x) (y)F 1 σ γx yF. Since the result holds for randomly sampled sequences x, y, and by assumption, the σi depend on the embedding layer and not on the input sequences, we can upper bound the rate of change and take the expectation over distinct sequences of tokens x, y. We have P(f ) = E(x,y)µ which concludes the proof for the LayerNorms. B.2. Proof of Proposition 2 (cid:20) (x) (y)F yF (cid:21) 1 σ γ, Proof. The proof derivation is simple for the case of linear layers. We detail it below for consistency. Let be linear layer with weights W1 Rd4d and let ν be the uniform distribution over the set of distinct pairs of sequences of tokens in (Rd)n. Let (x, y) be pair of two distinct sequences of tokens sampled according to ν. Let x, Rnd be two distinct sequences of tokens sampled according to µ.. By definition of the linear layer, simple application of Lemma 1 leads to (x) (y)F = W1(x y)F W12x yF. We obtain similar result for the second linear layer with weights in R4dd. As in Appendix B.1, since the upper bound holds for randomly sampled sequences x, y, we can bound the rate of change and take the expectation to conclude the proof. B.3. Proof of Proposition 3 Proof. Let be multihead self-attention module with weights (Oh, Qh, h, h)1hH , with Ah = (Qh)K h/ k, and let ν be the uniform distribution over the set of distinct pairs of sequences of tokens in (Rd)n. Let (x, y) be pair of two distinct sequences of tokens sampled according to ν. Let x, Rnd be two distinct sequences of tokens sampled according to µ. By the definition of multihead self-attention (see Section 2), we have (x) (y)F = (cid:88) h=1 Oh(cid:0)f att(x) att(x)(cid:1)F (cid:88) h=1 (cid:88) h=1 Oh(cid:0)f att(x) att(x)(cid:1)F Oh2f att(x) att(x)F. We recall that following Castin et al. (2024), the Lipschitz constant of on (cid:0)Rd(cid:1)n writes Lip(cid:0)fX (cid:1) = sup x,yX x=y (x) (y)F yF . 21 (triangular inequality) (Lemma 1) (2) Vision Transformer Benefits from Non-Smooth Components We recall the following result on the Lipschitz constant of self-attention. Theorem B.1. (Castin et al., 2024, Theorem 3.3) Let Q, K, Rkd and = QK/ self-attention module fatt with weights Q, K, is Lipschitz continuous on Bn , with k. Let > 0 and N. Lip(fattBn ) 3V 2 (cid:113) 2r4(4n + 1) + n. By assumption, the sequences of tokens are restricted to Bn . We can thus apply Theorem B.1 on each self-attention module att with weights Qh, h, h. Using the fact that the Lipschitz constant in Eq. (2) is supremum over individual rates of change, we have (x) (y)F (cid:88) Oh2 Lip(cid:0)f attBn (cid:1)x yF h=1 (cid:32) (cid:88) h=1 Oh 3V h2 (cid:113) Ah2 2r4(4n + 1) + yF. (cid:33) As in Appendix B.1, since the upper bound holds for randomly sampled sequences x, y, we can bound the rate of change and take the expectation to conclude the proof. B.4. Proof of Proposition 4 Proof. Let be multihead self-attention module with weights (Oh, Qh, h, h)1hH , with Ah = (Qh)K h/ k, and let ν be the uniform distribution over the set of distinct pairs of sequences of tokens in (Rd)n. Let (x, y) be pair of two distinct sequences of tokens sampled according to ν. We first show that the assumption on the energy of images leads to sequences of tokens with bounded Frobenius norm. The digital image embedded to obtain the sequence of tokens can be seen as discretization of its continuous intensity, denoted by : Ω R2 R+ (for convenience, we consider grayscale image, but similar derivations are straightforward for an RGB image). The total energy of the image is defined as the sum of the squared intensities over pixels (see Mallat, 2008, Chapter 1, page 2). In line with the signal processing literature (Goodman, 2005; Mallat, 2008), the image has finite energy Ex 0, which is bounded by by assumption. Summing over pixels, we have (cid:88) Ex = I(u, v)2 < . In vision transformers, images are split into square patches of size . The i-th patch, denoted by pi RP , covers an area Ωi and has an energy Ei 0. We have: u,vΩ (cid:88) = I(u, v)2 = (cid:88) (cid:88) I(u, v)2 = (cid:88) Ei. u,vΩ i=1 Denoting by = (x1, . . . , xn) (Rd)n the sequence of tokens obtained after embedding the image, where the i-th token xi is obtained by flattening the i-th patch pi and linearly projecting it in Rd. Since the input images have dimensions (see Dosovitskiy et al., 2021, Section 3.1), the flattened patches have dimension of = 2 C. We denote by Rdm the weights of the embedding layer. Using the property of the spectral norm 2 (which is the operator norm induced by the Euclidean norm ), we have u,vΩi i=1 where vec() denotes the operator that transforms matrix into column vector. By definition, we have xi = Evec(pi) E2vec(pi), vec(pi)2 = pi = (cid:88) u,vΩi I(u, v)2dudv = Ei. As such, the Frobenius norm of the sequence of tokens verifies (cid:118) (cid:117) (cid:117) (cid:116) E22vec(pi)2 = xi2 xF = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) (cid:88) (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) i=1 i=1 i=1 22 vec(pi)2 = (cid:118) (cid:117) (cid:117) (cid:116) (cid:88) i=1 Ei = E2 E, (3) Vision Transformer Benefits from Non-Smooth Components E, with α the spectral norm of E. This implies that the sequences as intended. In the rest of the proof, we denote = α of tokens are in Bn R, which corresponds to the setting of Proposition 3. Indeed, each token verifies xi R; otherwise, the Frobenius norm would be greater than R. We now proceed to bound (x) (y)F. For any [H], we introduce for convenience the function Sh : Rdn Rnn as Sh(x) = softmax (cid:32) (cid:33) (Qx)Kx = softmax(cid:0)xAhx(cid:1). Similarly to the proof of Proposition 3, we have used the triangular inequality and Lemma 1 that (x) (y)F = (cid:88) h=1 Oh(cid:0)f att(x) att(x)(cid:1)F (cid:88) h=1 (cid:88) h=1 Oh(cid:0)f att(x) att(x)(cid:1)F Oh2f att(x) att(x)F. Moreover, by the definition of the self-attention layer, we have h att(x) att(y)F = (cid:0)V hx(cid:1)Sh(x) (cid:0)V hy(cid:1)Sh(y)F h2xSh(x) ySh(y)F, (4) (5) where we used Lemma 1 for the inequality. Moreover, we have xSh(x) ySh(y)F = x(cid:0)Sh(x) Sh(y)(cid:1) + (x y)Sh(y)F x(cid:0)Sh(x) Sh(y)(cid:1)F + (x y)Sh(y)F xFSh(x) Sh(y)F + yFSh(y)F, (triangular inequality) where we used Lemma 1 for the last inequality. We recall that we have xF from Eq. (3). Recalling the fact that is row-stochastic leads to Sh(y)F via the simple derivation Sh(y) = S2 = (cid:88) (cid:88) S2 ij (cid:88) (cid:88) Sij n, i=1 j= i=1 j=1 (cid:124) (cid:123)(cid:122) (cid:125) =1 where we used = Sh(y) to alleviate the notations, this leads to xSh(x) ySh(y)F RSh(x) Sh(y)F + nx yF. (6) We now proceed to bound the term Sh(x) Sh(y)F. Since the softmax operator is applied row-wise, the i-th row of Sh(x) writes g(x)i = softmax(cid:0)(xAhx)i (cid:1) R1n. We define g(y)i similarly. Then, we have Sh(x) Sh(y)2 = (cid:88) (cid:88) i=1 j=1 (Sh(x) Sh(y))2 ij = (cid:88) i=1 g(x)i g(y)i2. We recall the following result on the Lipschitz constant of the softmax operator with respect to the Euclidean norm, i.e., the ℓ2 norm (we note that Nair (2026) states the result for any ℓp norm). Theorem B.2. (Nair, 2026, Theorem 1) Let and u, Rn. Then, softmax(u) softmax(v) 1 2 v. Applying Theorem B.2 leads for any [n] to g(x)i g(y)i = softmax(cid:0)(xAhx)i (cid:1) softmax(cid:0)(yAhy)i (cid:1) 1 2 (xAhx)i (yAhy)i. Hence, we obtain using the fact that is monotonically increasing that Vision Transformer Benefits from Non-Smooth Components Sh(x) Sh(y)F (cid:32) 1 4 (cid:88) i=1 (xAhx)i (yAhy)i2 (cid:33)1/2 (xAhx yAhy)i (cid:33)1/2 (cid:32) (cid:88) i=1 xAhx yAhyF (x y)Ahx yAh(x y)F (cid:0)(x y)AhxF + yAh(x y)F (cid:1) (cid:0)(x y)FAhxF + yAhFx yF (cid:1) (cid:0)(x y)FAh2xF + yFAh2x yF (cid:1). (Lemma 1) (Lemma 1) = = = 1 2 1 2 1 2 1 2 1 2 1 2 Since singular values are invariant to transposition and the Frobenius norm can be expressed as the sum of the singular values, we know that (x y)F = yF and that yF = yF. Recalling that by assumption, the Frobenius norm of sequences of tokens is bounded by from Eq. (3), we have Sh(x) Sh(y)F 1 (cid:0)x yFAh2xF + yFAhFx yF (cid:1) RAh2x yF. From Eq. (6), we obtain and from Eq. (5) we obtain This leads using Eq. (4) to xSh(x) ySh(y)F (cid:0)R2Ah2 + n(cid:1)x yF, att(x) att(y)F h (cid:0)R2Ah2 + n(cid:1)x yF, (x) (y)F (cid:88) h= Oh2V h2 (cid:0)R2Ah2 + n(cid:1)x yF, with and = α the rate of change and take the expectation to conclude the proof. E. As in Appendix B.1, since the upper bound holds for randomly sampled sequences x, y, we can bound 24 Vision Transformer Benefits from Non-Smooth Components C. Implementation details C.1. Vision transformers In vision transformers (ViT, Dosovitskiy et al., 2021), inputs are 2D images that are split into square patches Architecture. of size , which are flattened and linearly embedded in dimension d. classification token CLS is prepended to the sequence of tokens before adding positional embeddings. The obtained sequence of tokens = (x1, . . . , xn) (Rd)n is fed to succession of transformer encoders (Vaswani et al., 2017). Each block consists of multihead self-attention module followed by feedforward network implemented as two-layer MLP with GeLU activation (Hendrycks & Gimpel, 2016) and hidden dimension taken as 4 times the embedding dimension (Dosovitskiy et al., 2021; Vaswani et al., 2017). LayerNorm (Ba et al., 2016) is applied before each block, and residual connection is applied after each block. It leads to the 5 modules displayed in Fig. 2 (left). In our experiments, we use ViT models of size 86M and 632M with patch sizes 16 and 14, respectively. Implementation. Models are pretrained on ImageNet-21k. Their characteristics are given in Table 2. In our code, we follow the original ViT implementation from Dosovitskiy et al. (2021) and use convolutional layer to embed images (see Dosovitskiy et al., 2021, Hybrid Architecture\"). This is also the standard in the implementation from HuggingFace (2025). In Fig. 5, we display the implementation of the ViT-Base model with classification head for 10 classes (we renamed our package my_lib\" to respect the anonymity). Table 2. Details of ViT variants (Dosovitskiy et al., 2021) with the patch size, the number of layers, the number of attention heads, the embedding dimension, the number of parameters, and the link to the pretrained weights. model patch size seq. length layers heads embedding parameters ViT-Base ViT-Huge 16 14 197 257 12 32 12 768 1280 86M 632M C.2. Data preprocessing All our experiments are conducted on varied collection of 11 classification benchmarks: Cifar10, Cifar100 (Krizhevsky, 2009); variants from Cifar10-C (Hendrycks & Dietterich, 2019) with severity 5: Contrast, Gaussian Noise, Motion Blur, Snow, Speckle Noise; 2 domains from DomainNet (Peng et al., 2019), challenging benchmark typically used for domain generalization: Clipart, Sketch; Flowers102 (Nilsback & Zisserman, 2008) and Pets (Parkhi et al., 2012). The preprocessing follows Dosovitskiy et al. (2021) and Kolesnikov et al. (2020): for training data, we apply random cropping, 224224 image resizing, and random horizontal flip for training images. For validation and test data, the 224224 image resizing is applied before center cropping images. All images are normalized using the ImageNet (Deng et al., 2009) statistics. It ensures images with mean [0.485, 0.456, 0.406] and standard deviation [0.229, 0.224, 0.225]. For datasets that do not have predefined training and test sets (i.e., datasets from Cifar10-C and DomainNet), we manually create deterministic training and test sets following 80% 20% split. The deterministic part is crucial to ensure no data contamination. C.3. Finetuning setup The finetuning experiments of Section 5.2 follow the protocol from Dosovitskiy et al. (2021) with resolution of 224 224. Configurations. We consider ViT-Base model and finetune each of its trainable components in isolation: we freeze all the weights of the model, except the group studied which is optimized across the depth: the attention norm (LN1), the attention module (MHA), the feedforward norm (LN2), the first feedforward layer (FC1) and the second feedforward layer (FC2). The classification head is randomly initialized following Dosovitskiy et al. (2021). This leads to the 5 configurations described in Table 3 along with their corresponding number of trainable parameters. We add as baseline the full-finetuning (All), where all the models parameters are trainable. Memory load. The finetuning configurations have the same inference cost since they share the same ViT architecture. However, the number of trainable parameters differs. The GPU usage of training model consists of the memory load to store the model parameters, the optimizer states, the gradients, and the activations (Thor, 2025). In our setting, the 25 Vision Transformer Benefits from Non-Smooth Components # Python snippet to print the ViT architecture from my_lib import ViT model = ViT(name=\"base\", n_classes=10) print(model) # Corresponding output Transformer( (embedding): Embedding( (patching): PatchImages( (patching): Sequential( (0): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16)) (1): Flatten(start_dim=2, end_dim=-1) ) ) ) (blocks): ModuleList( (0-11): 12 TransformerBlock( (attn_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (attn): SelfAttention( (qkv_mat): Linear(in_features=768, out_features=2304, bias=True) (output): Linear(in_features=768, out_features=768, bias=True) ) (ffn_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (ffn): FeedForward( (fc1): Linear(in_features=768, out_features=3072, bias=True) (fc2): Linear(in_features=3072, out_features=768, bias=True) ) ) ) (output): Output( (output_layer): ClassificationLayer( (output_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True) (output): Linear(in_features=768, out_features=10, bias=True) ) ) ) Figure 5. ViT-Base Implementation. Table 3. Finetuning configurations. Configurations are denoted by the name of the trainable transformer component and ordered in terms of plasticity ranking (see Section 5.1). We report the number of trainable parameters on ViT-Base. configuration MHA FC FC2 LN2 LN1 parameters % of total 28M 28M 28M 18K 18K 0.02 33 33 0.02 memory load is the same between configurations except for the optimizer and the gradient computation. For model with parameters and precision of bytes, the memory required to store the gradients is because backpropagation computes gradient per parameter. The same memory is needed for the optimizer states with SGD (and the double for Adam (Kingma & Ba, 2014; Loshchilov & Hutter, 2019), which also computes the variance). In Table 4, the memory usage for one training step on Cifar10 for each configuration with default FP32 precision. Table 4. Memory load comparison. Memory usage of the optimizer and gradients for one training step (in MB). configuration MHA FC1 FC2 LN2 LN1 memory load 220 220 0.14 0.14 26 Vision Transformer Benefits from Non-Smooth Components Optimization. We optimize models with the Stochastic Gradient Descent (SGD), momentum of 0.9, no weight decay, cosine learning rate decay, batch size of 512, and gradient clipping at norm 1. The finetuning resolution is of 224. For each pair of dataset - configuration, we perform sweep over 4 learning rates, as summarized in Table 5, and conduct 3 runs with different seeds relative to network initialization and dataloaders. Table 5. Finetuning hyperparameters. We report the choice of optimizer, batch size, training steps, and learning rates. dataset optimizer batch size training steps learning rates η Cifar10 Cifar100 Contrast Gaussian Noise Motion Blur Snow Speckle Noise Clipart Sketch Flowers102 Pets"
        },
        {
            "title": "SGD\nSGD\nSGD\nSGD\nSGD\nSGD\nSGD\nSGD\nSGD\nSGD\nSGD",
            "content": "512 512 512 512 512 512 512 512 512 512 512 10000 10000 10000 10000 10000 10000 10000 20000 20000 5000 4000 {1e3, 3e3, 1e2, 3e2} {1e3, 3e3, 1e2, 3e2} {1e3, 3e3, 1e2, 3e2} {1e3, 3e3, 1e2, 3e2} {1e3, 3e3, 1e2, 3e2} {1e3, 3e3, 1e2, 3e2} {1e3, 3e3, 1e2, 3e2} {3e3, 1e2, 3e2, 6e2} {3e3, 1e2, 3e2, 6e2} {1e3, 3e3, 1e2, 3e2} {1e3, 3e3, 1e2, 3e2} Performance. For each run, we monitor the training using validation set (20% of the training set). The final performance is the test accuracy of the checkpoint that achieves the best validation accuracy. C.4. Plasticity setup Realistic setting. In real-world applications, the discrepancy between the pretraining and downstream data is not known priori. This motivates us to compute the plasticity images coming from the pretraining distribution and various downstream distributions, without any additional assumption. This differs from prior work, where the distribution shift can be categorized, e.g., into natural, subpopulation, or synthetic shift (Deng et al., 2023; Lee et al., 2023; Xie et al., 2024, 2025). Practical implementation. The sequences of tokens x, are obtained by embedding preprocessed images with the pretrained model studied. We loop over batches of size with forward passes on the GPU and store high-dimensional outputs on the CPU. This ensures fast computation and avoids out-of-memory issues. The total number of samples used to compute the plasticity is equal to b. We note that all the transformer components take as input sequences of tokens in Rd, except for the second layer of the feedforward ffc2, where the tokens must be in R4d. Akin to how vector in the plane can be mapped to 3D vector (u1, u2, 0), we lift each token xi into R4d by padding the remaining entries with zeros. D. Additional experiments In this section, we report the detailed results corresponding to the figures presented in the paper, along with additional experiments not shown in the main due to space constraints. D.1. Plasticity analysis In this section, we present the additional figures and experiments related to Sections 4 and 5.1. Theoretical plasticity ranking in Section 4. We numerically compute the plasticity upper bounds of Section 4 on ViT-Base. The sequence length is = 197, and the number of attention heads is = 12. Following Castin et al. (2024, i=1 xi2. The value Section 5), the average radius is computed over input sequences = (x1, . . . , xn) as = = 19.4 obtained on Cifar10 (Krizhevsky, 2009) is used as the reference for the computation of the bounds in Propositions 1 to 3. We display the upper bounds in Fig. 6. The upper bounds ranking follows our theoretical insight,s with the attention module having the largest upper bound, followed by the first and second feedforward layer, the LayerNorm preceding the feedforward network, and finally, the LayerNorm preceding the attention module. We note that the upper bound of the attention module is several orders of magnitude larger than the other components. Even with the dependency in n1/ (cid:113) 1 (cid:80)n 27 Vision Transformer Benefits from Non-Smooth Components empirically observed in Castin et al. (2024, Fig. 1), the order of the bound remains 106. We attribute this scale to the dependency of the bound on the number of heads, the radius r, and the sequence length n. As explained in Section 4, the bound is tight in terms of dependency in n, the numerical values of and being close leads to large bound in practice. We notice in Section 5.1 that the plasticity scales are more similar between modules than the upper bounds. This further confirms that the difference in scale between the upper bounds is due to the difficulty of bounding the self-attention Lipschitz constant. In particular, we observe in Section 5.1 that the plasticity computed as an average rate of change follows the same ranking but with lower magnitude, notably for the attention module. This is reminiscent of Ashlagi et al. (2021) where the authors showed that the gap between the Lipschitz constant and the average rate of change can be considerable. Figure 6. Plasticity upper bounds on ViT-Base. The sequence length is = 197, the number of heads is = 12 and the average (cid:113) 1 i=1 xi2. We obtain value of = 19.4. We can see that the radius is computed over input sequences = (x1, . . . , xn) as = attention module has the highest plasticity, followed by the first and second feedforward layers, the LayerNorm preceding the feedforward, and finally the LayerNorm preceding the attention module. This aligns with our theoretical insights. (cid:80)n Plasticity computation of Section 5.1. We extend the analysis of Section 5.1 to additional datasets and display the results in Figs. 7 to 16. Our findings are aligned with the theoretical analysis in Section 4 and shows that the attention module has the highest plasticity, followed by the first feedforward linear layer, then the second feedforward linear layer. The LayerNorms are more rigid with plasticity below 1. 28 Vision Transformer Benefits from Non-Smooth Components Figure 7. Plasticity analysis on Cifar10. The distribution of rates of change (x) (y)F/x yF on ViT-Base (left) follow the upper bound ranking predicted by our theory in Section 4. We observe along transformer blocks of ViT-Base (middle) that the attention module has the highest plasticity P(f ), followed by the first and second linear layers of the feedforward. The LayerNorms are the most rigid, with plasticity below 1. The same pattern is obtained on ViT-Huge (right), where the higher attention plasticity further validates our theory (see Proposition 3) since the sequence length is larger than with ViT-Base. Figure 8. Plasticity analysis on Cifar100. The distribution of rates of change (x) (y)F/x yF on ViT-Base (left) follow the upper bound ranking predicted by our theory in Section 4. We observe along transformer blocks of ViT-Base (middle) that the attention module has the highest plasticity P(f ), followed by the first and second linear layers of the feedforward. The LayerNorms are the most rigid, with plasticity below 1. The same pattern is obtained on ViT-Huge (right), where the higher attention plasticity further validates our theory (see Proposition 3) since the sequence length is larger than with ViT-Base. 29 Vision Transformer Benefits from Non-Smooth Components Figure 9. Plasticity analysis on Contrast. The distribution of rates of change (x) (y)F/x yF on ViT-Base (left) follow the upper bound ranking predicted by our theory in Section 4. We observe along transformer blocks of ViT-Base (middle) that the attention module has the highest plasticity P(f ), followed by the first and second linear layers of the feedforward. The LayerNorms are the most rigid, with plasticity below 1. The same pattern is obtained on ViT-Huge (right), where the higher attention plasticity further validates our theory (see Proposition 3) since the sequence length is larger than with ViT-Base. Figure 10. Plasticity analysis on Gaussian Noise. The distribution of rates of change (x) (y)F/x yF on ViT-Base (left) follow the upper bound ranking predicted by our theory in Section 4. We observe along transformer blocks of ViT-Base (middle) that the attention module has the highest plasticity P(f ), followed by the first and second linear layers of the feedforward. The LayerNorms are the most rigid, with plasticity below 1. The same pattern is obtained on ViT-Huge (right), where the higher attention plasticity further validates our theory (see Proposition 3) since the sequence length is larger than with ViT-Base. 30 Vision Transformer Benefits from Non-Smooth Components Figure 11. Plasticity analysis on Motion Blur. The distribution of rates of change (x) (y)F/x yF on ViT-Base (left) follow the upper bound ranking predicted by our theory in Section 4. We observe along transformer blocks of ViT-Base (middle) that the attention module has the highest plasticity P(f ), followed by the first and second linear layers of the feedforward. The LayerNorms are the most rigid, with plasticity below 1. The same pattern is obtained on ViT-Huge (right), where the higher attention plasticity further validates our theory (see Proposition 3) since the sequence length is larger than with ViT-Base. Figure 12. Plasticity analysis on Snow. The distribution of rates of change (x) (y)F/x yF on ViT-Base (left) follow the upper bound ranking predicted by our theory in Section 4. We observe along transformer blocks of ViT-Base (middle) that the attention module has the highest plasticity P(f ), followed by the first and second linear layers of the feedforward. The LayerNorms are the most rigid, with plasticity below 1. The same pattern is obtained on ViT-Huge (right), where the higher attention plasticity further validates our theory (see Proposition 3) since the sequence length is larger than with ViT-Base. 31 Vision Transformer Benefits from Non-Smooth Components Figure 13. Plasticity analysis on Speckle Noise. The distribution of rates of change (x) (y)F/x yF on ViT-Base (left) follow the upper bound ranking predicted by our theory in Section 4. We observe along transformer blocks of ViT-Base (middle) that the attention module has the highest plasticity P(f ), followed by the first and second linear layers of the feedforward. The LayerNorms are the most rigid, with plasticity below 1. The same pattern is obtained on ViT-Huge (right), where the higher attention plasticity further validates our theory (see Proposition 3) since the sequence length is larger than with ViT-Base. Figure 14. Plasticity analysis on Clipart. The distribution of rates of change (x) (y)F/x yF on ViT-Base (left) follow the upper bound ranking predicted by our theory in Section 4. We observe along transformer blocks of ViT-Base (middle) that the attention module has the highest plasticity P(f ), followed by the first and second linear layers of the feedforward. The LayerNorms are the most rigid, with plasticity below 1. The same pattern is obtained on ViT-Huge (right), where the higher attention plasticity further validates our theory (see Proposition 3) since the sequence length is larger than with ViT-Base. 32 Vision Transformer Benefits from Non-Smooth Components Figure 15. Plasticity analysis on Flowers102. The distribution of rates of change (x) (y)F/x yF on ViT-Base (left) follow the upper bound ranking predicted by our theory in Section 4. We observe along transformer blocks of ViT-Base (middle) that the attention module has the highest plasticity P(f ), followed by the first and second linear layers of the feedforward. The LayerNorms are the most rigid, with plasticity below 1. The same pattern is obtained on ViT-Huge (right), where the higher attention plasticity further validates our theory (see Proposition 3) since the sequence length is larger than with ViT-Base. Figure 16. Plasticity analysis on Pet. The distribution of rates of change (x) (y)F/x yF on ViT-Base (left) follow the upper bound ranking predicted by our theory in Section 4. We observe along transformer blocks of ViT-Base (middle) that the attention module has the highest plasticity P(f ), followed by the first and second linear layers of the feedforward. The LayerNorms are the most rigid, with plasticity below 1. The same pattern is obtained on ViT-Huge (right), where the higher attention plasticity further validates our theory (see Proposition 3) since the sequence length is larger than with ViT-Base. 33 Vision Transformer Benefits from Non-Smooth Components D.2. Finetuning analysis In this section, we provide the additional results, figures, and experiments related to Section 5.2. Performance comparison. In Table 6, we gather the finetuning performance for each configuration and dataset. It is used to obtain Fig. 2 (right). The relative gain metric is computed as the percentage improvement between the finetuning and the linear probing performance. Table 6. Full finetuning results. We investigate the benefits of plasticity by evaluating finetuning the trainable components of Table 3 on diverse set of 11 image classification benchmarks. We report the best top-1 accuracy (%) on the test set over the learning grid of each dataset (). Entries show the mean and standard deviation over three finetuning runs with different seeds. The best overall performance among the transformer components configurations of Table 3 is in bold. The full-finetuning configuration lets the 86M parameters of ViT-Base be trainable. The linear probing performance is obtained over the attention hidden representation of the last layer. configuration"
        },
        {
            "title": "MHA",
            "content": "FC1 FC2 LN2 LN1 full finetuning linear probing Cifar10 Cifar100 Contrast Gaussian Noise Motion Blur Snow Speckle Noise Clipart Sketch Flowers102 Pet 98.910.07 92.650.07 97.090.11 89.410.53 94.720.21 95.470.13 90.070.32 77.310.41 69.230.05 99.030.08 94.370.13 99.090.05 92.850.07 97.060.08 89.490.16 94.530.06 95.520.20 89.850.34 76.470.24 69.310.18 99.050.06 94.260.26 98.910.06 92.310.11 96.280.11 88.490.51 94.040.16 95.270.29 89.220.31 76.540.17 69.490.20 98.860.06 93.980.20 98.720.05 91.930.11 96.670.20 89.550.04 93.950.34 95.510.11 89.710.17 74.370.08 65.270.15 99.210.07 94.390.13 98.670.03 91.430.07 96.890.19 88.990.24 93.250.29 95.150.10 89.740.31 74.650.16 65.760.10 98.990.20 94.460. 99.020.02 92.740.05 97.230.18 87.141.16 94.670.14 95.420.13 89.580.43 78.500.49 71.300.26 99.150.05 94.570.29 Avg. 90.8 90.7 90.3 89. 89.8 90.9 91.95 65.43 73.25 49.20 59.70 59.25 51.15 42.76 29.08 96.34 88.33 64.2 For visualization purposes, we display in Fig. 17 the overall performance of each configuration on the 11 benchmarks. We observe similar patterns than for the relative gain in Fig. 2 (right): the higher the plasticity, the better the finetuning. Figure 17. Better performance (11 benchmarks). We compare transformer components, ordered in terms of decreasing plasticity, and report the average top-1 accuracy over diverse set of 11 benchmarks, with the pooled standard error computed over 3 finetuning runs. We can see that high plasticity results in better performance. 34 Vision Transformer Benefits from Non-Smooth Components Robustness analysis. In Fig. 18, we display the finetuning performance over learning rates and seeds for all benchmarks. Overall, we observe similar patterns to those in Fig. 4 (left), with plastic components resulting in more stable performance. In particular, we consistently see that finetuning the attention module leads to better and more stable performance. Figure 18. Robustness comparison (11 benchmarks). We display the distribution of the finetuning performance over the learning rates from Table 5 and 3 seeds relative to network initialization and dataloaders. We compare transformer components, ordered in terms of decreasing plasticity. Overall, plastic components result in more stable performance. In particular, the attention module consistently displays small performance variation. 35 Vision Transformer Benefits from Non-Smooth Components Gradient norm analysis. In Figs. 19 to 51, we display the evolution of the gradient norms and validation loss on all benchmarks, learning rates and seeds. We consistently see that the scale of gradient norms and validation loss descent follows the plasticity ranking from Section 5.1: we observe faster and better convergence for plastic components, with even more salient benefits on challenging datasets such as Cifar100, Clipart and Sketch. In particular, we observe lower gradient norms and less steep descent for the LayerNorms, which is aggravated for low learning rates η. This showcase the plasticity of LayerNorms to the choice of learning rates compared to components with higher plasticity. For components with high plasticity, we observe rather expected training evolution for low learning rates with increasing gradient norms and decreasing validation loss. However, for higher learning rates, we can see that the gradient norms first increase in steep fashion before slowly decreasing. This can be understood by the model escaping the pretraining minima, passing through sharp regions of the loss lanscape before converging to flat local minima. This behavior can be seen on Clipart Fig. 40, for instance, and is particularly salient for the attention module. This is reminescent of Park & Kim (2022) that showed that the multihead self-attention module flattens the loss landscape, which leads to better generalization (Chen et al., 2022; Foret et al., 2021; Ilbert et al., 2024). Figure 19. Training dynamics on Cifar10 with seed 0. We display the evolution during training of the gradient norms (top) and the validation loss (bottom) of each finetuning configuration of Table 3, with increasing learning rate η from left to right. Components are ordered in terms of decreasing plasticity in the legend. Plastic components have higher gradient norms, which leads to steeper descent in the validation loss and better downstream performance. The benefits of plasticity are even more salient with low learning rates. Overall, higher plasticity leads to better optimization and generalization. 36 Vision Transformer Benefits from Non-Smooth Components Figure 20. Training dynamics on Cifar10 with seed 42. Akin to Fig. 19, we observe the same consistent pattern of faster and better convergence for components with high plasticity. Figure 21. Training dynamics on Cifar10 with seed 3407. Akin to Fig. 19, we observe the same consistent pattern of faster and better convergence for components with high plasticity. 37 Vision Transformer Benefits from Non-Smooth Components Figure 22. Training dynamics on Cifar100 with seed 0. We display the evolution during training of the gradient norms (top) and the validation loss (bottom) of each finetuning configuration of Table 3, with increasing learning rate η from left to right. Components are ordered in terms of decreasing plasticity in the legend. Plastic components have higher gradient norms, which leads to steeper descent in the validation loss and better downstream performance. The benefits of plasticity are even more salient with low learning rates. Overall, higher plasticity leads to better optimization and generalization. Figure 23. Training dynamics on Cifar100 with seed 42. Akin to Fig. 22, we observe the same consistent pattern of faster and better convergence for components with high plasticity. 38 Vision Transformer Benefits from Non-Smooth Components Figure 24. Training dynamics on Cifar100 with seed 3407. Akin to Fig. 22, we observe the same consistent pattern of faster and better convergence for components with high plasticity. Figure 25. Training dynamics on Contrast with seed 0. We display the evolution during training of the gradient norms (top) and the validation loss (bottom) of each finetuning configuration of Table 3, with increasing learning rate η from left to right. Components are ordered in terms of decreasing plasticity in the legend. Plastic components have higher gradient norms, which leads to steeper descent in the validation loss and better downstream performance. The benefits of plasticity are even more salient with low learning rates. Overall, higher plasticity leads to better optimization and generalization. 39 Vision Transformer Benefits from Non-Smooth Components Figure 26. Training dynamics on Contrast with seed 42. Akin to Fig. 25, we observe the same consistent pattern of faster and better convergence for components with high plasticity. Figure 27. Training dynamics on Contrast with seed 3407. Akin to Fig. 25, we observe the same consistent pattern of faster and better convergence for components with high plasticity. 40 Vision Transformer Benefits from Non-Smooth Components Figure 28. Training dynamics on Gaussian Noise with seed 0. We display the evolution during training of the gradient norms (top) and the validation loss (bottom) of each finetuning configuration of Table 3, with increasing learning rate η from left to right. Components are ordered in terms of decreasing plasticity in the legend. Plastic components have higher gradient norms, which leads to steeper descent in the validation loss and better downstream performance. The benefits of plasticity are even more salient with low learning rates. Overall, higher plasticity leads to better optimization and generalization. Figure 29. Training dynamics on Gaussian Noise with seed 42. Akin to Fig. 28, we observe the same consistent pattern of faster and better convergence for components with high plasticity. 41 Vision Transformer Benefits from Non-Smooth Components Figure 30. Training dynamics on Gaussian Noise with seed 3407. Akin to Fig. 28, we observe the same consistent pattern of faster and better convergence for components with high plasticity. Figure 31. Training dynamics on Motion Blur with seed 0. We display the evolution during training of the gradient norms (top) and the validation loss (bottom) of each finetuning configuration of Table 3, with increasing learning rate η from left to right. Components are ordered in terms of decreasing plasticity in the legend. Plastic components have higher gradient norms, which leads to steeper descent in the validation loss and better downstream performance. The benefits of plasticity are even more salient with low learning rates. Overall, higher plasticity leads to better optimization and generalization. 42 Vision Transformer Benefits from Non-Smooth Components Figure 32. Training dynamics on Motion Blur with seed 42. Akin to Fig. 31, we observe the same consistent pattern of faster and better convergence for components with high plasticity. Figure 33. Training dynamics on Motion Blur with seed 3407. Akin to Fig. 31, we observe the same consistent pattern of faster and better convergence for components with high plasticity. 43 Vision Transformer Benefits from Non-Smooth Components Figure 34. Training dynamics on Snow with seed 0. We display the evolution during training of the gradient norms (top) and the validation loss (bottom) of each finetuning configuration of Table 3, with increasing learning rate η from left to right. Components are ordered in terms of decreasing plasticity in the legend. Plastic components have higher gradient norms, which leads to steeper descent in the validation loss and better downstream performance. The benefits of plasticity are even more salient with low learning rates. Overall, higher plasticity leads to better optimization and generalization. Figure 35. Training dynamics on Snow with seed 42. Akin to Fig. 34, we observe the same consistent pattern of faster and better convergence for components with high plasticity. 44 Vision Transformer Benefits from Non-Smooth Components Figure 36. Training dynamics on Snow with seed 3407. Akin to Fig. 34, we observe the same consistent pattern of faster and better convergence for components with high plasticity. Figure 37. Training dynamics on Speckle Noise with seed 0. We display the evolution during training of the gradient norms (top) and the validation loss (bottom) of each finetuning configuration of Table 3, with increasing learning rate η from left to right. Components are ordered in terms of decreasing plasticity in the legend. Plastic components have higher gradient norms, which leads to steeper descent in the validation loss and better downstream performance. The benefits of plasticity are even more salient with low learning rates. Overall, higher plasticity leads to better optimization and generalization. 45 Vision Transformer Benefits from Non-Smooth Components Figure 38. Training dynamics on Speckle Noise with seed 42. Akin to Fig. 37, we observe the same consistent pattern of faster and better convergence for components with high plasticity. Figure 39. Training dynamics on Speckle Noise with seed 3407. Akin to Fig. 37, we observe the same consistent pattern of faster and better convergence for components with high plasticity. 46 Vision Transformer Benefits from Non-Smooth Components Figure 40. Training dynamics on Clipart with seed 0. We display the evolution during training of the gradient norms (top) and the validation loss (bottom) of each finetuning configuration of Table 3, with increasing learning rate η from left to right. Components are ordered in terms of decreasing plasticity in the legend. Plastic components have higher gradient norms, which leads to steeper descent in the validation loss and better downstream performance. The benefits of plasticity are salient across all learning rates. Overall, higher plasticity leads to better optimization and generalization. Figure 41. Training dynamics on Clipart with seed 42. Akin to Fig. 40, we observe the same consistent pattern of faster and better convergence for components with high plasticity. 47 Vision Transformer Benefits from Non-Smooth Components Figure 42. Training dynamics on Clipart with seed 3407. Akin to Fig. 40, we observe the same consistent pattern of faster and better convergence for components with high plasticity. Figure 43. Training dynamics on Sketch with seed 0. We display the evolution during training of the gradient norms (top) and the validation loss (bottom) of each finetuning configuration of Table 3, with increasing learning rate η from left to right. Components are ordered in terms of decreasing plasticity in the legend. Plastic components have higher gradient norms, which leads to steeper descent in the validation loss and better downstream performance. The benefits of plasticity are salient across all learning rates. Overall, higher plasticity leads to better optimization and generalization. 48 Vision Transformer Benefits from Non-Smooth Components Figure 44. Training dynamics on Sketch with seed 42. Akin to Fig. 43, we observe the same consistent pattern of faster and better convergence for components with high plasticity. Figure 45. Training dynamics on Sketch with seed 3407. Akin to Fig. 43, we observe the same consistent pattern of faster and better convergence for components with high plasticity. 49 Vision Transformer Benefits from Non-Smooth Components Figure 46. Training dynamics on Pet with seed 0. We display the evolution during training of the gradient norms (top) and the validation loss (bottom) of each finetuning configuration of Table 3, with increasing learning rate η from left to right. Components are ordered in terms of decreasing plasticity in the legend. On Pet, the pretrained model already achieves good linear probing performance as shown in Table 6. This leads to lower gradient norms for all components compared to more challenging datasets. That being said, we observe for low learning rate that plastic components have higher gradient norms ans steeper descent in the validation loss. Figure 47. Training dynamics on Pet with seed 42. Akin to Fig. 46, we observe the same consistent patterns. 50 Vision Transformer Benefits from Non-Smooth Components Figure 48. Training dynamics on Pet with seed 3407. Akin to Fig. 46, we observe the same consistent patterns. Figure 49. Training dynamics on Flowers102 with seed 0. We display the evolution during training of the gradient norms (top) and the validation loss (bottom) of each finetuning configuration of Table 3, with increasing learning rate η from left to right. Components are ordered in terms of decreasing plasticity in the legend. On Flowers102, the pretrained model already achieves good linear probing performance as shown in Table 6. This leads to lower gradient norms for all components compared to more challenging datasets. That being said, we observe for low learning rate that plastic components have higher gradient norms ans steeper descent in the validation loss. 51 Vision Transformer Benefits from Non-Smooth Components Figure 50. Training dynamics on Flowers102 with seed 42. Akin to Fig. 49, we observe the same consistent patterns. Figure 51. Training dynamics on Flowers102 with seed 42. Akin to Fig. 49, we observe the same consistent patterns."
        }
    ],
    "affiliations": [
        "Noah's Ark"
    ]
}