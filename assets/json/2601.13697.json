{
    "paper_title": "Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning",
    "authors": [
        "Zhihang Yuan",
        "Chengyu Yue",
        "Long Huang",
        "Litu Ou",
        "Lei Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction tuning is a standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from a weak proxy, largely ignoring evolving uncertainty, and thus missing a key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes a small GPT-2 proxy with a LoRA ensemble and aggregates per-example gradients into a Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 0 2 ] . [ 1 7 9 6 3 1 . 1 0 6 2 : r Uncertainty-Aware Gradient Signal-to-Noise Data Selection for Instruction Tuning Zhihang Yuan1, Chengyu Yue1, Long Huang1, Litu Ou2, Lei Shi1, * 1Alibaba Cloud Computing 2The University of Edinburgh 1{yuanzhihang.yzh,yuechengyu.ycy, baixuan.hl, juetian.sl}@alibaba-inc.com 2litu.ou@ed.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "Instruction tuning is standard paradigm for adapting large language models (LLMs), but modern instruction datasets are large, noisy, and redundant, making full-data fine-tuning costly and often unnecessary. Existing data selection methods either build expensive gradient datastores or assign static scores from weak proxy, largely ignoring evolving uncertainty,and thus missing key source of LLM interpretability. We propose GRADFILTERING, an objective-agnostic, uncertainty-aware data selection framework that utilizes small GPT-2 proxy with LoRA ensemble and aggregates per-example gradients into Gradient Signal-to-Noise Ratio (G-SNR) utility. Our method matches or surpasses random subsets and strong baselines in most LLM-as-a-judge evaluations as well as in human assessment. Moreover, GRADFILTERING-selected subsets converge faster than competitive filters under the same compute budget, reflecting the benefit of uncertainty-aware scoring."
        },
        {
            "title": "Introduction",
            "content": "Instruction tuning has become standard recipe for adapting large language models (LLMs) to follow human instructions (Ouyang et al., 2022). Modern instruction datasets can contain hundreds of thousands of examples (Taori et al., 2023; Xu et al., 2023; Wang et al., 2023; Xu et al., 2024), making full-data fine-tuning expensive and often unnecessary: many examples are redundant or noisy, while relatively small subset can suffice to achieve strong performance (Zhou et al., 2023). This motivates the problem of data selection for instruction tuning: can we identify small subset of training examples that matches or even improves the performance of full-data fine-tuning, while reducing cost? long line of work has studied data valuation via influence functions and related gradient-based *Corresponding Author. juetian.sl@alibaba-inc.com 1 criteria. Classical influence-function methods estimate the effect of upweighting or removing training point on target loss using second-order Taylor expansions around the empirical risk minimizer (Koh and Liang, 2017), or first-order approximations such as TracIn (Pruthi et al., 2020). While theoretically grounded, these methods are computationally expensive and brittle for deep networks, and do not scale effectively to modern LLM fine-tuning pipelines. More recent work tailors data selection to instruction tuning. LESS (Xia et al., 2024) constructs gradient datastore with low-rank representations to enable optimizer-aware similarity search. Superfiltering (Li et al., 2024) adopts weak-to-strong strategy, using small proxy model to filter top-k subset based on Instruction-Following Difficulty (IFD) scores. These approaches achieve strong performance, but also have limitations: LESS requires computing and storing per-example gradients of strong model and is inherently constrained by its dependency on task-specific validation sets, while Superfiltering assigns each example static difficulty score from pre-trained proxy, without modeling interactions between training samples or the evolution of uncertainty during fine-tuning. In parallel, epistemic uncertainty has emerged as useful lens on data quality. Large-scale instruction datasets inevitably contain noisy, out-ofdistribution, or spurious examples. Uncertainty helps detect such samples by providing signal that is complementary to training dynamics (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017). LoRA-Ensemble (Mühlematter et al., 2024) shows that training multiple LoRA heads on shared backbone can recover much of the accuracy and calibration benefits of deep ensembles at fraction of the parameters and compute cost, suggesting that lightweight LoRA ensembles can serve as practical uncertainty probes. However, these ideas have not yet been leveraged for uncertainty-aware data valuation in instruction tuning. Our approach. We propose GRADFILTERING, gradient-based data selection framework utilizing LoRA ensemble on frozen small proxy backbone (i.e., GPT-2). This method captures intrinsic dynamics by tracking per-example gradients across ensemble members and epochs and aggregates them into Gradient Signal-to-Noise Ratio (G-SNR), metric that unifies learning progress (via early-to-late gradient-drop statistics) and epistemic uncertainty (via ensemble variance). Derived solely from the gradients of the training objective, G-SNR is objective-agnosticobviating the need for task-specific rewards or manual annotations. Finally, we rank examples by G-SNR to identify high-value subsets for fine-tuning target models. Our contributions are fourfold: 1. LoRA ensembles for uncertainty-aware data valuation. To the best of our knowledge, we are the first to leverage LoRA ensemble to model epistemic uncertainty for gradientbased data valuation and selection in instruction tuning. 2. GRADFILTERING. We propose GRADFILTERING, an efficient, objective-agnostic data selection method that runs LoRA ensemble on frozen LLM backbone with small GPT-2 proxy and aggregates per-example gradients into gradient signal-to-noise (G-SNR) utility. 3. Strong empirical gains. On Alpaca and Alpaca-GPT4 with LLaMA-2-7B/13B, models fine-tuned on GRADFILTERING-selected 515% subsets match or outperform Random and Superfiltering in 19/24 LLM-as-a-judge evaluation cases, and small human study confirms that these preferences align with human judgments. 4. Faster convergence. Training on GRADFILTERING-selected subsets converges faster than competitive filtering baselines under the same compute budget, without degrading final instruction-following quality."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Data Valuation and Selection Estimating the importance of individual training examples is long-standing problem in machine learning. Influence-function methods approximate the effect of upweighting or removing training point on the loss at target point via second-order Taylor expansions (Koh and Liang, 2017) or firstorder approximations such as TracIn (Pruthi et al., 2020). While theoretically appealing, these approaches are prohibitively expensive and often brittle for deep networks, failing to scale cleanly to modern LLM fine-tuning pipelines. Recent work adapts gradient-based data valuation to instruction tuning. LESS (Xia et al., 2024) introduces an optimizer-aware influence formulation, explicitly modeling Adam updates to bulid gradient datastore with low-rank (LoRA-based) representations for gradient similarity search. This requires computing and storing per-example gradient features of strong base model and tailoring selection to particular downstream task, rather than exploiting the training dynamics of the finetuning run itself. Superfiltering (Li et al., 2024) studies weak-tostrong data filtering for instruction tuning. It shows that small proxy models such as GPT-2 produce perplexity and instruction-following difficulty (IFD) rankings highly correlated with those of larger LLaMA-2 models, and uses the proxy to compute IFD scores and select top-k subset for fine-tuning the strong model. This substantially reduces filtering cost while often matching or surpassing fulldata baselines, but assigns each example static difficulty score from the pre-trained proxy, without modeling interactions between training samples or the evolution of gradients and epistemic uncertainty during fine-tuning. 2.2 Ensembles and Uncertainty Modeling Epistemic uncertainty (Amini et al., 2020) is particularly important in large-scale instruction tuning, where training data may be noisy or spurious and we would like to identify examples on which the model is most uncertain. classical line of work uses approximate Bayesian methods such as Monte Carlo dropout (Gal and Ghahramani, 2016) or deep ensembles (Lakshminarayanan et al., 2017) to quantify epistemic uncertainty via disagreement across multiple stochastic forward passes or independently trained models. However, full ensembles of large language models are computationally expensive, limiting their practicality in standard fine-tuning pipelines. In the era of LLMs, LoRA (Hu et al., 2022) plays central role by enabling parameter-efficient adaptation while keeping the backbone frozen. LoRAEnsemble (Mühlematter et al., 2024) extends un2 certainty quantification by training multiple independent LoRA adapters on shared backbone, emulating deep ensemble without duplicating the full model. This strategy retains much of the accuracy and calibration benefits of deep ensembles at fraction of the parameter and compute cost. Positioning our approach. Motivated by these lines of work, we design GRADFILTERING as an insitu, gradient-based data selection method that combines small proxy model (GPT-2) with LoRA ensemble on frozen LLM backbone."
        },
        {
            "title": "3 LoRA-Ensemble Approximation for\nGradient-Based Data Valuation",
            "content": "3.1 LoRA-Ensemble Approximation We employ parameter-efficient ensemble based on LoRA adapters, where all ensemble members share frozen backbone θ0, but each maintains its own low-rank adapter parameters θ(m) for = 1, . . . , . For each self-attention layer, we attach rank-r adapters to the query/key/value projections as in standard LoRA, and initialize the adapters of different ensemble members independently. Let θ(m) denote the LoRA parameters of member after epochs of fine-tuning, and θ(m) = θ0 + θ(m) (1) 0 the full parameter vector. All members start from the same pretrained backbone with randomly initialized adapters, i.e., θ(m) for = 1, . . . , , and are fine-tuned on for epochs with different random seeds or data orderings. This yields adapted models that share common backbone but follow distinct parameter trajectories. We denote by g(m,e) the per-example gradient for sample proi duced by LoRA member at epoch (formally defined in Equation 2). Under first-order optimization, small parameter updatesand hence local changes in loss around given checkpointare approximately linear in these gradients. Our data valuation method therefore relies on the collection of per-example gradients {g(m,e) }m,e as compact summary of how (xi, yi) drives the ensemble during training. In practice, we train the ensemble by minimizing the average per-member loss on each batch, Lens = 1 (xi), yi), and backpropθ(m) agating this objective with respect to each adapter. This vectorized implementation is equivalent (up m=1 L(f (cid:80)M (cid:80) to constant rescaling of the learning rate) to training independent LoRA models that reuse the same backbone computation. Crucially, we do not optimize the loss of an averaged prediction, L( 1 (xi), yi), which would explicitly encourage all members to correct the same ensemble error and thus suppress the epistemic disagreement that our gradient-variance statistics are intended to capture. θ(m) Gradient-based view of LoRA ensembles. Our use of LoRA ensemble relies on two structural properties of parameter-efficient fine-tuning. First, empirical studies suggest that LoRA and related adapters typically keep the fine-tuned model close to the pretrained backbone, so that first-order Taylor expansion around θ0 provides an accurate local linearization of outputs and losses in the adapter subspace (Li et al., 2025). In this locally linear regime, the per-example LoRA gradient g(m,e) is the direction of steepest loss change for member at epoch e, and its norm reflects how strongly (xi, yi) drives parameter updates. Second, independently initialized LoRA adapters {θ(m)}M m=1 can be interpreted as approximate samples from posterior over low-rank updates around θ0 (Mühlematter et al., 2024; Balabanov and Linander, 2024). Under this view, the collection {g(m,e) }M m=1 approximates samples from distribution over update directions induced by (xi, yi) at epoch e: the ensemble mean estimates its expected update magnitude, and the ensemble variance quantifies epistemic uncertainty about how the model should adapt on this example. Our data valuation scheme builds on these two moments: we prioritize examples whose expected gradient is large in early epochs and decays over training, and we down-weight examples whose gradient variance remains high at late epochs, corresponding to regions where the ensemble fails to reach stable consensus. These intuitions are instantiated in the uncertainty-aware gradient-drop score introduced in Section 4.3. Empirical geometry of LoRA training dynamics. To empirically support this view, we visualize the trajectories of LoRA ensemble members in low-dimensional embedding of their per-example gradient profiles (Figures 1a and 1b). For each LoRA member and epoch e, we construct highdimensional feature vector gm,e RN whose i-th coordinate encodes the relative gradient drop on"
        },
        {
            "title": "4 GRADFILTERING : Data Selection with",
            "content": "G-SNR Figure 2 summarizes our gradient-based data selection pipeline GRADFILTERING. Section 3 establishes that LoRA ensemble provides rich, non-degenerate latent space, justifying Phases 12 in Figure 2. In this section, we focus on Phase 3 and derive our Gradient Signal-to-Noise Ratio (GSNR) utility, which turns ensemble training dynamics into data-selection scores. 4.1 Problem Setup Let = {(xi, yi)}N i=1 be large-scale instruction response dataset, where xi is an instruction and yi its reference response. We are given pretrained language model and wish to fine-tune it for instruction following. Our goal is to assign each training example (xi, yi) scalar utility score ui and to use these scores to select subset (or to reweight D) such that fine-tuning on achieves comparable or better performance than using the full dataset. 4.2 Per-Example Gradient Statistics We characterize each training example by how difficult it is initially, how much the ensemble learns from it, and how stable its effect is across ensemble members, using its per-example LoRA gradients. Let be the total number of training epochs. For example in ensemble member at epoch {1, . . . , } we define g(m,e) = θ(m) L(cid:0)f θ(m) (xi), yi (cid:1), (2) where is the token-level training loss and gradients are taken only with respect to the LoRA adapters θ(m) . Thus g(m,e) directly measures how strongly (xi, yi) pushes the adapted parameters to change at given epoch. Gradient magnitude. For each example and epoch e, we summarize its instantaneous training signal by the ensemble-averaged LoRA-gradient norm G(e) = 1 (cid:88) m=1 (cid:13) (cid:13)g(m,e) (cid:13) (cid:13)2, (3) where g(m,e) is the per-example gradient of member at epoch e. Larger G(e) indicates that (xi, yi) induces stronger parameter update at that stage of training, reflecting its current difficulty or influence. (a) Alpaca trajectories with GPT-2 filter. (b) Alpaca-GPT4 trajectories with GPT-2 filter. Figure 1: Trajectories of LoRA ensemble members in low-dimensional embedding of gradient profiles for (a) Alpaca and (b) Alpaca-GPT4. Each polyline corresponds to one LoRA member; markers along the line denote successive epochs. example between epoch 1 and epoch e. We stack all {gm,e}m,e, apply PCA to reduce the dimensionality from to small (e.g., = 25) for denoising and stabilization, and then run t-SNE on the PCA embeddings to obtain two-dimensional points zm,e R2 in shared gradient-profile space. For each member m, we connect zm,1 zm,2 zm,T to form trajectory; this should be interpreted as mapping each epoch to global pattern of relative gradient drops across all training examples for that member, rather than as trajectory of individual examples. Empirically, each polyline starts from similar region of the embedding but quickly diverges, with different members moving toward distinct areas and then following stable, non-collapsing trajectories. At the same time, later-epoch points are more tightly clustered than early-epoch points, indicating that ensemble disagreement is gradually reduced as training proceeds. These patterns provide qualitative evidence that (i) LoRA adapters induce rich, structured distribution over update directions around the frozen backbone, and (ii) the mean and variance of per-example gradients across members capture non-degenerate epistemic uncertainty, thereby justifying our use of these statistics for data valuation. 3.2 Practical Considerations In practice, we set the ensemble size to = 5. As shown in Figure 1, the trajectories of the LoRA ensemble members change substantially between epochs 1 and 2, and then remain relatively stable afterwards. Motivated by this observation, we set the terminal epoch to be = 2 when computing gradient statistics, which further reduces computational cost while preserving the essential trainingdynamics signal. Figure 2: Overview of our gradient-based data selection pipeline. Phase 1 trains set of LoRA-ensemble members that share the same frozen backbone θ0 on the full dataset D, producing multiple adapters {θ(m) }M m=1 after epochs. Phase 2 collects per-example gradients respect to θ during training process at different epochs, yielding gradient profiles {g(m,e) }M m=1. Phase 3 calculates an uncertainty-aware utility score based on these profiles by combining (i) an information-gain term Gi = G(s) G(t) that measures gradient drop from early to later training, (cid:1) that captures ensemble variability, via an SNR-style form (cid:0)g(m,t) and (ii) disagreement term (t) 2 ui Gi 1 . Phase 4 selects the top-ranked subset based on the utility score ui for training the (t) G(s) +ϵ +ϵ targeted model. = Var i Ensemble disagreement via gradient variance. To quantify how consistently the ensemble reacts to each example, we measure the variance of gradient norms across ensemble members at each epoch, (e) = 1 (cid:88) (cid:13) (cid:13)g(m,e) m=1 (cid:32) (cid:13) 2 2 (cid:13) 1 (cid:33) (cid:88) (cid:13) (cid:13)g(m,e) (cid:13) (cid:13)2 m=1 (4) (e) serves as stability indicator: low values ini dicate that ensemble members agree on the update magnitude for (xi, yi), while high values signals persistent disagreement and potential ambiguity or noise. = 1 and G(m,t) m=1 G(m,s) time points: an early epoch and later epoch (s < t). Let G(m,s) denote the gradient norm of sample under ensemble member at epochs and t, and we define the ensem- (cid:80)M ble means G(s) = (cid:80)M m=1 G(m,t) 1 , as well as the later-stage varii (cid:80)M ance (t) = 1 raw drop = G(s) G(t) captures how much the samples gradient magnitude decreases as training proceeds: intuitively, examples that induce stable and effective descent tend to exhibit clearer reduction. (cid:1)2 (cid:0)G(t) (cid:0)G(m,t) and G(t) . The m=1 (cid:1)2 4.3 Gradient Signal-to-Noise Ratio (G-SNR) for Data Selection We propose Gradient Signal-to-Noise Ratio (GSNR) utility to select high-quality training examples from large candidate pool. The key motivation is global optimization view: because all samples share parameters, the utility of data point should reflect not only its local training signal but also how consistently this signal aligns with the overall descent direction across different optimization trajectories. Gradient-drop signal. We measure an examples learning progress using gradient-drop signal computed between an early training stage and later stage. Concretely, we train an ensemble of lightweight adapters (e.g., LoRA) and record per-example gradient-norm statistics at two Uncertainty normalization (G-SNR). To account for uncertainty in the gradient signal, we down-weight examples whose gradient statistics are highly variable across ensemble members. Our G-SNR utility is uG-SNR = G(s) G(t) G(s) + ϵ 1 (t) + ϵ , (5) where ϵ is small constant for numerical stability. The first factor is relative gradient drop that normalizes out scale effects (so that large-gradient examples are not trivially favored), while the second factor penalizes high-variance, unreliable signals by down-weighting examples whose late-stage gradients disagree across ensemble members. In this sense, G-SNR behaves like signal-to-noise ratio: it prefers large, consistent gradient drops (signal) 5 while suppressing examples with high ensemble disagreement (noise). }N Selection protocol. Given utilities {uG-SNR i=1, we select the top-α fraction of examples as the training subset. We use the same computation and selection procedure across datasets and base models, making G-SNR objective-agnostic: it only assumes access to per-example gradients under parametric loss, without relying on specific instruction template or external proxy score."
        },
        {
            "title": "5 Experiments",
            "content": "Overview. We empirically evaluate GRADFILTERING as gradient-based data selection method for instruction tuning along three axes. First, on small subsets (515%), we ask whether models fine-tuned on GRADFILTERING-selected data match or outperform random splits and the strong baseline Superfiltering in pairwise preference against full-data models under identical training setups. Second, we ablate alternative gradient-based utilities to examine their behavior and confirm that our uncertainty-aware, normalized G-SNR formulation is the most robust across datasets, base models, and adaptation regimes. Third, from an optimization standpoint, we compare training-loss trajectories and convergence, showing that GRADFILTERING-selected subsets yield faster convergence than competitive filtering baselines under the same compute budget. 5.1 Experiments Setup Datasets. We train all models on two standard instruction-tuning corpora, Alpaca (Taori et al., 2023) and Alpaca-GPT4 (Peng et al., 2023), each containing 52,000 instructionresponse pairs. For evaluation, we follow common practice and use the WizardLM evaluation set (Xu et al., 2023) (218 instructions) and the Vicuna evaluation set (Zheng et al., 2023) (80 instructions), both consisting of open-ended instruction-following prompts designed for pairwise preference judgments. Implementation details. We instantiate GRADFILTERINGon two backbone models from the LLaMA-2 family (Touvron et al., 2023): LLaMA2-7B and LLaMA-2-13B. For each backbone, we consider both LoRA-based adaptation (low-rank adapters on frozen backbone) and full-parameter fine-tuning, and we keep the training protocol (data, number of epochs, and optimization settings) fixed across different data selection strategies to ensure fair comparison. All models are optimized with Adam (Kingma, 2014), using learning rate of 2 105 for LLaMA-2-7B and 1 105 for LLaMA-2-13B. For the GPT-2 proxy used to compute G-SNR, we apply LoRA with rank r=8 and α=16, train with learning rate of 5 105, and record per-example gradients at epochs 1 and 2 as the initial and terminal references, respectively, as justified in Section 3.2. Evaluation setting. We adopt the LLM-as-ajudge paradigm to compare, in pairwise manner, the model fine-tuned on selected data against the full-data model, using the prompt template from Vicuna (Zheng et al., 2023), detailed in Appendix A.1. For evaluation, we use GPT-5.1 and Qwen3-235BInstruct as our proprietary and open-source judge models, respectively, and report scores averaged over both; the open-source judge is included to facilitate replication in case proprietary APIs become unavailable in the future. We further validate that LLM-judge preferences with five human annotators. 5.2 Main Results Table 1 reports Pairwise Winning Scores (PWS) (Li et al., 2024) for Alpaca and Alpaca-GPT4 across backbones (7B/13B), subset ratios (5%/10%/15%), and adaptation regimes (LoRA/Full). Overall, GRADFILTERING consistently outperforms random subsets and is better or comparable with current SOTA data filtering baseline Superfiltering (Li et al., 2024). Comparison to Random. Random subset selection is frequently below the full-data baseline (PWS< 1), especially at smaller ratios. In contrast, GRADFILTERING is much more robust: in many settings, models trained on only 5%15% of the data match or exceed the full-data counterpart, indicating that the selected examples preserve high training signal. Comparison to Superfiltering. Superfiltering is strong baseline and can be favorable in some configurations. However, GRADFILTERING provides more consistent gains across backbones and regimes, with particularly strong improvements in several 13B settings and in full-parameter finetuning where optimization dynamics and sample interactions are more pronounced. These results support the central hypothesis of GRADFILTER6 Dataset / Base Model Alpaca LLaMA2-7B Alpaca LLaMA2-13B Alpaca-GPT4 LLaMA2-7B Alpaca-GPT4 LLaMA2-13B 5% 10% 15% 100% 5% 10% 15% 100% 5% 10% 15% 100% 5% 10% 15% 100% Random Split (LoRA) 0.97 0.93 0.92 Superfiltering (LoRA) 1.02 1.10 1.02 1.01 1.07 1.06 Ours (LoRA) 1.00 0.96 0.99 0.96 1.00 1.18 1.10 1.11 1.00 1.19 1.20 1.12 1.00 0.91 0.89 0.98 1.00 0.90 0.91 0.91 1.00 0.94 1.12 1.13 1.00 0.98 1.01 1.00 1.00 1.04 1.02 1.15 1.00 1.05 1.03 1.10 Random Split (Full) Superfiltering (Full) Ours (Full) 0.96 0.95 0.98 1.00 0.94 0.98 0.94 1.00 0.92 0.97 0.99 1.00 0.94 0.93 0.97 1.00 1.15 1.04 1.03 1.00 1.00 1.12 1.15 1.05 1.08 1.11 1.00 1.01 1.06 1.05 1.00 1.08 1.13 1.01 1.16 1.16 1.12 1.00 1.01 1.09 1.05 1.00 1.09 1.10 1.11 1.00 1.00 1. 1.00 1.00 1.00 Table 1: Pairwise winning scores for instruction tuning. Columns group results for Alpaca and Alpaca-GPT4 fine-tuning on LLaMA2-7B and LLaMA2-13B, each using 5%, 10%, 15%, or 100% of the corresponding training set. Rows compare three data selection strategies (Random Split, Superfiltering, and Ours) under two adaptation regimes: LoRA (low-rank adapters on frozen backbone) and Full (full-parameter fine-tuning). The Pairwise Winning Score (PWS) is computed as 1 + (cid:0)Num(Win) Num(Lose)(cid:1)/Num(All). The consistent improvements across the evaluation benchmarks demonstrate the effectiveness of GRADFILTERING. ING: incorporating global training dynamics and gradient uncertainty yields stronger signal for selecting effective instruction-tuning examples than single-score heuristics alone. 5.3 Human Evaluation We compare LLaMA2-13B (full fine-tuning) trained on 10% data against the 100% baseline for Alpaca and Alpaca-GPT4, using 100 prompts sampled from WizardLM(Xu et al., 2023) and Vicuna(Zheng et al., 2023), with the criteria same as the previous pair-wise evaluation, i.e. Helpfulness, Relevance, Accuracy, and Level of Detail. The win/tie/lose counts are 44/19/37 (Alpaca) and 49/8/43 (Alpaca-GPT4), consistent with the LLMjudge trend, supporting that Table 1 reflects perceived quality. 5.4 Ablation Study Table 2 ablates the design of the gradient-based utility by replacing our default G-SNR (Eq. 5) with three simpler variants, and reports the resulting gain or drop in pairwise winning score relative to the GRADFILTERING results in Table 1. Why normalization and uncertainty matter. Across datasets, model sizes, and adaptation regimes, all three alternatives typically yield negative deltas, indicating that none of them matches the robustness of G-SNR. Using only the raw gradient drop (G1 GT , Alternative #1) performs worst overall, suggesting that absolute scale alone is not reliable indicator of data utility. Normalizing by the initial gradient magnitude (Alternative #2) or by the late-stage variance (Alternative #3) mitigates some of this instability but still consistently underperforms the full G-SNR formulation. These (a) Full-parameter. (b) LoRA. Figure 3: Training loss for LLaMA-2-13B on Alpaca with 10% of the data. Left: full-parameter fine-tuning; right: LoRA-based fine-tuning. In both settings, we compare convergence speed and final training loss between our selection (ours-10%) and Superfiltering-10%. trends support our design choice: combining relative gradient-drop term with an uncertainty-based penalty provides more stable and informative signal than either component in isolation. Negative utilities. Because G1 GT can be negative, G-SNR and its variants can also produce negative utilities. In practice, this simply pushes such examples toward the bottom of the ranking; under our top-α selection rule, only the relative ordering matters. Empirically, downweighting highly negative utilities is desirable, as they often correlate with examples whose gradients do not exhibit consistent improvement over training. 5.5 Convergence Analysis Beyond preference scores, GRADFILTERING is motivated by global optimization view: examples interact through shared parameters, so good subset should drive stable, efficient descent. Figure 3 compares training-loss curves for Superfiltering and GRADFILTERING under identical settings (LLaMA-2-13B, 10% Alpaca as examples), for 7 Utility / Base Model Alpaca LLaMA2-7B Alpaca LLaMA2-13B Alpaca-GPT4 LLaMA2-7B Alpaca-GPT4 LLaMA2-13B 5% 10% 15% 5% 10% 15% 5% 10% 15% 5% 10% 15% Utility Alternative #1 (LoRA) Utility Alternative #2 (LoRA) Utility Alternative #3 (LoRA) -0.04 -0.19 -0.21 -0.04 +0.04 -0.03 -0.44 -0.33 -0.32 -0.52 -0.23 -0.23 -0.07 -0.14 -0.17 -0.06 -0.20 -0.14 -0.37 -0.29 -0.33 -0.29 -0.11 -0.21 -0.09 +0.01 -0.16 -0.05 -0.11 -0.10 -0.35 -0.19 -0.34 -0.23 -0.14 -0. Utility Alternative #1 (Full) Utility Alternative #2 (Full) Utility Alternative #3 (Full) -0.31 -0.31 -0.22 -0.04 +0.00 -0.18 -0.41 -0.37 -0.20 -0.22 -0.14 -0.02 -0.23 -0.23 -0.13 -0.07 -0.13 -0.22 -0.18 -0.34 -0.28 -0.15 -0.25 -0.18 -0.16 -0.11 -0.02 -0.02 -0.10 -0.13 -0.10 -0.13 -0.41 -0.08 -0.20 -0.23 Table 2: Ablation on gradient-based utility functions for data selection. Columns follow the same configurations as Table 1 (Alpaca / Alpaca-GPT4 on LLaMA2-7B and LLaMA2-13B with 5%, 10%, and 15% of the training set). Rows compare three utility alternatives under LoRA and Full, where entries report the gain/drop in pairwise winning score relative to our default utility in Table 1. Utility Alternative #1 uses G1 GT , Alternative #2 uses (G1GT )/(G1+ε), Alternative #3 uses (G1GT )/(VT +ε), and our G-SNR is ((G1GT )/(G1+ε))(1/(VT +ε)) (See Equation 5). both full fine-tuning and LoRA. In both regimes, GRADFILTERING converges faster and reaches lower loss earlier, suggesting our uncertaintyaware, gradient-based criterion better matches the global training dynamics than optimizing single proxy. While GRADFILTERING could be suspected to favor easy data, Table 1 shows consistent preference gains over strong baselines, indicating improved response quality rather than trivial bias."
        },
        {
            "title": "6 Further Discussion",
            "content": "What does G-SNR capture? Our experiments suggest that G-SNR is useful proxy for data utility in instruction tuning: examples with large, consistent gradient drops under the LoRA ensemble tend to yield stronger downstream instruction-following performance than those with small or noisy gradient changes. Conceptually, G-SNR can be viewed as gradient-based analogue of dataset cartography: instead of tracking per-example loss trajectories, it summarizes how strongly an example pulls the model in parameter space between early and late training, and how stable this effect is across ensemble members. it Objective-agnostic and model-agnostic. Because G-SNR is computed purely from perexample gradients of parametric loss, is objective-agnostic: it does not require task-specific reward models, preference labels, or handcrafted difficulty scores, and can in principle be applied to any training objective for which gradients are available. Likewise, GRADFILTERING is largely modelagnostic: beyond requiring differentiable backbone and parameter-efficient adaptation mechanism (here, LoRA on LLaMA-2 with GPT-2 proxy), the framework does not assume particular architecture or instruction format. Due to computational constraints and page limits, we instantiate and evaluate GRADFILTERING only on supervised instruction tuning, but we argue that the same GSNR principle can naturally extend to other objectives (e.g., multi-task mixtures), which we leave for future work."
        },
        {
            "title": "7 Conclusion",
            "content": "We presented GRADFILTERING, gradient-based data selection framework for instruction tuning. The method fine-tunes small GPT-2 proxy with LoRA ensemble, records per-example adapter gradients during training, and aggregates them into Gradient Signal-to-Noise Ratio (G-SNR) utility that combines relative gradient drop with lateepoch gradient variance. Because it is defined purely in terms of per-example gradients, G-SNR is objective-agnostic and can, in principle, be paired with wide range of training objectives. Empirically, GRADFILTERING delivers strong and consistent gains. On Alpaca and Alpaca-GPT4 with LLaMA-2-7B/13B under both LoRA and fullparameter fine-tuning, models trained on GRADFILTERING-selected 515% subsets match or outperform random splits and the strong Superfiltering baseline in most LLM-as-a-judge settings, with small human study confirming the same preference trends. Moreover, GRADFILTERING-selected subsets converge faster and reach lower training loss than competitive filters under the same compute budget, supporting our view that uncertainty-aware gradient statistics provide an effective signal for curating large instruction-tuning corpora."
        },
        {
            "title": "Limitations",
            "content": "Despite its empirical robustness, GRADFILTERING has several limitations. First, it operates on gradient norms and their variance, ignoring gradient direction; examples that are locally uninformative in norm but crucial for aligning with rare or long-horizon behaviors may be under-valued. Second, G-SNR relies on specific early/late snapshot scheme and modest ensemble size (M = 5); although our t-SNE analyses indicate that these settings already induce diverse trajectories, different training schedules or proxies could change the behavior. Third, the proxy model must still be fine-tuned with backpropagation, which is cheaper than running strong teacher or constructing full gradient datastore, but not free. Finally, like other training-dynamics methods, G-SNR assumes that useful examples look useful early in training; regimes with delayed credit assignment or strong curriculum effects may violate this assumption."
        },
        {
            "title": "References",
            "content": "Alexander Amini, Wilko Schwarting, Ava Soleimany, and Daniela Rus. 2020. Deep evidential regression. Advances in neural information processing systems, 33:1492714937. Oleksandr Balabanov and Hampus Linander. 2024. Uncertainty quantification in fine-tuned llms using lora ensembles. arXiv preprint arXiv:2402.12264. Yarin Gal and Zoubin Ghahramani. 2016. Dropout as bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 10501059. PMLR. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, and 1 others. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3. Diederik Kingma. 2014. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980. Dongyue Li, Ziniu Zhang, Lu Wang, and Hongyang Zhang. 2025. Efficient ensemble for fine-tuning language models on multiple datasets. arXiv preprint arXiv:2505.21930. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. 2024. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. arXiv preprint arXiv:2402.00530. Dominik Mühlematter, Michelle Halbheer, Alexander Becker, Dominik Narnhofer, Helge Aasen, Konrad Schindler, and Mehmet Ozgur Turkoglu. 2024. Lora-ensemble: Efficient uncertainty modelling for self-attention networks. arXiv preprint arXiv:2405.14438. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277. Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. 2020. Estimating training data influence by tracing gradient descent. Advances in Neural Information Processing Systems, 33:1992019930. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and 1 others. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers), pages 1348413508. Pang Wei Koh and Percy Liang. 2017. Understanding In black-box predictions via influence functions. International conference on machine learning, pages 18851894. PMLR. Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024. Less: Selecting influential data for targeted instruction tuning. arXiv preprint arXiv:2402.04333. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. 9 Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. 2024. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, and 1 others. 2023. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Prompt for Evaluation We follow the LLM-as-a-judge protocol described in the main text, using fixed, symmetric prompt to compare responses from two candidate models on the same instruction. For each evaluation example, the judge model (GPT-5.1 or Qwen3-235BInstruct) is adopted. We adopt the prompt template from Vicuna (Zheng et al., 2023), with only minimal renaming of model identifiers."
        },
        {
            "title": "Prompt for Performance Evaluation",
            "content": "System Prompt You are helpful and precise assistant for checking the quality of the answer. User Prompt [Question] Question [The Start of Assistant 1s Answer] Answer 1 [The End of Assistant 1s Answer] [The Start of Assistant 2s Answer] Answer 2 [The End of Assistant 2s Answer] We would like to request your feedback on the performance of two AI assistants in response to the user question displayed above. Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on scale of 1 to 10, where higher score indicates better overall performance. Please first output single line containing only two values indicating the scores for Assistant 1 and 2, respectively. The two scores are separated by space. In the subsequent line, please provide comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment. Table 3: Prompt template used for LLM-as-a-judge evaluation. The judge model sees the instruction and two anonymized candidate responses (Model and Model B) and is asked to output preference label (A, B, or Tie) together with brief explanation. To avoid ordering bias, we shuffle the order of and B."
        }
    ],
    "affiliations": [
        "Alibaba Cloud Computing",
        "The University of Edinburgh"
    ]
}