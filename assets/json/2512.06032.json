{
    "paper_title": "The SAM2-to-SAM3 Gap in the Segment Anything Model Family: Why Prompt-Based Expertise Fails in Concept-Driven Image Segmentation",
    "authors": [
        "Ranjan Sapkota",
        "Konstantinos I. Roumeliotis",
        "Manoj Karkee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces a unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) a Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as a new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era."
        },
        {
            "title": "Start",
            "content": "THE SAM2-TO-SAM3 GAP IN THE SEGMENT ANYTHING MODEL FAMILY: WHY PROMPT-BASED EXPERTISE FAILS IN CONCEPT-DRIVEN IMAGE SEGMENTATION 5 2 0 2 4 ] . [ 1 2 3 0 6 0 . 2 1 5 2 : r Ranjan Sapkota1* Konstantinos I. Roumeliotis2 Manoj Karkee1 1Cornell University, Ithaca, NY 14850, USA 2University of the Peloponnese, Tripoli 22131, Greece December 9,"
        },
        {
            "title": "ABSTRACT",
            "content": "This paper investigates the fundamental discontinuity between the latest two Segment Anything Models: SAM2 and SAM3. We explain why the expertise in prompt-based segmentation of SAM2 does not transfer to the multimodal concept-driven paradigm of SAM3. SAM2 operates through spatial prompts points, boxes, and masks yielding purely geometric and temporal segmentation. In contrast, SAM3 introduces unified vision-language architecture capable of open-vocabulary reasoning, semantic grounding, contrastive alignment, and exemplar-based concept understanding. We structure this analysis through five core components: (1) Conceptual Break Between Prompt-Based and Concept-Based Segmentation, contrasting spatial prompt semantics of SAM2 with multimodal fusion and text-conditioned mask generation of SAM3; (2) Architectural Divergence, detailing pure vision-temporal design of SAM2 versus integration of vision-language encoders, geometry and exemplar encoders, fusion modules, DETR-style decoders, object queries, and ambiguity-handling via Mixture-of-Experts in SAM3; (3) Dataset and Annotation Differences, contrasting SA-V video masks with multimodal concept-annotated corpora of SAM3; (4) Training and Hyperparameter Distinctions, showing why SAM2 optimization knowledge does not apply to SAM3; and (5) Evaluation, Metrics, and Failure Modes, outlining the transition from geometric IoU metrics to semantic, open-vocabulary evaluation. Together, these analyses establish SAM3 as new class of segmentation foundation model and chart future directions for the emerging concept-driven segmentation era. This project repository is available on GitHub ( Link). Keywords Segment Anything Model SAM3 SAM2 Image Segmentation Object Segmentation"
        },
        {
            "title": "Introduction",
            "content": "Image segmentation, the process of partitioning an image into semantically meaningful regions forms the backbone of modern computer vision and automation systems [1, 2]. At its core, segmentation enables machines to identify, isolate, and interpret objects, surfaces, and boundaries at the pixel level [3, 4, 5], transforming raw visual data into structured representations suitable for downstream reasoning and decision-making [6, 7, 8]. Likewise, object segmentation, specialized form of segmentation, goes beyond detecting the presence of objects: it delineates precise contours and regions of interest, enabling fine-grained analysis of shapes, attributes, textures, and spatial relationships [9, 10]. Because segmentation provides pixel-level understanding, it is indispensable across numerous industries where accuracy, reliability, and contextual awareness are essential [11]. Applications spans many sectors: medical and biomedical imaging (tumor and organ segmentation) [12, 13, 14, 15], autonomous driving and intelligent transportation (lane detection, vehicle and pedestrian segmentation) [16, 17, 18, 19], agriculture and environmental monitoring (plant organs, weeds, fruitlets, soil health) [20, 21, 22, 23], manufacturing and industrial inspection (defect detection, quality control) [24, 25, 26, 27], robotics and automation (object grasping, scene parsing) [28, 29, 30, 31], satellite and remote sensing (land cover mapping, infrastructure monitoring) [32, 33, 34, 35], augmented SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 and virtual reality (scene reconstruction) [36, 37, 36], and security and surveillance (object tracking and anomaly detection) [38, 39, 40]. In each domain, segmentation acts as the interpretive layer that bridges perception and action, enabling tasks such as diagnosis, navigation, manipulation, planning, measurement, and anomaly detection [7]. Its importance continues to grow as visual systems shift from handcrafted pipelines toward foundation models that demand massive annotated datasets and semantic consistency [41, 42]. Within this landscape, advances in segmentation architectures directly influence the reliability, generalizability, and autonomy of AI-driven systems. Over the past several decades, object segmentation has evolved through multiple generations of computer vision algorithms, beginning with classical methods such as thresholding [43, 44], region growing [45, 46], active contours (Snakes) [47, 48], graph cuts [47, 49, 50], watershed segmentation [51, 52, 53, 54], and Markov Random Fields (MRFs) [55, 56, 57, 58]. With the rise of deep learning, convolutional neural networks (CNNs) transformed segmentation accuracy through architectures like Fully Convolutional Networks (FCN) [59], U-Net [60], Mask R-CNN [61, 62], DeepLab (v1v3+) [63, 64], PSPNet [65], and SegNet [66], each introducing advances in multiscale feature extraction, contextual aggregation, and instance-level reasoning. More recent segmentation approaches incorporated transformer-based architectures (e.g., DETR [67, 68, 69], SegFormer [70], Mask2Former [71]), advanced attention mechanisms (multi-head attention [72], cross-attention [73], deformable attention [74]), and self-supervised learning strategies such as MAE(Masked Autoencoders [75]) BYOL Bootstrap Your Own Latent [76]) and SimCLR(A Simple Framework for Contrastive Learning of Visual Representations [77]). Despite these advancements, traditional segmentation models typically require task-specific training, domain adaptation, or finely tuned architectures to generalize effectively [78, 79, 80]. The Segment Anything Model (SAM) family breaks from this paradigm by offering promptable, zero-shot segmentation at foundation scale, enabling masks to be generated without retraining and across domains unseen during training. This shift marks fundamental transition from model-centric segmentation pipelines to universal, prompt-driven segmentation frameworks. The Segment Anything family (SAM) reshaped modern segmentation by introducing prompt-driven, general-purpose vision models capable of zero-shot mask generation across diverse domains [81]. When SAM1 was released in 2023 as depicted in Figure 1, it established new paradigm in which segmentation could be initiated using simple visual cues such as points, boxes, or masks [81]. SAM1s innovation lay in its large ViT-based encoder and geometric prompt fusion mechanism, enabling rapid mask extraction without task-specific training. Building on this, SAM2 extended the framework into video via temporal memory structures and consistent mask propagation [82]. However, the release of SAM3 fundamentally altered the paradigm by introducing concept-driven multimodal segmentation creating discontinuity that renders SAM2 experience insufficient for SAM3 fine-tuning or deployment [83]. Figure 1 illustrates the evolutionary timeline of SAM and their capabilities. SAM1 (2023) Promptable Image Segmentation Zero-Shot Transfer on Images Interactive Annotation Tools Video Segmentation & Tracking Concept/Text-Based Segmentation SAM2 (2024) Promptable Video Segmentation Online Object Tracking Temporal Memory & Mask Propagation Open-Vocabulary Concept Segmentation Text-Only Prompts SAM3 (2025) Concept-Level Segmentation Open-Vocabulary Text Prompts Multimodal VisionLanguage Fusion Example-Based Concept Learning Automatic Instance Discovery Figure 1: Timeline of Segment Anything models (SAM1 to SAM3) and their core capabilities. Solid boxes denote natively supported functionalities; dashed boxes denote capabilities that are unsupported or only achievable via external components. 2 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL."
        },
        {
            "title": "2 Conceptual Break Between Prompt-Based and Concept-Based Segmentation",
            "content": "The transition from SAM2 to SAM3 represents one of the most substantial paradigm shifts in segmentation research since the emergence of prompt-based foundation models [83]. As illustrated in Figure 2a, SAM2 extends the original SAM1 framework into real-time video segmentation by incorporating temporal memory mechanism; however, the model remains entirely dependent on spatial visual prompts. Users must still provide points, bounding boxes, or initial mask priors to specify the target region [82], reinforcing SAM2s reliance on geometric guidance rather than semantic understanding. This limitation is further exemplified in Figure 2c, where SAM2 generates segmentation masks for orchard apples only after receiving explicit spatial cues. In contrast, SAM3 as shown in Figure 2b introduces multimodal vision-language detector capable of open-vocabulary reasoning, enabling segmentation driven by textual concepts rather than manual geometric prompts. Figure 2d demonstrates this advancement: SAM3 autonomously identifies and segments all concept-relevant apple instances from natural-language prompt, illustrating fundamental shift from prompt-based object localization to concept-driven semantic interpretation. This transition from SAM2 to SAM3 represents one of the most substantial paradigm shifts in segmentation foundations since the introduction of prompt-based segmentation [83]. Although SAM2 expanded the original Segment Anything Model (SAM1) framework into real-time video segmentation through temporal memory, it still relied solely on visual prompting: users provided points, boxes, or mask priors to indicate what to segment [82]. In contrast, SAM3 introduced unified multimodal framework capable of interpreting text, concepts, and example images as segmentation instructions [82, 83]. This shift unlocked open-vocabulary segmentation and semantic reasoning beyond explicit human clicks. Understanding this difference is crucial for any researcher training or deploying SAM3, particularly those familiar only with SAM2s prompt-driven paradigm. 2.1 Prompt Semantics in SAM2: Spatial but Not Semantic SAM2 requires explicit visual grounding before segmentation [84]. The user must indicate where an object resides through spatial prompts, typically by adding point on the target, drawing bounding box around the region of interest, or providing an initial mask that SAM2 refines [85, 86, 87]. The model interprets these cues purely as geometric signals. No internal semantic reasoning is performed [88, 89]; For example in orchard images, SAM2 does not inherently know what an apple, leaf, branch, blossom, or fruitlet is. It instead refines the visible structure around the prompted region based on learned visual patterns and segmentation priors [90, 91]. This makes SAM2 highly effective for interactive editing, video object tracking, and manual annotation workflows, where human or external system provides specific locations. However, the lack of semantic grounding limits its applicability in automated systems that require concept-level understanding [92, 83], such as orchard fruit load estimation, defect detection, or selective harvesting, where objects and attributes must be discovered without manual clicks. Figure 2 illustrates the fundamental conceptual and architectural transition from prompt-driven segmentation in SAM2 to concept-driven multimodal reasoning in SAM3. In Figure 2a, the SAM2 architecture is shown as strictly visiontemporal pipeline in which segmentation predictions depend on the current visual prompt and previously stored memory embeddings. Each frame is streamed through an image encoder, cross-attended with memory features from earlier frames, and then passed to mask decoder that refines the segmentation for that frame. Importantly, as the figure highlights, SAM2s prompt encoder accepts only spatial cues-points, boxes, or initial masks and the model operates entirely within geometric interpretation space. This design enables highly stable temporal mask propagation but precludes any understanding of object semantics or categories. Figure 2b contrasts this with SAM3s expanded architecture, where new detector module (highlighted in yellow in the original diagram) incorporates text embeddings produced by language encoder. The detector introduces open-vocabulary grounding, enabling the system to identify and segment concepts (penguin in the illustration), while the tracker fuses example-based visual prompts with the multimodal representations. SAM3s memory bank now stores semantically enriched tokens, allowing concept-level segmentation even when objects undergo appearance changes. This architectural shift from pure vision processing to vision-language fusion represents the core break that invalidates direct transfer of SAM2 expertise to SAM3. Figures 2c and 2d provide concrete orchard-image examples that further clarify the conceptual distinction. In Figure 2c, SAM2 segments an apple only when explicitly prompted with bounding box, demonstrating no semantic understanding: the system cannot interpret whether the region contains an apple, leaf, blossom, or fruitlet. The output mask is simply geometric refinement of the users spatial selection. In contrast, Figure 2d shows SAM3 segmenting all ripe apples from text prompt (ripe apples). Because SAM3 fuses image and text embeddings, semantic reasoning becomes 3 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 Figure 2: (a) SAM2 architecture: prompt-driven visiontemporal pipeline where segmentation depends on spatial prompts and memory retrieval across frames ; (b) SAM3 architecture: multimodal vision-language system with new detector enabling open-vocabulary, concept-level segmentation ;(c) SAM2 orchard workflow showing prompt-based apple segmentation without semantic understanding ; and (d) SAM3 workflow demonstrating text-prompted, conceptaware segmentation, identifying all relevant apple instances through multimodal fusion. available, enabling the model to differentiate between apples and surrounding foliage and to detect all concept-relevant instances automatically. 2.2 Concept Semantics in SAM3: Multimodal Fusion and Open-Vocabulary Reasoning SAM3 accepts textual prompts such as segment all ripe apples and optionally accepts example images as concept demonstrations. These instructions are interpreted semantically rather than spatially, requiring the model to align visual evidence with linguistic embeddings. SAM3 learns shared representation space in which visual features and text embeddings interact, enabling concept-level detection and segmentation. Instead of waiting for explicit human localization, SAM3 autonomously searches the image for features that match the textual description. This transforms segmentation from spatial interaction problem into semantic inference problem. Performance now depends on multilingual embeddings, concept alignment quality, and vision language fusion depth, rather than only on the quality 4 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. of spatial prompts. As result, expertise that is rooted in click strategies, bounding-box design, or temporal prompt propagation in SAM2 does not transfer directly when training SAM3, which operates as concept-driven multimodal segmentation system. Figure 3 illustrates the fundamental conceptual divergence between SAM2 and SAM3, highlighting why expertise in prompt-based spatial segmentation does not transfer naturally to concept-driven multimodal segmentation. As shown in the upper portion of Figure 3, SAM2 follows purely vision-centric workflow in which an image or video is processed only after the user supplies explicit spatial prompts such as points, boxes, or mask priors. This pipeline emphasizes geometric localization and temporal memory, enabling SAM2 to track and refine objects selected by the user but preventing it from understanding semantic categories or attributes. In contrast, the lower branch demonstrates how SAM3 integrates text prompts, optional example images, and unified vision-language fusion module to infer conceptlevel masks for all relevant instances in the scene. This multimodal design transforms segmentation from spatial interaction task into semantic reasoning task, enabling SAM3 to perform open-vocabulary and attribute-sensitive segmentation without manual localization. SAM2: Spatial Prompt Pipeline SAM3: Conceptual Multimodal Pipeline Input Image / Video Input Image Visual Prompt (Point/Box/Mask) Vision Encoder Text Prompt (e.g., Ripe Apples) Example Images (Optional) Temporal Memory VisionLanguage Fusion Mask Decoder Prompted Object Mask Concept-Conditioned Mask Decoder Concept-Level Masks (All Matching Instances) Figure 3: Compact comparison of SAM2 and SAM3 segmentation workflows. SAM2 follows purely vision-based, spatially prompted pipeline, whereas SAM3 integrates text prompts, example images, and vision-language fusion to produce open-vocabulary, concept-level masks."
        },
        {
            "title": "3 Architectural Divergence Undermining Knowledge Transfer",
            "content": "Figure 4 provides unified conceptual overview of why knowledge and training intuition gained from SAM2 cannot be directly applied to SAM3, highlighting five scientific dimensions that fundamentally separate the two models. As illustrated in the mindmap, the first major divergence arises from the prompt modalities: SAM2 relies exclusively on visual prompts such as clicks or bounding boxes, whereas SAM3 interprets natural-language descriptions and concept exemplars, requiring semantic grounding rather than spatial anchoring. The second branch emphasizes the architectural break, where SAM2 is limited to vision-plus-temporal-memory pipeline, while SAM3 incorporates full vision-language fusion mechanism capable of open-vocabulary reasoning. The third distinction involves training objectives; SAM2 optimizes mask quality and temporal stability, whereas SAM3 introduces contrastive, multimodal alignment, and concept-grounding losses. The mindmap also highlights dataset asymmetry: SAM2 trains on large-scale video mask datasets like SA-V, while SAM3 requires multimodal concept annotations linking text and pixel-level regions. Finally, evaluation metrics diverge sharply SAM2 is assessed through spatial and temporal consistency, while SAM3 requires semantic, attribute-based, and open-vocabulary evaluation. Together, Figure 4 demonstrates that the SAM2-to-SAM3 gap is not incremental but structural, demanding entirely different expertise for effective training and deployment. 5 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 Spatial & Temporal Semantic & OpenVocab Evaluation Metrics SAM2: Visual (Click/Box) Different Prompt Types SAM3: Text & Concepts Concept Datasets (Multimodal) Video Masks (SA-V) Dataset Differences SAM2 SAM3 Non-Transferability Training Objectives Architectural Break SAM3: VisionLanguage Fusion SAM2: Vision + Memory SAM3: Contrastive + Grounding SAM2: Mask + Temporal Figure 4: Mindmap summarizing the scientific reasons why expertise in SAM2 does not transfer to SAM3. The gap arises from differences in prompting, architecture, training objectives, datasets, and evaluation metrics. 3.1 SAM2: Pure Vision-Temporal Architecture SAM2 extends SAM1 by introducing temporal key value memory that supports long-sequence video reasoning and maintains object identity across frames. It combines ViT-based visual tokens with memory retrieval pathway to propagate masks over time. The learning paradigm optimizes mask consistency, spatial coherence, and temporal stability. Architecturally, SAM2 is still purely vision-based. It lacks text encoder, has no multimodal fusion layer, and does not maintain any concept embedding pipeline or semantic grounding mechanism. There is no open-vocabulary classification head. All training heuristics, including learning rate schedules, loss shaping, prompt augmentation, and temporal smoothing, are constrained to geometric and temporal signals. This narrow focus means that SAM2 expertise primarily concerns how to design prompts, how to stabilize masks across frames, and how to manage memory capacity and latency. 3.2 SAM3: Unified Multimodal Vision Language System SAM3 introduces unified multimodal architecture as illustrated in Figure 5 that integrates large-scale text encoder, semantic alignment module, and concept-conditioned mask decoder with the vision backbone. In typical imple6 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 mentations, transformer-based image encoder produces visual tokens, while separate language model, such as LLaMAor Qwen-family encoder, generates text embeddings for prompts. multimodal alignment layer fuses these two token streams through cross-attention, enabling the model to associate phrases like ripe apple or diseased leaf with specific visual patterns. Training SAM3 requires optimizing joint objectives: segmentation mask loss, cross-modal embedding loss, contrastive concept alignment loss, and semantic grounding consistency. These losses are fundamentally different from SAM2s temporal tracking objectives. Knowledge of prompt interactions, memory behavior, and click-based refinement in SAM2 offers little advantage when working with SAM3, which is dominated by the challenges of multimodal representation learning and concept-level reasoning. Figure 5: SAM3 architecture, highlighting new multimodal components in yellow, inherited SAM2 [82] modules in blue, and the Perception Encoder [93] in cyan. The model integrates vision, text, geometry, and exemplar prompts through dual encoderdecoder transformer, enabling concept-level segmentation beyond SAM2s purely promptdriven capabilities. Image sourced from [83] SAM3 introduces fundamentally expanded architecture that generalizes the prompt-driven segmentation paradigm of SAM2 into fully multimodal, concept-aware vision-language system. As illustrated in Figure 5, the architecture integrates three color-coded components: new SAM3 modules highlighted in yellow, inherited SAM2 modules shown in blue, and the Perception Encoder (PE) from Bolya et al. (2025)[93] in cyan reflecting the shift from geometric prompting to semantic concept reasoning. Whereas SAM2 relies on single vision encoder and mask decoder, SAM3 employs dual encoderdecoder transformer architecture that simultaneously processes vision, text, geometric prompts, and exemplar images [83]. VisionLanguage Encoders and PE Backbone. SAM3 uses 450M-parameter vision encoder and 300M-parameter causal text encoder trained on 5.4B imagetext pairs via contrastive learning [83]. The contrastive objective aligns visual embeddings and text embeddings in shared semantic space: exp(v, t/τ ) exp(v, tj/τ ) Lcon = log (Eq. 1) (cid:80) As illustrated in Figure 5, the PE backbone processes each frame once and produces unconditioned tokens for the fusion encoder, unlike SAM2 which repeatedly conditions computation on temporal memory. Geometry and Exemplar Encoder. SAM3 introduces geometry exemplar encoder that generates geometry tokens (for points/boxes) and exemplar tokens (from positive or negative visual examples). These tokens attend to one another and to PE embeddings, enabling concept conditioning even without explicit manual prompts. Fusion Encoder. The fusion encoder forms the core multimodal module: it integrates text tokens, geometry/exemplar tokens, and frame embeddings using six transformer blocks with cross-attention. This module transforms prompt inputs into semantic conditioning signals that guide mask generation, capability fundamentally absent in SAM2s architecture. 7 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 DETR-style Decoder and Object Queries. SAM3 adopts DETR-inspired decoder with = 200 learned object queries. These queries self-attend, cross-attend to conditioned frame embeddings, and employ iterative refinement techniques for improved localization. The probability that query matches concept is factorized as shown in equation 2: p(queryi concept) = p(local match) p(concept present in image), (Eq. 2) introducing presence head that stabilizes semantic classification under ambiguity. Ambiguity Handling with Mixture-of-Experts (MoE). To address semantic ambiguity, SAM3 uses mixture-ofexperts (MoE) segmentation head with winner-takes-all (WTA) optimization (Equation 3): LWTA = Lk , = arg min Lk. (Eq. 3) separate classification head selects the correct expert at inference, enabling SAM3 to choose among competing interpretations of ambiguous prompts something SAM2 cannot do. Why Prompt-Based Expertise Fails for Concept Segmentation. SAM2 expertise does not transfer to SAM3 because SAM2 optimizes only geometric losses (Equation: 4): Lmask = 1 IoU(Mpred, Mgt), Ltemp = Mt Mt12, (Eq. 4) where segmentation is guided strictly by spatial prompts. SAM3 instead optimizes multimodal alignment, semantic grounding, ambiguity resolution, and concept presence estimation. The operational question shifts from Where is the object? to What semantic concept does this region represent?. This transition is architectural, mathematical, and cognitive in nature making SAM2 prompt expertise insufficient for SAM3s concept-driven segmentation paradigm. Figure 6: Scientific overview of the six core reasons SAM2 expertise fails to transfer to SAM3. The infographic highlights differences in optimization objectives, multimodal prompting, semantic embedding spaces, DETR-style decoding, ambiguity modeling, and PE-driven representation learning illustrating the fundamental architectural and conceptual discontinuity between prompt-based and concept-driven segmentation. Summary of Key Scientific Reasons for Non-Transferability : Figure 6 visually summarizes the six core scientific reasons why expertise developed for SAM2 does not transfer to SAM3. The diagram highlights fundamental differences in optimization goals, prompt modalities, representational spaces, decoding mechanisms, ambiguity handling, and reliance on PE semantic embeddings. Together, these distinctions illustrate the architectural and functional discontinuity between the two models and motivate the detailed points discussed below in six points: Different Optimization Targets: SAM2 optimizes geometry; SAM3 optimizes semantics and multimodal alignment. Different Prompt Modalities: SAM2 uses points/boxes; SAM3 uses text, exemplars, and multimodal prompts. 8 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 Different Representational Spaces: SAM2 operates in pixel space; SAM3 aligns vision and language embeddings. Different Decoding Mechanisms: SAM3s DETR-style decoder reasons over concepts rather than object outlines. Ambiguity Modeling: SAM3 uses MoE and presence prediction structures absent in SAM2. PE Backbone Dependence: SAM3 relies on PE-trained embeddings that encode semantics not available to SAM2."
        },
        {
            "title": "4 Dataset and Annotation Differences",
            "content": "4.1 SAM2 and the SA-V Dataset SAM2 is trained primarily on large-scale video datasets with dense segmentation masks. The SA-V (Source Link) dataset is an example of such corpus: it provides millions of video clips paired with pixel-level mask annotations. These masks encode object boundaries and shapes but do not necessarily include rich semantic or attribute-level metadata. No text labels or concept annotations are required. As result, the model learns to optimize spatial consistency, boundary quality, and temporal propagation of objects across frames. SAM2 learns robust objectness and temporal tracking but cannot distinguish between conceptual categories such as ripe versus unripe fruit, healthy versus diseased foliage, or different defect types on manufacturing line. Training procedures developed for SAM2 revolve around improving temporal robustness, handling occlusion, and stabilizing propagation across long sequences rather than mastering semantic distinctions. 4.2 SAM3 and the Multimodal Concept Dataset Requirement SAM3, in contrast to SAM2s purely geometric training paradigm, depends on datasets that explicitly bind textual concepts to visual scenes and pixel-level masks, forming the foundation for its multimodal reasoning capabilities. As summarized in Table 1, SAM3s training corpora must provide not only segmentation masks but also rich linguistic descriptions and concept labels that capture object categories, attributes, states, and contextual semantics. These labels include fine-grained descriptors such as ripe apple, healthy leaf, stem, bruised fruit, or immature blossom, enabling the model to associate nuanced textual tokens with corresponding visual patterns. Because SAM3 learns through cross-modal alignment mapping text embeddings to vision embeddings the dataset must reflect the conceptual variety and semantic ambiguity present in real-world environments. This requirement introduces substantial challenges absent in SAM2. For example, concept ambiguity becomes central difficulty when terms such as ripe, healthy, or damaged vary by domain-specific definitions, measurement standards, or cultural interpretations. ripe apple in one cultivar, orchard system, or lighting condition may differ significantly in visual appearance from another, requiring large, diverse annotations to avoid confounding the models semantic grounding. Furthermore, as Table 1 illustrates, the multimodal datasets used for SAM3 include millions of noun-phrase annotations, hard-negative examples, and cross-domain concept variations spanning agriculture, medical imaging, industrial inspection, robotics, and biological sciences. This diversity is essential to prevent overfitting to narrow visual-linguistic correlations, yet it complicates dataset construction and quality control. Multilingual expressions, synonym sets, polysemous terms, and subtle linguistic nuances further expand the semantic space the model must master. Additionally, SAM3 must learn attribute-level distinctions such as color, texture, material state, or morphological condition that often require human expertise or context-specific annotation protocols. These complexities transform SAM3 training into multimodal alignment and semantic grounding problem rather than purely geometric segmentation task. The model must learn not only where objects are located but what they represent, how they differ, and how textual descriptions relate to those differences. As result, SAM3s training demands significantly more expressive datasets, sophisticated annotation pipelines, and robust concept validation mechanisms, making its data requirements far more intricate and scientifically demanding than those of SAM2."
        },
        {
            "title": "5 Training and Hyperparameter Distinctions",
            "content": "5.1 Why SAM2 Training Knowledge Does Not Apply to SAM3 SAM2 hyperparameters are designed to optimize vision-temporal pipeline. Typical tuning targets include the strength and depth of temporal memory, the configuration of key value embeddings, the size of attention windows, frame sampling strategies, and the resolution of visual prompts and masks [94, 95, 96]. Loss balancing in SAM2 focuses on trade-offs between segmentation accuracy and temporal stability [87, 97], with additional attention to preventing drift 9 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 Table 1: Comprehensive comparison of datasets and annotation pipelines for SAM2[82] and SAM3[83]. SAM2 relies on video mask propagation datasets such as SA-V (Source Link), whereas SAM3 requires large-scale multimodal concept datasets with textmask alignment, as documented in the SA-Co dataset engine (Source Link). Category SAM2 (SA-V Dataset) SAM3 (SA-Co Dataset Family) Primary Data Type Videos with dense pixel masks; no text labels Images + videos paired with nounphrase concepts, masks, and hard negatives Scale (Images) Scale (Videos) Total Masks Not explicitly reported; focus on video masklets 5.2M HQ images with 4M unique noun phrases (SA-Co/HQ) SA-V: millions of video clips with dense masks 52.5K annotated videos, 24.8K unique concepts (SA-Co/VIDEO) Millions of frame-level masks propagated temporally 52M high-quality human+AI-verified masks; 1.4B synthetic masks Concept Labels None (pure geometric masks)"
        },
        {
            "title": "Annotation Source",
            "content": "Annotation Pipeline Human-generated masks + SAM2 propagation Video masklet propagation; temporal refinement 4M unique NPs; 207K benchmark concepts; includes hard negatives Human annotators + AI annotators (LLMs + SAM3 proposals) 4-phase data engine: NP proposal mask proposal mask verification exhaustivity verification correction Semantic Metadata Not included; no attributes, states, or categories Includes attributes (e.g., ripe apple), states, objects, fine-grained categories Ambiguity Handling None; geometry-only Multi-annotator labels, ambiguity module, ontology-based NP curation Modeling Objective Enabled Temporal objectness, mask propagation Open-vocabulary concept segmentation; textvision alignment Benchmark Coverage SA-V videos Example Dataset Stats No explicit concept count; mask-only SA-Co/Gold, Silver, Bronze, Bio, VEval benchmarks 207K unique phrases, 121K media items, 3M mediaphrase pairs Data Diversity Primarily natural videos with object motion 15+ visual domains; curated ontology with 22.4M nodes or fragmentation over long video sequences [98, 99]. These design patterns assume that prompts are visual, supervision is geometric, and evaluation is based on pixel overlap or identity tracking. SAM3 introduces different class of hyperparameters. Trainers must set learning rates for the text encoder and image encoder, adjust the relative weighting of segmentation loss versus cross-modal contrastive loss, tune the temperature parameters in contrastive objectives, and determine the depth at which visual and textual tokens are fused. The model also depends on the scale assigned to semantic grounding losses that penalize misalignment between concepts and visual evidence. Even the shape of optimization schedules changes: freezing or partially freezing the text encoder during early training, staged unfreezing, and curriculum strategies on concept complexity become relevant. None of these design decisions are encountered in SAM2 training. As result, experience in SAM2 fine-tuning, which focuses on temporal memory and spatial prompts, does not translate directly into the multimodal, concept-alignment-centric world of SAM3. 5.2 Divergent Data Augmentation and Optimization Strategies The divergence extends to data augmentation and optimization strategies. SAM2 benefits mainly from geometric augmentations such as random scaling, cropping, flipping, and affine transformations that improve robustness to motion, viewpoint changes, and occlusion [100, 88, 101]. Color jitter and brightness changes are generally helpful but primarily 10 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 in the context of preserving temporal consistency [102]. For SAM2, the key objective is to maintain stable masks and object identities across diverse video conditions [103]. In contrast, SAM3 requires data augmentation strategies that preserve semantic meaning while still improving robustness, creating fundamentally different design landscape from SAM2. As illustrated in Figure 7, transformations that were harmless for SAM2such as strong color jitter, aggressive cropping, or heavy affine distortions can degrade SAM3s ability to interpret attributes like ripeness, damage severity, material type, or structural context. Because SAM3 aligns visual features with linguistic concepts, any augmentation that disrupts color fidelity, texture cues, or global spatial context risks breaking the link between text prompts and visual semantics. This makes concept-preserving augmentation essential for stable training. SAM2: Geometric & Temporal-Stability SAM3: Semantic & Multimodal-Stability 1) Geometric augmentations: random scaling, cropping, flipping, affine transforms robustness to motion, viewpoint, occlusion 1) Semantic-preserving augmentations: careful color/structure changes avoid distorting attributes (e.g., ripeness, damage) 2) Mild photometric changes: color jitter / brightness for temporal consistency primarily to stabilize video masks 2) Context-aware cropping & color: must preserve global scene context (tree canopy, branches) maintain textimage semantic alignment 3) Augmentation objective: maintain stable masks and IDs across frames optimize temporal consistency of object tracks 3) Augmentation objective: preserve concept meaning and attribute semantics support open-vocabulary, concept-level segmentation 4) Optimization strategy: standard mixed-precision training moderate memory footprint from vision-only backbone 4) Optimization strategy: mixed-precision + gradient checkpointing high memory footprint from joint visionlanguage processing 5) Batching/scheduling: simpler batching over video clips schedules tuned for temporal mask losses 5) Batching/scheduling: careful batching over imagetext pairs and concepts schedules balance segmentation and contrastive losses 6) Hyperparameter search: focused on temporal stability and IoU relatively low-dimensional search space 6) Hyperparameter search: involves fusion depth, loss weights, text LR, temperature no direct analogue in SAM2 pipelines Figure 7: Divergent data augmentation and optimization strategies for SAM2[82] and SAM3[83]. SAM2 emphasizes geometric and temporal robustness for video masks, whereas SAM3 requires semantic-preserving augmentations and multimodal optimization tuned for concept-level, open-vocabulary segmentation. Beyond augmentation, SAM3 also introduces new optimization challenges. Multimodal fusion substantially increases memory and compute requirements, making mixed-precision training, gradient checkpointing, and careful microbatching necessary for scalable fine-tuning. Hyperparameters now include contrastive-loss temperatures, cross-modal fusion depth, and segmentationalignment loss weightscomponents absent in SAM2s simpler geometric optimization. As shown again in Figure 7, these multimodal constraints make SAM3 hyperparameter search more complex, non-linear, and highly sensitive to semantic drift. Overall, the augmentation and optimization divergence reinforces the broader architectural discontinuity between prompt-driven SAM2 and concept-driven SAM3. SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL."
        },
        {
            "title": "6 Evaluation, Metrics, and Failure Modes",
            "content": "6.1 SAM2 Evaluation Metrics SAM2 is evaluated using metrics that focus on geometric segmentation and temporal performance. Common metrics include intersection over union (IoU) for mask overlap, boundary precision and recall for edge quality, and temporal stability indices that measure the consistency of masks frame-to-frame. In video segmentation benchmarks, identity preservation metrics assess whether the same object is tracked correctly across time. These metrics are well suited to scenarios such as video editing, object tracking, and interactive segmentation, where the user has already specified which object to follow. However, they do not measure whether the model has correctly understood semantic concept; they simply measure how accurately and consistently the model follows prompted region. 6.2 SAM3 Evaluation Metrics SAM3 introduces additional evaluation dimensions that focus on semantic correctness and concept-level understanding. The model must be judged not only by pixel-level IoU but also by how well it recalls all instances of concept given text prompt. Concept recall measures the proportion of relevant objects that are successfully segmented. Semantic localization error quantifies how much of the predicted mask corresponds to the intended concept versus irrelevant regions. Open-vocabulary generalization metrics evaluate performance on concepts that were not present in the training set or are used in new combinations. Attribute segmentation accuracy is important for prompts involving properties such as ripe, damaged, or healthy. Sensitivity to language ambiguity, synonym choices, and phrasing must also be considered. These metrics have no counterpart in SAM2 evaluation pipelines and require new benchmark design. Consequently, evaluation expertise built around SAM2 cannot be directly reused for SAM3 without incorporating these semantic and multimodal dimensions."
        },
        {
            "title": "7 Comprehensive Tables Describing the SAM2-to-SAM3 Gap",
            "content": "Table 2: Architectural and training differences between SAM2[82] and SAM3[83]. Category SAM2 SAM3 Primary Modality Prompt Type Reasoning Type Architecture Training Losses Dataset Type Optimization Focus Memory stability and propagation Vision-only (video) Points, boxes, masks Geometric and temporal ViT backbone + temporal memory Segmentation and temporal tracking losses Video with pixel masks Vision + text + example images Natural language prompts and concept exemplars Semantic, conceptual, and multimodal ViT backbone + text encoder + fusion module Segmentation, contrastive, semantic grounding losses Multimodal concept-annotated datasets Cross-modal alignment and concept fidelity Table 3: Evaluation differences between SAM2[82] and SAM3[83]. Metric Type Mask IoU Temporal Tracking Metrics Concept Recall Semantic Grounding Error Open-Vocabulary Generalization Attribute Segmentation Accuracy Prompt Sensitivity (Visual) Language Ambiguity Sensitivity SAM2 Yes Yes No No No No High Not applicable SAM3 Yes No Yes Yes Yes Yes Medium High Table 4 provides consolidated scientific comparison of SAM2 and SAM3, highlighting the mathematical, architectural, and evaluative differences that define the SAM2-to-SAM3 transition. As shown in the table, SAM2 is governed primarily by geometric losses such as IoU and temporal stability, whereas SAM3 introduces contrastive alignment, semantic grounding, and concept-conditioned segmentation objectives. These distinctions are not merely architectural but fundamentally reshape how segmentation is optimized, supervised, and interpreted. The evaluation metrics listed in Table 4 further demonstrate why SAM3 requires entirely new assessment protocols centered on semantic fidelity and 12 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. open-vocabulary reasoning. Collectively, this table underscores why expertise developed for SAM2 cannot be directly transferred to multimodal, concept-driven SAM3 workflows. Table 4: Scientific comparison of SAM2[82] and SAM3[83]: core equations, training objectives, and evaluation metrics. Boundary lines separate each conceptual block for clarity. Category SAM2 (Prompt-Based Vision Model) SAM3 (Concept-Driven Multimodal Model) Primary Objective Spatial mask segmentation and temporal propagation Concept-level segmentation conditioned on language, attributes, or exemplars Core Loss Function Mask Loss: Lmask = 1 IoU(Mpred, Mgt) Temporal Consistency: Ltemp = Mt Mt12 Segmentation Loss: Lseg = 1 IoU(Mpred, Mconcept) Contrastive Alignment: exp(v,t/τ ) Lcon = log exp(v,tj /τ ) Grounding Loss: Lground = fvision(x) ftext(c)2 (cid:80) Optimization Target Reduce geometric error; stabilize temporal mask propagation Maximize semantic alignment; improve concept fidelity Prompt Type Input Visual prompts (points, boxes, masks) Text prompts, concept exemplars, imagetext fusion Architecture ViT Backbone + Temporal Memory Video datasets with dense pixel masks (SA-V) ViT Backbone + Text Encoder + Cross-Modal Fusion + Concept Decoder Multimodal datasets linking segmentation masks with textual concept labels Dataset Requirements Concept Embeddings Key Evaluation Metrics No semantic embedding: ftext absent Joint embeddings: fvision(x) aligned with ftext(c) Mask IoU: IoU = Boundary F1 Score Temporal Identity Preservation Mp Mg Mp Mg P +F Concept Recall: Semantic Grounding Error: Bpred Bconcept1 Open-Vocabulary F1; Attribute Accuracy Failure Modes Prompt sensitivity; memory drift; occlusion-induced degradation Embedding misalignment; language ambiguity; semantic leakage"
        },
        {
            "title": "8 Conclusion and Future Directions: Toward a Concept-Driven Segmentation Era",
            "content": "The transition from SAM2 to SAM3 represents foundational redefinition of what it means to segment in modern computer vision. Rather than an incremental improvement, SAM3 introduces paradigm shift from geometric prompting to semantic reasoning, fundamentally altering how segmentation models understand visual content. SAM2 users skilled in designing visual prompts, stabilizing temporal memory, and optimizing mask propagation possess expertise that is deeply rooted in spatial interactions. In contrast, SAM3 demands proficiency in multimodal alignment, linguistic embeddings, semantic grounding, concept disambiguation, and large-scale vision-language pretraining. This shift moves segmentation from answering Where is the object? to addressing What concept does this region represent?, which disrupts long-standing assumptions about prompt design, dataset construction, and evaluation methodology. SAM3 requires researchers to build and curate multimodal datasets where each mask is linked not only to object identity but to concept-level attributes, synonyms, and contextual meanings. Its training pipeline optimizes for semantic alignment losses, presence modeling, ambiguity resolution, and cross-attention fusion loss terms and architectural components that have no analog in SAM2. In effect, SAM3 changes both the computational substrate and the cognitive framing of segmentation. This explains why prompt-based workflows from SAM2 cannot be transferred: the skill set, objectives, and representational assumptions are fundamentally different. Where SAM2 teaches model how to follow, SAM3 teaches model how to understand. Looking ahead, this conceptual leap will reshape research and industry practices across robotics, agriculture, medical imaging, manufacturing, autonomous systems, and multimodal AI. The broader community adopting SAM3 must prepare for three major transformations. First, dataset engineering will evolve toward richer semantic annotations, possibly requiring new tools for scalable concept labeling and cross-domain knowledge transfer. Second, evaluation protocols will increasingly emphasize open-vocabulary generalization, linguistic robustness, and semantic grounding in addition to traditional IoU metrics. Third, successful deployment of SAM3 in real-world systems will require interdisciplinary expertise integrating computer vision, natural language processing, and multimodal representation learning. Future research should explore more efficient training strategies for multimodal segmentation, including lightweight adapters, domain-specific concept banks, and curriculum learning for ambiguous or hierarchical concepts. Moreover, as SAM3 becomes integrated into embodied agents and decision-making systems, ensuring transparency, interpretability, and robustness under linguistic variability will be essential. The next generation of segmentation technologies will likely 13 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. Impact on Applications: Robotics, Medical, Agriculture From Spatial Prompts to Concept Prompts Semantic Evaluation: Concept Recall, OpenVocab F1 Toward Concept-Driven Segmentation Era New Training Objectives: Contrastive & Grounding Multimodal Data: Text + Masks Figure 8: Compact mindmap summarizing the shift toward concept-driven segmentation era, emphasizing the move from spatial prompts to concept prompts, multimodal data, new training objectives, semantic evaluation, and downstream application impact. move further toward unified perception-language-reasoning frameworks, where models understand not only objects and concepts but also intent and context. Ultimately, SAM3 marks the beginning of the semantic era in segmentation. By embracing multimodal reasoning, future researchers and practitioners can build systems that perceive the world not as regions of pixels, but as structured semantic environments paving the way for more intelligent, adaptable, and context-aware vision systems."
        },
        {
            "title": "Declaration of Competing Interest",
            "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."
        },
        {
            "title": "References",
            "content": "[1] Xiaolong Liu, Zhidong Deng, and Yuhan Yang. Recent progress in semantic image segmentation. Artificial Intelligence Review, 52(2):10891106, 2019. [2] Yanming Guo, Yu Liu, Theodoros Georgiou, and Michael Lew. review of semantic segmentation using deep neural networks. International journal of multimedia information retrieval, 7(2):8793, 2018. [3] Swarnendu Ghosh, Nibaran Das, Ishita Das, and Ujjwal Maulik. Understanding deep learning techniques for image segmentation. ACM computing surveys (CSUR), 52(4):135, 2019. [4] Thomas Blaschke, Charles Burnett, and Anssi Pekkarinen. Image segmentation methods for object-based analysis and classification. In Remote sensing image analysis: Including the spatial domain, pages 211236. Springer, 2004. 14 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 [5] Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos. IEEE transactions on pattern analysis and machine Image segmentation using deep learning: survey. intelligence, 44(7):35233542, 2021. [6] Sibei Yang, Meng Xia, Guanbin Li, Hong-Yu Zhou, and Yizhou Yu. Bottom-up shift and reasoning for referring image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1126611275, 2021. [7] Nikoleta Manakitsa, George Maraslidis, Lazaros Moysis, and George Fragulis. review of machine learning and deep learning for object detection, semantic segmentation, and human action recognition in machine and robotic vision. Technologies, 12(2):15, 2024. [8] Archana and PS Eliahim Jeevaraj. Deep learning models for digital image processing: review. Artificial Intelligence Review, 57(1):11, 2024. [9] Mingqi Gao, Feng Zheng, James JQ Yu, Caifeng Shan, Guiguang Ding, and Jungong Han. Deep learning for video object segmentation: review. Artificial Intelligence Review, 56(1):457531, 2023. [10] Rui Yao, Guosheng Lin, Shixiong Xia, Jiaqi Zhao, and Yong Zhou. Video object segmentation and tracking: survey. ACM Transactions on Intelligent Systems and Technology (TIST), 11(4):147, 2020. [11] Yan Xu, Rixiang Quan, Weiting Xu, Yi Huang, Xiaolong Chen, and Fengyuan Liu. Advances in medical image segmentation: comprehensive review of traditional, deep learning and hybrid approaches. Bioengineering, 11(10):1034, 2024. [12] Hyunseok Seo, Masoud Badiei Khuzani, Varun Vasudevan, Charles Huang, Hongyi Ren, Ruoxiu Xiao, Xiao Jia, and Lei Xing. Machine learning techniques for biomedical image segmentation: an overview of technical aspects and introduction to state-of-art applications. Medical physics, 47(5):e148e167, 2020. [13] Holger Roth, Chen Shen, Hirohisa Oda, Masahiro Oda, Yuichiro Hayashi, Kazunari Misawa, and Kensaku Mori. Deep learning and its application to medical image segmentation. Medical Imaging Technology, 36(2):6371, 2018. [14] Ujjwal Maulik. Medical image segmentation using genetic algorithms. IEEE Transactions on information technology in biomedicine, 13(2):166173, 2009. [15] Pierre-Henri Conze, Gustavo Andrade-Miranda, Vivek Kumar Singh, Vincent Jaouen, and Dimitris Visvikis. Current and emerging trends in medical image segmentation with deep learning. IEEE Transactions on Radiation and Plasma Medical Sciences, 7(6):545569, 2023. [16] Chen Chen, Chenyu Wang, Bin Liu, Ci He, Li Cong, and Shaohua Wan. Edge intelligence empowered vehicle detection and image segmentation for autonomous vehicles. IEEE Transactions on Intelligent Transportation Systems, 24(11):1302313034, 2023. [17] Long Chen, Shaobo Lin, Xiankai Lu, Dongpu Cao, Hangbin Wu, Chi Guo, Chun Liu, and Fei-Yue Wang. Deep neural network based vehicle and pedestrian detection for autonomous driving: survey. IEEE Transactions on Intelligent Transportation Systems, 22(6):32343246, 2021. [18] Noor Jannah Zakaria, Mohd Ibrahim Shapiai, Rasli Abd Ghani, Mohd Najib Mohd Yassin, Mohd Zamri Ibrahim, and Nurbaiti Wahid. Lane detection in autonomous vehicles: systematic review. IEEE access, 11:37293765, 2023. [19] Sarfraz Ahmed, Nazmul Huda, Sujan Rajbhandari, Chitta Saha, Mark Elshaw, and Stratis Kanarachos. Pedestrian and cyclist detection and intent estimation for autonomous vehicles: survey. Applied Sciences, 9(11):2335, 2019. [20] Meenakshi and Naresh. Soil health analysis and fertilizer prediction for crop image identification by inception-v3 and random forest. Remote Sensing Applications: Society and Environment, 28:100846, 2022. [21] Zifei Luo, Wenzhu Yang, Yunfeng Yuan, Ruru Gou, and Xiaonan Li. Semantic segmentation of agricultural images: survey. Information Processing in Agriculture, 11(2):172186, 2024. [22] Lian Lei, Qiliang Yang, Ling Yang, Tao Shen, Ruoxi Wang, and Chengbiao Fu. Deep learning implementation of image segmentation in agricultural applications: comprehensive review. Artificial Intelligence Review, 57(6):149, 2024. [23] Tanmay Anand, Soumendu Sinha, Murari Mandal, Vinay Chamola, and Fei Richard Yu. Agrisegnet: Deep aerial semantic segmentation framework for iot-assisted precision agriculture. IEEE Sensors Journal, 21(16):17581 17590, 2021. SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 [24] Rubén Usamentiaga, Dario Lema, Oscar Pedrayes, and Daniel Garcia. Automated surface defect detection in metals: comparative review of object detection and semantic segmentation using deep learning. IEEE Transactions on Industry Applications, 58(3):42034213, 2022. [25] Majid Mirbod, Ali Rajabzadeh Ghatari, Saber Saati, and Maryam Shoar. Industrial parts change recognition model using machine vision, image processing in the framework of industrial information integration. Journal of Industrial Information Integration, 26:100277, 2022. [26] Szu-Hao Huang and Ying-Cheng Pan. Automated visual inspection in the semiconductor industry: survey. Computers in industry, 66:110, 2015. [27] Max Ferguson, Ronay Ak, Yung-Tsun Tina Lee, and Kincho Law. Detection and segmentation of manufacturing defects with convolutional neural networks and transfer learning. Smart and sustainable manufacturing systems, 2(1):137164, 2018. [28] Guoguang Du, Kai Wang, Shiguo Lian, and Kaiyong Zhao. Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers: review. Artificial Intelligence Review, 54(3):16771734, 2021. [29] Andre Ückermann, Christof Elbrechter, Robert Haschke, and Helge Ritter. 3d scene segmentation for autonomous robot grasping. In 2012 IEEE/RSJ international conference on intelligent robots and systems, pages 17341740. IEEE, 2012. [30] Stefan Ainetter and Friedrich Fraundorfer. End-to-end trainable deep neural network for robotic grasp detection and semantic segmentation from rgb. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 1345213458. IEEE, 2021. [31] Emanuele Colleoni and Danail Stoyanov. Robotic instrument segmentation with image-to-image translation. IEEE Robotics and Automation Letters, 6(2):935942, 2021. [32] John Rogan and DongMei Chen. Remote sensing technology for mapping and monitoring land-cover and land-use change. Progress in planning, 61(4):301325, 2004. [33] Yongze Song, Graeme Wright, Peng Wu, Dominique Thatcher, Tom McHugh, Qindong Li, Shuk Jin Li, and Xiangyu Wang. Segment-based spatial analysis for assessing road infrastructure performance using monitoring observations and remote sensing data. Remote Sensing, 10(11):1696, 2018. [34] Leo Thomas Ramos and Angel Sappa. Multispectral semantic segmentation for land cover classification: An overview. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 17:1429514336, 2024. [35] Peiyan Jia, Chen Chen, Delong Zhang, Yulong Sang, and Lei Zhang. Semantic segmentation of deep learning remote sensing images based on band combination principle: Application in urban planning and land use. Computer communications, 217:97106, 2024. [36] Santiago Gonzalez Izard, Ramiro Sanchez Torres, Oscar Alonso Plaza, Juan Antonio Juanes Mendez, and Francisco José García-Peñalvo. Nextmed: automatic imaging segmentation, 3d reconstruction, and 3d model visualization platform using augmented and virtual reality. Sensors, 20(10):2962, 2020. [37] Ming-Der Yang, Chih-Fan Chao, Kai-Siang Huang, Liang-You Lu, and Yi-Ping Chen. Image-based 3d scene reconstruction and exploration in augmented reality. Automation in Construction, 33:4860, 2013. [38] Faisal Abdullah and Ahmad Jalal. Semantic segmentation based crowd tracking and anomaly detection via neurofuzzy classifier in smart surveillance system. Arabian Journal for Science and Engineering, 48(2):21732190, 2023. [39] Monica Gruosso, Nicola Capece, and Ugo Erra. Human segmentation in surveillance video with deep learning. Multimedia Tools and Applications, 80(1):11751199, 2021. [40] Yang He, Shadi Rahimian, Bernt Schiele, and Mario Fritz. Segmentations-leak: Membership inference attacks and defenses in semantic image segmentation. In European Conference on Computer Vision, pages 519535. Springer, 2020. [41] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Foundation models defining new era in vision: survey and outlook. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [42] Zhaozheng Chen and Qianru Sun. Weakly-supervised semantic segmentation with image-level labels: from traditional models to foundation models. ACM Computing Surveys, 57(5):129, 2025. [43] Ralf Kohler. segmentation system based on thresholding. Computer Graphics and Image Processing, 15(4):319338, 1981. SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 [44] Nitin Kumar. Thresholding in salient object detection: survey. Multimedia Tools and Applications, 77(15):1913919170, 2018. [45] James Tilton, Yuliya Tarabalka, Paul Montesano, and Emanuel Gofman. Best merge region-growing segmentation with integrated nonadjacent region object aggregation. IEEE Transactions on Geoscience and Remote Sensing, 50(11):44544467, 2012. [46] Marco Roggero et al. Object segmentation with region growing and principal component analysis. International Archives of Photogrammetry Remote Sensing and Spatial Information Sciences, 34(3/A):289294, 2002. [47] Ning Xu, Narendra Ahuja, and Ravi Bansal. Object segmentation using graph cuts based active contours. Computer Vision and Image Understanding, 107(3):210224, 2007. [48] Yiyang Chen, Pengqiang Ge, Guina Wang, Guirong Weng, and Hongtian Chen. An overview of intelligent image segmentation using active contour models. Intelligence & Robotics, 3(1):2355, 2023. [49] Neill DF Campbell, George Vogiatzis, Carlos Hernández, and Roberto Cipolla. Automatic 3d object segmentation in multiple views using volumetric graph-cuts. Image and Vision Computing, 28(1):1425, 2010. [50] Yuri Boykov and Gareth Funka-Lea. Graph cuts and efficient nd image segmentation. International journal of computer vision, 70(2):109131, 2006. [51] Ilya Levner and Hong Zhang. Classification-driven watershed segmentation. IEEE Transactions on Image Processing, 16(5):14371445, 2007. [52] Ola Hall, Geoffrey Hay, André Bouchard, and Danielle Marceau. Detecting dominant landscape objects through multiple scales: An integration of object-specific methods and watershed segmentation. Landscape Ecology, 19(1):5976, 2004. [53] Yanmin Peng and Rong Liu. Object segmentation based on watershed and graph cut. In 2010 3rd International Congress on Image and Signal Processing, volume 3, pages 14311435. IEEE, 2010. [54] Alan Mangan and Ross Whitaker. Partitioning 3d surface meshes using watershed segmentation. IEEE Transactions on Visualization and Computer Graphics, 5(4):308321, 1999. [55] Zoltan Kato, Josiane Zerubia, et al. Markov random fields in image segmentation. Foundations and Trends in Signal Processing, 5(12):1155, 2012. [56] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, and Xiaoou Tang. Deep learning markov random field for semantic segmentation. IEEE transactions on pattern analysis and machine intelligence, 40(8):18141828, 2017. [57] Chen Zheng, Yun Zhang, and Leiguang Wang. Semantic segmentation of remote sensing imagery using an object-based markov random field model with auxiliary label fields. IEEE Transactions on geoscience and remote sensing, 55(5):30153028, 2017. [58] Dragomir Anguelov, Taskarf, Vassil Chatalbashev, Daphne Koller, Dinkar Gupta, Geremy Heitz, and Andrew Ng. Discriminative learning of markov random fields for segmentation of 3d scan data. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR05), volume 2, pages 169176. IEEE, 2005. [59] Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks. Advances in neural information processing systems, 29, 2016. [60] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. [61] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. [62] KHGGP Dollár, Ross Girshick, et al. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. [63] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834848, 2017. [64] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan Yuille, and Li Fei-Fei. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8292, 2019. SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 [65] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 28812890, 2017. [66] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(12):2481 2495, 2017. [67] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020. [68] Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan Zhang, Lu Yuan, and Lei Zhang. Dynamic detr: Endto-end object detection with dynamic attention. In Proceedings of the IEEE/CVF international conference on computer vision, pages 29882997, 2021. [69] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1361913627, 2022. [70] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in neural information processing systems, 34:1207712090, 2021. [71] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. [72] Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. Multi-head attention: Collaborate instead of concatenate. arXiv preprint arXiv:2006.16362, 2020. [73] Hezheng Lin, Xing Cheng, Xiangyu Wu, and Dong Shen. Cat: Cross attention in vision transformer. In 2022 IEEE international conference on multimedia and expo (ICME), pages 16. IEEE, 2022. [74] Zhuofan Xia, Xuran Pan, Shiji Song, Li Erran Li, and Gao Huang. Vision transformer with deformable attention. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 47944803, 2022. [75] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1600016009, 2022. [76] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020. [77] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. [78] Marco Toldo, Andrea Maracani, Umberto Michieli, and Pietro Zanuttigh. Unsupervised domain adaptation in semantic segmentation: review. Technologies, 8(2):35, 2020. [79] Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-mei Hwu, Thomas Huang, and Honghui Shi. Differential treatment for stuff and things: simple unsupervised domain adaptation method for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1263512644, 2020. [80] Riaan Zoetmulder, Efstratios Gavves, Matthan Caan, and Henk Marquering. Domain-and task-specific transfer learning for medical segmentation tasks. Computer Methods and Programs in Biomedicine, 214:106539, 2022. [81] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer In Proceedings of the IEEE/CVF Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. international conference on computer vision, pages 40154026, 2023. [82] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [83] Nicolas Carion, Laura Gustafson, Yuan-Ting Hu, Shoubhik Debnath, Ronghang Hu, Didac Suris, Chaitanya Ryali, Kalyan Vasudev Alwala, Haitham Khedr, Andrew Huang, et al. Sam 3: Segment anything with concepts. arXiv preprint arXiv:2511.16719, 2025. 18 SAM2-to-SAM3 Gap in the Segment Anything Model SAPKOTA ET AL. 2025 [84] Jiayuan Zhu, Abdullah Hamdi, Yunli Qi, Yueming Jin, and Junde Wu. Medical sam 2: Segment medical images as video via segment anything model 2. arXiv preprint arXiv:2408.00874, 2024. [85] Chong Zhou, Xiangtai Li, Chen Change Loy, and Bo Dai. Edgesam: Prompt-in-the-loop distillation for sam. International Journal of Computer Vision, pages 117, 2025. [86] Jian Zhang, Xiangyu Chen, Mingwei Yu, Yonggang Guo, and Xiaohu Du. Two-stage landslide satellite image recognition in the southeastern tibet region based on cascade r-cnn and sam2. Earth Science Informatics, 18(2):414, 2025. [87] Xiaorui Sun, Jun Liu, Hengtao Shen, Xiaofeng Zhu, and Ping Hu. On efficient variants of segment anything model: survey: X. sun et al. International Journal of Computer Vision, 133(10):74067436, 2025. [88] Xiaoqi Zhao, Youwei Pang, Shijie Chang, Yuan Zhao, Lihe Zhang, Chenyang Yu, Hanqi Liu, Jiaming Zuo, Jinsong Ouyang, Weisi Lin, et al. Inspiring the next generation of segment anything models: Comprehensively evaluate sam and sam 2 with diverse prompts towards context-dependent concepts under different scenes. arXiv preprint arXiv:2412.01240, 2024. [89] Xinlei Yu, Changmiao Wang, Hui Jin, Ahmed Elazab, Gangyong Jia, Xiang Wan, Changqing Zou, and Ruiquan Ge. Crisp-sam2: Sam2 with cross-modal interaction and semantic prompting for multi-organ segmentation. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 12981307, 2025. [90] Juan Gutiérrez, Emilio Delgado, Carlos Breuer, José Conejero, and Roberto Rodriguez-Echeverria. Prompt once, segment everything: Leveraging sam 2 potential for infinite medical image segmentation with single prompt. Algorithms, 18(4):227, 2025. [91] Yuli Wang, Victoria Shi, Wen-Chi Hsu, Yuwei Dai, Sophie Yao, Zhusi Zhong, Zishu Zhang, Jing Wu, Aaron Maxwell, Scott Collins, et al. Optimizing prompt strategies for sam: advancing lesion segmentation across diverse medical imaging modalities. Physics in Medicine & Biology, 70(17):175014, 2025. [92] Chengxi Zeng, Yuxuan Jiang, and Aaron Zhang. Efficientsam3: Progressive hierarchical distillation for video concept segmentation from sam1, 2, and 3. arXiv preprint arXiv:2511.15833, 2025. [93] Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181, 2025. [94] Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu, et al. Segment anything in high quality. Advances in Neural Information Processing Systems, 36:2991429934, 2023. [95] Di Wang, Jing Zhang, Bo Du, Minqiang Xu, Lin Liu, Dacheng Tao, and Liangpei Zhang. Samrs: Scaling-up remote sensing segmentation dataset with segment anything model. Advances in Neural Information Processing Systems, 36:88158827, 2023. [96] Shuangrui Ding, Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Yuwei Guo, Dahua Lin, and Jiaqi Wang. Sam2long: Enhancing sam 2 for long video segmentation with training-free memory tree. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1361413624, 2025. [97] Kangxu Fan, Liang Liang, Hao Li, Weijun Situ, Wei Zhao, and Ge Li. Research on medical image segmentation based on sam and its future prospects. Bioengineering, 12(6):608, 2025. [98] Jun Yin, Fei Wu, Hao Su, Peng Huang, and Yuetong Qixuan. Improvement of sam2 algorithm based on kalman filtering for long-term video object segmentation. Sensors, 25(13):4199, 2025. [99] Haofeng Liu, Mingqi Gao, Xuxiao Luo, Ziyue Wang, Guanyi Qin, Junde Wu, and Yueming Jin. Resurgsam2: Referring segment anything in surgical video via credible long-term tracking. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 435445. Springer, 2025. [100] Henghui Ding, Chang Liu, Nikhila Ravi, Shuting He, Yunchao Wei, Song Bai, and Philip Torr. Pvuw 2025 challenge report: Advances in pixel-level understanding of complex videos in the wild. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 26692678, 2025. [101] Zhang Jiaxing and Tang Hao. Sam2 for image and video segmentation: comprehensive survey. arXiv preprint arXiv:2503.12781, 2025. [102] Yuqing Wang, Yun Zhao, and Linda Petzold. An empirical study on the robustness of the segment anything model (sam). Pattern Recognition, 155:110685, 2024. [103] Yuli Zhou, Guolei Sun, Yawei Li, Guo-Sen Xie, Luca Benini, and Ender Konukoglu. When sam2 meets video camouflaged object segmentation: comprehensive evaluation and adaptation. Visual Intelligence, 3(1):10, 2025."
        }
    ],
    "affiliations": [
        "Cornell University",
        "University of the Peloponnese"
    ]
}