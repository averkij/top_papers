{
    "paper_title": "Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs",
    "authors": [
        "Hang Guo",
        "Yawei Li",
        "Luca Benini"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Large Language Model (LLM) compression, such as quantization and pruning, have achieved notable success. However, as these techniques gradually approach their respective limits, relying on a single method for further compression has become increasingly challenging. In this work, we explore an alternative solution by combining quantization and sparsity. This joint approach, though promising, introduces new difficulties due to the inherently conflicting requirements on weight distributions: quantization favors compact ranges, while pruning benefits from high variance. To attack this problem, we propose Optimal Brain Restoration (OBR), a general and training-free framework that aligns pruning and quantization by error compensation between both. OBR minimizes performance degradation on downstream tasks by building on a second-order Hessian objective, which is then reformulated into a tractable problem through surrogate approximation and ultimately reaches a closed-form solution via group error compensation. Experiments show that OBR enables aggressive W4A4KV4 quantization with 50% sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory reduction compared to the FP16-dense baseline."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 2 7 7 1 1 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "OPTIMAL BRAIN RESTORATION FOR JOINT QUANTIZATION AND SPARSIFICATION OF LLMS Hang Guo, Yawei Li, Luca Benini ETH Zurich https://huggingface.co/HangGuo/OBR https://github.com/csguoh/OBR"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent advances in Large Language Model (LLM) compression, such as quantization and pruning, have achieved notable success. However, as these techniques gradually approach their respective limits, relying on single method for further compression has become increasingly challenging. In this work, we explore an alternative solution by combining quantization and sparsity. This joint approach, though promising, introduces new difficulties due to the inherently conflicting requirements on weight distributions: quantization favors compact ranges, while pruning benefits from high variance. To attack this problem, we propose Optimal Brain Restoration (OBR), general and training-free framework that aligns pruning and quantization by error compensation between both. OBR minimizes performance degradation on downstream tasks by building on second-order Hessian objective, which is then reformulated into tractable problem through surrogate approximation and ultimately reaches closed-form solution via group error compensation. Experiments show that OBR enables aggressive W4A4KV4 quantization with 50% sparsity on existing LLMs, and delivers up to 4.72 speedup and 6.4 memory reduction compared to the FP16-dense baseline."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) (Brown et al., 2020; Achiam et al., 2023; Dubey et al., 2024) have demonstrated remarkable capabilities across wide range of tasks, positioning them as promising foundation for achieving Artificial General Intelligence (AGI). However, as LLMs continue to grow in size with increasing parameter counts, efficiently serving them, especially in resource-constrained edge devices, remains significant challenge (Dettmers et al., 2022). To meet the demand for efficient LLM deployment, variety of methods have been proposed. One prominent line of work focuses on LLM quantization (Nagel et al., 2021), whose main objective is to remove outliers inherent in the LLM weights. To this end, existing works introduce either smoothing (Lin et al., 2024a; Xiao et al., 2023) or Hadamard rotation as preprocessing step (Ashkboos et al., 2024; Liu et al., 2024) to redistribute outliers before quantization. Thanks to the resulting flat distributions, recent state-of-the-arts (Liu et al., 2024; Sun et al., 2024; Hu et al., 2025) can achieve even 4-bit weight-activation-KV cache (W4A4KV4) inference while maintaining acceptable performance. Besides quantization, LLM pruning (Ma et al., 2023; Frantar & Alistarh, 2023) is often considered as another popular solution for compressing LLMs. And recent LLM pruning works (Sun et al., 2023; Zhang et al., 2024) have shown promising results on 50% unstructured and 2:4 semi-structured sparsity by additionally considering the statistics of activations during pruning. Despite the promising results at moderate compression, relying on single technique for further reduction is becoming increasingly difficult. As shown in Fig. 1(a), the quantization method QuaRot (Ashkboos et al., 2024) achieves competitive perplexity at moderate bit-width, but suffers from severe degradation under 4-bits. Similarly, pruning alone also faces analogous limitations, where aggressive sparsity inevitably leads to substantial degradation. In this work, we explore an alternative path beyond current LLM compression paradigms by jointly leveraging quantization and sparsification. The intuition arises from the observation that low-bit and sparse representations coexist. To be specific, we empirically find an average of 14.28% unstructured sparsity in the W4A4KV4 quantization-only"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: (a) Single compression techniques (Sun et al., 2023; Ashkboos et al., 2024) rapidly reaches limits under sub-4 bits while the joint counterpart can enable further compression. To enable unified comparison in single figure, pruning is represented using equivalent bit-widths. (b) INT4 + 2:4 sparse GEMM can achieve faster inference speed, higher throughput, and lower memory usage. Llama2-7B model, suggesting potential combination of quantization and pruning. Furthermore, recent hardware advances, such as NVIDIAs Ampere and Hopper architectures, have introduced native support for INT4-sparse GEMM kernels (Mishra et al., 2021; NVIDIA, 2022; 2021), making the combination of quantization and sparsity increasingly practical for efficient LLM inference. However, achieving effective joint quantization and sparsification is non-trivial, primarily due to the inherent conflict between their objectives. Specifically, quantization favors narrow numerical range in the weights to minimize quantization error, whereas pruning benefits from large variations in weight magnitudes to reveal naturally sparse patterns. For instance, Hadamard rotation is common practice in existing methods to smooth outliers for W4A4KV4 quantization. However, as evidenced by Sec. 5.1, using existing pruning methods to force zeros on the Hadamard-rotated weights leads to unacceptable performance degradation. Our approach. In this work, we propose Optimal Brain Restoration (OBR), general framework to enable joint quantization and sparsification. The core idea of our OBR is to intervene between pruning and quantization by computing an optimal compensation, thereby reconciling their conflicting requirements on weight distributions. To achieve this, we begin by formulating the second-order Hessian objective to minimize the impact of weight perturbations on downstream task performance. To make the optimization problem tractable, this objective is then approximated through rowwise decoupling, which eliminates inter-row correlations. Building on this surrogate, we further introduce group error compensation, which redistributes distortions from pruning and quantization to minimize overall error, yielding an explainable closed-form solution. By reconciling the conflicting requirements between quantization and sparsity, OBR provides an efficient and practical solution for LLM compression. To the best of our knowledge, OBR is among the first to enable W4A4KV4+50% sparsity LLMs, without requiring any additional retraining. We apply the proposed framework on Llama2 (Touvron et al., 2023), Llama3 (Dubey et al., 2024), and Qwen2.5 (QwenTeam, 2024) families, and demonstrate promising performance with OBR. In particular, our highly compressed model narrows the perplexity gap to merely 1.37 to its full-precision Llama2-70B counterpart. Furthermore, we evaluate the inference efficiency using INT4 sparse GEMM kernels. As shown in Fig. 1, OBR achieves up to 4.72 speedup and 6.4 memory reduction compared to FP16-dense baselines. We hope our work can serve as solid baseline and stimulate further research towards sparse low-bit LLMs."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Network Quantization for LLMs. Network quantization aims to accelerate inference by converting the full-precision representations into low-bit representations (Nagel et al., 2021). With the thriving of LLMs, many efforts (Tseng et al., 2024; Lin et al., 2024b) have focused on adapting quantization techniques for LLMs. For example, GPTQ (Frantar et al., 2022) improves upon the classic OBQ (Frantar & Alistarh, 2022) by enabling efficient post-training quantization on large-scale parameters and can outperform the common RTN baseline. Moreover, LLMs are also observed to"
        },
        {
            "title": "Preprint",
            "content": "contain outliers, where small number of elements exhibit disproportionately large magnitudes and heavily influence downstream performance. To address this, LLM.int8() (Dettmers et al., 2022) introduces mixed-precision scheme that preserves outliers in higher precision. Later, AWQ (Lin et al., 2024a) proposes to employ smoothing factors to transfer outliers from weights to activations, thus allowing for 8-bit weight quantization. SmoothQuant (Xiao et al., 2023) further trades off smoothing between weights and activations to achieve W8A8 quantization. To push toward even lower bit-widths, recent works (Chee et al., 2023; Hu et al., 2025; Sun et al., 2024) have predominantly leveraged the Hadamard transformation to flatten the weight distributions before quantization. For instance, QuaRot (Ashkboos et al., 2024) applies random rotation as preprocessing step, enabling quantization even to W4A4KV4 while maintaining performance. SpinQuant (Liu et al., 2024) and FlatQuant (Sun et al., 2024) further extend this idea by introducing learnable rotation matrices to further enhance quantization performance. Network Pruning for LLMs. Network pruning reduces computational and memory costs by eliminating weights that contribute little to the final prediction (LeCun et al., 1989; Han et al., 2015; Frankle & Carbin, 2018; Zhang et al., 2024). Early pruning methods primarily relied on magnitudebased criteria, which proved effective for small-scale networks. However, these simple approaches often struggle to maintain accuracy when applied to LLMs. To address this, variety of methods have been developed to either refine the pruning process or introduce more advanced selection criteria. For instance, LLM-Pruner (Ma et al., 2023) proposes to remove coupled components followed by LoRA (Hu et al., 2022) finetuning to restore accuracy. SparseGPT (Frantar & Alistarh, 2023) introduces one-shot pruning method based on OBD (LeCun et al., 1989), enabling efficient pruning without additional retraining. WANDA (Sun et al., 2023) demonstrates that information contained in activations is crucial for LLMs pruning, and introduces simple yet effective scoring metric for activation-aware sparsity. Joint Quantization and Sparsification. Before the rise of LLMs, several early works explored joint quantization and pruning on small networks. For instance, DJPQ (Wang et al., 2020) solves an optimization problem via gradient descent to balance sparsity and quantization error. OBQ (Frantar & Alistarh, 2022) proposes unified framework that simultaneously considers both pruning and quantization. In the context of LLMs, JSQ (Guo et al., 2024) adopts simulated annealing to identify optimal activation editing policies, and can achieve W8A8 quantization with 50% sparsity. Moreover, one recent work (Harma et al., 2024) also provides theoretical analysis suggesting that pruning followed by quantization is the optimal compression order. Despite these advancements, existing techniques still fall short in achieving aggressive compression levels such as W4A4KV4 with 50% sparsity, leaving room for further improvement in this domain."
        },
        {
            "title": "3 MOTIVATION",
            "content": "As shown in Fig. 1(a), relying on single method such as quantization or pruning is rapidly approaching its compression limits. For instance, solely decreasing the quantization bit-width or increasing the pruning ratio leads to drastic performance degradation. In contrast, since different compression techniques are largely orthogonal in nature (Guo et al., 2024), combining them effectively presents potential direction to squeeze out additional efficiency. For instance, as shown in Sec. B, the W4A4KV4 quantized Llama2-7B model in QuaRot (Ashkboos et al., 2024) naturally exhibits 14.28% average layer sparsity. Moreover, recent hardware advances have already supported INT4 sparse GEMM, which can achieve faster execution than dense INT4 kernels in practice. These observations motivate us to explore how to jointly leverage quantization and sparsity for more aggressive and practical LLM compression. However, realizing an effective joint quantization and sparsification scheme is notoriously challenging due to their inherently conflicting nature. Specifically, quantization typically favors compact numerical range of weights to minimize quantization error. For example, recent 4-bit quantization methods (Ashkboos et al., 2024; Liu et al., 2024; Sun et al., 2024) commonly adopt Hadamard transformation to rotate weights into smoother distributions for suppressing outliers before quantization. While such rotation is beneficial for quantization, it is detrimental to sparsity, which instead prefers weight distributions that exhibit large numerical disparities to better encourage sparsity. As demonstrated in Sec. 5.1, naively applying sparsification on top of rotated weights leads to severe performance degradation."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Given original LLM weights W, we first apply rotation to smooth out outliers, followed by pruning to introduce sparsity. The proposed OBR is employed to compute optimal compensation, which is added to the unpruned elements to mitigate the conflict between pruning and quantization. Finally, quantization is applied to obtain the sparse and quantized LLM weights."
        },
        {
            "title": "4 OPTIMAL BRAIN RESTORATION",
            "content": "In this work, we propose the Optimal Brain Restoration (OBR) framework, which adjusts weight distributions to reconcile the conflicting demands of pruning and quantization. Following previous practices (Harma et al., 2024; Guo et al., 2024), we adopt an order of pruning-then-quantization. As shown in Fig. 2, the overall process to generate low-bit and sparse weights using the proposed OBR can be formalized as: ˆW = quant(prune(rotate(W)) + WOBR), (1) where is the original LLM weights, WOBR is the compensation derived from OBR. In the following, we start in Sec. 4.1 by defining the necessary notations and objective function. Then we detail the generic formulation of the proposed OBR in Sec. 4.2, followed by the specific instantiations for quantization and pruning in Sec. 4.3."
        },
        {
            "title": "4.1 OBJECTIVE APPROXIMATION",
            "content": "Given the weight matrix RCoutCin in one standard linear layer and RCinL being the input activation representing the datasets statistics, our work employs the following classic optimization objective (LeCun et al., 1989; Frantar & Alistarh, 2022) which minimizes the perturbation of downstream task loss: min E[L] = E[L(X, + W) L(X, W)], (2) where is the perturbation on W, is the downstream task loss. To solve the optimization problem in Eq. (2), we first simplify the objective function. In detail, applying Taylor series on L(X, + W) at drives: = WL(X, W), + 1 2 vec(W)Hfullvec(W) + O(W3), (3) where WL(X, W) is the gradient, vec() : RCoutCin R1CoutCin is the vectorisation operator, and Hfull vec(W)vec(W) RCoutCinCoutCin is the layer-wise Hessian. 2L Assume that the model has been fully trained and reaches local minima, so the WL(X, W) 0. Further ignoring the last high order terms, Eq. (3) can be approximated into: 1 2 vec(W)Hfullvec(W). (4) Despite the above preliminary approximation, computing Hfull exactly is still infeasible in LLMs due to the O((CoutCin)2) complexity, we thus following previous works (Frantar & Alistarh, 2022) and estimate Hfull as: Hfull H, (5) where RCoutCout is the output-side curvature matrix which depicts the second-order sensitivity among output channels, 2XX RCinCin is the empirical Fisher matrix, and denotes the Kronecker product."
        },
        {
            "title": "Preprint",
            "content": "Based on Eq. (5), we propose to decouple the row-wise correlation of output channels in Hfull by approximating as an Identity matrix to make Hfull completely tractable. Finally, the original objective can be simplified into the following Cout independent optimization sub-problems: min E[ 1 2 vec(W)(I H)vec(W)] = 1 2 Cout(cid:88) i=1 E[wiHw ], (6) where wi R1Cin is the i-th row of W. Intuitively, Eq. (6) quantifies the impact of weight changes on the final downstream performance. For example, when is large, even small change in weights can result in large differences for downstream tasks."
        },
        {
            "title": "4.2 SOLUTION AND FRAMEWORK",
            "content": "To solve the simplified objective in Eq. (6), our proposed OBR employs the Group Error Compensation to optimally adjust weight distributions by shifting information from error-sensitive groups to the other robust ones. Since the rotation matrix acts on both and X, and thus cancels out during multiplication, in the following sections, we will omit the rotation operation and directly denote as the rotated matrix for notational clarity. Let Ji = 1 denote the i-th sub-problem, we begin by partitioning the elements of the i-th row wi into two disjoint groups using two index sets, i.e., the retain set Ri and the eviction set Ei, where Ri Ei = {1, . . . , Cin} and Ri Ei = . The retain set Ri collects weights that are less affected by compression, e.g., unpruned or less quantization-distorted, whereas the eviction set Ei corresponds to the indices of elements that are susceptible to compression effects. For clarity, we will omit the row index in the following. 2 wiHw With this grouping, our key idea is to compensate for compression-induced errors eE in eviction set by transferring its lost information to more robust retain set R. To enable this, we reorder the perturbation vector into [wR, wE]. Then the sub-problem becomes: arg min wR = 1 wHw = 1 2 [wR eE] (cid:20)HRR HRE HER HEE (cid:21) (cid:20) e (cid:21) . (7) Since Eq. (7) is an unconstrained optimization problem, we can directly obtain the closed-form solution by taking the partial derivatives w.r.t. wR, i.e., wR = HRRwR + HREeE 0. Then the optimal solution for wR which minimizes the row-wise error can be derived as: = H1 (8) In Fig. 3(a), we give an example on how to extract sub-Hassian HRR and HRE from H. According to the above formulation, the error in set is theoretically zero guaranteed by the closed-form solution. Since the retain set is assumed to be robust against compression-related errors, the total error can be decreased through transferring information from to R. Notably, Eq. (8) also offers strong explanation that the Hessian actually serves as bridge for error propagation between different groups. Specifically, in Eq. (8), the eE is first projected from Es space to the shared space via HRE, followed by the mapping to the Rs space through H1 RR, and the negative sign denoting the correction direction. RRHREeE."
        },
        {
            "title": "4.3 SPECIFIC IMPLEMENTATION",
            "content": "In this section, we apply the generic closed-form solution in Eq. (8) to the specific implementation for sparsification and quantization. OBR for Sparsification. As shown in Fig. 3(b), given the 0-1 mask from existing pruning algorithms, we define retain set R1 as the unpruned slots, and eviction set E1 as the pruned ones. In this way, the information loss due to pruning on set E1 can be compensated by transferring to set R1. Formally, since the pruning error on set E1 is eprune = wE1, using Eq. (8), the optimal OBR compensation for pruning can be derived as: E1 We then add wprune = [wR1 + wprune the incoming quantization error. Details are given below. HR1E1wE1. (9) to the unpruned elements wR1 to obtain the OBR-compensated sparse weight , 0]. After that, we perform another round of OBR on to further consider R1R R1 R1 wprune R1 = H"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: (a) Given Hessian approximation H, we extract the submatrices HRR and HRE based on the index sets and E. (b) The rotated dense weights are partitioned into R1 and E1 according to the binary pruning mask, followed by OBR to transfer information from wE1 to wR1. (c) The unpruned index set R1 is further divided into two groups: the first α fraction assigned to set E2, the remaining 1 α to set R2. OBR is used to compensate for quantization error in E2. OBR for Quantization. Different from pruning where the retain set and eviction set can be naturally obtained from the pruning mask, in quantization, we need to manually assign the grouping to obtain R2 and E2 for compensation with OBR. Thanks to the flat distribution introduced by Hadamard rotation, we find the discrepancy among unpruned elements is actually small (see Fig. 6). Inspired by this observation, we propose to take the first α proportion of elements in set R1 as the set E2, and the remaining 1 α proportion of elements as the set R2. In other words, R2 + E2 = R1, where is the number of elements. In Fig. 3(c), given quantization error on set E2 as equant = wE2 quant( wE2), we can obtain the OBR compensation for quantization as follows: E2 wquant R2 = H1 R2R HR2E2( wE2 quant( wE2 )). (10) Considering both quantization and pruning, the overall OBR-processed weights can be formalized as: + wquant ˆw = quant([wR2 + wprune , wE2 + wprune (11) 0]), , R2 R2 E2 where wprune joint low-bit and sparse weights. Algo. 1 provides more details of our proposed OBR. denote indexing from wprune and wprune using R2 and E2, and ˆw is the final R2 R1 CUDA Kernel Implementation. After transforming LLMs to both sparse and low-bit using the proposed OBR, we implement corresponding GEMM with the CUTLASS library1. Due to hardware support limitations, we perform 2:4 semi-structured sparsity and INT4 quantization on the weights W, and use INT4 quantization for the activations X. Related experiments are shown in Sec. 5.1."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Datasets and Models. We evaluate the proposed OBR framework on various open-source LLM families, including Llama2 (7B/13B/70B) (Touvron et al., 2023), Llama3 (8B/70B) (Dubey et al., 2024), and Qwen2.5-Instruct(7B/32B) (QwenTeam, 2024). To comprehensively assess the effectiveness of our method, we conduct experiments on both zero-shot classification and language modeling tasks. For zero-shot evaluation, we report accuracy on commonly used benchmarks including PIQA (Bisk et al., 2020), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), ARC-easy (Clark et al., 2018), ARC-challenge (Clark et al., 2018), and WinoGrande (Sakaguchi et al., 2021). In addition, we also follow prior LLM compression works (Sun et al., 2023) and evaluate the perplexity on the WikiText2 test set (Merity et al., 2016). Baselines. We compare our method against range of competitive baselines under sub-4-bit compression settings. Specifically, the full-precision model is included as an upper bound for reference. We also evaluate against quantization-only baselines (Ashkboos et al., 2024; Liu et al., 2024) under equivalent bit-widths, e.g., W4A4 model with 50% sparsity is compared to W3A4 quantized model. In addition, we include simple baseline that directly combines existing quantization and pruning techniques without any specially designed compensation. Furthermore, following the extension described in (Frantar & Alistarh, 2023), we adopt SparseGPT combined with GPTQ as strong joint sparsity-quantization baseline for comparison. 1https://github.com/NVIDIA/cutlass"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Comparison of perplexity score on WikiText2 and accuracy on zero-shot common sense reasoning tasks with Llama2(7B/13B/70B) and Llama3(8B/70B) model families. Since the Llama370B is sensitive to quantization as demonstrated in (Ashkboos et al., 2024), we keep the KV cache being 16-bit for acceptable performance. The best and the second best results are in red and blue. Model Method #Bits W-A-KV Sparsity ratio PIQA () BoolQ () HellaS. () Arc-e () Arc-c () WinoG. () 2-7B 2-13B 2-70B 3-8B 3-70B Floating-point QuaRot(quant-only) QuaRot+WANDA SparseGPT+GPTQ OBR RTN OBR GPTQ Floating-point QuaRot(quant-only) QuaRot+WANDA SparseGPT+GPTQ OBR RTN OBR GPTQ Floating-point QuaRot(quant-only) QuaRot+WANDA SparseGPT+GPTQ OBR RTN OBR GPTQ Floating-point QuaRot(quant-only) QuaRot+WANDA SparseGPT+GPTQ OBR RTN OBR GPTQ Floating-point QuaRot(quant-only) QuaRot+WANDA SparseGPT+GPTQ OBR RTN OBR GPTQ 16-16-16 3-4-4 4-4-4 4-4-4 4-4-4 4-4-4 16-16-16 3-4-4 4-4-4 4-4-4 4-4-4 4-4-4 16-16-16 3-4-4 4-4-4 4-4-4 4-4-4 4-4-4 16-16-16 3-4-4 4-4-4 4-4-4 4-4-4 4-416-16-16 3-4-16 4-4-16 4-4-16 4-4-16 4-4-16 0% 0% 50% 50% 50% 50% 0% 0% 50% 50% 50% 50% 0% 0% 50% 50% 50% 50% 0% 0% 50% 50% 50% 50% 0% 0% 50% 50% 50% 50% 79.11 51.96 50.27 63.38 68.77 68.93 80.52 55.01 51.36 71.27 72.74 72.91 82.70 67.74 51.52 79.11 78.67 79.22 80.85 55.28 49.62 66.21 67.95 66.87 84.49 52.77 50.82 60.12 61.92 67.36 77.71 39.72 37.83 63.27 66.39 67. 80.55 62.26 38.29 70.83 69.17 71.25 83.76 66.27 38.56 76.79 75.93 76.91 80.98 39.72 37.95 65.41 64.98 65.23 85.38 51.99 37.83 52.81 56.54 64.40 76.02 29.25 25.81 47.71 55.46 58.22 79.37 30.00 26.40 60.99 63.85 64. 83.81 56.55 27.67 77.20 76.09 77.23 79.17 30.78 26.42 53.58 54.06 55.41 84.96 30.65 26.25 35.02 37.81 55.26 74.49 31.36 25.00 50.93 55.98 55.93 77.48 31.10 26.18 61.87 65.95 65.57 81.06 50.67 27.06 77.61 77.57 77. 77.74 30.72 27.02 50.67 52.57 54.63 86.11 31.23 25.38 38.30 43.77 55.64 46.33 23.46 27.73 29.44 32.17 34.22 49.15 22.44 27.56 36.60 38.31 37.88 57.25 30.63 23.21 51.19 51.96 50.68 53.24 21.76 23.98 29.52 30.89 30. 64.16 23.12 26.96 23.29 25.17 33.11 69.14 52.33 49.25 54.70 60.22 61.48 72.14 51.07 49.49 62.90 64.17 63.22 77.98 62.43 50.04 73.95 74.51 74.11 73.40 50.36 47.83 57.22 55.96 58.80 80.51 50.51 45.70 53.51 52.01 50. Avg. () 70.47 38.01 35.98 51.57 56.49 53.45 73.20 41.98 36.54 60.74 62.37 62.60 77.76 55.72 36.34 72.64 72.45 72.61 74.23 38.10 35.47 53.77 54.40 55.16 80.93 40.05 35.49 43.84 46.20 55. Wiki2 () 5.47 132.97 5868.24 12.94 9.23 8.40 4.88 72.53 2289.41 7.89 7.29 7.06 3.32 8.19 169.67 4.78 4.84 4.69 6.13 196.23 1927.29 16.40 14.47 13.92 2.85 80.25 23245.17 41.39 33.38 16. Implementation Details. Since our OBR framework, as well as most other pruning and quantization methods (Frantar et al., 2022; Frantar & Alistarh, 2023; Sun et al., 2023), requires calibration data to estimate input statistics, we follow standard practice and use 128 samples from WikiText2 with sequence length of 2048 as the calibration set. For the Hadamard transformation, we test our OBR on rotation matrices from various existing works, including QuaRot (Ashkboos et al., 2024), SpinQuant (Liu et al., 2024), and FlatQuant (Sun et al., 2024). In addition, as our OBR treats pruning mask and quantizer as givens, it is potentially compatible with different pruning and quantization methods. Therefore, for pruning, we adopt the 0-1 mask generated by WANDA (Sun et al., 2023) as the default setting due to its strong performance and training-free nature. We will further discuss OBRs generality across other pruning algorithms in Sec. 5.2. For the grouping ratio α in OBR quantization, we simply use α = 50% as the default setting for all setups. For quantization, we include both the simple Round-To-Nearest (RTN) quantizer to obtain OBR RTN, and the more advanced GPTQ (Achiam et al., 2023) quantizer for OBR GPTQ as an extension."
        },
        {
            "title": "5.1 EXPERIMENT RESULTS",
            "content": "Main Results. As shown in Tab. 1, the QuaRot (quant-only), which relies solely on quantization for compression, suffers from severe performance degradation under 4-bit, e.g., 132.97 perplexity for W3A4KV4 quantized Llama2-7B model. Furthermore, effectively combining quantization and sparsity is non-trivial. For example, directly combining the existing quantization method Quarot (Ashkboos et al., 2024) with the pruning method WANDA (Sun et al., 2023) leads to unacceptable performance. For joint quantization and sparsification comparison, our OBR with simple RTN quantizer can achieve even better performance than SparseGPT+GPTQ in most cases. For example, our OBR RTN achieves even 3.71 better perplexity compared to SparseGPT+GPTQ on the Llama2-7B model. When using the more advanced quantizer GPTQ, our OBR GPTQ can achieve further 0.83 perplexity improvement. These experimental results demonstrate the effectiveness of the proposed OBR framework across different LLMs and tasks."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Comparison on runtime, FLOPs, and TOPS across different sequence lengths. We evaluate the performance of FP16-Dense, INT4-Dense, and INT4 2:4 Sparse GEMM on single NVIDIA A100-SXM4-80GB GPU. The GEMM computation follows typical LLM inference setting, where the weight matrix is R40964096 and the input activation is R32seq len4096. Table 2: Comparison under other quantization bit-widths on WikiText2 perplexity (wiki2) and average zero-shot accuracy (0-shot) using the Llama2-7B model."
        },
        {
            "title": "Method",
            "content": "sparisty W4A8KV8 W4A16KV16 wiki2 0-shot wiki2 0-shot Quarot(quant-only) Quarot+WANDA SparseGPT+GPTQ OBR RTN OBR GPTQ 0% 80.525 50% 5278.13 50% 50% 50% 8.53 7.24 6.87 39.98 35.95 59.41 62.16 63.39 80.25 5272.07 8.53 7.24 6.86 40.04 35.92 59.47 62.27 63. Practical Speedups. Given that recent GPU architectures such as Ampere and Hopper have naively supported INT4-sparse GEMM kernels, we compare the efficiency on batched matrix multiplication with other two baselines, i.e., INT4-dense and FP16-dense GEMM, in terms of latency, FLOPs, and TOPS. In Fig. 4, as input token length increases, the latency advantage of INT4+2:4 sparse GEMM becomes more pronounced. For example, at sequence length of 4096, the INT4+2:4 sparse GEMM achieves 5.9 speedup over FP16-dense and 1.4 speedup over INT4-dense GEMM. Furthermore, thanks to the 50% sparsity, INT4+2:4 sparse GEMM reduces theoretical FLOPs by 2 compared to its dense counterpart. Finally, when the GPU compute resources are fully saturated, i.e., with sequence length> 2048, the INT4+2:4 GEMM also achieves higher throughput in terms of TOPS. These results highlight the efficiency potential of low-bit sparse GEMM in real-world deployment compared to classic dense low-bit matrix multiplication. Comparison on other Bits. We further evaluate the OBR framework under more bit-width configurations. Given that LLMs are known to be memory-bound, we keep the weights quantized to low precision, i.e., 4-bit, while varying the activation and KV cache bit-width. Tab. 2 presents the results for W4A8KV8 and W4A16KV16 (weight-only quantization) settings. One can see that our OBR consistently outperforms all competitive baselines. Notably, OBR RTN with W4A8KV8+50% sparsity even surpasses weight-only quantization of SparseGPT+GPTQ by 1.29 perplexity. These results demonstrate the generality and effectiveness of OBR across different quantization bit-widths. Results with SpinQuant. To further validate the generality of other rotation schemes, we apply OBR to SpinQuant (Liu et al., 2024), which introduces learnable rotation matrices for improved performance. Similar to the setup of QuaRot, we treat the rotation matrix as given and do not learn dedicated rotation matrix for the joint quantization-sparsification setting. As shown in Tab. 3, our method achieves notable improvements over other competitive baselines e.g., OBR RTN achieves 7.69% average accuracy improvement against SparseGPT+GPTQ on zero-shot evaluation with Llama2-7B. Since the quantization-only W3A4KV4 baseline employs the rotation matrices specifically trained for quantization, our method is slightly inferior due to the task gap. We believe learning rotation matrices specifically for low-bit and sparse setups holds potential for further improvement. Other Sparsity Patterns. Semi-structured pruning, such as 2:4 sparsity, is now well-supported by modern hardware to achieve practical acceleration. To this end, we further include comparisons under semi-structured pruning settings in Tab. 4. One can see that the advantages of our OBR become more apparent as the compression becomes more challenging. In detail, both OBR RTN and OBR GPTQ consistently outperform other baselines under given setups. For example, under the challenging W4A4KV4+2:4 sparse setup, our OBR RTN reduces perplexity by 18.8 and improves the average accuracy on zero-shot evaluation by 5.86% over the SparseGPT+GPTQ. These promising results demonstrate the effectiveness of OBR in joint low-bit quantization and semi-structured sparsity."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Comparison of perplexity on WikiText2 and average accuracy on 0-shot commonsense reasoning tasks under SpinQuant (Liu et al., 2024) rotated weights. Method bits sparsity Llama2-7B Llama2-13B Llama2-70B Llama3-8B Llama3-70B wiki2 0-shot wiki2 0-shot wiki2 0-shot wiki2 0-shot wiki2 0-shot SpinQuant(quant-only) 3-4-4 SpinQuant+WANDA 4-4-4 4-4-4 SparseGPT+GPTQ 4-4-4 OBR RTN 4-4-4 OBR GPTQ 58.95 8.24 0% 50% 1589.54 36.17 45.42 22.57 50% 53.11 10.40 50% 53.45 10.70 50% 66.78 6.39 648.59 35.94 57.39 8.47 60.72 7.57 61.50 7.17 4.21 26.99 4.75 4.71 4.60 74.09 43.77 72.75 72.85 72.88 60.29 10.50 703.05 39.05 53.67 16.37 55.22 13.10 55.28 13.34 9. 63.64 18565.64 36.27 51.14 49.30 60.64 21.74 18.18 11.60 Table 4: Comparison on 4:8 and 2:4 sparsity with Llama2-7B models. The included baselines have all been quantized using QuaRot W4A4KV4 configuration. Figure 5: Applying the proposed OBR to WANDA (Sun et al., 2023) pruning algorithm in single compression tasks. Method sparsity wiki2 0-shot Floating-point SparseGPT+GPTQ OBR RTN OBR GPTQ SparseGPT+GPTQ OBR RTN OBR GPTQ - 4:8 4:8 4:8 2:4 2:4 2: 5.47 20.29 11.45 10.61 34.76 15.96 13.32 70.46 44.99 51.60 52.02 40.52 46.38 48. Table 5: Ablation on different pruning masks under W4A4KV4+50% sparsity using Llama27B and QuaRot rotation. pruning metirc wiki2 0-shot Magnitude: SparseGPT: [W2/diag(H1)] WANDA: X 8.92 9.28 8.40 56.51 55.45 53. Table 6: Ablation on partition ratio α. α 1 α Llama2-7B Llama2-13B wiki2 0-shot wiki2 0-shot 75% 25% 50% 50% 25% 75% 20% 80% 9.96 9.23 9.07 8.89 53.56 56.49 57.06 56.79 7.70 7.29 7.09 7. 60.22 62.37 63.20 61."
        },
        {
            "title": "5.2 ABLATION STUDIES",
            "content": "Different Pruning Masks. In the proposed OBR framework, the pruning mask is treated as given, making our method compatible with various existing pruning algorithms. In the above main experiments, we primarily adopt masks generated from WANDA (Sun et al., 2023) pruning. To further evaluate the effectiveness of other pruning metrics, we report in Tab. 5 the results using magnitudebased, SparseGPT-based (Frantar & Alistarh, 2023), and even Random pruning masks. Thanks to the error compensation from OBR, even the naive magnitude metric can achieve satisfactory performance. These results demonstrate the robustness of the proposed method across different pruning metrics. Partition Ratios for OBR Quantization. For quantization error compensation in OBR, we adopt simple strategy that splits weights into two groups with the first α proportion as the eviction set E2 and the remaining as the retain set R2, followed by the OBR error transfer. To further understand how the partitioning ratio affects error compensation, we conduct an ablation study with different α. As shown in Tab. 6, transferring the error from 20% elements to the remaining 80% leads to performance drop due to an insufficient compensating number. Conversely, migrating 75% of the error to only 25% of the elements also yields suboptimal results due to low-quality compensation. As trade-off, we adopt 50% partitioning ratio for constructing E2 and R2 as our final design."
        },
        {
            "title": "5.3 DISCUSSION",
            "content": "OBR for Pruning Only. As shown in Sec. 4.3, the proposed OBR can be potentially applied to single compression task to compensate for errors produced by given compression algorithm. To this end, we first extend our OBR framework to the pruning-only task. Specifically, we apply the proposed OBR to WANDA (Sun et al., 2023) by compensating for post-pruning weight distortions. The perplexity results on WikiText2 under different sparsity ratios are reported in Fig. 5. Equipped with our OBR, WANDA consistently achieves lower perplexity under given sparsity levels. For instance, at 60% sparsity, WANDA+OBR improves perplexity by 0.53 compared to the original WANDA, and this performance gain becomes more pronounced when sparsity increases. These"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Distribution visualization of different stages in the OBR framework. The weight matrix is taken from the layer.0.q proj layer from the Llama2-7B model. Due to the row-wise decoupling design in OBR, we visualize the distribution of the first row here and give full matrix visualization in Sec. D. The x-axis represents the Cin channel index, and the y-axis denotes the absolute value of weight elements. results suggest that OBR can potentially serve as generic post-processing enhancement for existing pruning algorithms to improve performance without retraining. Table 7: Results of OBR for RTN quantizer in quantization-only tasks. OBR for Quantization Only. We further apply the proposed OBR to pure quantization-based compression scenario. Specifically, similar to the process described in Sec. 4.3, we first redistribute the rotated weights using OBR compensation to prepare weights more suitable for subsequent quantization. Then, we use the RTN quantizer to obtain low-bit weights. We compare this variant with the baseline that directly applies RTN quantization to the rotated weights without OBR. The results are shown in Tab. 7. As can be seen, the compensation from OBR significantly improves RTN quantization, e.g., 2.17 reduction in perplexity and 3.88% gain in zero-shot accuracy. Although OBR is not specifically designed for quantization, OBR+RTN still achieves comparable results to GPTQ with 0.54 perplexity gap. These results demonstrate the potential of our proposed method in quantization-only tasks. Floating-point 16-16-16 GPTQ RTN OBR+RTN W-A-KV wiki2 0-shot 70.47 66.09 60.10 63.98 5.47 6.33 9.04 6.87 4-4-4 4-4-4 4-4-"
        },
        {
            "title": "Methods",
            "content": "Illustrative Visualization of OBR. In Fig. 6, we visualize the weight distribution at different stages of the proposed OBR pipeline. The wprune can effectively recover the information loss caused by pruning while preserving the original sparsity. Moreover, the compensation wOBR does not introduce additional outliers, and this flat distribution facilitates the subsequent quantization process. At last, the magnitude of the compensation introduced by OBR is comparable to that of the original weights, indicating that our OBR compensation is not noise but structured information capable of restoring the knowledge lost during compression."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we propose Optimal Brain Restoration (OBR), unified framework that jointly performs pruning and quantization by computing an optimal compensation to reconcile the conflicting requirements of different compression methods. We begin by formulating second-order Hessian-based objective that minimizes downstream task degradation. To make the optimization tractable, we introduce row-wise decoupling approximation. Furthermore, we develop group error compensation, which redistributes compression-induced errors through closed-form solution. By aligning the weight distribution with the distinct demands of each compression technique, OBR is among the first methods to support INT4 quantization combined with 50% sparsity for LLMs. Experimental results demonstrate that our approach significantly outperforms existing methods and achieves up to 4.72 practical speedup over the FP16-dense baseline."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated LLMs. Advances in Neural Information Processing Systems, 37:100213100240, 2024. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. PIQA: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence, volume 34, pp. 74327439, 2020. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:18771901, 2020. Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QUIP: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36: 43964429, 2023. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. GPT3.int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35: 3031830332, 2022. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The Llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. Elias Frantar and Dan Alistarh. Optimal brain compression: framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35:44754488, 2022. Elias Frantar and Dan Alistarh. SparseGPT: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning, pp. 1032310337. PMLR, 2023. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Jinyang Guo, Jianyu Wu, Zining Wang, Jiaheng Liu, Ge Yang, Yifu Ding, Ruihao Gong, Haotong Qin, and Xianglong Liu. Compressing large language models by joint sparsification and quantization. In International Conference on Machine Learning, 2024. Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in Neural Information Processing Systems, 28, 2015. Simla Burcu Harma, Ayan Chakraborty, Elizaveta Kostenok, Danila Mishin, Dongho Ha, Babak Falsafi, Martin Jaggi, Ming Liu, Yunho Oh, Suvinay Subramanian, et al. Effective interplay between sparsity and quantization: From theory to practice. arXiv preprint arXiv:2405.20935, 2024."
        },
        {
            "title": "Preprint",
            "content": "Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-rank adaptation of large language models. International Conference on Learning Representations, 1(2):3, 2022. Xing Hu, Yuan Cheng, Dawei Yang, Zukang Xu, Zhihang Yuan, Jiangyong Yu, Chen Xu, Zhe Jiang, and Sifan Zhou. Ostquant: Refining large language model quantization with orthogonal and scaling transformations for better distribution fitting. arXiv preprint arXiv:2501.13987, 2025."
        },
        {
            "title": "Yann",
            "content": "LeCun, John Denker, and"
        },
        {
            "title": "Sara",
            "content": "Solla."
        },
        {
            "title": "Optimal",
            "content": "brain damage. Advances in Neural Information Processing Systems, 2, 1989. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. AWQ: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of machine learning and systems, 6: 87100, 2024a. Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4A8KV4 quantization and system co-design for efficient LLM serving. arXiv preprint arXiv:2405.04532, 2024b. Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant: LLM quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024. Shuming Ma, Hongyu Wang, Shaohan Huang, Xingxing Zhang, Ying Hu, Ting Song, Yan Xia, and Furu Wei. Bitnet b1. 58 2b4t technical report. arXiv preprint arXiv:2504.12285, 2025. Xinyin Ma, Gongfan Fang, and Xinchao Wang. LLM-pruner: On the structural pruning of large language models. Advances in Neural Information Processing Systems, 36:2170221720, 2023. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016. Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint arXiv:2104.08378, 2021. Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko, Mart Van Baalen, and Tijmen Blankevoort. white paper on neural network quantization. arXiv preprint arXiv:2106.08295, 2021. NVIDIA. NVIDIA Hopper Architecture In-Depth, 2021. URL https://developer.nvidia. com/blog/nvidia-hopper-architecture-in-depth/. NVIDIA. Structured sparsity in the NVIDIA Ampere architecture, 2022."
        },
        {
            "title": "URL",
            "content": "https://developer.nvidia.com/blog/structured-sparsity-in-thenvidia-ampere-architecture-and-applications-in-search-engines/. QwenTeam. Qwen2.5: party of foundation models, September 2024. URL https://qwenlm. github.io/blog/qwen2.5/. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. Yuxuan Sun, Ruikang Liu, Haoli Bai, Han Bao, Kang Zhao, Yuening Li, Jiaxin Hu, Xianzhi Yu, Lu Hou, Chun Yuan, et al. Flatquant: Flatness matters for LLM quantization. arXiv preprint arXiv:2410.09426, 2024."
        },
        {
            "title": "Preprint",
            "content": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. QUIP#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396, 2024. Ying Wang, Yadong Lu, and Tijmen Blankevoort. Differentiable joint pruning and quantization for hardware efficiency. In European Conference on Computer Vision, pp. 259277. Springer, 2020. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 3808738099. PMLR, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. Plug-and-play: An efficient post-training pruning method for large language models. International Conference on Learning Representations, 2024."
        },
        {
            "title": "A SUMMARY OF OBR ALGORITHM",
            "content": "In Algo. 1, we provide detailed pseudocode to illustrate the process of obtaining joint low-bit and sparse LLM weights in the proposed OBR framework. Algorithm 1 Optimal Brain Restoration (OBR) Input: Hadamard rotated weight matrix RCoutCin, Hessian approximation RCinCin, partitioning ratio α. Output: Low-bit and sparse weight ˆW ZCoutCin . // Step1 Pruning {0, 1} = prune(W) Wprune // Step2 OBR compensation Initialize WOBR as zero matrices in RCoutCin for = 1 . . . Cout do R1 c,E1 c,R1 R1R1 + wprune // OBR for pruning R1 {i Mc,i = 1}, E1 {j Mc,j = 0} b1 HR1E1 wprune H1 R1 Wprune // OBR for quantization equant quantize( w) α R1 E2 {r1, . . . , rt}, R2 {rt+1, . . . , rR} b2 HR2E2 equant wquant R2 // Compensation Gathering + = wprune WOBR c,R1 + = wquant WOBR c,R2 E2 H1 R2R b2 R1 R2 end for Wquant Wprune + WOBR // Step3 Quantization ˆW quantize(Wquant) COEXISTENCE OF QUANTIZATION AND PRUNING. key motivation behind the proposed OBR is the compatibility of low-bit quantization and sparsity in the Hadamard-rotated LLMs. In this section, we provide empirical evidence to justify this motivation. Specifically, we visualize the sparsity distribution of Llama2-7B and Qwen2.5-7B models quantized by different rotation frameworks, i.e., QuaRot (Ashkboos et al., 2024), SpinQuant (Liu et al., 2024), and FlatQuant (Sun et al., 2024). Fig. 7 offers the results. Interestingly, even without any explicit pruning operations, the quantized LLMs inherently exhibit non-trivial sparsity. For instance, Llama27B with QuaRot reaches an average sparsity of 14.28%. Based on the observation of this coexistence, we design our OBR to achieve more aggressive LLM compression."
        },
        {
            "title": "C MORE EXPERIMENTS",
            "content": "Comparison with BitNet. BitNet-2B-4T (Ma et al., 2025) is recently proposed 1.58-bit LLM that is trained from scratch to achieve aggressive compression with strong performance. In this section, we give brief comparison between the BitNet-2B-4T model and Qwen2.5 compressed"
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Distribution of layer-wise sparsity across LLMs under different rotation methods. All models are quantized with W4A4KV4 RTN quantizer. Table 8: Comparison between BitNet-2B-4T and our OBR compressed Qwen2.5-Instruct models. methods quantization sparisty PIQA BoolQ HellaSwag ARC-E ARC-C WinoGrande Avg. Wiki2 W1.58A8KV BitNet-2B-4T Qwen2.5-1.5B + OBR W4A8KV16 Qwen2.5-1.5B + OBR W4A4KV4 W4A8KV16 Qwen2.5-3B + OBR W4A4KV4 Qwen2.5-3B + OBR 0% 76.55 80.43 50% 68.99 66.88 50% 67.25 68.01 50% 74.05 77.19 50% 72.14 76.67 68.39 52.68 51.18 62.86 60.43 74.66 62.50 56.99 60.06 60.69 49.40 35.24 32.94 41.30 41.13 72.22 60.77 55.96 62.90 65. 70.27 13.67 57.84 15.06 55.38 14.92 63.06 11.07 62.77 11.79 Table 9: Ablation experiments on other calibration dataset. We change the calibration set to the C4 (Raffel et al., 2020) dataset for the generation of activation statistics and keep other setups the same. dataset method Llama2-7B Llama2-13B Llama3-8B perplexity 0-shot perplexity 0-shot perplexity 0-shot wikitext2 c4 SparseGPT+GPTQ Ours RTN Ours GPTQ SparseGPT+GPTQ Ours RTN Ours GPTQ 12.94 9.23 8. 18.36 10.74 10.40 51.57 56.49 53.45 51.18 58.00 57.95 7.89 7.29 7.06 9.69 8.74 8.22 60.74 62.37 62. 60.48 62.88 63.16 16.40 14.47 13.92 23.02 18.23 17.90 53.77 54.40 55.16 53.87 56.02 57.12 using our OBR. As shown in Tab. 8, our post-training method achieves comparable performance. To be specific, Qwen2.5-3B+OBR (W4A4KV4+50%Sparsity) achieves better perplexity on WikiText2 and comparative performance on zero-shot accuracy. It should be noted that the performance of OBR can be further boosted when future, more advanced base LLMs are proposed. Moreover, the resulting W4A4KV4+50% sparse LLMs can be seamlessly deployed, such as in NVIDIA Ampere and Hopper, whereas BitNet requires specially designed kernels and customized implementations. At last, our method provides stronger generalization and flexibility. BitNet currently offers only one model size and typically requires training from scratch, which is computationally expensive and impractical for users with domain-specific or confidential data. In contrast, our OBR framework is general post-training compression approach that can be directly applied to existing models of different sizes, enabling users to efficiently adapt their own LLMs without re-training. Ablation on other Calibration Set. In the proposed OBR, we use the WikiText-2 (Merity et al., 2016) dataset to obtain activation statistics. To further verify the robustness across different calibration sets, we additionally experiment with the C4 (Raffel et al., 2020) dataset for calibration. The results are shown in Tab. 9. As can be seen, when switching to the C4 dataset, all compared methods suffer slight performance degradation on WikiText perplexity due to the train-test shift. However, models calibrated with C4 achieve better results on zero-shot tasks, and this advantage is more pronounced with our OBR. For example, in the Llama3-8B experiment with C4, SparseGPT+GPTQ achieves only 0.1% accuracy improvement, whereas the proposed OBR GPTQ delivers 1.96% gain. Moreover, both OBR RTN and OBR GPTQ consistently outperform the SparseGPT+GPTQ baseline across all calibration sets and base models under the same compression settings. The above results demonstrate the generalization of our method under other calibration sets."
        },
        {
            "title": "Preprint",
            "content": "Table 10: Comparison of perplexity score on WikiText2 and accuracy on zero-shot common sense reasoning tasks using the rotation matrix from FlatQuant (Sun et al., 2024). Model Method #Bits (W-A-KV) Sparsity ratio PIQA () BoolQ () HellaS. () Arc-e () Arc-c () WinoG. () Avg. () Wiki2 () Llama2-7B Llama2-13B Llama3-8B Qwen2.5-7B Qwen2.5-32B Floating-point FlatQuant(quant-only) FlatQuant(quant-only) SparseGPT+GPTQ Ours RTN Ours GPTQ Floating-point FlatQuant(quant-only) FlatQuant(quant-only) SparseGPT+GPTQ Ours RTN Ours GPTQ Floating-point FlatQuant(quant-only) FlatQuant(quant-only) SparseGPT+GPTQ Ours RTN Ours GPTQ Floating-point FlatQuant(quant-only) FlatQuant(quant-only) SparseGPT+GPTQ Ours RTN Ours GPTQ Floating-point FlatQuant(quant-only) FlatQuant(quant-only) SparseGPT+GPTQ Ours RTN Ours GPTQ 16-16-16 4-4-4 3-4-4 4-4-4 4-4-4 4-416-16-16 4-4-4 3-4-4 4-4-4 4-4-4 4-4-4 16-16-16 4-4-4 3-4-4 4-4-4 4-4-4 4-4-4 16-16-16 4-4-4 3-4-4 4-4-4 4-4-4 4-4-4 16-16-16 4-4-4 3-4-4 4-4-4 4-4-4 4-4-4 79.11 77.71 0% 77.48 74.62 0% 0% 75.68 73.94 50% 73.56 50.40 50% 74.32 72.91 50% 74.37 71.41 80.52 80.55 0% 79.00 79.39 0% 0% 78.56 78.04 50% 75.90 74.53 50% 76.66 73.94 50% 76.61 73. 80.85 80.98 0% 79.33 79.36 0% 0% 75.68 69.42 50% 69.97 74.95 50% 74.16 77.61 50% 73.99 77.16 80.14 85.96 0% 78.13 85.87 0% 0% 73.23 82.20 50% 73.56 83.70 50% 74.70 85.41 50% 76.66 85.08 76.02 74.49 46.33 73.64 72.56 43.00 69.44 67.85 40.96 65.36 61.11 34.73 65.88 64.94 37.88 65.92 64.06 38.82 79.37 77.48 49.15 77.44 76.47 48.72 75.35 70.66 44.97 69.81 68.86 40.19 71.44 71.30 42.06 71.39 72.10 42.49 69.14 68.27 64.17 62.75 65.82 66.38 72.14 70.17 70.09 67.09 68.27 68. 70.47 68.26 65.34 57.99 63.62 63.49 73.20 71.86 69.61 66.06 67.27 67.38 5.47 5.79 6.74 7.75 6.88 6.87 4.88 5.11 5.70 6.13 5.84 5.84 79.17 77.74 53.24 76.64 75.21 48.46 71.21 67.47 39.85 63.59 57.03 34.64 66.86 68.81 40.78 0.6661 65.80 66.08 66.74 69.11 41.30 6.13 74.23 6.97 71.84 65.17 9.14 60.89 13.32 9.12 9. 73.40 72.06 67.40 65.19 68.19 79.57 76.47 51.19 78.48 77.23 51.02 74.51 69.78 48.29 68.50 68.10 42.49 71.22 74.49 49.83 70.68 74.12 50.85 69.46 68.82 63.06 64.01 66.30 67.56 73.16 73.95 67.64 72.14 72.77 72.61 8.35 73.78 73.25 8.40 68.51 10.08 66.72 14.53 9.55 70.32 9.51 70. 77.66 77.54 74.23 74.76 76.04 76.28 5.32 5.82 6.79 8.06 6.81 6.79 81.39 90.54 80.96 89.39 78.94 87.83 85.25 77.02 58.62 0% 83.86 79.17 57.94 0% 0% 81.45 74.87 54.69 50% 80.20 89.94 0.7986 73.78 52.65 80.00 78.45 57.17 50% 77.86 90.00 80.00 77.31 59.22 50% 79.11 89.45 Performance on FlatQuant. In the main paper, we present the application of our OBR on the LLMs rotated by QuaRot (Ashkboos et al., 2024) or SpinQuant (Liu et al., 2024). To further evaluate the generalization ability of our method on other Hadamard rotation frameworks, we additionally include the comparison results with the FlatQuant (Sun et al., 2024) method. The experimental results are shown in Tab. 10. As can be observed, OBR continues to deliver strong performance compared to the SparseGPT+GPTQ baseline across various base models. Interestingly, comparing with QuaRot and SpinQuant, when using stronger rotation matrix from FlatQuant, the W4A4KV4 + 50% sparsity LLMs using our OBR can achieve performance on par with their FP16 counterparts. For example, the perplexity gap on Llama2-7B is merely 1.4, compared with the gap of 2.93 in QuaRot. This result further indicates the potential that our OBR can scale in parallel with more advanced rotation framework. Results on Qwen Families. In this section, we take Qwen2.5-Instruct (7B/32B) as representative to demonstrate the generalization capability of the proposed OBR on other LLMs. The experimental results are presented in Tab. 10. Given Qwen as the base models, OBR consistently outperforms other strong baselines across different scales. For instance, OBR RTN surpasses SparseGPT+GPTQ by 4.98 perplexity on the Qwen2.5-7B model. In addition, OBR RTN also outperforms the quantizationonly W3A4KV4 baseline by 0.53 perplexity. These results demonstrate the strong generalization ability of the proposed OBR across different LLM families. Calibration Time Cost of OBR. Tab. 11 reports the time cost for compressing models of different scales using OBR. As one can see, for smaller models such as the 7B model, OBR can produce W4A4KV4 + 50% LLMs in about 2 hours. For even larger models, such as the 70B, the proposed OBR completes in roughly 36 hours. Since our OBR adopts row-wise decoupling strategy, it requires solving linear equation for each row, making it slower than SparseGPT+GPTQ. Nevertheless, we emphasize that post-training compression needs to be performed only once per model. As result, this cost has only minimal impact on large-scale deployment. Moreover, the promising performance of OBR against other baselines under aggressive compression further justifies its advantages."
        },
        {
            "title": "Preprint",
            "content": "Table 11: Calibration time results for Llama model family. The reported times correspond to QuaRot (Ashkboos et al., 2024) rotation on single A100 GPU."
        },
        {
            "title": "Llama family",
            "content": "SparseGPT+GPTQ OBR RTN OBR GPTQ 2-7B 45min 2h10min 2h18min 2-13B 54min 4h12min 4h30min 2-70B 3-8B 3-70B 1h53min 35h30min 35h45min 48min 2h30min 2h40min 2h9min 35h28min 35h47min Figure 8: Visualization of the full weight matrix at different stages in the proposed OBR pipeline. The x-axis corresponds to the Cin dimension, and the y-axis is the Cout dimension. The weight matrix is taken from the layer.0.q proj layer from the Llama2-7B model, and absolute values are used to enhance visual clarity."
        },
        {
            "title": "D MORE VISUALIZATION",
            "content": "In Fig. 8, we present visualizations of the full weight matrices at different stages of OBR processing. It can be observed that the rotated weight matrix inherently exhibits strong row-wise independence, as indicated by the similarity patterns across rows in rotate(W). Moreover, the compensation terms Wprune and Wquant produced by OBR clearly contain useful information, since they share similar magnitude with the prune(rotate(W)). Therefore, if the OBR compensation were merely noise, perturbations of this magnitude would lead to significant errors. In addition, the overall compensation WOBR also demonstrates row-wise independence, where some rows have large magnitudes while others have small ones, yet column dimensions instead exhibit similar patterns. This observation further justifies our proposed row-wise decoupling strategy."
        },
        {
            "title": "E LIMITATION AND FUTURE WORK",
            "content": "While the proposed OBR can effectively redistribute weights to reconcile the differing distributional requirements of quantization and pruning, there are several avenues for further improvement. First, OBR relies on row-wise decoupling strategy to estimate the full Hessian. This approximation renders the original objective tractable, but it requires solving linear system for each row of the weight matrix. Although this overhead is acceptable in model compression tasks, where the compression algorithm needs to run only once, further accelerating the compression process for large-scale LLMs remains meaningful. Second, the current implementation of OBR treats the pruning"
        },
        {
            "title": "Preprint",
            "content": "mask and quantization rotation matrix as fixed given inputs. However, recent quantization studies (Liu et al., 2024; Sun et al., 2024) suggest that introducing gradient-based optimization can further boost performance. Thus, designing learnable pruning masks and rotation matrices compatible with our OBR framework could lead to additional gains. Third, although OBR significantly outperforms individual compression methods under equivalent sub-4-bit settings, its advantage narrows at higher bit-widths, where standalone methods have not yet reached their performance limits. Developing more advanced algorithms to maintain superior performance across various bit-widths is also promising direction, and we leave it for future work."
        }
    ],
    "affiliations": [
        "ETH Zurich"
    ]
}