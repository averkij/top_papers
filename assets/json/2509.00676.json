{
    "paper_title": "LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model",
    "authors": [
        "Xiyao Wang",
        "Chunyuan Li",
        "Jianwei Yang",
        "Kai Zhang",
        "Bo Liu",
        "Tianyi Xiong",
        "Furong Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on a base generative model, producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a competitive policy model -- matching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce a unified model excelling at both evaluation and generation, offering a simple path toward scalable, self-improving multimodal systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 6 7 6 0 0 . 9 0 5 2 : r LLaVA-Critic-R1: Your Critic Model is Secretly Strong Policy Model Xiyao Wang1, Chunyuan Li, Jianwei Yang, Kai Zhang2, Bo Liu3, Tianyi Xiong1, Furong Huang1 1University of Maryland, College Park 2The Ohio State University 3National University of Singapore In visionlanguage modeling, critic models are typically trained to evaluate outputsassigning scalar scores or pairwise preferencesrather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on base generative model, producing LLaVA-CriticR1, multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as top-performing critic but also as competitive policy modelmatching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce unified model excelling at both evaluation and generation, offering simple path toward scalable, self-improving multimodal systems. Date: Aug 29, 2025 Code Repository: https://github.com/LLaVA-VL/LLaVA-NeXT/LLaVA-Critic-R1 Collection: https://huggingface.co/collections/lmms-lab/llava-critic-r 1. Introduction Vision-language models (VLMs) have achieved impressive results in tasks ranging from image captioning and visual question answering to complex multimodal reasoning (OpenAI, 2023, Liu et al., 2023, Li et al., 2024, Deepmind, 2025, Team et al., 2025a,b). While most attention is paid to policy modelssystems trained to produce responsesprogress has also been driven by critic models, which evaluate those responses by assigning scores, ranking pairs, or providing structured feedback. Critic models are central in two stages of the VLM lifecycle: during training, they supply reward signals for reinforcement learning (Wang et al., 2024b, Xiong et al., 2025, Zang et al., 2025), and during inference, they enable test-time scaling by selecting the best candidates from multiple generations (Wang et al., 2024c, Zang et al., 2025, Wang et al., 2025b). Given this dual functionality, training powerful critic model is of central importance. In this context, high-quality critic data serves as foundational component, playing pivotal role in the effectiveness of critic training. In standard practice, critic datasets consist of question paired with two responses generated by different models, together with preference judgments provided by human annotators or by more capable reference models. In some cases, they also include pointwise annotations, in which single models response is scored by humans or stronger models. These annotated examples are then used to train the critic model through Supervised Fine-Tuning (SFT). More recent approaches augment preference annotations with humanor model-generated rationales, producing reasoning-rich critic data (Xiong et al., 2025, Zang et al., Corresponding author(s): Xiyao Wang xywang@umd.edu LLaVA-Critic-R1 Figure 1: LLaVA-Critic-R1 is trained on top of the base model Qwen-2.5-VL-7B. Building upon stronger reasoning VLM, ThinkLite-VL-7B, we further develop LLaVA-Critic-R1+ by applying the same RL critic training procedure. Left: Performance comparison of LLaVA-Critic-R1 with other base and reasoning VLMs on multiple visual reasoning, visual understanding, and visual reward benchmarks. LLaVA-Critic-R1 not only significantly outperforms other models in critic performance, but also demonstrates stronger policy capabilities. Right: Performance improvement of critic training and test-time self-critic scaling on five common visual reasoning and visual understanding benchmarks. Critic training alone significantly improves the base models performance. Building upon this, leveraging the dual policy and critic capabilities of LLaVA-Critic-R1 for \"Best-of-128\" self-critic scaling procedure at test time leads to further substantial boost in performance. 2025), which allows the critic to deliver more informative and interpretable feedback. In this paper, we explore novel critic training paradigm enables the critic model to autonomously generate reasoning through reinforcement learning. We begin by reorganizing critic data with preference labels into verifiable reinforcement learning task. We then perform reinforcement learning directly on base model to obtain the final critic model, leading to the new model family denoted as LLaVA-Critic-R1. Surprisingly, LLaVA-Critic-R1 emerges not only as strong critic but also as competitive policy model, (1) outperforming its base model by +5.7% on average across 26 diverse visual reasoning and understanding benchmarks, and matching or surpassing specialized reasoning VLMs trained with in-domain data, as shown in Figure 1. (2) Applying our RL critic training to strong reasoning VLMs produces LLaVA-Critic-R1+, which further advances policy performance while maintaining top-tier critic ability. (3) Moreover, the improved critic capability enables effective test-time scaling: self-critique boosts performance by +13.8% on five representative reasoning tasks without additional training. In summary, we highlight the following surprising empirical finding as the key insight of this paper: Key Findings: (i) The critic model trained with RL to judge response preference, can directly be as strong policy model with improved capabilities generalized to wide range of downstream tasks. (ii) This simple approach improves actor/critic capabilities simultaneously in one single model. The improved self-critic capability leads to better test-time scaling behavior. 2 LLaVA-Critic-R1 2. Critic Model Training with Reinforcement Learning 2.1. Critic Data Reformulation: From SFT to RL We begin with the pairwise training set of LLaVA-Critic, which contains roughly 40k instances collected from VLFeedback, RLHF, and RLHF-V. Each instance consists of: (1) natural image and an associated question, (2) two candidate responses generated by different vision-language models, (3) preference label, provided by either GPT-based annotators or humans, indicating which response is better (or if they are equally good), and (4) long chain-of-thought (Long-CoT) rationale generated by GPT, structured around five explicit evaluation metrics. While these AI-generated rationales enrich SFT training, we discard them in our RL setting for two reasons: (a) Avoiding knowledge distillation bias. Retaining GPTs evaluation criteria risks turning the model into passive reproducer of external judgments, rather than an autonomous critic. (b) Encouraging self-derived reasoning. Without fixed metric prompts, the model must internally decide how to compare responses, mirroring the open-ended reasoning process of strong human critic. Concretely, we strip away the five evaluation metrics and GPT rationales, retaining only the image, question, and two responses. The final RL critic prompt  (Table 1)  asks the model to decide which response is better (or indicate tie) and produce the decision in machine-verifiable format. This ensures that preference labels, whether from humans or GPT, serve as unambiguous ground-truth answers, enabling straightforward reward computation. The resulting formulation is compact, verifiable, and forces the model to reason for itself, rather than copy pre-digested justifications. Table 1: Prompt template used for constructing critic training data. Prompt Template: <image> You are provided with an image and question for this image. Please review the corresponding responses and choose which response is better for the given question. Your answer should be the response index if one response is better and \"Two responses are equally good.\" if the responses are equally good. Answer with response number or \"Two responses are equally good.\" directly. Your task is provided as follows: Question: {} Response 1: {} Response 2: {} 2.2. Incentivizing Critic Capability via Reinforcement Learning We directly conduct reinforcement finetuning without any knowledge distillation SFT. The reward signal for each data point during training is defined as follows: is consisted of the preference reward rpref and the format reward rformat, α is hyperparameter to balance these two rewards: = α rpref + (1 α) rformat (1) 3 LLaVA-Critic-R1 Preference reward rpref. The reward is computed based on whether the predicted preference exactly matches the ground truth preference. If yes, the reward is defined as +1, otherwise 0. Format reward rformat. We follow standard practice by encouraging the model to adhere to specific output format: the critic reasoning process is enclosed within special tokens <think>...</think>, and the final answer is wrapped with boxed. The format reward rformat is 1 if it correctly uses the special format tokens and 0 otherwise. The RL training prompt template is shown in Table 2. We adopt the widely used Group Relative Policy Optimization (GRPO; Shao et al. (2024)) as our training objective, and set α as 0.9 for Eq. 1 during training. Table 2: Prompt template used for RL training and test-time inference. Thinking Template: You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within <think> </think> tags. The final answer MUST BE put in boxed{} This design delivers two mutually reinforcing benefits: (1) Enhanced critic capability. The same model functions as high-quality evaluator at inference time, reliably selecting the best response from multiple candidates and enabling more effective test-time scaling without additional training. (2) Policy generalization. Although post-trained exclusively on critic data, the model acquires transferable reasoning strategies that extend to wide range of downstream perception, VQA, and reasoning tasks, matching or even surpassing specialized policy models. Together, these effects produce single model that simultaneously strengthens both policy and critic capabilities within single architecture. 3. Main Results: Critic Training and Test-Time Scaling 3.1. Critic Training 3.1.1. Baselines We use Qwen-2.5-VL-7B (Bai et al., 2025) as the baseline model. To evaluate the policy capability of critic training, we compare against four strong reasoning VLMs derived from in-domain policy training based on Qwen-2.5-VL-7B: ThinkLite-VL-7B (Wang et al., 2025e), Vision-R1-7B (Huang et al., 2025b), MM-Eureka-7B (Meng et al., 2025), and VL-Rethinker-7B (Wang et al., 2025a). Vision-R1 is first initialized by distilling the reasoning knowledge from Deepseek-R1, followed by RFT on 16K visual understanding and reasoning examples. The other three models are directly trained via RFT on top of Qwen-2.5-VL-7B, using varying amounts of diverse visual understanding and reasoning data, without relying on knowledge distillation. For LLaVA-Critic-R1, we train two variants: (1) base model version trained on top of Qwen-2.5-VL-7B using only 40k critic data to assess the policy improvement brought by critic training alone, and (2) policy model version trained by further applying critic training on ThinkLite-VL-7B using same 40k critic data, thereby equipping the reasoning model with both policy and critic capabilitiesreferred to as LLaVA-Critic-R1+ from policy model in Table 3. 4 LLaVA-Critic-R Table 3: General benchmark comparison. All models are prompted with the thinking template, except for the nonthinking column of Qwen-2.5-VL-7B. LLaVA-Critic-R1 shows strong critic ability and significantly improves policy performance, matching or surpassing existing reasoning VLMs. LLaVA-Critic-R1+ further enhances policy capability, achieving best results on multiple visual understanding and reasoning tasks. VLM-Agent scores are excluded from overall averages, as some models cannot perform these tasks. Benchmark Critic Training LLaVA-Critic-R1 Base Model Qwen-2.5-VL-7B In-domain Policy Training VL Reasoning Models (From Base) + (From Policy) Non-Thinking Thinking ThinkLite-VL-7B Vision-R1-7B MM-Eureka-7B VL-Rethinker-7B Perception & General VQA Image Reasoning Chart Understanding Video Reasoning Blink HallusionBenchImage MMStar MMBench MMVP MMHal RealWorldQA Avg. MathVistatestmini MathVisiontestmini MathVersetestmini MMMU val EMMAmini Blind VisuLogic ZeroBenchsub Avg. ChartQA OCRBench AI2D Charxivreasoning Avg. MMVUval VideoMMMU Avg. VLM Agent OSWorld Online-Mind2Web Avg. Visual RewardBench VLRewardBench MM-RLHF Avg. Overall Avg. 58.34.1 71.33.4 65.13.4 80.90.2 75.62.7 3.860.06 69.70.9 60.72.1 74.06.2 30.68.5 49.75.2 55.24.6 28.33.5 48.21.1 81.52.5 28.01.1 16.52.8 45.83.7 85.41.4 86.11.0 83.20.6 43.11.7 74.51.2 51.76.0 54.77. 53.26.9 3.50.8 16.32.3 9.91.5 61.510.8 74.72.9 68.16.8 57.383.1 59.00.5 72.31.4 65.70.7 82.30.9 77.32.0 3.880.05 69.22. 61.41.2 76.11.0 34.21.3 52.90.8 58.83.3 28.51.3 51.32.1 81.71.6 27.31.8 19.80.9 47.80.9 84.20.6 86.90.2 83.80.2 47.31.5 75.60.4 50.91.1 54.41.2 52.71.2 0.0 16.31.0 8.20.5 59.812.5 70.03.0 64.97. 58.231.47 3.1.2. Benchmark Evaluation Results 54.2 67.9 61.7 80.7 72.9 3.80 68.8 58.6 67.8 22.1 44.5 50.6 24.8 49.3 79.0 26.9 13.7 42. 84.0 85.1 82.6 41.4 73.3 45.7 46.9 46.3 2.7 14.0 8. 50.7 71.8 61.3 53.9 67.2 62.5 78.9 69.7 3.79 66.1 57.4 69.6 24.3 47.2 53.8 26.0 48.7 77.4 26.0 15.8 43. 84.4 85.3 79.9 41.9 72.9 49.4 49.1 49.3 52.1 71.8 62.0 58.5 70.9 65.0 81.4 75.3 3.83 66.8 60.2 75.1 32.9 52.1 55.5 29.8 49.2 83.3 25.5 18.9 46. 84.8 86.7 83.6 45.8 75.2 49.8 53.2 51.5 0.0 15.3 7. 47.3 67.0 57.2 56.7 68.8 60.2 78.9 69.3 3.81 70.4 58.3 73.5 30.7 51.9 50.5 34.5 38.0 78.0 31.0 16.7 46. 79.8 75.9 80.4 36.1 68.1 50.7 52.6 51.7 0.0 29.3 35.3 32.3 54.8 66.9 64.6 80.3 73.3 3.72 61.3 57.3 73.6 27.9 50.2 52.7 31.0 48.1 61.8 26.3 12.6 42. 82.1 82.9 82.3 40.2 71.9 50.8 52.6 51.7 0.0 53.8 65.9 59.8 56.6 68.3 65.0 80.4 77.0 3.76 66.7 59.7 74.2 36.2 54.7 57.4 28.0 47.5 68.6 25.4 16.2 45. 82.7 85.8 82.9 43.4 73.7 51.2 51.9 51.6 0.0 46.1 61.8 53.9 54.28 54.37 56.76 52. 54.16 53.09 We conduct comprehensive evaluation of all VLMs across six dimensions, encompassing total of 26 visual benchmarks. During evaluation, all models follow standardized thinking template formatgenerating reasoning within <think>...</think> tags followed by final answerto ensure consistency and fair comparison. For Qwen-2.5-VL-7B, we report two variants: one performing direct answer generation without the template (denoted as Qwen-2.5-VL-7B-Non-Thinking), and the other using the thinking template for long-cot reasoning to enhance the performance during inference (denoted as Qwen-2.5-VL-7B-Thinking). The detailed evaluation results are presented in Table 3. Perception & General VQA. On Perception & General VQA tasks, critic training demonstrates substantial improvement in the policy capabilities of VLMs. We select seven diverse benchmarks for evaluation, including Blink (Fu et al., 2024), HallusionBenchImage (Guan et al., 2024), MMStar (Chen et al., 2024), MMBench (Liu et al., 2024a), MMVP (Tong et al., 2024), MMHal (Sun et al., 2023), and RealWorldQA. LLaVA-Critic-R1 and 5 LLaVA-Critic-R LLaVA-Critic-R1+ achieve average performances of 60.7 and 61.4, respectivelyrepresenting improvements of +3.6% and +2.0% over their corresponding base models (Qwen-2.5-VL and ThinkLite-VL). On the seven evaluated benchmarks, LLaVA-Critic-R1+ attains the best performance on 6 out of 7 benchmarks, including (72.3), MMStar (65.7), MMBench (82.3), MMVP (77.3), and MMHal Blink (59.0), HallusionBench (3.88)."
        },
        {
            "title": "Image",
            "content": "Image Reasoning. On complex image reasoning tasks, LLaVA-Critic-R1 demonstrates strong capabilities. We evaluate the reasoning performance of various models on nine challenging benchmarks: MathVista (Lu et al., 2024), MathVision (Wang et al., 2024a), MathVerse (Zhang et al., 2024b), MMMU (Yue et al., 2024), EMMA (Hao et al., 2025), Blind (Rahmanzadehgervi et al., 2024), (Wu and Xie, 2023), VisuLogic (Xu et al., 2025), and ZeroBench (Roberts et al., 2025). LLaVA-Critic-R1+ outperforms ThinkLite-VL on 6 out of 9 benchmarks, achieving the best performance among all methods on MathVista (76.1), MMMU (58.8), Blind (51.3), and ZeroBench (19.8). Meanwhile, LLaVA-Critic-R1 shows consistent and significant improvements over Qwen-2.5-VL on 8 out of 9 benchmarks, with an average performance gain of 7.25%. These results underscore the substantial impact of critic training in enhancing the reasoning abilities of VLMs. Chart Understanding. Critic training leads to notable improvements in chart understanding. LLaVA-CriticR1 achieves the best performance on ChartQA (Masry et al., 2022)(85.4). Meanwhile, LLaVA-Critic-R1 + outperforms all other models on OCRBench (Liu et al., 2024b) (86.9), AI2D (Kembhavi et al., 2016) (83.8), and Charxiv (47.3), with particularly significant gains over Qwen-2.5-VL on the challenging Charxiv reasoning task (47.3 vs. 41.4). Moreover, compared to the performance gains achieved when using ThinkLite as the base model, critic training yields even more substantial improvements in chart understanding when applied to Qwen-2.5-VL (74.5 vs. 73.3 in average). Video Reasoning. We observe that critic training can also significant boost video reasoning ability. Evaluations on two video reasoning benchmarks, MMVU (Zhao et al., 2025b) and Video-MMMU (Hu et al., 2025), show that the base version of LLaVA-Critic-R1 attains the best performance on both. Specifically, it improves upon Qwen-2.5-VL from 45.7 to 51.7 on MMVU and from 46.9 to 54.7 on Video-MMMU, substantially outperforming other reasoning model baselines. VLM Agent. Surprisingly, we find that critic training also enhances the models capabilities as Graphic User Interface (GUI) agent of Qwen-2.5-VL. On OSWorld, performance improves from 2.7 to 3.5, and on Online-Mind2Web, it increases from 14.0 to 16.3, demonstrating the improved grounding and decisionmaking capabilities in complex, long-horizon tasks. In contrast, reasoning models with policy training lose their ability to function effectively as agentsall VL reasoning models fail to complete tasks correctly on OSWorld. In such cases, applying critic training does not recover the agent capabilities of these models. Visual RewardBench. We also evaluated the judgment capability of our critic model on the Visual RewardBench tasks. On two benchmarksVLRewardBench (Li et al., 2025) and MM-RLHF (Zhang et al., 2025c)both versions of LLaVA-Critic-R1 demonstrate strong performance as evaluators, achieving average scores of 68.1 and 64.9, respectively. Notably, we observe that policy training significantly degrades the critic capabilities of VLMs, as evidenced by the consistently poor performance of all reasoning VLMs on the visual reward benchmarks. In contrast, critic training effectively enhances both the critic and policy capabilities of the model. In summary, experimental results across massive visual benchmarks demonstrate that critic training not only substantially enhances the critic capabilities of VLMs, but also improves their performance as general policy across wide range of visual understanding and reasoning tasks. This dual improvement enables LLaVACritic-R1 to outperform other visual reasoning models trained with in-domain policy training, establishing it 6 as powerful VLM for both critic and policy roles. LLaVA-Critic-R1 In addition to employing Qwen-2.5-VL as the base model, we further More base model experiments. validate the generality of critic RL training for enhancing policy capability by applying it to both Mimo-VL and LLaMA-3.2-Vision using the same dataset. The corresponding experimental results are reported in Tables 9 and 10 of Appendix .1 and Appendix .2, respectively. We observe that critic RL training consistently yields significant improvements in both policy and critic performance, even when initialized from different base models. Notably, LLaVA-Critic-R1+ trained on Mimo-VL-RL achieves 7B-level state-of-the-art performance across multiple benchmarks, including MMMU (71.9), MathVista (82.1), MathVerse (74.1), and CharXivReasoning (62.5). These findings further substantiate the broad applicability and effectiveness of critic RL training in strengthening policy capability. 3.2. Test-Time Scaling with Self-Critic Given the strong dual capabilities of LLaVA-Critic-R1 as both policy and critic, this section further explores its potential as reward model by employing it for test-time scaling. This strategy aims to enhance the policy performance of LLaVA-Critic-R1 through self-critic during inference. Implementation. We begin by employing LLaVA-Critic-R1 as policy model to perform rounds of thinkingbased inference, generating candidate trajectories for each question. Subsequently, LLaVA-Critic-R1 serves as critic model to select the best response among the candidates through recursive pairwise comparison process. Specifically, all responses are paired, and for each pair, LLaVA-Critic-R1 evaluates and selects the more preferred one, resulting in n/2 candidates. This selection procedure is repeated iteratively until single response remains, which is then designated as the final answer. Since LLaVA-Critic-R1 is trained on data containing tie cases, it may sometimes assign equal preference scores to both responses in pair. In such cases, we randomly select one of the two responses to proceed to the next round. To ensure sufficient diversity among the generated trajectories during the thinking-based inference, we set the temperature parameter to 0.9. Baselines. We introduce three critic methods as baselines. The first is the Majority Vote, where we extract the predicted answers from the generated responses and select the most frequently occurring answer as the final output. The second is the Base Model Critic, where we use the corresponding base modelsQwen-2.5-VL-7B for the base version and ThinkLite-VL-7B for the policy versionas critic models to perform pairwise comparisons over the responses, recursively selecting one final answer. Besides, we include Ground Truth Oracle baseline to serve as an upper bound reference: if any of the generated answers match the ground-truth answer, the question is considered correctly answered. Results. We run 128 inferences for both LLaVA-Critic-R1 and LLaVA-Critic-R1+, and evaluate the Bestof-128 performance under different critic strategies. We conduct experiment on five multimodel reasoning and understanding benchmarks: MathVista, MathVision, MathVerse, MMMU, and MMStar. The results are summarized in Table 4. We observe that across all benchmarks, both versions of LLaVA-Critic-R1 achieve the highest performance when applying self-critic-based test-time scaling. Compared to the Majority Vote and Base Model Critic baselines, self-critic consistently yields substantial improvements. This highlights that, after dedicated critic training, LLaVA-Critic-R1 acquires significantly enhanced critic capabilities, enabling it to act as an effective critic without reliance on external supervisionthereby further improving policy performance through test-time self-critic. 7 LLaVA-Critic-R Table 4: Performance of test-time scaling with self-critic on 5 visual reasoning and understanding benchmarks. LLaVA-Critic-R1 and LLaVA-Critic-R1+ achieve average performance improvements of 16.5% and 11.1%, respectively, substantially outperforming both the majority vote baseline and approaches that employ the base model as the critic."
        },
        {
            "title": "MathVista MathVerse MathVision MMMU MMstar",
            "content": "LLaVA-Critic-R1 + Best-of-128 (GT) + Best-of-128 (Majority Vote) + Best-of-128 (Qwen-Critic) + Best-of-128 (Self-Critic) LLaVA-Critic-R1+ + Best-of-128 (GT) + Best-of-128 (Majority Vote) + Best-of-128 (ThinkLite-Critic) + Best-of-128 (Self-Critic) 74.0 94.8 76.4 77.1 78.9 76.1 92.7 77.3 77.9 78.7 49.7 87.5 54.7 55.2 60.9 52.9 86.2 56.9 57.5 61. 30.6 97.7 32.9 34.7 44.1 32.4 94.1 35.2 36.1 41.7 55.2 95.9 60.0 61.9 66.4 58.8 94.3 61.6 62.4 65.9 65.1 93.6 67.4 67.9 69.7 65.7 89.7 67.2 67.6 70. Specifically, the base model version of LLaVA-Critic-R1 achieves the best performance on MathVista (78.9), MathVision (44.1), and MMMU (66.4), while the policy model version performs best on MathVerse (61.3) and MMStar (70.1). It is worth noting that although LLaVA-Critic-R1+ exhibits stronger policy capabilities and outperforms the base model across various reasoning benchmarks, its weaker critic ability limits its effectiveness during test-time self-critic scaling, resulting in no significant performance advantage over the base model version in this setting. (a) MathVista (b) MathVerse (c) MathVision (d) MMMU (e) MMStar Figure 2: Test-time scaling curve of self-critic and majority vote on 5 visual reasoning and understanding benchmarks with LLaVA-Critic-R1. Self-critic yields significantly greater performance gains than majority vote, and reaches performance plateau at later sampling scale. Furthermore, despite the clear improvements brought by self-critic scaling, there remains substantial gap between its performance and the ground truth (GT) upper bound. In many cases, the model still fails to identify the correct answer from 128 generated responses, indicating that the current critic ability 8 LLaVA-Critic-R1 remains far from optimal. Importantly, such limitations are not captured by Visual RewardBench evaluations, underscoring the need for further advances in critic modeling. We also present the test-time scaling curve of the LLaVA-Critic-R1 base model across different benchmarks in Figure 2. The results demonstrate that as the scaling size increases from 2 to 64, the performance continues to improve, attributed to the models self-critic capability. The performance saturates around scaling size of 128. In contrast, the Majority Vote strategy reaches its performance ceiling much earlier, between scaling sizes 16 and 64, highlighting the advantage of self-critic during test-time scaling. 4. Ablation Studies 4.1. Ablation Study I: Why Critic Training can Improve Policy Capability? To elucidate the underlying reasons why RL-based critic training enhances models policy abilities, we consider two core hypotheses: Enhanced Visual Perception: Our constructed 40K critic training dataset consists of response pairs generated by various VLMs on natural image captioning and question-answering tasks. This setup requires the model to autonomously determine which response is more aligned with the image and to identify hallucinated content, thereby enhancing the models visual perception capabilities. Structured Reasoning: The format reward in GRPO training encourages the model to strictly follow \"think-then-answer\" generation pattern. This enforced reasoning structure strengthens the models step-by-step reasoning ability, which is critical for improving policy effectiveness in complex tasks. To validate our two hypotheses, we design and conduct two sets of controlled experiments. First, during the inference stage, we apply the thinking template from Table 2 to the Qwen-2.5-VL-7B, forcing it to perform \"think-then-answer\" inference process without any additional training. Second, using 40K critic data for GRPO training, we set the weight of the format reward to 1 and the weight of the answer reward to 0. This allows the model to learn exclusively how to follow the \"think-then-answer\" format, without acquiring any critic-related knowledge. The experimental results are presented in Table 5. With the exception of the Qwen-2.5-VL-7B-non-thinking mode, all models are prompted with the thinking template during inference. We find that in the setting with inference-time thinking alone, Qwen-2.5-VL-7B-Thinking shows significant performance improvement in both visual reasoning (42.1 to 43.2) and video reasoning (46.3 to 49.3) domains. After further GRPO training with the format reward to enforce stricter adherence to the thinking template, the models performance in these two domains improves further to 43.9 and 49.5, respectively. However, on other tasks that require stronger visual perception capabilities, the models performance degrades under both of these settings. In contrast, following critic training, LLaVA-Critic-R1 demonstrates notable performance increase on several Perception & General VQA and Chart Understanding benchmarks, indicating that its perception capabilities are enhanced. Furthermore, because critic training also implicitly forces the model to follow the thinking template, LLaVA-Critic-R1s performance in the two reasoning domains is further boosted to 45.8 and 53.2. This suggests that enhancing perception capabilities also contributes to performance improvements on visual reasoning tasks. These two experiments collectively validate our central hypothesis: the enhancement of policy ability through critic training stems from two synergistic factors: stricter ability to follow the thinking template and stronger visual perception capability gained from the data. These two abilities mutually reinforce each 9 LLaVA-Critic-R1 Table 5: Performance of Qwen-2.5-VL-7B base models, format reward only RL trained models (Format-RFT), and models with full critic training (LLaVA-Critic-R1). Bolded values indicate the best performance within each benchmark category."
        },
        {
            "title": "Benchmark",
            "content": "Qwen (Non-Thinking) Qwen (Thinking) Format-RFT LLaVA-Critic-R1 Perception & General VQA"
        },
        {
            "title": "Blink\nHallusionBenchImage\nMMStar\nMMBench\nMMVP\nMMHal\nRealWorldQA",
            "content": "Avg. MathVistatestmini MathVisiontestmini MathVersetestmini MMMUval EMMAmini Blind VisuLogic ZeroBenchsub Avg. ChartQA OCRBench AI2D Charxivreasoning Avg."
        },
        {
            "title": "MMVUval\nVideoMMMU",
            "content": "Avg."
        },
        {
            "title": "Visual\nRewardBench",
            "content": "VLRewardBench MM-RLHF Avg. Overall Avg. 54.2 67.9 61.7 80.7 72.9 3.80 68.8 58.6 67.8 22.1 44.5 50.6 24.8 49.3 79.0 26.9 13. 42.1 84.0 85.1 82.6 41.4 73.3 45.7 46.9 46.3 50.7 71. 61.3 53.9 67.2 62.5 78.9 69.7 3.79 66.1 57.4 69.6 24.3 47.2 53.8 26.0 48.7 77.4 26.0 15.8 43.2 84.4 85.3 79.9 41. 72.9 49.4 49.1 49.3 52.1 71.8 62.0 54.2 66.9 62.9 77.9 70.3 3.72 66. 57.4 70.2 25.6 47.9 54.7 26.8 48.9 79.6 25.0 16.5 43.9 84.2 85.5 79.8 42.3 73.0 48.7 50. 49.5 55.6 69.6 62.6 58.3 71.3 65.1 80.9 75.6 3.86 69.7 60.7 74.0 30.6 49.7 55.2 28.3 48.2 81.5 28.0 16. 45.8 85.4 86.1 83.2 43.1 74.5 51.7 54.7 53.2 61.5 74. 68.1 54.28 54.37 54.72 57.38 other, leading to significant improvement in the models overall policy performance. 4.2. Ablation Study II: Critic/Policy Training Synergy In this section, we conduct an ablation study to systematically explore how to best train model that achieves both strong policy performance and enhanced critic capability, given access to both critic and policy data. All our training is based on the Qwen-2.5-VL-7B model, using GRPO as the objective for RFT. For data sources, we use 40K pairwise critic data that we curated and 70K policy data from the ThinkLite-70K dataset, which 10 Table 6: An example where Qwen-2.5-VL follows the thinking template but fails in reasoning, while LLaVA-Critic-R1+ correctly interprets chart details and derives the accurate answer. LLaVA-Critic-R Example 1 Question: According to the image, in 2018, did Avengers: Infinity War achieve the highest box office revenue in North America? No LLaVA-Critic-R1+ Answer: Qwen-2.5-VL-7B-Thinking <think> The image is bar chart showing the box office revenue of the highest-grossing movies in North America in 2018. The chart lists Avengers: Infinity War at the top with revenue of 678.82 million U.S. dollars. There is no movie listed with higher revenue than Avengers: Infinity War in the chart. Therefore, Avengers: Infinity War achieved the highest box office revenue in North America in 2018. </think> Yes <think> To determine if Avengers: Infinity War achieved the highest box office revenue in North America in 2018, Ill follow these steps: 1. Identify the movie with the highest box office revenue from the chart. 2. Compare the revenue of Avengers: Infinity War with the highest revenue. Step 1: The chart shows Black Panther with the highest box office revenue at 700.06 million U.S. dollars. Step 2: Avengers: Infinity War has box office revenue of 678.82 million U.S. dollars, which is less than the revenue of Black Panther. Therefore, Avengers: Infinity War did not achieve the highest box office revenue in North America in 2018. </think> No includes multimodal math reasoning, natural image understanding, and chart understanding tasks. We explore three data configurations to assess their impact on model performance: (1) training with critic data only, (2) training with policy data only, and (3) joint training with both critic and policy data. In the first setting, the model trained solely on critic data is referred to as LLaVA-Critic-R1. In the second, the model trained on ThinkLite-70K is denoted as ThinkLite-VL-7B. For the third setting involving both data types, we investigate three training strategies: Mixed Training: directly combining critic and policy data for RFT; Critic-then-Policy Training: starting from the LLaVA-Critic-R1, followed by RFT on policy data (ThinkLite11 LLaVA-Critic-R1 70K); Policy-then-Critic Training: initializing from ThinkLite-VL-7B and further training on 40K critic data. In Table 7, we report the evaluation results of models trained with five different strategies across various benchmark categories. We find that LLaVA-Critic-R1, trained from the base model using only critic data, demonstrates the strongest critic capability, achieving the highest performance on visual reward benchmarks (68.1), along with moderate improvement in policy performance. In contrast, ThinkLite-VL, trained solely on policy data, exhibits strong policy performance across multiple benchmarks (average score of 56.72), but its critic capability remains suboptimal (57.2) due to the absence of critic training. Among the three models trained with both critic and policy data, we observe the following: Mixed training improves both policy and critic capabilities to some extent, but falls short of the performance achieved by models trained on individual datasets alone. Notably, its policy performance is significantly lower than that of ThinkLite-VL-7B. Critic-then-policy training slightly enhances policy performance (from 56.41 to 56.53) but causes notable degradation in critic ability (from 68.1 to 62.9). Policy-then-critic training emerges as the optimal strategy: continuing critic training from the ThinkLiteVL checkpoint not only improves policy performance (from 56.72 to 57.54) but also substantially recovers the critic capability that is weakened by policy training (from 57.2 to 64.9), thereby achieving better balance between the two competencies. 4.3. Ablation Study III: On the Best Utilization of Critic Data Given that supervised fine-tuning (SFT) has traditionally been the dominant paradigm for training critic models, in this section, we conduct an ablation study to investigate the impact of SFT on critic model performance under the assumption of access to high-quality chain-of-thought (CoT) critic data. We first explore three different SFT strategies: (1) fine-tuning only the LLM parameters, (2) jointly fine-tuning the LLM and the projection layer, and (3) full-parameter fine-tuning. Based on the checkpoints obtained from each strategy, we subsequently perform critic RFT. For SFT, we use the LLaVA-Critic-113K dataset (Xiong et al., 2025), which comprises 73K pointwise critic data and 40K pairwise critic data. Each data point includes not only the ground truth score but also reasoning explanation generated by GPT to justify the assigned score. For the RFT stage, we consistently use 40K pairwise critic samples with ground truth labels only. To ensure fair comparison, we also performe an additional SFT experiment using only the 40K pairwise critic data from LLaVA-Critic-113K, matching the data size used in LLaVA-Critic-R1. We still conduct evaluations on range of visual reasoning and understanding benchmarks, with results presented in Table 8. The results indicate that applying SFT-based knowledge distillation does not lead to significant improvements in model performance on the two visual reward benchmarks (VL-RewardBench and MM-RLHF). Moreover, this form of distillation substantially degrades the models policy capability, resulting in poor performance across several general visual benchmarks. This observation suggests that relying solely on SFT may impair the models generalization abilitya conclusion that has also been reported in recent study (Zhang et al., 2025a). In contrast, models trained via RFT based on the three different SFT checkpoints show substantial improvements on the reward benchmarks. Notably, the model initialized from the LLM-only SFT checkpoint achieves the highest performance on MM-RLHF (77.0). However, on VL-RewardBench, LLaVA-Critic-R1 which is 12 LLaVA-Critic-R1 Table 7: Ablation study of training strategies using critic and policy data. Bolded values indicate the best performance within each benchmark category."
        },
        {
            "title": "Benchmark",
            "content": "LLaVA-Critic-R1 ThinkLite-VL-7B Mixed Data Critic > Policy Policy > Critic"
        },
        {
            "title": "Policy Training",
            "content": "Critic Training + Policy Training"
        },
        {
            "title": "Image\nReasoning",
            "content": "Image"
        },
        {
            "title": "Blink\nHallusionBench\nMMStar\nMMBench\nMMVP\nMMHal\nRealWorldQA",
            "content": "Avg. testmini testmini testmini val MathVista MathVision MathVerse MMMU EMMA Blind VisuLogic ZeroBench mini sub"
        },
        {
            "title": "Chart\nUnderstanding",
            "content": "Avg. ChartQA OCRBench AI2D Charxiv reasoning Avg."
        },
        {
            "title": "MMVU\nVideoMMMU",
            "content": "val Avg."
        },
        {
            "title": "Visual\nRewardBench",
            "content": "VLRewardBench MM-RLHF Avg. Overall Avg. w. RewardBench Overall Avg. w.o. RewardBench 58.3 71.3 65.1 80.9 75.6 3.86 69.7 60. 74.0 30.6 49.7 55.2 28.3 48.2 81.5 28.0 16.5 45.8 85.4 86.1 83.2 43.1 74.5 51.7 54.7 53. 61.5 74.7 68.1 57.38 56.41 58.5 70.9 65.0 81.4 75.3 3.83 66.8 60. 75.1 32.9 52.1 55.5 29.8 49.2 83.3 25.5 18.9 46.9 84.8 86.7 83.6 45.8 75.2 49.8 53.2 51. 47.3 67.0 57.2 56.9 69.9 63.8 80.3 74.0 3.81 69.0 59.7 75.0 28.3 49.8 55.8 26.0 49.5 80.6 25.4 15.3 45. 84.1 86.3 82.5 44.2 74.3 49.1 52.1 50.6 58.6 71.2 64. 56.4 71.0 64.1 81.4 70.0 3.82 69.9 59.5 74.9 30.9 51.4 57.7 27.0 52.1 80.9 26.3 19.2 46.7 84.9 86.8 83.4 46.5 75. 50.1 55.0 52.6 58.1 67.6 62.9 59.0 72.3 65.7 82.3 77.3 3.88 69.2 61. 76.1 32.4 52.9 58.8 28.5 51.3 81.7 27.3 19.8 47.7 84.2 86.9 83.8 47.3 75.6 50.9 54.4 52. 59.8 70.0 64.9 56.76 56.72 56.31 55. 57.06 56.53 58.16 57.54 trained directly via RFT without any prior SFTachieves the best result. Besides, LLaVA-Critic-R1 consistently maintains the strongest policy performance among all evaluated models on general visual tasks. These results suggest that if the primary goal is to build critic model with strong judgment capabilities, performing long-CoT SFT followed by RFT offers certain advantages over cold-start RFT. However, if the objective is to achieve balanced capability of both policy and critic, the most effective strategy is to start from strong policy/reasoning model and directly apply cold-start RFT. 13 LLaVA-Critic-R1 Table 8: Ablation study of critic data utilization strategies. Models are trained with SFT, RFT, or both. Bolded values indicate the best performance within each benchmark category."
        },
        {
            "title": "Benchmark",
            "content": "Qwen-2.5-VL-7B LLM LLM+MMP"
        },
        {
            "title": "Full",
            "content": "Pairwise-Only LLM LLM+MMP"
        },
        {
            "title": "Full",
            "content": "LLaVA-Critic-R"
        },
        {
            "title": "SFT",
            "content": "SFT+RFT"
        },
        {
            "title": "Image\nReasoning",
            "content": "Image"
        },
        {
            "title": "Blink\nHallusionBench\nMMStar\nMMBench\nMMVP\nMMHal\nRealWorldQA",
            "content": "Avg. testmini testmini testmini val MathVista MathVision MathVerse MMMU EMMA Blind VisuLogic ZeroBench mini sub"
        },
        {
            "title": "Chart\nUnderstanding",
            "content": "Avg. ChartQA OCRBench AI2D Charxiv reasoning Avg."
        },
        {
            "title": "MMVU\nVideoMMMU",
            "content": "val Avg."
        },
        {
            "title": "Visual\nRewardBench",
            "content": "VLRewardBench MM-RLHF Avg. Overall Avg. 54.2 67.9 61.7 80.7 72.9 3.80 68.8 58.6 67.8 22.1 44.5 50.6 24.8 49.3 79.0 26.9 13. 42.1 84.0 85.1 82.6 41.4 73.3 45.7 46.9 46.3 50.7 71. 61.3 53.0 68.8 61.1 76.6 69.3 3.82 64.8 56.8 66.4 31.3 48.6 53.1 25.5 49.5 80.6 25.3 16.2 44.1 83.3 85.1 77.9 40. 71.7 48.6 50.1 49.4 52.5 72.5 62.5 52.9 68.2 61.1 77.1 68.0 3.80 65. 56.7 65.3 31.3 47.8 53.1 23.8 49.4 78.5 25.7 14.1 43.2 83.3 85.8 78.0 39.0 71.5 48.7 49. 49.3 51.6 72.9 62.3 54.5 67.5 61.7 77.1 70.7 3.78 64.6 57.1 67.3 28.9 47.8 54.2 25.8 48.5 79.6 25.2 15. 43.7 82.8 84.8 78.1 38.8 71.1 47.4 50.0 48.7 52.2 71. 61.9 51.4 66.9 59.7 77.5 69.0 3.79 63.7 56.0 66.5 29.3 45.9 53.7 26.0 46.7 78.0 24.8 15.3 42.9 82.3 85.5 79.7 39. 71.7 47.0 49.7 48.4 51.9 72.1 62.0 56.4 70.9 61.6 80.5 74.3 3.84 67. 59.2 72.9 27.6 49.3 52.9 26.3 49.8 79.1 24.5 15.3 44.2 84.9 85.9 81.9 41.1 73.5 50.8 52. 51.7 57.9 77.0 67.5 56.5 70.1 63.0 80.1 72.7 3.83 68.6 59.3 70.3 31.6 48.8 56.4 26.8 49.8 79.0 24.8 16. 44.9 83.2 86.3 81.2 41.5 73.1 49.4 53.0 51.2 56.8 75. 66.1 55.0 68.0 60.7 79.8 72.3 3.81 69.5 58.4 69.8 29.6 47.1 56.0 28.3 48.8 78.0 26.3 16.8 44.5 81.3 86.9 81.5 38. 72.1 49.5 52.9 51.2 57.7 71.2 64.5 58.3 71.3 65.1 80.9 75.6 3.86 69. 60.7 74.0 30.6 49.7 55.2 28.3 48.2 81.5 28.0 16.5 45.8 85.4 86.1 83.2 43.1 74.5 51.7 54. 53.2 61.5 74.7 68.1 54.28 54.35 53. 54.11 53.56 56.01 56.06 55.39 57. 4.4. Ablation Study IV: On the correction between Critic and Policy Ability In this section, we focus on analyzing the relationship between the models critic capability and its policy performance during critic training. Figure 3 illustrates the evolution of critic and policy performance over the critic RFT steps. Here, critic performance is measured on two visual reward benchmark, while policy performance is gauged on other general visual tasks in Table 3. striking observation is the strong positive correlation between the two curves during the initial phase of training (approximately steps 0 to 200). The continuous improvement in critic performance (red line) is accompanied by steady increase in policy performance (blue line). In the latter half of the training (approximately steps 200 to 400), this strong correlation appears to weaken. The critic performance curve begins to show fluctuations, while the policy performance curve peaks around step 350 and then starts to decline. This divergence could be attributed to various factors, such as the onset of overfitting, where LLaVA-Critic-R1, despite performing well on the reward benchmark, may provide feedback signals that are no longer optimal for the general task, consequently leading to degradation in policy performance. Nevertheless, the overall trend throughout the training process remains largely consistent, confirming significant relationship between critic and policy capabilities. 14 LLaVA-Critic-R1 Figure 3: Correlation between LLaVA-Critic-R1s critic and policy performance. This figure plots the performance of LLaVA-Critic-R1s two key capabilities as function of critic training steps. The blue curve represents the policy performance on general visual tasks, while the red curve denotes the critic performance on visual reward benchmarks. The strong positive correlation between the two performance metrics throughout the training process provides empirical evidence that improvements in the models critic ability are directly linked to enhancements in its policy ability. 5. Related Work Multimodal Critic Model Training. Critic model training has evolved from scalar approaches using BradleyTerry modeling (Ouyang et al., 2022, Gao et al., 2023) to more sophisticated architectures. Critic model provides step-wise supervision for complex reasoning (Lightman et al., 2023, Cobbe et al., 2021), while generative critic models enable richer feedback through natural language (Mahan et al., 2024, Zhang et al., 2024a, Zheng et al., 2023). For vision-language models, various approaches adopt distinct strategies for critic training. Wang et al. (2024c) and Wang et al. (2025c) integrate an external critic head into the VLM, which is trained via SFT to directly output step-wise scores during inference time. Zhang et al. (2025c) and Xiong et al. (2024) leverage knowledge distillation to train generative critic model, thereby providing richer visual feedback for both pairwise and pointwise data. In this paper, we investigate an approach that restructures critic data into verifiable RL task and trains generative critic model via RL, without relying on knowledge distillation. Zhang et al. (2025b) also explore this direction, but their method first applies knowledge distillation before RL training, and their analysis is limited to the impact on critic capability. In contrast, our work focuses on examining the effect of critic training on downstream general task performance, i.e. policy capability. Furthermore, as detailed in Section 4.2, our ablation study offers more systematic and comprehensive analysis of the integration between supervised fine-tuning (SFT) and RL. VLM Reasoning. Reinforcement fine-tuning has emerged as powerful paradigm for enhancing reasoning capabilities in vision-language models, extending successful approaches from text-only models like DeepSeekR1 (DeepSeek-AI, 2025). Recent studies have demonstrated that reinforcement fine-tuning can facilitate advanced visual reasoning through various strategies, ranging from distilling reasoning knowledge from stronger models before applying RL (Huang et al., 2025a, Deng et al., 2025, Wan et al., 2025, Wei et al., 2025), to exploring multimodal training data selection methods (Meng et al., 2025, Peng et al., 2025, Wang et al., 2025e,a,d) and refining GRPO for multimodal RL (Wang et al., 2025a, Liu et al., 2025, Wan et al., 2025, Yao et al., 2025, Yuan et al., 2025). However, these works predominantly employ multimodal reasoning and understanding data with ground truth answers directly for reinforcement fine-tuning. In this paper, we explore RL training from the perspective of leveraging critic data, showing that critic reinforcement fine-tuning can also effectively boost models policy capability. Moreover, we provide empirical evidence 15 LLaVA-Critic-R1 that critic training and policy training are orthogonal in the context of reinforcement fine-tuning to further improve the policy ability of VLMs. Test-Time Scaling. Recent advances demonstrate that scaling compute at inference time can be as effective as scaling model parameters (Snell et al., 2024), opening new avenues for improving model performance without retraining. Test-time scaling approaches broadly encompass: (1) search and sampling methods, where Monte Carlo Tree Search (Xie et al., 2024, Gao et al., 2024) and compute-optimal strategies (Snell et al., 2024, Wu et al., 2024) surpass simple best-of-N baselines through adaptive compute allocation; (2) long-horizon chain-of-thought reasoning (Wei et al., 2022), exemplified by OpenAI o1 (OpenAI, 2024, Jaech et al., 2024) and DeepSeek-R1 (DeepSeek-AI, 2025), which generate extensive internal reasoning chains with performance scaling smoothly with thinking time; and (3) critic model-guided aggregation and verification (Lightman et al., 2024, Brown et al., 2024), where process reward models (Zhao et al., 2025a, Khalifa et al., 2025) and Q-learning-based verifiers (Qi et al., 2024) critic model (Zhao et al., 2025a, Khalifa et al., 2025, Wang et al., 2024c, 2025c) guide selection among multiple outputs. Our work bridges these paradigms by training unified model with both strong policy and critic ability through reinforcement learning that enables effective self-critic at test time. Unlike existing approaches that rely on external verifiers or massive search, our method develops intrinsic judgment capabilities within the model itself, achieving test-time scaling through self-evaluation that surpasses traditional majority voting and approaches oracle performance on challenging visual reasoning tasks, offering scalable alternative to compute-intensive search methods while contributing to the broader goal of inference-time co-scaling between policy models and critic models. 6. Conclusion We find that critic RL training, originally intended to enhance the critic ability, can surprisingly deliver substantial improvement in policy performance, producing single model that excel in both critic and policy roles. This is achieved by reorganizing preference-labeled critic data into verifiable training signals and applying RL training to directly optimize multimodal judgment. Results on series of visual benchmarks further demonstrate the strong policy and critic abilities of the resulting models LLaVA-Critic-R1 and LLaVACritic-R1+. Furthermore, the strengthened critic enables test-time scaling for additional gains, highlighting its broader utility in multimodal learning. We hope the insights inspire further research to develop selfimprovement AI, where the model can make useful judgment its own response, and thus provide feedback for self-improvement."
        },
        {
            "title": "Acknowledgment",
            "content": "Wang and Huang are supported by DARPA Transfer from Imprecise and Abstract Models to Autonomous Technologies (TIAMAT) 80321, DARPA HR001124S0029-AIQ-FP-019, DOD-AFOSR-Air Force Office of Scientific Research under award number FA9550-23-1-0048, National Science Foundation NSF-IIS-2147276 FAI, National Science Foundation NAIRR240045, National Science Foundation TRAILS Institute (2229885). Private support was provided by Peraton."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 16 LLaVA-Critic-R1 Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. Computing Research Repository, arXiv:2407.21787, 2024. URL https://arxiv.org/abs/2407.21787. Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Google Deepmind. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities, 2025. URL https://arxiv.org/abs/2507.06261. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement, 2025. URL https: //arxiv.org/abs/2503.17352. Xingyu Fu, Yushi Hu, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, and Ranjay Krishna. Blink: Multimodal large language models can see but not perceive, 2024. URL https://arxiv.org/abs/2404.12390. Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pages 1083510866. PMLR, 2023. Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. Interpretable contrastive monte carlo tree search reasoning. arXiv preprint arXiv:2410.01707, 2024. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models, 2024. URL https://arxiv.org/abs/2310.14566. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Videommmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025a. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models, 2025b. URL https://arxiv.org/abs/2503.06749. 17 LLaVA-Critic-R1 Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. Muhammad Khalifa, Rishabh Agarwal, Lajanugen Logeswaran, Jaekyeom Kim, Hao Peng, Moontae Lee, Honglak Lee, and Lu Wang. Process reward models that think. arXiv preprint arXiv:2504.16828, 2025. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, Lingpeng Kong, and Qi Liu. Vl-rewardbench: challenging benchmark for vision-language generative reward models, 2025. URL https://arxiv.org/abs/2411.17451. Hunter Lightman, Karl Cobbe, Vineet Kosaraju, Yura Burda, Harri Edwards, Jan Leike, and Ilya Sutskever. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. URL https://arxiv.org/abs/2305. 20050. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=v8L0pN6EOi. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Xiangyan Liu, Jinjie Ni, Zijian Wu, Chao Du, Longxu Dou, Haonan Wang, Tianyu Pang, and Michael Qizhe Shieh. Noisyrollout: Reinforcing visual reasoning with data augmentation. arXiv preprint arXiv:2504.13055, 2025. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024a. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024b. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Fränken, Chelsea Finn, and Alon Albalak. Generative reward models. arXiv preprint arXiv:2410.12832, 2024. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 18 LLaVA-Critic-R1 Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, Ping Luo, Yu Qiao, Qiaosheng Zhang, and Wenqi Shao. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. URL https://arxiv.org/abs/2503.07365. OpenAI. Gpt-4v(ision) system card. 2023. URL https://api.semanticscholar.org/CorpusID: 263218031. OpenAI. Learning to reason with llms, 2024. learning-to-reason-with-llms/. URL https://openai.com/index/ Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. URL https://arxiv.org/abs/2503.07536. Jianing Qi, Hao Tang, and Zhigang Zhu. Verifierq: Enhancing llm test time compute with q-learning-based verifiers. arXiv preprint arXiv:2410.08048, 2024. Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind. In Proceedings of the Asian Conference on Computer Vision, pages 1834, 2024. Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, Vatsal Raina, Hanyi Xiong, Vishaal Udandarao, Jingyi Lu, Shiyang Chen, Sam Purkis, Tianshuo Yan, Wenye Lin, Gyungin Shin, Qiaochu Yang, Anh Totti Nguyen, David I. Atkinson, Aaditya Baranwal, Alexandru Coca, Mikah Dang, Sebastian Dziadzio, Jakob D. Kunz, Kaiqu Liang, Alexander Lo, Brian Pulfer, Steven Walton, Charig Yang, Kai Han, and Samuel Albanie. Zerobench: An impossible visual benchmark for contemporary large multimodal models, 2025. URL https://arxiv.org/abs/2502.09696. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf, 2023. URL https://arxiv.org/abs/2309.14525. Core Team, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, 19 LLaVA-Critic-R1 Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, and Bingquan Xia. Mimo-vl technical report, 2025a. URL https://arxiv.org/abs/2506.03569. Team, Wenyi Hong, Wenmeng Yu, Xiaotao Gu, Guo Wang, Guobing Gan, Haomiao Tang, Jiale Cheng, Ji Qi, Junhui Ji, Lihang Pan, Shuaiqi Duan, Weihan Wang, Yan Wang, Yean Cheng, Zehai He, Zhe Su, Zhen Yang, Ziyang Pan, Aohan Zeng, Baoxu Wang, Boyan Shi, Changyu Pang, Chenhui Zhang, Da Yin, Fan Yang, Guoqing Chen, Jiazheng Xu, Jiali Chen, Jing Chen, Jinhao Chen, Jinghao Lin, Jinjiang Wang, Junjie Chen, Leqi Lei, Letian Gong, Leyi Pan, Mingzhi Zhang, Qinkai Zheng, Sheng Yang, Shi Zhong, Shiyu Huang, Shuyuan Zhao, Siyan Xue, Shangqin Tu, Shengbiao Meng, Tianshu Zhang, Tianwei Luo, Tianxiang Hao, Wenkai Li, Wei Jia, Xin Lyu, Xuancheng Huang, Yanling Wang, Yadong Xue, Yanfeng Wang, Yifan An, Yifan Du, Yiming Shi, Yiheng Huang, Yilin Niu, Yuan Wang, Yuanchang Yue, Yuchen Li, Yutao Zhang, Yuxuan Zhang, Zhanxiao Du, Zhenyu Hou, Zhao Xue, Zhengxiao Du, Zihan Wang, Peng Zhang, Debing Liu, Bin Xu, Juanzi Li, Minlie Huang, Yuxiao Dong, and Jie Tang. Glm-4.1v-thinking: Towards versatile multimodal reasoning with scalable reinforcement learning, 2025b. URL https://arxiv.org/abs/2507.01006. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms, 2024. URL https://arxiv.org/abs/2401.06209. Zhongwei Wan, Zhihao Dou, Che Liu, Yu Zhang, Dongfei Cui, Qinjian Zhao, Hui Shen, Jing Xiong, Yi Xin, Yifan Jiang, et al. Srpo: Enhancing multimodal llm reasoning via reflection-aware reinforcement learning. arXiv preprint arXiv:2506.01713, 2025. Haozhe Wang, Chao Qu, Zuming Huang, Wei Chu, Fangzhen Lin, and Wenhu Chen. Vl-rethinker: Incentivizing self-reflection of vision-language models with reinforcement learning. arXiv preprint arXiv:2504.08837, 2025a. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024a. URL https://openreview. net/forum?id=QWTCcxMpPA. Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, Jinguo Zhu, Xiangyu Zhao, Yangzhou Liu, Yue Cao, Shenglong Ye, Xizhou Zhu, Lewei Lu, Haodong Duan, Yu Qiao, Jifeng Dai, and Wenhai Wang. Visualprm: An effective process reward model for multimodal reasoning, 2025b. URL https://arxiv.org/abs/ 2503.10291. Weiyun Wang, Zhangwei Gao, Lianjie Chen, Zhe Chen, et al. Visualprm: An effective process reward model for multimodal reasoning. arXiv preprint arXiv:2503.10291, 2025c. Xiyao Wang, Jiuhai Chen, Zhaoyang Wang, Yuhang Zhou, Yiyang Zhou, Huaxiu Yao, Tianyi Zhou, Tom Goldstein, Parminder Bhatia, Furong Huang, et al. Enhancing visual-language modality alignment in large vision language models via self-improvement. arXiv preprint arXiv:2405.15973, 2024b. Xiyao Wang, Zhengyuan Yang, Linjie Li, Hongjin Lu, Yuancheng Xu, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Scaling inference-time search with vision value model for improved visual comprehension. arXiv preprint arXiv:2412.03704, 2024c. Xiyao Wang, Zhengyuan Yang, Chao Feng, Yongyuan Liang, Yuhang Zhou, Xiaoyu Liu, Ziyi Zang, Ming Li, Chung-Ching Lin, Kevin Lin, et al. Vicrit: verifiable reinforcement learning proxy task for visual perception in vlms. arXiv preprint arXiv:2506.10128, 2025d. 20 LLaVA-Critic-R1 Xiyao Wang, Zhengyuan Yang, Chao Feng, Hongjin Lu, Linjie Li, Chung-Ching Lin, Kevin Lin, Furong Huang, and Lijuan Wang. Sota with less: Mcts-guided sample selection for data-efficient visual reasoning self-improvement. arXiv preprint arXiv:2504.07934, 2025e. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yana Wei, Liang Zhao, Jianjian Sun, Kangheng Lin, Jisheng Yin, Jingcheng Hu, Yinmin Zhang, En Yu, Haoran Lv, Zejia Weng, et al. Open vision reasoner: Transferring linguistic cognitive behavior for visual reasoning. arXiv preprint arXiv:2507.05255, 2025. Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms, 2023. URL https://arxiv.org/abs/2312.14135. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. Yuxi Xie, Anirudh Goyal, Wenyue Zheng, Min-Yen Kan, Timothy Lillicrap, Kenji Kawaguchi, and Michael Q. Shieh. Monte carlo tree search boosts reasoning via iterative preference learning. arXiv preprint arXiv:2405.00451, 2024. URL https://arxiv.org/abs/2405.00451. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. arXiv preprint arXiv:2410.02712, 2024. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1361813628, 2025. Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, and Jinguo Zhu. Visulogic: benchmark for evaluating visual reasoning in multi-modal large language models, 2025. URL https://arxiv.org/abs/2504.15279. Huanjin Yao, Qixiang Yin, Jingyi Zhang, Min Yang, Yibo Wang, Wenhao Wu, Fei Su, Li Shen, Minghui Qiu, Dacheng Tao, et al. R1-sharevl: Incentivizing reasoning capability of multimodal large language models via share-grpo. arXiv preprint arXiv:2505.16673, 2025. Ruifeng Yuan, Chenghao Xiao, Sicong Leng, Jianyu Wang, Long Li, Weiwen Xu, Hou Pong Chan, Deli Zhao, Tingyang Xu, Zhongyu Wei, et al. Vl-cogito: Progressive curriculum reinforcement learning for advanced multimodal reasoning. arXiv preprint arXiv:2507.22607, 2025. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multidiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, Kai Chen, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2.5-reward: simple yet effective multi-modal reward model, 2025. URL https://arxiv.org/abs/2501.12368. 21 LLaVA-Critic-R1 Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. arXiv preprint arXiv:2408.15240, 2024a. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024b. Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. Nemotron-research-tool-n1: Exploring tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024, 2025a. Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, Tianke Zhang, Changyi Liu, Kaiyu Jiang, Kaibing Chen, Kaiyu Tang, Haojie Ding, Jiankang Chen, Fan Yang, Zhang Zhang, Tingting Gao, and Liang Wang. R1-reward: Training multimodal reward model through stable reinforcement learning, 2025b. URL https://arxiv.org/abs/2505.02835. Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, Xue Wang, Yibo Hu, Bin Wen, Fan Yang, Zhang Zhang, Tingting Gao, Di Zhang, Liang Wang, Rong Jin, and Tieniu Tan. Mm-rlhf: The next step forward in multimodal llm alignment, 2025c. URL https://arxiv.org/abs/2502.10391. Jian Zhao, Runze Liu, Kaiyan Zhang, Zhimu Zhou, Junqi Gao, Dong Li, Jiafei Lyu, Zhouyi Qian, Biqing Qi, Xiu Li, et al. Genprm: Scaling test-time compute of process reward models via generative reasoning. arXiv preprint arXiv:2504.00891, 2025a. Yilun Zhao, Haowei Zhang, Lujing Xie, Tongyan Hu, Guo Gan, Yitao Long, Zhiyuan Hu, Weiyuan Chen, Chuhan Li, Zhijian Xu, et al. Mmvu: Measuring expert-level multi-discipline video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 84758489, 2025b. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. 22 LLaVA-Critic-R"
        },
        {
            "title": "Appendix",
            "content": ".1. Critic RL Training on Mimo-VL In this section, we perform critic RL training based on the state-of-the-art reasoning vision-language model, MiMo-VL (Team et al., 2025a). Specifically, we utilize two versions of MiMo-VLMiMo-VL-7B-SFT-2508 and MiMo-VL-7B-RL-2508as base models to develop two variants of LLaVA-Critic-R1+. The evaluation results on 24 visual reasoning and understanding benchmarks are presented in Table 9. The experimental results demonstrate that both versions of LLaVA-Critic-R1+ exhibit substantial performance improvements over their respective base models across most of visual reasoning and understanding tasks. Notably, the variant based on MiMo-VL-7B-RL-2508 achieves SOTA performance among all 7B-scale VLMs on several challenging benchmarks, including MMMU (71.8), MathVista (82.1), MathVerse (74.1), and Charxiv Reasoning (62.5), surpassing previously reported best results by significant margin. Furthermore, on both VLM reward benchmarks, critic RL training yields consistently large gains, highlighting its effectiveness in enhancing the models critic capability. In terms of average performance, LLaVA-CriticR1+ improves upon its base models by 2.1% (MiMo-VL-7B-SFT-2508) and 2.6% (MiMo-VL-7B-RL-2508), respectively, establishing it as the most capable 7B-level vision-language model for reasoning to date. These results further substantiate the effectiveness of our approach in enhancing the policy capabilities of VLMs. .2. Critic RL Training on LLaMA-3.2-Vision In this section, we conduct critic RL training on Llama-3.2-11B-Vision-Instruct to further verify the effectiveness of our approach. Since Llama-3.2-11B-Vision-Instruct does not accept video inputs, we evaluate the policy capability of LLaVA-Critic-R1 on 20 visual understanding and reasoning benchmarks. As shown in Table 10, critic RL training consistently improves the policy performance of LLaMA-3.2-Vision, yielding an average gain of 2.6% across the 20 tasks. The improvements are particularly pronounced on reasoning tasks, with MMMU increasing by 13.4% (from 43.2 to 49.0) and MathVision by 30.9% (from 23.6 to 30.9). .3. Case Study In this section, we present case studies (Table 11 to 12) to further compare the reasoning behaviors of LLaVA-Critic-R1+ and the base model. 23 LLaVA-Critic-R1 Table 9: Compare the evaluation results of LLaVA-Critic-R1+ obtained via critic RL training from two versions of MiMo-VL across 24 visual reasoning and understanding benchmarks. The variant trained from MiMo-VL-7B-RL-2508 achieves new 7B-level state-of-the-art performance on MMMU (71.9), MathVista (82.1), MathVerse (74.1), and ChartXiv Reasoning (62.5)."
        },
        {
            "title": "Benchmark",
            "content": "LLaVA-Critic-R"
        },
        {
            "title": "Base",
            "content": "LLaVA-Critic-R1+ Base MiMo-VL-SFT-7B MiMo-VL-RL-7B Perception & General VQA"
        },
        {
            "title": "Blink\nHallusionBenchImage\nMMStar\nMMBench\nMMVP\nMMHal\nRealWorldQA",
            "content": "Avg. MathVistatestmini MathVisiontestmini MathVersetestmini MMMUval EMMAmini Blind VisuLogic ZeroBenchsub Avg. ChartQA OCRBench AI2D Charxivreasoning Avg."
        },
        {
            "title": "MMVUval\nVideoMMMU",
            "content": "Avg."
        },
        {
            "title": "Visual\nRewardBench",
            "content": "VLRewardBench MM-RLHF Avg. Overall Avg. 62.41.9 76.31.3 71.90.4 84.70.2 79.00.4 4.410.18 69.80.6 64.10.5 81.90.8 55.21.6 70.52.4 69.71.3 36.82.0 74.60.1 83.21.1 25.60.4 21.02.2 57.60.9 88.50.6 85.03.6 85.40.3 57.62.8 79.11. 59.80.6 55.10.9 57.50.8 67.45.6 77.67.1 72.56.3 60.5 75.0 71.5 84.9 78.6 4.23 70.4 63.6 81.1 53.6 68.1 68.4 38.8 74.5 82.1 25.2 18.8 56. 87.9 81.4 85.7 54.8 77.5 59.2 54.2 56.7 61.8 70.5 66. 63.50.6 76.90.7 74.20.9 85.50.6 81.10.5 4.420.07 70.90.4 65.20.1 82.10.5 63.54.0 74.12.9 71.91.6 39.80.5 76.90.1 85.31.6 26.72.4 21.30.4 60.21.5 90.11.6 86.63.8 85.90.1 62.53.2 81.32.1 61.20.6 53.51.4 57.41.0 66.45.1 78.810.6 72.67.8 62.9 77.6 73.3 84.9 81.6 4.35 71. 65.1 81.6 59.5 71.2 70.3 40.3 76.8 83.7 24.3 20.9 58.7 88.5 82.8 85.8 59.3 79.1 60.6 52. 56.4 61.3 68.2 64.8 64.311.34 62.97 65.961. 64.30 24 LLaVA-Critic-R1 Table 10: Compare the evaluation results of LLaVA-Critic-R1 obtained via critic RL training from Llama-3.2-11B-VisionInstruct across 20 visual reasoning and understanding benchmarks."
        },
        {
            "title": "Benchmark",
            "content": "LLaVA-Critic-R"
        },
        {
            "title": "Base",
            "content": "Llama-3.2-11B-Vision-Instruct Perception & General VQA"
        },
        {
            "title": "Blink\nHallusionBenchImage\nMMStar\nMMBench\nMMVP\nMMHal\nRealWorldQA",
            "content": "Avg. MathVistatestmini MathVisiontestmini MathVersetestmini MMMUval EMMAmini Blind VisuLogic ZeroBenchsub Avg. ChartQA OCRBench AI2D Charxivreasoning Avg. 44.91.6 63.10.3 54.91.3 77.60.3 73.01.2 3.120.02 62.11.6 54.10.5 48.90.6 30.97.3 30.81.3 49.05.8 24.82.0 32.00.7 61.80.5 26.70.5 16.50.4 35.72.0 76.90.3 77.00.5 78.50.2 35.32.8 66.90. 43.3 62.8 56.2 77.3 71.9 3.10 60.5 53.6 48.3 23.6 29.5 43.2 22.8 31.3 62.3 26.2 16.1 33.7 76.6 77.5 78.3 32.5 66. Overall Avg. 48.391.22 47.17 25 LLaVA-Critic-R1 Table 11: An example where Qwen-2.5-VL fails in reasoning despite following the thinking template, while LLaVACritic-R1+ succeeds in reasoning correctly. LLaVA-Critic-R1+ accurately counts the number of figurines in the image and ultimately arrived at the correct answer Example 2 Question: Answer: Qwen-2.5-VL-7B-Thinking <think> How many figures are there? (Hint: include the figurine in the background) 10 LLaVA-Critic-R1+ To determine the number of figures in the image, Ill count each distinct figure, including the one in the background. 1. There is large figurine in the background. 2. There are 8 smaller figurines in the foreground. Adding these together: 1 (background) + 8 (foreground) = 9. </think> Final answer: 9 . <think> To determine the number of figures, Ill count each distinct figurine in the image, including the one in the background. 1. There is large figurine in the background. 2. There are 9 smaller figurines in the foreground. Adding these together: 1 (background) + 9 (foreground) = 10. So, the total number of figures is 10. </think> 10 LLaVA-Critic-R1 Table 12: An example where Qwen-2.5-VL fails in grounding for GUI agent tasks on OSWorld benchmark. The Resulting State images show the interface after executing the proposed action. Our model follows one of the possible valid trajectories to successfully click the Format button, whereas Qwen-2.5-VL clicks on an incorrect location. Example 3 Task: Possible Trajectory: Click Format Hover Align Text Click Centered Qwen-2.5-VL-7B Help me center align the heading in LibreOffice. {\"action\":\"left_click\",\"coordinate\":[539,177]} Resulting State: LLaVA-Critic-R1+ <think> To center align the heading, need to locate the paragraph formatting options in the toolbar. This is typically found in section of the toolbar that includes various alignment options. Since the Format menu is likely the target, Ill click on the Format menu, expecting to find options to center align. </think> {\"action\":\"left_click\",\"coordinate\":[515,160]} Resulting State:"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "The Ohio State University",
        "University of Maryland, College Park"
    ]
}