{
    "paper_title": "InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions",
    "authors": [
        "Pan Zhang",
        "Xiaoyi Dong",
        "Yuhang Cao",
        "Yuhang Zang",
        "Rui Qian",
        "Xilin Wei",
        "Lin Chen",
        "Yifei Li",
        "Junbo Niu",
        "Shuangrui Ding",
        "Qipeng Guo",
        "Haodong Duan",
        "Xin Chen",
        "Han Lv",
        "Zheng Nie",
        "Min Zhang",
        "Bin Wang",
        "Wenwei Zhang",
        "Xinyue Zhang",
        "Jiaye Ge",
        "Wei Li",
        "Jingwen Li",
        "Zhongying Tu",
        "Conghui He",
        "Xingcheng Zhang",
        "Kai Chen",
        "Yu Qiao",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Creating AI systems that can interact with environments over long periods, similar to human cognition, has been a longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuous and simultaneous streaming perception, memory, and reasoning remains largely unexplored. Current MLLMs are constrained by their sequence-to-sequence architecture, which limits their ability to process inputs and generate responses simultaneously, akin to being unable to think while perceiving. Furthermore, relying on long contexts to store historical data is impractical for long-term interactions, as retaining all information becomes costly and inefficient. Therefore, rather than relying on a single foundation model to perform all functions, this project draws inspiration from the concept of the Specialized Generalist AI and introduces disentangled streaming perception, reasoning, and memory mechanisms, enabling real-time interaction with streaming video and audio input. The proposed framework InternLM-XComposer2.5-OmniLive (IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module: Processes multimodal information in real-time, storing key details in memory and triggering reasoning in response to user queries. (2) Multi-modal Long Memory Module: Integrates short-term and long-term memory, compressing short-term memories into long-term ones for efficient retrieval and improved accuracy. (3) Reasoning Module: Responds to queries and executes reasoning tasks, coordinating with the perception and memory modules. This project simulates human-like cognition, enabling multimodal large language models to provide continuous and adaptive service over time."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 6 9 5 9 0 . 2 1 4 2 : r InternLM-XComposer2.5-OmniLive: Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions Pan Zhang1, Xiaoyi Dong1,2, Yuhang Cao1, Yuhang Zang1, Rui Qian1,2, Xilin Wei1,3, Lin Chen1,4, Yifei Li1,5, Junbo Niu1,6, Shuangrui Ding1,2, Qipeng Guo1, Haodong Duan1, Xin Chen1, Han Lv1, Zheng Nie1, Min Zhang1, Bin Wang1, Wenwei Zhang1, Xinyue Zhang1, Jiaye Ge1, Wei Li1, Jingwen Li1, Zhongying Tu1, Conghui He7, Xingcheng Zhang7, Kai Chen1, Yu Qiao1, Dahua Lin1,2, Jiaqi Wang1,(cid:66) 1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong, 3 Fudan University, 4 University of Science and Technology of China, 5 Tsinghua University, 6 Beihang University, 7 SenseTime Group internlm@pjlab.org.cn"
        },
        {
            "title": "Abstract",
            "content": "Creating AI systems that can interact with environments over long periods, similar to human cognition, has been longstanding research goal. Recent advancements in multimodal large language models (MLLMs) have made significant strides in open-world understanding. However, the challenge of continuous and simultaneous streaming perception, memory, and reasoning remains largely unexplored. Current MLLMs are constrained by their sequenceto-sequence architecture, which limits their ability to process inputs and generate responses simultaneously, akin to being unable to think while perceiving. Furthermore, relying on long contexts to store historical data is impractical for long-term interactions, as retaining all information becomes costly and inefficient. Therefore, rather than relying on single foundation model to perform all functions, this project draws inspiration from the concept of the Specialized Generalist AI and introduces disentangled streaming perception, reasoning, and memory mechanisms, enabling real-time interaction with streaming video and audio input. The proposed framework InternLM-XComposer2.5OmniLive (IXC2.5-OL) consists of three key modules: (1) Streaming Perception Module: Processes multimodal information in real-time, storing key details in memory and triggering reasoning in response to user queries. (2) Multimodal Long Memory Module: Integrates short-term and long-term memory, compressing short-term memories into long-term ones for efficient retrieval and improved accuracy. (3) Reasoning Module: Responds to queries and executes reasoning tasks, coordinating with the perception and memory modules. This project simulates human-like cogni- * indicates equal contribution. indicates interns at IXCLab, Shanghai AI Laboratory Figure 1. Inspired by human-like cognition and Specialized Generalist AI, we introduce InternLM-XComposer2.5-OmniLive (IXC2.5-OL), system that facilitates real-time interaction with: (1) streaming perception module supports streaming video and audio inputs; (2) multi-modal long memory module that compresses short-term memory into long-term memory; and (3) reasoning module that answers queries based on retrieved memories. tion, enabling multimodal large language models to provide continuous and adaptive service over time. All code and models of InternLM-XComposer2.5-OmniLive (IXC2.5OL) are publicly available at https://github.com/ InternLM/InternLMXComposer/tree/main/ InternLM-XComposer-2.5-OmniLive. 1. Introduction The goal of developing AI systems [55] that can understand and interact with environments over long periods, akin to human cognition, has been central focus of research for 1 decades. The rise of large-scale data corpora [54, 69, 95, 112] and multimodal large language models [83, 84, 107] has driven significant advances in free-form multimodal question answering. Recent developments, such as MiniOmni [123], VideoLLM-Online [12], and VITA [38], have made notable strides toward enabling more natural and immersive online interactions. However, challenges persist in creating systems capable of continuous interaction due to the intrinsic limitations of single decoder-only large language model architecture. Existing architectures [12, 38, 123, 149] encounter significant limitations in real-time and long-term streaming perception, reasoning, and memory. The sequence-tosequence decoder-only architecture used in current MLLMs forces switch between perception (e.g., seeing and hearing) and thinking, limiting the simultaneous processing of inputs and outputs. Additionally, existing works [33, 118, 145] rely on the integration of multimodal memories within context windows. The reliance on long contexts to store historical information proves impractical for long-term use, especially in scenarios requiring continuous AI assistance. Multimodal data, like video streams, can quickly accumulate millions of tokens within few hours, making it impractical to maintain context over multiple days of service. The cost and inefficiency of storing all historical clues within the context further limit the systems capacity to provide continuous and long-term service. In contrast, the human brain can effortlessly integrate perception and cognition, preserving long-term multimodal memories. This is believed to be closely related to the functional partitioning design of the human brain cortex, where different areas of the cortex are responsible for distinct tasks, such as perception, memory, and cognition. Inspired by the paradigm of Specialized Generalist AI [146], we propose system InternLM-XComposer2.5OmniLive (IXC2.5-OL) composed of fused specialized generalist models for streaming perception, reasoning, and memory, respectively. The system is designed to enable AI models to engage continuously with environments while retaining observations over time. By integrating short-term and long-term multimodal memory, our approach attempts to emulate human-like cognition, enabling more dynamic and sustained interactions. As shown in Figure 1, the IXC2.5-OL system consists (1) Streaming Perception Modof three key modules: ule: This module processes the multimodal information stream on-the-fly. To ensure perception accuracy and efficiency, the video and audio streams are handled separately. live video perception model processes the video stream, encoding the information and storing key details in memory. Meanwhile, an audio model recognizes the contents of human speech and other sounds, e.g., barking, knocking, or whistling. It triggers the reasoning process when human queries occur. (2) Multi-modal Long Memory Module: This component integrates both long-term and short-term memory, enabling the retrieval of detailed short-term information as well as long-term historical cues. It continuously compresses short-term memories into more informationrich long-term memories to enhance retrieval efficiency and accuracy. (3) Reasoning Module: The reasoning module, activated by the perception module, handles queries and performs reasoning tasks. As the component with the most model parameters, it serves as the core of the systems deep cognitive processes. The proposed system empowers AI with the ability to perceive, think, and memorize simultaneously. By overcoming the limitations of alternating perception and reasoning, IXC2.5-OL seeks to provide continuous, adaptive service, and long-term AI service. The proposed system will not only enhance the performance of AI assistants but will also contribute to the broader AI applications capable of continuously interacting and adapting to dynamic environments. The IXC2.5-OL demonstrates strong performance across both audio and video benchmarks. Among the opensource models, IXC2.5-OL achieves competitive results on audio recognition (ASR) benchmarks such as Wenetspeech [140] for Chinese and LibriSpeech [87] for English. For video understanding benchmarks, IXC2.5-OL achieves state-of-the-art results among models with less than 10B parameters, obtaining an M-Avg of 66.2% on MLVU [155] and an overall accuracy of 68.7% on MVBench [62]. Additionally, it demonstrates competitive performance on VideoMME [37] (60.6%) and MMBench-Video [34] (1.42). On recent streaming video bench StreamingBench [67], IXC2.5-OL achieves new SOTA results on open-source models (73.79%), highlighting its exceptional capabilities for real-time video interactions. To foster the development of the multimodal streaming interaction community, alongside the model parameters, the inference and deployment source code, encompassing both the web frontend and backend code, has also been released. All code and models of IXC2.5-OL are publicly available at https://github.com/InternLM/InternLMXComposer/tree/main/InternLM-XComposer2.5-OmniLive. 2. Related Works MLLMs for Text-Image Conversation. Large Language Models (LLMs) [5, 7, 9, 24, 46, 51, 81, 86, 90, 108 110, 136] have garnered significant attention for their remarkable capabilities in language comprehension and generation. Building on this success, Large Vision-Language Models (LVLMs) [3, 6, 1719, 28, 30, 31, 36, 56, 68, 82, 88, 132, 147, 147, 156] have been developed by integrating LLMs with vision encoders [4, 10, 14, 21, 22, 29, 70, 74, 2 75, 85, 91, 104, 115, 135, 138, 141, 150], extending their ability to comprehend visual content and enabling applications like text-image conversations. Earlier LVLMs were primarily designed for single-image, multi-round conversations, whereas recent advancements [1, 4, 30, 48, 58, 68, 103, 148, 153] have expanded their capabilities to process and understand multi-image inputs. Table 1. Overview of datasets used in pretraining and supervised fine-tuning (SFT) for the Audio Translation Module. The pretraining stage focuses solely on the automatic speech recognition (ASR) task, utilizing the GigaSpeech and WenetSpeech datasets. The SFT stage includes both ASR and audio classification (CLS) tasks, leveraging diverse datasets. For CommonVoice, we only use the English and Chinese splits. Additionally, 475 self-constructed Silence samples are used for CLS tasks. MLLMs for Video Understanding. In addition to advancements in image understanding, the field of MLLMs has seen growing efforts in video analysis [32, 34, 61, 73, 80, 98, 100, 113, 127]. To address the complexity of video inputs, existing approaches leverage techniques such as sparse sampling or temporal pooling [44, 66, 77, 79, 133], compressed video tokens [16, 49, 60, 63, 94, 119, 144], and memory banks [33, 43, 89, 98, 100, 118, 145]. Additionally, some methods utilize language as bridge for video understanding [45, 50, 142]. Beyond these videospecific strategies, video analysis can also be framed as interpreting high-resolution composite image generated from sampled video frames [52, 126, 149]. Recent advancements [12, 117, 120, 145] have increasingly focused on online video understanding, aiming to simulate real-world scenarios where AI processes video streams in real-time to comprehend the environment on-the-fly. However, existing solutions still lack the capability to simultaneously perform perception, memory, and reasoning, limiting their applicability for consistent and long-term human-AI interactions. MLLMs for Audio Understanding. Audio understanding can be effectively modeled as sequence-to-sequence (Seq2Seq) task [93], which enables powerful integration with large language models by incorporating audio tokenizers and encoders [25, 105, 137, 143]. In addition to receiving the audio input, recent research investigates streaming duplex speech models [78, 114, 116, 134] that allow speakers to interrupt freely. Beyond audio-text models, emerging research delves into audio-visual models [59, 96] and unified architectures that process audio, visual, and text modalities [38, 64, 139]. MLLMs for Omni-Modal Understanding. Integrating multiple modalities into single omni-modal foundation model represents promising research direction. Existing works [13, 38, 42, 64, 102, 121, 124, 139] explore models capable of processing omni-modal inputs, typically combining video and audio, to produce outputs in various formats. These outputs include text [38, 42, 64], audio [13, 102, 124], and omni-modal contents [121, 139]. In the current design of IXC2.5-OL, we handle the audio and video modalities separately to mitigate potential influence In future versions, our model will during joint training. incorporate joint training across all modalities, enabling seamless omni-modality integration."
        },
        {
            "title": "Pretrain ASR",
            "content": "GigaSpeech [11] WenetSpeech [140] 8,282,987 17,821,"
        },
        {
            "title": "SFT",
            "content": "LibriSpeech [87] VCTK [111] AISHELL-1 [8] AISHELL-4 [39] MD-RAMC [129] ASCEND [76] KeSpeech [106] DASR [27] CommonVoice [2] FSD50K [35] AudioSet [53] Silence 281,241 44,070 120,098 102,254 219,325 12,314 888,428 190,732 2,813,852 40,966 18,"
        },
        {
            "title": "CLS",
            "content": "3. Method As we briefly introduced in Sec.1, the IXC2.5-OL has three disentangled modules: 1) the Streaming Perception Module for on-the-fly visual and audio information processing, 2) the Multi-modal Long Memory Module for memory integration and retrieval, and 3) the Reasoning Module collect information from the perception and memory module, and handles queries and performs reasoning tasks. All the modules work simultaneously and interact asynchronously. 3.1. Streaming Perception Module the IXC2.5-OL could handle Besides nature language, video and audio natively. To realize this, the Streaming Perception Module contains an Audio Translation Module and Video Perception Module. Audio Translation Module contains an audio encoder, an audio projector, and Small Language Model (SLM). The audio encoder encodes the input audio sample into highdimension features, and the audio projector further maps the feature to the input space of the SLM. The SLM outputs both the class (e.g. laughing, clapping, or raining) of the audio and the natural language within the audio (i.e. the automatic speech recognition). In practice, we use the Whisper [92] model as the audio encoder and Qwen21.8B [128] as the SLM. The training contains two stages and we list the training data in Table 1. Video Perception Module provides coarse-grained visual 3 Figure 2. Pipeline of the InternLM-XComposer2.5-OmniLive. (IXC2.5-OL). The IXC2.5-OL is real-time interacting system that is constructed by three simultaneous modules: 1) the Streaming Perception Module, 2) the Multi-modal Long Memory Module, and 3) the Reasoning Module. information to the Multi-modal Long Memory Module. It processes the real-time video input stream and encodes each frame into semantic features. For efficiency, we use the OpenAI CLIP-L/14 [91] In practice. 3.2. Multi-modal Long Memory Module The Multi-modal Long Memory Module is the core design to handle extremely long video input and helps the Reasoning Module to get rid of millions of tokens from its context window. It shares similar idea from the VideoStreaming [89] that encodes video clips into short-term memories and integrates them into long-term memory. With the given questions, it retrieved the most related video clips for the Reasoning Module. Formally, the Multi-modal Long Memory Module is trained with three tasks: Video Clip Compression. With features of kth video clip extracted from the Perception Module Fk RT C, we initialize its short-term memory Hk RT by the spatial down-sampling and its global memory ˆHk R1C. We realize the compression by the auto-regressive and feature aggregation nature of LLMs: Model Dataset Hk, ˆHk = Compressor([Fk Hk ˆHk]). Memory Integration. Short-term memory represents the detailed information of each short video clip while the model still lacks macro view of the video. To this end, with the short-term and global memory of list of video clips, we integrate them into long-term memory by the Compressor in the following format: H1, H2, ..., Hk = Compressor([H1 H2... Hk ˆH1 ˆH2... ˆHk]). the = [ H1, H2, ..., Hk] RkC represents the video in high-compressed way and we denote it as the long-term memory. Video Clip Retrieval. When users raise questions, the Multi-modal Long Memory Module retrieves the questionrelated video clips and provides both the video clips and their short-term memory to the Reasoning Module. In practice, we first encode the question to the feature space of the memory. We concatenate the long-term memory with the tokenized question as the Compressor input, and we view the last token of the output features as the memoryspace-aligned question feature. Then we calculate the similarity between the question feature and each videos global memory, and select the most related clips for the Reasoning Module. Implementation Detail. We use Qwen2-1.8B [128] as the LLMs and construct several kinds of training data for the three aforementioned tasks. As shown in Table. 2, we train the Video Clip Compression task with short video captioning data from multiple sources, using the same prefix captioning task designed in VideoStreaming [89]. For the Memory Integration task and Video Clip Retrieval task, besides the off-the-shelf video grounding data, we also construct data for two unique tasks: Semantics Implicit Question and Reference Implicit Question. The Semantics Implicit Question means the question does not point to some object directly, but mentions the usage or meaning of the object, and the model should find out the object by understanding the implicit question. For example, when the user asks How about the weather today?, the model should find out some weather-related object in the past video stream, such as an umbrella, sun-glass, or something. Another example could be Im hungry, where can heat my sandwiches?, the model should find the microwave oven it has seen before. The Reference Implicit Question means the question uses pronouns rather than nouns. For example, What is this means the models should retrieve the current frames, although it does not mention any exact objects. Memory Module IXC2.5 ShareGPT4Video [15], Ego4D[41] ActivityNet [32] Semantics Implicit QA Reference Implicit QA ShareGPT4Video [15], ActivityNet [32] FunQA [122], TrafficQA [125] VideoChat2-IT[61], LLaVA-Video [152] Table 2. Video Datasets used in IXC2.5-OL. Both kinds of implicit questions are commonly used in real-world communication while current models failed to handle them, so we construct corresponding training data to empower the model with these capabilities. 3.3. Reasoning Module The Reasoning Module is initialized by an improved version of InternLM-XComposer2.5 (IXC2.5 in the following for simplified statement) and we add memory projector to align the memory feature with IXC-2.5. For given questions and both visual and memory information provided by the Memory Module, we formulate the input as: Question: < Que >, Here is the question related video clip < Img >; Here is the question related memory < Mem > In real-world usage, there exists some noisy input that should not be answered (e.g., the user says enn... or ok...), the model should keep salient and wait for the next question. To realize this, we add an additional Instruction Prediction process for each question to decide it should be answered or not. 3.4. System Pipeline As illustrated in Figure 3, the system comprises the Frontend, SRS Server, and Backend Server. Frontend. The frontend application, developed with JavaScript, enables the camera and microphone to capture video and audio stream inputs, which are then pushed to the SRS server. Concurrently, it establishes WebSocket connection with the backend to listen for audio outputs and interrupt signals. When audio data is received, the frontend plays it. Upon receiving an interrupt signal, the frontend suspends the audio playback and discards the pending audio. SRS Server. SRS (Simple Realtime Server) is straightforward and efficient real-time video server, adept at supporting multitude of real-time streaming protocols such as RTMP, WebRTC, HLS, HTTP-FLV, SRT, and others. It is renowned for its ability to reliably receive and deliver audio and video streams. 5 Figure 3. System pipeline of the IXC2.5-OL. The system comprises the Frontend, SRS Server, and Backend Server. The Frontend is utilized for capturing video and audio streams and for playing audio from the Backend Server. The SRS Server is employed for managing live streams. The Backend Server is responsible for reading audio and video, extracting memory, and answering questions. The green boxes in the figure represent thread or process. Backend Server. After establishing WebSocket connection with the frontend, the backend will pull streaming from the SRS Server and initiate separate threads to read audio and video. The audio reading thread will segment the audio stream into 4096-bit chunks and enqueue them into the Audio Queue. The Voice Activity Detection (VAD) [40] thread continuously reads data from Audio Queue and detects the start and end of voice activity. Upon detecting the start of voice activity, the backend sends an interrupt signal to the frontend to pause the currently playing audio, and at the same time, dispatches backup signal to the video process, directing it to save the current memory state. When detecting the end of voice activity, the entire voice segment will be enqueued into ASR Todo Queue. The ASR thread continuously reads audio segments from ASR Todo Queue, performs background noise classification and voice recognition on them, and then enqueues the results into LLM Todo Queue for use by the LLM. The video reading thread reads video frames at rate of 1 frame per second and enqueues them into Frame Queue. The compressor process reads video frames from the queue, recognizes them, extracts relevant memory, and stores it. Upon receiving backup signal from the VAD thread, the compressor process will save the current memory state for later retrieval. The LLM process reads text from the LLM Todo Queue and determines whether it is an instruction that requires response from the model. For texts identified as instructions, the compressor process will use the current instruction and the backed-up memory to perform memory grounding, in order to retrieve memories related to the instruction. The LLM process will then generate response based on the retrieved memories and the instruction, and enqueue the resulting output into TTS Todo Queue. An additional TTS thread (e.g., F5-TTS [20], MeloTTS [154]) will convert the text from the TTS Todo Queue into audio and send it to the frontend. 4. Experiments In this section, we validate the benchmark performance of our InternLM-XComposer2.5-OmniLive (IXC2.5-OL), including both audio and video benchmarks. 4.1. Audio Benchmarks We evaluate our audio models on two prominent automatic speech recognition (ASR) benchmarks: Wenetspeech [140] for Chinese (CN) and LibriSpeech [87] for English (EN). WenetSpeech includes two test sets: Test Net, which represents high-quality and relatively clean Chinese speech, and Test Meeting, which captures more challenging conversational scenarios. LibriSpeech consists of four splits: Dev clean and Test clean, which contain clean, high-quality English speech, and Dev other and Test other, which include noisier, more complex utterances. As shown in Table 3, our IXC2.5-OL demonstrates supe6 Table 3. Evaluation results on ASR tasks: CN refers to Chinese speech, while ENG refers to English speech. The performance is measured using WER (Word Error Rate). Method LLM Wenetspeech (CN) Librispeech (ENG) Test Net Test Meeting Dev clean Dev other Test clean Test other Qwen2-Audio [26] Mini-Omni [123] VITA [38] IXC2.5-OL Qwen2-7B [128] Qwen2-0.5B [128] Mixtral-8x7B [47] Qwen2-1.5B [128] 7.8 - 12.2 9.0 8.4 - 16.5 9.2 1.3 4.5 7.6 2.5 3.4 9.7 16.6 5.7 1.6 4.6 8.1 2. 3.6 9.2 18.4 5.8 Table 4. Evaluation results on MLVU benchmark. IXC2.5-OL has demonstrated excellent performance, surpassing both open-source models and closed-source APIs, achieving SOTA at the 7B model scale. Method Params Topic Rea. Anomaly Recog. Needle QA Ego Rea. Plot QA Action Or. Action Co. M-Avg Closed-source APIs. Claude-3-Opus Qwen-VL-Max GPT-4 Turbo GPT-4o Open-source models. MovieChat [99] LLaMA-VID [65] LLaVA-1.6 [71] ShareGPT4Video [15] VideoLlaMA2 [23] LongVA [149] IXC2.5 [148] InternVL2 [22] LLaVA-OneVision [57] Video-XL [97] IXC2.5-OL - - - - 7B 7B 7B 7B 7B 7B 7B 8B 7B 7B 7B 67.2 67.4 79.5 87.4 29.5 50.8 60.6 75.8 74.6 83.3 - - - - 84.1 43.5 63.5 68.0 74.5 25.0 34.5 41.0 51.5 64.5 58.5 - - - - 68.5 21.6 40.3 45.9 64.8 24.2 30.1 43.1 47.6 49.9 69.3 - - - - 76.6 40.2 40.9 47.4 57.1 24.7 32.7 38.4 43.2 43.8 50.0 - - - - 60.8 47.8 43.3 60.6 65.1 25.8 32.5 41.0 48.4 45.1 67.2 - - - - 75.1 18.2 25.0 26.5 56.7 28.6 23.9 25.5 34.0 34.0 38.6 - - - - 57.1 16.7 14.8 16.1 46.3 22.8 27.8 25.7 23.3 27.4 27.2 - - - - 41.3 36.5 42.2 49.2 64.6 25.8 33.2 39.3 46.4 48.5 56.3 58.8 64.0 64.7 64. 66.2 Evaluation results on Video-MME benchmark. Table 5. IXC2.5-OL demonstrates performance close to that of the opensource SOTA. Word Error Rates (WER) across both CN and EN benchmarks with merely lightweight 1.5B LLM. Method Params Short Medium Long Overall 4.2. Video Benchmarks Closed-source APIs. GPT-4V Claude 3.5 Sonnet GPT-4o mini GPT-4o Gemini 1.5 Pro Open-source models. ShareGPT4Video [15] VideoLlaMA2 [23] LongVA [149] Video-XL [97] VITA [38] IXC2.5 [148] InternVL2 [22] LLaVA-OneVision [57] mPLUG-Owl3 [131] MiniCPM-V 2.6 [130] IXC2.5-OL - - - - - 70.5 71.0 72.5 80.0 81.7 7B 7B 7B 7B 48.3 - 61.1 64.0 87B 65.9 7B 8B 7B 7B 8B 7B - - - 70.0 - 72.7 55.8 57.4 63.1 70.3 74.3 36.3 - 50.4 53.2 52.9 - - - 57.7 - 58.2 53.5 51.2 58.6 65.3 67.4 35.0 - 46.2 49.2 48.6 - - - 50.1 - 50.8 59.9 60.0 64.8 71.9 75.0 39.9 47.9 52.6 55.5 55.8 55.8 56.3 58.2 59.3 60.9 60.6 rior performance compared to recent streaming audio LLMs such as VITA and Mini-Omni, particularly achieving lower In Tables 4, 5, 7 and 8, we compare IXC2.5-OL with both closed-source APIs and open-source models on conventional video understanding benchmarks, including MLVU [155], Video-MME [37], MMBench-Video [34] and MVBench [62]. Furthermore, we also assess the performance of different models on the recently proposed StreamingBench [67], which is designed to better evaluate performance for real-time video interactions. The results of this comparison are presented in Table 6. For the video benchmarks, the base model utilizes 64 sampled frames for each video during evaluation. MLVU MLVU is comprehensive benchmark designed for evaluating Multimodal Large Language Models in Long Video Understanding tasks. The videos range from 3 minutes to 2 hours and include nine distinct evaluation tasks. Here, we evaluate seven multi-choice tasks, including Topic Reasoning, Anomaly Recognition, Needle QA, Ego Reasoning, Plot QA, Action Order, and Action Count. The deTable 6. Evaluation results on StreamingBench for Real-Time Visual Understanding. Metrics include Object Perception (OP), Causal Reasoning (CR), Clips Summarization (CS), Attribute Perception (ATP), Event Understanding (EU), Text-Rich Understanding (TR), Prospective Reasoning (PR), Spatial Understanding (SU), Action Perception (ACP), and Counting (CT). IXC2.5-OL excels among all open-source models, and falling just short of the closed-source API, Gemini 1.5 Pro."
        },
        {
            "title": "Human",
            "content": "Closed-source APIs. Claude 3.5 Sonnet GPT-4o Gemini 1.5 Pro Open-source models. VideoLLM-online [12] VideoLLaMA2 [23] VILA-1.5 [68] LongVA [149] InternVL2 [22] Kangaroo [72] MiniCPM-V 2.6 [130] Qwen2-VL [113] LLaVA-OneVision [57] IXC2.5-OL"
        },
        {
            "title": "Params",
            "content": "Real-Time Visual Understanding OP CR CS"
        },
        {
            "title": "ATP",
            "content": "EU TR PR SU"
        },
        {
            "title": "CT Overall",
            "content": "89.47 92.00 93.60 91.47 95.65 92.52 88.00 88.75 89.74 91.30 91.46 80.49 77.34 82.02 81.73 72.33 75.39 61.11 61.79 69.32 43.09 77.11 80.47 83.91 76.47 70.19 83.80 66.67 62.19 69.12 49.22 79.02 80.47 83.54 79.67 80.00 84.74 77.78 64.23 71.95 48.70 72.44 73.28 75.69 39.07 40.06 34.49 31.05 45.96 32.40 31.48 34.16 42.49 27.89 55.86 55.47 57.41 58.17 52.80 43.61 39.21 42.68 45.61 35.23 53.68 49.22 70.98 56.86 53.42 53.89 54.63 48.78 50.14 17.62 70.03 63.28 61.20 70.92 62.73 59.50 61.11 53.66 54.67 34.72 68.12 60.94 69.40 77.12 67.70 62.93 59.26 53.25 54.96 56.48 71.12 84.38 70.66 73.20 67.08 61.68 56.48 55.69 62.04 38.86 71.93 71.09 77.92 75.82 64.60 65.73 70.37 56.10 62.32 53.37 75.20 82.81 73.19 77.45 68.32 71.03 72.22 61.19 69.04 46.11 80.38 74.22 76.03 80.72 72.67 71.65 67.59 65.45 65.72 45.08 35.99 49.52 52.32 59.96 63.72 64.60 67.44 69.04 71. 82.83 73.77 78.66 82.95 72.50 76.01 61.11 60.67 71.59 58.85 73.79 - - - - 8B 7B 8B 7B 8B 7B 8B 7B 7B 7B Table 7. Evaluation results on MMBench-Video. Tasks include Coarse Perception (CP), Single-Instance Finegrained Perception (FP-S), Cross-Instance Finegrained Perception (FP-C), Hallucination (HL), Logic Reasoning (LR), Attribute Reasoning (AR), Relation Reasoning (RR), Commonsense Reasoning (CSR), and Temporal Reasoning (TP). Method Params Perception Reasoning Overall CP FP-S FP-C HL Mean LR AR RR CSR TP Mean Closed-source APIs. Claude 3.5 Sonnet Gemini 1.0 Pro Gemini 1.5 Pro GPT-4V GPT-4o Open-source models. MovieLLM [101] LLaVA-OneVision [57] PLLaVA [126] ShareGPT4Video [15] VideoStreaming [89] LLaVA-NeXT-Video [151] VILA1.5 [68] InternVL2 [22] Qwen2-VL [113] IXC2.5-OL - - - - - 7B 72B 7B 7B 7B 7B 13B 8B 7B 7B 1.57 1.61 1.99 1.83 2.23 0. 1.22 1.08 1.20 1.38 1.35 1. 1.41 1.63 1.39 1.56 2.04 1. 2.24 0.82 1.07 1.06 1.05 1. 1.15 1.45 1.37 1.51 1.07 1. 1.70 1.40 2.01 0.70 0.90 0. 1.00 0.8 0.97 1.26 1.15 1. 1.40 0.65 1.90 1.76 1.90 0. 0.21 0.52 0.32 0.32 0.58 0. 0.19 0.55 1.38 1.50 1.98 1. 2.19 0.81 1.03 1.02 1.04 1. 1.14 1.39 1.30 1.46 1.13 1. 1.98 1.45 2.11 0.52 0.76 0. 0.89 0.77 0.64 0.80 0.90 1. 1.70 1.57 2.02 1.91 2.12 1. 0.96 1.25 1.06 1.27 1.38 1. 1.34 1.56 1.48 1.55 1.92 1. 2.17 1.22 0.55 1.17 1.19 1. 1.30 1.30 1.38 1.49 1.54 1. 1.78 1.83 1.94 0.54 0.81 0. 1.01 1.01 1.27 1.40 1.14 1. 1.04 1.33 1.63 1.53 1.97 1. 0.48 1.01 0.99 1.10 1.03 1. 1.00 1.21 1.35 1.39 1.86 1. 2.08 0.97 0.70 1.03 1.03 1. 1.13 1.28 1.16 1.35 1.38 1. 1.94 1.68 2.15 0.87 0.94 1. 1.05 1.12 1.14 1.36 1.26 1. 1.53 1.61 1.20 0.15 1.49 0.93 1. 1.57 1.30 1.08 1.25 1.42 Table 8. Evaluatation results on MVBench. Tasks include Action Sequence (AS), Action Prediction (AP), Action Antonym (AA), Finegrained Action (FA), Unexpected Action (UA), Object Existence (OE), Object Interaction (OI), Object Shuffle (OS), Moving Direction (MD), Action Localization (AL), Scene Transition (ST), Action Count (AC), Moving Count (MC), Moving Attribute (MA), State Change (SC), Fine-grained Pose (FP), Character Order (CO), Egocentric Navigation (EN), Episodic Reasoning (ER), and Counterfactual Inference (CI). Method Params AS AP AA FA UA OE OI OS MD AL ST AC MC MA SC FP CO EN ER CI Avg Closed-source APIs. GPT-4V GPT-4o Open-source models. VideoLLaMA [144] VideoChat [60] MiniCPM-V 2.6 [130] VideoChat2 [62] Qwen2-VL [113] PLLaVA [126] LLaVA-OneVision [57] InternVL2 [22] - - 55.5 63.5 72.0 46.5 73.5 18.5 59.0 29.5 12.0 40.5 83.5 39.0 12.0 22.5 45.0 47.5 52.0 31.0 59.0 11.0 43.5 61.5 56.5 72.0 54.0 82.0 62.5 66.5 44.0 36.5 33.5 93.0 54.5 33.5 54.5 53.5 74.5 71.5 32.5 71.0 42.5 57.5 27.5 25.5 51.0 29.0 39.0 48.0 40.5 38.0 22.5 22.5 43.0 34.0 22.5 32.5 45.5 32.5 40.0 30.0 21.0 37.0 34.1 7B 33.5 26.5 56.0 33.5 40.5 53.0 40.5 30.0 25.5 27.0 48.5 35.0 20.5 42.5 46.0 26.5 41.0 23.5 23.5 36.0 35.5 7B 38.0 43.0 63.0 35.5 67.5 55.5 46.0 35.5 25.5 33.0 77.5 48.0 37.0 54.0 42.5 40.0 31.0 38.0 43.0 40.5 44.7 7B 66.0 47.5 83.5 49.5 60.0 58.0 71.5 42.5 23.0 23.0 88.5 39.0 42.0 58.5 44.0 49.0 36.5 35.0 40.5 65.5 51.1 7B 51.0 58.0 77.5 47.0 64.0 63.0 65.5 40.0 25.5 35.5 77.0 43.5 47.0 62.0 42.0 61.5 49.5 41.5 47.5 41.5 52.0 7B 34B 65.0 53.0 83.5 45.0 77.5 70.0 64.5 38.5 37.5 49.0 89.5 41.5 43.5 70.0 53.0 52.5 65.0 39.5 60.5 58.0 57.8 72B 63.0 58.0 84.5 46.5 85.5 64.0 73.5 41.5 37.0 69.0 95.0 47.5 47.5 75.5 53.5 52.0 70.5 34.0 64.0 54.5 60.8 75.0 62.0 83.5 40.5 69.5 96.0 72.0 29.5 58.0 53.0 88.5 39.5 83.0 97.0 51.0 78.5 65.0 33.0 48.0 67.0 64.5 8B IXC2.5-OL 7B 84.5 81.0 75.0 46.0 81.0 92.0 79.5 36.5 83.0 47.0 90.0 60.5 75.0, 93.0 58.0 60.5 74.0 42.0 53.0 62.0 68.7 tailed comparisons are given in Table 4. The IXC2.5-OL exhibits state-of-the-art (SOTA) performance among closedsource APIs, and open-source models with parameters less than 10 billion, surpassing the previous SOTA by 1.3% for Video-XL, 1.6% for GPT-4o. quality in terms of accuracy, consistency, and alignment with human judgment. The results are presented in Table 7. IXC2.5-OL demonstrates state-of-the-art performance on perception tasks and comparable performance on overall evaluations. Video-MME Video-MME is high-quality video benchmark. The videos are collected from 6 primary visual domains with 30 subfields to ensure broad scenario generalizability, encompassing both short-, medium-, and long-term videos, ranging from 11 seconds to 1 hour. As demonstrated in Table 5, the IXC2.5-OL exhibits competitive performance on this benchmark, comparable to previous SOTA MiniCPM-V 2.6. MVBench MVBench is video benchmark that emphasizes temporal understanding. It encompasses 20 challenging video tasks that cannot be effectively addressed using single frame. As shown in Table 8, IXC2.5-OL, despite having smaller 7B parameter size, has outperformed both the GPT-4 series and the 72B open-source model LLaVAOneVision, demonstrating its strong capability in understanding video temporal dynamics. 5. Conclusion We have presented IXC2.5-OL, real-time streaming model that advances multi-modal text, audio, and visual capabilities with long-term memory. IXC2.5-OL empowers users to engage in dynamic and interactive experiences. Our models real-time processing enables fluid and responsive interactions, allowing users to engage with ever-changing environments of multimodal data seamlessly, providing more intuitive and efficient user experience. Our future work will focus on reducing system latency to provide seamless user experience. StreamingBench StreamingBench is streaming video benchmark designed for real-time video evaluation. It comprises 18 tasks, showcasing 900 videos and 4,500 humancurated QA pairs. In this context, we focus on assessing visual understanding in real-time. Table 6 illustrates the comparative analysis, demonstrating that IXC2.5-OL excels among all open-source models, achieving 2.67% improvement over the previous state-of-the-art model, LLaVAOneVision, and falling just short of the closed-source API, Gemini 1.5 Pro. This performance solidifies IXC2.5-OLs remarkable prowess in real-time video interaction. MMBench-Video MMBench-Video is free-form QA video benchmark consisting of 600 videos and 2000 QA pairs. The duration of each video varies from 30 seconds to 6 minutes. Given the open-ended nature of the answers, the benchmark utilizes GPT-4-based evaluation to enhance"
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35: 2371623736, 2022. 3 [2] Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. Common voice: massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019. 3 [3] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. OpenFlamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023. 2 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: frontier large vision-language model with versatile abilities. arXiv.org, 2023. 2, 3 [5] Baichuan. Baichuan 2: Open large-scale language models. arXiv.org, 2023. [6] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and Sagnak Tasırlar. Introducing our multimodal models, 2023. 2 [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33:1877 1901, 2020. 2 [8] Hui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. Aishell-1: An open-source mandarin speech corpus and speech recognition baseline. In 2017 20th conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA), pages 15. IEEE, 2017. 3 [9] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei arXiv preprint Chu, et al. arXiv:2403.17297, 2024. 2 Internlm2 technical report. [10] Yuhang Cao, Pan Zhang, Xiaoyi Dong, Dahua Lin, and Jiaqi Wang. DualFocus: Integrating macro and micro perarXiv spectives in multi-modal large language models. preprint arXiv:2402.14767, 2024. [11] Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al. Gigaspeech: An evolving, multidomain asr corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021. 3 [12] Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, and Mike Zheng Shou. Videollm-online: Online video large language model for streaming video, 2024. 2, 3, 8 [13] Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, et al. EMOVA: Empowering language models to see, hear and speak with vivid emotions. arXiv preprint arXiv:2409.18042, 2024. 3 [14] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. 2 [15] Lin Chen, Xilin Wei, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Bin Lin, Zhenyu Tang, et al. ShareGPT4Video: Improving video understanding and generation with better captions. arXiv preprint arXiv:2406.04325, 2024. 5, 7, 8 [16] Shimin Chen, Xiaohan Lan, Yitian Yuan, Zequn Jie, and Lin Ma. Timemarker: versatile video-llm for long and short video understanding with superior temporal localization ability, 2024. [17] Xi Chen, Josip Djolonga, Piotr Padlewski, et al. PaLI-X: On scaling up multilingual vision and language model. arXiv preprint arXiv:2305.18565, 2023. 2 [18] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, et al. Pali-3 vision language models: Smaller, faster, stronger, 2023. [19] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: jointly-scaled multilingual language-image model, 2023. 2 [20] Yushen Chen, Zhikang Niu, Ziyang Ma, Keqi Deng, Chunhui Wang, Jian Zhao, Kai Yu, and Xie Chen. F5-tts: fairytaler that fakes fluent and faithful speech with flow matching. arXiv preprint arXiv:2410.06885, 2024. 6 [21] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv preprint arXiv:2312.14238, 2023. 2 [22] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, 2024. 2, 7, 8, 9 [23] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 7, [24] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv.org, 2022. 2 [25] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. QwenAudio: Advancing universal audio understanding via uni10 fied large-scale audio-language models. arXiv:2311.07919, 2023. 3 arXiv preprint [26] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. [27] Samuele Cornell, Taejin Park, Steve Huang, Christoph Boeddeker, Xuankai Chang, Matthew Maciejewski, Matthew Wiesner, Paola Garcia, and Shinji Watanabe. The chime-8 dasr challenge for generalizable and array agnostic distant automatic speech recognition and diarization. arXiv preprint arXiv:2407.16447, 2024. 3 [28] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Instructblip: Towards generalFung, and Steven Hoi. purpose vision-language models with instruction tuning, 2023. 2 [29] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model. arXiv preprint arXiv:2401.16420, 2024. 2 [30] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen, Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer24khd: pioneering large vision-language model handling arXiv preprint resolutions from 336 pixels to 4k hd. arXiv:2404.06512, 2024. 2, 3 [31] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, et al. Palm-e: An In arXiv preprint embodied multimodal language model. arXiv:2303.03378, 2023. 2 [32] Bernard Ghanem Fabian Caba Heilbron, Victor Escorcia and Juan Carlos Niebles. ActivityNet: large-scale video benchmark for human activity understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2015. 3, [33] Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent: memory-augmented multimodal agent for video understanding, 2024. 2, 3 [34] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. MMBench-Video: long-form multi-shot benchmark for holistic video understanding. arXiv preprint arXiv:2406.14515, 2024. 2, 3, 7 [35] Fonseca, Favory, Pons, Font, and Serra. Fsd50k: an open dataset of human-labeled sound events, in arxiv. arXiv preprint arXiv:2010.00475, 2020. 3 [36] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang, Zhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang Shen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shaohui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hongsheng Li, and Xing Sun. challenger to gpt-4v? early explorations of gemini in visual expertise. arXiv preprint arXiv:2312.12436, 2023. 2 [37] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, [38] Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Shaoqi Dong, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, Ran He, Rongrong Ji, Yunsheng Wu, Caifeng Shan, and Xing Sun. Vita: Towards open-source interactive omni multimodal llm, 2024. 2, 3, 7 [39] Yihui Fu, Luyao Cheng, Shubo Lv, Yukai Jv, Yuxiang Kong, Zhuo Chen, Yanxin Hu, Lei Xie, Jian Wu, Hui Bu, et al. Aishell-4: An open source dataset for speech enhancement, separation, recognition and speaker diarization in conference scenario. arXiv preprint arXiv:2104.03603, 2021. 3 [40] Zhifu Gao, Zerui Li, Jiaming Wang, Haoneng Luo, Xian Shi, Mengzhe Chen, Yabin Li, Lingyun Zuo, Zhihao Du, Zhangyu Xiao, and Shiliang Zhang. Funasr: fundamental end-to-end speech recognition toolkit. In INTERSPEECH, 2023. 6 [41] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022. 5 [42] Jiaming Han, Kaixiong Gong, Yiyuan Zhang, Jiaqi Wang, Kaipeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Xiangyu Yue. Onellm: One framework to align all modalities with language, 2023. 3 [43] Bo He, Hengduo Li, Young Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim. Ma-lmm: Memory-augmented large multimodal model for long-term video understanding. arXiv preprint arXiv:2404.05726, 2024. 3 [44] Suyuan Huang, Haoxin Zhang, Yan Gao, Yao Hu, and Zengchang Qin. From image to video, what do we need in multimodal llms? arXiv preprint arXiv:2404.11865, 2024. [45] Md Mohaiminul Islam, Ngan Ho, Xitong Yang, Tushar Nagarajan, Lorenzo Torresani, and Gedas Bertasius. Video recap: Recursive captioning of hour-long videos. arXiv preprint arXiv:2402.13250, 2024. 3 [46] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, et al. Mistral 7b, 2023. 2 [47] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. 7 11 [48] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multiimage instruction tuning, 2024. 3 [49] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. arXiv preprint arXiv:2311.08046, 2023. 3 [50] Kumara Kahatapitiya, Kanchana Ranasinghe, Jongwoo Park, and Michael Ryoo. Language repository for long arXiv preprint arXiv:2403.14622, video understanding. 2024. [51] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 2 [52] Wonkyun Kim, Changin Choi, Wonseok Lee, and Wonjong Rhee. An image grid can be worth video: Zero-shot video question answering using vlm, 2024. 3 [53] Qiuqiang Kong, Yong Xu, Wenwu Wang, and Mark Plumbley. Audio set classification with attention model: In 2018 IEEE International probabilistic perspective. Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 316320. IEEE, 2018. 3 [54] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4. International Journal of Computer Vision (IJCV), 128(7):19561981, 2020. 2 [55] Yann LeCun. path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 2022. [56] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: multi-modal model with in-context instruction tuning. arXiv.org, 2023. 2 [57] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 7, 8, 9 [58] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixtureof-experts model, 2024. 3 [59] Jungang Li, Sicheng Tao, Yibo Yan, Xiaojie Gu, Haodong Xu, Xu Zheng, Yuanhuiyi Lyu, Linfeng Zhang, and Xuming Hu. SAVEn-Vid: Synergistic audio-visual integration for enhanced understanding in long video context. arXiv preprint arXiv:2411.16213, 2024. 3 [60] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 3, 9 [61] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multimodal video understanding benchmark, 2023. 3, 5 [62] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video underIn Proceedings of the IEEE/CVF standing benchmark. Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 7, 9 [63] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint arXiv:2311.17043, 2023. 3 [64] Yadong Li, Haoze Sun, Mingan Lin, Tianpeng Li, Guosheng Dong, Tao Zhang, Bowen Ding, Wei Song, Zhenglin Cheng, Yuqi Huo, Song Chen, Xu Li, Da Pan, Shusen Zhang, Xin Wu, Zheng Liang, Jun Liu, Tao Zhang, Keer Lu, Yaqi Zhao, Yanjun Shen, Fan Yang, Kaicheng Yu, Tao Lin, Jianhua Xu, Zenan Zhou, and Weipeng Chen. Ocean-omni: To understand the world with omni-modality, 2024. 3 [65] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pages 323340. Springer, 2025. 7 [66] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 3 [67] Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang Liu, and Maosong Sun. Streamingbench: Assessing the gap for mllms to achieve streaming video understanding. arXiv preprint arXiv:2411.03628, 2024. 2, [68] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2024. 2, 3, 8 [69] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in In Proceedings of the European Conference on context. Computer Vision (ECCV), pages 740755, 2014. 2 [70] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, et al. Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models. arXiv preprint arXiv:2311.07575, 2023. 2 [71] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 7 [72] Jiajun Liu, Yibing Wang, Hanghang Ma, Xiaoping Wu, Xiaoqi Ma, Xiaoming Wei, Jianbin Jiao, Enhua Wu, and Jie Hu. Kangaroo: powerful video-language model supporting long-context video input. arXiv preprint arXiv:2408.15542, 2024. 8 [73] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. TempCompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. [74] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s, 2022. 2 12 [75] Ziyu Liu, Zeyi Sun, Yuhang Zang, Wei Li, Pan Zhang, Xiaoyi Dong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. RAR: Retrieving and ranking augmented mllms for visual recognition. arXiv preprint arXiv:2403.13805, 2024. 3 [76] Holy Lovenia, Samuel Cahyawijaya, Genta Indra Winata, Peng Xu, Xu Yan, Zihan Liu, Rita Frieske, Tiezheng Yu, Wenliang Dai, Elham Barezi, et al. Ascend: spontaneous chinese-english dataset for code-switching in multiturn conversation. arXiv preprint arXiv:2112.06223, 2021. 3 [77] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. arXiv preprint arXiv:2306.07207, 2023. 3 [78] Ziyang Ma, Yakun Song, Chenpeng Du, Jian Cong, Zhuo Chen, Yuping Wang, Yuxuan Wang, and Xie Chen. Language model can listen while speaking. arXiv preprint arXiv:2408.02622, 2024. 3 [79] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [80] Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, and Li Yuan. Video-bench: for evaluatA comprehensive benchmark and toolkit arXiv preprint ing video-based large language models. arXiv:2311.16103, 2023. 3 [81] OpenAI. Chatgpt. https://openai.com/blog/ chatgpt, 2022. 2 [82] OpenAI. Gpt-4 technical report, 2023. 2 [83] OpenAI. Gpt-4v(ision) system card, 2023. 2 [84] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2 [85] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. 3 [86] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS), 2022. 2 [87] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 52065210. IEEE, 2015. 2, 3, 6 [88] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv.org, 2023. 2 [89] Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Shuangrui Ding, Dahua Lin, and Jiaqi Wang. Streaming long video understanding with large language models, 2024. 3, 4, 5, 8 [90] Qwen. Introducing Qwen-7B: Open foundation and human-aligned models (of the state-of-the-arts), 2023. 2 [91] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine learning (ICML), 2021. 3, 4 [92] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision, 2022. 3 [93] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech In ICML, recognition via large-scale weak supervision. 2023. 3 [94] Michael S. Ryoo, Honglu Zhou, Shrikant Kendre, Can Qin, Le Xue, Manli Shu, Silvio Savarese, Ran Xu, Caiming Xiong, and Juan Carlos Niebles. xgen-mm-vid (blip-3video): You only need 32 tokens to represent video even in vlms, 2024. [95] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. 2022. 2 [96] Fangxun Shu, Lei Zhang, Hao Jiang, and Cihang Xie. Audio-visual llm for video understanding. arXiv preprint arXiv:2312.06720, 2023. 3 [97] Yan Shu, Peitian Zhang, Zheng Liu, Minghao Qin, Junjie Zhou, Tiejun Huang, and Bo Zhao. Video-xl: Extra-long vision language model for hour-scale video understanding. arXiv preprint arXiv:2409.14485, 2024. 7 [98] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video understanding. arXiv preprint arXiv:2307.16449, 2023. 3 [99] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Haozhe Chi, Xun Guo, Tian Ye, Yanting Zhang, et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1822118232, 2024. 7 [100] Enxin Song, Wenhao Chai, Tian Ye, Jenq-Neng Hwang, Xi Li, and Gaoang Wang. Moviechat+: Question-aware sparse memory for long video question answering. arXiv preprint arXiv:2404.17176, 2024. 3 [101] Zhende Song, Chenchen Wang, Jiamu Sheng, Chi Zhang, Gang Yu, Jiayuan Fan, and Tao Chen. Moviellm: Enhancing long video understanding with ai-generated movies. arXiv preprint arXiv:2403.01422, 2024. [102] Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, and Chao Zhang. visual large language models, 2024. 3 video-salmonn: Speech-enhanced audio- [103] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners, 2024. 3 [104] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-CLIP: clip model focusing on wherever you want. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [105] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. SALMONN: Towards generic hearing abilities for large language models. arXiv preprint arXiv:2310.13289, 2023. [106] Zhiyuan Tang, Dong Wang, Yanguang Xu, Jianwei Sun, Xiaoning Lei, Shuaijiang Zhao, Cheng Wen, Xingjun Tan, Chuandong Xie, Shuran Zhou, et al. Kespeech: An open source speech dataset of mandarin and its eight subdialects. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 3 [107] Gemini Team. Gemini: family of highly capable multimodal models, 2023. 2 [108] InternLM Team. Internlm: multilingual language model https : / / with progressively enhanced capabilities. github.com/InternLM/InternLM, 2023. 2 [109] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv.org, 2023. [110] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models, 2023. [111] Christophe Veaux, Junichi Yamagishi, and Kirsten MacDonald. Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit. 2017. 3 [112] Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det: Vast vocabulary visual detection dataset. In The IEEE International Conference on Computer Vision (ICCV), 2023. 2 [113] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 3, 8, 9 [114] Peng Wang, Songshuo Lu, Yaohua Tang, Sijie Yan, Wei Xia, and Yuanjun Xiong. full-duplex speech dialogue scheme based on large language models. arXiv preprint arXiv:2405.19487, 2024. 3 [115] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2023. 3 [116] Xiong Wang, Yangze Li, Chaoyou Fu, Lei Xie, Ke Li, Xing Sun, and Long Ma. Freeze-Omni: smart and low latency speech-to-speech dialogue model with frozen llm. arXiv preprint arXiv:2411.00774, 2024. [117] Yueqian Wang, Xiaojun Meng, Yuxuan Wang, Jianxin Liang, Jiansheng Wei, Huishuai Zhang, and Dongyan Zhao. Videollm knows when to speak: Enhancing time-sensitive video comprehension with video-text duet interaction format, 2024. 3 [118] Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng. Videollamb: Long-context video understanding with recurrent memory bridges, 2024. 2, 3 [119] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video unarXiv preprint derstanding via large language models. arXiv:2404.03384, 2024. 3 [120] Shiwei Wu, Joya Chen, Kevin Qinghong Lin, Qimeng Wang, Yan Gao, Qianli Xu, Tong Xu, Yao Hu, Enhong Chen, and Mike Zheng Shou. Videollm-mod: Efficient video-language streaming with mixture-of-depths vision computation, 2024. 3 [121] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and TatSeng Chua. Next-gpt: Any-to-any multimodal llm, 2024. 3 [122] Binzhu Xie, Sicheng Zhang, Zitang Zhou, Bo Li, Yuanhan Zhang, Jack Hessel, Jingkang Yang, and Ziwei Liu. Funqa: In European Towards surprising video comprehension. Conference on Computer Vision, pages 3957. Springer, 2025. [123] Zhifei Xie and Changqiao Wu. Mini-omni: Language models can hear, talk while thinking in streaming. arXiv preprint arXiv:2408.16725, 2024. 2, 7 [124] Zhifei Xie and Changqiao Wu. Mini-omni2: Towards opensource gpt-4o with vision, speech and duplex capabilities, 2024. 3 [125] Li Xu, He Huang, and Jun Liu. Sutd-trafficqa: question answering benchmark and an efficient network for In Proceedings of video reasoning over traffic events. the IEEE/CVF conference on computer vision and pattern recognition, pages 98789888, 2021. 5 [126] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava : Parameter-free llava extension from images to videos for video dense captioning, 2024. 3, 8, 9 [127] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos, 2024. 3 [128] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng 14 Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. 3, 5, 7 [129] Zehui Yang, Yifan Chen, Lei Luo, Runyan Yang, Lingxuan Ye, Gaofeng Cheng, Ji Xu, Yaohui Jin, Qingqing Zhang, Pengyuan Zhang, et al. Open source magicdata-ramc: rich annotated mandarin conversational (ramc) speech dataset. arXiv preprint arXiv:2203.16844, 2022. 3 [130] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. 7, 8, 9 [131] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mplug-owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. 7 [132] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv.org, 2023. 2 [133] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. In Advances in Neural Information Processing Systems (NeurIPS), 2024. [134] Wenyi Yu, Siyin Wang, Xiaoyu Yang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Yuxuan Wang, and Chao Zhang. SALMONN-omni: codec-free llm for full-duplex speech understanding and generation. arXiv preprint arXiv:2411.18138, 2024. 3 [135] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Contextual object detection with arXiv preprint large language models. Chen Change Loy. multimodal arXiv:2305.18279, 2023. 3 [136] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130b: An open bilingual pretrained model. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. 2 [137] Aohan Zeng, Zhengxiao Du, Mingdao Liu, Kedong Wang, Shengmin Jiang, Lei Zhao, Yuxiao Dong, and Jie Tang. GLM-4-Voice: Towards intelligent and human-like endto-end spoken chatbot. arXiv preprint arXiv:2412.02612, 2024. 3 [138] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023. [139] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, and Xipeng Qiu. Anygpt: Unified multimodal llm with discrete sequence modeling, 2024. 3 [140] Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, et al. Wenetspeech: 10000+ hours multi-domain 15 mandarin corpus for speech recognition. In ICASSP 20222022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 61826186. IEEE, 2022. 2, 3, 6 [141] Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-CLIP: Unlocking the long-text capability of clip. arXiv preprint arXiv:2403.15378, 2024. 3 [142] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang, Shoubin Yu, Mohit Bansal, and Gedas Bertasius. simple llm framework for long-range video question-answering. arXiv preprint arXiv:2312.17235, 2023. [143] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities. arXiv preprint arXiv:2305.11000, 2023. 3 [144] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. 3, 9 [145] Haoji Zhang, Yiqin Wang, Yansong Tang, Yong Liu, Jiashi Feng, Jifeng Dai, and Xiaojie Jin. Flash-vstream: Memorybased real-time understanding for long video streams, 2024. 2, 3 [146] Kaiyan Zhang, Biqing Qi, and Bowen Zhou. Towards building specialized generalist ai with system 1 and system 2 fusion. arXiv preprint arXiv:2407.08642, 2024. 2 [147] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Hang Yan, et al. Internlmxcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 2 [148] Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin Wang, Linke Ouyang, Songyang Zhang, Wenwei Zhang, Yining Li, Yang Gao, Peng Sun, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Hang Yan, Conghui He, Xingcheng Zhang, Kai Chen, Jifeng Dai, Yu Qiao, Dahua Lin, and Jiaqi Wang. Internlm-xcomposer-2.5: versatile large vision language model supporting long-contextual input and output. arXiv preprint arXiv:2407.03320, 2024. 3, [149] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 2, 3, 7, 8 [150] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding, 2024. 3 [151] Yuanhan Zhang, Bo Li, haotian Liu, Yong jae Lee, Liangke Gui, Di Fu, Jiashi Feng, Ziwei Liu, and Chunyuan Li. Llava-next: strong zero-shot video understanding model, 2024. 8 [152] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data, 2024. 5 [153] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language arXiv.org, model with multi-modal in-context learning. 2023. 3 [154] Wenliang Zhao, Xumin Yu, and Zengyi Qin. Melotts: Highquality multi-lingual multi-accent text-to-speech, 2023. 6 [155] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. MLVU: comprehensive benchmark for multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2024. 2, 7 [156] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing visionlanguage understanding with advanced large language models. arXiv.org, 2023."
        }
    ],
    "affiliations": [
        "Beihang University",
        "Fudan University",
        "SenseTime Group",
        "Shanghai Artificial Intelligence Laboratory",
        "The Chinese University of Hong Kong",
        "Tsinghua University",
        "University of Science and Technology of China"
    ]
}