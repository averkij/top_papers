{
    "paper_title": "GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning",
    "authors": [
        "Duolin Sun",
        "Meixiu Long",
        "Dan Yang",
        "Yihan Jiao",
        "Zhehao Tan",
        "Jie Feng",
        "Junjie Wang",
        "Yue Shen",
        "Peng Wei",
        "Jian Wang",
        "Jinjie Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 3 5 6 1 1 . 1 1 5 2 : r GroupRank: Groupwise Reranking Paradigm Driven by Reinforcement Learning 1st Duolin Sun Ant Group Hangzhou, China sunduolin.sdl@antgroup.com 1st Meixiu Long Sun Yat-sen University, Ant Group Guangzhou, China longmx7@mail2.sysu.edu.cn 2nd Dan Yang Ant Group Hangzhou, China luoyin.yd@antgroup.com 3th Yihan Jiao Ant Group Hangzhou, China jiaoyihan.yh@antgroup.com 4th Yue Shen Ant Group Hangzhou, China zhanying@antgroup.com 5th Zhehao Tan Ant Group Hangzhou, China tanzhehao.tzh@antgroup.com 6th Jie Feng Ant Group Hangzhou, China fengjie.feng@antgroup.com 7th Junjie Wang Ant Group Hangzhou, China wjj417805@antgroup.com 8th Peng Wei Ant Group Hangzhou, China peng.wei@antgroup.com 9th Jian Wang Ant Group Hangzhou, China bobblair.wj@antgroup.com 10th Jinjie Gu Ant Group Hangzhou, China jinjie.gujj@antgroup.com AbstractLarge Language Models (LLMs) have shown strong potential as rerankers to enhance the overall performance of retrieval-augmented generation (RAG) systems. However, existing reranking paradigms are constrained by core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trapoverlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, novel reranking paradigm. In this approach, the query and group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with heterogeneous reward function that integrates ranking metrics (e.g., NDCG, Recall) with distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high-quality labeled data, we further propose an innovative pipeline for synthesizing high-quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning-intensive retrieval benchmarks, BRIGHT and R2MED, our GroupRank achieves new state-of-the-art performance on the NDCG@10 metric, reaching 46.8 and 52.3, respectively, while also demonstrating strong generalization capability on traditional retrieval tasks. Our code, models and prompts can be found at https://github.com/AQ-MedAI/Diver.git I. INTRODUCTION In Retrieval-Augmented Generation (RAG) systems [1][4], reranking serves as critical gatekeeper for enhancing the quality of search results. typical system employs two-stage retrieve-and-rank architecture: lightweight retriever first recalls hundreds of candidate documents from massive corpus, after which more sophisticated and powerful reranker performs deep analysis to precisely order these candidates. By filtering the reranker out effectively compensates for the shortcomings of the initial retrieval stage, significantly boosting the systems overall precision and user satisfaction. irrelevant or redundant information, The advent of Large Language Models (LLMs) [5] [8] has sparked revolutionary shift in reranking technology. With their profound capabilities in natural language understanding, semantic reasoning, and contextual awareness, LLMs have demonstrated unprecedented potential in assessing the complex relevance between query and document. In particular, Large Reasoning Models (LRMs) [6], [9], specialized for complex inference tasks, are being adapted for reranking, where they not only perform deep semantic matching but also holistically evaluate documents utility, coherence, and reliability. These advanced models are continuously pushing the state-of-the-art on various retrieval benchmarks by enabling more fine-grained text interaction and reasoning. Despite these advancements, the prevailing reranking paradigmsnamely, Pointwise and Listwise methods exhibit inherent limitations as shown in Fig 1. Pointwise methods [10], which independently evaluate each querydocument pair, are simple and flexible but suffer from the Ranking Myopia Trap: the model lacks global view of the candidate set, hindering its ability to make an optimal global ranking decision. Conversely, Listwise methods, which learn to rank the entire list of documents end-to-end, can perceive global context and theoretically achieve superior performance. However, this approach suffers from List Rigidity, struggling to process large number of candidates (e.g., 100) directly. This often necessitates approximation techniques like sliding windows, which compromise the models valuable global perspective and increase implementation complexity. To overcome these limitations and fully leverage the reasoning capabilities of large models, we propose novel reranking model and training framework, termed GroupRank. This approach aims to synergize the flexibility of Pointwise methods with the high performance of Listwise approaches. Specifically, we feed query and group of candidate documents to the reranker simultaneously. The model, leveraging its intrinsic reasoning abilities, generates comprehensive score (e.g., for relevance, utility) for each document within the group in single forward pass. This groupwise scoring paradigm enhances the models global awareness by enabling inter-document comparisons, thereby mitigating the Ranking Myopia Trap problem. Concurrently, it maintains the flexibility to process an arbitrary number of documents, thus avoiding the List Rigidity dilemma. To effectively train our model, we further introduce novel heterogeneous reward during the Reinforcement Learning (RL) stage, which is crafted to bolster the performance of the GroupRank. It composes reward function that integrates ranking metrics (e.g., nDCG, Recall) and score distribution consistency to guide the model toward an optimal ranking policy. We validate the efficacy of our method through extensive experiments on several inference-intensive public retrieval benchmarks, including BRIGHT, R2MED and BEIR. The results demonstrate that our GroupRank achieves state-of-the-art (SOTA) performance on these challenging leaderboards, underscoring its superior capabilities in handling complex information retrieval tasks. Our main contributions are threefold: 1) We propose an innovative reranking framework named GroupRank. Its novel groupwise scoring mechanism ingeniously fuses the flexibility of Pointwise methods with the global context awareness of Listwise approaches. Furthermore, we construct Heterogeneous Reward to explicitly strengthen the reranking capability of our GroupRank during the reinforcement learning stage. 2) To surmount the bottleneck of scarce high-quality labeled data of GroupRank, we have designed and implemented an efficient synthetic data pipeline. This pipeline is capable of generating large-scale, high-quality, and general-purpose training data for both the retriever and reranker models. 3) Extensive experiments demonstrate that our proposed method achieves state-of-the-art (SOTA) results in both performance and efficiency. It particularly excels in retrieval tasks that require deep reasoning, thus providing robust technical support for the development of next-generation intelligent retrieval systems. II. RELATED WORK Large Language Models (LLMs) have demonstrated remarkable capabilities in Natural Language Processing (NLP) tasks. However, they face two fundamental challenges: knowledge cutoff, where their internal knowledge is static and confined to the pre-training data, and propensity for hallucination, i.e., generating factually incorrect content. Retrieval-Augmented Generation (RAG) [11], [12] has emerged as an effective paradigm to address these issues [13][15]. Within this framework, the reranking model has gained significant attention as critical bridge between the initial coarse-grained retrieval stage and the final refined generation stage [16]. The reranker is tasked with re-ordering the list of documents returned by the retriever to prioritize documents that are most relevant, informative, and beneficial for the subsequent generation task, thereby directly influencing the quality of the final generated answer [17]. A. LLMs for Reranking Recent advancements in LLMs have revolutionized reranking techniques. Researchers have begun to explore leveraging the sophisticated reasoning capabilities of LLMs for the reranking task. For instance, meticulously crafted prompts can guide base LLM to score candidate documents directly [18]. To further unlock the full potential of LLMs, the research focus has shifted towards training strategies that specialize LLMs as dedicated rerankers. This has given rise to several advanced training strategies, such as Supervised Fine-Tuning (SFT) [19] and two-stage frameworks that combine SFT with Reinforcement Learning (RL) [20], [21]. B. Reranking Paradigms Current LLM-based ranking methods can be categorized into three paradigms: pointwise, pairwise, and listwise methods. These methods differ in their input format and optimization objectives. The pointwise approach simplifies the complex ranking problem into an independent relevance scoring of each query-document pair, Fig. 1. Comparison of reranking paradigms. For ranking documents given query, three primary approaches are compared based on their complexity and efficiency. The Pointwise method operates without inter-document comparison, allowing for exact, concurrent solving with O(N ) complexity, though yielding poorer performance. The Listwise approach uses sliding windows for approximation, achieving better results due to comparison, but is sequential and non-concurrent with O(N/w) complexity. Our Groupwise method enables document comparison and score output, offering high effectiveness. Crucially, it retains the benefit of exact solving and concurrency, achieving O(N/c) complexity. Meanwhile, denotes the number of documents compared per request, and denotes the sliding window. thereby disregarding the relative positions of documents within the list [18], [22], [23]. While efficient, this approach loses global context by ignoring the interactions and relative ordering among documents. The pairwise approach compares pairs of documents to determine their relative relevance to the query, and then aggregates these comparisons using an algorithm (e.g., Bubble Sort) to form global ranking [24], [25]. This method can more accurately capture the global ranking structure but incurs significant computational complexity, typically O(N 2). The listwise approach, in contrast, processes an entire list of candidate documents at once, maximally leveraging the contextual reasoning capabilities of LLMs [26][29]. By performing global comparison, listwise methods often achieve superior performance. However, they are constrained by the LLMs limited context window and suffer from high inference latency. Building upon this body of research, we introduce novel groupwise framework that effectively integrates the strengths of both pointwise and listwise methods. This hybrid approach aims to enhance ranking performance while simultaneously reducing inference costs. Furthermore, we present concrete training methodology for our framework, detailing specific strategies for both Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). III. PRELIMINARIES Pointwise Reranking is the most straightforward indepenreranking paradigm as shown in Fig 1. It dently estimates the relevance between each document and the given query, transforming the reranking task into regression or classification problem. For each querydocument pair (q, di), the reranking model fθ predicts relevance score si as follows: si = fθ(q, di) (1) The final ranking is determined by sorting all documents according to their predicted scores in descending order. During training, the objective is to minimize the discrepancy between the predicted score si and the ground-truth label yi. If yi is continuous score (e.g., ranging from 0 to 10), the Mean Squared Error (MSE) loss is typically used; if yi is binary label (e.g., relevant/irrelevant), the Binary Cross-Entropy (BCE) loss is commonly applied. The overall loss for query can be expressed as: Lpointwise = (cid:88) i=1 L(cid:0)fθ(q, di), yi (cid:1) (2) where denotes the selected point-level loss function (e.g., MSE or BCE). a) Advantages.: The pointwise approach is simple to implement, highly flexible, and computationally efficient, since the scoring of each document can be processed independently and in parallel. b) Limitations.: Its main drawback lies in the Ranking Myopia Trap: the model treats each document in isolation without considering the global context of the candidate set. When multiple documents are partially relevant to the query, the model lacks the ability to capture their relative importance, which may lead to suboptimal ranking results. Listwise Reranking In contrast, Listwise Reranking jointly considers the entire candidate list during training, learning to optimize the global ranking structure directly. Instead of predicting independent relevance scores, it aims to produce an overall ranking that best approximates the ideal order. This paradigm aligns more closely with the ultimate goal of ranking and is often trained to directly optimize metrics such as Normalized Discounted Cumulative Gain (NDCG) as shown in Fig 1. Formally, the listwise reranking model fθ takes query and its candidate document set = {d1, d2, . . . , dk} as input, and outputs corresponding score vector: = fθ(q, D) = [s1, s2, . . . , sk] (3) The training objective is to minimize the discrepancy between the predicted score vector and the groundtruth label set = {y1, y2, . . . , yk}, formulated as: Llistwise = Llist(S, ) (4) where Llist denotes list-level loss function that quantifies ranking-level discrepancies. For an LLM reranker, the Llist refers to the difference between the models output and the ground truth ranking. c) Advantages.: The listwise paradigm incorporates global contextual awareness and models interdocument dependencies. Its training objective aligns directly with ranking performance metrics, typically leading to superior overall results compared to pointwise methods. d) Limitations.: However, it faces the List Rigidity Dilemma: the model is usually trained on lists of fixed length, making it less adaptable to variable-length candidate sets during inference. Moreover, when the candidate list is large (e.g., hundreds of documents), the computational cost becomes substantial. Approximation techniques such as sliding-window training can alleviate this issue but may weaken the models global perspective. A. Towards Balanced Paradigm The ranking myopia gap of the pointwise paradigm and the list rigidity dilemma of the listwise paradigm reveal their respective limitations. To strike balance between flexibility and global optimality, we propose the GroupRank, which integrates the adaptability of pointwise methods with the global modeling capacity of listwise approaches. The next section provides detailed description of this method. IV. METHODOLOGY This chapter elaborates on our comprehensive proposed method, which is characterized by two core innovations. First, we design and implement synthetic pipeline to construct high-quality training data for general-purpose retrieval as seen in Figure 2. Second, we leverage this data to train Robust Retriever [30] and novel GroupRank model as shown in Figure 3. Specifically, our workflow, as illustrated in Figure 2, begins by generating set of candidate documents for each query via the data synthesis pipeline. large language model (LLM) is then employed to annotate their relevance scores. Subsequently, this high-quality synthetic data is used to train the GroupRank. This model, by scoring documents in groups, is designed to overcome the limitations inherent in traditional Pointwise and Listwise approaches. Finally, we fine-tune the model using reinforcement learning algorithm named GRPO [31] to optimize its ranking performance and improve the prediction capability of its score distribution. A. High-Quality Synthetic Data Generation Pipeline To train reranking model capable of both accurately predicting the absolute relevance of documents and precisely determining their relative order, we introduce novel data synthesis pipeline as shown in Fig 2. The objective of this pipeline is to generate gold-standard training data that amalgamates the score magnitude Fig. 2. Workflow for High-Quality Training Data Generation. After filtering candidate documents via hybrid retrieval, we employ two parallel annotation methods: Pointwise (LLM-based individual scoring) and Listwise (LLM-based holistic ranking). Finally, we apply weighted fusion to these two sets of annotations to generate highly reliable final scores and ranked list. This output is ideal for training GroupRank and can also be used for training Retrievers or other Rerankers. from Pointwise approaches with the ranking fidelity of Listwise methods. This process comprises the following three stages: Query Selection and Initial Candidate Set Construction The queries used in this study are sourced from three distinct collections: 1) the training set from ReasonRank (13k queries); 2) an in-house collection of medical domain queries (approx. 1k); and 3) collection of coding-related queries from LeetCode (approx. 700). For each query in our knowledge base, we employ two mainstream retrieval techniques in parallel: sparse retrieval model (BM25) and dense retrieval model, to retrieve the top-100 candidate documents from large-scale corpus. BM25 focuses on keyword matching, whereas the dense model captures semantic similarity. To leverage the complementary strengths of both methods, we first normalize their respective retrieval scores and then combine them via weighted sum to produce an initial hybrid relevance score. This is formulated as: Shybrid(q, d) = 0.5 norm(cid:0)SBM25(q, d)(cid:1) + 0.5 norm(cid:0)SDense(q, d)(cid:1) (5) where Shybrid(q, d) is the final fused relevance score between query and document d. SBM25(q, d) and SDense(q, d) are the raw scores from the BM25 and dense models, respectively. The norm() function is crucial normalization step that maps scores from different models, which may have vastly different distributions, into unified and comparable range (e.g., [0, 1]). Based on this hybrid score, we select the top-50 documents to form candidate list q, {d1, d2, . . . , d50}, which serves as the input for the subsequent annotation phase. Hybrid Annotation Strategy To obtain highly reliable supervision signals for reranking, we adopt hybrid annotation strategy that integrates both Pointwise and Listwise philosophies, leveraging powerful Large Language Models (LLMs) as teacher models for annotation. Pointwise Scoring: We utilize the Qwen3-235Binstruct model to independently evaluate each query-document pair q, di. The model is prompted to provide comprehensive assessment based on multiple dimensions, such as relevance and utility, assigning fine-grained score, denoted as Spointwise(di), on scale of 0 to 10. The advantage of this approach is its ability to provide an absolute score with clear magnitude for each document. The annotation prompts are available in the Appendix. Listwise Ranking: We employ the Gemini-2.5pro model, which takes the entire list of 50 documents, q, {d1, d2, . . . , d50}, as single input. The model is tasked to perform global comparison and reasoning over the entire list to directly output ranked order from 1 to 50, denoted as Rlistwise(di). The primary strength of this method lies in its superior global context-aware reasoning, enabling it to generate highly reliable relative ordering. The annotation prompts are available in the Appendix. Label Fusion and Final Score Generation After obtaining both the Pointwise scores and the Listwise rankings, we once again utilize an ensemble method to generate final, robust supervision score. Specifically, we first transform the Listwise rank ri = Rlistwise(di) into score-like format. We apply negative logarithmic transformation: Slistwise(di) = log(ri). This transformation preserves the monotonic nature of the ranking while amplifying the score differences between higherranked documents, which aligns with the intuition of ranking tasks. Subsequently, we perform weighted fusion of the normalized Pointwise scores and the transformed Listwise scores to compute the final ground-truth supervision score, Sgt(di): Sgt(di) = α norm(cid:0)Spointwise(di)(cid:1) + (1 α) norm(cid:0)Slistwise(di)(cid:1) (6) where norm() is the normalization function and α is hyperparameter balancing the contribution of the two strategies, which we set to 0.5. Through this pipeline, for each query, we generate high-quality training set consisting of 50 documents. Each document is assigned robust score that encapsulates both ranking information and score magnitude, thereby laying solid foundation for training our subsequent GroupRank. All the training data construction processes, prompts, and code can be found in our codebase. B. Two-stage Training Strategy for the GroupRank In this section, we introduce our proposed training strategy as shown in Figure 3 for the GroupRank, which is designed to synthesize the flexibility of pointwise methods with the high performance of listwise approaches. Our GroupRank overcome the limitations of existing methods by jointly scoring group of documents in single pass. Specifically, the model takes query and set of candidate documents {d1, . . . , dn} as unified input. Leveraging the contextual understanding capabilities of Large Language Model, it performs comprehensive reasoning and comparison across all documents. Ultimately, it outputs relevance score si for each document di, resulting in set of scores {s1, . . . , sn}. Crucially, each score is an integer token generated by the LLM, ranging from 0 to 10, rather than floating-point value. The final ranking is then determined based on these scores. This approach yields two primary advantages: 1) Enhanced Global Perspective: By introducing explicit comparisons among documents during the scoring process, the model gains superior understanding of their relative merits. This enhances the accuracy of the global ranking. 2) High Flexibility: Unlike listwise methods, our GroupRank can process an arbitrary number of documents at inference time without being constrained by fixed list size or requiring sliding window techniques. This design effectively balances performance and efficiency. Cold Start Supervised Fine-Tuning (SFT) The training process commences with Cold Start phase. Pretrained Large Language Models (LLMs) cannot inherently perform relevance scoring according to specific instructions, nor can they guarantee adherence to required output format. Therefore, we employ Supervised Fine-Tuning (SFT) to instill these foundational abilities into the model. The objective of this stage is twofold: reward"
        },
        {
            "title": "Heterogeneous",
            "content": "based Reinforcement Learning Following the SFT phase, we employ Reinforcement Learning (RL) to further enhance the scoring and ranking capabilities of the GroupRank. We design novel heterogeneous reward function specifically tailored for the GroupRank. This function is designed to effectively guide and enhance the reranking process. Specifically, we utilize the GRPO algorithm for optimization. During training, for each input x, we sample set of output sequences = {y1, y2, . . . , yG}. Each sequence yi receives reward ri, which is subsequently normalized within the group to compute an advantage value ˆAi. The optimization objective can be formulated as follows: 1) To enable the model to comprehend the relevance scoring task itself. 2) To ensure the model strictly adheres to the specified format for outputting scores. JGRPO(θ) = 1 G (cid:88) i=1 1 yi yi (cid:88) t=1 (cid:16) min ri,t(θ) ˆAi,t, clip (ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t πθ(yi,t x, yi,<t) πref(yi,t x, yi,<t) ri,t(θ) = , DKL = DKL(πθ πref) (cid:17) βDKL, (7) where ϵ and β are hyper-parameters. Our proposed heterogeneous reward is composed of three primary components, designed to create comprehensive training signal for the reranker. a) Recall Reward (Rrecall): This component directly evaluates the reranker from the perspective of retrieval. It ensures the relevant documents are retrieved and ranked as highly as possible within the list. b) Groupwise Ranking Reward (Rrank): This component directly evaluates the quality of the ranked list generated by the model. To ensure holistic assessment, Fig. 3. The two-stage training paradigm for the Group Wise Reranker is designed to combine the flexibility of pointwise methods with the high performance of listwise approaches. Within the reinforcement learning (RL) component, the reward function incorporates not only format reward and recall reward, but also incorporates our novel ranking reward and distribution reward, specifically designed for the GroupRank. we employ weighted combination of several established ranking metrics: ranking but also to produce meaningful absolute scores that reflect documents relevance. NDCG (Normalized Discounted Cumulative Gain): Measures the overall quality of the ranked list, with particular emphasis on the relevance of top-ranked documents. RBO (Rank-Biased Overlap): Evaluates the stability and similarity of the top portion of ranked lists, assigning greater weight to head-of-the-list agreement. The ranking reward, Rrank, is weighted aggregation of these metrics. c) Groupwise Distribution Reward (Rdist): This component is designed to align the distribution of scores predicted by the model with synthetic ground-truth score distribution, denoted as Sgt. The reward is calculated by measuring the discrepancy between the two distributions, specifically using the Kullback-Leibler (KL) divergence. The rationale for introducing this reward is to preserve the magnitude information inherent in the scores. This enables the model not only to perform Let Spreds be the score distribution predicted by the model and Sgt be the ground-truth score distribution. The distribution reward is formulated as: Rdist = 1 DKL(Sgt Spreds) (cid:32) (cid:88) = 1 Sgt(xi) log i=1 (cid:33) (8) Sgt(xi) Spreds(xi) where xi represents set of documents in the candidate set of size . d) Heterogeous Reward: The heterogeneous reis weighted sum of these three ward signal, RH, components. RH = α Rrecall + β Rrank + γ Rdist (9) where α, β, and γ are hyperparameters that balance the relative importance of the ranking quality and score distribution alignment. We set the three of them to 0.2, 0.5, and 0.1 respectively. e) Final Reward: To ensure proper output structure, we implement format rewards considering two kinds of formats: (1) output format which ensures the presence of both <reason> and <answer> tags, and (2) answer format that validates the content within <answer tags adheres to the specified groupwise ranking json format (e.g., {[1]: 5, [2]: 0, . . . , [19]: 3, [20]: 7}). = RH, Both Formats are Good, 0, 1, Both Formats are Incorrect. Only Output Format is Good, (10) By maximizing reward R, our heterogeneous rewardbased GRPO algorithm guides the GroupRank to learn an optimal policy that excels at both generating highquality rankings and accurately predicting the distribution of document relevance scores. V. EXPERIMENT To comprehensively evaluate the performance of our proposed GroupRank, we conducted extensive experiments on two reasoning-intensive benchmarks, BRIGHT [32] and R2MED [33], to validate its reasoning capabilities. Furthermore, to demonstrate its effectiveness on traditional retrieval tasks, we also performed full evaluation on the BEIR [34] benchmark. A. Benchmarks BRIGHT: reasoning-intensive benchmark comprising 12 datasets across diverse domains, including science, code, and mathematics. R2MED: complex retrieval benchmark specifically designed for the medical domain, requiring sophisticated reasoning. BEIR: heterogeneous information retrieval benchmark designed for unified zero-shot evaluation, which allows us to assess performance on wide range of traditional retrieval tasks. B. Baselines and Metrics"
        },
        {
            "title": "We compare our GroupRank against several strong",
            "content": "baselines, which can be categorized as follows: Non-reasoning Baselines: RankT5 (3B) [20] and RankZephyr (7B) [21]. Reasoning-based (7B, 14B) [22], Rank-K (32B) [35] and ReasonRank (7B, 32B) [28]. Baselines: Rank-R The performance of all models is evaluated using NDCG@10 as the primary metric. C. Implementation Details In our experimental setup, first-stage retriever first fetches an initial candidate set of documents. The reranker then processes the top-100 documents from this set. For the BRIGHT benchmark, we employed the DIVER-Retriever-4B [30] model as the retriever. To enhance retrieval quality, the search queries were rewritten by GPT-4 before being used to query the corpus. as [8] For the R2MED benchmark, we also used the DIVER-Retriever-4B model as the retriever, but we utilized the original, unaltered queries for retrieval. use Qwen2.5-7B-Instruct and Qwen2.5-32BWe Instruct reranker. the backbones Supervised fine-tuning (SFT) is conducted using the Swift learning (RL) training is performed with the Verl [37] framework. For the 7B model, we adopt full-parameter training, whereas the 32B model is trained using LoRA [38]. Further details, including data preprocessing, prompts, and implementation specifics, can be found in our codebase. framework [36], while reinforcement for our VI. RESULTS A. Overall Performance The experimental results unequivocally demonstrate that our proposed GroupRank achieves exceptional performance on two highly challenging, reasoningintensive retrieval benchmarks: BRIGHT and R2MED. For both the 7B and 32B model scales, our approach comprehensively surpasses all existing state-of-the-art (SOTA) models, including Rank-K and ReasonRank, in terms of average NDCG@10. Specifically, our GroupRank-32B model establishes new SOTA records on both benchmarks. Remarkably, the 7B variant, in most cases, even outperforms larger-scale versions of the 32B versions of Rank-K prior methods (e.g., and ReasonRank), which amply demonstrates the advanced capabilities and high efficiency of our proposed method. These achievements robustly validate the effectiveness of both the GroupRank architecture and its accompanying high-quality data synthesis pipeline. Analysis on the BRIGHT Benchmark On the comreasoning-intensive BRIGHT benchmark, prehensive, our models performance is particularly outstanding. As shown in Table I, GroupRank-32B achieves an average NDCG@10 score of 39.24, while GroupRank-7B reaches 36.65. This not only significantly surpasses the previous SOTA model, ReasonRank-32B (35.58), but also elevates the state-of-the-art on the leaderboard. Notably, the performance of our 7B model (36.65) exceeds TABLE THE RESULTS (NDCG@10) ON THE BRIGHT BENCHMARK. BOLD AND UNDERLINED DENOTES THE TOP TWO RERANKERS. Models Bio. Earth. Econ. Psy. Rob. Stack. Sus. Leet. Pony AoPS TheoQ. TheoT. Avg. Diver-Retriever-4B 52.52 53.59 33. 45.16 28.38 30.39 35.00 12.97 14. 9.83 42.51 36.32 32.93 Non-reasoning reranker rankT5 (3B) RankZephyr (7B) Reasoning reranker Rank-R1 (7B) Rank-R1 (14B) Rank-K (32B) ReasonRank (7B)-w20s10 ReasonRank (32B)-w20s GroupRank-7B GroupRank-32B 33.01 42.60 22.80 19.51 18.96 18.79 8.61 29.85 2.28 13. 10.07 13.33 23.94 29.29 11.97 12.97 34.93 31.37 8.96 7.44 19.69 31. 11.70 31.67 17.24 23.50 40.85 49.57 51.06 51.63 53.89 56.85 59.48 31.38 41.15 42.34 43.43 47.59 53.06 56. 23.17 27.56 32.95 32.40 36.34 35.94 40.12 32.04 40.04 44.52 43.99 52.64 19.82 28.46 33.07 31.02 36.48 36.65 48.75 50.46 38.36 11.23 28.19 28.44 25.63 34. 35.05 39.16 35.53 43.76 41.57 39.81 44.47 3.77 6.96 12.82 15.38 15.21 43.99 15.48 19.48 43.32 6.60 18.45 21.62 20.07 14.81 15.53 16. 4.99 7.88 8.48 6.95 5.47 10.64 13.34 17.20 34.80 39.31 38.91 40.63 41.45 46.39 30.10 43.11 43.15 40.69 45.29 46.40 47. 21.39 30.83 33.28 32.49 35.58 36.65 39.24 TABLE II THE RESULTS (NDCG@10) ON THE R2MED BENCHMARK. BOLD AND UNDERLINED DENOTES THE TOP TWO RERANKERS. Models Biology Bioin. MedS. MedE. MedD. PMCT. PMCC. IIYiC. Diver-Retriever-4B 49.30 60.36 57.86 18.60 22. 58.67 44.12 31.78 Non-reasoning reranker RankT5 (3B) RankZephyr (7B) Reasoning reranker Rank-R1 (7B) Rank-R1 (14B) Rank-K (32B) ReasonRank (7B)-w20-s10 ReasonRank (32B)-w20-s10 GroupRank-7B GroupRank-32B 39.83 48.81 58.19 58.97 58.94 48.89 53.89 56.66 59.48 46.26 55.70 65.80 63.15 67.75 61.58 68.94 65.43 69. 31.73 54.88 56.65 61.88 58.94 61.73 68.39 64.16 66.28 21.59 26.71 29.94 27.69 30.26 20.52 28.19 27.26 28. 19.88 26.07 40.64 40.37 40.52 32.38 40.30 30.54 41.47 31.36 57.81 66.05 66.31 65.33 59.13 67.40 64.63 67. 17.78 10.92 35.11 43.19 42.54 36.90 45.09 43.17 52.23 18.36 18.74 29.78 32.66 31.52 21.45 29.14 30.86 32. Avg. 42.91 28.35 37.45 47.77 49.28 49.47 42.82 50.17 47.84 52.28 that of all previously published larger-scale models, including Rank-K-32B (33.28) and ReasonRank-32B (35.58), showcasing its exceptional efficiency and performance. Our model secures the top performance in multiple sub-domains, such as Economics (Econ.), Earth Science (Earth.), Biology (Bio.), and Programming (Leet.), proving its strong generalization capabilities when handling diverse and reasoning-intensive queries. Analysis on the R2MED Benchmark In the more specialized R2MED medical complex retrieval its benchmark, our method similarly demonstrates top-tier capabilities. As presented in Table II, our GroupRank-32B model again sets new SOTA record with an average NDCG@10 of 52.28, marking steady improvement over the previous SOTA, ReasonRank-32B (50.17). Concurrently, our 7B model achieves an excellent score of 47.84, exhibiting strong competitiveness among models of similar scale. The R2MED dataset comprises highly specialized and fine-grained medical domains, posing significant challenges to models knowledge mastery and reasoning abilities. Our models leading performance in various sub-domains, including Biology (Biology), Bioinformatics (Bioin.), and Clinical Trials (PMCT.), indicates that the GroupRank not only excels in general-purpose domains but also possesses powerful discriminative and ranking capabilities for complex information retrieval tasks in specialized fields. Our models state-of-the-art performance is driven by two key innovations. First, our GroupRank architecture jointly evaluates document groups, fusing the precision of pointwise methods with the global context of listwise approaches to overcome common pitfalls like ranking myopia trap and list rigidity. Second, our data synthesis pipeline uses powerful LLMs to generate high-quality training labels that incorporate both absolute relevance scores and relative ranking preferences. This synergistic combination of an advanced architecture and robust data strategy explains its superior performance on complex, reasoning-intensive retrieval tasks like BRIGHT and R2MED. TABLE III THE RESULTS (NDCG@10) ON THE BEIR BENCHMARK. BOLD AND UNDERLINED DENOTES THE TOP TWO RERANKERS. Models Diver-Retriever-4B rankT5 (3B) RankZephyr (7B) Reasoning reranker Rank-R1 (7B) Rank-R1 (14B) Rank-K (32B) ReasonRank (7B)-w20s10 ReasonRank (32B)-w20s10 GroupRank-7B GroupRank-32B Arguana 34.19 Dbpedia-Entity 71.41 Nfcorpus 44.80 11.23 12. 14.31 17.12 22.22 19.15 18.45 30.16 24.98 58.91 59.79 61.45 78.19 78.38 69.78 79.01 71.23 81.97 35.27 37. 36.57 41.02 45.28 40.31 44.44 40.17 46.63 NQ 89.69 91.81 93.08 92.01 92.39 93.25 91.52 94.65 76.22 96. Scidocs 24.32 17.68 19.01 18.98 21.72 21.36 22.17 23.96 14.49 25.70 AVG. 52.88 42.98 44. 44.66 50.09 52.10 48.59 52.10 46.45 55.09 B. Traditional Retrieval Benchmark retrieval range of To further validate the generalization capability of tasks, our approach on wider we evaluated GroupRank on five representative subsets of the BEIR benchmark. As presented in Table III, our GroupRank-32B model achieves an average NDCG@10 of 55.09, significantly outperforming all existing state-of-the-art (SOTA) models, including RankK (32B) and ReasonRank (32B). This result sets new SOTA and strongly demonstrates that our synthetic data pipeline and Groupwise training method not only excel in reasoning-intensive tasks but are also highly competitive in general-purpose text retrieval scenarios. In-depth Analysis of the 32B Model On datasetthe GroupRank-32B model demonspecific level, strates dominant performance. Except for arguana, it secures the top scores on the other four datasets: dbpedia-entity (81.97), nfcorpus (46.63), nq (96.16), and scidocs (25.70). Particularly on nq and dbpedia-entity, which demand fine-grained fact matching and entity understanding, our model substantially outperforms other reasoning-based rerankers. We attribute this success to the design of our Groupwise framework. By jointly scoring group of documents in single pass, the model is compelled to perform cross-document comparison and information integration. This fosters more holistic perspective and stronger discriminative power, effectively mitigating the ranking myopia often observed in Pointwise methods. Impact of Model Scale comparison between the 7B and 32B model variants reveals clear scaling effect. While our GroupRank-7B model achieves an impressive score of 30.16 on arguananotably surpassing its 32B counterpartits performance is comparatively weaker on datasets such as nq and scidocs. We hypothesize that the sophisticated group-wise comparison and reasoning abilities inherent to our Groupwise approach demand substantial model capacity. At the 7B scale, the model may struggle to fully assimilate and leverage the fine-grained ranking signals embedded within our high-quality synthetic data. Conversely, when scaled to 32B, the models enhanced representational power and reasoning capabilities synergize effectively with our Groupwise training paradigm. This combination unlocks the full potential of our method, leading to substantial performance leap across multiple datasets. This indicates that our approach aligns well with the scaling trends of large language models (LLMs) and holds considerable promise for future advancements. C. Efficiency Analysis Table IV presents an empirical comparison of the efficiency of our proposed groupwise method against other reranking approaches. The comparison is based on three key metrics: Generate, which indicates whether method requires generative token decoding; Batching, which denotes the capability for batch inference; and # LLM calls, representing the worst-case number of forward passes. This number of calls is function of several parameters: (the number of documents to rerank), (repeats), (sliding window step size), (the number of top documents to retrieve), and (the number of documents compared per step). Our Groupwise method strikes an optimal balance between theoretical efficiency and practical performance by combining low number of LLM calls (O(N/c)) with the flexibility of batch inference. Compared to methods with low theoretical call complexity, such as heapsort and bubblesort, groupwise holds decisive advantage in its ability to perform batch processing. These other methods are constrained to serial execution of LLM queries, resulting in high end-to-end latency. In contrast, the inherent parallelism of the groupwise method allows it to fully leverage batch processing, enabling high-throughput inference and significantly reducing practical latency. Furthermore, when compared to other batch-compatible methods like pointwise (O(N )) TABLE IV THE PROPERTIES OF SOME METHODS. Methods Generate Batching # LLM Calls Pointwise.qlm Pointwise.yes no Listwise.generation Listwise.likelihood Pairwise.allpair Pairwise.heapsort Pairwise.bubblesort Setwise.heapsort Setwise.bubblesort Groupwise O(N ) O(N ) O(r (N/s)) O(r (N/s)) O(N 2 ) O(k log2 ) O(k ) O(k logc ) O(k (N/(c 1))) O(N/c) and pairwise.allpair (O(N 2)), our groupwise approach requires substantially fewer LLM calls. In summary, the Groupwise method not only maintains low computational complexity but also overcomes the practical parallelization limitations that hinder other theoretically efficient algorithms. This makes it highly effective and practical solution for large-scale document reranking. VII. ABLATION STUDY To systematically validate the efficacy of our proposed GroupRank framework and to thoroughly dissect the contributions of its core components, we conduct series of comprehensive ablation studies on the BRIGHT benchmark. The results are summarized in Table V. A. Experimental Setup To ensure fair and reliable evaluation, we establish robust baseline retrieval system. Specifically, we employ hybrid retriever combining Diver-Retriever4B, state-of-the-art dense retriever, with BM25. This setup is designed to ensure high recall within the top100 candidate documents passed to the reranker, thereby allowing its capabilities to be fully showcased. All ablation experiments are conducted on our GroupRank32B model. B. Overall Efficacy of the GroupRank First, our complete GroupRank model significantly boosts the average BRIGHT score from 36.93, achieved TABLE ABLATION STUDY ON BRIGHT. Method Diver-Retriever-4B + BM25 (hybrid) GroupRank (ours) w/o SFT (only RL) w/o RL (only SFT) w/o Ranking Reward w/o Point Distribution Reward Score 36.93 42.18 38.17 40.70 40.57 41.00 by the strong hybrid retriever baseline, to 42.18, marking an absolute improvement of 5.25 points. This result unequivocally demonstrates that our framework, serving as post-ranking module, can effectively differentiate and reorganize the initial retrieval results. By promoting more relevant and useful documents to higher ranks, it substantially enhances the overall performance of the retrieval system. C. Contribution of the Training Strategy: Synergy of SFT and RL To investigate the necessity of our two-stage training paradigm (SFT followed by RL), we conduct experiments by ablating each stage. a) w/o RL (SFT Only): When the model is trained using only Supervised Fine-Tuning (SFT) without the subsequent Groupwise Ranking Policy Optimization (GRPO), its performance drops from 42.18 to 40.70. This indicates that while SFT on our high-quality synthetic data provides the model with strong foundation in relevance judgment and score calibration, it is fundamentally form of imitation learning. It lacks direct optimization for final ranking metrics such as NDCG. GRPO, through reinforcement learning, bridges this gap by fine-tuning the model end-to-end for the specific ranking objectives, thereby unlocking the models full reranking potential. b) w/o SFT (RL Only): Conversely, if we bypass the SFT stage and train the model from scratch directly with GRPO, the performance plummets to 38.17, only 1.24-point improvement over the retriever-only baseline. This highlights the critical role of SFT as cold start mechanism. RL exploration in vast policy space is notoriously sample-inefficient and unstable. SFT initializes the model in favorable policy region by leveraging high-quality data, which significantly reduces the difficulty of RL exploration and enables stable and efficient optimization from much-improved starting point. In summary, SFT and RL fulfill complementary and indispensable roles within our framework: SFT injects foundational knowledge and ensures stability, while RL performs targeted, goal-oriented refinement upon this foundation. D. Analysis of GRPO Reward Components Our GRPO algorithm utilizes composite reward function, which includes Ranking Reward and Point Distribution Reward. We ablate each component to analyze its impact. a) w/o Ranking Reward: Removing ranking-based metrics (e.g., NDCG, Recall, RBO) from the reward function causes the models performance to decrease to 40.57. This demonstrates that merely incentivizing the model to produce score distribution similar to the ground truth is insufficient. While the score distribution reward helps maintain reasonable score scale, it does not explicitly encourage placing the most relevant documents at the top. The ranking reward, being directly tied to final evaluation metrics, is the primary driver for learning the core capability of how to rank and is crucial for achieving high ranking performance. b) w/o Point Distribution Reward: Ablating the point distribution reward also leads to performance decline, with the score dropping to 41.00. Although this drop is less severe than when ablating the ranking reward, it still validates the components value. With only ranking reward, the model might resort to reward hacking. For instance, to maximize NDCG, it could learn to assign an extremely high score to the most relevant document and extremely low scores to all others, thereby destroying the intrinsic meaning and calibration of the scores. The Point Distribution Reward acts as an effective regularizer, compelling the model to optimize ranking while simultaneously preserving score granularity and scale. This prevents policy collapse and leads to more robust and generalizable ranking model. In conclusion, our ablation studies provide compelling evidence for the rationale and effectiveness of each design choice within the GroupRank framework. The results confirm that: (1) the two-stage SFT+RL training paradigm is essential for achieving optimal performance, with both stages being indispensable; and (2) the heterogeneous reward design in RL is highly effective, where the ranking reward is the main performance driver and the point distribution reward ensures model stability and score calibration. These components work in concert to achieve state-of-the-art performance on inferenceintensive retrieval tasks. VIII. ADDITIONAL EXPERIMENTS AND RESULTS To further explore the upper performance bound of our proposed Reranker on the BRIGHT benchmark, we conducted experiments based on the current state-ofthe-art retriever model, reason-embed-basic-qwen3-8b0928, using its top-100 retrieved results for reranking. To further improve the overall performance of the ranking system, we incorporated test-time scaling strategy. Specifically, we used the fused retrieval scores from reason-embed-basic-qwen3-8b-0928 and BM25 with reasoning as the first-stage ranking scores. During test-time scaling, we combined two complementary strategies: 1) Sliding Windows. We grouped the retrieved documents into batches of 20 and applied fixed sliding step size = 10. For example, the first batch contained documents doc1doc20, while the second batch contained doc11doc30. Each batch was then fed into the reranker, and the final document scores were obtained by averaging the predictions across all sliding windows. This listwise-style prediction approach enhances the robustness of the reranking process. 2) Self-Ensemble. We randomly shuffled the top-100 retrieved documents multiple times and divided them into groups of 20 for each inference pass. After performing reranking inferences, we averaged the predicted scores for each document before the final sorting. This ensemble mechanism further improves stability and reduces the variance of the reranking results. After obtaining the final reranker scores, we further combined them with the original retriever scores through weighted fusion. Specifically, we set the weights to Reranker : Retriever = 0.6 : 0.4 for the first seven datasets, and Reranker : Retriever = 0.2 : 0.8 for the remaining five datasets. The final results are summarized as follows: From the table VI, we can observe that under the sliding-window strategy, performing six rounds of iterative reasoning achieves an NDCG@10 score of 46.8, representing 0.4 improvement over the current stateof-the-art method BGE-Reasoner-0928, which ensembles seven models. Moreover, regardless of whether the sliding-window strategy is applied, the models prediction accuracy consistently increases with higher computational complexity. This is because multiple rounds of random grouping allow the model to better compare differences between documents, thereby producing more accurate predictions."
        },
        {
            "title": "CONCLUSION",
            "content": "This paper addresses the core challenges faced by rerankers within modern Retrieval-Augmented Generation (RAG) systems [39]. We observe that conventional Pointwise methods, which evaluate documents independently, suffer from ranking myopia trap due to the absence of inter-document comparisons. Conversely, Listwise methods, despite their global context awareness, are constrained by list rigidity dilemma, rendering them inflexible for ranking large and variably-sized document sets. To surmount these challenges, we present three primary contributions. First, we introduce novel training framework based on reinforcement learning, termed the GroupRank. This approach harmonizes the flexibility of Pointwise methods with the performance of Listwise approaches. By enabling the model to jointly evaluate and score group of documents at once, it introduces crucial inter-document comparisons to foster global perspective while retaining the flexibility to handle an arbitrary number of documents. We employ Groupwise Ranking Policy Optimization (GRPO) algorithm for training, guided by heterogeneous reward TABLE VI THE ADDITIONAL RESULTS (NDCG@10) ON THE BRIGHT BENCHMARK. INDICATES THE NUMBER OF PREDICTION TIMES Models Bio. Earth. Econ. Psy. Rob. Stack. Sus. Leet. Pony AoPS TheoQ. TheoT. Avg. N=1 N=2 N=3 N=4 N=5 N=6 N=1 N=2 N=3 N=4 N=5 N= 60.31 64.59 65.58 65.80 64.85 66.57 60.31 64.75 64.31 65.48 66.20 65.97 61.07 61.56 61.26 61.13 61.82 62.17 60.07 60.94 62.75 62.07 62.22 63.70 37.93 40.53 39.79 41.23 38.67 40.99 42.00 41.76 41.54 41.53 42.39 42. 51.89 54.74 54.80 54.78 54.11 55.33 53.80 53.61 55.71 55.45 55.30 55.01 37.33 38.68 41.23 40.43 40.23 41.34 41.78 40.95 42.00 41.37 42.56 40.59 w/o sliding windows 41.49 44.09 44.60 43.92 44.46 42. 45.95 47.70 48.70 49.83 49.93 50.73 sliding windows 44.35 45.14 45.38 43.92 43.40 44.69 47.80 48.92 49.52 48.84 50.13 50.42 30.84 31.66 32.42 31.01 31.35 32.11 32.24 32.89 32.40 32.25 32.65 32.47 45.24 46.40 46.94 46.12 46.26 46. 46.24 46.31 46.68 47.02 46.94 47.32 17.27 17.05 16.46 17.35 16.75 16.89 17.52 17.40 16.80 17.17 16.65 17.16 45.68 46.19 46.15 45.84 45.59 46.47 45.54 45.85 46.29 45.83 46.29 46.44 55.72 55.45 55.07 55.64 55.96 55. 54.02 55.10 56.28 54.74 55.80 55.57 44.23 45.72 46.08 46.09 45.83 46.45 45.47 46.13 46.64 46.31 46.71 46.82 rerankers outputs would guide the generative model in collaborative, joint RL framework, aligning retrieval quality with generation quality. This approach aims to create an end-to-end RL-driven system that progressively improves both ranking and generation. function that incorporates recalling reward, ranking reward (e.g., NDCG, RBO), and measure of distributional divergence. Second, we design and implement an innovative, high-quality data synthesis pipeline. This pipeline uniquely leverages the capabilities of Large Language Models in both Pointwise (scoring) and Listwise (ranking) annotation modes. By employing weighted fusion and log(rank) transformation, we generate training data that possesses both precise relevance scores and robust ranking order, laying solid foundation for end-to-end training of high-performance rerankers. Finally, extensive experimental results validate the efficacy of our approach. On challenging, reasoning-intensive retrieval benchmarks such as BRIGHT and R2MED, our GroupRank achieves new State-of-the-Art (SOTA) performance, demonstrating significant improvements on key metrics like NDCG@10 over previous leading models. Furthermore, our model exhibits strong competitiveness on traditional retrieval tasks. These results not only affirm the superiority of the Groupwise architecture but also underscore the value of our data synthesis strategy. In summary, the holistic optimization solution proposed in this study, particularly the GroupRank, presents an effective and scalable new paradigm for constructing more accurate and efficient RAG systems. Looking ahead, we plan to explore more cost-effective data synthesis methods and investigate the generalization of the Groupwise principle to broader range of information retrieval and recommendation tasks. IX. FUTURE WORK For future work, we plan to explore progressive reinforcement learning paradigm for RAG systems. First, the GroupRank would be trained incrementally, starting with simpler ranking tasks and gradually tackling more complex, reasoning-intensive document sets to the better capture inter-document relationships. Next,"
        },
        {
            "title": "REFERENCES",
            "content": "[1] Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, M. Wang, and H. Wang, Retrieval-augmented generation for large language models: survey, 2024. [Online]. Available: https://arxiv.org/abs/2312.10997 [2] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W. tau Yih, T. Rocktaschel, S. Riedel, and D. Kiela, Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. [Online]. Available: https://arxiv.org/abs/2005.11401 [3] M. Cheng, Y. Luo, J. Ouyang, Q. Liu, H. Liu, L. Li, S. Yu, B. Zhang, J. Cao, J. Ma, D. Wang, and E. Chen, survey on knowledge-oriented retrieval-augmented generation, 2025. [Online]. Available: https://arxiv.org/abs/2503.10677 [4] C. Gan, D. Yang, B. Hu, H. Zhang, S. Li, Z. Liu, Y. Shen, L. Ju, Z. Zhang, J. Gu et al., Similarity is not all you need: Endowing retrieval augmented generation with multi layered thoughts, arXiv preprint arXiv:2405.19893, 2024. [5] A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, C. Zheng, D. Liu, F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng, M. Li, M. Xue, M. Li, P. Zhang, P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li, T. Tang, W. Yin, X. Ren, X. Wang, X. Zhang, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Zhang, Y. Wan, Y. Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou, and Z. Qiu, Qwen3 technical report, 2025. [Online]. Available: https://arxiv.org/abs/2505.09388 [6] D. G. DeepSeek-AI, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arxiv, Preprint posted online on, vol. 22, pp. 1314, 2025. [7] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C. Li, D. Liu, F. Huang, G. Dong, H. Wei, H. Lin, J. Tang, J. Wang, J. Yang, J. Tu, J. Zhang, J. Ma, J. Yang, J. Xu, J. Zhou, J. Bai, J. He, J. Lin, K. Dang, K. Lu, K. Chen, K. Yang, M. Li, M. Xue, N. Ni, P. Zhang, P. Wang, R. Peng, R. Men, R. Gao, R. Lin, S. Wang, S. Bai, S. Tan, T. Zhu, T. Li, T. Liu, W. Ge, X. Deng, X. Zhou, X. Ren, X. Zhang, X. Wei, X. Ren, X. Liu, Y. Fan, Y. Yao, Y. Zhang, Y. Wan, Y. Chu, Y. Liu, Z. Cui, Z. Zhang, Z. Guo, and Z. Fan, Qwen2 technical report, 2024. [Online]. Available: https://arxiv.org/abs/2407.10671 [8] Qwen, :, A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Tang, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu, Qwen2.5 technical report, 2025. [Online]. Available: https://arxiv.org/abs/2412.15115 [9] F. Xu, Q. Hao, Z. Zong, J. Wang, Y. Zhang, J. Wang, X. Lan, J. Gong, T. Ouyang, F. Meng, C. Shao, Y. Yan, Q. Yang, Y. Song, S. Ren, X. Hu, Y. Li, J. Feng, C. Gao, and Y. Li, Towards large reasoning models: survey of reinforced reasoning with large language models, 2025. [Online]. Available: https://arxiv.org/abs/2501.09686 [10] C. Gan, D. Yang, B. Hu et al., Polyrag: Integrating polyviews into retrieval-augmented generation for medical applications, 2025. [Online]. Available: https://arxiv.org/abs/2504.14917 [11] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Kuttler, M. Lewis, W.-t. Yih, T. Rocktaschel, S. Riedel, and D. Kiela, Retrieval-augmented generation for knowledgeintensive nlp tasks, in Proceedings of the 34th International Conference on Neural Information Processing Systems, ser. NIPS 20. Red Hook, NY, USA: Curran Associates Inc., 2020. [12] J. Wang, M. Chen, B. Hu, D. Yang et al., Learning to plan for retrieval-augmented large language models from knowledge graphs, in Findings of the Association for Computational Linguistics: EMNLP 2024, Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, Eds. Miami, Florida, USA: Association for Computational Linguistics, Nov. 2024, pp. 78137835. [Online]. Available: https://aclanthology.org/2024.findings-emnlp.459/ [13] A. Asai, S. Min, Z. Zhong, and D. Chen, Retrieval-based language models and applications, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), Y.-N. V. Chen, M. Margot, and S. Reddy, Eds. Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 4146. [Online]. Available: https://aclanthology.org/2023.acl-tutorials.6/ [14] Y. Jiao, Z. Tan, D. Yang, D. Sun, J. Feng, Y. Shen, J. Wang, instruction-tuning [Online]. Available: and P. Wei, Hirag: Hierarchical-thought retrieval-augmented generation, 2025. https://arxiv.org/abs/2507.05714 [15] Z. Tan, Y. Jiao, D. Yang, L. Liu et al., Prgb benchmark: robust placeholder-assisted algorithm for benchmarking retrieval-augmented generation, 2025. [Online]. Available: https://arxiv.org/abs/2507.22927 [16] W. Fan, Y. Ding, L. Ning, S. Wang, H. Li, D. Yin, T.-S. Chua, and Q. Li, survey on rag meeting llms: Towards retrieval-augmented large language models, in Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, ser. KDD 24. New York, NY, USA: Association for Computing Machinery, 2024, p. 64916501. [Online]. Available: https://doi.org/10.1145/3637528.3671470 [17] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, Lost in the middle: How language models use long contexts, Transactions of the Association for Computational Linguistics, vol. 12, pp. 157173, 2024. [Online]. Available: https://aclanthology.org/2024.tacl-1.9/ [18] A. Abdallah, J. Mozafari, B. Piryani, M. M. Abdelgwad, and A. Jatowt, DynRank: Improve passage retrieval with dynamic zero-shot prompting based on question classification, in Proceedings International Conference 31st on Computational Linguistics, O. Rambow, L. Wanner, M. Apidianaki, H. Al-Khalifa, B. D. Eugenio, and S. Schockaert, Abu Dhabi, UAE: Association for Computational Eds. Linguistics, Jan. 2025, pp. 47684778. [Online]. Available: https://aclanthology.org/2025.coling-main.319/ the of [19] Y. Yu, W. Ping, Z. Liu, B. Wang, J. You, C. Zhang, M. Shoeybi, and B. Catanzaro, Rankrag: unifying context ranking with retrieval-augmented generation in llms, in Proceedings of the 38th International Conference on Neural Information Processing Systems, ser. NIPS 24. Red Hook, NY, USA: Curran Associates Inc., 2025. [20] H. Zhuang, Z. Qin, R. Jagerman, K. Hui, J. Ma, J. Lu, J. Ni, X. Wang, and M. Bendersky, Rankt5: Fine-tuning t5 for text ranking with ranking losses, 2022. [Online]. Available: https://arxiv.org/abs/2210.10634 [21] R. Pradeep, S. Sharifymoghaddam, and J. Lin, Rankzephyr: Effective and robust zero-shot listwise reranking is breeze! 2023. [Online]. Available: https://arxiv.org/abs/2312.02724 [22] S. Zhuang, X. Ma, B. Koopman, J. Lin, and G. Zuccon, Rank-r1: Enhancing reasoning in llm-based document rerankers via reinforcement learning, 2025. [Online]. Available: https: //arxiv.org/abs/2503.06034 [23] Y. Cai, Y. Zhang, D. Long, M. Li, P. Xie, and W. Zheng, Erank: Fusing supervised fine-tuning and reinforcement learning for effective and efficient text reranking, 2025. [Online]. Available: https://arxiv.org/abs/2509.00520 [24] Y. Fan, X. Chen, D. Ye, J. Liu, H. Liang, J. Ma, B. He, Y. Sun, and T. Ruan, Tfrank: Think-free reasoning enables [Online]. Available: practical pointwise llm ranking, 2025. https://arxiv.org/abs/2508. [25] W. Liu, X. Ma, Y. Zhu, L. Su, S. Wang, D. Yin, and Z. Dou, Coranking: Collaborative ranking with small and large ranking agents, 2025. [Online]. Available: https: //arxiv.org/abs/2503.23427 [26] Z. Qin, R. Jagerman, K. Hui, H. Zhuang, J. Wu, L. Yan, J. Shen, T. Liu, J. Liu, D. Metzler, X. Wang, and M. Bendersky, Large language models are effective text rankers with pairwise ranking prompting, in Findings of the Association for Computational Linguistics: NAACL 2024, K. Duh, H. Gomez, and S. Bethard, Eds. Mexico City, Mexico: Association for Computational Linguistics, Jun. 2024, pp. 15041518. [Online]. Available: https://aclanthology.org/2024.findings-naacl.97/ [27] X. Qin, J. Bai, J. Li, Z. Jia, and Z. Zheng, Tongsearch-qr: [Online]. retrieval, 2025. Reinforced query reasoning for Available: https://arxiv.org/abs/2506. [28] W. Liu, X. Ma, W. Sun, Y. Zhu, Y. Li, D. Yin, and Z. Dou, Reasonrank: Empowering passage ranking with strong reasoning ability, 2025. [Online]. Available: https: //arxiv.org/abs/2508.07050 [29] X. Ma, X. Zhang, R. Pradeep, and J. Lin, Zero-shot listwise reranking with large language model, 2023. document [Online]. Available: https://arxiv.org/abs/2305.02156 [30] M. Long, D. Sun, D. Yang, J. Wang, Y. Shen, J. Wang, P. Wei, J. Gu, and J. Wang, Diver: multi-stage approach for reasoning-intensive information retrieval, 2025. [Online]. Available: https://arxiv.org/abs/2508.07995 [31] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. K. Li, Y. Wu, and D. Guo, Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [Online]. Available: https://arxiv.org/abs/2402.03300 [32] H. Su, H. Yen, M. Xia, W. Shi, N. Muennighoff, H. yu Wang, H. Liu, Q. Shi, Z. S. Siegel, M. Tang, R. Sun, J. Yoon, S. O. Arik, D. Chen, and T. Yu, Bright: realistic and challenging benchmark for reasoning-intensive retrieval, 2025. [Online]. Available: https://arxiv.org/abs/2407.12883 [33] L. Li, X. Zhou, and Z. Liu, R2med: benchmark for reasoning-driven medical retrieval, 2025. [Online]. Available: https://arxiv.org/abs/2505. [34] N. Thakur, N. Reimers, A. Ruckle, A. Srivastava, and I. Gurevych, Beir: heterogenous benchmark for zero-shot evaluation of information retrieval models, 2021. [Online]. Available: https://arxiv.org/abs/2104.08663 [35] E. Yang, A. Yates, K. Ricci, O. Weller, V. Chari, B. V. Durme, and D. Lawrie, Rank-k: Test-time reasoning for listwise reranking, 2025. [Online]. Available: https://arxiv.org/abs/2505.14432 [36] M. Community, Ms-swift: comprehensive framework for training and deploying large language and multimodal models, https://github.com/modelscope/ 2025. ms-swift [Online]. Available: [37] G. Sheng, C. Zhang, Z. Ye et al., Hybridflow: flexible and efficient rlhf framework, arXiv preprint, 2024. [Online]. Available: https://arxiv.org/pdf/2409.19256 [38] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, 2021. [Online]. Available: https: //arxiv.org/abs/2106.09685 [39] D. Sun, D. Yang, Y. Shen, Y. Hanrag: Heuristic accurate noise-resistant retrieval-augmented generation for multi-hop question answering, 2025. [Online]. Available: https://arxiv.org/abs/2509.09713 Jiao et al., X. APENDIX Prompt 1: Prompt of Listwise Labeling use Gemini2.5-Pro You are an expert passage reranker. Your task is to rank the provided passages based on how well they address the users query, considering both **relevance and usefulness**. Follow these steps: 1. **Understand the Query:** Identify the core question or intent behind the users query. 2. **Evaluate Passages:** Think step-by-step to assess each passage. passage is **valuable** if it directly and effectively helps answer the query. It is **not valuable** if it merely discusses similar topics without providing direct answer. 3. **Rank & Output:** * First, briefly explain your reasoning process for the ranking. * Then, output single JSON array containing the integer IDs of **all** provided passages. The array must be sorted from the most valuable passage to the least valuable. The final output should look like this: <Your reasoning here> json The users query is: {your query} Here are the passages to evaluate: {your passages list} ...integeridshere... Prompt 2: Prompt of Pointwise Labeling use Qwen3-235B-instruct Your task is to rate how relevant and useful the document is for the query. document is **relevant and useful** if its content directly helps answer or address the query. document is **not relevant or useful** if it does not provide content that helps answer the query, even if it mentions similar topics. The answer should be Relevance score: X. where is number from 0-10. 0 means completely irrelevant, and 10 means highly relevant and provides complete, useful answer. Here is the query: {your query} Here is the document: {your passage} Note that your answer must ONLY be in the format Relevance score: X., where is number from 0-10. Dont output anything else. Prompt 3: Prompt of Groupwise Reranking Your task is to evaluate and rank documents based on how well they help answer the given query. Follow this evaluation priority: 1. PRIMARY: Usefulness & Helpfulness - Does the document provide actionable information, solutions, or direct answers that help address the users needs? 2. SECONDARY: Relevance - Does the document contain information related to the query topic? Evaluation Process: 1. First, identify the users core intent and what kind of help they need from the query 2. For each document, assess: - How directly it addresses the users intent - What actionable information or answers it provides - How much it helps solve the users problem or need 3. Compare documents against each other to ensure proper ranking 4. Assign scores that reflect the relative usefulness ranking Scoring Scale (0-10): - 9-10: Extremely helpful, directly answers the query with actionable information - 7-8: Very helpful, provides substantial useful information for the query - 5-6: Moderately helpful, contains some useful information but incomplete - 3-4: Minimally helpful, limited useful information despite topic relevance - 1-2: Barely helpful, mentions related topics but provides little useful information - 0: Not helpful at all, cannot assist with answering the query will provide you TOPK documents, each indicated by numerical identifier []. Score these documents based on their Usefulness and Relevance to the query. Query: QUERY Documents: PASSAGES ## Final Output Format You must structure your response in exactly two parts: provide your brief reasoning process first, then output final scores in JSON format like below, with document IDs as string keys and integer scores as values for all TOPK documents. The reasoning process and answer are enclosed within < reason > /reason and answer /answer tags, respectively. Do NOT output anything outside the specified tags. Follow this exact format: < reason > Analyzeeachdocumentsusef ulnessandrelevancetothequery, explainingyourscoringrationale < /reason > < answer > json {[1]: 5, [2]: 3, [3]: 8, ...} < /answer > TABLE VII CASE ON SUSTAINABLE LIVING. Query will provide you 20 documents, each indicated by numerical identifier []. Score these documents based on their Usefulness and Relevance to the query. Query: How to recognize products with neonicotinoid pesticides in them? Recently, the EU has temporarily banned neonicotinoid pesticides because there are strong indications that these pesticides are responsible for the decline in bee populations. Ive heard that this pesticide is not only used in bug sprays, but also in seemingly innocent products like flower bulbs, plant plugs and certain types of compost. Is this true? If so, how do make sure dont already have this stuff at home? How can recognise products or brands that contain/use these pesticides? Groupwise Rerranker Output < reason > The user is asking how to recognize products containing neonicotinoid pesticides, specifically in items like flower bulbs, plant plugs, and compost. They also want to know if this is true and how to avoid them at home. will prioritize documents that confirm the presence of neonics in these products, explain their persistence, and offer actionable advice on how to identify or avoid them. * **[1]**: This document discusses PFAS in compostable food packaging and biosolids. It is completely irrelevant to neonicotinoid pesticides. * **[2]**: This is list of product categories, including Bulbs and Soil and Fertilizers. While it lists relevant product types, it provides no information about neonicotinoids or how to identify them. Its barely helpful as list of product types. * **[3]**: This document discusses composting dairy products. It is irrelevant to neonicotinoids. * **[4]**: This document mentions concerns about pesticides in straw and the desire to find organic sources. While it doesnt name neonics, it highlights the general problem of pesticides in garden products and the need for organic alternatives, which is relevant strategy for avoiding neonics. * **[5]**: This document discusses horses in cities and electric carts. It is completely irrelevant. * **[6]**: This document is highly relevant. It explains that neonics remain active in soil for years and contaminate water, directly addressing the users concern about their persistence and environmental impact. This helps explain *why* one needs to be careful with products that might contain them. * **[7]**: This document lists various carbamate pesticides. It is irrelevant to neonicotinoids. * **[8]**: This document discusses moisture content in compost piles. It is irrelevant to neonicotinoids. * **[9]**: This document discusses composting in place and mentions avoiding cooked food, meat, grains, or dairy. It is irrelevant to neonicotinoids. * **[10]**: This document discusses pottery glaze chemistry. It is completely irrelevant. * **[11]**: This document discusses the widespread exposure to neonicotinoids and links them to various health effects. While it doesnt directly help identify products, it reinforces the users concern about avoiding them, making it moderately helpful for context. * **[12]**: This document discusses synthetic pyrethroid barrier treatments for mosquitoes. It is irrelevant to neonicotinoids. * **[13]**: This document lists various certifications like FSC, GREENGUARD, and Oeko-Tex. While these are general certifications for products, they do not specifically address neonicotinoids or pesticides in garden products. Its minimally helpful as general concept of certification but not specific enough. * **[14]**: This document discusses plant-derived pyrethrins for mosquito control. It is irrelevant to neonicotinoids. * **[15]**: This document is extremely helpful. It provides actionable information by directing the user to the EPA for pesticide registration and the National Pesticide Information Center (NPIC) for information on human health and insecticides. This is direct way to find out what products contain neonics. * **[16]**: This document is very helpful. It confirms that neonicotinoids are used as seed treatments, which directly answers the users question about their presence in seemingly innocent products like seeds (and by extension, plants grown from them). This is key piece of information for recognition. * **[17]**: This document is personal anecdote with no relevance to the query. * **[18]**: This document discusses global warming. It is completely irrelevant. * **[19]**: This document discusses the properties of pure compost. It is irrelevant to neonicotinoids. * **[20]**: This document is table of contents for Wikipedia page on neonicotinoids. It lists sections like Agricultural usage, Regulation, Environmental and species impact, and Bees. This confirms the users premise about their use and impact, and suggests where to find more detailed information, making it very helpful for context and further research. < /reason > < answer > json {[1]: 0, [2]: 1, [3]: 0, [4]: 4, [5]: 0, [6]: 8, [7]: 0, [8]: 0, [9]: 0, [10]: 0, [11]: 5, [12]: 0, [13]: 2, [14]: 0, [15]: 9, [16]: 8, [17]: 0, [18]: 0, [19]: 0, [20]: 7} < /answer >"
        }
    ],
    "affiliations": [
        "Ant Group",
        "Sun Yat-sen University"
    ]
}