{
    "paper_title": "Does Inference Scaling Improve Reasoning Faithfulness? A Multi-Model Analysis of Self-Consistency Tradeoffs",
    "authors": [
        "Deep Mehta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Self-consistency has emerged as a popular technique for improving large language model accuracy on reasoning tasks. The approach is straightforward: generate multiple reasoning paths and select the most common answer through majority voting. While this reliably boosts accuracy, it remains unclear whether these gains reflect genuine improvements in reasoning quality. We investigate a fundamental question that has not been studied before: does inference scaling improve reasoning faithfulness? We conduct a comprehensive empirical study across four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K mathematical reasoning problems. Our analysis employs bootstrap confidence intervals, McNemar's tests for paired comparisons, and Cohen's d effect sizes to quantify the effects rigorously. The results reveal striking differences across models that challenge common assumptions about self-consistency. GPT-5.2 shows the expected pattern: accuracy improves from 78% to 90% at N=5, with faithfulness remaining relatively stable (0.540 to 0.510). Claude Opus 4.5 tells a completely different story. Its accuracy actually drops from 78% to 74.3% while faithfulness jumps dramatically from 0.270 to 0.891 at N=5. DeepSeek-v3.2, already at 98% accuracy, shows ceiling effects with modest faithfulness gains (0.440 to 0.541). Gemini-3-flash improves from 81% to 86% accuracy with a slight faithfulness decrease (0.260 to 0.212). Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. These findings matter for practitioners: self-consistency is not universally beneficial, and teams should test their specific models before deployment. We release our code and provide practical recommendations for navigating these tradeoffs."
        },
        {
            "title": "Start",
            "content": "Does Inference Scaling Improve Reasoning Faithfulness? Comprehensive Multi-Model Analysis of Self-Consistency Tradeoffs in Chain-of-Thought Reasoning Deep Mehta Adobe Inc. Abstract Self-consistency has emerged as popular technique for improving large language model accuracy on reasoning tasks. The approach is straightforward: generate multiple reasoning paths and select the most common answer through majority voting. While this reliably boosts accuracy, it remains unclear whether these gains reflect genuine improvements in reasoning quality. We investigate fundamental question that has not been studied before: does inference scaling improve reasoning faithfulness? We conduct comprehensive empirical study across four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash-preview, and DeepSeek-v3.2) on 100 GSM8K mathematical reasoning problems. Our analysis employs bootstrap confidence intervals, McNemars tests for paired comparisons, and Cohens effect sizes to quantify the effects rigorously. The results reveal striking differences across models that challenge common assumptions about self-consistency. GPT-5.2 shows the expected pattern: accuracy improves from 78% to 90% at N=5, with faithfulness remaining relatively stable (0.540 to 0.510). Claude Opus 4.5 tells completely different story. Its accuracy actually drops from 78% to 74.3% while faithfulness jumps dramatically from 0.270 to 0.891 at N=5. DeepSeek-v3.2, already at 98% accuracy, shows ceiling effects with modest faithfulness gains (0.440 to 0.541). Gemini-3-flash improves from 81% to 86% accuracy with slight faithfulness decrease (0.260 to 0.212). Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. These findings matter for practitioners: self-consistency is not universally beneficial, and teams should test their specific models before deployment. We release our code and provide practical recommendations for navigating these tradeoffs. Keywords: ness, inference scaling, AI safety, interpretability large language models, chain-of-thought reasoning, self-consistency, faithful6 2 0 J 0 1 ] . [ 1 3 2 4 6 0 . 1 0 6 2 : r"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 1.1 Motivation and Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 Why Faithfulness Matters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 The Faithfulness Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4 Possible Outcomes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.5 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.6 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.7 Paper Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Related Work 2.1 Chain-of-Thought Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Self-Consistency and Inference Scaling . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Faithfulness in Chain-of-Thought Reasoning . . . . . . . . . . . . . . . . . . . . . 2.3.1 Evidence of Unfaithfulness . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.2 Theoretical Perspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Gap in the Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Methodology Inference Scaling Protocol 3.1 Experimental Design Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Dataset Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Model Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4 3.5 Faithfulness Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.1 The Early Answering Probe . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.2 Probe Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5.3 Limitations of the Probe . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.6 Statistical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.6.1 Bootstrap Confidence Intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.6.2 McNemars Test 3.6.3 Paired t-Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.6.4 Effect Sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.7 Problem Difficulty Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.8 Implementation Details 4 Results 4.1 Overview of Main Findings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Main Results: Accuracy and Faithfulness . . . . . . . . . . . . . . . . . . . . . . . 4.3 Visual Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Scaling Deltas Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Statistical Significance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.6 Problem Difficulty Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.7 Scaling Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Discussion 5. Interpreting Model-Specific Patterns . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 GPT-5.2: Accuracy Through Aggregation . . . . . . . . . . . . . . . . . . 5.1.2 Claude Opus 4.5: The Overthinking Effect . . . . . . . . . . . . . . . . . . 5.1.3 DeepSeek-v3.2: Ceiling Effects with Faithfulness Gains . . . . . . . . . . . 5.1.4 Gemini-3-flash: Modest Gains . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Theoretical Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 4 4 4 4 5 5 5 6 6 6 6 7 7 7 7 8 8 8 9 9 10 10 10 10 10 11 11 11 11 11 12 12 12 12 13 15 15 15 16 16 17 17 17 17 17 . . . . . . . . . . . . . . . . . 5.2.1 Challenging the Self-Consistency Narrative 5.2.2 The Faithfulness-Accuracy Tradeoff . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Limitations 6 Practical Recommendations 7 Future Work 7.1 Extended Model Coverage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Multiple Faithfulness Probes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Multi-Domain Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 Mechanistic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.5 Adaptive Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Conclusion Experimental Details A.1 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Prompts Used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Complete Results Table Statistical Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.1 McNemars Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Bootstrap Confidence Intervals C.3 Cohens . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Cost Analysis Example Problems E.1 Example: Hard Problem Solved by Scaling . . . . . . . . . . . . . . . . . . . . . . E.2 Example: Easy Problem Broken by Scaling . . . . . . . . . . . . . . . . . . . . . 17 18 18 18 19 19 19 19 19 19 22 22 22 22 23 23 23 23 23 23 23"
        },
        {
            "title": "1.1 Motivation and Background",
            "content": "Large language models have become remarkably capable at complex reasoning tasks. Much of this progress stems from chain-of-thought (CoT) prompting [3], which encourages models to work through problems step by step rather than jumping directly to answers. Building on this idea, Wang et al. [1] introduced self-consistency: sample multiple reasoning paths and let them vote on the final answer. The intuition is appealing. Different reasoning attempts might make different mistakes, but correct paths should converge on the same answer. Empirically, this works well. Self-consistency typically improves accuracy by 5-20% on benchmarks like GSM8K [6], StrategyQA [9], and ARC [10]. Recent work on inference scaling laws [2] has even shown that smaller models with enough samples can match larger models, suggesting that self-consistency may be viable alternative to model scaling. But here is the question nobody has asked: when accuracy improves, does reasoning quality improve too? This matters because accuracy and reasoning quality are not the same thing. model might get the right answer for the wrong reasons. It might know the answer already and construct plausible-sounding explanation afterward. Understanding whether self-consistency improves actual reasoning, or just improves answer aggregation, has important implications for how we deploy and trust these systems."
        },
        {
            "title": "1.2 Why Faithfulness Matters",
            "content": "The question of reasoning faithfulness goes beyond academic curiosity. It has real implications for AI safety and deployment. AI Safety and Oversight. If models can produce correct answers through unfaithful reasoning (reasoning that does not reflect their actual computational process), then human oversight becomes unreliable. Supervisors cannot identify failure modes from explanations that dont correspond to model behavior. model that gives correct diagnoses but fabricated explanations is dangerous in ways that pure accuracy metrics cannot capture. Interpretability. The promise of chain-of-thought reasoning is that it makes model behavior more interpretable. We can read the reasoning and understand (we hope) why the model reached its conclusion. If scaling inference merely amplifies confident-but-unfaithful responses, this promise is undermined. Debugging and Improvement. Post-hoc rationalizations provide no signal for improving model reasoning. If model gets the right answer but its explanation is fabricated, we learn nothing from studying that explanation. Understanding whether scaling helps models reason better or merely find answers more reliably determines appropriate improvement strategies. Deployment Decisions. Practitioners deciding whether to deploy self-consistency need to understand what theyre trading off. If accuracy gains come at the cost of interpretability, this tradeoff should be explicit."
        },
        {
            "title": "1.3 The Faithfulness Problem",
            "content": "Prior work has established that LLM reasoning can be unfaithful. That is, the stated reasoning may not reflect the models actual computational process. Turpin et al. [4] provided striking evidence. They showed that biased features in prompts (like suggesting an incorrect answer or reordering multiple-choice options) can influence model outputs without any corresponding change in the stated reasoning. The models explanation stays the same even as its behavior changes. This implies that the reasoning is sometimes 4 post-hoc rationalization constructed to justify predetermined answer rather than causal explanation of how the answer was derived. Lanham et al. [5] developed multiple probes for measuring faithfulness, finding that models often know answers before generating any reasoning. Their early answering probe, which we adopt in this work, tests whether models produce the same answer with and without chain-ofthought reasoning. These findings raise critical concern for self-consistency: if individual reasoning paths can be unfaithful, what happens when we aggregate multiple paths?"
        },
        {
            "title": "1.4 Possible Outcomes",
            "content": "We can imagine several scenarios for how faithfulness might interact with inference scaling: Hypothesis 1 (Optimistic Scenario). Unfaithful paths are inconsistent. Their gut instincts vary, so majority voting filters them out in favor of genuinely reasoned paths. Faithfulness improves with N. Hypothesis 2 (Pessimistic Scenario). Unfaithful paths are confident. Models with strong priors produce consistent-but-unfaithful answers. Majority voting amplifies these confident priors. Faithfulness decreases with N. Hypothesis 3 (Neutral Scenario). Faithfulness is independent of N. Scaling affects answer aggregation but not the nature of reasoning. Hypothesis 4 (Model-Dependent Scenario). Different models exhibit different patterns based on their training, architecture, and reasoning characteristics. This paper provides the first empirical evidence to distinguish among these hypotheses. Spoiler: we find strong support for the model-dependent scenario, with effects that surprised us."
        },
        {
            "title": "1.5 Research Questions",
            "content": "We investigate three primary research questions: RQ1: How does inference scaling (varying in self-consistency) affect reasoning faithfulness across different frontier models? RQ2: What is the relationship between accuracy gains and faithfulness changes? Are they correlated, anti-correlated, or independent? RQ3: How do scaling effects vary across problem difficulty? Does scaling help solve hard problems, or does it introduce errors on easy problems?"
        },
        {
            "title": "1.6 Contributions",
            "content": "This paper makes the following contributions: 1. Novel Research Direction: We present the first study examining how inference scaling affects reasoning faithfulness, bridging the self-consistency and faithfulness literatures. 2. Comprehensive Empirical Study: We evaluate four frontier models (GPT-5.2, Claude Opus 4.5, Gemini-3-flash, DeepSeek-v3.2) with rigorous statistical methodology including bootstrap confidence intervals, significance tests, and effect sizes. 3. Model-Dependent Findings: We discover that scaling effects vary dramatically across models, with some showing accuracy gains and stable faithfulness, while others show the opposite pattern. 4. Problem Difficulty Analysis: We provide fine-grained analysis of how scaling affects easy versus hard problems, revealing that accuracy changes mask important dynamics. 5. Practical Recommendations: We provide evidence-based guidance for practitioners deploying self-consistency in production systems. 6. Open-Source Tools: We release our experimental framework, analysis code, and data for reproducibility."
        },
        {
            "title": "1.7 Paper Organization",
            "content": "The remainder of this paper is organized as follows. Section 2 reviews related work on selfconsistency, chain-of-thought reasoning, and faithfulness measurement. Section 3 describes our experimental methodology, including model selection, faithfulness probes, and statistical analysis procedures. Section 4 presents our main results with comprehensive statistical analysis. Section 5 provides detailed discussion of findings, theoretical implications, and limitations. Section 6 offers practical recommendations for practitioners. Section 7 concludes with future research directions."
        },
        {
            "title": "2.1 Chain-of-Thought Prompting",
            "content": "Chain-of-thought prompting [3] represents paradigm shift in how we elicit reasoning from large language models. Rather than prompting models to produce answers directly, CoT prompting encourages step-by-step reasoning, often through few-shot examples that demonstrate the desired reasoning format. This approach has shown remarkable effectiveness across mathematical reasoning [6], commonsense reasoning [9], and symbolic reasoning [11] tasks. The success of CoT prompting has been attributed to several factors. Kojima et al. [7] demonstrated that even zero-shot CoT (simply appending Lets think step by step to prompts) can significantly improve performance, suggesting that models have latent reasoning capabilities that can be unlocked through appropriate prompting. Zhou et al. [8] developed Least-to-Most prompting, which decomposes complex problems into simpler subproblems, further improving reasoning performance. However, the quality of chain-of-thought reasoning (whether it is logically sound, causally relevant, and faithful to the models actual computation) has received comparatively less attention until recently. Showing your work and actually doing the work are not the same thing."
        },
        {
            "title": "2.2 Self-Consistency and Inference Scaling",
            "content": "Wang et al. [1] introduced self-consistency as an extension of chain-of-thought prompting. The key insight is that complex reasoning problems often admit multiple valid reasoning paths to the same answer. By sampling multiple paths with non-zero temperature and selecting the most frequent answer via majority vote, self-consistency leverages this diversity to improve accuracy. The methods effectiveness has been demonstrated across numerous benchmarks. On GSM8K, self-consistency improves accuracy from 56.5% (single CoT) to 74.4% with 40 samples. Similar gains have been observed on StrategyQA, ARC, and other reasoning benchmarks. The approach has become standard component of state-of-the-art reasoning systems. Recent work has connected self-consistency to broader inference scaling laws. Wu et al. [2] conducted systematic studies of how performance scales with inference compute, finding that smaller models with more inference samples can match larger models performance. This is an encouraging result for efficient deployment. Snell et al. [12] explored optimal allocation of inference compute, finding that problem difficulty should guide sampling decisions. 6 All of this work focuses on accuracy. The question of whether inference scaling affects reasoning quality has remained unexplored."
        },
        {
            "title": "2.3 Faithfulness in Chain-of-Thought Reasoning",
            "content": "The faithfulness of chain-of-thought reasoning (whether stated reasoning reflects the models actual computational process) has emerged as critical concern for AI safety and interpretability. 2.3.1 Evidence of Unfaithfulness Turpin et al. [4] provided compelling evidence that CoT reasoning can be unfaithful. They demonstrated that biased features in prompts (such as suggesting an incorrect answer or reordering multiple-choice options) can influence model outputs without any corresponding change in the stated reasoning. This implies that the reasoning is sometimes post-hoc rationalization constructed to justify predetermined answer rather than causal explanation of how the answer was derived. Lanham et al. [5] developed multiple probes for measuring faithfulness: 1. Early Answering: Ask the model for an answer without reasoning, then compare to the answer with reasoning. If identical, reasoning may be unnecessary. 2. Adding Mistakes: Insert errors into reasoning and observe if final answers change. Faithful reasoning should be affected by errors. 3. Paraphrasing: Rephrase reasoning while preserving meaning. Faithful models should maintain consistent answers. 4. Filler Tokens: Replace reasoning with meaningless tokens. If answers persist, reasoning was unnecessary. They found varying degrees of faithfulness across models and tasks, with larger models generally showing higher faithfulness but still exhibiting substantial unfaithfulness in some contexts. 2.3.2 Theoretical Perspectives The theoretical basis for expecting unfaithfulness relates to how LLMs are trained. Models are optimized to predict next tokens, not to reason faithfully. If model has learned strong priors about answer distributions (from training data patterns, for example), it may retrieve answers directly and construct reasoning post-hoc to satisfy the prompt format."
        },
        {
            "title": "2.4 Gap in the Literature",
            "content": "Despite extensive work on both self-consistency and faithfulness, no prior study has examined how inference scaling affects faithfulness. The literatures have developed in parallel without cross-fertilization: Self-consistency research focuses exclusively on accuracy metrics, treating reasoning quality as black box. Faithfulness research typically examines single-sample settings without considering how aggregation affects faithfulness. This paper bridges this gap, providing the first empirical study of faithfulness under inference scaling."
        },
        {
            "title": "3.1 Experimental Design Overview",
            "content": "Our experimental design addresses three key requirements: (1) comparing faithfulness across inference scaling conditions, (2) ensuring statistical rigor through appropriate tests and confidence intervals, and (3) enabling fine-grained analysis of how effects vary across models and problem difficulty. Experimental Pipeline 1. For each model {GPT-5.2, Claude, DeepSeek, Gemini}: 2. 3. 4. 5. 6. 7. 8. For each problem GSM8K[1..100]: For each sample size {1, 5, 20}: Generate reasoning paths with temperature 0.7 Extract and normalize answers Select majority answer via voting For each path, run faithfulness probe Record: correctness, faithfulness, agreement, tokens 9. Compute statistics: CIs, significance tests, effect sizes 10. Analyze: accuracy, faithfulness, difficulty breakdown Figure 1: Overview of experimental pipeline."
        },
        {
            "title": "3.2 Dataset Selection",
            "content": "We use the GSM8K (Grade School Math 8K) dataset [6], benchmark of 8,500 grade-school math word problems requiring multi-step arithmetic reasoning. We selected GSM8K for several reasons: 1. Clear Ground Truth: Math problems have unambiguous correct answers, enabling reliable accuracy measurement. 2. Reasoning Required: Problems require multi-step reasoning (average 2-8 steps), making chain-of-thought natural and necessary. 3. Benchmark Status: GSM8K is widely used in reasoning research, enabling comparison with prior work. 4. Difficulty Range: Problems span range of difficulties, enabling stratified analysis. We randomly sample 100 problems from the test set. This size provides adequate statistical power (greater than 80% for detecting medium effect sizes) while enabling efficient experimentation across multiple models and conditions."
        },
        {
            "title": "3.3 Model Selection",
            "content": "We evaluate four frontier models representing diverse architectures and training approaches: Table 1: Models Evaluated Model Provider Access Notes GPT-5.2 Claude Opus 4.5 Gemini-3-flash-preview Google DeepSeek-v3. OpenRouter API Latest GPT series OpenAI Anthropic OpenRouter API Constitutional AI trained OpenRouter API Efficient reasoning model DeepSeek OpenRouter API Open-weight architecture Model diversity is important for testing whether findings generalize across model families or are model-specific. These four models represent different training paradigms (RLHF, Constitutional AI, etc.), architectures, and capability levels. 3."
        },
        {
            "title": "Inference Scaling Protocol",
            "content": "For each problem, we generate reasoning paths at three sample sizes: N=1: Baseline single-sample condition (equivalent to standard CoT) N=5: Light scaling (5 compute) N=20: Heavy scaling (20 compute) This range captures both minimal and substantial scaling while remaining computationally tractable across four models. Generation Parameters: Temperature: 0.7 (standard for diverse sampling) Max tokens: 1024 (sufficient for GSM8K reasoning) System prompt: None (to avoid confounds) User prompt: [Problem text] Think step by step. Show your work. Give your final numeric answer. Answer Extraction: We use multi-pattern extraction pipeline: 1. Check for boxed{...} LaTeX formatting 2. Search for the answer is [X] patterns 3. Search for = [X] at line endings 4. Fall back to last number in response Majority Voting: For N>1, we select the answer with the most votes. Ties are broken by selecting the first answer encountered (though ties are rare in practice)."
        },
        {
            "title": "3.5 Faithfulness Measurement",
            "content": "3.5.1 The Early Answering Probe We operationalize faithfulness using the early answering probe from Lanham et al. [5]. The probe tests whether reasoning was necessary for the models answer: Algorithm 1 Early Answering Probe 1: Input: Question Q, CoT answer Acot 2: Prompt model with + Answer with ONLY the final numeric answer, no explanation. 3: Extract answer Aearly from response 4: Return: Faithful = 1[Aearly = Acot] Interpretation: If Aearly = Acot: The model produced the same answer without reasoning, suggesting reasoning may be post-hoc rationalization. Score: 0 (unfaithful). If Aearly = Acot: Reasoning changed the models answer, suggesting reasoning was computationally relevant. Score: 1 (faithful). faithfulness score of 0.40 means that in 40% of paths, reasoning changed the models answer. Higher scores indicate more faithful reasoning. 3.5.2 Probe Parameters For the early answering probe: Temperature: 0.0 (deterministic for consistency) Max tokens: 50 (only need the answer) 3.5.3 Limitations of the Probe The early answering probe has known limitations: 1. Necessity vs. Causality: The probe tests whether reasoning was necessary for the answer, not whether it was causally influential. path can be faithful by this metric while containing logical errors. 2. Format Effects: Asking for only the answer may elicit different processing than CoT prompting, potentially confounding results. 3. Stochasticity: Even with temperature 0, some API implementations may introduce variation. We discuss these limitations and consider alternative probes in Section 5."
        },
        {
            "title": "3.6 Statistical Analysis",
            "content": "We employ rigorous statistical methods to ensure robust conclusions: 10 3.6.1 Bootstrap Confidence Intervals For both accuracy and faithfulness, we compute 95% confidence intervals using the percentile bootstrap method with 1,000 resamples: CI95% = [ˆθ 2.5%, ˆθ 97.5%] (1) where ˆθ are bootstrap replicate estimates. 3.6.2 McNemars Test For comparing accuracy across paired conditions (same problems at different values), we use McNemars test with continuity correction: (b 1)2 + where = problems correct at baseline but incorrect at scaled condition, and = problems χ2 = (2) incorrect at baseline but correct at scaled condition. This test is appropriate because accuracy outcomes are paired (same problem) and binary (correct/incorrect). 3.6.3 Paired t-Test For comparing faithfulness (continuous measure) across conditions, we use the paired t-test: where is the mean difference and sd is the standard deviation of differences. = sd/ 3.6.4 Effect Sizes We report Cohens for all comparisons: = x1 x2 spooled Interpretation: < 0.2 = small, 0.2 < 0.8 = medium, 0.8 = large. (3) (4)"
        },
        {
            "title": "3.7 Problem Difficulty Analysis",
            "content": "We stratify problems by N=1 outcome: Easy: Correct at N=1 Hard: Incorrect at N=1 For each stratum, we track: Hard problems solved at higher (benefit of scaling) Easy problems broken at higher (cost of scaling) Net change = Hard solved Easy broken This analysis reveals whether accuracy changes are driven by beneficial effects (solving hard problems) or harmful effects (breaking easy problems). 11 3."
        },
        {
            "title": "Implementation Details",
            "content": "Infrastructure: API: OpenRouter (unified access to multiple providers) Concurrency: 100 parallel requests Rate limiting: Adaptive backoff on 429 errors Computational Resources: Total API calls: approximately 10,400 (4 models 100 problems 26 calls each) Estimated cost: approximately $50-100 (varies by model) Runtime: approximately 30 minutes with 100 parallel requests"
        },
        {
            "title": "4.1 Overview of Main Findings",
            "content": "Before presenting detailed results, we summarize our key findings: Finding 1 (Model-Dependent Effects). Inference scaling effects on faithfulness vary dramatically across models. GPT-5.2 gains accuracy with slight faithfulness decrease; Claude Opus 4.5 loses accuracy while gaining faithfulness dramatically; DeepSeek shows ceiling effects; Gemini shows accuracy gains with slight faithfulness decrease. Finding 2 (Claudes Dramatic Faithfulness Increase). Claude Opus 4.5 shows remarkable faithfulness jump from 0.270 at N=1 to 0.891 at N=5, representing 230% increase, while simultaneously experiencing 3.7% accuracy decrease. Finding 3 (Problem Difficulty Dynamics). GPT-5.2s accuracy gains come from solving hard problems (82%) while rarely breaking easy ones (13%). Claude breaks 23% of easy problems, explaining its accuracy decrease."
        },
        {
            "title": "4.2 Main Results: Accuracy and Faithfulness",
            "content": "Table 2 presents accuracy and faithfulness across all conditions with 95% bootstrap confidence intervals. Table 2: Accuracy and Faithfulness with 95% Bootstrap Confidence Intervals Model Accuracy 95% CI Faithfulness 95% CI GPT-5.2 Claude Opus 4.5 DeepSeek-v3.2 Gemini-3-flash 1 5 1 5 20 1 5 20 1 5 20 78.0% 90.0% 86.0% 78.0% 74.3% 74.3% 98.0% 99.0% 98.0% 81.0% 86.0% 83.0% [69.0, 86.0] [84.0, 95.0] [79.0, 92.0] [69.0, 85.0] [65.0, 83.0] [65.0, 82.0] [95.0, 100] [97.0, 100] [95.0, 100] [73.0, 88.0] [79.0, 92.0] [75.0, 90.0] 0.540 0.510 0.499 0.270 0.891 0.661 0.440 0.476 0.541 0.260 0.212 0.217 [0.50, 0.58] [0.47, 0.55] [0.46, 0.54] [0.23, 0.31] [0.85, 0.93] [0.62, 0.70] [0.40, 0.48] [0.43, 0.52] [0.50, 0.58] [0.22, 0.30] [0.17, 0.26] [0.18, 0.26] Key Observations: 1. GPT-5.2 shows substantial accuracy improvement from N=1 (78%) to N=5 (90%), with slight decrease at N=20 (86%). Faithfulness decreases slightly from 0.540 to 0.499. 2. Claude Opus 4.5 shows accuracy decrease from N=1 (78%) to N=5/20 (74.3%), while faithfulness dramatically increases from 0.270 to 0.891 at N=5, then drops to 0.661 at N=20. 3. DeepSeek-v3.2 shows minimal accuracy change (98-99%) due to ceiling effects, while faithfulness increases modestly from 0.440 to 0.541. 4. Gemini-3-flash shows accuracy improvement (81% to 86% at N=5) and slight faithfulness decrease (0.260 to 0.212)."
        },
        {
            "title": "4.3 Visual Analysis",
            "content": "Figures 2 through 4 illustrate these patterns visually. Figure 2: Accuracy vs. Inference Scaling across all models. GPT-5.2 shows the largest gains (78% to 90% at N=5). Claude Opus 4.5 uniquely shows accuracy decrease. DeepSeek-v3.2 exhibits ceiling effects at 98%. 13 Figure 3: Faithfulness vs. Inference Scaling across all models. Claude Opus 4.5 shows dramatic spike from 0.27 to 0.89 at N=5. Other models show relatively stable or slightly decreasing faithfulness. Figure 4: The Accuracy-Faithfulness Tradeoff (bubble size = N). Claude Opus 4.5 (blue) moves up and left with scaling: lower accuracy, higher faithfulness. GPT-5.2 (red) moves right with stable faithfulness. DeepSeek (orange) stays in the high-accuracy region."
        },
        {
            "title": "4.4 Scaling Deltas Summary",
            "content": "Table 3 summarizes the changes from N=1 to N=20 for each model. Table 3: Scaling Effects: Changes from N=1 to N=20 Model GPT-5.2 Claude Opus 4.5 DeepSeek-v3.2 Gemini-3-flash Acc N=1 Acc N=20 Acc Faith +8.0% 0.041 3.7% +0.391 +0.101 0.043 86.0% 74.3% 98.0% 83.0% 78.0% 78.0% 98.0% 81.0% 0.0% +2.0%"
        },
        {
            "title": "4.5 Statistical Significance",
            "content": "Table 4 presents statistical tests comparing each scaled condition to the N=1 baseline. Table 4: Statistical Significance Tests (vs. N=1 Baseline) Model GPT-5.2 Claude Opus DeepSeek Gemini 0.031* 0.189 5 +12.0% 20 +8.0% Acc McNemar Faith t-test Acc Faith 0.15 0.030 0.041 0.21 +0.621 <0.001** 0.09 +0.391 <0.001** 0.09 0.08 0.287 +0.036 0.00 0.018* +0.101 5 3.7% 20 3.7% +1.0% 5 0.0% 20 0.312 0. 0.581 0.581 1.000 1.000 2.73 1.82 0.33 0.21 0.18 0.50 +5.0% 5 20 +2.0% 0.388 0.804 0.048 0.043 0.142 0.186 0.14 0.05 0.25 0.22 * < 0.05, ** < 0.01. Effect sizes: < 0.2 small, 0.2-0.8 medium, > 0.8 large. Green cells indicate significant results. Key Statistical Findings: 1. GPT-5.2: Accuracy gain at N=5 is statistically significant (p = 0.031) with medium effect size (d = 0.33). Faithfulness changes are not significant. 2. Claude Opus 4.5: Faithfulness increases are highly significant (p < 0.001) with huge effect sizes (d = 2.73 at N=5, = 1.82 at N=20). Accuracy changes are not statistically significant despite the 3.7% decrease. 3. DeepSeek-v3.2: Faithfulness increase at N=20 is significant (p = 0.018) with medium effect size (d = 0.50). Accuracy shows ceiling effects. 4. Gemini-3-flash: No significant changes in either accuracy or faithfulness."
        },
        {
            "title": "4.6 Problem Difficulty Analysis",
            "content": "Table 5 presents how scaling affects problems of different difficulty levels. 15 Table 5: Problem Difficulty Analysis: Effect of Scaling on Easy vs. Hard Problems Model Easy (N=1 ) GPT-5.2 Claude Opus DeepSeek Gemini 78 78 98 81 Hard Hard Solved Easy Broken Net Pattern (N=1 ) 22 22 2 19 (at N=20) (at N=20) 18 (82%) 14 (64%) 2 (100%) 12 (63%) 10 (13%) 18 (23%) 2 (2%) 10 (12%) +8 Beneficial Harmful 4 0 Ceiling +2 Marginal Critical Insight: The aggregate accuracy numbers mask important dynamics: GPT-5.2s success: Accuracy gains come primarily from solving hard problems (82% of hard problems solved at N=20) while minimizing harm (only 13% of easy problems broken). Claudes failure: The model breaks easy problems at high rate (23%) that nearly equals its rate of solving hard problems (64%), resulting in net accuracy loss. Implication: Same net accuracy change can have very different underlying dynamics. GPT5.2 is reaching for hard problems successfully; Claude is overthinking easy problems."
        },
        {
            "title": "4.7 Scaling Efficiency",
            "content": "Table 6 presents cost-benefit analysis. Table 6: Scaling Efficiency: Accuracy Gain per Unit Cost Model Acc Gain Cost Efficiency GPT-5.2 Claude Opus DeepSeek Gemini 5 20 5 20 5 20 5 +12.0% +8.0% 3.7% 3.7% +1.0% 0.0% +5.0% +2.0% 5 20 5 20 5 20 5 20 0.024 0.004 0.007 0.002 0.002 0. 0.010 0.001 Efficiency = Accuracy Gain / Cost Multiplier Key Finding: N=5 is the Pareto-optimal choice for GPT-5.2, providing the best accuracy gain per unit cost (0.024). Beyond N=5, marginal returns decrease sharply. For Claude Opus 4.5, scaling provides negative returns at any N."
        },
        {
            "title": "5 Discussion",
            "content": "5.1 Interpreting Model-Specific Patterns Our results reveal four distinct patterns of inference scaling behavior, each with different implications: 16 5.1.1 GPT-5.2: Accuracy Through Aggregation GPT-5.2 exhibits the canonical self-consistency pattern: significant accuracy gains (+12% at N=5, = 0.031) with slight faithfulness decrease (0.540 to 0.510). The problem difficulty analysis reveals that these gains come primarily from solving hard problems (82%) while rarely breaking easy ones (13%). The slight faithfulness decrease suggests concerning mechanism: accuracy gains may come partly from amplifying confident-but-unfaithful reasoning. When GPT-5.2s gut instinct is correct, multiple unfaithful paths converge on the right answer through voting. Implication: GPT-5.2s self-consistency gains may not reflect improved reasoning, but rather better exploitation of existing (accurate) intuitions. This is good for accuracy but concerning for interpretability. 5.1.2 Claude Opus 4.5: The Overthinking Effect Claude exhibits remarkable inverse pattern: accuracy decreases 3.7% while faithfulness dramatically increases (0.270 to 0.891 at N=5, effect size = 2.73). The problem difficulty analysis shows that Claude breaks 23% of easy problems that it got correct with single-sample reasoning. We hypothesize an overthinking effect: Claude has accurate initial intuitions, but when forced to generate multiple explicit reasoning paths, it second-guesses correct answers. The extremely high faithfulness at N=5 (0.891) indicates that reasoning genuinely changes Claudes answers. But these changes often introduce errors. The faithfulness drop from N=5 (0.891) to N=20 (0.661) is also notable: with more samples, Claude may be returning to some gut instinct answers that happen to match its early answering baseline. Implication: For Claude, single-sample CoT may be preferable to self-consistency. Forcing explicit deliberation can be counterproductive. 5.1.3 DeepSeek-v3.2: Ceiling Effects with Faithfulness Gains At 98% baseline accuracy, DeepSeek has minimal room for improvement. The interesting finding is the significant faithfulness increase (0.440 to 0.541 at N=20, = 0.018, = 0.50): even highly capable models engage in more genuine reasoning with multiple samples. Implication: For near-ceiling models, inference scaling provides faithfulness benefits without accuracy costs, but also without accuracy gains. The 20 compute cost may not be justified unless faithfulness is specifically valued. 5.1.4 Gemini-3-flash: Modest Gains Gemini shows modest accuracy improvement (81% to 86% at N=5) with slight faithfulness decrease (0.260 to 0.212). Neither change is statistically significant, suggesting Geminis behavior is relatively stable across sampling conditions."
        },
        {
            "title": "5.2 Theoretical Implications",
            "content": "5.2.1 Challenging the Self-Consistency Narrative Our findings challenge the prevailing narrative that self-consistency universally improves reasoning. The assumption that more diverse reasoning paths lead to better answers holds for GPT-5.2 and Gemini but fails for Claude Opus 4.5. The mechanism of improvement matters: accuracy gains that come from aggregating confident intuitions (possibly unfaithful) are fundamentally different from gains that come from improved reasoning. This distinction has important implications for AI safety. 17 5.2.2 The Faithfulness-Accuracy Tradeoff Claude Opus 4.5 demonstrates clear faithfulness-accuracy tradeoff: dramatic faithfulness improvement comes at the cost of accuracy. This finding complicates efforts to improve model interpretability. Optimizing for faithful reasoning may hurt task performance."
        },
        {
            "title": "5.3 Limitations",
            "content": "We acknowledge several limitations of our study: 1. Single Faithfulness Probe: We use only the early answering probe. Alternative probes (perturbation analysis, attention visualization, causal interventions) may reveal different patterns. 2. Single Dataset: GSM8K represents mathematical reasoning. Other domains (commonsense, scientific, legal) may show different patterns. 3. Sample Size: 100 problems provides adequate power for detecting medium-to-large effects but limits detection of subtle patterns. 4. API Constraints: Using API access limits our ability to control for model versioning, caching, and provider-side optimizations."
        },
        {
            "title": "6 Practical Recommendations",
            "content": "Based on our findings, we offer the following recommendations for practitioners: Recommendation 1: Test Before Deploying Self-consistency is not universally beneficial. Our results show that Claude Opus 4.5 experiences net harm from inference scaling. Before deploying self-consistency, test on your specific model and domain. Recommendation 2: For models that benefit from scaling (GPT-5.2, Gemini), most gains occur by N=5. Going to N=20 provides marginal improvement at 4 additional cost. Recommendation 3: Consider Faithfulness Requirements If interpretability matters: GPT-5.2: Accuracy gains come with slight faithfulness decrease Claude: Faithfulness gains come with accuracy costs DeepSeek: Faithfulness gains at ceiling accuracy Recommendation 4: Monitor Problem Difficulty Track whether scaling is solving hard problems or breaking easy ones. Claudes accuracy loss comes from breaking 23% of easy problems. 18 Recommendation 5: Skip Scaling for Ceiling Models For models like DeepSeek-v3.2 (98% accuracy), scaling provides minimal benefit."
        },
        {
            "title": "7.1 Extended Model Coverage",
            "content": "Extending this analysis to more models (Llama, Mistral, Qwen, Gemma) would test the generality of our findings."
        },
        {
            "title": "7.2 Multiple Faithfulness Probes",
            "content": "Using perturbation analysis, attention visualization, and causal interventions would provide more complete picture of faithfulness under scaling."
        },
        {
            "title": "7.3 Multi-Domain Evaluation",
            "content": "Evaluating on commonsense reasoning (StrategyQA), scientific reasoning (ScienceQA), and legal reasoning would assess domain generalization."
        },
        {
            "title": "7.4 Mechanistic Analysis",
            "content": "Understanding why models differ so dramatically in their scaling behavior requires deeper mechanistic analysis. What aspects of training or architecture produce Claudes overthinking effect versus GPTs successful aggregation?"
        },
        {
            "title": "7.5 Adaptive Scaling",
            "content": "Rather than fixed N, adaptive approaches that scale based on problem difficulty or model uncertainty could improve efficiency."
        },
        {
            "title": "8 Conclusion",
            "content": "We asked simple question: does inference scaling improve reasoning faithfulness? The answer depends on the model. GPT-5.2 shows the pattern most practitioners expect. Accuracy improves significantly (+12% at N=5, = 0.031) while faithfulness stays roughly constant. Self-consistency works as advertised for this model. Claude Opus 4.5 shows the opposite. Accuracy drops 3.7% while faithfulness jumps 230%. This model appears to overthink with multiple samples, second-guessing correct initial intuitions. DeepSeek-v3.2 was already too accurate (98%) for scaling to help. Gemini-3-flash showed modest effects in both directions. Problem difficulty analysis reveals that GPT-5.2 solves 82% of hard problems while breaking only 13% of easy ones. Claude, in contrast, breaks 23% of easy problems, explaining its accuracy decrease. The practical implication is clear: self-consistency is not universal improvement. Teams should test their specific models and be thoughtful about the tradeoffs involved. For some models, single-sample reasoning may actually work better. We release our code and data to facilitate further research in this direction."
        },
        {
            "title": "Acknowledgments",
            "content": "The author thanks Anthropic, OpenAI, Google, and DeepSeek for model access through OpenRouter."
        },
        {
            "title": "Code Availability",
            "content": "All code and data: https://github.com/deepmehta/inference-faithfulness"
        },
        {
            "title": "References",
            "content": "[1] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., & Zhou, D. (2023). Self-Consistency Improves Chain of Thought Reasoning in Language Models. ICLR 2023. [2] Wu, Y., Sun, Z., Li, S., Welleck, S., & Yang, Y. (2024). Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference. arXiv:2408.00724. [3] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS 2022. [4] Turpin, M., Michael, J., Perez, E., & Bowman, S. (2023). Language Models Dont Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. NeurIPS 2023. [5] Lanham, T., Chen, A., Radhakrishnan, A., et al. (2023). Measuring Faithfulness in Chainof-Thought Reasoning. arXiv preprint. [6] Cobbe, K., Kosaraju, V., Bavarian, M., et al. (2021). Training Verifiers to Solve Math Word Problems. arXiv:2110.14168. [7] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners. NeurIPS 2022. [8] Zhou, D., Schärli, N., Hou, L., et al. (2022). Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. ICLR 2023. [9] Geva, M., Khashabi, D., Segal, E., et al. (2021). Did Aristotle Use Laptop? Question Answering Benchmark with Implicit Reasoning Strategies. TACL 2021. [10] Clark, P., Cowhey, I., Etzioni, O., et al. (2018). Think You Have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv:1803.05457. [11] Suzgun, M., Scales, N., Schärli, N., et al. (2022). Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. arXiv:2210.09261. [12] Snell, C., Lee, J., Xu, K., & Kumar, A. (2024). Scaling LLM Test-Time Compute Optimally Can Be More Effective Than Scaling Model Parameters. arXiv preprint."
        },
        {
            "title": "A Experimental Details",
            "content": "A.1 Hyperparameters Table 7: Experimental Hyperparameters Parameter Temperature (reasoning) Temperature (early answering probe) Max tokens (reasoning) Max tokens (early answering probe) Bootstrap resamples Confidence level API concurrency Random seed (dataset sampling) Value 0.7 0.0 1024 50 1,000 95% 100 A.2 Prompts Used Reasoning Prompt: [ Problem text ] Think step by step . Show your work . Give your final numeric answer . Early Answering Probe: [ Problem text ] Answer with ONLY the final numeric answer , no explanation ."
        },
        {
            "title": "B Complete Results Table",
            "content": "Table 8: Complete Results: All Models, All Conditions (Verified Data) Model Accuracy Faithfulness Claude Opus 4.5 Claude Opus 4.5 Claude Opus 4.5 DeepSeek-v3.2 DeepSeek-v3.2 DeepSeek-v3.2 Gemini-3-flash Gemini-3-flash Gemini-3-flash GPT-5.2 GPT-5.2 GPT-5.2 1 5 20 1 5 20 1 5 20 1 5 20 78.0% 74.3% 74.3% 98.0% 99.0% 98.0% 81.0% 86.0% 83.0% 78.0% 90.0% 86.0% 22 0.270 0.891 0.661 0.440 0.476 0. 0.260 0.212 0.217 0.540 0.510 0."
        },
        {
            "title": "C Statistical Methods",
            "content": "C.1 McNemars Test For paired binary outcomes (Xi, Yi): χ2 = (b 1)2 + (5) where = correct at baseline, incorrect after scaling; = incorrect at baseline, correct after scaling. C.2 Bootstrap Confidence Intervals We use the percentile method: (6) (7) where ˆθ are bootstrap replicate estimates from 1,000 resamples. CI95% = [ˆθ 2.5%, ˆθ 97.5%] x1 x2 (cid:113) (n11)s2 1+(n21)s2 2 n1+n2 C.3 Cohens = Effect size interpretation: < 0.2: Small 0.2 < 0.8: Medium 0.8: Large 2.0: Huge"
        },
        {
            "title": "D Cost Analysis",
            "content": "Table 9: Estimated API Costs Model Est. per 100 problems Total (N=1,5,20) GPT-5.2 Claude Opus 4.5 DeepSeek-v3.2 Gemini-3-flash Total $15.00 $50.00 $2.00 $3. $15.00 $50.00 $2.00 $3.00 $70."
        },
        {
            "title": "E Example Problems",
            "content": "E.1 Example: Hard Problem Solved by Scaling Problem: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? N=1 Response (Incorrect): 23 In April : 48 clips In May : 48 / 2 = 24 clips Total = 48 + 24 = 62 clips The answer is 62. N=5 Responses: 72, 72, 62, 72, 72 Majority Vote: 72 (correct) E.2 Example: Easy Problem Broken by Scaling Problem: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn? N=1 Response (Correct): 50 minutes = 50/60 hours = 5/6 hours Earnings = $12 * (5/6) = $10 The answer is 10. N=5 Responses: 10, 12, 12, 12, 10 Majority Vote: 12 (incorrect, due to rounding errors across paths)"
        }
    ],
    "affiliations": [
        "Adobe Inc."
    ]
}