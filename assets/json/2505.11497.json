{
    "paper_title": "QVGen: Pushing the Limit of Quantized Video Generative Models",
    "authors": [
        "Yushi Huang",
        "Ruihao Gong",
        "Jing Liu",
        "Yifu Ding",
        "Chengtao Lv",
        "Haotong Qin",
        "Jun Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As a commonly adopted solution, quantization has proven notable success in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, a novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (e.g., 4-bit or below). We begin with a theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules ($\\Phi$) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of $\\Phi$, we propose a rank-decay strategy that progressively eliminates $\\Phi$. Specifically, we repeatedly employ singular value decomposition (SVD) and a proposed rank-based regularization $\\mathbf{\\gamma}$ to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across $4$ state-of-the-art (SOTA) video DMs, with parameter sizes ranging from $1.3$B $\\sim14$B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of $+25.28$ in Dynamic Degree and $+8.43$ in Scene Consistency on VBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 7 9 4 1 1 . 5 0 5 2 : r QVGen: Pushing the Limit of Quantized Video Generative Models Yushi Huang1, 2 Ruihao Gong2, 3 Jing Liu4 Yifu Ding3 Chengtao Lv2 Haotong Qin5 Jun Zhang1 1Hong Kong University of Science and Technology 2SenseTime Research 3Beihang University 4Monash University 5ETH Zürich {huangyushi1, gongruihao, lvchengtao}@sensetime.com liujing_95@outlook.com haotong.qin@pbl.ee.ethz.ch yifuding@buaa.edu.cn eejzhang@ust.hk"
        },
        {
            "title": "Abstract",
            "content": "Video diffusion models (DMs) have enabled high-quality video synthesis. Yet, their substantial computational and memory demands pose serious challenges to real-world deployment, even on high-end GPUs. As commonly adopted solution, quantization has achieved notable successes in reducing cost for image DMs, while its direct application to video DMs remains ineffective. In this paper, we present QVGen, novel quantization-aware training (QAT) framework tailored for high-performance and inference-efficient video DMs under extremely low-bit quantization (i.e., 4-bit or below). We begin with theoretical analysis demonstrating that reducing the gradient norm is essential to facilitate convergence for QAT. To this end, we introduce auxiliary modules (Φ) to mitigate large quantization errors, leading to significantly enhanced convergence. To eliminate the inference overhead of Φ, we propose rank-decay strategy that progressively eliminates Φ. Specifically, we repeatedly employ singular value decomposition (SVD) and proposed rank-based regularization γ to identify and decay low-contributing components. This strategy retains performance while zeroing out inference overhead. Extensive experiments across 4 state-of-the-art (SOTA) video DMs, with parameter sizes ranging from 1.3B 14B, show that QVGen is the first to reach full-precision comparable quality under 4-bit settings. Moreover, it significantly outperforms existing methods. For instance, our 3-bit CogVideoX-2B achieves improvements of +25.28 in Dynamic Degree and +8.43 in Scene Consistency on VBench."
        },
        {
            "title": "Introduction",
            "content": "Recently, advancements in artificial intelligence-generated content (AIGC) have led to significant breakthroughs in text [60, 7], image [68, 31], and video synthesis [64, 28]. The development of video generative models, driven by the powerful diffusion transformer (DiT) [50] architecture, has been particularly notable. Leading video diffusion models (DMs), such as closed-source OpenAI Sora [49] and Kling [29], and open-source Wan [63] and CogVideoX [72], can successfully model motion dynamics, semantic scenes, physical consistency, etc. Despite their impressive performance, these models demand high computational resources and substantial peak memory, especially when generating long videos at high resolution. For example, Wan 14B [64] requires more than 30 minutes and 50GB of GPU memory to generate 10-second 720p resolution video clip on single H100 GPU. Even worse, deploying such models is infeasible on most customer-grade PCs, let alone Work done during internships at SenseTime Research. Corresponding authors. Preprint. Under review. (a) BF16 (b) W4A4 QVGen (Ours) (c) W4A4 EfficientDM [17] (d) W4A4 Q-DM [35] (e) W4A4 LSQ [8] (f) W4A6 SVDQuant [32] Text prompt: In the haunting backdrop of war-torn city, where ruins and crumbled walls tell story of devastation, poignant close-up frames young girl. Her face is smudged with ash, silent testament to the chaos around her. Her eyes glistening with mix of sorrow and resilience, capturing the raw emotion of world that has lost its innocence to the ravages of conflict. Figure 1: Comparison of samples generated by CogVideoX-2B [72] with fixed random seed. WxAy denotes x-bit per-channel weight and y-bit per-token activation quantization. Our approach far outperforms previous PTQ (i.e., (f)) and QAT (i.e., (c)-(e)) methods. To be noted, methods (c)-(f) have achieved noticeable performance for 4-bit image DMs. More visual results can be found in the appendix. resource-constrained edge devices. As result, their practical applications across various platforms face considerable challenges. In light of these problems, model quantization, which maps high-precision (e.g., FP16/BF16) data to low-precision (e.g., INT8/INT4) formats, stands out as compelling solution. For instance, employing 4-bit models with fast kernel implementation can achieve significant 3 speedup ratio with about 4 model size reduction compared with floating-point models on NVIDIA RTX4090 GPUs [32]. However, quantizing video DMs is more challenging than quantizing image DMs, and it has not received adequate attention. As shown in Fig. 1, applying prior high-performing approaches [32, 8, 35, 17] to quantize video DM into ultra-low bits ( 4 bits) is ineffective. In contrast to post-training quantization (PTQ), quantization-aware training (QAT) can obtain superior performance through training quantized weights. Nevertheless, it still leads to severe video quality degradation, as demonstrated by Fig. 1 (a) vs. (c)-(e). This highlights the need for an improved QAT framework to preserve video DMs exceptional performance under 4-bit or lower quantization. In this work, we present novel QAT framework, termed QVGen. It aims to improve the convergence without additional inference costs of low-bit Quantized DMs for Video Generation. Specifically, we first provide theoretical analysis showing that minimizing the gradient norm gt2 is the key to improving the convergence of QAT for video DMs. Motivated by this finding, we introduce auxiliary modules Φ for the quantized video DM to mitigate quantization errors. These modules effectively help narrow the discrepancy between the discrete quantized and full-precision models, leading to stable optimization and largely reduced gt2. The quantized DM thus achieves better convergence. Our observation also implies that the significant performance drops  (Fig. 1)  of the existing SOTA QAT method [35] may result from its high gt2  (Fig. 3)  . Moreover, to adopt Φ for improving QAT while avoiding its substantial inference overhead, we progressively remove Φ during training. Upon further analysis, we have found that the amount of small singular values in WΦ (the weight of Φ) increases throughout the training process. This indicates that the quantity of low-contributing components in WΦ, which are related to small singular values [75, 71], grows during QAT. As result, an increasing number of these components can be removed with minimal impact on training. Leveraging this insight, we introduce rank-decay strategy to progressively shrink WΦ. To be more specific, singular value decomposition (SVD) is first applied to recognize WΦs low-impact components. Then, rank-based regularization γ is utilized to gradually decay these components to . Such processes (i.e., decompose and then decay) are repeated until WΦ is fully eliminated, which also means that Φ is removed. In terms of results, this strategy incurs minimal performance impact while getting rid of the inference overhead. To summarize, our contributions are as follows: We introduce general-purpose QAT paradigm, called QVGen. To our knowledge, this is the first QAT method for video generation and achieves effective 3-bit and 4-bit quantization. 2 To optimize extremely low-bit QAT, we enhance quantized DM with auxiliary modules (Φ) to reduce the gradient norm. Our theoretical and empirical analysis validates the effectiveness of this method in improving convergence. To eliminate the significant inference overhead introduced by Φ, we propose rank-decay strategy that progressively shrinks Φ. It iteratively performs SVD and applies rank-based regularization γ to obtain and decay low-impact components of Φ, respectively. As result, this method incurs minimal impact on performance. Extensive experiments across advancing CogVideoX [72] and Wan [64] families demonstrate the SOTA performance of QVGen. Notably, our W4A4 model is the first time to show full-precision comparable performance. In addition, we apply QVGen to Wan 14B, one of the largest SOTA open-source models, and observe negligible performance drops on VBench-2.0 [78]."
        },
        {
            "title": "2 Related Work",
            "content": "Video diffusion models. Building upon the remarkable success of diffusion models (DMs) [19, 56, 5] in image generation [31, 68], the exploration in the field of video generation [72, 64, 28] is also becoming popular. In contrast to convolution-based diffusion models [22, 4], the success of OpenAI Sora [49] has spurred researchers to adopt the diffusion transformer (DiT) [50] architecture and scale it up for high-quality video generation. However, advanced video DiTs [64, 72, 28] often involve billions of parameters, lengthy multi-step denoising, and intensive computation over long frame sequences. This results in substantial time and memory overhead, which limits their practical deployment. To enable faster video generation, some works have introduced step-distillation [73, 39] on pre-trained models to shorten the denoising trajectory. Others focus on efficient attention [74, 67], feature caching [45, 83], or parallel inference [11, 10] to accelerate per-step computations. Moreover, to achieve memory-efficient inference, existing research has explored efficient architecture design [66, 40], structure pruning [2, 77], and model quantization [76, 6, 59]. These methods aim to achieve both model size and computational cost reduction. Model quantization. Quantization [26] is predominant technique for minimizing storage and accelerating inference. It can be categorized into post-training quantization (PTQ) [37, 46] and quantization-aware training (QAT) [61, 82, 8, 12]. PTQ compresses models without re-training, making it fast and data-efficient. Nevertheless, it may result in suboptimal performance, especially under ultra-low bit-width (e.g., 3/4-bit). Conversely, QAT applies quantization during training or finetuning and typically achieves higher compression rates with less performance degradation. For DMs, previous quantization research [35, 32, 17, 34, 18, 23, 24, 55, 65, 54, 63] primarily focuses on image generation. Video DMs, which incorporate complex temporal and spatial modeling, are still challenging for low-bit quantization. QVD [59] and Q-DiT [6] first apply PTQ to convolutionbased [70, 13] and DiT-based [81] video DMs, respectively. Furthermore, ViDiT-Q [76] employs mixed-precision and fine-grained PTQ to improve performance. However, it experiences video quality loss with 8-bit activation quantization and has not been extended to more advanced models [72, 64]. Consequently, QAT for advanced video DMs is urgently needed. In this work, we identify and address the ineffective low-bit QAT for the video DM. Our proposed method significantly enhances model performance and incurs zero inference overhead in ultra-low-bit settings. Moreover, our framework is orthogonal to existing QAT methods, which target gradient estimation [12, 8], oscillation reduction [47, 35], mixed-precision schemes [61, 51], etc."
        },
        {
            "title": "3 Preliminaries",
            "content": "Video diffusion modeling. The video DM [22, 81] extends image diffusion frameworks [35, 56] into the temporal domain by learning dynamic inter-frame dependencies. Let x0 Rf hwc be latent video variable, where denotes the count of video frames, each of size with channels. DMs are trained to denoise samples generated by adding random Gaussian noise ϵ (0, I) to x0: xτ = ατ x0 + στ ϵ, (1) where ατ , στ > 0 are scalar values that collectively control the signal-to-noise ratio (SNR) according to given noise schedule [57] at timestep τ [1, . . . , ]3. One typical training objective (i.e., predict 3N denotes the maximum timestep. 3 the noise [20]) of denoiser ϵθ with parameter θ can be formulated as follows: L(θ) = Ex0,ϵ,C,τ [ϵ ϵθ(xτ , C, τ )2 where represents conditional guidance, like texts or images, and denotes the Frobenius norm. Additionally, v-prediction [53] (i.e., predict dxτ dτ ) is also prevailing option [72, 64, 28] as the target. During inference, we can employ ϵθ with various sampling methods [43, 79] progressively denoising from random Gaussian noise xN (0, I) to clean video variable. The raw video is obtained by decoding the variable via video variational auto-encoder (VAE) [72, 14, 64]. ], (2) Quantization. The current video DM based on the diffusion transformer (DiT) [50] architecture primarily consists of linear layers. Given an input Rmk, full-precision linear layer with weight Rnm and the layers quantized version can be formulated as: = WX, ˆY = Qb(W)Qb(X), where Rnk and ˆY Rnk 4 represent the outputs of the full-precision (e.g., FP16/BF16) and quantized linear layers, respectively. Qb() denotes the function of b-bit quantization. In this paper, we adopt asymmetric uniform quantization. For example, Qb(X) can be represented as: (3) Qb(X) = (clamp( s + z, 0, 2b 1) z) s, where = max(X)min(X) 2b1 , = min(X) . (4) Here, quantization parameters and denote the scaler and zero shift, respectively. clamp(, , ) bounds the integer values into [0, 2b 1]. To ensure the differentiability of the rounding function for QAT, straight-through estimator (STE) [3] is widely applied as: Qb(X) = 0 +z2b1. (5) Similar to existing works [35, 80], we employ the full-precision model as the teacher to guide the training of the quantized model in knowledge distillation-based (KD-based) manner. Therefore, the training loss can be defined as: = Ex0,C,τ [ˆϵθ(xτ , C, τ ) ϵθ(xτ , C, τ )2 ], (6) where ˆϵθ denotes the quantized denoiser of video DM."
        },
        {
            "title": "4 QVGen",
            "content": "Figure 2: Overview of the proposed QVGen. (a) This framework integrates auxiliary modules Φ to improve training convergence (Sec. 4.1). (b) To maintain performance while eliminating inference overhead induced by Φ, we design rank-decay schedule that progressively shrinks the entire Φ to through iteratively applying the following two strategies (Sec. 4.2): 1) SVD to identify the low-impact components in Φ; 2) rank-based regularization γ to decay the identified components to . Considering the substantial video-quality drops (see Fig. 1) observed in existing QAT methods [35, 17, 8], we believe that the quantized video DM suffers from poor convergence. In the following subsections, we propose QVGen (see Fig. 2) to address this issue while maintaining inference efficiency. 4Here, denotes the output channel, signifies the token dimension, and is the token number. We omit the batch dimension for clarity. 4 4. Improving Convergence with Auxiliary Modules Φ To begin with, we analyze the convergence of quantized video DM using the regret, which is widely used in the analysis of deep learning optimizers [27, 44]. It is defined as: R(T ) = (cid:80)T t=1 ft(θt) ft(θ), (7) where signifies the total number of training iterations and ft() is the unknown cost function at iteration t. Here, θt represents the parameters of the quantized video DM at training step t, constrained within convex compact set Sd, while θ = arg minθSd (cid:80)T t=1 ft(θ) is the optimal parameters. In QAT, θt is updated by gradient descent, with the learning rate ηt and gradient gt, as: θt+1 = θt ηtgt. (8) Theorem 4.1. Assume that ft is convex5 and θi, θj S, θi θj D. Then the average regret is upper-bounded as: R(T ) smaller value of R(T ) provided in the appendix) suggests that for large (i.e., dD 2T ηT gt2 is critical for improving convergence behavior of QAT. + 1 dD 2T ηT ηt 2 gt2 2. implies closer convergence to the optimum. Thm. 4.1 (with proof becomes negligible), minimizing t=1 (9) (cid:80)T lower gt2 is typically observed in more stable training processes [58, 69]. Therefore, to reduce gt2, we aim to stabilize the QAT process by mitigating aggressive training losses (e.g., loss spikes) [30, 33]. Specifically, we introduce learnable auxiliary module Φ to enhance each quantized linear layer of video DM. This module is trained to mitigate severe quantization-induced errors during QAT, thereby preventing aggressive training losses. The forward computation of such Φ-equipped layer becomes: ˆY = Qb(W)Qb(X) + Φ(Qb(X)), where Φ(Qb(X)) = WΦQb(X). Here, WΦ is initialized before QAT by the weight quantization error, defined as Qb(W). (10) (b) Wan 1.3B [64] (a) CogVideoX-2B [72] To validate the effectiveness of Φ, we conduct experiments for CogVideoX 2B [72] and Wan 1.3B [64]. Compared with the previous SOTA QAT method Q-DM [35], the proposed approach exhibits consistently lower gt2 and reduced training loss, as depicted in Fig. 3. This aligns well with both the theoretical and the empirical analysis discussed earlier. Therefore, incorporating Φ in QAT effectively reduces the gradient norm and leads to better convergence for QAT. In addition, as evidenced by Fig. 3, the substantial performance degradation of Q-DM (e.g., depicted in Fig. 1) for the video generation task could be attributed to its relatively large gt2. Figure 3: (Upper) gt2 vs. #steps and (Lower) training loss vs. #steps across different video DMs and 4-bit QAT methods under the same training settings.Φ denotes our approach in Sec. 4.1. 4.2 Progressively Shrinking Φ via Rank-Decay However, during inference, the auxiliary module Φ introduces non-negligible overhead. Concretely, Φ incurs additional matrix multiplications between b-bit activations Qb(X) and full-precision weights WΦ. This is inapplicable to low-bit multiplication kernels and thus hinders inference acceleration. In addition, the storage of full-precision WΦ for each Φ leads to significant memory overhead, exceeding that of the quantized diffusion model by severalfold. To improve QAT while eliminating the inference overhead, we propose to progressively remove Φ throughout the training process. This allows the model to benefit from Φ during QAT, while 5This may not hold for deep networks. However, it facilitates the analysis of deep learning models [27, 44]. 5 ultimately yielding standard quantized model [35, 8] with no extra inference cost. To achieve this goal, straightforward solution is to decay all parameters of Φ directly. However, we have noticed that it is ineffective and suboptimal (see Tab. 4). This calls for fine-grained decay strategy. (a) CogVideoX-2B [72] (b) Wan 1.3B [64] Figure 4: Singular value variation in WΦ across training iterations for 4-bit video DMs. We visualize the average of the singular values {σs}s=1,2,...,210 {σd} across layers of all Attention blocks [62] and feed-forward networks (FFNs), respectively. 0 step denotes the initialization state before the start of QAT. Based on the above analysis, we investigate the contribution of fine-grained components in Φ. Specifically, we apply singular value decomposition (SVD) [32, 36] to WΦ at various training steps: WΦ = (cid:80)d s=1 σsusvT , (11) where = min{n, m} and σ1 σ2 . . . σd are the singular values. The vectors us and vs denote the left and right singular vectors associated with σs, respectively. By tracking the evolution of the average σs, we observe two key findings (exemplified by Fig. 4 (a) Attention): WΦ contains substantial number of small singular values. For example, approximately 73% of the average σs are 14 smaller than the largest one σ1; The presence of these small σs becomes increasingly pronounced as QAT progresses, with the proportion rising from 73% (at the 0-th step) to 99% (at the 2K-th step). These findings suggest that an increasing number of orthonormal directions {us, vs} contribute little, as their associated singular values σs are small [75, 71]. Hence, as training proceeds, only an increasingly low-rank portion of Φ (i.e., Σd , where < d) is needed, and the remaining s=1σsusvT components can be decayed without noticeably affecting performance. Motivated by this, we propose novel rank-decay schedule that progressively shrinks Φ by repeatedly identifying and eliminating the above-mentioned low-impact parts. First, to attain these parts, we reformulate the computation of Φ as: σ1u1, . . . , Φ(Qb(X)) = LRQb(X), (12) σrvr]T Rrm for given where = [ rank r. In practice, we set to reduce training costs, as WΦ already exhibits non-negligible σsvs are the components number of small singular values before QAT. Consequently, in WΦ represent the s-th level of contribution. Then, with rank-based regularization γ applied, the forward computation of quantized linear layer during training is modified as: σrur] Rnr and = [ σ1v1, . . . , σsus and where γ is defined as: ˆY = Qb(W)Qb(X) + (γ L)RQb(X), (13) γ = concat([1]n(1λ)r, [u]nλr) Rnr. Here, follows cosine annealing schedule that decays from 1 to 0, λ (0, 1] represents the shrinking ratio, and denotes element-wise multiplication. Eq. (13) and Eq. (14) allow us to progressively eliminate the low-impact components of Φ (i.e., [ σrur] σrvr]T ). Once reaches 0, we truncate {L, R} to {L, R}, and and [ rewrite WΦ as: σ(1λ)r+1u(1λ)r+1, . . . , σ(1λ)r+1v(1λ)r+1, . . . , (14) (cid:26)L = L[:, : (1 λ)r] = R[: (1 λ)r, :] WΦ = LR. (15) In Eq. (15), the rank of WΦ is shrunk from to (1 λ)r. During the subsequent training phase, the above procedures (i.e., both decomposition and decay) are iteratively applied. Ultimately, we fully eliminate Φ by reducing to 0, which incurs negligible impact on model performance (see Tab. 3). It is worth noting that we set λ = 1 2 for Eq. (14) in this work, based on its effectiveness demonstrated in Tab. 4. Overall, the overview of the rank-decay schedule is exhibited in Fig. 2 (b)."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Implementation Details Models. We conduct experiments on open-source SOTA video DMs, including CogVideoX-2B and 1.5-5B [72], and Wan 1.3B and 14B [64]. Classifier-free guidance (CFG) [21] is used for all models, and the frame number of generated videos is fixed to 49 for CogVideoX-2B and 81 for the others. Baselines. We adopt previous powerful PTQ and QAT methods as baselines: ViDiT-Q [76], SVDQuant [32], LSQ [8], Q-DM [35], and EfficientDM [17]. Since these methods were designed for image DMs or convolutional neural networks (CNNs), we adapt these works to video DMs using their open-source code (if available) or the implementation details provided in the corresponding papers. Without specific clarification, static per-channel weight quantization with dynamic per-token activation quantization, common practice in the community [76, 41], is used for all the linear layers. Training. We employ 16K captioned videos from OpenVidHQ-4M [48] as the training dataset. The AdamW [42] optimizer is utilized with weight decay of 104. We employ cosine annealing schedule to adjust the learning rate over training. During QAT, we train Wan 14B [64] and CogVideoX1.5-5B [72] for 16 epochs on 32 H100 GPUs and 16 H100 GPUs, respectively. For the other DMs, we employ 8 training epochs on 8 H100 GPUs. Additionally, we allocate the same training iterations for each decay phase (i.e., shrinking the remaining to (1 λ)r). The same training settings are applied to all QAT baselines. Evaluation. We select 8 dimensions in VBench [25] with unaugmented prompts to comprehensively evaluate the performance following previous studies [76, 52]. Moreover, for quantized huge ( 5B parameters) DMs, we additionally report the results on VBench-2.0 [78] with augmented prompts to measure the adherence of videos to physical laws, commonsense reasoning, etc. More detailed experimental setups can be found in the appendix. Table 1: Performance comparison across different quantization methods on VBench [25]. indicates PTQ methods and * signifies QAT methods. Full Prec. denotes the BF16 model. represents that we apply fine-grained per-group weight-activation quantization with group size of 64 and keep some linear layers unquantized, which is the same as the official settings of SVDQuant [32] (details can be found in the appendix). Best and second-best results are highlighted in bold and underline formats, respectively. The gaps between them are marked in red subscripts. Method #Bits (W/A) Imaging Quality Aesthetic Quality Motion Smoothness Dynamic Degree Background Consistency Subject Consistency Scene Consistency Overall Consistency CogVideoX-2B (CFG = 6.0, 480p, fps = 8) Full Prec. 16/16 59.15 ViDiT-Q [76] SVDQuant [32] 4/6 4/6 54.72 58.27 54.49 43.01 47. 97.43 92.18 95.28 67.78 43.22 40.83 94.79 90.76 92. SVDQuant [32] 4/4 LSQ [9] 4/4 Q-DM [35] 4/4 EfficientDM [17] 4/4 QVGen (Ours) 4/4 LSQ [9] Q-DM [35] EfficientDM [17] QVGen (Ours) 3/3 3/3 3/3 3/3 51.60 58.73 54.96 55.96 60.16+1.43 54.61+0.41 98.06+0.03 49.40 54.20 52.71 51.97 97.69 97.57 98.00 98. 56.46 50.88 52.86 58.36+1.90 50.54+5.96 98.37+0.34 97.98 98.03 97.13 40.35 40.41 44.58 42.22 45.00 48.61 46.67 67.22+18.61 94.38+0.28 94.03 92.97 93.82 94.10 0.56 5.56 28.61 53.89+25.28 94.55+0. 94.08 93.93 93.15 Full Prec. 16/16 64.30 ViDiT-Q [76] SVDQuant [32] 4/6 4/6 56.24 58. 58.21 50.18 51.27 97.37 94.81 97.05 70.28 52.43 49. 95.94 89.67 93.74 Wan 1.3B (CFG = 5.0, 480p, fps = 16) SVDQuant [32] 4/4 LSQ [9] 4/4 Q-DM [35] 4/4 EfficientDM [17] 4/4 QVGen (Ours) 4/4 LSQ [9] Q-DM [35] EfficientDM [17] QVGen (Ours) 3/3 3/3 3/3 3/ 57.57 59.11 60.40 60.70 63.08+2.38 54.67+1.10 98.250.10 46.30 49.09 52.50 53.57 94.21 98.35 97.22 96.18 58.80 56.19 42.32 67.35+8.55 49.71+2.85 98.93+0.71 46.86 44.95 33.52 98.22 95.13 96. 72.22 71.11 76.67 56.39 77.78+1.11 94.08+0.34 93.16 92.66 93.37 93.74 23.61 76.94 70.28 84.14+7.20 93.62+1.52 91.86 92.09 92.10 7 92. 81.02 87.45 91.78 92.41 91.86 91.70 93.01+0.60 89.18 87.75 88.26 90.50+1.32 93.84 82.53 91.71 77.96 91.67 89.26 91.70 92.57+0. 89.42 83.82 74.79 92.25+2.83 36.24 26.25 27.69 25.67 24.06 28.02 27.76 31.42+3.40 4.80 7.33 15.42 23.85+8.43 28. 13.45 14.18 12.73 10.38 13.28 11.77 15.32+2.04 0.89 1.79 0.04 5.71+3.92 25.06 20.41 21.34 22.89 23.17 23.87 24.28 24.61+0. 13.80 15.98 20.42 22.92+2.50 24.67 19.58 23.26 21.91 18.83 21.63 21.19 23.01+1.38 15.51 16.89 11.38 20.11+3.22 5.2 Performance Analysis Comparison with baselines. We report VBench [25] score comparisons in Tab. 1. In W4A4 quantization, recent QAT methods [8, 17, 35] show non-negligible performance degradation. With W3A3, performance drops become more pronounced. In contrast, the proposed QVGen achieves substantial performance recovery in 3-bit models and comparable results to full-precision models in 4-bit quantization. Specifically, it shows higher scores or less than 2% decrease in all metrics for W4A4 CogVideoX-2B [72], except Scene Consistency. For PTQ baselines [76, 32], all fail to generate meaningful content in W4A4 per-channel and per-token settings. Therefore, we apply W4A6 quantization for these methods and also utilize fine-grained per-group W4A4 quantization for SVDQuant [32]. In these cases, W4A4 QVGen outperforms them by large margin, particularly with 8.37 and 14.61 higher Aesthetic Quality and Subject Consistency for Wan 1.3B compared to W4A4 SVDQuant. In addition to these findings, we observe that for Wan 1.3B, Dynamic Degree recovers easily during QAT, even surpassing that of the full-precision model. However, for CogVideoX-2B, this metric significantly drops. Moreover, Scene Consistency is the most challenging metric to maintain across models and methods. Beyond the quantitative results, we provide qualitative results in Fig. 1 and the appendix, where QVGen markedly improves visual quality over prior methods. Although clear gap remains between 3-bit and 4-bit outputs, our work lays foundation for practical 3-bit or lower video DM quantization. Method #Bits (W/A) Table 2: Performance for huge video DMs on VBench [25]. Dynamic Degree Background Consistency Motion Smoothness Subject Consistency Aesthetic Quality Imaging Quality Scene Consistency Overall Consistency Full Prec. 16/16 66.25 QVGen (Ours) QVGen (Ours) 4/ 3/3 66.76 54.44 Full Prec. 16/16 67.89 QVGen (Ours) QVGen (Ours) 4/4 3/3 66.87 48.70 CogVideoX1.5-5B (CFG = 6.0, 720p, fps = 16) 59.49 59.52 35.85 61.54 59.41 29. 98.42 98.38 97.23 59.72 64.44 58. 96.57 95.83 96.48 Wan 14B (CFG = 5.0, 720p, fps = 16) 97.32 97. 99.05 70.56 76.11 93.33 96.31 96. 97.34 95.28 94.88 90.17 94.08 94. 94.71 39.14 28.47 13.27 33.91 19. 2.81 26.18 24.45 17.15 26.17 25. 13.97 (b) Wan 14B [64] (a) CogVideoX1.5-5B [72] Results for huge DMs. To demonstrate the scalability of our method, we further test two huge video DMs, including CogVideoX1.55B and Wan 14B, at 720p resolution. As illustrated in Tab. 2, our 3-bit and 4bit models follow the same pattern seen in smaller models (Tab. 1). However, 3-bit quantization incurs much larger drops on demanding metrics such as Scene and Overall Consistency, underscoring the challenge of pushing these larger models to 3 bits. In Fig. 5, we further assess the models with VBench-2.0 [78]; the W4A4 DMs incur only negligible overall performance loss. Figure 5: Performance for huge video DMs on VBench-2.0 [78]. Our 4-bit models exhibit minimal drop of 1% in total score. 5.3 Ablation Studies To demonstrate the effect of each design, we employ W4A4 Wan 1.3B [64] with 5 dimensions of VBench [25] for ablation studies. More ablations can be found in the appendix. Table 3: Ablation results of each component. Naive denotes naive QAT in KD-based manner. Scene Consistency Overall Consistency Aesthetic Quality Imaging Quality Dynamic Degree Method Effect of different components. We evaluate the contribution of each component in Tab. 3. The auxiliary module Φ (Sec. 4.1) yields substantial performance improvements across all metrics. Naive +Φ +rank-decay 63.080.33 54.670.08 77.780.11 15.320.19 21.63 22.98 23.01+0. 13.28 15.51 52.50 54.75 76.67 77.89 60.40 63.41 8 Furthermore, the rank-decay schedule (Sec. 4.2) effectively eliminates inference overhead, while inducing less than 0.6% drop in most metrics. It even leads to slight improvement in Overall Consistency. Choice of the shrinking ratio λ. To determine proper shrinking ratio λ in Eq. (14), we conduct experiments in Tab. 4. By maintaining the same number of training iterations for each decay phase 6, small ratio results in an excessively rapid descent of in Eq. (14) from 1 to 0, potentially destabilizing the training process. On the other hand, larger ratio may cause the premature removal of highcontributing components during each phase. An extreme scenario would involve 100% ratio (i.e., λ = 1), in which the entire WΦ is removed in single decay phase, leading to significant performance degradation. Thus, we adopt moderate ratio of λ = 1 2 in QVGen. Table 4: Results of different shrinking ratios λ in Eq. (14) for each decay phase. λ = 1 means directly decaying the entire WΦ. λ = 1 2 is used in this work. Table 5: Results of different initial ranks for Eq. (15). = 0 represents Naive in Tab. 3. We employ = 32 in this work. λ 1/4 1/2 3/4 1 Imaging Quality Aesthetic Quality Dynamic Degree Scene Consistency Overall Consistency 54.23 76.84 63.02 63.08+0.06 54.67+0.05 77.780.13 15.32+0.14 62.89 61.05 77.91 76. 15.04 13.82 54.62 52.48 15.18 22.85 23.01+0.12 22.89 21.81 0 8 16 32 Imaging Quality Aesthetic Quality Dynamic Degree Scene Consistency Overall Consistency 52.50 54.47 54.62 60.40 62.71 62.99 63.08+0.02 54.67+0.05 77.78+1.04 15.320.08 63.06 76.67 74.62 76.58 13.28 14.42 14. 15.40 54.30 76.74 21.63 22.81 23.00 23.01+0.01 22.92 Choice of the initial rank r. We further present the results for different initial ranks of WΦ in Tab. 5. As increases, the performance gains diminish and eventually deteriorate (i.e., at = 64). We attribute this trend to the same issue associated with small shrinking ratio discussed earlier. Specifically, increasing to 2r introduces an additional decay phase, which shortens the training time allocated to each phase and may lead to overly rapid decay (e.g., at = 64). Table 6: Results of different decay strategies. Details of each method can be found in the appendix. Rank-based denotes the rank-decay strategy in this work. Scene Aesthetic Consistency Quality Analysis of different fine-grained decay strategies. To further demonstrate the effectiveness of our rank-decay strategy, we evaluate alternative decay strategies in Tab. 6. Inspired by network pruning [16, 15], we introduce Sparse-based strategy that progressively prunes the largest values in WΦ during training. Additionally, motivated by residual quantization [38], we design Res. Q.-based strategy, which first quantizes WΦ into 44-bit tensors with the same shape and then progressively removes them one by one. Among all these methods, the Rank-based strategy outperforms others across all the metrics by large margin. This result highlights the superiority of our rank-decay schedule. Rank-based 63.08+1.36 54.67+0.61 77.78+3.54 15.32+1.15 Sparse-based 61.15 Res. Q.-based 61.72 22.52 22.31 23.01+0.49 Overall Consistency Imaging Quality Dynamic Degree Decay Strategy 74.24 72.41 54.06 54.01 13.86 14.17 Additionally, both Sparse-based and Res. Q.-based strategies require at least 1.8 training hours at the same setups compared with our Rank-based approach (see the appendix). 5.4 Efficiency Discussion (a) Model Size (GB) (b) Inference Latency (s) We report the per-step latency of W4A4 DiT components on one A800 GPU in Fig. 6 (b), using the same settings as Sec. 5.2. Adapted from the fast CUDA kernel implementation by Ashkboos et al. [1], W4A4 QVGen achieves 1.21 and 1.44 speedups for Wan 1.3B and 14B, respectively. Besides, it exhibits 4 memory savings compared to the BF16 format, as shown in Fig. 6 (a). Nevertheless, we believe that the acceleration ratio could be further improved with advanced kernel-fusion techniques, which we leave to future work. In addition, it is worth noting that our QVGen adheres to standard uniform quantization, enabling drop-in deployment with existing W4A4 kernels across various devices. Figure 6: Evaluation of memory compression and inference acceleration. Blue color and yellow color denote Wan [64] 1.3B and 14B, respectively. 6We also employ fixed total number of training epochs."
        },
        {
            "title": "6 Conclusions and Limitations",
            "content": "In this work, we are the first to explore the application of quantization-aware training (QAT) in video DMs. Specifically, we provide theoretical analysis that identifies that lowering the gradient norm is essential to improve convergence. Then, we propose an auxiliary module (Φ) to achieve this. Additionally, we design rank-decay schedule to progressively eliminate Φ for zero inference overhead with minimal impact on performance. Extensive experiments for 3-bit and 4-bit quantization validate the effectiveness of our framework, QVGen. In terms of limitations, we focus on video generation in this work. However, we believe that our methods can be generalized to more tasks, e.g., image generation and natural language processing (NLP), which we will explore in the future."
        },
        {
            "title": "References",
            "content": "[1] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms, 2024. 9 [2] Haitam Ben Yahia, Denis Korzhenkhov, Ioannis Lelekas, Amir Ghodrati, and Amirhossein Habibian. Mobile video diffusion. arXiv, 2024. 3 [3] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation, 2013. 4 [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023. [5] Junsong Chen, Jincheng YU, Chongjian GE, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-$alpha$: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2024. 3 [6] Lei Chen, Yuan Meng, Chen Tang, Xinzhu Ma, Jingyan Jiang, Xin Wang, Zhi Wang, and Wenwu Zhu. Q-dit: Accurate post-training quantization for diffusion transformers, 2024. 3 [7] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, 10 Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. 1 [8] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmendra S. Modha. Learned step size quantization, 2020. 2, 3, 4, 6, 7, [9] Steven K. Esser, Jeffrey L. McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and In International Conference on Dharmendra S. Modha. Learned step size quantization. Learning Representations, 2020. 7 [10] Jiarui Fang, Jinzhe Pan, Xibo Sun, Aoyu Li, and Jiannan Wang. xdit: an inference engine for diffusion transformers (dits) with massive parallelism. arXiv preprint arXiv:2411.01738, 2024. 3 [11] Jiarui Fang, Jinzhe Pan, Jiannan Wang, Aoyu Li, and Xibo Sun. Pipefusion: Patch-level pipeline parallelism for diffusion transformers inference. arXiv preprint arXiv:2405.14430, 2024. 3 [12] Ruihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang Li, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie Yan. Differentiable soft quantization: Bridging full-precision and low-bit neural networks, 2019. [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. International Conference on Learning Representations, 2024. 3 [14] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, Poriya Panet, Sapir Weissbuch, Victor Kulikov, Yaki Bitterman, Zeev Melumian, and Ofir Bibi. Ltx-video: Realtime video latent diffusion, 2024. 4 [15] Song Han, Huizi Mao, and William J. Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding, 2016. 9 [16] Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for efficient neural networks, 2015. [17] Yefei He, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. EfficientDM: Efficient In The Twelfth International quantization-aware fine-tuning of low-bit diffusion models. Conference on Learning Representations, 2024. 2, 3, 4, 7, 8 [18] Yefei He, Luping Liu, Jing Liu, Weijia Wu, Hong Zhou, and Bohan Zhuang. Ptqd: Accurate post-training quantization for diffusion models, 2023. 3 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 68406851. Curran Associates, Inc., 2020. 3 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. [21] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. 7 [22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models, 2022. 3 [23] Yushi Huang, Ruihao Gong, Jing Liu, Tianlong Chen, and Xianglong Liu. Tfmq-dm: Temporal feature maintenance quantization for diffusion models, 2024. 3 [24] Yushi Huang, Ruihao Gong, Xianglong Liu, Jing Liu, Yuhang Li, Jiwen Lu, and Dacheng Tao. Temporal feature matters: framework for diffusion model quantization, 2025. 3 [25] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024. 7, 8 [26] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference, 2017. 3 [27] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. 5 11 [28] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2025. 1, 3, [29] Kuaishou. Kling ai. https://klingai.kuaishou.com/, 6 2024. Accessed: 2024-06-30. 1 [30] Abhay Kumar, Louis Owen, Nilabhra Roy Chowdhury, and Fabian Güra. Zclip: Adaptive spike mitigation for llm pre-training, 2025. 5 [31] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. 1, 3 [32] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Junxian Guo, Xiuyu Li, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. SVDQuant: Absorbing outliers by low-rank component for 4-bit diffusion models. In The Thirteenth International Conference on Learning Representations, 2025. 2, 3, 6, 7, 8 [33] Xiaolong Li, Zhi-Qin John Xu, and Zhongwang Zhang. Loss spike in training neural networks, 2024. 5 [34] Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer. Q-diffusion: Quantizing diffusion models, 2023. 3 [35] Yanjing Li, Sheng Xu, Xianbin Cao, Xiao Sun, and Baochang Zhang. Q-DM: An efficient lowbit quantized diffusion model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2, 3, 4, 5, 6, 7, 8 [36] Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large language models, 2023. 6 [37] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction, 2021. [38] Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang, and Wen Gao. Performance guaranteed network acceleration via high-order residual quantization, 2017. 9 [39] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025. 3 [40] Dongyang Liu, Shicheng Li, Yutong Liu, Zhen Li, Kai Wang, Xinyue Li, Qi Qin, Yufei Liu, Yi Xin, Zhongyu Li, Bin Fu, Chenyang Si, Yuewen Cao, Conghui He, Ziwei Liu, Yu Qiao, Qibin Hou, Hongsheng Li, and Peng Gao. Lumina-video: Efficient and flexible video generation with multi-scale next-dit, 2025. 3 [41] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware training for large language models, 2023. 7 [42] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. [43] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps, 2022. 4 [44] Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic bound of learning rate, 2019. 5 [45] Zhengyao Lv, Chenyang Si, Junhao Song, Zhenyu Yang, Yu Qiao, Ziwei Liu, and Kwan-Yee K. Wong. Fastercache: Training-free video diffusion model acceleration with high quality. In The Thirteenth International Conference on Learning Representations, 2025. 3 [46] Markus Nagel, Rana Ali Amjad, Mart van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training quantization, 2020. 3 [47] Markus Nagel, Marios Fournarakis, Yelysei Bondarenko, and Tijmen Blankevoort. Overcoming oscillations in quantization-aware training, 2022. 3 12 [48] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation, 2025. 7 [49] OpenAI. Video generation models as world simulators. https://openai.com/index/ video-generation-models-as-world-simulators/, 2024. Accessed: 2025-04-09. 1, 3 [50] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 1, 3, 4 [51] Jorn Peters, Marios Fournarakis, Markus Nagel, Mart van Baalen, and Tijmen Blankevoort. Qbitopt: Fast and accurate bitwidth reallocation during training, 2023. 3 [52] Weiming Ren, Huan Yang, Ge Zhang, Cong Wei, Xinrun Du, Wenhao Huang, and Wenhu Chen. Consisti2v: Enhancing visual consistency for image-to-video generation, 2024. 7 [53] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models, 2022. 4 [54] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models, 2023. 3 [55] Junhyuk So, Jungwon Lee, Daehyun Ahn, Hyungjun Kim, and Eunhyeok Park. Temporal dynamic quantization for diffusion models, 2023. 3 [56] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. ArXiv, abs/2010.02502, 2021. 3 [57] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021. 3 [58] Sho Takase, Shun Kiyono, Sosuke Kobayashi, and Jun Suzuki. Spike no more: Stabilizing the pre-training of large language models, 2024. 5 [59] Shilong Tian, Hong Chen, Chengtao Lv, Yu Liu, Jinyang Guo, Xianglong Liu, Shengxi Li, Hao Yang, and Tao Xie. Qvd: Post-training quantization for video diffusion models, 2024. 3 [60] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023. 1 [61] Mart van Baalen, Christos Louizos, Markus Nagel, Rana Ali Amjad, Ying Wang, Tijmen Blankevoort, and Max Welling. Bayesian bits: Unifying quantization and pruning, 2020. 3 [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. 6 [63] Changyuan Wang, Ziwei Wang, Xiuwei Xu, Yansong Tang, Jie Zhou, and Jiwen Lu. Towards accurate post-training quantization for diffusion models, 2024. 1, [64] WanTeam, :, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models, 2025. 1, 3, 4, 5, 6, 7, 8, 9 [65] Junyi Wu, Haoxuan Wang, Yuzhang Shang, Mubarak Shah, and Yan Yan. Ptq4dit: Post-training quantization for diffusion transformers, 2024. 3 [66] Yushu Wu, Zhixing Zhang, Yanyu Li, Yanwu Xu, Anil Kag, Yang Sui, Huseyin Coskun, Ke Ma, Aleksei Lebedev, Ju Hu, Dimitris Metaxas, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapgen-v: Generating five-second video within five seconds on mobile device, 2024. 3 [67] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, Jianfei Chen, Ion Stoica, Kurt Keutzer, and Song Han. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity, 2025. 3 [68] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In The Thirteenth International Conference on Learning Representations, 2025. 1, 3 [69] Zeke Xie, Zhiqiang Xu, Jingzhao Zhang, Issei Sato, and Masashi Sugiyama. On the overlooked pitfalls of weight decay and how to mitigate them: gradient-norm perspective, 2024. 5 [70] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model, 2023. 3 [71] Huanrui Yang, Minxue Tang, Wei Wen, Feng Yan, Daniel Hu, Ang Li, Hai Li, and Yiran Chen. Learning low-rank deep neural networks via singular vector orthogonality regularization and singular value sparsification, 2020. 2, 6 [72] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer, 2025. 1, 2, 3, 4, 5, 6, 7, 8 [73] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. 3 [74] Jintao Zhang, Jia wei, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In The Thirteenth International Conference on Learning Representations, 2025. [75] Xiangyu Zhang, Jianhua Zou, Kaiming He, and Jian Sun. Accelerating very deep convolutional networks for classification and detection, 2015. 2, 6 [76] Tianchen Zhao, Tongcheng Fang, Haofeng Huang, Rui Wan, Widyadewi Soedarmadji, Enshu Liu, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, and Yu Wang. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation. In The Thirteenth International Conference on Learning Representations, 2025. 3, 7, 8 [77] Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Yibing Song, Gao Huang, Fan Wang, and Yang You. Dynamic diffusion transformer. In The Thirteenth International Conference on Learning Representations, 2025. 3 [78] Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, and Ziwei Liu. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness, 2025. 3, 7, 8 [79] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics, 2023. 4 [80] Xingyu Zheng, Xianglong Liu, Haotong Qin, Xudong Ma, Mingyuan Zhang, Haojie Hao, Jiakai Wang, Zixiang Zhao, Jinyang Guo, and Michele Magno. Binarydm: Accurate weight binarization for efficient diffusion models, 2025. 4 [81] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. 3 [82] Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen. Incremental network quantization: Towards lossless cnns with low-precision weights, 2017. [83] Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, and Linfeng Zhang. Accelerating diffusion transformers with token-wise feature caching. In The Thirteenth International Conference on Learning Representations, 2025."
        }
    ],
    "affiliations": [
        "Beihang University",
        "ETH Zürich",
        "Hong Kong University of Science and Technology",
        "Monash University",
        "SenseTime Research"
    ]
}