{
    "paper_title": "Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions",
    "authors": [
        "Siqi Kou",
        "Qingyuan Tian",
        "Hanwen Xu",
        "Zihao Zeng",
        "Zhijie Deng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities in math and coding, often bolstered by post-training on the chain-of-thoughts (CoTs) generated by stronger models. However, existing strategies for curating such training data predominantly rely on heuristics, limiting generalizability and failing to capture subtleties underlying in data. To address these limitations, we leverage influence functions to systematically attribute LLMs' reasoning ability on math and coding to individual training examples, sequences, and tokens, enabling deeper insights into effective data characteristics. Our Influence-based Reasoning Attribution (Infra) uncovers nontrivial cross-domain effects across math and coding tasks: high-difficulty math examples improve both math and code reasoning, while low-difficulty code tasks most effectively benefit code reasoning. Based on these findings, we introduce a simple yet effective dataset reweighting strategy by flipping task difficulty, which doubles AIME24 accuracy from 10\\% to 20\\% and boosts LiveCodeBench accuracy from 33.8\\% to 35.3\\% for Qwen2.5-7B-Instruct. Moreover, our fine-grained attribution reveals that the sequence-level exploratory behaviors enhance reasoning performance in both math and code, and the token-level influence patterns are distinct for math and code reasoning: the former prefers natural language logic connectors and the latter emphasizes structural syntax."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 9 4 9 9 1 . 5 0 5 2 : r Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions Siqi Kou, Qingyuan Tian, Hanwen Xu, Zihao Zeng, and Zhijie Deng Shanghai Jiao Tong University {happy-karry, qy.tian, dctnorin, zengzihao, zhijied}@sjtu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable reasoning capabilities in math and coding, often bolstered by post-training on the chain-of-thoughts (CoTs) generated by stronger models. However, existing strategies for curating such training data predominantly rely on heuristics, limiting generalizability and failing to capture subtleties underlying in data. To address these limitations, we leverage influence functions to systematically attribute LLMs reasoning ability on math and coding to individual training examples, sequences, and tokens, enabling deeper insights into effective data characteristics. Our Influence-based Reasoning Attribution (Infra) uncovers nontrivial cross-domain effects across math and coding tasks: high-difficulty math examples improve both math and code reasoning, while low-difficulty code tasks most effectively benefit code reasoning. Based on these findings, we introduce simple yet effective dataset reweighting strategy by flipping task difficulty, which doubles AIME24 accuracy from 10% to 20% and boosts LiveCodeBench accuracy from 33.8% to 35.3% for Qwen2.5-7B-Instruct. Moreover, our fine-grained attribution reveals that the sequence-level exploratory behaviors enhance reasoning performance in both math and code, and the tokenlevel influence patterns are distinct for math and code reasoning: the former prefers natural language logic connectors and the latter emphasizes structural syntax."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) for reasoning, with OpenAI-o1 [15] and DeepSeek-R1 [9] as popular examples, have shown great promise in solving complex math and coding problems. Recently, the community has witnessed the prevalence of reproducing such reasoning capacities on open-source smallto medium-sized LLMs [19, 5, 29]. An initial stage of the solutions often involves post-training the model on some chain-of-thought (CoT) reasoning traces curated by leading models (e.g., R1) for diverse problems [34, 23, 24, 37, 13, 25]. As data-centric paradigm, the core research question here is: which attributes of the training data are effective in stimulating reasoning capabilities? Pioneering studies addressing this question predominantly adopt heuristic approaches. Typically, they first establish quantitative data quality metrics based on human expertise or empirical preferences, then selectively retain high-quality data for model training to cultivate robust reasoning capabilities with minimal data inputs. For example, s1K [24] filters 1k (question, answer) pairs with well-structured formatting, longer CoT length, and broader domain coverage from an initial pool of 59k data for training math reasoning LLMs. Similarly, LIMO [37] suggests incorporating more challenging math questions with complex reasoning chains to enable better math reasoning. Beyond focusing exclusively on math, Sky-T1 [25] targets competitive reasoning performance across both math and coding tasks. It notices that the naive incorporation of code data from APPS [10] Corresponding author. Preprint. Under review. Figure 1: An illustration of our key findings towards the question: Which attributes of training data effectively stimulate reasoning capabilities? Mixing challenging math problems with easier coding tasks leads to the highest influence scores for mathematical and coding reasoning (left). Guided by this insight, we curate an improved dataset and observe enhanced performance (right). degrades math performance and advocates mitigating this by introducing more difficult math questions and code tasks for training. Nevertheless, the underlying mechanism of such cross-domain influence remains underexplored. Furthermore, these heuristic strategies suffer from unreliable generalization to other reasoning scenarios and cannot clearly explain how some fine-grained reasoning patterns in the training data (e.g., verification, backtracking, etc.) affect the learned models. To bridge the gap, we leverage influence functions [17]a classical technique for tracing the impacts of individual training data on model behaviorto systematically identify which training examples, along with their internal patterns and tokens, most significantly enhance the reasoning capabilities on math and coding tasks. Following previous works on influence functions for LLMs [8, 30], we define an easy-to-implement and cost-effective influence function for reasoning-oriented supervised finetuning (SFT). We further extend the instance-wise influence function to more fine-grained variants at the sequence and token levels for an in-depth data attribution. We dub our approach as Infra. We begin by investigating cross-domain influence in basic math and code reasoning scenarios without long CoT. To this end, we fine-tune LLaMA3-8B-Base [7] on mixture of MetaMathQA [39] and OSS-Instruct [33] datasets and compute the influence function on the accuracy of GSM8k [3] and MBPP [10]. We rank all training data by their influence scores and find that, while in-domain data yield the highest scores as expected, cross-domain data also contribute nontrivially. Furthermore, aggregating these scores by category and difficulty reveals that symbolic math examples and highdifficulty math problems are particularly effective in improving code reasoning. Extending Infra to complex long CoT reasoning, we fine-tune Qwen2.5-7B-Instruct [36] on BespokeStratos-17k2 dataset and measure influence using AIME, MATH500 [11], and LiveCodeBench [16] benchmarks. Consistent with earlier findings, we observe cross-domain gains, with harder math problems better helping code reasoning. Going step further, we find that both high-difficulty math and code examples are more influential on math reasoning, whereas low-difficulty code tasks contribute most significantly to code reasoning (see Figure 1). Motivated by these insights, we flip easy math problems as hard and hard code tasks as easy in the training data. This reweighted dataset doubles AIME accuracy and improves LiveCodeBench accuracy from 33.8% to 35.3%. Furthermore, we perform attribution at sequence and token levels in long CoT. Sequence-level attribution shows that the exploration behavior of seeking alternative approaches after reaching correctness (refer to Figure 6), which is common in long CoTs, improves both math and code reasoning performance. Despite being seen as overthinking [2, 32], our studies suggest it is advantageous. Besides, we observe distinct token-level influence patterns for math and code reasoning. In math, the most influential tokens are natural language with logical connectors, whereas code CoTs rely more on syntax markers. This divergence explains why easier code problems with clearer structural solutions benefit code reasoning when combined with math CoT that already provides logical skills."
        },
        {
            "title": "2 Related Work",
            "content": "LLM reasoning. Reasoning is cognitive process that involves using evidence, arguments, and logic to arrive at conclusions or make judgments. It is widely regarded as foundational element of 2https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k 2 advanced Large Language Models (LLMs) and an essential milestone on the path toward Artificial General Intelligence (AGI) [12, 28, 29, 15, 35]. very recent approach to achieve reasoning capacity in LLMs is through post-training, such as OpenAI-o1 [15], and Deepseek-R1 [9], which expose the model to large-scale curated reasoning examples after the initial pretraining phase to refine its inferential abilities [18]. These reasoning datasets predominantly fall into two categories: (1) Mathematical reasoning: In earlier work, the construction of high-quality mathematical datasets primarily relied on increasing the quantity of problems and enhancing their difficulty levels [20, 40]. Nevertheless, LIMO dataset [37] demonstrated that complex reasoning capabilities can be elicited through surprisingly small datasets (hundreds of examples). In addition, some researchers also opted to distill high-quality reasoning data from strong LLMs [25], leveraging their outputs to construct more targeted and informative training sets for enhancing reasoning performance in weak LLMs. (2) Code generation: As highly structured and formalized type of data, code has non-negligible impact on the development of reasoning abilities in large language models. Beyond simply testing LLMs on newly coding test cases [16], many efforts have focused on investigating how and when code data influences the development of reasoning abilities in language models [41, 21]. In our work, we consider mathematical capacity and coding ability as two distinct manifestations of advanced reasoning, and we aim to analyze and understand the interactions between these capabilities to gain deeper insights into the underlying mechanisms of LLM reasoning. Data attribution and influence functions. Training Data Attribution (TDA) methods seek to interpret models predictions by analyzing the particular training instances that contributed to shaping its learned representations. Most modern TDA methods can broadly be divided into two categories: retraining-based methods [22, 31, 14] and gradient-based methods [38, 27, 17]. However, applying traditional data attribution methods to large language models has remained significant challenge, primarily due to issues of computational tractability and the sheer scale of model parameters. Nonetheless, there are several works successfully apply data attribution on LLMs by influence function. Researchers in Anthropic adapt EK-FAC influence functions to large-scale Transformers, by which they figured out what kind of pretraining data influences completions of models up to 50B parameters [8]. More specifically, for reasoning capabilities, studies have shown that code data encountered during the pretraining-phase plays critical role in the development of mathematical reasoning abilities in language models. [30]. In this work, we extend similar methodological approaches by employing influence functions to attribute the development of reasoning capabilities during the supervised fine-tuning (SFT) phase, with particular focus on analyzing the interplay between code and mathematical data."
        },
        {
            "title": "3 Methodology",
            "content": "This section reviews the basics of influence functions [17, 8] and presents Infra, our adaptation for attributing LLM reasoning performance on math and code problems. In particular, we compute instance-level influence scores using mean log-likelihood proxy, and further shift to the sequence and token levels to uncover how specific reasoning steps and tokens shape model behavior. 3.1 Preliminary: Influence Functions Given model parameterized by θ and trained on dataset Dtrain = {zi}N i=1, influence functions [17] estimate the influence of training point zm on θ (or function thereof) without retraining the model. Specifically, it is measured by computing the change in θ if zm is upweighted by an infinitesimal amount ϵ. This perturbation can be formalized as the response function3: θ(ϵ) = arg min θRD (θ, Dtrain, ϵ) = arg min θRD 1 (cid:88) i=1 (zi, θ) + ϵL (zm, θ) , (1) where L() is the training loss. The influence of zm on θ is then defined as the first-order Taylor approximation to the response function around ϵ = 0 and can be computed using the implicit theorem: Iθ(zm) = dθ dϵ (cid:12) (cid:12) (cid:12) (cid:12)ϵ=0 = H1θL (zm, θ) , (2) 3For simplicity, we show the response function for optimal parameters. For non-converged or non-convex models, the actual response function is the Proximal Bregman response function (refer to [8] for details). 3 where = 2 θJ (θ, Dtrain) is the Hessian of the cost function. Direct interpretation of Iθ(zm) can be difficult due to its high dimensionality, so it is common to instead compute the influence of zm on scalar-valued function of the parameters (θ). Using the chain rule for derivatives, this influence admits the closed-form: If (zm) = df (θ) dϵ (cid:12) (cid:12) (cid:12) (cid:12)ϵ=0 = θf (θ)T H1θL (zm, θ) . (3) complete derivation of Equation 3 is delayed to Appendix A. Consequently, (θ) is expected to increase after upweighting the sample zm and then retraining the model if If (zm) > 0, as (θ(ϵ)) (θ) If (zm)ϵ = θf (θ)T H1θL (zm, θ) ϵ. For transformer-based LLMs with billions of parameters, the above is intractable. To address this, Grosse et al. [8] propose to approximate using the Eigenvalue-Corrected Kronecker-Factored Approximate Curvature (EK-FAC) method [6], which introduces simplifying assumptions such as layer-wise independence and restricts computation only to the MLP parameters within the model. Given the effectiveness of such strategy, we also employ it to effectively estimate influence scores. (4) 3.2 Attributing LLM Reasoning to Training Data via Influence Functions We now introduce Infra, our adaptation of influence functions to attribute LLM reasoning on challenging math and code tasks. As mentioned, our setting is mainly an SFT process with CoTs generated by stronger model to improve the reasoning abilities of the LLM at hand. We are interested in identifying the most influential training data to improve model performance. Since task accuracy is non-differentiable with respect to θ, we instead adopt smooth surrogate: the mean log-likelihood over set of correctly answered examples. Let Dcorrect = {(xi, yi)}n i=1 denote collection of problems xi paired with correct answers yi, we define the surrogate objective as: (θ) = 1 (cid:88) i= log p(yixi; θ), (5) where is the size of Dcorrect. The robustness of Dcorrect against variation is ablated in Appendix C. Instance-level influence scores. Plugging Equation 5 into Equation 3 yields the instance-level influence score assigned to each SFT training example zm reflecting its effect on (θ). Consistent with [8], we restrict our focus to positively influential data, which refers to data points that yield an increase in the log-likelihood of correct answers and thus more effectively enhances the models reasoning performance. Sequence-level influence scores. Reasoning traces of recent models often exhibit sequence-level cognitive behaviors, such as verification or exploration (refer to Figure 6). To attribute the contribution of an individual sentence in zm, we employ simple counterfactual tactic: we remove from the example and measure how the influence scores changes. Let zy denote the input with sentence erased. Then the sequence-level influence of is given by If (y) = If (zm) If (zy ), (6) which isolates the influence of on the target function (θ). Token-level influence scores. Tokens that mark critical transitionssuch as waitfrequently appear in long CoT. Attributing influence at the token level may therefore help elucidate the underlying mechanisms that guide the models reasoning. Due to the autoregressive nature of LLMs, the training gradient of training sequence zm of length decomposes as: θL (zm, θ) = (cid:88) t= θ log p(zm,tzm,<t, θ), (7) where zm,t denotes the t-th token and zm,<t = {zm,1, . . . , zm,t1}. Plugging this into Equation 3 yields the token-level influence of zm,t:4 If (zm,t) = θf (θ)T H1θ log p(zm,tzm,<t, θ). (8) 4This term captures the influence of zm,t as the output for the model to fit, ignoring its role as input in other cases, for simplicity. Figure 2: Cross-domain influence analysis of LLaMA3-8B-Base fine-tuned on combined MetaMathQA and OSS-Instruct for math and code performance. The most beneficial examples for math performance predominantly come from the math domain, while code-domain data also contributes non-trivially (left). similar cross-domain benefit is observed for code performance (right)."
        },
        {
            "title": "4 Experiments",
            "content": "We begin by detailing the experimental setup (4.1), and then present the main findings, progressing from coarseto fine-grained analyses (4.24.4). 4.1 Experimental Setup We conduct experiments under two SFT settings and interpret math and code reasoning behaviors using influence functions in both scenarios. All training and influence scores computation are carried out on servers equipped with 8 NVIDIA A100 80GB GPUs. Base models trained w/o long CoT. We fine-tune the Llama3-8B-Base model [7] using mixed training dataset comprising MetaMathQA-100k [39] and OSS-Instruct-75k [33]. MetaMathQA-100k includes reformulated questions bootstrapped from training splits of GSM8k [3] and MATH [11] paired with brief answers (100 tokens) generated from GPT-3.5-Turbo [26]. OSS-Instruct-75k provides synthetically generated instructions covering range of coding tasks. We evaluate the resulting model on the test splits of GSM8k and MBPP [1], filtering correctly answered data to compute influence scores. The MBPP benchmark consists of 1,000 Python programming problems, each comprising task description and three automated test cases. Instruction-tuned models trained w/ long CoT. We fine-tune the Qwen2.5-7B-Instruct model [36] on the Bespoke-Stratos-17k reasoning dataset5 (BS-17k), which includes SFT distillation data from DeepSeek-R1 [9], comprising questions, reasoning traces, and answers. We employ the AIME24, AIME25, MATH500, and LiveCodeBench [16] benchmarks to evaluate reasoning performance. AIME is prestigious high school mathematics competition known for its challenging problems. MATH500 is subset of 500 problems drawn from the MATH [11] benchmark. LiveCodeBench evaluates LLMs on diverse coding tasks, including self-repair, code execution, and test output prediction, and currently hosts 400 coding problems. Influence scores computation. We estimate the Hessian using EK-FAC on the full SFT training set, truncating sequences to 4096 tokens to reduce memory usage. We set = 100 in Equation 5 by randomly sampling correctly answered math and code examples. 4.2 Instance-level Attribution Finding 1: Code data can positively influence math performance, and vice versa. 5https://huggingface.co/datasets/bespokelabs/Bespoke-Stratos-17k Figure 3: Average influence score of the training dataset combining MetaMathQA and OSS-Instruct, evaluated on MBPP and GSM8K performance. Results are grouped by training data category (left) and MATH problem difficulty (right). To investigate cross-domain influence after fine-tuning LLaMA3-8B-Base on MetaMathQA and OSS-Instruct, we rank training samples based on their positive influence on the mean likelihood of correct answers in math and coding tasks, respectively, and categorize them by domain. As shown in Figure 2 (a), the most influential samples for improving math performance predominantly originate from the math domain. However, influence scores from code-domain data are not narrowly concentrated in the low range (010); instead, substantial number exhibit scores in the 1520 range, indicating non-trivial contribution from code to math. similar pattern of cross-domain benefit is observed in Figure 2 (b). This also holds in long CoT reasoning scenarios as shown in Appendix B. To investigate how various training data types influence code reasoning, we further aggregate training samples by category and compute average influence scores per category. As illustrated in Figure 3 (a), in-domain Python data yields the highest average influence on MBPP (a benchmark of 1,000 Python problems). Within the math domain, symbolic problem-answer pairssuch as those introducing variables in FOBAR and SV formats shown in Figure 4most effectively enhance coding capabilities. Moreover, college-level math questions from the MATH dataset, which utilize LaTeX-based formal expressions, contribute more positively to code performance than simpler, conversational high-school problems from GSM8k. This suggests that, beyond domain relevance, the complexity and formality of the dataespecially the use of precise symbolic languagealso play critical role in enabling models to generalize effectively to code reasoning tasks. Figure 4: Different types of MATH questions from MetaMathQA [39] dataset. Finding 2: Challenging math problems exhibit higher influence scores on both math and code reasoning, while simpler code problems more effectively enhance code tasks when combined with math data. The optimal strategy for co-optimizing reasoning across both domains is to mix challenging math problems with easier code tasks. To examine how training data difficulty contributes to model performance, we first categorize MATH training data into different difficulty levels and compute the average influence score for each level. As shown in Figure 3(b), higher-difficulty problems (Level 5 and 4) contribute more significantly to performance improvements on GSM8k and MBPP compared to lower-difficulty ones (Level 3, 2, and 1). This may be attributed to the fact that high-difficulty MATH problems induce more complex reasoning chains and thus better transfer logical capabilities to other reasoning-intensive tasks. 6 Figure 5: Left: Average influence scores of math and code training data from varying difficulty levels on reasoning performance. For instance, MathCode denotes the influence of math data on code reasoning tasks. Right: Distribution of math and code samples across difficulty levels in the BS17k dataset. The original distribution is shown alongside the adjusted distribution obtained via the difficulty-flip strategy. See Table 1 for comparison of SFT results under different mixing strategies. Table 1: Comparisons of SFT results with different difficulty-mixing strategies applied to the training dataset on 7B and 14B models. We report pass@1 accuracy of LiveCodeBench. Model AIME24 AIME25 MATH500 LiveCodeBench Qwen2.5-Instruct-7B Bespoke-Stratos-17k Difficulty-reverse-Flipped Difficulty-Flipped (Ours) Qwen2.5-Instruct-14B Bespoke-Stratos-17k Difficulty-reverse-Flipped Difficulty-Flipped (Ours) 10.0 13.0 20.0 20.0 20.0 23.0 6.7 10.0 16.7 13.3 23.3 23. 77.2 76.4 78.2 84.4 83.0 84.4 33.8 30.0 35.3 45.3 43.8 45.5 To further investigate the role of difficulty in long CoT reasoning scenarios, we fine-tune Qwen2.57B-Instruct on the BS17k dataset and analyze influence scores grouped by difficulty levels. The results, shown in Figure 5(a), indicate that challenging tasks in both mathematics and coding are more beneficial for math reasoning. In contrast, easier math problems offer limited gains across both math and coding evaluations. This observation aligns with findings from the w/o long CoT setting and prior works such as LIMO [37], which highlight the utility of difficult math problems in developing reasoning capabilities. On the other hand, we find that simpler code problems are more effective for improving performance on coding tasks when mixed with math data. We hypothesize that, in addition to logical reasoning, programming tasks rely heavily on learning structural and syntactic patterns. When paired with math data that enhances logical thinking, simple coding tasks with clearer structure and more consistent syntax facilitate the models acquisition of fundamental programming patterns, thereby improving code generation performance. Based on these insights, we design an optimized data mixing strategy: we replace simple math problems in the original dataset with more challenging ones sourced from larger scale OpenThoughts114k6 dataset, and conversely, we replace difficult coding problems with simpler ones. The modified dataset retaining the original size of 17k examples, compared in Figure 5(b), is used to retrain the model. As shown in Table 1, this new difficulty flipped mixing strategy yields consistent improvements across AIME, MATH, and LiveCodeBench benchmarks. In contrast, applying the reverse strategysimplifying hard math problems and complicating easy coding tasksresults in the worst performance, further validating our finding. 6https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k. Note that this dataset is curated using the same pipeline as BS-17k, with identical question sources and answers distilled from Deepseek-R1. 7 Figure 6: Left: An example of long CoT illustrating cognitive behaviors: verification (systematic error-checking) and exploration (searching for another approach after reaching the correct answer). Right: Distribution of different cognitive behaviors in BS-17k training dataset and their average impact on math and code reasoning performance. Table 2: Sequence-level attribution of cognitive behaviors in long CoT. Left: Comparison of influence scores of the example in Figure 6 on math and code reasoning, w/ and w/o verification and exploration sentences. Right: Comparison of SFT results w/ and w/o exploration behaviors in BS-17k dataset. Domain full CoT w/o Ver. w/o Exp. w/o both Model MATH500 LiveCodeBench Math Code 2.2e+08 2.2e+07 1.5e+08 1.6e+ 9.0e+07 8.4e+06 7.0e+07 7.5e+06 w/ Exp. w/o Exp. 77.2 73.8 33.8 32.0 4.3 Sequence-level Attribution Finding 3: The presence of searching for another approach after reaching correct answers in math reasoning traces benefits to both math and code reasoning. While previously considered unnecessary overthinking, our sequence-level influence analysis and SFT ablations demonstrate its positive impact, suggesting such exploratory behaviour may promote generalizable reasoning skills. We are interested in the influence of different cognitive behaviors on reasoning performance. Following prior work [4], we focus on five key behaviors: exploration (seeking alternative approaches after reaching correct answer), verification (systematic error-checking), backtracking (abandoning ineffective strategies), subgoal setting (breaking problems into manageable steps), and backward chaining (reasoning from desired outcomes to initial inputs). To identify these behaviors in the BS-17k dataset, we use Qwen-32B-instruct as classifier, with details provided in Appendix E. As shown in Figure 6 (right), exploration behavior is notably prevalent in the training dataset. However, prior research often views exploration as detrimental, considering it form of overthinking that can reduce efficiency [2, 32]. We therefore seek to assess whether this cognitive behavior has positive or negative impact using influence functions. Specifically, we truncate sentences in the training data where exploration behavior is present and examine the change in the influence score, as defined in Equation 6. The results, presented in Table 2 (left), show that exploration behavior is not redundant; on the contrary, it has positive effect on both math and code reasoning performance, with the positive impact of exploration even exceeding that of verification. To further validate this, we use GPT-4o to truncate all exploration behaviors in the BS-17k dataset for SFT, with the instruction details shown in Appendix D. The SFT results in Table 2 (right) show significant performance drop when exploration behavior is removed. We attribute this decline to explorations role in enabling flexible problem-solving, essential for adapting to diverse reasoning tasks. Beyond exploration, we compare average influence scores across other cognitive behaviors. As shown in Figure 6 (right), backtracking is crucial for mathematical reasoning, while subgoal setting Figure 7: Left: Visualization of top 5% influential tokens in math CoT. Right: Visualization of top 5% influential tokens in code CoT. is more impactful in programming tasks. This may be because programming requires breaking down high-level goals into modular components, making subgoal setting essential. 4.4 Token-level Attribution Finding 4: Token-wise attribution analysis reveals distinct paradigms in math and code reasoning. In math CoT, influential tokens are natural language with logical connectors, whereas code CoT are dominated by structured code with syntax markers. To investigate the most influential tokens for stimulating reasoning, we select the top 100 highly influential examples on math and code reasoning, compute token-wise influence scores using Equation 8, and highlight the top 5% most influential tokens. Interestingly, as shown in Figure 7, the initial tokens in CoTsuch as Okay, so I...are frequently highlighted, suggesting that these openers help orient the models cognitive process to initiate reasoning. Further analysis reveals that, in math CoTs, the influential tokens are predominantly natural language logical connectors, such as Wait, However, Verify, Hence, First, Therefore, and Alternatively. In contrast, in code CoTs, the most influential tokens are structural or syntactic elements such as markdown-style headings (e.g., ### Solution), fenced code blocks (e.g., ``` bash``` ), and syntax markers (e.g., def (self, A: List [int])-> int: ), which reflect the highly structured nature of code reasoning. This contrast highlights divergence in reasoning paradigms: math reasoning relies more heavily on logical discourse, while code reasoning is facilitated by explicit structure and formatting. These divergent patterns may explain why easier code problems with clearer structural formats are particularly beneficial for enhancing code reasoning when integrated with math CoTs that already provide strong logical skills."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose fine-grained influence function framework to trace how training data on SFT phase shapes LLM reasoning in math and code tasks. Our analysis reveals that cross-domain examplesespecially high-difficulty math and low-difficulty codeboost reasoning performance across domains. We further extend influence functions to the sequence level, revealing that exploratory behaviors in long CoT consistently enhance performance, challenging prior assumptions that such behaviors reflect overthinking. Token-level analysis reveals distinct paradigms in math and code reasoning. Our work highlights the utility of influence-based attribution for data-centric optimization and opens path toward more targeted and interpretable reasoning supervised training. Limitations. The main limitations of this work are as follows. We approximate the Hessian by considering only the MLP parameters and treating the attention as fixed to approximate influence functions for simplicity. Besides, our analysis is limited to mathematical and coding reasoning tasks; extending this framework to other domains, such as commonsense reasoning, remains an open direction for future work."
        },
        {
            "title": "References",
            "content": "[1] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [2] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. [3] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [4] Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. [5] Zitian Gao, Boye Niu, Xuzheng He, Haotian Xu, Hongzhang Liu, Aiwei Liu, Xuming Hu, and Lijie Wen. Interpretable contrastive monte carlo tree search reasoning. arXiv preprint arXiv:2410.01707, 2024. [6] Thomas George, César Laurent, Xavier Bouthillier, Nicolas Ballas, and Pascal Vincent. Fast approximate natural gradient descent in kronecker factored eigenbasis. Advances in neural information processing systems, 31, 2018. [7] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [8] Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, and Ethan Perez. Studying large language model generalization with influence functions. arXiv preprint arXiv:2308.03296, 2023. [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [10] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), . [11] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), . [12] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. arXiv preprint arXiv:2212.10403, 2022. [13] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024. [14] Andrew Ilyas, Sung Min Park, Logan Engstrom, Guillaume Leclerc, and Aleksander Madry. Datamodels: Understanding predictions with data and data with predictions. In International Conference on Machine Learning, pages 95259587. PMLR, 2022. [15] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 10 [16] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, 2021. [17] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International conference on machine learning, pages 18851894. PMLR, 2017. [18] Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip HS Torr, Fahad Shahbaz Khan, and Salman Khan. Llm post-training: deep dive into reasoning large language models. arXiv preprint arXiv:2502.21321, 2025. [19] Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. T\" ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. [20] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9, 2024. [21] Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, and Junxian He. Codei/o: Condensing reasoning patterns via code input-output prediction. arXiv preprint arXiv:2502.07316, 2025. [22] Robert Ling. Residuals and influence in regression, 1984. [23] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, et al. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413, 2024. [24] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. [25] NovaSky. Sky-t1: Train your own o1 preview model, 2024. URL https://novasky-ai. github.io/posts/sky-t1. [26] OpenAI. Gpt-3.5-turbo. Technical report, OpenAI, 2022. [27] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating training data influence by tracing gradient descent. Advances in Neural Information Processing Systems, 33: 1992019930, 2020. [28] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. Reasoning with language model prompting: survey. arXiv preprint arXiv:2212.09597, 2022. [29] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, et al. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982, 2024. [30] Laura Ruis, Maximilian Mozes, Juhan Bae, Siddhartha Rao Kamalakara, Dwarak Talupuru, Acyr Locatelli, Robert Kirk, Tim Rocktäschel, Edward Grefenstette, and Max Bartolo. Procedural knowledge in pretraining drives reasoning in large language models. arXiv preprint arXiv:2411.12580, 2024. [31] Lloyd Shapley et al. value for n-person games. 1953. [32] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. 11 [33] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code generation with oss-instruct. Proceedings of Machine Learning Research, 235: 5263252657, 2024. [34] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. [35] Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, et al. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-though. arXiv preprint arXiv:2501.04682, 2025. [36] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [37] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [38] Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep Ravikumar. Representer point selection for explaining deep neural networks. Advances in neural information processing systems, 31, 2018. [39] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [40] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. [41] Xinlu Zhang, Zhiyu Zoey Chen, Xi Ye, Xianjun Yang, Lichang Chen, William Yang Wang, and Linda Ruth Petzold. Unveiling the impact of coding data instruction fine-tuning on large language models reasoning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2594925957, 2025."
        },
        {
            "title": "A Derivation of Influence Score",
            "content": "Given the influence of zm on model parameters θ Iθ(zm) = dθ dϵ (cid:12) (cid:12) (cid:12) (cid:12)ϵ=0 = H1θL (zm, θ) , (9) we can obtain its influence on function of parameters (θ) by applying the chain rule for derivatives: If (zm) = (cid:12) (cid:12) (cid:12) (cid:12)ϵ=0 df (θ) dϵ (cid:12) = θf (θ)T dθ (cid:12) (cid:12) dϵ (cid:12)ϵ=0 = θf (θ)T H1θL(cid:0)zm, θ(cid:1). (10)"
        },
        {
            "title": "B Cross Domain Influence Analysis in Long CoT Scenarios",
            "content": "In this section, we provide additional instance-level attribution experiment on long CoT reasoning scenarios. We fine-tune Qwen2.5-7B-Instruct on Bespoke-Stratos-17K reasoning dataset. As shown in Figure 8(a), the most influential samples for improving math performance predominantly from the math domain, but the samples from code domain are also significant. In Figure 8(b), there is similar pattern of cross-domain benefit. This is consistent with the conclusions we obtained in the experimental section 4.2. 12 Figure 8: Cross-domain influence analysis of Qwen2.5-7B-Instruct fine-tuned on Bespoke-Stratos17k dataset for math and code reasoning performance. Figure 9: To assess whether the exploration behavior has positive or negative impact, we use GPT-4o to truncate all exploration behaviors in the BS-17K dataset for SFT. If reasoning contains any searching for another approach after reaching correct answers, like \"Alternatively, maybe theres different way to approach the problem?\", the exploration content will be truncated."
        },
        {
            "title": "C Robustness on n",
            "content": "In this section, we evaluate the robustness of the influence function estimates with respect to the size of the correct subset Dcorrect. Specifically, we fine-tune the LLaMA3-8B-Base model on mixed training corpus comprising MetaMathQA and OSS-Instruct, and compute influence scores on the math and code performance. We calculate the Pearson correlation between the rankings of training examples induced by influence scores using varying values of n, using = 100 as the reference. Results in Table 3 shows the robustness of for influence scores estimation. Table 3: Pearson correlation coefficient of rankings on training data across different choices of n, indicating stable influence estimation. 10 Math Code 0.52 0.51 0.60 0.62 50 0.70 0.60 100 1.0 1."
        },
        {
            "title": "D Case of Truncating Exploration Behavior",
            "content": "To evaluate the impact of exploration behaviors in reasoning processes, we systematically truncate exploratory content from the BS-17K during SFT. Specifically, any post-correct-answer exploration (e.g., \"Alternatively, maybe theres different way to approach the problem?\") is removed to isolate the core problem-solving trajectory, as shown in Figure 9. 13 Figure 10: Left: Exploration: When performing reasoning, seeking alternative approaches after reaching correct answer. We capture this behavior and calculate the number of exploration steps by analyzing the content like \"Another way to look at this is...\" etc.. Right: Verification: The behavior of reasoning from desired outcomes to initial inputs when performing reasoning. We capture and calculate the number of backward chaining instances by finding the content like \"To solve this equation, lets start with what we want to prove\" etc.."
        },
        {
            "title": "E Examples for Reasoning Behaviors Classifier",
            "content": "The five cases below show the prompts of five behaviors on reasoning performance and the corresponding answers. As shown in Figure 10 and 11, the prompts include task description, examples of each reasoning behavior, task format, etc.. 14 Figure 11: Left: Backtracking: The behavior of realizing path wont work and explicitly going back to try different approach. We capture this behavior and calculate the number of backtracking steps by finding the content like \"This approach leads to contradiction. Going back to the original equation...\" etc.. Right: Backward Chaining: The behavior of systematic error-checking when performing reasoning. We capture and calculate the number of backward chaining instances by finding the content like \"To ensure this solution is valid, Ill check if it satisfies all the given constraints.\" etc.."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University"
    ]
}