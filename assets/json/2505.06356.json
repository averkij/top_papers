{
    "paper_title": "Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA",
    "authors": [
        "Karthik Reddy Kanjula",
        "Surya Guthikonda",
        "Nahid Alam",
        "Shayekh Bin Islam"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmful content manifests in different modalities. We present a comprehensive analysis of common toxicity categories and propose targeted mitigation strategies, resulting in the creation of a refined toxicity-mitigated dataset. This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training dataset. We offer guidelines for implementing robust toxicity detection pipelines. Our findings underscore the need to actively identify and filter toxic content - such as hate speech, explicit imagery, and targeted harassment - to build more responsible and equitable multimodal systems. The toxicity-mitigated dataset is open source and is available for further research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 6 5 3 6 0 . 5 0 5 2 : r Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: Case Study on LLaVA Karthik Reddy Kanjula*1, Surya Guthikonda*3,1, Nahid Alam2,1, Shayekh Bin Islam 4,1 1Cohere for AI Community, 2Cisco Meraki, 3Indiana University Bloomington, 4Bangladesh University of Engineering and Technology karthikreddykanjula99@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmful content manifests in different modalities. We present comprehensive analysis of common toxicity categories and propose targeted mitigation strategies, resulting in the creation of reﬁned toxicity-mitigated dataset. This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training dataset. We offer guidelines for implementing robust toxicity detection pipelines. Our ﬁndings underscore the need to actively identify and ﬁlter toxic content - such as hate speech, explicit imagery, and targeted harassment - to build more responsible and equitable multimodal systems. The toxicity-mitigated dataset is open source and is available for further research. 1. Introduction Vision Language Models (VLMs) have emerged as key paradigm in artiﬁcial intelligence, enabling machines to understand and reason about the visual world through natural language. Recent advances in large language models (LLMs) and general-purpose image encoders such as CLIP [19] and SigLIP [27] have signiﬁcantly boosted VLM capabilities. Architectures such as Flamingo [1], LLaVA [13, 14], KOSMOS [17, 18], Florence-2 [25] and Molmo [7] demonstrate strong performance across tasks including image captioning, Visual Question Answering (VQA), and complex reasoning. Qwen2-VL [24] introduced Multimodal Rotary Position Embedding (M-RoPE) [23] and dynamic resolution techniques, while PaLIs joint modal- *Equal contribution. Work does not belong to position referred in 2. ity scaling and cross-lingual learning [4, 5] have furthered vision-language understanding. LLaVA [14] architecture demonstrates mechanisms for processing visual tokens, enabling it to develop interpretable representations of visual content that align with textual semantics [16]. Despite these advances, there is limited research in understanding the toxic content of the image-text pretraining dataset. VLMs are trained on billions of image-text pairs from diverse sources, such as LAION-5B [21] with 5.85 billion CLIP-ﬁltered samples. While large-scale, web-scraped datasets enable robust training, they often contain toxic content that harms model performance and introduces ethical risks [22]. Models like LLaVA [14], which rely on these datasets, can inherit toxic or biased material, posing challenges for safe, responsible AI development. Current VLM datasets can contain toxic and culturally insensitive content [26], including hate speech, explicit violence, sexual material, harmful stereotypes, and discriminatory representations. This content may appear in both visual and textual descriptions [3]. Recent jailbreaking techniques [10] underscore the urgency of mitigating these risks. These issues highlight the need for careful examination to ensure responsible and ethical deployment of vision-language models. In this paper, we describe multimodal approach to detect and remove harmful content from the LLaVA [14] pretrain dataset. By combining multiple ﬁlters with large pretrained VLMs, we remove harmful visual and textual content while preserving the datasets richness and diversity. This approach produces toxicity-mitigated dataset that minimizes societal risks and promotes responsible AI development. Our key contributions include: 1. Applied Toxic-BERT to ﬂag 892 image captions as toxic with over 80% conﬁdence. 2. Combined ﬁndings from LlavaGuard [9] and Command R+ [6] (7,111 images) with Toxic-BERTs [8] results (892 images) to identify total of 7,531 unique toxic images. 3. In total, we removed the 7,531 toxic images from the pretraining dataset, resulting in toxicity-mitigated version for LLaVA pretraining 2. Related Work In recent years, the LLaVA architecture and its associated datasets [13, 14] have been widely adopted VLM tasks. However, to the best of our knowledge, the presence of toxic content in the LLaVA [14] pretraining dataset has not yet been systematically analyzed. In the context of VLMs, toxicity refers to harmful or offensive content that may emerge from biases or aggressive language present in the training data. Since these models are often trained on large-scale datasets that can contain historical prejudices or explicit material, such toxicity may surface in unpredictable and concerning ways. 2.1. Toxicity Filtering in Dataset SPA-VL [28] proposes Safety Preference Alignment dataset for Vision Language Models named SPA-VL. In terms of breadth, SPA-VL covers 6 harmfulness domains, 13 categories, and 53 subcategories, and contains 100,788 image, chosen resamples of the quadruple (question, sponse, rejected response). ELITE [12] shows that current VLMs can incorporate self-ﬁltering mechanisms to remove toxic content from training datasets. Chen et al. [2] propose self-ﬁltering method. The authors demonstrate that VLMs themselves can serve as ﬁlter for instruction-ﬁnetuning. [22] introduce PixelProse. To ensure data Singla et al. integrity, the authors rigorously analyze the dataset for problematic content, including child sexual abuse material (CSAM), personally identiﬁable information (PII), and toxicity using Google APIs. 2.2. Safety Alignment In Model Architecture Zhao et al. [29] propose harmful content mitigation techniques without relying on pre-ﬁltering or ﬁne-tuning techniques that incur higher cost. Instead, they introduce SafeCLIP, lightweight method that leverages LVLMs inherent multimodal alignment for zero-shot toxic image detection. By projecting CLIPs discarded CLS token into its text space and matching it with toxic descriptors, SafeCLIP detects harmful content without any architectural changes-adding minimal latency and enabling dynamic safety corrections during inference. Zou et al. [30] demonstrate that the inclusion of visual modality shifts activations toward safer direction, which is key factor contributing to the degradation of safety alignment. The authors propose ShiftDC, method for disentangling and calibrating VLM activations to restore safety alignment. 2.3. Evaluating Toxicity MM-SafetyBench [15] shows that VLMs safety can be breached if the input image contains relevant harmful images related to the text prompt. To address this, MMSafetyBench [15] proposes framework designed to conduct safety-critical evaluations of VLMs against such image-based manipulations. VHELM [11] proposes comprehensive framework for evaluating VLMs on biases, fairness, safety and toxicity. ELITE [12] evaluator explicitly incorporates toxicity score to accurately assess harmfulness in multimodal contexts, where VLMs often provide speciﬁc, convincing, but unharmful descriptions of images. ELITE ﬁlters out ambiguous and low-quality image-text pairs from existing benchmarks using the ELITE evaluator and generates diverse combinations of safe and unsafe image-text pairs. 3. Methodology 3.1. Dataset Toxicity Analysis LLaVA Pretrain dataset has 558,000 image-caption pairs in English language. Upon manually identifying the types of images present in the LLaVA Pretrain dataset by cherry picking, we discovered some toxic images. This led us to design systematic approach for detecting and mitigating harmful content. To methodically identify and ﬁlter the toxic content that exists in both modalities, we used two different models. LlavaGuard 7B [9] for images and ToxicBERT [8] for text. Our choice was guided by their specialized capabilities and performance in detecting harmful content within their respective modalities. LlavaGuard 7B was selected for its ability to identify and classify toxic visuals based on predeﬁned safety guidelines. This ensures comprehensive coverage of harmful visual elements. We chose Toxic-BERT for text because of its strong ability to identify offensive and harmful language through contextual analysis, which outperforms conventional keyword-based ﬁltering methods. These models provided reliable foundation for systematically identifying, understanding, and mitigating toxicity within the LLaVA Pretrain dataset. The LLaVA Guard safety taxonomy presents guideline for assessing visual content safety across nine distinct policy categories: Hate/Harassment (O1), Violence (O2), Sexual Content (O3), Nudity (O4), Criminal Planning (O5), Weapons/Substance Abuse (O6), Self-Harm (O7), Animal Cruelty (O8), and Disasters/Emergencies (O9). Each individual category interprets prohibited content (such as hateful speech, explicit nudity, or content promoting violence), also allowing for educational, informational, or artistic use cases within appropriate contexts. This assessment process requires evaluators to rate content as Safe or Unsafe, assign the relevant policy category, and provide detailed rationale on why speciﬁc policy description is chosen. O9: Disasters or Emergencies O1: Hate, 3.1% Humiliation, Harassment O3: Sexual O2: Violence, O1: Hate, Humiliation, Harassment (5.2%) Content Harm, or 33.1% O2: Violence, Harm, or Cruelty (16.3%) 16.3% Cruelty 5.2% Categories O3: Sexual Content (33.1%) O4: Nudity Content (19.5%) O5: Criminal Planning (0.5%) O6: Weapons or Substance Abuse (22.0%) 19.5% O4: Nudity Content O7: Self-Harm (0.0%) O8: Animal Cruelty (0.3%) O9: Disasters or Emergencies (3.1%) 22.0% O6: Weapons or Substance Abuse Figure 1. Image Toxicity Analysis on LLaVA Pre-train Dataset using LlavaGuard > 0.8: 892 > 0.8: 425 > 0.8: > 0.8: 185 > 0.8: 29 1.0 0.8 ) ( c 0.6 e n 0.4 0.2 0.0 toxicity obscene threat insult identity_attack Toxicity Type Figure 2. Image Caption Toxicity Analysis on LLaVA Pre-train Dataset using Toxic-BERT This process creates structured approach to identify and ﬁlter potentially harmful content in LLaVA pretrain image datasets, as visualized in the toxicity analysis chart (Figure 1). In addition, prompt optimization pipeline was introduced to ensure precise ﬁltering - capturing potentially harmful content while reducing false positives. 3.2. Dataset Toxicity Filtering The overall process for creating the toxicity-mitigated dataset is shown in Figure 3. LlavaGuard output provides rating, category, and rationale explaining why an image violates the guidelines. We then reﬁne this by identifying genuinely toxic images. This is done by developing an optimized prompt using the Cohere prompt tuner1. We use this prompt as preamble (System Prompt) and pair it with the LlavaGuard output of each image ﬂagged as unsafe. Command R+ [6] then analyzes these results to identify the truly unsafe image IDs. In our analysis, LlavaGuard identiﬁed 7,600 images as toxic and the ﬁnal Command R+ output identiﬁed 7,111 images as unsafe. To detect harmful language in image captions, we utilized Toxic-BERT, model ﬁne-tuned on the Jigsaw Toxic 1https://docs.cohere.com/v2/docs/prompt-tuner Figure 3. Dataset Toxicity Filtering Method Comment Classiﬁcation Challenge dataset. This model is adept at identifying various forms of toxicity, including threats, obscenity, and identity-based hate. Its contextual understanding allows for nuanced detection beyond simple keyword matching, effectively capturing offensive language that might be context-dependent. Applying Toxic-BERT to our dataset, we identiﬁed 892 image captions as toxic with conﬁdence level that exceeds 80%, as depicted in Figure 2. LlavaGuard and Command R+ identiﬁed 7,111 images and Toxic-BERT identiﬁed 892 images; there are in total 7,531 unique toxic images. We then removed those 7,531 images from the pretraining dataset to create the toxicity-mitigated pre-train dataset. 4. Results In this work, we create toxicity-mitigated version of the LLaVA image-text pre-training dataset. We applied ToxicBERT to ﬂag 892 image captions as toxic with over 80% conﬁdence. We then combined the ﬁndings of LlavaGuard and Command R+ (7,111 images) with Toxic-BERTs results (892 images) to identify total of 7,531 unique toxic images. In total, we removed 7,531 toxic image-text pairs from the pretraining dataset, resulting in toxicitymitigated version for LLaVA pretraining. This data set is open source and is available to everyone for further research. 5. Future Work Our research in multimodal safety is advanced through dataset toxicity mitigation. It would be interesting to verify the accuracy of the toxicity-mitigated dataset by applying user evaluation process or through other toxicitymitigation pipelines. In future we want to implement safety at instruction tuning and alignment techniques using SPAVLs safety preference alignment [28] during instruction tuningwith its domain-speciﬁc harmfulness taxonomies and automated labelingto better deﬁne safety boundaries; adopting SafeCLIP [29] inspired dynamic safety projection for efﬁcient, real-time toxic detection; and employing capability-preserving model merging from Howard et al. [20] to sustain reasoning and visual performance. 6. Conclusion In this work, we introduced fully open-source, toxicitymitigated version of the LLaVA image-text pretraining dataset, offering valuable resource for the broader research community. Future evaluations will leverage established benchmarks such as MM-SafetyBench [15], VHELM [11], and ELITE [12] to assess performance, robustness, and safety. We hope this effort encourages researchers to prioritize the qualitative aspects of dataparticularly toxic or harmful contentin their endeavors, ultimately fostering more responsible and ethical AI development."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: Visual Language Model for Few-Shot Learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 1 [2] Ruibo Chen, Yihan Wu, Lichang Chen, Guodong Liu, Qi He, Tianyi Xiong, Chenxi Liu, Junfeng Guo, and Heng Huang. Your vision-language model itself is strong ﬁlter: Towards high-quality instruction tuning with data selection, 2024. 2 [3] Wei Chen, Lin Li, Yongqi Yang, Bin Wen, Fan Yang, Tingting Gao, Yu Wu, and Long Chen. Comm: coherent interleaved image-text dataset for multimodal understanding and generation, 2024. 1 [4] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. PaLI: JointlyScaled Multilingual Language-Image Model. arXiv preprint arXiv:2209.06794, 2022. 1 [5] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, et al. PaLI-X: On Scaling up Multilingual Vision and Language Model. arXiv preprint arXiv:2305.18565, 2023. 1 [6] Cohere. Command R. https://cohere.com/command, 2024. 1, 3 [7] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and PixMo: Open Weights and Open Data for State-of-theArt Multimodal Models. arXiv preprint arXiv:2409.17146, 2024. 1 [8] Laura Hanu and Unitary team. Detoxify. https://github.com/unitaryai/detoxify, 2020. 1, 2 Github. [9] Lukas Helff, Felix Friedrich, Manuel Brack, Kristian Kersting, and Patrick Schramowski. LLavaGuard: VLM-based Safeguards for Vision Dataset Curation and Safety Assessment. arXiv preprint arXiv:2406.05113, 2024. 1, [10] Haibo Jin, Leyang Hu, Xinuo Li, Peiyan Zhang, Chonghan Chen, Jun Zhuang, and Haohan Wang. Jailbreakzoo: Survey, landscapes, and horizons in jailbreaking large language and vision-language models, 2024. 1 [11] Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Somerville Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, and Percy Liang. Vhelm: holistic evaluation of vision language models, 2024. 2, 4 [12] Wonjun Lee, Doehyeon Lee, Eugene Choi, Sangyoon Yu, Ashkan Yousefpour, Haon Park, Bumsub Ham, and Suhyun Kim. Elite: Enhanced language-image toxicity evaluation for safety, 2025. 2, 4 [13] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved Baselines with Visual Instruction Tuning, 2023. 1, 2 [14] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning, 2023. 1, [15] Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, and Yu Qiao. Mm-safetybench: benchmark for safety evaluation of multimodal large language models, 2024. 2, 4 [16] Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, and Fazl Barez. Towards interpreting visual information processing in vision-language models, 2024. 1 [17] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-G: Generating Images in Context with Multimodal Large Language Models. ArXiv, abs/2310.02992, 2023. 1 [18] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding Multimodal Large Language Models to the World. arXiv preprint arXiv:2306.14824, 2023. 1 [19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1 [20] Neale Ratzlaff, Man Luo, Xin Su, Vasudev Lal, and Phillip Howard. Training-free mitigation of language reasoning degradation after multimodal instruction tuning, 2024. 4 [21] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. 1 [22] Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang, Abhinav Bhatele, Gowthami Somepalli, and Tom Goldstein. From pixels to prose: large dataset of dense image captions, 2024. 1, 2 [23] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced Transformer with Rotary Position Embedding, 2021. [24] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv preprint arXiv:2409.12191, 2024. 1 [25] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing uniﬁed representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4818 4829, 2024. 1 [26] Xiang Yue, Yueqi Song, Akari Asai, Seungone Kim, Jean de Dieu Nyandwi, Simran Khanuja, Anjali Kantharuban, Lintang Sutawika, Sathyanarayanan Ramamoorthy, and Graham Neubig. Pangea: Fully Open Multilingual Multimodal LLM for 39 Languages. arXiv preprint arXiv:2410.16153, 2024. 1 [27] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Sigmoid Loss for Language Image PreLucas Beyer. In Proceedings of the IEEE/CVF International Training. Conference on Computer Vision, pages 1197511986, 2023. 1 [28] Yongting Zhang, Lu Chen, Guodong Zheng, Yifeng Gao, Rui Zheng, Jinlan Fu, Zhenfei Yin, Senjie Jin, Yu Qiao, Xuanjing Huang, Feng Zhao, Tao Gui, and Jing Shao. Spa-vl: comprehensive safety preference alignment dataset for vision language model, 2025. 2, 4 [29] Wei Zhao, Zhe Li, Yige Li, and Jun Sun. Zero-shot defense against toxic images via inherent multimodal alignment in lvlms, 2025. 2, [30] Xiaohan Zou, Jian Kang, George Kesidis, and Lu Lin. Understanding and rectifying safety perception distortion in vlms, 2025."
        }
    ],
    "affiliations": [
        "Bangladesh University of Engineering and Technology",
        "Cisco Meraki",
        "Cohere for AI Community",
        "Indiana University Bloomington"
    ]
}