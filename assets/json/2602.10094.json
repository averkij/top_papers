{
    "paper_title": "4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere",
    "authors": [
        "Yihang Luo",
        "Shangchen Zhou",
        "Yushi Lan",
        "Xingang Pan",
        "Chen Change Loy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks."
        },
        {
            "title": "Start",
            "content": "4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere Yihang Luo 1 Shangchen Zhou 1 Yushi Lan 2 Xingang Pan 1 Chen Change Loy 1 6 2 0 2 0 1 ] . [ 1 4 9 0 0 1 . 2 0 6 2 : r Figure 1. 4RC (pronounced ARC) enables unified and complete 4D Reconstruction via Conditional querying from monocular videos in single feed-forward pass. It jointly recovers camera poses and dense per-frame geometry, while supporting flexible querying of dense 3D motion from arbitrary source frames to any target timestamp."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction We present 4RC, unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing methods that typically decouple motion from geometry or produce limited 4D attributes, such as sparse trajectories or two-view scene flow, 4RC learns holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces novel encode-once, queryanywhere and anytime paradigm: transformer backbone encodes the entire video into compact spatio-temporal latent space, from which conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in minimally factorized form, decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across wide range of 4D reconstruction tasks. Project Page: https://yihangluo.com/projects/4RC/. 1S-Lab, Nanyang Technological University 2VGG, University of Oxford. Preprint. February 11, 2026. 3D reconstruction has seen remarkable progress over the past decades. Classical geometric pipelines such as Structure-from-Motion (SfM) (Schonberger & Frahm, 2016) and Multi-View Stereo (MVS) (Yao et al., 2018; 2019; Schonberger et al., 2016) established solid foundation. More recently, learning-based approaches, exemplified by DUSt3R-like pointmap predictor (Wang et al., 2024b; Leroy et al., 2024; Wang et al., 2025b;a;d; Lin et al., 2025; Lan et al., 2026) have enabled direct feed-forward inference of dense 3D geometry, advancing general-purpose 3D perception in terms of efficiency, scalability, and generalization. Despite this progress, existing approaches largely focus on static geometry, while real-world scenes are inherently dynamic. truly general visual perception system must therefore reason not only about 3D structure, but also about how the scene evolves over time. This motivates the task of 4D reconstruction, which aims to jointly model 3D geometry and motion. Such representation is fundamental for applications ranging from video synthesis (Gu et al., 2025; Wu et al., 2024; Lee et al., 2025b) and scene understanding to robotics (Lee et al., 2025a; Huang et al., 2026), where reasoning about object trajectories, deformations, and interactions is essential. Existing approaches to 4D reconstruction, however, remain 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere fragmented and limited in flexibility. common strategy decomposes the problem into sequential subtasks, typically separating motion estimation from 3D reconstruction. For example, SpatialTracker (Xiao et al., 2024; 2025) performs reconstruction and tracking in staged manner, relying on iterative refinement, and producing only sparse 3D trajectories. MonST3R (Zhang et al., 2025c) further requires post-hoc optimization to establish correspondences across time. Although recent feed-forward methods such as ST4RTrack (Feng et al., 2025) and Dynamic Point Map (Sucar et al., 2025) pioneer direct 4D prediction, they are restricted to pairwise views and thus struggle to model long-term and complex motion. Concurrently, TraceAnything (Liu et al., 2025) represents motion using Bezier curves, enabling long-range 3D trajectory tracking, but often at cost of reduced geometry quality. Any4D (Karhade et al., 2025) supports feed-forward 3D reconstruction, but only predicts scene flow for the first frame and is unable to model 3D motion for the remaining frames. V-DPM (Sucar et al., 2026) extends VGGT to 4D, but suffers from slow inference and limited flexibility at inference. Motivated by these limitations, we investigate whether unified, feed-forward model can enable complete and flexible 4D prediction. In this work, we propose 4RC, unified feed-forward approach for 4D reconstruction from monocular videos. Unlike previous approaches that require multiple stages, 4RC learns holistic and compact 4D representation that jointly encodes scene geometry and motion across the entire video sequence. This representation serves as centralized 4D latent from which geometry and motion can be efficiently queried and decoded. Instead of directly reconstructing full 3D point cloud for each frame at each timestamp, we adopt compact factorized output formulation. Specifically, we represent each frame with viewpointinvariant base geometry together with time-dependent relative motion, parameterized as 3D displacements. By querying the model at different timestamps, 4RC can recover both geometry and motion information, such as point trajectories between any frame and any target time. This design enables both flexible and efficient 4D reconstruction. Our contributions can be summarized as follows: unified feed-forward transformer framework for 4D reconstruction from monocular videos, which jointly models 3D geometry and motion within single network, eliminating the need for auxiliary estimators or per-scene optimization. An encode-once, query-anywhere and anytime paradigm built upon compact 4D latent representation. This allows our conditional decoder to flexibly retrieve dense 3D geometry and motion for arbitrary query frames at any target timestamp. minimally factorized 4D representation that decomposes each frame into viewpoint-invariant base geometry and time-dependent relative motion, enabling unified and flexible reconstruction of dynamic scenes. Extensive experiments demonstrate that 4RC achieves competitive performance on standard benchmarks across wide range of 3D and 4D reconstruction tasks, including camera pose estimation, video depth prediction, point cloud reconstruction, 3D point tracking, and dense motion modeling. 2. Related Work Feed-forward 3D Reconstruction. Reconstructing 3D geometry from 2D images is long-standing problem in computer vision. Traditional pipelines such as SfM (Schonberger & Frahm, 2016) and MVS (Schonberger et al., 2016; Yao et al., 2018; 2019) recover camera parameters and dense geometry through multi-stage optimization, achieving strong performance but at high computational cost. Recent work has shifted toward feed-forward 3D reconstruction, aiming to replace these complex pipelines with single neural network that directly predicts 3D attributes. DUSt3R (Wang et al., 2024b) demonstrates that dense stereo reconstruction can be achieved in one forward pass, while VGGT (Wang et al., 2025a) further unifies camera pose estimation and depth prediction across multiple views using transformer backbone. These methods highlight that, given sufficient data and model capacity, feed-forward architectures can effectively solve static 3D reconstruction. Extensions to dynamic settings, such as MonST3R (Zhang et al., 2025c), Pi3 (Wang et al., 2025d), DA3 (Lin et al., 2025) and related approaches (Wang et al., 2025b; Lan et al., 2026), jointly estimate camera parameters and per-frame geometry from dynamic data. Despite operating on dynamic scenes, these methods only reconstruct geometry for each view and thus require separate pipelines to explicitly model 3D motion or temporal correspondence. Point Tracking. Modeling motion over time has traditionally been studied through optical flow (Sun et al., 2010) and point tracking (Harley et al., 2022). Optical flow methods (Sun et al., 2018; Hui et al., 2018; Teed & Deng, 2020) estimate dense pixel-wise displacements between adjacent frames. These methods are typically limited to short temporal windows and often suffer from drift errors when applied to long video sequences (Zhou et al., 2023). To address long-range correspondence, 2D point tracking methods aim to track sparse points across entire videos. PIPs (Harley et al., 2022) introduced deep tracking framework for point tracking, followed by TAP-Net (Doersch et al., 2022), TAPIR (Doersch et al., 2023), and CoTracker (Karaev et al., 2023a), which rely on correlation-based matching and iterative updates to propagate tracks over time. These approaches operate purely in 2D and typically depend on carefully de2 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere signed matching and update mechanisms. Recent 3D point tracking approaches extend this paradigm by decoupling geometry reconstruction from motion modeling. SpatialTracker (Xiao et al., 2024), and subsequent methods (Ngo et al., 2024; Xiao et al., 2025; Zhang et al., 2025a) combine pre-trained depth estimator with lifted 2D tracking pipeline (Karaev et al., 2023a) to operate in 3D. Despite enabling 3D tracking, their multi-stage pipelines remain limited in efficiency and flexibility, and they do not learn unified spatiotemporal representation. In contrast, 4RC directly models dense geometry and motion jointly within unified feed-forward framework, without decoupled stages or tracking heuristics. 4D Reconstruction. The goal of 4D reconstruction is to recover representation that captures both the 3D structure of scene and how it evolves over time. Early methods (Wang et al., 2023a; 2024a; Lei et al., 2024; Wang et al., 2025c) typically formulate this problem as test-time optimization, which can produce high-quality results but requires costly per-scene optimization. Recent efforts have gradually shifted toward feed-forward formulations of 4D reconstruction. St4RTrack (Feng et al., 2025) predicts point maps for pairs of views, jointly encoding static geometry and dynamic motion; however, its pairwise formulation inherently limits the temporal range of the reconstruction. We also acknowledge several recent concurrent works that explore feed-forward formulations for 4D reconstruction. TraceAnything (Liu et al., 2025) represents scenes using continuous trajectory fields parameterized by Bezier curves. Although this formulation enables smooth and long-range motion modeling, it often struggles to represent complex or high-frequency dynamics and may compromise geometric accuracy. Any4D (Karhade et al., 2025) jointly predicts scene flow and 3D geometry from canonical reference view, but lacks the flexibility to infer motion originating from arbitrary viewpoints. Similarly, V-DPM (Sucar et al., 2026) extends VGGT to dynamic settings, but relies on an inflexible decoding scheme that aggregates information from all views, leading to high computational costs. Concurrently, D4RT (Zhang et al., 2025b) introduces Perceiver-like (Jaegle et al., 2021) model for unified 2D and 3D point tracking. While demonstrating strong performance and supporting flexible spatial-temporal point queries, its design is primarily focused on per-point tracking rather than dense, frame-level 4D reconstruction. In contrast, our method, 4RC, employs flexible query-based decoder that efficiently recovers complete and dense 4D attributes for any view at any timestamp, without expensive per-point computation. 3. Method Our goal is to develop unified and feed-forward model, 4RC, that takes monocular video as input and reconstructs the full underlying 4D attributes of the scene. The core of our approach lies in encoding the entire video sequence into compact 4D representation, which can then be queried on-demand to decode the geometry and motion of any query frame at any target timestamp, as illustrated in Figure 2. 3.1. Problem Formulation Given monocular video sequence = {Ii}N i=1, where Ii RHW 3 denotes the RGB frame captured at timestamp ti and is the total number of frames, our goal is to recover the full 4D attributes of the scene, capturing both its 3D structure and temporal evolution. Specifically, for any query frame Ii and an arbitrary target timestamp τ {ti}N i=1, we define time-indexed 3D point map: tiτ RHW 3, (1) which represents the 3D positions of points observed in frame Ii as they appear at time τ . When τ = ti, tiτ corresponds to the static 3D geometry of the frame. When τ = ti, it describes the dynamic time-dependent point maps of the scene by mapping the points from the source frame to their locations at the target time. Factorized 4D Attributes. Directly predicting point maps tiτ for all possible (i, τ ) pairs is redundant and ini tractable. Once the underlying 3D geometry at the source time is known, the geometry at other times can be expressed through relative motion. We therefore adopt factorized representation: tiτ = ti + tiτ , (2) where ti tiτ denotes the base 3D geometry at time ti, and represents the 3D displacement from time ti to τ . This formulation offers both conceptual and practical advantages. The base geometry ti is reconstructed from image Ii under the perspective camera model, property that allows us to directly leverage recent advances of effective geometry representation in monocular 3D reconstruction (Lin et al., 2025). Meanwhile, the displacement field tiτ explicitly captures temporal motion. This provides clear motion cues that are useful for downstream applications, while avoiding the need to re-predict complex geometry at every time step. As result, the representation remains temporally consistent, especially in static regions and under rigid motion. Unless otherwise stated, all point maps are viewpoint-invariant and expressed in world coordinate system defined by the camera of the first frame (Wang et al., 2024b; 2025b;a; Lin et al., 2025). Relation with Other Work. The key distinction between 4RC and several prior or concurrent approaches lies in the flexibility and completeness of our 4D output. Recent feed-forward 3D reconstruction methods focus solely on 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere Figure 2. Overall architecture of 4RC. Video frames are patchified and augmented with camera and time tokens, then jointly encoded by single transformer into compact 4D latent representation F, from which conditional decoder with disentangled geometry and motion heads enables flexible querying of 3D geometry and motion for arbitrary source views at arbitrary target timestamps. predicting the base 3D geometry for each input frame, i.e., ti , and thus fail to capture the motion within the scene. Traditional 3D point tracking methods, on the other hand, estimate sparse trajectories initialized from selected points and therefore cannot recover dense 4D geometry. Concurrent feed-forward 4D reconstruction methods also exhibit limitations in motion modeling. St4RTrack is restricted to pairwise motion. TraceAnything models trajectory fields using Bezier curves, which limits its ability to capture accurate geometry and complex motion. Any4D predicts motion only relative to the first frame, i.e., t1τ i=1, and therefore cannot support motion queries from other source frames. V-DPM regresses the point map tiτ for all source frames {1, . . . , } at given target timestamp τ , by attending to all frames jointly, which incurs substantial computational overhead and limits inference flexibility. In contrast, 4RC enables flexibly querying dense 3D motion from any single source frame to any target timestamp within unified and fully feed-forward framework. with τ {ti}N 1 3.2. 4D Representation Encoder The encoder processes the input video to produce unified 4D representation: = E(V). (3) We adopt plain ViT-based transformer architecture that alternates between frame-wise self-attention and global selfattention. Similar to the camera token in VGGT (Wang et al., 2025a), which primarily encodes camera geometry information for subsequent decoding, we further append each views patchified tokens with dedicated time token Ti. This time token aggregates temporal information for that view and serves as conditioning signal for target-time motion decoding, as described in Section 3.3. The encoder produces unified spatio-temporal latent representation = {Fi}N i=1. j=1 { ˆCi} { ˆTi} consists of patch Each Fi = { ˆZi,j}M tokens ˆZi,j RD corresponding to the i-th frame, together with an encoded camera token ˆCi and time token ˆTi. We treat as an ordered sequence of frame-level token sets. 3.3. Conditional 4D Decoder Geometry Head. To recover the base geometry for each input frame, we use geometry decoder Dg. Given the encoded spatial tokens ˆZi and camera tokens ˆCi, the geometry decoder predicts per-frame depth and rays, together with camera parameters: (cid:16) ˆDi, ˆRi, ˆθi (cid:17) = Dg (cid:16) ˆZi, ˆCi (cid:17) , (4) 2 1 where ˆDi RHW is the depth map, ˆRi 1 2 6 is the ray map, and ˆθi denotes the camera parameters (i.e., field of view, rotation, and translation). The base point map is then obtained from ( ˆDi, ˆRi, θi) under the perspective ti camera model. The geometry decoder Dg follows dualDPT (Ranftl et al., 2021; Lin et al., 2025) design with lightweight camera head. Motion Head. To recover motion for any query frame Iq at target timestamp τ , we use lightweight transformerbased motion decoder Dm with layers of alternating self-attention and cross-attention. We initialize the query tokens ˆZq from the encoder output F. The decoder outputs dense 3D displacement field: ˆP tqτ = Dm (cid:16) ˆZq, ˆTτ , ˆZτ (cid:17) . (5) Specifically, to condition on the target time, we inject time embedding ˆTτ via Adaptive Layer Normalization (AdaLN) (Perez et al., 2018) in the self-attention blocks, and then apply cross-attention to the target spatial token set ˆZτ . This design supports dense motion estimation and point tracking while remaining compatible with our per-frame geometry decoding. 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere 3.4. Training Scheme We train 4RC in an end-to-end manner with joint supervision over geometry and motion attributes. Following prior works (Wang et al., 2025a; Lin et al., 2025), we normalize the ground-truth scene scale such that the average Euclidean distance of all valid 3D points to the origin is 1. The overall training objective is defined as: = Ldepth + Lray + Lcam + Lmotion. (6) For all loss terms except the camera parameter loss Lcam, we adopt an aleatoric uncertainty formulation (Wang et al., 2024b). We denote the loss function as ℓ(ˆy, y, Σ), where Σ represents the predicted pixel-wise uncertainty map, which adaptively down-weights unreliable regions during training. To better supervise both geometry and motion, we apply gradient-based constraints (Lin et al., 2025) in the spatial and temporal domains separately. For geometry learning, we enforce spatial smoothness on the predicted depth maps ˆD = { ˆDi} by applying image-space gradients x. The depth loss is formulated as: Ldepth = ℓ( ˆD, D, ΣD) + ℓ(x ˆD, xD, ΣD). (7) Similarly, the motion loss supervises the displacement field P, but we incorporate an additional temporal gradient term that constrains the first-order temporal derivative of the displacement (i.e., velocity) to encourage temporally consistent motion behavior: Lmotion = ℓ( ˆP, P, ΣM ) +ℓ(t ˆP, tP, ΣM ). (8) 4. Experiments We conduct extensive experiments to evaluate the effectiveness of 4RC on standard 4D reconstruction tasks. We compare against established state-of-the-art methods as well as concurrent work for completeness, and further perform ablation studies to analyze the contribution of key design components in our framework. 4.1. Training Setup Datasets. We train 4RC on diverse collection of largescale public datasets, covering both dynamic and static scenes, as well as synthetic and real-world videos. Specifically, our training data includes PointOdyssey (Zheng et al., 2023), Dynamic Replica (Karaev et al., 2023b), Kubric (Greff et al., 2022), Waymo (Sun et al., 2020), DL3DV (Ling et al., 2024), ScanNet++ (Yeshwanth et al., 2023), and MVSSynth (Huang et al., 2018). These datasets jointly provide rich supervision for geometry, motion, and camera poses under varied scene layouts and motion patterns. Detailed dataset statistics are provided in the appendix. Implementation Details. Our encoder adopts single Vision Transformer based on DINOv2 (Oquab et al., 2023). The motion decoder is lightweight, consisting of = 4 layers of self-attention and cross-attention. We initialize both the encoder and the geometry decoder with pretrained weights from DA3 (Lin et al., 2025), which is trained on large-scale 3D data and provides strong geometric priors. During training, input images are resized to randomly sampled resolution, with the longer side up to 504 pixels. The aspect ratio is uniformly sampled from [0.5, 2.0] to improve generalizability. The training sequence length is randomly sampled from [2, 18] views, with longer sequences facilitating larger and more complex motions. To avoid the quadratic cost of computing all 2 motion pairs, we randomly sample one query view per iteration and predict its motion in different timesteps during training. Standard data augmentations including color jittering and Gaussian blur are applied. The model is trained end-to-end using the training loss described in Section 3.4. We use the AdamW optimizer (Kingma & Ba, 2015; Loshchilov & Hutter, 2019) for 50 epochs with cosine learning rate schedule. Training is performed on 16 A100 GPUs with batch size of 1 per GPU. Additional implementation details and hyperparameters are provided in the appendix. 4.2. 4D Reconstruction Qualitative Results. Figure 3 provides qualitative comparisons of 4RC in modeling 3D tracking. These visual results demonstrate the effectiveness of our method in handling complex motion patterns, such as occlusions, non-rigid motion, and large movements. We further evaluate our method on diverse in-the-wild videos in Figure 4, demonstrating its strong performance on both static and dynamic scenes. Dense Tracking. To demonstrate the capability of our method to track dense motion from arbitrary query views, we first quantitatively evaluate dense 3D tracking by sampling 24 frames from the Kubric and Waymo test sets, with 50 samples each, using the middle view (i.e., the 11th frame) as the query. Traditional point tracking methods fail on dense tracking due to out-of-memory issues, while Any4D can only predict the motion field for the first view. We report both the Average Percentage of Points (APD) within threshold and the End-Point Error (EPE) after global Sim(3) alignment with RANSAC. As shown in Table 1(a), 4RC achieves state-of-the-art performance among concurrent 4D reconstruction methods on both datasets. On the challenging Waymo dataset, which contains highly dynamic scenes, our method substantially outperforms the concurrent method VDPM, resulting in 36% gain in APD. Notably, our method uses flexible per-frame decoding, in contrast to V-DPMs computationally expensive global aggregation decoding. 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere Figure 3. Qualitative comparison of dynamic tracking on DAVIS (Perazzi et al., 2016). We visualize the dynamic reconstruction results, including the geometry at the first and last frames, as well as the dynamic object trajectories rendered as rainbow-colored paths from the first view. As shown in the top example, our method successfully handles occlusion when the motorcycle becomes temporarily invisible. In contrast, the two-view method St4RTrack lacks global temporal context and therefore predicts an incorrect trajectory. In the second and third examples, our method accurately reconstructs complex and large-scale motions while preserving high-quality geometry, while other methods produce inconsistent motion trajectories and degraded geometry. Table 1. 4D reconstruction evaluation on tracking. We evaluate our method on dense-view tracking (a), as well as sparse-view tracking (b) on dynamic datasets. Our method demonstrates state-of-the-art capability in dense tracking from arbitrary views compared to concurrent 4D reconstruction methods, and also achieves strong performance on the sparse point tracking setting, even when compared to tracking-specific methods. The top-2 results are highlighted as best and second . Method (a) Dense Tracking (b) Sparse Point Tracking Kubric Waymo PO DR ADT PStudio APD EPE APD EPE APD EPE APD EPE APD EPE APD EPE VGGT + CoTracker3 (Karaev et al., 2024) SpatialTrackerV2 (Xiao et al., 2025) St4RTrack (Feng et al., 2025) TraceAnything (Liu et al., 2025) Any4D (Karhade et al., 2025) V-DPM (Sucar et al., 2026) - - 50.65 59.98 - 71.12 - - 3.938 1.808 - 2.849 - - 19.98 21.25 - 41.44 - - 6.359 4.313 - 1. 63.19 0.5890 80.93 0.2417 77.81 0.3015 78.11 0.2715 73.66 0.3944 80.87 0.2218 95.48 0.0594 85.63 0.1583 71.64 0.3101 78.36 0.2367 82.79 0.2279 74.05 0.2537 52.02 0.9154 68.28 0.5060 82.77 0.1998 74.15 0.2926 71.47 0.3642 81.28 0.2171 73.83 0.3114 78.76 0.2088 83.36 0.1955 83.04 0.1901 80.80 0.2357 89.59 0.1165 4RC (Ours) 85.44 1.022 56.63 1. 85.86 0.2498 88.65 0.1484 87.82 0.1480 87.32 0.1304 Sparse Point Tracking. We then evaluate 4RC on 3D sparse point tracking, which measures sparse motion relative to the first frame, although our method can fully capture dense motion. Following the WorldTrack benchmark (Feng et al., 2025), tracking performance is assessed in the world coordinate system. The benchmark includes two datasets, Aerial Digital Twin (ADT) (Pan et al., 2023) and Panoptic Studio (PS) (Joo et al., 2019) from TAPVid-3D (Zhang et al., 2025a), as well as two test sets derived from PointOdyssey (PO) and Dynamic Replica (DR). We compare our method against tracking-specific methods Cotracker3 (Karaev et al., 2024) and SpatialTrackerV2 (Xiao et al., 2025), along with concurrent 4D reconstruction methods. The predicted trajectory is aligned to the ground truth using global Sim(3) transformation via RANSAC. As shown in Table 1(b), 4RC achieves strong performance even when compared with methods specifically designed for point tracking, outperforming SpatialTrackerV2 on 3 out of 4 datasets. 6 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere Table 2. Camera pose estimation and multi-View 3D reconstruction evaluation. We compare our method with both 3D reconstruction approaches and concurrent 4D reconstruction methods. Our approach achieves state-of-the-art performance among 4D methods, while remaining competitive with 3D reconstruction methods Pi3, without exclusive training on large-scale reconstruction datasets. Method (a) Camera Pose Estimation (b) Multi-View 3D Reconstruction TUM-dynamics ScanNet 7-Scenes NRGBD ATE RPEt RPEr ATE RPEt RPEr Acc Comp NC Acc Comp NC DUSt3R (Wang et al., 2024b) MASt3R (Leroy et al., 2024) MonST3R (Zhang et al., 2025c) Spann3R (Wang & Agapito, 2024) CUT3R (Wang et al., 2025b) VGGT (Wang et al., 2025a) Pi3 (Wang et al., 2025d) St4RTrack (Feng et al., 2025) TraceAnything (Liu et al., 2025) Any4D (Karhade et al., 2025) V-DPM (Sucar et al., 2026) 0.083 0.038 0.098 0.056 0.046 0.012 0.014 - - 0.030 0.014 0.017 0.012 0.019 0.021 0.015 0.010 0.009 - - 0.023 0.010 3.567 0.448 0.935 0.591 0.473 0.311 0.309 - - 0.463 0. 0.081 0.078 0.077 0.096 0.099 0.036 0.031 - - 0.074 0.035 0.028 0.020 0.018 0.023 0.022 0.015 0.013 - - 0.035 0.014 0.784 0.475 0.529 0.661 0.600 0.376 0.346 - - 1.076 0.410 0.146 0.185 0.248 0.298 0.126 0.087 0.044 0.240 0.232 0.141 0.097 0.181 0.180 0.266 0.205 0.154 0.091 0.063 0.234 0.359 0.177 0.124 0.736 0.144 0.701 0.085 0.672 0.272 0.650 0.416 0.727 0.099 0.787 0.073 0.758 0.022 0.681 0.241 0.584 0.347 0.738 0.081 0.772 0. 0.154 0.063 0.287 0.417 0.076 0.077 0.025 0.219 0.527 0.072 0.060 0.870 0.794 0.758 0.684 0.837 0.910 0.911 0.754 0.643 0.847 0.897 4RC (Ours) 0.010 0.008 0. 0.032 0.012 0.437 0.034 0.051 0.783 0. 0.034 0.912 Table 3. Depth estimation on the Bonn and Sintel datasets. We compare methods that explicitly predict video depth. Method Bonn Sintel Rel δ < 1.25 Rel δ < 1.25 0.155 DUSt3R (Wang et al., 2024b) 0.252 MASt3R (Leroy et al., 2024) MonST3R (Zhang et al., 2025c) 0.067 Spann3R (Wang & Agapito, 2024) 0.144 0.078 CUT3R (Wang et al., 2025b) 0.193 Fast3R (Yang et al., 2025) 0.055 VGGT (Wang et al., 2025a) 0.050 Pi3 (Wang et al., 2025d) 4RC (Ours) 0.051 83.3 70.1 96.3 81.3 93.7 77.5 97.1 97.4 97. 0.656 0.641 0.378 0.622 0.421 0.653 0.297 0.246 0.311 45.2 43.9 55.8 42.6 47.9 44.9 68.8 67.7 62.2 they do not explicitly estimate camera poses. Multi-View Reconstruction. Following prior work (Wang & Agapito, 2024; Wang et al., 2025b; 2024b), we evaluate scene-level multi-view 3D reconstruction on the 7Scenes (Shotton et al., 2013) and NRGBD (Azinovic et al., 2022) datasets. Reconstruction quality is measured using Accuracy (Acc), Completeness (Comp), and Normal Consistency (NC). Quantitative results are reported in Table 2 (b). 4RC achieves the best performance among 4D reconstruction methods, attaining the highest Acc/Comp on 7-Scenes and the best NC on NRGBD. This highlights the effectiveness of our proposed design. For example, we obtain 0.034 accuracy on 7-Scenes, far better than TraceAnythings 0.240; the latter jointly models geometry and motion in trajectory field, which often compromises geometric quality. Depth Estimation. We also evaluate video depth estimation on Sintel (Butler et al., 2012) and Bonn (Palazzolo et al., 2019) datasets. Following prior work (Wang et al., Figure 4. Visualization of in-the-wild examples. 4RC demonstrates accurate geometry reconstruction and motion modeling in both static and dynamic scenes. 4.3. 3D Reconstruction Camera Pose Estimation. We evaluate camera pose estimation on the Sintel (Butler et al., 2012), TUMdynamics (Sturm et al., 2012), and ScanNet (Dai et al., 2017) datasets. Performance is measured using Absolute Trajectory Error (ATE), Relative Translation Error (RPEt), and Relative Rotation Error (RPEr), all computed after global Sim(3) alignment with the ground truth, following established protocols (Teed & Deng, 2021; Zhang et al., 2025c; Wang et al., 2025b). Table 2 (a) shows that 4RC achieves top-tier camera pose estimation and reconstruction quality within single unified model. On the challenging TUM-dynamics dataset, 4RC attains the best ATE and RPEt among all methods, including specialized 3D reconstruction methods such as Pi3, which are trained on much larger datasets. This demonstrates that our unified 4D representation is effective for both motion modeling and producing accurate camera trajectories. Notably, 4RC achieves the best performance among concurrent feed-forward 4D reconstruction methods. We exclude St4RTrack and TraceAnything as 7 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere Table 4. Ablation of our motion head design and factorized motion. In (a), we evaluate the effectiveness of our motion head design by removing each component. (b) shows that representing the motion output in factorized form performs better than directly predicting the point cloud. Methods Kubric Waymo APD EPE APD EPE 4RC (Ours) 85.44 1.022 56. 1.611 (a) Motion Head Design (i) w/o Cross Attn. (ii) w/o Self Attn. (iii) w/o AdaLN 80.83 80.57 82.51 1.136 1.127 1.105 54.19 53.50 56. 1.618 1.686 1.689 (b) Factorized Motion (i) Points (World) (ii) Points (Local) 74.64 70.70 1.412 1.547 37.08 19. 2.359 3.226 2025b), predicted depth maps are aligned to the ground truth using per-sequence scale factor. While most existing 4D reconstruction methods do not explicitly output depth and therefore cannot be directly evaluated on depth benchmarks, 4RC includes an explicit depth prediction as part of its factorized 4D representation. On the Bonn dataset, 4RC achieves the best δ < 1.25 score and matches the second-best Rel. On Sintel, there is small gap compared to specialized 3D reconstruction methods such as Pi3, which are trained exclusively on large-scale 3D datasets that are more than twice the size of our training datasets. 4.4. Ablation Studies We conduct ablation studies to evaluate the key design choices in 4RC, focusing on the motion head and the factorized motion representation. Motion Head Design. Our motion head enables motion querying from arbitrary input views at arbitrary target timestamps. To analyze the contribution of each component in the motion head, we construct several variants by removing individual modules: (i) cross-attention between query tokens and target-time latent features, (ii) self-attention, and (iii) time-token conditioned AdaLN. All variants use the same number of layers and have comparable parameter sizes. As shown in Table 4 (a), removing any component consistently degrades performance, indicating that all modules are necessary for effective motion decoding. Among them, removing either attention module results in the largest performance drop. In Figure 5, we also quantitatively observe that without cross-attention, the decoder struggles to model complex non-rigid motions, such as hand and leg movements, producing over-smoothed trajectories that do not align with the true motion. This suggests that self-attention and adaptive normalization alone are insufficient for handling large and detailed temporal displacements, and direct access to targettime features is critical for accurate motion estimation. Figure 5. Qualitative ablation visualizations. The first row shows the effectiveness of cross-attention in the motion head: without it, although the model outputs rough trajectories, it fails to capture fine details such as the motion of the girls legs and hands when she is at the peak of jump. The second row illustrates that outputting motion as point clouds can lead to inconsistent trajectories as it requires re-predicting base geometry for each time step. Factorized Motion. We further evaluate the effectiveness of our factorized motion representation by comparing it with alternative output parameterizations commonly used in 3D reconstruction (Wang et al., 2025a;d). Specifically, we replace our displacement-based formulation with two point-based variants: directly predicting 3D coordinates in (i) shared world coordinate system, or (ii) each views own camera coordinate system. As reported in Table 4 (b), both point-based variants perform worse than our factorized representation. This performance gap arises mainly from differences in representation. Direct point prediction entangles geometry and motion in single output space, forcing the network to jointly learn shape and temporal correspondences, which significantly increases learning difficulty. Qualitative results in Figure 5 further support this observation. Our formulation explicitly decouples static geometry from time-dependent motion via displacement fields, reducing unnecessary recomputation of geometry and improving temporal consistency. 5. Conclusion We present 4RC, unified feed-forward transformer framework for 4D reconstruction from monocular videos. Central to our approach is novel encode-once, query-anywhere and anytime paradigm, in which compact 4D representation of the entire video is learned once and subsequently queried to recover geometry and motion at arbitrary time instances. This paradigm effectively bridges the global spatio-temporal modeling with flexible, on-demand query-based reconstruction, achieving both accurate 4D reconstruction and high efficiency. Extensive experiments demonstrate that 4RC consistently outperforms prior methods across wide range of challenging 4D reconstruction benchmarks. Looking ahead, unified models such as 4RC, which jointly reason about geometry and motion, represent promising direction toward more general-purpose perceptual systems. 8 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere"
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of machine learning, with particular focus on 4D reconstruction. The proposed approach has the potential to benefit applications in robotics, augmented/virtual reality, and content creation. While the method may have many potential societal consequences, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Azinovic, D., Martin-Brualla, R., Goldman, D. B., Nießner, M., and Thies, J. Neural RGB-D surface reconstruction. In CVPR, 2022. Bˆarsan, I. A., Liu, P., Pollefeys, M., and Geiger, A. Robust dense mapping for large-scale dynamic environments. In ICRA, 2018. Butler, D. J., Wulff, J., Stanley, G. B., and Black, M. J. naturalistic open source movie for optical flow evaluation. In ECCV, 2012. Dai, A., Chang, A. X., Savva, M., Halber, M., Funkhouser, T., and Nießner, M. Scannet: Richly-annotated 3D reconstructions of indoor scenes. In CVPR, 2017. Doersch, C., Gupta, A., Markeeva, L., Recasens, A., Smaira, L., Aytar, Y., Carreira, J., Zisserman, A., and Yang, Y. TAP-Vid: benchmark for tracking any point in video. NeurIPS, 2022. Doersch, C., Yang, Y., Vecerik, M., Gokay, D., Gupta, A., Aytar, Y., Carreira, J., and Zisserman, A. TAPIR: Tracking any point with per-frame initialization and temporal refinement. In ICCV, 2023. Feng, H., Zhang, J., Wang, Q., Ye, Y., Yu, P., Black, M. J., Darrell, T., and Kanazawa, A. St4RTrack: Simultaneous 4D reconstruction and tracking in the world. In ICCV, 2025. Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. Vision meets robotics: The kitti dataset. IJRR, 2013. Greff, K., Belletti, F., Beyer, L., Doersch, C., Du, Y., Duckworth, D., Fleet, D. J., Gnanapragasam, D., Golemo, F., Herrmann, C., Kipf, T., Kundu, A., Lagun, D., Laradji, I., Liu, H.-T. D., Meyer, H., Miao, Y., Nowrouzezahrai, D., Oztireli, C., Pot, E., Radwan, N., Rebain, D., Sabour, S., Sajjadi, M. S. M., Sela, M., Sitzmann, V., Stone, A., Sun, D., Vora, S., Wang, Z., Wu, T., Yi, K. M., Zhong, F., and Tagliasacchi, A. Kubric: scalable dataset generator. In CVPR, 2022. as shader: 3D-aware video diffusion for versatile video generation control. arXiv preprint arXiv:2501.03847, 2025. Harley, A. W., Fang, Z., and Fragkiadaki, K. Particle video revisited: Tracking through occlusions using point trajectories. In ECCV, 2022. Hu, W., Gao, X., Li, X., Zhao, S., Cun, X., Zhang, Y., Quan, L., and Shan, Y. Depthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR, 2025. Huang, P.-H., Matzen, K., Kopf, J., Ahuja, N., and Huang, J.-B. DeepMVS: Learning multi-view stereopsis. In CVPR, 2018. Huang, W., Chao, Y.-W., Mousavian, A., Liu, M.-Y., Fox, D., Mo, K., and Fei-Fei, L. PointWorld: Scaling 3D world models for in-the-wild robotic manipulation, 2026. Hui, T.-W., Tang, X., and Loy, C. C. Liteflownet: lightweight convolutional neural network for optical flow estimation. In CVPR, 2018. Jaegle, A., Gimeno, F., Brock, A., Zisserman, A., Vinyals, O., and Carreira, J. Perceiver: General perception with iterative attention, 2021. Joo, H., Simon, T., Li, X., Liu, H., Tan, L., Gui, L., Banerjee, S., Godisart, T., Nabbe, B., Matthews, I., Kanade, T., Nobuhara, S., and Sheikh, Y. Panoptic studio: massively multiview system for social interaction capture. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(1):190204, 2019. Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., and Rupprecht, C. CoTracker: It is better to track together. 2023a. Karaev, N., Rocco, I., Graham, B., Neverova, N., Vedaldi, A., and Rupprecht, C. DynamicStereo: Consistent dynamic depth from stereo videos. CVPR, 2023b. Karaev, N., Makarov, I., Wang, J., Neverova, N., Vedaldi, A., and Rupprecht, C. CoTracker3: Simpler and better point tracking by pseudo-labelling real videos. arXiv preprint arXiv:2410.11831, 2024. Karhade, J., Keetha, N., Zhang, Y., Gupta, T., Sharma, A., Scherer, S., and Ramanan, D. Any4D: Unified feedforward metric 4D reconstruction, 2025. arXiv preprint. Ke, B., Obukhov, A., Huang, S., Metzger, N., Daudt, R. C., and Schindler, K. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR, 2024. Gu, Z., Yan, R., Lu, J., Li, P., Dou, Z., Si, C., Dong, Z., Liu, Q., Lin, C., Liu, Z., Wang, W., and Liu, Y. Diffusion Kingma, D. P. and Ba, J. Adam: method for stochastic optimization. In ICLR, 2015. 9 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere Kopf, J., Rong, X., and Huang, J.-B. Robust consistent video depth estimation. In CVPR, 2021. Lan, Y., Luo, Y., Hong, F., Zhou, S., Chen, H., Lyu, Z., Yang, S., Dai, B., Loy, C. C., and Pan, X. STream3R: Scalable sequential 3D reconstruction with causal transformer. In ICLR, 2026. Lee, S., Jung, Y., Chun, I., Lee, Y.-C., Cai, Z., Huang, H., Talreja, A., Dao, T. D., Liang, Y., Huang, J.-B., and Huang, F. TraceGen: World modeling in 3D trace space enables learning from cross-embodiment videos. arXiv preprint arXiv:2511.21690, 2025a. Lee, Y.-C., Zhang, Z., Huang, J., Wang, J.-H., Lee, J.-Y., Huang, J.-B., Shechtman, E., and Li, Z. Generative video motion editing with 3D point tracks. arXiv preprint arXiv:2512.02015, 2025b. Lei, J., Weng, Y., Harley, A., Guibas, L., and Daniilidis, K. MoSca: Dynamic Gaussian fusion from casual videos via 4D motion scaffolds. arXiv preprint arXiv:2405.17421, 2024. Leroy, V., Cabon, Y., and Revaud, J. Grounding image matching in 3D with MASt3R, 2024. Lin, H., Chen, S., Liew, J. H., Chen, D. Y., Li, Z., Shi, G., Feng, J., and Kang, B. Depth anything 3: Recovering the visual space from any views. arXiv preprint arXiv:2511.10647, 2025. Ling, L., Sheng, Y., Tu, Z., Zhao, W., Xin, C., Wan, K., Yu, L., Guo, Q., Yu, Z., Lu, Y., et al. DL3DV-10k: largescale scene dataset for deep learning-based 3D vision. In CVPR, 2024. Liu, X., Xiao, Y., Chen, D. Y., Feng, J., Tai, Y.-W., Tang, C.-K., and Kang, B. Trace Anything: Representing any video in 4d via trajectory fields. arXiv preprint, 2025. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In ICLR, 2019. Ngo, T. D., Zhuang, P., Gan, C., Kalogerakis, E., Tulyakov, S., Lee, H.-Y., and Wang, C. Delta: Dense efficient long-range 3d tracking for any video. arXiv preprint arXiv:2410.24211, 2024. Oquab, M., Darcet, T., Moutakanni, T., Vo, H. V., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Howes, R., Huang, P.-Y., Xu, H., Sharma, V., Li, S.-W., Galuba, W., Rabbat, M., Assran, M., Ballas, N., Synnaeve, G., Misra, I., Jegou, H., Mairal, J., Labatut, P., Joulin, A., and Bojanowski, P. DINOv2: Learning robust visual features without supervision. In arXiv preprint arXiv:2304.07193, 2023. Palazzolo, E., Behley, J., Lottes, P., Gigu`ere, P., and Stachniss, C. ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals. arXiv, 2019. Pan, X., Charron, N., Yang, Y., Peters, S., Whelan, T., Kong, C., Parkhi, O., Newcombe, R., and Ren, C. Y. Aria digital twin: new benchmark dataset for egocentric 3D machine perception. In ICCV, 2023. Perazzi, F., Pont-Tuset, J., McWilliams, B., Gool, L. V., Gross, M., and Sorkine-Hornung, A. benchmark dataset and evaluation methodology for video object segmentation. In CVPR, 2016. Perez, E., Strub, F., De Vries, H., Dumoulin, V., and Courville, A. FiLM: Visual reasoning with general conditioning layer. In AAAI, 2018. Ranftl, R., Bochkovskiy, A., and Koltun, V. Vision transformers for dense prediction. ArXiv preprint, 2021. Schonberger, J. L. and Frahm, J.-M. Structure-from-motion revisited. In CVPR, 2016. Schonberger, J. L., Zheng, E., Pollefeys, M., and Frahm, J.- M. Pixelwise view selection for unstructured multi-view stereo. In ECCV, 2016. Shao, J., Yang, Y., Zhou, H., Zhang, Y., Shen, Y., Guizilini, V., Wang, Y., Poggi, M., and Liao, Y. Learning temporally consistent video depth from video diffusion priors, 2024. Shotton, J., Glocker, B., Zach, C., Izadi, S., Criminisi, A., and Fitzgibbon, A. Scene coordinate regression forests for camera relocalization in RGB-D images. In CVPR, June 2013. Sturm, J., Engelhard, N., Endres, F., Burgard, W., and Cremers, D. benchmark for the evaluation of RGB-D SLAM systems. In IROS, 2012. Sucar, E., Lai, Z., Insafutdinov, E., and Vedaldi, A. Dynamic point maps: versatile representation for dynamic 3D reconstruction. arXiv preprint arXiv:2503.16318, 2025. Sucar, E., Insafutdinov, E., Lai, Z., and Vedaldi, A. V-DPM: 4D video reconstruction with dynamic point maps. In arXiv preprint arXiv:2601.09499, 2026. Sun, D., Roth, S., and Black, M. J. Secrets of optical flow estimation and their principles. In CVPR, 2010. Sun, D., Yang, X., Liu, M.-Y., and Kautz, J. PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume. In CVPR, 2018. 10 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., and Anguelov, D. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, 2020. Teed, Z. and Deng, J. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, 2020. Teed, Z. and Deng, J. DROID-SLAM: Deep visual SLAM for monocular, stereo, and RGB-D cameras. NeurIPS, 2021. Wang, H. and Agapito, L. 3D reconstruction with spatial memory. arXiv preprint arXiv:2408.16061, 2024. Xiao, Y., Wang, J., Xue, N., Karaev, N., Makarov, I., Kang, B., Zhu, X., Bao, H., Shen, Y., and Zhou, X. SpatialTrackerV2: 3D point tracking made easy. In ICCV, 2025. Xu, G., Lin, H., Luo, H., Wang, X., Yao, J., Zhu, L., Pu, Y., Chi, C., Sun, H., Wang, B., et al. Pixel-perfect depth with semantics-prompted diffusion transformers. arXiv preprint arXiv:2510.07316, 2025. Yang, J., Sax, A., Liang, K. J., Henaff, M., Tang, H., Cao, A., Chai, J., Meier, F., and Feiszli, M. Fast3R: Towards 3D reconstruction of 1000+ images in one forward pass. In CVPR, 2025. Yang, L., Kang, B., Huang, Z., Zhao, Z., Xu, X., Feng, J., and Zhao, H. Depth anything V2. arXiv preprint arXiv:2406.09414, 2024. Wang, J., Chen, M., Karaev, N., Vedaldi, A., Rupprecht, C., and Novotny, D. VGGT: Visual geometry grounded transformer. In CVPR, 2025a. Yao, Y., Luo, Z., Li, S., Fang, T., and Quan, L. Mvsnet: Depth inference for unstructured multi-view stereo. ECCV, 2018. Wang, Q., Chang, Y.-Y., Cai, R., Li, Z., Hariharan, B., Holynski, A., and Snavely, N. Tracking everything everywhere all at once. In ICCV, 2023a. Yao, Y., Luo, Z., Li, S., Shen, T., Fang, T., and Quan, L. Recurrent MVSNet for high-resolution multi-view stereo depth inference. CVPR, 2019. Wang, Q., Ye, V., Gao, H., Austin, J., Li, Z., and Kanazawa, A. Shape of motion: 4D reconstruction from single video. arXiv preprint arXiv:2407.13764, 2024a. Yeshwanth, C., Liu, Y.-C., Nießner, M., and Dai, A. Scannet++: high-fidelity dataset of 3D indoor scenes. In ICCV, 2023. Wang, Q., Zhang, Y., Holynski, A., Efros, A. A., and Kanazawa, A. Continuous 3D perception model with persistent state. arXiv preprint arXiv:2501.12387, 2025b. Zhang, B., Ke, L., Harley, A. W., and Fragkiadaki, K. TAPIP3D: Tracking any point in persistent 3D geometry. NeurIPS, 2025a. Wang, S., Leroy, V., Cabon, Y., Chidlovskii, B., and Revaud, J. Dust3R: Geometric 3D vision made easy. In CVPR, 2024b. Wang, S., Yang, X., Shen, Q., Jiang, Z., and Wang, X. Gflow: Recovering 4D world from monocular video. In AAAI, 2025c. Wang, Y., Shi, M., Li, J., Huang, Z., Cao, Z., Zhang, J., Xian, K., and Lin, G. Neural video depth stabilizer. In ICCV, October 2023b. Wang, Y., Zhou, J., Zhu, H., Chang, W., Zhou, Y., Li, Z., Chen, J., Pang, J., Shen, C., and He, T. Pi3: Permutationequivariant visual geometry learning. arXiv preprint arXiv:2507.13347, 2025d. Wu, R., Gao, R., Poole, B., Trevithick, A., Zheng, C., Barron, J. T., and Holynski, A. CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models. arXiv:2411.18613, 2024. Xiao, Y., Wang, Q., Zhang, S., Xue, N., Peng, S., Shen, Y., and Zhou, X. SpatialTracker: Tracking any 2D pixels in 3D space. In CVPR, 2024. 11 Zhang, C., Le Moing, G., Koppula, S., Rocco, I., Momeni, L., Xie, J., Sun, S., Sukthankar, R., Barral, J. K., Hadsell, R., Ghahramani, Z., Zisserman, A., Zhang, J., and Sajjadi, M. S. M. Efficiently reconstructing dynamic scenes one d4rt at time. arXiv preprint, 2025b. Zhang, J., Herrmann, C., Hur, J., Jampani, V., Darrell, T., Cole, F., Sun, D., and Yang, M.-H. MonST3R: simple approach for estimating geometry in the presence of motion. ICLR, 2025c. Zhang, Z., Cole, F., Li, Z., Rubinstein, M., Snavely, N., and Freeman, W. T. Structure and motion from casual videos. In ECCV, 2022. Zheng, Y., Harley, A. W., Shen, B., Wetzstein, G., and Guibas, L. J. PointOdyssey: large-scale synthetic dataset for long-term point tracking. In ICCV, 2023. Zhou, S., Li, C., Chan, K. C., and Loy, C. C. ProPainter: Improving propagation and transformer for video inpainting. In ICCV, 2023. 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere"
        },
        {
            "title": "Appendix",
            "content": "A. Additional Implementation Details A.1. Architecture Details We adopt the ViT-Giant (ViT-G) architecture from DINOv2 (Oquab et al., 2023) as our encoder, which consists of 40 transformer layers with feature dimension of 1,536 and employs 24 attention heads. The encoder weight is initialized from Depth Anything 3 (DA3) (Lin et al., 2025). For the geometry head, we follow dual-DPT (Ranftl et al., 2021; Lin et al., 2025) design equipped with lightweight MLP as the camera head. For the motion head, we employ transformer-based decoder consisting of 4 layers of alternating selfand cross-attention with hidden dimension of 1,536 and 16 attention heads. To generate high-resolution dense motion outputs, we leverage DPT (Ranftl et al., 2021) upsampling strategy where we extract the feature tokens from the 19-th, 27-th, 33-rd, and 39-th blocks of the encoder. We therefore apply the motion head to these layers, concatenate the resulting outputs, and fuse them through the DPT head to regress the final dense motion displacement field. A.2. Dataset Details We train 4RC on 7 datasets covering both dynamic and static environments. Table 5 details the statistics and sampling ratio of each dataset during training. For 3D motion learning, we leverage four dynamic datasets with ground-truth motion: PointOdyssey (Zheng et al., 2023), Dynamic Replica (Karaev et al., 2023b), Waymo (Sun et al., 2020), and Kubric (Greff et al., 2022). The motion supervision in these datasets varies from dense motion to sparse trajectories. Specifically for Kubric, we curate two subsets: 4,000 clips from the MOVi-F release (24 frames each) with dense motion annotations, and 6,000 clips from the CoTracker3 (Karaev et al., 2024) rendered training set (120 frames each) with sparse trajectory annotations. To ensure high-quality geometric reconstruction on static backgrounds, we additionally include three static datasets: DL3DV (Ling et al., 2024), ScanNet++ (Yeshwanth et al., 2023), and MVS-Synth (Huang et al., 2018). Table 5. Training dataset statistics. We train 4RC on mixture of 7 datasets. The motion annotation varies between dense maps and sparse trajectories depending on the dataset source. Static datasets naturally provide motion annotations, i.e., zero movement. Index Dataset Scene Type Real / Synthetic Dynamic / Static Motion Annotation Sampling (%) 1 2 3 4 5 6 7 PointOdyssey (Zheng et al., 2023) Dynamic Replica (Karaev et al., 2023b) Waymo (Sun et al., 2020) Kubric (Greff et al., 2022) DL3DV (Ling et al., 2024) ScanNet++ (Yeshwanth et al., 2023) MVS-Synth (Huang et al., 2018) Mixed Mixed Outdoor Object Mixed Indoor Outdoor Synthetic Synthetic Real Synthetic Real Real Synthetic Dynamic Dynamic Dynamic Dynamic Static Static Static Sparse Sparse Dense Dense & Sparse Dense Dense Dense 22.12 29.20 4.42 26.55 8.85 3.54 5.31 A.3. Training Details During training, we apply standard data augmentations, including Gaussian blur (p = 0.2), ColorJitter (p = 0.1), and RandomGrayscale (p = 0.05). Video frames are sampled in strict temporal order with random interval ranging from 1 to 5 frames. For motion supervision, we adopt probabilistic sampling strategy. Specifically, in 20% of the training iterations, we supervise the model using all available motion ground truth. In the remaining 80%, we employ sparse supervision by retaining only the top 2030% of points with the largest displacement magnitudes. Empirically, we find that this strategy filters out static or low-motion regions, prevents the dominance of zero-motion signals and accelerates convergence. For the ray map loss Lray and the camera parameter loss Lcam in Equation 6, we adopt the loss formulation from DA3 (Lin et al., 2025) for supervision. B. Additional Experiments and Results B.1. Streaming Version of 4RC To support causal and online 4D reconstruction, we further introduce streaming variant of 4RC (S-4RC) which builds upon the STream3R (Lan et al., 2026) architecture. Specifically, we replace our encoder with the pretrained STream3R 12 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere Figure 6. The visualization of S-4RC results. S-4RC can infer 3D geometry and motion in an online manner, which is beneficial for downstream tasks such as robotic motion planning and egocentric understanding. backbone, which enforces unidirectional causal attention. The model is then fine-tuned using the proposed 4RC training objectives. Unlike standard 4RC, which processes the entire video in an offline manner, S-4RC operates sequentially and achieves per-frame latency. We cache the 4D latent representation for all processed frames. This enables flexible motion queries from the current view to any past timestamp, as well as point tracking from past views to the current time. As shown in Table 6 and Figure 6, S-4RC achieves competitive performance in 4D reconstruction while operating in an online manner, without access to global temporal context. Note that S-4RC is trained for 20 epochs on 8 A100 GPUs. Table 6. 4D reconstruction evaluation for S-4RC. S-4RC enables online and streaming 4D reconstruction and achieves competitive performance compared to 4RC, even without access to global temporal context. Method PO DR ADT PStudio Kubric Waymo Point Tracking Dense Tracking APD EPE APD EPE APD EPE APD EPE APD EPE APD EPE S-4RC 73.29 0.3863 83.47 0.1970 86.12 0.1674 83.81 0.1795 75.60 85.86 0.2498 88.65 0.1484 87.82 0.1480 87.32 0.1304 85.44 4RC 1.168 1.022 46.02 56.63 1.971 1.611 B.2. Additional Quantitative Evaluation on 4D Reconstruction As complement to the evaluation in Table 1, following WorldTrack (Feng et al., 2025) and TAPVid-3D (Zhang et al., 2025a), we apply global median scale alignment to match the predicted points with the ground truth. This alignment is feasible since both the predictions and the ground-truth points are represented in shared world coordinate system defined by the camera of the first frame. We additionally include staged pipeline baseline composed of Monst3R (Zhang 13 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere et al., 2025c) and SpaTracker (Xiao et al., 2024). Comprehensive evaluations in Table 7 demonstrate that our method outperforms approaches specifically designed for point tracking as well as concurrent 4D reconstruction methods, achieving state-of-the-art results on 4 out of 6 datasets. Table 7. 4D reconstruction evaluation on tracking under global median scale alignment. As complement to Table 1, we further evaluate our method on dense-view tracking (a) and sparse-view tracking (b) under global median scale alignment on dynamic datasets. Our method maintains strong performance across both evaluation protocols. Method (a) Dense Tracking (b) Sparse Point Tracking Kubric Waymo PO DR ADT PStudio APD EPE APD EPE APD EPE APD EPE APD EPE APD EPE VGGT + CoTracker3 (Karaev et al., 2024) Monst3R + SpaTracker (Xiao et al., 2024) SpaTrackerV2 (Xiao et al., 2025) St4RTrack (Feng et al., 2025) TraceAnything (Liu et al., 2025) Any4D (Karhade et al., 2025) V-DPM (Sucar et al., 2026) - - - 35.33 27.37 - 52.22 - - - 3.465 1.952 - 3.131 - - - 2.51 2.06 - 31. - - - 49.08 0.6532 74.73 0.2884 72.21 0.3548 66.28 0.3107 47.65 0.5917 55.49 0.8823 51.95 0.5362 50.16 0.4837 69.57 0.3780 73.43 0.2732 92.22 0.0915 74.16 0.2272 10.139 67.95 0.3140 73.74 0.2682 76.01 0.2680 69.67 0.2637 12.564 39.83 1.0593 60.63 0.5758 75.65 0.2511 71.33 0.2727 60.86 0.4194 68.39 0.3012 56.71 0.4320 60.03 0.3344 79.79 0.1994 76.38 0.2378 66.06 0.3426 76.36 0.1957 - 1.957 4RC (Ours) 55.38 1. 39.55 1.864 80.27 0.2681 82.91 0.1889 84.28 0.1766 69.04 0.2603 B.3. Additional Quantitative Evaluation on Depth Estimation We additionally include the KITTI dataset (Geiger et al., 2013) and extend the video depth evaluation in the main paper. We compare with broader set of baselines, including single-frame depth methods Marigold (Ke et al., 2024) and DepthAnything-V2 (Yang et al., 2024), video depth methods NVDS (Wang et al., 2023b), DepthCrafter (Hu et al., 2025), and ChronoDepth (Shao et al., 2024), and joint depth-and-pose estimation approaches Robust-CVD (Bˆarsan et al., 2018) and CausalSAM (Zhang et al., 2022). All results are aligned using per-sequence scale and shift, enabling more comprehensive and fair comparison for video depth evaluation. As shown in Table 8, our method significantly outperforms existing depth estimation approaches and achieves competitive performance compared to the dynamic 3D reconstruction method Pi3 (Wang et al., 2025d). Notably, our method is not trained on large-scale 3D reconstruction datasets and is able to model dynamic object motion, rather than focusing solely on 3D reconstruction. Table 8. Depth estimation on Bonn, Sintel, and KITTI datasets. We compare series of methods that explicitly predict video depth using per-sequence scale & shift alignment. Method Bonn Sintel KITTI Rel δ < 1.25 Rel δ < 1.25 Rel δ < 1.25 Marigold (Ke et al., 2024) 0.091 Depth-Anything-V2 (Yang et al., 2024) 0.106 0.167 NVDS (Wang et al., 2023b) 0.100 ChronoDepth (Shao et al., 2024) 0.075 DepthCrafter (Hu et al., 2025) - Robust-CVD (Kopf et al., 2021) 0.169 CasualSAM (Zhang et al., 2022) 0.156 DUSt3R-GA (Wang et al., 2024b) 0.167 MASt3R-GA (Leroy et al., 2024) 0.066 MonST3R-GA (Zhang et al., 2025c) 0.157 Spann3R (Wang & Agapito, 2024) 0.074 CUT3R (Wang et al., 2025b) 0.049 VGGT (Wang et al., 2025a) 0.044 Pi3 (Wang et al., 2025a) 4RC (Ours) 0.048 93.1 92.1 76.6 91.1 97.1 - 73.7 83.1 78.5 96.4 82.1 94.5 97.2 97.5 97.3 0.532 0.367 0.408 0.687 0.292 0.703 0.387 0.531 0.327 0.333 0.508 0.540 0.202 0.229 0. 51.5 55.4 48.3 48.6 69.7 47.8 54.7 51.2 59.4 59.0 50.8 55.7 72.7 73.2 67.0 0.149 0.140 0.253 0.167 0.110 - 0.246 0.135 0.137 0.157 0.207 0.106 0.057 0.038 0.058 79.6 80.4 58.8 75.9 88.1 - 62.2 81.8 83.6 73.8 73.0 88.7 96.6 98.4 95. 14 4RC : 4D Reconstruction via Conditional Querying Anytime and Anywhere Figure 7. Visualization using 4RC on in-the-wild videos of camera poses, static reconstruction, dynamic reconstruction, and 3D tracking. B.4. More Visualizations We further provide additional visualizations of our 4RC results, including camera poses, static reconstruction, dynamic reconstruction, and 3D tracking on in-the-wild videos in Figure 7. B.5. Video Demo We also provide demo video on our project page to showcase the qualitative 4D reconstruction results of 4RC and S-4RC. C. Limitations While our method achieves unified and flexible feed-forward 4D reconstruction and shows stronger performance than concurrent 4D reconstruction methods, several limitations remain. First, our approach struggles in scenarios where geometric recovery is inherently difficult. These include regions with extreme depth (e.g., distant clouds), transparent objects, or floating artifacts where the base geometry lacks sharp depth boundaries. We expect that improved depth estimation methods (Xu et al., 2025) and future advances in 3D reconstruction will help alleviate these issues. Second, we observe performance degradation in scenes with extreme or highly chaotic motion. This limitation mainly arises from the diversity of motion annotation in existing datasets, which provide insufficient supervision for such complex dynamics. Future work will explore scaling up training data to cover broader range of motion patterns and kinematic diversity."
        }
    ],
    "affiliations": [
        "Nanyang Technological University"
    ]
}