{
    "paper_title": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering",
    "authors": [
        "DongGeon Lee",
        "Ahjeong Park",
        "Hyeri Lee",
        "Hyeonseo Nam",
        "Yunho Maeng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Non-factoid question-answering (NFQA) poses a significant challenge due to its open-ended nature, diverse intents, and the need for multi-aspect reasoning, which renders conventional factoid QA approaches, including retrieval-augmented generation (RAG), inadequate. Unlike factoid questions, non-factoid questions (NFQs) lack definitive answers and require synthesizing information from multiple sources across various reasoning dimensions. To address these limitations, we introduce Typed-RAG, a type-aware multi-aspect decomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies NFQs into distinct types -- such as debate, experience, and comparison -- and applies aspect-based decomposition to refine retrieval and generation strategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and aggregating the results, Typed-RAG generates more informative and contextually relevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, a benchmark dataset covering diverse NFQ types. Experimental results demonstrate that Typed-RAG outperforms baselines, thereby highlighting the importance of type-aware decomposition for effective retrieval and generation in NFQA. Our code and dataset are available at https://github.com/TeamNLP/Typed-RAG."
        },
        {
            "title": "Start",
            "content": "Typed-RAG: Type-aware Multi-Aspect Decomposition for Non-Factoid Question Answering DongGeon Lee1* Ahjeong Park2 Hyeri Lee3 Hyeonseo Nam4 Yunho Maeng5, 6 1Pohang University of Science and Technology 3Independent Researcher 4KT 6LLM Experimental Lab, MODULABS 5Ewha Womans University 2Sookmyung Womens University donggeonlee@postech.ac.kr {keira.hyeri.lee, namhs2030}@gmail.com ahjeong@sookmyung.ac.kr yunhomaeng@ewha.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "question-answering Non-factoid (NFQA) poses significant challenge due to its and open-ended nature, diverse intents, the need for multi-aspect reasoning, which renders conventional factoid QA approaches, including generation retrieval-augmented inadequate. Unlike factoid ques- (RAG), tions, non-factoid questions (NFQs) lack definitive answers and require synthesizing information from multiple sources across various reasoning dimensions. To address these limitations, we introduce Typed-RAG, type-aware multi-aspect decomposition framework within the RAG paradigm for NFQA. Typed-RAG classifies NFQs into distinct typessuch as debate, experience, and comparisonand applies aspect-based decomposition to refine retrieval and generation strategies. By decomposing multi-aspect NFQs into single-aspect sub-queries and aggregating the results, Typed-RAG generates more informative and contextually relevant responses. To evaluate Typed-RAG, we introduce Wiki-NFQA, benchmark dataset covering diverse NFQ types. Experimental results demonstrate that Typed-RAG outperthereby highlighting the forms baselines, type-aware decomposition importance of for effective retrieval and generation in NFQA. Our code and dataset are available at https://github.com/TeamNLP/Typed-RAG. 5 2 0 2 1 2 ] . [ 2 9 7 8 5 1 . 3 0 5 2 : r"
        },
        {
            "title": "Introduction",
            "content": "In real-world scenarios, individuals often seek answers to non-factoid questions (NFQs) that require comprehensive responses reflecting multiple perspectives and contextual nuances, rather than simple factual replies. These questions include types such as comparisons, experiences, and debates, re- *Both authors contributed equally to this work. Corresponding author. 1 flecting complex human information needs (Bolotova et al., 2022). NFQs pose significant challenge in retrievalaugmented generation (RAG) systems due to their diverse intents and answer perspectives. Unlike factoid questions (FQs), which follow 1:1 questionanswer relationship and require short, factual answers (e.g., Whats the capital of France?) (Lee et al., 2022; Sun et al., 2024), NFQs lack definitive answer (e.g., Is AI beneficial to society?), making single-aspect responses insufficient to fully satisfy user needs. We define these as multi-aspect queries, requiring multi-domain reasoning and synthesis from multiple sources, which conventional factoid question-answering (FQA) systems struggle to handle. Recent non-factoid question-answering (NFQA) approaches, including query-focused summarization and type-specific methods (Deng et al., 2024; An et al., 2024), struggle to generalize across diverse NFQs and underutilize large language models (LLMs) and RAG frameworks. While standard RAG enhances contextuality (Lewis et al., 2020; Izacard and Grave, 2021), it fails to address NFQ heterogeneity, which arises from differences in question intent and the need for multi-perspective reasoning. As result, it produces overly uniform responses that lack the multi-aspect depth necessary for comprehensive NFQA. To address these challenges, we propose TypedRAG, type-aware multi-aspect decomposition framework for NFQA. By integrating question type classification with pre-trained classifier into the RAG pipeline, our approach refines retrieval and generation strategies for distinct NFQ types. key feature, multi-aspect decomposition, breaks multi-aspect NFQs into single-aspect sub-queries, enabling targeted retrieval and response generation before synthesizing well-rounded answer. By handling each aspect separately and synthesizing responses, our system generates answers aligned Figure 1: An overview of Typed-RAG. Non-factoid questions are classified by the type classifier and processed based on their type. Prompts for the multi-aspect decomposer and answer aggregator handle the unique requirements of each type. Details of the prompt can be found in Appendix A.4. with user intent. We evaluate Typed-RAG on Wiki-NFQA, benchmark dataset of diverse NFQ types from Wikipedia. Results show it outperforms baseline models, including standard LLMs and RAG, effectively capturing NFQ complexity and generating nuanced, user-aligned answers. Our contributions are summarized as follows: We introduce Typed-RAG, novel type-aware multi-aspect decomposition method in retrievalaugmented generation framework for NFQA that integrates question type classification and multiaspect decomposition to enhance answer generation. We develop optimized retrieval and generation strategies tailored to different NFQ types, improving the systems ability to address diverse and complex user queries. We construct the Wiki-NFQA dataset to facilitate NFQA research and provide benchmark for evaluating QA systems on non-factoid questions. We demonstrate that Typed-RAG outperforms existing baseline models, validating our approachs effectiveness in generating high-quality answers to NFQs. By addressing the unique challenges of NFQA, we aim to bridge the gap between user information needs and QA system capabilities, paving the way for more effective and contextually aware questionanswering technologies."
        },
        {
            "title": "2 Related Work",
            "content": "sively categorized (Burger et al., 2003; Chaturvedi et al., 2014; Bolotova et al., 2022). Factoid questions seek straightforward answers, while NFQs require more complex, subjective, or multifaceted responses (Chaturvedi et al., 2014; Bolotova et al., 2022). Bolotova et al. (2022) identifies six NFQ types: Instruction, Reason, Evidence-based, Comparison, Experience, and Debate. Our study advances NFQA by employing RAG strategies to manage these question types effectively. Retrieval-Augmented Generation (RAG) RAG integrates external knowledge into LLMs response generation, enhancing the accuracy of the output. The effectiveness of RAG depends on providing question-relevant search results to the LLM as generator. Therefore, improving the retrieval process is critical to prevent the LLMs hallucination (Huang et al., 2023; Lee and Yu, 2025). Advanced RAG methods actively employ query rewriting and decomposition techniques (Press et al., 2023; Rackauckas, 2024; Chan et al., 2024) to aggregate diverse information, capturing both explicitly stated facts and subtle, indirectly conveyed knowledge. However, the application of LLMs and RAG for solving NFQA remains largely underexplored. Deng et al. (2024) tackles NFQA with graph networks and Transformer-based language model, but does not employ large-scale pretrained LLM. Similarly, although An et al. (2024) proposed paradigm integrating logic-based data structure with RAG to address How-To NFQ, it did not cover all NFQ types comprehensively. Non-Factoid Question Taxonomy To tackle the complexity of real-world QA, questions are extenEvaluation Metrics Traditional metrics like ROUGE and BERTScore (Zhang et al., 2020), 2 used to evaluate answers from FQA systems using LLMs and RAG, often fail to capture the semantic richness and nuanced quality variations in nonfactoid answers. To overcome this, Yang et al. (2024) proposed LINKAGE, listwise ranking framework for NFQA evaluation. LINKAGE leverages LLM as scorer to rank candidate answers against reference answers ordered by quality and demonstrates stronger correlations with human annotations, outperforming traditional metrics and emphasizing its potential as superior evaluation methodology."
        },
        {
            "title": "3 Method",
            "content": "This section introduces the RAG pipeline specifically designed for NFQ types. The overall methodology is illustrated in Figure 1. The types of NFQ differ in their intent, the directionality of their aspects, and the degree of contrast between them. NFQs often manifest as multi-aspect queries, where answers span multiple perspectives. These aspects can be categorized into two types: contrasting aspects, which exhibit high contrast and opposing directions, and related aspects, which have lower contrast and align in the same direction. Based on these characteristics, we designed methods that effectively align with user expectations.12 In the following sections, we discuss these question types and their processing rationale. Evidence-based Evidence-based type questions seek to understand the characteristics or definitions of specific concepts, objects, or events, typically requiring fact-based explanations. As single-aspect queries, they do not undergo multi-aspect decomposition. Instead, we apply straightforward RAG approach, retrieving relevant passages and generating accurate answers while preserving clarity and simplicity. The retriever first retrieves passages using the question as query, and the generator produces responses based on these passages. Comparison Comparison-type questions are used to examine differences, similarities, or superiority between keywords, with answers tailored to the comparisons purpose and targets. 1Using the prompts described in Appendix A.4, our approach involves either applying multi-aspect decomposer, tailored to the question type, or aggregating answers generated for multiple single-aspect queries. 2Details for each type are illustrated in Figure 16 (a)(e), and detailed examples for each NFQ type are discussed in the Appendix B.3. Reasoning behind each methodological design is provided in Appendix C.1. As multi-aspect queries, they require decomposition. First, keyword extractor identifies the comparison purpose (compare_type) and targets (keywords_list). Subsequently, the retriever is employed to search for passages that are related to each keyword. Following the deduplication of passages, the remaining results are reranked according to their relevance. Finally, the generator combines the information to create response that aligns with the comparison purpose, thereby effectively addressing the question. Experience Experience-type questions seek advice or recommendations, providing explanations based on personal experiences. As multi-aspect they require decomposition. The requeries, triever first retrieves relevant passages, followed by similarity-based reranking using extracted keywords by the keyword extractor. In the end, the generator produces an optimized response aligned with the intent of the question, effectively meeting its requirements. Reason/Instruction Reason-type questions aim to explore the causes of specific phenomena, while instruction-type questions focus on understanding procedures or methods. The intent behind these question types is to receive clear and comprehensive answers that present reasons or steps in wellstructured manner. These questions are treated as multi-aspect queries and require multi-aspect decomposer. First, the query is decomposed into single-aspect queries using single-aspect query generator. Each generated query is then processed individually by the retriever and generator to produce separate answers. Finally, an answer aggregator combines these individual responses into concise and accurate final answer. Debate Debate-type questions are hypothetical questions designed to explore diverse perspectives, including opposing viewpoints. The purpose of this question type is to generate balanced response that reflects multiple perspectives. Being multi-aspect question, it naturally requires the multi-aspect decomposer. The question is first processed by single-aspect query generator, which extracts the discussion topic and various opinions. Generated queries for each opinion are then handled by the retriever and generator to produce individual responses. Finally, debate mediator combines the topic and perspectives to produce wellbalanced final response. 3 Model Scorer LLM Methods Wiki-NFQA Dataset NQ-NF SQD-NF TQA-NF 2WMH-NF HQA-NF MSQ-NF Llama-3.2-3B Mistral-7B Mistral-7B GPT-4o mini Mistral-7B GPT-4o mini 0.5893 LLM RAG 0.5294 Typed-RAG 0.7659 0.4934 LLM RAG 0.4187 Typed-RAG 0.8366 0.6356 LLM RAG 0.5635 Typed-RAG 0. 0.4656 LLM RAG 0.4411 Typed-RAG 0.8413 0.5119 0.4944 0.6493 0.4506 0.3553 0.7139 0.5450 0.5069 0.6333 0.4222 0.3817 0.7444 0.6191 0.5470 0. 0.5380 0.4586 0.7013 0.6363 0.6233 0.6709 0.5921 0.5450 0.7767 0.3565 0.4150 0.4544 0.3070 0.2859 0.3692 0.4821 0.4789 0. 0.3175 0.2890 0.3987 0.4825 0.4530 0.5624 0.3669 0.2957 0.5470 0.5255 0.5323 0.6035 0.3965 0.3562 0.6653 0.4262 0.4047 0. 0.2917 0.2866 0.4482 0.5081 0.4438 0.4512 0.3384 0.3079 0.4929 Table 1: Evaluation results on the Wiki-NFQA dataset using Mean Reciprocal Rank (MRR), comparing the performance of various language models, scorer LLMs, and methods. Answers were ranked using LINKAGE (Yang et al., 2024) scored with the MRR metric."
        },
        {
            "title": "4.1 Model",
            "content": "We compare our Typed-RAG to LLM-based and RAG-based QA systems as the baselines. In the experiments, we use black-box LLM and two open-weights LLMs with varying number of parameters: (i) GPT-4o-mini-2024-07-18 (GPT-4o mini), (ii) Mistral-7B-Instruct-v0.2 (Mistral-7B; Jiang et al., 2023), and (iii) Llama-3.2-3B-Instruct (Llama-3.2-3B). All the inputs in LLMs (including the generator of RAG), are formatted using prompt templates. The prompt templates we used in our experiments are in Appendix A.3."
        },
        {
            "title": "4.2 Listwise Ranking Evaluation (LINKAGE)",
            "content": "To evaluate an NFQA system, we adopt LINKAGE (Yang et al., 2024), listwise ranking approach, as our primary evaluation metric. LINKAGE ranks each candidate answer according to its relative quality in comparison to reference list of answers. The ranking process is formally defined as follows: rankci = LLM(PL, qi, ci, Ri) (1) Here, qi denotes the i-th question, and ci refers to the candidate answer being evaluated. The list of reference answers Ri, associated with qi, consists of individual reference answers ri ordered in descending quality. The evaluation process uses scorer (LLM), guided by the LINKAGE prompt PL, to determine rankci, the rank of ci relative to the reference list Ri. Ranking Metrics To quantify the ranking results from LINKAGE, we utilize two complementary metrics: Mean Reciprocal Rank (MRR) (Voorhees and Tice, 2000) and Mean Percentile Rank (MPR). The MRR metric measures the rank position of candidate answers, with higher values indicating that answers are ranked closer to the top. It is calculated by averaging the reciprocal ranks of the candidate answers across all questions. On the other hand, MPR normalizes ranks into percentiles, reflecting the relative positions of answers within their respective reference lists. higher MPR suggests better overall ranking performance across all references. While MRR emphasizes top-ranked answers, MPR provides insight into relative performance throughout the entire list."
        },
        {
            "title": "4.3 Dataset Construction",
            "content": "To evaluate NFQA methods, we construct the Wiki-NFQA dataset, specialized resource tailored for NFQA, which is derived from existing Wikipedia-based datasets (Kwiatkowski et al., 2019; Joshi et al., 2017; Rajpurkar et al., 2016; Yang et al., 2018; Trivedi et al., 2022; Ho et al., 2020). We extract non-factoid questions through systematic filtering process, then generate highquality reference answers to ensure the datasets suitability for NFQA evaluation.3 Reference Answers Generation Since these datasets have only single-grade ground truth answer, we generate diverse qualities of reference 3The filtering process is discussed in Appendix B.1. 4 Figure 2: Mean Percentile Rank (MPR) performance comparison of LLM, RAG, and Typed-RAG on six different non-factoid question categories from the Wiki-NFQA dataset. Results are reported using different model configurations (Llama-3.2-3B and Mistral-7B) and scorer LLMs (Mistral-7B and GPT-4o mini). The y-axis represents the MPR score (%), with higher values indicating better performance. answers for LINKAGE evaluation, following Yang et al. (2024). After the construction of reference answers, we annotate the quality level of generated reference answers using the GPT-4o-2024-11-20."
        },
        {
            "title": "5 Experimental Results",
            "content": "Overall, Typed-RAG consistently outperforms both LLM-based and RAG-based methods across all subdatasets of the Wiki-NFQA dataset, NFQ categories, and model configurations. An integrated analysis of the MRR results  (Table 1)  and MPR results (Figure 2) reveals that Typed-RAG not only improves the ranking positions of generated responses but also enhances their relative quality. These results demonstrate that responses generated by Typed-RAG are consistently rated higher in relevance and comprehensiveness by the scorer LLMs.5 Impact of Scorer LLMs and Base Models The performance of all methods is affected by the choice of scorer LLMs and base models. It is important to note that scores obtained from different scorer LLMs should not be directly compared, as each scorer evaluates responses using its own learned criteria and internal representation of language. Instead, the relative ranking of methods within the same scorer LLM provides more 4Prompt details about the reference answers generation are in Appendix A.1, and A.2. 5Examples of Typed-RAGs responses for different types of non-factoid questions are provided in Appendix E. meaningful interpretation of performance. notable trend observed in Table 1 and Figure 2 is that scores tend to decrease when switching from Mistral-7B to GPT-4o mini as the scorer. This can be attributed to GPT-4o mini being more powerful and sophisticated model, potentially making it more critical evaluator. Stronger models generally have more refined understanding of relevance, coherence, and factual accuracy, leading them to assign lower scores when responses have minor inconsistencies or lack sufficient depth. This phenomenon is consistent across different base models and methods, reinforcing the idea that more advanced scorer imposes stricter evaluation criteria. However, despite the overall score reductions, Typed-RAG maintains clear advantage over both the LLM-based and RAG-based baselines, indicating its robustness to changes in scorer strictness. Limitations of RAG and Benefits of Typed-RAG RAG-based methods consistently underperform compared to direct LLM-based generation, as shown in Table 1 and Figure 2. key reason is that retrieved factual information often introduces noise rather than aiding response generation in NFQA tasks. Typed-RAG addresses these issues by leveraging multi-aspect decomposition strategy, optimizing retrieval for NFQA. By structuring retrieval around distinct facets of non-factoid questions, Typed-RAG reduces irrelevant noise and ensures more relevant information retrieval. As seen 5 Another limitation of our evaluation setup is that we use the same model to assess the quality of its own generated responses. This self-evaluation approach may introduce bias, as the model could struggle to distinguish quality differences among answers it produced. To mitigate this issue, future work could explore evaluation using stronger LLMs, human assessments, or ensemble scoring methods. By adopting these strategies, we can improve the reliability of quality assessments and reduce potential biases in our evaluation framework."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Chris Develder, our mentor from the NAACL 2025 SRW pre-submission mentorship program, as well as the anonymous reviewers for their valuable feedback on this paper. This research was supported by Brian Impact Foundation, non-profit organization dedicated to the advancement of science and technology for all. We would also like to acknowledge the support of the Korea Institute of Human Resources Development in Science and Technology (KIRD). in Table 1 and Figure 2, Typed-RAG consistently outperforms both RAG and LLM-only approaches, particularly in reasoning-intensive datasets, demonstrating its effectiveness in enhancing response quality."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced Typed-RAG, novel approach to non-factoid question answering (NFQA) that integrates type-aware multi-aspect decomposition within RAG framework. By classifying questions into specific types and tailoring RAG accordingly, Typed-RAG addresses the diverse and complex nature of NFQs. key feature of our method is the decomposition of multi-aspect questions into single-aspect sub-queries, allowing for targeted retrieval and generation that collectively produce comprehensive and nuanced answers. The experimental results on the Wiki-NFQA dataset substantiate the effectiveness of TypedRAG in enhancing answer quality for non-factoid questions. The consistent outperformance across various types of NFQ, baselines, and scorer LLMs indicates that integrating type-aware multi-aspect decomposition is robust approach for NFQA. Our method not only elevates the answers ranks but also improves their relative quality in the eyes of the scorer LLMs, leading to better user satisfaction."
        },
        {
            "title": "Limitations",
            "content": "Although our work is the first to introduce RAG to NFQA, it has several limitations. key limitation is the lack of direct comparison between Typed-RAG and existing query rewriting and decomposition methodologies. While TypedRAG provides structured approach to these tasks, its performance relative to other techniques remains unexplored. Various query rewriting and decomposition techniques have been proposed to enhance retrieval quality, yet this study does not empirically evaluate how Typed-RAG performs relative to these approaches in terms of query reformulation effectiveness, retrieval relevance, and computational overhead. systematic comparison with these methods would provide clearer understanding of Typed-RAGs advantages and limitations. Future work should incorporate benchmark evaluations against these established techniques to better position Typed-RAG within the landscape of query rewriting and decomposition research."
        },
        {
            "title": "References",
            "content": "Kaikai An, Fangkai Yang, Liqun Li, Junting Lu, Sitao Cheng, Lu Wang, Pu Zhao, Lele Cao, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. 2024. Thread: logic-based data organization paradigm for how-to question answering with retrieval augmented generation. arXiv preprint arXiv:2406.13372. Valeriia Bolotova, Vladislav Blinov, Falk Scholer, W. Bruce Croft, and Mark Sanderson. 2022. nonfactoid question-answering taxonomy. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, page 11961207. Valeriia Bolotova-Baranova, Vladislav Blinov, Sofya Filippova, Falk Scholer, and Mark Sanderson. 2023. WikiHowQA: comprehensive benchmark for multi-document non-factoid question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 52915314. Association for Computational Linguistics. John Burger, Claire Cardie, Vinay Chaudhri, Robert Gaizauskas, Sanda Harabagiu, David Israel, Christian Jacquemin, Chin-Yew Lin, Steve Maiorano, George Miller, Dan Moldovan, Bill Ogden, John Prager, Ellen Riloff, Amit Singhal, Rohini Shrihari, Tomek Strazalkowski, Ellen Voorhees, and Ralph Weishedel. 2003. Issues, tasks and program structures to roadmap research in question & answering (Q&A). In Document Understanding Conferences Roadmapping Documents, pages 135. Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. RQ-RAG: Learning to refine queries for retrieval augmented generation. In 1st Conference on Language Modeling. Snigdha Chaturvedi, Vittorio Castelli, Radu Florian, Ramesh M. Nallapati, and Hema Raghavan. 2014. Joint question clustering and relevance prediction for open domain non-factoid question answering. In Proceedings of the 23rd International Conference on World Wide Web, page 503514. Yang Deng, Wenxuan Zhang, Weiwen Xu, Ying Shen, and Wai Lam. 2024. Nonfactoid question answering as query-focused summarization with IEEE Transgraph-enhanced multihop inference. actions on Neural Networks and Learning Systems, 35(8):1123111245. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multihop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232. Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874880. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 67696781. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. DongGeon Lee and Hwanjo Yu. 2025. REFIND: Retrieval-augmented factuality hallucination detecarXiv preprint tion in large language models. arXiv:2502.13622. Nayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pascale Fung, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Factuality enhanced language models for open-ended text generation. In Advances in Neural Information Processing Systems 35, pages 34586 34599. Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for In Advances in knowledge-intensive NLP tasks. Neural Information Processing Systems 33, pages 94599474. 7 Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and Zhaopeng Tu. 2024. Encouraging divergent thinking in large language models through multi-agent debate. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1788917904. OpenAI. 2024. GPT-4o system card. arXiv preprint arXiv:2410.21276. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 56875711. Zackary Rackauckas. 2024. RAG-Fusion: new take International on retrieval-augmented generation. Journal on Natural Language Computing, 13:3747. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392. Hao Sun, Hengyi Cai, Bo Wang, Yingyan Hou, Xiaochi Wei, Shuaiqiang Wang, Yan Zhang, and Dawei Yin. 2024. Towards verifiable text generation with evolving memory and self-reflection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 82118227. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554. Ellen M. Voorhees and Dawn M. Tice. 2000. The TREC-8 question answering track. In Proceedings of the Second International Conference on Language Resources and Evaluation. Sihui Yang, Keping Bi, Wanqing Cui, Jiafeng Guo, and Xueqi Cheng. 2024. LINKAGE: Listwise ranking among varied-quality references for non-factoid QA evaluation via LLMs. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 69857000. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations."
        },
        {
            "title": "A Prompt Details",
            "content": "A.1 Reference List Construction Prompt Template to Generate the Highest Standard Reference Answer Given non-factoid question:\"{question}\" and its answer:\"{ground_truth}\" Use your internal knowledge to rewrite this answer. Figure 3: Prompt template proposed by Yang et al. (2024) to generate the highest standard reference answer using LLMs internal knowledge. Prompt Template to Generate Diverse Qualities of Reference Answers Generate three different answers to non-factoid question from good to bad in quality, each inferior to the golden answer give you. Ensure that the quality gap from good to bad is very significant among these three answers. Golden answer is the reasonable and convincing answer to the question. Answer 1 can be an answer to the question, however, it is not sufficiently convincing. Answer 2 does not answer the question or if it does, it provides an unreasonable answer. Answer 3 is completely out of context or does not make any sense. Here are 3 examples for your reference. 1.Non-factoid Question: how can we get concentration on something? Golden Answer: To improve concentration, set clear goals, create distraction-free environment, use time management techniques like the Pomodoro Technique, practice mindfulness, take regular breaks, stay organized, limit multitasking, practice deep work, maintain physical health, and seek help if needed. Output: Answer 1: Improve focus: set goals, quiet space, Pomodoro Technique, mindfulness, breaks, organization, limit multitasking, deep work, health, seek help if needed. Answer 2: Just like and enjoy the work you do, concentration will come automatically. Answer 3: If you are student, you should concentrate on studies and dont ask childish questions. 2.Non-factoid Question: Why doesnt the water fall off earth if its round? Golden Answer: Earths gravity pulls everything toward its center, including water. Even though Earth is round, gravity keeps water and everything else anchored to its surface. Gravitys force is strong enough to counteract the Earths curvature, preventing water from falling off. Output: Answer 1: This goes along with the question of why dont we fall off the earth if it is round. The answer is because gravity is holding us (and the water) down. Answer 2: Same reason the people dont. Answer 3: When rain drops fall through the atmosphere CO2 becomes dissolved in the water. CO2 is normal component of the Earths atmosphere, thus the rain is considered naturally acidic. 3.Non-factoid Question: How do determine the charge of the iron in FeCl3? Golden Answer: Since chloride ions (Cl-) each carry charge of -1, and there are three chloride ions in FeCl3, the total negative charge from chloride ions is -3. To balance this, the iron ion (Fe) must have charge of +3 to ensure the compound has neutral overall charge. Therefore, the charge of the iron ion in FeCl3 is +3. Output: Answer 1: Charge of Fe in Fecl3 is 3. Iron has either 2 as valancy or 3. in this case it bonds with three chlorine molecules. therefore its valency and charge is three. Answer 2: If two particles (or ions, or whatever) have opposite charge, then one has positive charge and one has negative charge. Answer 3: take piece of iron. Wrap copper wire around the iron in tight close coils. run charge through the wire. Below are the non-factoid question, and the golden answer. Non-factoid Question: {question} Golden Answer: {ground_truth} Output: Figure 4: Prompt template proposed by Yang et al. (2024) to generate diverse qualities of reference answers. A.2 Reference Answers Annotation System Prompt for Reference Answers Annotation Your task is to evaluate the relevance and quality of multiple candidate answers for given non-factoid question. Please evaluate the quality of each answer in step-by-step manner. Follow the structured guidelines below to ensure consistency and accuracy in your evaluation. # Notes on Candidate Answers Multiple candidate answers can come in two forms: - Single choice answer: single string, e.g., `\"born again\"`. - Multiple choice answer: list of strings, e.g., `[traffic calming, aesthetics]`. When evaluating multiple choice answers, treat the entire list as single unit. Do **not** split them into individual components; instead, evaluate the overall quality as whole. # Evaluation Criteria Assign label to each candidate answer based on the following criteria: - 3: The answer provides comprehensive, accurate, and contextually relevant response that directly addresses the question. - 2: The answer is accurate and relevant but lacks depth or comprehensive coverage. - 1: The answer is somewhat relevant but contains inaccuracies, vagueness, or insufficient detail. - 0: The answer is irrelevant, incorrect, or fails to address the question meaningfully. **If there are two or more answers that you think are close in quality, you can give the same label.** # Response Format - Assign label to each answer strictly in the format: `Answer X: [[Y]]`, where `X` is the answer number, and `Y` is the integer score (0-3). - Do **not** include any additional comments or explanations outside this format."
        },
        {
            "title": "Input Prompt Template for Reference Answers Annotation",
            "content": "# Inputs - Non-Factoid Question: {question} - Candidate Answers: {reference_answers} Figure 5: System prompt (top) and input prompt template (bottom) adapted from Yang et al. (2024) for annotating the quality level of generated reference answers. 10 A.3 Prompt templates for Baseline Methods Prompt template for LLM You are an assistant for answering questions. Answer the following question. ### Question {question} ### Answer Figure 6: Prompt template for LLM method. Prompt template for RAG You are an assistant for answering questions. Refer to the references below and answer the following question. ### References {reference_passages} ### Question {question} ### Answer Figure 7: Prompt template for RAG method. 11 A.4 Prompt templates for Typed-RAG A.4.1 Debate Prompt Template for Generating Sub-queries in Debate-type Questions You are query analysis assistant. Based on the query type, apply the relevant prompt to transform the query to better align with the users intent, ensuring clarity and precision. The input question is debate-type question (i.e., invites multiple perspectives). As \"Query Analyst\", please evaluate this question and proceed with the following steps. 1. Extract the debate topic. 2. Identify 2 to 5 key perspectives on this topic. 3. Generate sub-query reflecting each perspectives bias. Ensure each sub-query fits Retrieval-Augmented Generation (RAG) framework, seeking passages that align with the viewpoint. ### Output format {\"debate_topic\": {topic}, \"dist_opinion\": [list of perspectives], \"sub-queries\": {\"opinion1\": \"biased sub-query for opinion1\", \"opinion2\": \"biased sub-query for opinion2\", ...} ### Example Query: \"Is Trump good president?\" Answer: { \"debate_topic\": \"Donald Trumps presidency\", \"dist_opinion\": [\"positive\", \"negative\", \"neutral\"], \"sub-queries\": { \"positive\": \"Was Donald Trump one of the best presidents for economic growth?\", \"negative\": \"Did Trumps presidency harm the U.S. economy and leadership?\", \"neutral\": \"Can we assess Trumps tenures strengths and weaknesses?\" } } ### Input Query: {query} ### Output Answer: Prompt Template for Debate Mediator in Debate-type Questions You are acting as the mediator in debate. Below is topic and responses provided by participants, each with their own perspective. Your task is to synthesize these responses by considering both the debate topic and each participants viewpoint, providing fair and balanced summary. Ensure the response maintains balance, captures key points, and distinguishes any opposing opinions. Present the answer *short and concise*, phrased in direct format without using phrases like \"participants in the debate\" or \"in the debate.\" ### Input format - Debate topic: {debate_topic} - Participants responses: - Response 1: \"{response content}\" (Perspective: {perspective 1}) - Response 2: \"{response content}\" (Perspective: {perspective 2}) - ... - Response N: \"{response content}\" (Perspective: {perspective N}) ### Output format short and concise summary from the mediators perspective based on the discussion, phrased as direct answer without reference to the debate structure or participants ### Inputs Debate topic: {debate_topic} Participants responses: {responses} ### Output Summary: Figure 8: Prompt templates for generating sub-queries and debate mediator in debate-type questions. A.4.2 Experience Figure 9 shows the prompt template for responding to experience-type questions. The retrieved passages are subsequently reranked based on the extracted keywords. After reranking, we use the prompt template for RAG (Figure 7) to generate answers. Prompt Template for Keyword Extraction in Experience-type Questions You are query analysis assistant. Based on the query type, apply the relevant prompt to transform the query to better align with the users intent, ensuring clarity and precision. The input question is an experience-type question (i.e., get advice or recommendations on particular topic.). As \"Query Analyst\", please evaluate this question and proceed with the following steps. 1. Identify the topic intended to be gathered from experience-based questions. 2. Extract the key entities in the question, considering the intent of asking about experience, to facilitate an accurate response. ### Output format `[\"Keyword 1\", ..., \"Keyword N\"]` (List of string, separated with comma) ### Example Question (Input): \"What are some of the best Portuguese wines?\" Answer (Output): [\"Portuguese wines\", \"best\"] ### Input Question: {question} ### Output Answer: Figure 9: Prompt template for keyword extraction in experience-type questions. 13 A.4.3 Reason & Instruction Prompt Template for Generating Sub-queries in Reason-type Questions You are query analysis assistant. Based on the query type, apply the relevant prompt to transform the query to better align with the users intent, ensuring clarity and precision. The input query is reason-type question (i.e., question posed to understand the reason behind particular concept or phenomenon). As \"Query Analyst\", please evaluate this query and proceed with the following steps. 1. Break down the original instruction into multiple sub-queries that preserve the core intent but use varied language and structure. These multiple sub-queries should aim to capture different linguistic expressions of the original instruction while still aligning with its intended meaning. 2. Create at least 2 to 5 distinct multiple sub-queries. ### Output format `[\"sub-query 1\", ..., \"sub-query N\"]` (List of string, separated with comma) ### Input Query: {query} ### Output Multiple sub-queries: Figure 10: Prompt template for generating sub-queries in reason-type questions. Prompt Template for Generating Sub-queries in Instruction-Type Questions You are query analysis assistant. Based on the query type, apply the relevant prompt to transform the query to better align with the users intent, ensuring clarity and precision. The input query is an instruction-type question (i.e., Instructions/guidelines provided in step-by-step manner). As \"Query Analyst\", please evaluate this query and proceed with the following steps. 1. Break down the original instruction into multiple sub-queries that preserve the core intent but use varied language and structure. These multiple sub-queries should aim to capture different linguistic expressions of the original instruction while still aligning with its intended meaning. 2. Create at least 2 to 5 distinct multiple sub-queries. ### Output format `[\"sub-query 1\", ..., \"sub-query N\"]` (List of string, separated with comma) ### Input Query: {query} ### Output Multiple sub-queries: Figure 11: Prompt template for generating sub-queries in instruction-type questions. Prompt Template for Aggregating Answers to an Original Question You are an assistant tasked with aggregating answers to question. You are provided with the original question and multiple question-answer pairs. These queries preserve the core intent of the original question but use varied language and structure. Your goal is to review the question-answer pairs and synthesize concise and accurate response to the original question based on the information provided. Using the information from the question-answer pairs, generate brief and clear answer to the original question. ### Inputs Original Question: {original_question} Question-Answer Pairs: {qa_pairs_text} ### Output Aggregated Answer: Figure 12: Prompt template for aggregating answers to an original question. Used by reason-type questions and instruction-type questions. 14 A.4.4 Comparison Prompt Template for Keyword Extraction in Comparison-type Questions You are query analysis assistant. Based on the query type, apply the relevant prompt to transform the query to better align with the users intent, ensuring clarity and precision. Determine if the input query is compare-type question (i.e., compare/contrast two or more things, understand their differences/similarities.) as \"Query Analyst\". If so, perform the following: 1. Identify the type of comparison: \"differences\", \"similarities\", or \"superiority\". 2. Extract the subjects of comparison and represent them as specific, contextualized phrases. ### Output format {\"is_compare\": true/false, \"compare_type\": \"\", \"keywords_list\": []} ### Example Query: \"Who is more intelligent than humans on earth?\" Analysis: {\"is_compare\": true, \"compare_type\": \"superiority\", \"keywords_list\": [\"human intelligence\", \"the intelligence of other beings\"]} ### Input Query: {query} ### Output Analysis: Figure 13: Prompt template for keyword extraction in comparison-type questions. Prompt Template for Generating Response to Comparison-type Questions You are an assistant for answering questions. You are given the extracted parts of long document and question. Refer to the references below and answer the following question. The question is compare-type with specific comparison type and keywords indicating the items to compare. Answer based on this comparison type and the target keywords provided. ### Inputs Question: {question} Comparison Type: {comparison_type} Keywords: {keywords} References: {reference_passages} ### Output Answer: Figure 14: Prompt template for generating response to comparison-type questions. 15 A.5 Listwise Ranking Evaluation (LINKAGE) Prompt Template for LINKAGE Please impartially rank the given candidate answer to non-factoid question accurately within the reference answer list, which are ranked in descending order of quality. The top answers are of the highest quality, while those at the bottom may be poor or unrelated. Determine the ranking of the given candidate answer within the provided reference answer list. For instance, if it outperforms all references, output [[1]]. If its deemed inferior to all four references, output [[4]]. Your response must strictly following this format: \"[[2]]\" if candidate answer could rank 2nd. Below are the users question, reference answer list, and the candidate answer. Question:{question} Reference answer list:{reference_answers} Candidate answer:{candidate_answer} Figure 15: Prompt template proposed by Yang et al. (2024) to evaluate Typed-RAG and baseline methods using LINKAGE (Yang et al., 2024). 16 Construction of the Wiki-NFQA"
        },
        {
            "title": "Dataset",
            "content": "B.1 Filtering Non-Factoid Questions To extract NFQs from existing Wikipedia-based datasets, we use the nf-cats6 (Bolotova et al., 2022), RoBERTa-based pre-trained question category classifier. Since this model categorizes questions into factoid and non-factoid types, we retain only those classified as non-factoid for further processing. To ensure more rigorously curated dataset, we heuristically filter the data following the question patterns outlined in the NFQ taxonomy proposed by Bolotova et al. (2022), thereby constructing the Wiki-NFQA dataset. The statistics for the Wiki-NFQA dataset are presented in Table 3 (Appendix B.4). B.2 Reference Answers Construction In detail, we use three different LLMs to obtain different styles of reference answers: (i) GPT-3.5turbo-16k, (ii) Mistral-7B-Instruct-v0.27 (Jiang et al., 2023), (iii) Llama-3.1-8B-Instruct8. Each LLM generates three quality answers, for total of nine reference answers. In addition, we use GPT-4o-2024-08-06 (OpenAI, 2024) to generate the highest standard reference answer, which is distinct from the other nine answers. B.3 Representative Examples of NFQ Types Table 2 presents representative example questions for each type of NFQ in the Wiki-NFQA dataset. The NFQ taxonomy follows the classification proposed by Bolotova-Baranova et al. (2023). The Evidence-based type requires answers grounded in verifiable sources, while Comparison questions seek distinctions between concepts. Experience questions solicit subjective opinions or recommendations, whereas Reason questions aim to uncover the rationale behind events or concepts. Instruction questions request procedural guidance, and Debate questions involve discussions on controversial or interpretative topics. B.4 Dataset Statistics The statistics of the Wiki-NFQA dataset can be found in Table 3. The total number of questions in the dataset is 945. 6https://huggingface.co/Lurunchik/nf-cats 7https://huggingface.co/mistralai/ Mistral-7B-Instruct-v0.2 8https://huggingface.co/meta-llama/Llama-3. 1-8B-Instruct C.1 Typed-RAG We designed methodologies to meet user expectations by analyzing the intent and aspects of each question type in the NFQ system. Evidence-based questions seek specific, reliable factual information and are inherently singleaspect. Since the intent of these questions is to obtain evidence-based facts, the response remains singular in perspective. The absence of complex contextual reasoning or multi-aspect decomposition allows them to be handled through direct and straightforward answering methods. Comparison questions aim to provide information about differences, similarities, or relative advantages between two or more items. These questions are multi-aspect, as the intent varies depending on whether the goal is to highlight similarities, differences, or the superiority of one option over another. The presence of multiple comparison criteria results in answers that must be tailored accordingly. Depending on the type of comparison, the aspects can be categorized as either contrasting, when focusing on differences, or related, when emphasizing similarities. Answering these questions effectively requires identifying the comparison type and retrieving relevant information for each aspect separately before synthesizing them into balanced response. Experience-based questions focus on obtaining advice, recommendations, or insights based on personal experiences, making them multi-aspect. Since individual experiences are inherently subjective, responses vary widely depending on the perspective of the respondent. This variability results in answers that contrast with one another rather than aligning along shared perspective. To generate informative responses, it is essential to accurately identify the users intent and define the key aspects that should be addressed. Unlike comparison questions, which focus on feature-based differences, experience-based questions have broader scope, reflecting the diversity of personal opinions and experiences. Reason and instruction questions focus on understanding causes behind phenomenon or procedural steps for achieving goal. Although these two subtypes share broad category, their aspectual properties differ significantly. Reason-type questions aim to identify the causes of an event or phenomenon and are multi-aspect be-"
        },
        {
            "title": "Example of Question",
            "content": "Evidence-based How does sterilisation help to keep the money flow even?"
        },
        {
            "title": "Comparison",
            "content": "what is the difference between dysphagia and odynophagia"
        },
        {
            "title": "Experience",
            "content": "What are some of the best Portuguese wines?"
        },
        {
            "title": "Reason",
            "content": "Kresy, which roughly was part of the land beyond the so-called Curson Line, was drawn for what reason?"
        },
        {
            "title": "Instruction",
            "content": "How can you find lodge to ask to be member of?"
        },
        {
            "title": "Debate",
            "content": "I Can See Your Voice, reality show from South Korea, offers what kind of performers chance to make their dreams of stardom reality? Table 2: Representative example questions for each type of non-factoid question (NFQ) in the Wiki-NFQA dataset. cause the explanation depends on the surrounding context. Different conditions may lead to different causal explanations, making single-perspective answer insufficient. Since multiple explanations can exist based on varying assumptions, responses often contrast with one another. Instruction-type questions, on the other hand, focus on procedural steps or methodologies and are also multi-aspect due to the variability in approaches. The methods for accomplishing task may differ based on the specific objective, level of detail required, or alternative techniques available. In contrast to reason-type questions, instructiontype responses tend to align in similar direction rather than opposing one another, as procedural explanations often contain variations but remain conceptually related. Effectively answering these questions requires retrieving diverse procedural descriptions and synthesizing them into coherent instructional format. Debate-type questions seek to gather balanced perspectives on contentious issue, making them inherently multi-aspect. Unlike factual inquiries, debate questions involve subjective stances that depend on underlying assumptions, resulting in contrasting viewpoints. Since different perspectives arise from distinct premises, responses must be structured to fairly represent multiple arguments rather than favoring single standpoint. To ensure balanced synthesis of perspectives, responses should incorporate reasoning from opposing viewpoints and structure the final answer in mediator-like format. In our implementation, we reference the debate mediator prompt from Liang et al. (2024) to ensure that responses objectively aggregate and present diverse perspectives. C.2 Parameter Settings of LLMs In accordance with the LINKAGE settings, we consistently use nucleus sampling parameter (top_p = 0.95) and maximum output tokens of 512 for all LLM cases. The temperature is generally set to 0.8; however, it is lowered to 0.1 specifically for the purpose of annotating reference answers. To generate answers for NFQA using the RAGbased QA system, the retriever identifies five passages, which are then provided to the generator as references. For Wikipedia-based datasets, BM25 serves as the retriever, leveraging the Wikipedia corpus preprocessed by Karpukhin et al. (2020) as the external corpus."
        },
        {
            "title": "D Detailed Analysis per Dataset",
            "content": "We further analyze the performance on individual datasets to understand the strengths of Typed-RAG in different contexts. NQ-NF, SQuAD-NF, and TriviaQA-NF These datasets consist of non-factoid questions that are often open-ended or require explanatory answers. Typed-RAG achieves significant improvements in MRR and MPR scores, indicating its proficiency in generating detailed and relevant responses. For example, on the SQuAD-NF dataset with the Mistral7B base model and GPT-4o mini scorer, TypedRAG achieves an MRR of 0.7444, outperforming the LLM (0.4222) and RAG (0.3817) methods. HotpotQA-NF and MuSiQue-NF These datasets involve multi-hop reasoning, where the answer requires combining information from multiple sources. Typed-RAG shows marked improvements, particularly in MPR scores. The 18 NFQ Type NQ-NF SQD-NF TQA-NF 2WMHQA-NF HQA-NF MSQ-NF Total Evidence-based Comparison Experience Reason Instruction Debate Total 99 5 0 19 2 1 126 130 18 20 85 21 300 251 4 8 23 3 7 296 10 0 1 55 8 5 79 22 8 10 15 4 62 43 1 2 21 11 4 82 555 (58.73%) 36 (3.81%) 41 (4.34%) 218 (23.07%) 49 (5.19%) 46 (4.87%) 945 Table 3: Statistics of the Wiki-NFQA dataset. ability of Typed-RAG to decompose questions into types and aspects allows it to navigate complex reasoning paths more effectively than the baselines. 2WikiMultiHopQA-NF Although all methods show relatively lower performance on this dataset, Typed-RAG still outperforms both LLM and RAG approaches. This suggests that even in challenging scenarios requiring extensive reasoning, TypedRAG provides tangible benefit. 19 p - n h , ) ( a ) ( . o k c x t n r n , ) ( ) ( . t t q t - h r s r l e : 6 1 g e e e w , r t s - n o e n , ) ( , a fi p . a g r e a h a , r q p u t c a e e . n t t h e f a e i q e p P , ) ( . w g n e c h"
        },
        {
            "title": "E Case Study",
            "content": "NFQ Type Example of Question Response of Typed-RAG Evidence-based How does sterilisation help to keep the money flow even? The term \"sterilisation\" is often used in economics to refer to the act of preventing currency or securities from affecting the monetary base due to interest rate differentials. When central bank engages in open market operations to buy or sell securities, it may want to prevent the money flow from being disrupted by interest rate differentials in different markets. Sterilisation helps to keep the money flow even by offsetting the effect of open market operations on the monetary base. When central bank buys securities, it injects cash into the economy and increases the monetary base. However, if this leads to an appreciation of the domestic currency, it may reduce the competitiveness of exports and increase the cost of imports. To prevent this, the central bank may sell an equivalent amount of foreign currency in the foreign exchange market, which reduces the monetary base and helps to offset the initial effect of the open market operation. if central bank sells securities, it Similarly, decreases the monetary base and may lead to depreciation of the domestic currency, which can make exports more competitive and imports more expensive. To prevent this, the central bank may buy an equivalent amount of foreign currency in the foreign exchange market, which increases the monetary base and helps to offset the initial effect of the open market operation. In summary, sterilisation is tool used by central banks to keep the money flow even and maintain control over the monetary base, despite open market operations that may otherwise disrupt the flow of funds. It allows central banks to implement monetary policy while minimizing the impact on currency values and other macroeconomic variables."
        },
        {
            "title": "Continued on next page",
            "content": "21 NFQ Type Example of Question Response of Typed-RAG Comparison what is the difference between dysphagia and odynophagia Dysphagia and odynophagia are two distinct conditions related to swallowing. Dysphagia is the medical term for difficulty swallowing. It can be caused by various conditions, including neurological disorders, stroke, cancer, or structural problems in the throat or esophagus. Symptoms of dysphagia include the sensation of food sticking in the throat or chest, coughing or choking during eating, and pain or discomfort while swallowing. On the other hand, odynophagia is the medical term for pain while swallowing. It is symptom of an underlying condition, such as sore throat, inflammation, or infection. The pain can be described as sharp or throbbing and may be worsened by swallowing solid or acidic foods. Odynophagia can also be symptom of gastroesophageal reflux disease (GERD) or eosinophilic esophagitis. In summary, dysphagia is condition that makes it difficult to swallow, while odynophagia is symptom of pain while swallowing. person with dysphagia may also experience pain while swallowing, but the primary issue is the difficulty in moving food from the mouth to the stomach. Conversely, person with odynophagia primarily experiences pain while swallowing, but they may still be able to swallow relatively normally."
        },
        {
            "title": "Continued on next page",
            "content": "22 NFQ Type Example of Question Response of Typed-RAG Experience What are some of the best Portuguese wines?"
        },
        {
            "title": "Reason",
            "content": "Kresy, which roughly was part of the land beyond the so-called Curson Line, was drawn for what reason? Based on the available references in the document, some Portuguese wines that are often considered excellent include: 1. The reference 14098516 might be referring to Port Wine, fortified wine produced in the Douro Valley region. Port Wine is known for its sweet and rich flavors and comes in various styles such as Tawny, Ruby, and White Port. 2. Reference 5043240 could be referring to Vinho Verde, light, slightly effervescent wine from the Vinho Verde region in the north of Portugal. It is typically low in alcohol and is often served chilled. 3. The Douro region is also famous for its red wines, such as those referenced in 2866435. These wines are known for their robust flavors and are often blended from variety of grape varieties. 4. The reference 4891311 might be referring to Madeira Wine, another fortified wine from the Madeira Islands. Madeira Wine is known for its complex flavors and can range from sweet to dry. 5. Lastly, the reference 309483 could be referring to Dã Wines, which come from the Dã region in central Portugal. These wines are known for their elegance and balance, with flavors of red fruit, herbs, and minerals. So, based on the given references, some of the best Portuguese wines include Port Wine, Vinho Verde, Douro Red Wines, Madeira Wine, and Dã Wines. The Curzon Line refers to the boundary proposed by Lord Curzon, then Viceroy of India, in 1920, as potential eastern border for future independent Poland. Kresy, or Eastern Borderlands, was the area of land beyond this proposed border. It was drawn for various reasons, including historical, ethnic, and strategic considerations. The area had been part of the Polish state in the past, and there was significant Polish population there. Ethnically, the region was diverse, with various groups including Poles, Ukrainians, Belarusians, and Jews. Strategically, the region was important due to its location near the borders of several neighboring countries and its rich natural resources."
        },
        {
            "title": "Continued on next page",
            "content": "23 NFQ Type Example of Question Response of Typed-RAG Instruction How can you find lodge to ask to be member of? To find lodge to inquire about membership, consider the following steps: 1. Contact the Grand Lodge of your jurisdiction (state or country) for list of lodges in your area. You can usually find this information on their website or by contacting them directly. 2. Research the lodges on the list to learn more about their history, traditions, and meeting times. This can be done through their websites, social media pages, or by contacting them directly. 3. Attend few meetings as visitor to get feel for the lodge and its members. You may need to request an invitation from the lodges secretary or master. 4. Speak with current members about their experiences and the expectations for new members. This can help you determine if the lodge is good fit for you. 5. Consider your availability and the lodges meeting schedule to ensure you can commit to attending meetings regularly. Remember, every lodge is unique, so its essential to find one that aligns with your values and interests. Good luck in your search! (Note: The references provided do not contain information relevant to finding lodge to ask to be member of.) Based on the information provided in the references, \"I Can See Your Voice\" is South Korean reality show that offers singers chance to audition without revealing their appearance or singing voice to the judges initially. This format is designed to evaluate contestants based on their charisma, rhythm, and overall performance without any biases influenced by their physical appearance or singing voice. Therefore, the show provides an opportunity for talented singers who might not have been given chance due to their looks or voice in their initial auditions to showcase their abilities and pursue their dreams of stardom."
        },
        {
            "title": "Debate",
            "content": "I Can See Your Voice, reality show from South Korea, offers what kind of performers chance to make their dreams of stardom reality? Table 4: Examples of Typed-RAG responses for different types of non-factoid questions."
        }
    ],
    "affiliations": [
        "Ewha Womans University",
        "Independent Researcher",
        "KT",
        "LLM Experimental Lab, MODULABS",
        "Pohang University of Science and Technology",
        "Sookmyung Womens University"
    ]
}