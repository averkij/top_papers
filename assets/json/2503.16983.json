{
    "paper_title": "Enabling Versatile Controls for Video Diffusion Models",
    "authors": [
        "Xu Zhang",
        "Hao Zhou",
        "Haoming Qin",
        "Xiaobin Lu",
        "Jiaxing Yan",
        "Guanzhong Wang",
        "Zeyu Chen",
        "Yi Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite substantial progress in text-to-video generation, achieving precise and flexible control over fine-grained spatiotemporal attributes remains a significant unresolved challenge in video generation research. To address these limitations, we introduce VCtrl (also termed PP-VCtrl), a novel framework designed to enable fine-grained control over pre-trained video diffusion models in a unified manner. VCtrl integrates diverse user-specified control signals-such as Canny edges, segmentation masks, and human keypoints-into pretrained video diffusion models via a generalizable conditional module capable of uniformly encoding multiple types of auxiliary signals without modifying the underlying generator. Additionally, we design a unified control signal encoding pipeline and a sparse residual connection mechanism to efficiently incorporate control representations. Comprehensive experiments and human evaluations demonstrate that VCtrl effectively enhances controllability and generation quality. The source code and pre-trained models are publicly available and implemented using the PaddlePaddle framework at http://github.com/PaddlePaddle/PaddleMIX/tree/develop/ppdiffusers/examples/ppvctrl."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 3 8 9 6 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Enabling Versatile Controls for Video Diffusion Models",
            "content": "Xu Zhang1, Hao Zhou1, Haoming Qin1,2, Xiaobin Lu1,3, Jiaxing Yan1, Guanzhong Wang1, Zeyu Chen1, Yi Liu1 1 PaddlePaddle Team, Baidu Inc., 2 Xiamen University, 3 Sun Yat-sen University, https://pp-vctrl.github.io Figure 1. Examples generated by VCtrl (also termed PP-VCtrl) using reference frames and text prompts. VCtrl enables users to guide large pretrained video diffusion models using diverse controls, including Canny edges (top), segmentation masks (middle), and human keypoints (bottom), generating high-quality videos that accurately adhere to the provided control signals."
        },
        {
            "title": "Abstract",
            "content": "Despite substantial progress in text-to-video generation, achieving precise and flexible control over fine-grained spatiotemporal attributes remains significant unresolved challenge in video generation research. To address these limitations, we introduce VCtrl (also termed PP-VCtrl), novel framework designed to enable fine-grained control over pre-trained video diffusion models in unified manner. VCtrl integrates diverse user-specified control signalssuch as Canny edges, segmentation masks, and human keypointsinto pretrained video diffusion models via generalizable conditional module capable of uniformly encoding multiple types of auxiliary signals without modifying the underlying generator. Additionally, we design unified control signal encoding pipeline and sparse residual connection mechanism to efficiently incorporate control representations. Comprehensive experiments and human evaluations demonstrate that VCtrl effectively enhances controllability and generation quality. The source code and pre-trained models are publicly available and implemented using the PaddlePaddle framework at https:// github.com/PaddlePaddle/PaddleMIX/tree/ develop/ppdiffusers/examples/ppvctrl. 1. Introduction Significant advancements in text-to-video diffusion models [14, 26, 30, 40, 47, 65] have revolutionized video creation and editing by enabling automatic synthesis from natural language descriptions. Despite these advances, existing text-driven methods often struggle to achieve precise control over fine-grained spatiotemporal elements, such as motion trajectories, temporal coherence, and scene transitions. This limitation typically necessitates iterative and inefficient prompt engineering to achieve desired results. To address these challenges, researchers have explored the use of supplementary conditioning signals, including structural cues [7, 29, 72], motion data [23, 33, 58, 60], and geometric information [8, 43, 70]. However, current research predominantly adopts task-specific approaches, such as human image animation [21, 36], text-guided inpainting [72], and motion-guided generation [31, 58, 66], leading to fragmented methodologies and limited cross-task flexibility. This fragmentation highlights the need for more unified frameworks that can generalize across diverse video synthesis tasks while maintaining fine-grained control over spatiotemporal dynamics. Several unified frameworks have emerged to address these limitations, yet critical challenges remain unresolved. First, existing methods like Text2Video-Zero [28], ControlA-Video [6], and Videocomposer [56] primarily adapt image generation models instead of architectures explicitly designed for video generation, resulting in compromised temporal coherence and visual quality. Second, unlike the image domain where ControlNet [68] provides unified and extensible control framework, current video generation approaches [6, 56] remain tightly coupled to particular base models, restricting their scalability and broader applicability. Third, despite abundant raw video data, the lack of effective preprocessing and filtering strategies has resulted in scarcity of high-quality datasets for controllable video generation. Collectively, these issues contribute to fragmented methodological landscape, impeding progress toward generalizable and unified framework for controllable video generation. In this work, we introduce VCtrl (also termed PP-VCtrl), novel architecture designed to enable fine-grained control over pre-trained video diffusion models in unified manner. Our approach introduces an auxiliary conditioning module while maintaining the original generators architecture intact. By leveraging the rich representations learned during pretraining, VCtrl achieves flexible control across diverse conditioning signals with minimal additional computational overhead. Our approach features unified control signal encoding process that transforms diverse conditioning inputs into unified representation, while incorporating task-aware masks to enhance adaptability across different applications. The integration with the base network is accomplished through sparse residual connection mechanism, facilitating controlled feature propagation while maintaining computational efficiency. Additionally, we develop an efficient data filtering pipeline leveraging advanced preprocessing techniques, recaptioning methods, and task-aware annotation strategies to substantially enhance semantic alignment and overall video generation quality. In summary, VCtrl addresses the fragmented, domainspecific landscape in controllable video generation by providing unified, generalizable framework. Our method: (1) enables unified control over video diffusion models through generalizable architecture that handles multiple control types through conditional module without modifying the base generator; (2) features unified control signal encoding pipeline combined with sparse residual connections, complemented by an efficient data filtering pipeline, enabling precise spatiotemporal control with high computational efficiency; and (3) demonstrates comparable or superior performance to specialized task-specific methods across various controllable video generation tasks, as validated through comprehensive experiments and user studies. 2. Related Work 2.1. Video Diffusion Models Recent years have witnessed significant progress in text-tovideo synthesis (T2V) [1, 5, 11, 12, 14, 19, 26, 47, 55, 69]. Research in this field can be broadly categorized into two main directions. The first direction [2, 10, 13, 57, 71] extends established text-to-image (T2I) frameworks by incorporating specialized components for temporal modeling. The second direction [17, 18, 30, 50, 65] focuses on developing dedicated T2V frameworks trained from scratch. Despite these advancements, existing methods often rely on task-specific architectures and exhibit limited flexibility in handling diverse conditioning signals, which restricts their generalizability and practical applicability. 2.2. Controllable Generation Controllable Image Generation. Advancements in image diffusion models have introduced sophisticated control mechanisms through architectural innovations and refined training strategies. The inherent properties of the diffusion process enable fundamental manipulation capabilities, such as color variation [37] and region-specific inpainting [44]. For spatial control, ControlNet [68] proposes an innovative architecture that augments pre-trained models with spatial conditioning. Similarly, T2I-Adapters [39] achieve multi-condition control via lightweight feature alignment modules. Text-based control is realized through combination of prompt engineering [3], CLIP feature manipulation [9], and cross-attention modulation [15]. These approaches collectively highlight the potential of unified architectures to handle diverse control Figure 2. Overview architecture of VCtrl. control signal (e.g., Canny edges, semantic masks, or pose keypoints) is first encoded by the control encoder. The resulting representation is then additively combined with latent and incorporated into the Video Diffusion Model via the proposed VCtrl module, which leverages sparse residual connection mechanism. After several iterative denoising steps, the refined latent is decoded by pretrained VAE to produce the final video. signals while maintaining model efficiency 3. Methodology Controllable Video Generation. Several works [19, 30, 65] have explored text-based guidance for conditional video generation. However, they often lack fine-grained controls. To address this limitation, recent research has shifted towards integrating additional conditions into video diffusion models. For instance, several studies [35, 53, 63] focus on generating videos conditioned on sequences of human pose maps and reference images. Furthermore, DRAGNUWA [66] and 3DTrajMaster [8] introduce trajectory information to enable fine-grained temporal control. Despite their impressive results, these models often rely on complex condition encoding schemes and domain-specific training strategies. To overcome these challenges, methods such as Text2VideoZero [29], Control-A-Video [7], and VideoComposer [56] adapt text-to-image models for controllable video generation. However, these approaches primarily repurpose image generation models rather than directly leveraging architectures specifically designed for video generation, resulting in suboptimal temporal coherence and visual consistency. To address these limitations, we propose unified framework that supports versatile control conditions, demonstrating scalability and adaptability to more complex scenarios. Our approach is compatible with both text-to-video and image-to-video models, enabling wide range of video generation tasks. We propose unified framework for controllable video generation. Given an input control signal (e.g., Canny edges, semantic masks, or pose keypoints) paired with text prompt, our approach enables precise spatiotemporal control in video synthesis while maintaining high visual fidelity. The overall architecture is illustrated in Figure 2. This section is structured as follows: Section 3.1 reviews essential preliminaries on diffusion models; Section 3.2 introduces the unified control signal encoding process; Section 3.3 details the architecture of our proposed VCtrl module; Section 3.4 presents the sparse residual connection mechanism; Section 3.5 outlines the training methodology; and Section 3.6 describes the data filtering pipeline. 3.1. Preliminaries Diffusion models (DMs) [16, 51] are class of generative models. Latent Diffusion Models (LDMs) [46] extend this framework by operating in learned latent space. For video generation, given an input video RF HW 3, we first encode it into compressed latent representation using pre-trained encoder E: = E(x) Rf hwch, (1) where < , < H, < typically, and ch represents the channel dimension in the latent space. The diffusion process then occurs in this latent space through noiseaddition steps, producing noisy latents z1, z2, ..., zT , where noise is injected into to obtain noise-corrupted latent zt following the defined noise schedule [16]. The reverse process learns to denoise these latents using network ϵθ trained via: = Ez,t,c,ϵ (cid:2)ϵ ϵθ(zt, t, c)2 2 (cid:3) , (2) where [1, ], ϵ (0, I), and represents conditioning vector that provides additional context or constraints to guide the denoising process. 3.2. Unified Control Signal Encoding Existing models often incorporate control mechanisms tailored to specific tasks, limiting their generalizability. To address this, we propose unified control signal encoding process capable of handling diverse control types. Our method integrates control by utilizing control videos as the primary input for encoding control signals, enabling flexible adaptation to diverse controllable generation tasks. This approach enables natural generalization across wide range of control tasks. Subsequently, these signals are represented through cohesive control signal encoding framework, referred to as the Control Encoder. The Control Encoder in VCtrl is designed to process these various control signals in unified manner. It accepts videobased control signal vc RF HW 3 as input, where each frame represents specific control signal at given time step. This format naturally accommodates wide range of control types. The control video is first encoded into latent representation zc using pre-trained variational autoencoder E: (3) zc = E(vc) Rf hwch. To further enhance adaptability, we incorporate task-aware mask sequence Mc {0, 1}f hw alongside the input conditions. For Canny edge and human pose control, this mask indicates whether each frame is conditioned, while for segmentation mask control, it indicates the segmented area. We then concatenate this mask along the channel dimension with the encoded information zc, resulting in combined representation that captures both the latent features and the task-aware control signals. This representation, denoted as: zm = zc Mc, (4) thereby enabling more effective representation of various types of control inputs and enhancing the models adaptability across different tasks. 3.3. VCtrl Module In line with [68], we define the term network block as collection of neural layers that are typically combined to create single unit within neural network, e.g., resnet block, conv-bn-relu block, multi-head attention block, transformer block, etc. For an input feature map xi, the output feature map of the i-th block is calculated as follows, where Θi denotes the parameters associated with that block: yi = i(xi; Θi). (5) b(; Θi In the context of VCtrl, we define base network consisting of total blocks, represented as b) for each block. We freeze the parameters of all blocks in the base model, while introducing parallel sub-network, referred to as the VCtrl module, with trainable parameters Θi at control c(; Θi points selected at fixed intervals, represented as c) for each block. In this study, we utilize CogVideoX [19] as an example to illustrate the capability of VCtrl in augmenting conditional control within large pretrained video diffusion model. The VCtrl module is predicated on lightweight Transformer Encoder architecture, as depicted in orange in Figure 2. Its primary aim is to proficiently receive and process variety of input modalities through series of compact blocks. This architecture seamlessly integrates the initial feature map x0 from the base network with control information extracted from an externally sampled conditioning signal zm, thereby encoding temporal features and enhancing the models capacity to capture intricate temporal relationships through multi-input framework. By design, the VCtrl comprises approximately one-fifth the number of blocks relative to the base network, represented as c) for each block. To ensure precise alignment of control signals with latent representations, we propose DistAlign layer, which adaptively scales the control signals to match latent dimensions. This approach effectively mitigates noise interference arising from discrepancies in signal scales, enhancing the stability and consistency of the generative process. c(; Θi 3.4. Sparse Residual Connection Mechanism To integrate external conditioning information while preserving the stability of large pre-trained models, we propose sparse residual connection mechanism that injects control signals via parallel, lightweight VCtrl sub-networks. In our approach, the base networks parameters remain completely frozen, and the control information is introduced at fixed intervals through additional trainable branches. Let the base network consist of blocks, and suppose we select control points. We define the indices of the blocks of the base network where the control branches are attached as follows: (cid:26) = (k 1) (cid:23) (cid:22) (cid:27)N + 1 . k=1 (6) At each control point I, the output of the base netb(xi b), as defined in Equawork block is given by yi = b; Θi tion 5. In parallel, VCtrl sub-network processes the same input along with an external conditioning vector zm, yielding yi = c(xi c; Θi c). and yi To reconcile potential differences in the spatial and temporal dimensions between yi c, an adaptive average pooling operation, denoted by AdaptiveAvgPool(), is applied to yi c. Specifically, adaptive average pooling automatically adjusts the hidden dimensions of the feature map yi to exactly match those of yi b, ensuring compatibility for feature fusion. The final output at control point i, represents the input feature map of the next block in the base network, is then obtained through residual fusion: xi+1 = yi + AdaptiveAvgPool(yi c). (7) The proposed mechanism employs sparse alignment and residual fusion to enhance generative capabilities. Sparse alignment maintains one-to-one correspondence between VCtrl blocks and base network layers, ensuring balanced injection of control signals while preserving hierarchical structure. Residual fusion utilizes adaptive average pooling to merge control signals effectively with the original features. By freezing the base network and training lightweight VCtrl sub-networks, our method integrates control signals efficiently across Transformer layers with minimal computational overhead. 3.5. Training Given an input video RF HW 3, we first encode it into compact latent representation using pretrained encoder. We progressively introduce noise into this latent representation through iterative steps, obtaining the noisy latent zt at each timestep according to predefined noise schedule. To achieve external controllability in video generation, we introduce new video conditioning signals zm(see Equation 4), while also utilizing existing signals c, such as textual prompt or reference frame, derived from the base network. During training, we freeze the parameters of the pretrained base diffusion model and solely optimize the VCtrl module. The controllable denoising network ϵθ learns to predict the noise added at each timestep guided by these conditioning inputs. We consider ϵ to be drawn from normal distribution ϵ (0, I): LVCtrl = Ez,t,c,zm,ϵ (cid:2)ϵ ϵθ(zt, t, c, zm)2 2 (cid:3) , (8) where ϵ (0, I), represents the training objective for the controllable video diffusion model. This loss directly guides the finetuning process of the diffusion model with the proposed VCtrl module, enabling effective generation of videos that adhere to specified textual and control video constraints. Figure 3. Our Data Filtering Pipeline. Videos refined by an aesthetic filter are recaptioned and processed to extract Canny edges, human keypoints, and segmentation masks, providing training data for diverse controllable tasks. 3.6. Data We utilize three publicly available video datasetsWebVid10M, MiraData [25], and Vript [64]to form an initial corpus comprising approximately 800K text-video pairs. After systematic data processing and filtering, we generate three datasets specific to canny, mask, and human posture control tasks respectively. To ensure high-quality training data, we apply hierarchical filtering pipeline illustrated in Figure 3. Specifically, we conduct the following filtering steps: 1) Visual Filter: We perform visual filtering, including scene segmentation, border removal, and aesthetic filtering [59]. Scene segmentation divides the original video into segments by comparing hash values of consecutive frames. Black borders are identified and removed based on the standard deviation of the color histogram. Subsequently, aesthetic filters exclude low-quality frames, resulting in video clips with resolutions ranging from 446 336 to 1280 720, each containing maximum of 160 frames. 2) CLIP Score Filter: We employ recaptioning model [20] to regenerate detailed and accurate captions for aesthetically filtered videos. To ensure semantic relevance, we calculate CLIP scores comparing both original and regenerated captions against their respective videos, retaining captions above defined quality threshold. 3) Task-Aware Filter: The refined videos undergo task-aware preprocessing for conditional video generation, including: Canny edge detection using hysteresis thresholds and Gaussian smoothing (σ = 1.0)[4]; semantic mask extraction performed via segmentation model[45], maintaining consistent videolevel segmentation and incorporating dynamic multi-target masking and random dilation for robustness; and human pose estimation using pose extraction model [62] that detects 133 keypoints per individual visualized against uniform background, with temporal smoothing applied to ensure motion consistency. 4. Experiments 4.1. Implementation Details We implement VCtrl using generalizable video diffusion architecture compatible with various block-structured base networks for video generation. Due to its superior temporal coherence and strong scalability, in this work, we specifically select CogVideoX-5B [65] as our primary base network, along with its I2V variant (CogVideoX-5B-I2V), which additionally accepts an extra reference frame as input. All input videos, paired with their corresponding condition videos, are set to resolution of 720 480 or 480 720, and 49 consecutive frames are extracted from each video pair to be used as training data. We set the learning rate to 1 105 and employ the Adam optimizer with β1 = 0.9, β2 = 0.999, applying gradient clipping with maximum norm of 1.0 to ensure training stability. To further enhance robustness, we employ truncated normal distribution-based random cropping method, which adaptively selects the cropping center and boundaries based on the videos aspect ratio. The standard deviation is set to 0.25 of the maximum allowable offset in height or width. 4.2. Evaluation Metrics Our evaluation framework addresses two primary dimensions: video quality assessment and control precision analysis. For video quality, we adopt Frechet Video Distance (FVD) [54], Subject Consistency[34], and Aesthetic Score [27]. Due to the absence of standardized metrics for evaluating control precision, we propose three novel metrics inspired by prior works in controllable image and video generation [24, 32, 58], specifically tailored for video generation conditioned on Canny edge maps (Canny-to-Video), binary subject masks (Mask-to-Video), and human keypoint sequences (Pose-to-Video): Canny Matching. Given the ground-truth edge sequence {C gt i=1 and the generated edge sequence {C pred }F i=1 , both extracted using Canny edge detector [4], respectively. We then binarize the obtained edge maps into binary values. Subsequently, edge alignment is quantified using an adaptive Dice coefficient [52] defined as follows: }F (cid:88) (9) i="
        },
        {
            "title": "2\nF",
            "content": "Scanny = + ϵ + ϵ gt + gt pred pred where ϵ = 1e5 prevents division by zero, and denotes pixel count. Masked Subject Consistency (MS-Consistency). Given binary subject masks {Mi}F i=1, we quantify the consistency between the generated video pred and ground truth gt videos by calculating the RGB pixel-wise L1 distance: Smask = (cid:88) i=1 Mi(V pred Mi1 gt )1 (10) Model Canny Matching FVD CogVideoX-5B [65] CogVideoX-5B-I2V [65] Text2Video-Zero [29] Control-A-Video [7] VCtrl-Canny VCtrl-I2V-Canny - - 0.20 0.14 0.24 0.28 1596.51 989.32 1761.82 1298.26 985.31 345.00 Table 1. Quantitative evaluation for Canny-to-Video generation. We report Canny Matching for control effectiveness (higher is better) and FVD for video quality (lower is better). where 1 indicates the L1 norm. Pose Similarity. Adopting VitPose [62] detector, we compute Object Keypoint Similarity (OKS) [61] between generated pose sequences ppred and ground truth pose sequences pgt : Spose ="
        },
        {
            "title": "1\nF",
            "content": "F (cid:88) i="
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) k=1 (cid:32) exp ppred pgt 2σ2 kAi 2 2 (cid:33) (11) where σk denotes keypoint-specific tolerance, Ai is bounding box area, and represents the total number of keypoints (17). 4.3. Qualitative Evaluation For comprehensive qualitative evaluation, we examine our approach from two primary perspectives. First, we illustrate that the proposed VCtrl-I2V model can address diverse video generation tasks. As depicted in Figure 1, our method successfully supports style transfer (Canny), video editing (Mask), and character animation (Pose), consistently producing high-fidelity video content despite substantial motion. Crucially, it preserves spatiotemporal coherence across frames, ensuring smooth and temporally consistent transitions. Second, in Figure 4, we visually compare our models against representative baselines. Control-A-Video [7] generates videos with temporal inconsistencies, exhibiting abrupt content changes between frames. Text2Video-Zero [29] maintains better control adherence but suffers from background artifacts and low quality subject rendering. Our VCtrl-Canny achieves strict canny constraint satisfaction while preserving visual fidelity. The I2V-enhanced version further improves temporal coherence (evident in stable color profiles) and maintains first-frame fidelity throughout the sequence. 4.4. Quantitative Evaluation We present comprehensive quantitative evaluation of our methods against existing representative approaches across three video generation tasks. For each task, we select suitable benchmarks and both established and newly proposed metrics to ensure thorough comparison. Figure 4. Qualitative comparison to previous methods. We compare our method with Control-A-Video [7] and Text2Video-Zero [29], demonstrating superior visual coherence and stronger adherence to the Canny edge conditions. Model MS-Consist. FVD CogVideoX-5B [65] CogVideoX-5B-I2V [65] CoCoCo [72] VCtrl-Mask VCtrl-I2V-Mask - - 0. 0.36 0.63 1592.88 1132.28 961.17 480.86 228.78 Table 2. Quantitative evaluation for Mask-to-Video generation. We report MS-Consistency (higher is better) for control effectiveness and FVD (lower is better) for video quality. Model Pose Similarity FVD CogVideoX-5B-I2V [65] Moore-AnimateAnyone [38] ControlNeXt-SVD [41] VCtrl-I2V-Pose 0. 0.82 0.82 0.98 837.44 702.59 255.50 175.20 Table 3. Quantitative evaluation for Pose-to-Video generation. We report pose matching for control effectiveness and FVD for video quality. Canny-to-Video. We quantitatively evaluate our approach on the Canny-to-Video generation task using videos derived from the Davis dataset [42], where ground-truth edges are extracted using Canny edge detection. The quantitative results presented in Table 1 demonstrate that our VCtrl-based methods surpass existing methods in both control precision and visual quality. Specifically, VCtrl-Canny and VCtrl-I2VCanny achieve improvements of 0.04 and 0.08, respectively, over Text2Video-Zero [29] in terms of the Canny Matching metric. Regarding visual quality measured by the FVD score, VCtrl-Canny and VCtrl-I2V-Canny reduce scores by roughly 292 and 313 points, respectively, compared to Text2VideoZero, and by 611 and 644 points compared to their corresponding base models (CogVideoX-5B and CogVideoX-5BI2V, respectively). Mask-to-Video. We quantitatively evaluate our approach on Mask-to-Video generation using dataset derived from Davis [42], where ground-truth masks are extracted using semantic mask extraction. As summarized in Table 2, our proposed methods consistently outperform existing approaches. Specifically, VCtrl-Mask and VCtrl-I2V-Mask achieve improvements of 0.04 and 0.31 in Masked Subject Consistency and reduce the FVD scores by approximately 480 and 732 points, respectively, compared to CoCoCo [72]. Pose-to-Video. We quantitatively evaluate our approach on the Pose-to-Video generation task using an evaluation set of 100 videos selected from publicly available datasets [22, 48, 49, 67], where ground-truth poses are extracted using human pose estimation. As summarized in Table 3, our proposed method consistently outperforms existing approaches. Specifically, VCtrl-I2V-Pose achieves significant improvement of approximately 0.16 over in Pose Similarity and reductions of roughly 80 points in the FVD score compared to ControlNeXt-SVD [41]. Despite employing relatively simple architecture without sophisticated modules utilized in prior domain-specific methods [35, 63, 66, 72], our comprehensive evaluations demonstrate that VCtrl consistently achieves competitive or superior performance, significantly enhancing the generation capabilities of the base models. Moreover, improvements observed across established video quality metrics such as FVD are consistently corroborated by our newly proposed metrics (Canny Matching, MS-Consistency, and Pose Similarity), which provide more intuitive and precise measure of control effectiveness. These results underscore the effectiveness, efficiency, and adaptability of VCtrl in diverse controllable video generation scenarios. Task Model Canny-to-Video Mask-to-Video Control-A-Video [7] Text2Video-Zero [29] VCtrl-Canny CoCoCo [72] VCtrl-Mask Pose-to-Video Moore-AnimateAnyone [38] ControlNeXt [41] VCtrl-I2V-Pose Overall Temporal Consist. Quality Text Alignment Facial Identity Consist. Pose Consist. Background Consist. 1.47 1.44 2.96 2.05 3. 1.38 2.85 3.30 1.52 1.27 3.13 1.90 3. 1.25 2.71 3.21 2.26 2.38 3.42 2.21 3. - - - - - - - - 1.26 2.50 3.06 - - - - - 1.36 3.04 3.39 - - - 2.50 3. - - - Table 4. User study comparing VCtrl with competing methods. All methods are evaluated using identical inputs for each task, with scores ranging from 1 (lowest) to 5 (highest). Layout FVD Subject Consist. Aesthetic Score Canny Matching Even End Space 1005.98 1449.24 949.46 0.880 0.847 0.884 0.450 0.450 0.473 0.226 0.124 0.248 Table 5. Comparison of control layout designs across multiple metrics. The Space layout achieves superior overall performance, demonstrated by higher visual quality scores, improved Canny matching, and lower FVD score. Model FVD Subject Consist. Aesthetic Score Canny Matching VCtrl-Small 1001.75 VCtrl-Medium 949.46 937.37 VCtrl-Large 0.882 0.884 0.889 0.459 0.473 0. 0.205 0.248 0.231 Table 6. Comparison of VCtrl variants with different model complexities. Models of varying sizes are evaluated comprehensively on visual quality, subject consistency, aesthetic score, and control precision. 4.5. Ablative Study Connection Layout Design. We investigate architectural variations of VCtrl by evaluating three distinct control block connection layouts: even, end, and space. These layouts explore different strategies for integrating control signals using the sparse residual connection mechanism introduced in Section 3.4. Figure 5 illustrates the conceptual differences among these layouts. To ensure fair evaluation, each variant is trained under identical conditions for 35,000 optimization steps on the Canny-to-Video task. Comparing results in Table 5, the space layout consistently delivers better performance, despite employing sparser integration of control signals compared to the even layout. Conversely, concentrating control blocks exclusively toward the end of the network yields the weakest results, suggesting that distributed integration of control signals is essential for optimal performance. Complexity. To systematically investigate the balance between computational complexity and control performance, we conduct experiments with three variants of VCtrl, varying the ratio of VCtrl blocks to base network blocks: VCtrlSmall (1:15), VCtrl-Medium (1:5), and VCtrl-Large (1:2). Each variant was trained for an identical total of 35,000 optimization steps to ensure fair comparison. Table 6 presents Figure 5. Control Layouts. (a) Even: control signals uniformly injected throughout the network; (b) End: control signals densely injected toward the end of the network; (c) Space: control signals sparsely and evenly distributed across the network. quantitative comparison across multiple metrics in video generation task guided by Canny edges. Despite significant parameter reductions compared to the base network and VCtrl-Large, VCtrl-Medium maintains robust performance. This suggests that lightweight models can maintain control effectiveness while enhancing computational efficiency, which is crucial for many real-world applications. Thus, we adopt VCtrl-Medium as our primary model variant for the Canny, Pose, and Mask tasks, as it achieves an optimal trade-off between model complexity and performance. 4.6. User Study We conducted user study to quantitatively evaluate our proposed methods against established baselines on three conditional video generation tasks. For each task, we selected 20 representative video samples, which were independently assessed by domain experts in blind evaluation setting. Participants rated each video on scale from 1 (lowest) to 5 (highest) across multiple task-aware criteria, including Overall Quality, Temporal Consistency, Text Alignment, Facial Identity Consistency, Pose Consistency, and Background Consistency. The detailed results are summarized in Table 4. Our proposed methods consistently outperform existing baselines: VCtrl-Canny shows superior overall quality and temporal consistency; VCtrl-Mask significantly surpasses CoCoCo [72] in overall and background consistency metrics; and VCtrl-I2V-Pose notably improves overall quality, temporal coherence, facial identity, and pose consistency. These findings underscore the effectiveness and robustness of our proposed approaches for versertile controllable video generation scenarios. 5. Conclusion We introduce unified framework for controllable video generation, effectively integrating diverse controls through unified control signal encoding strategy and generalizable conditional module. Our sparse residual connection mechanism seamlessly incorporates these unified representations into pretrained video diffusion models, enabling precise and flexible video synthesis. Extensive experiments validate our frameworks effectiveness across various demonstrated through quantitative evaluations and human assessments. Additionally, the lightweight and modular design ensures broad compatibility, facilitating future adaptation to wider range of video generation architectures and applications. controllable generation tasks,"
        },
        {
            "title": "References",
            "content": "[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477, 2023. 2 [2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 2 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. arXiv preprint arXiv:2211.09800, 2022. 2 [4] John Canny. computational approach to edge detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, (6):679698, 1986. 5, 6 [5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter1: Open diffusion models for high-quality video generation, 2023. 2 [6] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. arXiv preprint arXiv:2305.13840, 2023. [7] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models, 2023. 2, 3, 6, 7, 8 [8] Xiao Fu, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation. In ICLR, 2025. 2, 3 [9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2 [10] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, MingYu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2293022941, 2023. 2 [11] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing textto-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709, 2023. 2 [12] Jiaxi Gu, Shicong Wang, Haoyu Zhao, Tianyi Lu, Xing Zhang, Zuxuan Wu, Songcen Xu, Wei Zhang, Yu-Gang Jiang, and Hang Xu. Reuse and diffuse: Iterative denoising for text-tovideo generation. arXiv preprint arXiv:2309.03549, 2023. [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2 [14] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity arXiv preprint video generation with arbitrary lengths. arXiv:2211.13221, 2022. 2 [15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 2 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. 3, 4 [17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 2 [18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. [19] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2, 3, 4 [20] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500, 2024. 5 [21] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 2 [22] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1275312762, 2021. 7 [23] Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. Vmc: Video motion customization using temporal attention adaption for text-to-video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 92129221, 2024. 2 [24] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. Humansd: native skeleton-guided diffusion model for human image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1598815998, 2023. [25] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. Advances in Neural Information Processing Systems, 37:4895548970, 2025. 5 [26] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion video synthesis with stable diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2268022690, 2023. 2 [27] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 51485157, 2021. 6 [28] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. IEEE International Conference on Computer Vision (ICCV), 2023. 2 [29] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Textto-image diffusion models are zero-shot video generators. arXiv preprint arXiv:2303.13439, 2023. 2, 3, 6, 7, 8 [30] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, [31] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Yuexian Zou, and Ying Shan. Image conductor: Precision control for interactive video synthesis. arXiv preprint arXiv:2406.15339, 2024. 2 [32] Zejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and Lingyun Sun. Image synthesis from layout with localityaware mask adaption. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13819 13828, 2021. 6 [33] Pengyang Ling, Jiazi Bu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Tong Wu, Huaian Chen, Jiaqi Wang, and Yi Jin. Motionclone: Training-free motion cloning for controllable video generation. arXiv preprint arXiv:2406.05338, 2024. 2 [34] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. 2023. 6 [35] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying Shan, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. arXiv preprint arXiv:2304.01186, 2023. 3, 7 [36] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 41174125, 2024. 2 [37] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. [38] MooreThreads. animation Moore-animateanyone: Characreenactment). ter https : / / github . com / MooreThreads / Moore - AnimateAnyone, 2024. 7, 8 (animateanyone, face [39] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. 2 [40] OpenAI. Sora: Creating video from text. https:// openai.com/sora, 2024. 2 [41] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, MingChang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. 7, 8 [42] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander SorkineHornung. benchmark dataset and evaluation methodology for video object segmentation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 7 [43] Haonan Qiu, Zhaoxi Chen, Zhouxia Wang, Yingqing He, Menghan Xia, and Ziwei Liu. Freetraj: Tuning-free trajectory control in video diffusion models. arXiv preprint arXiv:2406.16863, 2024. 2 [44] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2): 3, 2022. 2 [45] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. 3 [47] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. 2 [48] Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019. 7 [49] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13653 13662, 2021. 7 [50] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [51] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 3 [52] Carole Sudre, Wenqi Li, Tom Vercauteren, Sebastien Ourselin, and Jorge Cardoso. Generalised dice overlap as deep learning loss function for highly unbalanced segmentations. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support: Third International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, Quebec City, QC, Canada, September 14, Proceedings 3, pages 240248. Springer, 2017. 6 [53] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: High-quality identity-preserving human image animation. arXiv preprint arXiv:2411.17697, 2024. 3 [54] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. 6 [55] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2 [56] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. arXiv preprint arXiv:2306.02018, 2023. 2, 3 [57] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yuming Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua Lin, Yu Qiao, and Ziwei Liu. Lavie: High-quality video generation with cascaded latent diffusion models, 2023. [58] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 2, 6 [59] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Towards explainable in-the-wild video quality assessment: database and language-prompted approach. In Proceedings of the 31st ACM International Conference on Multimedia, pages 10451054, 2023. 5 [60] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anything using entity representation. In European Conference on Computer Vision, pages 331348. Springer, 2024. 2 [61] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation and tracking. In European Conference on Computer Vision (ECCV), 2018. 6 [62] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. Advances in neural information processing systems, 35:3857138584, 2022. 5, 6 [63] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In CVPR, 2024. 3, 7 [64] Dongjie Yang, Suyuan Huang, Chengqiang Lu, Xiaodong Han, Haoxin Zhang, Yan Gao, Yao Hu, and Hai Zhao. Vript: video is worth thousands of words. Advances in Neural Information Processing Systems, 37:5724057261, 2025. 5 [65] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. CoRR, 2024. 2, 3, 6, [66] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 2, 3, 7 [67] Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network for arXiv preprint pose-guided human video generation. arXiv:1910.09139, 2019. 7 [68] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding In conditional control to text-to-image diffusion models. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 2, 4 [69] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. 2 [70] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. 2 [71] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video arXiv preprint generation with latent diffusion models. arXiv:2211.11018, 2022. [72] Bojia Zi, Shihao Zhao, Xianbiao Qi, Jianan Wang, Yukai Shi, Qianyu Chen, Bin Liang, Kam-Fai Wong, and Lei Zhang. Cococo: Improving text-guided video inpainting for better consistency, controllability and compatibility. arXiv preprint arXiv:2403.12035, 2024. 2, 7, 8,"
        }
    ],
    "affiliations": [
        "PaddlePaddle Team, Baidu Inc.",
        "Sun Yat-sen University",
        "Xiamen University"
    ]
}