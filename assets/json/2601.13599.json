{
    "paper_title": "Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion",
    "authors": [
        "Linrui Ma",
        "Yufei Cui",
        "Kai Han",
        "Yunhe Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability. However, existing block-based diffusion studies tend to introduce autoregressive priors, which, while offering benefits, can cause models to lose this global coherence at the macro level. To regain global contextual understanding while preserving the advantages of the semi-autoregressive paradigm, we propose Diffusion in Diffusion, a 'draft-then-refine' framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilize snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model's global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using only 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 2 ] . [ 2 9 9 5 3 1 . 1 0 6 2 : r Preprint. Under review. Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion Linrui Ma, Yufei Cui, Kai Han& Yunhe Wang Noahs Ark Lab, Huawei Montreal, Canada & Beijing, China {kai.han, yunhe.wang}@huawei.com"
        },
        {
            "title": "Abstract",
            "content": "One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability. However, existing block-based diffusion studies tend to introduce autoregressive priors, which, while offering benefits, can cause models to lose this global coherence at the macro level. To regain global contextual understanding while preserving the advantages of the semi-autoregressive paradigm, we propose DIFFUSION IN DIFFUSION draft-then-refine framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with larger bidirectional receptive field. We utilize snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion models global capabilities. Empirical results demonstrate that our approach sets new benchmark for discrete diffusion models on the OpenWebText dataset. Using only 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models."
        },
        {
            "title": "Introduction",
            "content": "Today, research on discrete diffusion models (Austin et al., 2021; Lou et al., 2024) is divided into two major paradigms: global diffusion models and block diffusion models (also known as Semi-Autoregressive diffusion models). Global diffusion models (Sahoo et al., 2024; Nie et al.) possess strong global planning capabilities and complete bidirectional context, resulting in text with high global consistency. However, they also prevent the application of Key-Value Cachethe foundation for inference acceleration in modern language models (Vaswani et al., 2017; Brown et al., 2020) leading to an O(L2) complexity during inference and extremely slow processing. In contrast, block diffusion models (Arriola et al., 2025) represent an emerging semi-AR architecture that incorporates autoregressive priors. They enable parallel diffusion execution within blocks while employing inter-block autoregression for sequence generation. This enables rapid inference leveraging KV Cache while benefiting from the performance gains of autoregressive priors. However, its macrolevel autoregressive paradigm imposes shortsightedness, sacrificing diffusions core essencethe global receptive fieldand degenerating into local model. It also introduces irreversibility similar to AR, preventing modifications to already written content. Current research often seeks balance between the following trade-offs: stronger global planning capabilities necessitate sacrificing inference speed, while better autoregressive priors and faster inference require sacrificing global perspective. We contend that this dichotomy is not irreconcilable. Future architectures should not be inherently AR models Project Leader Corresponding Authors 1 Preprint. Under review. with minor local diffusion biases, but rather novel diffusion frameworks that preserve semiAR advantages while retaining global vision and planning capabilities. By reintroducing global capabilities into Block Diffusion models, we aim to break the impossible triangle of speed vs. quality vs. global consistency in discrete diffusion, resulting in unified architecture that combines the strengths of both approaches. In this study, we propose novel generative framework called DIFFUSION IN DIFFUSION (also known as Structured Block Diffusion), designing multi-stage draft-then-revise paradigm to bring back the global ability to the aforementioned semi-autoregressive diffusion models. Inspired by the human writing processrapidly drafting first version followed by holistic revision to ensure coherenceour approach decomposes generation into progressive stages: During the initial drafting phase, the model employs small blocksize for fast, efficient generation, focusing first on specific grammar and local content. In subsequent revision phases, the model re-examines and refines the previously generated sequence using larger blocksize (global receptive field). We also innovatively introduce Remasking mechanism into Block-Diffusion Models, which selectively transforms tokens within the generated sequence back into noise. This enables the model to resample these positions within more holistic context for optimization. Our work makes three key contributions: 1. Structural Diffusion Framework: We propose multi-stage generative framework that progressively increases the block size through multiple stages. This enables the model to first rapidly generate drafts using semi-regressive methods, then fine-tune content with high quality via bidirectional global diffusion, effectively combining the strengths of both paradigms. 2. Snapshot Confidence Remask: We evaluated multiple inter-stage remask token selection strategies and determined that snapshot confidence remaskbased on token generation-time confidenceis the most effective approach, outperforming methods like Post-hoc likelihood evaluation. 3. Mix-Scale Training Strategy: We also propose an enhanced Block Diffusion training approach. By mixing block masks of minimal and maximal scales, the model gains multi-granularity information processing capabilities, enabling both generation and revision functions. Empirical results demonstrate that our approach sets new benchmark for discrete diffusion models on the OpenWebText dataset using only one-quarter of the fine-tuning budget of comparable models: generating perplexity drops from 25.7 to 21.9, significantly narrowing the gap with autoregressive benchmarks."
        },
        {
            "title": "2 Preliminaries",
            "content": "2.1 Autoregressive Models In discrete (tokenized) sequence = [x1, . . . , xL] of length L, each token xi is one-hot vector of model vocabulary V. The standard autoregressive model decomposes the joint probability distribution p(x) into product of conditional probabilities: log pθ(x) = i=1 Here, x<i denotes the prefix token preceding position i. The conditional distribution pθ(xix<i) is typically modeled via causal attention mechanism. Although AR models can generate highly fluent and natural text and support key-value caching, their inherently sequential nature precludes straightforward parallel generation. log pθ(xix<i), (1) 2.2 Block Diffusion Models Block Diffusion Language Models (BD3-LMs)(Arriola et al., 2025) lie between autoregressive and diffusion paradigms. They predefine fixed block size (assuming = L/B) and 2 Preprint. Under review. partition the entire sequence into blocks (denoted as x1, . . . , xB). Each block is subsequently treated as metatoken at the autoregressive level, log pθ(x) = b=1 log pθ(xbx<b), (2) Here, xb denotes the token sequence in the bth block of x, while x<b represents all preceding blocks. Unlike autoregressive models, the conditional distribution within blocks, pθ(xbx<b), is modeled via discrete diffusion process. Forward Masking Process Following the masked diffusion framework (Austin et al., 2021; Sahoo et al., 2024), in the forward process q(xb xb), each token in is independently converted to the special masking token [MASK] (denoted as m) via according to transition probabilities derived from noisy scheduling parameter αt [0, 1]: q(xb xb) = (cid:16) j=1 αtI(xb t,j = xb ) + (1 αt)I(xb t,j = m) (cid:17) , (3) where goes from 0 to 1, larger denotes noisier timestep, and = 1 indicates full noise, and xb represents the j-th token within the b-th block. Reverse Denoising Process & Training The objective of the generation process (i.e., the denoising procedure) is to learn to reverse this noise-adding process. The aim is to train the denoising network xθ to predict clean current block xb based on the context provided by the current noisy block xb (subject to its current level of random noise) and the previously denoised clean blocks x<b. Here, BD3-LM employs key-value cache to leverage prior blocks x<b, whilst simultaneously enabling parallel denoising within blocks and decoding multiple tokens concurrently. Consistent with conventional MDLM practices, the models training objective is to minimise the negative evidence lower bound (NELBO)(Sahoo et al., 2024): LBD(x; θ) = b=1 t[0,1] q(xb xb) (cid:20) α 1 αt LCE(pθ(xb , x<b), xb) . (4) (cid:21) This objective effectively trains the model to perform infilling or conditional generation for the current block for any noise level given the history."
        },
        {
            "title": "3 Structural Block Diffusion",
            "content": "We introduce Structural Block Diffusion, multi-stage generation paradigm designed to overcome the myopic limitation of standard autoregressive block diffusion. The core intuition is to decompose the generation process into drafting phase and one or more revision phases. Our approach progressively increases the size of diffusion blocks at each stage, thereby enlarging the bidirectional receptive field for each generation. Between stages, low-confidence filtering scheme selects the tokens most in need of modification. This partially circumvents the irreversibility inherent in the AR paradigm, enabling the model to revisit earlier generation errors with more global perspective and greater contextual information. 3.1 Overview of Multi-Stage Generation Formally, we define generation process consisting of stages. Let B(k) denote the block size and γ(k) denote the re-masking ratio for stage {1, . . . , K}. We enforce progressive scaling of the block size such that B(1) < B(2) < < B(K) = L. In the initial stage (k = 1), the model generates draft sequence ˆx(1) using small block size (e.g., B(1) = 4), which prioritizes local coherence and generation speed but may lack global consistency. For subsequent refinement stages > 1, the process operates as follows: 3 Preprint. Under review. Figure 1: Overview of DIFFUSION IN DIFFUSION method 1. Confidence Estimation: We estimate the confidence score si for each token ˆx in the sequence generated from the previous stage. We choose to record the confidence at the moment of every tokens sampling and form confidence trajectory, we name this snapshot confidence(details in Sec. 3.2). (k1) 2. Inter-Stage Remasking: We identify the set of tokens to retain, Mkeep, by selecting the top (1 γ(k)) tokens with the highest confidence scores. The remaining tokens are reset to the mask token m. 3. Global Refinement: We perform block diffusion sampling on (k) init using the larger block size B(k). Unlike the empty initialization in Stage 1, Stage starts from partially filled sequence. Then we utilize this enhanced global context vision to do infillings for the remasked tokens and thereby improve the overall coherence. When B(K) equals the sequence length L, the final stage degenerates into full-sequence masked diffusion process (similar to MDLM (Sahoo et al., 2024)), but with better partial initialization and useful structural prior from the first stages draft. Full procedure is summarized in Algorithm 1. 3.2 Inter-Stage Remasking Strategy The transition between stages acts as quality filter. Instead of carrying over the entire sequence ˆx(k1) to the next stage, we aim to selectively preserve high-quality tokens while resetting uncertain ones to the mask token m. This allows the subsequent stage, equipped with larger receptive field (block size), to focus its generation capacity on infilling the problematic regions. There are two key components in this stage: confidence-measuring metric and remasking policy. Snapshot Confidence Estimation To select tokens in need of correction from the draft sequence ˆx(k1) generated in the first stage, an intuitive approach involves performing another forward pass on the generated sequence to obtain likelihood values (posterior confidence). However, this method is highly prone to overconfidence the model repeatedly affirms its own hallucinatory tokens, thereby failing to provide any meaningful guidance. 4 Preprint. Under review. k=1, Ratios {γ(k)}K k=2 Algorithm 1 Structural Block Diffusion Sampling Require: L, xθ, Stages K, Block Sizes {B(k)}K 1: mL 2: 0L 3: for = 1 to do B(k) L/B(k) 4: if > 1 then 5: 6: 7: 8: 9: 10: 11: 12: 13: argsort(S)[1 : γ(k) L] xI {Inter-stage Remasking} end if Kcache, Vcache for = 1 to B(k) do xb x[(b 1)B(k) : bB(k)] xb, SAMPLE(xθ, xb, Kcache, Vcache) {Block Diffusion} , Kb, Vb xθ(xb) x1:b1 xb xb+1:B(k) 1:b1 b+1:B(k) Kcache, Vcache Kcache Kb, Vcache Vb 14: 15: 16: 17: 18: end for 19: return end for We regard the uncertainty exhibited by the model at the moment of decision as free and useful indicator signal, hence we propose Snapshot Confidence. More formally, when the token at position transitions from to specific value V, that token is effectively , x<b) denote the probability determined at the particular time step t. Let pθ(xi = vxb predicted by the denoiser at this transition step (where is the block containing i). We define the snapshot confidence si for token as: si = pθ( ˆx (k1) xb , x<b), (5) where denotes the diffusion timestep where token was unmasked. This metric preserves the dynamic uncertainty inherent in the generation process, thereby providing more reliable indication of potential errors compared to static post-generation evaluation. Ratio-based Masking Policy After gaining the snapshot confidence trajectory = [s1, . . . , sL], we use masking ratio γ(k) [0, 1]to determine in high level how many tokens to be revised. This ratio controls the extent of revision: low γ(k) indicates trusting if the stage 1 draft and only revising minority of tokens, while high γ(k) indicates introducing more global revision forces. Moreover, this γ(k) also acts as user-defined glider controlling the tradeoff between Performance and Efficiency. We first sort the confidence trajectory in an ascent order, then the set of indices to be masked, is chosen from the bottom γ-quantile. Then the input sequence for the subsequent (k) init, is constructed by remasking the selected tokens at position to the stage k, denoted as mask token m: (k) init,i = (cid:40) ˆx (k1) if otherwise (6) By remasking the lowest-confidence tokens from the sequence generated in the previous stage, we pre-construct structured skeleton for the subsequent phase. This enables the effective correction of local inconsistencies through conditional filling operations, leveraging more reliable global dependencies. 5 Preprint. Under review. 3.3 Mixed-Scale Training Objective Standard training of block diffusion models typically fixes the block size as static hyperparameter during fine-tuning (e.g., = 16 or = 32) (Arriola et al., 2025). While effective for single-stage generation, this approach leads to models that are highly specialized to specific granularity. We observe that model trained exclusively on small blocks fails to generalize to the global context required for the refinement stage, often exhibiting high perplexity or incoherent infilling when presented with large block size (e.g., = 1024). Meanwhile, training only on large blocks is also inefficient in computation and degrades the model to standard MDLM, hurting the block diffusion local modeling ability required for the initial drafting phase. To strike balance between these two approaches, we propose hybrid-scale training strategy. We reformulate our optimisation problem by treating the block size as random variable sampled from the distribution PB. The objective function is thereby transformed into the expected value of the block size: LMixed(θ) = EBPB [LBD(x; θ, B)] , (7) where LBD(x; θ, B) is the block diffusion loss (Eq. 4) computed using block decomposition size B. Bimodal Block Distribution The design of PB is crucial for balancing the drafting and refining capabilities. We employ bimodal distribution parameterized by mixing coefficient λ [0, 1]: PB(s) = 1 λ if = Bdraft (Small) if = Bglobal (Large) λ otherwise 0 (8) where Bdraft (e.g., 4) corresponds to the drafting stage and Bglobal (e.g., 1024) corresponds to the revision stage. Empirically, we find that small fraction of global exposure is sufficient to unlock the models revision capabilities without compromising local generation quality. We set λ = 0.1, meaning 10% of the training samples are processed as single large blocks (effectively fullsequence masked diffusion), while the remaining 90% use small autoregressive blocks. This asymmetry reflects the inference computational budget: the model performs intensive generation at the small scale, while the large scale is primarily used for sparse edits and consistency checks. This mixed objective also prevents the model from overfitting to the positional biases of autoregressive boundaries, making the models performance across varying block sizes during inference more robust."
        },
        {
            "title": "4 Experiments",
            "content": "Setup We opted to follow the baseline configuration outlined in Arriola et al. (2025) when evaluating our approach (Gokaslan & Cohen, 2019) on the OpenWebText (OWT) dataset. We utilised Transformer backbone network with 110 million parameters (12 layers, 768 hidden dimensions). To ensure fair comparisons, we initialise the model using the official pre-trained checkpoint provided by Arriola et al. (2025), which was pre-trained for 850,000 gradient steps at the maximum block size (L = L). From this initialization, we fine-tune the model using our Mixed-Scale Training objective (Section 3.3) to enable both drafting and revision capabilities. Distinct from the baseline recipe, which fine-tunes for 150K gradient steps on fixed small block size, we fine-tune for only 40K gradient steps (One-quarter of 150K), translating to 467B tokens. This reduced training budget highlights the data efficiency of our approach. During fine-tuning, we adopt the variance-reduction technique from Arriola et al. (2025) by adaptively learning the range of masking rates (optimizing parameters β, ω) to minimize the gradient variance of the diffusion loss. All other hyperparameters, including the optimizer and learning rate schedule, follow the original implementation. All of our methods utilize draft block size of 4. 6 Preprint. Under review. 4.1 Generation Quality We quantitatively assess the quality of generated samples by computing perplexity (Gen PPL). As diffusion models cannot compute perplexity autonomously, this metric is derived using pre-trained GPT-2-Large model. Unlike the static likelihood evaluation on groundtruth data, this metric measures the coherence and fluency of the models actual open-ended generations. Table 1 shows the unconditional generation results for multiple different models at lengths = 1024 and = 2048. = 1024 = 2048 Gen. PPL NFEs Gen. PPL NFEs Model AR SEDD MDLM Prior Block Diffusion Work SSD-LM (L = 25) BD3-LM (L = 16) BD3-LM (L = 8) BD3-LM (L = 4) 14.1 52.0 46.8 37.2 33.4 30.4 25.7 25.0 1K 1K 1K 40K 1K 1K 1K 1.5K 13.2 41.3 35.3 35.3 31.5 28.2 23.6 22. Structural Block Diffusion (Using 26% Tuning Data) Ours (Stage 1 only) 25.1 Ours (Full 2-Stage) 22.5 21.2 20.6 1.0K 1.1K 1.2K 1.5K 27.4 24.6 22.6 21.9 2K 2K 2K 80K 2K 2K 2K 3K 2.0K 2.2K 2.5K 3.0K Table 1: Generative perplexity (Gen. PPL; ) and number of function evaluations (NFEs) for unconditional generation of lengths = 1024 and = 2048. Generative perplexity is evaluated using GPT2-Large. Numbers for AR, SEDD, MDLM, SSD-LM and BD3-LM models are borrowed from Arriola et al. (2025). As shown in Table 1, our Stage 1 only model serves as the drafting baseline with Gen PPL of 27.4, slightly higher than BD3LM(L = 4), given that our training budget is quartered. Applying the Stage 2 global revision dramatically reduces the perplexity to 21.9 (a relative improvement of 20%). This result empirically validates our core hypothesis: while small blocks are sufficient for local syntax, they suffer from myopic errors that accumulate over long sequences. The second stage effectively corrects these long-range inconsistencies by leveraging the global receptive field. Notably, this performance is achieved using only 26% of the tuning budget (40K steps) compared to the fully converged baselines (150K steps), highlighting the extreme data efficiency of our structural refinement approach. Our method establishes new state-of-the-art for diffusion-based models at this scale. We outperform the strongest baseline, BD3-LM (L = 4), reducing Gen PPL from 25.0 (same NFEs) to 21.9 at = 1024. This advantage widens for longer sequences (L = 2048), where our method achieves Gen PPL of 20.6 compared to the baselines best of 22.8 (same NFEs). By effectively narrowing the gap to autoregressive models (14.1), Structural Block Diffusion demonstrates that the Draft-then-Revise paradigm significantly enhances the expressivity of diffusion architectures. Quality-Efficiency Trade-off To address the concern of increased computation for multistage generations, we explicitly analyze this Quality-Speed trade-off using NFEs. Our method offers flexible Pareto frontier rather than fixed cost: Minimal Overhead (1.1K NFEs): With just 10% increase in compute over the standard baseline, our method achieves PPL of 24.6, already surpassing the standard BD3-LM (25.7). Preprint. Under review. Iso-compute Superiority (1.5K NFEs): When we allow the baseline BD3-LM more compute steps (1.5K) to match our re-masking budget, it only improves to 25.0. In contrast, our method reaches 21.9 under the same computational budget. This further proves that our performance gains stem from framework design rather than from increased sampling steps. Maximum Quality (3.0K NFEs): To get optimal performance, further extending the compute budget yields the best result of 20.6, opening up possibilities that are impossible for single-pass models. 4.2 Ablations and Analysis We conducted comprehensive ablation experiments to validate multiple key design choices. Unless otherwise specified, all ablation experiments were performed on the OWT dataset using the default two-stage configuration."
        },
        {
            "title": "4.2.1 Impact of Revision Scope (Block Size & Ratio).",
            "content": "The efficacy of the revision stage depends on two hyperparameters: the Stage 2 block size B(2) (determining the receptive field) and the masking ratio γ (determining the extent of modification). Figure 2 visualizes the Generative Perplexity across different Stage 2 block sizes B(2) and revision ratios γ. Global Context is Necessary We observe clear performance gain as the block size increases. As B(2) increases from 4 to 1024, the perplexity curves shift downwards monotonically. Employing small block sizes (e.g., B(2) {4, 16}) in the second stage fails to beat the Stage 1 baseline (the gray dashed line), and in some cases even degrades quality. We start to get significant gains when B(2) 64, with the best performance achieved at B(2) = 1024. This confirms that the success of the revision phase hinges entirely upon the newly added access to the global context. Figure 2: Ablation on Revision Scope. Generative Perplexity (Gen PPL) as function of the Stage 2 revision ratio γ across varying block sizes B(2). The gray dashed line represents the Stage 1 baseline (BS=4). The U-Shaped Trade-off Regarding the masking ratio γ, results exhibit convex U-shaped trend across all effective block sizes, minimizing perplexity between 0.25 to 0.5. Too low ratios (γ 0.1) overly constrain the revision process, while too high ratios (γ 0.75) degrade performance by discarding the critical structural skeleton provided by the draft. Interestingly, as γ 1.0, performance explodes beyond the baseline, indicating the Block Diffusion model degrades to an MDLM model, demonstrating that the retention of partial drafts is crucial for stabilising the high variance inherent in purely non-autoregressive generation. 4.2.2 Effectiveness of Remasking Strategy. We investigate how the token selection policy affects refinement quality using fixed revision setting (B(2) = 1024, γ = 0.5). We compare our Snapshot Confidence strategy against two variants: (1) Random Masking, and (2) Post-hoc Confidence. Table 2 reveals two critical insights regarding the difficulty of effective refinement: 8 Preprint. Under review. Remasking Strategy Gen. PPL () Baseline (Stage 1 Output) Random Masking Post-hoc Confidence Snapshot Confidence (Ours) 27.36 30.26 29.85 21.85 Table 2: Ablation on Remasking Strategy. Comparison of Generative Perplexity (Gen. PPL) using different token selection policies for the revision stage (Stage 2). All experiments use B(2) = 1024 and γ = 0.5. Notably, only our Snapshot Confidence strategy improves upon the Stage 1 baseline, while other strategies degrade performance. 1. Blind Masking is Hurtful Random masking significantly degrades performance, increasing the PPL from 27.36 to 30.26. This demonstrates that indiscriminately disrupting structural frameworks yields detrimental effects: when meaningful tokens are arbitrarily removed, the model struggles to reconstruct coherent text. 2. Post-hoc Scores Fail The method based on Post-hoc confidence selection also failed to surpass the baseline level (29.85). We attribute this to model overconfidence: once the sequence is fully generated, the model tends to persistently believe its prior hallucinations. This strategy likely preserves errors without providing any useful signals, while simultaneously obscuring valid but low-probability tokens, leading to further deterioration in performance. By contrast, our snapshot confidence strategy significantly reduces the PPL to 21.85. By capturing the uncertainty dynamics during diffusion, this strategy successfully locks in tokens that the model initially struggles to generate, enabling the second stage to focus on correcting actual errors rather than disrupting valid structures. 4.2.3 Training Mix Configuration. Finally, we justify our specific choice of the Mixed-Scale Training objective (Section 3.3). We compare our default Bimodal distribution (B {4, 1024}) against three alternatives: Baseline (No Mix): Fine-tuning only on small blocks (B = 4). Comparison (Scale): Mixing small blocks with an intermediate size (B {4, 512}). Comparison II (Complexity): uniform mixture of all power-of-two scales {4, 16, 64, . . . , 1024}. Training Configuration Stage 1 PPL Stage 2 PPL Baseline (No Mix) Uniform Mixture ({4, 16, . . . , 1024}) Bimodal Mix (4, 512) Bimodal Mix (4, 1024) 27.95 31.98 27.26 27. 31.97 22.60 21.46 21.85 Table 3: Ablation on Training Mix Strategy. Evaluating the impact of the Mixed-Scale Training objective. We report the PPL for the initial draft (Stage 1, = 4) and the final revision (Stage 2, = 1024). The Baseline is naively Block-Diffusion trained model. Table 3 summarizes the perplexity at both drafting (Stage 1) and revision (Stage 2) phases. It first shows that mixed training is necessary for the framework. The Baseline model, trained only with the vanilla Block Diffusion Training, completely fails at the revision stage (PPL degrades from 27.95 to 31.97). This confirms that the standard block diffusion model cannot be generalised to global scenarios under zero-shot conditions, and explicit training of large blocks is prerequisite for achieving structural refinement. Preprint. Under review. Bimodal vs. Uniform The complex Uniform Mixture strategy performs poorly in the drafting phase (Stage 1 PPL 31.98 vs. 27.36 for Bimodal). We attribute this to an overly intrusive optimisation process: excessively granular training block levels undermine the models foundational capabilities, resulting in low-quality drafts that prove more challenging to refine. Bimodal training delivers clearer, more explicit multi-task signals. Optimal Revision Scale Comparing the bimodal strategies, Mix(4, 512) and Mix(4, 1024) perform comparably, with the 512-mix achieving marginally lower perplexity (21.46 vs. 21.85). While intermediate scales are easier to optimize, we select Mix(4, 1024) as our default to ensure the model is theoretically capable of strictly full-sequence modeling without windowing artifacts, securing robust performance for the target evaluation length of = 1024."
        },
        {
            "title": "5 Related Works",
            "content": "Discrete Diffusion Language Models Discrete diffusion models have now emerged as strong competitors to autoregressive paradigms. They are renowned for their nonautoregressive parallel generation mechanism, which offers greater controllability and flexibility than traditional AR models. Early explorations like D3PM (Austin et al., 2021) modeled the forward noise process using transition matrix that extended the Gaussian diffusion model to discrete state space. Subsequently, masked discrete diffusion models (Gu et al., 2022; Li et al., 2022) gained prominence by explicitly incorporating mask token to assist in reversing data corruption. Recently, approaches like SEDD (Lou et al., 2024) and MDLM (Sahoo et al., 2024) unified discrete diffusion processes with continuous time steps. By modulating noise timestep into the model, they further reduced the language modeling perplexity of diffusion models. Recent attempts to scale masked discrete diffusion models to 7B-scale (Nie et al.; Ye et al., 2025) and beyond have demonstrated advantages in planning, mathematical, and coding tasks, further expanding their potential. The core architectural advantage of these models lies in their global bidirectional attention mechanism, which enables exceptional performance in global text structure planning and infilling tasks. However, this mechanism precludes the use of key-value caching techniques, resulting in quadratic inference complexity with sequence length. This limitation severely restricts their applicability in long-context generation tasks. Semi-Autoregressive Block Generation To address the efficiency challenges, researchers proposed the semi-autoregressive paradigm. Characterized by autoregressive behavior at the macro level and diffusion at the micro level, this approach leverages KV Cache to reduce computational while preserving certain degree of local bidirectional parallelism. SSD-LM (Han et al., 2023) initially introduced the semi-AR diffusion model, achieving linear inference complexity through iterative text block generation. Subsequently, the Block Discrete Denoising Diffusion Model (Arriola et al., 2025) further optimized this paradigm. It employs inter-block autoregression and intra-block bidirectional diffusion, enabling KV cache utilization while allowing flexible control over generation length. However, this macro-level autoregressive approach also introduces inherent compromises. In semi-AR models, blocks can be abstracted as metatokens. These metatokens are constrained by strict unidirectional and irreversible dependencies, resulting in the loss of diffusion models inherent global planning capabilities. Moreover, early stage generation errors become irrevocable and cannot be corrected based on subsequent context, inevitably compromising consistency in long-text generation. Iterative Refinement and Multi-Stage Generation Our approach is also closely related to the Draft-then-Revise paradigm found in multi-stage generation. In the AR domain, Speculative Decoding (Leviathan et al., 2023; Chen et al., 2023) leverages small model to rapidly draft tokens and large model to verify them, aiming to accelerate inference without degrading quality. More fundamentally, there are models like the Levenshtein Transformer (Gu et al., 2019) optimize text through iterative insertion and deletion operations. While Speculative Decoding mainly focuses on speeding up generation, our proposed Diffusion in Diffusion focuses on improving quality by reintroducing global planning and coherence 10 Preprint. Under review. to BD3-LMs. Rather than simply rejecting tokens based on likelihood, we use snapshot confidence to identify structural defects and bring back the global receptive field for repair. Fundamentally, our approach is nested iterative diffusion process carefully designed to restore the long-range modeling capabilities of global diffusion within an efficient blockbased diffusion framework."
        },
        {
            "title": "6 Conclusion",
            "content": "This work aims to address the fundamental trade-off of global coherence and inference efficiency in semi-autoregressive diffusion models. We propose DIFFUSION IN DIFFUSION, structural diffusion framework that improves both the global coherence and the generation quality of existing block diffusion models with minimal overhead. This approach introduces multi-stage refinement mechanism based on snapshot confidence and mix-scale training strategy, re-injecting the long-range planning capability of Global Diffusion into an efficient block-generation framework. Results demonstrate that this Draft-then-Revise paradigm achieves state-of-the-art generation perplexity performance on OpenWebText with exceptional data efficiency, significantly narrowing the performance gap with traditional autoregressive models. 11 Preprint. Under review."
        },
        {
            "title": "References",
            "content": "Marianne Arriola, Subham Sekhar Sahoo, Aaron Gokaslan, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Justin Chiu, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=tyEyYT267x. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/ OpenWebTextCorpus, 2019. Jiatao Gu, Changhan Wang, and Junbo Zhao. Levenshtein transformer. Advances in neural information processing systems, 32, 2019. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1069610706, June 2022. Xiaochuang Han, Sachin Kumar, and Yulia Tsvetkov. Ssd-lm: Semi-autoregressive simplexbased diffusion language model for text generation and modular control. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in neural information processing systems, 35:43284343, 2022. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In International Conference on Machine Learning, pp. 3281932848. PMLR, 2024. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, JUN ZHOU, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. Large language diffusion models. In ICLR 2025 Workshop on Deep Generative Model in Machine Learning: Theory, Principle and Efficacy. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, URL https://proceedings.neurips.cc/paper files/paper/2017/file/ Inc., 2017. 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025."
        }
    ],
    "affiliations": [
        "Beijing, China",
        "Montreal, Canada",
        "Noahs Ark Lab, Huawei"
    ]
}