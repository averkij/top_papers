{
    "paper_title": "EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World?",
    "authors": [
        "Yuqian Yuan",
        "Ronghao Dang",
        "Long Li",
        "Wentong Li",
        "Dian Jiao",
        "Xin Li",
        "Deli Zhao",
        "Fan Wang",
        "Wenqiao Zhang",
        "Jun Xiao",
        "Yueting Zhuang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing object's appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users' interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop a mixed-format human-in-the-loop annotation framework with four types of questions and design a novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as a crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing a robust foundation for developing reliable core models for embodied systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 7 8 2 5 0 . 6 0 5 2 : r EOC-Bench: Can MLLMs Identify, Recall, and Forecast Objects in an Egocentric World? Yuqian Yuan1,2,3 Ronghao Dang2,3 Long Li2,3 Wentong Li1 Dian Jiao1 Xin Li2,3 Deli Zhao2,3 Fan Wang2,3 Wenqiao Zhang1 Jun Xiao1 Yueting Zhuang1 1Zhejiang University 2DAMO Academy, Alibaba Group 3Hupan Lab"
        },
        {
            "title": "Abstract",
            "content": "The emergence of multimodal large language models (MLLMs) has driven breakthroughs in egocentric vision applications. These applications necessitate persistent, context-aware understanding of objects, as users interact with tools in dynamic and cluttered environments. However, existing embodied benchmarks primarily focus on static scene exploration, emphasizing objects appearance and spatial attributes while neglecting the assessment of dynamic changes arising from users interactions. To address this gap, we introduce EOC-Bench, an innovative benchmark designed to systematically evaluate object-centric embodied cognition in dynamic egocentric scenarios. Specially, EOC-Bench features 3,277 meticulously annotated QA pairs categorized into three temporal categories: Past, Present, and Future, covering 11 fine-grained evaluation dimensions and 3 visual object referencing types. To ensure thorough assessment, we develop mixed-format human-inthe-loop annotation framework with four types of questions and design novel multi-scale temporal accuracy metric for open-ended temporal evaluation. Based on EOC-Bench, we conduct comprehensive evaluations of various proprietary, open-source, and object-level MLLMs. EOC-Bench serves as crucial tool for advancing the embodied object cognitive capabilities of MLLMs, establishing robust foundation for developing reliable core models for embodied systems."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of multimodal large language models (MLLMs) [1, 2, 3, 4] has paved the way for the development of intelligent systems that can comprehend and interact with the visual world. Among these innovations, egocentric vision, where systems perceive environments from human-like first-person perspective, has gained significant attention due to its critical applications in fields such as augmented reality [5], embodied AI [6, 7] and robotic manipulation [8, 9, 10]. Understanding objects precisely within egocentric contexts presents unique challenges that extend beyond conventional vision tasks. It demands continuously evolving, context-aware comprehension of objects, encompassing their types, usages, states, and interactions, as users dynamically interact with tools and undertake various operational tasks. In egocentric environments, particularly in densely cluttered settings like kitchens and laboratories, objects exhibit three critical properties: (1) Fleeting visibility, indicating dynamic changes in state and position due to frequent occlusions and shifts in viewpoint; (2) Visual ambiguity, arising from similar-looking items in close spatial proximity; and (3) Temporal dependency, where current states rely on historical interactions and inform future *Equal contribution. Preprint."
        },
        {
            "title": "Benchmark",
            "content": "#Videos #Samples Question Type"
        },
        {
            "title": "Visual\nPrompts",
            "content": "MVBench [11] VideoRefer-Bench [12] Charades-STA [13] ScanQA [14] SQA3D [15] Env-QA [16] OpenEQA [17] VSI-Bench [18] ECBench [19] EOC-Bench (Ours) 3,673 598 1,334 - - 3,489 180 288 386 4,000 1,400 4,233 4,976 2,143 12,760 1,600 5,000 4,324 3,277 Close Open/Close Open Open Open Open Open Open/Close Open/Close Open/Close Template Human Automatic/Human Automatic/Human Human Template Human Template/Human Human"
        },
        {
            "title": "Human",
            "content": "(6%) P, B, Table 1: Comparison of widely adopted Embodied/General VideoQA benchmarks with our EOC-Bench. P, B, and represent visual prompts for object referencing, specifically as point, box, mask and arrow, respectively. outcomes. Successful object perception in these scenarios requires models capable of sustaining persistent visual grounding while simultaneously processing spatiotemporal details. Unfortunately, existing benchmarks fail to systematically evaluate this capability. As shown in Table 1, existing embodied benchmarks, such as the closed-vocabulary ScanQA [14] and SQA3D [15], focus on understanding static scene through closed-vocabulary queries based on task-specific datasets. Consequently, these benchmarks lack the scope to evaluate task-generalized MLLMs for broader cognitive capabilities. More recently, OpenEQA [17], VSI-Bench [18], and ECBench [19] have developed open-vocabulary benchmarks to evaluate MLLMs question-answering (QA) capabilities in indoor embodied video contexts. Despite these promising advancements, current benchmarks primarily concentrate on static scene exploration, such as home tours, and predominantly evaluate appearance and spatial attributes. They often overlook dynamic interactions within egocentric operational environments, where users engage actively with tools and perform various tasks involving objects. Building these capabilities is crucial for advancing embodied system. To bridge this gap, we introduce EOC-Bench, novel object-centric video benchmark designed to rigorously evaluate the object cognition capabilities of MLLMs in egocentric operational scenarios. Drawing on the premise that effective AI assistants must comprehensively comprehend objects across temporal dimensions, EOC-Bench structures questions into three temporally grounded categories: Past, Present, and Future. Past: The Past category evaluates MLLMs ability to perceive and understand the historical dynamics of objects, skill crucial for enhancing long-term task execution. As illustrated in Fig. 1, this capability is further subdivided into four types: Object State Retrospection, Object Location Retrospection, Object Relationship Evolution, and Absolute Time Perception. The last is particularly vital, as accurate timestamp awareness of model can contextualize interactions and temporal changes, which has received little attention in previous benchmarks. Present: The Present category test MLLMs perception of scene information at the current moment. Importantly, resolving these questions often require more than just observing the current frame; comprehensive understanding of the entire video is necessary for accuracy. As shown in Fig. 1, in addition to common abilities such as Immediate State Recognition, Purpose and Function Inference, and Object Relationship, we introduce Anomaly Perception to handle embodied tasks in specific scenarios. This capability tests whether MLLMs can avoid being misled by counterintuitive arrangements within the scene and answer questions based on factual information about the objects. Future: By observing the world, human can not only understand past events but also predict future occurrences. The capability to foresee future events in objects is crucial for avoiding dangers and adapting to new situations. For instance, as shown in the State Change Prediction part in Fig. 1, if model identifies heat-sensitive object near heat source, it can anticipate temperature changes and alert people to move the object to prevent hazards. Future prediction types are divided into State Change Prediction, Dynamic Relationship Prediction, and Trajectory and Motion Prediction, assessing MLLMs proficiency in forecasting dynamic interactions and movements. To ensure comprehensive evaluation, we develop mixed-format annotation framework featuring diverse question types (e.g., true/false, single-choice, multiple-choice and open-ended questions), as visualized in Fig. 1. Specially, for open-ended questions, we focus on continuous temporal analysis and introduce multi-scale temporal accuracy metric to quantitatively assess temporal perception performance. Additionally, traditional text-based prompts for object referencing often fail 2 Figure 1: Overview of EOC-Bench. EOC-Bench assesses Embodied Object Cognition capabilities of MLLMs in egocentric videos across three dimensions - Past, Present, and Future - encompassing 11 categories. EOC-Bench includes 3,277 object-level QA pairs utilizing mixed-format human-inthe-loop annotation framework across diverse tasks and conditions. EOC-Bench aims to reveal the limitations of MLLMs and promote the development of robust egocentric cognition systems. to clearly specify objects in dynamic egocentric scenes. Descriptions like \"the leftmost bowl\" become meaningless when containers are rearranged during washing, and \"the spoon\" lacks clarity among multiple candidates in the kitchen. To address this issue, we introduce visual referencing prompts, including point, box and mask, as shown in Fig. 1, which provide persistent, unequivocal object references while preserving the spatiotemporal context essential for precise object comprehension. The final benchmark includes 3,277 question-answer pairs, covering 11 fine-grained evaluation dimensions and 3 object referencing types. We have conducted meticulous human-in-the-loop labeling process, followed by comprehensive cross-checking and verification to ensure quality. Building upon our EOC-Bench benchmark, we systematically evaluate the egocentric object cognition capabilities of range of MLLMs, including both open-source and proprietary general-purpose models [3, 4, 1, 20, 21], as well as specialized object-level MLLMs [12, 22, 23]. Notably, all mainstream MLLMs exhibit clear deficiencies in object-level temporal perception, particularly concerning absolute temporal awareness, where they significantly lags behind human-level performance, emphasizing its difficulty and relevance for our community."
        },
        {
            "title": "2.1 General Video Understanding Benchmarks",
            "content": "With the advancement of MLLMs [3, 24, 1, 21, 25, 26, 27, 4, 2, 28, 29, 30, 31, 32, 33, 34], which have demonstrated strong visual understanding and reasoning capabilities, there is an increasing emphasis on comprehensively and systematically evaluating their video understanding abilities [11, 35, 36, 37]. Existing video understanding benchmarks primarily focus on general-purpose video comprehension tasks, such as action recognition [38, 39, 40], video caption [41, 42, 43], temporal grounding [38, 44, 13], temporal reasoning [45, 46, 47, 44], long video understanding [48, 49, 11, 50, 36, 51], video referring [12] and expert-level reasoning [52, 53]. For instance, Video-MME [35] conducts an extensive evaluation of MLLMs across variety of video-related tasks, such as recognition and perception. Similarly, MVBench [11] introduces an innovative framework for constructing spatialtemporal tasks. However, general VideoQA benchmarks predominantly focus on YouTube videos that capture everyday life, human actions, and movies, often neglecting to include egocentric videos and embodied-specific QA formats. 3 Figure 2: Overall data distribution of EOC-Bench. (a) EOC-Bench encompasses three temporal dimensions: Past, Present, and Future, comprehensively evaluating 11 embodied cognitive abilities. (b) The dataset comprises videos from four distinct open video sources as well as self-recorded videos. (c) It spans wide range of scenarios, offering rich diversity of contexts for analysis."
        },
        {
            "title": "2.2 Embodied Video Understanding Benchmarks",
            "content": "In embodied scenarios, VideoQA-based evaluations serve as effective tools for assessing models comprehension of its environment and tasks. Datasets such as ScanQA [14], SQA3D [15], and Env-QA [54] are typically used for traditional scene question answering, characterized by closed vocabulary. These datasets often exhibit strong text bias and offer relatively limited variety of question forms. On the other hand, RoboVQA [16], EgoPlan-Bench [55] & EgoPlan-Bench2 [56], and PCA-Bench [57] are introduced test the task-planning abilities of MLLMs. EgoSchema [58] utilizes first-person footage from Ego4D [59] to enable video reasoning tasks. More recently, VSI-Bench [18] has been developed to specifically evaluate visual-spatial intelligence in MLLMs, and STI-Bench [60] has been further developed to evaluate the spatial-temporal world understanding. OpenEQA [17] and ECBench [19] systematically investigate the embodied indoor cognition of MLLMs, providing wider scope of evaluation diversity. However, these benchmarks mainly focus on static scene exploration, neglecting dynamic first-person operational interactions involving hand and object movements. Furthermore, while these benchmarks try to assess models embodied cognitive abilities through text-based object referencing, they fall short of adequately evaluating models capabilities in object-level spatiotemporal reasoning, which is crucial for real-world interactions. In contrast, we have meticulously crafted EOC-Bench to systematically analyze the object-level embodied cognition of MLLMs in complex dynamic operational scenes."
        },
        {
            "title": "3.1 Overview",
            "content": "As illustrated in Fig. 2, we introduce EOC-Bench, meticulously crafted benchmark designed to quantitatively assess the object cognition abilities of MLLMs using dynamic egocentric videos. EOC-Bench comprises 3,277 question-answer pairs derived from 656 real-world videos. These videos are sourced from four publicly available first-person datasets: EPIC-KITCHENS [61], Ego4D [59], Charades-ego [62], and MECCANO [63], as well as our self-recorded videos captured in various environments. EOC-Bench includes three dimensions: Past, Present and Future, with total of 11 tasks aimed at evaluating models object comprehension capabilities including memory, perception and knowledge in ego-centric world. Notably, to achieve accurate object referencing in dynamic scenarios, we introduce three types of visual object prompts: bounding boxes, points and masks."
        },
        {
            "title": "3.2.1 Video Collection",
            "content": "Our benchmark integrates four established egocentric video datasets: EPIC-KITCHENS [61], which features kitchen-related scenarios; Ego4D [59], encompassing broad array of daily activities; 4 Charades-ego [62], capturing activity instances across various rooms; and MECCANO [63], depicting industrial-like environments where participants construct toy models. These datasets collectively cover both indoor and outdoor environments, covering wide spectrum of activities. To enhance scenario diversity, we develop stratified sampling strategy. Initially, we sample 1,000 videos each from Charades-ego [62] and Ego4D [59] and annotate them for scene categories using Qwen2-VL72B [24]. We further enhance scene diversity by randomly sampling from videos featuring the same setting, followed by thorough manual quality control to eliminate clips with low information. This process results in 294 high-quality videos from Charades-ego and 201 from Ego4D. For datasets like EPIC-KITCHENS and MECCANO with uniform scenes, we randomly choose 239 and 12 representative videos, respectively. All selected videos are uniformly trimmed to durations of 3-10 minutes for efficient annotation. To address gaps in existing datasets, we self-curate 110 videos capturing three under-represented domains: anomaly perception, physical world dynamics, and electrical appliance operation. To ensure diversity, 5 volunteers contribute to the collection process."
        },
        {
            "title": "3.2.2 Capability Taxonomy",
            "content": "Drawing inspiration from established general VideoQA benchmarks [11, 36], we propose hierarchical taxonomy to systematically characterize embodied object cognition capabilities, as shown in Figure 2-(a). EOC-Bench comprehensively encompasses three temporal dimensions of first-person video understanding: Past, Present, and Future. Past. This dimension assesses models ability to perceive and interpret the temporal dynamics of objects, critical skill for long-term and complex operations. This capability enables models to enhance their current understanding by integrating insights from past interactions. The Past dimension is specifically divided into four categories: Object State Retrospection (OSR): Evaluates the capability to monitor changes in object attributes including color, shape, size, posture, temperature, and motion. Object Location Retrospection (OLR): Measures historical positioning accuracy across multiple granularity: macro-level (room-scale), meso-level (platform/container positioning), and micro-level (precise location). Object Relationship Evolution (ORE): Examines changes in object relationships, encompassing spatial relationships, motion state dynamics, and temporal sequence relationships. Absolute Time Perception (ATP): Assesses absolute time cognition precision through two key aspects, including pinpointing specific time points and understanding time durations. Present. This category focuses on evaluating MLLMs ability to understand current scenes, with focus on the perceptual abilities. Crucially, while emphasizing immediate perception of object states and environmental conditions, some questions necessitate integration of information from preceding frames, demanding comprehensive understanding of the video for accurate responses. This aspect is categorized into four types: Immediate State Recognition (ISR): Evaluates the models ability to identify the current state of objects, including attributes such as material, shape, functional state, surface condition, pose, motion state, and temperature. Object Relationship (OR): Analyzes inter-object dynamics, including spatial, functional, or comparative relationships between existing objects. Purpose and Function Inference (PFI): Requires deducing the potential uses or functions of objects based on their external characteristics, materials, configurations, and the contextual scenarios in which they are observed. Anomaly Perception (AP): Measures the models proficiency in detecting unusual or incongruous visual inputs, with an emphasis on counter-sense co-occurrence. For instance, Fig. 1 illustrates scenario where cosmetic product is placed in an atypical setting, such as kitchen, to assess common sense interference in visual interpretation. Future. In embodied intelligence systems, predictive capabilities extend beyond mere observation, empowering proactive adaptation to environmental changes. The capability to foresee future events is crucial for avoiding hazards and flexibly adapting to changing circumstances. This dimension 5 Figure 3: Statistic analysis of EOC-Bench: (a) substantial diversity in object categories and usage taxonomies, (b) wide range of video durations correlated with question count, and (c) balanced distribution of response options across each question type. relies on the models ability to utilize physical laws and common sense knowledge for prediction and inference. This dimension is divided into three categories: Trajectory and Motion Prediction (TMP): Anticipates the future path or dynamic motion changes of an object based on its current motion and location, enabling models to understand and interact with moving objects more effectively. State Change Prediction (SCP): Predicts future changes in an objects state due to ongoing actions or environmental fluctuations, enabling preemptive response to imminent changes. Dynamic Relationship Prediction (DRP): Foresees potential alterations in inter-object relationships, aiding in the prevention of upcoming collisions or other interactions."
        },
        {
            "title": "3.2.3 Construction of Question-Answer Pairs",
            "content": "To ensure the high quality of our benchmark, we have developed sophisticated human-in-the-loop data curation pipeline specifically for the creation of EOC-Bench, and we recruit 10 highly trained university students as annotators to participate in the annotation process. Our methodology adopts category-independent approach, assigning volunteers predetermined number of tasks related to various cognitive abilities. This strategy guarantees balanced representation of question-answer (QA) pairs, covering both rare and common cognitive abilities. EOC-Bench features mixed-format annotation framework with four types of labeling: True/False, Single-Choice, Multiple-Choice questions, which require explicit options, while Open-Ended questions are crafted to primarily focus on absolute timestamps information for temporal perception abilities. Despite leveraging human-annotated data sources and implementing meticulously designed QA generation protocol, certain ambiguities and errors may still occur, such as visual prompt offsets, omissions, and ambiguous options. To address these issues, thorough filtering process is carried out post-labeling. This involves rigorous cross-checking and verification among annotators to ensure both format accuracy and content validity."
        },
        {
            "title": "3.2.4 Evaluation Metrics",
            "content": "Our EOC-Bench includes diverse question types: True/False (T F), Single-Choice Answer (SCA), Multiple-Choice Answer (MCA) and Open-Ended Questions (OQ). Following established practices [11, 35], we adopt conventional Accuracy based on exact matches for the first three tasks. For Open-Ended Questions, which require assessing open-ended continuous temporal predictions, we introduce novel metric, Multi-Scale Temporal Accuracy (MST A), to accurately evaluate OQ tasks. Specially, we develop relative error percentage tolerance mechanism to accommodate varying error tolerance across different time durations, whether long or short periods. Given ground truth timestamp Tgt and predicted time Tpred, we first calculate the absolute deviation = Tpred Tgt. We then establish dynamic error margins using relative percentage thresholds = {1%, 10%, 20%, 30%}, setting scale-adaptive boundaries {α Tgt α C}. These thresholds are derived from human error analysis, which is detailed in the Section A.1. prediction satisfies threshold α when α Tgt. The final MST score is computed by averaging performance across temporal scales using: MST = 1 4 (cid:88) αC 1 (T α Tgt) . (1) 6 By utilizing various thresholds, MST strikes balance between strictness and flexibility: lower thresholds demand precise alignment, while higher thresholds allow for variability in responses."
        },
        {
            "title": "3.3 Benchmark Statistics",
            "content": "EOC-Bench comprises 3,277 QA pairs, systematically evaluating MLLMs across 11 cognitive perspectives. These include 1,422 questions focused on the Past dimension, 1,348 on the Present, and 507 on the Future. Each question is associated with one or more objects, and the corresponding visual prompts are annotated on the final frame of the video. The benchmark incorporates wide array of object types, encompassing 728 categories that cover various usage scenarios. The category distribution, along with the top 20 categories, is displayed in Fig. 3-(a). Additionally, Fig. 3-(b) illustrates the distribution of average video durations, which vary widely from several seconds to over six minutes. To maintain an even probability distribution for each response option, we rearranged the order of different answer types, as depicted in Fig. 3-(c)."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Based on EOC-Bench, we comprehensively evaluate diverse range of general-purpose MLLMs, including both proprietary MLLMs and open-source models. For proprietary MLLMs, we evaluate GPT-4o [3], GPT-4o-mini [3] and Gemini-2.0-flash [4]. Among open-source MLLMs, we test Qwen2.5-VL [1], InternVL2.5 [20], VideoLLaMA2&3 [21, 25], LLaVA-OneVision [27], LLaVAVideo [64], NVILA [65], LongVA [31] and VideoLLaVA [28]. Additionally, we assess the objectfocused MLLMs including VideoRefer [12], ViP-LLaVA [23], Osprey [22] and SPHINX-V [66]. For all models, we perform zero-shot inference to assess their object cognition capabilities using their default settings. More detailed configurations are provided in the Section C.1."
        },
        {
            "title": "4.2 Main Results",
            "content": "In this section, we provide detailed performance comparison and analysis. Table 2 reports the main experimental results. Baselines. The Random entry in the first row denotes random guessing. For multiple-choice answers, we randomly select the number of options and the corresponding choices. For open-ended questions in Absolute Time Perception (ATP) task within the Past dimension, values are randomly selected between 0 and video length. Additionally, we also assess human performance on EOC-Bench using video input with three volunteers. Proprietary MLLMs. Despite significant performance gap compared to human capabilities, the leading proprietary model, GPT-4o [3], delivers commendable results with 61.83%. GPT-4o [3] successfully meets the passing criteria across various subtasks, showcasing its potential in multiple domains. However, the model faces challenges in the Past dimension, particularly with Absolute Time Perception (ATP) and Object Relationship Evolution (ORE), even when timestamps are provided for each frame. This indicates the models limited capacity to perceive and remember temporal changes. The difficulties encountered by GPT-4o [3] in these areas underscore significant opportunity for improvement, highlighting the need for advancements in temporal awareness and memory retention. Open-source MLLMs. Top-tier open-source models, like InternVL2.5-78B [20], still show noticeable gap compared to closed-source models, trailing GPT-4o [3] by 9.5%. Other state-ofthe-art Video-LLMs on existing benchmarks, such as Qwen2.5-VL [1], VideoLLaMA3 [21], and NVILA [65], underperform on our tasks, particularly in Object Relationship Evolution (ORE) and Absolute Time Perception (ATP). substantial number of these models are tagged with grey marks, indicating significant limitations in their memory recall capabilities. Object-level MLLMs. Object-level MLLMs, such as the recent VideoRefer [12], outperform many competitive models, highlighting the effectiveness of the object-level representation learning. However, they still face challenges in the Object Relationship Evolution (ORE) task when dealing with dense, similar objects in complex operational scenes, and in the Absolute Time Perception (ATP) task with dynamic temporal changes. Given the scarcity of open-source object-level video MLLMs, we also evaluated some image-level MLLMs, like ViP-LLaVA [23], Osprey [22] and SPHINX-V [66]. Method Random Human Input Mean OSR OLR - - 24.87 94.63 29.36 96.95 26.56 93.49 Past ORE 26.46 94.71 ATP Mean ISR OR Present PFI AP Mean TMP SCP DRP Mean Future 10.92 74.30 24.75 90.67 26.30 99.33 23.30 98. 21.29 96.77 26.47 93.14 24.41 97.99 27.80 95.12 21.96 95.79 34.09 90. 26.43 94.67 Gemini-2.0-flash [4] GPT-4o-mini [3] Gemini-2.0-flash [4] GPT-4o [3] VideoLLaVA-7B [28] LongVA-7B [31] NVILA-8B [65] VideoLLaMA2.1-7B [25] Qwen2.5-VL-3B [1] VideoLLaMA3-2B [21] LLaVA-OV-7B [27] VideoLLaMA2-72B [25] LLaVA-Video-7B [64] Qwen2.5-VL-7B [1] InternVL2.5-8B [20] VideoLLaMA3-7B [21] LLaVA-OV-72B [27] LLaVA-Video-72B [64] Qwen2.5-VL-72B [1] InternVL2.5-38B [20] InternVL2.5-78B [20] Osprey-7B [22] SPHINX-V-13B [66] ViP-LLaVA-7B [23] VideoRefer-7B [12] 32f 32f 32f 32f 8f 32f 32f 16f 1fps 1fps 32f 16f 32f 1fps 32f 1fps 32f 32f 1fps 32f 32f 1f 1f 1f 16f 45.45 49.47 57.38 61.83 34.11 35.34 37.69 37.74 38.17 38.41 40.46 41.55 41.82 43.13 45.15 46.04 47.88 49.59 49.87 52.31 52.33 27.36 29.21 32.82 40.44 50.42 53.26 63.46 66.04 31.86 36.84 37.40 44.88 38.78 37.12 40.72 43.77 44.32 47.37 45.71 45.15 46.81 49.03 51.25 55.40 53. 22.71 25.48 35.73 47.37 Proprietary Multimodal Foundation Models 34.42 52.35 65.10 71.93 22.84 29.68 32.56 46.56 10.32 21.10 28.60 34.46 29.78 39.47 47.87 54. 61.98 58.46 68.84 71.46 56.34 49.26 57.52 52.85 Open-Source Multimodal Foundation Models 37.94 43.36 46.61 42.82 48.78 46.88 45.53 51.22 48.51 46.34 54.47 52.85 50.95 56.91 51.22 59.62 63.96 20.33 23.31 36.86 55.01 27.58 17.83 20.89 19.22 23.96 21.17 22.84 24.23 22.56 21.45 39.00 24.51 26.46 26.74 40.11 30.92 33. 13.14 15.32 12.09 11.64 7.66 11.26 9.53 6.46 9.76 8.18 9.76 15.54 12.91 24.02 8.48 10.89 12.01 27.97 28.69 29.69 30.08 30.34 29.57 30.15 32.03 31.82 31.38 37.87 35.00 34.81 39.59 38.41 39.89 41.35 41.04 38.19 44.39 47.24 49.92 49.92 54.10 50.08 54.27 57.29 55.44 57.96 64.15 63.32 61.31 64.15 66.67 Object-level Multimodal Models 15.88 13.37 17.55 23.40 7.41 3.83 8.26 10. 16.78 16.79 25.00 34.69 42.88 41.71 42.88 48.91 35.10 36.58 41.59 37.17 38.94 43.36 43.07 37.46 43.66 44.54 48.97 48.67 51.33 47.20 47.79 54.28 50.74 29.50 31.27 35.99 39.82 69.35 67.74 69.68 78.18 40.97 48.06 49.03 51.94 45.16 48.39 52.58 58.06 55.81 59.35 54.84 62.58 64.52 63.87 67.10 71.29 67. 32.58 44.19 46.45 53.55 51.96 58.82 65.69 62.75 37.25 42.16 46.08 39.22 38.24 38.24 46.08 45.10 49.02 49.02 41.18 49.02 49.02 50.00 57.84 64.71 52.94 29.41 39.22 26.47 38.24 61.50 58.31 65.95 67.32 39.24 40.36 44.88 45.18 45.18 47.03 50.37 48.37 51.56 53.93 52.60 56.01 59.87 58.38 58.98 63.35 61. 36.13 39.47 40.73 46.88 52.20 56.59 58.54 69.61 40.98 39.02 42.44 40.00 42.93 43.41 47.32 49.27 45.85 48.78 49.76 52.20 58.05 56.10 56.10 60.98 67.80 39.51 41.46 34.63 41.95 54.70 50.00 64.02 68.69 31.78 42.06 38.32 36.92 36.57 36.11 37.38 50.47 40.65 46.30 38.79 49.54 46.73 55.14 60.65 54.67 50. 30.37 31.02 29.91 35.51 48.86 54.55 57.95 68.97 44.32 40.91 44.32 44.32 50.00 43.18 46.59 51.14 47.73 46.59 53.41 48.86 54.55 47.73 54.55 57.95 54.55 28.41 39.77 40.91 43.18 52.53 53.45 60.75 69.11 37.67 40.63 41.03 39.45 41.45 40.28 43.00 50.10 43.98 47.35 45.76 50.49 52.66 54.24 57.76 57.79 58. 33.73 36.74 33.73 39.45 Table 2: Performance of representative MLLMs on EOC-Bench. The best results are marked with orange . The results below random guess are marked with grey . Entries in grey indicate image-level methods that use only the last frame as input. : We manually added timestamp before each frame. [Nf] denotes that the model takes frames uniformly sampled from video as input. While these models underperform in the Past dimension, which requires memory of previous frames, they still deliver reasonably performance in the Present and Future dimensions."
        },
        {
            "title": "4.3 Analysis Across Different Question Types",
            "content": "We conduct an analysis of the models results across different question types to facilitate more comprehensive horizontal and vertical examination, as illustrated in Table 3. Smaller MLLMs Often Struggle with Multiple-Choice Questions. Many MLLMs face challenges in answering multiple-choice questions (MCA), often scoring lower than random guess (indicated by grey mark). This issue is particularly evident in smaller models, those with 7B parameters or fewer. We surmise that these smaller models have overfitted to simple single-choice questions during training, hindering their ability to follow instructions for handling questions with multiple options. Few MLLMs are Time-sensitive. The OQ metric, which measures the models ability to perceive past time, indicates that the some models perform below random guessing levels, with 9/21. Even the strongest open-source model scores only 24.02%, just 13.1% above random chance. This underscores crucial capability that is lacking in most models, yet is essential in the field of embodied AI. Larger MLLMs Excel in Handling Future-Oriented Problems. Future-oriented tasks demand combination of commonsense reasoning and extensive knowledge. Our observations indicate that as the size of the model increases, so does its reasoning capability. For instance, Qwen2.5-VL [1] with 3B, 7B, and 72B parameters, as well as VideoLLaMA3 [21] with 2B and 7B parameters, demonstrate significantly improved performance in these tasks. This trend suggests that larger MLLMs are better equipped to tackle problems that require forward-thinking and predictive reasoning, due to their enhanced capacity to integrate and process complex patterns of information. Past-Oriented Questions Pose Greater Challenges to MLLMs. Through comparative analysis of similar problem types, we discover that models generally perform worse on questions related to past events compared to other categories. While smaller models may grapple with future-oriented problems, larger models often fall short when addressing past-oriented questions. This difficulty in accurately recalling and processing past information is prevalent issue among current MLLMs, indicating significant area for improvement in their design and training."
        },
        {
            "title": "SCA MCA T F",
            "content": "OQ"
        },
        {
            "title": "SCA MCA T F",
            "content": "OQ"
        },
        {
            "title": "Future\nSCA MCA T F",
            "content": "- 26.27 18.34 50.00 10.92 29. 20.56 50.00 10.92 24.72 14.95 50. 23.53 18.60 50."
        },
        {
            "title": "Proprietary Multimodal Foundation Models",
            "content": "GPT-4o [3] GPT-4o-mini [3] Gemini-2.0-flash [4] VideoLLaVA-7B [28] VideoLLaMA2.1-7B [25] VideoLLaMA2-72B [25] LongVA-7B [31] NVILA-8B [65] InternVL2.5-8B [20] InternVL2.5-38B [20] InternVL2.5-78B [20] LLaVA-OV-7B [27] LLaVA-OV-72B [27] LLaVA-Video-7B [64] LLaVA-Video-72B [64] Qwen2.5-VL-3B [1] Qwen2.5-VL-7B [1] Qwen2.5-VL-72B [1] VideoLLaMA3-2B [21] VideoLLaMA3-7B [21] VideoRefer-7B [12] 32f 32f 32f 8f 16f 16f 32f 32f 32f 32f 32f 32f 32f 32f 32f 1fps 1fps 1fps 1fps 1fps 16f 69.03 57.31 68.68 54.44 32.80 28.49 63.86 58.76 63.28 34.46 21.10 28.60 67.00 52.07 66.15 49.06 26.26 20. 55.56 44.44 44.44 34.46 21.10 28.60 69.29 60.79 70.66 58.08 41.24 37.63 68.85 67.14 71.43 72.76 58.20 68. 63.95 34.88 34.88 61.46 54.08 59.18 Open-Source Multimodal Foundation Models 41.55 47.67 53.15 41.73 49.73 54.95 64.45 64.32 54.18 61.32 56.23 62.73 50.50 54.25 61.45 49.98 57.15 54.27 11.11 9.01 13.15 16.40 0.53 23.99 26.28 26.63 0 12.17 0 10.23 1.44 13.67 27.16 3.70 17. 54.80 52.78 51.67 54.24 55.37 57.63 62.71 61.58 57.63 61.02 57.06 60.45 56.67 62.22 54.44 57.06 55.00 13.14 11.64 6.46 15.32 12.09 9.76 10.89 12.01 9.53 17.22 9.76 24.02 7.66 8.18 8.48 11.26 15.54 41.24 45.40 52.21 41.24 47.41 55.11 62.67 62.55 49.43 55.49 52.33 59.27 50.44 49.81 57.12 47.29 52.33 8.36 8.36 5.23 9.06 0 22.30 10.10 16.38 0 1.74 0 2.44 0 6.27 21.60 1.05 9.41 33.33 55.56 55.56 44.44 66.67 55.56 55.56 55.56 55.56 77.78 55.56 66.67 66.67 66.67 33.33 55.56 44.44 13.14 11.64 6.46 15.32 12.09 9.76 10.89 12.01 9.53 17.22 9.76 24.02 7.66 8.18 8.48 11.26 15. 42.34 51.29 52.77 41.97 51.85 56.55 66.70 65.13 58.67 66.05 59.78 65.22 51.85 58.39 63.19 53.23 60.89 14.95 9.28 22.68 23.71 1.55 26.80 43.30 40.21 0 22.68 0 18.56 3.09 23.71 34.54 6.19 26.29 58.57 50.00 51.43 61.43 57.14 62.86 67.14 68.57 61.43 67.14 67.14 62.86 58.57 68.57 61.43 64.29 62.86 39.63 41.27 56.63 42.11 48.30 49.23 61.30 65.94 50.77 59.75 53.87 62.85 46.25 51.35 66.07 45.68 56.46 11.63 10.81 18.92 24.42 0 23.26 41.86 30.23 0 23.26 0 17.44 2.67 16.00 29.33 6.90 24.00 54.24 10.59 57.12 0 55.56 10. 54.43 0 60.00 46.75 0 54.08 54.46 51.49 50.00 53.06 54.08 60.20 57.14 55.10 55.10 50.00 58.16 54.46 57.43 51.49 52.04 50. 50.00 Table 3: Performance of representative MLLMs across different question types: SCA (SingleChoice Answer), MCA (Multi-Choice Anwer), (True/False), OQ (Open-Ended Question). The best results are marked with orange . The results below random guess are marked with grey . : We manually added timestamp before each frame. Figure 4: Comparison of mainstream MLLMs on EOC-Bench. Left: Performance on 11 evaluation tasks within EOC-Bench. Right: Performance across different question types spanning Past, Present and Future categories."
        },
        {
            "title": "4.4 Qualitative Results",
            "content": "To intuitively showcase the performance of mainstream MLLMs, including both proprietary MLLMs and open-source models, across various evaluation dimensions of EOC-Bench, we provide detailed comparison illustrated in Fig. 4. We assess the models across the 11 evaluation tasks, as well as multiple question types spanning three temporal categories."
        },
        {
            "title": "4.5 Multi-Frame Gain",
            "content": "We assess the multi-frame gain for frames 1, 8, and 32 within EOC-Bench. The strong proprietary MLLMs, GPT-4o [3] and Gemini-2.0-flash [4], exhibits substantial performance boost, gaining 24.6% and 20.1% when moving from single-frame input to 32-frame input setting. This improvement is particularly pronounced in past-oriented tasks, with an improvement of 49.2% and 60.2%. These findings underscore the critical role of multi-frame reasoning in the EOC-Bench, especially for memory recall tasks. The ability to access information from previous frames can significantly enhance 9 # Frames GPT-4o [3] Gemini-2.0-flash [4] InternVL2.5-78B [20] VideoLLaMA3-7B [21] 1f 49.6 47.8 47.6 42."
        },
        {
            "title": "Mean",
            "content": "8f 58.6 51.2 51.3 45.5 32f 61.8 57.4 52.3 46.2 γ 24.6 20.1 9.9 10."
        },
        {
            "title": "Past",
            "content": "8f 50.6 37.7 38.9 34.3 32f 54.9 47.9 41.4 36.1 γ 49.2 60.2 24.0 26. 1f 36.8 29.9 33.1 28."
        },
        {
            "title": "Present",
            "content": "8f 64.7 63.4 64.3 55.0 32f 67.3 66.0 61.7 55.1 γ 11.8 2.0 3.2 0. 1f 60.2 64.7 59.8 54."
        },
        {
            "title": "Future",
            "content": "8f 65.2 57.0 56.4 49.7 32f 69.1 60.8 58.2 50.5 γ 19.1 14.5 4.3 8. 1f 58.0 53.1 55.8 46.5 Table 4: Performance of representative MLLMs with varying input frames. 1f denotes using only the last frame, while 8f/32f refers to frames that are uniformly sampled, including the last frame. γ represents the rate of increase in performance from 1f to 32f. both current and future understanding. Other open-source models , such as InternVL2.5-78B [20], and VideoLLaMA3-7B [21], demonstrate similar trends. However, their ability to effectively process multiple frames is comparatively weaker, resulting in less pronounced performance improvements. This highlights the potential benefits of enhancing multi-frame processing capabilities in MLLMs to achieve more substantial performance gains across variety of tasks."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we presented EOC-Bench, an innovative benchmark aimed at evaluating the embodied, object-level cognition capabilities of MLLMs. EOC-Bench thoroughly assesses MLLMs within the scenes involving dynamic egocentric interactions across three temporal dimensions: Past, Present and Future. To ensure high quality, we developed mixed-format human-in-the-loop annotation framework and introduced multi-scale temporal accuracy metric to enhance the precision of openended questions. Extensive evaluations conducted on EOC-Bench across range of proprietary and open-source models, have revealed that many MLLMs face challenges in effectively performing embodied object cognition tasks, particularly in recalling and processing past information as well as in absolute time perception. We hope EOC-Bench will drive progress in developing MLLMs capable of understanding more complex and diverse physical world."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [3] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [4] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [5] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar Parkhi, Richard Newcombe, and Yuheng Carl Ren. Aria digital twin: new benchmark dataset for egocentric 3d machine perception. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2013320143, 2023. [6] Sanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Martín-Martín, Fei Xia, Kent Elliott Vainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments. In Conference on robot learning, pages 477490. PMLR, 2022. [7] Yifei Huang, Jilan Xu, Baoqi Pei, Yuping He, Guo Chen, Lijin Yang, Xinyuan Chen, Yaohui Wang, Zheng Nie, Jinyao Liu, et al. Vinci: real-time embodied smart assistant based on egocentric vision-language model. arXiv preprint arXiv:2412.21080, 2024. [8] Ying Zheng, Lei Yao, Yuejiao Su, Yi Zhang, Yi Wang, Sicheng Zhao, Yiyi Zhang, and Lap-Pui Chau. survey of embodied learning for object-centric robotic manipulation. arXiv preprint arXiv:2408.11537, 2024. [9] Xiaoqi Li, Mingxu Zhang, Yiran Geng, Haoran Geng, Yuxing Long, Yan Shen, Renrui Zhang, Jiaming Liu, and Hao Dong. Manipllm: Embodied multimodal large language model for object-centric robotic manipulation. In CVPR, pages 1806118070, 2024. 10 [10] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In ICML, pages 84698488. PMLR, 2023. [11] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In CVPR, 2024. [12] Yuqian Yuan, Hang Zhang, Wentong Li, Zesen Cheng, Boqiang Zhang, Long Li, Xin Li, Deli Zhao, Wenqiao Zhang, et al. Videorefer suite: Advancing spatial-temporal object understanding with video llm. CVPR, 2025. [13] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall: Temporal activity localization via language query. In ICCV, pages 52675275, 2017. [14] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In CVPR, pages 1912919139, 2022. [15] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022. [16] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, et al. Robovqa: Multimodal long-horizon reasoning for robotics. In ICRA, pages 645652, 2024. [17] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, et al. Openeqa: Embodied question answering in the era of foundation models. In CVPR, 2024. [18] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In CVPR, 2025. [19] Ronghao Dang, Yuqian Yuan, Wenqi Zhang, Yifei Xin, Boqiang Zhang, Long Li, et al. Ecbench: Can multi-modal foundation models understand the egocentric world? holistic embodied cognition benchmark. In CVPR, 2025. [20] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [21] Boqiang Zhang, Kehan Li, Zesen Cheng, Zhiqiang Hu, Yuqian Yuan, Guanzheng Chen, Sicong Leng, Yuming Jiang, Hang Zhang, Xin Li, et al. Videollama 3: Frontier multimodal foundation models for image and video understanding. arXiv preprint arXiv:2501.13106, 2025. [22] Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu. Osprey: Pixel understanding with visual instruction tuning. In CVPR, pages 2820228211, 2024. [23] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. Vip-llava: Making large multimodal models understand arbitrary visual prompts. In CVPR, pages 1291412923, 2024. [24] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [25] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. [26] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. [27] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [28] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [29] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. [30] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. [31] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, et al. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. [32] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm. arXiv preprint arXiv:2407.02392, 2024. [33] Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, Siliang Tang, Jun Xiao, Hui Lin, Yueting Zhuang, and Beng Chin Ooi. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation. In ICML, 2025. [34] Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan Li, Lei Zhang, He Wanggui, Hao Zhou, Zheqi Lv, Hao Jiang, Juncheng Li, Siliang Tang, and Yueting Zhuang. Hyperllava: Dynamic visual and language expert tuning for multimodal large language models, 2024. [35] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [36] Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbenchvideo: long-form multi-shot benchmark for holistic video understanding. In NeurIPS, 2025. [37] Perception test: diagnostic benchmark for multimodal video models, 2023. [38] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: large-scale video benchmark for human activity understanding. In CVPR, pages 961970, 2015. [39] Jiaying Liu, Sijie Song, Chunhui Liu, Yanghao Li, and Yueyu Hu. benchmark dataset and comparison study for multi-modal human action analytics. TOMM, 2020. [40] Andong Deng, Taojiannan Yang, and Chen Chen. large-scale study of spatiotemporal representation learning with new benchmark on action recognition. In ICCV, pages 2051920531, 2023. [41] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In ICCV, pages 706715, 2017. [42] Rikito Takahashi, Hirokazu Kiyomaru, Chenhui Chu, and Sadao Kurohashi. Abstractive multi-video captioning: Benchmark dataset construction and extensive evaluation. In LREC-COLING, pages 5769, 2024. [43] Jiawei Wang, Liping Yuan, Yuchen Zhang, and Haomiao Sun. Tarsier: Recipes for training and evaluating large video description models. arXiv preprint arXiv:2407.00634, 2024. [44] Ilker Kesen, Andrea Pedrotti, Mustafa Dogan, Michele Cafagna, et al. Vilma: zero-shot benchmark for linguistic and temporal grounding in video-language models. arXiv preprint arXiv:2311.07022, 2023. [45] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. [46] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next phase of question-answering to explaining temporal actions. In CVPR, pages 97779786, 2021. [47] Ziyao Shangguan, Chuhan Li, Yuxuan Ding, Yanan Zheng, Yilun Zhao, Tesca Fitzgerald, and Arman Cohan. Tomato: Assessing visual temporal reasoning capabilities in multimodal foundation models. In ICLR, 2025. [48] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, et al. Mlvu: comprehensive benchmark for multi-task long video understanding. In CVPR, 2025. [49] Longvideobench: benchmark for long-context interleaved video-language understanding, 2025. [50] Hongjie Zhang, Yi Liu, Lu Dong, Yifei Huang, Zhen-Hua Ling, Yali Wang, et al. Movqa: benchmark of versatile question-answering for long-form movie understanding. arXiv preprint arXiv:2312.04817, 2023. 12 [51] Kirolos Ataallah, Chenhui Gou, Eslam Abdelrahman, Khushbu Pahwa, Jian Ding, and Mohamed Elhoseiny. Infinibench: comprehensive benchmark for large multimodal models in very long video understanding. arXiv preprint arXiv:2406.19875, 2024. [52] Yilun Zhao, Lujing Xie, Haowei Zhang, Guo Gan, Yitao Long, Zhiyuan Hu, Tongyan Hu, Weiyuan Chen, et al. Mmvu: Measuring expert-level multi-discipline video understanding. arXiv preprint arXiv:2501.12380, 2025. [53] Kairui Hu, Penghao Wu, Fanyi Pu, Wang Xiao, Yuanhan Zhang, Xiang Yue, Bo Li, and Ziwei Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. [54] Difei Gao, Ruiping Wang, Ziyi Bai, and Xilin Chen. Env-qa: video question answering benchmark for comprehensive understanding of dynamic environments. In ICCV, pages 16751685, 2021. [55] Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. Egoplan-bench: Benchmarking egocentric embodied planning with multimodal large language models. CoRR, 2023. [56] Lu Qiu, Yi Chen, Yuying Ge, Yixiao Ge, Ying Shan, and Xihui Liu. Egoplan-bench2: benchmark for multimodal large language model planning in real-world scenarios. arXiv preprint arXiv:2412.04447, 2024. [57] Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, and Baobao Chang. Pca-bench: Evaluating multimodal large language models in perception-cognition-action chain. arXiv preprint arXiv:2402.15527, 2024. [58] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. Egoschema: diagnostic benchmark for very long-form video language understanding. In NeurIPS, 2023. [59] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In CVPR, pages 1899519012, 2022. [60] Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, and Bo Zhao. Sti-bench: Are mllms ready for precise spatial-temporal world understanding? arXiv preprint arXiv:2503.23765, 2025. [61] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In ECCV, pages 720736, 2018. [62] Gunnar Sigurdsson, Abhinav Gupta, Cordelia Schmid, Ali Farhadi, and Karteek Alahari. Charades-ego: large-scale dataset of paired third and first person videos. arXiv preprint arXiv:1804.09626, 2018. [63] Francesco Ragusa, Antonino Furnari, Salvatore Livatino, and Giovanni Maria Farinella. The meccano dataset: Understanding human-object interactions from egocentric videos in an industrial-like domain. In WACV, 2021. [64] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. [65] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. [66] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. In ICLR, 2025. [67] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023."
        },
        {
            "title": "Appendix",
            "content": "In this document, we offer additional details about our benchmark. The Appendix is organized as follows: A: Additional details on EOCBench; B: More experimental analysis; C: Experimental setup; D: Additional dataset analysis; E: Limitations and broader impacts; F: Asset license and consent; G: More exemplar visualizations."
        },
        {
            "title": "A Additional Details on EOCBench",
            "content": "A.1 Human Error Analysis for Evaluation Metrics To accurately evaluate the Open-Ended Questions (OQ) task, we have developed novel metric, Multi-Scale Temporal Accuracy (MST A) for comprehensive temporal perception, as introduced in the Section 3.2.4 of the main paper. Here, we provide additional details on the choice of dynamic error margins in MST through the carefully designed human error analysis. Specially, we first asked three volunteers to answer this type of question, and then analyzed the error ratio compared to the ground truth, expressed as = (Tpred Tgt)/Tgt. We compared all values from all questions and created histogram of these results, as shown in Fig. 5-(a). It can be observed that the error ratios are primarily concentrated around 0, with absolute values rarely exceeding 30%. We then conduct quantile analysis of r, selecting quantiles at 50%, 75%, 90%, 95%, with corresponding error ratio being 1.4%, 11.9%, 20% and 30%, respectively. Based on these analyses, we set the threshold for dynamic error margins at {1%, 10%, 20%, 30%} for subsequently scale-adaptive boundaries as described in the main paper. 1% threshold demands near-exact alignment, signifying extreme precision, whereas 30% threshold caters to greater variability in responses, accommodating almost all human answers within this margin. Fig. 5-(b) depicts the statistical distribution of human error ratio. This dynamic threhold scheme balances strictness and flexibility, ensuring that our framework captures the spectrum of human error while maintaining stringent evaluation criteria. Figure 5: Statistics distribution of human error ratio. (a) displays histogram depicting the density and spread of human error ratios; (b) presents pie chart categorizing the error ratios into quantitative segments. 14 A.2 Additional Annotation Details A.2.1 Human-in-the-Loop Annotation Pipeline To ensure the high quality of our benchmark, we employ rigorous human-in-the-loop annotation pipeline comprising two stages: initial annotation stage, followed by cross-checking and verification stage. Additional details are provided below: Initial annotation. In the initial annotation phase, each annotator is assigned questions belonging to specific category, such as Past, Present, or Future, along with set of randomly selected videos to ensure the data diversity and relatively uniform distribution. To ensure thorough understanding of each category, annotators are provided with detailed guide for each category. During the annotation process, annotators are permitted to pause and examine any frame within the video. Cross-check and verification. During the cross-check procedure, each annotator reviews the work of another, focusing primarily on two key aspects: the quality of the question-answer pairs and the accuracy of the annotated visual prompts. If visual prompt, like point, box and mask, is of low quality or missing, the annotator can either carefully reannotate it or discard it; 185 object prompts were reannotated and 34 were discarded. For question-answer pairs that are deemed low quality, the reviewer must definitely discuss the issues with the original annotator to finalize the annotation. In total, 76 questions required collaborative resolution during this verification phase. Besides, we provide brief biographies of all 10 annotators who participated in the annotation process in the Table 5. ID 1 2 3 4 5 6 7 8"
        },
        {
            "title": "Academic Status",
            "content": "First-Year Masters Student First-Year Masters Student First-Year Masters Student Second-Year Masters Student Second-Year Masters Student Second-Year Masters Student Recent Masters Graduate First-Year Ph.D. Student Third-Year Ph.D. Student Recent Ph.D. Graduate"
        },
        {
            "title": "Computer Science\nComputer Science\nRobotics\nComputer Science\nComputer Science\nRobotics\nRobotics\nComputer Science\nComputer Science\nComputer Science",
            "content": "Table 5: Brief biographies of the 10 human annotators in EOC-Bench."
        },
        {
            "title": "B More Experimental Analysis",
            "content": "B.1 Performance of Various Visual Object Prompts To validate the effectiveness of visual prompts for object referencing, we conduct additional experiments on the representative MLLMs with various visual prompts, including point, box, and mask. The comparison results are presented in Table 6. In the main paper, we employ box prompt as the default setting. Boxes, compared to masks, are easier to obtain in practical applications. Additionally, compared to points, boxes offer more precise references. B.2 Quantitative Error Analysis in EOC-Bench To quantify and identify the primary challenges of our EOC-Bench, we perform comprehensive error analysis on representative MLLMs, examining both choice-based and open-ended questions. B.2.1 Choice-based Questions For choice-based questions, we conduct analysis on the top-performing MLLM, GPT-4o [3]. Specially, we randomly sampled 300 choice-based erroneous QAs from EOC-Bench, with 30 QAs for each task. We then meticulously examined these errors and categorized them into four primary types:"
        },
        {
            "title": "Future",
            "content": "InternVL2.5-8B [20] Qwen2.5VL-7B [1] VideoLLaMA3-7B [21] LLaVA-Video-7B [64] 41.83 41.21 45.24 41.13 36.35 31.81 37.04 32.82 45.48 49.18 52.89 49.04 47.51 46.35 47.93 43.39 42.05 43.13 46.04 39. 33.45 31.38 35.00 32.67 49.70 53.93 56.01 48.96 45.36 47.35 50.49 43.39 42.12 41.31 46.10 40.91 35.04 30.64 35.41 32.40 48.07 52.08 55.93 48. 46.15 42.60 49.94 43.39 Table 6: Performance of representative MLLMs with different visual prompt inputs. Perception Error. This type of error involves issues with perception in the current frame, including interference from previous frames, insufficient attention to finer details, counting errors, and intra-frame interferences. Memory Error. This error type reflects incorrect observation or recall of information from previous frames, including interference from current frames and missing observations, suggesting that the 32 sampled frames are insufficient to answer the memory-related questions. Relational Reasoning Error. This type of error involves difficulties in perceiving or inferring simple relationships between objects. Knowledge Error. This category encompasses errors in reasoning, common sense, and calculation. Figure 6: Quantitative error analysis by type for choice-based questions in EOC-Bench. In the Past category, as illustrated in Fig. 6, memory errors are predominant, accounting for 93% of the errors. These are primarily due to insufficient processing of historical frames (73%) and interference from current frame (17%). The remaining 10% are missing observation errors, which highlight the inherent constraints of fixed-frame sampling strategies. These findings point to significant weakness of GPT-4o [3] in temporal context modeling, particularly its difficulty in effectively retaining and using cross-frame information for video understanding tasks. In the Present category, perception errors account for 61%, followed by knowledge errors (22%) and memory errors (7%). Notably, intra-frame interference constitutes significant portion of perception errors, revealing the models limitations in regional-level visual perception and its susceptibility to hallucinatory artifacts. These observations suggest that spatial perception remains persistent challenge. In the Future category, approximately 59% of errors are knowledge-related issues, indicating limitations in reasoning abilities and common sense understanding. Figure 7: Quantitative error analysis for open-ended questions in EOC-Bench. Left: Density analysis of temporal perception deviations (error ratio) among humans and models. Right: Model accuracy across different time thresholds for dynamic error margins."
        },
        {
            "title": "Model",
            "content": "Frames API Checkpoint / HF Checkpoint"
        },
        {
            "title": "Do\nSample",
            "content": "Max New Temp. Top-P"
        },
        {
            "title": "Tokens",
            "content": "0 0 0 1 1 1 GPT-4o-mini [3] GPT-4o [3] Gemini-2.0-Flash [4] InternVL2.5-8B [20] InternVL2.5-38B [20] InternVL2.5-78B [20] LongVA-7B [31] LLaVA-Video-7B [64] LLaVA-Video-72B [64] LLaVA-OneVision-7B [27] LLaVA-OneVision-72B [27] Qwen2.5-VL-3B [1] Qwen2.5-VL-7B [1] Qwen2.5-VL-72B [1] VideoLLaMA2.1-7B [25] VideoLLaMA2-72B [25] VideoLLaMA3-2B [21] VideoLLaMA3-7B [21] NVILA-8B [65] VideoLLaVA-7B [28] VideoRefer-7B [12] ViP-LLaVA-7B [23] Osprey-7B [22] SPHINX-V-13B [66]"
        },
        {
            "title": "Proprietary Multimodal Foundation Models",
            "content": "gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06 gemini-2.0-flash Open-Source Multimodal Foundation Models OpenGVLab/InternVL2_5-8B OpenGVLab/InternVL2_5-38B OpenGVLab/InternVL2_5-78B lmms-lab/LongVA-7B lmms-lab/LLaVA-Video-7B-Qwen2 lmms-lab/LLaVA-Video-72B-Qwen2 lmms-lab/llava-onevision-qwen2-7b-ov lmms-lab/llava-onevision-qwen2-72b-ov-sft Qwen/Qwen2.5-VL-3B-Instruct Qwen/Qwen2.5-VL-7B-Instruct Qwen/Qwen2.5-VL-72B-Instruct DAMO-NLP-SG/VideoLLaMA2.1-7B DAMO-NLP-SG/VideoLLaMA2-72B DAMO-NLP-SG/VideoLLaMA3-2B DAMO-NLP-SG/VideoLLaMA3-7B Efficient-Large-Model/NVILA-8B-Video LanguageBind/Video-LLaVA-7B DAMO-NLP-SG/VideoRefer-7B llava-hf/vip-llava-7b-hf sunshine-lwt/Osprey-Chat-7b Afeng-x/SPHINX-V-Model 32 32 32 32 32 32 32 32 32 32 32 1fps 1fps 1fps 16 32 1fps 1fps 32 8 16 1"
        },
        {
            "title": "False\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse",
            "content": "1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 1024 Table 7: Model configurations for evaluating mainstream MLLMs in EOCBench (Temp.: temperature). B.2.2 Open-Ended Questions To assess open-ended questions related to temporal perception accuracy, we conducted densitybased analysis of deviations between ground-truth timestamps and model-generated responses, as visualized in Fig. 7-(Left). The distribution of human responses exhibits pronounced peak followed by rapid decay, suggesting that most human answers achieve minimal error ratios, with only sporadic instances of higher inaccuracies. In contrast, the five top-performing modelsGPT-4o [3], LLaVAVideo-72B [64], VideoLLaMA3-7B [21], Qwen2.5-VL-72B [1] and NVILA-8B [65]demonstrate flatter distributions with broader spreads. This pattern suggests that these models exhibit greater variability in temporal perception, frequently producing larger errors in specific cases. The observed disparity highlights substantial discrepancy between current MLLMs and human-level temporal perception, suggesting that some model predictions rely on haphazard estimation rather than precise temporal understanding. As illustrated in Fig. 7-(Right), the figure also presents model accuracy across various time thresholds, specifically 0.01, 0.1, 0.2, and 0.3."
        },
        {
            "title": "C Experimental Setup",
            "content": "C.1 Model Configurations The configurations of the mainstream MLLMs we evaluate, including the official checkpoints, the number of frame samples, and details regarding the Do Sample, Max New Tokens, Temperature and Top-P parameters, are provided in Table 7. C.2 Additional Implementation Details We utilize the official repository of each MLLMs to perform evaluations on our EOC-Bench benchmark. Pre-sampled images from 1-frame, 8-frame, 16-frame, 32-frame, and 1 fps sequences serve as input for the corresponding models according to their default settings. Besides, for proprietary MLLMs, including GPT-4o, GPT-4o-mini and Gemini, we introduce timestamps prior to each frame to enhance the models temporal awareness. The open-source models are evaluated with their default settings, and all evaluations are conducted using NVIDIA A100 GPUs. C.3 Carefully Crafted Prompts Visual Prompts. We employ the SoM [67] method to overlay various spatial markers onto the images in the final frame of the video. For single object, only its visual prompt is highlighted in red on the last frame. In the case of multiple objects, we overlay both their identifying numbers and visual prompts in various colors to facilitate differentiation. An example is presented in Fig. 8. Text Prompts. The text prompts used for inference are consistent across all models and are as follows: System Prompt: have overlaid the box on the last frame of the video , < object 1 >: red ; < object 2 >: blue , < object 3 >: green ; < object 4 >: yellow ; < object 5 >: purple ; < object 6 >: orange ; Single choice USER: {Question} Options : {Options} Answer directly using the letters of the options given and wrap your response in < choice > </ choice >. For example , if the answer is , then output < choice >A </ choice >. Multi choice USER: {Question} Options : {Options} Answer directly using the letters of the options given . There are multiple answers , so wrap your response in < choice > </ choice >. For example , if the answer is and , then output < choice >A , </ choice >; if the answer is , and , then output < choice >A , , </ choice >. Open ended USER: {Question} Please output the answer directly in seconds . Figure 8: Illustrative examples of annotation formats for visual prompts."
        },
        {
            "title": "D Additional Dataset Analysis",
            "content": "Fig. 9 provides an additional statistical analysis through the form word cloud, capturing the range of questions and answers encompassed in EOC-Bench. The predominance of terms related to dynamics 18 and changessuch as change, changed, seconds, and happenedindicates substantial focus on the temporal and transformational aspects within the dataset. These terms are essential for assessing the memory capabilities of robots, as they require understanding and recalling sequences of events and alterations over time. Moreover, the word cloud highlights the significance of spatial understanding, with frequent terms like relative, relative position, and relationship. These words underscore the importance of comprehending the spatial dynamics between objects. Analyzing these spatial relationships allows the model to infer how objects are positioned relative to each other, providing insights essential for effective planning and execution."
        },
        {
            "title": "E Limitations and Broader Impacts",
            "content": "Limitations. Our EOC-Bench demonstrates the commendable assessment of embodied object cognition, yet certain limitation remain. The video inputs for EOC-Bench are limited to durations of under six minutes, which may not adequately evaluate the cognitive abilities of MLLMs in terms of prolonged visual memory. In our future work, we are dedicated to progressing our research by collecting video resources of longer durations. We aims to explore the effects of increasing video input durations, particularly in terms of the models ability to retain prolonged visual information. Broader Impacts. As benchmark specifically designed for evaluating in the domain of embodied ego-centric cognition, EOC-Bench is set to draw considerable interest from researchers keen on examining cognitive processes related to focusing on specific objects. Moreover, EOC-Bench aims to assist contemporary MLLMs in transcending the limitations inherent in images, videos, and texts alone, by shifting their focus toward the visual prompt inputs encountered in real-world scenarios."
        },
        {
            "title": "F Asset License and Consent",
            "content": "In our EOC-Bench, we utilize four open-source datasets: EPIC-KITCHENS [61], Ego4D [59], Charades-ego [62] and MECCANO [63]. All datasets are publicly accessible and freely available for academic research. Table 8 provides detailed list of the resources used in this research work, along with their respective licenses."
        },
        {
            "title": "URL",
            "content": "EPIC-KITCHENS [61] CC BY-NC 4.0 Ego4D [59] Charades-ego [62] MECCANO [63] MIT license Non-Commercial Use CC BY-NC 4.0 https://epic-kitchens.github.io/2025 https://github.com/facebookresearch/Ego4d https://prior.allenai.org/projects/charades-ego https://iplab.dmi.unict.it/MECCANO/ Table 8: Open-source resources used in this work."
        },
        {
            "title": "G More Exemplar Visualizations",
            "content": "G.1 Failure Case Studies Fig. 10 displays representative cases from top-performing GPT-4o [3] on our EOC-Bench. These cases systematically demonstrate the models failure patterns across multiple error categories, including current visual perception errors, common sense errors, and historical frame errors, while covering diverse question types from EOC-Bench. G.2 Visual Samples Across Tasks To intuitively illustrate the characteristics of our EOC-Bench, we further showcase samples spanning 11 tasks, organized as follows: Object State Retrospection  (Fig. 11)  Figure 9: Worldcloud of questions and answers in EOC-Bench. Immediate State Recognition  (Fig. 15)  Object Location Retrospection  (Fig. 12)  Object Relationship Evolution  (Fig. 13)  Absolute Time Perception  (Fig. 14)  Object Relationship  (Fig. 16)  Anomaly Perception  (Fig. 18)  Dynamic Relationship Prediction  (Fig. 21)  Trajectory and Motion Prediction  (Fig. 19)  State Change Prediction  (Fig. 20)  Purpose and Function Inference  (Fig. 17)  Figure 10: Failure cases of the top-performing GPT-4o on EOC-Bench. 21 Figure 11: Visualization of samples in Object State Retrospection (Past). 22 Figure 12: Visualization of samples in Location Retrospection (Past). Figure 13: Visualization of samples in Object Relationship Evolution (Past). 24 Figure 14: Visualization of samples in Absolute Time Perception (Past). 25 Figure 15: Visualization of samples in Immediate State Recognition (Present). Figure 16: Visualization of samples in Object Relationship (Present). 27 Figure 17: Visualization of samples in Purpose and Function Inference (Present). 28 Figure 18: Visualization of samples in Anomaly Perception (Present). Figure 19: Visualization of samples in Trajectory and Motion Prediction (Future). 30 Figure 20: Visualization of samples in State Change Prediction (Future). 31 Figure 21: Visualization of samples in Dynamic Relationship Prediction (Future)."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "Zhejiang University"
    ]
}