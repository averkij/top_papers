{
    "paper_title": "Pre-training Distillation for Large Language Models: A Design Space Exploration",
    "authors": [
        "Hao Peng",
        "Xin Lv",
        "Yushi Bai",
        "Zijun Yao",
        "Jiajie Zhang",
        "Lei Hou",
        "Juanzi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses generated by the teacher model. In this paper, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD). We first conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, validating the effectiveness of PD. Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits. We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while a larger teacher LLM does not necessarily guarantee better results. We hope our exploration of the design space will inform future practices in pre-training distillation."
        },
        {
            "title": "Start",
            "content": "Pre-training Distillation for Large Language Models: Design Space Exploration Hao Peng1, Xin Lv2, Yushi Bai1, Zijun Yao1, Jiajie Zhang1, Lei Hou1, Juanzi Li1 1Tsinghua University 2Zhipu AI {peng-h24}@mails.tsinghua.edu.cn 4 2 0 2 1 2 ] . [ 1 5 1 2 6 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Knowledge distillation (KD) aims to transfer knowledge from large teacher model to smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses In this pagenerated by the teacher model. per, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD). We first conduct preliminary experiment using GLM-4-9B as the teacher LLM to distill 1.9B parameter student LLM, validating the effectiveness of PD. Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits. We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while larger teacher LLM does not necessarily guarantee better results. We hope our exploration of the design space will inform future practices in pre-training distillation."
        },
        {
            "title": "Introduction",
            "content": "Knowledge distillation (KD; Hinton, 2015) aims to distill the knowledge of large teacher model into smaller and efficient student model for model compression (Gou et al., 2021). It has been widely applied in computer vision (Ahn et al., 2019; Tian et al., 2020; Bergmann et al., 2020; Zhao et al., 2022), natural language processing (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020a; Xu et al., 2024), and speech recognition (Chebotar and Waters, 2016; Fukuda et al., 2017; Tan and Wang, 2021) domains. In recent years, knowledge distillation has been standard practice to enhance Work is done when interned at Zhipu.AI Figure 1: Results of the pre-trained 1.9B, 3.8B, and 6.8B student LLMs, using only LM loss, vanilla PD configuration ( 3.1), and better PD configuration (PD) after our exploration. Details are placed in appendix A.6. large language models (LLMs) with knowledge from more advanced LLMs, such as GPT-4 (OpenAI, 2023). This technique is typically used during the post-training stage of LLMs, where the student model learns directly using language modeling (LM) loss from set of queries and responses generated by teacher LLMs. Post-training KD is simple and widely applicable, leading to the development of various advanced LLMs (Taori et al., 2023; Vicuna, 2023; Sun et al., 2024; Cui et al., 2024), which significantly advances the development of LLMs. The success of post-training distillation raises the question of whether distillation LLMs in the pre-training stage is feasible. In this paper, we extend knowledge distillation to the pre-training phase of LLMs, named pretraining distillation (PD). We primarily investigate pre-training with logits-based KD (Gou et al., 2021), where the student model learns from the teacher model generated logits of each token in the pre-training corpora using KD loss, such as KullbackLeibler divergence. The intuition is that the logits from the teacher model contain richer information and can serve as label smoothing (Gou et al., 2021), which could potentially accelerate the training of the student LLM and enhance its performance. Although the potential advantage of pre-training distillation is clear, there is limited exploration on how to better apply PD. Therefore, in this paper, we take an initial step in exploring the design space of pre-training distillation. Considering the key factors impacting distillation, we explore the design space of PD in four aspects: (1) Logits processing, focusing on the post-processing of the teacher LLMs logits to reduce the memory overhead, including truncation and normalization. (2) Loss selection, focusing on the choice of pretraining distillation loss. (3) Scaling law, covering varying sizes of student and teacher LLMs, as well as pre-training corpus size. (4) Offline or online, meaning logits are generated either from pretrained teacher LLM (offline) or simultaneously during the pre-training of teacher LLM (online). Figure 1 illustrates the effectiveness of the explored better PD configuration (PD). We conduct extensive experiments to explore the design space of PD. Specifically, we first conduct preliminary study using GLM-4-9B (GLM et al., 2024) as the teacher model to generate logits for 100 billion tokens, distilling 1.9B student LLM from scratch using negative log-likelihood loss. Due to the large vocabulary size (about 150k) of GLM-4-9B, we truncate the logits using top-p-k truncation to reduce storage space: first using topp (Holtzman et al., 2019) truncation with = 0.95, followed by top-100 truncation. The truncation reduces storage space by 4, 000 to about 15 TB of disk space. The preliminary PD yields an average performance improvement of 1.6% across comprehensive set of English and Chinese datasets, compared to standard pre-training with LM loss, which demonstrates the effectiveness of PD. Based on the preliminary experiment, we explore the design space of PD using controlled experiments: (1) Logits processing. We investigate the impact of different and values on top-p-k truncation results, and different normalization temperatures. We find no significant difference between various and values, with smaller or effectively reducing logits storage. The temperature for normalization should not be too high, and adaptive temperature shows no significant benefit. (2) Loss selection. We explore the choice of KD loss and the combination of KD loss with LM loss. We find that KullbackLeibler divergence and negative loglikelihood loss result in similar improvements, but MSE loss suffers significant drop. The best combination of LM and KD loss is using the WarmupStable-Decay (WSD; Hu et al., 2024) method to schedule for the proportion of KD loss, paired with WSD learning rate scheduler. This suggests that using higher proportion of KD loss when maintaining maximum learning rate can enhance model performance. (3) Scaling law. We find that larger student LLMs generally benefit more from pre-training distillation, and larger teacher LLM does not necessarily guarantee better results, potentially due to the capacity gap between student and teacher LLMs (Mirzadeh et al., 2020). We further conduct PD using 500 billion tokens, and find the improvement of PD is generally consistent. (4) Offline or online. We observe that using online logits for PD also yields improvement, although not as significant as offline logits. This suggests that one can save online logits on the fly during pre-training with no additional inference cost for PD on series of smaller LLMs. In summary, we hope that our thorough exploration of the pre-training distillation design space will contribute to future practices."
        },
        {
            "title": "2 Design Space for PD",
            "content": "Considering text = {xt}T t=1, student LLM parameterized by θS, and teacher LLM parameterized by θT , we formalize the objective of distillation pretraining as follows: θ = arg minθS = arg minθS [(1 α)Llm + αLkd] (1) Llm denotes the traditional one-hot language modeling pretraining loss, which can be formalized as: Llm = 1 (cid:88) t=1 log PθS (xtx<t) (2) Lkd denotes the distillation loss, which can be formalized as: Lkd = 1 (cid:88) t=1 L(PθS (xtx<t), (PθT (xtx<t))) (3) denotes the distillation loss function, such as KullbackLeibler divergence. PθS and PθT represent probability of the student and the teacher LLM, respectively. represents truncation and normalization operations conducted on the teacher LLMs logits, and τ is the temperature for normalization. (z) = softmax( Truncate(z) τ ) (4) Considering the key factors in Equation 1, we explore the design space of pre-training distillation in four dimensions: (1) The method for processing the teacher LLM logits, including the truncation method and temperature τ for normalization. (2) The choice of loss function, including the selection of distillation loss function and the combination factor α of language modeling loss and distillation loss. (3) The scaling law of pre-training distillation, including the size of student and teacher LLMs, as well as the corpus size for pre-training the student LLM. (4) The strategy of obtaining PθT (xtx<t), either offline, i.e., the logits generated from the pre-trained teacher LLM, or online, i.e., the logits generated simultaneously during the teacher LLMs pre-training. In this work, we aim to conduct systematic empirical study to investigate the impact of these four aspects on pre-training distillation and inform future practices in pre-training distillation."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we conduct preliminary experiment to introduce the basic experimental settings of pre-training distillation and validate the efficacy of pre-training distillation ( 3.1) and empirical studies for these four main design dimensions of pre-training distillation for LLMs ( 3.2 to 3.5)."
        },
        {
            "title": "3.1 Preliminary Experiment",
            "content": "We first conduct preliminary experiment to validate the feasibility of pre-training distillation. We use GLM-4-9B as the teacher LLM to distill of 1.9B student LLM from scratch. To enhance training efficiency, we employ two-stage paradigm: (1) store the teacher LLMs generated logits on the disk, (2) use these logits to train the student LLM. Experimental Setup We first pre-train 1.9B student LLM using pre-training distillation, namely LLM-KD. Specifically, we randomly sample 100 billion tokens as pre-training data. We then obtain their logits from the teacher LLM and keep the text chunk size as 4096, which is the same as the pretraining context length of the student LLM. Due to the large vocabulary size (approximately 150k items), storing the logits of the whole vocabulary using float32 requires around 58.6 PB of disk space, which is unaffordable. To reduce storage resources, we truncate the logits: we first select the topp (Holtzman et al., 2019) logits with = 0.95, and then use top-k truncation with = 100, resulting in 4, 000 reduced storage requirement of approximately 15 TB disk space for the 100B tokens. We re-normalize the logits with temperature τ = 1.0. We use negative log-likelihood loss to conduct pretraining distillation, i.e., set α = 1 in Equation 1 and set = (PθT (xtx<t)) log PθS (xtx<t) in Equation 3, where denotes our logits truncation method with re-normalization with temperature τ = 1.0. Given the limited capacity of the student LLM, its performance on some evaluation datasets, such as MMLU (Hendrycks et al., 2021) and C-Eval (Huang et al., 2024), is close to random guessing, making the results incomparable. Therefore, we conduct supervised fine-tuning (SFT; Ouyang et al., 2022) with additional 10B high-quality instruct-tuning data after pre-training. In the SFT stage of these 10B tokens, we employ only language modeling loss rather, i.e., set α = 0 in Equation 1. We employ the same settings as in pre-training distillation, except that we only use language modeling (LM) loss for pre-training baseline 1.9B LLM for comparison, namely LLMLM. We conduct pre-training with Adam optimizer (Kingma, 2014), 2, 048 batch size, 4, 096 max sequence length, cosine learning rate scheduler with 6104 maximum learning rate, 6105 minimum learning rate, and 1% warmup rate. More experimental details are placed in appendix A.1. Evaluation Datasets We select several representative datasets to evaluate the pre-trained LLMs, including English language understanding and commonsense reasoning datasets: HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2020), PIQA (Bisk et al., 2020), MMLU (Hendrycks et al., 2021); Chinese language understanding and commonsense reasoning datasets: KBQA (Duan, 2016; Duan and Tang, 2018), C3 (Sun et al., 2020a), C-Eval (Huang et al., 2024); and math dataset: GSM8k (Cobbe et al., 2021). When conducting evaluation, the sampling temperature is set to 0. More evaluation details are shown in appendix A.1. Experimental Results The performance of pretrained LLM-LM and LLM-KD is presented in Table 1. We can observe that generally LLM-KD performs better than LLM-LM, though the improvement is marginal, indicating that pre-training distillation is feasible, but the current distillation configurations may not be optimal. Therefore, in the following sections ( 3.2 to 3.5), we will explore the design space of pre-training distillation to identify more effective configurations."
        },
        {
            "title": "3.2 Design Dimension #1: Logits Processing",
            "content": "This section explores the impact of logit processing in pre-training distillation, specifically in Equation 1, including the method for truncating logits HellaSwag WinoGrande PIQA MMLU KBQA C3 C-Eval GSM8k Average LLM-LM LLM-KD 53.3 54.2 1.7% 54.8 55.2 37.7 38.3 0.7% 0.5% 0.5% 1.3% 1.9% 3.2% 24.6% 1.6% 72.9 72. 54.7 55.8 25.9 26.7 28.0 27.8 8.6 10.8 3.6 3.5 Table 1: Preliminary experimental results on the evaluation datasets. is relative to LLM-LM. τ 0.05 1.6 0.1 2. 0.2 2.5 0.5 2.7 1.0 1. 2.0 5.0 10.0 2.5 0.1 1.0 Table 2: Relative improvements (%) compared to LLMLM using different τ in logits normalization. cantly reduces storage space, as shown in Figure 2 and 3. In this section, we empirically investigate the impact of different and k. Specifically, we set = 100 to study the impact of varying on top-p-100 truncation, and set = 0.95 to analyze the effect of different values on top-0.95-k truncation. The results are shown in Figure 2 and 3. We can observe that (1) for top-p-100 truncation, different leads to similar improvements. possible explanation is that in distillation pre-training, student LLM primarily captures the mass of the logits. This suggests that smaller can be used to further reduce storage space. (2) For top-0.95-k truncation, all values of lead to improvements, with = 50 yielding the best results. For = 1, which is adopted in AFM pre-training (Gunter et al., 2024), is equivalent to using the LM loss but with labels generated from the teacher LLM and also yields an improvement. This may be due to the teacher LLM conducting implicit noise filtering in pre-training corpora. In general, pre-training distillation with different and values in top-p-k truncation shows improvements with limited differences, and one can adopt smaller and in logits truncation to save storage disk space. Temperature τ Another factor is the temperature τ in logits normalization. lower temperature sharpens the logits distribution, while higher temperature results in more uniform distribution. We first examine the impact of different static τ , as shown in Table 2. We can observe that lower temperatures (τ 2.0) lead to similar improvement, whereas at higher temperatures (τ 5.0), the improvement is limited. This suggests that learning from more uniform distribution may be not efficient for student LLM. We also explore adaptive temperature, where temperature dynamically adjusts based on each sample, i.e., each token in Figure 2: Relative improvements compared to LLMLM using different in top-p-100 logits truncation and logits sizes per token with different p. The sizes are estimated using 10 million tokens. Figure 3: Relative improvements compared to LLMLM using different in top-0.95-k logits truncation and logits sizes per token with different k. and the temperature τ for normalization. If not stated otherwise, all experiments adopt the same setup as the preliminary experiment, except for the processing of logits. More experimental details and results are placed in appendix A.2. Logits Truncation As mentioned in the preliminary experiment ( 3.1), storing the logits of the entire vocabulary requires significant disk storage space. To save resources, we design two-stage top-p-k truncation method: truncating with top-p first, followed by top-k truncation. When the logits distribution is sharp, top-p truncation is enough; when the distribution is more uniform with longtailed non-trivial values, top-k truncation works as secondary truncation. Compared to vanilla top-p and top-k truncation, the top-p-k method signifiHellaSwag WinoGrande PIQA MMLU KBQA C3 C-Eval GSM8k Average NormKD WTTM AdaKDSD AdaKDH 51.2 51.4 54.7 54.7 54.1 56.2 54.5 57. 71.0 72.9 73.0 73.4 26.6 26.7 25.7 25.6 3.2 3.6 3.7 3.7 54.6 55.1 56.1 57.0 29.0 27.3 25.9 27.0 8.0 9.2 11.8 10. 37.2 37.8 38.2 38.8 1.3% 0.2% 1.2% 2.8% Table 3: Experimental results of LLMs pre-trained with different adaptive temperature τ methods. pre-training distillation. We investigate two representative methods: NormKD (Chi et al., 2023) and WTTM (Zheng and YANG, 2024). NormKD applies adaptive temperature to both teacher and student logits, while WTTM applies only to the teacher logits. In this experiment, along with temperature τ , the loss calculation method is also modified. For details, refer to their original papers, and relevant hyper-parameters in loss calculation are listed in appendix A.2. We also implement compact version of the adaptive temperature method, named AdaKD, which applies higher temperature to smooth sharper teacher logits and lower temperature for less sharp logits to help the student LLM focus on the most important parts (Wei and Bai, 2024). We use standard deviation and entropy to measure the sharpness of the logits, referred to as AdaKDSD and AdaKDH, and adaptively calculate the temperature accordingly. AdaKDSD adopts the standard deviation as the temperature τ . AdaKDH adopts τH in Equation 5. τH = τmax (τmax τmin) Hmax (5) denotes the entropy of each sample. More experimental details are placed in appendix A.2. The results are presented in Table 3. We can observe that AdaKDH performs the best, but compared to static temperature (τ = 0.5), adaptive temperature does not show significant additional improvement."
        },
        {
            "title": "3.3 Design Dimension #2: Loss Selection",
            "content": "This section explores loss selection in pre-training distillation, including the types of distillation loss and the selection of combinations with the LM loss, i.e., α in Equation 1. For all the experiments in this section, all settings remain the same as those in the preliminary experiment, except for the choice of loss. More results are placed in appendix A.3. Distillation Loss Function We first explore the impacts of different distillation loss functions L. Specifically, we examine three common-used types of loss function: negative log-likelihood (NLL) as α 0.1 0. 0.5 1.5 0.6 1.4 0.7 2. 0.8 2.0 0.9 3.6 0.95 2. 1.0 1.6 Table 4: Relative improvements (%) compared to LLMLM using different α in combination of Llm and Lkd. used in the preliminary experiment ( 3.1), KullbackLeibler divergence (KLD), and mean squared error (MSE) loss. To control for variables, we omit LM loss and only use the distillation loss, setting α = 1 in Equation 1. The experimental results are presented in Table 5. We can find that the LLMs trained with NLL and KLD loss both perform better than LLM-LM. While LLM-KLD generally outperforms LLM-NLL, the latter demonstrates superior performance on more challenging datasets, such as MMLU and C-Eval. The student LLM trained with MSE loss exhibits significant performance decline, as observed in previous studies (Muralidharan et al., 2024). This finding contrasts with prior research in image classification (Kim et al., 2021), which finds MSE loss is the most superior choice in knowledge distillation, indicating that the pretraining distillation of LLMs involves new training dynamics and requires further investigation. Combination of Llm and Lkd We examine the impact of different combinations of Llm and Lkd. We set Lkd as negative log-likelihood loss for all experiments. Specifically, we first explore the effect of different values of static α, ranging in {0.0, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0}. The results are shown in Table 4, and we can observe that as α increases, the PD performance improves generally, then declines, with the best performance at α = 0.9. This suggests that while higher proportion of distillation loss can boost the distillation performance, an appropriate ratio (about 10%) of LM loss can further enhance pre-training distillation performance. We further explore dynamic scheduling of α in the following ways: (1) α linearly increases from 0 to 1, namely Linear Inc, or decreases from 1 to 0, namely, Linear Dec, during pre-training. The intuition of the former is that training initially with HellaSwag WinoGrande PIQA MMLU KBQA C3 C-Eval GSM8k Average 0-α+WSD-LR LLM-NLL LLM-KLD LLM-MSE Linear Inc Linear Dec Period 1-α+WSD-LR WSD-α+Cos-LR WSD-β+WSD-LR WSD-α+WSD-LR 54.1 54.2 55.3 44.6 53.6 53.4 52.9 56.1 54.0 53.1 56.4 55.1 55.2 56.7 55.0 55.2 56.6 55.0 57.2 55.4 55.2 57. 73.1 72.5 73.5 69.6 73.1 72.9 72.3 73.6 72.7 73.7 73.6 27.5 27.8 26.7 25.2 25.9 29.6 28.4 27.0 25.1 27.5 31. 3.8 3.5 3.6 2.8 3.4 3.6 3.4 3.8 3.7 3.6 2.6 55.6 55.8 56.7 52.2 56.4 56.0 55.1 58.3 57.6 55.7 57. 27.5 26.7 25.4 25.6 28.9 30.5 27.9 29.1 29.4 25.0 33.8 8.5 10.8 11.5 3.9 8.5 11.4 9.4 11.6 10.6 11.2 12. 38.2 1.2% 38.3 38.7 34.9 38.1 39.2 38.0 39.6 38.6 38.1 40.7 1.6% 2.6% 7.6% 1.1% 4.1% 0.9% 5.0% 2.3% 1.1% 8.0% Table 5: Experimental results of LLMs pre-trained with different pre-training loss. is relative to LLM-LM. 0-α and 1-α denote setting α = 0 and α = 1.0, respectively. 0-α+WSD-LR represents LLM-LM training with the WSD scheduler, which serves as baseline. Cos-LR means cosine learning rate scheduler. β 1 α, and WSD-β denotes applying the WSD scheduler to the proportion of LM loss. learning rate scheduler generally provides benefits, with greater gains when combined with KD loss. (3) The WSD α scheduler with the WSD learning rate scheduler yields the best performance, and the improvement of WSD β (β 1 α) scheduler with WSD-LR is limited, suggesting that using KD loss when maintaining high learning rate effectively enhances model performance. Compared to WSD-LR with only KD loss, WSD-α performs better, indicating that small proportion of LM loss can further enhance distillation performance."
        },
        {
            "title": "3.4 Design Dimension #3: Scaling Law",
            "content": "We investigate the scaling law of pre-training distillation, including the impact of varying sizes of student and teacher LLMs, as well as the pre-training corpus size. All experimental settings are the same as the preliminary experiment, except for the sizes of LLMs and pre-training corpus. More experimental details are placed in appendix A.4. Model Size We first investigate the performance with varying sizes of student and teacher LLMs in pre-training distillation. Specifically, we adopt teacher LLMs of 9B and 32B to distill student LLMs of 330M, 670M, 1.9B, 3.8B, and 6.8B. For each size of the student LLM, we pre-train baseline LLM using only the LM loss, i.e., setting α = 0 in Equation 1. The relative improvements compared to baseline LLMs are illustrated in Figure 4. We can observe that: (1) Larger student LLMs generally benefit more from pre-training distillation. (2) Distilling from larger teacher LLM does not necessarily yield better performance. This may be due to the capacity gap between teacher and student LLMs (Mirzadeh et al., 2020; Gou et al., 2021). Increasing the student LLM size or using Figure 4: Relative improvements compared to LLMLM using varying sizes of student and teacher LLMs. LM loss may help mitigate the effects of the capacity gap with the teacher LLM; the latter is that using KD loss first may provide better optimization initialization (Yim et al., 2017). (2) α periodically varies between 0 and 0.9, namely Period, setting α to 0.9 at every fourth batch and 0 for the other batches (Kiefel and Shah, 2024). (3) We employ nonlinear scheduler, warmup-stable-decay (WSD; Hu et al., 2024), for scheduling α, namely WSD-α. Specifically, we first linearly increase α from 0 to 1.0 during the warm up stage, then stay α = 1.0, and finally apply cosine decay to reduce α from 1.0 to 0. We set the warmup ratio at 10% and the decay ratio at 1%. Furthermore, we employ the WSD learning rate scheduler (Hu et al., 2024), namely WSD-LR, setting its warmup and decay ratios as those of WSD-α. The intuition is that when the learning rate stays at its maximum, utilizing KD loss may enhance training efficiency. The results are shown in Table 5. We can observe that: (1) linear decrease in α outperforms linear increase, indicating that involving more KD loss in the early pre-training stage is more beneficial. (2) The WSD HellaSwag WinoGrande PIQA MMLU KBQA C3 C-Eval GSM8k Average LLM-Online-100B-L LLM-Online-100B LLM-Online-100B* 30.1 49.5 52.9 53.0 54.2 55.4 62.1 70.5 72. 24.5 25.2 26.6 0.7 3.0 3.6 40.2 54.2 57.0 25.9 25.5 25.4 2.4 8.0 10.0 29.8 36.3 37. 20.9% 3.9% 0.5% Table 6: Experimental results of different LLMs pre-trained with online logits. is relative to LLM-LM. iment. The results are illustrated in Figure 5. We can observe that: (1) Compared to student LLMs trained only with LM loss, pre-training distillation consistently yields improvements throughout the pre-training process, remaining effective with more tokens. (2) The gains from pre-training distillation increase initially during pre-training and then converge with slight decrease, and are still significant are the end of pre-training. This suggests that pre-training distillation not only enhances training efficiency but also improves the performance upper bound of student LLMs. Due to computational limitations, we do not reach trillion-level tokens for pre-training which are used by most advanced LLMs (Team et al., 2024; Dubey et al., 2024; Gunter et al., 2024; GLM et al., 2024; Team, 2024; Liu et al., 2024). We believe that pre-training distillation is also effective using several trillion tokens and encourage future LLM development to incorporate pre-training distillation."
        },
        {
            "title": "3.5 Design Dimension #4: Offline or Online",
            "content": "This section explores how logits are obtained, either offline or online. Offline means that logits are obtained from pre-trained teacher LLM, which is the setting for all previous experiments. Online refers to storing logits generated simultaneously during the pre-training of the teacher LLM. The advantage of online is that it does not require additional inference from the teacher LLM if one stores the logits during teacher pre-training. Another potential advantage is that learning from online logits is similar to curriculum learning (Soviany et al., 2022), which may help mitigate the capacity gap and improve learning efficiency. Due to the high cost of pretraining GLM-4-9B from scratch, we preliminarily pre-train GLM-4-9B from scratch using 400 billion tokens while storing the logits for each token. We first distill two 1.9B student LLMs using the setup in 3.1: LLM-Online-100B-L and LLM-Online100B, which adopt the first and the last 100 billion tokens during teacher LLMs pre-training process, respectively. Experimental details are presented in appendix A.5. The results are presented in Table 6. Both LLMs yield poor performance, particularly Figure 5: Experimental results of the checkpoints saved every 10, 000 step (about 83B tokens) during the pretraining of 1.9B and 3.8B LLMs on 500B tokens. The last data point is from the checkpoint saved at the end. smaller teacher LLM can both reduce this gap and hence improve distillation performance. From compression perspective, larger LLMs compress information more effectively and achieve better compression rates (Deletang et al., 2024), potentially making it harder for smaller LLMs to learn. Our experiments demonstrate that pre-training distillation is effective when the size of the student LLM reaches about 10% or more of the teacher LLM size, and as the proportion increases, the benefits of pre-training distillation grow until reaches the turning point. Due to computational constraints, we do not explore the turning point of performance gain to the proportion, which we leave as future work. Furthermore, scaling the student LLM to larger sizes may yield new interesting findings, such as the weak-to-strong generalization (Burns et al., 2024): using small teacher LLM to help train large student LLM. Due to computational constraints, we leave these explorations as future work. Corpus Size We further investigate the impact of pre-training corpus size. Specifically, we use GLM4-9B as the teacher LLM and distill 1.9B and 3.8B student LLMs with 500 billion tokens. We also pretraining corresponding baseline LLMs with only LM loss. We save checkpoint every 10, 000 optimization step (about 83B tokens) and save the last checkpoint at the end of pre-training. All the other settings are consistent with the preliminary experLLM-Online-100B-L. The reason may be that the teacher LLM is far from convergence, and hence the logits contain substantial noise. We adjust the loss calculation with α = 0.1 and use top-0.9550 truncation to train LLM-Online-100B*, which performs slightly better than LLM-LM, although it still underperforms LLM-KD using offline logits. This indicates that even logits generated by non-converged teacher LLM can help pre-training student LLM, suggesting that using online logits is also effective and better practice is to utilize the logits from the later stages of the teacher LLMs pretraining. We suggest that if one aims to pre-train only an LLM, using offline logits of pre-trained teacher LLM is better; if one aims to pre-train series of LLMs of varying sizes, one can first pretrain the largest LLM while storing online logits, and then pre-train smaller LLMs with online logits."
        },
        {
            "title": "4 Related Work",
            "content": "Knowledge distillation aims to transfer knowledge from large teacher model into smaller student model for model compression. It is first formalized by Hinton (2015), which adopts the teacher models logits as soft targets to train the student model, which can provide richer information (Gou et al., 2021) and is also similar to label smoothing (Kim and Kim, 2017) and regulation (Müller et al., 2019; Ding et al., 2019). In this paper, we also focus on logits-based knowledge distillation. Knowledge distillation has been widely applied in in computer vision (Komodakis and Zagoruyko, 2017; Ahn et al., 2019; Wang et al., 2020b; Bergmann et al., 2020; Zhao et al., 2022; Habib et al., 2023), natural language processing (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020a; Chen et al., 2020; Taori et al., 2023; Xu et al., 2024), and speech recognition (Chebotar and Waters, 2016; Fukuda et al., 2017; Tan and Wang, 2021) domains. Since the emergence of ChatGPT (OpenAI, 2022), knowledge distillation has become one of the most crucial techniques for enhancing large language models (LLMs). Typically, KD is applied during the post-training phase in sequencelevel (Kim and Rush, 2016) to efficiently align them with humans (Xu et al., 2024), where student LLMs are trained using teacher-forcing language modeling loss from instructions and corresponding responses generated by advanced proprietary LLMs, such as GPT-4 (OpenAI, 2023). Alpaca (Taori et al., 2023) is the first public LLM distilled from ChatGPT, providing practical approach for improving open-source LLMs. Due to the compactness and efficacy of post-training KD, it is widely applied in developing various LLMs (Xu et al., 2023; Taori et al., 2023; Vicuna, 2023; Mitra et al., 2023; Ding et al., 2023; Sun et al., 2024; Qi et al., 2024; Cui et al., 2024), which significantly advances the development of LLMs. For pre-training distillation of language models, there are two main categories of related work: (1) Distilling small language models in the preChatGPT era (Sanh et al., 2019; Jiao et al., 2020; Wang et al., 2020a; Xu et al., 2020; Sun et al., 2020b; Zhang et al., 2020; Liu et al., 2020; Hou et al., 2020). These approaches are usually based on models with only several million parameters, such as BERT (Kenton and Toutanova, 2019), and hence their training configurations may not be directly applicable for billion-level LLMs. (2) Distilling LLMs (Gu et al., 2024; Muralidharan et al., 2024; Kiefel and Shah, 2024; Turuvekere Sreenivas et al., 2024; Team et al., 2024; Gunter et al., 2024). MiniLLM (Gu et al., 2024) is trained based on pre-trained LLM rather than from scratch. Gemma 2 (Team et al., 2024), AFM (Gunter et al., 2024), LokiLM (Kiefel and Shah, 2024), and Minitron (Turuvekere Sreenivas et al., 2024) employ pre-training distillation but provide limited details on the distillation process. While Muralidharan et al. (2024) explores the best practices for pruning and distillation of LLMs, it mainly focuses on pruning and does not systematically explore pretraining distillation. In this work, we systematically explore the design space of pre-training distillation and conduct extensive experiments to find key impact factors and better configurations. Our findings can also be applied to previous pruning and distillation work, and we hope these explorations could inform future practices in pre-training distillation."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we systematically explore the design space of pre-training distillation, including four main impacting factors: logits processing, loss selection, scaling law, and strategies for obtaining logits, i.e., offline or online. We conduct extensive experiments to study each design dimension and identify better configurations. We also draw some interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation while larger teacher LLMs do not guarantee better results. We hope our exploration will inform future practices in pre-training distillation. sense in natural language. In Proceedings of AAAI, volume 34, pages 74327439."
        },
        {
            "title": "Limitations",
            "content": "The main limitation of this work is that we do not explore the interactions between different factors in pre-training distillation, that is, the different combinations of factors. This is unaffordable, as these experiments are too resource-intensive given the complexity of factor combinations. Our controlled variable experiments have already incurred significant computational costs, which emit significant amount of carbon dioxide and negatively impact the environment (Strubell et al., 2019). While searching the combinations of factors could identify best practices, we believe our experiments and explorations are sufficiently solid to inform future practices in pre-training distillation."
        },
        {
            "title": "Ethical Considerations",
            "content": "We discuss the ethical considerations of this work: (1) Intellectual property. We strictly adhere to the copyright licenses of all the used models and datasets. (2) Intended use. Our work explores the design space of pre-training distillation, aiming to inform future practices in pre-training distillation. (3) Potential risk control. We believe the data used has been properly anonymized. As an empirical study, we do not publish additional artifacts. (4) AI assistance. We adopt ChatGPT for paraphrasing some sentences and grammar checks."
        },
        {
            "title": "References",
            "content": "Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil Lawrence, and Zhenwen Dai. 2019. Variational information distillation for knowledge transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 91639171. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. 2023. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of EMNLP, pages 48954901. Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. 2020. Uninformed students: Studentteacher anomaly detection with discriminative latent embeddings. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 41834192. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonCollin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. 2024. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Proceedings of ICML. Yevgen Chebotar and Austin Waters. 2016. Distilling knowledge from ensembles of neural networks for speech recognition. In Interspeech, pages 3439 3443. Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, and Jingjing Liu. 2020. Distilling knowledge learned in bert for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 78937905. Zhihao Chi, Tu Zheng, Hengjia Li, Zheng Yang, Boxi Wu, Binbin Lin, and Deng Cai. 2023. Normkd: Normalized logits for knowledge distillation. arXiv preprint arXiv:2308.00520. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. 2024. Ultrafeedback: Boosting language models with scaled ai feedback. In Proceedings of ICML. Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent Orseau, et al. 2024. Language modeling is compression. In The Twelfth International Conference on Learning Representations. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 30293051. Qianggang Ding, Sifan Wu, Hao Sun, Jiadong Guo, and Shu-Tao Xia. 2019. Adaptive regularization of labels. arXiv preprint arXiv:1908.05474. Nan Duan. 2016. Overview of the nlpcc-iccpol 2016 shared task: Open domain chinese question answering. In Natural Language Understanding and Intelligent Applications, pages 942948. Springer International Publishing. Nan Duan and Duyu Tang. 2018. Overview of the nlpcc 2017 shared task: Open domain chinese question In Natural Language Processing and answering. Chinese Computing, pages 954961. Springer International Publishing. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Takashi Fukuda, Masayuki Suzuki, Gakuto Kurata, Samuel Thomas, Jia Cui, and Bhuvana Ramabhadran. 2017. Efficient knowledge distillation from an ensemble of teachers. In Interspeech, pages 3697 3701. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793. Jianping Gou, Baosheng Yu, Stephen Maybank, and Dacheng Tao. 2021. Knowledge distillation: survey. International Journal of Computer Vision, 129(6):17891819. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024. Minillm: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations. Tom Gunter, Zirui Wang, Chong Wang, Ruoming Pang, Andy Narayanan, Aonan Zhang, Bowen Zhang, Chen Chen, Chung-Cheng Chiu, David Qiu, et al. 2024. Apple intelligence foundation language models. arXiv preprint arXiv:2407.21075. Gousia Habib, Tausifa Jan Saleem, and Brejesh Lall. 2023. Knowledge distillation in vision transformers: critical review. arXiv preprint arXiv:2302.02108. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In International Conference on Learning Representations. Geoffrey Hinton. 2015. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations. Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. 2020. Dynabert: Dynamic bert with adaptive width and depth. Advances in Neural Information Processing Systems, 33:97829793. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. 2024. Minicpm: Unveiling the potential of small language models arXiv preprint with scalable training strategies. arXiv:2404.06395. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36. Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2020. Tinybert: Distilling bert for natural language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 41634174. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. 2019. study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322. Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT. Justin Kiefel and Shrey Shah. 2024. Lokilm: Technical report. arXiv preprint arXiv:2407.07370. Seungwook Kim and Hyo-Eun Kim. 2017. Transferring knowledge to smaller network with class-distance loss. Taehyeon Kim, Jaehoon Oh, Nak Yil Kim, Sangwook Cho, and Se-Young Yun. 2021. Comparing kullbackleibler divergence and mean squared error loss in knowledge distillation. Yoon Kim and Alexander Rush. 2016. Sequencelevel knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 13171327. Diederik Kingma. 2014. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980. Nikos Komodakis and Sergey Zagoruyko. 2017. Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. In Proceedings of ICLR. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong et al. 2024. Ruan, Damai Dai, Daya Guo, Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434. Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. 2020. Fastbert: selfdistilling bert with adaptive inference time. In Proceedings of ACL, pages 60356044. Seyed Iman Mirzadeh, Mehrdad Farajtabar, Ang Li, Nir Levine, Akihiro Matsukawa, and Hassan Ghasemzadeh. 2020. Improved knowledge distillation via teacher assistant. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 51915198. Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, et al. 2023. Orca 2: Teaching small language models how to reason. arXiv preprint arXiv:2311.11045. Rafael Müller, Simon Kornblith, and Geoffrey Hinton. 2019. When does label smoothing help? Advances in neural information processing systems, 32. Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. 2024. Compact language models via pruning and knowledge distillation. arXiv preprint arXiv:2407.14679. OpenAI. 2022. Chatgpt. https://openai.com/ index/chatgpt/. Accessed: 2024-10-04. OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, and Juanzi Li. 2024. Adelie: Aligning large language models on information extraction. arXiv preprint arXiv:2405.05008. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Winogrande: An adIn versarial winograd schema challenge at scale. Proceedings of AAAI, volume 34, pages 87328740. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, distilled version of bert: smaller, faster, cheaper and lighter. In NeurIPS EM 2 Workshop. Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and Nicu Sebe. 2022. Curriculum learning: survey. International Journal of Computer Vision, 130(6):1526 1565. Emma Strubell, Ananya Ganesh, and Andrew Mccallum. 2019. Energy and policy considerations for deep learning in nlp. In Proceedings of ACL, pages 3645 3650. Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. 2020a. Investigating prior knowledge for challenging chinese machine reading comprehension. Transactions of the Association for Computational Linguistics, 8:141155. Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020b. Mobilebert: compact task-agnostic bert for resource-limited devices. In Proceedings of ACL, pages 21582170. Ke Tan and DeLiang Wang. 2021. Towards model compression for deep learning based speech enhancement. IEEE/ACM transactions on audio, speech, and language processing, 29:17851794. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Alpaca: strong, replicable instruction-following model. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Qwen Team. 2024. Qwen2.5: party of foundation models. Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive representation distillation. In Proceedings of ICLR. Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. 2024. Llm pruning and distillation in practice: The minitron approach. arXiv e-prints, pages arXiv2408. Vicuna. 2023. Vicuna: An open-source chatbot impressing GPT-4 with 90%* ChatGPT quality. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020a. Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:57765788. Xiaobo Wang, Tianyu Fu, Shengcai Liao, Shuo Wang, Zhen Lei, and Tao Mei. 2020b. Exclusivityconsistency regularized knowledge distillation for face recognition. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part XXIV 16, pages 325342. Springer. Yukang Wei and Yu Bai. 2024. perature knowledge distillation. arXiv:2404.12711. Dynamic temarXiv preprint Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, and Ruohui Huang. 2024. Conifer: Improving complex constrained instructionfollowing ability of large language models. arXiv preprint arXiv:2404.02823. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou. 2020. Bert-of-theseus: Compressing bert by progressive module replacing. In Proceedings of EMNLP, pages 78597869. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. 2024. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116. Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. 2017. gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 41334141. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? In Proceedings of ACL, pages 47914800. Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, and Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert. In Proceedings of EMNLP, pages 509521. Borui Zhao, Quan Cui, Renjie Song, Yiyu Qiu, and Jiajun Liang. 2022. Decoupled knowledge distillation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1195311962. Kaixiang Zheng and EN-HUI YANG. 2024. Knowledge distillation based on transformed teacher matching. In The Twelfth International Conference on Learning Representations. experiment before evaluation. The results on all evaluation datasets are shown in Table 11 and 12. We report the averaged performance in Figure 5. A.5 Offline or Online We pre-train new 9B LLM from scratch as the teacher LLM, with 1, 728 batch size, 4, 096 max sequence length, cosine learning rate scheduler with 6 104 maximum learning rate, 6 105 minimum learning rate, and 1% warmup rate. Due to the high cost, we only adopt 400B tokens and store their logits simultaneously, which consumes about 180TB of disk storage space. This indicates that, since the teacher LLM has not yet converged, the logits are more uniform and contain more noise. A.6 Better Configuration for PD Based on our exploration, we select better configuration for pre-training distillation. For logits processing, we use top-0.95-50 truncation and apply temperature of τ = 2.0 for normalization. For loss selection, we adopt KLD as the distillation loss and combine it with LM loss using WSD-α and WSD-LR. The WSD hyper-parameters are the same as in 3.3, except for the maximum value of α, which is set to 0.9. We use GLM-4-9B as the teacher LLM to distill 1.9B and 3.8B student LLMs. We adopt offline logits for PD. The results on all evaluation datasets are shown in Table 13. We report the averaged performance in Figure 1."
        },
        {
            "title": "A Experimental Details and more Results",
            "content": "This section introduces the experimental details and additional results. All experiments are conducted on Nvidia H800 GPUs. A.1 Preliminary Experiment The architecture of the 1.9B student LLM is shown in Table 7. For the SFT phase, we utilize mixture of 10B high-quality instruction-tuning data and an additional 10B pre-training text corpus. For the instruction-tuning data, we only compute the language modeling loss for the response part. In the SFT stage, we adopt 256 batch size, cosine learning rate scheduler with 4 105 maximum learning rate, 4 106 minimum learning rate, and 1% warmup rate. As for evaluation, we adopt zeroshot evaluation for HellaSwag, WinoGrande, PIQA, and KBQA; 5-shot evaluation for C3 and C-Eval; 6shot evaluation for MMLU; and 8-shot evaluation for GSM8k. We set the sampling temperature to 0. A.2 Logits Processing We first employ NormKD (Chi et al., 2023) and WTTM (Zheng and YANG, 2024) as the adaptive temperature calculation methods. Our implementation differs slightly from the original versions, as we use truncated logits instead of logits of the entire vocabulary. For NormKD, we set the hyperparameter T_norm to 1.0 and α to 0.5 in Equation 1. For WTTM, we set the hyper-parameters γ to 0.1 and β to 1.0. For τH in Equation 5, denotes the entropy of each sample, and Hmax is the largest entropy and is estimated on 10 million tokens. We set τmax = 2.0, τmin = 0.1, and Hmax = 4.8. Experimental results of 3.2 on all evaluation datasets are presented in Table 8 and 9. A.3 Loss Selection For the WSD scheduler (Hu et al., 2024), we adopt linear scheduler during the warmup stage and cosine scheduler during the decay stage. The experimental results using different α on all the evaluation datasets are shown in Table 10. A.4 Model Size The architectures of different sizes of student LLMs are shown in Table 7. When pre-training 1.9B and 3.8B student LLMs on 500 billion tokens, we save checkpoint every 10, 000 optimization step. We also save the checkpoint at the end. For each checkpoint, we conduct SFT as in the preliminary Hidden Size FFN Hidden Size #Layers #Attention Heads #Query Groups Tie 330M 670M 1.9B 3.8B 6.8B 1, 024 1, 024 2, 048 3, 072 4, 096 4, 096 4, 096 6, 912 8, 192 12, 12 24 24 28 28 16 16 16 24 32 2 2 2 8 8 True False False False False Table 7: Model architectures of student LLMs of varying sizes. #Query Groups denotes the number of query groups in grouped-query attention (GQA, Ainslie et al., 2023). Tie represents whether to tie the word embeddings and output weights. All the models are trained with BFLOAT16 (Kalamkar et al., 2019) format. HellaSwag WinoGrande PIQA MMLU KBQA C3 C-Eval GSM8k Average top-0.5-100 top-0.6-100 top-0.7-100 top-0.8-100 top-0.85-100 top-0.9-100 top-0.95-1 top-0.95-3 top-0.95-5 top-0.95-10 top-0.95-20 top-0.95-50 top-0.95-100 54.2 55.2 54.4 54.4 54.6 53. 52.4 53.3 53.8 54.4 53.8 54.0 54.2 55.8 55.0 57.5 56.7 53.7 54.9 55.6 56.6 55.7 54.2 56.2 54.1 55.2 72.9 73.7 72.7 72.5 73.6 72. 72.6 72.7 73.0 72.9 73.9 72.9 72.5 27.1 27.2 27.8 27.0 26.2 27.9 27.1 27.9 28.5 28.8 26.3 33.2 27.8 3.6 2.0 2.9 3.5 3.4 3. 3.6 2.3 3.6 4.0 2.8 3.9 3.5 56.3 56.6 56.7 56.0 56.5 55.5 56.6 55.9 56.4 56.0 57.4 55.9 55.8 28.1 25.9 27.0 26.2 26.8 28. 28.2 25.8 29.0 27.3 24.2 31.5 26.7 9.8 11.0 9.4 10.6 10.8 9.2 11.4 10.5 9.7 10.7 10.6 11.2 10.8 38.5 38.3 38.5 38.4 38.2 38. 38.4 38.1 38.7 38.5 38.2 39.6 38.3 Table 8: Experimental results on all the evaluation datasets using different and in top-p-k truncation. HellaSwag WinoGrande PIQA MMLU KBQA C3 C-Eval GSM8k Average τ = 0.05 τ = 0.1 τ = 0.2 τ = 0.5 τ = 1.0 τ = 2.0 τ = 5.0 τ = 10.0 53.1 52.6 53.5 54.7 54.2 54.1 52.5 52.1 57.0 54.2 56.9 57.0 55.2 56.7 55.8 57.1 72.0 72.6 73.2 74.2 72.5 73.2 72.8 73.0 29.2 28.6 27.8 28.2 27.8 27.8 23.5 27.3 3.4 2.6 3.6 3.9 3.5 3.7 3.3 3. 55.8 56.1 56.2 56.1 55.8 56.2 56.2 53.9 26.8 30.6 27.3 26.0 26.7 27.0 27.9 30.2 9.2 10.8 10.8 9.8 10.8 10.5 9.6 8.0 38.3 38.5 38.7 38.7 38.3 38.7 37.7 38.1 Table 9: Experimental results on all the evaluation datasets using different τ in logits normalization. HellaSwag WinoGrande PIQA MMLU KBQA C3 C-Eval GSM8k Average α = 0 α = 0.1 α = 0.5 α = 0.6 α = 0.7 α = 0.8 α = 0.9 α = 0.95 α = 1.0 53.3 53.4 53.8 53.7 53.6 54.3 55.1 53.4 54.2 54.8 56.0 54.4 55.7 56.6 56.6 57.4 57.1 55.2 72.9 72.9 72.6 73.4 73.4 72.4 73.0 72.1 72. 28.0 26.4 26.9 27.8 28.5 28.2 29.6 28.7 27.8 3.6 3.2 3.4 3.4 3.8 3.8 3.5 3.4 3.5 54.7 55.8 55.9 54.4 55.0 55.5 57.2 56.4 55.8 25.9 24.1 29.8 28.8 29.6 26.6 25.6 28.4 26.7 8.6 9.6 9.6 8.6 10.1 10.5 11.1 9.7 10.8 37.7 37.7 38.3 38.3 38.8 38.5 39.1 38.7 38. Table 10: Experimental results on all the evaluation datasets using different α in Equation 1. HellaSwag WinoGrande PIQA MMLU KBQA C3 C-Eval GSM8k Average 330M 670M 1.9B 3.8B 6.8B 330M 670M 1.9B 3.8B 6.8B 330M 670M 1.9B 3.8B 6.8B 37.4 42.3 53.3 59.0 63.0 37.7 43.4 54.2 61.4 66.0 37.1 43.0 53.7 60.8 66.2 Baseline: LM Loss 54.1 51.9 54.8 57.8 59. 67.4 68.6 72.9 75.4 75.5 24.0 26.7 28.0 34.5 36.7 Teacher LLM: GLM-4-9B 51.8 50.9 55.2 60.2 62.3 68.8 69.4 72.5 75.6 76.3 23.5 25.7 27.8 39.1 41. Teacher LLM: GLM-4-32B 51.5 51.5 57.9 57.6 62.3 67.4 69.5 73.4 75.0 76.6 24.2 27.0 26.2 33.9 41.4 2.0 2.3 3.6 4.6 4.6 1.8 2.4 3.6 5.0 5. 2.0 2.2 3.4 2.7 5.1 47.3 48.9 54.7 57.8 61.8 45.8 49.4 55.8 61.0 64.4 45.2 50.2 54.6 60.8 63.7 26.2 24.8 25.9 33.4 37.1 25.2 26.2 26.7 39.5 43. 24.5 26.4 26.3 38.0 41.4 2.3 3.0 8.6 13.7 20.9 2.1 3.1 10.8 17.1 25.5 1.4 3.9 8.0 14.7 22.7 32.6 33.6 37.7 42.0 44.9 32.1 33.8 38.3 44.9 48. 31.6 34.2 37.9 42.9 47.4 Table 11: Experimental results on all the evaluation datasets of baseline LLMs trained with only LM loss and distilled LLMs using varying sizes of teacher and student LLMs. HellaSwag WinoGrande PIQA MMLU KBQA C3 C-Eval GSM8k Average 10,000 20,000 30,000 40,000 50,000 59, 10,000 20,000 30,000 40,000 50,000 59,604 10,000 20,000 30,000 40,000 50,000 59,604 10,000 20,000 30,000 40,000 50,000 59,604 52.3 56.4 58.5 59.8 60.6 61.1 53.8 58.1 60.0 60.9 61.8 61.9 58.6 63.5 65.7 67.1 68.0 68. 60.8 65.3 67.2 68.3 69.1 69.5 1.9B LLM pre-trained with LM Loss 55.4 57.6 58.6 57.6 58.0 58.8 72.1 74.0 74.5 74.8 75.8 75.4 27.8 31.9 33.6 35.7 37.8 37.7 3.4 4.0 4.2 4.3 4.6 4. 1.9B LLM pre-trained with KD Loss 57.1 58.7 59.1 60.0 59.9 60.3 73.0 74.3 74.6 74.9 75.4 75.5 26.0 31.4 34.4 35.1 38.5 38.9 3.1 3.7 4.6 4.9 4.3 4.6 3.8B LLM pre-trained with LM Loss 59.9 61.3 63.6 63.2 64.2 63.1 74.4 75.6 76.1 76.6 76.7 77.3 33.1 41.0 42.8 45.2 46.0 46.9 4.7 4.4 2.8 1.3 4.5 2.3 3.8B LLM pre-trained with KD Loss 61.5 63.1 65.2 65.4 67.4 66. 75.6 76.3 76.4 76.7 77.3 77.7 31.7 41.6 47.0 49.4 51.3 52.4 4.8 5.7 6.2 6.9 6.7 6.8 56.3 58.2 59.4 60.4 62.0 60.9 56.3 59.6 60.0 61.7 61.9 61.8 60.2 63.2 65.1 65.8 66.9 66. 61.0 64.0 66.4 67.1 68.5 68.5 26.4 31.2 38.0 36.9 40.3 39.7 25.9 31.5 35.8 38.0 41.4 40.3 36.8 42.3 47.3 46.1 48.0 47.8 36.6 44.8 47.5 50.2 50.9 52.3 8.0 10.3 12.3 14.5 14.9 15. 10.7 14.5 18.0 19.0 20.6 19.4 12.8 20.5 23.7 25.8 28.5 29.3 19.0 26.5 30.9 35.0 36.5 36.2 37.7 40.5 42.4 43.0 44.2 44.2 38.2 41.5 43.3 44.3 45.5 45.4 42.6 46.5 48.4 48.9 50.3 50. 43.9 48.4 50.9 52.4 53.5 53.7 Table 12: Experimental results on all the evaluation datasets of different checkpoints saved every 10, 000 optimization step when pre-training the LLMs on 500 billion tokens. 59604 is the last checkpoint saved at the end. HellaSwag WinoGrande PIQA MMLU KBQA C3 C-Eval GSM8k Average 1.9B 3.8B 6.8B 56.9 62.4 67.4 59.1 61.2 65.1 73.9 76.0 76.6 29.8 38.1 44.3 3.7 5.0 5.6 59.0 62.8 67. 35.2 38.5 44.7 12.4 21.5 27.4 41.2 45.7 49.8 Table 13: Experimental results on all the evaluation datasets of better pre-training distillation configuration."
        }
    ],
    "affiliations": [
        "Tsinghua University",
        "Zhipu AI"
    ]
}