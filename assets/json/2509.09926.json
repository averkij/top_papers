{
    "paper_title": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios",
    "authors": [
        "Jiahao Chen",
        "Zhiyuan Huang",
        "Yurou Liu",
        "Bing Su"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-tailed learning has garnered increasing attention due to its wide applicability in real-world scenarios. Among existing approaches, Long-Tailed Semi-Supervised Learning (LTSSL) has emerged as an effective solution by incorporating a large amount of unlabeled data into the imbalanced labeled dataset. However, most prior LTSSL methods are designed to train models from scratch, which often leads to issues such as overconfidence and low-quality pseudo-labels. To address these challenges, we extend LTSSL into the foundation model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate that fine-tuned foundation models can generate more reliable pseudolabels, thereby benefiting imbalanced learning. Furthermore, we explore a more practical setting by investigating semi-supervised learning under open-world conditions, where the unlabeled data may include out-of-distribution (OOD) samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World scenarios) to improve the discriminative ability. Experimental results on multiple benchmarks demonstrate that our method achieves superior performance compared to previous approaches, even when utilizing only 1\\% of the unlabeled data compared with previous works."
        },
        {
            "title": "Start",
            "content": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios Jiahao Chen, Zhiyuan Huang, Yurou Liu, Bing Su Renmin University of China nicelemon666@gmail.com 5 2 0 2 2 1 ] . [ 1 6 2 9 9 0 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Long-tailed learning has garnered increasing attention due to its wide applicability in real-world scenarios. Among existing approaches, Long-Tailed Semi-Supervised Learning (LTSSL) has emerged as an effective solution by incorporating large amount of unlabeled data into the imbalanced labeled dataset. However, most prior LTSSL methods are designed to train models from scratch, which often leads to issues such as overconfidence and low-quality pseudolabels. To address these challenges, we extend LTSSL into the foundation model fine-tuning paradigm and propose novel framework: LoFT (Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate that fine-tuned foundation models can generate more reliable pseudolabels, thereby benefiting imbalanced learning. Furthermore, we explore more practical setting by investigating semi-supervised learning under open-world conditions, where the unlabeled data may include out-of-distribution (OOD) samples. To handle this problem, we propose LoFTOW (LoFT under Open-World scenarios) to improve the discriminative ability. Experimental results on multiple benchmarks demonstrate that our method achieves superior performance compared to previous approaches, even when utilizing only 1% of the unlabeled data compared with previous works. Introduction Real-world data often follows long-tailed or imbalanced distribution, where small number of head classes dominate the majority of samples, while the remaining tail classes are represented by only limited number of instances (Cui et al. 2019). This imbalance poses significant challenges for model training, particularly in achieving satisfactory performance on tail classes. To address this issue, Long-Tailed Semi-Supervised Learning (LTSSL) has emerged as an effective solution by incorporating large amount of unlabeled data into the imbalanced labeled dataset (Wei et al. 2021; Wei and Gan 2023). The basic idea of LTSSL is to generate pseudo-labels for unlabeled data and select highconfidence samples to guide model training (Ouali, Hudelot, and Tami 2020). While the current methods have achieved notable success and demonstrated promising results, they still face dilemmas hindering further improvement. Previous LTSSL approaches typically rely on training Convolutional Neural Networks (CNNs) from scratch (Wei Copyright 2025 Figure 1: Differences among supervised learning, semisupervised learning, and semi-supervised learning in openworld scenarios. Pentagrams in yellow and green denote samples of head classes and tail classes, respectively. et al. 2021), which presents several challenges. First, CNNs are known to be overconfident (Guo et al. 2017), often assigning high-confidence scores to incorrect predictions. Although methods like FixMatch (Sohn et al. 2020) employ weak-to-strong pipeline, using weakly augmented samples to determine labels and strong augmented samples to determine the logits, this overconfidence issue persists, especially for tail classes, as shown in Fig. 2. Second, in the early training stages, the model produces unreliable predictions, resulting in low-quality pseudo-labels. As result, current LTSSL approaches often require more training iterations and carefully designed strategies to dynamically manage the use of unlabeled data (Wei and Gan 2023). Both of the dilemmas limit the application of LTSSL. To alleviate the above issues, we propose novel and effective Long-tailed semi-supervised learning via parameter efficient Fine-Tuning (LoFT) framework. For details, LoFT is built upon transformer-based foundation models, e.g., vision transformer (ViT) (Dosovitskiy et al. 2020), which are pre-trained on large-scale dataset and fine-tuned via Parameter-Efficient Fine-Tuning (PEFT). We find that our LoFT can well tackle the above issues in existing methods as forementioned. Firstly, the model fine-tuned via LoFT is native well calibrated for both head classes and tail classes, as shown in Fig. 2. Compared with previous works, using fixed threshold to filter reliable samples, LoFT can decrease the interference of samples assigning erroneous pseudo-labels. Second, foundation models have high generalization ability for downstream tasks, which can reduce training time and reduce the impact of low model discrimination ability in the early stage of training, thereby reducing the impact of pseudo-label uncertainty. Beyond the traditional LTSSL setting, we also explore more challenging and realistic scenario, referred to as Longtailed Semi-Supervised Learning in Open-World Scenarios. The key distinction lies in the composition of the unlabeled data: unlike standard LTSSL, the unlabeled set in openworld scenarios may contain substantial proportion of outof-distribution (OOD) samples that do not belong to any of the labeled classes, as shown in Fig. 1. For example, when training wildlife classification model using images from known species, the unlabeled data may include rare or unknown animals not present in the labeled dataset, introducing additional complexity and ambiguity into the training process. If existing LTSSL methods are directly applied in such scenarios, OOD samples may be erroneously assigned in-distribution labels, thereby misleading the learning process. Furthermore, models trained from scratch generally lack the capacity to effectively identify or reject these OOD samples (Hendrycks and Gimpel 2016). To address this issue, our proposed LoFT-OW (LoFT under Open-World scenarios) framework inherently incorporates an OOD detection mechanism that enables the model to natively filter out irrelevant samples, as evidenced by the results in Tab. 1. By mitigating the negative impact of OOD instances, LoFT facilitates more discriminative representation learning and enhances robustness under diverse data conditions. Our contributions can be summarized as follows: We address the LTSSL problem and propose LoFT, novel framework that leverages Parameter-Efficient FineTuning (PEFT) of transformer-based foundation models (e.g., ViT). Through comprehensive experiments, we analyze the confidence behavior of LoFT and observe that it is inherently well-calibrated. We further demonstrate that this property can be effectively utilized to improve the quality of pseudo-labels. We extend LTSSL to more realistic Open-World Scenario, named LoFT-OW, where unlabeled data may contain OOD samples. LoFT incorporates built-in OOD detection mechanism, filtering out irrelevant samples and improving model robustness and representation learning in diverse real-world data conditions. We conduct experiments on traditional LTSSL benchmarks, including CIFAR-LT and ImageNet127, and observe that LoFT achieves competitive performance. Furthermore, LoFT achieves superior performance in the more challenging open-world scenarios, outperforming previous methods even when using only 1% of the unlabeled data compared with previous works, highlighting its strong discriminative capability."
        },
        {
            "title": "Related work",
            "content": "semi-supervised Long-tailed learning Long-Tailed Semi-Supervised Learning (Peng et al. 2023; Hou and Jia 2025; Wei et al. 2021) (LTSSL) aims to improve the performance of models trained on long-tailed labeled data by leveraging additional unlabeled data. The basic idea is to generate pseudolabels for the unlabeled samples and incorporate them into the training process. CReST (Wei et al. 2021) observes that models trained under imbalanced distributions can still generate high-precision pseudolabels for tail classes. Based on this insight, it proposes the class-rebalancing self-training framework to improve performance. In (Wei and Gan 2023), the authors relax the assumption of consistent class distributions between labeled and unlabeled data and introduce ACR, method that dynamically refines pseudo-labels by estimating the true class distribution of unlabeled data under unified formulation. ADELLO (Sanchez Aimar et al. 2023) presents FlexDA, dynamic logit adjustment and distillation-based framework that enhances calibration and achieves strong performance in LTSSL settings. Recently, foundation models (Radford et al. 2021), pre-trained on large-scale datasets, have demonstrated strong generalization capabilities across variety of downstream tasks, including those with long-tailed distributions (Shi et al. 2024; Tian et al. 2022; Dong et al. 2022). However, how to effectively leverage foundation models to benefit LTSSL remains an open and underexplored research direction. In this paper, we aim to address this challenge and propose LoFT, novel framework designed to integrate the strengths of foundation models into the LTSSL paradigm. Long-tailed Confidence calibration Confidence calibration aims to align the predicted confidence scores with the true accuracy, which is important for safety measurement, out-of-distribution detection (Liu et al. 2024). Prior studies have shown that modern CNNs tend to be overconfident (Tomani et al. 2021; Guo et al. 2017), particularly under long-tailed distributions (Zhong et al. 2021). MiSLAS (Zhong et al. 2021) addresses this issue by introducing two-stage training pipeline that incorporates three key techniques: mixup (Zhang et al. 2017) pre-training, labelaware smoothing, and batch normalization () shifting. These techniques collectively enhance the models calibration capability. UniMix (Xu, Chai, and Yuan 2021) extends the mixup strategy to imbalanced scenarios by adopting an advanced mixing factor and sampling strategy that favors minority classes, thereby improving calibration performance under long-tailed distributions. Recently, adapting foundation models to imbalanced learning has attracted increasing attention. However, the issue of confidence calibration in this setting remains largely underexplored. As previously (a) ImageNet-LT (b) Places365-LT Figure 2: The reliability diagrams on (a) ImageNet-LT and (b) Places365-LT based on training from scratch and PEFT, respectively. The horizontal axis represents confidence, and the vertical axis represents accuracy. discussed, well-calibrated model is crucial for generating high quality pseudo-labels, which are essential for effective semi-supervised learning. In this work, we investigate confidence calibration within the context of LTSSL to further enhance performance under long-tailed distributions. Preliminary Notation Our target is to solve K-way classification problem, where each input sample is associated with label = [K] = {1, . . . , K}, with and denoting the input and output spaces, respectively. In the context of LTSSL, we are given labeled training set DS = {(xi, yi)}NS i=1 and an unlabeled set DU = {xi}NU i=1. The labeled set DS follows long-tailed distribution, i.e., the class priors satisfy PS(Y = 1) = PS(Y = 2) = . . . = PS(Y = K), where PS(Y ) denotes the class distribution in DS. In contrast, we impose no assumptions on the class distribution of the unlabeled set DU ; it may or may not follow the same distribution as DS. For the test set DT = {(xi, yi)}NT i=1, we assume an almost uniform class distribution, i.e., PT (Y = k) 1/K for all [K]. Our target is to learn hypothesis : RK that estimates PS(Y = x) by leveraging both DS and DU , thereby achieving generalization on the balanced test distribution. Zero-shot classification We adopt CLIP (Radford et al. 2021) as the zero-shot classification backbone. CLIP consists of an image encoder ϕI : Rd and text encoder ϕT : Rd, which project visual and textual inputs into shared embedding space. To perform classification, we define set of prompt-based textual queries {tk}K k=1 for each class [K], such as photo of [Class]. These are encoded as class prototypes wk = ϕT (tk). Given an input image , its embedding is obtained as zx = ϕI (x). CLIP computes the cosine similarity between zx and each class prototype: sim(zx, wk) = wk zx wk ."
        },
        {
            "title": "The final prediction is made by applying a softmax over the",
            "content": "similarity scores: fzs(x) = softmax ([sim(zx, w1), . . . , sim(zx, wK)]) . This zero-shot formulation enables CLIP to classify unseen categories without fine-tuning but lacks adaptation to the long-tailed and distribution-shifted data in LTSSL. Out-of-Distribution (OOD) Detection. In the openworld setting of LTSSL, it is essential to detect test samples that fall outside the support of the training distribution DS, referred to as out-of-distribution (OOD) samples. Given the classifier trained on DS, common approach to estimate the confidence of prediction is through the maximum softmax probability (MSP), defined as: MSP(x) = max k[K] efk(x) j=1 efj (x) (cid:80)K , (1) where fk(x) denotes the logit corresponding to class k. lower MSP score typically indicates higher likelihood that is OOD. This simple yet effective criterion enables the model to reject uncertain or unfamiliar inputs that do not align with the long-tailed labeled distribution PS(Y ) or the test distribution PT (Y ). Observation Experimental setup We conduct experiments using ViTB/16 (Dosovitskiy et al. 2020) on three long-tailed benchmarks: CIFAR-100-LT, ImageNet-LT, and Places365-LT, with focus on confidence calibration and OOD detection tasks. To ensure the generalizability of our findings, we adopt both CLIP (Radford et al. 2021) and OpenCLIP () as foundation models, which are pre-trained using different strategies. Prior studies (Shi et al. 2024; Dong et al. 2022) have shown that fine-tuning only small subset of parameters can yield competitive performance. Building on this insight, we employ AdaptFormer (Chen et al. 2022) as our base parameter-efficient fine-tuning method, combined with the Logit Adjustment criterion. The learning rate, number of training epochs, and parameter initialization settings follow those in (Shi et al. 2024). For comparison, we also train model from scratch using CNN-based architectures, following the cRT approach (Kang et al. 2019). Following prior work (Guo et al. 2017), we use Expected Calibration Error (ECE) to evaluate calibration performance. For OOD detection, we report AUC, AP-in, AP-out, and FPR metrics to comprehensively assess models. Confidence calibration As shown in Fig. 2, we visualize the confidenceaccuracy diagram on ImageNet-LT and Places365-LT. Following previous works (Liu et al. 2019), we divide the classes into three groups, Many, Medium, and Few, based on the number of training samples per class. We observe that models trained from scratch tend to exhibit significant overconfidence on the unseen test set, particularly for the tail classes. Specifically, the scratchtrained model yields an ECE of 0.1372 across the entire dataset. Moreover, the tail classes suffer from more pronounced overconfidence compared to head classes. In contrast, models fine-tuned using PEFT demonstrate substantially improved calibration, with tail classes no longer exhibiting such severe overconfidence. We attribute this improvement to the extensive pretraining of foundation models on large-scale data, which reduces model uncertainty and enhances calibration. Additionally, PEFT modifies only small subset of parameters, thereby preserving the generalization capabilities of the foundation models while effectively adapting to the target task. OOD detection As shown in Tab. 1, we fine-tune the foundation models on CIFAR-100-LT and evaluate its performance on variety of OOD datasets, including SVHN (Goodfellow et al. 2013), CIFAR-10 (Krizhevsky, Hinton et al. 2009), Tiny ImageNet (Le and Yang 2015), LSUN (Yu et al. 2015), and Places365 (Zhou et al. 2017). We adopt the MSP as the OOD detection strategy and compare our approach against baseline methods, including OE (Hendrycks, Mazeika, and Dietterich 2018) and OCL (Miao et al. 2024). Across multiple evaluation metrics, the model fine-tuned on OpenCLIP achieves the best overall performance, with an average score of 86.51 across the six datasets. These results demonstrate that our fine-tuned model effectively identifies out-of-distribution samples and generalizes well to diverse OOD scenarios. Remark Previous experiments show that the fine-tuned model is well calibrated and can effectively detect out-ofdistribution samples. These two properties are particularly beneficial for long-tailed semi-supervised learning, where pseudo-labeling confidence and the presence of OOD data pose major challenges. Accurate confidence calibration ensures that the model assigns meaningful probabilities, reducing the risk of incorporating noisy pseudo-labels, especially for tail classes. Meanwhile, robust OOD detection allows the model to filter irrelevant samples from the unlabeled set, preventing harmful supervision signals. Together, these capabilities enhance the reliability and generalization of the model under long-tailed and open-world settings, laying solid foundation for more effective LTSSL learning. OOD Dataset Method AUC AP-in AP-out FPR Texture SVHN CIFAR-10 Tiny ImageNet"
        },
        {
            "title": "LSUN",
            "content": "Place"
        },
        {
            "title": "Average",
            "content": "OE OCL PEFT PEFT OE OCL PEFT PEFT OE OCL PEFT PEFT OE OCL PEFT PEFT OE OCL PEFT PEFT OE OCL PEFT PEFT OE OCL PEFT PEFT 76.01 75.92 87.86 91.32 81.82 78.64 86.62 90.68 62.60 60. 83.97 86.39 68.22 69.56 81.34 83.35 76.81 79.14 78.16 81.29 75.68 77. 84.65 86.04 73.52 73.56 83.77 86.51 85.28 82.99 92.79 94.66 73.25 69. 73.87 81.80 66.16 63.21 84.42 86.95 79.36 79.97 88.30 89.85 85.33 86. 86.32 88.45 60.99 62.80 71.67 74.25 75.06 74.12 82.90 85.99 57.47 66. 80.15 86.22 89.10 86.26 94.26 95.98 57.77 55.71 82.61 85.38 51.82 54. 70.20 72.98 60.94 66.58 65.86 70.49 86.51 88.39 93.00 93.65 67.27 69. 81.01 84.12 87.45 70.01 49.45 38.26 80.98 86.38 47.29 41.00 93.53 94. 61.98 57.38 88.54 85.91 70.03 66.02 83.79 75.07 75.45 69.50 83.55 79. 58.36 55.43 86.30 81.93 60.43 54.60 Table 1: The results on OOD tasks on different datasets. PEFT and PEFT denote the fine-tuned model from CLIP and OpenCLIP, respectively."
        },
        {
            "title": "Method",
            "content": "LoFT In modern LTSSL, models are typically optimized by jointly minimizing supervised classification loss on labeled data, used to learn initial discriminative representations, and regularization loss on unlabeled data, which further refines the learned features and enhances generalization. For the supervised classification loss, we adopt the Logit Adjustment (Menon et al. 2020) as the criterion on the labeled long-tailed dataset. The optimization objective is: (cid:17) (cid:16) yb, (cid:0)W(x)(cid:1)+τ log PS(Y ) Ls = , (2) (cid:88) 1 DS xDS where W() denotes weak augmentation operation (e.g., random crop or horizontal flip), τ is scaling hyperparameter, and PS(Y ) represents the empirical class prior estimated from the labeled dataset. For the regularization loss on unlabeled samples, we follow the basic principle from prior work (Sohn et al. 2020), Figure 3: Illustration of the proposed LoFT-OW. H(p, q) denotes the cross-entropy. where weakly augmented view is used to generate pseudolabels, and strongly augmented view is used to obtain logits for optimization. To better handle uncertain predictions, we partition unlabeled samples into high-confidence and low-confidence subsets based on their Maximum Softmax Probability (MSP), and apply different optimization strategies accordingly. Specifically, we define binary mask Mx to indicate whether an unlabeled sample is considered highconfidence, computed as: (cid:26)1, 0, MSP(x) > cu MSP(x) cu Mx = (3) The optimization objective for unlabeled samples is: Lu = 1 DU (cid:88) xDU λ1Mx H(cid:0)ˆy, (A(x))(cid:1) + λ2(1 Mx) H(cid:0)f (W(x)), (A(x))(cid:1), (4) where ˆy = arg max (W(x)) denotes the hard pseudo-label derived from the weakly augmented view, and A() denotes strong augmentation. λ1 and λ2 are hyperparameters. In Eq. 4, for high-confidence samples (Mx = 1), we apply hard pseudo-labels by assigning the most probable class using the models prediction. For low-confidence samples (Mx = 0), we apply soft pseudo-labels by leveraging the full predicted probability distribution, which provides smoother supervision and better captures prediction uncertainty. We analyze that, as shown in Fig. 2, under our finetuning framework, the models confidence score is strongly correlated with prediction accuracy. Since high-confidence samples are generally more reliable, we apply hard supervision to them, while soft supervision is used for lowconfidence samples to mitigate overfitting and enhance generalization. Furthermore, as discussed previously, our finetuned model exhibits better calibration for tail classes compared to models trained from scratch. Consequently, we do not distinguish between head and tail classes when determining the confidence mask in Eq. 3, e.g., setting different thresholds for head or tail classes, which also reduces the number of required hyper-parameters. Finally, the overall training objective is: = Ls + Lu (5) LoFT-OW (LoFT under Open-World scenarios) Traditional LTSSL methods typically assume that all unlabeled data originates from the same distribution as the labeled dataa condition that rarely holds in real-world scenarios. In practice, unlabeled data are often collected from broad, unconstrained sources such as the web or dynamic field environments, where it is highly likely that substantial portion of samples lie outside the distribution of the predefined labeled classes. These OOD samples, if not properly handled, can degrade model performance by introducing misleading supervision. To address this challenge, we propose an extension of our framework to open-world settings, termed LoFT-OW (LoFT under Open-World scenarios). LoFT-OW is designed to effectively detect and filter out OOD samples during training, thereby mitigating their adverse effects and enhancing performance in long-tailed, semi-supervised learning. As shown in Fig. 3, we adopt two-stage filtering strategy to identify out-of-distribution (OOD) samples. In the first stage, we employ zero-shot filtering mechanism, where the foundation model assigns confidence scores to each unlabeled sample. Only those with confidence exceeding high-confidence threshold tHC are retained, resulting in cleaner and more reliable pseudo-labeled subset, denoted as (cid:101)DU . This filtered dataset is typically smaller in size and can be leveraged for subsequent fine-tuning. Beyond this initial stage, we further exploit the strong OOD detection capability of the fine-tuned model, which has been verified previously. We define the filtering function as follows:"
        },
        {
            "title": "M ood",
            "content": "x = (cid:26)1, 0, MSP(x) > cood MSP(x) cood , (6) where cood is the hyper-parameter to control the filtering strength. Then the optimization object for the unlabeled set under open-world scenarios is: Lu = 1 (cid:101)DU (cid:88) λ1M ood Mx H(cid:0)ˆy, (A(x))(cid:1) (cid:101)DU (1 Mx) H(cid:0)f (W(x)), (A(x))(cid:1), + λ2M ood (7) Method FixMatch +ACR +ACR+BEM +TCBC +CPE +CCL CLIP OpenCLIP PEFT LoFT LoFT-OW PEFT LoFT LoFT-OW γ = γl = γu = 10 γ = γl = γu = 20 γu = 1 (uniform) γu = 1/10 (reversed) CIFAR-100-LT N1 = 50 N1 = 150 M1 = 400 M1 = 300 M1 = 400 M1 = 300 M1 = 400 M1 = 300 MC = 400 MC = 300 N1 = 150 N1 = 150 N1 = 150 N1 = 50 N1 = N1 = 50 45.2 55.7 55.8 - 50. 53.5 75.5 78.8 76.5 78.0 81.8 79.3 56.5 65.6 66. 59.4 59.8 63.5 79.7 81.1 79.9 81.7 83.2 81.6 40. 48.0 48.6 - 43.8 46.8 74.0 75.3 73. 75.3 78.4 75.4 50.7 58.9 59.8 53.9 55. 57.5 78.4 79.3 78.6 81.1 81.2 80.8 45.5 66.0 - - - 59.8 75.5 78.0 76.6 78.0 80.3 78.6 58. 73.4 - 63.2 - 67.9 79.7 81.0 80. 81.7 83.6 82.1 44.2 57.0 - - - 54.4 75.5 77.3 76.4 78.0 79.8 79.7 57.3 67.6 - 59.9 60.8 64.7 79.7 80.6 80.0 81.7 82.3 82.0 Table 2: The results on CIFAR-100-LT with different hyper-parameters of γu and γl."
        },
        {
            "title": "Method",
            "content": "training iterations Accuracy Reversed: M1 M2 MC, i.e., γu = 1/γl. FixMatch +BEM +ACR +ACR+BEM +CCL"
        },
        {
            "title": "OpenCLIP",
            "content": "PEFT LoFT LoFT-OW PEFT LoFT LoFT-OW 250000 250000 250000 250000 250000 10000 10000 10000 10000 10000 10000 42.3 58.2 63.6 63.9 67. 71.7 73.3 73.1 72.5 73.9 74.2 Table 3: The results on ImageNet-127."
        },
        {
            "title": "Experiments",
            "content": "Experimental Setup To validate the efficacy of our method under long-tailed distributions and in open-world semi-supervised learning scenarios, we conduct experiments on two long-tailed benchmarks: CIFAR-100-LT (Cui et al. 2019), ImageNet127 (Wei et al. 2021). For ImageNet-127, we only use 1% of the unlabeled data compared with ACR. For CIFAR-100LT, let Nk denote the number of labeled samples for class k, with N1 N2 NK. The imbalance ratio of the labeled dataset is defined as γl = N1 . Similarly, let Mc NC denote the number of unlabeled samples for class c, and the imbalance ratio of the unlabeled dataset is defined as γu = maxc Mc , without assuming any specific class distriminc Mc bution. We consider three representative settings: Consistent: M1 M2 MC with γu = γl. Uniform: M1 = M2 = = MC, i.e., γu = 1. To simulate the open-world setting, we introduce the COCO (Lin et al. 2014) dataset as OOD source. COCO contains diverse set of object categories that are semantically disjoint from those in the target classification task, making it suitable candidate for evaluating OOD robustness. We mix the COCO dataset with the current unlabeled set to form more realistic and challenging unlabeled pool, which better reflects the distributional uncertainty encountered in openworld scenarios. We set tHC = 0.95 for all datasets. We compare LoFT and LoFT-OW with FixMatch (Sohn et al. 2020), as well as equiped with different methods, ACR (Wei and Gan 2023), ACR+BEM (Zheng et al. 2024), TCBC (Li et al. 2024), CPE (Ma et al. 2024), and CCL (Zhou et al. 2024). All experiments are performed on single NVIDIA A40 GPU. More dataset introduction and hyper-parameter of our method are in the Appendix."
        },
        {
            "title": "Results on LoFT",
            "content": "CIFAR-100-LT As shown in Tab. 2, LoFT consistently outperforms PEFT across all settings on CIFAR-100-LT, using both CLIP and OpenCLIP backbones. With OpenCLIP, LoFT achieves the best results in all cases (up to 83.2%), demonstrating its effectiveness. In terms of imbalance levels, LoFT performs well under all γ values. Performance slightly decreases as γ increases (e.g., from γ = 10 to γ = 20), indicating increased difficulty with more severe imbalance, but LoFT still maintains clear margin over PEFT. Moreover, LoFT remains robust under uniform and reversed unlabeled distributions (γu = 1 and 1/10), further validating its ability to handle various class distributions. ImageNet-127 As shown in Tab. 3, our method outperforms other methods on large-scale long-tailed dataset, demonstrating the strong generalization ability of LoFT. Figure 4: Visualizations of unlabeled samples and their predicted confidence scores on ImageNet-127. Samples with green background are assigned reliable pseudo-labels with high confidence, while the sample with red background is identified as an OOD instance. Figure 5: Ablation studies on hyper-parameter cu. The horizontal axis represents the value of cu, and the vertical axis represents the accuracy. Compared to PEFT, LoFT consistently achieves higher accuracy with both CLIP and OpenCLIP backbones, reaching 73.3% and 73.9%, respectively. These improvements over strong baselines and prior methods (e.g., FixMatch+CCL at 67.8%) highlight LoFTs effectiveness beyond small-scale datasets, confirming its robustness and scalability in realworld, large-scale long-tailed semi-supervised learning scenarios. Moreover, we visualize the unlabeled samples and their prediction scores, as shown in Fig. 4. For samples containing meaningful content within the label space, LoFTOW generates reliable pseudo-labels. In contrast, for uninformative OOD samples, LoFT-OW assigns low confidence scores, facilitating their detection. Results on LoFT-OW As shown in Tab. 2 and Tab. 3, LoFT-OW achieves strong performance on both CIFAR-100-LT and ImageNet-127, with fewer training iterations and less data. While its accuracy on CIFAR-100-LT is slightly lower than LoFT due to the inclusion of OOD unlabeled data, which introduces distributional shifts and may hinder representation learning, LoFT-OW remains competitive across all imbalance settings. Notably, on the larger and more complex ImageNet127 dataset, LoFT-OW outperforms all baselines, including LoFT, demonstrating its superior scalability. This highlights the effectiveness of LoFT-OW in leveraging OOD data when generalizing to more diverse and large-scale benchmarks. Ablation studies We perform two ablation experiments on the CIFAR-100-LT benchmark (N = 50, = 400, imbalance ratio = 10), using CLIP as our foundation model. Figure 6: Ablation studies on hyper-parameter cood. The horizontal axis represents the value of cood, and the vertical axis represents the accuracy. Effect of the hyper-parameter cu The hyper-parameter cu controls the balance between hard and soft pseudo-label assignments. With large value of cu, more unlabeled samples are assigned hard pseudo-labels, encouraging confident and deterministic supervision but potentially introducing noise if the predictions are incorrect. In contrast, smaller value of cu leads to greater proportion of soft pseudolabels, which provides more nuanced guidance by preserving model uncertainty, thereby reducing the risk of reinforcing incorrect predictions. As shown in Fig. 5, the test accuracy rises from 74.0 % at cu = 0.2 to maximum of 78.8 % at cu = 0.6, then gradually declines to 75.3 % at cu = 0.95. This behavior indicates that moderate confidence cutoff best balances the benefit of incorporating pseudo-labels with the risk of introducing erroneous predictions. Effect of the hyper-parameter cood The hyper-parameter cood controls the sensitivity of OOD detection among unlabeled samples. larger value of cood enforces stricter filtering, ensuring higher quality among the retained samples but resulting in fewer valid pseudo-labeled instances. Conversely, smaller cood allows more samples to pass the filter, increasing quantity but potentially compromising quality due to the inclusion of OOD data. Fig. 6 shows that accuracy improves from 75.6 % at cood = 0.1 to 76.5 % at cood = 0.6 before falling to 75.2 % at cood = 0.7. These results suggest that moderate OOD cutoff effectively excludes outof-distribution samples without discarding too much valuable unlabeled data. Combined with previous experiments, both cu and cood present the optimal result at the value of 0.6. In the standard LTSSL scenario, cu = 0.6 corresponds to confidence level high enough to regard predictions as reliable pseudo-labels. In the open-world setting, cood = 0.6 similarly acts as boundary above which samples are very likely to be in-distribution, thus improving data filtering. Conclusion In this work, we revisit LTSSL and propose LoFT, parameter-efficient framework built on transformer-based foundation models. LoFT tackles key LTSSL challenges such as overconfidence, poor early pseudo-labels, and tail class inefficiency. Leveraging pre-trained models, it enhances calibration, reduces training overhead, and improves pseudo-label quality. We further extend LTSSL to openworld settings with LoFT-OW, which incorporates OOD detection to filter irrelevant samples. Extensive experiments show that LoFT performs competitively in both standard and open-world LTSSL, offering practical solution for realworld imbalanced learning. References Chen, S.; Ge, C.; Tong, Z.; Wang, J.; Song, Y.; Wang, J.; and Luo, P. 2022. Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural Information Processing Systems, 35: 1666416678. Cui, Y.; Jia, M.; Lin, T.-Y.; Song, Y.; and Belongie, S. 2019. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 92689277. Dong, B.; Zhou, P.; Yan, S.; and Zuo, W. 2022. Lpt: Longtailed prompt tuning for image classification. arXiv preprint arXiv:2210.01033. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. Goodfellow, I. J.; Bulatov, Y.; Ibarz, J.; Arnoud, S.; and Shet, V. 2013. Multi-digit number recognition from street view imagery using deep convolutional neural networks. arXiv preprint arXiv:1312.6082. Guo, C.; Pleiss, G.; Sun, Y.; and Weinberger, K. Q. 2017. On calibration of modern neural networks. In International conference on machine learning, 13211330. PMLR. Hendrycks, D.; and Gimpel, K. 2016. baseline for detecting misclassified and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136. Hendrycks, D.; Mazeika, M.; and Dietterich, T. 2018. Deep anomaly detection with outlier exposure. arXiv preprint arXiv:1812.04606. Hou, Y.; and Jia, Y. 2025. Square Peg in Square Hole: Meta-Expert for Long-Tailed Semi-Supervised Learning. arXiv preprint arXiv:2505.16341. Kang, B.; Xie, S.; Rohrbach, M.; Yan, Z.; Gordo, A.; Feng, J.; and Kalantidis, Y. 2019. Decoupling representation and classifier for long-tailed recognition. arXiv preprint arXiv:1910.09217. Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple layers of features from tiny images. Le, Y.; and Yang, X. 2015. Tiny imagenet visual recognition challenge. CS 231N, 7(7): 3. Li, L.; Tao, B.; Han, L.; Zhan, D.-c.; and Ye, H.-j. 2024. Twice class bias correction for imbalanced semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 1356313571. Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Dollar, P.; and Zitnick, C. L. 2014. Microsoft coco: Common objects in context. In European conference on computer vision, 740755. Springer. Liu, K.; Fu, Z.; Jin, S.; Chen, C.; Chen, Z.; Jiang, R.; Zhou, F.; Chen, Y.; and Ye, J. 2024. Rethinking out-of-distribution detection on imbalanced data distribution. Advances in Neural Information Processing Systems, 37: 109152109176. Liu, Z.; Miao, Z.; Zhan, X.; Wang, J.; Gong, B.; and Yu, S. X. 2019. Large-scale long-tailed recognition in an open world. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 25372546. Ma, C.; Elezi, I.; Deng, J.; Dong, W.; and Xu, C. 2024. Three heads are better than one: Complementary experts for longtailed semi-supervised learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, 14229 14237. Menon, A. K.; Jayasumana, S.; Rawat, A. S.; Jain, H.; Veit, A.; and Kumar, S. 2020. Long-tail learning via logit adjustment. arXiv preprint arXiv:2007.07314. Miao, W.; Pang, G.; Bai, X.; Li, T.; and Zheng, J. 2024. Outof-distribution detection in long-tailed recognition with calIn Proceedings of the AAAI ibrated outlier class learning. Conference on Artificial Intelligence, volume 38, 4216 4224. An Ouali, Y.; Hudelot, C.; and Tami, M. 2020. overview of deep semi-supervised learning. arXiv preprint arXiv:2006.05278. Peng, H.; Pian, W.; Sun, M.; and Li, P. 2023. Dynamic In re-weighting for long-tailed semi-supervised learning. Proceedings of the IEEE/CVF winter conference on applications of computer vision, 64646474. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natIn International conference on ural language supervision. machine learning, 87488763. PMLR. Sanchez Aimar, E.; Jonnarth, A.; Felsberg, M.; and Kuhlmann, M. 2023. Balanced Product of Calibrated ExIn Proceedings of the perts for Long-Tailed Recognition. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1996719977. Shi, J.-X.; Wei, T.; Zhou, Z.; Shao, J.-J.; Han, X.-Y.; and Li, Y.-F. 2024. Long-Tail Learning with Foundation Model: Heavy Fine-Tuning Hurts. In Forty-first International Conference on Machine Learning. Sohn, K.; Berthelot, D.; Carlini, N.; Zhang, Z.; Zhang, H.; Raffel, C. A.; Cubuk, E. D.; Kurakin, A.; and Li, C.-L. 2020. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33: 596608. Tian, C.; Wang, W.; Zhu, X.; Dai, J.; and Qiao, Y. 2022. Vlltr: Learning class-wise visual-linguistic representation for long-tailed visual recognition. In European Conference on Computer Vision, 7391. Springer. Tomani, C.; Gruber, S.; Erdem, M. E.; Cremers, D.; and Buettner, F. 2021. Post-Hoc Uncertainty Calibration for Domain Drift Scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1012410132. Wei, C.; Sohn, K.; Mellina, C.; Yuille, A.; and Yang, F. 2021. Crest: class-rebalancing self-training framework for imbalanced semi-supervised learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1085710866. Wei, T.; and Gan, K. 2023. Towards Realistic Long-Tailed Semi-Supervised Learning: Consistency Is All You Need. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 34693478. Xu, Z.; Chai, Z.; and Yuan, C. 2021. Towards calibrated model for long-tailed visual recognition from prior perspective. Advances in Neural Information Processing Systems, 34: 71397152. Yu, F.; Seff, A.; Zhang, Y.; Song, S.; Funkhouser, T.; and Xiao, J. 2015. Lsun: Construction of large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365. Zhang, H.; Cisse, M.; Dauphin, Y. N.; and Lopez-Paz, D. 2017. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412. Zheng, H.; Zhou, L.; Li, H.; Su, J.; Wei, X.; and Xu, X. 2024. Bem: Balanced and entropy-based mix for long-tailed semi-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2289322903. Improving Zhong, Z.; Cui, J.; Liu, S.; and Jia, J. 2021. In Proceedings of calibration for long-tailed recognition. the IEEE/CVF conference on computer vision and pattern recognition, 1648916498. Zhou, B.; Lapedriza, A.; Khosla, A.; Oliva, A.; and Torralba, A. 2017. Places: 10 million Image Database for Scene IEEE Transactions on Pattern Analysis and Recognition. Machine Intelligence. Zhou, Z.-H.; Fang, S.; Zhou, Z.-J.; Wei, T.; Wan, Y.; and Zhang, M.-L. 2024. Continuous contrastive learning for long-tailed semi-supervised recognition. Advances in Neural Information Processing Systems, 37: 5141151435."
        }
    ],
    "affiliations": [
        "Renmin University of China"
    ]
}