{
    "paper_title": "Why Do Transformers Fail to Forecast Time Series In-Context?",
    "authors": [
        "Yufa Zhou",
        "Yixiao Wang",
        "Surbhi Goel",
        "Anru R. Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Time series forecasting (TSF) remains a challenging and largely unsolved problem in machine learning, despite significant recent efforts leveraging Large Language Models (LLMs), which predominantly rely on Transformer architectures. Empirical evidence consistently shows that even powerful Transformers often fail to outperform much simpler models, e.g., linear models, on TSF tasks; however, a rigorous theoretical understanding of this phenomenon remains limited. In this paper, we provide a theoretical analysis of Transformers' limitations for TSF through the lens of In-Context Learning (ICL) theory. Specifically, under AR($p$) data, we establish that: (1) Linear Self-Attention (LSA) models $\\textit{cannot}$ achieve lower expected MSE than classical linear models for in-context forecasting; (2) as the context length approaches to infinity, LSA asymptotically recovers the optimal linear predictor; and (3) under Chain-of-Thought (CoT) style inference, predictions collapse to the mean exponentially. We empirically validate these findings through carefully designed experiments. Our theory not only sheds light on several previously underexplored phenomena but also offers practical insights for designing more effective forecasting architectures. We hope our work encourages the broader research community to revisit the fundamental theoretical limitations of TSF and to critically evaluate the direct application of increasingly sophisticated architectures without deeper scrutiny."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 6 7 7 9 0 . 0 1 5 2 : r Why Do Transformers Fail to Forecast Time Series In-Context? Yufa Zhou1*, Yixiao Wang1*, Surbhi Goel2, Anru R. Zhang1 1Duke University, 2University of Pennsylvania {yufa.zhou,yixiao.wang,anru.zhang}@duke.edu, surbhig@cis.upenn.edu Abstract Time series forecasting (TSF) remains challenging and largely unsolved problem in machine learning, despite significant recent efforts leveraging Large Language Models (LLMs), which predominantly rely on Transformer architectures. Empirical evidence consistently shows that even powerful Transformers often fail to outperform much simpler models, e.g., linear models, on TSF tasks; however, rigorous theoretical understanding of this phenomenon remains limited. In this paper, we provide theoretical analysis of Transformers limitations for TSF through the lens of In-Context Learning (ICL) theory. Specifically, under AR(p) data, we establish that: (1) Linear Self-Attention (LSA) models cannot achieve lower expected MSE than classical linear models for in-context forecasting; (2) as the context length approaches to infinity, LSA asymptotically recovers the optimal linear predictor; and (3) under Chain-of-Thought (CoT) style inference, predictions collapse to the mean exponentially. We empirically validate these findings through carefully designed experiments1. Our theory not only sheds light on several previously underexplored phenomena but also offers practical insights for designing more effective forecasting architectures. We hope our work encourages the broader research community to revisit the fundamental theoretical limitations of TSF and to critically evaluate the direct application of increasingly sophisticated architectures without deeper scrutiny. The only thing we know about the future is that it will be different. Peter Drucker"
        },
        {
            "title": "1 Introduction",
            "content": "Time series forecasting (TSF), fundamental and longstanding challenge in machine learning, involves predicting future observations based on historical data [BD02, BJRL15, DGH06, Ham20]. TSF has broad applicability across diverse fields such as electronic health records, traffic analysis, energy consumption, and financial market predictions [MJK15, MMM23]. In contrast, Transformers [VSP+17] have emerged as cornerstone architecture in modern deep learning, achieving groundbreaking success across wide array of sequence modeling tasks, including language modeling [HLG+24, JKL+24, GDJ+24, YLY+25, GYZ+25], computer vision [DBK+21, PX23, MGA+24], visual-language modeling [LLWL23, JXX+24], and video modeling [DPD+25, JSX+24]. Encouraged by their remarkable performance in language modeling, substantial efforts have been dedicated to adapting Transformers and Large Language Models (LLMs) to TSF [ZZP+21, LYL+22, WXWL21, ZMW+22, GFQW23, CJA+24, PJG+24, JWM+24, JZC+24]. Nevertheless, empirical evidence consistently reveals that Transformer-based models frequently underperform compared to simpler, linear *Equal Contribution. 1Code: https://github.com/MasterZhou1/ICL-Time-Series 1 forecasting methods, despite their quadratic time complexity and significantly larger parameter counts [ZCZX23, TMG+24, ERC+24, LCT25]. Such findings have prompted the development of lightweight linear models and frequency-domain approaches that typically outperform Transformers on long-horizon forecasting tasks [XZX24, LQLX23, YLL+25, WWS+24, ERC+24]. However, comprehensive theoretical understanding of why Transformers exhibit such limitations remains scarce. Existing theoretical studies on Transformer for TSF mainly relied on Neural Tangent Kernel analyses or generic In-Context Learning (ICL) theory, yielding theoretical bounds often disconnected from practical relevance and failing to provide clear representational insights [KLS+25, CLZZ25, SGS+24, LIPO23, WHC+25]. In sharp contrast, our work uniquely addresses the core representational limitations of Transformers through rigorous theoretical examination grounded explicitly in classical AutoRegressive (AR) models, fundamental frameworks dominating traditional TSF [Ham20]. By adopting Linear Self-Attention (LSA), simplified yet powerful abstraction that eliminates the Softmax function for analytical tractability [ACDS23, MHM24, ZFB24, ACS+24, YWS+24], we uncover novel and essential constraints inherent in the attention mechanism itself. Despite AR models inherent linearity rendering linear methods optimal, assessing the representational gap between optimal linear models and Transformers is highly non-trivial endeavor. Under minimal assumptionsspecifically, only assuming data adheres to stable AR(p) processwe establish substantial theoretical results that clearly delineate the representational boundaries of Transformers. related setting was studied in [CLZZ25], which analyzes LSA on one-dimensional linear dynamical system (a special case of AR(2)), while we generalize to AR(p) under minimal stability assumptions, significantly increasing difficulty and scope. Our findings indicate that even optimally parameterized LSA Transformers cannot outperform classical linear predictors in terms of expected MSE. With infinitely long historical context their predictions can theoretically converge to those of linear regression if training is sufficiently good, yet even this convergence arises not from any structural advantage of LSA but from the inherent stability of time series, which collapses their representational space to that of linear models on AR processes. In contrast, for any finite context length there exists provable strictly positive gap, which diminishes at rate no faster than 1/n as the context length grows. We further analyze how predictions evolve and how errors accumulate under iterative Chain-of-Thought (CoT) inference. Our theoretical analyses are complemented by empirical validations, providing practical insights into Transformer architectural design and clarifying their fundamental limitations in TSF contexts. Ultimately, our work calls for reconsideration of the suitability and effectiveness of naively applying complex Transformer-based architectures to TSF. We advocate for deeper theoretical exploration to systematically unravel the foundational differences driving Transformers divergent performance across domains, bridging the gap between representational capability and practical efficacy. Our primary contributions are summarized as follows: We show that linear self-attention is essentially restricted/compressed representation of linear regression, so it cannot outperform linear predictors (Section 3.1). We establish strictly positive performance gap between LSA and linear predictor, and proves that this gap vanishes at rate no faster than 1/n as context length increases (Section 3.2). We characterize prediction behavior and error compounding rate in Chain-of-Thought inference, highlighting fundamental limitations of iterative Transformer predictions (Section 3.3). We empirically corroborate our theoretical findings, offering insights into architectural implications and emphasizing the inherent representational limitations of Transformers for TSF (Section 4)."
        },
        {
            "title": "1.1 Related Work",
            "content": "Negative Results of Transformers on TSF. Empirical studies consistently find Transformerand LLMbased models struggle to surpass simpler linear baselines for time series forecasting. Language modeling components add minimal value [TMG+24], and linear variants (NLinear, DLinear) outperform Transformers on long-horizon benchmarks [ZCZX23]. Analyses further question the utility of self-attention [KPLK24], show limited gains from scaling model size [LCT25], and motivate lightweight methods such as FITS [XZX24], RLinear [LQLX23], OLinear [YLL+25], TimeMixer [WWS+24], and CNN-based TSLANet [ERC+24]. Beyond forecasting, Transformers also struggle with zero-shot temporal reasoning [MTG+24] and anomaly detection tasks [ZY25]. Existing theoretical explanations remain limited. Kernel-based analyses attribute Transformer failures to asymmetric feature learning within Neural Tangent Kernel regime [JGH18], but their synthetic assumptions and lack of universal lower bounds restrict their practical applicability [KLS+25]. Similarly, recent In-Context Learning theory for linear dynamical systems provides data-dependent lower bound largely reflecting intrinsic noise rather than representational shortcomings [CLZZ25]. In contrast, our work studies general AR(p) processes, employs distinct analytic techniques, and explicitly identifies representational constraints of Transformers relative to classical linear forecasters. Due to the space constraint, we leave more related work to Appendix A."
        },
        {
            "title": "2 Preliminaries",
            "content": "Notations. We write [n] := {1, 2, . . . , n}. For (cid:82)d , let = (a1, . . . , ad ] (cid:82)mn, xi (cid:82)m is the i-th column; i,: and :, denote the i-th row and j-th column; a:b,: and :,c:d denote row/column submatrices. 1d , 0d and 1dd , 0dd are all-ones/all-zeros vectors and matrices; Id is the d-identity. denotes certain norm for vector or matrix. For symmetric A, (cid:82)dd , (A B) iff is positive semidefinite (definite). For sequence {at means at increases monotonically to a. For (cid:82)mn and (cid:82)pq, the Kronecker product (cid:82)mpnq satisfies (A B)(i1)p+k, ( j1)q+ℓ := Ai, Bk,ℓ (i [m], [n], [p], ℓ [q]). For (cid:82)pp symmetric, vech(X ) (cid:82)p(p+1)/2 stacks the lower triangle (including diagonal); for (cid:82)mn, vec(X ) (cid:82)mn stacks columns. ). For = [x1, . . . , xn }, at"
        },
        {
            "title": "2.1 Time Series",
            "content": "We begin by formally defining the notion of time series considered in this paper. Definition 2.1 (Time Series). time series is finite sequence of random variables {x discrete time {1, . . . , }. We write x1:T := (x1, . . . , multivariate if each (cid:82)d with > 1, and univariate if = 1. }T t=1, indexed by ) for the full sequence. The process is called In this work, we primarily focus on univariate Auto-Regressive processes, particularly the AR(p) model, cornerstone of classical time series analysis [Ham20, BJRL15]. Definition 2.2 (AR(p) Process [Ham20]). real-valued stochastic process {xi sive model of order p, denoted AR(p), if there exist coefficients ρ (0, σ2 ϵ ) such that for all > 0, 1, . . . , ρ }T i=1 follows an autoregresi.i.d. (cid:82) and white noise ϵ xi+1 = (cid:88) j=1 ρ xi j+1 + ϵ i+1, 3 1z ρ with fixed initial values {xp+1, . . . , x0 all roots outside the unit circle, i.e. > 1, to ensure weak stationarity, the process satisfies: (1) (cid:69)[xi (2) (cid:69)[x 2 }. Assuming the characteristic polynomial 1 ρ 0, and (3) (cid:69)[xi xn+1 n+1i, where γ := (cid:69)[xi xi+k ] and rk := γ ] = γ ] = γ /γ 0. pz has ] = 0, Further classical resultsincluding the ordinary least squares (OLS) solution for AR models, as well as the formulation and properties of linear predictorsare deferred to Appendix D."
        },
        {
            "title": "2.2 Transformer Architecture",
            "content": "For theoretical tractability, we adopt the Linear Self-Attention (LSA), which omits Softmax and has been widely used in prior theoretical works [VONR+23, ACDS23, ZFB24, MHM24, VVOSG24, GSR+24, GYW+25, SJA25, ZSLS25], with growing empirical interest [KVPF20, SIS21, ACS+24, DG24, YWS+24]. Definition 2.3 (Linear Self-Attention (LSA)). Let (cid:82)(d+1)(m+1) be the input matrix and define the (cid:82)(m+1)(m+1). We denote the attention weights P, (cid:82)(d+1)(d+1). Then the causal mask := (cid:21) (cid:20)Im 0 0 0 linear self-attention output is defined as LSA(H) := + 1 M (H QH) (cid:82)(d+1)(m+1) . Throughout this paper, we focus on LSA-only Transformers. Definition 2.4 (L-Layer LSA-Only Transformer). Let LSA1, . . . , LSAL be sequence of linear selfattention layers as defined in Definition 2.3. The L-layer Transformer is defined recursively via function composition: TF(H) := LSAL LSAL1 LSA1 (H) (cid:82)(d+1)(m+1) ."
        },
        {
            "title": "2.3 In-Context Time Series Forecasting",
            "content": "Given univariate sequence x1:n of AR order (Definition 2.2), we build Hankel matrix Hn (cid:82)(p+1)(np+1) (Definition 2.5) whose final column is zero-padded in its last entry as label slot for xn+1. Setting = and = p, we feed Hn into the L-layer LSA-only Transformer TF (Definition 2.4) and read the forecast directly from the label slot: (cid:98)xn+1 := [TF(Hn )](p+1, np+1) (cid:82). The Hankel construction (Definition 2.5) encodes the autoregressive structure in context: identifying an AR(p) process requires at least lags, and each column of the Hankel matrix provides exactly this information. Unlike raw token sequences, the Hankel layout already fixes the relative order of observations, so it implicitly carries position encoding. This both respects the time-series autoregressive dependencies and avoids the need for additional positional embeddings that can make Transformers harder to train. The formal justification is given in Appendix E.3. Definition 2.5 (Hankel Matrix). For (x1, . . . , xn ) (cid:82)n and n, define Hn := x1 x2 ... p+1 x2 x3 ... p+1 p+2 xnp xnp+1 ... xn1 xn xnp+1 xnp+2 ... xn 0 (cid:82)(p+1)(np+1) , where each column is sliding window of length p+1, with the last zero marking the prediction."
        },
        {
            "title": "3 Main Results",
            "content": "We organize our theoretical contributions into three parts. First, in Section 3.1 we provide high-level feature-space perspective: By Hankelizing the input and analyzing the induced σ-algebra, we show that one-layer linear self-attention (LSA) effectively compresses history into restricted cubic feature class which asymptotically collapses to the last lags, thereby anticipating structural disadvantage relative to linear regression (LR). Building on this intuition, Section 3.2 establishes our core result: for autoregressive (AR) and more generally linear stationary processes, the optimal one-layer LSA predictor suffers strict finite-sample excess risk over LR, quantified by positive Schurcomplement gap that vanishes only asymptotically at an explicit 1/n rate. While stacking additional LSA layers yields monotone improvements, LR remains the fundamental benchmark that cannot be surpassed. Finally, Section 3.3 turns to multistep forecasting: we prove that chain-of-thought (CoT) rollout, in stark contrast to its benefits in language tasks, compounds errors exponentially and collapses forecasts to the mean, with LSA uniformly dominated by LR at every horizon. All formal proofs are deferred to Appendices E, F, and H. In contrast to prior work that mainly studies LR in ICL settings [ACDS23, ZFB24], time series settings introduce intrinsic temporal dependencies among input variables, making the analysis substantially more complex and non-trivial."
        },
        {
            "title": "3.1 Feature-Space View",
            "content": "Restricted feature class. We first reparameterize P, in Definition 2.3 to A, to obtain the simplified form of the LSA prediction (Lemma E.3). Let Φ = Φ(Hn; A, b) denote the one-layer LSA features induced by querykey weighting and value aggregation. The predictor admits cubic lifting: Lemma 3.1 (Cubic lifting for one-layer LSA). There exist coefficients {β } such that j,r,k (cid:98)x LSA n+1 (A, b) = (cid:88) β ϕ(p) j,r,k (x1:n ), j,r,k ϕ(p) j,r,k are degree-3 monomials in {x }. j,r,k Hence one-layer LSA is linear functional over cubic feature space (p) LSA. Proof deferred to Appendix E. Lemma 3.1 shows that the LSA readout lives in cubic feature space of the raw inputs. By the L2-projection property of conditional expectation (orthogonality onto sub-σalgebras), we obtain Proposition 3.2: any predictor operating on Φ cannot achieve lower MSE than the optimal predictor operating on the full context x1:n. In short, the attention-derived representation is informationally coarser than the raw context; adding architectural complexity does not reveal additional predictive signal beyond the last lags, but only reweights existing information. Proposition 3.2 (Information monotonicity). σ(Φ) σ(x1:n Jensen, ). Hence, by conditional orthogonality/- (cid:69) (cid:2)(xn+1 (x1:n ))2(cid:3) inf (cid:69) (cid:2)(xn+ g(Φ))2(cid:3) . inf Proof deferred to Appendix E. Furthermore, Proposition 3.3 formalizes the asymptotic picture: as the number of observed contexts , the (p+1)-row Hankel design ensures access to exactly the relevant lags; by ergodicity, empirical Hankel Gram blocks converge to their Toeplitz limits, cross-row correlations stabilize, and the cubic coordinates concentrate onto the last-p-lag subspace. Consequently, one-layer LSA cannot outperform LR and can at best match it asymptotically (Proposition E.14). See Appendix for an optimal LSA parameter choice achieving this limit. 5 Proposition 3.3 (Asymptotic collapse of LSA features). As for stable AR(p), the coordinates ϕ(p) }. Thus the optimal one-layer LSA readout j,r,k asymptotically reduces to linear function of the last lags. ) converge in L2 to scaled copies of {xnp+1, . . . , xn (x1:n Takeaway 1: Attention is Not All You Need for TSF Feature-Space. LSA operates on strictly coarser σ-algebra than the raw context; it can at best reweight the last-p lags and cannot unlock signal beyond them. For time series with pronounced linear structure (e.g., AR/ARMA), this is fundamental representational limitation. This aligns with empirical findings that simple linear models often outperform Transformers in TSF [ZCZX23, KPLK24, TMG+24, LZS25]."
        },
        {
            "title": "3.2 A Strict Finite-Sample Gap (Core Result)",
            "content": "In this section we rigorously establish and quantitatively characterize the fundamental representational limitation of one-layer LSA. Our main technical device is Kronecker-product lifting of the Hankelderived features: the one-layer LSA readout can be written as (cid:98)x LSA n+1 = η (cid:101) Z, := (cid:0) vech G(cid:1) x, := xnp+1:n (cid:82)p, ) is the Hankel Gram matrix, vech() denotes half-vectorization, and (cid:101) where = G(x1:n η is an affine reparameterization of (A, b) (see Appendix F.2). This lifts the cubic dependence of LSA into an ordinary linear regression in the lifted space. Let (cid:82)qp with := dim(vech G). We further define := (cid:69)[x ], and the induced Schur complement is (cid:101)S := (cid:69)[Z ], (cid:101)r := (cid:69)[Z ], Γ (cid:101)r. Intuitively, captures the component of the linear signal in the last lags that remains orthogonal to the span of lifted LSA featuresnamely, the exact representation gap characterized in Theorem 3.4. Moreover, we prove in Theorem 3.4 that 0 for any finite n, establishing strict finite-sample + o(1/n) under Gaussianity in Theogap. We then derive an explicit first-order expansion rem 3.5, and show in Theorems H.3 and H.5 and Lemma H.4 that both the strictness and the 1/n rate persist for general linear stationary processes, with the leading constant adjusted by cumulant spectra (Appendix H). := Γ (cid:101)S1 (cid:101)r Bp = 1 p Theorem 3.4 (Strict finite-sample gap: AR(p)). For any and stable AR(p), min A,b (cid:69) (cid:2)( (cid:98)x LSA n+1 xn+1 )2(cid:3) min (cid:69) (cid:2)(w xnp+1:n xn+1 )2(cid:3) + ρ ρ, 0. What this says. Even after optimizing over all one-layer LSA parameters, the best-in-class LSA risk is strictly larger than the best-in-class linear risk by the explicit quadratic form ρ ρ; the gap is structural (positive definite), not an estimation or optimization artifact. Proof Sketch. (i) Since the lifted loss is strictly convex quadratic in (cid:101) η = (cid:101)S1 η L( (cid:101) (cid:101) linear predictor attains the Bayes one-step risk σ2 follows from block positive definiteness of the joint covariance (cid:101)r, ρ, yielding the class optimum min η) = σ2 (cid:101) ϵ , so the excess risk equals ρ η, it has the unique minimizer ρ. (ii) Under AR(p), the optimal ρ. (iii) Strictness ϵ + ρ (cid:19)(cid:153) (cid:69) (cid:150)(cid:18)Z (cid:19) (cid:18)Z = (cid:19) (cid:18) (cid:101)S (cid:101)r (cid:101)r Γ 0. Specifically, we reduce the claim to showing that the (non-lifted) joint covariance of (vech G, x) is positive definite. We prove this via an innovation-based elimination from the newest to older indices, establishing that no nontrivial linear combination can have zero variance. Using the Schur-complement 0. All blocks admit closed forms as HankelToeplitz moments up to orders 4 property then yields and 6 (from earlier definitions of (cid:101)S, (cid:101)r in this section). Hence, is computable from process moments; in the Gaussian case these moments follow from Isserlis theorem. warm-up AR(1) calculation is given in Appendix F.1. See Appendix F.2 for the complete proof. First-order gap rate under Gaussianity. We now quantify the finite-n excess risk in Theorem 3.4. For zero-mean stationary Gaussian AR(p), we expand the lifted moments around their population (rankone) limit and evaluate the Schur complement via singular block inverse. Theorem 3.5 (Explicit 1/n rate: Gaussian). For Gaussian AR(p), = Γ (cid:101)S (cid:101)r 1 (cid:101)rn = 1 Bp + o(1/n), Bp 0 (generically Bp 0). Consequently, for any fixed ρ > 0 there exists cr > 0 such that min A,b (cid:69)[( (cid:98)x LSA n+1 xn+1 )2] min (cid:69)[(w xnp+1:n xn+1 )2] + cr . Proof Sketch. The argument proceeds in two steps: (1) Moment expansions. We first expand the lifted covariance quantities using standard tools (Isserlis theorem for Gaussian moments and Toeplitz summation for time averages). This shows that the main term has simple block-diagonal structure determined entirely by the process autocovariances, while the finite-sample corrections appear at order 1/n. (2) Singular block inversion. Because part of the leading block vanishes in the limit, the inverse matrix develops component that scales linearly with n. Applying first-order block-inverse expansion reveals that the excess-risk matrix itself scales like 1/n, with computable correction term depending on the second-order moment structure of the process. In short, the proof shows that after separating the dominant block structure and carefully controlling has an explicit 1/n-expansion governed entirely by process moments. the inverse, the residual term complete proof is provided in Appendix F.3. Remark 3.6 (Why the rate is 1/n). Let = vech(Γ is rank-one along u. Finite introduces Θ(1/n) perturbations that regularize the orthogonal directions, so the Schur complement Γ (cid:101)rn is Θ(1/n). The overlap of Hankel windows is the source of these firstorder terms. ). At the population limit (cid:101)S = (uu) Γ (cid:101)r (cid:101)S1 p+1 ϵ ψ k0 = (cid:80) tk, (cid:80) Beyond Gaussianity: strict gap and 1/n rate. We work under linear stationarity with Wold represen- ] = 0, possesstation ing finite fourth and sixth moments. Replacing Isserlis theorem by the momentcumulant formula preserves positive definiteness of the joint covariance, so the strictly positive gap still holds (Theorem H.3); furthermore, the convergence rate remains 1/n via the non-Gaussian expansions (Lemma H.4) culminating in the rate result (Theorem H.5). < , and i.i.d. symmetric innovations {ϵ } with (cid:69)[ϵ ψ t Multi-layer LSA yields monotone improvement. Because the stacked Kronecker feature set enlarges with depth, the optimal LSA risk is monotone nonincreasing in by projection isotonicity (Proposition F.18, using the MoorePenrose formulation Equations (12) and (13) and the residual-covariance view Lemma F.17). Moreover, if the (L+1)-st layer contributes any L2-nonredundant directionequivalently 7 Var(cid:0)PH plement strictly decreases (Proposition F.18), yielding (g (L) x)(cid:1) > 0 for HL = span{g (0) x, . . . , (L1) x}then the MoorePenrose Schur commin {b(ℓ),A(ℓ)}L ℓ=0 (cid:69)(cid:2)( (L+1) (cid:98)x n+1 xn+1 )2(cid:3) < min {b(ℓ),A(ℓ)}L1 ℓ=0 (cid:69)(cid:2)( (L) (cid:98)x n+1 xn+1 )2(cid:3). Remark 3.7. potential misinterpretation is that we claim Transformers cannot solve or fit time series at all. This is NOT the case. Rather, our result shows that Transformers cannot outperform simple linear models beyond certain extent. While they may be capable of solving time series tasks to some degree, their performance does not substantially exceed that of linear models. Takeaway 2: Strictly Positive Gap between LSA and Linear Predictor 0 (Theorem 3.4). Strictness. For any finite n, one-layer LSA has strict excess risk over p-lag LR equal to ρ Rate. The gap admits an explicit 1/n expansion with PSD leading constant Bp (generically PD) (Theorem 3.5). Robustness. Under linear stationarity with finite moments, strictness and the 1/n rate persist; nonGaussianity only alters the constant via cumulants (Theorems H.3 and H.5 and Lemmas H.4 and H.6). Depth. Stacking layers enlarges the feature span, so risk is monotone nonincreasing in L; generically it strictly improves when the new layer adds nonredundant direction (Proposition F.18), yet the LR baseline remains unbeatable at finite (Equations (12) and (13)). ρ with n"
        },
        {
            "title": "3.3 Chain-of-Thought Rollout: Multi-Step Collapse",
            "content": "In this section, we show that under CoT-style inference, LSA collapses to the mean with an error rate that grows exponentially. We first define CoT in Definition 3.8. Definition 3.8 (Chain-of-Thought (CoT) Inference). Given time series (x1, . . . , xn p, initialize the Hankel matrix Hn Let TF be the L-layer LSA-based Transformer in Definition 2.4. For each step = 1, 2, . . . , : ) and context length (cid:82)(p+1)(np+1) as in Definition 2.5 with the last column zero-padded. )](p+1, np+t). 1. Predict the next value: (cid:98)xn+t := [TF(Hn+t1 2. Overwrite the zero in the last column of Hn+t1 with (cid:98)xn+t . 3. Append the column (xnp+t+1, . . . , xn, (cid:98)xn+1, . . . , (cid:98)xn+t Repeating yields CoT rollouts (cid:98)xn+1, . . . , (cid:98)xn+T by feeding model outputs back into the Hankel input. ) to form Hn+t with last entry set to 0. Theorem 3.9 (Collapse and error compounding for CoT). Under AR(p), the Bayes h-step forecast equals = A(w) the noise-free recursive rollout of the one-step Bayes predictor. Any stable linear CoT recursion (cid:98)st+1 (cid:98)st collapses exponentially to 0. For Bayes = ρ, MSE (h) = (cid:69)[(xn+h (cid:98)x n+h )2] = σ2 ϵ h1 (cid:88) k=0 ψ2 Var(x ), Var(x ) MSE (h) 2σ2 ϵ 1β 2 β 2h. Thus, even for the optimal predictor, CoT error compounds to the unconditional variance at an exponential rate governed by the spectrum of A(ρ). Here β < 1 and > 0 are constants depending only on the AR(p) process. 8 Proof deferred to Appendix G. Because conditional expectation is the L2 projection, for any measurable = g(x1:n ) (including any L-layer LSA CoT rollout) one has with strictness unless finite-sample gap at = 1 (Theorem 3.4), equality fails generically for all h. g)2] = MSE (cid:69)[(xn+h (cid:98)x n+h a.s. (Equations (16) and (17)). Since one-layer LSA already has strict g)2] MSE (h) + (cid:69)[( (cid:98)x n+h (h), )2] τ Var(x Corollary 3.10 (Earlier failure of LSA CoT). Define the failure horizon Hτ(g) := inf{h 1 : (cid:69)[(xn+h (cid:98)x ) for all τ (0, 1), with strict inequality on set of τ of gh positive measure. In words: LSA CoT reaches the large-error regime no later than (and generically earlier than) Bayes linear regression. (cid:98)x LSA) Hτ( )}. Then Hτ( ) MSE(h), so whenever the Proof deferred to Appendix G. Quantitatively, Var(x gap to variance remains positive, LSAs CoT error approaches Var(x ) at least exponentially fast; if it overshoots (the left side becomes negative), Corollary 3.10 still guarantees earlier threshold crossing. For AR(1), closed forms make the compounding explicit: MSE(h) = σ2(1 ρ2h) with half-life h1/2 = log(1/2)/ log(ρ2). ) MSELSA(h) Var(x Takeaway 3: CoT Collapse in TSF TSF. CoT rollout forms stable linear dynamical system that collapses to the mean and whose error compounds exponentially to Var(x ); Bayes/LR is horizonwise optimal and LSA CoT is uniformly dominated at each horizon (Proposition G.1, Lemma G.2, Theorem G.3, Equation (17), and Corollary G.4). Contrast. Notably, CoT behaves very differently in TSF compared to other domains: in language tasks, test-time scaling shows longer inference chains improve problem solving [LLZM24, SLXK25], and CoT can help in in-context linear regression [HWL25]; in stark contrast, in TSF, CoT leads to rapidly compounding forecast errors."
        },
        {
            "title": "4 Numerical Verification",
            "content": "We defer experimental details, including datasets, model configurations, and the Softmax attention vs. LSA comparison, to Appendix C."
        },
        {
            "title": "4.1 Evaluation",
            "content": "To comprehensively assess model performance in TSF, we employ two complementary inference modes for evaluation and visualization: Teacher-Forcing (TF) TSF: This method evaluates the model under idealized conditions by providing ground-truth historical values as inputs at each time step. It is commonly used to measure predictive accuracy and to visualize the models capacity to fit the true data distribution. Chain-of-Thought (CoT) TSF: This iterative inference approach simulates real-world deployment It enables the evaluation of by using the models own past predictions as inputs for future steps. long-horizon stability and the extent of error accumulation during rollout. 9 } denote ground-truth test time series and { Evaluation Metrics. Let {x1, x2, . . . , (cid:98)x1, (cid:98)x2, . . . , (cid:98)x the corresponding model predictions. We evaluate forecasting accuracy using the Mean Squared Error (MSE): MSE := 1 )2. In rollouts where predictions are generated via TF or CoT, we further (cid:98)x compute the cumulative MSE up to step k: CMSE(k) := 1 )2, [T ]. This metric reflects how prediction errors accumulate over time as inference unfolds, providing smoother depiction of trajectory-level performance when pointwise errors are noisy or scattered [PLSH23, WCS+23]. (x (x (cid:80)T (cid:80)k (cid:98)x t=1 t=1 }"
        },
        {
            "title": "4.2 Experiments",
            "content": "(a) TF Values (c) TF Cumulative MSE (e) Context Scaling (b) CoT Values (d) CoT Cumulative MSE (f) LSA Layer Scaling Figure 1: Experimental results. (ab) Predictions under Teacher-Forcing (TF) and Chain-of-Thought (CoT). (cd) Cumulative MSE for TF and CoT rollouts. (ef) Scaling experiments varying the history length and the number of LSA layers. Overall, LSA tracks AR(p) but never surpasses the OLS baseline, confirming its representational limits. For all experiments, we adopt common set of hyperparameters. Synthetic AR series are generated with Gaussian noise of zero mean and standard deviation σϵ = 0.05 and total length 50,000, split into training/validation/test with proportions 0.70/0.15/0.15. Models are trained on 10 RTX 2080 GPUs for up to 100 epochs, using batch size of 512 and the Adam optimizer with learning rate 103. Teacher Forcing vs. Chain-of-Thought. We compare 50-step predictions under Teacher-Forcing (TF) and Chain-of-Thought (CoT) rollout (Section 4.1) using one-layer LSA with = 8 on AR(5). Figures 1(ab,cd) show trajectories and cumulative MSEs. Under TF, both LSA and OLS track the AR(5) process, but OLS consistently yields lower MSE, indicating LSA fits yet never exceeds the linear baseline. Under CoT rollout, errors accumulate: both methods collapse to the mean, with LSA failing earlier, consistent with Section 3.3. Context and Layers Scaling. We vary history length and LSA layers with {3, 5, 7}, averaged over 7 seeds with error bars indicating the standard error of the mean (SEM). For context scaling, = + {5, 25, 50, 100, 200} with one LSA layer; for layer scaling, {1, . . . , 5} with = 100. Figures 1(e f) show Teacher-Forcing MSE: larger improves LSA but never closes the gap to OLS, and more layers give diminishing gains saturating at the OLS level, consistent with Section 3.2."
        },
        {
            "title": "5 Discussion",
            "content": "Architectural Considerations. Beyond LSA, several components may influence Transformer performance. The Softmax in standard attention may enhance expressivity by exponentiating inputs, effectively expanding the representation space and approximating an infinite series to capture richer dependencies (see experimental results in Appendix C). Given the limitations of single-head LSA, simply aggregating multiple heads over the same data source is unlikely to improve expressivity; however, allocating different heads to distinct modalities may offer benefits through data fusion. Additionally, the role of feedforward MLP layers deserves closer scrutiny. Although not the focus of our analysis, prior work [ZCZX23, TMG+24, KPLK24, LZS25] suggests that MLPs play as key contributors in time series taskspotentially explaining the performance of LLMs in TSF. We leave these directions to future work. Difference Between Language and Time Series. Attention serves as learned compression mechanism, essential in language modeling where meaning depends on long-range, abstract dependencies [DRD+24, Sut23, GFRW24, HZSH24]. In contrast, time series with low-order dynamics (e.g., AR(p)) hinge on local, position-specific patterns, where such compression can obscure predictive signals. Because attention applies fixed contextual weights, it often fails to capture these direct dependencies, explaining the locality-agnostic behavior noted in [LJX+19]. When compression is misaligned with the data-generating process, classical models typically outperform attention."
        },
        {
            "title": "6 Conclusion",
            "content": "We study the limits of Transformers in time-series forecasting via in-context learning theory. For autoregressive (AR) processes, we prove that Linear Self-Attention (LSA) cannot beat the optimal linear predictor, yielding strictly positive gap in expectation. Our analysis further shows that although LSA asymptotically recovers the linear predictor under teacher-forcing, errors compound under Chain-ofThought rollout, ultimately causing collapse-to-mean behavior. Experiments across teacher-forcing, CoT, and scaling corroborate our theory: LSA matches but never exceeds the linear baseline. These findings clarify the inherent limits of attention in time-series forecasting and highlight the need for architectures beyond self-attention to capture temporal structure."
        },
        {
            "title": "Acknowledgment",
            "content": "SG gratefully acknowledges support from OpenAIs Superalignment Grant. ARZ was partially supported by NSF Grant CAREER-2203741."
        },
        {
            "title": "References",
            "content": "[ACDS23] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. Advances in Neural Information Processing Systems, 36:4561445650, 2023. [ACS+24] Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit Sra. Linear attention is (maybe) all you need (to understand transformer optimization). In The Twelfth International Conference on Learning Representations, 2024. 11 [ASA+23] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. In The Eleventh International Conference on Learning Representations, 2023. [AST+24] Abdul Fatir Ansari, Lorenzo Stella, Ali Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor, Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, and Bernie Wang. Chronos: Learning the language of time series. Transactions on Machine Learning Research, 2024. Expert Certification. [BD02] Peter Brockwell and Richard Davis. Introduction to time series and forecasting. Springer, 2002. [BJL+24] Yuxuan Bian, Xuan Ju, Jiangtong Li, Zhijian Xu, Dawei Cheng, and Qiang Xu. Multi-patch prediction: Adapting language models for time series representation learning. In Forty-first International Conference on Machine Learning, 2024. [BJRL15] George EP Box, Gwilym Jenkins, Gregory Reinsel, and Greta Ljung. Time series analysis: forecasting and control. John Wiley & Sons, 2015. [Bri01] David Brillinger. Time series: data analysis and theory. SIAM, 2001. [CJA+24] Defu Cao, Furong Jia, Sercan Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. TEMPO: Prompt-based generative pre-trained transformer for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024. [CLZZ25] Frank Cole, Yulong Lu, Tianhao Zhang, and Yuxuan Zhao. In-context learning of linear dynamical systems with transformers: Error bounds and depth-separation. arXiv preprint arXiv:2502.08136, 2025. [CPW24] Lijie Chen, Binghui Peng, and Hongxun Wu. Theoretical limitations of multi-layer transformer. arXiv preprint arXiv:2412.02975, 2024. [DBK+21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. [DCL21] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure attention loses rank doubly exponentially with depth. In International conference on machine learning, pages 27932803. PMLR, 2021. [DG24] Tri Dao and Albert Gu. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. In International Conference on Machine Learning (ICML), 2024. [DGH06] Jan De Gooijer and Rob Hyndman. 25 years of time series forecasting. International journal of forecasting, 22(3):443473, 2006. [DKSZ24] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. decoder-only foundation model for time-series forecasting. In Forty-first International Conference on Machine Learning, 2024. 12 [DLS+23] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36:7029370332, 2023. [DPD+25] Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, and Xinlong Wang. Autoregressive video generation without vector quantization. In The Thirteenth International Conference on Learning Representations, 2025. [DRD+24] Grégoire Delétang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent In ICLR, Orseau, Marcus Hutter, and Joel Veness. Language modeling is compression. 2024. [ERC+24] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu, and Xiaoli Li. Tslanet: Rethinking transformers for time series representation learning. In International Conference on Machine Learning, pages 1240912428. PMLR, 2024. [FWX+24] Xinyao Fan, Yueying Wu, Chang Xu, Yuhao Huang, Weiqing Liu, and Jiang Bian. MG-TSD: Multi-granularity time series diffusion models with guided learning process. In The Twelfth International Conference on Learning Representations, 2024. [FZG+23] Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, and Liwei Wang. Towards revealing the mystery behind chain of thought: theoretical perspective. Advances in Neural Information Processing Systems, 36:7075770798, 2023. [FZS22] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. [GD24] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. First Conference on Language Modeling, 2024. [GDJ+24] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [GFQW23] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Wilson. Large language models are zero-shot time series forecasters. Advances in Neural Information Processing Systems, 36:1962219635, 2023. [GFRW24] Micah Goldblum, Marc Finzi, Keefer Rowan, and Andrew Gordon Wilson. The no free lunch theorem, kolmogorov complexity, and the role of inductive biases in machine learning. In Proceedings of the 41st International Conference on Machine Learning, 2024. [GSC+24] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur In International Dubrawski. Moment: family of open time-series foundation models. Conference on Machine Learning, pages 1611516152. PMLR, 2024. [GSR+24] Khashayar Gatmiry, Nikunj Saunshi, Sashank J. Reddi, Stefanie Jegelka, and Sanjiv Kumar. Can looped transformers learn to implement multi-step gradient descent for in-context learning? In Forty-first International Conference on Machine Learning, 2024. 13 [GTLV22] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? case study of simple function classes. Advances in Neural Information Processing Systems, 35:3058330598, 2022. [GYW+25] Angeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos, and Jason Lee. How In The 28th International well can transformers emulate in-context newtons method? Conference on Artificial Intelligence and Statistics, 2025. [GYZ+25] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [Hah20] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156171, 2020. [Ham20] James Hamilton. Time series analysis. Princeton university press, 2020. [HCL24] Yu Huang, Yuan Cheng, and Yingbin Liang. In-context convergence of transformers. In Forty-first International Conference on Machine Learning, 2024. [HJ94] Roger Horn and Charles Johnson. Topics in matrix analysis. Cambridge university press, 1994. [HJA20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [HLG+24] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [HLJ24] Jiachen Hu, Qinghua Liu, and Chi Jin. On limitation of transformer for learning hmms. arXiv preprint arXiv:2406.04089, 2024. [HPCY25] Jianliang He, Xintian Pan, Siyu Chen, and Zhuoran Yang. In-context linear regression demystified: Training dynamics and mechanistic interpretability of multi-head softmax attention. In Forty-second International Conference on Machine Learning, 2025. [HWL25] Jianhao Huang, Zixuan Wang, and Jason Lee. Transformers learn to implement multistep gradient descent with chain of thought. In The Thirteenth International Conference on Learning Representations, 2025. [HWW+24] Yang Hu, Xiao Wang, Lirong Wu, Huatian Zhang, Stan Li, Sheng Wang, and Tianlong Chen. Fm-ts: Flow matching for time series generation. arXiv preprint arXiv:2411.07506, 2024. [HZSH24] Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He. Compression represents intelligence linearly. In First Conference on Language Modeling, 2024. [Iss18] Leon Isserlis. On formula for the product-moment coefficient of any order of normal frequency distribution in any number of variables. Biometrika, 12(1/2):134139, 1918. [JGH18] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. Advances in neural information processing systems, 31, 2018. 14 [JKL+24] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [JSX+24] Yang Jin, Zhicheng Sun, Kun Xu, Liwei Chen, Hao Jiang, Quzhe Huang, Chengru Song, Yuliang Liu, Di Zhang, Yang Song, Kun Gai, and Yadong Mu. Video-lavit: Unified videolanguage pre-training with decoupled visual-motional tokenization. In International Conference on Machine Learning, pages 2218522209, 2024. [JWM+24] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-LLM: Time series forecasting by reprogramming large language models. In International Conference on Learning Representations (ICLR), 2024. [JXX+24] Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Yadong Mu, et al. Unified language-vision pretraining in llm with dynamic discrete visual tokenization. In International Conference on Learning Representations, 2024. [JZC+24] Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang, Shirui Pan, and Qingsong Wen. Position: What can large language models tell us about time series analysis. In Forty-first International Conference on Machine Learning, 2024. [Kal06] O. Kallenberg. Foundations of Modern Probability. Probability and Its Applications. Springer New York, 2006. [KLS+25] Yekun Ke, Yingyu Liang, Zhenmei Shi, Zhao Song, and Chiwun Yang. Curse of attention: kernel-based perspective for why transformers fail to generalize on time series forecasting and beyond. Second Conference on Parsimony and Learning (CPAL 2025), 2025. [KPLK24] Kim, Park, Lee, and Kim. Are self-attentions effective for time series forecasting? In 38th Conference on Neural Information Processing Systems (NeurIPS 2024), 2024. [KVPF20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pages 51565165. PMLR, 2020. [LCBH+23] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. The Eleventh International Conference on Learning Representations, 2023. [LCT25] Zeyan Li, Libing Chen, and Yin Tang. Does scaling law apply in time series forecasting? arXiv preprint arXiv:2505.10172, 2025. [LHZ+24] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long. itransformer: Inverted transformers are effective for time series forecasting. In The Twelfth International Conference on Learning Representations, 2024. [LIPO23] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. In InTransformers as algorithms: Generalization and stability in in-context learning. ternational Conference on Machine Learning, pages 1956519594. PMLR, 2023. 15 [LJX+19] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting. Advances in neural information processing systems, 32, 2019. [LKZ+25] Haoxin Liu, Harshavardhan Kamarthi, Zhiyuan Zhao, Shangqing Xu, Shiyu Wang, Qingsong Wen, Tom Hartvigsen, Fei Wang, and Aditya Prakash. How can time series analysis benefit from multiple modalities? survey and outlook. arXiv preprint arXiv:2503.11835, 2025. [LLWL23] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [LLZM24] Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu Ma. Chain of thought empowers transIn The Twelfth International Conference on formers to solve inherently serial problems. Learning Representations, 2024. [LQH+24] Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Autotimes: Autoregressive time series forecasters via large language models. Advances in Neural Information Processing Systems, 37:122154122184, 2024. [LQLX23] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An investigation on linear mapping. ArXiv, abs/2305.10721, 2023. [LQS+25] Yong Liu, Guo Qin, Zhiyuan Shi, Zhi Chen, Caiyin Yang, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Sundial: family of highly capable time series foundation models. arXiv preprint arXiv:2502.00816, 2025. [LSY25] Jiecheng Lu, Yan Sun, and Shihao Yang. In-context time series predictor. In The Thirteenth International Conference on Learning Representations, 2025. [LWN+24] Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, and Qingsong Wen. Foundation models for time series analysis: tutorial and survey. In In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, page 65556565, 2024. [LYL+22] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X. Liu, and Schahram Dustdar. Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting. In International Conference on Learning Representations, 2022. [LYLH24] Jingwei Liu, Ling Yang, Hongyan Li, and Shenda Hong. Retrieval-augmented diffusion models for time series forecasting. Advances in Neural Information Processing Systems, 37:27662786, 2024. [LZS25] Zida Liang, Jiayi Zhu, and Weiqiang Sun. Why attention fails: The degeneration of transformers into mlps in time series forecasting. arXiv preprint arXiv:2509.20942, 2025. [MBS+24] Sean McLeish, Arpit Bansal, Alex Stein, Neel Jain, John Kirchenbauer, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, Jonas Geiping, Avi Schwarzschild, et al. Transformers can do arithmetic with the right embeddings. Advances in Neural Information Processing Systems, 37:108012108041, 2024. 16 [MGA+24] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. [MHM24] Arvind V. Mahankali, Tatsunori Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. In The Twelfth International Conference on Learning Representations, 2024. [MJK15] Douglas Montgomery, Cheryl Jennings, and Murat Kulahci. Introduction to time series analysis and forecasting. John Wiley & Sons, 2015. [MMM23] Ricardo Masini, Marcelo Medeiros, and Eduardo Mendes. Machine learning advances for time series forecasting. Journal of economic surveys, 37(1):76111, 2023. [MS23] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of logTransactions of the Association for Computational Linguistics, precision transformers. 11:531545, 2023. [MTG+24] Mike Merrill, Mingtian Tan, Vinayak Gupta, Thomas Hartvigsen, and Tim Althoff. Language models still struggle to zero-shot reason about time series. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 35123533, 2024. [NNSK23] Yuqi Nie, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. time series is worth 64 words: Long-term forecasting with transformers. In The Eleventh International Conference on Learning Representations, 2023. [PJG+24] Zijie Pan, Yushan Jiang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, and Dongjin Song. s2 ip-llm: Semantic space informed prompt learning with llm for time series forecasting. In Forty-first International Conference on Machine Learning, 2024. [PLSH23] Quang Pham, Chenghao Liu, Doyen Sahoo, and Steven Hoi. Learning fast and slow for online time series forecasting. In The Eleventh International Conference on Learning Representations, 2023. [PNP24] Binghui Peng, Srini Narayanan, and Christos Papadimitriou. On limitations of the transformer architecture. In First Conference on Language Modeling, 2024. [PX23] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [Rud21] Walter Rudin. Principles of mathematical analysis. 2021. [SAL+24] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [SGS+24] Michaël Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, and Gabriel Peyré. How In Proceedings of the 41st do transformers perform in-context autoregressive learning? International Conference on Machine Learning, pages 4323543254, 2024. 17 [SHT23] Clayton Sanford, Daniel Hsu, and Matus Telgarsky. Representational strengths and limitations of transformers. Advances in Neural Information Processing Systems, 36:3667736707, 2023. [SIS21] Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International conference on machine learning, pages 93559366. PMLR, 2021. [SJA25] Haoyuan Sun, Ali Jadbabaie, and Navid Azizan. In-context learning of polynomial kernel regression in transformers with glu layers. arXiv preprint arXiv:2501.18187, 2025. [SLXK25] Charlie Victor Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. [Sut23] Ilya Sutskever. An observation on generalization. Workshop on Large Language Models and Transformers, Simons Institute, 2023. https://www.youtube.com/watch?v=AKMuA_ TVz3A&ab_channel=SimonsInstitute. [TMG+24] Mingtian Tan, Mike Merrill, Vinayak Gupta, Tim Althoff, and Tom Hartvigsen. Are language models actually useful for time series forecasting? Advances in Neural Information Processing Systems, 37:6016260191, 2024. [TSSE21] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. Csdi: Conditional scorebased diffusion models for probabilistic time series imputation. Advances in neural information processing systems, 34:2480424816, 2021. [VONR+23] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, João Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pages 3515135174. PMLR, 2023. [VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [VVOSG24] Max Vladymyrov, Johannes Von Oswald, Mark Sandler, and Rong Ge. Linear transformers are versatile in-context learners. Advances in Neural Information Processing Systems, 2024. [WCS+23] Qingsong Wen, Weiqi Chen, Liang Sun, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan, et al. Onenet: Enhancing time series forecasting models under concept drift by online ensembling. Advances in Neural Information Processing Systems, 36:6994969980, 2023. [WHC+25] Dennis Wu, Yihan He, Yuan Cao, Jianqing Fan, and Han Liu. Transformers and their roles as time series foundation models. arXiv preprint arXiv:2502.03383, 2025. [WKF+25] Zihan Wang, Fanheng Kong, Shi Feng, Ming Wang, Xiaocui Yang, Han Zhao, Daling Is mamba effective for time series forecasting? NeurocomputWang, and Yifei Zhang. ing, 619:129178, 2025. [WLK+24] Gerald Woo, Chenghao Liu, Akshat Kumar, Caiming Xiong, Silvio Savarese, and Doyen Sahoo. Unified training of universal time series forecasting transformers. In International Conference on Machine Learning, pages 5314053164. PMLR, 2024. [WWS+24] Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Zhang, and JUN ZHOU. Timemixer: Decomposable multiscale mixing for time series forecasting. In International Conference on Learning Representations (ICLR), 2024. [WXWL21] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting. Advances in neural information processing systems, 34:2241922430, 2021. [WZZ+23] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Transformers in time series: survey. In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, pages 67786786, 2023. [XZX24] Zhijian Xu, Ailing Zeng, and Qiang Xu. FITS: Modeling time series with $10k$ parameters. In The Twelfth International Conference on Learning Representations, 2024. [YLL+25] Wenzhen Yue, Yong Liu, Haoxuan Li, Hao Wang, Xianghua Ying, Ruohao Guo, Bowei Xing, and Ji Shi. Olinear: linear model for time series forecasting in orthogonally transformed domain. arXiv preprint arXiv:2505.08550, 2025. [YLY+25] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [YQ24] Xinyu Yuan and Yan Qiao. Diffusion-ts: Interpretable diffusion for general time series generation. In The Twelfth International Conference on Learning Representations, 2024. [YWS+24] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear In International Conference on attention transformers with hardware-efficient training. Machine Learning, pages 5650156523. PMLR, 2024. [ZCGS24] Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh Gupta, and Jingbo Shang. Large language In Proceedings of the Thirty-Third International Joint models for time series: survey. Conference on Artificial Intelligence, pages 83358343, 2024. [ZCZX23] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series forecasting? In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 1112111128, 2023. [ZFB24] Ruiqi Zhang, Spencer Frei, and Peter Bartlett. Trained transformers learn linear models in-context. Journal of Machine Learning Research, 25(49):155, 2024. [ZMW+22] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In International conference on machine learning, pages 2726827286. PMLR, 2022. [ZNS+23] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series analysis by pretrained lm. Advances in neural information processing systems, 36:43322 43355, 2023. [ZSLS25] Yedi Zhang, Aaditya Singh, Peter Latham, and Andrew Saxe. Training dynamics of in-context learning in linear attention. Proceedings of the 42nd International Conference on Machine Learning, 2025. 19 [ZY25] Zihao Zhou and Rose Yu. Can LLMs understand time series anomalies? In The Thirteenth International Conference on Learning Representations, 2025. [ZZP+21] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 11106 11115, 2021."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Preliminaries 2.1 Time Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Transformer Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 In-Context Time Series Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Main Results 3.1 Feature-Space View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Strict Finite-Sample Gap (Core Result) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Chain-of-Thought Rollout: Multi-Step Collapse . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Numerical Verification 4.1 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Discussion 6 Conclusion More Related Work A.1 Transformers for Time Series Forecasting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 In-Context Learning Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Representational Limitations of Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . More Discussion Experimental Details C.1 Dataset and Model Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Softmax Attention vs. LSA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Time Series Fundamentals Transformers Expressivity for TSF E.1 Fundamentals of Linear Self-Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Function Space Constraints of Linear Self-Attention . . . . . . . . . . . . . . . . . . . . . . . E.3 Nested Transformer Feature Spaces and Risk Monotonicity in Time Series Prediction . . Closed-Form Gap Characterization between LSA and Linear Models F.1 Warm Up via AR(1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 finite-sample optimality gap for one-layer LSA . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Order of the finite-sample gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.4 finitesample gap for stacked LSA layers (with monotone improvement) . . . . . . . Chain-of-Thought (CoT) Rollout in TSF: Collapse-to-Mean and Error Compounding DeGaussifying the Gap: Linear Stationary Processes 21 1 3 3 4 4 5 5 6 8 9 9 10 11 11 22 22 22 23 24 24 24 25 26 26 28 29 31 31 34 38"
        },
        {
            "title": "Appendix",
            "content": "Roadmap. In Appendix A, we review related work on Transformers for time series forecasting, incontext learning theory, and representational limitations of attention. Appendix provides further discussion on our perspective of TSF. In Appendix C, we include more experiment details. Appendix reviews classical results on time series. Appendix analyzes the expressivity of LSA Transformers for TSF. Appendix establishes the finite-sample gap between LSA and linear models, with detailed proofs. Appendix extends our analysis to Chain-of-Thought rollout, characterizing collapse-to-mean and error compounding. Finally, Appendix relaxes Gaussian assumptions and generalizes our results to linear stationary processes."
        },
        {
            "title": "A More Related Work",
            "content": "A.1 Transformers for Time Series Forecasting Early adaptations of Transformers for time series forecasting primarily modified attention mechanisms to capture long-term dependencies efficiently. Informer introduced ProbSparse attention to mitigate quadratic complexity [ZZP+21], while Pyraformer employed hierarchical pyramidal attention for multiscale modeling [LYL+22]. Autoformer and FEDformer further integrated domain-specific inductive biases, utilizing auto-correlation and frequency decomposition, respectively, to model seasonal-trend components explicitly [WXWL21, ZMW+22]. Additional variants include locality-enhanced attention [LJX+19], inverted architectures [LHZ+24], and tokenization-based representations treating sequences as textual patches [NNSK23]. comprehensive overview is presented in [WZZ+23]. More recent studies leverage pretrained Large Language Models (LLMs) to transfer NLP-style capabilities to forecasting tasks. Zero-shot forecasting was initially demonstrated using pretrained LLMs without task-specific tuning [GFQW23]. Subsequent works explored specialized prompt-based strategies [CJA+24, PJG+24], reprogramming pretrained LLMs directly [JWM+24], and unified, datasetagnostic training paradigms [WLK+24, LSY25]. Broader frameworks proposed foundational-model perspectives for time series tasks [GSC+24], discrete vocabulary tokenization [AST+24], multi-patch prediction [BJL+24], and generalized decoder-only architectures [DKSZ24, ZNS+23, LQH+24]. Recent surveys systematically summarize these emerging paradigms and highlight open challenges and opportunities in deploying LLMs for time series analysis [ZCGS24, LWN+24, LKZ+25, JZC+24]. A.2 In-Context Learning Theory Recent work theoretically interprets in-context learning (ICL) as Transformer forward pass implicitly performing variants of gradient descent (GD). Early studies empirically demonstrated Transformers can closely approximate ordinary-least-squares predictors [GTLV22], while subsequent constructive analyses showed one linear self-attention (LSA) layer corresponds exactly to one GD step, with the global training objective implementing preconditioned, Bayes-optimal GD step [VONR+23, ASA+23, ACDS23, MHM24]. Further training-dynamics analyses establish gradient flow convergence of LSA to learn the class of linear models [ZFB24], provide finite-time convergence guarantees and parameter evolution for multi-head Softmax attention [HCL24, HPCY25], and identify phase transitions revealing when linear-attention mimics full Transformer behaviors [ACS+24, ZSLS25]. Extensions include multi-step GD via chain-of-thought prompting [HWL25] and kernelized polynomial regression through gated linear units [SJA25]. Other works establish positive approximation guarantees for ICL in dynamical and autoregressive settings but lack universal lower bounds or explicit representational constraints [LIPO23, SGS+24, WHC+25, CLZZ25]. A.3 Representational Limitations of Transformers Despite their success, Transformers exhibit fundamental limitations in expressivity. Pure self-attention without MLPs suffers doubly exponential rank collapse with depth, severely constraining representational capacity [DCL21]. Self-attention cannot model periodic finite-state languages unless the depth or number of heads scales with input length [Hah20]. Complexity analyses show that log-precision Transformers are no more powerful than TC0 circuits, implying provable failure on linear systems and context-free languages under standard complexity separations [MS23]. Communication-complexity arguments further reveal that Transformers cannot compose functions over sufficiently large input domains [PNP24], and their performance is task-dependent, achieving logarithmic complexity in input size for sparse averaging tasks, but requiring linear complexity for triple-detection [SHT23]. [CPW24] establish unconditional depthwidth trade-offs, proving that solving sequential L-step function composition tasks over input of tokens requires either Ω(L) layers or nΩ(1) hidden dimensions. Empirically, Transformers struggle with compositional generalization [DLS+23] and fail to outperform RNNs in modeling Hidden Markov dynamics [HLJ24]. While chain-of-thought prompting and positional embeddings can recover arithmetic and step-wise reasoning [FZG+23, MBS+24], these function as external aids, underscoring the architectures inherent limitations."
        },
        {
            "title": "B More Discussion",
            "content": "Other Data Distributions. Although our analysis focuses on AR processes, similar considerations apply more broadly. The difficulty for attention arises from its misalignment in capturing input dependencies, as seen in AR(p). We further discuss Moving Average (MA) processes and, more generally, ARMA models in Appendix H. Multivariate Time Series. In the case of uncorrelated multivariate time series, each dimension evolves independently, reducing training to separate LSA models. This eliminates the opportunity to exploit cross-variable dependencies and limits the potential of learning shared structure. Consequently, pretraining on collection of uncorrelated time series may fail to produce useful shared representations. For the correlated case, one could impose structural assumptions on inter-variable dependencies to enable tractable analysis. We leave the investigation of such multivariate models, such as Vector AutoRegression (VAR) [Ham20], to future work. Optimization Dynamics. While our analysis primarily addresses representational limitations of attention mechanisms, future work could explore optimization dynamics and training difficulties of Transformers for time series forecasting. Understanding these issues might yield complementary insights into observed empirical shortcomings. Real-World Complexities. Real-world forecasting tasks include many complexities not modeled in this study, such as data randomness, intricate temporal dependencies, training instability, noisy signals, and external factors like market sentiment [LWN+24, LKZ+25]. Exploring these practical challenges could help bridge theoretical findings with real-world performance. Architectural and Framework Varieties. Our framework intentionally abstracts away practical components such as Rotary Position Embeddings (RoPE) [SAL+24] and Mixture-of-Experts (MoE) [FZS22]. Future work may assess whether these enhancements alleviate the representational limitations we identify. State Space Models like Mamba [GD24, DG24] also present promising alternative for time series forecasting [WKF+25]. Beyond architectural changes, generative modeling paradigms such as diffusion models [HJA20] and flow matching [LCBH+23] offer alternative approaches for time series [TSSE21, YQ24, HWW+24, LYLH24, FWX+24, LQS+25], potentially overcoming the limitations of Transformer-based Next-Token Prediction objectives."
        },
        {
            "title": "C Experimental Details",
            "content": "This section provides additional details for Section 4. C.1 Dataset and Model Configuration Synthetic data. We generate stable AR(p) processes (Definition 2.2) by sampling coefficient vectors (roots outside the unit circle), adding Gaussian noise, discarding short burn-in, and retaining long sequence. Each sequence is split into train/validation/test segments. From long sequences to training examples. We fix history length > p. For each series x1:T , sliding window of length with stride 1 defines training pairs with input history tn+1:t and target (cid:82)(p+1)(np+1) (Definition 2.5), which t+1. Each history is transformed into Hankel matrix (t) serves as the input to the LSA model. Models. Our main model is an L-layer LSA-only Transformer TF (Definition 2.4) with feature dimension = p. We read prediction from the label slot: (cid:98)x t+1 (p+1,np+1). As baseline, we fit classical AR(p) predictor by OLS on the same training series used for LSA. = (cid:2)TF(H (t) )(cid:3) Training. All windows are shuffled and batched. Models are trained with teacher forcing using MSE loss and Adam. We sweep p, n, and L, while keeping the noise level and optimization hyperparameters fixed. Performance is reported on the held-out test split. C.2 Softmax Attention vs. LSA Though theoretically more challenging, we could examine the experimental behavioral comparison between Softmax attention and LSA. We define Softmax attention as follows: Definition C.1 (Softmax Attention). Let (cid:82)(d+1)(m+1) be the input matrix and define the causal mask := (cid:21) (cid:20)Im 0 0 0 (cid:82)(m+1)(m+1) . We denote the reparameterized weights P, (cid:82)(d+1)(d+1). Then the (masked) softmax attention output is defined as Attn(H) := Softmax(cid:0)H QH(cid:1), where the Softmax() is applied column-wise to ensure attention weights are normalized. Thus Attn(H) (cid:82)(d+1)(m+1). We compare 1-layer LSA and softmax attention models under identical settings (Adam optimizer and hyperparameters as in Section 4), differing only in architecture. Empirically, Softmax Attention performs slightly better than LSA (Figure 2), which is unsurprising given the greater expressivity underlying the strong performance of Transformers. However, the analytical complexity of the Softmax operator makes 24 (a) TF Values (c) TF Cumulative MSE (b) CoT Values (d) CoT Cumulative MSE Figure 2: Experimental results on comparison of LSA and Softmax Attention. (ab) Predictions under Teacher-Forcing (TF) and Chain-of-Thought (CoT). (cd) Cumulative MSE for TF and CoT rollouts. Overall, both LSA and Softmax Attention tracks AR(p) but never surpass the OLS baseline. Moreover, Softmax Attention is slightly better than LSA. theoretical understanding more challenging, and we leave deeper investigation of this gap to future work."
        },
        {
            "title": "D Time Series Fundamentals",
            "content": "We first recall classical results for stationary autoregressive (AR) processes, which form the theoretical backbone for our later analysis. These results connect population-level covariances to model coefficients, show how consistent estimates can be obtained from finite data, and establish natural linear performance baseline. Definition D.1 (YuleWalker Equations [Ham20]). Let {xi Definition 2.2. Define the Toeplitz autocovariance matrix Γ }T i=1 follow stationary AR(p) process as in (cid:82)pp as Γ := γ 0 γ 1 ... p1 γ γ 1 γ 0 ... p2 γ , γ γ ... p1 p2 ... γ 0 γ := γ 1 γ 2 ... γ . Then the AR coefficient vector ρ := (ρ 1, . . . , ρ ) satisfies the YuleWalker system: Γ ρ = γ. 25 This moment-matching condition links the autocovariance structure to the autoregressive coefficients. Theorem D.2 (OLS Consistency for AR(p) Estimation [Ham20]). Let {xi ρ process as in Definition 2.2. Let (cid:98) }T i=1 be generated by an AR(p) := (X )1X be the ordinary least squares estimator obtained from ... := ) , . x1 x2 ... xnp p1 ... xn2 p+1 ... xn1 := (x p+1, . . . , xn Then, as , a.s. ρ. ρ (cid:98) Hence, in the large-sample limit, OLS recovers the Bayes-optimal linear predictor in mean squared error. Theorem D.3 (Linear Baseline under AR(p) Dynamics). Let {xi in Definition 2.2. Fix context window x1:n have }T i=1 be generated by an AR(p) process as = x1:n we (cid:82)n with p. For any linear predictor (cid:98)x LR n+ (cid:69) (cid:2)(w x1:n xn+1 )2(cid:3) < Var(xn+1 ), min w(cid:82)n where the expectation is over the stationary joint distribution of (x1, . . . , xn+1 Proof. By Definition 2.2, (cid:69)[xi linear) predictor is ). ] = 0 and the noise variance is σ2. The Bayes-optimal (and hence optimal (cid:98)x LR n+1 = (cid:69)[xn+ x1:n ] = ρ xn j+1. (cid:88) j= Its mean squared error is (cid:69) (cid:148)(cid:0) (cid:98)x LR n+1 xn+1 (cid:1)2(cid:151) = σ2 < σ2 + Var(xn ) = Var(xn+ ), which is strictly below the variance baseline."
        },
        {
            "title": "E Transformers Expressivity for TSF",
            "content": "E.1 Fundamentals of Linear Self-Attention We work with Hankelized inputs for time-series forecasting. For context length and total length p, define the Hankel matrix = (cid:2) (1) (2) . . . (np+1)(cid:3) (cid:82)(p+1)(np+1) , where each (i) (cid:82)p+1 stacks length-p window and (p + 1)-st label slot; we reserve the last column (np+1) as the prediction token (see Definition 2.5 for construction details). Definition E.1 (Linear Self-Attention (single layer)). Let P, (cid:82)(p+1)(p+1) be trainable. Define := QH (cid:82)(np+1)(np+1) , := diag(cid:0)I np, 0(cid:1) (cid:82)(np+1)(np+1) . The one-layer LSA update is LSA(H) := + 1 P (cid:0)H S(cid:1) (cid:82)(p+1)(np+1) , (1) where masks the prediction token to avoid self-use during the update. 26 The multi-layer variant is defined by composing residual bilinear updates. Definition E.2 (L-layer LSA Transformer). For parameters {Pℓ, Qℓ}L ℓ=1, define each layer as LSAℓ(H) := + 1 (cid:0)H (H QℓH)(cid:1), Pℓ and the overall L-layer LSA Transformer as TF(H) := LSAL LSA1 (H). We now formalize the readout at the prediction slot for single LSA layer, which will later serve as the basic building block in our theoritical analysis. Lemma E.3 (Closed-form readout of one-layer LSA on Hankel input [ACDS23]). Following the construction in Definition 2.5 and Definition E.1, the final column of is the prediction token, Suppose (np+1) = (cid:21) (cid:20)x np+1:n , np+1:n (cid:82)p. = (cid:21) (cid:20)0p(p+1) , = (cid:2) 0(p+1)1 (cid:3), with (cid:82)p+1 and (cid:82)(p+1)p. Then the prediction-slot entry satisfies (cid:2)LSA(H)(cid:3) p+1, np+ = Gn np+1:n, where we define the empirical Gram matrix Gn := 1 np (cid:88) i=1 (i) (i) . Proof. Let = QH. Since the last column of is zero, = [ (1) . . . (np) 0 ]. Because [H] p+1, np+1 = 0 and p+1P = b, (cid:2)LSA(H)(cid:3) p+1, np+ = 1 (H S)e np+1 = 1 np (cid:88) j= ( j) j, np+1. For p, j, np+1 = ( j) Qx (np+1) = ( j) np+1:n, which gives the stated expression. 27 E.2 Function Space Constraints of Linear Self-Attention In this subsection we take function-space perspective on linear self-attention. Rather than analyzing specific computations, we characterize the class of functions that one-layer LSA readout can realize on Hankelized inputs. This higher-level view makes explicit that LSA predictions are confined to restricted polynomial feature space, and thusdespite the nontrivial attention mechanismcannot achieve fundamentally lower risk than classical linear regression on AR(p) processes. Lemma E.4 (DoobDynkin [Kal06]). Let : Ω and : Ω be measurable mappings between measurable spaces. Then is σ(g)-measurable if and only if there exists measurable : such that = g. Theorem E.5 (Projection Monotonicity in L2). Let be Hilbert space and M2 subspaces. For any H, letting PMi denote the orthogonal projection onto Mi, we have PM1 Z PM2 Z. M1 be closed Functional form of the one-layer readout. Let = [ (1) Hankel matrix from Definition 2.5. Following Definition E.1, the one-layer LSA update is . . . (np+1) ] (cid:82)(p+1)(np+1) be the LSA(H) = + 1 P(cid:0)H (H QH)(cid:1). Following Definition 2.5, the final column of is the prediction token, (np+1) = (cid:21) (cid:20)x np+1:n 0 , np+1:n (cid:82)p. By Lemma E.3, for suitable trainable P, Q, (cid:2)LSA(H)(cid:3) p+1, np+1 = Gn np+1:n, Gn := 1 np (cid:88) i=1 (i) (i) . Definition E.6 (LSA feature space). For j, [p + 1] and [p], define the cubic coordinates φ(p) j,r,k (x1:n ) := 1 np (cid:88) i=1 i+ j1 i+r1 np+k. Collect them into the feature map Φ(p)(x1:n ) := (cid:0)φ(p) j,r,k (x1:n )(cid:1) j,r[p+1], k[p] (cid:82) m, = (p + 1)2, and define the linear self-attention feature space (cid:166)φ(p) j,r,k (p) LSA := span ( ) : j, [p + 1], [p](cid:169) . Since each coordinate is finite sum of degree-3 monomials, Φ(p) is continuous. Theorem E.7 (Representation of one-layer LSA readout). There exist coefficients {β the trainable parameters in P, Q, such that }, depending on j,r,k = (cid:2)LSA(H)(cid:3) (cid:98)xn+ p+1, np+1 β j,r,k φ(p) j,r,k (x1:n ) (p) LSA. p+1 (cid:88) p+1 (cid:88) (cid:88) = j= r=1 k=1 28 Proof. By Lemma E.3, for suitable trainable P, Q, (cid:2)LSA(H)(cid:3) p+1, np+ = GnAx np+1:n, Gn := 1 np (cid:88) i= (i) (i) . Expanding the product gives GnAxnp+1:n = p+1 (cid:88) p+1 (cid:88) (cid:88) (b jAr,k ) φ(p) j,r,k (x1:n ), where we define β j=1 j,r,k := jAr,k. This matches the claimed representation. k=1 r=1 Theorem E.8 (Risk monotonicity under LSA features). Let (x1:n, xn+1 ables. Under squared loss, ) be jointly defined random vari- (cid:69) (cid:148)(cid:0)xn+ (x1:n )(cid:1)2(cid:151) inf inf (cid:69) (cid:148)(cid:0)xn+1 g(Φ(p)(x1:n ))(cid:1)2(cid:151) . x1:n Equality holds if and only if xn+1 Φ(p)(x1:n Proof. By optimality of conditional expectations, the minimizers are (cid:69)[xn+1 Since Φ(p) is deterministic function of x1:n, DoobDynkin (Lemma E.4) gives σ(Φ(p)(x1:n ). Conditional expectation is the L2 projection, so Theorem E.5 yields the inequality; the equality condition is standard. ] and (cid:69)[xn+1 )) σ(x1:n x1:n ). Φ(p)(x1:n )]. Theorem E.9 (Single-layer LSA cannot beat the linear predictor under AR(p)). Suppose {x stationary AR(p) process with context x1:n and p. Let (cid:98)x LSA Theorem E.7. Then (cid:69) (cid:148)(cid:0)w } follows n+1 denote any one-layer LSA readout as in inf (cid:69) (cid:148)(cid:0) (cid:1)2(cid:151) . xn+1 (cid:98)x LSA n+1 xn+ x1:n n+1 is measurable function of Φ(p)(x1:n (cid:1)2(cid:151) inf w(cid:82)n ), hence its best risk is at least that of ); Theorem E.8 then lower-bounds this by the Bayes risk given x1:n. ] is linear in x1:n, so the Bayes risk equals the optimal linear risk. Hence the Proof. By Theorem E.7, (cid:98)x LSA the Bayes predictor given Φ(p)(x1:n Under AR(p), (cid:69)[xn+1 claim. x1:n E.3 Nested Transformer Feature Spaces and Risk Monotonicity in Time Series Prediction We take function-space perspective: with Hankel inputs and masked label slot, the one-layer LSA readout operates in restricted feature class. As the sequence length grows, these features collapse to scaled copies of the last lags, which clarifies both our Hankel design (p+1 rows) and why one-layer LSA cannot fundamentally beat linear regression on AR(p). Setup. Throughout this subsection we work under the AR(p) model in Definition 2.2, the Hankel construction in Definition 2.5, the one-layer LSA update in Definition E.1, and the cubic coordinates in Definition E.6. Let be the mask from Definition E.1. Theorem E.10 (Ergodic Theorem, Birkhoff [Kal06]). Let ξ be random element in measurable space with distribution µ, and let : be µ-preserving transformation with invariant σ-field I. Then for any measurable function 0 on S, we have 1 n1 (cid:88) k=0 (T kξ) a.s. (cid:69)[ (ξ) I], where the convergence is almost surely. 29 Lemma E.11 (Asymptotic feature collapse). Define the normalized masked Hankel Gram Gn := 1 H . a.s. Γ p+1, where [Γ p+1 ] = γi and γ = (cid:69)[x t+h ]. Moreover, for the cubic coordiThen, entrywise, Gn nates φ(p) j,r,k in Definition E.6, φ(p) j,r,k (x1:n ) a.s. γ jr np+k, j, [p+1], [p]. Hence, span(cid:8)φ(p) j,r,k (x1:n )(cid:9) a.s. span{x np+1, . . . , xn }. Proof. Each entry of Gn is time average of xi+a1 xi+b1; by Birkhoffs ergodic theorem (Theorem E.10), Gn i+ j1 i+r1. j,r,k, factor out np+k and apply the same theorem to 1 p+1 almost surely. For φ(p) Γ (cid:80) Corollary E.12 (Nested spaces). Let (cid:101)H (p) LSA := span{x np+1, . . . , xn Theorem E.13 (Risk plateau on AR(p)). Under AR(p), the Bayes predictor is linear in the last lags: ] = 0. By Lemma E.11 and Corollary E.12, any one-layer xn+1 }. Therefore, the minimal LSA based on Hankel features operates (as ) on span{x np+1, . . . , xn ϵ and does not decrease for p: achievable MSE equals the linear Bayes risk σ2 n+1 with (cid:69)[ϵ }. Then (cid:101)H n j+1 for all p. (p+1) LSA x1:n = (cid:80)p (cid:101)H (p) LSA + ϵ n+1 j=1 ρ (cid:69)(cid:148)(xn+1 (LSA features of order q))2(cid:151) = σ2 ϵ , p. inf Proposition E.14 (Asymptotic exact recovery: constructive parameter choice). Let the one-layer readout at the prediction slot be written as in Lemma E.3: (cid:2)LSA(H)(cid:3) p+1, np+1 = GnA np+1:n, with (cid:82)p+1 and (cid:82)(p+1)p. Define := (0, . . . , 0, 1) (cid:82)p+1, := (cid:21) (cid:20)J Γ 1 01p (cid:82)(p+1)p, where (cid:82)pp is the anti-diagonal permutation (reversal) matrix and Γ Toeplitz matrix. Then, as , is the autocovariance i.e., the one-layer LSA readout exactly recovers the Bayes-optimal linear predictor in the limit. GnA np+1:n a.s. ρ np+1:n, Proof. By Lemma E.11, Gn Γ p+1 = [γ p, . . . , γ 1, γ 0 ], we get p+1. Since bΓ = [γ p, . . . , γ 1 Γ p+1A ] Γ 1 = [γ 1, . . . , γ ] Γ 1 = ρ , using the YuleWalker relation ρ = Γ 1 γ with γ = (γ 1, . . . , γ ). Why exactly p+1 Hankel rows. The first rows are necessary to capture the true lags of the AR(p) process, while the (p+1)-st row serves as the masked prediction slot. With fewer than p+1 rows, the model cannot access all relevant lags. With more than p+1 rows, the extra rows are asymptotically redundant, since the optimal predictor ultimately depends only on the last lags. Closed-Form Gap Characterization between LSA and Linear Models F.1 Warm Up via AR(1) Warm-up, not global optimum. Motivated by the asymptotic constructive choice in Proposition E.14 (where has the last entry 1 and the last row of is 0), we study the univariate AR(1) case and restrict (b, A) to the one-dimensional ray induced by that limiting structure. This gives computable warm start that illustrates how Isserlis theorem (Theorem F.3) enters the calculation; it is not the global optimum over all (b, A), but it tends to linear regression as . Proposition F.1 (AR(1) warm start along the asymptotic ray). Let {x process of Definition 2.2: } be the stationary Gaussian AR(1) = ρ t1 + ϵ , ρ < 1, ϵ i.i.d. (0, σ2 ϵ ), and set σ2 := σ2 (Definition 2.5) with mask = diag(I n1, 0) and define the normalized Gram ϵ /(1 ρ2) = var(x ). For context = 1 and 3, let Hn be the Hankel matrix Gn := 1 HnM = 1 n1 (cid:88) i=1 (cid:18) 2 xi xi+ (cid:19) . xi xi+1 2 i+1 Let Sn := 1 (cid:80)n1 i=1 xi xi+1. Restrict (b, A) to (cid:20)0 = (cid:21) , = (cid:21) (cid:20)α , α (cid:82), so the one-layer LSA readout at the prediction slot is Consider the surrogate loss against the AR(1) linear term: (α) = (b GnA) xn = α Sn xn. (cid:98)yn L(α) = (cid:69)[( (α) ρ xn )2] = (cid:69)[x 2 (αSn (cid:98)yn ρ)2] = α2Dn 2αρNn + ρ2σ2. Define the geometric sums (for 1) Km := (cid:88) k=1 ρ2k = ρ2(1 ρ2m) 1 ρ2 , Hm := (cid:88) k=1 ρ2k = ρ2(cid:0)1 (m + 1)ρ2m + mρ2(m+1)(cid:1) (1 ρ2)2 . Then, with Γ := Cov(x , t+k ) = σ2ρk, Nn := (cid:69)[x 2 nSn ] = Dn := (cid:69)[x 2 nS2 ] = σ4 σ n2 (cid:148)(n 1)ρ + 2 ρ (cid:151) , Kn1 (cid:148)(n 1)n ρ2 + (n 1) + (cid:0)4(n 1) 2(cid:1)Kn1 (cid:151) + 4Hn + (cid:0)4(n 1) + 2(cid:1)Kn2 + 8Hn1 . (2) (3) Consequently the unique minimizer along this ray and its value are α = ρ Nn Dn , L(α) = ρ2σ2 min α ρ2N 2 Dn . 31 Equivalently, (cid:69)(cid:2)( min α (cid:98)x LSA n+ xn+1 )2(cid:3) = σ2 ϵ + ρ2σ2 ρ2N 2 Dn , and, by the law of large numbers and dominated convergence, α 1 σ2 , min α L(α) 0, i.e., this warm start tends to the linear-regression limit. Proof. Step 1: Nnpairwise expansion (4th order). Nn = 1 n1 (cid:88) i= (cid:69)[x 2 xi xi+1 ]. For the zero-mean Gaussian quadruple (xn, xn, xi, xi+1 Γ ), by Isserlis (Theorem F.3) the three pairings give (xn, xn (xn, xi (xn, xi+ )(xi, xi+1 )(xn, xi+1 )(xn, xi ) Γ ) Γ ) Γ = σ4ρ, 1 Γ 0 ni ni1 Γ ni ni1 = σ4ρ2n2i1, = σ4ρ2n2i1. Hence (cid:69)[x 2 xi xi+1 ] = σ4(cid:0)ρ + 2ρ2n2i1(cid:1), and summing over yields (2). Step 2: Dnpairwise expansion (6th order). Dn = 1 n2 n1 (cid:88) n1 (cid:88) i= j=1 (cid:69)[x 2 xi xi+1 j+1 ] splits into diagonal (i = j) and off-diagonal (i < j) parts. Diagonal = j. With := xn, := xi, := xi+1, + 2Γ 2 + 2Γ 2 (cid:69)[X 2Y 2Z 2] = Γ 3 0 Y 0 = σ2ρni1, Γ = σ2ρni, Γ = σ2, Γ Substituting Γ gives the diagonal contribution Y Γ Γ 0 + 2Γ 2 Γ 0 + 8Γ Γ Γ . Y = σ2ρ and summing over = 1, . . . , 1 σ6(cid:128)(n 1) + 2ρ2(n 1) + 2 n2 (cid:88) k=1 ρ2k + 10 n1 (cid:88) k= ρ2k(cid:138) = σ6(cid:128)(n 1) + 2ρ2(n 1) + 2Kn2 + 10Kn1 (cid:138) . Off-diagonal < j. Let Y1 := xi, Y2 := xi+1, Z1 := j, Z2 := j+1. Grouping the 15 pairings by how the two copies of xn are paired gives the per(i, j) contributions listed below (the factor 2 indicates the symmetric counterpart): (xn, xn (xn, xn (xn, xn {(xn, Y1 {(xn, Z1 {(xn, Y1 {(xn, Y1 {(xn, Y2 {(xn, )(Y1, Y2 )(Y1, Z1 )(Y1, Z2 ), (xn, Y2 ), (xn, Z2 ), (xn, Z1 ), (xn, Z2 ), (xn, Z1 ), (xn, Z2 ) ) )(Z1, Z2 )(Y2, Z2 )(Y2, Z1 ) )}, (Z1, Z2 )}, (Y1, Y2 )}, (Y2, Z2 )}, (Y2, Z1 )}, (Y1, Z2 )}, (Y1, Z1 σ6ρ2 σ6ρ2( ji) σ6ρ2( ji) ) (2) 2σ6ρ2(ni) ) (2) 2σ6ρ2(n j) ) (2) 2σ6ρ2(ni) ) (2) 2σ6ρ2(ni) ) (2) 2σ6ρ2(ni1) ) (2) 2σ6ρ2(ni1) 32 Summing over 1 < 1 and reindexing, (cid:88) 1i< jn1 (cid:69)[x 2 xi xi+1 j+ ] = σ6(cid:128)(cid:129)n 1 2 (cid:139) ρ2 + 2 k=1 n1 (cid:88) (n 1 k)ρ2k + 6 n1 (cid:88) (n 1 i)ρ2(ni) n1 (cid:88) (n 1 i)ρ2(n1i) + 2 + i=1 n2 (cid:88) (n 1 k)ρ2k(cid:138) k=1 + 4Hn1 + 2(n 1)Kn2 + 2Hn2 (cid:138) . i=1 (cid:139) = σ6(cid:128)(cid:129)n 1 ρ2 + (2n 8)Kn1 2 Finally, we obtain the final expression of Dn (cid:69)[x 2 xi xi+1 j+ ] n1 (cid:88) n1 (cid:88) i=1 j=1 (cid:88) Dn = 1 n2 = 1 n2 1i= jn1 (cid:69)[x 2 xi xi+1 j+1 ] + 2 (cid:88) 1i< jn1 (cid:69)[x 2 xi xi+1 j+1 ] σ = = (cid:138) (cid:128)(n 1) + 2ρ2(n 1) + 2Kn2 + 10Kn1 (cid:128)(cid:129)n 1 n2 + 2σ6 n2 (cid:148)(n 1)n ρ2 + (n 1) + (4(n 1) 2) Kn1 (cid:139) ρ2 + (2n 8)Kn1 + 4Hn1 σ6 n2 + 2(n 1)Kn + 2Hn2 (cid:138) . + (4(n 1) + 2) Kn2 + 8Hn1 + 4Hn (cid:151) . Adding the diagonal part and multiplying by 1/n2 yields (3). Step 3: Limit and conclusion. Since Dn As Sn (cid:69)[x t+ ] = ρσ2 almost surely and 2 > 0, is strictly convex, so the minimizer and value follow. is integrable, α 1/σ2 and minα L(α) 0. Remark F.2 (Warm start vs. global optimum). The optimization above is restricted to the asymptotic ray = [0, 1], = [α, 0]. It serves as computational warm-up to practice Isserlis-based moment calculations and to quantify the finite-sample gap. The global optimum over all (b, A) may differ, but this warm start already converges to the linear-regression limit as . Auxiliary lemmas used in Appendix F.1 Theorem F.3 (Isserlis Theorem (Wicks Formula) [Iss18]). Let (X 1, . . . , normal random vector. ) be zero-mean multivariate If is odd, i.e., = 2m + 1, then If is even, i.e., = 2m, then (cid:69)[X 1X 2 2m+1 ] = 0. (cid:69)[X 1X 2 2m ] = (cid:88) pPn (cid:89) {i, j}p (cid:69)[X j ], where Pn denotes the set of all possible pairwise partitions (perfect matchings) of {1, 2, . . . , 2m}. 33 F.2 finite-sample optimality gap for one-layer LSA Goal. We prove that for any fixed sample size n, the best one-layer LSA readout on Hankel inputs cannot match linear regression on the last lags. The argument rewrites the readout as quadratic form in Kroneckerlifted feature, solves the induced convex problem in closed form, and identifies strictly positive Schurcomplement gap. In other words, this section provides the first rigorous separation between LSA and linear regression: the gap is not an artifact of optimization, but structural property of the linear self attention. Setup (recalled). Let {x let Hn be the Hankel matrix from Definition 2.5 and put } be zero-mean stationary AR(p) process as in Definition 2.2. For p, := 1 HnM = 1 np (cid:88) i=1 (i) (i) (cid:82)(p+1)(p+1) , := diag(I np, 0). Denote the prediction window := np+1:n (cid:69)[ϵ2 n+1 ] = 0, (G, x). Given parameters (cid:82)p+1 and (cid:82)(p+1)p, the one-layer LSA (cid:82)p and := xn+1 n+1 with (cid:69)[ϵ ] = σ n+1 = ρ + ϵ ϵ and ϵ readout at the prediction slot is n+1 (b, A) = GAx. (cid:98)xn+1 (4) Kronecker reparameterization. Let Dp+1 be the (p+1)-dimensional duplication matrix so that vec(G) = Dp+1 vech(G). Using vec(AX B) = (B A) vec(X ) and the mixedproduct rule, (A G b) = (cid:0)(vech G) p+1 (cid:1) (b vec(A )) = (cid:0)(vech G) (cid:1) η, (cid:101) where η := (D (cid:101) p+1 Ip ) (b vec(A )) (cid:82)qp, := (p+1)(p+2) 2 . Introduce the lifted feature and its moments := (vech G) (cid:82)qp, (cid:101)S := (cid:69)[Z ], (cid:101)r := (cid:69)[Z ], Γ := (cid:69)[x ]. η: Then the meansquared error decomposes as strictly convex quadratic in (cid:101) L(b, A) := (cid:69)(cid:2)( = σ2 (b, A) y)2(cid:3) η η η 2 (cid:101) ρ + (cid:101)S (cid:101) (cid:101) (cid:98)xn+1 ϵ + ρΓ (cid:101)r ρ. (5) Two technical facts we will establish and use. (cid:19) = Σ (cid:19)(cid:153) := (cid:69) (cid:150)(cid:18)Z (F1) The block second-moment matrix (cid:19) (cid:18)Z (cid:18) (cid:101)r (cid:101)S (cid:101)r Γ Equivalently, (cid:101)S 0 and the Schur complement Γ η = (cid:101)S1 ϵ + ρΓ Because the original parameters (b, A) satisfy the rank-one Kronecker constraint (cid:101) Ip (cid:101)r 0. (cid:101)r ρ with minimum value 1 ρ ρ (cid:101)S )(b vec(A)), their achievable risk is no smaller than Lmin. (F2) The unconstrained minimizer of (5) is (cid:101) = σ2 is strictly positive definite. (cid:101)r ρ. (cid:101)S1 Lmin (cid:101)r (cid:101)r η = (D p+1 The core of the proof is thus (F1); (F2) is elementary convex optimization once (F1) holds. 34 Step 1: strict positive definiteness of basic block We begin by showing that the basic statistics at time already enjoy strict nondegeneracy. Lemma F.4 (Strict covariance of (cid:0)vech G, x(cid:1)). Let := vech (cid:82)q and := np+1:n covariance matrix (cid:82)p. Then the Cov(cid:0)[ ](cid:1) 0. , Proof. Write Z0 := [ , ] and suppose, for contradiction, that there exists nonzero vector with Var(vZ0 ) = 0. We will force all coordinates of to be zero by an innovation-by-innovation elimination. Let Ft := σ(ϵ : t). Proceed by traversing the rearranged vector Z0 as shown below, in rowwise from bottom to top, eliminating coefficients accordingly. = (cid:101)Z0 np (cid:88) m=1 2 np (cid:88) m=1 np (cid:88) m=1 np (cid:88) m=1 xm xm+ xm xm+2 ... xm xm+p np+1 np+2 ... np (cid:88) m=1 2 m+2 ... np (cid:88) m=1 xm+2 xm+p ... np (cid:88) m= 2 m+p np (cid:88) m=1 2 m+1 np (cid:88) m= np (cid:88) m=1 xm+1 xm+2 ... xm+1 xm+p (cid:80)np Bottom block (involving xn). The last row of the Hankel Gram contributes, among others, the terms m=1 2 and xn1 xn. Collecting the coeffim+p and (cid:80)np cients in that multiply {x 2 m=1 xm+p1 xm+p, whose final summands are 2 }, we can write n, xn1 xn, . . . , xnp xn, xn = (terms Fn1-measurable) + (cid:0)u Z0 xnp:n1 + c(cid:1) xn, + xn = ρ xnp:n1 + ϵ with ϵ Fn1 for some reals a, and vector (cid:82)p determined by v. Since xn and ϵ independent of all earlier innovations, the conditional variance is (cid:12) (cid:12) Fn1 (cid:1) = Var(cid:0)(u xnp:n Fn1 Var(cid:0)v + c)ϵ Z0 + ϵ2 (cid:1). If = 0 on set of positive probability and ϵ novations), then this conditional variance is > 0 on that set; hence Var(vZ0 Thus = 0 a.s. With = 0, the conditional variance reduces to Var(cid:0)(u xnp:n1 (u xnp:n1 Hence all coefficients of that touch xn vanish. has finite fourth moment (true e.g. for Gaussian in- ) > 0, contradiction. (cid:1) = + = 0 a.s. By Lemma F.10, this implies = 0 and = 0. ϵ , forcing xnp:n + c)2σ2 Fn1 + c)ϵ Induction upward. Assume we have eliminated all coefficients attached to xn, xn1, . . . , xnr+1 and to every Gram entry that involves these variables (this means we have removed the last block-rows of the lower-triangular tableau of the sums defining g). Focus on the remaining first time that appears, xnr . Exactly the same conditioning on Fnr1, writing xnr nr , shows that the coefficient of 2 nr must be 0, and then the linear coefficient of xnr must be 0. By Lemma F.10 again, all coefficients in that touch xnr vanish. = φ xnrp:nr Proceeding for = 1, . . . , removes all entries of associated with and with the last block-rows of g. Finally, the block that involves no recent xs also collapses by the same argument (conditioning on Ft at the appropriate times). We conclude = 0, contradicting the assumption. Therefore Cov(Z0 ) 0. + ϵ 35 Lemma F.5 (Two linearalgebra tools). (i) If Σ = (cid:18)Σ Σ AA BA (cid:19) AB (cid:82)(m+n)(m+n) is positive definite, then Σ Σ BB the Schur complement Σ Σ every nonzero v, Var(vX ) > 0. Proof. (i) is standard; see, e.g., any matrix analysis text. (ii) is immediate from Cov(X )v = Var(vX ). 0. (ii) If is squareintegrable with Cov(X ) 0 then, for Σ BA AB AA Σ1 BB Step 2: lifting to the Kronecker level We now show that the lift = (vech G) is also strictly nondegenerate in the block sense. Lemma F.6 (Strict PD of the Kroneckerlifted block). With := vech G, := np+1:n and := x, Σ = (cid:19) (cid:18) (cid:101)r (cid:101)S (cid:101)r Γ = (cid:69) (cid:19)(cid:153) (cid:150)(cid:18)Z (cid:19) (cid:18)Z 0. Proof. Take any (u, w) = (0, 0) with (cid:82)qp, (cid:82)p, and reshape into matrix (cid:82)qp so that = vec(U). Consider the scalar := + = (cid:88) s=1 (cid:128) xs + ws (cid:88) ℓ=1 Uℓs gℓ (cid:138) = (cid:0)w + g(cid:1). Condition on x. Using the tower property, Var(Y ) = (cid:69)(cid:2)Var(Y x)(cid:3) + Var(cid:0)(cid:69)[Y x](cid:1) (cid:69)(cid:2)Var(cid:0)x (U g) x(cid:1)(cid:3). Given x, is deterministic vector and is still random. Hence Var(cid:0)x (U g) x(cid:1) = Cov(g x) x. By Lemma F.4 and the Schurcomplement Lemma F.5(i), Cov(g x) = Cov(g) Cov(g, x) Γ 1 Cov(x, g) 0. Therefore, for any = 0, Cov(g x) > 0 on set of positive probability (because Γ and is non-degenerate); taking expectations yields Var(Y ) > 0. = x; since Γ make Var(Y ) = 0, which proves Σ 0. 0 If instead = 0 then = 0 and 0, Lemma F.5(ii) implies Var(Y ) > 0 unless = 0. Thus no nonzero (u, w) can Step 3: the gap via Schur complement We are ready to state and prove the main result. Theorem F.7 (Finite-sample optimality gap of one-layer LSA). Let the setup above hold. Define := Γ (cid:101)r 1 (cid:101)S (cid:101)r. Then Equivalently, min b,A (cid:69)(cid:2)( (cid:98)xn+1 (b, A) xn+1 )2(cid:3) σ2 ϵ + ρ ρ, 0. min b,A (cid:69)(cid:2)( (cid:98)xn+1 (b, A) xn+1 )2(cid:3) min (cid:69)(cid:2)(w xn+1 )2(cid:3) + ρ ρ, so one-layer LSA has strictly positive excess risk over linear regression for any finite n. Proof. By Lemma F.6, (cid:101)S 0 and its Schur complement in Σ is Γ η gives (cid:101) (cid:101) η = (cid:101)S1 (cid:101)r ρ and (cid:101)r (cid:101)S (cid:101)r 0. Minimizing (5) over = σ2 ϵ + ρΓ ρ ρ (cid:101)r 1 (cid:101)S (cid:101)r ρ = σ2 ϵ + ρρ. min η (cid:101) Because (b, A) can realize only the rankone Kronecker set of (cid:101) Strict positivity of the excess term follows from 0. η, minb,A min η L, proving the bound. (cid:101) Remark F.8 (Population limit and order of limits). If is replaced by the population covariance Γ then = vech(Γ ) is deterministic. Writing := g, p+1 p+1, (cid:101)S = (uu ) Γ p, (cid:101)r = Γ = (cid:101)r + (cid:101)S (cid:101)r = Γ p, so = 0 and the gap vanishes. For finite n, (cid:101)S is strictly PD and 0; taking before inverting collapses the gap, illustrating an order-of-limits effect. As shown in Proposition E.14, in the asymptotic regime we can indeed prove that the one-layer LSA readout exactly recovers the Bayes-optimal linear predictor in the limit. Theorem F.9 (Uniform excess-risk over parameter family). Fix 0 < < and := {ρ (cid:82)p : < ρ ((ρ)). Then, uniformly for all ρ R, < R}. Set λ λ min := infρR min min b,A (cid:69)(cid:2)( (cid:98)xn+1 (b, A) xn+1 )2(cid:3) σ2 ϵ + λ min 2. Proof. By Theorem F.7, the excess is ρ(ρ)ρ with (ρ) 0 for each ρ. Continuity of ρ (cid:55) (ρ) and compactness of = {ρ : ρ R}, the Extreme Value Theorem (Theorem F.11) ensures that λ ((ρ)) attains strictly positive minimum on R. Hence ρ(ρ)ρ λ λ 2 ρ2 min minr 2. min Auxiliary lemmas used in Appendix F.2 Lemma F.10 (No non-trivial zero-variance combination of consecutive samples). For any integers and 1 and any coefficients c1, . . . , ck, := (cid:88) j=1 i+ j1 = 0 a.s. = c1 = = ck = 0. Proof. Let Fi+k2 := σ(ϵ Conditioning, : + 2). Write xi+k1 = φ xi+k2:i1 + ϵ i+k1 with ϵ i+k Fi+k2. If = 0 a.s., then Var(Y Fi+k2 ck = 0. Var(Y Fi+k2 ) = c2 ) = 0 a.s., hence ck i+k Var(ϵ ) = c2 σ2 ϵ . = 0. Iterate backwards on to conclude c1 = = Theorem F.11 (Extreme Value Theorem [Rud21]). Let (cid:82)n be nonempty compact set, and let : (cid:82) be continuous. Then is bounded on K, and there exist points xmin, xmax such that (xmin ) = inf xK (x), (xmax ) = sup xK (x). 37 F.3 Order of the finite-sample gap Goal. We quantify the finitesample excess risk in Theorem F.7 and show it decays at rate 1/n. The proof expands the lifted second moments to first order around their population (rank-one) limit and evaluates the Schur complement via singular block inverse. Setup (recalled). Let {x with absolutely summable autocovariances (cid:80) the Hankel matrix from Definition 2.5, mask = diag(I np, 0), and } be zero-mean stationary Gaussian AR(p) process as in Definition 2.2, = [ (1) . . . (np+1) ] be < . For p, let Hn h(cid:90) γ Gn = 1 HnM = 1 np (cid:88) m=1 (m) (m) (cid:82)(p+1)(p+1) . Let := np+1:n (p+1)(p+2) 2 (cid:82)p, Γ . Define the lifted moments p+1 := (cid:69)[x (m) (m)], Γ := (cid:69)[x ], and := vech(Γ p+ ) (cid:82)q with = Sn := (cid:69)(cid:2)(vec Gn x)(vec Gn x)(cid:3), rn := (cid:69)(cid:2)(vec Gn x) (cid:3), and their half-vectorized versions (cid:101)Sn := (cid:69)(cid:2)(vech Gn x)(vech Gn x)(cid:3), (cid:101)rn := (cid:69)(cid:2)(vech Gn x) (cid:3). Lemma F.12 (First-order expansions of (cid:101)Sn and (cid:101)rn). Let Lp+1 be the elimination matrix with vech(A) = Lp+1 vec(A) for symmetric A. Then, as with fixed, Sn = (vec Γ p+1 )(vec Γ rn = (vec Γ ) Γ p+1 p+1 + 1 ) Γ + 1 (vec) + o(1/n), (vec) + o(1/n), (6) (7) (vec) for deterministic , (vec) depending only on {γ }, p. Consequently = (uu ) Γ (cid:101)Sn + 1 CS + o(1/n), = Γ (cid:101)rn + 1 Cr + o(1/n), (vec) with CS = [u/u, Q] and := Ip, writing := u2, ) and Cr = (Lp+1 (Lp+1 Ip Ip )C = (Lp+ Ip )C (vec) . Moreover, for any orthonormal (cid:98)Sn := (cid:101)SnP = (cid:98)rn := (cid:101)rn = (cid:21) (cid:20)c Γ 0 (cid:20)u Γ 0 0 + 1 (cid:21) + 1 (cid:20)δ (cid:20)C11 B (cid:21) + o(1/n), (cid:21) + o(1/n), (8) where (cid:2) C11 B ) Proof. Stationarity gives (Γ st {1, . . . , p}. All variables are jointly Gaussian with zero mean; Isserlis theorem is used throughout. ts for indices i, j, k, ℓ {1, . . . , + 1} and s, = γ p+1 ) p (cid:3) = CS and (cid:2) δ = γ (cid:3) = Cr . ji and (Γ 38 Computation of rn. By linearity and Gn = 1 (cid:80)np m=1 (m) (m),, rn = (cid:88) i, j,s,t (cid:69)[Gi xs ] (e ei es )e , (cid:69)[Gi xs ] = 1 np (cid:88) m=1 (cid:69)[x (m) (m) xs ], with (m) = xm+i1. Let = xm+i1, = xm+ j1, = xnp+s, = xnp+t . Isserlis yields (cid:69)[abcd] = (cid:69)[ab](cid:69)[cd] + (cid:69)[ac](cid:69)[bd] + (cid:69)[ad](cid:69)[bc] = γ (Γ ) st + γ(np+s)(m+i1) γ(np+t)(m+ j1) ji + γ(np+t)(m+i1) γ(np+s)(m+ j1). With := + 1 {1, . . . , p}, (cid:69)[x (m) (m) xs ] = γ (Γ ) st ji + γ k+si γ k+t + γ k+ti γ k+s j. Summing over m, (cid:69)[Gi xs ] = The first term equals γ uniformly bounded for fixed a, b, and hence ) st ji ji (Γ (Γ γ + γ < , the partial sums (cid:80)np k= γ γ k+b are k+a np (cid:88) (cid:0)γ k+si γ k+t + γ k+ti γ k+s (cid:1). γ (Γ ) st ji + 1 k=1 st . Since (cid:80) ) where 1 np (cid:88) k=1 (cid:0)γ k+si γ k+t + γ k+ti γ k+s (cid:1) = 1 (r) j,st + o(1/n), (r) j,st := (cid:88) k= (cid:0)γ k+si γ k+t + γ k+ti γ k+s (cid:1) converges absolutely. Note that (cid:88) γ (Γ ) st (e ei es )e ji i, j,s,t = (cid:88) i, j,s,t = (cid:88) i, = vec(Γ (Γ (Γ ) (Γ ) st (e ei ) (ese p+1 ) ) e ei p+1 (cid:0)(cid:88) s,t (Γ ) st ese (cid:1) ) Γ p+ Therefore = (cid:0)vec Γ (cid:1) Γ p+1 rn + 1 (cid:88) (cid:128) γ (Γ ) st ji + (r) j,st (cid:138) (e ei es )e + o(1/n), i, j,s,t which is (7) with (vec) defined by the bracketed coefficients. Computation of Sn. By definition, (cid:88) Sn = (cid:88) i, j,k,ℓ (cid:69)[Gi Gkℓ xs ] (cid:0)(e ei es ) (eℓ ek et )(cid:1), s,t where (cid:69)[Gi Gkℓ xs ] = 1 n2 np (cid:88) np (cid:88) m= m=1 (cid:69)[x (m) (m) (m) (m) ℓ xs ]. Let = xm+i1, = xm+ j1, = xm+k1, = xm+ℓ1, = xnp+s, = xnp+t . Isserlis for six variables decomposes into the term (cid:69)[e ](cid:69)[abcd] and the 12 cross pairings where and/or pair with {a, b, c, d}. For the four-variable factor, (cid:69)[abcd] = γ γℓk + γ(m+i1)(m+k1) γ(m+ j1)(m+ℓ1) ji + γ(m+i1)(m+ℓ1) γ(m+ j1)(m+k1). Let = m. Then Averaging over m, m, (cid:69)[abcd] = γ γℓk ji + γ ik+h γ jℓ+h + γ iℓ+h γ jk+h. 1 (cid:88) m,m (cid:69)[abcd] =γ γℓk + (1 ji (n p)2 n2 )γ γℓk ji np1 (cid:88) + h=(np1) n2 (cid:128)γ γ ik+h jℓ+h + γ γ iℓ+h jk+h (cid:138) . Because (n h)/n2 = (1/n) (1 h/(n p)) np < , 1 n2 (cid:88) m,m (cid:69)[abcd] = γ γℓk ji + 1 (S,0) j,kℓ + o(1/n), (cid:128)γ γ ik+h jℓ+h + γ γ iℓ+h jk+h (cid:138) . Multiplication by (Γ entrywise. st gives the leading block γ ) γℓk ji (Γ st , which matches (cid:0)vec Γ ) p+1 (cid:1)(cid:0)vec Γ p+1 (cid:1) Γ Each cross pairing contributes product of three covariances with at most linear dependence on m, m. For instance, the pairing {e, a}, { , b}, {c, d} yields γ(np+s)(m+i1) γ(np+t)(m+ j1) γ(m+k1)(m+ℓ1) = γ si+k γ j+k γℓk, (cid:1)γℓk, which after setting = + 1. Summing over m, produces 1 equals 1 times finite constant plus o(1/n) by absolute summability through Toeplitz summation (see Lemma F.16). Enumerating all 12 cross pairings and collecting like terms gives the absolutely convergent series (cid:0) (cid:80)np k=1 j+k si+k γ γ (S,1) j,kℓ;st = (cid:88) q= (cid:148)γ si+q γ j+q γℓk + γ si+q γ tk γ jℓ+q + γ si+q γ tℓ γ jk+q + γ j+q γ ti+q γℓk + γ j+q γ tk γ iℓ+q + γ j+q γ tℓ γ ik+q ti+q γ j+q γℓk + γ ti+q γ sk γ jℓ+q + γ ti+q sℓ γ γ jk+q + (cid:88) q=1 (cid:148)γ + γ j+q γ si+q γℓk + γ j+q γ sk γ iℓ+q + γ j+q sℓ γ γ ik+q (cid:151) (cid:151) . 40 γ and (cid:80) j,kℓ := (cid:88) (S,0) h(cid:90) Boundary corrections of order 1/n proportional to γ Consequently, γℓk (Γ ) st are absorbed into the final constant. ji (cid:69)[Gi Gkℓ xs ] = γ γℓk (Γ ) st ji + 1 (cid:128) (S,0) j,kℓ (Γ ) st + (S,1) j,kℓ;st (cid:138) + o(1/n). Substituting into the tensor expansion of Sn yields (6) with (cid:138) (cid:0)(e + (cid:88) ) st (Γ (vec) (S,1) j,kℓ;st (S,0) j,kℓ (cid:128) p = (cid:88) i, j,k,ℓ s,t ei es ) (eℓ ek et )(cid:1). Passing to vech. Since vech(A) = Lp+1vec(A) for symmetric A, it follows that (cid:101)Sn = (Lp+1 Ip ) Sn (Lp+1 Ip ) , = (Lp+ Ip ) rn, (cid:101)rn which, together with (6)(7), gives the stated expansions with = (uu) Γ p, and with the indicated CS, Cr . Finally, for any orthonormal = [u/u, Q] and = Ip, the block forms (cid:101)rn follow by inserting the expansions and collecting the top/orthogonal for (cid:98)Sn p, 0) and the 1/n blocks are those of CS and Cr . components; the leading block equals diag(c Γ Dominated convergence (using γ Cβ h) justifies all o(1/n) remainde and = Γ (cid:101)SnP and (cid:98)rn = = Lemma F.13 (Singular block inverse and first-order Schur complement). In the basis of Lemma F.12, let = (cid:98)Sn (cid:21) (cid:20)A0 0 0 0 + 1 (cid:21) (cid:20)A1 B + o(1/n), (cid:98)rn = (cid:21) (cid:20)r0 0 + 1 (cid:21) (cid:20)δ + o(1/n), with A0 = Γ 0 and r0 = uΓ p. Then, for all large n, (cid:98)S (cid:98)r 1 (cid:98)rn = Γ + 1 (cid:148) 1 A1 + 1 C 1B 2 1d + C 1d + 2 Sym(δ)(cid:151) + o(1/n), where Sym(M ) = 1 2 (M + ). Equivalently, in the original coordinates, (cid:101)S (cid:101)r = Γ 1 (cid:101)rn Bp : = 1 Bp + 1 + 1 A1 + o(1/n), 1B 2 1d + C 1d + 2 Sym(δ). (9) Proof. Write the block decomposition of (cid:98)Sn from Lemma F.12 as = (cid:101)Sn (cid:21) (cid:20)A D , = A0 + 1 A1 + o(1/n), = 1 + o(1/n), = + o(1/n), with A0 = Γ 0. For large n, 0, and the block inverse formula gives 1 (cid:101)S = (cid:150)A1 + A1E(D EA1E)1EA1 A1E(D EA1E)1 (cid:153) (D EA1E)1EA1 (D EA1E)1 . Since A1 = A1 0 1 A1 0 A1A1 0 + o(1/n) and EA1E = n2 BA1 0 + o(1/n2), 1E = 1 + o(1/n), (D 1E)1 = 1 + o(n). 41 Substituting and collecting orders yields 1 ((cid:101)S 1 ((cid:101)S 1 ((cid:101)S ) 11 ) 12 ) 22 1 = 0 = = 1 1 1 + 1 0 A1A 0 1 + o(1), 1 0 1 + o(n). 1 0 1 1B 0 + o(1/n), Now expand (cid:98)rn = [ r0; 0 ] + 1 [ δ; ] + o(1/n) with r0 = Γ and A1 0 = (1/c)Γ . Then (cid:98)r (cid:101)S 1 (cid:98)rn = 0 1 ((cid:101)S ) 11r0 + 2 0 1 ((cid:101)S ) 12 1 + n2 ((cid:101)S 1 22d + 2 ) Sym(δ 1 0 r0 ) + o(1/n). Each term is explicit: 0 1 0 r0 = Γ p, 0 1 0 A1A 1 0 r0 = 1 A1, 0 1 0 1BA 1 0 = 1 1B, 2 1 ((cid:101)S ) 12 Combining yields 1 = 2 1 C Sym(δ 2 1d, 1 0 1 n2 ) = 2 1 22d = 1 ) ((cid:101)S 1 Sym(δ). 1d + o(1/n), (cid:101)S (cid:98)r 1 (cid:98)rn = Γ + 1 (cid:148) 1 A1 + 1 C 1B 2 1d + C 1d + 2 Sym(δ)(cid:151) + o(1/n). Since the orthogonal basis change preserves the Schur complement, the same expansion holds in the original coordinates, giving (9). Theorem F.14 (First-order gap). With := Γ = 1 Bp + o(1/n), Bp := 1 A1 1 C Hence the optimal one-layer LSA excess risk satisfies (cid:101)r (cid:101)S1 (cid:101)rn, 1B + 2 Sym(cid:0)B 1d δ(cid:1) 1d. min b,A (cid:69)(cid:2)( (cid:98)xn+1 (b, A) xn+1 )2(cid:3) σ2 ϵ + ρ ρ = σ2 ϵ + 1 ρ Bp ρ + o(1/n). Moreover, Bp that for all n0 and all ρ with ρ r, 0; if Bp 0 (a generic nondegeneracy), then for any > 0 there exist n0 and cr > 0 such (cid:69)(cid:2)( (cid:98)x LSA n+1 xn+ )2(cid:3) (cid:69)(cid:2)( (cid:98)x LR n+1 xn+1 )2(cid:3) + cr . Proof. By Lemma F.13, we have (cid:101)S (cid:101)r 1 (cid:101)rn = Γ + 1 (cid:148) 1 A1 + 1 1B 2 1d + C 1d + 2 Sym(δ)(cid:151) + o(1/n). Thus with := Γ (cid:101)S (cid:101)r 1 (cid:101)rn = 1 Bp + o(1/n), Bp = 1 A1 1 C 1B + 2 Sym(cid:0)B 1d δ(cid:1) 1d. By Lemma F.6, each Theorem F.7: 0, hence Bp = limn 0. The excessrisk bound follows from min b,A (cid:69)[( (cid:98)xn+1 xn+1 )2] σ2 ϵ + ρ ρ = σ2 ϵ + 1 ρ Bp ρ + o(1/n). 0, let λ If Bp min Therefore, for any > 0, there exist n0 and cr ) > 0. For large n, (1/n)Bp = 1 0r 2 > 0 such that for all n0 and all ρ r, λ 2 /(2n), hence ρ λ (Bp = λ 0 0 ρ λ 0 2n ρ2. (cid:69)[( (cid:98)x LSA n+1 xn+1 )2] (cid:69)[( (cid:98)x LR n+1 xn+ )2] + cr . Remark F.15 (Why the rate is 1/n). At the population limit (cid:101)S = (uu)Γ is rank-one along u. Finite introduces O(1/n) perturbations CS, Cr that regularize the orthogonal directions, so the Schur complement (cid:101)rn is O(1/n). The overlap of Hankel windows is the source of these firstorder terms. Γ (cid:101)S1 (cid:101)r Auxiliary lemmas used in Appendix F.3 Lemma F.16 (Toeplitz-type summation). Let : (cid:90) (cid:82) (or (cid:67)) be absolutely summable, (cid:80) For 1 define h(cid:90) ah < . Sn := 1 n2 (cid:88) (cid:88) m=1 m=1 mm. Then Sn = 1 (cid:88) (cid:128) 1 h(cid:90) (cid:138) n ah + = 1 (cid:88) h(cid:90) ah + (cid:128) 1 n2 (cid:88) h(cid:90) ah (cid:138) = 1 (cid:88) h(cid:90) ah + o(1/n). Proof. Count the number of pairs (m, m) {1, . . . , n}2 with difference = h; it equals if < and 0 otherwise. Hence (cid:88) amm = n1 (cid:88) (n h) ah = (cid:88) (cid:128) 1 m,m=1 h=(n1) (cid:138) n ah. + Divide by n2 and use absolute summability to obtain the stated bound. F.4 finitesample gap for stacked LSA layers (with monotone improvement) Goal. For any fixed sample size and depth 1, we show that the best Llayer linear selfattention (LSA) readout on Hankel inputs has finitesample excess risk over linear regression on the last lags. To avoid unnecessary technicalities about duplicate features across layers, we work with the convex relaxation of the LSA parameters and allow singular secondmoment matrices; the MoorePenrose inverse then gives clean positivesemidefinite (psd) gap. We also prove that the optimal risk is monotone nonincreasing in depth L. Setup (layered). Let {x Hn be the Hankel matrix (Definition 2.5), = diag(I np, 0), and } be zeromean stationary AR(p) process (Definition 2.2). For p, let (0) := 1 HnM (cid:82)(p+1)(p+1) . 43 Write := np+1:n Initialize (0) = 0 and (0) (cid:82)p and := xn+1 (G(0), x), (cid:69)[ϵ := Hn. For ℓ = 0, . . . , 1 define the layer update n+1 with ϵ = ρ +ϵ n+1 n+1 ] = 0, (cid:69)[ϵ2 n+ ] = σ2 ϵ . (ℓ+1) = (ℓ) + (ℓ) (ℓ) (ℓ) x, (ℓ+1) = 1 H (cid:0)H (ℓ+1) (ℓ+1) (cid:1) , (10) where (ℓ+1) last coordinate is (ℓ+1) (the mask remains unchanged). The Llayer predictor is coincides with Hn except that the last row uses the current layers vector of entries whose (L) (cid:98)x n+1 = (L) = L1 (cid:88) ℓ=0 (ℓ) (ℓ) (ℓ) x. (11) oneshot convex relaxation for depth L. Let (ℓ) := vech G(ℓ) (cid:82)q with = (p+1)(p+2) the stacked Kronecker lift , and set [L] := (0) ... (L1) (cid:82)dL , dL = Lqp. For each layer, b(ℓ)G(ℓ)A(ℓ) can be written as η(ℓ)(g (ℓ) x) with η(ℓ) = (D and hence p+1 , . . . , (η(L1))(cid:3) Ip )(cid:0)b(ℓ) vec(A(ℓ))(cid:1), . (L) (cid:98)x n+1 = η[L] [L] , η[L] := (cid:2)(η(0)) Relaxing the rankone Kronecker constraint on parameters leads to linear regression of on [L]. Define the second moments (cid:101)SL := (cid:69)[Z [L] [L]], (cid:101)rL := (cid:69)[Z [L] ], Γ := (cid:69)[x ]. We allow (cid:101)SL to be singular (duplicates across layers are harmless). The standard normalequation calculation with the MoorePenrose inverse gives min η[L] (cid:69)(cid:2)(η[L] [L] y)2(cid:3) = σ2 ϵ + ρΓ ρ ρ + (cid:101)rL (cid:101)S (cid:101)r ρ, (12) so the Llayer LSA family (which is subset of the relaxed linear models) obeys the lower bound min {b(ℓ),A(ℓ)} (cid:69)(cid:2)( (L) (cid:98)x n+1 xn+1 )2(cid:3) σ2 ϵ + ρ ρ, n,L n,L := Γ + (cid:101)rL (cid:101)S (cid:101)r 0. (13) Lemma F.17 (Why Σ the covariance of the best linearprediction residual of on [L]. 0 even if (cid:101)SL is singular). Let Σ (cid:101)S+ (cid:101)r 0, and the MoorePenrose Schur complement Γ n,L L := (cid:69)(cid:2) [Z [L]; x] [Z [L]; x](cid:3) = (cid:2) (cid:101)SL (cid:101)rL (cid:101)rL is psd. Equivalently, (cid:3). Then 0 and equals (cid:101)r n,L Γ Proof. Σ covariance Γ gives = (cid:101)S+ is covariance hence psd. For any matrix B, the prediction [L] (cid:55) BZ [L] yields residual (cid:101)SL B. Minimizing over (in the leastsquares sense on the range of (cid:101)SL) (cid:101)rL + B (cid:101)r (cid:101)rL and residual covariance Γ (cid:101)S+ (cid:101)r L (cid:101)rL, which is psd by definition of covariance. 44 Depth helps: simple embedding argument. We now show that the optimal risk is monotone in L; the proof does not rely on invertibility nor on strictness. Proposition F.18 (Monotone improvement with depth). For every 1, min {b(ℓ),A(ℓ)}L ℓ= (cid:69)(cid:2)( (L+1) (cid:98)x n+1 xn+1 )2(cid:3) min {b(ℓ),A(ℓ)}L1 ℓ=0 (cid:69)(cid:2)( (L) (cid:98)x n+1 xn+1 )2(cid:3), (L) n+1 is defined in (11) under the update rule (10). In particular, the best twolayer risk is no worse where (cid:98)x than the best onelayer risk: min b(0),A(0), b(1),A(1) (cid:69)(cid:2)( (2) (cid:98)x n+1 xn+1 )2(cid:3) min b(0),A(0) (cid:69)(cid:2)( (1) (cid:98)x n+ xn+1 )2(cid:3). Proof. Fix any parameter set {b(ℓ), A(ℓ)}L1 ℓ=0 for the Llayer model (11). Construct an (L+1)layer model by keeping the first layers unchanged and appending zero layer: b(L) = 0, A(L) = 0. Then (L+1) = (L) (L) and thus (cid:98)x n+1, so the (L+1)layer loss equals the Llayer loss. Taking minima over the (cid:98)x respective parameter sets yields the displayed inequality. The = 1 2 case is immediate, and the general case follows identically. (L+1) n+1 = What we have (and what we do not claim). Equation (13) gives finitesample, depthL gap )2(cid:3) (cid:69)[( (cid:69)(cid:2)( (L) (cid:98)x n+1 )2] + ρ xn+1 xn+ (cid:98)x LR n+1 When duplicate features across layers make (cid:101)SL singular, the bound remains valid via (cid:101)S+ prets ρ n,L strengthen complement; this requires tracking how each layer injects new ϵ and is omitted here for clarity. and interρ as the linearprojection residual variance. Under additional nondegeneracy, one can has strictly positive Schur ndirections into the last Hankel row 0 (strict gap) by proving that the stacked covariance Σ 0. ρ, n,L n,L n,L (i) The relaxation (12)(13) can be written with deduplicated stacked feature (cid:101)Z [L] = Remarks. [ (0) x, s(1) x, . . . , s(L1) ], where s(ℓ) keeps only the new lastrow/column monomials created (cid:101)Z [L]] is typically invertible at finite n. All formulas remain the same with at layer ℓ; then (cid:101)SL := (cid:69)[(cid:101)Z [L] (cid:101)S+ . (ii) In the population limit , G(ℓ) concentrates around Γ p+1, the stacked feature collapses 0; the order of limits matters, exactly as in the onelayer to rankone Kronecker line, and analysis. n,L Chain-of-Thought (CoT) Rollout in TSF: Collapse-to-Mean and Error"
        },
        {
            "title": "Compounding",
            "content": "We study the freerunning (a.k.a. CoT) rollout where predictor feeds its own outputs back as inputs instead of conditioning on future ground truth. For linear timeseries models this produces clean, analyzable dynamical system that (i) collapses to the mean and (ii) accumulates prediction error to the unconditional variance at an exponential rate. We also show that, for every forecast horizon, the Bayes (linearregression) forecast is pointwise optimal, hence any linear selfattention (LSA) CoT rollout is uniformly worse and thus reaches largeerror regime no later than linear regression. Setup. Let {x companion matrix, } be zeromean, stable AR(p) process as in Definition 2.2, and let A(ρ) denote the A(ρ) = ρ 1 1 0 ... 0 ρ 2 0 1 ... 0 . . . ρ . . . . . . . . . p1 0 0 ... 1 ρ , 0 0 ... 0 ). Then st+1 with spectral radius ϱ(A(ρ)) < 1. Write st := (x , . . . , tp+1 η t+1 := (ϵ t+1, 0, . . . , 0). = A(ρ)st + η t+1 with CoT rollout. Given an initial state sn produces, in CoT mode, noiseless recursion = (xn, . . . , xnp+ ), linear predictor with coefficients (cid:82)p (cid:98)st+1 = A(w) (cid:98)st , = sn, (cid:98)s = with forecast (cid:98)xn+t Proposition G.1 (Bayes multistep forecast equals recursive rollout). For any horizon 1, the Bayes forecast conditional on the history satisfies 1 (cid:98)st . n+h := (cid:69)[xn+h (cid:98)x x1:n ] = ρ (cid:101)x n+hp:n+h1, where the rolledout state (cid:101)xk is defined recursively by = (cid:101)xk (cid:168)xk, n, (cid:98)x k, > n. Equivalently, the optimal hstep forecast is obtained by repeatedly applying the onestep predictor with coefficients ρ and feeding predictions back in place of future observations. Proof. Base case (h = 1). By the AR(p) definition, = ρ xn+1 xnp+1:n + ϵ n+1, ϵ n+1 x1:n, (cid:69)[ϵ n+1 ] = 0, so (cid:69)[xn+1 x1:n ] = ρ xnp+1:n = ρ (cid:101)x np+1:n. 46 Induction step. Assume the claim holds up to horizon h. Then xn+h+1 = ρ xn+hp+1:n+h + ϵ n+h+1. Taking conditional expectation on x1:n yields (cid:69)[xn+h+1 x1:n ] = ρ (cid:69)[xn+hp+1:n+h x1:n ]. By the induction hypothesis, for indices the conditional expectation equals the observed value, while for indices > it equals the Bayes forecasts (cid:98)x , i.e. the recursively defined (cid:101)x. Therefore x1:n (cid:69)[xn+h+1 ] = ρ (cid:101)x n+hp+1:n+h. Conclusion. By induction, the identity holds for all horizons 1. Thus the Bayes multistep forecast is exactly the recursive rollout of the onestep predictor with weight vector ρ. Lemma G.2 (Exponential decay for any stable CoT). If ϱ(A(w)) < 1, then for every consistent matrix norm there exist > 0 and β (0, 1) such that (cid:98)st 0 exponentially fast. and (cid:98)xn+t Cβ sn = A(w)t sn. Because ϱ (A(w)) < 1, Lemma G.6 applies to Proof. Unrolling the linear recursion gives (cid:98)st A(w): for any consistent operator norm there exist > 0 and β (0, 1) such that A(w)t β for all (cid:78). By submultiplicativity of the induced norm, For the scalar forecast, (cid:98)xn+t (cid:98)xn+t in t, establishing the claim. = 1 (cid:98)st e1 (cid:98)st (cid:98)st = e = A(w)t sn A(w)t sn β sn . 1 (cid:98)st . Let denote the dual norm of the chosen vector norm. Then . Since β (0, 1), the right-hand side decays exponentially β sn Thus, any stable linear model (including the Bayes predictor and any LSA fit) collapses to the mean under CoT; the only question is how quickly its error compounds. Bayes multistep error (ground truth model). Let ψ ψ2 equals the linear recursion with = ρ (no noise injected). The forecast error is classical: < . The hstep Bayes forecast (cid:98)x be the impulse response of the AR(p), i.e., ] x1:n = 1 and (cid:80) tk with ψ = (cid:69)[xn+h = (cid:80) k0 n+h ψ ϵ 0 MSE (h) := (cid:69)(cid:2)(xn+h (cid:98)x n+h )2(cid:3) = σ2 ϵ h1 (cid:88) k=0 ψ2 k. (14) (cid:80) ψ2 = Var(x k0 ) by Lemma G.7, and by Lemma G.8 the tail decays exponenHence MSE(h) σ2 ϵ tially: Var(x ) MSE (h) = σ2 ϵ (cid:88) ψ2 2σ2 ϵ 1 β 2 β 2h, for some > 0, β (0, 1). (15) kh For AR(1) this is exact: MSE(h) = σ2 ϵ (cid:80)h1 k= ρ2k = σ2(1 ρ2h), where σ2 = σ2 ϵ /(1 ρ2). Theorem G.3 (CoT collapse and compounding error). For any stable AR(p), (cid:98)x n+t 0, (cid:69)(cid:2)(xn+t (cid:98)x n+t )2(cid:3) Var(x ), with exponential tail (15). Thus CoT error accumulates to the process variance at an exponential rate determined by the spectrum of A(ρ). 47 LSA (or any alternative) is uniformly dominated at every horizon. Fix horizon and let (cid:98)x LSA n+h be the CoT forecast delivered by any trained onelayer LSA model (or an Llayer stack run in CoT; the argument is identical). Since CoT is noiseless, (cid:98)x LSA n+h is deterministic measurable function of the history x1:n. By the L2 projection property of the conditional expectation, (16) (17) (cid:69)(cid:2)(xn+h g(x1:n ))2(cid:3) = MSE (h) + (cid:69)(cid:2)( (cid:98)x n+h g(x1:n ))2(cid:3) g. Plugging = (cid:98)x LSA n+h yields the horizonwise dominance with strict inequality unless (cid:98)x LSA finitesample gap (Section F.2), equality fails generically already at = 1, and thus for all horizons. n+h almost surely. Because onelayer LSA has strict MSELSA(h) := (cid:69)(cid:2)(xn+h n+h coincides with (cid:98)x (cid:98)x LSA n+h )2(cid:3) MSE (h), Corollary G.4 (Earlier threshold crossing for LSA). Fix any τ (0, 1) and define the failure horizon Hτ(g) := inf(cid:8)h 1 : (cid:69)(cid:2)(xn+h (cid:98)x ) for every τ, with strict inequality for all τ on set of positive measure (whenever Then Hτ( (17) is strict at some h). In words: for any error threshold, LSA under CoT reaches the largeerror regime no later than the Bayes linear predictor. ))2(cid:3) τ Var(x (cid:98)x LSA) Hτ( (x1:n gh )(cid:9). Quantitative rates. Combining Lemma G.2 with the orthogonality identity (16) shows that Var(x ) MSELSA(h) Var(x ) MSE (h) 2σ2 ϵ 1 β 2 β 2h, for some > 0, β (0, 1). Hence whenever the left-hand side remains positive, it must also collapse to zero at least exponentially fast; if it turns negative (overshoot), Corollary G.4 guarantees that LSA in CoT still reaches the largeerror regime strictly earlier and more severely than linear regression. Remark G.5 (AR(1): closed forms and rapid compounding). For AR(1), MSE(h) = σ2(1 ρ2h) so that the residual to the variance decays like ρ2h. The halflife to reach 50% of the unconditional variance = log(1/2)/ log(ρ2); e.g., with ρ = 0.9 one already has MSE(5) 0.65 σ2 and MSE(10) is h1/2 0.88 σ2. This quantifies the rapid accumulation of CoT error even for the Bayes predictor; by (17), LSA CoT is uniformly worse at every horizon. Takeaways. (i) Any linear CoT rollout collapses to the mean (Lemma G.2), so its longhorizon RMSE saturates at the unconditional standard deviation of the process. (ii) The Bayes/linearregression forecast is horizonwise optimal (Theorem G.3 and (16)); any LSA CoT forecast is uniformly dominated at each horizon (17). (iii) Consequently, for any fixed error threshold, LSA reaches the largeerror regime at least as early as linear regression (Corollary G.4). (iv) Both approaches exhibit exponential convergence of the error to the variance, with rate governed by the spectral radius of the corresponding companion matrix; for AR(1) the entire trajectory is explicit. Auxiliary lemmas used in Appendix Lemma G.6 (Exponential Decay from Spectral Radius Bound). Let (cid:82)pp be square matrix with spectral radius strictly less than one, i.e., ϱ(A) < 1. Then for any consistent matrix norm , there exist constants > 0 and β (0, 1) such that At Cβ for all (cid:78). 48 Proof. By the Gelfand formula (see, e.g., [HJ94, Chapter 5]), we have ϱ(A) = lim At 1/t , for any sub-multiplicative (i.e., consistent) matrix norm . Thus, for any ε > 0 such that ϱ(A)+ε < 1, there exists t0 (cid:78) such that At (ϱ(A) + ε)t =: β , t0. Define := max{1, max 0t<t0 At β }. Then for all (cid:78), we have At Cβ as claimed. Lemma G.7 (Wold variance identity). For stable AR(p) with Wold expansion ϵ ϵ ), the unconditional variance satisfies i.i.d. (0, σ2 = (cid:80) ψ ϵ tk, where k0 Var(x ) = σ2 ϵ (cid:88) ψ2 k. Proof. From the Wold representation = (cid:80) k0 ψ ϵ tk, we have (cid:69)[x ] = 0 and Var(x ) = (cid:69)[x 2 ] = (cid:69) (cid:153) (cid:138)2 . ψ ϵ tk (cid:150) (cid:128)(cid:88) k0 Cross terms vanish because the ϵ σ2 ϵ (cid:80) ψ2 k. k0 tk are independent with mean zero, leaving (cid:80) ψ2 k0 (cid:69)[ϵ tk ] = Lemma G.8 (Exponential tail bound). Let A(ρ) be the AR(p) companion matrix with spectral radius ϱ(A(ρ)) < 1. Then the impulse response coefficients obey ψ Cβ for some > 0 and β (0, 1). Consequently, σ2 ϵ (cid:88) kh ψ2 2σ2 ϵ 1 β 2 β 2h. Proof. By state recursion, ψ 1 A(ρ)ke1. Because ϱ (A(ρ)) < 1, Lemma G.6 applies to A(ρ): for any consistent operator norm there exist > 0 and β (0, 1) such that A(ρ)t β for all (cid:78). Hence ψ = Cβ k. Thus, (cid:88) kh ψ2 (cid:88) kh 2β 2k = 2 1 β 2 β 2h, and multiplying by σ2 ϵ yields the claim. 49 DeGaussifying the Gap: Linear Stationary Processes Goal. We remove the Gaussian assumption and establish the same finitesample excessrisk gap for onelayer LSA under broad linearstationary model. The strict gap requires only independence of innovations and finite moments; with absolutely summable autocovariances we also recover the 1/n firstorder expansion. Model. Let {x } t(cid:90) be zeromean linear stationary process with Wold representation = (cid:88) k0 ψ ϵ tk, (cid:88) k0 ψ < , (18) } are i.i.d. with (cid:69)[ϵ where {ϵ (cid:69)[ϵ4 ] < and Var(ϵ2 All Hankel/masking notation follows Section F.2: ] = σ2 := (cid:69)[x t+h ) < . Let γ ] = 0, (cid:69)[ϵ2 ], so (cid:80) h(cid:90) γ < . ϵ > 0, and with symmetric distribution. We assume = 1 np (cid:88) m=1 (m) (m) , := np+1:n (cid:82)p, := xn+1, with the onelayer LSA predictor (cid:98)x LSA n+ = GAx, (cid:82)p+1, (cid:82)(p+1)p. As in Section F.2, introduce := (vech G) x, (cid:101)S := (cid:69)[Z ], (cid:101)r := (cid:69)[Z ], Γ := (cid:69)[x ]. Linear regression predictor. For consistency with the Hankel construction, we restrict attention to the last lags := xnp+1:n (cid:82)p as regression covariates. The best linear predictor of := xn+1 from is = + en+1, (cid:69)[x en+1 ] = 0, where (cid:82)p is the leastsquares coefficient vector and en+1 is the linear prediction error. We denote the corresponding fitted value by n+1 := (cid:98)x LR Lemma H.1 (Strict covariance of (vech G, x) without Gaussianity). Let := vech and := np+1:n. Under (18), Cov(cid:0)[ , ](cid:1) 0 for every finite n. x. = [ , ]. Suppose Var(vZ0 Proof. Let Z0 HankelGram sums from bottom (time n) upward. Collect coefficients of {x 2 to write ) = 0 for some nonzero v. Traverse the tableau of } n, xn1 xn, . . . , xnp xn, xn Z0 with U, V, measurable w.r.t. Fn1 := σ(ϵ 0 Write xn = ξ + ψ ϵ = + xn : 1). + 2 n, n, where ξ is Fn1-measurable and ϵ Fn1. By symmetry, Cov(ϵ n, ϵ2 ) = 0. Thus Z0 Var(v Fn1 ) = Var(V ψ + ψ2 ϵ2 0 Since both coefficients are strictly positive, this vanishes only if = = 0. Inductively repeating the elimination for xn1, xn2, . . . forces = 0, contradiction. Hence the covariance is strictly positive definite. 0 Var(ϵ2). ϵ + 2ψ4 ) = (V ψ Fn1 )2σ2 ϵ 0 50 Lemma H.2 (Kronecker lift remains strictly PD). With = (vech G) and = np+1:n, Σ := (cid:69) (cid:20)Z (cid:21) (cid:20)Z (cid:21) = (cid:21) (cid:20) (cid:101)S (cid:101)r (cid:101)r Γ 0. Proof. By Lemma H.1, Cov([g, x]) 0; hence the conditional covariance Cov(g x) = Cov(g) Cov(g, x)Γ 1 Cov(x, g) 0. For any nonzero = vec(U) (cid:82)qp and (cid:82)p, put := uZ + = (U + w). Then Var(Y ) (cid:69)[x Cov(g x) x] > 0 unless = 0; if = 0 then Var(Y ) = Var(w x) > 0 since Γ 0. Thus Σ 0. Theorem H.3 (Strict finitesample gap under linear stationarity). Under the model above, for every finite n, min b,A (cid:69) (cid:2)( (cid:98)x LSA n+ (cid:98)x LR n+1 )2(cid:3) = nw , 0. Proof. As in (5), the LSA risk relative to the regression predictor can be written L(η) = (cid:69) (cid:2)( Γ = )2(cid:3) (cid:98)x LR n+1 (cid:101)S η 2 η + η (cid:98)x LSA n+1 pw . (cid:101)r The minimizer is η = (cid:101)S1 (cid:101)r w, giving the optimal value (Γ (cid:101)r 1 (cid:101)S (cid:101)r)w = nw . By Lemma H.2, (cid:101)S 0, hence 0 by the Schur complement. (i) The gap above is defined relative to the best linear regression predictor (cid:98)x LR Remarks. n+1. (ii) If in addition the regression residual en+1 is independent of (G, x) (as in an exact AR(p) model with lags), then the same gap translates directly to strict excess risk gap relative to y. Cumulant identities for linear processes. Let κ (finite for the orders we use). For any t1, . . . , , := cumr (ϵ 0 ) denote the orderr cumulant of ϵ cum(cid:0)x t1 , . . . , (cid:1) = κ (cid:88) ψ u(cid:90) t1 ψ u. (19) This follows from multilinearity of cumulants and independence of {ϵ Lemma H.4 (Moment expansions via cumulants). Assume (cid:80) h(cid:90) γ finite). Let := vech(Γ ) and set := (uu) Γ p, := Γ p+1 < , (cid:69)ϵ p. For with fixed p, 6 < (so κ } (see, e.g., [Bri01, Theorem 2.3.2]) 4, κ 6 are (20) (cid:101)Sn = + 1 CS + o(1/n), (cid:101)rn = + 1 Cr + o(1/n), for finite matrices CS, Cr determined by {γ Proof. We work entrywise. Write indices i, j, k, ℓ {1, . . . , p+1}, s, {1, . . . , p}, and } and κ 4, κ 6. = xm+i1, = xm+ j1, = xm+k1, = xm+ℓ1, = xnp+s, = xnp+t . (cid:80)np Recall Gi = 1 m=1 xm+i1 xm+ j1. We analyze ( (cid:101)rn )(i j,s),t = (cid:69)[Gi xs ] = 1 np (cid:88) m=1 (cid:69)[ab ], (21) ((cid:101)Sn )(i j,s),(kℓ,t) = (cid:69)[Gi Gkℓ xs ] = 1 n2 np (cid:88) m,m=1 (cid:69)[ab cd ]. (22) (I) The rn expansion. By the momentcumulant formula (see Lemma H.6), for four variables, (cid:69)[abe ] = (cid:69)[ab](cid:69)[e ] + (cid:69)[ae](cid:69)[b ] + (cid:69)[a ](cid:69)[be] + cum(a, b, e, ). The pairwise terms equal γ (Γ ) st + γ(np+s)(m+i1)γ(np+t)(m+ j1) + γ(np+t)(m+i1)γ(np+s)(m+ j1). ji Summing over gives n γ (Γ ) st ji + 1 np (cid:88) k=1 (cid:0)γ k+si γ k+t + γ k+ti γ k+s (cid:1). < , Toeplitz summation (see Lemma F.16) implies that the convolutions are uniγ Because (cid:80) formly bounded and converge; thus γ (Γ ) st ji = γ (Γ ) st ji γ (Γ ) st , ji and 1 np (cid:88) k=1 (cid:128)γ k+si γ k+t + γ k+ti γ k+s (cid:138) = 1 (r,2) j,st + o(1/n), for some absolutely convergent constant (cid:88) (r,2) j,st := k=1 (cid:128)γ k+si γ k+t + γ k+ti γ k+s (cid:138) . For the fourthorder cumulant, (19) with = 4 yields cum(a, b, e, ) = κ 4 (cid:88) ψ u(cid:90) m+i1u ψ m+ j1u ψ np+su ψ np+tu. Summing over (equivalently = + 1) and using (cid:88) (cid:88) ψ iu+k ju+k su tu ψ ψ ψ ψ4 ℓ 1 < gives k1 1 np (cid:88) m=1 cum(a, b, e, ) = 1 (r,4) j,st + o(1/n) (r,4) with an absolutely convergent constant j,st pieces and reorganizing in tensor form yields := κ 4 (cid:80) (cid:80) u(cid:90) ψ k1 iu+k ψ ju+k ψ su ψ tu. Collecting and therefore (cid:101)rn Ip (II) The Sn expansion. For = (Lp+1 = (vec Γ rn p+1 ) rn = + 1 Cr ) Γ + 1 + o(1/n). (vec) + o(1/n), Then Sn = (cid:88) i, j,k,ℓ (cid:88) (cid:69)(cid:2)Gi Gkℓ xs (cid:3) (cid:128)(e ei es ) (eℓ ek et )(cid:138) . s,t (cid:69)(cid:2)Gi Gkℓ xs (cid:3) = 1 n2 np (cid:88) np (cid:88) m=1 m= (cid:69)[abcde ]. 52 Sixthorder moment decomposition. By the momentcumulant formula in Lemma H.6, (cid:69)[abcde ] = (cid:88) PM3 (cid:89) (cid:69)[uv] + (cid:88) (u,v)P πΠ 4,2 cum4 (cid:0)π(4)(cid:1) (cid:69)(cid:0)π(2)(cid:1) + cum6 (a, b, c, d, e, ), (23) where M3 is the set of the 15 perfect matchings of {a, b, c, d, e, }, Π 4,2 is the set of the 15 partitions into 4block and 2block, π(4) denotes the 4tuple in the 4block and π(2) the paired variables. Write γ := (cid:69)[x t+h ]. (a) Triplepairings. The three pairings that keep (e, ) together are = {(a, b), (c, d), (e, )}, P1 = {(a, c), (b, d), (e, )}, P2 = {(a, d), (b, c), (e, )}. The leading pairing P0 contributes 1 n2 (cid:88) m,m γ γℓk (Γ ) st = γ γℓk (Γ ) st ji ji + 1 (S,bd) j,kℓ;st + o(1/n), with (24) The two additional pairings P1, P2 yield, after the change of variable = m and Toeplitz summation (see Lemma F.16), ji = 2p γ γℓk (Γ ) st . (S,bd) j,kℓ;st 1 (cid:88) (cid:128)γ m,m ik+h γ jℓ+h + γ iℓ+h γ jk+h (cid:138)(Γ ) st = 1 (S,0) j,kℓ;st + o(1/n), where (S,0) j,kℓ;st := (cid:88) h(cid:90) (cid:128)γ ik+h γ jℓ+h + γ iℓ+h γ jk+h (cid:138) (Γ ) st (absolutely convergent). (25) Therefore, the total contribution of the three pairings with (e, ) paired is γ γℓk (Γ ) st ji (cid:128) + 1 (S,bd) j,kℓ;st + (S,0) j,kℓ;st (cid:138) + o(1/n). All other 12 pairings necessarily contain at least one crosspair between {a, b, c, d} and {e, }. After the change of variable := + 1 (or := + 1 as appropriate) and absolute summan times finite constant plus o(1/n). Collecting the 12 distinct bility of {γ crosspairings (those where or pairs with one of a, b, c, d) gives the explicit constant via Toeplitz summation (see Lemma F.16) }, each such term equals 1 (S,2) j,kℓ;st := (cid:88) q=1 (cid:148)γ si+q γ j+q γℓk + γ si+q γ tk γ jℓ+q + γ si+q γ tℓ γ jk+q + γ j+q γ ti+q + γ ti+q γ j+q + γ j+q γ si+q γℓk γℓk γℓk + γ j+q + γ + γ ti+q j+q γ tk γ sk γ sk γ γ γ iℓ+q jℓ+q iℓ+q + γ j+q + γ + γ ti+q j+q tℓ γ γ sℓ γ γ sℓ γ γ ik+q jk+q (cid:151) . ik+q Therefore the total contribution of triplepairings is γ γℓk (Γ ) st ji + 1 (S,bd) j,kℓ;st + 1 (S,0) j,kℓ;st + 1 (S,2) j,kℓ;st + o(1/n). (26) 53 (b) {4, 2} partitions. For linear processes (cid:80) (x t1 mulant satisfies cum4 (cid:80) = (cid:80) r(cid:90) ψ ψ r(cid:90) ψ , t3 t1 < . Define the absolutely convergent series ) = κ , t2 , ψ 4 ϵ tr t r with i.i.d. innovations, the fourth cu- (ϵ) and ψ , where κ = cum4 ψ 4 t3 t4 Sab (r) := (cid:88) q(cid:90) ψ q+i1r ψ q+ j1r , Scd (r) := (cid:88) q(cid:90) ψ q+k1r ψ q+ℓ1r . By stationarity and Toeplitz summation, only the three families of partitions in which the 4-block contains either {a, b} or {c, d} contribute at order 1/n; all other {4, 2} partitions are o(1/n). The nonvanishing 1/n constants are (S,4) j,kℓ;st := κ 4 (cid:88) (cid:148)γ r(cid:90) ts Sa (r) Scd (r) + γℓk Sa (r) ψ sr ψ tr + γ ji Scd (r) ψ sr ψ tr (cid:151) . (27) (c) Sixthorder cumulant. Using cum6 6 double sum over (m, m) reduces (by Toeplitz summation) to , . . . , t6 (x ) = κ (cid:80) (cid:81)6 u=1 r(cid:90) ψ tu with κ 6 = cum6 (ϵ), the 1 c (S,6) j,kℓ;st + o(1/n), (S,6) j,kℓ;st := κ (cid:88) r(cid:90) Sab (r) Scd (r) ψ sr ψ tr . (28) (d) Collecting the pieces. Combining (26), (27) and (28) in (23) yields (cid:69)(cid:2)Gi Gkℓ xs (cid:3) = γ γℓk (Γ ) st ji (cid:128) + 1 (S,bd) j,kℓ;st + (S,0) j,kℓ;st + (S,2) j,kℓ;st + (S,4) j,kℓ;st + (S,6) j,kℓ;st (cid:138) + o(1/n). Therefore with the explicit block (vec) := (cid:88) i, j,k,ℓ (cid:88) (cid:128) s,t = (cid:0)vec Γ Sn p+1 (cid:1)(cid:0)vec Γ p+1 (cid:1) Γ + 1 (vec) + o(1/n), (S,bd) j,kℓ;st + (S,0) j,kℓ;st + (S,2) j,kℓ;st + (S,4) j,kℓ;st + (S,6) j,kℓ;st (cid:138) (cid:128)(e ei es ) (eℓ ek et )(cid:138) . Finally, since (cid:101)Sn = (Lp+1 and CS ensure that all series above converge absolutely and justify the o(1/n) remainder. = + 1 ). Absolute summability of {γ = (Lp+1 (vec) ) Ip CS } and {ψ ), we obtain (cid:101)Sn ) Sn (Lp+1 (Lp+1 Ip +o(1/n), where = (uu)Γ 4, κ 6, }, and finiteness of κ Ip Ip Theorem H.5 (Order of the nonGaussian gap). Under Lemma H.4, let = [u/u, Q] and := Ip. Then the blockinverse expansion of Lemma F.13 applies verbatim, and = 1 0 given by the same closed form as in (9) after replacing the Gaussian CS, Cr by those from + o(1/n), 1 (cid:101)rn (cid:101)S (cid:101)r = Γ Bp with Bp Lemma H.4. Generically Bp 0. Remarks. pendence of innovations with finite fourth moment (and Var(ϵ2) > 0) suffices. (cid:80) (i) No Gaussianity is needed for strict PD and the positive Schurcomplement gap: inde- (ii) If in addition < and (cid:69)ϵ6 < , the exact 1/n order persists for general linear stationary processes; γ Gaussianity only simplifies the constants via Wick pairings. 54 Discussion on AR/MA/ARMA models. Stable AR, MA, and ARMA processes satisfy the assumptions above, so Theorems H.3 and H.5 apply directly. Nevertheless, caution is warranted in interpreting the result. For MA or ARMA models, the onestep prediction error en+1 still carries dependence on portions of the past beyond the last lags. This prevents direct characterization of the meansquared error gap between LSA and linear regression with respect to the true target xn+1. Hence the finitelag linear regression predictor (cid:98)x LR n+1 does not coincide with the globally optimal (infiniteorder) linear predictor. In particular, for MA models there may exist richer linear predictors that exploit the entire past more effectively. Our analysis should therefore be understood not as claim of global optimality across all linear predictors, but rather as an insight into the structural gap that persists even when comparing LSA against the natural plag linear regression benchmark. Auxiliary lemmas used in Appendix Lemma H.6 (Momentcumulant formula). For random variables 1, . . . , with finite moments up to order r, the joint moment can be expressed in terms of cumulants as (cid:150) (cid:89) (cid:69) (cid:153) i=1 = (cid:88) πPr (cid:89) Bπ cum(cid:0)X : B(cid:1), where Pr denotes the set of all partitions of {1, . . . , r}, and cum() denotes the joint cumulant."
        }
    ],
    "affiliations": [
        "Duke University",
        "University of Pennsylvania"
    ]
}