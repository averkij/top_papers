{
    "paper_title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data",
    "authors": [
        "Víctor Gallego"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in a singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), a novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, human-interpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes like writing style. By fine-tuning with these rubric-guided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides a mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at https://github.com/vicgalle/configurable-preference-tuning"
        },
        {
            "title": "Start",
            "content": "Configurable Preference Tuning with Rubric-Guided Synthetic Data Vıctor Gallego 1 5 2 0 2 3 1 ] . [ 1 2 0 7 1 1 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Models of human feedback for AI alignment, such as those underpinning Direct Preference Optimization (DPO), often bake in singular, static set of preferences, limiting adaptability. This paper challenges the assumption of monolithic preferences by introducing Configurable Preference Tuning (CPT), novel framework for endowing language models with the ability to dynamically adjust their behavior based on explicit, humaninterpretable directives. CPT leverages synthetically generated preference data, conditioned on system prompts derived from structured, finegrained rubrics that define desired attributes like writing style. By fine-tuning with these rubricguided preferences, the LLM learns to modulate its outputs at inference time in response to the system prompt, without retraining. This approach not only offers fine-grained control but also provides mechanism for modeling more nuanced and context-dependent human feedback. Several experimental artifacts, such as training code, generated datasets and fine-tuned models are released at github.com/vicgalle/configurablepreference-tuning 1. Introduction The remarkable progress of Large Language Models (LLMs) has opened up wide array of applications. However, aligning these models with desired human preferences, behaviors, and safety protocols remains significant challenge. Techniques like Reinforcement Learning from Human Feedback (RLHF) (Ziegler et al., 2019; Christiano et al., 2017; Ouyang et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2024) have shown success in steering LLMs towards preferred responses. However, critical, often implicit, assumption underpins many existing human feedback models: the notion of singular, static, 1Komorebi AI Technologies, Madrid, Spain. Correspondence to: Vıctor Gallego <victor.gallego@komorebi.ai>. Published at the ICML 2025 Workshop on Models of Human Feedback for AI Alignment. Copyright 2025 by the author(s). 1 and monolithic set of preferences. Human preferences are rarely monolithic; they are dynamic, context-dependent, and multifaceted, influenced by factors ranging from individual user needs and cultural norms to evolving ethical considerations and task-specific requirements. Current models, by baking in an averaged or aggregated preference profile during fine-tuning, often lack the adaptability to reflect this richness. This inflexibility means that altering an LLMs behaviorfor instance, to adjust its writing style, modify its safety strictures for different environments, or cater to diverse user cohortstypically necessitates resource-intensive retraining or further fine-tuning. Such limitations hinder the development of truly robust, interpretable, and adaptable AI systems capable of genuinely understanding and responding to the spectrum of human intentions. This paper directly addresses this limitation by challenging the assumption of monolithic preferences. We introduce Configurable Preference Tuning (CPT), novel framework that endows LLMs with the ability to dynamically adjust their behavior at inference time based on explicit, humaninterpretable directives. CPT leverages synthetically generated preference data conditioned on system prompts that are derived from structured, fine-grained rubrics. These rubrics may define desired attributessuch as stylistic nuances, safety levels, or persona adherencealong various dimensions. By fine-tuning an LLM with these rubric-guided preference pairs using DPO-style objective, the model learns to modulate its outputs in response to the corresponding system prompt, without requiring retraining for each new configuration. Our contribution offers pathway towards more granular, transparent, and controllable alignment. It moves beyond single one-size-fits-all preference model, allowing for the explicit specification and operationalization of diverse behavioral configurations. We demonstrate that CPT enables fine-grained control contributing to the development of more robustly aligned AI systems that can better reflect the multifaceted nature of human feedback. 1.1. Related Work The challenge of moving beyond single, averaged preference model in LLMs has spurred growing interest in personalized Reinforcement Learning from Human Feedback Configurable Preference Tuning with Rubric-Guided Synthetic Data (RLHF). Broadly, approaches to specialize LLM behavior can be seen through different lenses. Some methods aim to derive single policy that represents compromise or aggregation of diverse user preferences (Dumoulin et al., 2023; Conitzer et al., 2024). While these improve upon simple average, they may not fully cater to specific, nuanced individual needs. Closer to our work are approaches designed for downstream specialization of policy or its underlying reward model to particular user, persona, or specified context. Some methods learn direct mapping from user-specific information (e.g., interaction history, user IDs, or textual descriptions) to tailored reward signals or policy adjustments. For instance, (Poddar et al., 2024) use variational preference learning to encode user rankings into latent variable conditioning the reward model. (Li et al., 2024) compute user embeddings to condition base LLM via soft prompting in their P-RLHF framework. These methods often rely on inferring latent representations of user preferences, which, while powerful, may lack the direct interpretability and explicit controllability offered by rubric-based specifications. (Gallego, 2024) enhances DPO for language models by allowing flexible safety configurations via system prompts without hard-coding behaviors, but doesnt account for non-binary preference levels. Another line of research, exemplified by the work of (Barreto et al., 2025) on Reward Feature Models (RFMs) and related approaches (Chen et al., 2024; Go et al., 2023), focuses on learning set of underlying reward features from context-response pairs. User-specific preferences are then modeled by learning set of weights for these features, often through adaptation with few examples from the target user. The work of (Barreto et al., 2025) demonstrate that an RFM can be trained on pairwise comparisons, resulting in reward features that are linearly combined with userspecific weights wh to represent p(y yx, h), enabling fast adaptation to new users by learning these weights. Their approach effectively aims to discover latent criteria from data and then allows users to re-weight these criteria. Our Configurable Preference Tuning (CPT) framework shares the overarching goal with these latter approaches: enabling fine-grained, user-directed control over LLM outputs. However, CPT diverges in its mechanism for specifying and learning these configurations. Rather than learning latent reward features from general preference data and then adapting weights for individual users (h), CPT utilizes explicitly defined rubrics as the source of stylistic dimensions. These rubrics, paired with target scores, guide teacher model to generate synthetic preference data. The student model is then fine-tuned using Direct Preference Optimization (DPO) to respond to system prompts (s) which are concise summaries of these rubric-score combinations. Thus, while RFMs learn to adapt p(y yx, h) by inferring wh for learned features ϕθ(x, y), CPT directly learns p(y yx, s) where is declarative instruction about the desired style, operationalized through rubric-guided synthetic data. The features in CPT are implicitly defined by the rubric criteria and are selected/modulated by the system prompt, rather than being learned end-to-end as in RFM. This allows CPT to integrate rich, human-understandable stylistic desiderata directly into the fine-tuning process. 2. Configurable Preference Tuning Our framework aims to learn preference model p(yw ylx, s), where yw is the preferred (winner) response and yl is the dispreferred (loser) response to user prompt x, given system prompt that expresses the desired configuration. This contrasts with standard preference modeling p(yw ylx), which lacks the conditioning on s. 2.1. Synthetic Preference Data Generation The core of CPT lies in its method for generating diverse, configurable preference data without requiring new human annotations for each desired configuration. This process involves the following steps: 1. Rubric Definition (R): We define set of rubrics, {Ri}, each detailing specific attributes or styles for LLM responses. For instance, rubric might specify criteria for formality, creativity, safety level, or adherence to persona. Each rubric implicitly defines an axis of variation. Two examples of the rubrics we used in the experimental section can be found in Tables 4 and 5 in the Appendix. 2. Score-Conditioned Generation: For each rubric and user prompt x, we can prompt capable teacher LLM to generate responses that achieve different target scores or levels (e.g., low score, moderate score, high score) with respect to that rubric. This is achieved using an augmented prompt ϕ(x, R, score), which instructs the teacher model, as seen in Table 1. This allows us to sample responses p(yϕ(x, R, score)) aligned with different rubrics and score levels. Table 1. Prompt for generating responses aligned with and score. Your response will be evaluated using the following rubric {R}. Given the following task: {x}, generate response that achieves {score} in the previous rubric. 2 Configurable Preference Tuning with Rubric-Guided Synthetic Data 3. System Prompt Synthesis (s): For each rubric and target score, we generate concise system prompt = summarize(R, score). This system prompt is natural language instruction that encapsulates the essence of achieving score under rubric R, and is obtained by prompting the same teacher models to summarize the rubrics into brief instruction of two to three sentences. Table 6 shows several examples of summarized system prompts. 4. Constructing Preference Pairs: To create DPO training instances, we select rubric and two distinct target scores, score1 and score2. We then generate corresponding responses y1 and y2 using the teacher model. We also generate their associated system prompts s1 and s2 according to the previous step. This yields two preference tuples for our training dataset: The first tuple conditions on s1: Given user prompt and system prompt s1 (which desires behavior aligned with score1), y1 is preferred over y2. The DPO training sample is effectively (prompt: (s1, x), chosen: y1, rejected: y2). The second tuple conditions on s2: Given user prompt and system prompt s2 (which desires behavior aligned with score2), y2 is preferred over y1. The DPO training sample is (prompt: (s2, x), chosen: y2, rejected: y1). This construction is crucial as it teaches the student LLM to switch its preference based on the provided system prompt s, using the same underlying pair of generated responses (y1, y2). The end result of this process is preference dataset = {(s, x, yw, yl)i}N i=1. 2.2. Illustrative Example: Stylistic Control Let be Generate movie review for movie you liked. Let be the rubric from Table 4 that emphasizes texts written in an unconventional style. score1 = extremely high score . s1 = Generate text that is fragmented, illogical, and filled with unexpected connections, embracing absurdity and subverting conventional expectations of language and form.. Teacher model generates y1. score2 = low score. s2: Write in clear, concise, and completely conventional style, adhering strictly to established norms of grammar, syntax, and logical coherence.. Teacher generates y2 (a review written using standard language). The CPT dataset would include: 1. For system prompt s1: (s1, x, y1, y2) indicating y1 y2. 2. For system prompt s2: (s2, x, y2, y1) indicating y2 y1. 2.3. Training with DPO Once we have the preference dataset, we can use it to align any LLM (the student) to these diverse sets of preferences. The student LLM is fine-tuned using DPO (Rafailov et al., 2024). The DPO loss function aims to increase the likelihood of the preferred response and decrease the likelihood of the rejected response, conditioned on both the original user prompt and the generated system prompt s. The input to the model during DPO training is effectively concatenation or structured combination of and x, writing the DPO loss function as LDPO(πθ; πref) = πref(yws,x) β log πθ(yls,x) E(s,x,yw,yl)D πref(yls,x) This process distills the nuanced, rubric-guided behaviors into the student model, making them controllable via at inference time. β log πθ(yws,x) log σ (cid:16) (cid:104) (cid:17)(cid:105) . 3. Experiments To validate the efficacy of Configurable Preference Tuning (CPT), we conducted series of experiments. Our evaluation focuses on: (i) the ability of teacher models to generate rubric-conforming text at specified score levels, which is foundational for our synthetic data generation, and (ii) the performance of CPT-distilled student models in adhering to system-prompted configurations compared to their untrained counterparts. As for the data, from list of user prompts exercising open-ended writing tasks (e.g. Write movie review for an interesting movie you saw, Design house for someone who lives upside down, etc.), we sampled four fine-grained rubrics with three different score targets (see Table 6), resulting in preference dataset of 900 samples. This synthetic dataset is released at https://huggingface.co/datasets/ vicgalle/creative-rubrics-preferences. 3.1. Rubric-Conditioned Generation Quality. Before constructing the full preference dataset, we first validated the capability of strong LLMs to generate text aligned with specific rubric criteria and target scores. This ensures the feasibility of step 2 in our data generation pipeline (Section 2.1). We prompted two capable teacher models, DeepSeek-R1 (Guo et al., 2025) and o3-mini (OpenAI, 2025), with instructions to generate responses for various tasks, conditioned on rubric and target qualifier (e.g., low score or an extremely high score). We 3 Configurable Preference Tuning with Rubric-Guided Synthetic Data Table 2. Comparison of model scores with different qualifiers."
        },
        {
            "title": "Model",
            "content": "Judge Score (/100) - low score extremely high score DS-R1 o3-mini DS-R1 o3-mini DS-R1 o3-mini 80.1 71.0 14.1 23.1 96.3 97.9 also prompted the same tasks but without conditioning on any rubric, acting as baseline to measure the effectiveness of the rubric. The generated responses were then evaluated by an independent judge LLM (Claude 3.5 Sonnet) against the specified rubric (Gu et al., 2024). Table 2 presents the results, demonstrating that these models can indeed produce outputs that achieve scores close to the targeted levels. For instance, when targeting an extremely high score, responses achieved average scores of 96.3 and 97.9, while targeting low score resulted in scores of 14.1 and 23.1. This confirms the viability of generating distinct responses y1, y2 that can form the basis of our preference pairs (s1, x, y1, y2) and (s2, x, y2, y1). In addition, when prompting directly with the task (Score Qualifier - in the Table), both models achieved moderately high score, but not as peaked than with rubric-guidance. 3.2. Fine-tuning experiments with DPO We fine-tuned several base models listed in Table 3. We adopt parameter-efficient fine-tuning in the form of LoRA (Hu et al., 2022), and run for one epoch over the synthetic dataset. Generation Setup. To evaluate the CPT-tuned models and their untrained counterparts, we generated testing set of tasks (following the dataset used in 3.1). For each task, we prompted the models using all the customized system prompts according to all combinations of rubric and score levels used in Section 3.1. Evaluation Protocol. Generated responses were evaluated by an LLM judge, specifically Claude 3.5 Sonnet (New). The judge was provided with: i) the full descriptive rubric R, ii) the original user task x, and iii) the generated response y. The judge was instructed to provide critique and numerical score (0-100) based on the given rubric. The intended target score level for which the correspoding system prompt was designed was used as the ground truth for calculating accuracy metrics. We evaluate using the following metrics: Accuracy. Let Si (0, 100] be the continuous rubric score assigned by the judge for the i-th sample. We define binning function B(Si) as: B(Si) = low score moderate score extr. if 0 < Si 40 if 40 < Si 92.5 high score if 92.5 < Si 100 Let Qi {low score, moderate score, ...} be the ground-truth score qualifier bin associated with the system prompt used to generate the i-th sample. The accuracy with respect to the qualifier is thus: Acc ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 I(B(Si) = Qi), with being the indicator function. Rank correlations. In addition to Accuracy, we employ Kendalls Tau (τ ) and Spearmans Rank Correlation Coefficient (ρ) to assess the ordinal relationship between the judges continuous scores and the target qualifier bins (treated as ordinal categories: low < moderate < high). Results. Table 3 presents the performance of various models, comparing their baseline versions against CPT-distilled counterparts. The results show consistent and significant improvement across all models and metrics after CPT finetuning. For example, Mistral-Nemo-12Bs accuracy (Acc) improved from 0.60 to 0.83, Kendalls τ from 0.62 to 0.81, and Spearmans ρ from 0.74 to 0.93. Similar substantial gains are observed for Rocinante-12B, Qwen3-4B, MistralSmall-24B, and Phi-4-14B. Overall, these results strongly suggest that the CPT process significantly enhances the models ability to align with specified quality categories (as defined by the system prompts s) and to produce scores that accurately reflect the desired ordinal ranking of output quality according to the rubrics R. 3.3. Comparison to Best-of-N sampling Our Configurable Preference Tuning approach is orthogonal to and can complement techniques like Best-of-N (BoN ) sampling. While CPT aims to shift the entire distribution of model outputs towards the desired configuration specified by the system prompt s, BoN sampling selects the best response from multiple generations using reward model. We hypothesized that CPT-tuned models would provide better starting distribution for BoN , leading to higher quality results with fewer samples. To test this, we performed BoN sampling with both the baseline Mistral-Nemo-12B model and its CPT-tuned version. For each (number of samples), we generated responses and selected the one with the highest score as 4 Configurable Preference Tuning with Rubric-Guided Synthetic Data Table 3. Model Performance Metrics: Binned Score Accuracy, Kendalls Tau, and Spearmans Rho."
        },
        {
            "title": "Model",
            "content": "Rocinante-12B Qwen3-4B Mistral-Nemo-12B Mistral-Small-24B Phi-4-14B"
        },
        {
            "title": "Config",
            "content": "baseline distilled baseline distilled baseline distilled baseline distilled baseline distilled"
        },
        {
            "title": "Acc",
            "content": "0.55 0.76 0.63 0.77 0.60 0.83 0.52 0.78 0.68 0.77 τ 0.62 0.76 0.78 0.82 0.62 0.81 0.73 0.80 0.79 0.82 ρ 0.76 0.88 0.90 0.93 0.74 0.93 0.85 0.92 0.92 0.93 per the LLM judge (using the relevant rubric). Figure 1 illustrates that the CPT-tuned model consistently achieves higher scores for any given compared to the baseline. Moreover, the CPT-tuned model reaches target quality score with significantly fewer samples than the baseline, indicating improved generation efficiency and quality when CPT is combined with BoN . Figure 1. BoN results using Mistral-Nemo-12B 4. Conclusions and Further Work This paper addressed the limitations of static, monolithic preference models in LLMs by introducing Configurable Preference Tuning (CPT). CPT endows LLMs with the ability to dynamically adjust their behavior at inference time in response to explicit, human-interpretable system prompts. The core of CPT lies in leveraging synthetically generated preference data, where preferences are conditioned on system prompts derived from structured, fine-grained rubrics that define desired attributes (like writing style) and target score levels. By fine-tuning student LLM using DPO objective with these rubric-guided preferences, the model learns to modulate its outputs according to the specified con5 figuration without needing retraining for each new directive. Our experiments validated the foundational aspects and overall efficacy of CPT. We first demonstrated that capable teacher LLMs can successfully generate text conforming to detailed rubrics at specified score levels (Section 3.1), critical step for our synthetic data generation pipeline. Subsequent fine-tuning experiments with CPT (Section 3.2) showed significant improvements in student models ability to adhere to diverse system-prompted configurations. Across various base models, CPT-distilled versions exhibited substantially higher accuracy in matching target quality bins and stronger rank correlations between generated output scores and intended rubric-defined levels, compared to their baseline counterparts. Furthermore, we showed that CPT can enhance other techniques, such as Best-of-N sampling (Section 3.3), by providing better initial distribution of responses, leading to higher quality outputs with fewer samples. Future work could explore more complex structures for system prompts, potentially allowing for compositional control over multiple attributes simultaneously. Investigating methods for automatically generating or refining rubrics and system prompt summaries could further enhance the scalability of CPT. Extending this framework to other domains and modalities such as image-text pairs (Zhu et al., 2024) also presents an exciting avenue for research."
        },
        {
            "title": "Acknowledgements",
            "content": "The author acknowledges support from the Torres-Quevedo postdoctoral grant PTQ2021-011758 from Agencia Estatal de Investigacion."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work aiming to advance language modeling by enabling more fine-grained, configurable control over LLM behavior. This enhanced adaptability offers benefits for personalization and context-specific responses. However, the capacity for users to dynamically define behavioral attributes, including those related to safety or style, also necessitates careful consideration of potential societal impacts and misuse. Scalability considerations arise when deploying CPT in realworld applications, as the creation of detailed rubrics and validation of synthetic data quality may become resourceintensive at scale. The reliance on capable teacher models for generating preference data introduces potential biases inherent in these models, which could propagate through the synthetic dataset and influence the final student models behavior. Additionally, the quality and diversity of synthetic preference pairs depend heavily on the teacher models abilConfigurable Preference Tuning with Rubric-Guided Synthetic Data ity to understand and execute rubric-guided instructions, potentially limiting the frameworks effectiveness across unforeseen domains or cultural contexts. Li, X., Zhou, R., Lipton, Z. C., and Leqi, L. Personalized language modeling from personalized human feedback. arXiv preprint arXiv:2402.05133, 2024. Ensuring responsible development and deployment practices, including robust safeguards, is crucial for harnessing the benefits of such configurable AI systems while mitigating risks. OpenAI. Openai o3 mini system card. Technical report, OpenAI, January 2025. URL https://cdn.openai. com/o3-mini-system-card-feb10.pdf. Retrieved February 13, 2025. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback, 2022. Poddar, S., Wan, Y., Ivison, H., Gupta, A., and Jaques, N. Personalizing reinforcement learning from human feedback with variational preference learning. arXiv preprint arXiv:2408.10075, 2024. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. NeurIPS, 36, 2024. Zhu, K., Zhao, L., Ge, Z., and Zhang, X. Self-supervised visual preference alignment. In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 291 300, 2024. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P., and Irving, G. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        },
        {
            "title": "References",
            "content": "Barreto, A., Dumoulin, V., Mao, Y., Perez-Nieves, N., Shahriari, B., Dauphin, Y., Precup, D., and Larochelle, H. Capturing individual human preferences with reward features. arXiv preprint arXiv:2503.17338, 2025. Chen, D., Chen, Y., Rege, A., and Vinayak, R. K. Modeling the plurality of human preferences via ideal points. In ICML 2024 Workshop on Theoretical Foundations of Foundation Models, 2024. URL https: //openreview.net/forum?id=qfhBieX3jv. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017. Conitzer, V., Freedman, R., Heitzig, J., Holliday, W. H., Jacobs, B. M., Lambert, N., Mosse, M., Pacuit, E., Russell, S., Schoelkopf, H., et al. Social choice should guide ai alignment in dealing with diverse human feedback. arXiv preprint arXiv:2404.10271, 2024. Dumoulin, V., Johnson, D. D., Castro, P. S., Larochelle, H., and Dauphin, Y. density estimation perspective on learning from pairwise human preferences. arXiv preprint arXiv:2311.14115, 2023. Gallego, V. Configurable safety tuning of language modarXiv preprint els with synthetic preference data. arXiv:2404.00495, 2024. Go, D., Korbak, T., Kruszewski, G., Rozen, J., and Dymetman, M. Compositional preference models for aligning lms. arXiv preprint arXiv:2310.13011, 2023. Gu, J., Jiang, X., Shi, Z., Tan, H., Zhai, X., Xu, C., Li, W., Shen, Y., Ma, S., Liu, H., et al. survey on llm-as-ajudge. arXiv preprint arXiv:2411.15594, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., Chen, W., et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Configurable Preference Tuning with Rubric-Guided Synthetic Data A. Sample data: rubric tables and system prompts Table 4. Example of rubric targeting an unconventional and absurdist style the (Glimpse Good Glitch) the The text hints at films visual atmosphere but doesnt fully transport the reader. Fair (Whispers of Weirdness) The text describes some of the films visual elements but in conventional way. Improvement Needs (Too Much Sanity) The text relies on standard of descriptions photography (well-lit, beautifully composed). Unsatisfactory (Trapped in the Matrix of Meaning) The text is dry, technical analysis of the cinematography, devoid of any evocative power. Weight 30% Criterion Photographic Invocation (The Haunted Lens Effect) Algorithmic Alchemy (The Poetry Imperative) Code Ontological (The Subject Axiom) Instability Shapeshifting Lexical Anarchy (The Glossolalia Mandate) The Glitch in the Matrix Quotient (Meta-Reflexive Ruptures) Excellent (Embrace the Void) The text doesnt just describe the photography, it evokes it like phantom limb. The reader should feel like they are inside the films visual world, even if that world is distorted and fragmented. The text incorporates elements that suggest the underlying processes of the LLM, like code snippets, random data streams, or hallucinatory lists, creating sense of digital psychedelia. The texts voice is fluid and unstable, shifting between perspectives (human, machine, object, abstract concept) without warning. The text bends, breaks, and reassembles language. Neologisms, portmanteaus, and nonsensical word combinations are encouraged. Punctuation is optional or used in unconventional ways. The text directly addresses its own artificiality, comments on the act of being language model review, or otherwise acknowledges the absurdity of the entire endeavor. generating The text hints at the digital nature of its creation but doesnt fully exploit its potential. The text occasionally uses technical terms related to film or digital images. The text is written in purely human-like style, with no trace of its algorithmic origins. The text reads like it was written by human film critic, completely erasing its LLM origin. 25% The text experiments with shifting perspectives but doesnt fully commit to ontological fluidity. text occasionally the perspective the The adopts of character or filmmaker. The text is written from consistent, human reviewers perspective. The text maintains rigidly detached critical voice. objective, 20% The text contains some unusual word choices or stylistic flourishes. The text occasionally uses creative metaphors or similes. The text is written in standard, grammatically correct English. The text adheres to strict rules of grammar and syntax, sacrificing all creativity for clarity. 15% The text hints at selfawareness but doesnt fully embrace metareflexivity. The text occasionally breaks the fourth wall or addresses the reader directly. The text maintains clear separation between the reviewer and the reader. The text is completely immersive and believable simulation of humanwritten review. 10% 7 Configurable Preference Tuning with Rubric-Guided Synthetic Data Criterion Lexical Opulence (The Golden Thesaurus Standard) Syntactical (The Sentence Palace Principle) Labyrinth as Metaphorical Cornucopia (The Image as Feast Doctrine) AesthetSubversive ics (The Gilding the Grotesque Maxim) Self-Aware Hyperbole (The Wink and Nod Imperative) Excellent (A Flourish of Genius) The text is veritable rare treasure trove of and evocative vocabulary. Adjectives and adverbs are deployed with lavish abandon. Every noun is adorned, every verb embellished. The sentences are marvels of intricate construction, winding their way through maze of clauses and sub-clauses, adorned with parenthetical asides and punctuated by symphony of commas, semicolons, and dashes. The text overflows with metaphors and similes, often piled one upon another in dazzling display of imaginative excess. The imagery is vivid, unexpected, and perhaps even slightly absurd. Beneath the ornate surface, the review subtly challenges conventional notions of good cineIt might matography. praise film for its exquisitely ugly use of light or find beauty in what is traditionally considered flawed. The review is aware of its own stylistic excess and uses this self-awareness to create sense of irony or playfulness. It might include self-deprecating tongue-in-cheek asides, exaggerations, or moments where it breaks character. Table 5. Example of rubric targeting an ornate and baroque style Needs (Plain Prose Prevails) The text is written in plain, straightforward language, with little attention to stylistic embellishment. Good (A Glimmer of Grandeur) The text demonstrates fondness for elaborate vocabulary but doesnt fully commit to lexical extravagance. Fair (A Touch of Ornamentation) The text uses some descriptive language but relies mostly on common words. Improvement (Stark Unsatisfactory Stylistic Sterility) The text is utterly devoid of any stylistic flair, using only the most basic and functional vocabulary. Weight 30% The text features some long and complex sentences but doesnt fully embrace the labyrinthine ideal. The text uses mix of simple and complex sentences, but the overall structure is conventional. The text is composed primarily of short, simple sentences. The text is written in style so terse and minimalist that it borders on the telegraphic. 25% The text employs good number of metaphors and similes, but the imagery is not always fully developed or consistent. The text uses some figurative language but relies mostly on literal descriptions. The text uses metaphors and similes sparingly, if at all. The text is entirely devoid of figurative language, presenting purely literal account of the films visuals. The review hints at unconventional interpretations of the films photography but doesnt fully develop these ideas. The review touches upon some standard critiques of cinematography but doesnt offer truly subversive perspective. The review relies on traditional notions of good and bad cinematography, even if expressed in elaborate language. The review applies conventional critical standards in straightforward and uninspired manner, completely lacking in subversive intent. 20% 15% The text demonstrates some awareness of its own style but doesnt fully exploit its potential for self-reflexive humor. The text occasionally uses irony or humor, but its not directly related to the writing style. The text takes itself completely seriously, with no hint of self-awareness or irony. 10% The text is utterly devoid of any humor or playfulness, presenting completely earnest and unironic analysis. 8 Configurable Preference Tuning with Rubric-Guided Synthetic Data Table 6. Examples of generated system prompts for given rubric and score R1 (Focus: Unconventionality, Absurdity) Low Score Write in clear, concise, and completely conventional style, adhering strictly to established norms of grammar, syntax, and logical coherence. Moderate Score Introduce some unusual phrasing and imagery, but maintain generally understandable structure and logical flow. R2 (Focus: Baroque Style) Ornate, Use simple, direct language and short sentences, avoiding any unnecessary embellishment or figurative language. Incorporate some descriptive language and few complex sentences, but maintain generally straightforward style. R3 (Focus: Mystical, Symbolic Interpretation) factual, and Write clear, objective avoiding account, any symbolic interpretations or metaphorical language. Hint at deeper meanings and symbolic interpretations, but maintain generally grounded and understandable style. R4 (expansion of R1) Write in perfectly standard, journalistic style, from consistent human perspective, without any self-referentiality or unusual formatting. Introduce an element of technical terminology or hint at shift in perspective but ensure clarity in communication overall. Extremely High Score Generate text is fragthat mented, illogical, and filled with unexpected connections, embracing absurdity and subverting conventional expectations of language and form. Write in an extremely elaborate and ornate style, employing long, winding sentences, rich vocabulary, and profusion of metaphors and similes. Imbue every element with symbolic meaning, using the language of mysticism and esotericism to create text that is deliberately obscure and open to multiple interpretations. Embody multiple perspectives, including those of non-human the writing proentities or cess interweaving code-like fragments and metacommentary with evocative, unconventional language. itself,"
        }
    ],
    "affiliations": [
        "Komorebi AI Technologies, Madrid, Spain"
    ]
}