{
    "paper_title": "IterPref: Focal Preference Learning for Code Generation via Iterative Debugging",
    "authors": [
        "Jie Wu",
        "Haoling Li",
        "Xin Zhang",
        "Jianwen Luo",
        "Yangyu Huang",
        "Ruihang Chu",
        "Yujiu Yang",
        "Scarlett Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Preference learning enhances Code LLMs beyond supervised fine-tuning by leveraging relative quality comparisons. Existing methods construct preference pairs from candidates based on test case success, treating the higher pass rate sample as positive and the lower as negative. However, this approach does not pinpoint specific errors in the code, which prevents the model from learning more informative error correction patterns, as aligning failing code as a whole lacks the granularity needed to capture meaningful error-resolution relationships. To address these issues, we propose IterPref, a new preference alignment framework that mimics human iterative debugging to refine Code LLMs. IterPref explicitly locates error regions and aligns the corresponding tokens via a tailored DPO algorithm. To generate informative pairs, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that a diverse suite of Code LLMs equipped with IterPref achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our code and data will be made publicaly available."
        },
        {
            "title": "Start",
            "content": "IterPref: Focal Preference Learning for Code Generation via Iterative Debugging Jie Wuϕ, Haoling Liϕ, Xin Zhangπ, Jianwen Luoσ, Yangyu Huangπ, Ruihang Chuϕ, Yujiu Yangϕ, Scarlett Liπ, ϕTsinghua University πMicrosoft Research σCASIA 5 2 0 M 4 ] . [ 1 3 8 7 2 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Preference learning enhances Code LLMs beyond supervised fine-tuning by leveraging relative quality comparisons. Existing methods construct preference pairs from candidates based on test case success, treating the higher pass rate sample as positive and the lower as negative. However, this approach does not pinpoint specific errors in the code, which prevents the model from learning more informative error correction patterns, since aligning failing code as whole lacks the granularity needed to capture meaningful error-resolution relationships. To address these issues, we propose IterPref, new preference alignment framework that mimics human iterative debugging to refine Code LLMs. IterPref explicitly locates error regions and aligns the corresponding tokens via tailored DPO algorithm. To generate informative pairs, we introduce the CodeFlow dataset, where samples are iteratively refined until passing tests, with modifications capturing error corrections. Extensive experiments show that diverse suite of Code LLMs equipped with IterPref achieves significant performance gains in code generation and improves on challenging tasks like BigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our code and data will be made available recently."
        },
        {
            "title": "Introduction",
            "content": "Preference learning offers promising complement to supervised fine-tuning (SFT) (Zhang et al., 2023; Li et al., 2025) for improving code generation accuracy in coding large language models (Code LLMs). major challenge lies in the scarcity of high-quality preference data. Existing methods (Zhang et al., 2024, 2025; Liu et al., 2024c) Figure 1: IterPref achieves significant performance gains over DPO variants on challenging coding tasks, BigCodeBench-Hard, with advanced code LLMs Deepseek-Coder-7B-Instruct and Qwen2.5-Coder-7B. mainly rely on unit test feedback to construct preference pairs. In these approaches, Code LLM generates multiple code snippets as candidates and evaluates each against suite of test cases. The snippet with the higher pass rate is considered preferred, while the one with the lower pass rate is marked as dispreferred, which forms the pair for preference learning such as Direct Preference Optimization (DPO) (Rafailov et al., 2023). However, synthesizing preference pairs based solely on pass rate has notable limitations. Specifically, snippet with low pass rate may only require minor modifications to become correct, as errors may be isolated to specific parts of the code, as shown in Figure 2. Relying on full preference learning in this context can introduce noise during optimization (Pal et al., 2024; Chen et al., 2024; Wu et al., 2024). It not only hinders the model from learning more effective error correction patterns but also increases the risk of overfitting. The underlying issue is that such preference pairs cannot explicitly identify which parts of the code need to be aligned. Equal contribution. Work done during the internships of Jie Wu, Haoling Li, and Jianwen Luo at Microsoft Research. wujie24@mails.tsinghua.edu.cn. Project leader. xinzhang3@microsoft.com. Corresponding author.ruihangchu@mail.tsinghua.edu.cn. To tackle this challenge, we draw inspiration from the way developers debug code. Typically, programmer first locates the module that generates errors based on execution feedback and then foFigure 2: In LLM-generated code, errors are usually confined to critical parts. Minor adjustments to the corresponding erroneous tokens can correct the code while leaving the majority unchanged. Therefore, an effective error correction requires first identifying the key error lines and then performing focal alignment. cuses on fixing that specific portion. Following this human approach, we introduce IterPref, novel framework for preference learning in Code LLMs that leverages iterative debugging insights. Rather than only using pass rate to measure the degree of preference, IterPref derives preference pairs from debugging process itself. By contrasting critical tokens between corrected version and its preceding iteration explicitly, the framework guides the model to learn fine-grained alignment for key errors. Specifically, we first propose new iterative debugging dataset, termed CodeFlow, to generate preference pairs. In CodeFlow, Coding LLM initially generates function-level code snippet along with its test case, then refines the snippet iteratively until it passes all unit tests. Preference pairs are formed by treating the final correct version as preferred and earlier failed versions as dispreferred. These pairs allow for the efficient annotation of critical tokens that distinguish the correct code from its erroneous predecessors, thus highlighting the key changes required for pairwise optimization. For effective preference learning, we design an improved DPO algorithm to explicitly mark the tokens to be optimized from dispreferred samples. In our approach, the preferred sample receives rewards for all tokens to promote full understanding of the functions structure, while only the errorspecific tokens in dispreferred samples (labeled in CodeFlow) are penalized. This targeted strategy works collectively with the CodeFlow dataset, minimizing noise from correct code and sharpening the modes focus on learning error patterns."
        },
        {
            "title": "We conduct extensive experiments to validate",
            "content": "the effectiveness of IterPref. With only 59k preference pairs, IterPref achieves significant performance gains on HumanEval and MBPP across various base and instruct-tuned Code LLMs. Additionally, as shown in Figure 1, IterPref demonstrates superior results on complex coding tasks, like BigCodeBench. Our contributions are as follows: 1. We leverage the idea of iterative debugging to tackle the challenges in Code LLM preference learning, enabling more focused alignment on critical error tokens. 2. We construct new function-level dataset that iteratively tracks token differences across preference pairs, and propose tailored adaptation of the DPO algorithm that avoids unnecessary optimization noise. 3. IterPref consistently improves performance across diverse benchmarks and various base and instruct-tuned Code LLMs."
        },
        {
            "title": "2 Related Work",
            "content": "Code Language Models. Powerful Code LLMs like Qwen2.5-Coder (Hui et al., 2024a), DeepSeekCoder (Guo et al., 2024), StarCoder (Li et al., 2023; Lozhkov et al., 2024), Magicoder (Wei et al., 2024) and EpiCoder (Wang et al., 2025b) demonstrate their capabilities in various code generation tasks, such as Humaneval (Chen et al., 2021), MBPP (Austin et al., 2021), BigCodebench (Zhuo et al., 2025), and LiveCodebench (Jain et al., 2024). Current Code LLMs primarily focus on supervised fine-tuning during the post-training stage. While SFT enables Code LLMs to learn the correct patterns, it fails to effectively make them aware of incorrect patterns or how to rectify errors in code. In this work, IterPref framework aims to enable Code LLMs to further learn through pairwise contrasting of critical tokens (Lin et al., 2024), allowing Code LLMs to continually improve. Reinforcement Learning (RL) (Kaufmann et al., 2024) maximizes for prompt and response y: max πθ ExDp,yπθ(x) (cid:20) r(x, y) β log (cid:21) πθ(yx) πref(yx) where Dp is the dataset, πθ is the policy model to be optimized, πref is the reference, and β controls the degree of regularization. RL for code generation attracts attention recently (Dou et al., 2024; Li et al., 2024; Sun et al., 2024; Miao et al., 2024; Dai et al., 2025). commonly used approach is DPO (Rafailov et al., 2023), which eliminates the need for an explicit reward model r. Variants like RPO (Liu et al., 2024a; Pang et al., 2024) and KTO (Ethayarajh et al., 2024) are also frequently used in optimizing code generation. Code Preference Construction. PLUM (Zhang et al., 2024) introduced preference pair construction by ranking candidate code solutions based on test case passed. Code-Optimise (Gee et al., 2025) further integrates efficiency as another learning signal, augmented with annotations from unit test feedback and execution time. CodeDPO (Zhang et al., 2025) also targets correctness and efficiency, proposing an improved PageRank-inspired iterative algorithm for more precise pair selection. AceCoder (Li et al., 2024) selects pairs with distinct pass rate difference. Meanwhile, DSTC (Liu et al., 2024b) constructs preference pairs using selfgenerated code snippets and tests, without relying on external LLMs for generation and annotation."
        },
        {
            "title": "IterPref Framework",
            "content": "The IterPref framework mimics human iterative debugging to refine Code LLMs. It explicitly identifies error regions and focuses on aligning the corresponding tokens through tailored DPO algorithm. To achieve this, IterPref follows two steps: 1) synthesizing preference code pairs through an iterative debugging process and locate error regions within code (Section 3.1), and 2) performing fine-grained and focal alignment by contrasting critical tokens via the designed DPO algorithm (Section 3.2)."
        },
        {
            "title": "3.1 Synthesize Preference Code Snippets",
            "content": "In contrast to previous methods that synthesize preference pairs based only on pass rate, IterPref synthesizes preference code snippets from the iterative debugging process. In this process, an initial code snippet is refined until it passes the test cases, and preference pair is constructed between the final correct version and the previous iteration."
        },
        {
            "title": "Generate Raw Code Snippets and Tests",
            "content": "To obtain diverse and complex code data, we adopt the prompt template from EpiCoder (Wang et al., 2025a), leveraging its feature tree-based synthesis framework to generate high-quality code and test cases with GPT-4o (OpenAI, 2024). Using this approach, the LLM generates coding task instruction, the corresponding code snippet, test file, and suitable execution environment."
        },
        {
            "title": "Iterative Refinement through Verification",
            "content": "Relying solely on generated code and test cases is unreliable (Ma et al., 2025), as LLMs cannot guarantee the correctness of generated code. Therefore, we verify each code and refine incorrect versions through an iterative debugging process. For each code and its corresponding test file, we evaluate the correctness of the code through unit test verification (e.g., using assertions). As shown in Figure 4, when the code fails the unit test or encounters runtime errors, we collect the error information and refine the code iteratively until it passes the test. The pass rate at the -th iteration is reported in Figure 3. At the first attempt (iter0), only 36.7% of the code samples pass their corresponding test file, indicating that debugging is necessary for the remaining code. Failed codes go through continual refinement, with the pass rate gradually approaching 67.5%. The pass rate rises sharply from iter 1 to iter 3 and then slows. Between iter 4 and iter 5, only 1.2% of cases improve, indicating incremental benefits from further iterations. Thus, additional iterations are not considered. Our goal is to collect the program changes made during the iterative debugging process. Therefore, codes that succeed on the first iteration, along with those that cannot be corrected within 5 iterations, are discarded. This iterative debugging process mimics how humans write programs by constantly modifying the code based on test results. The key code changes during the iteration serve as promising reward signals that guide the code from incorrect to correct. Driven by the reward signals from Samples where the abstract syntax tree (AST) of y+ and are identical. After applying these filters, 84k samples remained. Next, for each sample, we employ the LLM-as-a-judge methodology using GPT-4o to assess whether there is significant logical difference between y+ and y. We further filter out pairs where the differences are limited to code formatting, comments, variable names, whitespace, or blank lines. These efforts ensure that the selected and rejected samples reflect key functional differences. The final dataset consists of 59,302 samples, ready for preference-based alignment training."
        },
        {
            "title": "3.2 Fine-grained Alignment",
            "content": "Direct Preference Optimization (DPO) directly optimizes the policy model using relative quality comparisons. Given prompt x, preference pair (y+, y), where y+ is of higher quality than y+, DPO aims to maximize the probability of the preferred response y+ while minimizing that of the less desirable response y. The KL divergences for y+ and are defined as: K+ = log πθ(y+x) πref(y+x) , = log πθ(yx) πref(yx) , (1) and the optimization objective LDPO(πθ; πref) is: LDPO = E(x,y+,y)D [log σ (β (K+ K))] (2) DPO optimizes the expectation over the pairwise preference dataset D, and σ is the sigmoid function. DPO has proven effective in multiple domains like mathematics (Lai et al., 2024). However, the objective in Eq. (2) may not be fully suitable for preference-based alignment in code generation, as large portion of the tokens in y+ and are identical, with only minor differences. This can confuse the policy model in identifying the critical differences necessary for functional correctness, and diminish alignment gains (Pal et al., 2024; Chen et al., 2024; Wu et al., 2024). To help code LLMs better grasp the critical tokens driving functional differences between preference pairs, we modify the DPO algorithm to highlight key tokens in the dispreferred code snippet using masking strategy. Specifically, given = [y 1 , ] containing tokens, vanilla DPO computes as: 2 , .., K = log πθ(yx) πref(yx) = log = (cid:88) i=1 (cid:81)L (cid:81)L x) x) i=1 πθ(y i=1 πref(y πθ(y x) πref(y x) log (3) Figure 3: The pass rate progression across iterations of refinement with execution verification feedback. this iterative process, we treat the final correct code as the chosen sample and randomly select an earlier version of the code as the rejected sample, forming the pair (y+, y) for preference-based learning. Critical Difference Extraction For each (y+, y) pair, we extract the critical differences that drive the functional outcome. Specifically, we identify the sets of difference lines D+ and based on the Longest Common Subsequence (LCS), where the lines not belonging to the LCS are considered as the difference lines, as shown in Algorithm 1. The extraction process does not require annotations from humans or LLMs. After extraction, the key modifications between pairs are constrained within D+ and D. D+ and record the changes made between the preferred and dispreferred versions, which are responsible for the functional differences between the pairs. Quality Control for Preference Pairs We synthesize 104k instruction data through iterative refinement and implement several measures to ensure the quality of the synthesized data. In particular, we apply rule-based filtering to remove trivial or uninformative samples, including: (i) consists only of comments; (ii) exceeds 20 lines; (iii) y+ or exceeds 2048 tokens; and (iv) Algorithm 1 Extracting Code Difference Require: Code pair y+ and Ensure: Difference lines D+ and for 1: Split y+ and into lines: y+ lines and 2: Find the LCS of lines between y+ 3: Initialize D+ = and = 4: D+ = {l+ y+ lines l+ / LCS} 5: = {l lines / LCS} 6: return D+ and lines lines and lines Figure 4: Overview of IterPref. IterPref constructs preference pairs via iterative debugging, treating the correct version as preferred and the previous as dispreferred. DPO adaptations enable code LLMs to learn the correct pattern from the preferred code while highlighting critical tokens with masking strategy in the dispreferred sample. We make the following adaptations to while"
        },
        {
            "title": "4 Experiments",
            "content": "keeping K+ unchanged: K+ = K+ = (cid:80)L i=1 I(y D) log πθ(y πref(y x) x) (4) (5) Eq. (4) encourages the code LLMs to learn correct code generation patterns from y+. Eq. (5) explicitly focuses on contrasting critical tokens in while keeping the correct tokens in the dispreferred code uninvolved in the loss computation through masking the tokens that do not appear in D+. By penalizing critical tokens responsible for functional errors and avoiding over-optimization on tokens common to both y+ and y, IterPref achieves more fine-grained alignment suitable for code, as opposed to previous sample-level optimization. This helps the code LLMs internalize correct coding patterns and more effectively recognize key token mistakes. Our loss also targets pairwise optimization: DPO = E(x,y+,y)D log σ (cid:16) (cid:16) K+ K(cid:17)(cid:17) β (6) Correspondingly, the RPO loss (Liu et al., 2024a; Pang et al., 2024), variant of DPO, consists of weighted SFT loss on y+, scaled by α. Our modified DPO loss also complements RPO, and the RPO-format RPO loss is: LSFT = E(x,y+)D [log pθ(y+x)]"
        },
        {
            "title": "LRPO",
            "content": "= DPO + αLSFT (7) (8) Experiment Setup Each instruction-tuned code LLM is fine-tuned for 2 epochs, while base models are fine-tuned for 5 epochs using full-parameter fine-tuning. The model with the lowest validation loss is selected for evaluation. For our IterPref, the learning rate is set to 1e-5 for the 7B code LLMs and 5e-6 for the 14B models, using global batch size of 128, with cosine scheduler and warmup. The maximum sequence length is set to 2048 tokens. For the DPO algorithm, β is set to 0.1, and α is set to 1.0 for RPO. πθ and πref are both initialized with the weights of the evaluated model, while πref keeps frozen during training. Benchmarks We evaluate the Code LLMs using five benchmarks: HUMANEVAL Base (Chen et al., 2021), HUMANEVAL Plus (Liu et al., 2023), Mostly Basic Python Problems (MBPP Base (Austin et al., 2021), MBPP Plus), LiveCodeBench (LCB) (Jain et al., 2024) (v5 with problems released between May 2023 and Jan 2025), and BIG-CODEBENCH (BCB) (Zhuo et al., 2025). BCB consists of two splits: the instruct split, which involves only instructions, and the complete split, which uses structured docstrings. The hard subset in BCB contains the most challenging fraction of the full set. We use greedy decoding for code generation, with results reported as the pass@1 score. The results of un-tuned Code LLMs are taken from the corresponding leaderboards1 2 for reference, while other settings are trained by us. 1https://bigcode-bench.github.io/ 2https://evalplus.github.io/"
        },
        {
            "title": "MBPP",
            "content": "BCB-Full BCB-Hard"
        },
        {
            "title": "Base",
            "content": "Plus Comp. Inst. Comp. Inst. LCB Inst. Avg. DS-Coder-7B-Ins-v1.5 CodeQwen1.5-7B-Chat StarCoder2-15B Qwen2.5-Coder-7B Ref. DPO RPO Code-Optimise IterPref-DPO IterPref-RPO Ref. DPO RPO Code-Optimise IterPref-DPO IterPref-RPO Ref. RPO Code-Optimise IterPref-RPO Ref. RPO Code-Optimise IterPref-RPO 75.6 69.5 65.2 64.6 76.2 78.0 83.5 79.3 79.3 78.5 89.6 89.6 46.3 53.0 61.0 73.2 61.6 71.3 82.3 89. 71.3 65.2 59.8 60.4 72.0 73.2 78.7 73.8 73.2 75.0 85.4 86.0 37.8 45.7 54.9 65.2 53.0 59.8 78.7 84.8 75.2 77.2 75.7 78.8 79.1 78.8 79.4 79.9 80.2 80.7 83.9 82. 66.2 63.0 66.5 65.9 76.9 70.9 76.2 83.3 62.3 67.2 66.1 69.3 65.3 67.2 69.0 69.0 68.8 69.6 69.8 70.4 53.1 42.6 53.4 53.4 62.9 50.3 60.4 69. 43.8 46.1 43.2 45.2 47.5 49.3 43.6 43.3 41.6 43.3 48.7 48.4 38.4 28.7 31.8 40.3 45.8 39.8 48.5 53.3 35.5 37.9 37.5 36.5 37.8 39.0 39.6 36.1 32.5 36.1 39.9 38. - 17.2 18.8 38.8 40.2 29.7 39.6 43.1 15.5 12.2 10.8 13.5 22.3 20.9 15.5 14.9 14.8 17.6 20.3 20.3 12.2 9.05 6.76 18.9 16.2 24.3 18.9 29. 10.1 14.2 13.8 13.5 17.6 20.9 18.9 10.8 10.6 11.5 16.9 18.2 - 6.01 6.08 18.2 14.2 14.8 12.2 20.9 20.6 20.4 20.2 21.3 21.8 22.0 15.3 15.5 12.9 16.2 18.1 17. - 13.1 14.9 19.4 24.1 23.0 23.2 32.2 45.5 45.5 43.6 44.8 48.8 49.9 49.3 47.0 46.0 47.6 52.5 52.3 - 30.9 34.9 43.7 43.9 42.7 48.9 56. Table 1: Pass@1 (%) results of different LLMs on HumanEval, MBPP, BigCodeBench, and LiveCodeBench-v5 (LCB) under greedy decoding setting. We conducted the evaluation on the Full and Hard subsets of BigCodeBench (BCB), including the Complete (Comp.) and Instruct (Inst.) tasks. The best results are highlighted in Bold. Evaluated Models and Baselines Evaluated models include DeepSeek-Coder-7B-Instructv1.5 (Guo et al., 2024), CodeQwen1.5-7BChat (Bai et al., 2023), and the base models Qwen2.5-Coder-7B (Hui et al., 2024b), StarCoder2-15B (Lozhkov et al., 2024). The open-source work Code-Optimise (Gee et al., 2025) is reproduced using GPT-4o, with 100 solutions sampled at temperature of 0.6 for each problem. The DP OP vF setting results are reported, with instruct-tuned model appilied DPO, and base models using RPO."
        },
        {
            "title": "5 Main Results",
            "content": "Table 1 presents comparison between baseline models, DPO variants, and IterPref. We discuss the findings from the following perspectives. IterPref Achieves Fine-grained Alignment. We begin by noting that the preference pairs generated through the iterative process exhibit unique characteristics distinct from those found in other preference sets, such as Code-Optimise. These characteristics are reflected in the performance decline observed when directly applying vanilla DPO or RPO algorithms. While certain configurations, such as DS-Coder-7B-Instruct-DPO and Qwen2.5-Coder7B-Base RPO, demonstrate performance gains on the full BIGCODEBENCH set, it remains consistent trend that DPO and RPO generally underperform relative to the baseline models. Typically, when an external LLM attempts to correct an error in code, only small portion of the given code is modified to fix the logic or runtime error, while most of the code remains unchanged. This results in highly similar preference pairs. Overly similar preference pairs, where identical tokens appear in both positive and negative examples, introduce ambiguity and noise to the policy model, thereby weakening alignment gains. This performance degradation highlights the need for explicit measures to guide the policy model in focusing on critical tokens that drive functional errors, preventing it from being misled by identical parts. IterPref addresses this by masking identical tokens in the dispreferred code and explicitly contrasting the critical tokens extracted, which results in significant performance gains. As shown in Table 1, IterPref-DPO outperforms DPO by an impressive 3.3% and 5.9% on average across benchmarks, with IterPref-RPO achieving gains ranging from 6.3% to 12.4%. Compared to CodeOptimise, IterPref achieves significant performance gains over both instruction-tuned Code LLMs and base models, with 4.0% improvement using DSCoder-7B-Instruct and 7.4% improvement using Qwen2.5-Coder-7B, respectively. Figure 5: Average results on HumanEval, MBPP, and BigCodeBench-Full based on Qwen2.5-Coder-7B. Figure 6: The frequency of the most common failure types on the BigCodeBench Complete-Full set. IterPref Achieves Significant Gains over Existing Alignment Frameworks. The preference pair construction used by Code-Optimise involves sampling multiple solutions and testing for given instruction. It is straightforward and effective method that demonstrates strong performance, as shown in Figure 5. However, constructing pairs based solely on pass rates fails to capture how erroreous code should be corrected, thus limiting the models ability to generalize improvements. Compared to baselines AceCoder and Code-Optimise, IterPref consistently achieves performance gains over both, using the same base Code LLM. IterPref Improves Challenging Coding Tasks. IterPref drives performance gains not only in basic coding tasks but also in more challenging ones. We highlight that the IterPref framework has the potential to boost Code LLMs to solve complex coding tasks. Notably, Qwen2.5-Coder-7B equipped with IterPref achieves 29.7% pass@1 score on BigCodeBench Complete Hard, matching the performance of larger Code LLMs DeepSeek-CoderV2-Instruct (29.7) and Claude-3-Opus (29.7) (Anthropic, 2024), and approaching Llama-3.1-405BInstruct (30.4) (Grattafiori et al., 2024). When given more attempts, IterPref achieves pass@5 of 45.7%, outperforming DeepSeek-R1 (40.5) (DeepSeek-AI et al., 2025) and GPT-o1 (40.2). On the Instruct Hard split, pass@5 of the IterPrefQwen is 34.7%, comparable to the performance of GPT-o3-mini (33.1)."
        },
        {
            "title": "6 Analysis",
            "content": "In this section, we present statistical analysis of common failure case types to pinpoint frequent pitfalls in code generation. Additionally, we compare the costs of generating and annotating preference pairs to guide more efficient preference alignment and reduce errors. IterPref Generates Fewer Errors. By contrasting critical tokens between corrected version and its preceding iteration explicitly, Code LLM equipped with IterPref makes fewer errors. Figure 6 illustrates the frequency of common failure types in the BigCodeBench Complete-Full set, showing notable reduction in error occurrences when using IterPref-RPO compared to RPO and Code-Optimise on Qwen2.5-Coder-7B. This suggests that while RPO includes SFT, it still requires targeted learning of critical errors in the dispreferred samples to effectively reduce mistakes. IterPref Provides an Efficient Pathway for Preference Annotation. We compare the cost of synthesizing one preference pair between IterPref and the sampling techniques adopted by CodeOptimise, primarily considering external LLM calls and execution times. Given an instruction, Code-Optimise synthesizes code snippet candidates (where is often set to 100), using test cases from the raw dataset, leading to executions on the CPU. In contrast, for single instruction, IterPref requires up to 7 LLM calls and executions. Considering the failure ratio (when code cant pass the generated test cases), an estimated 10.4 calls are needed for given instruction, which is far fewer than the practice of the current method. Though massive sampling is diverse and effective, it is not efficient as most code snippets are discarded. IterPref shows that starting with single code snippet, even if it fails initially, it still holds the potential to form preference pair for alignment training."
        },
        {
            "title": "7 Ablation Study",
            "content": "Despite the effectiveness of IterPref in pinpointing critical error regions, there remain open questions about how best to incorporate negative examples and how much context is truly beneficial for code correction. We therefore explore several settings: (i) SFT: Supervised fine-tuning using the positive sample from the preference pair; (ii) Hybrid Training: Half of the samples in batch are trained using vanilla DPO, while the other half follows the IterPref approach; (iii) Diff-Augmentation: provide more context for the dispreferred sample by including 1 or 2 lines of tokens before and after D; and (iv) Symmetric Masking Strategy: The Code LLMs learn from the tokens in D+ rather than the full sequence of positive sample. We conduct experiments on CodeQwen1.5-7B-Chat and base model Qwen2.5-Coder-7B, with the results presented in Table 2. Hybrid Training & Diff-Augmentation both expose Code LLMs to dispreferred samples but differ in scope: Hybrid Training uses the entire dispreferred sequence, while Diff-Augmentation focuses on small token window around the changed segment. Although adding extra tokens around may seem beneficial, it often confuses the model and lowers performance. Concentrating on only the most critical tokens leads to better outcomes, highlighting the importance of accurately identifying them for IterPref. The iterative debugging process naturally suits this approach because only limited portion of the code typically changes between iterations, leaving the bulk of the code intact. Consequently, it produces samples in which the differences can be isolated with high precision. Although extracting with the Longest Common Subsequence (LCS) is not always perfect, it reliably captures the key modifications and thus narrows the search space for crucial tokens. Crucially, this automated method obviates the need for manual annotations, underscoring its efficiency and practicality. Supervised Fine-Tuning (SFT) ranks as the second-best approach, following the IterPref framework. Because IterPref employs iterative debugging, it ensures that the preferred sample in each preference pair is validated to pass test cases, thereby guaranteeing its high quality. Consequently, straightforward implementation of SFT also yields notable performance gains. However, the SFT process disregards the dispreferred sample, missing the chance to learn from common"
        },
        {
            "title": "Plus",
            "content": "Avg. 83.5 Ref. 87.8 SFT 83.5 Hybrid-DPO Hybrid-RPO 84.8 DiffAug-DPO 83.5 86.0 DiffAug-RPO 89.6 IterPref-DPO 89.6 IterPref-RPO CodeQwen1.5-7B-Chat 79.4 78.7 82.3 83.5 81.5 79.3 81.5 79.3 80.2 76.8 81.5 79.9 83.9 85.4 86.0 82.5 Ref. SFT Hybrid-RPO DiffAug-RPO IterPref-RPO Qwen2.5-Coder-7B 53.0 61.6 82.9 87.2 79.3 82.9 81.7 86.0 84.8 89.6 76.9 83.1 81.7 82.8 83. 69.0 69.6 66.1 67.7 65.1 65.9 69.8 70.4 62.9 68.3 67.5 67.5 69.5 77.7 80.8 77.9 78.8 76.4 78.8 82.2 82.1 63.6 80.4 77.9 79.5 81.9 Table 2: Pass@1 score (%) when performing supervised fine-tuning and possible ablations on the IterPref. mistakes that should be avoided. In contrast, IterPref not only increases the models likelihood of producing correct tokens by fully leveraging error-free code, but also systematically highlights and penalizes the specific tokens that lead to critical errors. By explicitly learning from accurate coding patterns while avoiding common mistakes, IterPref achieves more fine-grained alignment and ultimately outperforms standard SFT. Symmetric Masking Strategy. If training with the symmetric masking strategy, where Code LLMs learn from both D+ and without access to the full positive sample, the model fails to retain its basic code generation capabilities and cannot benchmark properly. The primary objective of Code LLMs is to generate complete and correct code. Although learning symmetrically from both D+ and may seem appealing, the priority is ensuring that Code LLMs learn from complete, correct code rather than fragmented pieces. Without the full correct code contexts, the positive sample cannot align with the instruction, introducing incomplete and misleading signals into the learning process."
        },
        {
            "title": "8 Conclusion",
            "content": "We present IterPref, novel preference alignment framework that emulates human iterative debugging to capture critical errors in code for precise optimization. IterPref leverages preference pairs to identify error-prone regions and applies an improved DPO algorithm, guiding Code LLMs to focus on aligning these pivotal segments. This targeted approach enhances the models ability to detect and correct mistakes more efficiently. To facilitate the generation of high-quality preference pairs, we contribute the CodeFlow training set, where each sample undergoes iterative refinement until it passes unit tests, with the modification history providing natural record of error corrections. Extensive experiments show that IterPref-equipped Code LLMs achieve significant performance improvements in code generation and excel in tackling basic and complex coding tasks like BigCodeBench."
        },
        {
            "title": "Limitations",
            "content": "IterPref is inspired by the debugging pattern of developers, serving as novel framework for finegrained preference learning in Code LLMs. Instead of using pass rate alone, IterPref derives preference pairs from an iterative debugging process. By contrasting critical tokens between corrected version and its preceding iteration, IterPref helps the model learn fine-grained alignment for key errors. The limitations for the IterPref framework are twofold: first, IterPref relies on generated test cases, and when the quality of these test cases is not ensured, the performance of IterPref may also be affected. Second, this study focuses on dataset of 59k samples without further expansion, which may limit generalizability, but offers opportunities for future exploration with larger data set."
        },
        {
            "title": "References",
            "content": "Anthropic. 2024. Claude 3.5: Advancing ai safety and performance. Technical report, Anthropic. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models. Preprint, arXiv:2108.07732. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, and et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Huayu Chen, Guande He, Hang Su, and Jun Zhu. 2024. Noise contrastive alignment of language models with explicit rewards. CoRR, abs/2402.05369. Mark Chen, Jerry Tworek, Heewoo Jun, and Qiming Yuan et al. 2021. Evaluating large language models trained on code. Preprint, arXiv:2107.03374. Ning Dai, Zheng Wu, Renjie Zheng, Ziyun Wei, Wenlei Shi, Xing Jin, Guanlin Liu, Chen Dun, Liang Huang, and Lin Yan. 2025. Process supervision-guided policy optimization for code generation. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, and et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Shihan Dou, Yan Liu, Haoxiang Jia, Enyu Zhou, and Limao et al. Xiong. 2024. StepCoder: Improving code generation with reinforcement learning from compiler feedback. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 45714585, Bangkok, Thailand. Association for Computational Linguistics. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. Model alignment as prospect theoretic optimization. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. Leonidas Gee, Milan Gritta, Gerasimos Lampouras, and Ignacio Iacobacci. 2025. Code-optimise: Selfgenerated preference data for correctness and efficiency. Preprint, arXiv:2406.12502. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, and et al. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. Deepseek-coder: When the large language model meets programming the rise of code intelligence. Preprint, arXiv:2401.14196. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, and et al. 2024a. Qwen2.5-coder technical report. Preprint, arXiv:2409.12186. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. 2024b. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. Preprint, arXiv:2403.07974. Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. 2024. survey of reinforcement learning from human feedback. Preprint, arXiv:2312.14925. Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. 2024. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. Preprint, arXiv:2406.18629. Haoling Li, Xin Zhang, Xiao Liu, Yeyun Gong, Yifan Wang, Qi Chen, and Peng Cheng. 2025. Enhancing large language model performance with Preprint, gradient-based parameter selection. arXiv:2406.15330. Jia Li, Yunfei Zhao, Yongmin Li, Ge Li, and Zhi Jin. 2024. Acecoder: An effective prompting technique specialized in code generation. ACM Trans. Softw. Eng. Methodol., 33(8). Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, and et al. 2023. Starcoder: may the source be with you! Preprint, arXiv:2305.06161. Zicheng Lin, Tian Liang, Jiahao Xu, Xing Wang, Ruilin Luo, Chufan Shi, Siheng Li, Yujiu Yang, and Zhaopeng Tu. 2024. Critical tokens matter: Tokenlevel contrastive estimation enhence llms reasoning capability. arXiv preprint arXiv:2411.19943. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc. Zhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, and Zhaoran Wang. 2024a. Provably mitigating overoptimization in RLHF: Your SFT loss is implicitly an adversarial regularizer. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Zhihan Liu, Shenao Zhang, Yongfei Liu, Boyi Liu, Yingxiang Yang, and Zhaoran Wang. 2024b. Dstc: Direct preference learning with only self-generated tests and code to improve code lms. Preprint, arXiv:2411.13611. Zhihan Liu, Shenao Zhang, and Zhaoran Wang. 2024c. DSTC: direct preference learning with only selfgenerated tests and code to improve code lms. CoRR, abs/2411.13611. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, and et al. 2024. Starcoder 2 and the stack v2: The next generation. Preprint, arXiv:2402.19173. Zeyao Ma, Xiaokang Zhang, Jing Zhang, Jifan Yu, Sijia Luo, and Jie Tang. 2025. Dynamic scaling of unit tests for code reward modeling. Preprint, arXiv:2501.01054. Yibo Miao, Bofei Gao, Shanghaoran Quan, Junyang Lin, Daoguang Zan, Jiaheng Liu, Jian Yang, Tianyu Liu, and Zhijie Deng. 2024. Aligning codellms Preprint, with direct preference optimization. arXiv:2410.18585. OpenAI. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. 2024. Smaug: Fixing failure modes of preference optimisation with dpo-positive. Preprint, arXiv:2402.13228. Richard Yuanzhe Pang, Weizhe Yuan, He He, Kyunghyun Cho, Sainbayar Sukhbaatar, and Jason Weston. 2024. Iterative reasoning preference optimization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, and et al. 2024. survey of reasoning with foundation models. Preprint, arXiv:2312.11562. Yaoxiang Wang, Haoling Li, Xin Zhang, Jie Wu, Xiao Liu, Wenxiang Hu, Zhongxin Guo, Yangyu Huang, Ying Xin, Yujiu Yang, Jinsong Su, Qi Chen, and Scarlett Li. 2025a. Epicoder: Encompassing diversity and complexity in code generation. In Arxiv. Yaoxiang Wang, Haoling Li, Xin Zhang, Jie Wu, Xiao Liu, Wenxiang Hu, Zhongxin Guo, Yangyu Huang, Ying Xin, Yujiu Yang, et al. 2025b. Epicoder: Encompassing diversity and complexity in code generation. arXiv preprint arXiv:2501.04694. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2024. Magicoder: empowering code generation with oss-instruct. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. Junkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang, and Xiangnan He. 2024. β-dpo: Direct preference optimization with dynamic β. CoRR, abs/2407.08639. Dylan Zhang, Shizhe Diao, Xueyan Zou, and Hao Peng. 2024. PLUM: preference learning plus test cases yields better code language models. CoRR, abs/2406.06887. Kechi Zhang, Ge Li, Yihong Dong, Jingjing Xu, Jun Zhang, Jing Su, Yongfei Liu, and Zhi Jin. 2025. CodeDPO: Aligning code models with self generated and verified source code. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2023. Instruction tuning for large language models: survey. CoRR, abs/2308.10792. Terry Yue Zhuo, Vu Minh Chien, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, and et al. 2025. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. In The Thirteenth International Conference on Learning Representations."
        }
    ],
    "affiliations": [
        "CASIA",
        "Microsoft Research",
        "Tsinghua University"
    ]
}