{
    "paper_title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation",
    "authors": [
        "Guido Manni",
        "Clemente Lauretti",
        "Loredana Zollo",
        "Paolo Soda"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces a novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networks -- a generator for class-conditioned image translation, a discriminator for authenticity assessment and classification, and a dedicated classifier -- within a three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudo-labeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers a practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 9 2 4 6 0 . 8 0 5 2 : r SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation Guido Mannia,b, Clemente Laurettib, Loredana Zollob and Paolo Sodaa aUnit of Artificial Intelligence and Computer Systems, Department of Engineering, UniversitÃ  Campus Bio-Medico di Roma, Rome, Italy, bUnit of Advanced Robotics and Human-Centred Technologies, Department of Engineering, UniversitÃ  Campus Bio-Medico di Roma, Rome, Italy, I I Keywords: Semi-supervised learning Few-shot learning Medical imaging Deep learning GAN-based methods T T Deep learning has revolutionized medical imaging, but its effectiveness is severely limited by insufficient labeled training data. This paper introduces novel GAN-based semi-supervised learning framework specifically designed for low labeled-data regimes, evaluated across settings with 5 to 50 labeled samples per class. Our approach integrates three specialized neural networksa generator for class-conditioned image translation, discriminator for authenticity assessment and classification, and dedicated classifierwithin three-phase training framework. The method alternates between supervised training on limited labeled data and unsupervised learning that leverages abundant unlabeled images through image-to-image translation rather than generation from noise. We employ ensemble-based pseudolabeling that combines confidence-weighted predictions from the discriminator and classifier with temporal consistency through exponential moving averaging, enabling reliable label estimation for unlabeled data. Comprehensive evaluation across eleven MedMNIST datasets demonstrates that our approach achieves statistically significant improvements over six state-of-the-art GAN-based semi-supervised methods, with particularly strong performance in the extreme 5-shot setting where the scarcity of labeled data is most challenging. The framework maintains its superiority across all evaluated settings (5, 10, 20, and 50 shots per class). Our approach offers practical solution for medical imaging applications where annotation costs are prohibitive, enabling robust classification performance even with minimal labeled data. Code is available at https://github.com/GuidoManni/SPARSE. 1. Introduction Deep learning has demonstrated remarkable potential in revolutionizing medical imaging [1]. However, insufficient labeled data for model training is one of the main challenge hindering its effectiveness that arises from several constraints such as: the stringent privacy regulations and ethical guidelines governing patient data access and distribution [2], and the necessity of specialized medical expertise for data annotation, resource limited by healthcare professionals primary commitment to patient care [3]. These constraints often results in what is known as the low-data regime - situation where the number of labeled medical images falls below the threshold needed for reliable convergence of deep networks, typically ranging from dozens to few hundred samples depending on the complexity of the task and model architecture. To address this issue, researchers have explored unsupervised, supervised and semi-supervised learning. The approaches in the first category tackles the low-data regime through unsupervised learning, which exploits the abundant unlabeled medical images to learn meaningful representations. Models are trained to capture underlying data patterns through tasks like image reconstruction, anomaly detection, or feature learning. This approach is particularly valuable in medical imaging where unlabeled data is available, but fundamental challenge remains: without sufficient labeled validation data, it is difficult to ensure that the extracted features are clinically relevant rather than merely statistically significant in the data distribution. In the second guido.manni@unicampus.it (G. Manni); c.lauretti@unicampus.it (C. Lauretti); l.zollo@unicampus.it (L. Zollo); p.soda@unicampus.it (P. Soda) case of supervised learning, researchers have explored various strategies to mitigate the limited availability of labeled samples, such as transfer learning, data augmentation and synthetic data generation. Transfer learning leverages models pre-trained on large datasets by fine-tuning them for specific tasks with limited data [4], but its effectiveness diminishes when the target domain differs significantly from the source domain [5]. Data augmentation techniques artificially expand training datasets through various transformations [6], but cannot introduce new information. Synthetic data generation through simulation or generative models has emerged as another strategy [7, 8], though generating fully synthetic datasets that realistically capture real-world data distributions remains challenging [9, 10]. The third approach is semi supervised learning (SSL) that simultaneously leverages labeled and unlabeled data. Traditional SSL methods relied on machine learning techniques such as self-training and co-training. However, recent advances in generative models, particularly Generative Adversarial Networks (GANs) based methods, have improved SSL performance, not by generating completely synthetic data but by learning to extract meaningful features from the real unlabeled data distribution to enhance the learning process [11]. Despite these advances, existing SSL methods often struggle in low-data regime as it happens in medical imaging. In this paper we tackle this issue and we introduce the following contributions: novel GAN-based semi-supervised learning method specifically designed for medical image classification in low labeled-data regimes. Guido Manni et al. Page 1 of 15 dynamic training schedule that alternates between supervised phases and unsupervised phases to optimize learning efficiency. An image-to-image translation mechanism employed as secondary task that, unlike purely generative approaches that create images de novo from noise vectors, modifies existing real unlabeled images to preserve authentic anatomical features while enriching feature representations beyond what traditional generative approaches provide. confidence-weighted temporal ensemble technique that combines predictions from multiple model components and previous training iterations, significantly improving pseudo-labeling reliability in low-data scenarios. comprehensive empirical evaluation demonstrating competitive performance against six state-of-the-art SSL methods across eleven benchmark datasets for medical image classification tasks. The remainder of this paper is organized as follows: section 2 reviews the related works in the field, providing context for our research contributions. Section 3 details our proposed method. Section 4 describes the experimental configuration, including datasets, parameters, and evaluation metrics. Section 5 presents our results and provides comprehensive analysis. Finally, section 6 concludes the paper with summary of our findings and suggestions for future research directions. 2. Related Works Semi-supervised learning methods aim to leverage both labeled and unlabeled data to improve model performance, particularly in scenarios where labeled data is scarce or expensive to obtain. recent survey [12] has established clear taxonomy of SSL approaches, distinguishing between two main classes: inductive methods, which construct classifiers that can generate predictions for any input, and transductive methods, which optimize directly over predictions for given set of unlabeled data points. Inductive methods can be further subdivided into three categories based on how they incorporate unlabeled data: (1) wrapper methods, which iteratively train classifiers on labeled data and use their predictions to generate pseudo-labels for unlabeled samples; (2) unsupervised preprocessing methods, which extract features or determine initial parameters from unlabeled data before supervised training; and (3) intrinsically semi-supervised methods, which directly incorporate unlabeled data into the objective function or optimization procedure. Since our proposed approach falls within this third category, and it exploits GANs, the remainder of this section reviews the GAN-based SSL methods across various domains, focusing on approaches that have introduced key architectural innovations, while the interested readers can refer to [11] for comprehensive review of approaches within this categories. Semi-supervised learning is built on the fundamental assumption that the data distribution in the input space contains substantial information about label distribution in the output space. Within this context, GANs are particularly suitable candidates for SSL applications, given their inherent ability to model underlying data distributions and reveal patterns in the input space. The first significant work in this context introduced the SGAN model [13], which expands the traditional GAN architecture by augmenting the discriminator to perform dual functions that distinguishes between real and synthetic samples while simultaneously predicting class labels for input data. This dual-purpose approach represented an important extension of the framework through pseudo-labeling, where the discriminator/classifier is trained on both labeled data and generated samples with known class labels. SGAN exemplifies what is known as two-player model in GANbased SSL, where the traditional generator-discriminator architecture is maintained but the discriminator is extended to perform both adversarial discrimination and classification tasks simultaneously. Building on these foundations, MatchGAN [14] introduced an innovative approach that leveraged the Wasserstein distance and conditional generation. As semi-supervised conditional GAN, MatchGAN utilizes the label space in the target domain along with unlabeled samples to generate additional labeled training data. The framework assigns labels from the pool of labeled samples to unlabeled samples, then passes these through the generator to create synthetic versions of images based on the target labels. This work also introduces match loss term that compares the generated images to the original labeled images from which the target labels are sampled. breakthrough in GANbased SSL came with the introduction of TripleGAN [15], which addressed the difficulty of simultaneously optimizing both generator and discriminator performance. TripleGAN pioneered the three-player model architecture by incorporating an additional classifier that works independently from the discriminator, creating tripartite interaction between generator, discriminator, and classifier. In this three-player setup, the classifier works in conjunction with the generator to characterize conditional distributions between images while limiting the discriminators role to identifying fake imagelabel pairs. This separation of concerns allows each component to specialize in its primary task, potentially leading to improved overall performance compared to two-player models where the discriminator must balance competing objectives. Recent developments have significantly expanded upon the TripleGAN framework [16, 17, 18]. EC-GAN [16] proposed mechanism where generated images are immediately processed by classifier to produce pseudo-labels. This classifier-generator interaction is regulated through hyperparameter-weighted loss function that precisely controls the influence of generated samples on classifier training. SECCGAN [17] introduced co-supervised learning paradigm where conditional GAN is trained alongside the classifier, providing semantics-conditioned, confidence-aware synthesized examples during training. CISSL-GAN [18] Guido Manni et al. Page 2 of then extended the Triple-GAN framework to address semisupervised learning with class-imbalanced data through dynamic class-rebalancing sampler that strategically selects pseudo-labeled samples from unlabeled data. The analysis of the literature reported so far shows that the following several open issues still exist in current GAN-based SSL approaches: The analysis of the literature reported so far shows that the following several open issues still exist in current GAN-based SSL approaches: None of the existing methods specifically addresses the challenges of extremely low labeled data regimes, such as the case where only 5-10 labeled samples per class are available. Current approaches like SGAN [13], TripleGAN [15], and EC-GAN [16] primarily rely on generation-based paradigms for unsupervised learning, lacking effective mechanisms to integrate supervised and unsupervised signals. Existing methods rely on single discriminator or on single classifier, while none has investigated possible advantages given by the use of an ensemble of models that, in other domain, has proven to provide complementary outputs that enhance model robustness [19, 20]. Next section introduces our methodology that addresses these limitations. 3. Methods We propose novel semi-supervised learning strategy called SPARSE (Semi-supervised Pseudo-labeling via Adversarial Representation tranSlation Enhancement), designed to achieve robust classification performance in extremely low labeled-data regimes. Our approach integrates three specialized neural networks: generator (G ) that performs class-conditioned image translation, discriminator (D) that assesses image authenticity while providing classification signals, and dedicated classifier (C ) that focuses exclusively on the classification task. Our approach consists of three main phases: 1. Supervised training phase (Figure 1a): it jointly trains the three aforementioned networks, , and . Each model is trained with supervised dataset ğ·ğ‘ ğ‘¢ğ‘ = , where ğ‘ is the number of few-shot {(ğ‘¥ğ‘–, ğ‘¦ğ‘–)}ğ‘ ğ‘–=1 is the input sample - an image in our expersamples, ğ‘¥ğ‘– iments, and ğ‘¦ğ‘– is the corresponding one-hot encoded ground truth vector, with ğ‘¦ğ‘– {0, 1}ğ¾1 where ğ¾ is the total number of classes. This initial phase is crucial for maintaining classification accuracy and preventing drift in the unsupervised learning process, represented in panel of the same figure, by providing supervised signals from the limited labeled data. 2. Self-supervised pre-training phase (Figure 1b), which consists of two components. The first is an ensemblebased pseudo-labeling block that combines the confidenceweighted scores provided by and given set of ğ‘€ unsupervised samples, with ğ·ğ‘¢ğ‘›ğ‘  = { ğ‘¥ğ‘–}ğ‘€ ğ‘–=1 ğ‘€ >> ğ‘. This ensemble outputs the pseudo- , with ğ‘¦ğ‘– {0, 1}ğ¾1, assigned to labels { ğ‘¦ğ‘–}ğ‘€ ğ‘–=1 each sample in ğ·ğ‘¢ğ‘›ğ‘  . The second component of the self-supervised pretraining phase introduces classconditioned image translation task that uses randomly , with ğ‘§ğ‘– {0, 1}ğ¾1. sampled class conditions {ğ‘§ğ‘–}ğ‘€ ğ‘–=1 Hence, this phase leverages the abundant unlabeled data to improve feature representations and model generalization capabilities through image-to-image translation tasks. 3. Synthetic data enhancement phase (Figure 1c): it paired with one-hot encoded receives as input ğ·ğ‘¢ğ‘›ğ‘  class vectors {ğ‘§ğ‘–}ğ‘ƒ randomly sampled from uniform ğ‘–=1 distribution, which are processed by the generator to create synthetic training samples ğ·ğ‘ ğ‘¦ğ‘› = {ğ‘ ğ‘–}ğ‘ƒ , ğ‘–=1 with ğ‘ƒ >> ğ‘. Subsequently, ğ·ğ‘ ğ‘¦ğ‘› is used to train the classifier with these same class vectors { ğ‘§ğ‘–}ğ‘ƒ ğ‘–=1 serving as supervision signals. This final phase aims to expand the effective training set by creating synthetic samples that augment the limited labeled data. The training schedule alternates between two phases: the supervised phase (executed at every epoch) and the combined self-supervised and synthetic data enhancement phase (executed every ğœ‡ epochs, where ğœ‡ is hyperparameter), ensuring stable and effective utilization of the entire dataset. It is worth noting that our approach addresses two primary issues in semi-supervised learning in an extremely low labeled-data regime. The first is the availability of insufficient labeled samples for effective supervised learning, which may affect panel (a) of Figure 1: the self-supervised pretraining in panel (b) synthesizes new samples that are used in panel (c) to train the classifier with large amount of samples. The second issue concerns the integration of supervised and unsupervised learning signals. While the paradigm in the literature [13, 15, 16] defines pre-tasks where generative model conditionally generates new samples that enhance the downstream classification task, our approach integrates an image-to-image translation pre-task that enrich the model with more semantic information than standard generative step (Figure 1b). The rest of this section details such three phases: next subsection 3.1 presents panel (a) of Figure 1, whilst subsection 3.2 described both panels (b) and (c) of the same figure. Furthermore, subsection 3.3 introduces how to use our approach in inference. 3.1. Supervised Training Phase The supervised phase in (Figure 1a) is crucial for maintaining classification accuracy and preventing drift in the unsupervised learning process. It occurs at every epoch, utilizing the limited labeled data to train simultaneously the encoder of the generator ğ¸G , and to perform classification. We leverage deep supervision by adding classification tail to the bottleneck of the generators encoder ğ¸G denoted as set of interconnected neurons in panel (a) of Figure 1; it is located at the bottleneck because Guido Manni et al. Page 3 of 15 Figure 1: Three-phase framework for semi-supervised learning with limited labeled data. Our approach integrates three specialized networks: generator (G ) for class-conditioned image synthesis, discriminator (D) for authenticity assessment and classification signalling, and dedicated classifier (C ) for the primary classification task. The generator comprises an encoder (EG ) and decoder (DG ) for image-to-image translation.(a) Initial Training Phase: Joint training of these three networks using limited labeled data. (b) Self-Supervised Pre-training: Two-part approach combining (1) ensemble-based pseudo-labeling using confidence-weighted voting and temporal ensembling, and (2) class-conditioned image translation with Wasserstein GAN training and cycle consistency. (c) Synthetic Data Enhancement Phase: Training classifier on generated samples using target class conditions as supervision signals, where (1) input images paired with target class conditions are processed by the generator to create synthetic training samples, and (2) generated samples are used to train the classifier with target class conditions as supervision signals. Fire symbols indicate trainable networks while ice symbols represent frozen weights during respective training phases. it serves as the information compression point between the encoder and decoder paths. By applying classification supervision at this critical juncture, we ensure that the most compact representation in the network encodes both structural information needed for generation and semantic information required for classification. The supervised loss function use by all the three networks ) combines four specialized loss terms, each addressing ( specific challenge in few-shot learning and weighted by coefficients to balance their contribution. ğ‘ ğ‘¢ğ‘ ğ‘ ğ‘¢ğ‘ = ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ + ğ›¼ ğ‘šğ‘¢ğ‘¡ğ‘¢ğ‘ğ‘™ + ğ›½ ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ + ğ›¾ ğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ (1) ğ‘šğ‘¢ğ‘¡ğ‘¢ğ‘ğ‘™ The prototype loss ( ) helps create discriminağ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ tive class-specific features by learning robust prototypical representations for each class. The mutual learning loss ) enables knowledge sharing between the three ( models, leveraging their complementary perspectives on the data. The entropy minimization loss ( ) encourages the models to make confident predictions, helping combat the uncertainty inherent in limited-data scenarios. Finally, the mixup loss ( ) provides regularization through data augmentation, helping prevent overfitting which is particularly crucial when training with few samples. The rest of this section details the computation of each of these ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ ğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ four losses functions. The prototype loss creates discriminative classspecific representations by comparing softmax probabilities with class prototypes: ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ ğ‘ğ‘Ÿğ‘œğ‘¡ğ‘œğ‘¡ğ‘¦ğ‘ğ‘’ = ğ‘ ğ‘–= log exp(ğ‘‘(ğ‘ğ‘‡ (ğ‘¥ğ‘–), ğ‘ğ‘¦ğ‘– ğ‘˜=1 exp(ğ‘‘(ğ‘ğ‘‡ (ğ‘¥ğ‘–), ğ‘ğ‘˜)) )) ğ¾ (2) where ğ‘ğ‘‡ (ğ‘¥ğ‘–) represents the softmax probabilities of input ğ‘¥ğ‘– with temperature ğ‘‡ , ğ‘ğ‘˜ is the prototype of class ğ‘˜ computed as the mean of the class probabilities: ğ‘ğ‘˜ = 1 ğ‘†ğ‘˜ ğ‘¥ğ‘–ğ‘†ğ‘˜ ğ‘ğ‘‡ (ğ‘¥ğ‘–) (3) where ğ‘†ğ‘˜ is the set of samples from class ğ‘˜. The distance function ğ‘‘(, ) is defined as the negative sum of squared differences: ğ‘‘(ğ‘ğ‘‡ (ğ‘¥), ğ‘ğ‘˜) = ğ¾ (ğ‘ğ‘‡ ,ğ‘—(ğ‘¥) ğ‘ğ‘˜,ğ‘—)2 ğ‘—=1 (4) where ğ¾ is the total number of classes. The mutual learning loss facilitates knowledge transfer between models through combination of supervised ğ‘šğ‘¢ğ‘¡ğ‘¢ğ‘ğ‘™ Guido Manni et al. Page 4 of cross-entropy and KL divergence: ğ‘šğ‘¢ğ‘¡ğ‘¢ğ‘ğ‘™ = ğ‘ğ‘’ + ğœ†ğ‘˜ğ‘™ ğ‘˜ğ‘™ (5) where ğœ†ğ‘˜ğ‘™ term, ğ‘ğ‘’ is the weight coefficient for the KL divergence is the sum of cross-entropy losses for each model: ğ‘ğ‘’ = CE(ğ‘ğ‘š(ğ‘¥), ğ‘¦) ğ‘š{ğ¸G ,D,C } (6) ğ‘˜ğ‘™ and is the symmetric KL divergence between each models probabilities and the average of other models probabilities: ğ‘˜ğ‘™ = ğ‘š{ğ¸G ,D,C } KL(ğ‘ğ‘š(ğ‘¥) 1 2 ğ‘›ğ‘š ğ‘ğ‘›(ğ‘¥)) (7) The entropy minimization loss promotes confiğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ dent predictions: ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦ = ğ¾ ğ‘˜=1 ğ‘ğ‘˜(ğ‘¥) log(ğ‘ğ‘˜(ğ‘¥)) (8) Finally, the mixup loss provides regularization by ğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ training on interpolated samples and labels. For each pair of is samples (ğ‘¥ğ‘–, ğ‘¦ğ‘–) and (ğ‘¥ğ‘—, ğ‘¦ğ‘—), the interpolation weight ğœ†ğ‘šğ‘–ğ‘¥ sampled from Beta distribution: ğœ†ğ‘šğ‘–ğ‘¥ Beta(ğ›¼ğ‘šğ‘–ğ‘¥, ğ›¼ğ‘šğ‘–ğ‘¥) (9) The Beta distribution is particularly suitable for generating interpolation weights as it is bounded between [0,1] and can be symmetric around 0.5, ensuring balanced mixing of samples while maintaining their relative contributions. The hyperparameter ğ›¼ğ‘šğ‘–ğ‘¥ in the Beta distribution controls the strength of interpolation - higher values of ğ›¼ğ‘šğ‘–ğ‘¥ lead to interpolation weights closer to 0.5, while lower values favour weights closer to 0 or 1. This weight is then used to create interpolated samples and labels: ğ‘¥ = ğœ†ğ‘šğ‘–ğ‘¥ğ‘¥ğ‘– + (1 ğœ†ğ‘šğ‘–ğ‘¥)ğ‘¥ğ‘— ğ‘¦ = ğœ†ğ‘šğ‘–ğ‘¥ğ‘¦ğ‘– + (1 ğœ†ğ‘šğ‘–ğ‘¥)ğ‘¦ğ‘— The mixup loss is then computed as: ğ‘šğ‘–ğ‘¥ğ‘¢ğ‘ = ğ‘ğ‘’(ğ‘( ğ‘¥), ğ‘¦) where ğ‘ğ‘’ is the cross-entropy loss. (10) (11) (12) 3.2. Unsupervised Training Phase The unsupervised training phase, illustrated in panel (b) of Figure 1, executes every ğœ‡ epochs. This phase leverages unlabeled data through an image-to-image translation framework with three distinct stages. First, we use an ensemble-based pseudo-labeling mechanism to estimate class probabilities for unlabeled samples (subsection 3.2.1). These probability estimates then guide class-conditioned image translation process, which learns to generate class-specific variations of input images (subsection 3.2.2). Finally, we employ these generated samples in synthetic data enhancement phase. Here, the synthetic images serve as additional training data to strengthen the classifiers ability to distinguish between classes (subsection 3.2.3). 3.2.1. Ensemble-based Pseudo-labeling To effectively utilize unlabeled samples, we require reliable class probability estimates. Our approach addresses three key challenges: (1) quantifying model uncertainty, (2) aggregating predictions from multiple models, and (3) maintaining temporal stability throughout training. We tackle these challenges through an ensemble mechanism (Figure 1b) that integrates confidence-weighted voting, temporal ensembling, and adaptive thresholding. At epoch ğ‘¡, each unlabeled image ğ‘¥ğ‘– from ğ·ğ‘¢ğ‘›ğ‘  = { ğ‘¥ğ‘–}ğ‘€ ğ‘–=1 is processed by the models trained during the previous initial training phase. We apply temperature scaling to obtain calibrated class probabilities: ğ‘ğ‘š( ğ‘¥ğ‘–, ğ‘¡) = softmax ) ( ğ‘™ğ‘š( ğ‘¥ğ‘–, ğ‘¡) ğ‘‡ (13) where ğ‘™ğ‘š( ğ‘¥ğ‘–, ğ‘¡) represents the logits (raw pre-softmax outputs) at epoch ğ‘¡. The from model ğ‘š {D, } for input ğ‘¥ğ‘– temperature parameter ğ‘‡ controls prediction sharpness: lower values (ğ‘‡ < 1) yield more confident predictions, while higher values (ğ‘‡ > 1) produce smoother probability distributions. Each models prediction reliability is quantified using an entropy-based confidence measure: ğ‘ğ‘š( ğ‘¥ğ‘–, ğ‘¡) = 1 ğ»(ğ‘ğ‘š( ğ‘¥ğ‘–, ğ‘¡)) ğ»ğ‘šğ‘ğ‘¥ (14) where the entropy ğ»(ğ‘ğ‘š( ğ‘¥ğ‘–, ğ‘¡)) is computed as: ğ»(ğ‘ğ‘š( ğ‘¥ğ‘–, ğ‘¡)) = ğ¾ ğ‘˜= ğ‘š ( ğ‘¥ğ‘–, ğ‘¡) log ğ‘(ğ‘˜) ğ‘(ğ‘˜) ğ‘š ( ğ‘¥ğ‘–, ğ‘¡) (15) and ğ»ğ‘šğ‘ğ‘¥ = log ğ¾ represents the maximum possible entropy for ğ¾ classes. The term ğ‘(ğ‘˜) ğ‘š ( ğ‘¥ğ‘–, ğ‘¡) denotes model ğ‘šs predicted probability for class ğ‘˜. This confidence score ranges from 0 (complete uncertainty) to 1 (complete certainty). We combine predictions from both models by weighting each according to its confidence score: ğ‘ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘’ğ‘‘( ğ‘¥ğ‘–, ğ‘¡) = ğ‘š{D,C } ğ‘ğ‘š( ğ‘¥ğ‘–, ğ‘¡) ğ‘ğ‘š( ğ‘¥ğ‘–, ğ‘¡) ğ‘š{D,C } ğ‘ğ‘š( ğ‘¥ğ‘–, ğ‘¡) (16) The denominator ensures proper normalization. This weighting scheme allows more confident models to contribute more strongly to the final prediction. To enhance prediction stability, we incorporate historical information by blending current predictions with past predictions: ğ‘ğ‘’ğ‘›ğ‘ ( ğ‘¥ğ‘–, ğ‘¡) = ğ›¼ ğ‘ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘’ğ‘‘( ğ‘¥ğ‘–, ğ‘¡)+(1ğ›¼)ğ‘ğ‘’ğ‘šğ‘( ğ‘¥ğ‘–, ğ‘¡1) (17) Guido Manni et al. Page 5 of 15 Here, ğ‘ğ‘’ğ‘šğ‘( ğ‘¥ğ‘–, ğ‘¡ 1) captures the temporal history through an exponential moving average (EMA) of past ensemble predictions. The parameter ğ›¼ [0, 1] balances current information (higher ğ›¼) against historical stability (lower ğ›¼). This temporal smoothing prevents abrupt prediction changes that could destabilize the training process. After computing the current ensemble prediction, we update the EMA: ğ‘ğ‘’ğ‘šğ‘( ğ‘¥ğ‘–, ğ‘¡) = ğ›½ ğ‘ğ‘’ğ‘šğ‘( ğ‘¥ğ‘–, ğ‘¡ 1) + (1 ğ›½) ğ‘ğ‘’ğ‘›ğ‘ ( ğ‘¥ğ‘–, ğ‘¡) (18) The momentum parameter ğ›½ [0, 1] determines the temporal memory span. Large values (e.g., ğ›½ = 0.99) maintain longer memory for stable predictions, while smaller values allow faster adaptation to recent changes. With reliable probability estimates in hand, we select only the most confident predictions for pseudo-labeling: where ğœ†ğ‘Ÿğ‘’ğ‘ and the adversarial loss metric to assess image realism: is the weight coefficient for the reconstruction loss, uses the Wasserstein distance ğ‘ğ‘‘ğ‘£ ğ‘ğ‘‘ğ‘£ = ğ”¼ ğ‘¥ğ·ğ‘¢ğ‘›ğ‘  [D(G ( ğ‘¥, ğ‘§ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡))], (23) Next, the classification losses ğ‘ğ‘™ğ‘  ensure accurate conditioning on target classes using cross-entropy from both the discriminator and generator classifiers: and ğ‘ğ‘™ğ‘  ğ‘ğ‘™ğ‘  = ğ”¼ ğ‘¥ğ·ğ‘¢ğ‘›ğ‘  ğ‘ğ‘™ğ‘  = ğ”¼ ğ‘¥ğ·ğ‘¢ğ‘›ğ‘  ğ¾ ğ‘˜=1 ğ¾ ğ‘˜=1 ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ log(ğ‘D ğ‘§ğ‘˜ ğ‘˜ (G ( ğ‘¥, ğ‘§ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡))) (24) ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ log(ğ‘G ğ‘§ğ‘˜ ğ‘˜ (G ( ğ‘¥, ğ‘§ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡))) (25) (ğ‘¡) = { ğ‘¥ğ‘– ğ·ğ‘¢ğ‘›ğ‘  max ğ‘˜ (ğ‘(ğ‘˜) ğ‘’ğ‘›ğ‘ ( ğ‘¥ğ‘–, ğ‘¡)) > ğœ(ğ‘¡)} (19) where maxğ‘˜(ğ‘(ğ‘˜) ğ‘’ğ‘›ğ‘ ( ğ‘¥ğ‘–, ğ‘¡)) is the highest class probability from the ensemble prediction. The subset (ğ‘¡) contains selected samples at epoch ğ‘¡, and the adaptive threshold ğœ(ğ‘¡) is defined as: ğœ(ğ‘¡) = ğ‘„ğœŒ({max (ğ‘(ğ‘˜) ğ‘’ğ‘›ğ‘ ( ğ‘¥ğ‘–, ğ‘¡)) ğ‘¥ğ‘– ğ·ğ‘¢ğ‘›ğ‘ }) ğ‘˜ (20) Here, ğ‘„ğœŒ denotes the ğœŒ-th percentile of maximum probabilities across all unlabeled samples in the current batch, where ğœŒ [0, 1] is the percentile threshold parameter. This percentile-based approach automatically adjusts to the models current performance level. It selects (1ğœŒ)100% of the most confident samples. For instance, setting ğœŒ = 0.8 selects the top 20% most confident predictions. This adaptive mechanism prevents error accumulation from unreliable pseudo-labels while naturally accommodating the models improving performance. For each selected confident sample ğ‘¥ğ‘– (ğ‘¡), we create discrete pseudo-label: ğ‘¦ğ‘– = one-hot(arg max ğ‘˜ ğ‘(ğ‘˜) ğ‘’ğ‘›ğ‘ ( ğ‘¥ğ‘–, ğ‘¡)) (21) This produces one-hot encoded pseudo-label ğ‘¦ğ‘– {0, 1}ğ¾1 for each unlabeled sample ğ‘¥ğ‘– . The complete set of pseudo- (ğ‘¡) labels { ğ‘¦ğ‘–} for all selected samples then feeds into the ğ‘–=1 subsequent class-conditioned image translation process. 3.2.2. Class-conditioned Image Translation Using the pseudo-labels { ğ‘¦ğ‘–}ğ‘€ ğ‘–=1 obtained from our ensemble mechanism, we implement class-conditioned image translation process that leverages and working in tandem (Figure 1b right). Now the generator , which consists of complete U-Net architecture, not just the encoder as in the supervised phase, learns to perform class-conditioned image translation while preserving semantic features relevant to classification. Its training objective combines four components: ğ‘¢ğ‘›ğ‘  ğ‘¢ğ‘›ğ‘  = ğ‘ğ‘‘ğ‘£ + ğ‘ğ‘™ğ‘  + ğ‘ğ‘™ğ‘  + ğœ†ğ‘Ÿğ‘’ğ‘ ğ‘Ÿğ‘’ğ‘ (22) ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡ is the ğ‘˜ğ‘¡â„ element of the target class one-hot represents the probability for class from the represents the probability for class where ğ‘§ğ‘˜ encoding, ğ‘D ğ‘˜ discriminator, and ğ‘G ğ‘˜ from the generators classifier. The reconstruction loss maintains content consistency using L1 distance: ğ‘Ÿğ‘’ğ‘ ğ‘Ÿğ‘’ğ‘ = ğ”¼ ğ‘¥ğ·ğ‘¢ğ‘›ğ‘  (G ( ğ‘¥, ğ‘§ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡), ğ‘§ğ‘ ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’) ğ‘¥1] [ is the one-hot encoding of the most probable class according to ğ‘ğ‘’ğ‘›ğ‘ ( ğ‘¥, ğ‘¡), connecting this translation process with our previous pseudo-labeling step. where ğ‘§ğ‘ ğ‘œğ‘¢ğ‘Ÿğ‘ğ‘’ (26) The discriminator serves dual role: it assesses image realism through Wasserstein distance metric while also providing classification signals. Its objective function reflects these dual tasks: = ğ‘ğ‘‘ğ‘£ + ğœ†ğ‘ğ‘™ğ‘  ğ‘ğ‘™ğ‘  + ğœ†ğ‘”ğ‘ ğ‘”ğ‘ (27) and ğœ†ğ‘”ğ‘ where ğœ†ğ‘ğ‘™ğ‘  are weight coefficients for the classification loss and gradient penalty terms, respectively. Its adversarial component implements the Wasserstein distance as follows: ğ‘ğ‘‘ğ‘£ = ğ”¼ ğ‘¥ğ·ğ‘¢ğ‘›ğ‘  The gradient penalty term [D( ğ‘¥)]+ğ”¼ ğ‘¥ğ·ğ‘¢ğ‘›ğ‘  [D(G ( ğ‘¥, ğ‘§ğ‘¡ğ‘ğ‘Ÿğ‘”ğ‘’ğ‘¡))] (28) enforces the Lipschitz ğ‘”ğ‘ constraint: ğ‘”ğ‘ = ğ”¼ ğ‘¥ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ [( ğ‘¥ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ D( ğ‘¥ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘)2 1)2] (29) where ğ‘¥ğ‘–ğ‘›ğ‘¡ğ‘’ğ‘Ÿğ‘ is sampled uniformly along straight lines between pairs of real and generated images. 3.2.3. Synthetic Data Enhancement After establishing the image translation process, we leverage translated images as form of synthetic labeled data to enhance the classifiers performance (Figure 1c). The key process involves taking unlabeled input images { ğ‘¥ğ‘–}ğ‘€ ğ‘–=1 from uniform and sampling random target classes {ğ‘§ğ‘–}ğ‘ƒ ğ‘–=1 Guido Manni et al. Page 6 of 15 distribution over the ğ¾ classes in one-hot format, where ğ‘ƒ is the number of synthetic samples generated. We then use these randomly sampled classes to condition the generator to translate the original images of unknown classes. The . resulting translated images have known target classes {ğ‘§ğ‘–}ğ‘ƒ ğ‘–=1 These translated images with known target classes can then be used to train the classifier in supervised manner: = ğ‘ğ‘™ğ‘ (C (G ( ğ‘¥, ğ‘§)), ğ‘§) (30) where ğ‘§ represents the target class condition provided to the generator for synthetic data generation. This approach provides us with effectively labeled training samples, since we know exactly what class condition was used to generate each image. Importantly, as demonstrated in [21], the visual quality of generated images is not critical for our classification objective: instead, we focus on ensuring that the translation process captures and preserves discriminative features that are useful for classification. 3.3. Inference configuration Our approach in the inference phase can be deployed in different configurations that exploits the model trained as reported in section 3. In particular, in the rest of the manuscript we consider the following two set-ups for inference: SPARSE: it used only the classifier . SPARSEens: it exploits both the discriminator and the classifier , which are combined in late fusion by averaging the estimates of posterior probabilities per class. It is worth noting that in our ensemble configuration, we should also consider the potential use of the generators encoder EG . However, since it is trained to minimizer loss function that balances both generation and classification objectives, this results in performance degradation - finding we experimentally verified, though omitted from the manuscript for conciseness. 4. Experimental Configuration This section describes our experimental methodology: it details the materials (subsection 4.1), followed by our training configuration (subsection 4.2). 4.1. Materials We conducted experiments using eleven datasets from MedMNIST repository [22]: BloodMNIST, BreastMNIST, ChestMNIST, DermaMNIST, OCTMNIST, OrganAMNIST, OrganCMNIST, OrganSMNIST, PathMNIST, PneumoniaMNIST and TissueMNIST. As shown in Table 1, these datasets span different medical imaging modalities and classification tasks, with varying number of classes (2-11) and dataset sizes (from hundreds to hundreds of thousands of samples). To evaluate our methods effectiveness in extremely low labeled-data regimes, we conducted experiments across four few-shot settings: 5-shot, 10-shot, 20-shot, and 50-shot per class, with 5-shot representing the most challenging scenario. For each N-shot setting, we constructed the training set using labeled samples per class, with the remaining samples treated as unlabeled data. For data preprocessing, we applied transformation pipeline consisting of random horizontal flipping for data augmentation and tensor conversion, with input images maintaining their original 128128 pixel resolution. To ensure reproducibility, we utilized the original validation/test splits provided by the MedMNIST authors. 4.2. Training Configuration The training schedule alternates between supervised and unsupervised learning phases. The supervised phase occurs at every epoch, while the unsupervised phase is executed every ğœ‡ epochs as already described in section 3. All models were trained for 1000 epochs using the AdamW optimizer, , maintaining identical configurations throughout all experiments which are reported in table 2. During training, we implemented model checkpoint strategy that saved the model state achieving the best validation accuracy, which was then used for final evaluation. For all the models, we did not investigate any hyperparameter configuration since their tuning is out of the scope of this manuscript. Nevertheless, because the No Free Lunch Theorem for optimization [23] states that no universal set of hyperparameters will optimize models performance across all possible datasets, this approach ensures fair comparison among all the approaches. 5. Results This section presents the experimental results evaluating our frameworks effectiveness in extremely low labeled data regimes in medical image classification. We compare our approach against the six state-of-the-art semi-supervised GAN architectures which are already presented in section 2, using their original implementations without additional optimization or modifications to ensure fair comparison with our method. To assess model performance, we employ classification accuracy per class as our primary evaluation metric. For each dataset, we first compute the accuracy across all available classes and then the final score is obtained by averaging these accuracies across all datasets, providing robust evaluation of the models generalization capabilities across varying medical imaging tasks, modalities, and data distributions. Individual results for each dataset are presented in the appendix. Table 3 presents the performance of all models across different few-shot settings, where we systematically vary the number of samples per class from extremely limited (5-shot) to more moderate (50-shot) situations. The first two rows show the results of our approach using only the classifier (SPARSE) or using the ensemble between the discriminator and (SPARSEens). The following six rows displays the results of the six competitors. We note that the two variants of our approach consistently outperform existing competitors Guido Manni et al. Page 7 of 15 Table 1 Characteristics of MedMNIST datasets used in our experiments."
        },
        {
            "title": "Task Type",
            "content": "# Samples"
        },
        {
            "title": "Training",
            "content": "Val/Test BloodMNIST BreastMNIST ChestMNIST DermaMNIST OCTMNIST OrganAMNIST OrganCMNIST OrganSMNIST PathMNIST PneumoniaMNIST Chest X-Ray TissueMNIST Blood Cell Microscope Breast Ultrasound Chest X-Ray Dermatoscope Retinal OCT Abdominal CT Abdominal CT Abdominal CT Colon Pathology Multi-Class (8) Binary-Class (2) Binary-Class (2) Multi-Class (7) Multi-Class (4) Multi-Class (11) Multi-Class (11) Multi-Class (11) Multi-Class (9) Binary-Class (2) Kidney Cortex Microscope Multi-Class (8) 17,092 780 112,120 10,015 109,309 58,830 23,583 25,211 107,180 5,856 236,386 11,959 1,712/3,421 78/156 78,468 11,219/22,433 1,003/2,005 10,832/1,000 6,491/17,778 2,392/8,216 2,452/8,827 10,004/7,180 524/624 165,466 23,640/47,280 7,007 97,477 34,561 12,975 13,932 89,996 4,708 Table 2 Training hyperparameters used in our experiments Table 3 Model Performance Across Different Few-Shot Settings Parameter General Training Parameters Training Epochs Base Learning Rate Optimizer ğœ‡ (unsupervised phase frequency) ğ‘‡ (temperature parameter) Value 1000 0.0002 AdamW 10 2.0 Loss Weights for Supervised Objective ğ›¼ (mutual learning loss weight) ğ›½ (entropy minimization loss weight) ğ›¾ (mixup loss weight) ğœ†ğ‘˜ğ‘™ (KL divergence weight) mixup parameters ğ›¼ğ‘šğ‘–ğ‘¥ (beta distribution parameter) loss weights for generator ğœ†ğ‘Ÿğ‘’ğ‘ (reconstruction loss weight) loss weights for discriminator ğœ†ğ‘ğ‘™ğ‘  (classification loss weight) ğœ†ğ‘”ğ‘ (gradient penalty weight) Ensemble Parameters ğ›¼ (temporal ensemble weight) ğ›½ (EMA momentum) ğœŒ (percentile threshold) 0.1 0.01 0.5 0.5 0.2 10.0 1.0 10.0 0.6 0.99 0.75 across all settings, and the ensemble setting (SPARSEens) achieves the best performance in all scenarios. As the number of labeled samples increases from 5 to 50 shots per class, we observe steady improvement in performance across all models, though the relative advantage of our approach remains consistent. To further investigate these findings, the rest of this section examines our improvements in the 5-shot setting (section 5.1), which represents the most challenging scenario where Model Average Accuracy per class 5-shot 10-shot 20-shot 50-shot SPARSE SPARSEens SGAN [13] MatchGAN [14] EC-GAN [16] TripleGAN [15] SEC-GAN [17] CISSL [18] 63.21 66.22 25.80 39.88 35.09 64.23 58.73 45.84 68.50 70. 25.96 48.73 34.66 68.79 63.79 46.80 73.44 75.71 24.92 51.90 34.29 71.40 66.95 49.19 77.15 78.28 27.28 54.15 47.41 76.25 73.30 51.20 the extreme scarcity of labeled data tests the true effectiveness of semi-supervised learning. We then complement the results with detailed analysis in the 50-shot setting (section 5.2), which helps us understand how our method scales when more labeled data becomes available. Statistical significance testing of the performance differences is presented within these analyses. Finally, section 5.3 investigates how the frequency of unsupervised training affects model performance. 5.1. Performance in 5-shot We now deepen our analysis on the most challenging 5shot learning scenario, where only 5 labeled samples per class are available for training while the remaining samples are treated as unlabeled. To provide robust statistical evaluation, we employ the Wilcoxon signed-rank test, non-parametric method that compares paired accuracy values across our 11 datasets, with Benjamini-Hochberg FDR correction for multiple comparisons. Table 4 presents the results of this statistical comparison, with the upper triangular part showing corrected ğ‘-values with effect sizes (ğ‘Ÿ) and their interpretations, where Guido Manni et al. Page 8 of 15 Table 4 5-Shot statistical comparison of model performance. The upper triangular part shows p-values from statistical comparison (significant values ğ‘ < 0.05 highlighted in bold), while the lower triangular part shows Win-Tie-Loss (W-T-L) statistics. The r-value represents the effect size (Pearsons correlation coefficient) with interpretations: small (ğ‘Ÿ < 0.3), medium (0.3 ğ‘Ÿ < 0.5), large (0.5 ğ‘Ÿ < 0.7), and very large (ğ‘Ÿ 0.7). Best performing model is highlighted in bold. Model Statistical Comparison (5 shot) SPARSE SPARSEens SGAN MatchGAN CISSL SEC-GAN TripleGAN ECGAN SPARSE - 4.1e-02 r=0.818 (very large) 2.1e-02 r=0.818 (very large) 6.0e-02 r=0.636 (very large) 8.7e-02 r=0.455 (large) 2.4e-01 r=0.455 (large) SPARSEens (10-0-1) - 2.1e-02 r=1.000 (very large) 4.4e-02 r=0.636 (very large) 2.2e-02 r=0.818 (very large) 5.3e-02 r=0.636 (very large) 7.7e-01 r=0.091 (small) 1.5e-01 r=0.455 (large) 4.4e-02 r=0.636 (very large) 3.5e-02 r=0.800 (very large) SGAN (1-0-10) (0-1-10) - 2.2e-02 r=0.636 (very large) 2.2e-02 r=0.636 (very large) 2.1e-02 r=0.818 (very large) 2.1e-02 r=0.818 (very large) 4.4e-02 r=0.600 (very large) MatchGAN (2-0-9) (2-0-9) (9-0-2) - 2.6e-01 r=0.273 (medium) 6.0e-02 r=0.455 (large) CISSL (3-0-8) (1-0-10) (9-0-2) (7-0-4) - 4.1e-02 r=0.636 (very large) SEC-GAN (3-0-8) (2-0-9) (10-0-1) (8-0-3) (9-0-2) - 6.0e-02 r=0.455 (large) 4.4e-02 r=0.455 (large) 8.7e-02 r=0.455 (large) TripleGAN (5-0-6) (3-0-8) (10-0-1) (8-0-3) (8-0-3) (8-0-3) - 1.0e-01 r=0.636 (very large) 4.4e-02 r=0.455 (large) 2.7e-02 r=0.818 (very large) 4.4e-02 r=0.455 (large) ECGAN (2-0-9) (1-1-9) (8-1-2) (2-0-9) (3-0-8) (1-0-10) (3-0-8) - significant results (ğ‘ < 0.05) are highlighted in bold, and the lower triangular part displaying Win-Tie-Loss (W-T-L) statistics from the row models perspective when compared against the column model. The primary finding is that our ensemble model, SPARSEens, achieves statistically significant superiority over most competing approaches, also with number of wins larger than losses, demonstrating robust performance across diverse medical imaging datasets. We now discuss the effectiveness of the ensemble approach by comparing SPARSEens to its base model, SPARSE. The results show that the ensemble model demonstrates statistically significant improvement (ğ‘ = 4.1 102, ğ‘Ÿ = 0.818), securing wins on 10 out of 11 datasets with only single loss. This marked improvement is achieved through ensemble averaging at inference time. Although both configurations share an identical training procedure, the classifier and discriminator networks develop complementary decision boundaries due to their architectural differences. Averaging their predictions effectively reduces prediction variance, critical advantage in the extreme 5-shot regime. Let us now focus on how SPARSEens performs with respect to the external competitors. The analysis shows that SPARSEens obtains statistically significant improvements over range of generative models, including SGAN (ğ‘ = 2.1 102, ğ‘Ÿ = 1.000), MatchGAN (ğ‘ = 4.4 102, ğ‘Ÿ = 0.636), CISSL (ğ‘ = 2.2 102, ğ‘Ÿ = 0.818) and EC-GAN (ğ‘ = 3.5 102, ğ‘Ÿ = 0.800). The largest performance gap is observed against SGAN, where our method was superior across all 11 datasets. The closest competition came from TripleGAN and SEC-GAN; while the differences did not reach statistical significance (ğ‘ = 1.5101 and ğ‘ = 5.3102, respectively), SPARSEens still outperformed them on the majority of datasets with Win-Tie-Loss equal to of 8-0-3 and 9-0-2. The consistent outperformance of SPARSEens over this diverse set of competitors stems from crucial methodological distinction. All competing methods are purely generative, creating images de novo from noise vector. In contrast, our framework employs image-to-image translation, which modifies existing real, unlabeled images. This strategy preserves the authentic and complex anatomical features of the medical data, providing more robust training signal. This fundamental advantage is further amplified when compared to two-player models like SGAN and MatchGAN, which suffer from an internal optimization conflict by tasking single network with both discrimination and classification. Our three-player design avoids this issue. Even when compared to advanced three-player models like TripleGAN that also separate these tasks, our methods reliance on translating real images, rather than generating them from noise, appears to be the decisive factor for success in this data-scarce context. Guido Manni et al. Page 9 of Table 5 50-Shot statistical comparison of model performance. The upper triangular part shows p-values from statistical comparison (significant values ğ‘ < 0.05 highlighted in bold), while the lower triangular part shows Win-Tie-Loss (W-T-L) statistics. The r-value represents the effect size (Pearsons correlation coefficient) with interpretations: small (ğ‘Ÿ < 0.3), medium (0.3 ğ‘Ÿ < 0.5), large (0.5 ğ‘Ÿ < 0.7), and very large (ğ‘Ÿ 0.7). Best performing model is highlighted in bold. Model Statistical Comparison (50 shot) SPARSE SPARSEens SGAN MatchGAN CISSL SEC-GAN TripleGAN ECGAN SPARSE - 1.0e-01 r=0.636 (very large) 2.0e-03 r=1.000 (very large) 2.0e-03 r=1.000 (very large) 5.0e-03 r=0.818 (very large) 1.5e-02 r=0.636 (very large) 3.5e-01 r=0.455 (large) 4.0e-03 r=0.818 (very large) SPARSEens (9-0-2) - 2.0e-03 r=1.000 (very large) 2.0e-03 r=1.000 (very large) 2.0e-03 r=1.000 (very large) 2.0e-03 r=1.000 (very large) 1.0e-01 r=0.636 (very large) 2.0e-03 r=1.000 (very large) SGAN (0-0-11) (0-0-11) - 8.0e-03 r=0.818 (very large) 2.0e-03 r=1.000 (very large) 2.0e-03 r=1.000 (very large) 2.0e-03 r=1.000 (very large) 1.1e-02 r=0.818 (very large) MatchGAN (0-0-11) (0-0-11) (10-0-1) - 9.0e-01 r=0.091 (small) 4.0e-03 r=0.818 (very large) 2.0e-03 r=1.000 (very large) CISSL (1-0-10) (0-0-11) (11-0-0) (6-0-5) - 1.9e-02 r=0.455 (large) 8.0e-03 r=0.636 (very large) 1.7e-01 r=0.455 (large) 5.5e-01 r=0.091 (small) SEC-GAN (2-0-9) (0-0-11) (11-0-0) (10-0-1) (8-0-3) - 3.1e-02 r=0.818 (very large) 4.0e-03 r=0.818 (very large) TripleGAN (3-0-8) (2-0-9) (11-0-0) (11-0-0) (9-0-2) (10-0-1) - 2.0e-03 r=1.000 (very large) ECGAN (1-0-10) (0-0-11) (10-0-1) (3-0-8) (5-0-6) (1-0-10) (0-0-11) - 5.2. Performance in 50-shot We extend our analysis to the 50-shot setting to examine how model performance scales with increased labeled data availability, while still remaining within the low-labeled data regime. This configuration provides ten times more labeled samples per class compared to the 5-shot scenario, allowing us to assess the scaling properties of our methods. Table 5 presents the statistical comparison results using the Wilcoxon signed-rank test with Benjamini-Hochberg FDR correction. The table follows the same format as the 5-shot analysis, with ğ‘-values and effect sizes in the upper triangular portion and Win-Tie-Loss statistics in the lower triangular portion. The primary finding reveals that SPARSEens maintains its statistical superiority over most competing approaches even as more labeled data becomes available. When comparing the two variants of our approach, SPARSEens achieves Win-Tie-Loss equal to of 9-0-2 against the base SPARSE model. However, the ğ‘-value of 1.0 101 (ğ‘Ÿ = 0.636) indicates no statistical significance: this narrowing gap can be attributed to the changing role of ensemble averaging in different data regimes. With only 5 shots per class, individual model predictions are inherently less reliable and more prone to overfitting, making the ensemble approach particularly valuable for reducing prediction variance. The discriminator and classifier develop highly complementary decision boundaries due to the scarcity of supervised signals. However, with 50 labeled samples per class, the classifier receives sufficient supervision to develop more stable and generalizable representations independently, reducing its reliance on the discriminators complementary perspective. Examining SPARSEenss performance against we note that it achieves statistically significant improvements over SGAN (ğ‘ = 2.0 103, ğ‘Ÿ = 1.000), MatchGAN (ğ‘ = 2.0 103, ğ‘Ÿ = 1.000), CISSL (ğ‘ = 2.0 103, ğ‘Ÿ = 1.000), and EC-GAN (ğ‘ = 2.0 103, ğ‘Ÿ = 1.000), with Win-TieLoss equal to 11-0-0 against each. These results represent stronger statistical evidence compared to the 5-shot setting, where ğ‘-values ranged from 2.1 102 to 4.4 102. Furthermore SPARSEens achieves statistical significance against SEC-GAN (ğ‘ = 2.0 103, ğ‘Ÿ = 1.000, W-T-L: 00-11), similarly to what happen in the 5-shot setting. When comparing against TripleGAN, we notice that the Win-TieLoss ratio is 2-0-9 in favor of our approach, similar to the 5-shot settings, but now the performance differences are not statistically significant at ğ‘ = 0.1. These results suggest that performance differences become more consistent across datasets as labeled data increases. The patterns observed in this comparison can be attributed to differences in how methods utilize additional supervision. Our image-to-image translation approach leverages the increased labeled data to learn more accurate class-conditional transformations. With 50 labeled samples per class, the generator receives Guido Manni et al. Page 10 of 15 Figure 2: Average classification accuracy as function of the number of labeled samples per class (shots) for different unsupervised training frequencies (ğœ‡). Results are averaged across all eleven MedMNIST datasets and computed on the validation set. The parameter ğœ‡ controls how frequently the unsupervised training phase is executed, with ğœ‡ = 0 representing supervised learning only (no unsupervised phase), ğœ‡ = 1 indicating unsupervised training at every epoch, and higher values (ğœ‡ 10, 25, 50, 100) representing unsupervised training every ğœ‡ epochs. stronger supervision signals, enabling better preservation of discriminative features during translation. Additionally, the quality of pseudo-labels generated in the unsupervised phase improves due to more reliable initial classifiers. 5.3. Impact of Unsupervised Training Frequency In our approach, the hyperparameter ğœ‡ controls how frequently the unsupervised training phase is executed, with the unsupervised phase running every ğœ‡ epochs. We investigate how varying this parameter affects our models performance across different few-shot settings, as it represents the balance between supervised and unsupervised learning signals in our semi-supervised framework. Figure 2 presents the average accuracy across all datasets as function of the number of shots per class for different values of ğœ‡. These results were obtained using the validation set to avoid any bias that could arise from hyperparameter selection on the test set. The key finding is that ğœ‡ = 10 consistently achieves the highest performance across all shot settings. In the 5-shot setting, ğœ‡ = 10 reaches 74.5% accuracy, representing an 8.5% point improvement over supervised-only learning (ğœ‡ = 0 at 66.0%). This performance advantage narrows as labeled data increases, reducing to approximately 2% points in the 50-shot setting (85.1% vs 83.1%). The figure reveals distinct patterns in how unsupervised learning frequency affects performance. Moving from supervised-only learning to any incorporation of unsupervised learning produces improvements, particularly evident in low-shot scenarios. Moderate frequencies (ğœ‡ {1, 10, 25}) achieve optimal performance, with ğœ‡ = 10 emerging as optimum. Very high frequencies (ğœ‡ {50, 100}) lead to performance degradation compared to the optimum, though they still outperform supervised-only learning in data-scarce settings. These findings validate our design choice of alternating between supervised and unsupervised learning phases, demonstrating that the optimal frequency (ğœ‡ = 10) remains consistent across different data availability scenarios and that properly scheduled unsupervised learning is particularly crucial in extreme low-data regimes. 6. Conclusions This paper has introduced novel GAN-based semisupervised learning framework specifically designed for extremely low labeled-data regimes. Our approach addresses Guido Manni et al. Page 11 of 15 the fundamental challenge of insufficient labeled data through three key innovations: (1) dynamic training schedule that alternates between supervised and unsupervised phases, (2) an image-to-image translation mechanism that enriches feature representations by learning class-conditional transformations from real unlabeled images, and (3) confidence-weighted temporal ensemble technique for reliable pseudo-labeling. By leveraging these components within three-player GAN architecture, our method effectively combines the complementary strengths of generator, discriminator, and dedicated classifier. The comprehensive empirical evaluation across eleven MedMNIST datasets demonstrates the effectiveness of our approach. In the extreme 5-shot setting, our ensemble configuration achieved statistically significant improvements over six state-of-the-art semi-supervised methods, with effect sizes ranging from large to very large. The methods superiority stems from its fundamental design choice of performing image-to-image translation rather than generating images from noise, which leverages real medical images as the foundation for learning discriminative features crucial for classification tasks. This advantage is further amplified by our temporal ensemble mechanism, which aggregates predictions across training epochs to produce more reliable pseudo-labels in data-scarce scenarios. Our analysis of the unsupervised training frequency revealed that moderate alternation between supervised and unsupervised phases (ğœ‡ = 10) balances both learning signals. This finding provides practical guidance for deployment, as the optimal frequency remained consistent across different data availability scenarios, eliminating the need for extensive hyperparameter tuning in clinical applications. Despite these promising results, several avenues for future research remain. The computational requirements of maintaining multiple networks may pose challenges in resourceconstrained clinical settings, motivating the development of more efficient architectures. Additionally, extending the framework to incorporate domain-specific medical knowledge and multi-modal imaging data could further enhance its clinical applicability. In conclusion, our GAN-based semi-supervised learning framework represents significant advancement in addressing the labeled data scarcity challenge in medical imaging. By effectively leveraging both labeled and unlabeled data through image translation and ensemble techniques, our method enables robust classification performance even with as few as five labeled samples per class, offering practical solution for medical imaging applications where annotation costs are prohibitive."
        },
        {
            "title": "Acknowledgements",
            "content": "Guido Manni is Ph.D. student enrolled in the National Ph.D. in Artificial Intelligence, XXXVII cycle, course on Health and life sciences, organized by UniversitÃ  Campus Bio-Medico di Roma. This work was partially founded by: i) Piano Nazionale Ripresa Resilienza (PNRR) - HEAL ITALIA Extended Partnership - SPOKE 2 Cascade Call - \"Intelligent Health\" with the project BISTOURY - 3D-guided roBotIc Surgery based on advanced navigaTiOn systems and aUgmented viRtual realitY (CUP: J33C22002920006) and ii) PNRR MUR project PE0000013-FAIR. Resources are provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS) and the Swedish National Infrastructure for Computing (SNIC) at Alvis @ C3S. References [1] X. Chen, X. Wang, K. Zhang, K.-M. Fung, T. C. Thai, K. Moore, R. S. Mannel, H. Liu, B. Zheng, Y. Qiu, Recent advances and clinical applications of deep learning in medical image analysis, Medical Image Analysis 79 (2022) 102444. doi:https://doi.org/10.1016/j.media.2022.102444. URL https://www.sciencedirect.com/science/article/pii/ S1361841522000913 [2] V. Chiruvella, A. K. Guddati, Ethical issues in patient data ownership, Interactive Journal of Medical Research 10 (2) (2021) e22269. doi: 10.2196/22269. [3] S. Bull, P. Y. Cheah, S. Denny, I. Jao, V. Marsh, L. Merson, N. Shah More, L. T. Nhan, D. Osrin, D. Tangseefa, D. Wassenaar, M. Parker, Best practices for ethical sharing of individual-level health research data from lowand middle-income settings, Journal of Empirical Research on Human Research Ethics 10 (3) (2015) 302313. doi:10.1177/1556264615594606. [4] F. Ruffini, E. M. Ayllon, L. Shen, P. Soda, V. Guarrasi, Benchmarking foundation models and parameter-efficient fine-tuning for prognosis prediction in medical imaging (2025). arXiv:2506.18434. URL https://arxiv.org/abs/2506.18434 [5] D. Bau, B. Zhou, A. Khosla, A. Oliva, A. Torralba, Network dissection: Quantifying interpretability of deep visual representations, in: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 33193327. doi:10.1109/CVPR.2017.354. [6] J. Chen, N. Yang, Y. Pan, H. Liu, Z. Zhang, Synchronous medical image augmentation framework for deep learning-based image segmentation, Computerized Medical Imaging and Graphics 104 (2023) 102161. doi:https://doi.org/10.1016/j.compmedimag.2022.102161. URL https://www.sciencedirect.com/science/article/pii/ [7] M. B. Sariyildiz, K. Alahari, D. Larlus, Y. Kalantidis, Fake it till you make it: Learning transferable representations from synthetic imagenet clones, in: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 80118021. doi:10.1109/ CVPR52729.2023.00774. [8] S. Azizi, S. Kornblith, C. Saharia, M. Norouzi, D. J. Fleet, Synthetic data from diffusion models improves imagenet classification (2023). arXiv:2304.08466. URL https://arxiv.org/abs/2304.08466 [9] S. Geng, C.-Y. Hsieh, V. Ramanujan, M. Wallingford, C.-L. Li, P. W. Koh, R. Krishna, The unmet promise of synthetic training images: Using retrieved real images performs better (2025). arXiv:2406.05184. URL https://arxiv.org/abs/2406.05184 [10] M. SalmÃ¨, L. Tronchin, R. Sicilia, P. Soda, V. Guarrasi, Beyond the generative learning trilemma: Generative model assessment in data scarcity domains (2025). arXiv:2504.10555. URL https://arxiv.org/abs/2504.10555 [11] A. R. Sajun, I. Zualkernan, Survey on implementations of generative adversarial networks for semi-supervised learning, Applied Sciences 12 (3) (2022). doi:10.3390/app12031718. URL https://www.mdpi.com/2076-3417/12/3/1718 [12] J. E. van Engelen, H. H. Hoos, survey on semi-supervised learning, Machine Learning 109 (2) (2020) 373440. doi:10.1007/ s10994-019-05855-6. URL https://doi.org/10.1007/s10994-019-05855- [13] A. Odena, Semi-supervised learning with generative adversarial networks, arXiv preprint arXiv:1606.01583 (2016). Guido Manni et al. Page 12 of 15 [14] J. Sun, B. Bhattarai, T.-K. Kim, Matchgan: self-supervised semisupervised conditional generative adversarial network, in: Proceedings of the Asian Conference on Computer Vision, 2020. [15] C. Li, K. Xu, J. Zhu, J. Liu, B. Zhang, Triple generative adversarial networks, IEEE Transactions on Pattern Analysis and Machine Intelligence 44 (12) (2022) 96299640. doi:10.1109/TPAMI.2021.3127558. [16] A. Haque, Ec-gan: Low-sample classification using semi-supervised algorithms and gans (student abstract), in: Proceedings of the AAAI conference on artificial intelligence, Vol. 35, 2021, pp. 1579715798. [17] H. Zhen, Y. Shi, J. J. Yang, J. M. Vehni, Co-supervised learning paradigm with conditional generative adversarial networks for sampleefficient classification, Applied Computing and Intelligence 3 (1) (2023) 1326. doi:10.3934/aci.2023002. URL https://www.aimspress.com/article/doi/10.3934/aci.2023002 [18] Y. Xie, Q. Wan, H. Xie, Y. Xu, T. Wang, S. Wang, B. Lei, Fundus imagelabel pairs synthesis and retinopathy screening via gans with classimbalanced semi-supervised learning, IEEE Transactions on Medical Imaging 42 (9) (2023) 27142725. doi:10.1109/TMI.2023.3263216. [19] Y. Cao, T. A. Geddes, J. Y. H. Yang, P. Yang, Ensemble deep learning in bioinformatics, Nature Machine Intelligence 2 (9) (2020) 500508. doi:10.1038/s42256-020-0217-y. URL https://doi.org/10.1038/s42256-020-0217-y [20] V. Guarrasi, N. C. DAmico, R. Sicilia, E. Cordelli, P. Soda, Pareto optimization of deep networks for covid-19 diagnosis from chest x-rays, Pattern Recognition 121 (2022) 108242. [21] Z. Dai, Z. Yang, F. Yang, W. W. Cohen, R. R. Salakhutdinov, Good semi-supervised learning that requires bad gan, Advances in neural information processing systems 30 (2017). [22] J. Yang, R. Shi, D. Wei, Z. Liu, L. Zhao, B. Ke, H. Pfister, B. Ni, Medmnist v2: large-scale lightweight benchmark for 2d and 3d biomedical image classification, Scientific Data 10 (1) (2023) 41. [23] D. Wolpert, W. Macready, No free lunch theorems for optimization, IEEE Transactions on Evolutionary Computation 1 (1) (1997) 6782. doi:10.1109/4235.585893. Guido Manni et al. Page 13 of 15 Appendix Comparison of accuracy scores across different methods on medical image datasets using 5-shot learning. Best performance for each dataset is highlighted in bold. Dataset SPARSE SPARSEens SGAN MatchGAN CISSL SEC-GAN TripleGAN ECGAN bloodmnist breastmnist chestmnist dermamnist octmnist organamnist organcmnist organsmnist pathmnist pneumoniamnist tissuemnist 0.868 0.542 0.545 0.511 0.488 0.780 0.754 0.567 0.751 0.796 0.351 0.887 0.667 0.571 0.576 0.465 0.801 0.792 0.588 0.778 0.806 0.353 0.173 0.667 0.539 0.124 0.263 0.180 0.104 0.051 0.100 0.585 0.052 0.302 0.792 0.535 0.626 0.333 0.114 0.268 0.216 0.187 0.710 0.302 0.463 0.615 0.610 0.566 0.254 0.422 0.436 0.215 0.367 0.785 0.308 0.813 0.719 0.533 0.535 0.363 0.683 0.683 0.487 0.453 0.854 0. 0.862 0.677 0.510 0.563 0.528 0.780 0.782 0.561 0.676 0.762 0.363 0.241 0.667 0.532 0.626 0.265 0.198 0.200 0.141 0.186 0.767 0.040 Comparison of accuracy scores across different methods on medical image datasets using 10-shot learning. Best performance for each dataset is highlighted in bold. Dataset SPARSE SPARSEens SGAN MatchGAN CISSL SEC-GAN TripleGAN ECGAN bloodmnist breastmnist chestmnist dermamnist octmnist organamnist organcmnist organsmnist pathmnist pneumoniamnist tissuemnist 0.895 0.604 0.563 0.629 0.490 0.820 0.765 0.662 0.835 0.854 0.417 0.907 0.698 0.568 0.626 0.515 0.834 0.808 0.694 0.873 0.848 0.434 0.171 0.688 0.517 0.114 0.246 0.194 0.148 0.050 0.099 0.577 0.052 0.407 0.760 0.532 0.626 0.240 0.108 0.744 0.495 0.372 0.775 0.302 0.505 0.615 0.559 0.626 0.316 0.393 0.405 0.230 0.332 0.831 0. 0.851 0.781 0.536 0.527 0.481 0.795 0.676 0.521 0.657 0.838 0.355 0.890 0.677 0.549 0.567 0.606 0.797 0.793 0.605 0.792 0.881 0.409 0.266 0.792 0.532 0.625 0.247 0.103 0.167 0.188 0.050 0.802 0.040 Comparison of accuracy scores across different methods on medical image datasets using 20-shot learning. Best performance for each dataset is highlighted in bold. Dataset SPARSE SPARSEens SGAN MatchGAN CISSL SEC-GAN TripleGAN ECGAN bloodmnist breastmnist chestmnist dermamnist octmnist organamnist organcmnist organsmnist pathmnist pneumoniamnist tissuemnist 0.925 0.698 0.563 0.652 0.609 0.880 0.859 0.724 0.894 0.823 0.450 0.939 0.792 0.574 0.654 0.667 0.885 0.881 0.737 0.909 0.817 0.472 0.173 0.646 0.525 0.111 0.248 0.161 0.081 0.064 0.095 0.585 0.052 0.649 0.667 0.562 0.623 0.334 0.098 0.784 0.573 0.335 0.781 0. 0.556 0.708 0.574 0.623 0.292 0.441 0.423 0.212 0.439 0.815 0.326 0.913 0.750 0.565 0.612 0.463 0.822 0.771 0.587 0.649 0.792 0.440 0.917 0.688 0.582 0.627 0.642 0.843 0.809 0.654 0.801 0.821 0.470 0.311 0.646 0.532 0.622 0.259 0.134 0.171 0.181 0.084 0.756 0.075 Guido Manni et al. Page 14 of Comparison of accuracy scores across different methods on medical image datasets using 50-shot learning. Best performance for each dataset is highlighted in bold. Dataset SPARSE SPARSEens SGAN MatchGAN CISSL SEC-GAN TripleGAN ECGAN bloodmnist breastmnist chestmnist dermamnist octmnist organamnist organcmnist organsmnist pathmnist pneumoniamnist tissuemnist 0.966 0.844 0.613 0.708 0.648 0.928 0.884 0.746 0.906 0.813 0. 0.968 0.813 0.616 0.701 0.683 0.930 0.888 0.763 0.917 0.860 0.472 0.172 0.646 0.522 0.133 0.241 0.178 0.183 0.023 0.069 0.783 0.052 0.819 0.708 0.532 0.625 0.278 0.141 0.797 0.623 0.333 0.798 0.302 0.612 0.698 0.604 0.619 0.339 0.485 0.429 0.290 0.369 0.850 0.335 0.936 0.781 0.568 0.617 0.643 0.867 0.837 0.682 0.841 0.842 0.449 0.956 0.792 0.577 0.660 0.826 0.887 0.841 0.686 0.856 0.821 0. 0.807 0.750 0.532 0.624 0.247 0.155 0.619 0.499 0.116 0.815 0.052 Guido Manni et al. Page 15 of"
        }
    ],
    "affiliations": [
        "Unit of Advanced Robotics and Human-Centred Technologies, Department of Engineering, UniversitÃ  Campus Bio-Medico di Roma, Rome, Italy",
        "Unit of Artificial Intelligence and Computer Systems, Department of Engineering, UniversitÃ  Campus Bio-Medico di Roma, Rome, Italy"
    ]
}