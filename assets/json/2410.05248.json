{
    "paper_title": "SFTMix: Elevating Language Model Instruction Tuning with Mixup Recipe",
    "authors": [
        "Yuxin Xiao",
        "Shujian Zhang",
        "Wenxuan Zhou",
        "Marzyeh Ghassemi",
        "Sanqiang Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "To induce desired behaviors in large language models (LLMs) for interaction-driven tasks, the instruction-tuning stage typically trains LLMs on instruction-response pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the need for higher-quality supervised fine-tuning (SFT) datasets, which typically involves expensive data filtering with proprietary LLMs or labor-intensive data generation by human annotators. However, these approaches do not fully leverage the datasets' intrinsic properties, resulting in high computational and labor costs, thereby limiting scalability and performance gains. In this paper, we propose SFTMix, a novel recipe that elevates instruction-tuning performance beyond the conventional NTP paradigm, without the need for well-curated datasets. Observing that LLMs exhibit uneven confidence across the semantic representation space, we argue that examples with different confidence levels should play distinct roles during the instruction-tuning process. Based on this insight, SFTMix leverages training dynamics to identify examples with varying confidence levels, then applies a Mixup-based regularization to mitigate overfitting on confident examples while propagating supervision signals to improve learning on relatively unconfident ones. This approach enables SFTMix to significantly outperform NTP across a wide range of instruction-following and healthcare domain-specific SFT tasks, demonstrating its adaptability to diverse LLM families and scalability to datasets of any size. Comprehensive ablation studies further verify the robustness of SFTMix's design choices, underscoring its versatility in consistently enhancing performance across different LLMs and datasets in broader natural language processing applications."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 8 4 2 5 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "SFTMIX: ELEVATING LANGUAGE MODEL INSTRUCTION TUNING WITH MIXUP RECIPE Yuxin Xiao1, Shujian Zhang2, Wenxuan Zhou2, Marzyeh Ghassemi1, Sanqiang Zhao2 1Massachusetts Institute of Technology, 2Zoom Video Communications {yuxin102, mghassem}@mit.edu {shujian.zhang, wenxuan.zhou, sanqiang.zhao}@zoom.com"
        },
        {
            "title": "ABSTRACT",
            "content": "To induce desired behaviors in large language models (LLMs) for interactiondriven tasks, the instruction-tuning stage typically trains LLMs on instructionresponse pairs using the next-token prediction (NTP) loss. Previous work aiming to improve instruction-tuning performance often emphasizes the need for higherquality supervised fine-tuning (SFT) datasets, which typically involves expensive data filtering with proprietary LLMs or labor-intensive data generation by human annotators. However, these approaches do not fully leverage the datasets intrinsic properties, resulting in high computational and labor costs, thereby limiting scalability and performance gains. In this paper, we propose SFTMix, novel recipe that elevates instruction-tuning performance beyond the conventional NTP paradigm, without the need for well-curated datasets. Observing that LLMs exhibit uneven confidence across the semantic representation space, we argue that examples with different confidence levels should play distinct roles during the instruction-tuning process. Based on this insight, SFTMix leverages training dynamics to identify examples with varying confidence levels, then applies Mixupbased regularization to mitigate overfitting on confident examples while propagating supervision signals to improve learning on relatively unconfident ones. This approach enables SFTMix to significantly outperform NTP across wide range of instruction-following and healthcare domain-specific SFT tasks, demonstrating its adaptability to diverse LLM families and scalability to datasets of any size. Comprehensive ablation studies further verify the robustness of SFTMixs design choices, underscoring its versatility in consistently enhancing performance across different LLMs and datasets in broader natural language processing applications."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) have recently demonstrated outstanding performance across broad spectrum of natural language processing (NLP) tasks (Zhao et al., 2023; Minaee et al., 2024). After being pre-trained on large corpora of raw text, LLMs undergo critical instruction-tuning stage (Ouyang et al., 2022; Zhang et al., 2023) to develop their instruction-following capabilities based on supervised fine-tuning (SFT) datasets, making them more suitable for interaction-driven applications. SFT datasets (Taori et al., 2023; Wang et al., 2023; Xu et al., 2024) typically consist of instruction-response pairs spanning various task types, aligning LLMs toward desired behavior. During this stage, LLMs are usually trained through next-token prediction (NTP), where LLMs predict the next token in response given both the instruction and the preceding tokens in that response. Previous research efforts in this field have predominantly focused on enhancing the quality of instruction-tuning datasets. One line of research direction seeks to better understand the intrinsic properties of these datasets (Kung et al., 2023; Lin et al., 2024) and selects informative instructionresponse pairs through heuristics-based filters (Zhao et al., 2024) or LLM scoring (Chen et al., 2024). Another line of work generates high-quality responses by querying advanced proprietary LLMs (Chen et al., 2024) or relying on human annotators (Zhou et al., 2023). However, both strategies come with significant computational or labor costs, limiting the scalability of SFT datasets. Work done during an internship at Zoom. Corresponding author."
        },
        {
            "title": "Preprint",
            "content": "In this paper, we take different perspective by exploring how to elevate instruction-tuning performance beyond the conventional NTP training paradigm, without relying on well-curated datasets To address this challenge, we propose SFTMix, novel Mixup-based recipe for language model instruction tuning. Our design builds upon the key observation that an LLMs confidence distribution over its instruction-tuning dataset is uneven across the semantic representation space. We argue that data with varying confidence levels should contribute differently during the instruction-tuning process. Hence, we extend data cartography (Swayamdipta et al., 2020) to the realm of causal language generation as training dynamics and leverage reference LLM to derive the confidence of each instruction-response pair, based on the perplexities computed over the instruction-tuning process. Using this information, we divide the original SFT dataset into confident subset and relatively unconfident subset of equal size. To guide the learning of the unconfident examples by propagating supervision signals, and to mitigate overfitting to the confident semantic regions, we design Mixup-based (Zhang et al., 2018) regularization for LLM instruction tuning. This approach explores the utility of Mixup in causal language generation and improves its effectiveness by exploiting the confidence information derived from training dynamics. Specifically, consider an instruction-response pair from the confident subset and another from the unconfident subset. We interpolate them linearly at the token level in the representation space and generate convex combination of the representations from the LLM under instruction tuning. The one-hot encodings of the corresponding tokens are interpolated in similar fashion. We then compute regularization between the interpolated encodings and representations, in addition to the original NTP loss during instruction tuning. In this way, SFTMix fosters the synergy between the two subsets with diverging confidence levels and enhances the interaction capabilities of LLMs across diverse downstream applications. We demonstrate the effectiveness of our proposed SFTMix recipe in both instruction-following and domain-specific SFT settings. In particular, SFTMix significantly surpasses the conventional NTP instruction-tuning baseline in both singleand multi-turn conversations, as measured in MT-Bench (Zheng et al., 2024) and AlpacaEval-2 (Dubois et al., 2024). This improvement is consistent across different LLM families (e.g., Llama (Dubey et al., 2024) and Mistral (Jiang et al., 2023)) and scales of SFT datasets (e.g., Alpaca-52K (Taori et al., 2023) and UltraChat-200K (Tunstall et al., 2023)). Moreover, in the healthcare domain, Llama-3.1-8B (Dubey et al., 2024) and Mistral-7B-v0.1 (Jiang et al., 2023), instruction-tuned on MedAlpaca-263K (Han et al., 2023) using SFTMix, achieve an average of 1.5% absolute increase in accuracy across four benchmarks compared to baselines. We further explore the applicability of SFTMixs variants through extensive ablations and illustrate the potential of this LLM instruction-tuning recipe for broader use cases. We summarize our contributions in this paper as follows: We introduce SFTMix, novel recipe designed to elevate LLM instruction-tuning performance beyond the conventional NTP paradigm, without relying on well-curated datasets. Motivated by the observation that LLMs exhibit varying confidence levels across the semantic space, SFTMix leverages LLMs training dynamics for more insightful data interpretation and confidence-based splitting, facilitating more effective instruction tuning. SFTMix further incorporates Mixup-based regularization that interpolates between examples with different confidence levels during instruction tuning, mitigating overfitting to confident examples while improving generalization on relatively unconfident ones. We demonstrate that SFTMix significantly outperforms the NTP baseline across variety of instruction-following and healthcare domain-specific SFT tasks, with consistent improvements across different LLM families and dataset sizes. Comprehensive ablation analysis substantiates the robustness of our design choices in SFTMix, shedding light on its potential for broader NLP applications."
        },
        {
            "title": "2 RELATED WORK",
            "content": "LLM Instruction Tuning. To align LLMs with users open-ended intents or adapt them to specific domains, Ouyang et al. (2022) proposed instruction-tuning LLMs on human-annotated demonstrations using supervised learning. More specifically, given pair of instructions and desired responses, the conventional NTP paradigm trains an LLM to predict each token in the response sequentially"
        },
        {
            "title": "Preprint",
            "content": "during the instruction-tuning stage (Zhang et al., 2023). Jain et al. (2024) improved instructiontuning performance by adding noise to the token embeddings during training, while Shi et al. (2024) further suggested modeling the instructions as well. On this basis, previous work (Chiang et al., 2023; Ding et al., 2023; Taori et al., 2023; Wang et al., 2023; Xu et al., 2024) collected instructionfollowing datasets by distilling powerful proprietary LLMs or crowdsourcing user conversations. To enhance data quality, the community has employed various techniques, including heuristic-based filters (Schoch et al., 2023; Zhao et al., 2024), LLM scoring (Chen et al., 2024), and human curation (Zhou et al., 2023). Other efforts (Kung et al., 2023; Lin et al., 2024) have focused on gaining deeper understanding of the intrinsic properties of SFT datasets. However, acquiring high-quality SFT data often entails substantial computational and labor costs. In this paper, we aim to optimize data utilization through insightful data interpretation and improve the effectiveness of instruction tuning beyond the conventional NTP paradigm, without relying on well-curated datasets. Data Characterization via Training Dynamics. Data characterization (Albalak et al., 2024; Wang et al., 2024) seeks to assess and analyze the quality and relevance of training data, enabling more effective data filtering and elevated model performance. In particular, Swayamdipta et al. (2020) leveraged the training dynamics of pre-trained language model (Liu, 2019) to create data maps, which have subsequently inspired advancements in active learning (Zhang & Plank, 2021; Zhang et al., 2022; Kung et al., 2023), curriculum learning (Christopoulou et al., 2022; Lin et al., 2024; Poesina et al., 2024), and dataset pruning (Chimoto et al., 2024; He et al., 2024; Lin et al., 2024; Seedat et al., 2024). Here, we explore applying training dynamics to causal language generation by categorizing an SFT dataset into confident and relatively unconfident subsets, which facilitates the subsequent Mixup-based regularization during LLM instruction tuning. Mixup-Based Learning. To alleviate memorization and sensitivity to adversarial examples during training, Zhang et al. (2018) proposed Mixup, which trains models on convex combinations of pairs of input features and their corresponding labels. Its variants (Verma et al., 2019; Hendrycks et al., 2020; Uddin et al., 2021; Choi et al., 2022) further suggest interpolating feature representations at different stages, guided by various training signals. Theoretical analyses (Zhang et al., 2021; Carratino et al., 2022; Chidambaram et al., 2022; Park et al., 2022; Pinto et al., 2022) have demonstrated its data-adaptive regularization and generalization effects, leading to strong out-ofdistribution robustness and well-calibrated uncertainty estimation. Empirical studies have further validated its effectiveness under the semi-supervised learning setting (Berthelot et al., 2019; 2020; Li et al., 2020; 2022) and in diverse NLP applications (Chen et al., 2020; Guo et al., 2020; Sun et al., 2020; Park & Caragea, 2022; Yang et al., 2022). Building on this success, we explore its utility in LLM instruction tuning and propose Mixup-based regularization to reduce overfitting to confident examples and support the learning of relatively unconfident ones."
        },
        {
            "title": "3 SFTMIX",
            "content": "In Section 3.1, we begin by reviewing the conventional instruction-tuning task of NTP. We then introduce SFTMix, novel recipe for LLM instruction tuning. SFTMix first leverages training dynamics to determine subspaces with distinct confidence levels (Section 3.2). Subsequently, it incorporates Mixup-based regularization (Section 3.3) to mitigate overfitting to confident examples and propagate their supervision signals to promote the learning of relatively unconfident ones. We illustrate the overall pipeline of SFTMix in Figure 1. 3.1 PRELIMINARIES OF THE CONVENTIONAL NTP INSTRUCTION-TUNING PARADIGM Consider an SFT dataset = {(Xi, Yi)}D i=1, consisting of pairs of instructions Xi and desired responses Yi. Here, both Xi and Yi are sequences of tokens, where Xi = (x1, . . . , xMi) and Yi = (y1, . . . , yNi). The conventional NTP task minimizes the following loss for predicting Yi given Xi: ℓNTP(D) = (cid:88) Ni(cid:88) i= n=1 log p(yn Xi, y1, . . . , yn1) = (cid:88) Ni(cid:88) i=1 n= H(Yn, σ(Zn)). (1) This loss equals the sum of negative cross-entropy between Yn and Zn after softmax σ, where Yn is the one-hot encoding of the n-th token in Yi, and Zn = LLM(Xi, y1, . . . , yn1) is the corresponding representation generated by the LLMs causal language modeling head."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: The overall pipeline of the proposed SFTMix recipe for LLM instruction tuning. Given an SFT dataset D, we (1) train reference LLM on using NTP and (2) compute Conf (Yi Xi) for (Xi, Yi) based on the training dynamics of the reference LLM. On this basis, we (3) divide into confident subset Dc and relatively unconfident one Du of equal size. Finally, given pairs of examples from each subset, we (4) interpolate their one-hot encodings and predicted representations linearly at the token level and (5) incorporate Mixup-based regularization ℓMixup(Dc, Du) in addition to the NTP loss ℓNTP(D) during LLM instruction tuning. To improve the effectiveness of SFT, existing work (Zhou et al., 2023; Chen et al., 2024) has primarily focused on providing higher-quality SFT datasets. However, these approaches often lack an insightful understanding of SFT datasets and incur significant computational and labor costs, limiting scalability and performance gains. In response, we propose SFTMix, novel instructiontuning recipe in the following sections, which enhances instruction-tuning performance beyond the conventional NTP paradigm without relying on well-curated datasets. 3.2 TRAINING DYNAMICS IDENTIFY SUBSETS WITH DISTINCT CONFIDENCE LEVELS Suppose we identify checkpoints of reference LLM when instruction-tuning it using the NTP task in Section 3.1. We aim to capture the training dynamics of the reference LLM by computing its confidence in generating each pair (Xi, Yi) D. More specifically, we define confidence based on the perplexity of Yi given Xi at each checkpoint {1, . . . , C}: Ni(cid:88) Ni(cid:88) Perpc(Yi Xi) = log p(yn Xi, y1, . . . , yn1) = 1 Ni n=1 1 Ni log σ(Zn), (2) n=1 (3) Conf (Yi Xi) = 1 (cid:88) c=1 Perpc(Yi Xi). Note that Zn here is produced by the reference LLM at checkpoint c. The reference LLMs confidence in predicting Yi given Xi is the negative average perplexity over the checkpoints, as lower perplexity indicates higher likelihood of generation. LLMs Exhibit Uneven Confidence across the Semantic Representation Space. Here, we present case study by instruction-tuning Llama-3.1-8B (Dubey et al., 2024) on Alpaca-52K (Taori et al., 2023) and collect the LLMs confidence for each training data point across five checkpoints. We use the last hidden state of the final token in (Xi, Yi) as its embedding and plot 2,500 highconfidence and 2,500 low-confidence data points in Figure 2 (a) via t-SNE (Van der Maaten & Hinton, 2008). Correspondingly, we present one confident and one unconfident example from these data points in Figure 2 (b). We observe that embeddings of data points with contrasting confidence levels are clearly separated in Figure 2 (a), indicating that the distribution of the LLMs confidence is uneven across the semantic representation space. This observation is further supported by the examples in Figure 2 (b), where the LLM exhibits high confidence in the example discussing deterministic grammar rules and low confidence in the example concerning creative content in e-commerce."
        },
        {
            "title": "Preprint",
            "content": "(a) (b) Figure 2: (a) Embeddings of 2,500 high-confidence and 2,500 low-confidence examples in Alpaca52K by Llama-3.1-8B trained on Alpaca-52k using NTP. (b) Corresponding confident and unconfident examples. The clear separation between embeddings of high-confidence and low-confidence examples suggests that the LLM exhibits varying confidence levels across different semantic regions, as further illustrated by the different topics in the provided examples. Data with Distinct Confidence Levels Should Play Different Roles during Instruction Tuning. The insight from the case study motivates us to contend that data with varying confidence levels should contribute differently during instruction tuning. Highly confident data points typically lie further from the classification decision boundary, posing higher risk of overfitting. In contrast, less confident data points are often closer to the boundary, making them harder to learn. To address this, we propose promoting the flow of supervision signals between confident and less confident regions to mitigate overfitting and enhance generalization during LLM instruction tuning. On this basis, we divide the original SFT dataset into confident subset Dc and relatively unconfident subset Du of equal size according to Conf (Yi Xi). To foster synergy between them, we design Mixup-based regularization tailored to the specific challenges of instruction tuning, detailed in the next section. 3.3 MIXUP-BASED REGULARIZATION FACILITATES LLM INSTRUCTION TUNING To instruction-tune an LLM (different from the reference LLM used to obtain learning dynamics) with our SFTMix recipe, we introduce novel regularization ℓMixup in addition to the conventional ) Dc and NTP loss ℓNTP. Specifically, consider confident instruction-response pair (X ) Du. Let Yc relatively unconfident pair (X be the one-hot encoding vectors of the n-th token in and u, respectively, with Zc as the corresponding representations predicted by the instruction-tuning LLM. We linearly interpolate the two pairs as follows: and Yu and Zu , , Zn = λZc + (1 λ)Zu + (1 λ)Yu n, where λ Beta(α, α) and α is hyperparameter. Suppose that = min(N ) represents and the length of the shorter response between . We define the Mixup-based regularization ℓMixup(Dc, Du) between the confident and relatively unconfident subsets and the overall instructiontuning loss ℓSFTMix used in our SFTMix recipe as follows: n, Yn = λYc , (4) ℓMixup(Dc, Du) = D/2 (cid:88) i(cid:88) H( Yn, σ( Zn)), ℓSFTMix(D) = ℓNTP(D) + µ ℓMixup(Dc, Du). (5) n=1 Here, µ is hyperparameter to control the regularization effect. i="
        },
        {
            "title": "Preprint",
            "content": "Dataset Instruction-Tuning LLM Recipe Single-Turn Multi-Turn Overall Win Rate LC Win Rate MT-Bench AlpacaEvalAlpaca-52K UltraChat-200K Llama-3.1-8B Mistral-7B-v0.1 Llama-3.1-8B Mistral-7B-v0. NTP SFTMix NTP SFTMix NTP SFTMix NTP SFTMix 4.9100 5.2125 5.1650 5. 6.1875 6.2750 5.7625 5.9813 3.8150 3.9525 4.0675 4.5425 5.0125 5.3500 4.6938 4. 4.3625 4.5825 4.6163 4.9100 5.6000 5.8125 5.2281 5.4313 4.0714 4.9031 4.3560 4. 5.0665 5.1149 4.4899 4.6117 8.6528 10.3195 9.1759 9.4994 8.4505 9.3810 7.7732 8. Table 1: Evaluation of instruction-following capabilities of LLMs trained with NTP or SFTMix. We report the average score based on five rounds of evaluation, with the scores from the best-performing instruction-tuning recipe in bold. Standard errors are provided in Appendix A. SFTMix outperforms NTP on both MT-Bench and AlpacaEval-2, irrespective of LLM families and SFT data sizes. Altogether, as shown in Figure 1, our SFTMix recipe first identifies subspaces with distinct confidence levels using training dynamics, then facilitates the propagation of supervision signals between these subspaces through Mixup-based regularization. Since LLMs exhibit varying confidence levels across diverse semantic regions, the Mixup-based regularization encourages linear behavior and smoother decision boundary between confident and unconfident data points (Zhang et al., 2018; Verma et al., 2019). In this way, SFTMix regularizes overfitting in confident regions and propagates supervision signals (Bengio et al., 2009; Chapelle et al., 2009; Sohn et al., 2020) to enhance generalization in less confident regions, thereby improving the instruction-tuning performance."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we assess the effectiveness of SFTMix against the NTP baseline in both instructionfollowing (Section 4.1) and domain-specific (Section 4.2) SFT tasks. SFTMix consistently improves instruction-tuning performance across different LLM families and SFT datasets of varying scales. 4.1 INSTRUCTION-FOLLOWING SFT Instruction-following SFT trains LLMs on labeled datasets consisting of instructional prompts and corresponding desired responses, enhancing their conversational capabilities in downstream interaction-driven applications. Here, we compare SFTMix with the conventional NTP paradigm by applying them to the instruction tuning of two pre-trained LLMs from different model families (i.e., Llama (Dubey et al., 2024) and Mistral (Jiang et al., 2023)) on two instruction-following datasets of varying scales (i.e., Alpaca-52K (Taori et al., 2023) and UltraChat-200K (Tunstall et al., 2023)). We then evaluate the instruction-tuned LLMs on two widely-adopted benchmarks: MT-Bench (Zheng et al., 2024) and AlpacaEval-2 (Dubois et al., 2024). Datasets. Here, we focus on the following datasets with different sizes. Alpaca-52K (Taori et al., 2023) builds on the pipeline from Wang et al. (2023) by prompting text-davinci-003 (Ouyang et al., 2022) to generate diverse instructions and appropriate responses, which results in 52,000 single-turn interactions. We follow its default system prompt and conversation template when preparing training inputs. UltraChat-200K (Tunstall et al., 2023) filters out uninformative responses from the original UltraChat dataset (Ding et al., 2023) and downsamples it to 200,000 multi-turn interactions. To adapt these for our SFT pipeline, we expand each multi-turn interaction into multiple single-turn interactions by incorporating the chat history into the instructions. Implementation Details. We experiment with Llama-3.1-8B (Dubey et al., 2024) and Mistral7B-v0.1 (Jiang et al., 2023) due to their recent release and state-of-the-art performance compared to other models of similar sizes. By default, we use different instances of the same LLM type to obtain training dynamics and for instruction tuning. We determine the instruction-tuning hyperparameters through coarse sweep on Llama-3.1-8B with Alpaca-52K and adopt them as the default settings in our experiments. Specifically, we train each LLM on Alpaca-52K for three epochs and on UltraChat200K for one epoch, leveraging eight H100 GPUs. We use the AdamW optimizer with learning"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Examples from the extraction category in MT-Bench. Compared to its NTP-tuned counterpart, Llama-3.1-8B instruction-tuned on Alpaca-52K using SFTMix accurately interprets the queries from both turns and correctly extracts the relevant information from the prompt. rate of 2e6 and weight decay of 0.1, along with cosine learning rate scheduler featuring 0.1 warm-up ratio. Each gradient update during instruction tuning accumulates four batches, with each batch containing eight training examples. We set α = 0.5 for sampling λ in Equation 4 and µ = 0.2 when constructing ℓSFTMix in Equation 5. The NTP baseline follows the same instruction-tuning setting without the Mixup-based regularization ℓMixup in Equation 5. Evaluation Benchmarks. To evaluate how SFTMix improves LLMs instruction-following abilities compared to NTP, we assess LLMs instruction-tuned with each method on MT-Bench (Zheng et al., 2024) and AlpacaEval-2 (Li et al., 2023; Dubois et al., 2024). MT-Bench is challenging benchmark of 80 multi-turn, human-designed questions, where GPT-4-Turbo (Achiam et al., 2023) rates the quality of LLM-generated responses on ten-point scale. Similarly, AlpacaEval-2 employs GPT-4-Turbo to compare the tested LLMs responses against GPT-4-Turbos reference responses and calculates the expected win rate while adjusting for length bias. We repeat the evaluation five times for each setting and report the average score in Table 1 and standard error in Appendix A. SFTMix Enhances LLMs Instruction-Following Capabilities. As illustrated in Table 1, instruction-tuning with SFTMix consistently outperforms NTP across all metrics in both evaluation benchmarks, regardless of the base LLM or SFT dataset. Notably, SFTMix yields greater improvement in multi-turn conversational abilities (with an average increase of 0.3 points) compared to single-turn performance (an average increase of 0.2 points) in MT-Bench. Across the eight categories in MT-Bench, we observe significant gains in extraction tasks for Llama-3.1-8B, and in writing, coding, and STEM for Mistral-7B-v0.1 (full details in Appendix A). In AlpacaEval2, the improvement is particularly significant in the length-controlled (LC) win rate, which better aligns with human judgment by adjusting for GPT-4-Turbos preference for longer responses. While instruction-tuning with the larger, higher-quality UltraChat-200K dataset results in higher overall scores in MT-Bench and raw win rates in AlpacaEval-2, it also produces longer responses, leading to relatively lower LC win rates. Overall, our proposed recipe, SFTMix, enhances instruction-"
        },
        {
            "title": "Preprint",
            "content": "LLM MedQA MedQA-5 PubMedQA MedMCQA Macro Ave MedAlpaca-7B PMC-LLaMA-7B BioMedGPT-LM-7B Meditron-7B BioMistral-7B Llama-3.1-8B + NTP on MedAlpaca-263K or SFTMix on MedAlpaca-263K Mistral-7B-v0.1 + NTP on MedAlpaca-263K or SFTMix on MedAlpaca-263K 38.94 27.94 38.62 35.09 43. 59.68 59.31 60.88 49.18 49.10 51.77 33.96 21.24 34.72 26.73 37.58 53.23 54.52 55.38 43.94 44.62 45.72 57.20 54.87 58.27 56.93 50. 73.40 75.40 77.80 72.33 75.40 77.40 34.90 24.57 35.57 34.03 44.14 52.79 53.65 54.15 47.98 48.15 49.03 41.25 32.16 41.80 38.20 43. 59.78 60.72 62.05 53.36 54.32 55.98 Table 2: Evaluation results on four healthcare-related benchmarks by prior biomedical LLMs and LLMs trained on MedAlpaca-263K using either NTP or SFTMix. We report the mean accuracy (%) over three rounds of three-shot evaluation and bold the scores from SFTMix-tuned LLMs. Standard errors are provided in Appendix A. SFTMix achieves an approximate 1.5% absolute increase in macro-average accuracy compared to NTP for both Llama-3.1-8B and Mistral-7B-v0.1. following capabilities and text quality across LLMs from different model families and with datasets of varying sizes and quality, surpassing the performance of the conventional NTP paradigm. Case Study from MT-Bench. Figure 3 presents test example from the extraction category in MT-Bench, showing responses generated by Llama-3.1-8B instruction-tuned on Alpaca-52K using either NTP or SFTMix. In this example, the LLM trained with SFTMix accurately interprets the instructions from both the firstand second-turn queries, correctly extracting the relevant information from the prompt. Notably, it succeeds in answering the second-turn query, which involves calculating the profit margin and performing ratio comparison. In contrast, the LLM trained with NTP struggles to differentiate between revenue and profit, leading to incorrect responses in both turns. 4.2 DOMAIN-SPECIFIC SFT In healthcare domain-specific SFT, we train two LLMs on large-scale medical conversation dataset using SFTMix and assess their performance against NTP-tuned counterparts on four healthcarerelated question-answering benchmarks. Dataset. MedAlpaca-263K (Han et al., 2023) consists of medical NLP tasks reformatted for instruction tuning and healthcare-related conversations of varying quality crowd-sourced from online platforms, which amounts to total of 263,257 single-turn interactions. We train Llama-3.1-8B and Mistral-7B-v0.1 on MedAlpaca-263K using either NTP or SFTMix for two epochs and follow the remaining hyperparameter settings described in Section 4.1. Evaluation Benchmarks. We compare the effectiveness of SFTMix to NTP in domain-specific SFT by evaluating the performance of the instruction-tuned LLMs on the following benchmarks: MedQA (Jin et al., 2021) includes 1,273 four-choice questions from the US Medical License Exam, testing broad range of medical knowledge. MedQA-5 is the variant of MedQA where each question contains five options. PubMedQA (Jin et al., 2019) consists of 500 expert-labeled three-choice questions where the model must predict the answer by reasoning based on provided PubMed abstract. MedMCQA (Pal et al., 2022) comprises 4,183 four-choice questions from the Indian Medical Entrance Exams, covering 2,400 healthcare-related topics across 21 medical subjects. We adopt the three-shot evaluation setting from Labrak et al. (2024) and report the mean accuracy over three evaluation rounds in Table 2. Standard errors are provided in Appendix A. Additionally, we include prior biomedical LLMs of similar sizes, including MedAlpaca-7B (Han et al., 2023), PMC-LLaMA-7B (Wu et al., 2024), BioMedGPT-LM-7B (Luo et al., 2023), Meditron-7B (Chen et al., 2023), and BioMistral-7B (Labrak et al., 2024), as reference models for comparison."
        },
        {
            "title": "Preprint",
            "content": "Ablation Direction NTP SFTMix Section 5.1 Section 5.2 Section 5. Section 5.4 Loss ℓ = ℓNTP + µ ℓMixup NTP Loss Mixup Regularization ST MT-Bench MT AlpacaEval-2 Overall WR LC WR - 4.9100 3. 4.3625 4.0714 8.6528 ℓNTP(Full) ℓNTP(Full) ℓNTP(Full) ℓNTP(Full) - µ ℓMixup(Conf, Unconf) µ ℓMixup(Conf , Unconf ) ℓMixup(Conf, Unconf) ℓMixup(Conf, Unconf) µ ℓMixup(Conf, Unconf) ℓNTP(Conf) µ ℓMixup(Conf, Unconf) ℓNTP(Unconf) ℓNTP(High) - ℓNTP(High + Low) - ℓNTP(High + Low) µ ℓMixup(High, Low) 5. 3.9525 4.5825 4.9031 10.3195 4.8500 4. 4.5563 4.5786 10.0483 4.7050 5.0125 4.9775 5.1800 6.1175 5.9000 5. 4.1075 4.0000 4.1075 3.9050 5.2575 5.1825 5.0975 4.4062 4.5062 4.5425 4.5425 5.6875 5.5412 5. 3.9450 3.5821 4.4496 4.2030 7.2636 6.5871 5.9382 8.2856 7.2964 9.7824 8.9392 11.4490 11.9590 11. Table 3: Ablation studies on variants of SFTMix. We identify four ablation directions and evaluate the instruction-following abilities of Llama-3.1-8B trained with the corresponding loss functions. Conf and Unconf are the confident and unconfident subsets of the Full Alpaca-52K dataset, with confidence derived from the training dynamics of Llama-3.1-8B. Conf and Unconf are based on Gemma-2Bs training dynamics. High refers to Alpaca-GPT4-26K (higher quality), while Low refers to Alpaca-26K (relatively lower quality). SFTMix Adapts LLMs to Domain-Specific Tasks More Effectively. Table 2 demonstrates that SFTMix consistently surpasses NTP across all benchmarks for both backbones. In particular, SFTMix leads to 1.33% absolute improvement (from 60.72% to 62.05%) for Llama-3.1-8B and 1.66% increase (from 54.32% to 55.98%) for Mistral-7B-v0.1 in macro-average accuracy across the four benchmarks. These models also significantly outperform existing biomedical LLMs across all benchmarks by clear margin."
        },
        {
            "title": "5 ABLATION AND ANALYSIS",
            "content": "Following the improvements of SFTMix in instruction-following and domain-specific SFT tasks, we conduct extensive ablation studies to analyze the contribution of each design choice and explore its impact across applications. We identify four ablation directions and summarize the results of training Llama-3.1-8B on Alpaca-52K using variants of SFTMix in Table 3. 5.1 GENERALIZING THE TRAINING DYNAMICS FROM WEAKER REFERENCE LLM Inspired by Burns et al. (2024), we investigate the generalization of training dynamics from weaker reference LLM to stronger instruction-tuning LLM. Specifically, we identify training dynamics with weaker reference LLM, Gemma-2B (Team et al., 2024), to divide an SFT dataset into confident subset (Conf ) and relatively unconfident subset (Unconf ). These subsets are then fed into the Mixup regularization ℓMixup(Conf , Unconf ) when instruction-tuning Llama-3.1-8B. This alternative approach yields comparable scores on MT-Bench and AlpacaEval-2 to the original SFTMix recipe, which uses the same LLM for both training dynamics and Mixup-based instruction tuning. This finding aligns with the weak-to-strong generalization reported by Burns et al. (2024) and highlights the potential for scaling SFTMix to even stronger LLMs. 5.2 INCORPORATING MIXUP AS REGULARIZATION IS MORE EFFECTIVE Equation 5 uses the Mixup regularization ℓMixup to alleviate overfitting and encourage generalization. To fully explore its effect, we experiment with setting µ = 1 in Equation 5 (i.e., ℓ = ℓNTP(Full) + ℓMixup(Conf, Unconf)) or only minimizing ℓMixup without ℓNTP (i.e., ℓ = ℓMixup(Conf, Unconf)) during instruction tuning. Table 3 shows that these two variants achieve higher scores on MT-Bench but perform worse on AlpacaEval-2 compared to the baseline of using the conventional NTP method. Furthermore, our SFTMix recipe, which employs ℓMixup as regularization, still outperforms both variants across both benchmarks. This finding highlights the importance of incorporating the traditional NTP task during SFT and supports the conclusion that Mixup is more effective when used as regularization alongside the standard cross-entropy loss in LLM instruction tuning."
        },
        {
            "title": "5.3 SFTMIX EFFECTIVELY UTILIZES ENTIRE INSTRUCTION-TUNING DATASETS",
            "content": "As part of our SFTMix recipe, we apply the NTP loss ℓNTP to the entire SFT dataset D. Here, we consider variants where ℓNTP is applied selectively to either the confident or relatively unconfident halves of the dataset. Specifically, we experiment with ℓ = ℓNTP(Conf)+µ ℓMixup(Conf, Unconf) and ℓ = ℓNTP(Unconf) + µ ℓMixup(Conf, Unconf). As shown in Table 3, while both variants achieve the same overall score on MT-Bench, the variant applying ℓNTP to the confident subset (Conf) performs better on AlpacaEval-2. Notably, both variantswhere ℓNTP is applied to only half the dataset outperform the baseline where ℓNTP is applied to the entire dataset. We attribute this improvement to the impact introduced by our Mixup regularization ℓMixup. Nevertheless, our SFTMix recipe, which leverages the full dataset for NTP, outperforms both variants, demonstrating its ability to effectively utilize larger set of potentially lower-quality training examples during instruction tuning."
        },
        {
            "title": "5.4 TRAINING DYNAMICS ARE CRUCIAL FOR PERFORMING MIXUP",
            "content": "Building on the previous ablation study, which suggests the possibility of generalizing training dynamics from weaker LLM in our SFTMix recipe, we explore whether we can directly substitute training dynamics with known data quality. To test this hypothesis, we replace half of the original responses in Alpaca-52K with higher-quality GPT-4-generated versions, forming AlpacaGPT4-26K (High) (Peng et al., 2023), while referring to the remaining original responses as Alpaca-26K (Low). We then train Llama-3.1-8B using three approaches: NTP on Alpaca-GPT426K (ℓNTP(High)), NTP on its combination with Alpaca-26K (ℓNTP(High + Low)), and, in the final approach, the addition of the Mixup-based regularizer between Alpaca-GPT4-26K and Alpaca26K (ℓMixup(High, Low)). The use of higher-quality responses from GPT-4 indeed enhances instruction-tuning performance on both MT-Bench and AlpacaEval-2, as shown in Table 3. However, simply applying Mixup between two datasets of varying quality does not necessarily improve performance further, as indicated by the drop in the overall MT-Bench score from 5.5412 to 5.4500 and the length-controlled win rate in AlpacaEval-2 from 11.9590 to 11.1768. To investigate this observation, we plot the LLMs confidence distributions for both datasets in Figure 4. The substantial overlap in confidence distributions suggests that data quality does not necessarily correlate with training dynamics-based confidence. This highlights the importance of training dynamics in determining the model-specific role of data points, which is crucial for effectively applying our SFTMix recipe. Figure 4: Distribution of confidence in datasets of varying qualities by Llama-3.1-8B."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we propose SFTMix, novel recipe for LLM instruction tuning. We observe that LLMs exhibit uneven confidence distributions across the semantic representation space. Based on this motivation, we utilize training dynamics to identify data subsets of varying confidence levels and incorporate Mixup-based regularization. In this way, we aim to mitigate overfitting on the confident subset while propagating supervision signals to promote the generalization of the relatively unconfident subset. Extensive empirical results in both instruction-following and domain-specific SFT tasks demonstrate the effectiveness of SFTMix over the conventional NTP paradigm across different LLM families and SFT data scales. Comprehensive ablation studies further substantiate the contribution of SFTMixs design choices, highlighting its versatility in consistently enhancing performance across different LLMs and datasets in broader NLP applications. Due to computational constraints, we did not apply SFTMix to LLM pre-training or instruction-tune larger LLMs using this recipe. Integrating SFTMix with parameter-efficient pre-training and fine-tuning methods (Hu et al., 2022; Dettmers et al., 2024) is another promising direction for future work."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. survey on data selection for language models. arXiv preprint arXiv:2402.16827, 2024. Yoshua Bengio, Jerˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pp. 4148, 2009. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: holistic approach to semi-supervised learning. Advances in neural information processing systems, 32, 2019. David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. In International Conference on Learning Representations, 2020. Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, Ilya Sutskever, and Jeffrey Wu. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. In Forty-first International Conference on Machine Learning, 2024. Luigi Carratino, Moustapha Cisse, Rodolphe Jenatton, and Jean-Philippe Vert. On mixup regularization. Journal of Machine Learning Research, 23(325):131, 2022. Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20:542542, 2009. Jiaao Chen, Zichao Yang, and Diyi Yang. Mixtext: Linguistically-informed interpolation of hidden space for semi-supervised text classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 21472157, 2020. Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training better alpaca with fewer data. In The Twelfth International Conference on Learning Representations, 2024. Zeming Chen, Alejandro Hernandez Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Kopf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079, 2023. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/. Muthu Chidambaram, Xiang Wang, Yuzheng Hu, Chenwei Wu, and Rong Ge. Towards understanding the data dependency of mixup-style training. In International Conference on Learning Representations, 2022. Everlyn Chimoto, Jay Gala, Orevaoghene Ahia, Julia Kreutzer, Bruce Bassett, and Sara Hooker. Critical learning periods: Leveraging early training dynamics for efficient data pruning. In Findings of the Association for Computational Linguistics ACL 2024, pp. 94079426, 2024. Hyeong Kyu Choi, Joonmyung Choi, and Hyunwoo J. Kim. Tokenmixup: Efficient attention-guided token-level data augmentation for transformers. In Advances in Neural Information Processing Systems, 2022. Fenia Christopoulou, Gerasimos Lampouras, and Ignacio Iacobacci. Training dynamics for curIn Proceedings of the 2022 riculum learning: study on monolingual and cross-lingual nlu. Conference on Empirical Methods in Natural Language Processing, pp. 25952611, 2022."
        },
        {
            "title": "Preprint",
            "content": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 30293051, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Demi Guo, Yoon Kim, and Alexander Rush. Sequence-level mixed sample data augmentation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 55475552, 2020. Tianyu Han, Lisa Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexander Loser, Daniel Truhn, and Keno Bressem. Medalpacaan open-source collection of medical conversational ai models and training data. arXiv preprint arXiv:2304.08247, 2023. Muyang He, Shuo Yang, Tiejun Huang, and Bo Zhao. Large-scale dataset pruning with dynamic uncertainty. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 77137722, 2024. Dan Hendrycks, Norman Mu, Ekin Dogus Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: simple method to improve robustness and uncertainty under data shift. In International conference on learning representations, 2020. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. Neel Jain, Ping-yeh Chiang, Yuxin Wen, John Kirchenbauer, Hong-Min Chu, Gowthami Somepalli, Brian Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Aniruddha Saha, et al. Neftune: Noisy embeddings improve instruction finetuning. In The Twelfth International Conference on Learning Representations, 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 25672577, 2019. Po-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang, and Nanyun Peng. Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 18131829, 2023. Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. BioMistral: collection of open-source pretrained large language models for medical domains. In Findings of the Association for Computational Linguistics ACL 2024, 2024. Changchun Li, Ximing Li, Lei Feng, and Jihong Ouyang. Who is your right mixup partner in positive and unlabeled learning. In International Conference on Learning Representations, 2022."
        },
        {
            "title": "Preprint",
            "content": "Junnan Li, Richard Socher, and Steven C.H. Hoi. Dividemix: Learning with noisy labels as semisupervised learning. In International Conference on Learning Representations, 2020. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 5 2023. Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965, 2024. Yinhan Liu. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie. Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine. arXiv preprint arXiv:2308.09442, 2023. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. Large language models: survey. arXiv preprint arXiv:2402.06196, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: large-scale In Conference on multi-subject multi-choice dataset for medical domain question answering. health, inference, and learning, pp. 248260. PMLR, 2022. Chanwoo Park, Sangdoo Yun, and Sanghyuk Chun. unified analysis of mixed sample data augmentation: loss function perspective. In Advances in Neural Information Processing Systems, 2022. Seo Yeon Park and Cornelia Caragea. data cartography based mixup for pre-trained language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 42444250, 2022. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023. Francesco Pinto, Harry Yang, Ser Nam Lim, Philip Torr, and Puneet Dokania. Using mixup as regularizer can surprisingly improve accuracy & out-of-distribution robustness. Advances in Neural Information Processing Systems, 35:1460814622, 2022. Eduard Poesina, Cornelia Caragea, and Radu Ionescu. novel cartography-based curriculum learning method applied on RoNLI: The first Romanian natural language inference corpus. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 236253, 2024. Stephanie Schoch, Ritwick Mishra, and Yangfeng Ji. Data selection for fine-tuning large language models using transferred shapley values. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pp. 266275, 2023. Nabeel Seedat, Nicolas Huynh, Boris van Breugel, and Mihaela van der Schaar. Curated LLM: Synergy of LLMs and data curation for tabular augmentation in low-data regimes. In Forty-first International Conference on Machine Learning, 2024. Zhengyan Shi, Adam Yang, Bin Wu, Laurence Aitchison, Emine Yilmaz, and Aldo Lipani. Instruction tuning with loss over instructions. arXiv preprint arXiv:2405.14394, 2024."
        },
        {
            "title": "Preprint",
            "content": "Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems, 33:596608, 2020. Lichao Sun, Congying Xia, Wenpeng Yin, Tingting Liang, Yu Philip, and Lifang He. Mixuptransformer: Dynamic data augmentation for nlp tasks. In Proceedings of the 28th International Conference on Computational Linguistics, pp. 34363440, 2020. Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah Smith, and Yejin Choi. Dataset cartography: Mapping and diagnosing datasets with In Proceedings of the 2020 Conference on Empirical Methods in Natural training dynamics. Language Processing (EMNLP), pp. 92759293, 2020. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clementine Fourrier, Nathan Habib, et al. Zephyr: Direct distillation of lm alignment. arXiv preprint arXiv:2310.16944, 2023. M Shahab Uddin, Mst. Sirazam Monira, Wheemyung Shin, TaeChoong Chung, and Sung-Ho Bae. Saliencymix: saliency guided data augmentation strategy for better regularization. In International Conference on Learning Representations, 2021. Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis Mitliagkas, David LopezPaz, and Yoshua Bengio. Manifold mixup: Better representations by interpolating hidden states. In International conference on machine learning, pp. 64386447. PMLR, 2019. Jiahao Wang, Bolin Zhang, Qianlong Du, Jiajun Zhang, and Dianhui Chu. survey on data selection for llm instruction tuning. arXiv preprint arXiv:2402.05123, 2024. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1348413508, 2023. Chaoyi Wu, Weixiong Lin, Xiaoman Zhang, Ya Zhang, Weidi Xie, and Yanfeng Wang. Pmc-llama: toward building open-source language models for medicine. Journal of the American Medical Informatics Association, pp. ocae045, 2024. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, 2024. Huiyun Yang, Huadong Chen, Hao Zhou, and Lei Li. Enhancing cross-lingual transfer by manifold mixup. In International Conference on Learning Representations, 2022. Hongyi Zhang, Moustapha Cisse, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018. Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup help with robustness and generalization? In International Conference on Learning Representations, 2021."
        },
        {
            "title": "Preprint",
            "content": "Mike Zhang and Barbara Plank. Cartography active learning. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 395406, 2021. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792, 2023. Shujian Zhang, Chengyue Gong, Xingchao Liu, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Allsh: Active learning guided by local sensitivity and hardness. In Findings of the Association for Computational Linguistics: NAACL 2022, pp. 13281342, 2022. Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Long is more for alignment: simple but tough-to-beat baseline for instruction fine-tuning. In Forty-first International Conference on Machine Learning, 2024. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 2023. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36, 2024. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. LIMA: Less is more for alignment. In Thirty-seventh Conference on Neural Information Processing Systems, 2023."
        },
        {
            "title": "A EXPERIMENT DETAILS",
            "content": "In Section 4, we assess the effectiveness of SFTMix against NTP in both instruction-following and domain-specific SFT tasks. Here, we report the detailed experiment results with standard errors. Dataset Instruction-Tuning LLM Alpaca-52K UltraChat-200K Llama-3.1-8B Mistral-7B-v0.1 Llama-3.1-8B Mistral-7B-v0.1 Recipe NTP SFTMix NTP SFTMix NTP SFTMix NTP SFTMix MT-Bench Single-Turn Multi-Turn Overall Win Rate LC Win Rate AlpacaEval-2 4.9100 0.06 5.2125 0.03 3.8150 0.06 3.9525 0.07 4.3625 0.05 4.5825 0.03 4.0714 0.14 4.9031 0.21 8.6528 0.19 10.3195 0. 5.1650 0.08 5.2775 0.03 4.0675 0.13 4.5425 0.10 4.6163 0.05 4.9100 0.05 4.3560 0.15 4.5386 0.18 6.1875 0.04 6.2750 0.07 5.0125 0.02 5.3500 0. 5.6000 0.02 5.8125 0.02 5.0665 0.12 5.1149 0.16 5.7625 0.03 5.9813 0.08 4.6938 0.01 4.8813 0.03 5.2281 0.01 5.4313 0.03 4.4899 0.18 4.6117 0. 9.1759 0.17 9.4994 0.28 8.4505 0.08 9.3810 0.25 7.7732 0.21 8.7650 0.21 Table 4: Evaluation of instruction-following capabilities of LLMs trained with NTP or SFTMix. We report the average score and standard errors based on five rounds of evaluation, with the scores from the best-performing instruction-tuning recipe in bold. SFTMix outperforms NTP on both MT-Bench and AlpacaEval-2, irrespective of LLM families and SFT data sizes. LLM MedQA MedQA-5 PubMedQA MedMCQA Macro Ave MedAlpaca-7B PMC-LLaMA-7B BioMedGPT-LM-7B Meditron-7B BioMistral-7B 38.94 0.37 27.94 0.65 38.62 1.51 35.09 0.64 43.86 0.33 59.68 0.33 Llama-3.1-8B + NTP on MedAlpaca-263K 59.31 0.56 or SFTMix on MedAlpaca-263K 60.88 0.29 Mistral-7B-v0.1 49.18 0.30 49.10 0.33 + NTP on MedAlpaca-263K or SFTMix on MedAlpaca-263K 51.77 0. 33.96 0.26 21.24 0.56 34.72 0.46 26.73 0.19 37.58 0.62 53.23 0.21 54.52 0.21 55.38 0.13 43.94 0.23 44.62 0.39 45.72 0.44 57.20 0.71 54.87 0.62 58.27 0.25 56.93 1.27 50.13 0.66 73.40 0.86 75.40 0.57 77.80 0.16 72.33 0.20 75.40 0.68 77.40 0. 34.90 0.39 24.57 0.27 35.57 0.52 34.03 0.36 44.14 0.33 52.79 0.01 53.65 0.18 54.15 0.11 47.98 0.22 48.15 0.11 49.03 0.22 41.25 0.40 32.16 0.40 41.80 0.52 38.20 0.39 43.93 0.27 59.78 0.51 60.72 0.08 62.05 0.24 53.36 0.43 54.32 0.62 55.98 0. Table 5: Evaluation results on four healthcare-related benchmarks by prior biomedical LLMs and LLMs instruction-tuned on MedAlpaca-263K using either NTP or SFTMix. We report the mean accuracy (%) and standard errors over three rounds of three-shot evaluation and bold the scores from SFTMix-tuned LLMs. FTMix achieves an approximate 1.5% absolute increase in macroaverage performance compared to NTP for both Llama-3.1-8B and Mistral-7B-v0.1. Instruction-Tuning MT-Bench LLM Recipe Writing Roleplay Reasoning Math Coding Extraction STEM Humanities Overall Llama-3.1-8B Mistral-7B-v0.1 NTP SFTMix NTP SFTMix 6.79 6.81 6.23 7. 4.93 5.15 5.20 5.42 2.94 3.12 4.81 4.51 1.82 1.85 1.21 1. 2.63 2.63 2.63 3.45 5.96 6.80 6.82 6.90 4.46 4.77 4.44 4. 5.37 5.53 5.59 5.68 4.36 4.58 4.62 4.91 Table 6: Categorical evaluation of instruction-following capabilities of LLMs trained on Alpaca-52K using NTP or SFTMix. We report the average score in each of the eight categories in MT-Bench based on five rounds of evaluation, with the scores from the best-performing instruction-tuning recipe in bold. By using SFTMix, we observe significant gains in extraction tasks for Llama-3.1-8B, and in writing, coding, and STEM for Mistral-7B-v0.1."
        }
    ],
    "affiliations": [
        "Massachusetts Institute of Technology",
        "Zoom Video Communications"
    ]
}