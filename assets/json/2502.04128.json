{
    "paper_title": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis",
    "authors": [
        "Zhen Ye",
        "Xinfa Zhu",
        "Chi-Min Chan",
        "Xinsheng Wang",
        "Xu Tan",
        "Jiahe Lei",
        "Yi Peng",
        "Haohe Liu",
        "Yizhu Jin",
        "Zheqi DAI",
        "Hongzhan Lin",
        "Jianyi Chen",
        "Xingjian Du",
        "Liumeng Xue",
        "Yunlin Chen",
        "Zhifei Li",
        "Lei Xie",
        "Qiuqiang Kong",
        "Yike Guo",
        "Wei Xue"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inference-time compute. However, current state-of-the-art TTS systems leveraging LLMs are often multi-stage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale a particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose a simple framework Llasa for speech synthesis that employs a single-layer vector quantizer (VQ) codec and a single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inference-time compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available."
        },
        {
            "title": "Start",
            "content": "Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis Zhen Ye 1 Xinfa Zhu 2 Chi-Min Chan 1 Xinsheng Wang 1 Xu Tan 3 Jiahe Lei 4 Yi Peng 1 Haohe Liu 5 Yizhu Jin 1 Zheqi DAI 6 Hongzhan Lin 7 Jianyi Chen 1 Xingjian Du 8 Liumeng Xue 1 Yunlin Chen 9 Zhifei Li 9 Lei Xie 2 Qiuqiang Kong 6 Yike Guo 1 Wei Xue 1 5 2 0 2 6 ] . e [ 1 8 2 1 4 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in text-based large language models (LLMs), particularly in the GPT series and the o1 model, have demonstrated the effectiveness of scaling both training-time and inferencetime compute. However, current state-of-the-art TTS systems leveraging LLMs are often multistage, requiring separate models (e.g., diffusion models after LLM), complicating the decision of whether to scale particular model during training or testing. This work makes the following contributions: First, we explore the scaling of train-time and inference-time compute for speech synthesis. Second, we propose simple framework Llasa for speech synthesis that employs single-layer vector quantizer (VQ) codec and single Transformer architecture to fully align with standard LLMs such as Llama. Our experiments reveal that scaling train-time compute for Llasa consistently improves the naturalness of synthesized speech and enables the generation of more complex and accurate prosody patterns. Furthermore, from the perspective of scaling inferencetime compute, we employ speech understanding models as verifiers during the search, finding that scaling inference-time compute shifts the sampling modes toward the preferences of specific verifiers, thereby improving emotional expressiveness, timbre consistency, and content accuracy. In addition, we released the checkpoint and training code for our TTS model (1B, 3B, 8B) and codec model publicly available. *Equal contribution 1The Hong Kong University of Science and Technology 2ASLP Lab, Northwestern Polytechnical University 3Independent Researcher 4University of Science and Technology Beijing 5University of Surrey 6Chinese University of Hong Kong 7Hong Kong Baptist University 8University of Rochester 9Shanghai Mobvoi Information Technology Co., Ltd. Correspondence to: Wei Xue <weixue@ust.hk>. Models: Hugging Face Collection Llasa Training Code: GitHub Repository Codec Training Code: GitHub Repository Inference-time Scaling Code: GitHub Repository 1. Introduction Recent years have witnessed the remarkable success of large language models (LLMs) in the text domain, represented by the GPT series(Brown et al., 2020; Achiam et al., 2023; Radford et al., 2019). These advances have demonstrated that increasing model size and training data consistently yields better performance across wide array of natural language understanding and generation tasks. However, as the text domain approaches data saturation, new directions are emerging, such as o1 models (Jaech et al., 2024) that emphasize extensive computational effort at test timethereby exhibiting an inference-time scaling effect. By investing more resources during inference, these models are able to produce higher-quality outputs and handle more complex tasks, offering flexible avenue for performance improvement after the training phase. Meanwhile, text-to-speech (TTS) research has also made impressive strides. Many existing TTS systems focus on devising better model architecturesleveraging well-designed modules, larger datasets, and increased model sizeto push synthesis quality ever higher. While these efforts have propelled the field forward, they also tend to narrow the communitys perspective: the focus on better architectures can overshadow investigations into broader, potentially transformative research questions. In contrast, the text LLM community has converged on relatively standard frameworka simple Transformer model with tokenizerwhich allows researchers to concentrate on fundamental issues such as training-time scaling laws (Kaplan et al., 2020), inferencetime scaling behaviors (Snell et al., 2024), and downstream adaptations (e.g., fine-tuning (Hu et al., 2021), pruning, and quantization(Zhu et al., 2024a)). Such common design philosophy has catalyzed rapid progress and deeper insights 1 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis into the text domain. Motivated by these observations, we propose aligning TTS with the minimalist yet powerful paradigm of text LLMs. We introduce single Transformer-based TTS model that relies on well-designed speech tokenizer. More specifically, our TTS system, named Llasa, is initialized from the Llama (Touvron et al., 2023) model with an expanded vocabulary that incorporates speech tokens and is trained using the nexttoken prediction paradigm. Although this model may not always match the performance of highly customized TTS systems, its streamlined design creates unified foundation for exploring wider range of research questionsbeyond architecture exploration. In particular, we systematically investigate both trainingtime and inference-time scaling effects under this unified TTS framework. Our experiments show that scaling trainingtime compute (e.g., increasing model size or training data) not only improves naturalness but also enhances expressive prosody, effectively capturing the meaning conveyed in text without explicit labels. Additionally, we examine the utility of inference-time scaling by incorporating speech understanding models as verifiers in the search process. We find that spending more computational effort during inference aligns generation outputs more closely with specific verifier biases, yielding better emotional expressiveness, timbre consistency, and content accuracy. Evaluations on LibriSpeech test sets (Panayotov et al., 2015), seed-tts-eval (Anastassiou et al., 2024) and ESD datasets (zho, 2022) demonstrate state-of-the-art results, and further highlight how in-context learning capabilities can be combined with search-based refinements to control factors such as speaker identity or emotion. In summary, our paper makes several key contributions. We design TTS model named Llasa that is fully aligned with standard LLM architectures by utilizing single Transformer and well-designed speech tokenizer, thereby creating simple, flexible, and scalable system. Additionally, we find that increasing training-time compute for Llasa leads to significant improvements in speech naturalness and prosody accuracy, which reflects deeper semantic understanding of the input text. We further demonstrate that scaling inference-time computeachieved by incorporating speech understanding verifiersenhances emotional expressiveness, timbre consistency, and content accuracy in synthesized speech. Furthermore, by providing open access to our models and frameworks, we aim to foster further research and development in the TTS community. 2. Methods 2.1. Overview Our TTS framework is designed to fully align with the standard text LLM paradigm, keeping only two main components: (1) tokenizer and (2) single Transformer-based LLM. We initialize the Transformer parameters ϕ from an existing LLM (e.g., Llama), and inherit its tokenizer for the text portion. Hence, the core new challenge is to convert raw speech waveforms into sequences of discrete tokens such that the Transformer can model them in an autoregressive manner. To achieve this, we introduce our speech tokenizer, Xcodec2, which encodes waveforms into speech tokens and can decode them back to high-quality audio. Unlike some prior tokenizers for TTS, ours requires no additional information during decoding, ensuring that all aspects of the speech signal, such as content, prosody, and timbre, are captured by the LLM. Formally, let: 1. Tokenizetext(X) = {x1, . . . , xT } be the text tokenizer, which converts input text into text tokens. 2. Tokenizespeech(Y ) = {y1, . . . , yS} be the speech tokenizer, which converts speech waveform into discrete tokens. 3. Detokenizespeech({y1, . . . , yS}) = ˆY be the speech decoder, which reconstructs the waveform ˆY from its token representation. transcription and Yi Given dataset = {(Xi, Yi)}N i=1, where Xi is the text is the corresponding audio, we represent each pair (Xi, Yi) as token sequence (x1, . . . , xT , y1, . . . , yS). Our Transformer, with parameters ϕ, learns the joint distribution of text and speech tokens via (x1, . . . , xT , y1, . . . , yS) =P (x1, . . . , xT ) (y1, . . . , ySx1, . . . , xT ) Since the text tokens {x1, . . . , xT } are always given as input during training and inference, the model focuses on learning the conditional probability: (y1, . . . , ySx1, . . . , xT ) = (cid:89) s=1 (ysx1, . . . , xT , y1, . . . , ys1) Therefore, the loss is calculated over the speech tokens {y1, . . . , yS}. The objective is to minimize the negative log-likelihood: = (cid:88) s=1 2 log (ysx1, . . . , xT , y1, . . . , ys1) Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis which makes the model learn to predict each speech token ys conditioned on both the text tokens {x1, . . . , xT } and the previously generated speech tokens {y1, . . . , ys1}. 2.2. Speech Tokenizer As highlighted by AudioLM (Borsos et al., 2023), discrete speech representations can be categorized into both semantic tokens and acoustic tokens. Language modeling on semantic tokens excels at capturing high-level information such as content and emotion, while modeling with acoustic tokens focuses on low-level details, including timbre and other acoustic characteristics. Our X-codec2 tokenizer builds on the concepts from prior work X-codec (Ye et al., 2024). We fuse these semantic and acoustic features into unified codebook but introduce crucial modification: replacing residual vector quantization with single vector quantizer to ensure 1D causal dependency. This design naturally aligns with the left-to-right autoregressive mechanism of LLMs and also reflects the inherently left-to-right temporal structure of audio signals. Our X-codec2 consists of three main components: the Encoder, the VQ module, and the Decoder. Encoder Given raw speech waveform Y, we employ two separate encoders to derive its semantic and acoustic representations: Semantic Encoder Encs: We adopt pre-trained Wav2Vec2-BERT (Barrault et al., 2023) to obtain multilingual speech semantic features that capture content and emotional cues. Acoustic Encoder Enca: Following the design in Xin et al. (2024) and Kumar et al. (2024), this module uses multiple residual convolutional blocks with Snake activation functions to encode low-level acoustic details. We then concatenate the two outputs to form fused feature embedding, = (cid:2)Encs(Y), Enca(Y)(cid:3), which serves as the input to our vector quantizer. Vector Quantization To obtain discrete tokens, we apply FSQ() (Mentzer et al., 2024) to H: Hq = FSQ(H), where Hq is the quantized feature. We adopt FSQ due to its stability in training and high codebook usage efficiency. Notably, FSQ does not require an explicit VQ objective term (e.g., codebook commitment loss), simplifying optimization during training. Decoder From the quantized representation Hq, we aim to reconstruct both semantic and acoustic information: Semantic Reconstruction: We follow Ye et al. (2024) and employ semantic decoder to predict semantic features, using an ℓ2 loss for reconstruction. It is worth noting that during inference, predicting semantic features is unnecessary; this component is designed to provide supervisory signal to ensure the codebook retains sufficient semantic information. Acoustic Reconstruction: Following Vocos (Siuzdak), we replace the ConvNeXt backbone with Transformerbased decoder that predicts the short-time Fourier transform (STFT) magnitude and phase; an inverse STFT (iSTFT) head then converts the predicted spectra back to time-domain waveforms. Codec Training The training process closely follows that of X-Codec, simultaneously optimizing both semantic and acoustic reconstruction. We incorporate multi-period discriminator (MPD) (Kong et al., 2020), multi-scale STFT (MS-STFT) discriminator, and spectral discriminator, with FFT sizes {78, 126, 206, 334, 542, 876, 1418, 2296} (Anonymous, 2025), for adversarial training. Additionally, following (Anonymous, 2025), we incorporate perceptual loss during the final steps of the training process to further enhance intelligibility. 2.3. Scaling Train-time Compute Our primary goal in this section is not to locate computeoptimal configuration for TTS, but rather to show that, akin to text-based LLMs, increasing train-time resources (either by enlarging the model or expanding the training dataset) consistently improves performance. Specifically, we investigate two scaling strategies: Fix Training Data, Vary Model Size. We fix the training data at 250k hours and scale the size of the Transformer ϕ. Concretely, we adopt Llama 3.2 with 1B and 3B parameters, as well as Llama 3.1 with 8B parameters, which we denote as Llasa-1B-250k, Llasa-3B-250k, and Llasa-8B-250k, respectively, to observe how increased model capacity influences TTS quality. Fix Model Size, Vary Training Data. We choose an LLM initialized from Llama 3.2 1B and train on three progressively larger subsets of our dataset D, containing 80k, 160k, and 250k hours of speech, respectively. Notably, the 80k and 160k subsets are randomly sampled as 1/3 and 2/3 partitions of the full 250k dataset, which we denote as Llasa-1B-80k, Llasa-1B-160k, and Llasa-1B-250k (identical to the previously mentioned Llasa-1B-250k model). 3 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis We evaluate these on two aspects: Text Understanding Ability longstanding challenge in Text-to-Speech (TTS) technology is that TTS systems often fail to fully comprehend the meaning of text as humans do, which leads to mechanical pronunciation, lack of emotion, unnatural pauses, and difficulties in distinguishing homographs. Following BASE TTS (Łajszczak et al., 2024), we use seven categories of textsQuestions, Emotions, Compound Nouns, Complex Syntax, Foreign Words, Paralinguistics, and Punctuationfor English evaluation. Additionally, we propose seven categories tailored for Chinese: Questions, Emotions, Paralinguistics, Chinese Poetry, Rare Characters, Polyphonic Words, and Tongue Twisters (details are provided in Appendix A). In each case, the TTS system must exhibit deeper textual understanding to produce natural and context-appropriate speech (e.g., correct pronunciation for polyphonic words and more expressive speech for emotional content). By examining the synthesized audio for each category, we measure how increased training data or parameter count benefits the systems text understanding ability. zero-shot TTS capabilities In-context Learning Ability We also evaluate the models al., 2023)whether it can produce intelligible, high-quality speech for speakers unseen during training. This aligns with prior zero-shot TTS protocols, which typically assess how well model generalizes to new speaker identities, timbres, and emotional expressions with no additional fine-tuning. (Wang et 2.4. Scaling Inference-time Compute Recent research has begun exploring the scaling behavior of LLMs during inference, showing that additional computational resourcesoften in the form of sophisticated search strategiescan further enhance performance. Concretely, such approaches adjust the models output distribution at test time by generating multiple candidate outputs from baseline model and then applying post-hoc filtering and refinement via verifiers or scoring mechanisms, thereby elevating the quality of the generated content. When extending this concept to text-to-speech (TTS), we hypothesize that generating multiple speech candidates and performing targeted search among them can yield outputs that more closely match the task requirements. In line with prior work (Snell et al., 2024; Ma et al., 2025), our search framework centers on two fundamental design choices: Verifier Selection For TTS, many off-the-shelf speech understanding models can serve as verifiers (or reward models) to evaluate synthesized audio from multiple perspectives. These include speaker verification models for measuring timbre similarity, emotion representation models (Ma et al., 4 2023) for gauging emotional content, prosodic analyzers (e.g., pitch and duration (Li et al., 2023; Ren et al., 2022)) to ensure natural intonation and rhythm, speech quality metrics (Saeki et al., 2022; Reddy et al., 2021) (e.g., SpeechMos) to evaluate naturalness, and automatic speech recognition (ASR) models (Radford et al., 2023; Gao et al., 2023a) to assess transcription accuracy. By integrating feedback from these diverse verifiers, we ensure that the generated speech meets our requirements across multiple aspects, all in fully automated process. Algorithms We categorize the two different reward methods as follows: Output Reward Models (ORMs):These models assess the speech segment only after it has been fully generated, evaluating it holistically. ORMs typically follow simpler design but may be less efficient due to the absence of intermediate guidance signals. common search strategy based on ORMs is the Best-of-N approach, where multiple candidate outputs are generated, scored using reward model, and the highest-scoring output is selected. Process Reward Models (PRMs): These models evaluate the generation process step by step, optimizing at each incremental stage (e.g., every second in 10second clip). Unlike conventional reward models that produce single score for the final output, PRMs provide sequence of scores, allowing for more fine-grained feedback throughout the generation process. While this approach enables detailed control, it also increases the risk of overfitting to intermediate rewards and of converging to local optima. The most common search algorithm leveraging PRMs is beam search, which systematically explores the solution space while optimizing both the sampling and evaluation of intermediate steps. In our experiments, we explore both PRMs and ORMs to analyze how different search strategies impact the final quality of synthesized speech. more detailed discussion of these methods and their outcomes is provided in the next experiments section. 3. Experiments In this section, first, we compare our proposed speech tokenizer with existing codecs to assess its effectiveness. Second, we evaluate the performance of our TTS systems. We explore the effect of scaling both train-time and inferencetime compute and compare our models against other baseline TTS systems. Lastly, we evaluate the extensibility of our framework in Appendix 4, particularly its applicability to speech understanding tasks, highlighting its versatility and potential for broader applications Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis Table 1. Comparison between different codec models. Bold values indicate the best for each token rate. We use token rate instead of bitrate because, from the perspective of LLMs, it is more intuitive: dividing the speech context window length by the token rate directly gives the generated audio duration in seconds. Model Token Rate Ground Truth DAC Encodec Encodec DAC SpeechTokenizer Mini X-codec BigCodec WavTokenizer Mini Encodec DAC SpeechTokenizer Mini StableCodec SemantiCodec X-codec WavTokenizer X-codec2 (ours) - 600 600 150 100 100 100 100 80 75 75 75 50 50 50 50 50 50 40 50 Codebook Size - 1024 1024 1024 1024 1024 2048 1024 8192 4096 2048 1024 1024 1024 2048 15625 32768/8192 1024 4096 65536 Codebook Layer - 12 8 2 2 2 8 2 1 1 6 1 1 1 4 2 2 1 1 1 Frame Rate - 50 75 75 50 50 12.5 50 80 75 12.5 75 50 50 12.5 25 25 50 40 50 WER 1.96 2.00 2.15 4.90 13.27 3.92 2.96 2.49 2.76 3.98 3.61 28.92 74.55 5.01 4.89 5.12 6.89 3.42 11.20 2. STOI 1.00 0.95 0.94 0.85 0.73 0.77 0.91 0.86 0.93 0.90 0.89 0.77 0.62 0.64 0.85 0.91 0.84 0.83 0.85 0.92 PESQWB 4.64 4.01 2.77 1.56 1.13 1.25 2.25 2.33 2.68 2.13 1.99 1.23 1.06 1.14 1.64 2.24 1.66 1.84 1.62 2.43 PESQNB 4.55 4.15 3.18 1.94 1.40 1.59 2.80 2.88 3.27 2.63 2.51 1.48 1.20 1.30 2.09 2.91 2.18 2.38 2.06 3.04 SPKSIM 1.00 0.95 0.89 0.60 0.32 0.36 0.73 0.72 0.84 0.65 0.65 0.25 0.08 0.17 0.50 0.62 0.58 0.52 0.48 0.82 UT MOS 4.09 4.00 3.09 1.58 1.29 2.28 3.56 4.21 4.11 3.79 3.38 1.25 1.25 1.27 3.03 4.23 2.71 4.05 3.57 4.13 3.1. Codec Experiments 3.1.1. TRAINING DETAILS We train our codec model on corpus of approximately 150k hours of multilingual speech, drawn from the Emilia dataset (En/Zh/De/Fr/Ja/Ko)(He et al., 2024) and MLS (En/Fr/De/Nl/Es/It/Pt/Pl)(Pratap et al., 2020a). All audio is sampled at 16 kHz. We set the total downsampling ratio to 320, use codebook size of 65536, and employ projection dimension of 8 in our VQ module. During training, we randomly crop 6-second segments from the audio. The learning rate is 1 104, preceded by 3000-step warmup. In total, we train for 1.4 million steps, we activate perceptual loss at the final 0.2 million steps. 3.1.2. EVALUATION RESULTS For evaluation, we use the test-clean subset of LibriSpeech (Panayotov et al., 2015), which contains 2620 utterances at 16 kHz. We evaluate our system using several metrics. HuBERTbased ASR system for Word Error Rate (WER) 1. ShortTime Objective Intelligibility (STOI). Perceptual Evaluation of Speech Quality (PESQ). WavLM-based speaker verification model for speaker similarity (SPK SIM) 2, and 1https://huggingface.co/facebook/ hubert-large-ls960-ft 2https://github.com/microsoft/UniSpeech/ tree/main/downstreams/speaker_verification UTMOS 3. We compare our codec against multiple baselines, including DAC (Kumar et al., 2024), SpeechTokenizer (Zhang et al., 2024), BigCodec (Xin et al., 2024), StableCodec (Parker et al., 2024), SemantiCodec (Liu et al., 2024), X-codec (Ye et al., 2024), Mini (Défossez et al., 2024), EnCodec (Défossez et al., 2022), and WavTokenizer (Ji et al., 2024). All baseline results are obtained using their official checkpoints. As shown in Table 1, X-codec2 achieves the best performance at token rate of 50 for most metrics. Moreover, its UTMOS score closely matches that of the ground truth, indicating that the reconstructed audio faithfully preserves the original speech quality. We also observe that certain models exceed the ground truth in UTMOS when operating at low token rates. We suspect this occurs because, under limited token constraints, the decoder behaves partly as generative modelyielding plausible speech output but the alignment with the input was less precise. Additionally, we found that metrics such as WER at low token rate can achieve good results by integrating speech semantic information, as demonstrated by models like Mini, X-codec, and SpeechTokenizer. Another important observation is that the acoustic reconstruction capability of codecs at low token rates remains relatively limited. For instance, DAC operating at 600 tokens achieves SPK SIM of 0.95 and PESQ score exceeding 4. In contrast, current codecs at lower token rates attain SPK SIM values below 0.85 and PESQ scores around 3. However, compared to earlier models like DAC 3https://github.com/tarepan/SpeechMOS 5 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis and Encodec, there has been significant improvement in performance at low token rates. We believe that there is substantial potential for further advancements in low token rate codecs. 3.2. TTS experiments 3.2.1. EXPERIMENTAL DETAILS All TTS models are trained for 3 epochs with batch size of 2 million tokens and maximum learning rate of 5e-5. We employ cosine learning rate schedule with warmup phase covering 3% of an epoch, and the final learning rate is set to 10% of the peak learning rate. During training, text sequences are tokenized and placed on the left, followed by tokenized speech sequences on the right, forming concatenated sequence that is then cropped to maximum length of 2048 tokens. Our training dataset integrates multiple high-quality speech datasets, including Libriheavy (Kang et al., 2024), the Chinese-English subset from the Emilia corpus (He et al., 2024), WenetSpeech4TTS (Ma et al., 2024), and our internal data. This diverse collection constitutes large-scale training corpus of 250,000 hours, comprising mixed Mandarin Chinese and English speech data. All textual content maintains its original punctuation. Our TTS systems are evaluated through series of comprehensive experiments designed to assess various aspects of performance. Evaluation of Text understanding ability Similar to (Łajszczak et al., 2024), we evaluated the models text understanding capabilities by focusing on their ability to accurately comprehend and synthesize speech from complex textual inputs. This assessment aimed to evaluate the text understanding abilities of TTS systems. The full testset is in Appendix A, each sentence was synthesized five times by each model. Due to the absence of speech prompts, timbre and style were generated randomly. The evaluation criteria are also in Appendix A; linguistic expert rated the TTS outputs using discrete 3-point scale, and we calculated the average score for each category. Evaluation of In-Context Learning Ability To assess the in-context learning capability of our model, we conducted experiments on three test sets: Seed-TTS-Eval, LibriSpeech test-clean, and the ESD dataset. The first two datasets primarily evaluate the models ability to clone the voice of an unseen speaker given short speech clip, focusing on speaker similarity. For both, speaker similarity (SIM) and Word Error Rate (WER) are used as key evaluation metrics: Seed-TTS-Eval4 (Anastassiou et al., 2024) consists of three subsets: test-zh, test-en, and test-hard. These experiments focus on cross-sentence speaker similarity and the generation of intelligible speech. LibriSpeech test-clean (Panayotov et al., 2015) is widely used benchmark for English zero-shot TTS evaluation (Wang et al., 2023), providing standardized setting to assess the models ability to generate natural and intelligible speech from unseen speakers. The third test set, ESD (Emotional Speech Dataset)(Zhou et al., 2021; zho, 2022), evaluates the models ability to clone emotions in speech. This dataset consists of 10 native English speakers and 10 native Chinese speakers, covering five emotion categories: neutral, happy, angry, sad and surprised. For reproducibility, we selected the longest utterance from each speaker as the prompt and the second longest as the ground truth, resulting in total of 100 evaluation samples (50 English, 50 Chinese). We used Emotion2Vec-PlusLarge (Ma et al., 2023) 5 to measure emotional similarity. Through these evaluations, we aimed to provide comprehensive assessment of our TTS models, ensuring their effectiveness in speaker identity preservation, intelligibility, and emotional expressiveness across diverse linguistic and contextual challenges. 3.2.2. SCALING TRAIN-TIME COMPUTE Text Understanding Ability Figure 1 presents expert scores for Chinese TTS tasks, including Emotion, Paralinguistics, Poetry, Polyphonic Characters, Tongue Twisters, Questions, and Rare Characters. Increasing both model size (1B 3B 8B) and training data (80k 160k 250k hours) generally improves performance. Specifically, simpler tasks like Questions already achieve strong results, with only marginal gains from further scaling. In contrast, larger models (e.g., 8B parameters) yield significant improvements in emotional speech, Chinese poetry, and tongue twisters, where deeper semantic comprehension is essential. Meanwhile, rare characters benefit most from broader data coverage, as increasing model size alone has little effect. We also evaluate our models on English TTS tasksCompound Nouns, Emotions, Foreign Words, Paralinguistics, Punctuations, Questions, and Syntactic Complexity. As with Chinese, scaling up both the model size (1B 3B 8B) and training data (80k 160k 250k hours) generally yields higher expert scores. We observe that both Compound Nouns and Foreign Words primarily benefit from increased training data rather than model scaling, suggesting that wider variety of examples is necessary for correct pronunciation and lexical coverage. 4https://github.com/BytedanceSpeech/ seed-tts-eval 5https://huggingface.co/emotion2vec/ emotion2vec_plus_large 6 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis Figure 1. Comparison of mean expert score for Chinese and English In-context Learning Ability. Based on Table 2, Table 4, and Table 3, we observe that as the model size and training data increase, the metrics for speaker similarity, word error rate, and emotional similarity consistently improve, reflecting the enhancement of in-context learning ability. As shown in Figure 2, our simplest method for improving SIM is best-of-N . The orange line indicates that as inference-time compute grows, SIM improves markedly. Next, the blue line shows that PRM beam search outperforms ORM under the same compute budget. 3.2.3. SCALING INFERENCE-TIME COMPUTE In this section, we conduct series of experiments to explore how increasing inference-time compute affects the performance of different strategies, such as PRM and ORM. All experiments are conducted using the Llasa-1B-250K model and evaluated on seed-tts-eval test-hard testset. We begin by evaluating zero-shot TTS performance using SIMO and WER metrics, with direct inference serving as the baseline. Our initial goal is to improve SIM, utilizing two key strategies: PRM with beam search and ORM with bestof-N . The similarity verifier is WavLM-finetuned speaker verification model. For beam search, we generate candidate beams conditioned on the speech prompt, where each beam is expanded by tokens per step. We set = 25, corresponding to 0.5 seconds in our experiments. Each beam then expands into = 16 new candidate sequences. From the resulting pool of sequences, we select the top based on their similarity scores. This process repeats until an end-ofsequence (EOS) token is generated or the sequence length reaches 2048. For best-of-N , we generate multiple independent responses and select the one with the highest similarity score as the final output. To match the compute budget of beam search, we also produce candidates. However, when we also aim to optimize WER ( using Whisper Large v3 as verifier), we select the lowest-WER candidate at the final PRM step from the sequences. The red line reveals that WER remains poor, especially for larger beam widths, sometimes lagging behind the baseline. We suspect that maximizing SIM through PRM leads to locally optimal speech with inadequate diversity. To address this, we propose partial PRM strategy, applying PRM only during the first seconds, then switching to ORM. Here, = 2 in our experiments. This hybrid approach (the green line) achieves higher SIM than best-of-N while maintaining WER near ground truth, indicating sufficient diversity. Finally, substituting the later ORM step with WER-based verifier (the purple line) simultaneously boosts both SIM and WER as inference-time compute increases, demonstrating that this mixed strategy strikes an effective balance between speaker similarity and transcription accuracy. From another perspective, we also compare the impact of inference-time scaling across different model sizes and test sets. In these evaluations, we fix the beam width at 16. As shown in Tables 3, 4, and 2, our results show that inferencetime scaling not only improves speaker similarity but also enhances emotion similarity. Additionally, we generally observe that larger models benefit more from inference-time scaling. However, for certain relatively simple tasks and metrics, the performance gap between large and small models 7 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis Figure 2. Illustration of inference-time compute scaling for speaker similarity and word error rates. after inference-time scaling remains minimal. This finding suggests possible parallel with text-based LLMs: rather than solely focusing on scaling model training, in some settings, more efficient approach may be to train smaller models with less compute and then leverage inference-time compute to enhance outputs. 3.2.4. COMPARE WITH BASELINE MODEL In the previous sections, we analyzed our codec and explored the impact of scaling train-time and inference-time compute on the performance of TTS systems. In this section, we directly compare our model against other TTS baselines. For Seed-TTS-Eval, we select the following baseline models: Seed-TTS (Anastassiou et al., 2024), MaskGCT (Wang et al., 2024), E2-TTS (32 NFE) (Eskimez et al., 2024), F5TTS (32 NFE) (Chen et al., 2024b), CosyVoice (Du et al., 2024a), CosyVoice 2 (Du et al., 2024b), and FireRedTTS (Guo et al., 2024). Our results are taken from the original papers whenever available; otherwise, they are sourced from CosyVoice 2. The results, as shown in Table 2, indicate that for direct inference, our model achieves WER performance comparable to these state-of-the-art (SOTA) models. However, one notable limitation of our approach is SIM-O. As discussed earlier, the reconstruction capability of single-token codecs remains constrained compared to mel-based vocoder reconstructions or RVQ codec-based reconstructions used in the baselines. For LibriSpeech test-clean, we compare against the following baselines: ELLA-V (Song et al., 2024), VALL-E (Han et al., 2024), CLaM-TTS (Kim et al.), VALL-E (Wang et al., 2023), VALL-E 2 (Chen et al., 2024a), Voicebox (Le et al., 2024), and MELLE (Meng et al., 2024b). As shown in Table 3, we observe similar trends in continuous TTS for LibriSpeech test-clean. However, an interesting finding is that our model achieves high SIM-r score, particularly on LibriSpeech test-clean, where our best SIMR (0.626) is already very close to the ground truth (GT) codec resynthesis (0.638). Given that the continuous generation task is fully aligned with the autoregressive training paradigm, this suggests that, from generative modeling perspective, single Transformer-based architecture does not inherently suffer disadvantages from metric Sim-r and WER compared to carefully designed AR+NAR hybrid architectures. The only drawback of Sim-o may arise at the system level, particularly in the final step of codec acoustic reconstruction, where converting the intermediate representation back into waveform may introduce limitations due to single VQ as mentioned in Sec 3.1.2. From an inference-time scaling perspective, our approach outperforms all baselines. While this comparison might not be entirely fairas our inference process utilizes more computational resourcesit presents an alternative viewpoint: if the goal is to achieve the best possible quality, disregarding computational constraints, inference-time scaling provides viable solution. Notably, as shown in table 2 our model achieves WER of 3.12 on the test-hard of Seed-TTS-Eval, demonstrating that allocating more compute at inference time is particularly beneficial for synthesizing challenging speech, effectively addressing cases where previous models have struggled. 4. Extending to Speech Understanding Tasks While we have primarily demonstrated the viability of our single Transformer + tokenizer approach for TTS, we also explored its performance on speech understanding task, in particular, ASR. The only modification is to swap the position of speech and text tokens: speech tokens come first, followed by text tokens, and during training we apply the cross-entropy loss solely to the text tokens. We use the same tokenizer X-codec2. ASR models are trained on Libriheavy (Kang et al., 2024), MLS English (Pratap et al., 2020b), and GigaSpeech (Chen et al., 2021) under learning rate of 2 105, batch size of 1M tokens, 0.03 8 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis Table 2. Results of Llasa and recent TTS models on the SEED test sets. denotes close-sourced models. SIM-o is computed with the original speech and SIM-r with the reconstructed speech. For Llasa series, sim-o values include sim-r in parentheses. For WER, we employ Whisper-large-v3 (Radford et al., 2023) and Paraformerzh (Gao et al., 2023b) as the automatic speech recognition (ASR) engines for English and Mandarin, respectively. For SIM, we use WavLM-large fine-tuned on the speaker verification task (Chen et al., 2022) Model Human Our Codec Resyn. Seed-TTS FireRedTTS MaskGCT E2 TTS (32 NFE) F5-TTS (32 NFE) CosyVoice CosyVoice 2 CER 1.26 1.92 1.12 1.51 2.27 1.97 1.56 3.63 1.45 test-zh sim-o 0.755 0.677 0.796 0.635 0.774 0.730 0.741 0.723 0. WER 2.14 2.91 2.25 3.82 2.62 2.19 1.83 4.29 2.57 test-en sim-o 0.734 0.619 0.762 0.460 0.714 0.710 0.6l47 0.609 0.652 test-hard WER - - 7.59 17.45 10.27 - 8.67 11.75 6.83 sim-o - - 0.776 0.621 0.748 - 0.713 0.709 0. Llasa-1B-80k Llasa-1B-160k Llasa-1B-250k Llasa-3B-250k Llasa-8B-250k Llasa-1B-80k Llasa-1B-160k Llasa-1B-250k Llasa-3B-250k Llasa-8B-250k Llasa-1B-80k Llasa-1B-160k Llasa-1B-250k Llasa-3B-250k Llasa-8B-250k Llasa-8B-250k 2.69 2.22 1.89 1.60 1.59 1.52 1.29 1.11 1.06 1.04 0.53 0.53 0.45 0.50 0. 0.648 (0.779) 0.658 (0.783) 0.669 (0.794) 0.675 (0.792) 0.684 (0.798) 0.811 (0.849) 0.815 (0.851) 0.818 (0.855) 0.824 (0.856) 0.827 (0.856) Train-time Scaling 3.71 3.60 3.22 3.14 2.97 0.541 (0.685) 0.563 (0.701) 0.572 (0.708) 0.579 (0.708) 0.574 (0.706) Partial PRM (spk sim) 2.30 2.29 2.03 1.89 1.84 0.761 (0.798) 0.774 (0.804) 0.781 (0.809) 0.784 (0.812) 0.783 (0.806) Partial PRM (spk sim)+ORM (WER) 0.809 (0.840) 0.812 (0.841) 0.818 (0.845) 0.823 (0.848) 0.825 (0.848) 1.43 1.49 1.46 1.31 1.39 0.761 (0.792) 0.775 (0.798) 0.782 (0.801) 0.783 (0.803) 0.783 (0.799) Chunking: if len(char) > 100 2 chunks, > 200 3 chunks, . . . 17.11 16.73 12.13 13.37 11.09 16.09 14.10 11.30 11.22 10.59 7.22 6.35 5.24 5.39 4.38 3.12 0.618 (0.765) 0.627 (0.770) 0.638 (0.779) 0.652 (0.782) 0.660 (0.787) 0.759 (0.824) 0.768 (0.830) 0.773 (0.833) 0.780 (0.836) 0.785 (0.839) 0.732 (0.789) 0.745 (0.799) 0.750 (0.803) 0.759 (0.808) 0.767 (0.812) 0.770 (0.791) warmup ratio, and 2 training epochs. Table 5 presents ASR results on LibriSpeech. Notably, on the test-clean set, our model is competitive with Whisper Large v3. Performance on test-other, however, remains weakerlikely due to our smaller, relatively clean training set and the lack of data augmentation. Despite these limitations, our experiments confirm that an entirely discrete ASR paradigm operating on quantized speech tokens can still achieve promising results, comparable in many respects to mainstream approaches like Whisper, which relies on continuous Mel features. 5. Related work 5.1. Scaling Train-time and Inference-time Compute In the text domain, large language models (LLMs) like GPT (Brown et al., 2020; Kaplan et al., 2020) show that increasing data, compute, and model size enhances performance, pushing LLMs toward cognitive intelligence. However, further scaling during training faces limits due to data and compute constraints. To overcome this, scaling law during testing is proposed: greater computational effort in inference improves performance (Ji et al., 2025). Techniques like repeat sampling, self-correction, and tree search enhance reasoning depth and accuracy in complex tasks. While this principle has been extensively validated in the text domain, its impact on speech remains largely unexplored. Base-TTS (Łajszczak et al., 2024) provides brief investigation into the emergent abilities arising from scaling both model size and data volume but does not separately examine the effects of data scaling versus model scaling. Additionally, it does not explore performance across different languages. To the best of our knowledge, we are the first to systematically investigate inference scaling in the speech modality. 5.2. LLM-based TTS Significant progress has been made in using large language models (LLMs) for TTS tasks, including multi-speaker synthesis (Kharitonov et al., 2023) and zero-shot voice cloning (Wang et al., 2023; Chen et al., 2024a). VALL-E (Wang et al., 2023) pioneered treating TTS as conditional language modeling problem by converting waveforms into neural codec tokens. However, it employed multi-stage approacha coarse autoregressive (AR) model followed by non-autoregressive (NAR) residual modelwhich com9 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis Table 3. Objective performance comparison on continuation zeroshot speech synthesis tasks. WER-H (%) denotes evaluation with the HuBERT-Large ASR model. SIM-o is computed with the original speech and SIM-r with the reconstructed speech."
        },
        {
            "title": "System",
            "content": "Ground Truth Our Codec Resyn. ELLA-V VALL-E CLaM-TTS VALL-E VALL-E 2 Voicebox MELLE Continuation SIM-o 0.668 0.580 0.303 0.363 0.477 - 0.504 0.593 0.508 WER-H 2.15 2.49 2.91 2.32 2.36 3.8 2.32 2.0 1.98 Train-time Scaling Llasa-1B-80k Llasa-1B-160k Llasa-1B-250k Llasa-3B-250k Llasa-8B-250k 2.57 2.48 2.47 2.35 2. 0.457 0.475 0.478 0.484 0.483 Partial PRM (spk sim) 2.43 2.36 2.37 2.26 2.24 0.699 0.712 0.712 0.715 0.714 Llasa-1B-80k Llasa-1B-160k Llasa-1B-250k Llasa-3B-250k Llasa-8B-250k Partial PRM (spk sim)+ORMs (WER) Llasa-1B-80k Llasa-1B-160k Llasa-1B-250k Llasa-3B-250k Llasa-8B-250k 0.700 0.710 0.712 0.714 0. 1.76 1.66 1.62 1.57 1.49 SIM-r - 0.638 0.340 0.397 0.513 0.508 0.529 0.616 0.539 0.614 0.625 0.627 0.628 0.626 0.738 0.744 0.743 0.745 0.741 0.738 0.743 0.744 0.742 0.740 plicates training and inference. Extensions like VALL-E (Zhang et al., 2023) enable cross-lingual synthesis, and Spear-TTS (Kharitonov et al., 2023) integrates multiple AR models to support multi-speaker TTS with minimal supervision. Recent TTS systems have often combined an AR language model with additional components, such as diffusion (Betker, 2023; Lajszczak et al., 2024; Anastassiou et al., 2024; Du et al., 2024a; Guo et al., 2024), to generate more natural, controllable speech when trained on large datasets. Although these multi-stage pipelines can yield high-quality results, they remain cumbersome and less amenable to largescale training. On the other hand, single-stage systems like MELL-E (Meng et al., 2024a) and KALL-E (Zhu et al., 2024b) avoid multi-stage generation but rely on continuous acoustic features (e.g., spectrograms or latent variables). Storing and processing these features at scale can be prohibitive, hindering training on tens or hundreds of billions of tokens. In contrast, our approach uses single-stage AR Transformer that directly models discrete speech tokens, 10 Table 4. Evaluation results Emotion2Vec-Plus-Large. for Emotion Similarity using"
        },
        {
            "title": "Model",
            "content": "GT en 0.94 Train-time scaling Llasa-1B-80k Llasa-1B-160k Llasa-1B-250k Llasa-3B-250k Llasa-8B-250k 0.753 0.762 0.768 0.769 0. zh 0.94 0.815 0.822 0.836 0.852 0.861 Process Reward Models (emotion sim) Llasa-1B-80k Llasa-1B-160k Llasa-1B-250k Llasa-3B-250k Llasa-8B-250k 0.933 0.936 0.937 0.949 0. 0.970 0.971 0.974 0.975 0.974 Table 5. ASR Performance on LibriSpeech Test Sets Model Test Clean Test Other whisper large v3 whisper large v2 Llasa-asr-1b Llasa-asr-3b 1.8 2.7 2.3 1.9 3.6 5.2 7.2 5.9 similar to how text LLMs handle words and subwords. This design avoids multi-stage complexity and the large memory footprint of continuous representations, making it far more scalable while retaining the flexibility of standard LLM. 6. Conclusion This paper presents Llasa, scalable TTS system that aligns with text LLM architectures, using single Transformer and tokenizer. We systematically explore train-time and inference-time compute scaling, showing that larger models and datasets improve speech naturalness, prosody, and text comprehension. Additionally, inference-time scaling, leveraging speech understanding models as verifiers, enhances speaker similarity, emotional expressiveness, and content accuracy. Our experiments confirm state-of-the-art performance with strong zero-shot TTS capabilities. We release our models publicly to drive further research."
        },
        {
            "title": "References",
            "content": "Emotional voice conversion: Theory, databases and esd. Speech Communication, 137:118, 2022. ISSN 01676393. Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anastassiou, P., Chen, J., Chen, J., Chen, Y., Chen, Z., Chen, Z., Cong, J., Deng, L., Ding, C., Gao, L., et al. Seedtts: family of high-quality versatile speech generation models. arXiv preprint arXiv:2406.02430, 2024. Anonymous. Scaling transformers for low-bitrate highIn The Thirteenth Interquality speech coding. national Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=4YpMrGfldX. Barrault, L., Chung, Y.-A., Meglioli, M. C., Dale, D., Dong, N., Duppenthaler, M., Duquenne, P.-A., Ellis, B., Elsahar, H., Haaheim, J., et al. Seamless: Multilingual expressive and streaming speech translation. arXiv preprint arXiv:2312.05187, 2023. Betker, J. Better speech synthesis through scaling. CoRR, abs/2305.07243, 2023. Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grangier, D., Tagliasacchi, M., et al. Audiolm: language modeling approach to audio generation. IEEE/ACM transactions on audio, speech, and language processing, 31: 25232533, 2023. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J., et al. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021. Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li, J., Kanda, N., Yoshioka, T., Xiao, X., et al. Wavlm: Largescale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, 16(6):15051518, 2022. Chen, S., Liu, S., Zhou, L., Liu, Y., Tan, X., Li, J., Zhao, S., Qian, Y., and Wei, F. Vall-e 2: Neural codec language models are human parity zero-shot text to speech synthesizers. arXiv preprint arXiv:2406.05370, 2024a. Défossez, A., Copet, J., Synnaeve, G., and Adi, Y. High fidelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. Défossez, A., Mazaré, L., Orsini, M., Royer, A., Pérez, P., Jégou, H., Grave, E., and Zeghidour, N. Moshi: speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024. Du, Z., Chen, Q., Zhang, S., Hu, K., Lu, H., Yang, Y., Hu, H., Zheng, S., Gu, Y., Ma, Z., et al. Cosyvoice: scalable multilingual zero-shot text-to-speech synthesizer based on supervised semantic tokens. arXiv preprint arXiv:2407.05407, 2024a. Du, Z., Wang, Y., Chen, Q., Shi, X., Lv, X., Zhao, T., Gao, Z., Yang, Y., Gao, C., Wang, H., et al. Cosyvoice 2: Scalable streaming speech synthesis with large language models. arXiv preprint arXiv:2412.10117, 2024b. Eskimez, S. E., Wang, X., Thakker, M., Li, C., Tsai, C.-H., Xiao, Z., Yang, H., Zhu, Z., Tang, M., Tan, X., et al. E2 tts: Embarrassingly easy fully non-autoregressive zeroIn 2024 IEEE Spoken Language Technology shot tts. Workshop (SLT), pp. 682689. IEEE, 2024. Gao, Z., Li, Z., Wang, J., Luo, H., Shi, X., Chen, M., Li, Y., Zuo, L., Du, Z., Xiao, Z., et al. Funasr: fundamental end-to-end speech recognition toolkit. arXiv preprint arXiv:2305.11013, 2023a. Gao, Z., Zhang, S., McLoughlin, I., and Yan, Z. Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition, 2023b. URL https://arxiv.org/abs/2206.08317. Guo, H.-H., Liu, K., Shen, F.-Y., Wu, Y.-C., Xie, F.-L., Xie, K., and Xu, K.-T. Fireredtts: foundation text-tospeech framework for industry-level generative speech applications. arXiv preprint arXiv:2409.03283, 2024. Han, B., Zhou, L., Liu, S., Chen, S., Meng, L., Qian, Y., Liu, Y., Zhao, S., Li, J., and Wei, F. Vall-e r: Robust and efficient zero-shot text-to-speech synthesis via monotonic alignment. arXiv preprint arXiv:2406.07855, 2024. He, H., Shang, Z., Wang, C., Li, X., Gu, Y., Hua, H., Liu, L., Yang, C., Li, J., Shi, P., et al. Emilia: An extensive, multilingual, and diverse speech dataset for large-scale speech generation. In 2024 IEEE Spoken Language Technology Workshop (SLT), pp. 885890. IEEE, 2024. Chen, Y., Niu, Z., Ma, Z., Deng, K., Wang, C., Zhao, J., Yu, K., and Chen, X. F5-tts: fairytaler that fakes fluent and faithful speech with flow matching. arXiv preprint arXiv:2410.06885, 2024b. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 11 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Ji, S., Jiang, Z., Wang, W., Chen, Y., Fang, M., Zuo, J., Yang, Q., Cheng, X., Wang, Z., Li, R., et al. Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling. arXiv preprint arXiv:2408.16532, 2024. Ji, Y., Li, J., Ye, H., Wu, K., Xu, J., Mo, L., and Zhang, M. Test-time computing: from system-1 thinking to system-2 thinking. arXiv preprint arXiv:2501.02497, 2025. Kang, W., Yang, X., Yao, Z., Kuang, F., Yang, Y., Guo, L., Lin, L., and Povey, D. Libriheavy: 50,000 hours asr corIn ICASSP pus with punctuation casing and context. 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 10991 10995. IEEE, 2024. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Kharitonov, E., Vincent, D., Borsos, Z., Marinier, R., Girgin, S., Pietquin, O., Sharifi, M., Tagliasacchi, M., and Zeghidour, N. Speak, read and prompt: High-fidelity text-to-speech with minimal supervision. Trans. Assoc. Comput. Linguistics, 11:17031718, 2023. Kim, J., Lee, K., Chung, S., and Cho, J. Clam-tts: Improving neural codec language model for zero-shot text-to-speech. In The Twelfth International Conference on Learning Representations. Kong, J., Kim, J., and Bae, J. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. Advances in neural information processing systems, 33:1702217033, 2020. Kumar, R., Seetharaman, P., Luebs, A., Kumar, I., and Kumar, K. High-fidelity audio compression with improved rvqgan. Advances in Neural Information Processing Systems, 36, 2024. Lajszczak, M., Cámbara, G., Li, Y., Beyhan, F., van Korlaar, A., Yang, F., Joly, A., Martín-Cortinas, Á., Abbas, A., Michalski, A., Moinet, A., Karlapati, S., Muszynska, E., Guo, H., Putrycz, B., Gambino, S. L., Yoo, K., Sokolova, E., and Drugman, T. BASE TTS: lessons from building billion-parameter text-to-speech model on 100k hours of data. CoRR, abs/2402.08093, 2024. Łajszczak, M., Cámbara, G., Li, Y., Beyhan, F., van Korlaar, A., Yang, F., Joly, A., Martín-Cortinas, Á., Abbas, A., Michalski, A., et al. Base tts: Lessons from building billion-parameter text-to-speech model on 100k hours of data. arXiv preprint arXiv:2402.08093, 2024. Le, M., Vyas, A., Shi, B., Karrer, B., Sari, L., Moritz, R., Williamson, M., Manohar, V., Adi, Y., Mahadeokar, J., et al. Voicebox: Text-guided multilingual universal speech generation at scale. Advances in neural information processing systems, 36, 2024. Li, X., Liu, S., Lam, M. W., Wu, Z., Weng, C., and Meng, H. Diverse and expressive speech prosody prediction with denoising diffusion probabilistic model. arXiv preprint arXiv:2305.16749, 2023. Liu, H., Xu, X., Yuan, Y., Wu, M., Wang, W., and Plumbley, M. D. Semanticodec: An ultra low bitrate semantic audio codec for general sound. arXiv preprint arXiv:2405.00233, 2024. Ma, L., Guo, D., Song, K., Jiang, Y., Wang, S., Xue, L., Xu, W., Zhao, H., Zhang, B., and Xie, L. Wenetspeech4tts: 12,800-hour mandarin tts corpus for large speech generation model benchmark. arXiv preprint arXiv:2406.05763, 2024. Ma, N., Tong, S., Jia, H., Hu, H., Su, Y.-C., Zhang, M., Yang, X., Li, Y., Jaakkola, T., Jia, X., et al. Inference-time scaling for diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732, 2025. Ma, Z., Zheng, Z., Ye, J., Li, J., Gao, Z., Zhang, S., and Chen, X. emotion2vec: Self-supervised pre-training arXiv preprint for speech emotion representation. arXiv:2312.15185, 2023. Meng, L., Zhou, L., Liu, S., Chen, S., Han, B., Hu, S., Liu, Y., Li, J., Zhao, S., Wu, X., Meng, H., and Wei, F. Autoregressive speech synthesis without vector quantization. CoRR, abs/2407.08551, 2024a. Meng, L., Zhou, L., Liu, S., Chen, S., Han, B., Hu, S., Liu, Y., Li, J., Zhao, S., Wu, X., et al. Autoregressive speech synthesis without vector quantization. arXiv preprint arXiv:2407.08551, 2024b. Mentzer, F., Minnen, D., Agustsson, E., and Tschannen, M. Finite scalar quantization: VQ-VAE made simple. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=8ishA3LxN8. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 52065210. IEEE, 2015. 12 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis Parker, J. D., Smirnov, A., Pons, J., Carr, C., Zukowski, Z., Evans, Z., and Liu, X. Scaling transformers for low-bitrate high-quality speech coding. arXiv preprint arXiv:2411.19842, 2024. Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. Wang, Y., Zhan, H., Liu, L., Zeng, R., Guo, H., Zheng, J., Zhang, Q., Zhang, X., Zhang, S., and Wu, Z. Maskgct: Zero-shot text-to-speech with masked generative codec transformer. arXiv preprint arXiv:2409.00750, 2024. Xin, D., Tan, X., Takamichi, S., and Saruwatari, H. Bigcodec: Pushing the limits of low-bitrate neural speech codec. arXiv preprint arXiv:2409.05377, 2024. Ye, Z., Sun, P., Lei, J., Lin, H., Tan, X., Dai, Z., Kong, Q., Chen, J., Pan, J., Liu, Q., et al. Codec does matter: Exploring the semantic shortcoming of codec for audio language model. arXiv preprint arXiv:2408.17175, 2024. Zhang, X., Zhang, D., Li, S., Zhou, Y., and Qiu, X. Speechtokenizer: Unified speech tokenizer for speech language In The Twelfth International Conference on models. Learning Representations, 2024. Zhang, Z., Zhou, L., Wang, C., Chen, S., Wu, Y., Liu, S., Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S., and Wei, F. Speak foreign languages with your own voice: Cross-lingual neural codec language modeling. CoRR, abs/2303.03926, 2023. Zhou, K., Sisman, B., Liu, R., and Li, H. Seen and unseen emotional style transfer for voice conversion with new emotional speech dataset. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 920924. IEEE, 2021. Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. survey on model compression for large language models. Transactions of the Association for Computational Linguistics, 12:15561577, 2024a. Zhu, X., Tian, W., and Xie, L. Autoregressive speech synthesis with next-distribution prediction. arXiv preprint arXiv:2412.16846, 2024b. Pratap, V., Xu, Q., Sriram, A., Synnaeve, G., and Collobert, R. Mls: large-scale multilingual dataset for speech research. ArXiv, abs/2012.03411, 2020a. Pratap, V., Xu, Q., Sriram, A., Synnaeve, G., and Collobert, R. Mls: large-scale multilingual dataset for speech research. arXiv preprint arXiv:2012.03411, 2020b. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., and Sutskever, I. Robust speech recognition via largescale weak supervision. In International conference on machine learning, pp. 2849228518. PMLR, 2023. Reddy, C. K., Gopal, V., and Cutler, R. Dnsmos: nonintrusive perceptual objective speech quality metric to evaluate noise suppressors. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 64936497. IEEE, 2021. Ren, Y., Hu, C., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. Fastspeech 2: Fast and high-quality end-to-end text to speech, 2022. URL https://arxiv.org/abs/ 2006.04558. Saeki, T., Xin, D., Nakata, W., Koriyama, T., Takamichi, S., and Saruwatari, H. Utmos: Utokyo-sarulab sysarXiv preprint tem for voicemos challenge 2022. arXiv:2204.02152, 2022. Siuzdak, H. Vocos: Closing the gap between time-domain and fourier-based neural vocoders for high-quality audio synthesis. In The Twelfth International Conference on Learning Representations. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Song, Y., Chen, Z., Wang, X., Ma, Z., and Chen, X. Ella-v: Stable neural codec language modeling with alignment-guided sequence reordering. arXiv preprint arXiv:2401.07333, 2024. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 13 Score 3 Correct emotion recognition and appropriate rendering / 正确识别 情感并恰当表达 Natural rendering of paralinguistic cues with appropriate emphasis / 自然表达语调学关键词恰当 强调 Accurately captures the poetic structure, imagery, and emotional depth / 准确捕捉诗歌的结构 意象和情感深度 Correct pronunciation and meaning of polyphonic characters / 多 音字发音和意义正确 Correct intonation patterns that clearly convey the questioning nature / 语调模式正确清晰传达 问句的性质 Clear and accurate articulation of the tongue twister without errors / 清晰准确地表达绕口令无错 误 Accurate pronunciation and insightful interpretation of rare characters / 生僻字发音和解释准确 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis A. Test set for text understanding ability A.1. Chinese Evaluation criteria Table 6. Evaluation Criteria for Chinese Test Set Category Emotion Paralinguistic Chinese Poetry Polyphonic Characters Questions Tongue Twisters Score 1 No detectable emotion / 无可检测 的情感 No recognition of paralinguistic cues like interjections / 未 识 别 出 语 调 学 关 键 词 如哎 呀或嘘 Fails to capture the poetic structure and imagery / 未能捕捉诗歌的结 构和意象 Incorrect pronunciation and meaning of polyphonic characters / 多 音字发音错误意义不正确 Intonation pattern incorrect, failing to convey questioning tone / 语调模式不正确未能传达问 句的语气 Inability to articulate the tongue twister, resulting in errors / 无法 清晰表达绕口令导致错误 Rare Characters Mispronunciation or incorrect interpretation of rare characters / 生 僻字发音错误或解释不正确 A.2. Chinese Samples A.2.1. EMOTION 1. 她激动地喊道我做到了真的做到了 2. 他愤怒地说你再这样我就受不了了 3. 她悲伤地低声哭泣为什么会这样 4. 他欣喜地笑着说这真是太棒了 5. 她紧张地结结巴巴我不知道该怎么办 6. 他轻松地说道没关系我们可以解决的 7. 她失望地叹了口气我本以为会更好 8. 他兴奋地跳了起来我们赢了 9. 她害怕地颤抖不要靠近我 10. 他感激地说谢谢你你帮了我大忙 11. 她羞涩地笑了笑我不敢相信 12. 他绝望地喊道一切都结束了吗 13. 她自豪地说这是我的成就 14. 他焦虑地问我们还能挽回吗 15. 她愉快地唱起歌来今天真是美好的一天 16. 他疲惫地叹息我需要休息 17. 她兴奋地说道快看那里有烟花 18. 他冷静地回答我们需要保持镇定 19. 她惊讶地说这是真的吗 20. 他满足地微笑这一切都很值得 Score 2 Emotion present but not convincingly rendered / 存在情感但表达 不够令人信服 Attempts to render paralinguistic cues but unnatural / 明确意图表 达关键词但表达不自然 Captures some poetic elements but lacks depth / 捕捉了一些诗歌元 素但缺乏深度 Attempts correct pronunciation but with minor errors / 尝试正确发音 但有小错误 Intonation pattern largely correct but with minor flaws / 语调模式 大体正确但有细微瑕疵 Attempts articulation with some errors / 尝试表达绕口令但有部分 错误 Attempts correct pronunciation and interpretation with minor mistakes / 尝试正确发音和解释但 有小错误 14 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis A.2.2. PARALINGUISTIC 1. 哎呀这雨下得噼里啪啦的看来今天的郊游计划又泡汤喽 2. 哇塞烟花在夜空中嗖地一声炸开瞬间绽放出五彩斑斓的光芒好美呀 3. 哼他总是大大咧咧的走路都咚咚咚地响一点都不安静呢 4. 嘿哟这箱子可真沉啊我费了好大劲才吭哧吭哧地搬起来 5. 咦这只小猫怎么一直喵喵喵地叫呀是不是饿了呢 6. 哟呵你看那只小狗摇着尾巴汪汪汪地跑过来好可爱呀 7. 唉那我就有点好奇了 8. 哇厨房里传来咕噜咕噜的声音肯定是妈妈煮的汤快好了好香啊 9. 哈哈小朋友们在操场上嘻嘻哈哈地玩耍笑声一阵接着一阵呢 10. 哇哦海浪拍打着沙滩发出哗哗的声音好像在演奏一首美妙的乐章呢 11. 哎呦这蚊子嗡嗡嗡地在耳边飞来飞去烦死我了 12. 哎呀妈呀这只大鹅伸长了脖子嘎嘎嘎地叫着气势汹汹地朝我冲过来 13. 嗯我觉得这主意不错她迟疑地说 14. 嘘小声点我们不能被发现他低语道 15. 额我不知道该怎么说她尴尬地回应 16. 哦没关系我可以处理她自信地说 17. 啊天哪她惊讶地喊道 18. 呃好吧那我们开始吧她决定性地说 19. 嘘露西嘘. . . . . . 别吵醒你弟弟汤姆小声叮嘱两人轻手轻脚地走过婴儿房 20. 啊忘了带钥匙了她懊恼地说 A.2.3. CHINESE POETRY 1. 床前明月光疑是地上霜举头望明月低头思故乡 2. 恰同学少年风华正茂书生意气挥斥方遒指点江山激扬文字粪土当年万户侯曾记否到中流击 水浪遏飞舟 3. 人生易老天难老岁岁重阳今又重阳战地黄花分外香 4. 雄关漫道真如铁而今迈步从头越从头越苍山如海残阳如血 5. 红军不怕远征难万水千山只等闲 6. 踏遍青山人未老风景这边独好 7. 江山如此多娇引无数英雄竞折腰惜秦皇汉武略输文采唐宗宋祖稍逊风骚一代天骄成吉思汗 只识弯弓射大雕俱往矣数风流人物还看今朝 8. 天若有情天亦老人间正道是沧桑 9. 才饮长沙水又食武昌鱼万里长江横渡极目楚天舒 10. 风雨送春归飞雪迎春到已是悬崖百丈冰犹有花枝俏俏也不争春只把春来报待到山花烂漫时她 在丛中笑 11. 朱雀桥边野草花乌衣巷口夕阳斜旧时王谢堂前燕飞入寻常百姓家 12. 劝君莫惜金缕衣劝君惜取少年时花开堪折直须折莫待无花空折枝 13. 红豆生南国春来发几枝愿君多采撷此物最相思 14. 绿蚁新醅酒红泥小火炉晚来天欲雪能饮一杯无 15. 生当作人杰死亦为鬼雄至今思项羽不肯过江东 16. 遥想公瑾当年小乔初嫁了雄姿英发羽扇纶巾谈笑间樯橹灰飞烟灭故国神游多情应笑我早生华 发人生如梦一尊还酹江月 17. 多情自古伤离别更那堪冷落清秋节今宵酒醒何处杨柳岸晓风残月此去经年应是良辰好景虚 设便纵有千种风情更与何人说 18. 红藕香残玉簟秋轻解罗裳独上兰舟云中谁寄锦书来雁字回时月满西楼花自飘零水自流一种相 思两处闲愁此情无计可消除才下眉头却上心头 19. 昨夜雨疏风骤浓睡不消残酒试问卷帘人却道海棠依旧知否知否应是绿肥红瘦 20. 帘外雨潺潺春意阑珊罗衾不耐五更寒梦里不知身是客一晌贪欢独自莫凭栏无限江山别时容易 见时难流水落花春去也天上人间 15 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis A.2.4. POLYPHONIC CHARACTERS 1. 这种弹弓的弹力很强 2. 人参苗长得参差不齐还能让人参观吗 3. 下午要召开工作会议你去通知一下张会计 4. 这几张茶几几乎都要散架了 5. 这里有很多畜牧场养殖了很多牲畜 6. 人要是行干一行行一行一行行行行行行行行干哪行都行要是不行干一行不行一行一行不行行行 不行行行不行干哪行都不行 7. 他在长跑比赛中行动迅速长期的训练让他在行动中表现出色重视每一次长跑的行动细节 8. 他在行业会议上行动自如行走于各类行动之间行使政策得以行动实施令在场行业内人士纷纷称赞他的 行动能力 9. 他得到了一份得力的行政支持得亏了他得以顺利行动 10. 她乐于助人有着乐观的工作态度在乐队中乐声悠扬乐迷们对她的乐器演奏赞不绝口 11. 他着急地着手准备着实需要更多时间完成任务着迷于工作的细节 12. 在好莱坞的片场她很好学也演好了每一个角色赢得了观众的好评 13. 她干劲十足地干了所有使库房干燥的工作 14. 穿上便服就可以买到便宜的商品也可也搭乘便车 15. 这几天天天天气不好 16. 来到杨过曾经生活过的地方小龙女动情的说我也想过过过儿过过的生活 17. 我有一个小本本本来很干净 18. 今天下雨我骑车差点摔倒好在我一把把把把住了 19. 校长说校服上除了校徽别别别的让你们别别别的你非别别的 20. 你去班上数数数数数不好的有多少 A.2.5. QUESTIONS 1. 今天的会议通知你收到了吗咱们几点在哪个会议室集合呀 2. 这部新上映的电影口碑据说很不错你打算去看吗看完觉得怎么样 3. 你最近工作那么忙有时间好好休息吗身体吃得消吗 4. 周末咱们一起去郊外野餐怎么样你有空吗 5. 你知道明天的天气预报吗会不会下雨呀 6. 你在大学学的是什么专业毕业后从事的工作和专业对口吗 7. 这道数学题我怎么都解不出来你会做吗能教教我吗 8. 你喜欢什么类型的音乐是流行摇滚还是古典呢 9. 你去过国外旅游吗最喜欢哪个国家为什么 10. 你觉得我们这次的项目计划可行吗还有哪些地方需要改进 11. 难道我们遇到一点困难就应该退缩吗这可不是我们一贯的作风 12. 父母含辛茹苦把我们养大我们难道不应该好好孝顺他们吗 13. 浪费粮食这种行为难道不应该受到谴责吗 14. 保护环境是每个人的责任难道我们可以置之不理吗 15. 他为了集体利益付出了那么多我们难道不应该感激他吗 16. 老师每天辛苦备课批改作业我们难道不应该尊重他们的劳动成果吗 17. 机会摆在面前我们难道要眼睁睁地看着它溜走吗 18. 努力学习才能有更好的未来难道这一点还需要怀疑吗 19. 大家都在为了目标拼搏奋斗我们难道能偷懒懈怠吗 20. 诚实守信是做人的基本准则难道我们可以随意违背吗 A.2.6. TONGUE TWISTERS 1. 老龙恼怒闹老农老农恼怒闹老龙农怒龙恼农更怒龙恼农怒龙怕农 2. 四是四十是十十四是十四四十是四十莫把四字说成十休将十字说成四若要分清四十和十四经 常练说十和四 16 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis 3. 石狮寺前有四十四个石狮子寺前树上结了四十四个涩柿子四十四个石狮子不吃四十四个涩柿子四十四 个涩柿子倒吃四十四个石狮子 4. 粉红墙上画凤凰凤凰画在粉红墙红凤凰粉凤凰红粉凤凰花凤凰 5. 哥哥挎筐过宽沟快过宽沟看怪狗光看怪狗瓜筐扣瓜滚筐空哥怪狗 6. 坡上立着一只鹅坡下就是一条河宽宽的河肥肥的鹅鹅要过河河要渡鹅不知是鹅过河还是河渡 鹅 7. 三哥三嫂子借给我三斗三升酸枣子等我上山摘了三升三斗酸枣子再奉还三哥三嫂子这三斗三升酸枣 子 8. 墙上一个窗窗上一支枪窗下一箩糠枪落进了糠糠埋住了枪窗要糠让枪糠要枪上墙墙要枪上 窗互相不退让糠赶不走枪枪也上不了窗和墙 9. 蓝教练是女教练吕教练是男教练蓝教练不是男教练吕教练不是女教练蓝南是男篮主力吕楠是女篮 主力吕教练在男篮训练蓝南蓝教练在女篮训练吕楠 10. 任命是任命人名是人名任命人名不能错错了人名错任命 11. 小华和胖娃一同种庄稼小华种棉花胖娃种西瓜小华的棉花开了花胖娃的西瓜结了瓜小华摘棉 花胖娃摘西瓜棉花西瓜装一塌小华胖娃笑哈哈 12. 黑豆放在黑斗里黑斗里边放黑豆黑豆放黑斗黑斗放黑豆不知黑豆放黑斗还是黑斗放黑豆 13. 石小四和史肖石一同来到阅览室石小四年十四史肖石年四十年十四的石小四爱看诗词年四十的史 肖石爱看报纸年四十的史肖石发现了好诗词忙递给年十四的石小四年十四的石小四见了好报纸忙递给 年四十的史肖石 14. 天上七颗星地上七块冰台上七盏灯树上七只莺墙上七枚钉吭唷吭唷拔脱七枚钉喔嘘喔嘘赶走七 只莺乒乒乓乓踏坏七块冰一阵风来吹来七盏灯一片乌云遮掉七颗星 15. 白老八门前栽了八颗白果树从北边飞来了八个白八哥儿不知在哪住白老八拿了八个巴达棍儿要打八个白 八哥儿八个白八哥儿飞上了八颗白果树不知道白老八拿这八个巴达棍儿打着了八个白八哥儿还是打着了 八颗白果树 16. 针蓝线蓝领子蓝蓝针蓝线蓝领蓝蓝针蓝线连蓝领针蓝线蓝领子蓝 17. 白猫满白毛房里一白猫白猫满白毛毛白白猫白白猫白毛毛 18. 炖冻冬瓜冬瓜冻冻冬瓜炖冻冬瓜是炖冻冬瓜不炖冻冬瓜不是炖冻冬瓜炖冻冬瓜吃炖冻冬瓜不炖冻 冬瓜不吃炖冻冬瓜 19. 补皮裤皮裤破补皮裤皮裤不破不补裤 20. 父母的父母扶父母父母扶父母的父母父母是父母的父母父母的父母是父母 A.2.7. RARE CHARACTERS 1. 嵇康在刑场抚琴那曲广陵散如黄钟大吕余音袅袅展现出他不向世俗低头的狷介juàn jiè风骨 2. 林黛玉常常顾影自怜在潇湘馆里她的情思如缱绻qiˇan quˇan的丝线缠绕着无尽的哀愁 3. 尼采的思想犹如浩瀚星空中的璀璨流星以其特立独行的哲思打破了人们习以为常的谫陋jiˇan lòu认 知 4. 妲己凭借着自己的妖娆妩媚在商纣王的宫廷中弄权她的行为可谓是牝鸡司晨pìn jı sı chén加速了商 朝的灭亡 5. 李白在月下独酌酒入愁肠他的才情如奔涌的江水挥洒出一篇篇脍炙人口的诗篇尽显其倜傥不羁tì tˇang bù jı的风采 6. 杨绛先生一生笔耕不辍她的文字温润如玉蕴含着对生活的深刻洞察她的智慧与豁达令人肃然起敬堪 称一代懿范yì fàn 7. 蒲松龄笔下的狐仙鬼怪或狡黠灵动或温婉善良在他构建的奇幻世界里演绎着人间的悲欢离合充满 了谲诡jué guˇı的色彩 8. 阿基米德在浴缸中顿悟浮力原理的那一刻灵感如电光石火般闪现他的智慧光芒照亮了科学发展的漫漫长 路成为了后世敬仰的圭臬guı niè 9. 李清照在国破家亡之际写下了许多凄婉动人的词句她的愁绪如氤氲yın yun的雾气弥漫在字里行 间令人为之动容 10. 苏轼一生宦海沉浮却始终保持着豁达乐观的心态他在黄州的岁月里寄情山水写下了许多脍炙人口的 佳作尽显其旷达超逸kuàng dá chao yì的情怀 11. 王尔德以其华丽而叛逆的文风著称他的作品中充满了对传统道德的揶揄yé yú和对人性的深刻剖析 12. 阮籍常常在山野间纵酒放歌他的行为举止看似荒诞不经实则是对当时黑暗社会的一种隐晦的訾议zˇı 17 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis yì 13. 居里夫人在简陋的实验室里经过无数次的尝试和失败终于发现了镭元素她的坚韧和执着成为了科学界 的楷模是当之无愧的巾帼豪杰jın guó háo jié 14. 卡夫卡的作品中充满了荒诞与迷茫他笔下的人物常常在孤独和绝望中挣扎展现出一种难以言喻的幽眇 you miˇao情感 15. 屈原在汨罗江畔徘徊他的心中充满了对国家和人民的忧虑最终以投江的方式表达了自己的忠贞不渝他 的精神成为了中华民族的亢宗之子kàng zong zhı zˇı 16. 张爱玲的文字犀利而又细腻她以独特的视角描绘了旧上海的繁华与落寞她的才情和孤傲令人侧目是文 坛上一颗璀璨的明珠散发着独特的容修态kua róng xiu tài 17. 王阳明在龙场悟道创立了心学他的思想如醍醐灌顶tí hú guàn dˇıng对后世的哲学发展产生了深远的 影响 18. 泰戈尔的诗歌充满了对自然和生命的热爱他的文字如潺潺溪流流淌着温暖和希望他的作品具有一种独 特的骀荡dài dàng之美 19. 杜甫在战乱年代目睹了百姓的疾苦他的诗歌如黄钟毁弃huáng zhong huˇı qì发出了对社会不公的强 烈控诉成为了历史的真实写照 20. 列夫托尔斯泰在他的作品中深刻地揭示了人性的善恶美丑他的思想如振聋发聩zhèn lóng fa kuì的钟 声唤醒了人们对生活的思考 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis A.3. English Evaluation critieria Copy from Base-TTS (Łajszczak et al., 2024) for better reading. Table 7. English evaluation criteria, copy from Base-TTS (Łajszczak et al., 2024) Categories Example sentence Evaluation criteria Compound Nouns The Beckhams decided to rent charming stone-built quaint countryside holiday cottage. Emotions Foreign Words \"Oh my gosh! Are we really going to the Maldives? Thats unbelievable!\" Jennie squealed, bouncing on her toes with uncontained glee. Mr. Henry, renowned for his mise en place, orchestrated seven-course meal, each dish pièce de résistance. Paralinguistics \"Shh, Lucy, shhh, we mustnt wake your baby brother,\" Tom whispered, as they tiptoed past the nursery. Punctuations She received an odd text from her brother: Emergency @ home; call ASAP! Mom & Dad are worried...#familymatters. Questions Syntactic Complexities But the Brexit question remains: After all the trials and tribulations, will the ministers find the answers in time? The movie that De Moya who was recently awarded the lifetime achievement award starred in 2022 was box-office hit, despite the mixed reviews. 1 = fails to recognise compound nouns 2 = fails to realise the phrasal stress naturally 3 = natural phrasal stress 1 = no audible emotions 2 = emotion present but insufficient 3 = correct emotion recognition and appropriate rendering 1 = pronounces foreign words with incorrect anglicized pronunciation 2 = applies foreign accent but not entirely correctly 3 = correct rendering in the intended language or accepted anglicized reading 1 = no recognition of paralinguistic keywords such as \"shhh\" or \"phew\" 2 = clear intention to render keywords distinctly, but rendering unnatural 3 = natural rendering, e.g. making speech voiceless on \"shhh\" and other whispered speech 1 = glitches on uncommon punctuations such as # or & 2 = no glitch but incorrect rendering 3 = no glitch and correct pausing and verbalization, e.g. @ as \"at\". 1 = intonation pattern incorrect 2 = intonation pattern largely correct but with minor flaws 3 = correct intonation 1 = failure to parse the syntax correctly 2 = parses the syntax largely correctly but the rendering is not entirely natural 3 = parsing correct and rendering natural 19 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis A.4. English Samples A.4.1. QUESTIONS 1. You went to the party, even though explicitly told you not to? 2. There is another aircraft still in the air??? 3. Now, seriously, youre saying am the one to blame? 4. But she clearly doesnt want to? 5. To Hungary and back? 6. Youre copper? 7. What is Data Informed Diplomacy, with all its various manifestations? 8. Whats really happening, and is there more than meets the eye? 9. How on earth is this Financial Report organized? 10. Where has Jason Abhisheki moved all the flowers to? 11. What do we do in this situation, and what are the implications for Jordans water supply? 12. But the Brexit question remains: After all the trials and tribulations, will the ministers find the answers in time? 13. Sorry, can you restate your name and address please? 14. Heres the full story for today, would you like to learn more? 15. Mr. Chairman, your highly anticipated interview with Channel 4 has turned into catastrophe, hasnt it? 16. Johnny boy, dont go around acting tough if you cant back it up, right? 17. Are you in favor of the Latex usage policy or youre just sucking up to leadership? 18. Is it bird, or is it plane? 19. Madam, have you tried turning it off and on again? 20. Were you the one with the hand-held camera or the one with weird-looking android phone? A.4.2. EMOTIONS 1. Her hands shaking with excitement, Alice Monroe stuttered, \"oh..I-I cant believe it! Is this really my acceptance letter to Harvard?\" Marco cannot believe it either: \"God damn it! How did you pull this off?\" 2. surge of anger flashed across the face of Matthew, as he bellowed, \"You have crossed the line this time, and wont stand for it any longer! Get out!\" 3. Gazing at the panoramic view from mountain in Iceland, Jeff Farahmand sighed deeply, whispering, \"This... this is truly the face of the Divine. What more can ask for?\" 4. \"You mustnt underestimate how profoundly Ive missed your presence,\" Ito murmured, his eyes glistening with tears as he embraced his long lost sister. \"Youre finally back, but where do find our lost years?\" 5. \"Oh my gosh! Are we really going to the Maldives? Thats unbelievable!\" Jennie squealed, bouncing on her toes with obvious glee. 6. \"I can confidently declare that this is the most exquisite chocolate cake my taste buds have ever had the pleasure to encounter!\" Mo proclaimed, savoring every bite. He could not stop eating! 7. proud smile spread across his face as he softly said, \"Son, your accomplishments fill my heart with such joy and pride.\" But then the smile suddenly ceased. Mikes hearts were pounding like door knocks. His dads face now looks like that of the devil himself. 20 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis 8. Choking back sobs, Mahmoud whimpered, \"I simply cant fathom life without you by my side. Dont go!\" 9. His voice trembled with palpable fear as he stuttered, \"Theres... theres stranger at the window. Where the hell are you all waiting for?!\" 10. Tears of joy trickled down her cheeks as she yelled, \"Graduating as valedictorian... this is dream come true!\" 11. Janes eyes wide with terror, she screamed, \"The brakes arent working! What do we do now? Were completely trapped!\" 12. profound sense of realization washed over Beal as he whispered, \"Youve been there for me all along, havent you? never truly appreciated you until now.\" 13. Beth collapsed into his arms, sobbing uncontrollably, \"I failed them, failed them all. Theyre all dead! Nothing we can do will ever bring them back. How can ever live with myself again? How?\" 14. His face lit up with pure delight as he exclaimed, \"We did it! We won the championship! knew we could do it together!\" 15. Overcome with guilt, Martin hung his head and muttered, \"Im so sorry. never meant to hurt you like this. Can you ever forgive me?\" It was obvious what the answer would be. 16. The queen danced around the room, eyes twinkling with mischief, \"Guess what? got the lead role in the play! Can you believe it? Well, cant.\" 17. Staring into the distance, the firefighter said with melancholic smile, \"She used to sit right there, you know. can still hear her laugh if close my eyes.\" Outside the window, the rain was pouring down and gushing through every cracks. 18. The detectives voice, full of determination and fire, was heard loud and clear in the room, \"No one will tell me what can or cannot do. Ill prove them all wrong! Get me my gun. What are you all looking at me for?\" 19. Overwhelmed with confusion and despair, David Darlan cried out, \"What do you want from me? Why cant you just tell me whats wrong? Leave me alone!\" 20. With gentle touch and loving smile, she reassured, \"Dont worry, my love. Well get through this together, just like we always have. love you.\" A.4.3. COMPOUND NOUNS 1. In the heart of Lagos, there is public park with serene duck pond. Nearby, the childrens outdoor play area is full of joyful laughter. Nobody knows the darkness descending soon. 2. At the family reunion, my grandfather, or father-in-law for some, told many tongue-in-cheek jokes. 3. The physics teacher asked the students to build new model solar system. Students were told to bring tape measure and pair of scissors, to cut the scale-model planet rings. 4. On this fateful day in 1987, the students boarded the little yellow school bus, chattering excitedly about their field trip to the zoo. 5. Hello, we are representatives from Northern Airlines. Please look out from the big second-floor window seat. 6. After years of work, Heisenberg finally published ground-breaking cutting-edge research paper on quantum physics. 7. Recipe for delicious breakfast sandwich: avocado, egg, and cheese on bagel, cooked over stovetop frying pan. 8. There is nothing more peaceful than blue water fountain with wooden greenhouse. Near there, Joseph installed hard stone birdbath. 9. Prague, Czechia: Good morning, Harari! Here come the big shopping carts and last-minute video game shoppers. 10. My dog knocked over the tea table and all the books scattered across the second living room floor. 11. The hiking trail up Yahu Mountain provides spectacular view of the sunrise. Along the path, the wooden signposts with triple-checked trail maps and green distance markers guided us. 12. The fish clock tower was striking again, reminding us of that profound changing of the guard. 21 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis 13. Dean Graham sat on the packed wooden park bench, feeding the pigeons while enjoying the pleasant weather. 14. The Beckhams decided to rent charming stone-built quaint countryside holiday cottage. 15. The construction of the new Newtown-council town hall has made huge trouble; rush-hour traffic jam has never been worse. 16. Owen Farrell has taken England to the Rugby World Cup glory, with razor-thin-margin victory against New Zealand in France. 17. Scientists at AWS teams are making last-minute pre-launch model preparations. 18. Bad weather in Northern Europe has caused god-awful flight check-in time of 6 AM, when even the airport food court isnt open. 19. Jake Park boasts beautiful hand-built wooden bird feeder. 20. We visited quaint bed-and-breakfast establishment, complete with lighthouse lamp room. A.4.4. SYNTACTIC COMPLEXITY 1. The complex houses married and single soldiers and their families. 2. Time flies like an arrow; fruit flies like banana. 3. The rat the cat the dog chased killed ate the malt. 4. After the writer the editor the publisher hired fired quit, the company found itself in quite bind. 5. The old man the boats on the shore were manned by had long history of seafaring. 6. Anyone who feels that if so many more students whom we havent actually admitted are sitting in on the course than ones we have that the room had to be changed, then probably auditors will have to be excluded, is likely to agree that the curriculum needs revision. 7. While John, who had been working late every night for month on his novel, finally took break to enjoy the fresh air, his neighbor, painter who often found inspiration in the midnight moon, was just beginning her creative process. 8. In the old village with its winding roads, colorful marketplaces, sense of history that permeates every brick, and single traffic light, youll find peace and simplicity. 9. The chef seasoning the fish tossed it gently. 10. As the sun dipped below the horizon, casting golden glow over the ocean, Emily, who had spent her life dreaming of distant shores, stood on the deck of the ship, feeling mixture of anticipation and nostalgia as her adventure began. 11. During the meeting, where Coke executives debated the future of the company, Thomas, young intern who had discovered solution, mustered the courage to speak, shifting the direction of the conversation, that preceded his intervention. 12. The movie that De Moya who was recently awarded the lifetime achievement award starred in 2022 was box-office hit, despite the mixed reviews. 13. In the garden, where the flowers that the gardener who retired last year still bloomed, the children who play there every afternoon find peace and joy. 14. The scientist, Mateusz Gorka, who proposed the theory, which many experts in the field, including those who had initially been skeptical bordering on disbelieving, now support, was nominated for prestigious award. 15. Although the meal that the chef, who had just returned from culinary tour of Italy, prepared was delicious, the Greek guests barely noticed. 16. The book that the woman who the man who the child spoke to this morning was reading became topic of conversation among the friends who had all read it. 17. Despite the fact that the road that led to the Five Villages, which was known for its scenic beauty, was narrow and winding, tourists flocked there throughout the year. 22 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis 18. CNN journalists tracking the stories behind the officials who served during the tumultuous period when the protests rocked the nation to its core noticed significant inconsistencies in the official reports provided. 19. The musicians who performed the symphony that the composer, whose work had often been overlooked in his lifetime, wrote in his early years received standing ovation. 20. Cars displayed in these showrooms with ENERGY-EFFICIENT AND GREEN decals prominently featured across the windshield arent announcing environmentalism; theyre virtue signaling. A.4.5. FOREIGN WORDS 1. With an ample supply of joie de vivre, Mary danced through the streets of Nice, stopping only to enjoy nice cafe with warm croissant. 2. The modern exhibit was mélange of styles, from German Expressionism to French Impressionism, capturing the Zeitgeist of the time. 3. As gesture of camaraderie, the Spanish torero invited his rival, Leo the Monster, to tapas bar, where they enjoyed jamón ibérico and the noche. 4. During Anthonys wanderlust-filled travels, he discovered the gemütlich atmosphere of many Austrian villages. 5. CloudCorps CEO believes in gesamtkunstwerk, like integrating symphony into harmonious ensemble. 6. Mr. Henry, renowned for his mise en place, orchestrated seven-course meal, each dish pièce de résistance. 7. The fiesta, filled with música, dance, and the warmth of amigos, continued until dawn, embodying the true spirit of Catalan celebration. 8. At the G20 Summit, leaders discussed rapprochement, trying to step away from the Schadenfreude of political rivalries. 9. After tiring day, Sarah treated herself to spa experience, enjoying the sauna and the jacuzzi, and relaxing with glass of Riesling. 10. Lassos novella, rich in allegory and imbued with sense of ennui, drew from his experiences living in French château up near the border. 11. The master from Osaka, Japan, dedicated himself to crafting the perfect \"nigiri,\" with \"umami\" flavors dancing on the palate. 12. Mikhail Gorbachevs Reforms: Perestroika and Glasnost Define New Era. 13. Lakshmis yoga practice, centered around the Sanskrit concept of \"ahimsa,\" influenced her approach to life, mirroring the teachings of Mahatma Gandhi. 14. As they strolled through the Grand Bazaar in Istanbul, they were drawn to the beautiful \"kilims,\" the best of Turkic craftsmanship. 15. Inspired by the ancient Chinese philosophy of \"Feng Shui,\" Li rearranged her house to create \"qi\" flow throughout. 16. Embracing the Japanese aesthetic of \"wabi-sabi,\" Hokusais masterpieces were on full display here. 17. During Rio de Janeiros famous Carnaval do Brasil, the streets pulsated with the rhythms of \"samba\". 18. The novels protagonist, guided by the ancient Greek concept of \"arete,\" seeks excellence and virtue, journey reminiscent of warrior-philosopher-kings. 19. As an aficionado of Scandinavian design, Ole Gunnarsson appreciated the principle of \"hygge,\" evident in his Danish home. 20. These soldiers - theyre supposed to practice with sense of \"bushido\", the samurai code of honor, but theyre behaving like the imperial beasts they are. 23 Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis A.4.6. PUNCTUATIONS 1. After moment of silence, Elena Ivanova finally spoke..., her words barely audible over the cracking thunder of torrential downpour. 2. What!?! Youre telling me youve never seen single episode of Game of Thrones before????! (This was not heard by Prof. Johnson, Dr. Lewis, etc.) 3. \"Can anyone hear me over there??? Please, we need help!!! NOW!!!!\" 4. The Power of & and % in the Digital Age. won the first prize in this conference. 5. His latest invention (a device meant to assist in everyday chores (something he never seemed to run out of)), was nothing short of brilliant. 6. She read the label and was surprised to find that the \"natural\" ingredients were actually ..... heavily processed. 7. He relayed his conversation with the bartender, saying, \"I told him, Your signature cocktail is simply Margarita with fancy garnish.\" 8. The presently announced laws were announced in 35N, 80W. Specific provisions are to be found in 12 and 17. 9. Please ensure you replace [username] and [password] with your actual credentials before logging in, like jA8!fR3$mQ1. 10. When Maria asked, Whats happening tonight? replied, Well, John wholl be there at 8:00 p.m. said, \"Lets meet at Sarahs place; bring games, snacks, etc., and dont be late!\" 11. \"In the case of Johnson v. Smith, the court found that the defendants actions e.g., his failure to fulfill the terms of the contract (see sections 4.1, 4.2, and 4.3), etc. amounted to breach of trust. 12. When asked for his thoughts, he simply replied, Ill be gone in 5 minutes, and left. 13. saw Gordon listing the ingredients as follows: <tomatoes>, <fresh basil> (or dried, if unavailable - but its essential), <olive oil>, <garlic>; salt and pepper. 14. She received an odd text from her brother: Emergency @ home; call ASAP! Mom & Dad are worried...#familymatters. 15. The sign at the parks entrance stated, Please adhere to the following rules: no littering; no pets (except service animals); no loud music after 9 p.m. 16. The Art of /Slash/ and backslash was the best received talk on modern internet lingo. 17. Jebs email was brief, but to the point: Meeting rescheduled for 3 p.m. tomorrow apologies for any inconvenience. Regards, J.. 18. The Dead Sea poems contained several annotations, some of which were quite puzzling: [Section unclear]; [Translation disputed]; [Original wording lost]. 19. Her travel blog post was filled with enthusiastic descriptions: Best trip ever!!!; Amazing people & culture!; Cant wait to go back...#wanderlust. 20. He shouted, Everyone, please gather round! Heres the plan: 1) Set-up at 9:15 a.m.; 2) Lunch at 12:00 p.m. (please RSVP!); 3) Playing e.g., games, music, etc. from 1:15 to 4:45; and 4) Clean-up at 5 p.m. A.4.7. PARALINGUISTICS 1. Principal Dickson began, addressing the Parkside assembly: \"Ahem, Id like to talk to you about something real serious.\" 2. \"Aha! Now understand,\" said Staff Sgt. Miller, piecing together the evidence. \"The culprit left this behind. Phew.\" 3. \"Ouch! That stings,\" Lilly cried, as her mother carefully applied the antiseptic. \"Not beyond salvation, eh?\" She dryly asked. 4. \"Shh, Lucy, sshhh, we mustnt wake your baby brother,\" Tom whispered, as they tiptoed past the nursery. 5. \"Hmm, what do you think, is it too high or two low, um... Dr. Carter?\" Haim asked, handing over the instrument. Llasa: Scaling Train-Time and Inference-Time Compute for Llama-based Speech Synthesis 6. \"Uh, well, Lisa,\" Tarek stuttered, nervously extending the ring he bought for god-knows how much, \"mmm..will you marry me?\" 7. \"Yawn,\" Robert said, stretching out on the park bench, \"this sunshine makes me sleepy.\" 8. \"Oops! did it again!\" little Katie exclaimed, spilling her milk. 9. \"Whoa, can you believe this, Mike?\" Susan said, staring at the intruder. \"Wow, youre right. These men aint meanin well.\" 10. James leaned back in his chair, wiped his forehead, and sighed, \"Phew, haha, that was tough meeting. Thanks for being there, Karen.\" 11. psst. psst. look right here. 12. \"Aha! Ive found it, Professor Green,\" exclaimed Muzi Han, holding up the rare manuscript. \"This could change our entire understanding of history.\" 13. \"Ouch, be careful, Henry!\" warned his sister, as he climbed the rickety ladder. 14. David whispered to Emily as the lights dimmed in the theater, \"Shh, its starting.\" 15. \"Hmm, dont know about this, Jim,\" Mary said, looking at the folder paper. \"It doesnt seem right.\" 16. \"Uh, are you sure about this?\" Tim asked nervously, looking at the steep slope before them. \"Whoa, its higher than thought,\" he continued, his voice filled with trepidation. \"Aha, but look at the view,\" Emily responded with excitement, \"its worth the climb!\" 17. Ta-da! well? What do you think? This is the best right? 18. \"Oops, sorry, Dad!\" Jack apologized. \"Ugh! you again\". Dad was impatient. 19. \"Whoa, what game, Alex!\" Chris exclaimed. \"Ive never seen anything like that final play.\" 20. \"Phew, we made it, Martha,\" Tom said, collapsing into the seat after the completion of the Manhattan Project."
        }
    ],
    "affiliations": [
        "ASLP Lab, Northwestern Polytechnical University",
        "Chinese University of Hong Kong",
        "Hong Kong Baptist University",
        "Independent Researcher",
        "Shanghai Mobvoi Information Technology Co., Ltd.",
        "The Hong Kong University of Science and Technology",
        "University of Rochester",
        "University of Science and Technology Beijing",
        "University of Surrey"
    ]
}