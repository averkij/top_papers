{
    "paper_title": "SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training",
    "authors": [
        "Jianyi Wang",
        "Shanchuan Lin",
        "Zhijie Lin",
        "Yuxi Ren",
        "Meng Wei",
        "Zongsheng Yue",
        "Shangchen Zhou",
        "Hao Chen",
        "Yang Zhao",
        "Ceyuan Yang",
        "Xuefeng Xiao",
        "Chen Change Loy",
        "Lu Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield a prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings. In this work, we propose a one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within a single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with a predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of a series of losses, including a proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in a single step."
        },
        {
            "title": "Start",
            "content": "SeedVR2: One-Step Video Restoration via Diffusion Adversarial Post-Training Jianyi Wang1,2 Zongsheng Yue1 Shanchuan Lin2 Zhijie Lin2 Yuxi Ren2 Meng Wei2 Shangchen Zhou1 Hao Chen2 Yang Zhao2 Ceyuan Yang2 5 2 0 2 J 5 ] . [ 1 1 0 3 5 0 . 6 0 5 2 : r Xuefeng Xiao2 Chen Change Loy1 Lu Jiang2 1Nanyang Technological University 2ByteDance Seed https://iceclear.github.io/projects/seedvr2/"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in diffusion-based video restoration (VR) demonstrate significant improvement in visual quality, yet yield prohibitive computational cost during inference. While several distillation-based approaches have exhibited the potential of one-step image restoration, extending existing approaches to VR remains challenging and underexplored, particularly when dealing with high-resolution video in real-world settings. In this work, we propose one-step diffusion-based VR model, termed as SeedVR2, which performs adversarial VR training against real data. To handle the challenging high-resolution VR within single step, we introduce several enhancements to both model architecture and training procedures. Specifically, an adaptive window attention mechanism is proposed, where the window size is dynamically adjusted to fit the output resolutions, avoiding window inconsistency observed under high-resolution VR using window attention with predefined window size. To stabilize and improve the adversarial post-training towards VR, we further verify the effectiveness of series of losses, including proposed feature matching loss without significantly sacrificing training efficiency. Extensive experiments show that SeedVR2 can achieve comparable or even better performance compared with existing VR approaches in single step."
        },
        {
            "title": "Introduction",
            "content": "Diffusion models [16, 38, 51, 62] are becoming the the de-facto model for real-world image restoration (IR) [36, 68, 84, 85, 87, 88, 96] and video restoration (VR) [30, 67, 74, 79, 97]. Though these approaches show promise in generating realistic details, they typically require tens of steps to generate video sample, leading to considerably high computational cost and latency. Such significant cost is further amplified when processing long videos at high resolutions. Inspired by recent advances in diffusion acceleration [42, 44, 56, 82], several one-step diffusion IR approaches [10, 15, 27, 28, 48, 54, 71, 72, 75, 85, 90, 99] have been proposed, showing potential in generating promising results comparable to that of multi-step approaches. The majority of these methods [10, 15, 27, 48, 54, 71, 72, 75, 99] rely on distillation from pre-trained teacher model, suffering from an undesired upper bound constrained by the teacher model. The high computational cost of the teacher model further makes it less practical to apply these methods to VR. The closest to our work are recent distillation-free one-step IR methods that either learn from discriminator prior [28] or generative prior [85, 90]. These methods save computational cost by training on an implicit teacher model, i.e., diffusion prior [49, 55] with LoRA [18]. Given the limited capability of Work was done during Jianyi Wangs internship at ByteDance Seed (iceclearwjy@gmail.com). Preprint. Under review. Figure 1: Speed and performance comparisons. Our SeedVR2 demonstrates impressive restoration capabilities, offering fine details and enhanced visual realism. While achieving comparable performance with SeedVR [67], our SeedVR2 is over 4 faster than existing diffusion-based video restoration approaches [14, 74, 79, 97] (We use 50 sampling steps for these baselines to maintain stable performance), even with at least four times the parameter count (Zoom-in for best view). existing video diffusion as prior, our work turns to explore one-step VR without depending on any teacher models or frozen prior, avoiding introducing the possible bias learned by these models. Achieving one-step VR, especially under high resolutions, is challenging, yet underexplored. In this paper, we introduce new method, SeedVR2, for one-step VR towards real-world scenarios. Our method follows Adversarial Post-Training (APT) [34] to adopt pre-trained diffusion transformer, i.e., SeedVR [67] as initialization, and continues to fully tune the whole network using the adversarial training objective against real data. Compared with previous one-step IR methods, SeedVR2 eliminates the substantial cost associated with pre-computing video samples from the diffusion teacher during distillation. Moreover, without the constraint from diffusion teacher or prior, SeedVR2 presents the potential to surpass the initial model, demonstrating comparable or even superior performance to multi-step VR diffusion models. While it is applicable to directly adopt APT [34] for VR, we empirically observe several key aspects that can be improved based on the nature of VR. First, given the low-quality input as condition, we observe more stable training process of VR compared with text-to-video generation [34], i.e., no obvious mode collapse is observed with only single stage of adversarial training. However, we notice performance drop when handling heavy degradations. We hereby adopt progressive distillation [53] before the adversarial training to maintain the restoration capability under one-step generation. Second, when applying window attention with predefined window size on highresolution VR, e.g., over 2K resolution, we observe visible boundary artifacts between window patches. We conjecture this is due to the improper settings of the window size and training resolutions, e.g., too large window sizes compared with relatively small training resolutions, making the model insufficiently trained on handling window shifting. Such predefined window manner may further limit the robustness of 3D Rotary Positional Embedding (RoPE) [63] inside each window when dealing with inputs with various resolutions. To tackle this problem, we propose an adaptive window attention mechanism to dynamically adjust the window size within certain range, significantly improving the robustness of the model when handling arbitrary-resolution inputs. Third, adversarial training with the exceptionally large generator and discriminator can be unstable even with APT, i.e., performance deterioration can be observed after long training, e.g., 20k iterations. We follow 2 Huang et al. [19] to enhance the training stability by introducing RpGAN [20] and an additional approximated R2 regularization loss. While L1 loss and LPIPS loss [92] are commonly used in VR training for better perception-distortion tradeoff [2], the necessity to calculate LPIPS in pixel space makes it unaffordable for high-resolution video training. Training latent LPIPS model [21] is also not applicable due to the lack of video-specific data. We instead propose feature matching loss to replace the LPIPS loss for efficient adversarial training. Specifically, we directly extract multiple features from different layers of the discriminator and measure the feature distance between the prediction and ground-truth. We empirically show that such feature matching loss is an effective alternative in our case. To our knowledge, SeedVR2 is among the first to demonstrate the feasibility of one-step video restoration or super-resolution using diffusion transformer. Benefiting from the adversarial training with specific designs for VR, we are able to train the largest-ever VR GAN (16B for the generator and discriminator in total), which can achieve high-quality restoration in single sampling step with high efficiency. The main contributions of our work are as follows: We present an effective adaptive window attention mechanism, enabling efficient high-resolution (e.g., 1080p) restoration in single forward step with faithful details, as shown in Figure 1. With the adversarial post-training framework, we explore effective design improvements specific to video restoration, focusing on the loss function and progressive distillation. Extensive experiments validate the effectiveness of our design, and demonstrate the superiority of our method over existing methods, both quantitatively and qualitatively."
        },
        {
            "title": "2 Related Work",
            "content": "Video Restoration. Traditional video restoration (VR) methods [4, 5, 8, 26, 31, 32, 69, 83] primarily concentrate on synthetic datasets, suffering from limited effectiveness in real-world scenarios. More recent efforts [6, 73, 95] have shifted focus towards real-world scenarios, but still struggle with generating realistic textures due to constrained generative capabilities. Inspired by the rapid progress in diffusion models [16, 47, 51, 57, 59, 78], several diffusion-based VR methods [14, 30, 74, 79, 97] have emerged, demonstrating remarkable performance. While fine-tuning on diffusion prior [51, 94] improves efficiency, these methods still inherit the inherent limitations of the diffusion prior, i.e., inefficient autoencoder and inflexible resolution scalability as discussed by Wang et al. [67]. The most recent work [67] proposes to fully train large diffusion transformer model with shifted window attention and casual video autoencoder, achieving impressive performance with relatively high efficiency. However, the need for tens of steps to sample video still leads to unfriendly latency in real-world applications. By introducing APT [34] into diffusion-based VR, our approach is capable of achieving one-step VR with high quality, which, to the best of our knowledge, is among the earliest explorations of one-step diffusion-based VR. Diffusion Acceleration. As discussed by Lin et al. [34], most of the existing approaches either distill the deterministic probability flow learned by diffusion teacher model using fewer steps (i.e., deterministic methods) or approximate the same distribution of diffusion teacher model (i.e., distributional methods). Specifically, deterministic methods include progressive distillation [53], consistency distillation [40, 41, 42, 60, 61], and rectified flow [37, 39, 77]. Though these methods can be easily trained with simple regression loss, blurry results can be commonly observed with very few steps, i.e., less than 8 steps [41, 42, 61]. In addition to directly predicting the outputs of the teacher model, distributional methods turn to adversarial training [7, 22, 44, 55, 76], score distillation [43, 82], both [3, 56, 81], and combining with deterministic methods [24, 33, 50] to resemble the distribution of teacher model. Most recent approaches [34, 76] instead directly fine-tune pre-trained diffusion model on real data with adversarial training, leading to superior performance with one-step sampling. While several acceleration approaches [34, 35, 65, 89] have been extended to video generation, the one-step acceleration for video diffusion restoration is still underexplored, inspiring us to make an early attempt in this direction. One-step Restoration. While conventional GAN-based real-world restoration approaches [6, 70, 91, 95] can achieve one-step restoration, their poor generation ability usually leads to suboptimal results. To improve the sampling efficiency of diffusion-based approaches [68, 79, 84, 97], ResShift [87, 88] shifts the initial sampling distribution from standard Gaussian distribution to the distribution of low3 quality images, achieving fast sampling of up to 4 steps. Recent advances further achieve one-step sampling via distillation [10, 15, 27, 48, 54, 71, 75, 99], adversarial training [28], or tuning on prior with additional trainable layers [72, 85, 90]. However, all these methods focus on image restoration and may not be suitable for VR due to the lack of temporal design and unsatisfactory generation quality. Compared with these methods, our method achieves one-step VR with substantially better quality, especially under high-resolution real-world scenarios."
        },
        {
            "title": "3 Methodology",
            "content": "The objective of SeedVR2 is to perform one-step Video Restoration (VR) by upscaling an input video into high-resolution output. SeedVR2 builds upon previous works [34, 67], with preliminary concepts introduced in Sec.3.1. The remainder of this section discusses VR-specific design improvements. Specifically, Sec.3.2 proposes an adaptive window attention mechanism to enhance test-time robustness for high-resolution videos. Sec. 3.3 explores one-step distillation within the adversarial post-training, and presents loss enhancements to improve training stability and model generalization."
        },
        {
            "title": "3.1 Preliminaries: Diffusion Adversarial Post-Training",
            "content": "Diffusion Adversarial Post-Training (APT) [34] is diffusion acceleration approach that converts multi-step diffusion model to one-step generator. There are mainly two training stages in APT, i.e., deterministic distillation and Adversarial Post-Training (APT). During the deterministic distillation, distilled model is first trained following discrete-time consistency distillation [60, 61] with mean squared error loss. The teacher model generates distillation supervision with constant classifier-free guidance [17] scale of 7.5 and predefined negative prompt. As for adversarial training, the discriminator is first initialized by the pre-trained diffusion network, and then additional cross-attention-only transformer blocks are introduced to generate logits for loss calculation. To stabilize the adversarial training while avoiding higher-order gradient computation, APT proposes an approximated R1 loss [52] to regularize the discriminator, and the final loss for the discriminator is non-saturating GAN loss [11] combined with the approximated R1 loss. Our method employs similar network architecture to APT, where both the generator and discriminator are diffusion transformers, as shown in Figure 2."
        },
        {
            "title": "3.2 Adaptive Window Attention",
            "content": "To improve the robustness of window attention for high-resolution inputs with arbitrary sizes, we propose an adaptive window attention mechanism that allows the window size to be dynamically adjusted to fit the input resolution, as shown in Figure 2. During training, given video feature Rdtdhdwdc, where dh dw = 45 80 (i.e., the feature resolution under 720p), the window size of our attention is calculated accordingly as follows: (cid:24) dh nh (cid:24) min(dt, 30) nt (cid:24) dw nw pw = ph = pt = (1) (cid:25) (cid:25) (cid:25) , , , where nt, nh and nw decide the number of windows along dimension dt, dh and dw, respectively. The ceiling function is represented as , and the term min(dt, 30) sets an upper bound to dt to avoid the gap of sequence length between training and inference. Note that although the resolutions of our training data are around 720p, the aspect ratio of width and height can vary lot, leading to various window sizes during training. Such design ensures better generalization ability toward inputs of different resolutions with diverse window sizes. To further improve test-time robustness on high-resolution inputs, we introduce resolution-consistent windowing strategy. Given test-time video feature ˆX ˆdt ˆdh ˆdw ˆdc , we first derive spatial proxy resolution dh dw that is consistent with the training resolution while maintaining the aspect ratio of the test input as follows: (cid:115) dh = dh dw ˆdh ˆdw (cid:115) , dw = dh dw ˆdw ˆdh , (2) 4 Figure 2: Model architecture and the partition of the adaptive attention window. We improve the Swin-MMDIT [67] with an adaptive window partition, i.e., the window size is ensured via 3 3 partition on the resized LQ input (Height Width = 960 960). The features for calculating the feature matching loss are extracted before the cross-attention layers used in APT [34]. and dh dw = where dh dw = 45 80 is the training resolution. This ensures dh dw. The final window size for test-time attention is then obtained by substituting (dt, dh, dw) in Eq. (1) with ( ˆdt, dh, dw). This adaptive partition strategy enhances consistency between training and testing configurations and substantially alleviates boundary artifacts in high-resolution predictions, as illustrated in Figure 4. = ˆdh ˆdw dh dw"
        },
        {
            "title": "3.3 Training Procedures",
            "content": "Large-scale adversarial training is challenging. Benefiting from the low-quality condition in VR, we do not observe mode collapse [11] when starting from adversarial training. However, undesired artifacts can be observed after training for thousands of iterations, indicating that the unstable training issue still exists. Our approach improves the training stability from the following two aspects, i.e., distillation and loss. Progressive Distillation. Directly adopting adversarial training to obtain one-step model from an initial multi-step one may undermine the restoration ability of the model due to the large gap between the initial model and the target model. We conduct progressive distillation [53] to alleviate such problem. To be specific, we start with the teacher model initialized from SeedVR [67] with 64 sampling steps and progressively distill the student model to one step with distillation stride of 2. Each distillation procedure takes about 10K iterations with simple mean squared error loss. We also progressively increase the temporal length of the training data from images to video clips with diverse number of frames during adversarial training, leading to robust VR performance toward videos with various lengths, including images. Benefiting from such training strategy, we further obtain 3B model distilled from the original 7B one, achieving comparable performance with only half of the model size. Loss Improvement. Inspired by R3GAN [19], we first replace the non-saturating GAN loss [11] used in APT by RpGAN loss [20] to avoid the potential mode dropping problem. We further introduce an approximate R2 regularization to penalize the gradient norm of on fake data while supporting modern deep learning software stacks: LaR2 = D( ˆx, c) D(N ( ˆx, σI), c)2 2, (3) where ˆx denotes the sample prediction converted from the velocity field output from the model, is the text condition, σ controls the variance of the perturbing Gaussian noise, and represents the identity matrix. We observe that the above loss improvements ensure more stable training without mode collapse after training for thousands of iterations. 5 Besides GAN loss, L1 loss and LPIPS loss are commonly used in VR for perception-distortion tradeoff [2]. However, to compute LPIPS loss, we have to first decode the prediction from the latent space to pixel space, leading to an unaffordable computational cost in our scenario. Instead of LPIPS loss, we propose to adopt feature matching loss via directly extracting features from the discriminator for efficient loss calculation. Specifically, we extract the features of predictions and ground-truths before the attention-only transformer blocks (i.e., the 16th, 26th, and 36th blocks of the transformer backbone) of the discriminator. Then, our feature matching loss LF can be written as: LF = 1 3 (cid:88) i=16,26,36 DF ( ˆx, c) DF (x, c)1, (4) where DF () denotes the feature from the i-th block of discriminator. By default, we set the loss weight as 1.0 for L1 loss, feature matching loss, and GAN loss when updating the generator. When updating the discriminator, we apply weight of 1.0 for GAN loss and the weights of the approximate R1 and R2 regularization are both 1000. Note that the discriminator is fixed when updating the generator. In this way, the discriminator in our feature matching loss acts in similar way to the VGG network [58] in LPIPS loss. Besides, the feature matching loss should also work with other GAN losses [1, 11, 13, 45] to further stabilize adversarial training for restoration tasks."
        },
        {
            "title": "4 Experiments",
            "content": "Implementation Details. We train SeedVR2 on 72 NVIDIA H100-80G GPUs with around 100 frames of 720p per batch with sequence parallel [25] and data parallel [29]. Each stage of training takes about one day. We first train 7B SeedVR model [67] from scratch following the new attention design in this paper. Then, we initialize the model parameters from 7B SeedVR model and follow the training strategies discussed in Sec. 3.3 for our SeedVR2 models. We mostly follow the training settings in APT [34] for adversarial training. We follow UAV [97] to synthesize about 10M image pairs and 5M video pairs for training. Experimental Settings. Following previous work [97], we evaluate synthetic benchmarks, including SPMCS [80], UDM10 [64], REDS30 [46], and YouHQ40 [97], applying the same degradation settings as in training. The test resolution is 720p with an upscaling factor of 4. Furthermore, we assess performance on the commonly used real-world dataset (VideoLQ [6]) and self-collected AIGC dataset (AIGC28), which comprises 28 AI-generated videos with diverse resolutions and scenes. We employ range of metrics to assess both frame-level and overall video quality. For synthetic pair datasets, we adopt full-reference metrics, including PSNR, SSIM, LPIPS [93], and DISTS [9]. For real-world and AI-generated content (AIGC) test data, where ground truth is unavailable, we rely exclusively on no-reference metrics, i.e., NIQE, CLIP-IQA, MUSIQ, and DOVER1. To ensure test efficiency, the maximum output resolution is constrained to 1080p, with duration unchanged."
        },
        {
            "title": "4.1 Comparison with Existing Methods",
            "content": "Quantitative Comparisons. We compare our approach with all state-of-the-art real-world video restoration approaches. For diffusion-based methods [14, 67, 74, 79, 97], we adopt 50 sampling steps with wavelet color fix post-processing [68], and keep other official settings unchanged. As shown in Table 1, our approach demonstrates superior performance in terms of perceptual metrics such as LPIPS and DISTS on synthetic benchmarks including SPMCS, UDM10 and YouHQ40. Note that RealViformer [95] and MGLD-VSR [79] involve REDS [46] in the train data, leading to high performance on the corresponding test set. As for real-world benchmarks, our method achieves comparable performance compared with other diffusion-based methods on VideoLQ and further obtains the highest NIQE, MUSIQ and DOVER scores on AIGC28, demonstrating our effectiveness. Qualitative Comparisons. As observed in several previous studies [2, 12, 84, 86], existing image and video quality assessment metrics do not perfectly align with human perception. For example, non-reference metrics such as MUSIQ [23] and CLIP-IQA [66] prefer sharp results but may ignore the quality of details. We notice that such phenomenon becomes more evident under high resolutions, e.g., 1080p. As shown in Figure 3, while our method does not show dominant metric performance on VideoLQ, the results generated by our approach are comparable to SeedVR [67] and outperform other baselines by large margin. 1We adopt the technical score ranging from 0 to 100 following the official code. 6 Table 1: Quantitative comparisons on VSR benchmarks from diverse sources, i.e., synthetic (SPMCS, UDM10, REDS30, YouHQ40), real (VideoLQ), and AIGC (AIGC28) data. The best and second performances are marked in red and orange , respectively. Datasets Metrics RealViformer [95] 24.185 0.663 0.378 0.186 26.70 0.796 0.285 0.166 23.34 0.615 0.328 0.154 23.26 0.606 0.362 0.193 MGLD-VSR [79] 23.41 0.633 0.369 0.166 26.11 0.772 0.273 0.144 22.74 0.578 0.271 0.097 22.62 0.576 0.356 0.166 PSNR SSIM LPIPS DISTS PSNR SSIM LPIPS DISTS PSNR SSIM LPIPS DISTS PSNR SSIM LPIPS DISTS NIQE MUSIQ CLIP-IQA DOVER NIQE MUSIQ CLIP-IQA DOVER 4.153 54.65 0.411 7.035 3.994 62.82 0.647 11. 3.864 53.49 0.333 8.109 4.049 60.98 0.570 14.27 UAV [97] 21.69 0.519 0.508 0.229 24.62 0.712 0.323 0.178 21.44 0.514 0.397 0.181 21.32 0.503 0.404 0.196 4.079 52.90 0.386 6.975 4.541 62.79 0.653 13.09 VEnhancer [14] 18.20 0.507 0.455 0.194 21.48 0.691 0.349 0.175 19.83 0.545 0.508 0.229 18.68 0.509 0.449 0.175 5.122 42.66 0.269 7.985 4.176 60.99 0.461 15.31 STAR [74] 22.58 0.609 0.420 0.229 24.66 0.747 0.359 0.195 22.04 0.593 0.487 0.229 22.15 0.575 0.451 0. 5.915 40.50 0.243 6.891 5.004 55.59 0.435 14.82 SeedVR-7B [67] 20.78 0.575 0.395 0.166 24.29 0.731 0.264 0.124 21.74 0.596 0.340 0.122 20.60 0.546 0.323 0.134 4.933 48.35 0.258 7.416 4.294 56.90 0.453 14.77 Ours 3B 22.97 0.646 0.306 0.131 25.61 0.784 0.218 0.106 21.90 0.598 0.350 0.135 22.10 0.595 0.284 0.122 4.687 51.09 0.295 8.176 3.801 62.99 0.561 15.77 Ours 7B 22.90 0.638 0.322 0.134 26.26 0.798 0.203 0.101 22.27 0.606 0.337 0.127 22.46 0.600 0.274 0. 4.948 45.76 0.257 7.236 4.015 59.97 0.497 15.55 SPMCS UDM10 REDS30 YouHQ40 VideoLQ AIGC28 Overall Quality Visual Quality Visual Fidelity Methods-{Steps} Table 2: Our one-step video restoration compared to existing methods. User Study. To further validation, we follow APT [34] to conduct GSB test, i.e., the preference score is calculated as GB G+S+B , where is the number of good samples preferred by the subjects, is the bad samples not preferred, and denotes the number of similar samples without preference. Thus, the score ranges from 100% to 100% and 0% indicates equal performance. We randomly select 25 samples from VideoLQ [6] and AIGC28, respectively, resulting in 50 LQ videos for test in total. We set our approach (7B) as the datum and compare it with existing methods [14, 67, 74, 79, 95, 97]. Given the LQ videos as reference, three experts are asked to evaluate the generated video quality from the following three criteria: visual fidelity, visual quality and overall quality. The visual fidelity measures the content similarity between the LQ reference and the generated result. The visual quality focuses on the realism of the generated results. The overall quality indicates the final preference after taking the above two factors into account. The subjects are given pair of videos generated by different methods each time and asked to make their preferences for each criteria. +2% -38% -32% -82% -86% -94% 0% -26% -26% 0% -12% -12% +4% -22% -24% +2% +10% +10% RealViformer-1 [95] VEnhancer-50 [14] UAV-50 [98] MGLD-VSR-50 [79] STAR-50 [74] SeedVR-7B-50 [67] 0% +16% +16% 0% 0% 0% Ours-3B-1 Ours-7B-1 As shown in Table 2, our approach is comparable to the multi-step SeedVR [67] and clearly excels other approaches with better visual quality, aligning with the visual results shown in Figure 3. Particularly, VEnhancer [14] focuses on generative restoration, thus showing poor fidelity in realworld VR scenarios. Restricted by the limited generative capability of the diffusion prior, existing approaches [74, 79, 97] tend to generate inferior results with high-resolution inputs, indicating the necessity to train large VR model without relying on the fixed prior. While our methods, i.e., ours-3B and ours-7B clearly outperform several baselines [14, 74, 79, 95, 97], the performance between these two models is different. Specifically, ours-3B receives more preference from the subjects than ours-7B, aligning with the results in Table 1. Recall that ours-3B is distilled from the 7B initial model. Such performance gain may indicate the effectiveness of the distillation stage. 7 Figure 3: Qualitative comparisons on both real-world [6] and AIGC videos. With single sampling step, our SeedVR2 achieves comparable performance to SeedVR [67], and further excels other baselines with superior restoration capabilities, i.e., successfully removing the degradations while maintaining the textures of the bird, text, building, and the dogs face (Zoom-in for best view). And we believe our 7B model could receive further improvement with the scaling of computational resources."
        },
        {
            "title": "4.2 Ablation Study",
            "content": "The Effect of Adaptive Window Attention. We first examine the effectiveness of the proposed adaptive window attention. We train the model with the predefined-size window attention and the proposed adaptive window attention, respectively. Both models share the same training settings for 20k iterations. As shown in Figure 4, when generating high-resolution (e.g., 1080p) results, window boundary inconsistency can be observed with predefined-size attention window. We conjecture that such drawbacks indicate the limited model capability of handling overlapping windows, which is associated with the improper setting of window size compared with training resolutions. Specifically, applying 64 64 window over the compressed latent with downsampling factor of 8 makes the model insufficiently trained on window-overlapping cases, which are rare on the 720p training pairs. Moreover, we find that the diffusion transformer with RoPE embeddings [63] shows more robust performance across range of resolutions after training on data with various sizes. Shifting to the window attention with mostly predefined window size [67] may weaken the generalization ability on other window sizes, i.e., the variable-sized windows near the boundary as shown in Figure 4. We show that the proposed adaptive window attention can significantly improve the model robustness by eliminating the above bad cases. 8 Figure 4: Comparisons of the window attention with predefined size (i.e., ours w/ predefined win. atten.) and our adaptive window attention (i.e., ours w/ adaptive win. atten.). Boundary artifacts can be observed on high-resolution restoration with the predefined-size window attention (Zoom-in for best view). Table 3: Ablation study on training losses and progressive training. For fairness, all baselines are trained on 72 NVIDIA H100-80G cards for 20k iterations. The comparison is conducted on YouHQ40 [97]."
        },
        {
            "title": "Metrics",
            "content": "PSNR SSIM LPIPS DISTS Non-satu. + R1 22.55 0.612 0.310 0.136 RpGAN + R1 + R2 22.56 0.603 0.278 0.109 RpGAN + R1 + R2 + L1 22.91 0.616 0.251 0.099 RpGAN + R1 + R2 + L1 + LF 22.91 0.620 0.244 0.092 Prog. Training 23.96 0.667 0.227 0. The Effect of Losses and Progressive Distillation. Training large-scale GAN can be challenging due to its unstable nature. We verify the significance of various losses used in our method. We train each baseline with different loss combinations for 20k iterations and keep other settings the same. As shown in Table 3, compared with the vanilla loss used in APT [34] (i.e., non-saturating GAN loss [11] + R1), the model trained with RpGAN [20], R1 and R2 loss demonstrates significant improvement on perceptual metrics such as LPIPS and DISTS. We further observe more stable training procedure without mode collapse, which exists under the settings of APT after long training. Besides, the adoption of L1 loss and the proposed feature matching loss both improve the metric performance, indicating the significance of these losses for restoration tasks. In practice, we notice that large loss weight of L1 loss and feature matching loss improves the fidelity, but may lead to mildly over-smooth results compared with assigning large weight to the GAN loss. Such an observation is consistent with the perception-distortion theory [2]. As result, we reduce the loss weight of L1 loss and the feature matching loss to 0.1 for the final model to enable better visual quality as reported in Sec. 4.1. Finally, as indicated in Table 3, applying progressive distillation is necessary to maintain strong restoration ability, which is expected since the distillation effectively minimizes the gap between the initial model and the one-step adversarial training."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we have presented SeedVR2, an early exploration on the one-step diffusion transformer model toward real-world restoration. SeedVR2, building on the adversarial post-training with pre-trained diffusion model as initialization, tackles one-step video restoration through tailored designs such as an adaptive window attention and several training enhancements, along with feature matching loss, which are crucial for stabilizing large-scale adversarial training and improving the restoration performance. Despite the large parameter size, SeedVR2 is over four times faster than existing multi-step diffusion VR methods, with comparable or even superior performance as shown by our experiments. In the future, we will improve the robustness of SeedVR2 towards complex degradations and further optimize the parameter size to facilitate real-time applications. We believe our proposed SeedVR2 could provide useful insights for future works. 9 Acknowledgement: This research is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG2-PhD-2022-01-033[T]). We sincerely thank Zhibei Ma for data processing, Jiashi Li for hardware maintenance. We also give thanks to Feng Cheng for performance feedback, Yi Li, Fangyuan Kong and Rui Zhang for training suggestions."
        },
        {
            "title": "References",
            "content": "[1] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In Proceedings of International Conference on Machine Learning (ICML), 2017. [2] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [3] Clement Chadebec, Onur Tasar, Eyal Benaroche, and Benjamin Aubin. Flash diffusion: Accelerating any conditional diffusion model for few steps image generation. arXiv preprint arXiv:2406.02347, 2024. [4] Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. BasicVSR: The search for essential components in video super-resolution and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [5] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. BasicVSR++: Improving video super-resolution with enhanced propagation and alignment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [6] Kelvin C.K. Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Investigating tradeoffs in real-world video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [7] Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou, and Yi-Zhe Song. Nitrofusion: High-fidelity singlestep diffusion through dynamic adversarial training. arXiv preprint arXiv:2412.02030, 2024. [8] Zhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo, and Tao Mei. Learning spatial adaptation and temporal coherence in diffusion models for video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [9] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture similarity. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020. [10] Linwei Dong, Qingnan Fan, Yihong Guo, Zhonghao Wang, Qi Zhang, Jinwei Chen, Yawei Luo, and Changqing Zou. Tsd-sr: One-step diffusion with target score distillation for real-world image superresolution. arXiv preprint arXiv:2411.18263, 2024. [11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2014. [12] Jinjin Gu, Haoming Cai, Chao Dong, Jimmy Ren, Radu Timofte, Yuan Gong, Shanshan Lao, Shuwei Shi, Jiahao Wang, Sidi Yang, et al. Ntire 2022 challenge on perceptual image quality assessment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [13] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2017. [14] Jingwen He, Tianfan Xue, Dongyang Liu, Xinqi Lin, Peng Gao, Dahua Lin, Yu Qiao, Wanli Ouyang, and Ziwei Liu. Venhancer: Generative space-time enhancement for video generation. arXiv preprint arXiv:2407.07667, 2024. [15] Xiao He, Huaao Tang, Zhijun Tu, Junchao Zhang, Kun Cheng, Hanting Chen, Yong Guo, Mingrui Zhu, Nannan Wang, Xinbo Gao, et al. One step diffusion-based super-resolution with time-aware distillation. arXiv preprint arXiv:2408.07476, 2024. [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), volume 33, 2020. [17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 10 [18] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. Proceedings of International Conference on Learning Representations (ICLR), 2022. [19] Nick Huang, Aaron Gokaslan, Volodymyr Kuleshov, and James Tompkin. The gan is dead; long live the gan! modern baseline gan. In ICML 2024 Workshop on Structured Probabilistic Inference {&} Generative Modeling, 2024. [20] Alexia Jolicoeur-Martineau. The relativistic discriminator: key element missing from standard gan. In Proceedings of International Conference on Learning Representations (ICLR), 2019. [21] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling Diffusion Models into Conditional GANs. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. [22] Minguk Kang, Richard Zhang, Connelly Barnes, Sylvain Paris, Suha Kwak, Jaesik Park, Eli Shechtman, Jun-Yan Zhu, and Taesung Park. Distilling diffusion models into conditional gans. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. [23] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [24] Jonas Kohler, Albert Pumarola, Edgar Schönfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, and Ali Thabet. Imagine flash: Accelerating emu diffusion models with backward distillation. arXiv preprint arXiv:2405.05224, 2024. [25] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 2023. [26] Dasong Li, Xiaoyu Shi, Yi Zhang, Ka Chun Cheung, Simon See, Xiaogang Wang, Hongwei Qin, and Hongsheng Li. simple baseline for video restoration with grouped spatial-temporal shift. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [27] Jianze Li, Jiezhang Cao, Yong Guo, Wenbo Li, and Yulun Zhang. One diffusion step to real-world super-resolution via flow trajectory distillation. arXiv preprint arXiv:2502.01993, 2025. [28] Jianze Li, Jiezhang Cao, Zichen Zou, Xiongfei Su, Xin Yuan, Yulun Zhang, Yong Guo, and Xiaokang Yang. Distillation-free one-step diffusion for real-world image super-resolution. arXiv preprint arXiv:2410.04224, 2024. [29] Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed. Proceedings of the VLDB Endowment, 2020. [30] Xiaohui Li, Yihao Liu, Shuo Cao, Ziyan Chen, Shaobin Zhuang, Xiangyu Chen, Yinan He, Yi Wang, and Yu Qiao. Diffvsr: Enhancing real-world video super-resolution with diffusion models for advanced visual quality and temporal consistency. arXiv preprint arXiv:2501.10110, 2025. [31] Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Rakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van Gool. VRT: video restoration transformer. IEEE Transactions on Image Processing (TIP), 2024. [32] Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu Timofte, and Luc Gool. Recurrent video restoration transformer with guided deformable attention. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2022. [33] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation. arXiv preprint arXiv:2402.13929, 2024. [34] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for one-step video generation. arXiv preprint arXiv:2501.08316, 2025. [35] Shanchuan Lin and Xiao Yang. Animatediff-lightning: Cross-model diffusion distillation. arXiv preprint arXiv:2403.12706, 2024. [36] Xinqi Lin, Jingwen He, Ziyan Chen, Zhaoyang Lyu, Ben Fei, Bo Dai, Wanli Ouyang, Yu Qiao, and Chao Dong. Diffbir: Towards blind image restoration with generative diffusion prior. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. 11 [37] Xingchao Liu, Chengyue Gong, et al. Flow straight and fast: Learning to generate and transfer data with rectified flow. In Proceedings of International Conference on Learning Representations (ICLR), 2023. [38] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In Proceedings of International Conference on Learning Representations (ICLR), 2023. [39] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for highquality diffusion-based text-to-image generation. In Proceedings of International Conference on Learning Representations (ICLR), 2023. [40] Cheng Lu and Yang Song. Simplifying, stabilizing and scaling continuous-time consistency models. In Proceedings of International Conference on Learning Representations (ICLR), 2025. [41] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. [42] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinário Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. [43] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: universal approach for transferring knowledge from pre-trained diffusion models. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2023. [44] Yihong Luo, Xiaolong Chen, Xinghua Qu, Tianyang Hu, and Jing Tang. You only sample once: Taming one-step text-to-image synthesis by self-cooperative diffusion gans. arXiv preprint arXiv:2403.12931, 2024. [45] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2017. [46] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu Lee. NTIRE 2019 challenge on video deblurring and super-resolution: Dataset and study. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (CVPR-W), 2019. [47] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In Proceedings of International Conference on Machine Learning (ICML), 2022. [48] Mehdi Noroozi, Isma Hadji, Brais Martinez, Adrian Bulat, and Georgios Tzimiropoulos. You only need one step: Fast super-resolution with stable diffusion via scale distillation. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. [49] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In Proceedings of International Conference on Learning Representations (ICLR), 2023. [50] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, XING WANG, and Xuefeng Xiao. In Proceedings of Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. Advances in Neural Information Processing Systems (NeurIPS), 2024. [51] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [52] Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of generative adversarial networks through regularization. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2017. [53] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In Proceedings of International Conference on Learning Representations (ICLR), 2022. [54] Shoaib Meraj Sami, Md Mahedi Hasan, Jeremy Dawson, and Nasser Nasrabadi. Hf-diff: High-frequency perceptual loss and distribution matching for one-step diffusion-based image super-resolution. arXiv preprint arXiv:2411.13548, 2024. 12 [55] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, 2024. [56] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. [57] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. [58] Simonyan and Zisserman. Very deep convolutional networks for large-scale image recognition. In Proceedings of International Conference on Learning Representations (ICLR), 2015. [59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Proceedings of International Conference on Machine Learning (ICML), 2015. [60] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. In Proceedings of International Conference on Learning Representations (ICLR), 2024. [61] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. Proceedings of International Conference on Machine Learning (ICML), 2023. [62] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In Proceedings of International Conference on Learning Representations (ICLR), 2021. [63] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. [64] Xin Tao, Hongyun Gao, Renjie Liao, Jue Wang, and Jiaya Jia. Detail-revealing deep video super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2017. [65] Fu-Yun Wang, Zhaoyang Huang, Weikang Bian, Xiaoyu Shi, Keqiang Sun, Guanglu Song, Yu Liu, and Hongsheng Li. Animatelcm: Computation-efficient personalized style video generation without personalized video data. In SIGGRAPH Asia 2024 Technical Communications, 2024. [66] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023. [67] Jianyi Wang, Zhijie Lin, Meng Wei, Yang Zhao, Ceyuan Yang, Chen Change Loy, and Lu Jiang. Seedvr: Seeding infinity in diffusion transformer towards generic video restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [68] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin C.K. Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. International Journal of Computer Vision (IJCV), 2024. [69] Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. Edvr: Video restoration with enhanced deformable convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (CVPR-W), 2019. [70] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind superresolution with pure synthetic data. In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCV-W), 2021. [71] Yufei Wang, Wenhan Yang, Xinyuan Chen, Yaohui Wang, Lanqing Guo, Lap-Pui Chau, Ziwei Liu, Yu Qiao, Alex Kot, and Bihan Wen. Sinsr: diffusion-based image super-resolution in single step. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [72] Rongyuan Wu, Lingchen Sun, Zhiyuan Ma, and Lei Zhang. One-step effective diffusion network for real-world image super-resolution. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2024. [73] Liangbin Xie, Xintao Wang, Shuwei Shi, Jinjin Gu, Chao Dong, and Ying Shan. Mitigating artifacts in real-world video super-resolution models. In Proceedings of the AAAI Conference on Artificial Intelligence, 2023. 13 [74] Rui Xie, Yinhong Liu, Penghao Zhou, Chen Zhao, Jun Zhou, Kai Zhang, Zhenyu Zhang, Jian Yang, Zhenheng Yang, and Ying Tai. Star: Spatial-temporal augmentation with text-to-video models for realworld video super-resolution. arXiv preprint arXiv:2501.02976, 2025. [75] Rui Xie, Chen Zhao, Kai Zhang, Zhenyu Zhang, Jun Zhou, Jian Yang, and Ying Tai. Addsr: Accelerating diffusion-based blind super-resolution with adversarial diffusion distillation. arXiv preprint arXiv:2404.01717, 2024. [76] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-toimage generation via diffusion gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [77] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Jiashi Feng, et al. Perflow: Piecewise rectified flow as universal plug-and-play accelerator. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2024. [78] S. Yang, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In Proceedings of International Conference on Learning Representations (ICLR), 2021. [79] Xi Yang, Chenhang He, Jianqi Ma, and Lei Zhang. Motion-guided latent diffusion for temporally consistent real-world video super-resolution. In Proceedings of the European Conference on Computer Vision (ECCV), 2024. [80] Peng Yi, Zhongyuan Wang, Kui Jiang, Junjun Jiang, and Jiayi Ma. Progressive fusion video superresolution network via exploiting non-local spatio-temporal correlations. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. [81] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2024. [82] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [83] Geunhyuk Youk, Jihyong Oh, and Munchurl Kim. FMA-Net: Flow-guided dynamic filtering and iterative feature refinement with multi-attention for joint video super-resolution and deblurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [84] Fanghua Yu, Jinjin Gu, Zheyuan Li, Jinfan Hu, Xiangtao Kong, Xintao Wang, Jingwen He, Yu Qiao, and Chao Dong. Scaling up to excellence: Practicing model scaling for photo-realistic image restoration in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [85] Zongsheng Yue, Kang Liao, and Chen Change Loy. Arbitrary-steps image super-resolution via diffusion inversion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [86] Zongsheng Yue and Chen Change Loy. Difface: Blind face restoration with diffused error contraction. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024. [87] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Resshift: Efficient diffusion model for image super-resolution by residual shifting. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2023. [88] Zongsheng Yue, Jianyi Wang, and Chen Change Loy. Efficient diffusion model for image restoration by residual shifting. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2024. [89] Yuanhao Zhai, Kevin Lin, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Chung-Ching Lin, David Doermann, Junsong Yuan, and Lijuan Wang. Motion consistency model: Accelerating video diffusion with disentangled motion-appearance distillation. Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2024. [90] Aiping Zhang, Zongsheng Yue, Renjing Pei, Wenqi Ren, and Xiaochun Cao. Degradation-guided one-step image super-resolution with diffusion priors. arXiv preprint arXiv:2409.17058, 2024. 14 [91] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind image super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [92] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [93] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [94] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint arXiv:2311.04145, 2023. [95] Yuehan Zhang and Angela Yao. RealViformer: Investigating attention for real-world video super-resolution. Proceedings of the European Conference on Computer Vision (ECCV), 2024. [96] Shangchen Zhou, Kelvin C.K. Chan, Chongyi Li, and Chen Change Loy. Towards robust blind face restoration with codebook lookup transformer. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), 2022. [97] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-A-Video: Temporal-consistent diffusion model for real-world video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [98] Yupeng Zhou, Zhen Li, Chun-Le Guo, Li Liu, Ming-Ming Cheng, and Qibin Hou. SRFormerV2: Taking closer look at permuted self-attention for image super-resolution. arXiv preprint arXiv:2303.09735, 2024. [99] Yuanzhi Zhu, Ruiqing Wang, Shilin Lu, Junnan Li, Hanshu Yan, and Kai Zhang. Oftsr: One-step flow for image super-resolution with tunable fidelity-realism trade-offs. arXiv preprint arXiv:2412.09465, 2024."
        },
        {
            "title": "A Limitations and Societal Impacts",
            "content": "To the best of our knowledge, SeedVR2 is the first one-step diffusion model towards video restoration (VR). Its effectiveness has also been verified with extensive experiments in the main paper. However, we further identify several limitations of current SeedVR2 in practice. We also discuss the potential societal impacts. Limitations. While our one-step method significantly saves time during sampling, the causal video VAE requires over 4x more time to encode and decode video compared to the naive VAE commonly used by existing methods [14, 74, 79, 97]. In addition, when dealing with 720p video with 100 frames, the casual video VAE takes over 95% of the total time. How to effectively improve the efficiency the video VAE without significantly sacrificing the performance should be valuable future work. Besides the VAE efficiency, we notice that our method is sometimes not robust to heavy degradations and very large motions, and shares some failure cases with existing methods, e.g., fail to fully remove the degradation or simply generate unpleasing details. Moreover, due to the strong generation ability, SeedVR2 tends to overly generate details on inputs with very light degradations, e.g., 720p AIGC videos, leading to oversharpened results occasionally. Thus, we have to tune the model with careful hyperparameter settings. Improving the robustness of the model towards complex real-world degradations and ensuring satisfactory lower bound of performance remains challenge for future work. Societal Impacts. Our approach is likely to push forward the progress of restoration applications toward real-world image and video restoration. Specifically, our approach may inspire future work to develop fast restoration methods with strong performance. The release of our model weights and validation sets could further contribute to the restoration community in developing their own large restoration models. Of particular concern is the misconduct of applying our method to enhance illegal content, such as NSFW. To mitigate this risk, we plan to include the corresponding detection tool in our public code to restrict the use of our method. 15 Figure 5: Qualitative comparisons on both real-world [6] and AIGC videos. It is noticeable that the GAN-based approach [95] generates blurry results due to limited generation ability. Previous multi-step diffusion-based VR [14, 74, 79, 97] either fail to restore the low-quality video with faithful details or tend to generate oversharpened details. Even with single sampling step, our approach clearly excels over these methods with large margin. (Zoom-in for best view)."
        },
        {
            "title": "B Parameter Size and Inference Speed",
            "content": "We provide detailed statistic regarding the number of parameters and inference time in Figure 1 of the main paper. We apply 50 sampling steps and keep other settings the same as the official repo for other baselines to maintain high-quality generation results of these methods. The results are listed in Table 4 for reference. Table 4: Comparison of model parameters and inference time on 720p video with 100 frames. Metrics VEnhancer UAV MGLD-VSR STAR SeedVR-7B Ours-7B Ours-7B Number of Parameters (M) (Generator only) Inference time s/video (100 768 1344) 2044. 691.0 1430.8 2041.0 8239.6 3391.5 8239. 2029.2 1284.5 1181.0 2326.0 1284.8 269. 299."
        },
        {
            "title": "C Additional Results",
            "content": "We show additional comparisons in Figure 5. generated by our SeedVR2, please refer https://iceclear.github.io/projects/seedvr2/ for details. For more image and video demos to the video demo in the project page:"
        }
    ],
    "affiliations": [
        "ByteDance",
        "Nanyang Technological University"
    ]
}