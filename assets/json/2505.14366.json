{
    "paper_title": "Towards Embodied Cognition in Robots via Spatially Grounded Synthetic Worlds",
    "authors": [
        "Joel Currie",
        "Gioele Migno",
        "Enrico Piacenti",
        "Maria Elena Giannaccini",
        "Patric Bach",
        "Davide De Tommaso",
        "Agnieszka Wykowska"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), a core capability for embodied cognition essential for Human-Robot Interaction (HRI). As a first step toward this goal, we introduce a synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, a natural language description, and a ground-truth 4X4 transformation matrix representing object pose. We focus on inferring Z-axis distance as a foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as a foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 6 6 3 4 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Towards Embodied Cognition in Robots via Spatially Grounded\nSynthetic Worlds",
            "content": "Joel Currie * joel.currie@iit.it Gioele Migno * gioele.migno@iit.it Enrico Piacenti * enrico.piacenti@iit.it Maria Elena Giannaccini elena.giannaccini@abdn.ac.uk Patric Bach patric.bach@abdn.ac.uk Davide De Tommaso * davide.detommaso@iit.it Agnieszka Wykowska * agnieszka.wykowska@iit.it Figure 1: Synthetic environment and dataset elements. minimal 3D scene is procedurally generated with non-uniform scaled cube and overhead camera. Each instance yields an RGB image, language prompt, and 44 transformation matrix (cid:17) representing object reference frame pose (cid:0)𝑅𝐹𝑂𝐵𝐽 (cid:1) with respect to the camera reference frame (𝑅𝐹𝐶𝐴𝑀 ), enabling (cid:16)𝐶𝐴𝑀𝑇𝑂𝐵𝐽 structured spatial representations for supervised learning in embodied AI. ABSTRACT We present conceptual framework for training Vision-Language Models (VLMs) to perform Visual Perspective Taking (VPT), core capability for embodied cognition essential for Human-Robot Interaction (HRI). As first step toward this goal, we introduce synthetic dataset, generated in NVIDIA Omniverse, that enables supervised learning for spatial reasoning tasks. Each instance includes an RGB image, natural language description, and ground-truth 44 transformation matrix representing object pose. We focus on inferring Z-axis distance as foundational skill, with future extensions targeting full 6 Degrees Of Freedom (DOFs) reasoning. The dataset is publicly available to support further research. This work serves as foundational step toward embodied AI systems capable of spatial understanding in interactive human-robot scenarios."
        },
        {
            "title": "KEYWORDS",
            "content": "Visual Perspective Taking, Visual Language Models, Spatial Reasoning, Synthetic Data, Embodied-AI, Human-Robot Interaction"
        },
        {
            "title": "1 INTRODUCTION\nEffective Human-Robot Interaction (HRI), like human-human inter-\naction requires a suite of socio-cognitive capacities [15, 19]. Among\nthese, Visual Perspective Taking (VPT) - the capacity to infer what\nanother sees from their point of view - plays a critical role [4, 5, 7].\nVPT is foundational to many downstream interaction capabilities,\nincluding joint action [9], social navigation [14] and mental/affec-\ntive/goal state inference [2, 10, 18]. Consider a toy example: you ask\na collaborator \"Can you pass me the object to the left?\". To achieve\nthe desired action the collaborator must not only identify the ref-\nerenced object but also reason about the spatial relationships from\ndistinct viewpoints, their own and yours. This requires the ability to\nrepresent how the world appears from another agent’s perspective,\nand to map effectively between diverging frames of reference.",
            "content": "*Social Cognition in Human-Robot Interaction Unit, Italian Institute of Technology, Genova, Italy University of Aberdeen, Aberdeen, United Kingdom Existing VPT solutions in robotics often rely on explicit geometric modelling [8, 13, 17] and hand-crafted perspective transformations - typically through rule-based [21] or spatial reasoning pipelines [7]. While these methods are effective in constrained environments, they often lack flexibility, generalisability and scalability necessary for real-world HRI. By contrast, Vision Language Models (VLMs) are method demonstrating impressive flexibility [3], performing well in tasks such as scene understanding [12]. However, despite these strengths, current VLMs struggle with precise spatial reasoning, especially when inferring precise object poses, relative orientations or viewpoint-specific relations [11, 12, 20]. Recent findings have suggested this deficit in spatial reasoning is not limitation in model architecture, but instead likely to be due to lack of training data that explicitly ties spatial relationships to grounded, visual scenes [3, 16, 20]. Simulated environments offer promising solution for generating scalable datasets trivially, as large datasets are often bottleneck in VLM training. More importantly, they also act as proxy for embodiment, allowing the reduction of error between inferred representations and reality by enabling supervised learning from generated synthetic data in which structured spatial relationships are easily extractable and inherently exactly precise. We contribute an early-stage framework for training VLMs to perform embodied cognitive tasks such as VPT, grounded in spatial reasoning. As first step toward this vision, we present proofof-concept dataset [6] composed of simple synthetic scenes with ground-truth transformation matrices. Our approach aims to support the future development of spatially aware, embodied robots capable of understanding what/how others see and where an object is relative to me/others."
        },
        {
            "title": "2 METHOD\nWe propose a conceptual pipeline for training VLMs to perform VPT\nand other embodied spatial reasoning tasks in HRI. The overarching\ngoal is to develop a system that, given a single RGB image and a\nnatural-language prompt describing an object, can infer its full 6\nDegrees of Freedom (DOFs) pose relative to both the frame of the\nrobot’s viewpoint and that of another agent in the environment. As\nan initial step, we present a proof-of-concept synthetic dataset [6],\nprocedurally generated using NVIDIA Omniverse Replicator [1],\ncontaining simple 3D scenes (see Figure 1). Each scene includes a\nsingle cube with randomised dimensions and material properties, a\nstatic object position, and a virtual camera with randomised height\n(Z-axis translation). Ground-truth transformation matrices provide\nprecise supervision for object-to-camera pose.",
            "content": "The current dataset [6] targets simplified version of the full task: inferring object translation along the Z-axis only, while holding rotation fixed on all axes, and X/Y translation constant. This design isolates key spatial relation and allows for controlled evaluation of VLMs ability to map visual and linguistic input to structured spatial representations. Our conceptual pipeline consists of three stages: (i) object pose estimation from image-text input, yielding trans- (cid:17), (ii) inference of relative viewpoint formation matrix (cid:16)𝐶𝐴𝑀𝑇𝑂𝐵𝐽 (cid:17), and transformation between an agent and the camera (cid:16)𝐶𝐴𝑀𝑇𝐴𝐺𝑇 (iii) perspective mapping via transformation composition, produc- (cid:17), the objects pose from the agents perspective. By ing (cid:16)𝐴𝐺𝑇𝑇𝑂𝐵𝐽 structuring spatial supervision in this way, we aim to advance the development of robots capable of performing embodied cognitive tasks such as perspective taking, spatial reasoning, and viewpointinvariant object understandingin real-world HRI. This work lays the foundation for agents that not only perceive and describe the world but also reason about it from multiple embodied perspectives. Future work will expand the dataset to include additional DOFs, more complex scenes, and integration with robotic platforms to support real-time, perspective-aware behaviour. DATASET AVAILABILITY We release our synthetic dataset of minimal 3D scenes, each containing an RGB image, natural language prompt, and ground-truth 44 pose matrix. The dataset [6] is available at: https://huggingface.co/datasets/jwgcurrie/synthetic-distance. ACKNOWLEDGEMENT This work has received support from the Project \"Future Artificial Intelligence Research (hereafter FAIR)\", PE000013 funded by the European Union - NextGenerationEU PNRR MUR - M4C2 - Investimento 1.3 - Avviso Creazione di \"Partenariati estesi alle università, ai centri di ricerca, alle aziende per il finanziamento di progetti di ricerca di base\" CUP J53C22003010006. REFERENCES [1] Naveed Ahmed, Imad Afyouni, Hamzah Dabool, and Zaher Al Aghbari. 2024. systemic survey of the Omniverse platform and its applications in data generation, simulation and metaverse. Frontiers in Computer Science 6 (2024), 1423129. [2] Daniel Batson, Shannon Early, and Giovanni Salvarani. 1997. Perspective taking: Imagining how another feels versus imaging how you would feel. Personality and social psychology bulletin 23, 7 (1997), 751758. [3] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. 2024. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1445514465. [4] Joel Currie, Katrina Louise McDonough, Agnieszka Wykowska, Maria Elena Giannaccini, and Patric Bach. 2024. Mind Meld or Mismatch: Comparison of Visual Perspective Taking Towards Humans and Robots in Face-to-Face Interactions. https://doi.org/10.31219/osf.io/zh7sg [5] Joel Currie, Katrina Louise Mcdonough, Agnieszka Wykowska, Maria Elena Giannaccini, and Patric Bach. 2024. More Than Meets the Eye? An Experimental Design to Test Robot Visual Perspective-Taking Facilitators Beyond Mere-Appearance. In Companion of the 2024 ACM/IEEE International Conference on Human-Robot Interaction (HRI 24). Association for Computing Machinery, New York, NY, USA, 359363. https://doi.org/10.1145/3610978.3640684 [6] Joel Currie, Gioele Migno, Enrico Piacenti, Maria Elena Giannaccini, Patric Bach, Davide De Tommaso, and Agnieszka Wykowska. 2025. synthetic-distance (Revision c86eff8). https://doi.org/10.57967/hf/5351 [7] Fethiye Irmak Doğan, Sarah Gillet, Elizabeth J. Carter, and Iolanda Leite. 2020. The impact of adding perspective-taking to spatial referencing during humanrobot interaction. Robotics and Autonomous Systems 134 (2020), 103654. https://doi.org/ 10.1016/j.robot.2020.103654 [8] Tobias Fischer and Yiannis Demiris. 2016. Markerless perspective taking for humanoid robots in unconstrained environments. In 2016 IEEE International Conference on Robotics and Automation (ICRA). 33093316. https://doi.org/10.1109/ ICRA.2016.7487504 [9] Martin Freundlieb, Ágnes Kovács, and Natalie Sebanz. 2016. When do humans spontaneously adopt anothers visuospatial perspective? Journal of experimental psychology: human perception and performance 42, 3 (2016), 401. [10] Tiziano Furlanetto, Cristina Becchio, Dana Samson, and Ian Apperly. 2016. Altercentric interference in level 1 visual perspective taking reflects the ascription of mental states, not submentalizing. Journal of Experimental Psychology: Human Perception and Performance 42, 2 (2016), 158. 2 [11] Qingying Gao, Yijiang Li, Haiyun Lyu, Haoran Sun, Dezhi Luo, and Hokin Deng. [n. d.]. Vision Language Models See What You Want but not What You See. https://doi.org/10.48550/arXiv.2410.00324 arXiv:2410.00324 [cs] [12] Gracjan Góral, Alicja Ziarko, Michal Nauman, and Maciej Wołczyk. [n. d.]. Seeing Through Their Eyes: Evaluating Visual Perspective Taking in Vision Language Models. https://doi.org/10.48550/arXiv.2409.12969 arXiv:2409.12969 [cs] [13] A. S. Johnson, B. Clarke, and C. Jones. 2015. Robotic Visual Perspective Taking via Geometric Reasoning. IEEE Transactions on Robotics 31, 6 (2015), 13521367. https://doi.org/10.1109/TRO.2015.2495016 ISSN: 1552-3098. [14] Maria Kozhevnikov, Michael A. Motes, Bjoern Rasch, and Olessia BlaPerspective-taking vs. mental rotation transformations Applied Cognihttps://doi.org/10.1002/acp.1192 jenkova. 2006. and how they predict spatial navigation performance. tive Psychology 20, 3 (2006), 397417. arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/acp. [15] Séverin Lemaignan, Mathieu Warnier, E. Akin Sisbot, Aurélie Clodic, and Rachid Alami. 2017. Artificial cognition for social humanrobot interaction: An implementation. Artificial Intelligence 247 (June 2017), 4569. https://doi.org/10.1016/j. artint.2016.07.002 [16] Dezhi Luo, Yijiang Li, and Hokin Deng. 2025. The Philosophical Foundations of Growing AI Like Child. arXiv preprint arXiv:2502.10742 (2025). [17] Luis Felipe Marin-Urias, Akin Sisbot, and Rachid Alami. 2008. Geometric tools for perspective taking for humanrobot interaction. In 2008 Seventh Mexican International Conference on Artificial Intelligence. IEEE, 243249. [18] Bradley Mattan, Pia Rotshtein, and Kimberly Quinn. 2016. Empathy and visual perspective-taking performance. Cognitive neuroscience 7, 1-4 (2016), 170181. [19] Manisha Natarajan, Esmaeil Seraj, Batuhan Altundas, Rohan Paleja, Sean Ye, Letian Chen, Reed Jensen, Kimberlee Chestnut Chang, and Matthew Gombolay. 2023. Human-robot teaming: grand challenges. Current Robotics Reports 4, 3 (2023), 81100. [20] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. 2024. RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics. arXiv preprint arXiv:2411.16537 (2024). [21] J.G. Trafton, N.L. Cassimatis, M.D. Bugajska, D.P. Brock, F.E. Mintz, and A.C. Schultz. 2005. Enabling effective human-robot interaction using perspective-taking in robots. IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 35, 4 (July 2005), 460470. https://doi.org/10.1109/TSMCA.2005.850592 Conference Name: IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans."
        }
    ],
    "affiliations": [
        "Italian Institute of Technology, Genova, Italy",
        "University of Aberdeen, Aberdeen, United Kingdom"
    ]
}