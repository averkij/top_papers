{
    "paper_title": "What matters for Representation Alignment: Global Information or Spatial Structure?",
    "authors": [
        "Jaskirat Singh",
        "Xingjian Leng",
        "Zongze Wu",
        "Liang Zheng",
        "Richard Zhang",
        "Eli Shechtman",
        "Saining Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Representation alignment (REPA) guides generative training by distilling representations from a strong, pretrained vision encoder to intermediate diffusion features. We investigate a fundamental question: what aspect of the target representation matters for generation, its \\textit{global} \\revision{semantic} information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as a target representation. To study this, we first perform a large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising; spatial structure, rather than global performance, drives the generation performance of a target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of \\emph{spatial} information. We replace the standard MLP projection layer in REPA with a simple convolution layer and introduce a spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in $<$4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across a diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). %, etc. Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. The code and project page are available at https://end2end-diffusion.github.io/irepa"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 4 9 7 0 1 . 2 1 5 2 : r Preprint. WHAT MATTERS FOR REPRESENTATION ALIGNMENT: GLOBAL INFORMATION OR SPATIAL STRUCTURE? Jaskirat Singh12 Xingjian Leng2 Zongze Wu1 Liang Zheng2 Richard Zhang1 Eli Shechtman1 1Adobe Research 2ANU 3New York University Saining Xie"
        },
        {
            "title": "ABSTRACT",
            "content": "Representation alignment (REPA) guides generative training by distilling representations from strong, pretrained vision encoder to intermediate diffusion features. We investigate fundamental question: what aspect of the target representation matters for generation, its global semantic information (e.g., measured by ImageNet-1K accuracy) or its spatial structure (i.e. pairwise cosine similarity between patch tokens)? Prevalent wisdom holds that stronger global semantic performance leads to better generation as target representation. To study this, we first perform large-scale empirical analysis across 27 different vision encoders and different model scales. The results are surprising spatial structure, rather than global performance, drives the generation performance of target representation. To further study this, we introduce two straightforward modifications, which specifically accentuate the transfer of spatial information. We replace the standard MLP projection layer in REPA with simple convolution layer and introduce spatial normalization layer for the external representation. Surprisingly, our simple method (implemented in <4 lines of code), termed iREPA, consistently improves convergence speed of REPA, across diverse set of vision encoders, model sizes, and training variants (such as REPA, REPA-E, Meanflow, JiT etc). Our work motivates revisiting the fundamental working mechanism of representational alignment and how it can be leveraged for improved training of generative models. Figure 1: What matters for representation alignment? Left: Correlation analysis across 27 diverse vision encoders. Surprisingly, contrary to the prevailing wisdom, we find that spatial structure, rather than global performance (measured by linear probing accuracy), drives the generation performance of target representation. Right: We further study this by introducing two simple modifications to accentuate the transfer of spatial features from target representation to diffusion model. Our simple approach consistently improves the convergence speed of REPA across diverse settings."
        },
        {
            "title": "INTRODUCTION",
            "content": "Representation alignment has emerged as powerful technique for accelerating the training of diffusion transformers (Ma et al., 2024; Peebles & Xie, 2023). By aligning internal diffusion Done during internship at Adobe Research Project: https://end2end-diffusion.github.io/irepa 1 Preprint. Figure 2: Motivating examples spatial structure matters. Metrics comparison showing inverse relationship between ImageNet accuracy and generation quality. Left: PE-G, despite having significantly higher validation accuracy (82.8% vs. 53.1%), shows worse generation quality compared to SpatialPE-B (Bolya et al., 2025). Right: Similarly, WebSSL-1B (Fan et al., 2025) also shows much better global performance (76.0% vs. 53.1%), but worse generation. Spatial Self-Similarity: We find that spatial structure instead provides better predictor of generation quality than global performance. See 3 for spatial structure metric. All results reported at 100K using SiT-XL/2 and REPA. representations with pretrained self-supervised visual encoders, recent methods have demonstrated significant improvements in the convergence speed and final performance (Yu et al., 2024; Leng et al., 2025a). However, despite these empirical successes, there remains limited understanding of the precise mechanisms through which self-supervised features enhance diffusion model training. fundamental question persists: is the improvement primarily driven by incorporating better global semantic information, as commonly measured through linear probing performance, or does it stem from better capturing spatial structure, characterized by the relationships between patch token representations? Understanding these mechanisms is crucial for advancing generative model training, as it directly impacts ones ability to select the optimal target representation and maximize its benefits. Currently, prevalent assumption is that encoder performance for representation alignment correlates strongly with ImageNet-1K validation accuracy, proxy measure of global semantic understanding (Oquab et al., 2024; Chen et al., 2021). That is, target representations with better ImageNet performance are hypothesized to lead to better generation (Yu et al., 2024). As such, increases in linear probing accuracy of diffusion features are frequently cited as evidence for the effectiveness of representation alignment, reinforcing the emphasis on global semantic information as the primary driver of improvement. The following quote from REPA (Yu et al., 2024) captures the current understanding: When diffusion transformer is aligned with pretrained encoder that offers more semantically meaningful representations (i.e., better linear probing results), the model not only captures better semantics but also exhibits enhanced generation performance, as reflected by improved validation accuracy with linear probing and lower FID scores. In this paper, we challenge this conventional wisdom by systematically investigating what truly drives representation alignment: global semantic information or spatial structure? Through large-scale empirical analysis across diverse vision encoders, including recent large vision foundation models such as WebSSL (Fan et al., 2025), DINOv3 (Simeoni et al., 2025), perceptual encoders (Bolya et al., 2025), C-RADIO (Heinrich et al., 2024), we uncover 3 surprising findings. Higher validation accuracy does not imply better representation for generation. Contrary to prevailing assumptions, vision encoders with higher global semantic performance, measured by ImageNet-1K linear probing accuracy, can often underperform for generation. For instance, consider PE-Spatial-B, spatially-tuned model derived from PE-Core-G (Bolya et al., 2025). We find that while PE-Spatial-B shows much worse validation accuracy1 on patch tokens than PE-Core-G (53.1% vs. 82.8%), it leads to better generation with REPA (Figure 2). Similarly, WebSSL-1B (Fan et al., 2025) shows much better global performance (76.0% vs. 53.1%), but worse generation. In Section 2, we find that this pattern holds across various target representations, suggesting fundamental principle in how representation alignment benefits diffusion training. Spatial structure not global information determines generation performance. To quantify this insight, we consider several straightforward metrics to measure the spatial self-similarity structure 1Similar to REPA (Yu et al., 2024), we use linear probing accuracy on patch tokens to measure global semantic performance of external representation as only patch tokens are used for representation alignment. 2 Preprint. (Shechtman & Irani, 2007) between patch tokens (3). We then perform large-scale analysis computing the correlation between generation FID with REPA, linear probing accuracy, and spatial structure across 27 vision encoders and 3 model sizes (SiT-B, SiT-L, SiT-XL). Surprisingly, all spatial structure metrics exhibit remarkably better correlation (Pearson > 0.852) with generation FID scores, far exceeding the predictive power of ImageNet-1K validation accuracy (r = 0.26). These findings are also supported by additional experiments showing that external representations with very limited global information can be used to get significant gains with REPA. For instance, SAM2 leads to better generation with REPA than other encoders with 60% higher ImageNet accuracy  (Fig. 3)  . Similarly, while not the best, we find that classical spatial features such as SIFT (Lowe, 1999) and HOG (Dalal & Triggs, 2005) can also be used to achieve decent gains with representation alignment (3). Accentuating spatial features improves convergence performance. To further study this, we next introduce two straightforward modifications (<4 lines of code), which specifically accentuate the transfer of spatial information from target representation to diffusion model: (1) We first introduce spatial regularization layer which boosts the spatial contrast of the target representations. (2) Next, we show that the standard MLP-based projection layer (used to map diffusion features to target representation dimensions) causes loss of spatial information (Figure 6a). To avoid this, we replace it with simple convolution layer. The results are surprising: our simple method, termed iREPA, consistently improves convergence speed of REPA across diverse variation in encoders, model sizes, and training recipes e.g., REPA-E (Leng et al., 2025a) and Meanflow (Geng et al., 2025) w/ REPA. We highlight the main contributions of this paper below: We perform large-scale empirical analysis showing that spatial structure and not global semantic information drives the effectiveness of representation alignment. We introduce straightforward and fast-to-compute Spatial Structure Metric (SSM), which shows significantly higher correlation with downstream FID performance than linear probing scores. We propose simple modifications to better accentuate the transfer of spatial information from the target representation to diffusion features. Our simple method shows consistent improvements in the convergence speed over REPA across variations in target representation, model architectures as well as training recipe variants (REPA, REPA-E, Meanflow w/ REPA, JiT w/ REPA etc)."
        },
        {
            "title": "2 MOTIVATION: GLOBAL INFORMATION MATTERS LESS",
            "content": "We first provide several motivating examples showing that target representation with higher global performance (ImageNet-1K accuracy) does not imply better generation performance with REPA. We later show in 3, that these previously-unexplained observations can instead be better explained by measuring the spatial structure of the target representation. Recent vision encoders. We examine different recent vision encoders, including Perceptual Encoders (Bolya et al., 2025), WebSSL (Fan et al., 2025), and DINOv3 (Simeoni et al., 2025). Consider PE-Spatial-B (80M), small spatially tuned model derived from PE-Core-G (1.88B) (Bolya et al., 2025). As seen in Figure 2, we find that while PE-Core-G achieves much higher ImageNet-1K accuracy (82.8% vs 53.1%), it performs worse when used as the target representation for REPA (FID 32.3 vs 21.0). Similarly, WebSSL-1B (1.2B) achieves much higher ImageNet-1K accuracy (76.0% vs 53.1%) but performs worse when used as target representation for REPA (FID 26.1 vs 21.0). SAM2 outperforms vision encoders with much higher ImageNet-1K accuracy. To further understand how little global information impacts generation performance, we analyze SAM2-S vision encoder (46M) (Ravi et al., 2024) small model with very little global information and validation accuracy of only 24.1% (Fig. 3a). Surprisingly, when used for REPA, SAM2-S achieves better FID than other vision encoders with significantly higher ImageNet-1K accuracy e.g., PE-Core-G (82.8%). Larger models within same encoder family can have similar or worse generation performance. common perception is that larger models within the same encoder family have better representations (measured by ImageNet-1K accuracy). However, for representation alignment, larger model variants often lead to similar (DINOv2) or even worse (PE, Cradio) generation performance (Fig. 3b). Notably (Yu et al., 2024) also make similar observation for DINOv2 and explain it as we hypothesize is due to all DINOv2 models being distilled from the DINOv2-g model and thus sharing similar representations. We later show that this trend can be better explained using spatial structure (3). 3 Preprint. (a) SAM2 vs better encoders. (c) Adding global info. hurts FID. (b) Larger models but worse gFID. Figure 3: Higher global information does not imply better REPA performance. Top row: Several trends showing global performance does not correlate well with generation FID when using REPA. (a) SAM2-S with only validation accuracy of 24.7% results in better generation performance with REPA compared with other models with 60% higher validation accuracy. (b) Larger encoders within same family can have better validation but worse generation performance. (c) Adding global information to patch tokens via CLS token improves global performance but hurts generation. Bottom row: We show that spatial structure rather then global performance provides better indicator for generation. Please see 3 for large-scale detailed analysis across different spatial structure metrics. All results are reported w/o classifier free guidance, SiTXL/2 w/ REPA and 250 NFE (Yu et al., 2024) for inference. Adding more global information can hurt generation. To test whether additional global information benefits representation alignment, we conduct controlled experiments that inject global semantics, using the CLS token, into local patch tokens (with DINOv2 as encoder). The CLS token mixing operation is pnew = pi + α c, where pi denotes the ith patch token, the CLS token, and α [0, 0.5] controls the mixing strength. As shown in Figure 3c, as α increases from 0 to 0.5, linear probing accuracy improves monotonically from 70.7% to 78.5%. However, generation quality deteriorates significantly, with FID scores worsening from 19.2 at α = 0 to 25.4 at α = 0.5. The above observations highlight that global performance of representation is not good indicator of its REPA performance. We next show (3) spatial structure instead provides better signal. Finding 1. Better global semantic information (e.g., ImageNet-1K accuracy) does not imply better representation for generation."
        },
        {
            "title": "3 SPATIAL STRUCTURE MATTERS MORE",
            "content": "We hypothesize that spatial structure, rather than global information, drives the generation performance with target representation. To quantify this, we first consider several straightforward metrics to measure the spatial self-similarity structure (Shechtman & Irani, 2007) of target representations. We then show that all spatial structure metrics not only correlate much higher with gFID than ImageNet-1K accuracy, but can be also used to explain previously unexplained observations in 2. Measuring spatial self-similarity structure. Given an image and vision encoder E, we define: Self-similarity. Let = E(I) RT be the extract patch representations, with = patches. Let kernel KX (, ) measure appearance self-similarity (Shechtman & Irani, 2007) between patch tokens. Here, we use the cosine kernel KX (t, t) = xt,xt . xt2xt 2 Spatial distance. Let NT 2 be the spatial location of each of the tokens in coordinate space and d(, ) be the Manhattan distance between pairs of tokens. We then measure spatial self-similarity metric as how self-similarity KX varies with lattice distance between patch tokens. Intuitively, larger values indicate stronger spatial organization (closer patches 4 Preprint. Figure 4: Spatial structure shows higher correlation with generation quality than linear probing. Correlation analysis across 27 diverse vision encoders, SiT-XL/2 and REPA. Linear probing shows weak correlation with FID (Pearson = 0.260), while all spatial structure metrics: LDS (r = 0.852), SRSS (r = 0.885), CDS (r = 0.847), and RMSC (r = 0.888), demonstrate much stronger correlation with generation performance. See Fig. 10 for detailed plots with encoder labels. more similar to each other than patches further away). By default, we use simple correlogram contrast (local vs. distant) metric (Huang et al., 1997): LDS(X , P) = Ed(t,t)(0,rnear)KX (t, t) Ed(t,t)rfarKX (t, t). The final spatial self-similarity score (LDS) is computed as the expectation over patch representation . We use rnear = rfar = H/2 here, though we found correlation to be robust to their exact choices. Please see Appendix for exact details and alternative spatial metrics (CDS, SRSS, RMSC) which also perform effectively (see Fig. 4). Spatial structure correlates much higher with generation performance than linear probing. We next perform large-scale correlation analysis across 27 diverse vision encoders. As shown in Figure 4, we find that while typically used linear probing shows very weak correlation (Pearson = 0.26), all SSM metrics show much higher correlation with generation performance (Pearson > 0.85). Generalization across model scales. We verify the correlation across different model scales (SiT-B, SiT-L, SiT-XL) in Figure 5. Linear probing shows weak correlation across model scales (r < 0.306), while spatial structure shows much higher correlation with generation performance (r > 0.826). Finding 2. Spatial structure correlates much higher with generation performance than linear probing. We next use spatial metrics to explain previously unexplained trends. Spatial structure can explain previously unexplained trends. As discussed in 2, global performance (ImageNet-1K accuracy) does not serve as predictive measure of effectiveness for representation alignment. We find that instead the above spatial metrics serve as better predictors. (1) PE-Spatial-B vs PE-Core-G: Figure 2 shows that PE-Core-G achieves much higher ImageNet-1K accuracy (82.8% vs 53.1%), but performs worse when used as target representation for REPA (FID 32.3 vs 22.0). Looking at spatial structure metric makes things clearer. As seen in Figure 2, despite lower global performance, PE-Spatial-B shows much better spatial structure than PE-Core-G; leading to better generation performance as observed. (2) SAM2 outperforms better vision encoders: 2 shows that while SAM2 achieves much lower ImageNet-1K accuracy (24.1%), it leads to better generation than encoders with 60% higher accuracy. As in Fig. 3a; these gains can be directly explained through SAM2s better spatial structure. (3) Larger models in same encoder family underperform: As shown in Fig. 3b, while larger models in same encoder family show better ImageNet-1K accuracy, they can have worse spatial structure, leading to worse generation performance with REPA. This also aligns with observations from Yu et al. (2024), where they find that larger models can have similar or worse generation performance. 4) Adding global information to patch tokens via CLS token hurts generation: In Figure 3c, we observe that increasing global information by mixing the CLS token with patch tokens improves global performance. However, mixing CLS token reduces spatial contrast among patch tokens. This leads to patch tokens having high similarity with otherwise unrelated tokens (e.g., from foreground object to background). The reduced spatial structure thus leads to worse generation performance. If spatial structure matters more, can we use SIFT or HOG features for REPA? Surprisingly, yes. We find that while certainly not the best, classical spatial features like SIFT (Lowe, 1999), HOG (Dalal & Triggs, 2005) and intermediate VGG features (Simonyan & Zisserman, 2014) Preprint. Figure 5: Correlation analysis across model scales. Across different model scales, we find that spatial structure (right) consistently shows higher correlation with gFID than linear probing (left). all lead to performance gains with REPA. This provides further evidence that representation alignment can benefit from spatial features alone without need for additional global information. Can we use spatial metrics to explain gains with REPA? Yes. As shown, the introduced spatial metrics can be used to explain both gains with REPA as well as our improved training recipe (iREPA) which we introduce next in 4. iREPA: IMPROVING REPRESENTATION ALIGNMENT BY"
        },
        {
            "title": "4\nACCENTUATING WHAT MATTERS",
            "content": "To further investigate the role of spatial structure in representation alignment, we introduce two straightforward modifications to the original REPA training recipe, which enhance the transfer of spatial features from the teacher (vision encoder) to the student (diffusion transformer) model. Convolutional projection layer instead of MLP. The standard REPA approach uses 3-layer MLP projection to map diffusion feature dimensions to that of the external representation. However, as shown in Figure 6a, we observe that this projection is lossy and diminishes the spatial contrast between patch tokens. We therefore replace the MLP with lightweight convolutional layer (kernel size 3, padding 1), that operates directly on the spatial grid. The convolutional structure naturally preserves local spatial relationships through its inductive bias. Spatial normalization layer. Similar to Simeoni et al. (2025), we find that patch tokens of pretrained vision encoders consist of significant global component. This is evidenced by the high linear probing scores for the mean of patch tokens (Figure 14). Also, we see in Figure 6b that while mixing of this global information (mean of patch tokens) with the patch tokens helps improve global performance, it reduces the spatial contrast between individual patch tokens. This leads tokens (e.g., foreground object) showing high similarity with otherwise unrelated tokens (e.g., background). Given results from 3, we hypothesize that we can sacrifice this global information (mean of patch tokens) to improve the spatial contrast between the patch tokens. The improved contrast should provide better spatial signal (pairwise similarity between patch tokens) leading to better REPA performance. To this end, we add simple spatial normalization layer (Ulyanov et al., 2016) to the patch tokens of the target representation: = γE[x] (cid:112)Var[x] + ϵ , where RBT represents the patch tokens, the expectation and variance are computed across the spatial dimension, and ϵ = 106 for numerical stability. Algorithm 1 Summary of key iREPA modifications to standard REPA training # 1. Conv projection instead of MLP proj_layer = nn.Conv2d(D_in, D_out, kernel_size=3, padding=1) # 2. Spatial normalization on encoder features [B, T, D] = - gamma * x.mean(dim=1, keepdim=True) = / (x.std(dim=1, keepdim=True) + 1e6)"
        },
        {
            "title": "4.1 EXPERIMENTS",
            "content": "In this section, we validate the performance of the improved training recipe through extensive experiments on Imagenet 256 256. In particular, we investigate the following research questions: 6 Preprint. (a) Simpler projection layer for REPA. Standard MLP projection layer in REPA (middle) loses spatial information while transferring features from target representation (left) to diffusion features. Instead using simpler convolution layer leads to better spatial information transfer (right). (b) Spatial normalization layer. Patch tokens of pretrained vision encoders have global component which limits spatial contrast. This causes the tokens in one semantic region (e.g., tomato) to show quite decent cosine similarity with unrelated tokens (e.g., background or cucumber). We hypothesize that we can sacrifice this global information to improve the spatial contrast between patch tokens - leading to better generation performance. (c) Overall impact of improved training recipe (iREPA) on spatial structure of diffusion features. Figure 6: Motivating two straightforward modifications (iREPA) to enhance spatial feature transfer from target representation to diffusion features. All results reported with SiTL/2 with REPA at 100K. Can iREPA consistently improve the convergence speed of diffusion transformers over REPA across diverse external representations? (Figure 7, 8, 12, 13, 15) Is iREPA scalable in terms of model size and generalize across variations in training settings? (Table 1 [a,b,c], 2, 4, 5, 6, and, Figure 12, 16) Does iREPA generalize across more recent representation alignment methods such as REPA-E (Leng et al., 2025a), MeanFlow w/ REPA (Geng et al., 2025)? and pixel-space diffusion models such as JiT w/ REPA (Li & He, 2025b)? (Table 3 [a,b]) Convergence Speed. We evaluate the convergence behavior of iREPA across diverse vision encoders (DINOv3-B, WebSSL-1B, PE-Core-G, CLIP-L, MoCov3, PE-Lang-G), and model sizes (SiT-XL/2 and SiT-B/2). Results are shown in Fig. 7. We find that iREPA consistently helps achieve faster convergence over baseline REPA across variations in both target representation and model sizes. Target representation. We analyze the generalization of iREPA across different vision encoders in Fig. 8 and Table 4. We observe that iREPA consistently improves the generation quality across all vision encoders. Additional comparisons across all 27 encoders are provided in Appendix C, E. Enc. Size IS FID sFID Pr. Rc. Model Size IS FID sFID Pr. Rc. Aln. Depth IS FID sFID Pr. Rc. PE-B (90M) 59.6 22.5 73.3 17.5 +iREPA 6.23 0.63 0.60 6.04 0.66 0.62 PE-L (320M) 48.6 28.7 76.3 17.6 +iREPA 6.53 0.59 0.60 6.36 0.65 0.61 PE-G (1.88B) 42.7 32.3 70.8 19.5 +iREPA 6.70 0.57 0.59 6.35 0.64 0.61 SiT-B +iREPA SiT-L +iREPA SiT-XL +iREPA 27.5 49.50 7.00 0.46 0.59 34.1 43.37 7.87 0.50 0.60 55.7 24.10 6.25 0.62 0.60 66.9 20.28 6.14 0.63 0.62 70.3 19.06 5.83 0.65 0.61 77.9 16.96 6.26 0.66 0.61 Layer 4 +iREPA Layer 6 +iREPA Layer 8 +iREPA 28.1 49.0 41.7 36.0 6.83 0.46 0.60 6.61 0.52 0.62 26.9 50.6 42.4 35.3 7.00 0.46 0.59 6.90 0.53 0. 24.7 54.8 38.3 38.9 7.35 0.44 0.58 7.19 0.51 0.62 (a) Vision Encoder size (b) Model size Table 1: Variation in training settings. We find that iREPA leads to consistent gains over baseline REPA across diverse training settings. Unless otherwise specified all results are reported using Dinov2 as encoder, SiTXL/2, 100K steps and vanilla-REPA as baseline. (c) Encoder depth (SiT-B/2) Encoder size. We analyze the generalization of iREPA across different encoder sizes. Table 1a shows results analyzing generalization of iREPA across different encoder sizes; PE-B (90M), PE-L (320M), PE-G (1.88B). We see that use of iREPA consistently helps improve the performance across 7 Preprint. Figure 7: Accentuating spatial features helps consistently improve convergence speed. Figure 8: Variation in target representation. Across all 27 vision encoders, we find that accentuating transfer of spatial features from target representation to diffusion features (iREPA) leads to consistent improvements in convergence speed. See Appendix for more results across diverse settings. all encoder sizes. Interestingly, the percentage improvement also increases with increasing encoder size (22.2% for PE-B, 38.8% for PE-L, 39.6% for PE-G). Scalability. We analyze the scalability of iREPA across different model scales in Table 1b. We observe that the spatial improvements not only consistently improve performance, but larger percentage gains are seen with larger models; showing that spatial improvements are scalable with model size. Encoder depth. Table 1c analyzes generalization of iREPA across different alignment depths. All results are with SiT-B/2 at 100K iterations using DINOv3-B as target representation. We observe consistent improvements over baseline REPA across different alignment depths. Abalation on different components. We also study the role of different components in iREPA in Table 2. We observe that both spatial normalization and convolution projection layer significantly improve the generation quality over baseline REPA; with the best results achieved by using both. Training recipe. Lastly, we analyze the generalization of iREPA across different training recipes such as REPA-E (Leng et al., 2025a) and MeanFlow w/ REPA (Geng et al., 2025). Table 3 shows that spatial improvements with iREPA lead to convergence gains across different training recipes. Classifier-free guidance. We evaluate the generation quality of iREPA with CFG in Table 4. presents results across different vision encoders at 400K training iterations. We find that across different encoders, iREPA leads to faster convergence both with and without classifier-free guidance. Pixel-space diffusion (JiT). We also evaluate iREPA on pixel-space diffusion models such as JiTB (Li & He, 2025b). Figure 9 shows results across with both REPA and iREPA using JiT (Li & He, 2025b). We observe that accentuating transfer of spatial information consistently achieves faster convergence with REPA across various vision encoders (e.g., DiNOv2, DiNOv3, PE etc.). Preprint. Figure 9: Convergence comparison with pixel-space diffusion (JiT). FID convergence curves comparing REPA vs iREPA with pixel space JiT (Li & He, 2025b) across different vision encoders. Accentuating transfer of spatial structure helps consistently improve the convergence speed of across different vision encoders for pixel-space diffusion models. All results are reported with JiTB/16 (Li & He, 2025b), 256 batch size and without classifier-free guidance. Refer Table 10 for further results. Method FID IS sFID FID IS sFID FID IS sFID FID IS sFID DINOv2-B DINOv3-B WebSSL-1B PE-Core-G Baseline REPA iREPA (w/o spatial norm) iREPA (w/o conv proj) iREPA (full) 19.06 18.52 17.66 16.96 70.3 73.3 72.8 77.9 5.83 6.11 6.03 6. 21.47 17.76 18.28 16.26 63.4 74.7 70.8 78.8 6.19 5.81 6.18 6.14 26.10 21.17 18.44 16.66 53.0 64.6 71.0 77.5 6.90 6.27 6.22 6. 32.35 24.97 21.72 18.19 42.7 57.4 61.5 75.0 6.70 6.21 6.26 6.03 Table 2: Ablation on different components. We see that across different encoders - both spatial normalization layer and convolution projection layer (4) lead to significant gains in convergence speed; with best results obtained using both. All results reported at 100K steps and SiT-XL/2. Finding 3. Accentuating transfer of spatial structure from target representation to diffusion features helps boost convergence speed with representation alignment (REPA)."
        },
        {
            "title": "5 RELATED WORK",
            "content": "We discuss the most relevant related work here and provide detailed discussion in Appendix H. Representation alignment for generation. Many recent works explore use of external representations for improving diffusion model training (Pernias et al., 2023; Fuest et al., 2024). Notably, recent works (Yu et al., 2024; Yao & Wang, 2025; Leng et al., 2025a;b; Kouzelis et al., 2025) show that significant performance gains can be achieved by aligning internal diffusion features with clean image features from pretrained vision encoder. (Zhang et al., 2025; Wu et al., 2025) extend this idea to video generation and 3D generation respectively. (Ma et al., 2025a) shows that representation alignment can be used to improve training of unified models. Despite these emperical successes, there remains limited understanding of the precise mechanisms through which self-supervised features enhance diffusion model training. In this paper, we try to understand what aspect of the target representation matters for generation, and use it to propose an improved training recipe. Spatial vs global information tradeoff in pretrained vision encoders. Recent works explore the tradeoff between global and spatial information in pretrained vision encoders (Bolya et al., 2025; Simeoni et al., 2025). (Simeoni et al., 2025) show that continued training of self-supervised vision representations can lead to increased similarity between global CLS token and patch tokens leading to worse performance on dense spatial tasks. (Bolya et al., 2025; Heinrich et al., 2024) specifically train spatial-tuned models for dense spatial tasks. In this paper, we show that for generation, spatial structure of vision encoder matters more then its global information. We hope this motivates future research on better selecting and training external representations for generation."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we study what truly drives the effectiveness of representation alignment, global information or the spatial structure of the target representation? Through large-scale empirical analysis 9 Preprint. Encoder IS FID sFID Prec. Rec. w/o CFG w/ CFG WebSSL-1B 52.8 26.5 +iREPA-E 87.0 13.2 PE-G 50.9 25.9 +iREPA-E 80.0 16.4 DINOv3-B 82.2 14.4 +iREPA-E 93.6 11.7 DINOv2-B 87.5 12.9 +iREPA-E 91.3 12.1 5.20 5. 5.68 5.40 4.68 4.57 5.40 4.86 0.620 0.699 0.612 0.667 0.694 0. 0.708 0.712 0.585 0.598 0.576 0.616 0.596 0.613 0.586 0.602 Encoder 4 NFE 1 NFE 4 NFE 1 NFE IS FID IS FID IS FID IS FID WebSSL-1B 27.22 51.40 24.14 58.67 +iREPA 16.59 69.06 23.74 31.48 45.67 27.33 55.70 100.74 13.89 78.67 20.69 87.85 DINOv3-B 28.36 49.58 25.47 57.01 +iREPA 15.56 72.41 22.55 33.63 44.52 29.67 53.75 124.54 11.05 98.93 17. 93.30 DINOv2-B 28.77 48.87 25.43 56.63 +iREPA 15.28 71.95 22.17 33.63 44.52 29.67 53.75 111.44 12.59 90.25 18.62 94.40 (a) REPA-E (SiT-XL/2) (b) MeanFlow w/ REPA (SiT-B/2) Table 3: Variation in training recipes. We apply our spatial improvements on top of REPA-E (a) and Meanflow with REPA (b); achieving consistent gains. For Meanflow, we report results at 1 and 4 NFEs. CFG value of 2.0 is used for Meanflow. All results are reported at 100K training iterations. w/o CFG w/ CFG Vision Encoder Steps IS FID sFID Prec. Rec. IS FID sFID Prec. Rec. DINOv2-B +iREPA DINOv2-B +iREPA DINOv3-B +iREPA DINOv3-B +iREPA WebSSL-1B +iREPA WebSSL-1B +iREPA PE-Core-G +iREPA PE-Core-G +iREPA 100K 69.20 100K 77.92 400K 127.4 400K 128.6 100K 63.64 100K 78.79 400K 126.7 400K 132.9 100K 53.87 100K 77.47 400K 116.9 400K 130. 100K 42.74 100K 75.01 400K 109.4 400K 132.0 19.3 16.9 7.76 7.52 21.4 16.2 8.10 7.13 25.5 16.6 9.39 7.48 32.3 18.1 10.4 7.78 5.89 6.26 5.06 4. 6.14 6.14 5.06 4.93 6.57 6.18 5.14 4.91 6.70 6.03 5.00 5.02 0.64 0.66 0.70 0.71 0.63 0.66 0.70 0.71 0.61 0.66 0.70 0. 0.57 0.64 0.69 0.70 0.61 0.61 0.66 0.65 0.60 0.61 0.66 0.66 0.59 0.61 0.64 0.65 0.59 0.61 0.64 0.65 157.2 179.3 263.0 268. 144.0 181.9 261.2 272.4 124.0 177.6 250.5 271.6 97.2 176.8 238.4 275.4 6.35 5.15 1.98 1.93 7.57 4.87 1.99 1.89 9.59 5.09 2.24 1. 14.1 5.66 2.44 1.93 5.91 6.23 4.60 4.59 6.09 6.10 4.58 4.58 6.37 6.11 4.61 4.58 6.56 6.08 4.57 4.59 0.769 0.783 0.799 0. 0.762 0.780 0.799 0.799 0.756 0.787 0.809 0.798 0.714 0.771 0.805 0.796 0.536 0.544 0.610 0.600 0.526 0.547 0.609 0.600 0.515 0.538 0.580 0. 0.525 0.544 0.585 0.606 Table 4: Results across different encoders with and w/o classifier-free guidance. SiT-XL/2 is used base model. See Appendix for detailed results on SiT-B/2 & SiT-L/2 across all encoders. we uncover surprising finding: spatial structure and not global information, drives the effectiveness of representation alignment. We further study this by introducing two simple modifications which accentuate the transfer of spatial information from target representation to diffusion features. Our simple method, termed iREPA, consistently improves convergence speed with REPA across diverse variations in vision encoders and training recipes. We hope our work will motivate future research to revisit the fundamental working mechanism of representational alignment and how we can better leverage it for improved training of generative models."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We thank You Jiacheng (@YouJiacheng), Shuming Hu (@ShumingHu), @gallabytes whose comments on motivated the exploration in this direction (Jiacheng, 2025a;b; Hu, 2025). The authors were glad to find out their original predictions were wrong, which opened the door to new insights. We also thank Zhengyang Geng for providing the meanflow with REPA implementation and for useful discussion on hyperparameter configuration. We also thank Boyang Zheng, Fred Lu, Nanye (Willis) Ma and Sihyun Yu for insightful discussions and guidance on RAE experiments. 10 Preprint."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We provide all implementation details and hyperparameters in Appendix G. We also open-source our code, model checkpoints and analysis results."
        },
        {
            "title": "REFERENCES",
            "content": "Kristian Abstreiter, Sarthak Mittal, Stefan Bauer, Bernhard Scholkopf, and Arash Mehrjou. Diffusionbased representation learning. In ICML 2021 Workshop on Unsupervised Reinforcement Learning, 2021. Stability AI. Improved autoencoders ... https://huggingface.co/stabilityai/ sd-vae-ft-mse, n.d. Accessed: April 11, 2025. Anthropic. Claude code. https://claude.ai/code, 2025. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1561915629, 2023. Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2266922679, 2023. Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf: Speeded up robust features. In Computer VisionECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006. Proceedings, Part 9, pp. 404417. Springer, 2006. Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Dollar, and Christoph Feichtenhofer. Perception encoder: The best visual embeddings are not at the output of the network. arXiv:2504.13181, 2025. Shoufa Chen, Mengmeng Xu, Jiawei Ren, Yuren Cong, Sen He, Yanping Xie, Nicu Sebe, and Wai Lam. Gentron: Diffusion transformers for image and video generation. arXiv preprint arXiv:2312.04557, 2023. Shoufa Chen, Chongjian Ge, Shilong Zhang, Peize Sun, and Ping Luo. Pixelflow: Pixel-space generative models with flow. arXiv preprint arXiv:2504.07963, 2025. Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96409649, 2021. Xinlei Chen, Zhuang Liu, Saining Xie, and Kaiming He. Deconstructing denoising diffusion models for self-supervised learning. arXiv preprint arXiv:2401.14404, 2024. Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Hanoona Rasheed, Peize Sun, PoYao Huang, Daniel Bolya, Suyog Jain, Miguel Martin, Huiyu Wang, Nikhila Ravi, Shashank Jain, Temmy Stark, Shane Moon, Babak Damavandi, Vivian Lee, Andrew Westbury, Salman Khan, Philipp Krahenbuhl, Piotr Dollar, Lorenzo Torresani, Kristen Grauman, and Christoph Feichtenhofer. Perceptionlm: Open-access data and models for detailed visual understanding. arXiv:2504.13180, 2025. N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR05), volume 1, pp. 886893 vol. 1, 2005. doi: 10.1109/CVPR.2005.177. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. 11 Preprint. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. David Fan, Shengbang Tong, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, et al. Scaling language-free visual representation learning. arXiv preprint arXiv:2504.01017, 2025. Michael Fuest, Pingchuan Ma, Ming Gui, Johannes Schusterbauer, Vincent Tao Hu, and Bjorn Ommer. Diffusion models and representation learning: survey. arXiv preprint arXiv:2407.00783, 2024. Zhengyang Geng, Mingyang Deng, Xingjian Bai, Zico Kolter, and Kaiming He. Mean flows for one-step generative modeling. arXiv preprint arXiv:2505.13447, 2025. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. Greg Heinrich, Mike Ranzinger, Hongxu, Yin, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, and Pavlo Molchanov. Radiov2.5: Improved baselines for agglomerative vision foundation models, 2024. URL https://arxiv.org/abs/2412.07679. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Shuming Hu. post on representation alignment discussion. https://x.com/ShumingHu/ status/1957215951826612321, 2025. Accessed: 2025. Jing Huang, S.R. Kumar, M. Mitra, Wei-Jing Zhu, and R. Zabih. Image indexing using color correlograms. In Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 762768, 1997. doi: 10.1109/CVPR.1997.609412. Drew Hudson, Daniel Zoran, Mateusz Malinowski, Andrew Lampinen, Andrew Jaegle, James McClelland, Loic Matthey, Felix Hill, and Alexander Lerchner. Soda: Bottleneck diffusion models for representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1946119471, 2024. Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 93079315, June 2024. You Jiacheng. post on representation alignment. https://x.com/YouJiacheng/status/ 1957073253769380258, 2025a. Accessed: 2025. You Jiacheng. post on representation alignment - follow-up. https://x.com/YouJiacheng/ status/1957134245870105001, 2025b. Accessed: 2025. Dengyang Jiang, Mengmeng Wang, Liuzhuozheng Li, Lei Zhang, Haoyu Wang, Wei Wei, Guang Dai, Yanning Zhang, and Jingdong Wang. No other representation component is needed: Diffusion transformers can provide representation guidance by themselves. arXiv preprint arXiv:2505.02831, 2025. Jiatao Jing, Gu Wu, Yuanpeng Zhou, Yuxin Xu, Zixun Sun, Xinhao Ni, Yazhou Wang, Chunmeng Li, Hao Tang, Ruihua Chen, et al. Dart: Denoising autoregressive transformers for scalable text-to-image generation. arXiv preprint arXiv:2410.08159, 2024. 12 Preprint. Minguk Kang, Richard Zhang, Connelly Barnes Nga Tran, Saurabh Kar, and Jun-Yan Zhu. Distilling diffusion models into conditional gans. arXiv preprint arXiv:2405.05967, 2024. Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Theodoros Kouzelis, Efstathios Karypidis, Ioannis Kakogeorgiou, Spyros Gidaris, and Nikos Komodakis. Boosting generative image modeling via joint image-feature synthesis. arXiv preprint arXiv:2504.16064, 2025. Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. Tuomas Kynkaanniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role In International Conference on Learning of imagenet classes in frechet inception distance. Representations, 2023. Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025a. Xingjian Leng, Jaskirat Singh, Ryan Murdock, Ethan Smith, Rebecca Li, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Family of End-to-End Tuned VAEs for Supercharging T2I Diffusion Transformers. https://end2end-diffusion.github.io/repa-e-t2i/, 2025b. Tianhong Li and Kaiming He. Back to basics: Let denoising generative models denoise. arXiv preprint arXiv:2511.13720, 2025a. Tianhong Li and Kaiming He. Back to basics: Let denoising generative models denoise. arXiv preprint arXiv:2511.13720, 2025b. Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. David Lowe. Object recognition from local scale-invariant features. In Proceedings of the seventh IEEE international conference on computer vision, volume 2, pp. 11501157. Ieee, 1999. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pp. 2340. Springer, 2024. Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 77397751, 2025a. Zehong Ma, Longhui Wei, Shuai Wang, Shiliang Zhang, and Qi Tian. Deco: Frequency-decoupled pixel diffusion for end-to-end image generation. arXiv preprint arXiv:2511.19365, 2025b. Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. In International Conference on Machine Learning, pp. 79587968. PMLR, 2021. OpenAI. Gpt-5. https://openai.com, 2025. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal, pp. 131, 2024. Sayak Paul. Cmmd-pytorch: Pytorch implementation of cmmd metric. https://github.com/ sayakpaul/cmmd-pytorch, 2024. Accessed: 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. 13 Preprint. Pablo Pernias, Dominic Rampas, Mats Leon Richter, Christopher Pal, and Marc Aubreville. In The Wurstchen: An efficient architecture for large-scale text-to-image diffusion models. Twelfth International Conference on Learning Representations, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. URL https://arxiv.org/abs/2408.00714. Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. Orb: An efficient alternative to sift or surf. In 2011 International conference on computer vision, pp. 25642571. Ieee, 2011. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Axel Sauer, Katja Schwarz, and Andreas Geiger. Projected gans converge faster. In Advances in Neural Information Processing Systems, volume 34, pp. 1748017492, 2021. Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. arXiv preprint arXiv:2301.09515, 2023. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. In European Conference on Computer Vision, pp. 218234. Springer, 2024. Eli Shechtman and Michal Irani. Matching local self-similarities across images and videos. In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pp. 18, 2007. doi: 10.1109/ CVPR.2007.383198. Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick Labatut, and Piotr Bojanowski. DINOv3, 2025. URL https://arxiv.org/ abs/2508.10104. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. George Stein, Jesse C. Cresswell, Rasa Hosseinzadeh, Yi Sui, Brendan Leigh Ross, Valentin Villecroze, Zhaoyan Liu, Anthony L. Caterini, J. Eric T. Taylor, and Gabriel Loaiza-Ganem. Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models. In Advances in Neural Information Processing Systems, 2023. Shitao Tang, Fuyang Chen, Aviral Joshi Bose, Kun Zhang, Shengjie Bi, Zhenyu Wang, Jiacheng Feng, Haozhe Cai, Alexander Liu, Antonio Torralba, et al. Ardit: Autoregressive diffusion transformers. arXiv preprint arXiv:2406.00843, 2024. Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. Runqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation regularization. arXiv preprint arXiv:2506.09027, 2025. Preprint. Shuai Wang, Ziteng Gao, Chenhui Zhu, Weilin Huang, and Limin Wang. Pixnerd: Pixel neural field diffusion. arXiv preprint arXiv:2507.23268, 2025a. Shuai Wang, Zhi Tian, Weilin Huang, and Limin Wang. Ddt: Decoupled diffusion transformer. arXiv preprint arXiv:2504.05741, 2025b. Ziqiao Wang, Wangbo Zhao, Yuhao Zhou, Zekai Li, Zhiyuan Liang, Mingjia Shi, Xuanlei Zhao, Pengfei Zhou, Kaipeng Zhang, Zhangyang Wang, Kai Wang, and Yang You. Repa works until it doesnt: Early-stopped, holistic alignment supercharges diffusion training. arXiv preprint arXiv:2505.16792, 2025c. Haoyu Wu, Diankun Wu, Tianyu He, Junliang Guo, Yang Ye, Yueqi Duan, and Jiang Bian. Geometry forcing: Marrying video diffusion and 3d representation for consistent world modeling. arXiv preprint arXiv:2507.07982, 2025. Mengping Yang, Ceyuan Yang, Yichi Zhang, Qingyan Bai, Yujun Shen, and Bo Dai. Revisiting the evaluation of image synthesis with gans. In Advances in Neural Information Processing Systems, 2023. Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. arXiv preprint arXiv:2501.01423, 2025. Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024. Xiangdong Zhang, Jiaqi Liao, Shaofeng Zhang, Fanqing Meng, Xiangpeng Wan, Junchi Yan, and Yu Cheng. Videorepa: Learning physics for video generation through relational alignment with foundation models. arXiv preprint arXiv:2505.23656, 2025. 15 Preprint."
        },
        {
            "title": "A DETAILED CORRELATION ANALYSIS",
            "content": "(a) Linear Probing vs FID (SiT-XL-2) (b) Linear Probing vs FID (SiT-B-2) (c) Spatial Structure vs FID (SiT-XL-2) (d) Spatial Structure vs FID (SiT-B-2) Figure 10: Detailed correlation analysis with encoder labels. Top row shows linear probing accuracy vs FID correlation for SiT-XL-2 and SiT-B-2 models. Bottom row shows spatial structure (LDS) vs FID correlation. Each point is labeled with its corresponding encoder number (see legend). The spatial structure metric demonstrates significantly stronger correlation with generation quality across both model scales. Preprint. SPATIAL SELF-SIMILARITY METRICS Figure 11: Explaining semantic region self-similarity (SRSS) metric. Visual explanation of SRSS metric: For each image, we compute mask using SAM2 and select anchor-positive-negative triplets. The SSM measures the difference in cosine similarity between anchor-positive pairs (within mask) and anchor-negative pairs (outside mask). Intuitively, larger values indicate that patches on the same semantic region are more similar than patches on unrelated regions indicating better spatial structure preservation. Setup. Given an image and vision encoder E, we extract patch tokens = E(I) RT with = and indices [T ] placed on an lattice. Let : [T ] [T ] be the Manhattan distance, and let the (cosine) self-similarity kernel be KX (t, t) = xt, xt xt2 xt [1, 1], standard local self-similarity measure (Shechtman & Irani, 2007). Patch token representation: = E(I) RT where = is the spatial grid of patches (patch index set [T ]). Also let : [T ] [T ] be Manhattan distance on the grid. Self-similarity kernel: : [T ] [T ] measuring self-similarity (Shechtman & Irani, 2007) between patch tokens. We use the cosine kernel KX (t, t) = xt, xt/(xt2xt2). Spatial self-similarity metric: functional : measures how self-similarity between patch tokens varies with lattice distance d. Intuitively, larger values indicate stronger spatial organization (near patches more similar to each other than far away patches). We next discuss several straightforward metrics for measuring the spatial self-similarity structure of the target representations. Local vs. Distant Similarity (LDS). We first consider simple correlogram contrast (local vs. distant) metric (Huang et al., 1997): LDS(X ) = E[KX (t, t) d(t, t) < rnear] E[KX (t, t) d(t, t) rfar] . where rnear and rfar are the hyperparameters. By default we use rnear = rfar = H/2. We found correlation to be robust to their exact choices. Intuitively, larger values indicate that on average, patches that are closer to each other are more similar than patches that are further away. Correlation Decay Slope (CDS). Given the patch tokens = E(I) RT D, we compute the spatial correlogram gX (δ) = E[KX (t, t)d(t, t) = δ] for distances δ {1, . . . , }. We then fit least-squares line (cid:98)gX (δ) α + βδ and define: CDS(X ) = ˆβ where ˆβ is the fitted slope. Larger CDS values indicate faster similarity decay with distance, suggesting stronger spatial organization. Semantic-Region Self-Similarity (SRSS). For each image, we first sample binary mask RHW containing random object using SAM2 (Ravi et al., 2024) and downsampled to . We then select three types of points: anchor, positive, and negative. The anchor and positive points are sampled from within the mask (should have similar semantics), while the negative point is sampled from outside the mask (less related to anchor). Conceptually, if the encoder feature representation preserves the spatial structure, the anchor and positive points should have higher cosine similarity, Preprint. while the anchor and negative points should have lower cosine similarity. Thus, we define the Spatial Structure Metric (SSM) as: SSM(P) = EanchorM [cos(panchor, ppos) cos(panchor, pneg)] where positive points are within Manhattan distance rnear from the anchor within the mask, and negative points are at distance rf ar outside the mask. RMS Spatial Contrast (RMSC). Finally, we consider simple contrast metric to capture the spatial contrast between patch token representations. Given normalized features ˆxt = xt/xt2 for each patch t, we compute: RMSC(X ) = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 ˆxt x2 2 (cid:80)T where = 1 t=1 ˆxt is the mean of normalized features across all spatial locations. Higher RMSC values indicate greater spatial diversity in the feature representations, suggesting preserved spatial structure, whereas lower values indicate more uniform feature distributions indicating loss of spatial structure (typically happens global information dimnishes spatial structure (Simeoni et al., 2025)). Preprint."
        },
        {
            "title": "C VISION ENCODER VARIATION",
            "content": "(a) SiT-B-2 (130M parameters) (b) SiT-L-2 (458M parameters) (c) SiT-XL-2 (675M parameters) Figure 12: Variation in vision encoders. Ablation studies showing iREPA improvements across different vision encoders for SiT-B-2, SiT-L-2, and SiT-XL-2 models. iREPA consistently improves generation quality across all encoders and model sizes. 19 Preprint."
        },
        {
            "title": "D ENCODER DEPTH VARIATION",
            "content": "(a) SiT-B-2 with 6 encoder layers (b) SiT-B-2 with 8 encoder layers Figure 13: Effect of encoder depth on iREPA performance. Comparison of SiT-B-2 performance with different encoder depths (6 vs 8 layers) across various vision encoders. The results show consistent improvements with iREPA regardless of encoder depth, with slightly better performance observed with 8 encoder layers. Both configurations demonstrate significant FID improvements when spatial regularization is applied. 20 Preprint."
        },
        {
            "title": "E COMPREHENSIVE RESULTS ACROSS VISION ENCODERS",
            "content": "E.1 SIT-B/2 RESULTS"
        },
        {
            "title": "Vision Encoder",
            "content": "IS FID sFID Prec. Rec. CLIP-ViT-L +iREPA DINOv2-B +iREPA DINOv2-L +iREPA DINOv3-B16 +iREPA 59.44 24.84 69.70 21.26 64.92 22.75 70.88 21.40 66.65 22.46 72.41 20.99 62.57 23.91 75.48 19.65 DINOv3-H16+ +iREPA 61.91 24.82 71.76 21. DINO-B +iREPA LangPE-G +iREPA MoCov3-B +iREPA PE-B +iREPA PE-G +iREPA SpatialPE-B +iREPA SpatialPE-L +iREPA WebSSL-1B +iREPA WebSSL-2B +iREPA 58.37 24.71 57.55 24.41 58.42 25.37 66.88 21.62 57.61 25.60 61.41 22. 58.86 25.78 69.50 21.15 56.09 26.84 70.43 21.52 59.19 24.28 61.50 22.71 62.13 23.87 68.83 21.07 63.20 23.48 73.54 19.74 66.56 22.27 72.13 20. WebSSL-300M 62.06 23.64 66.83 21.46 +iREPA WebSSL-3B +iREPA 65.91 22.55 71.98 20.64 6.46 6.64 6.54 6.77 6.53 6. 6.77 6.68 6.57 6.84 6.32 6.43 6.32 6.65 6.25 6.46 6.44 6. 6.45 6.58 6.37 6.41 6.51 6.59 6.33 6.63 6.69 6.89 6.48 6. 6.53 6.71 0.584 0.601 0.593 0.597 0.594 0.600 0.584 0.606 0.579 0. 0.586 0.587 0.583 0.604 0.579 0.596 0.58 0.601 0.568 0.598 0.586 0. 0.59 0.607 0.593 0.612 0.596 0.609 0.590 0.606 0.598 0.607 0.645 0. 0.653 0.653 0.650 0.650 0.647 0.649 0.645 0.655 0.635 0.638 0.641 0. 0.640 0.639 0.64 0.659 0.641 0.651 0.646 0.644 0.65 0.651 0.646 0. 0.654 0.648 0.646 0.653 0.654 0.650 Table 5: SiT-B/2 performance across vision encoders at 400K iterations. iREPA consistently improves generation quality. All baselines are reported using vanilla-REPA (Yu et al., 2024) for training. 21 Preprint. E.2 SIT-L/2 RESULTS"
        },
        {
            "title": "Vision Encoder",
            "content": "IS FID sFID Prec. Rec. CLIP-ViT-L +iREPA DINOv2-B +iREPA DINOv3-B16 +iREPA DINOv3-H16+ +iREPA LangPE-G +iREPA MoCov3-B +iREPA PE-G +iREPA WebSSL-1B +iREPA 107.7 115.8 113.3 113. 115.3 118.1 110.7 120.5 102.8 112.7 102.1 104.7 98.38 117.1 107.9 121. 10.6 9.41 9.61 9.36 9.41 9.06 10.6 9.64 11.2 9.37 11.0 10. 12.6 9.65 10.5 8.62 5.20 5.15 5.11 5.11 5.24 5.25 5.32 5. 5.13 5.02 5.14 5.00 5.31 5.33 5.13 5.01 0.682 0.689 0.684 0. 0.687 0.685 0.677 0.681 0.684 0.690 0.685 0.690 0.670 0.683 0.686 0. 0.643 0.653 0.653 0.655 0.651 0.650 0.652 0.663 0.637 0.653 0.635 0. 0.646 0.651 0.644 0.653 Table 6: SiT-L/2 performance across vision encoders at 400K iterations. iREPA consistently improves generation quality across all encoders, with particularly strong gains in FID and IS metrics. All baselines are reported using vanilla-REPA (Yu et al., 2024) for training. 22 Preprint."
        },
        {
            "title": "F ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "Figure 14: Global information in mean patch tokens. We find that in addition to [CLS] token, mean of patch tokens also contains substantial global semantic information. While this helps improve global performance, it reduces the contrast between individual patch tokens, potentially hindering spatial structure transfer. We find that we can remove some of this global information (through mean of patch tokens) to improve spatial structure transfer during representation alignment (4)."
        },
        {
            "title": "Vision Encoder",
            "content": "IS FID sFID Prec. Rec. DINOv2-B 262.97 +iREPA (Ours) 268.79 DINOv3-B 261.18 +iREPA (Ours) 272.41 WebSSL-1B 250.53 +iREPA (Ours) 271.59 PE-G 238.37 +iREPA (Ours) 275. 1.98 1.93 1.99 1.89 2.24 1.90 2.44 1.93 4.60 4.59 4.58 4. 4.61 4.58 4.57 4.59 0.799 0.799 0.799 0.799 0.809 0.798 0.805 0. 0.610 0.600 0.609 0.600 0.580 0.609 0.585 0.606 Table 7: Generation quality with classifier-free guidance. Comparison of REPA vs iREPA with CFG (scale 2.0) across different vision encoders. All experiments use SiT-XL/2 trained for 400K iterations with 250 sampling steps. iREPA consistently improves both IS and FID metrics across all encoders. Figure 15: Accentuating spatial features helps consistently improve convergence speed. Results for SiT-XL/2 with REPA and iREPA. 23 Preprint. Figure 16: Visualizing the impact of two straightforward improvements to enhance spatial feature transfer. First, we find that standard MLP projection layer (top-left) losses spatial information while transferring features from the pretrained encoder (after projection) to diffusion model (before projection). Instead using simpler convolution layer better preserves the spatial information transfer (top-right). Second, we observe that vision encoder features often have limited spatial contrast (2. This causes the tokens in one semantic region (e.g., dog) to show quite decent cosine similarity with unrelated tokens (e.g., background). We address this with simple spatial regularization layer, which accentuates the spatial contrast in the learned representation leading to better generation performance. Best results are obtained while using both (refer Table 2). 24 Preprint."
        },
        {
            "title": "G IMPLEMENTATION DETAILS",
            "content": "Training setup. We follow the same setup as in REPA (Yu et al., 2024) unless otherwise specified. All training is conducted on the ImageNet (Deng et al., 2009) training split. For preprocessing, we adopt the protocol from ADM (Dhariwal & Nichol, 2021), where each image is center-cropped and resized to 256 256 resolution. We use stabilityai/sd-vae-ft-mse VAE (AI, n.d.) throughout our diffusion training and inference. For spatial normalization layer we use γ [0.6, 0.8] and for projection layer we use convolutional layer with kernel size 3 and padding 1. For optimization, we use AdamW (Kingma & Ba, 2014; Loshchilov, 2017) with constant learning rate of 1 104, and global batch size of 256. During training, we use bfloat16 mixed precision and torch.compile to accelerate training, and gradient clipping and exponential moving average (EMA) to the generative model for stable optimization. For REPA-E (Leng et al., 2025a) and JiT (Li & He, 2025a) experiments, we use the official opensource implementation. For MeanFlow (Geng et al., 2025) experiments, we received the implementation (including REPA) from original authors and adapt it to introduce the two straightforward changes for iREPA (4). Evaluation. For image generation evaluation, we strictly follow the ADM setup (Dhariwal & Nichol, 2021). We report generation quality using Frechet inception distance (gFID) (Heusel et al., 2017), structural FID (sFID) (Nash et al., 2021), inception score (IS) (Salimans et al., 2016), precision (Prec.) and recall (Rec.) (Kynkaanniemi et al., 2019), measured on 50K generated images. For sampling, we follow the approach in REPA (Yu et al., 2024), using the SDE Euler-Maruyama sampler with 250 steps. For JiT (Li & He, 2025a), we use 50 inference steps following official implementation."
        },
        {
            "title": "H MORE DISCUSSION ON RELATED WORK",
            "content": "Classical spatial features in computer vision. Spatial feature extraction has long been fundamental to computer vision. Classical methods like SIFT (Lowe, 1999), HOG (Dalal & Triggs, 2005), SURF (Bay et al., 2006), and ORB (Rublee et al., 2011) providing robust local descriptors for tasks ranging from object detection to image matching. While these handcrafted features excel at capturing local spatial patterns and geometric invariances, recent work in generative modeling has primarily focused on representations from modern self-supervised methods that demonstrate strong global classification performance, such as DINOv2 (Oquab et al., 2024) and CLIP (Radford et al., 2021). Our findings suggest different perspective: since spatial structure preservation is critical for generation quality, even classical spatial features could potentially improve diffusion training when properly aligned. This highlights the potential of leveraging the full spectrum of spatial feature extractors, from traditional handcrafted descriptors to modern learned representations, provided that they maintain strong spatial coherence. Pretrained visual encoders for generative models. Pretrained visual encoders have supported generative models in several capacities: as discriminators to accelerate GAN convergence (Goodfellow et al., 2020; Sauer et al., 2021; 2022; 2023; Radford et al., 2021), as teachers in adversarial distillation for diffusion models (Sauer et al., 2024; Kang et al., 2024), and more recently as alignment targets. In GANs (Goodfellow et al., 2020), pretrained features have not only improved convergence speed but also enabled scaling to large datasets, as demonstrated by StyleGAN-XL (Sauer et al., 2022) and StyleGAN-T (Sauer et al., 2023) with CLIP (Radford et al., 2021) features. For diffusion models, adversarial distillation leverages pretrained encoders to guide student networks toward higher-fidelity samples, showing clear improvements in perceptual quality. In particular, REPA (Yu et al., 2024) aligns diffusion features with external encoders, demonstrating that representation alignment can improve both generation convergence and quality. Building on this direction, we focus not only on alignment but specifically on spatial structure perseverance rather than their discriminative capabilities. Denoising transformers. Transformer architectures have become the dominant backbone for scalable generative modeling, with various formulations including diffusion transformers (Peebles & Xie, 2023) and flow matching variants (Ma et al., 2024). Recent architectures like GenTron (Chen et al., 2023) scale transformers to over 3B parameters for text-to-image synthesis. U-ViT (Bao et al., 2023) demonstrates that pure transformer backbones without U-Net inductive biases can achieve competitive performance. ARDiT (Tang et al., 2024) and DART (Jing et al., 2024) explore 25 Preprint. autoregressive formulations that combine denoising with sequential generation, enabling flexible trade-offs between quality and speed. Despite these architectural advances, training these models from scratch remains computationally expensive, often requiring millions of iterations to achieve good generation quality. While representation alignment methods like REPA (Yu et al., 2024) have shown that pretrained features can dramatically accelerate convergence, the mechanisms behind this improvement remained unclear. Our analysis reveals that the key benefit comes from preserving spatial structure rather than semantic alignment, explaining why certain encoders provide stronger acceleration than others and guiding the design of more effective alignment strategies. While (Wang et al., 2025c) propose improvements to REPA training through early stopping, they hypothesize that REPA predominantly distills global semantic information while leaving structural information untapped. In contrast, our analysis reveals that spatial structure (not global semantic information) already plays very significant role in the effectiveness of REPA (2, 3). Thus, while we indeed find that spatial structure remains underexploited (4) in REPA, we surprisingly find that majority of improvements in REPA are already coming from introducing an inductive bias for spatial structure (not global information). Denoising as self-supervised learning task. The connection between denoising and representation learning has been explored from multiple perspectives. Early work by Abstreiter et al. (2021) extended diffusion objectives for representation learning, demonstrating that denoising naturally learns meaningful features. SODA (Hudson et al., 2024) introduces diffusion model with an information bottleneck to learn compact representations. Chen et al. (2024) deconstruct diffusion models and find that simple denoising autoencoder suffices for strong self-supervised performance. Wang & He (2025) introduce dispersive loss to encourage internal representation diversity in diffusion models, improving generative performance without external encoders. Similarly, Jiang et al. (2025) propose Self-Representation Alignment (SRA) to align diffusion transformers latent features across noise levels, providing self-guidance without an auxiliary model. These works establish denoising as fundamental self-supervised task that naturally encourages learning of robust features. Our findings complement this view by showing that when diffusion models are aligned with strong spatial representations from self-supervised encoders, both the generative and discriminative capabilities improve, suggesting that spatial structure preservation is key factor in this synergy. Scaling self-supervised vision encoders. Recent years have seen remarkable progress in scaling self-supervised vision models to unprecedented sizes and datasets. DINOv3 (Simeoni et al., 2025) trains 7B parameter ViT on 1.7 billion images without labels by aligning representations from different augmentations. WebSSL (Fan et al., 2025) demonstrates that visual models trained on more than 2 billion images can match language-supervised models like CLIP (Radford et al., 2021) on vision-language tasks without language supervision. C-RADIO (Heinrich et al., 2024) combines multiple teacher models through distillation to create versatile encoders that excel across diverse visual domains. I-JEPA (Assran et al., 2023) explores predictive architectures that learn by predicting masked regions in representation space, instead of reconstructing pixels directly. SAM (Ravi et al., 2024) specializes in promptable segmentation through large-scale supervised training. The Perception Encoder family (Bolya et al., 2025; Cho et al., 2025) shows that intermediate features often outperform final representations for dense prediction tasks. While these models are typically evaluated on global tasks like image classification, our work reveals crucial insight: strong performance on discriminative benchmarks does not necessarily translate to better generation quality. Instead, we find that encoders preserving spatial structure, regardless of their classification accuracyprovide the most benefit for diffusion training. This suggests that the evaluation metrics for self-supervised encoders should be reconsidered when targeting generative applications, with spatial coherence being as important as semantic understanding."
        },
        {
            "title": "I NOTE ON LLM USAGE",
            "content": "We use GPT-5 (OpenAI, 2025) for considering different variations of the spatial structure metrics discussed in the paper 3. Furthermore, all figures in the paper are directly generated from our experiment logs and checkpoints using Claude-Code (Anthropic, 2025). Additionally we use LLM help for searching and formulating relevant work in Appendix H. We use cursor in some parts to help with paper writing. 26 Preprint."
        },
        {
            "title": "J ADDITIONAL DISCUSSION ON SPATIAL STRUCTURE METRICS",
            "content": "Spatial structure metrics can be used to measure the representation gap. Yu et al. (2024) use linear probing to measure the representation gap, using the increase in validation accuracy to explain the effectiveness of REPA. We find that spatial structure metrics (SSM) can also be used to measure the representaiton gap and explain the improvements across models. As shown in Figure 17, we see that representation alignment helps close the gap between SSM performance of pretrained encoder like DINOv2 and the diffusion features. Figure 17: Representation alignment as bridging the spatial feature gap. a) We find that representation alignment can also be seen as bridging the spatial feature gap between diffusion and vision encoder patch features. b) iREPA (4) accentuates the spatial features of the vision encoder (at cost of some global information) helping achieve better generation performance. c) iREPA helps consistently improve performance across different vision encoders. d) Spatial structure improvements scale with model size. All results are reported with SiT-XL/2 at 400k iterations. Figure 18: Spatial structure better correlates with generation quality than linear probing. Correlation analysis across 27 vision encoders. Left two: Linear probing accuracy vs FID for SiT-XL-2 (Pearson = 0.26) and SiT-B-2 (r = 0.12) shows weak correlation. Right two: Spatial structure (LDS) vs FID for SiT-XL-2 (r = 0.85) and SiT-B-2 (r = 0.89) shows strong correlation. See Figure 10 in Appendix for detailed plots with encoder labels. 27 Preprint."
        },
        {
            "title": "K ADDITIONAL ANECDOTAL COMPARISONS",
            "content": "Figure 19: Motivating anecdotes from recent SSL encoders spatial structure matters. Top row: Metrics comparison showing inverse relationship between ImageNet accuracy and generation quality. Left: PE-g achieves higher accuracy (82.8% vs 72.4%) but worse FID (32.3 vs 22.0) than Spatial-PE-g, with much lower spatial structure (LDS: 0.1 vs 0.4). Right: WebSSL-dino1b shows higher accuracy (76.0% vs 71.7%) but worse FID (26.1 vs 19.3) than DINOv2-b, with weaker spatial structure (LDS: 0.2 vs 0.4). Bottom row: Spatial cosine similarity visualizations confirm that encoders with better generation (Spatial-PE-g, DINOv2-b) maintain clear spatial coherence, while those optimized for classification (PE-g, WebSSL) lose spatial structure. Figure 20: Motivating examples spatial structure matters. Top: Metrics comparison showing inverse relationship between ImageNet accuracy and generation quality. Left: PEcore-G, despite having significantly higher validation accuracy (82.8% vs. 53.1%), shows worse generation quality compared to PESpatial-B (Bolya et al., 2025). Right: Similarly, WebSSL-1B (Fan et al., 2025) also shows much better global performance (76.0% vs. 53.1%), but worse generation. Bottom: We find that spatial structure instead provides better predictor of generation quality than global performance. See 3 for spatial structure metric. All results reported at 100K using SiT-XL/2 and REPA. 28 Preprint."
        },
        {
            "title": "L FURTHER DISCUSSION",
            "content": "Role of Spatial Normalization We visualize the impact of spatial normalization on vision encoder representations. Vision encoders can have significant global components or overlays that limit the contrast between spatial tokens. For example, tokens in one semantic region can be highly correlated with tokens in unrelated regions (e.g., background), reducing the spatial distinctiveness of features. Spatial normalization removes this global overlay to enhance contrast between different spatial tokens, allowing the model to better preserve local spatial structure while reducing the dominance of global information that can interfere with generation quality. The following figures demonstrate this effect across different examples, showing how spatial normalization transforms the feature representations to emphasize spatial structure: Figure 21: Visualizing impact of spatial normalization (Example 1). The heatmaps show token similarity patterns before and after spatial normalization. Without normalization (left), global components create high correlations across unrelated regions. With spatial normalization (right), local spatial structure is enhanced while reducing global interference, resulting in more distinct semantic boundaries. Figure 22: Visualizing impact of spatial normalization (Example 3). The heatmaps show token similarity patterns before and after spatial normalization. Without normalization (left), global components create high correlations across unrelated regions. With spatial normalization (right), local spatial structure is enhanced while reducing global interference, resulting in more distinct semantic boundaries. 29 Preprint. Figure 23: Visualizing impact of spatial normalization (Example 2). Feature similarity maps demonstrate how spatial normalization improves spatial contrast. The original features (left) exhibit global overlay that reduces distinction between foreground and background regions. After normalization (right), spatial tokens become more locally coherent, with clearer separation between different semantic regions. Figure 24: Visualizing impact of spatial normalization (Example 4). The heatmaps show token similarity patterns before and after spatial normalization. Without normalization (left), global components create high correlations across unrelated regions. With spatial normalization (right), local spatial structure is enhanced while reducing global interference, resulting in more distinct semantic boundaries. 30 Preprint."
        },
        {
            "title": "M ADDITIONAL RESULTS",
            "content": "M.1 ADDITIONAL RESULTS AT HIGHER RESOLUTIONS Setup. We conduct further experiments on ImageNet-512 (Deng et al., 2009) to evaluate the generalization of thr proposed spatial improvements at higher resolutions. We follow the same setup as REPA (Yu et al., 2024), and report results across different choice of pretrained encoders (DINOv2, DINOv3, WebSSL, PE, etc.). All results are reported using SiT-XL and SiT-B using 50 NFE at inference w/o classifier free guidance. Fig. 25 shows the results with REPA before and after application of spatial improvements (iREPA). We observe that the spatial improvements also generalize to higher resolutions. Furthermore, consistent gains are observed across different choice of pretrained encoders (DINOv2, DINOv3, WebSSL, PE, etc.). Figure 25: Convergence results at 512x512 resolution. Accentuating spatial features helps consistently improve convergence speed across different resolutions for both Imagenet 256 (Figure 7) and Imagenet 512 (above). 31 Preprint. M.2 ADDITIONAL RESULTS ON MULTIMODAL T2I TASKS Setup. To further study the generalizability of the proposed spatial improvements beyond ImageNet, we also perform extensive experiments on multimodal T2I tasks. Following REPA (Yu et al., 2024), we adopt MMDiT (Esser et al., 2024) as the diffusion backbone and apply REPA with various pretrained vision encoders (DINOv2, CLIP, WebSSL, PE, etc.). Same as REPA (Yu et al., 2024), all models are trained for 150K steps with batch size of 256, and evaluated using an ODE sampler with 50 NFE. Fig. 26 shows the results with REPA before and after application of spatial improvements (iREPA). We observe that the spatial improvements also generalize to multimodal T2I tasks. Furthermore, consistent gains are observed across different choice of pretrained encoders (DINOv2, CLIP, WebSSL, PE, etc.). Figure 26: Text-to-image generation across encoder variants. Accentuating transfer of spatial features from the target representation to the diffusion features consistently improves convergence speed for both imagenet (refer 4) and multimodal T2I tasks (above). Furthermore, consistent gains are observed across different choice of pretrained encoders (DINOv2, CLIP, WebSSL, PE, etc.). 32 Preprint. M.3 CORRELATION ANALYSIS WITHOUT OUTLIERS (MOCOV3-L AND MAE-L) We repeat the correlation analysis from Section 3 after removing MoCOv3-L and MAE-L, which were identified as outliers. Figure 28 shows the updated correlations across different model sizes. We observe that spatial structure still shows much higher correlation with generation performance over linear probing accuracy. Interestingly, after removing the outliers linear probing actually shows small positive correlation with gFID (i.e., as linear probing performance increases, the generation becomes worse). This trend is consistent with the observations discussed in Sec. 2, wherein often target representations with higher global semantic performance (linear probing accuracy) show similar or worse generation performance with representation alignment (REPA). Figure 27: Correlation analysis without outliers. Across all model sizes (B, L, XL) spatial structure still shows much higher correlation (Pearson > 0.85) with generation performance over linear probing accuracy (Pearson < 0.38) after removing the outliers (MoCOv3-L and MAE-L). Interestingly, after removing the outliers linear probing actually shows small positive correlation with gFID (i.e., as linear probing performance increases, the generation becomes worse). This trend is consistent with the observations discussed in Sec. 2, wherein often target representations with higher global semantic performance (linear probing accuracy) show similar or worse generation performance with representation alignment (REPA). 33 Preprint. M.4 ADDITIONAL QUALITATIVE RESULTS Figure 28: Additional Qualitative Results comparing generation outputs before and after application of spatial improvements with REPA. All results are reported using PE-G (Bolya et al., 2025) as the pretrained vision encoder, 400K steps (80 epochs) and with classifier-free guidance scale of 4.0. Similar to quantitative improvements (4), we also observe that spatial improvements (iREPA) also help improve the visual quality and coherence of the generated outputs. Preprint. M.5 ADDITIONAL RESULTS WITH ALTERNATIVE EVALUATION METRICS In addition to traditional evaluation metrics (Inception Score, FID, sFID, Precision, Recall), we also verify the robustness of the proposed findings with alternative evaluation metrics for generation quality (Stein et al., 2023; Yang et al., 2023; Kynkaanniemi et al., 2023; Jayasumana et al., 2024). In particular, we use the CMMD metric (Jayasumana et al., 2024) with the PyTorch implementation from (Paul, 2024). Standard reference set from OpenAI ADM evaluation suite (Dhariwal & Nichol, 2021) is used as reference images. We next verify the robustness of the key findings from both 3 and 4 with the CMMD metric. Spatial structure correlates much higher with generation performance than linear probing. To study robustness of analysis from 3, we repeat the large-scale correlation analysis across different vision encoders with CMMD metric (Jayasumana et al., 2024). Results are shown in Figure 29. All results are reported using SiT-B/2 (100K steps) and REPA. All spatial metrics show much higher correlation (Pearson > 0.88) with generation performance (CMMD) than linear probing (Pearson = 0.074) demonstrating that key empirical findings from 3 are robust to the choice of evaluation metric (FID or CMMD). Figure 29: Correlation analysis with CMMD metric. We repeat the correlation analysis from 3 with CMMD metric (Jayasumana et al., 2024) instead of gFID. All results are reported using SiT-B/2 (100K steps) and REPA. All spatial metrics show much higher correlation (Pearson > 0.88) with generation performance (CMMD) than linear probing (Pearson = 0.074). This demonstrates that key empirical findings from 3 are robust to the choice of evaluation metric. Accentuating transfer of spatial features helps improve generation quality. We next study the generalizability of analysis from 4 with the CMMD (Jayasumana et al., 2024) metric in addition to traditional evaluation metrics (gFID, sFID, IS, etc.). Results are shown in Table 8. Across various vision encoders, accentuating transfer of spatial features (iREPA) helps improve convergence speed with both CMMD and traditional evaluation metrics (IS, FID, sFID, Prec., Recall). Furthermore, consistent improvements are observed both with and without classifier-free guidance (CFG). 35 Preprint. w/o CFG w/ CFG Vision Encoder Steps CMMD IS FID sFID Prec. Rec. CMMD IS FID sFID Prec. Rec. DINOv2-B +iREPA DINOv2-B +iREPA DINOv3-B +iREPA DINOv3-B +iREPA WebSSL-1B +iREPA WebSSL-1B +iREPA PE-Core-G +iREPA PE-Core-G +iREPA CLIP-L +iREPA CLIP-L +iREPA 100K 100K 400K 400K 100K 100K 400K 400K 100K 100K 400K 400K 100K 100K 400K 400K 100K 100K 400K 400K 0.702 0.652 0.455 0.438 0.749 0.651 0.474 0.441 0.825 0.653 0.512 0.445 0.922 0.697 0.525 0.458 0.790 0.657 0.487 0. 69.20 77.92 127.4 128.6 63.64 78.79 126.7 132.9 53.87 77.47 116.9 130.8 42.74 75.01 109.4 132.0 54.07 74.46 117.8 130.1 19.3 16.9 7.76 7. 21.4 16.2 8.10 7.13 25.5 16.6 9.39 7.48 32.3 18.1 10.4 7.78 25.2 17.8 8.97 7.42 5.89 6.26 5.06 4.89 6.14 6.14 5.06 4. 6.57 6.18 5.14 4.91 6.70 6.03 5.00 5.02 6.65 6.33 4.98 5.03 0.64 0.66 0.70 0.71 0.63 0.66 0.70 0.71 0.61 0.66 0.70 0. 0.57 0.64 0.69 0.70 0.61 0.65 0.70 0.71 0.61 0.61 0.66 0.65 0.60 0.61 0.66 0.66 0.59 0.61 0.64 0.65 0.59 0.61 0.64 0. 0.60 0.61 0.65 0.65 0.529 0.484 0.320 0.310 0.571 0.481 0.336 0.314 0.622 0.484 0.354 0.311 0.714 0.525 0.366 0.322 0.605 0.489 0.339 0. 157.2 179.3 263.0 268.8 144.0 181.9 261.2 272.4 124.0 177.6 250.5 271.6 97.2 176.8 238.4 275.4 124.5 172.0 258.7 271.8 6.35 5.15 1.98 1. 7.57 4.87 1.99 1.89 9.59 5.09 2.24 1.90 14.1 5.66 2.44 1.93 9.77 5.67 2.15 1.96 5.91 6.23 4.60 4.59 6.09 6.10 4.58 4. 6.37 6.11 4.61 4.58 6.56 6.08 4.57 4.59 6.59 6.33 4.64 4.64 0.77 0.78 0.80 0.80 0.76 0.78 0.80 0.80 0.76 0.79 0.81 0. 0.71 0.77 0.81 0.80 0.75 0.78 0.81 0.80 0.54 0.54 0.61 0.60 0.53 0.55 0.61 0.60 0.52 0.54 0.58 0.61 0.53 0.54 0.59 0. 0.52 0.54 0.59 0.61 Table 8: Additional results with alternative evaluation metrics. We provide additional results with the CMMD metric (Jayasumana et al., 2024). All results are reported using SiT-XL/2, 256 batch size and traditional REPA as baseline. We adopt the pytorch implementation from (Paul, 2024) for computing the CMMD metric. Standard reference set from OpenAI ADM evaluation suite (Dhariwal & Nichol, 2021) is used as reference images. Across various vision encoders, accentuating transfer of spatial features (iREPA) helps improve convergence speed with both CMMD and traditional evaluation metrics (IS, FID etc.). This demonstrates that key empirical findings from 4 are robust to the choice of evaluation metric. 36 Preprint. M.6 SPATIAL IMPROVEMENTS WITH SAM2 Table 9 and Figure 30 study the impact of spatial normalization (4) on SAM2-S (46M) encoder features2. As shown in Table 9, we observe that for SAM2, while spatial normalization layer helps improve performance, the improvements can be marginal. This is because spatial normalization relies on removing the global component (mean of patch tokens) to enhance spatial contrast. Since SAM2 already has little to no global information (validation accuracy < 24%), spatial normalization only slightly improves the spatial contrast (see Figure 30). Figure 30: Impact of spatial normalization on SAM2. We observe that for SAM2, while use of spatial normalization layer does help enhance the spatial contrast, the improvements can be marginal. This is because spatial normalization (4) relies on removing the global component (mean of patch tokens) to enhance spatial contrast. Since SAM2 already has little to no global information (validation accuracy < 24%), spatial normalization only slightly improves the spatial contrast."
        },
        {
            "title": "Steps",
            "content": "IS FID sFID Prec. Rec. SAM2-S +iREPA SAM2-S +iREPA 50.69 100K 53.14 100K 400K 110.77 400K 114.62 25.32 24.52 9.54 9. 6.52 6.28 4.93 4.89 0.631 0.629 0.699 0.704 0.588 0.591 0.638 0.640 Table 9: Impact of spatial improvements on SAM2.. All results are reported with SiT-XL/2, without classifier-free guidance, SAM2-S (46M) as vision encoder and with traditional REPA as the baseline. 2Note that similar to (Bolya et al., 2025), we use the intermediate output of vision encoder for SAM2 features and not the mask logits. As shown in (Bolya et al., 2025), while mask logits lead to sharper spatial maps, the mask logits itself are not suitable as target representation. Preprint. M.7 ADDITIONAL RESULTS WITH FULL-FINETUNING ACCURACY INSTEAD OF LINEAR"
        },
        {
            "title": "PROBING ACCURACY",
            "content": "Correlation analysis with full-finetuning accuracy instead of linear probing accuracy. We repeat the correlation analysis from 3 with the validation accuracy after full-finetuning3 instead of linear probing. Results are shown in Fig. 31. All results are reported using SiT-XL/2 (100K steps) and REPA. For full-finetuning validation accuracy, instead of linear probing, we perform full-finetuning of vision encoder with learning rate of 5e 5, warmup ratio of 0.1, and total of 30 epochs. All spatial metrics show much higher correlation with generation performance than full-finetuning accuracy. Interestingly, gFID actually shows weak positive correlation with the validation accuracy after full-finetuning (Pearson = 0.317), i.e., as validation accuracy increases, the gfid increases, and generation performance becomes worse. This is consistent with the observations discussed in 2, wherein often target representations with higher global semantic performance show similar or worse generation performance with representation alignment (REPA). Figure 31: Correlation analysis with full-finetuning accuracy instead of linear probing accuracy. We repeat the correlation analysis from 3 with the validation accuracy after full-finetuning instead of linear probing. All results are reported using SiT-XL/2 (100K steps) and REPA. All spatial metrics show much higher correlation with generation performance than full-finetuning accuracy. Interestingly, gFID actually shows weak positive correlation with the validation accuracy after full-finetuning (Pearson = 0.317), i.e., as validation accuracy increases, the gfid increases, and generation performance becomes worse. This is consistent with the observations discussed in 2, wherein often target representations with higher global semantic performance show similar or worse generation performance with representation alignment (REPA). 3Please note that while the final findings remain similar, in context of REPA linear probing might be more accurate for estimating global information in pretrained vision encoders. This is because REPA uses the pretrained encoder features themselves for regularization. Finetuning the encoder itself can impact the amount of global information. Same as REPA (Yu et al., 2024), we therefore use linear probing accuracy as default for measuring global information. 38 Preprint. M.8 ADDITIONAL RESULTS ON PIXEL-SPACE DIFFUSION MODELS"
        },
        {
            "title": "Method",
            "content": "#Params IS FID sFID Prec. Rec. PixelFlow (Chen et al., 2025) PixDDT (Wang et al., 2025b) PixNerd (Wang et al., 2025a) DeCo w/o LFreqFM DeCo + REPA (Ma et al., 2025b) JiT (Li & He, 2025a) JiT+iREPA (Ours) 459M 434M 458M 426M 426M 459M 459M 24.67 36.24 43.01 46.44 48.35 29.37 50.72 54.33 46.37 37.49 34.12 31.35 49.06 29. 9.71 17.14 10.65 10.41 9.34 11.21 9.42 - - - - - 0.40 0.51 0.58 0.63 0.62 0.64 0.65 0.62 0. Table 10: Generation performance of pixel-space diffusion models. Despite its simplicity, when combined with JiT (Li & He, 2025a) iREPA outperforms recently proposed state-of-art pixel-space diffusion methods e.g., DeCo (Ma et al., 2025b). All results are reported with 200K training iterations with batch size of 256 and evaluated using 50-step Euler sampling without classifier-free guidance. Results for DeCo are obtained directly from (Ma et al., 2025b)."
        }
    ],
    "affiliations": [
        "ANU",
        "Adobe Research",
        "New York University"
    ]
}