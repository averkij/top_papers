{
    "paper_title": "Boosting Latent Diffusion Models via Disentangled Representation Alignment",
    "authors": [
        "John Page",
        "Xuesong Niu",
        "Kai Wu",
        "Kun Gai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Latent Diffusion Models (LDMs) generate high-quality images by operating in a compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of a generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this yields certain performance gains, using the same alignment target for both VAEs and LDMs overlooks their fundamentally different representational requirements. We advocate that while LDMs benefit from latents retaining high-level semantic concepts, VAEs should excel in semantic disentanglement, enabling encoding of attribute-level information in a structured way. To address this, we propose the Semantic disentangled VAE (Send-VAE), explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs. Our approach employs a non-linear mapper network to transform VAE latents, aligning them with VFMs to bridge the gap between attribute-level disentanglement and high-level semantics, facilitating effective guidance for VAE learning. We evaluate semantic disentanglement via linear probing on attribute prediction tasks, showing strong correlation with improved generation performance. Finally, using Send-VAE, we train flow-based transformers SiTs; experiments show Send-VAE significantly speeds up training and achieves a state-of-the-art FID of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256x256."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 ] . [ 1 3 2 8 5 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "BOOSTING LATENT DIFFUSION MODELS VIA DISENTANGLED REPRESENTATION ALIGNMENT John Page, Xuesong Niu, Kai Wu, Kun Gai Kolors Team, Kuaishou Technology"
        },
        {
            "title": "ABSTRACT",
            "content": "Latent Diffusion Models (LDMs) generate high-quality images by operating in compressed latent space, typically obtained through image tokenizers such as Variational Autoencoders (VAEs). In pursuit of generation-friendly VAE, recent studies have explored leveraging Vision Foundation Models (VFMs) as representation alignment targets for VAEs, mirroring the approach commonly adopted for LDMs. Although this approach yields certain performance gains, using the same alignment target to both VAEs and LDMs overlooking their fundamentally different representational requirements. In this paper, we advocate that the representational requirements of LDMs and VAEs are fundamentally different: while LDMs benefit from latents that retain high-level semantic concepts crucial for generative modeling, VAEs should excels in semantic disentanglement, enabling the encoding of fine-grained, attribute-level information in structured way. To address this discrepancy, we propose the Semantic-disentangled VAE (Send-VAE), which is explicitly optimized for disentangled representation learning through aligning its latent space with the semantic hierarchy of pre-trained VFMs to improve the VAEs semantic disentanglement capabilities. Specifically, our approach involves using sophisticated non-linear mapper network to transform VAEs latent representations, aligning them with the representations from vision foundation models. This mapper network is designed to bridge the representation gap between attribute-level disentanglement and the high-level semantics provided by VFMs, thereby facilitating effective guidance for VAE learning. We further evaluate the VAEs semantic disentanglement capability by implementing linear probing on attribute prediction tasks, demonstrating strong correlation with improved downstream generation performance. Finally, utilizing on the proposed SendVAE, we train popular flow-based transformers SiTs, and experimental results indicate that Send-VAE can significantly speed up SiT training and establishes new state-of-the-art FID score of 1.21 and 1.75 with and without classifier free guidance, respectively, on ImageNet 256 256 resolution. Code is available at https://github.com/Kwai-Kolors/Send-VAE"
        },
        {
            "title": "INTRODUCTION",
            "content": "Latent diffusion models (LDMs) Albergo & Vanden-Eijnden (2023); Rombach et al. (2022); Peebles & Xie (2023); Ma et al. (2024) have recently achieved remarkable success in high-resolution image synthesis, establishing new benchmarks in visual fidelity and detail. critical component of these models is the image tokenizer, typically implemented via variational autoencoder (VAE) Kingma & Welling (2013). By compressing images into structured latent space, VAEs reduce the computational demands associated with generating high-resolution images. Consequently, the quality of VAE directly influences both the efficiency of model training and the fidelity of the output from downstream generation models. Despite its importance, the defining characteristics of generationfriendly VAE, which can facilitate effective learning of downstream generation models, remain underexplored. Equal contribution. Project lead."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Our Send-VAE enhances VAEs by aligning their latent representations with the semantically rich representations of pre-trained vision foundation models using specialized mapper network. Unlike the direct alignment methods typically used during diffusion model training, this mapper network efficiently bridges the representation gap, enabling seamless integration of semantic information. Notably, the usage of Send-VAE results in significantly more efficient and effective training of diffusion models. Conventional VAE training predominantly optimizes for pixel-level reconstruction, often disregarding alignment with the generative objectives of LDMs. Recent effortson VAE Yao et al. (2025); Chen et al. (2025a); Zha et al. (2025), inspired by REPA Yu et al. (2025), have aimed to bridge the gap by explicitly aligning the VAE latents with the representation from large-scale, pre-trained visual foundation models such as CLIP Radford et al. (2021) or DINOv2 Oquab et al. (2024). In contrast, REPA-E Leng et al. (2025) extends REPA to an end-to-end joint training strategy through backpropagating the representation alignment loss of diffusion transformers to VAE. Although these approaches have yielded notable performance gains in downstream generation tasks, they typically assume that VAEs and LDMs share the same alignment target, overlooking their fundamentally distinct representational demands. Drawing inspiration from the tokenizer analysis in Beyer et al. (2025), we hypothesize that the semantic disentanglement ability of VAE is the key factor, which makes the VAE can better encoder attribute-level semantic information. To verify this hypothesis, we first conduct linear probing experiments on attribute prediction benchmarks to quantify the semantic disentanglement ability of various VAEs. The results reveal striking positive correlation between the linear separability of attributes in VAE latent space and the downstream diffusion models generation quality. This compelling evidence suggests that the richness and accessibility of attribute-level semantic information is more fundamental characteristic of VAEs latent space, conducive to effective diffusion modeling. Consequently, we advocate for the performance on these low-level attribute prediction tasks via linear probing as novel, more intrinsic metric for evaluating quality of VAEs latent space. Based on this observation, we propose the semantic-disentangled VAE (Send-VAE), which leverages the semantically rich representations from pre-trained vision foundation models to guide the learning of VAE. Unlike previous attempts that directly align the VAEs latent representations with those from vision foundation models, Send-VAE incorporates sophisticated non-linear mapper network between the VAE and vision foundation models. Such mapper network effectively bridges the representation gap, thus facilitating seamless injection of semantic information and enhancing VAEs semantic disentanglement capacity. As shown in Fig. 1 right, when integrated with flow-based transformers SiTs Ma et al. (2024), Send-VAE can significantly accelerate the SiT training compared with REPA and achieves new state-of-the-art FID score of 1.21 and 1.75 with and without classifier-free guidance on ImageNet 256 256 generation. In summary, this paper makes the following key contributions: We identify semantic disentanglement as core property of generation-friendly VAEs, verified by the strong correlation between low-level attribute prediction accuracy and downstream generative performance."
        },
        {
            "title": "Preprint",
            "content": "We propose Send-VAE, simple yet effective method for enhancing semantic disentanglement via alignment with vision foundation models through sophisticated non-linear mapper network. We demonstrate that Send-VAE substantially accelerates diffusion model training and establishes new state-of-the-art results on ImageNet 256256 generation."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Tokenizers for Image Generation. Image tokenizers are designed to transform high-dimension image inputs into more compact and structured latent representations, facilitating modeling by downstream generative models. These tokenizers can be broadly categorized into continuous and discrete types. Continuous tokenizers, exemplified by Variational Autoencoders (VAEs) Kingma & Welling (2013), are widely adopted in diffusion-based generation models Rombach et al. (2022); Peebles & Xie (2023); Ma et al. (2024); whereas discrete tokenizers, represented by VQGAN Esser et al. (2021), are commonly used in autoregressive (AR) generation models. However, as these tokenizers are typically trained with pixel-level reconstruction objective, their latent spaces may not be well aligned with the requirements of generation tasks. To tackle this challenge, recent studies have explored training VAEs by aligning their latent representations with the feature spaces of vision foundation models, drawing inspiration from recent advances in the training of diffusion transformers Yu et al. (2025). For instance, VA-VAE Yao et al. (2025) aligns the latent representations of VAE with pre-trained vision foundation models, significantly improving the generation performance of high-dimensional tokenizers while preserving their original reconstruction capabilities. Inspired by MAE He et al. (2022), MAETok Chen et al. (2025a) incorporates masked image modeling into tokenizer training and leverages multiple target features to learn semantically rich latent space. Similar strategies have also been explored in discrete tokenizers Xiong et al. (2025); Li et al. (2025). While these strategies deliver appreciable gains, applying an identical alignment target to both VAEs and LDMs neglects their inherently distinct representational requirements. In contrast to explicit alignment-based approaches, REPA-E Leng et al. (2025) adopts an end-to-end joint training framework, backpropagating the representation alignment loss from diffusion transformers to the VAE. Although REPA-E achieves notable performance improvements, its straightforward joint training paradigm leaves fundamental question unresolved: what characteristics make VAE particularly well-suited for generation tasks? We posit that the representational needs of LDMs and VAEs differ at fundamental level, and argue that an effective generation-friendly VAE should exhibit strong semantic disentanglement capabilities. To this end, we harness the semantically rich representations from pre-trained vision foundation models to guide the VAEs learning process. Diffusion models for image generation. Diffusion models have emerged as powerful class of generative models, formulating image synthesis as progressive denoising process that transforms Gaussian noise into realistic images. Early methods such as DDPM Ho et al. (2020) and DDIM Song et al. (2021) operate directly in the pixel space, requiring numerous iterative steps for high-fidelity generation. To improve efficiency, latent diffusion models (LDMs)Rombach et al. (2022) compress images into lower-dimensional latent space using pre-trained autoencoders, enabling faster and more scalable training. Most early diffusion modelsNichol & Dhariwal (2021); Rombach et al. (2022) adopt U-Net architectures for noise prediction, while recent advances explore transformerbased designs Peebles & Xie (2023); Ma et al. (2024) to better capture long-range dependencies. In addition to architectural improvements, recent studies have explored leveraging pretrained visual representations to enhance the efficiency and performance of diffusion models, enabling better feature representation and faster convergence. For instance, MaskDiT Zheng et al. (2024) and SDDiT Zhu et al. (2024) adopt training paradigms from MAE He et al. (2022) and iBOT Zhou et al. to enhance feature learning within the Diffusion Transformer (DiT) framework. REPA Yu et al. (2025) aligns the latent features of diffusion model with those from frozen, high-capacity encoder pretrained on large-scale external data, thereby regularizing the generative process. Building upon this idea, SARA Chen et al. (2025b) further introduces structural and adversarial alignment objectives, while SoftREPA Lee et al. (2025) extends the framework to multimodal settings by aligning noisy image representations with soft text embeddings. To avoid reliance on additional pretrained visual models, Dispersive Loss Wang & He (2025) encourages internal representations to disperse in the hidden space, and demonstrates that representation regularization alone can effectively enhance gen-"
        },
        {
            "title": "Preprint",
            "content": "erative modeling. These works explore representation learning of the denoising network within fixed latent space, while overlooking the representation learning of the VAE."
        },
        {
            "title": "3 METHOD",
            "content": "In this section, we provide comprehensive introduction to the design of Send-VAE. We begin by analyzing the behavior of three publicly available VAEs including VA-VAE (f16d32) Yao et al. (2025), E2E-VAE Leng et al. (2025), and IN-VAE Leng et al. (2025). We observe that there is strong correlation between the performance of linear probing on attribute prediction tasks and the downstream generation performance. Based on the analysis, we hypothesize that generativefriendly VAE necessitates strong semantic disentanglement capability. Thus, we propose SendVAE, which injects semantic information into VAE through the use of pre-trained vision foundation models. Finally, we regard linear probing on attribute prediction tasks as measurement of the VAEs semantic disentanglement capability and verify the effectiveness of our Send-VAE."
        },
        {
            "title": "3.1 OBSERVATIONS",
            "content": "To answer the question of what characteristics should generative-friendly VAE possess, we first investigate the behavior of VAE latent space using three recently proposed evaluation methods, including semantic gap Yu et al. (2025), latent space uniformity Yao et al. (2025), and latent space discrimination Chen et al. (2025a). For semantic gap, the linear probing on ImageNet classification is adopted following REPA Yu et al. (2025). Next, for latent space uniformity, we calculate Gini coefficients of data point distribution using kernel density estimation (KDE) as done in VA-VAE Yao et al. (2025). As for latent space discrimination, we fit Gaussian mixture model (GMM) into the latent space following MAETok Chen et al. (2025a). We include three publicly available VAEs: VA-VAE (f16d32) Yao et al. (2025), E2E-VAE Leng et al. (2025), IN-VAE Leng et al. (2025) and our Send-VAE, with the final results shown in Fig 2. The uniformity and discrimination of latent space are not directly correlated with generation performance. As shown in Fig 2, we observe that while VA-VAE shows improved uniformity and enhanced downstream generation performance compared with IN-VAE, such conclusion does not hold true for E2E-VAE. similar situation also occurs in the evaluation of latent space discrimination. We argue that these metrics only partially reflect the impact of VAEs on generation performance, and cannot accurately describe the characteristics of generation-friendly VAE. The semantic disentanglement ability is the key factor. Aligning the hidden states of diffusion model with pretrained vision foundation models is first proposed in REPA Yu et al. (2025) to reduce the semantic gap between them, which has been proven to accelerate the convergence of diffusion models. As for VAEs, we can observe that while directly injecting semantic information can improve generation performance partially (VA-VAE achieves significant performance gains compared with IN-VAE), it is not necessary requirement for generation-friendly VAE considering the further performance gains achieved by E2E-VAE. Motivated by the observation in Beyer et al. (2025), we hypothesize that the semantic disentanglement ability of VAE is the key factor and conduct linear probing on attribute prediction tasks to verify it. As show in Fig 2 right, strong correlation between generation performance and the linear probing performance can be observed, which verifies our hypothesis. Meanwhile, our Send-VAE can achieve more powerful semantic disentanglement ability, thus resulting in better generation performance. 3.2 SEMANTIC DISENTANGLED VAE Based on the above hypothesis, we try to enhance the semantic disentanglement ability of VAE and propose our Send-VAE. Specifically, Send-VAE utilizes sophisticated non-linear mapper network to transform the latent representations of VAE, and aligns the patch-wise transformed representations with pre-trained vision foundation models. Different from the simple multilayer perceptron (MLP) used in VA-VAE and REPA, our mapper network consists of patch embedding layer, stack of vision transformer (ViT) Dosovitskiy et al. (2021) layers, and the final MLP projector. The reason for this is the difference between the training objectives of vision foundation models and VAEs, which leads to substantial representation gap. Therefore, compared with direct alignment, sophisticated non-linear mapper network is designed to mitigate the representation gap and enable"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: We conduct experiments with three recently proposed evaluation methods for VAE latent space, and show their correlation with down stream generation performance (g-FID). Experimental results on four VAEs with identical specifications indicate that these metrics do not accurately reflect the impact of VAEs on downstream generative performance. Conversely, we find that the ability of VAEs regarding low-level attributes is the key factor. effective knowledge distillation from semantically rich visual representations to VAE. The overall framework is shown in Fig 1. Formally, given clean image x, let be the latent representation of output by VAE Vθ, be frozen vision foundation model, and = (x) RN is the encoded representation of x, where N, are the number of patches and the embedding dimension of , respectively. Following the noise injection mechanism of SiT Ma et al. (2024), PE-VAE first inject random Gaussian noise into and get zt, where is the time step. Then, the mapper network hϕ is applied to transform zt into hϕ(zt), and the alignment loss can be calculated using patch-wise cosine similarity between hϕ(zt) and (x): Lalign = 1 (cid:88) (1 n=1 hϕ(zt)[n] (x)[n] hϕ(zt)[n]f (x)[n] ), (1) where is the patch index. In practice, we use Lalign to finetune pre-trained VAE for fast convergence. And the original VAE training loss function LVAE used in AI (n.d.), is also included, which consists of reconstruction losses (LMSE, LLPIPS), GAN loss (LGAN ) and KL divergence loss LKL. Thus, the overall training objective can be formulated as: L(θ, ϕ) = λalignLalign + LVAE, (2) where θ and ϕ refer to the parameters of VAE and mapper network."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In this section, we conduct comprehensive experiments on the ImageNet dataset Deng et al. (2009) at 256256 resolution to validate the design choices of Send-VAE, and benchmark its generation performance to demonstrate its superiority over existing approaches. 4.1 IMPLEMENTATION DETAILS We follow the same set up as in REPA-E Leng et al. (2025) unless otherwise specified. All training is conducted on the training split of ImageNet Deng et al. (2009). The data preprocessing protocol is same as in ADM Dhariwal & Nichol (2021) including center-crop and resizing to 256x256 resolution. For VAE training, we train 80 epoch with global batch size of 1024, AdamW Loshchilov & Hutter (2019) optimizer is adopted and the learning rate is set to 3.0104. As for the initialization, we experiment with publicly available VAEs, including SD-VAE (f8d4) Rombach et al. (2022), VA-VAE (f16d32) Yao et al. (2025), and IN-VAE (f16d32), which is trained on ImageNet following Rombach et al. (2022). Experimentally, we choose VA-VAE as the default setting. As for alignment loss Lalign, we use DINOv2 Oquab et al. (2024) as the vision foundation model, and λalign is set to 1.0."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Qualitative comparisons among VA-VAE, E2E-VAE, and Send-VAE. Results for both methods are sampled using the same seed, noise and class label. The classifier-free guidance scale is set to 4.0. For diffusion models, we choose SiT-XL/1 and SiT-XL/2 for VAEs with 4 and 16 downsampling rates, respectively, where 1 and 2 denote the patch sizes in the transformer embedding layer. We train either 80 epoch or 800 epoch with global batch size of 256, and gradient clipping and exponential moving average (EMA) are applied stable optimization. The learning rate is set to 1.0 104 and AdamW optimizer is used. REPA loss is also included following the setting in Yu et al. (2025). For sampling, the SDE Euler-Maruyama sampler is used, the number of function evaluations (NFE) is set to 250 by default and the cfg scale is set to 2."
        },
        {
            "title": "4.2 EVALUATION METRICS",
            "content": "For image generation evaluation, we strictly follow the ADM setup Dhariwal & Nichol (2021). Generation quality is assessed using Frechet Inception Distance (gFID) Heusel et al. (2017), Structural FID (sFID) Nash et al. (2021), Inception Score (IS) Salimans et al. (2016), Precision, and Recall Kynkaanniemi et al. (2019), computed on 50K generated samples. For sampling, we adopt the SDE EulerMaruyama solver with 250 steps, following the protocols of REPA Yu et al. (2025) and REPA-E Leng et al. (2025). For VAE evaluation, we report reconstruction FID (rFID) on 50K validation images from ImageNet at 256256 resolution. 4.3 SYSTEM-LEVEL COMPARISON ON IMAGENET 256X256 GENERATION To verify the effectiveness of Send-VAE, we conduct system-level comparison on ImageNet 256x256 generation with and without classifier free guidance (CFG), and present the results in Table 1. As we can see, using the same vision foundation model DINOV2, Send-VAE can achieve notable performance gains compared with E2E-VAE and set new state-of-the-art generation FID score of 1.21 and 1.75 with and without classifier free guidance on ImageNet 256x256 generation. These results highly demonstrate the effectiveness of enhance the semantic disentanglement ability"
        },
        {
            "title": "Preprint",
            "content": "Table 1: System-level comparison on ImageNet 256x256 generation with and without classifier free guidance (CFG). Our Send-VAE can significant accelerate the convergence of diffusion models, which achieves gFID socre of 2.88/1.41 wo/w CFG for only 80 epoch of training. Although the performance gap between Send-VAE and E2E-VAE is narrowing when training longer, Send-VAE still achieves further improvements. Tokenizer Method Training Epoch #params rFID AutoRegressive (AR) Generation w/o CFG Generation w/ CFG gFID sFID IS Prec. Rec. gFID sFID IS Prec. Rec. MaskGiT VQGAN VQVAE MaskGIT Chang et al. (2022) LlamaGen Sun et al. (2024) VAR Tian et al. (2024) 300 350 2.0B - - 227M 2.28 6. - 182.1 0.80 0.51 - - - - - 3.1B 0.59 9.38 8.24 112.9 0.69 0.67 2.18 5.97 263.3 0.81 0.58 LFQ tokenizers MagViT-v2 Yu et al. (2024) 1080 307M 1.50 3. LDM MAR Li et al. (2024) 800 945M 0.53 2.35 Latent Diffusion Models (LDM) - - - - 200.5 - - - - 1.80 1.78 227.8 0.79 0.62 1.55 - - - 365.4 0.83 0.57 319.4 - - 303.7 0.81 0.62 MaskDiT Zheng et al. (2024) 1600 675M 5.69 10.34 177.9 0.74 0.60 2.28 5.67 276.6 0.80 0.61 DiT Peebles & Xie (2023) SiT Ma et al. (2024) SD-VAE Rombach et al. (2022) FastDiT Yao et al. (2024) MDT Gao et al. (2023a) MDTv2 Gao et al. (2023b) REPA Yu et al. (2025) VA-VAE Yao et al. (2025) LightingDiT Yao et al. (2025) MAETok Chen et al. (2025a) LightingDiT Yao et al. (2025) E2E-VAE Leng et al. (2025) REPA Yu et al. (2025) Send-VAE REPA Yu et al. (2025) 1400 1400 400 1300 800 80 800 800 80 80 800 675M 675M 9.62 6.85 121.5 0.67 0.67 2.27 4.60 278.2 0.83 0.57 8.61 6.32 131.7 0.68 0.67 2.06 4.50 270.3 0.82 0. 675M 0.61 7.91 5.45 131.3 0.67 0.69 2.03 4.63 264.0 0.81 0.60 675M 675M 675M 6.23 5.23 143.0 0.71 0.65 1.79 4.57 283.0 0.81 0.61 - - - - - 1.58 4.52 314.7 0.79 0.65 5.90 5.73 157.8 0.70 0.69 1.42 4.70 305.7 0.80 0.65 675M 0.28 4.29 - - - - - - - - - 675M 0.28 2.17 4.36 205.6 0.77 0.65 1.35 4.15 295.3 0.79 0.65 675M 0.48 2.21 - 208.3 - - 1.73 - 308.4 - - 675M 675M 675M 675M 0.28 0.31 3.46 4.17 159.8 0.77 0.63 1.67 4.12 266.3 0.80 0. 1.83 4.22 217.3 0.77 0.66 1.26 4.11 314.9 0.79 0.66 2.88 4.67 175.3 0.78 0.62 1.41 4.41 301.7 0.79 0.65 1.75 4.41 218.57 0.79 0.64 1.21 4.10 315.1 0.79 0.66 of VAE. Meanwhile, we can notice that Send-VAE can significantly speed up the convergence of diffusion models, evidenced by the superior generation performance (narrowing the gFID score from 3.46 to 2.88 for unconditional generation) when training with only 80 epoch. These results demonstrate that Send-VAE is generation-friendly VAE, which can facilitate the learning of diffusion models. Meanwhile, some qualitative results are shown in Fig.1 using Send-VAE and SiT-XL/1. As for reconstruction, we observe that the reconstruction performance of Send-VAE is slightly inferior to that of VA-VAE. We attribute this to the semantic disentangled latent space of Send-VAE, which prevents it from capturing excessive fine-grained low-level details. Besides, we also provide qualitative comparisons among VA-VAE, E2E-VAE and Send-VAE in Fig 3 We generates images from the same label and initial noise using checkpoints trained by 10 epoch, 20 epoch, and 80 epoch, respectively. As we can see, training diffusion models using Send-VAE demonstrate superior image generation quality compared to VA-VAE and E2E-VAE. Meanwhile, Send-VAE can significantly speed up the training process of diffusion models, evidenced by the more structurally meaningful images during early stages of training process. Some visualization results are presented in Fig 4 to show that training diffusion models with Send-VAE can generate high-quality images. 4.4 ABLATION STUDIES In this section, we provide detailed ablation studies to demonstrate the effectiveness of each design in Send-VAE. Unless otherwise specified, we train SiT-B/1 with REPA loss for 80 epoch, and report the downstream unconditional generation performance. Ablation on Depth of Mapper Network. We ablate the depth of our proposed mapper network to analyze its impact on downstream generation performance. As shown in Table 2, mapper with one layer of ViT achieves the best performance (gFID=8.42), outperforming both shallower (0 layer) and"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Qualitative Results on ImageNet 256 256 using Send-VAE and SiT-XL. Table 2: Ablation on the depth of mapper network. Depth gFID sFID IS Prec. Rec. 0 2 9.20 8.42 9.47 7.06 104. 0.73 5.05 108.3 0.74 5.33 100. 0.73 0.57 0.60 0.60 Table 3: Ablation on noise injection. Noise Injection gFID sFID IS Prec. Rec. 8.42 7. 5.05 108.3 0.74 5.37 115.3 0.74 0.60 0.60 deeper (2 layer) configurations. We argue that the insufficient capacity of shallow mapper fails to bridge the representation gap between VAE and visual foundation models, resulting in decrease in the semantic disentanglement ability of VAE. While for the deeper one, it weaken the foundational models impacts on VAE due to the stronger fitting capability. Such experimental results demonstrate the necessity of employing mapper network to bridge representation gap, which can facilitate effective semantic injection. Ablation on Injecting Noise to Latent Representations. Table 3 presents the ablation results of injecting noise to latent representations. As we can see, injecting noise during the alignment process can bring significant performance gains. We attribute its effectiveness to form of data augmentation, which ensures that even with noise injected, the latent representation extracted by the VAE retains rich disentangled semantic information, making it better suited for the denoising process of the downstream diffusion model. Ablation on Vision Foundation Models. We also investigate the influence of vision foundation models and present the ablation results in Table 4. Specifically, we include four types of vision founTable 4: Ablation on different vision foundation models (VFMs) VFMs gFID sFID IS Prec. Rec. CLIP I-JEPA DINOv2 DINOv 9.85 9.70 7.57 7.16 5.59 5. 5.37 5.57 100.8 102.9 115.3 125. 0.71 0.72 0.74 0.75 0.62 0. 0.60 0."
        },
        {
            "title": "Preprint",
            "content": "Table 5: Ablation on the Initialization of VAE."
        },
        {
            "title": "VAE Initialization",
            "content": "gFID sFID IS Prec. Rec. SD-VAE +Lalign IN-VAE +Lalign VA-VAE +Lalign 21.41 11. 17.43 8.25 11.40 7.57 5.30 5. 5.93 4.68 6.58 5.37 65.0 95. 72.7 105.2 93.5 115.3 0.62 0. 0.64 0.74 0.71 0.74 0.63 0. 0.63 0.60 0.59 0.60 dation models, including CLIP Radford et al. (2021), I-JEPA Assran et al. (2023), DINOv2 Oquab et al. (2024), and DINOv3 Simeoni et al. (2025). As we can see, regardless of the type of vision foundation models, adding Lalign consistently improve the generation performance of diffusion models. Among them, the DINO family (DINOv2 and DINOv3) achieves the best performance, which is consistent with the findings of REPA and REPA-E. We argue that the object-centric features of DINO can more effectively facilitate the VAE in learning semantic disentangled latent space, thus resulting in superior generation performance. Ablation on the Initialization of VAE. To demonstrate the generalization of our method to various VAE initialization, we conducted experiments on three commonly used VAEs, including SDVAE AI (n.d.), IN-VAE Leng et al. (2025) and VA-VAE Yao et al. (2025). The results are shown in Table 5. As we can see, across all variations, our Lalign can consistently improves final generation performance, which demonstrates that insensitiveness of our method to the VAE initialization. 4.5 MEASUREMENT OF SEMANTIC DISENTANGLEMENT ABILITY To give system-level measurement of semantic disentanglement capability, we adopt linear probing on attribute prediction benchmarks across distinct domains to measure the semantic disentanglement ability of various VAEs. Specifically, three attribute prediction benchmarks are used to ensure comprehensive evaluation, including CelebA Liu et al. (2015), DeepFashion Liu et al. (2016) and AwA Lampert et al. (2013). We conduct linear probing on the flattened latent representation from VAE encoder and show the results in Table 6. As we can see, among all benchmarks, the performance of attribute prediction is positively correlated with the down-stream generation performance. These results strongly support our hypothesis, and making the linear probing on attribute prediction task suitable metric to evaluate the goodness of VAE for diffusion. Meanwhile, we observe that Send-VAE can significantly enhance the semantic disentanglement ability of VAE and achieve superior generation performance. Table 6: System-level measurement of semantic disentanglement ability of various VAEs. F1 score is adopted for all benchmarks. Benchmarks IN-VAE VA-VAE E2E-VAE Send-VAE 0. 0.1177 0.6441 0.6647 0.1385 0.6623 7. CelebA 0.6222 0.6347 DeepFasion 0.0786 0. AwA gFID 0.5567 0.5948 17.43 11. 8."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we try to answer the question: what properties make VAE generation-friendly. We revisit the role of representation alignment in LDMs and VAEs, highlighting their fundamentally different representational requirements. We hypothesize that VAEs benefit from latents with strong semantic disentanglement rather than purely high-level semantics, which is verified by the strong correlation between the linear separability of low-level attributes within VAE latent space and the generation performance. To address this gap, we introduce Send-VAE, semantic-disentangled VAE explicitly optimized by aligning its latent space with the semantic hierarchy of pre-trained Vision Foundation Models (VFMs) through non-linear mapper network. This design effectively bridges the gap between attribute-level disentanglement and high-level VFM semantics, enabling VAEs to learn structured, fine-grained representations. Empirical results on ImageNet 256256 show that Send-VAE not only accelerates training of flow-based transformers such as SiTs, but also achieves new state-of-the-art performance, with FID scores of 1.21 and 1.75 with and without classifier-free guidance, respectively."
        },
        {
            "title": "REFERENCES",
            "content": "Stability AI. Improved autoencoders ... https://huggingface.co/stabilityai/ sd-vae-ft-mse, n.d. Accessed: April 11, 2025. Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=li7qeBbCR1t. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding In Proceedings of the IEEE/CVF Conference on Computer Vision and predictive architecture. Pattern Recognition, pp. 1561915629, 2023. Lao Beyer, Tianhong Li, Xinlei Chen, Sertac Karaman, and Kaiming He. Highly compressed tokenizer can generate without training. arXiv preprint arXiv:2506.08257, 2025. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1131511325, 2022. Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. In Forty-second International Conference on Machine Learning, 2025a. Hesen Chen, Junyan Wang, Zhiyu Tan, and Hao Li. Sara: Structural and adversarial representation alignment for training-efficient diffusion models. arXiv preprint arXiv:2503.08253, 2025b. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https: //openreview.net/forum?id=YicbFdNTTy. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021."
        },
        {
            "title": "Preprint",
            "content": "Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 2316423173, 2023a. Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389, 2023b. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in neural information processing systems, 32, 2019. Christoph Lampert, Hannes Nickisch, and Stefan Harmeling. Attribute-based classification for zero-shot visual object categorization. IEEE transactions on pattern analysis and machine intelligence, 36(3):453465, 2013. Jaa-Yeon Lee, Byunghee Cha, Jeongsol Kim, and Jong Chul Ye. Aligning text to image in diffusion models is easier than you think. arXiv preprint arXiv:2503.08250, 2025. Xingjian Leng, Jaskirat Singh, Yunzhong Hou, Zhenchang Xing, Saining Xie, and Liang Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers. arXiv preprint arXiv:2504.10483, 2025. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37: 5642456445, 2024. Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: In The Thirteenth International ConferAutoregressive image generation with folded tokens. ence on Learning Representations, 2025. URL https://openreview.net/forum?id= QE1LFzXQPL. Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), December 2015. Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. Deepfashion: Powering robust clothes recognition and retrieval with rich annotations. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pp. 2340. Springer, 2024. Charlie Nash, Jacob Menick, Sander Dieleman, and Peter Battaglia. Generating images with sparse representations. In International Conference on Machine Learning, pp. 79587968. PMLR, 2021."
        },
        {
            "title": "Preprint",
            "content": "Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=a68SUt6zFt. Featured Certification. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=St1giarCHLP. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. Runqian Wang and Kaiming He. Diffuse and disperse: Image generation with representation regularization. arXiv preprint arXiv:2506.09027, 2025. Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. Gigatok: Scaling visual tokenizers to 3 billion parameters for autoregressive image generation. arXiv preprint arXiv:2504.08736, 2025. Jingfeng Yao, Cheng Wang, Wenyu Liu, and Xinggang Wang. Fasterdit: Towards faster diffusion transformers training without architecture modification. Advances in Neural Information Processing Systems, 37:5616656189, 2024. Jingfeng Yao, Bin Yang, and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1570315712, 2025. Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=gzqrANCF4g."
        },
        {
            "title": "Preprint",
            "content": "Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=DJSZGGZYVi. Kaiwen Zha, Lijun Yu, Alireza Fathi, David Ross, Cordelia Schmid, Dina Katabi, and Xiuye Gu. Language-guided image tokenization for generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1571315722, 2025. Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://openreview.net/forum?id=vTBjBtGioE. Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. Image bert pre-training with online tokenizer. In International Conference on Learning Representations. Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and Chang Wen Chen. Sd-dit: Unleashing the power of self-supervised discrimination in diffusion transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 84358445, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 LLM USAGE LLMs are only used to meticulously refine the draft by correcting grammatical errors and improving sentence fluency. During the conceptualization and research design phases of the paper, we do not rely on LLMs; all research ideas and innovations are independently developed by our team."
        }
    ],
    "affiliations": [
        "Kolors Team, Kuaishou Technology"
    ]
}