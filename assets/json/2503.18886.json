{
    "paper_title": "CFG-Zero*: Improved Classifier-Free Guidance for Flow Matching Models",
    "authors": [
        "Weichen Fan",
        "Amber Yijia Zheng",
        "Raymond A. Yeh",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Classifier-Free Guidance (CFG) is a widely adopted technique in diffusion/flow models to improve image fidelity and controllability. In this work, we first analytically study the effect of CFG on flow matching models trained on Gaussian mixtures where the ground-truth flow can be derived. We observe that in the early stages of training, when the flow estimation is inaccurate, CFG directs samples toward incorrect trajectories. Building on this observation, we propose CFG-Zero*, an improved CFG with two contributions: (a) optimized scale, where a scalar is optimized to correct for the inaccuracies in the estimated velocity, hence the * in the name; and (b) zero-init, which involves zeroing out the first few steps of the ODE solver. Experiments on both text-to-image (Lumina-Next, Stable Diffusion 3, and Flux) and text-to-video (Wan-2.1) generation demonstrate that CFG-Zero* consistently outperforms CFG, highlighting its effectiveness in guiding Flow Matching models. (Code is available at github.com/WeichenFan/CFG-Zero-star)"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 6 8 8 8 1 . 3 0 5 2 : r CFG-Zero: Improved Classifier-Free Guidance for Flow Matching Models Weichen Fan1 Amber Yijia Zheng2 Raymond A. Yeh2 Ziwei Liu1,(cid:66) 1S-Lab, Nanyang Technological University 2Department of Computer Science, Purdue University weichen.fan@u.nus.edu, {zheng709,rayyeh}@purdue.edu, ziwei.liu@ntu.edu.sg Figure 1. Comparison for the prompt: dense winter forest with snow-covered branches, the golden light of dawn filtering through the trees, and lone fox leaving delicate paw prints in the fresh snow. Images generated using SD3.5 [5] with CFG and CFG-Zero (Ours)."
        },
        {
            "title": "Abstract",
            "content": "Classifier-Free Guidance (CFG) is widely adopted technique in diffusion/flow models to improve image fidelity and controllability. In this work, we first analytically study the effect of CFG on flow matching models trained on Gaussian mixtures where the ground-truth flow can be derived. We observe that in the early stages of training, when the flow estimation is inaccurate, CFG directs samples toward incorrect trajectories. Building on this observation, we propose CFG-Zero, an improved CFG with two contributions: (a) optimized scale, where scalar is optimized to correct for the inaccuracies in the estimated velocity, hence the in the name; and (b) zero-init, which involves zeroing out the first few steps of the ODE solver. Experiments on text-toimage (Lumina-Next, Stable Diffusion 3, and Flux) and textto-video (Wan-2.1) generation demonstrate that CFG-Zero consistently outperforms CFG, highlighting its effectiveness (Code is available at in guiding Flow Matching models. github.com/WeichenFan/CFG-Zero-star) 1. Introduction Diffusion and flow-based models are the state-of-the-art (SOTA) for generating high-quality images and videos, with recent advancements broadly categorized into Score Matching [10, 27, 30, 3436] and Flow Matching [5, 6, 20, 24, 45] approaches. Flow matching directly predicts velocity, enabling more interpretable transport process and faster convergence compared with score-based diffusion methods. Hence, recent SOTA in text-to-image/video models increasingly adopt flow matching. In this paper, we follow the unifying perspective on diffusion and flow models presented by Lipman et al. [22]. We broadly use the term flow matching models to refer to any model trained using flow matching, where samples are generated by solving an ordinary differential equation (ODE). Next, classifier-free guidance (CFG) [9, 44] is widely used technique in flow matching models to improve sample quality and controllability during generation. In textto-image tasks, CFG improves the alignment between generated images and input text prompts. In other words, CFG is used because the conditional distribution induced by the learned conditional velocity does not fully match with the users intended conditional distribution; see example in Fig. 2. We hypothesize that this mismatch arises from two fundamental factors. First, it may be from dataset limitations, where the users interpretation of text prompt and its corresponding image differs from the dataset distribution. Second, it could result from learning limitation, versarial methods [7] that rely on one-step generation, diffusion models [4] have demonstrated significantly improved performance in generating high-quality samples. Early diffusion models were primarily score-based generative models, including DDPM [10], DDIM [34], EDM [16], and Stable Diffusion [30], which focused on learning the SDEs governing the diffusion process. Next, Flow Matching [21] provides an alternative approach by directly modeling sample trajectories using ordinary differential equations (ODEs) instead of SDEs. This enables more stable and efficient generative processes by learning continuous flow field that smoothly transports samples from prior distribution to the target distribution. Several works, including Rectified Flow [24], SD3 [5], Lumina-Next [45], Flux [20], Vchitect-2.0 [6], LuminaVideo [23] HunyuanVideo [18], SkyReels-v1 [33], and Wan2.1 [39] have demonstrated that ODE-based methods achieve faster convergence and improved controllability in text-to-image and text-to-video generation. As result, Flow Matching has become compelling alternative to stochastic diffusion models, offering better interpretability and training stability. Thus, our analysis is based on Flow Matching models, which aim to provide more accurate classifier-free guidance. Guidance in Diffusion Models. Achieving better control over diffusion models remains challenging yet essential. Early approaches, such as classifier guidance (CG) [4], introduce control by incorporating classifier gradients into the sampling process. However, this method requires separately trained classifiers, making it less flexible and computationally demanding. To overcome these limitations, classifierfree guidance (CFG) [9] was proposed, enabling guidance without the need for an external classifier. Instead, CFG trains conditional and unconditional models simultaneously and interpolates between their outputs during sampling. Despite its effectiveness, CFG relies on an unbounded empirical parameter, known as the guidance scale, which determines how strongly the generated output is influenced by the conditional model. Improper tuning of this scale can lead to undesirable artifacts, either over-saturated outputs with excessive conditioning or weakened generation fidelity due to under-conditioning. In fact, previous studies [1] have observed that CFG estimation does not provide an optimal denoising direction. As result, several studies have explored adaptive or dynamically scaled guidance to address these issues, including ADG [31], Characteristic-Guidance [43], ReCFG [41], Weight-Scheduler [40], and CFG++ [2]. Additionally, AutoG [17] replaces the unconditional model with smaller, less-trained version of the model itself. Other researchers [19] have proposed limiting the use of CFG to specific interval during sampling. (Left) Conditional generation. (Right) CFG generaFigure 2. tion. (Prompt: mysterious underwater city with bioluminescent corals and towering glass domes.) where the learned velocity fails to accurately capture the datasets distribution. In this work, we focus on the latter issue. When the model is underfitted, mismatch exists between the conditional and unconditional predictions during sampling, causing CFG to guide the sample in direction that deviates significantly from the optimal trajectory. Specifically, the velocity estimated by CFG in the first step at x0 may contradict the optimal velocity. This suggests that skipping this prediction could lead to better results. We empirically analyze the effect of CFG when the learned velocity is underfitted, i.e., inaccurate, using mixture of Gaussians as the data distribution. In this setting, the ground-truth (optimal) velocity has closed-form solution, allowing us to compare it with the learned velocity throughout training. Based on the observations, we then propose CFG-Zero, which introduces two key improvements over the vanilla CFG: optimized scale and the zero init technique. In Sec. 4, we provide an empirical analysis to motivate our approach with supporting observations. Furthermore, we validate that the observations go beyond mixture of Gaussians by conducting experiments on ImageNet for the task of class-conditional generation. Finally, in Sec. 5, we apply CFG-Zero to text-guided generation, showing that our method achieves strong empirical performance in guiding SOTA flow matching models. Our main contributions are summarized as follows: We analyze the sources of error in flow-matching models and propose novel approach to mitigate inaccuracies in the predicted velocity. We empirically show that zeroing out the first ODE solver step for flow matching models improves sample quality when the model is underfitted. Extensive experiments validate that CFG-Zero achieves competitive performance in both discrete and continuous conditional generation tasks, demonstrating its effectiveness as an alternative to CFG. 2. Related Work Diffusion and Flow-based Models. Unlike generative adUnlike previous approaches, our work is motivated by an 2 observation that CFG prediction is inaccurate when model is underfitted, and specifically in the first step, i.e., the prediction is even worse than zeroing out the first ODE solver step. Thus, we study the error of CFG within the Flow Matching, where we derive the upper bound of the error term and propose to minimize it. From this analysis, we derive dynamic parameterization technique that adjusts the unconditional output, leading to more stable and effective guidance in Flow Matching based diffusion models. 3. Preliminaries We briefly recap flow matching following the unifying perspective presented in Lipman et al. [22]. Conditional flow matching. Given source distribution p(xy) and an unknown target distribution q(xy), conditional flow matching (CFM) defines probability path pt(xy), where [0, 1] is continuous time variable that interpolates between and q, such that p0(xy) = p(xy) and p1(xy) = q(xy). An effective choice for pt(xy) is the linear probability path pt(xy) (1 t) p(xy) + q(xy), (1) where sample xt = (1 t)x0 + tx1, with x0 p(xy) and x1 q(xy). Next, continuous flow model is trained by learning time-dependent velocity field (xy) that governs the trajectory of over t. Here, the velocity is represented using deep-net with trainable parameters θ. flow matching model is trained by minimizing the CFM loss expressed as dt xt = vθ LCFM(θ) = Et,x0,x1 (cid:13) (cid:13)vθ (cid:0)xty(cid:1) (cid:0)x1 (cid:1)(cid:13) 2 2 . (cid:13) (2) At generation time, new sample can be obtained by using any ODESolver, e.g., the midpoint method [37]. Classifier free guidance (CFG) [9, 44] improves the quality of conditional generation by steering sample toward the given input condition, e.g., class label or text prompt. In CFG, single flow model vθ (xy) is trained to output both conditional and unconditional velocity fields. This is done by introducing = , which does not contain any conditioning information. At inference, the guided velocity field is formed by (xy) (1 w) vθ ˆvθ (xy = ) + vθ (xy), (3) where ω is the guidance scale and denotes the null condition. When ω = 1, it is equivalent sampling only using the conditional velocity vθ Closed form velocity for Gaussian distributions. When both the source and target consist of Gaussian mixtures, the optimal velocity has closed form. (xy), i.e., no guidance. For example, let the source distribution = (0, I) and target distribution = (µ, I) both be single Gaussian. Algorithm 1 CFG-Zero 1: Input: Trained velocity vθ, noise sample x0, guidance weight ω, and number of steps to zero out K. # equals to 1 by default vθ # Optimized scale () ()) + ω vθ (y) 2: vθ ()2 (y)vθ vθ 3: vt() (1 ω) 4: # Solve ODE 5: for = 0 to do if < then 6: 7: 8: 9: end if 10: 11: end for 12: Return generated sample xT xt+1 xt # Zero-init else xt+1 ODEStep(vt(), xt) Then, the corresponding optimal velocity (x) = (cid:104) (2t 1)I (cid:105)(cid:104) (1 t)2I + t2I (cid:105)1 (x tµ) + µ. (4) With closed-form optimal , we can now empirically study the gap between the optimal and learned velocity (cid:13) (cid:13) throughout training, specifically in the underfiting regime, which motivated our proposed CFG-Zero. () vθ ()(cid:13) (cid:13)v 4. Methodology We propose guidance algorithm CFG-Zero with two improvements from the standard CFG: (a) optimized scale, where scalar parameter is optimized to compensate for inaccuracies in the learned velocity (Sec. 4.1); (b) zeroinit, where we zero out the first step of the ODE solver (Sec. 4.2). These modifications can be easily integrated into the existing CFG code base and introduce minimal additional computational cost. The overall algorithm is summarized in Alg. 1. 4.1. Optimizing an additional scaler in CFG As motivated in the introduction, we aim to study the use of CFG in the setting where the velocity is underfitted, i.e., CFG is used in the hope that the guided velocity vθ approximates the ground-truth flow v, i.e., (xy) vθ (xy). (5) To further improve this approximation, we introduce an optimizable scaler R>0 to CFG, (xy) (1 ω) vθ vθ = ω vθ (x) + ω vθ (x) + (1 + ω)vθ (xy) (xy), (6) (7) ground-truth. As shown in Fig. 3(b), during the early stages of training, the difference between both types of estimated velocities and the ground-truth velocity at = 0 is greater than when just using an all-zero velocity (do nothing), i.e., (cid:13) (cid:13)vθ 0(xy) 0(xy)(cid:13) 2 2 0 (cid:13) 0(xy)2 2 . (12) Based on this observation, we propose zero-init, which zeros out the velocity for the first few steps of the ODESolver. As an estimation of all zeros for the velocity would be more accurate. Next, we investigate whether this behavior is specific to the small scale mixture of Gaussians dataset or could be generalized to real data. Specifically, we consider the experiment of ImageNet-256. 4.3. Validation beyond Mixture of Gaussians We experiment on the ImageNet-256 [3] benchmark where we train class-conditioned flow model, i.e., given an input class label, generate an image from the class. We report results across multiple evaluation metrics, including Inception Score (IS) [32], Frechet Inception Distance (FID) [8], sFID [26], Precision, and Recall. Validating Zero-Init. To study the impact of zero-init on real dataset with flow matching models, we compare samples generated by the standard CFG with and without the zero-init throughout the training epochs, as shown in Tab. 1. Theoretically, if model is well-trained, zero-init could degrade its performance. For quick validation, we use smaller version of DiT [27] with 400M parameters as our base model and train it from scratch on the ImageNet-256 dataset using the Flow Matching loss [21]. The results show that CFG with Zero-Init consistently outperforms standard CFG up to 160 epochs. This indicates that zeroing out the first step of the ODESolver is suitable during early training. Beyond 160 epochs, standard CFG surpasses Zero-Init, which is consistent with our hypothesis that as the velocity is trained to be more accurate, the benefit of zero-init lessens. Validation of CFG-Zero. To highlight the differences between our method and other classifier-free guidance approaches, we conduct experiments using pre-trained SiTXL model [25] (700M parameters) on the ImageNet-256 benchmark. The model has not yet reached the turning point mentioned earlier. All experiments are performed using the standard guidance scale. In Tab. 2, our results demonstrate that CFG-Zero achieves the best overall performance, outperforming both CFG++ [2] and ADG [31] across key metrics. Specifically, CFG-Zero attains the highest Inception Score of 258.87, highlighting its ability to generate diverse and high-quality images. Furthermore, CFG-Zero achieves the best FID Score of 2.10 and sFID Score of 4.59, indicating improved perceptual quality and stronger alignment with the target (a) (b) Figure 3. Results on mixture of Gaussians in R2. Left: The JensenShannon divergence between the models final flow sample distribution and the target distribution v.s. training epoch. Right: The velocity error norm vθ 0 , with the ground truth norm shown in gray v.s. training epoch. 0 where ω 1 + ω. The choice of learning scaler is inspired by classifier guidance [4], where they introduce scaling factor to balance between the gradient and the unconditioned direction. The remaining challenge is how to optimize s. In hypothetical case where we do have groundtruth flow , then one can formulate the approximation in Eq. (5) as least-squares, i.e., minimizing over the loss L(s) (cid:13) (cid:13)vθ (xy) t (xy)(cid:13) 2 2 . (cid:13) (8) However, as is unknown, we instead minimize over an upperbound of L(s) in Eq. (8) established using triangle inequality as follows: L(s) = (cid:13) (cid:13) (cid:13)ωsvθ (xy)(cid:13) 2 2 + (cid:13)vθ (cid:13) + ω (cid:13) (x) + (1 + w)vθ (xy)2 (cid:13)vθ 2 (xy) (xy)(cid:13) 2 (cid:13) 2 (xy) vθ (x)(cid:13) 2 2 . (cid:13) (9) Observe that only the last term has dependencies on s, i.e., optimizing Eq. (9) is equivalent to (cid:13) (cid:13)vθ (xy) vθ (x)(cid:13) 2 2 , (cid:13) min (10) where the solution is projection of the condition velocity onto the unconditional velocity, i.e., = (vθ (x)vθ (x))/(cid:13) (cid:13)vθ (x)(cid:13) 2 (cid:13) . (11) We empirically validate the approach on toy example consisting of Gaussian mixture, with results shown in Fig. 3(a), where we observe that samples generated with CFG-Zero more closely match the target distribution than those with CFG. 4.2. Zero-init for ODE Solver During the empirical validation of s, we also studied how well the guided velocity from CFG-Zero matches the 4 Epochs Methods Metrics Model Method Aesthetic Score Clip Score IS FID sFID Precision Recall 10 20 40 160 CFG Zero-Init CFG Zero-Init CFG Zero-Init CFG Zero-Init CFG Zero-Init 53.27 52.78 257.23 255.79 339.39 338.40 383.06 383.45 222.13 218.90 28.57 28. 11.00 10.65 12.61 12.29 13.53 12.18 2.84 2.85 18.52 17.32 11.64 10. 11.17 10.47 10.99 10.39 4.56 4.97 0.61 0.62 0.92 0.92 0.94 0. 0.94 0.94 0.81 0.80 0.36 0.37 0.24 0.25 0.23 0.24 0.24 0. 0.56 0.56 Table 1. Validation on ImageNet-256. We evaluate model at different training stages and observe turning point at 160 epochs, where zero-init results in poorer performance when the model converges. This experiment validates that high-dimensional models also suffer from inaccuracies in initial sampling. Method IS FID sFID Precision Recall Baseline w/ CFG w/ ADG [31] w/ CFG++ [2] w/ CFG-Zero 125.13 257.03 257.92 257.04 258.87 9.41 2.23 2.37 2.25 2.10 6.40 4.61 5.51 4.65 4.59 0.67 0.81 0.80 0.79 0. 0.67 0.59 0.58 0.57 0.61 Comparison of different guidance strategy on Table 2. ImageNet-256 benchmark. Lower FID is better () and higher IS is better (). Baseline here denotes using the conditional prediction only. distribution. In terms of fidelity metrics, our method maintains competitive Precision of 0.80comparable to both CFG and ADGwhile achieving the highest Recall of 0.61. This suggests that our approach better captures the underlying data distribution, leading to more representative and well-balanced samples. (Both ADG [31] and CFG++ [2] are not designed for classifier-free guidance in Flow Matching, and therefore may not perform well.) 5. Experiments In this section, we evaluate CFG-Zero on large-scale models for text-to-image (Lumina-Next [45], SD3 [5], SD3.5 [5], Flux [20]) and text-to-video (Wan2.1 [39] generation. Note: Flux is CFG-distilled, so directly applying classifier-free guidance may yield different results. 5.1. Text-to-Image Generation To evaluate the effectiveness of our proposed method, CFG-Zero, in continuous class-conditional image generation, we conducted experiments using four state-of-the-art flow matching models: Lumina-Next [45], SD3 [5], SD3.5 [5], and Flux [20]. These models were selected for their strong performance in class-conditional image synthesis. We applied both CFG-Zero and the standard Lumina-Next [45] SD3 [5] SD3.5 [5] Flux [20] CFG CFG-Zero CFG CFG-Zero CFG CFG-Zero CFG CFG-Zero 6.85 7.03 6.73 6.80 6.96 7. 7.06 7.12 34.09 34.37 34.00 34.11 34.60 34.68 34.60 34.69 Table 3. Quantitative evaluation of Text-to-Image generation, using Lumina-Next, Stable Diffusion 3, Stable Diffusion 3.5, and Flux. The evaluation is based on Aesthetic Score and CLIP Score as key metrics. Results indicate that CFG-Zero consistently enhances image quality and improves alignment with textual prompts across different models. CFG under default settings on diverse set of self-curated prompts, allowing us to provide fair comparisons. Quantitative Evaluation. Tab. 3 presents the quantitative comparison of CFG-Zero and CFG across all tested models. The results indicate that CFG-Zero consistently achieves superior performance, as evidenced by higher Aesthetic Score [38] and CLIP Score [28, 29]. The improvement in Aesthetic Score suggests that CFG-Zero enhances the visual appeal of generated images, producing outputs with more coherent textures, lighting, and structure. Additionally, the increase in CLIP Score demonstrates that CFG-Zero ensuring that generated images better capture the semantics of the given prompts. These results validate the effectiveness of our proposed modifications in refining the quality of diffusion-based generation. text-image alignment, improves Qualitative Evaluation. Fig. 4 provides comparisons of the images generated using CFG-Zero and vanilla CFG. Our method produces high-fidelity outputs that exhibit richer details, sharper textures, and better preservation of object structures compared to the baseline CFG. Notably, CFG-Zero mitigates common artifacts observed in CFG-generated images, particularly those that introduce unintended distortions or elements unrelated to the given prompt. This reduction in artifacts highlights the robustness of CFG-Zero in preserving semantic consistency, ensuring that generated images adhere more closely to the given prompts. Additionally, we observe that CFG-Zero have better color consistency and level of detail, reducing blurry artifacts that are sometimes present in CFG-based outputs. These improvements are particularly evident in complex prompts that require the precise rendering of intricate textures or fine-grained semantic attributes. Further visual comparisons and additional generated samples can be found in the Appendix. 5 Figure 4. Qualitative comparisons between CFG and CFG-Zero. Experiments are conducted using Lumina-Next, Stable Diffusion 3, and Stable Diffusion 3.5, with each model evaluated under its recommended optimal sampling steps and guidance scale settings. CFG results are shown in orange and Ours are highlighted in green boxes. Method Color Shape Texture Spatial Lumina-Next [45] + CFG-Zero SD3 [5] + CFG-Zero SD3.5 [5] + CFG-Zero 0.51 0.52 0.81 0.83 0.76 0. 0.34 0.36 0.57 0.58 0.59 0.60 0.41 0.45 0.71 0.72 0.70 0. 0.19 0.29 0.31 0.31 0.27 0.28 Table 4. Quantitative evaluation on T2I-CompBench [12], using Lumina-Next, Stable Diffusion 3, and Stable Diffusion 3.5. Compared to CFG, CFG-Zero demonstrates consistent improvements across all evaluated aspects. Benchmark Results. We compare our method against standard CFG across three different Flow Matching models using T2I-CompBench [11, 12]. As shown in Tab. 4, integrating CFG-Zero leads to notable improvements in Color, Shape, and Texture quality in generated images. Meanwhile, the Spatial dimension remains comparable to the indicating that CFG-Zero improves image fibaseline, delity without compromising structural coherence. User Study. To further assess CFG-Zero, we conduct user study to compare its performance against standard CFG across various flow matching models. Participants were presented with image pairs generated using both CFG-Zero and CFG and were asked to evaluate them based on three key aspects: detail preservation, color conFigure 5. User study on Lumina-Next, Stable Diffusion 3, Stable Diffusion 3.5, and Flux. The win rate of our method compared to CFG is presented. sistency, and image-text alignment. The overall preference score was then computed as the percentage of times CFG-Zero was favored over CFG. Fig. 5 summarizes the results. CFG-Zero demonstrates clear advantage over CFG across all tested models. Notably, our method achieves the highest win ratio on SD3.5, with an overall win ratio score of 72.15%, primar6 Method Total Score subject consistency aesthetic quality imaging quality color spatial relationship temporal style motion smoothness Vchitect-2.0 [2B] [6] CogVideoX-1.5 [5B] [42] Wan2.1 [14B] [39] w/ CFG-Zero Wan2.1 [1B] [39] w/ CFG-Zero 81.57 82.17 83.99 84.06 80.52 80.91 61.47 96. 93.33 93.34 93.89 94.93 65.60 62.79 69.13 69.22 61.67 64.24 86.87 65. 67.48 67.55 65.40 68.13 86.87 87.55 83.43 85.39 87.57 89.36 54.64 80. 80.46 79.28 72.75 73.84 25.56 25.19 25.90 25.98 24.13 23.36 97.76 98. 98.05 98.00 97.24 98.16 Table 5. Qualitative evaluation on VBench [13]. We use the Wan-2.1 [39] model as our base model. Compared to vanilla CFG, CFG-Zero improves both frame quality and overall video smoothness. Figure 6. Qualitative comparisons between CFG-Zero and CFG. Experiments are conducted using Wan-2.1 [1B] [39], under its recommended optimal sampling steps and guidance scale settings. ily driven by significant improvements in detail preservation (82.33%) and color fidelity (81.45%). This suggests that CFG-Zero effectively enhances fine-grained structures and maintains consistent color distributions compared to its baseline. In terms of image-text alignment, CFG-Zero also performs favorably, surpassing CFG in most cases. Specifically, with CFG++, SD3.5 exhibits strong text-image consistency (53.33% win ratio), indicating that our method improves coherence between generated images and textual prompts across different architectures. 5.2. Text-to-Video Generation To further evaluate the effectiveness of our proposed method, we further conduct experiments on the text-tovideo generation task, using the most recent state-of-the-art model, Wan-2.1 [39]. Benchmark Results. As shown in Tab. 5, we evaluate our method using all metrics from VBench [13, 14]. Compared to CFG, Wan-2.1 [1.3B] equipped with CFG-Zero achieves higher Total Score. Specifically, our method improves Aesthetic Quality by 2.57 and Imaging Quality by 2.73, indicating that CFG-Zero enhances video fidelity. Additionally, CFG-Zero improves Motion Smoothness (+0.92) and Spatial Relationship (+1.09), demonstrating superior temporal coherence and spatial understanding. However, we also observe decrease in Temporal Style (- 0.77), which can be attributed to the base models poor capability in generating stylized videos. Qualitative Evaluation. Fig. 6 presents visual comparison between CFG-Zero and CFG on Wan2.1-14B [39], demonstrating that the refined velocity produced by our method leads to more plausible content with natural motion. 5.3. Ablation Studies Different Sampling Steps. To further investigate the impact of sampling steps, we conduct experiments using strong baseline, SD3.5. As shown in Fig. 7, our method consistently enhances both image quality and text-matching accuracy across different sampling steps, demonstrating its Metrics w/ CFG w/ CFG-Zero w/ Scaler w/ CFG-Zero Aesthetic Score CLIP Score 6.96 34.60 7.00 34.65 6.96 34. 7.10 34.68 Table 6. Effectiveness of CFG-Zero. Comparison of vanilla CFG, CFG with zero-init, dynamic scaling, and CFG-Zero, highlighting the impact of zero-init and dynamic scaling in improving performance. Model Zero-out / Total (steps) Aesthetic Score Clip Score Lumina-Next [45] SD3 [5] SD3.5 [5] First 3 / 30 First 2 / 30 First 1 / 30 First 3 / 28 First 2 / 28 First 1 / 28 First 3 / 28 First 2 / 28 First 1 / 28 6.78 7.06 7.03 6.95 6.98 6. 6.78 6.99 7.10 32.86 34.73 34.37 34.01 34.33 34.11 34.02 34.54 34.68 Table 7. Ablation study on zero-out steps. For SD3.5 [5], more initial zero-out steps lead to worse performance, while LuminaNext [45] and SD3 [5] achieve the highest Aesthetic Score and Clip Score with first 7% zero out. Resolution 81x1280x720 81x832x480 FLOPs Memory 19.4 18.46 MB 0.84 8.00 MB 1024x1024 512x512 1.6e3 4.1e4 64 KB 16 KB Table 8. Computational costs. FLOPs [15] and GPU memory usage of our method for 5-second video generation at 720p/480p using Wan2.1 [39], and at 1024/512 resolution using SD3 [5]. The results indicate that for some models, zeroing out can be beneficial not only in the first step but also in the initial few steps. Specifically, both Lumina-Next and SD3 achieve optimal performance when the first 2 steps are zeroed out. However, SD3.5 exhibits decline in performance when higher proportion of initial steps are zeroed out, suggesting that SD3.5 is well-trained and approaching convergence. Computational Cost. Tab. 8 summarizes the FLOPs and GPU memory usage of our method. For 720p and 480p resolutions using Wan2.1, our method requires 19.4M and 0.84M FLOPs, with memory usage of 18.46MB and 8.00MB. For SD3, the computational cost is significantly lower at 10241024 and 512512, requiring only 1.6e3M and 4.1e4M FLOPs, with memory usage of 64KB and 16KB. These results confirm that CFG-Zero introduces minimal computational costs. 6. Conclusion (a) (b) Figure 7. Abalation study on different sampling steps. Comparison of CLIP Score and Aesthetic Score between our method and CFG across different sampling steps. (a) (b) Figure 8. Abalation study on different guidance scale. Comparison of CLIP Score and Aesthetic Score between our method and CFG across different guidance scale. robustness in varied sampling settings. Different Guidance Scale. As shown in Fig. 8, we conduct an experiment using SD3.5 to analyze the impact of different guidance scales on our method. The results in sub-figure (a) demonstrate that CFG-Zero achieves higher CLIP scores than CFG across all guidance scales, validating its effectiveness in improving image-text alignment. Similarly, sub-figure (b) shows that our method consistently attains higher aesthetic scores, highlighting its ability to generate visually appealing images. Effectiveness of CFG-Zero. We conduct an ablation study using SD3.5 as the baseline to assess the impact of each component in CFG-Zero. As shown in Tab. 6, we compare four variants: vanilla CFG, CFG with zeroinit, pure optimized scaler, and CFG-Zero. The optimized scaler reduces the gap between predicted and true velocity, enhancing stability, while zero-init improves performance by skipping the first step. Combining both, CFG-Zero achieves the highest Aesthetic Score (7.10) and CLIP Score (34.68), outperforming all variants. These results confirm that both modifications help to enhance image quality and text alignment. Effect of Zeroing Out Initial Steps. To assess whether extending the zero-out strategy beyond the first step can further improve performance, we conduct an ablation study using Lumina-Next, SD3, and SD3.5, as shown in Tab. 7. We introduce CFG-Zero, an improved classifier-free guidance method for flow-matching diffusion models. CFG-Zero addresses CFGs limitations with two key techniques: (1) an optimized scale factor for accurate velocity estimation and (2) zero-init technique to stabilize early sampling. Theoretical analysis and extensive experiments on text-to-image (SD3.5, Lumina-Next, Flux) and text-tovideo (Wan-2.1) models show that CFG-Zero outperforms standard CFG, achieving higher aesthetic scores, better text alignment, and fewer artifacts. Ablation studies further validate the effectiveness of both components in improving sample quality without significant computational cost."
        },
        {
            "title": "References",
            "content": "[1] Arwen Bradley and Preetum Nakkiran. free guidance is predictor-corrector. arXiv:2408.09000, 2024. 2 ClassifierarXiv preprint [2] Hyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. CFG++: Manifold-constrained In Int. Conf. classifier free guidance for diffusion models. Learn. Represent., 2025. 2, 4, 5 [3] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In IEEE Conf. Comput. Vis. Pattern Recog., 2009. 4 [4] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat GANs on image synthesis. In Adv. Neural Inform. Process. Syst., 2021. 2, 4 [5] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Int. Conf. Mach. Learn., 2024. 1, 2, 5, 6, [6] Weichen Fan, Chenyang Si, Junhao Song, Zhenyu Yang, Yinan He, Long Zhuo, Ziqi Huang, Ziyue Dong, Jingwen He, Dongwei Pan, et al. Vchitect-2.0: Parallel transformer for scaling up video diffusion models. arXiv preprint arXiv:2501.08453, 2025. 1, 2, 7 [7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 2020. 2 [8] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local Nash equilibrium. In Adv. Neural Inform. Process. Syst., 2017. 4 [9] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 1, 2, 3 [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Adv. Neural Inform. Process. Syst., 2020. 1, 2 [11] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2I-CompBench: comprehensive benchmark for open-world compositional text-to-image generation. In Adv. Neural Inform. Process. Syst., 2023. 6, 13 [12] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2I-CompBench++: An Enhanced and Comprehensive Benchmark for Compositional Text-toImage Generation . IEEE Trans. Pattern Anal. Mach. Intell., 2025. 6, 12, [13] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In IEEE Conf. Comput. Vis. Pattern Recog., 2024. 7 [14] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, YingCong Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench++: Comprehensive and versatile bencharXiv preprint mark suite for video generative models. arXiv:2411.13503, 2024. 7 [15] Vivek Jain, Charles Schmidt, Paritosh Goyal, Cliff Young, et al. Benchmarking deep learning inference: Characteristics of AI workloads on edge vs. cloud. In International Symposium on Workload Characterization (IISWC). IEEE, 2020. 8 [16] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Adv. Neural Inform. Process. Syst., 2022. 2 [17] Tero Karras, Miika Aittala, Tuomas Kynkaanniemi, Jaakko Lehtinen, Timo Aila, and Samuli Laine. Guiding diffusion model with bad version of itself. In Adv. Neural Inform. Process. Syst., 2024. 2 [18] Weijie Kong et al. Hunyuanvideo: systematic framework for large video generative models, 2024. 2 [19] Tuomas Kynkaanniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko Lehtinen. Applying guidance in limited interval improves sample and distribution quality in diffusion models. In Adv. Neural Inform. Process. Syst., 2025. [20] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1, 2, 5 [21] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In Int. Conf. Learn. Represent., 2023. 2, 4 [22] Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky TQ Chen, David LopezPaz, Heli Ben-Hamu, and Itai Gat. Flow matching guide and code. arXiv preprint arXiv:2412.06264, 2024. 1, 3 [23] Dongyang Liu, Shicheng Li, Yutong Liu, Zhen Li, Kai Wang, Xinyue Li, Qi Qin, Yufei Liu, Yi Xin, Zhongyu Li, Bin Fu, Chenyang Si, Yuewen Cao, Conghui He, Ziwei Liu, Yu Qiao, Qibin Hou, Hongsheng Li, and Peng Gao. Luminavideo: Efficient and flexible video generation with multiscale Next-DiT, 2025. 2 [24] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In Int. Conf. Learn. Represent., 2023. 1, 2 [25] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. SiT: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In Eur. Conf. Comput. Vis., 2024. 4 [26] Pierre Nguyen, Arthur Leclaire, Leo Gautron, Philippe Robert, and Nicolas Papadakis. Sliced Wasserstein generative models. In Int. Conf. Learn. Represent., 2022. 4 [27] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Int. Conf. Comput. Vis., 2023. 1, 9 [44] Qinqing Zheng, Matt Le, Neta Shaul, Yaron Lipman, Aditya Grover, and Ricky TQ Chen. Guided flows for genarXiv preprint erative modeling and decision making. arXiv:2311.13443, 2023. 1, 3 [45] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making LuminaarXiv preprint T2X stronger and faster with Next-DiT. arXiv:2406.18583, 2024. 1, 2, 5, 6, 8 [28] William Peebles, Shuang Li, Michal Lukasiewicz, Richard Zhang, Alexei Efros, and Eli Shechtman. Image reward: Learning and evaluating human preferences for textto-image generation. In Adv. Neural Inform. Process. Syst., 2023. 5 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual In Int. Conf. models from natural language supervision. Mach. Learn., 2021. 5 [30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., 2022. 1, [31] Seyedmorteza Sadat, Otmar Hilliges, and Romann Weber. Eliminating oversaturation and artifacts of high guidance scales in diffusion models. In Int. Conf. Learn. Represent., 2024. 2, 4, 5 [32] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques In Adv. Neural Inform. Process. Syst., for training GANs. 2016. 4 [33] SkyReels-AI. SkyReels v1: Human-centric video foundation model. https://github.com/SkyworkAI/ SkyReels-V1, 2025. 2 [34] Jiaming Song, Chenlin Meng, and Stefano Ermon. DenoisIn Int. Conf. Learn. Repreing diffusion implicit models. sent., 2021. 1, 2 [35] Yang Song and Stefano Ermon. Generative modeling by esIn Adv. Neural timating gradients of the data distribution. Inform. Process. Syst., 2019. [36] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In Int. Conf. Learn. Represent., 2021. 1 [37] Endre Suli and David Mayers. An introduction to numerical analysis. Cambridge University Press, 2003. 3 [38] Hossein Talebi and Peyman Milanfar. Nima: Neural image assessment. IEEE Trans. Image Process., 2018. [39] Wan Team. Wan: Open and advanced large-scale video generative models. 2025. 2, 5, 7, 8, 13, 14, 15 [40] WANG Xi, Nicolas Dufour, Nefeli Andreou, CANI MariePaule, Victoria Fernandez Abrevaya, David Picard, and Vicky Kalogeiton. Analysis of classifier-free guidance weight schedulers. Trans. Mach. Learn. Res., 2024. 2 [41] Mengfei Xia, Nan Xue, Yujun Shen, Ran Yi, Tieliang Gong, and Yong-Jin Liu. Rectified diffusion guidance for conditional generation. arXiv preprint arXiv:2410.18737, 2024. 2 [42] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. CogVideoX: Text-to-video In Int. Conf. diffusion models with an expert transformer. Learn. Represent., 2025. 7 [43] Candi Zheng and Yuan Lan. Characteristic guidance: Nonlinear correction for diffusion model at large guidance scale. arXiv preprint arXiv:2312.07586, 2023."
        },
        {
            "title": "Appendix",
            "content": "A1. Additional Experiments A1.1. Experiments on Mixed Gaussian Comparison of flow trajectory. We present the flow sampling trajectories with 10 steps of different methods in Fig. A1. As shown in the last column, samples guided by CFG move across the target distribution, while using only Cond leads to high variance in the sampled distribution. In contrast, CFG-Zero effectively guides the samples toward the target distribution without excessive variance. d r - C (a) = 0 (b) = 0.2 (c) = 0.4 (d) = 0.6 (e) = 0.8 (f) = 1. Figure A1. Flow sampling trajectory. Each panel shows the sample trajectories at different time step. Ablation study on the number of zero-Initialization steps is presented in Fig. A2. Specifically, we initialize the first 1, 2, 3, or 4 steps with zeros and observe that during the early training epochs, increasing the number of zero-initialized steps can be beneficial. However, as the number of zero-init steps increases, the learned model achieves better velocity compared to using zero initialization. At this stage, it becomes better to avoid zeroing out additional steps. (a) Zero-init vθ 0 (b) Zero-init vθ 0 and vθ 1 (c) Zero-init vθ 0 to vθ 2 (d) Zero-init vθ 0 to vθ 3 Figure A2. Ablation study of zero-init steps. 11 A1.2. Experiments on Text-to-Video Generation A2. Additional Visual Results In this section, we provide additional visual comparisons between our method and CFG. Fig. A3 presents videos generated by Wan2.1-1B, while Fig. A4 showcases those produced by the larger Wan2.1-14B model. Compared to CFG, our videos exhibit finer details, more vibrant colors, and smoother motion. Figs. A5 to A8 provide qualitative comparisons between CFG-Zero and CFG. All results are shown without cherry-picking. A3. Implementation Details We first present our code, which can be easily integrated with any Flow-Matching-based model. def optimized_scale(positive_flat, negative_flat): # Calculate dot production dot_product = torch.sum(positive_flat * negative_flat, dim=1, keepdim=True) # Squared norm of uncondition squared_norm = torch.sum(negative_flat ** 2, dim=1, keepdim=True) + 1e-8 # st_star = v_condˆT * v_uncond / v_uncondˆ2 st_star = dot_product / squared_norm return st_star # Get the velocity prediction noise_pred_uncond, noise_pred_text = model(...) positive = noise_pred_text.view(Batchsize,-1) negative = noise_pred_uncond.view(Batchsize,-1) # Calculate the optimized scale st_star = optimized_scale(positive,negative) # Reshape for broadcasting st_star = st_star.view(Batchsize, 1, 1, 1) # Perform CFG-Zero* sampling if sample_step == 0: # Perform zero init noise_pred = noise_pred_uncond * 0. else: # Perform optimized scale noise_pred = noise_pred_uncond * st_star + guidance_scale * (noise_pred_text - noise_pred_uncond * st_star) A3.1. Text-to-Image Details Quantitative Evaluation. We evaluate Lumina-Next, Stable Diffusion 3, Stable Diffusion 3.5, and De-distill Flux on our self-curated text prompt benchmark, which consists of 200 short and long prompts covering diverse range of objects, animals, and humans. Each model is assessed using its default optimal settings. For fair comparison, we generate 10 images per prompt for each model. Benchmark Results. We compare our method with CFG using Lumina-Next, SD3, and SD3.5 on T2I-CompBench [12], available at https://github.com/Karine-Huang/T2I-CompBench/tree/main. Each image is generated 10 times with different random seeds to ensure fair comparison, and all models are evaluated using their optimal settings. User Study. Our user study includes 76 participants, all familiar with text-to-image generation. Each participant an12 swers questionnaire consisting of 25 questions, with each question randomly sampled from our generated images in T2ICompBench [11, 12] to ensure fair comparisons. A3.2. Text-to-Video Details We evaluate Wan2.1 [39] both quantitatively and qualitatively, with all videos generated using the default settings specified in the official repository [39] (https://github.com/Wan-Video/Wan2.1). The VBench evaluation strictly follows the official guidelines (https://github.com/Vchitect/VBench/tree/master). 13 Figure A3. Additional visual results. Videos generated by Wan2.1-1B [39] 14 Figure A4. Additional visual results. Videos generated by Wan2.1-14B [39] Figure A5. Additional visual results. Qualitative comparison between CFG (left) and CFG-Zero (right). (Images generated by SD3.) 16 Figure A6. Additional visual results. Qualitative comparison between CFG (left) and CFG-Zero (right). (Images generated by LuminaNext.) 17 Figure A7. Additional visual results. Qualitative comparison between CFG (left) and CFG-Zero (right). (Images generated by LuminaNext.) Figure A8. Additional visual results. Qualitative comparison between CFG (left) and CFG-Zero (right). (Images generated by SD3.5.)"
        }
    ],
    "affiliations": [
        "Department of Computer Science, Purdue University",
        "S-Lab, Nanyang Technological University"
    ]
}