{
    "paper_title": "LettinGo: Explore User Profile Generation for Recommendation System",
    "authors": [
        "Lu Wang",
        "Di Zhang",
        "Fangkai Yang",
        "Pu Zhao",
        "Jianfeng Liu",
        "Yuefeng Zhan",
        "Hao Sun",
        "Qingwei Lin",
        "Weiwei Deng",
        "Dongmei Zhang",
        "Feng Sun",
        "Qi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "User profiling is pivotal for recommendation systems, as it transforms raw user interaction data into concise and structured representations that drive personalized recommendations. While traditional embedding-based profiles lack interpretability and adaptability, recent advances with large language models (LLMs) enable text-based profiles that are semantically richer and more transparent. However, existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors. In this paper, we introduce LettinGo, a novel framework for generating diverse and adaptive user profiles. By leveraging the expressive power of LLMs and incorporating direct feedback from downstream recommendation tasks, our approach avoids the rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ Direct Preference Optimization (DPO) to align the profile generator with task-specific performance, ensuring that the profiles remain adaptive and effective. LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference data derived from task performance. Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness. This work enhances profile generation as a key innovation for next-generation recommendation systems."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 0 3 8 1 . 6 0 5 2 : r LettinGo: Explore User Profile Generation for Recommendation System Lu Wang Microsoft Corporation Beijing, China wlu@microsoft.com Pu Zhao Microsoft Corporation Beijing, China puzhao@microsoft.com Hao Sun Microsoft Corporation Beijing, China hasun@microsoft.com Dongmei Zhang Microsoft Corporation Beijing, China dongmeiz@microsoft.com Di Zhang Peking University Beijing, China zhangdi@stu.pku.edu.cn Jianfeng Liu Microsoft Corporation Beijing, China jianfengliu@microsoft.com Qingwei Lin Microsoft Corporation Beijing, China qlin@microsoft.com Feng Sun Microsoft Corporation Beijing, China sunfeng@microsoft.com Fangkai Yang Microsoft Corporation Beijing, China fangkaiyang@microsoft.com Yuefeng Zhan Microsoft Corporation Beijing, China yuefzh@microsoft.com Weiwei Deng Microsoft Corporation Beijing, China dedeng@microsoft.com Qi Zhang Microsoft Corporation Beijing, China qizhang@microsoft.com"
        },
        {
            "title": "Abstract",
            "content": "User profiling is pivotal for recommendation systems, as it transforms raw user interaction data into concise and structured representations that drive personalized recommendations. While traditional embedding-based profiles lack interpretability and adaptability, recent advances with large language models (LLMs) enable text-based profiles that are semantically richer and more transparent. However, existing methods often adhere to fixed formats that limit their ability to capture the full diversity of user behaviors. In this paper, we introduce LettinGo, novel framework for generating diverse and adaptive user profiles. By leveraging the expressive power of LLMs and incorporating direct feedback from downstream recommendation tasks, our approach avoids the rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ Direct Preference Optimization (DPO) to align the profile generator with task-specific performance, ensuring that the profiles remain adaptive and effective. LettinGo operates in three stages: (1) exploring diverse user profiles via multiple LLMs, (2) evaluating profile quality based on their impact in recommendation systems, and (3) aligning the profile generation through pairwise preference Work done during internship at Microsoft. data derived from task performance. Experimental results demonstrate that our framework significantly enhances recommendation accuracy, flexibility, and contextual awareness. This work enhances profile generation as key innovation for next-generation recommendation systems. CCS Concepts Information systems Online advertising; Recommender systems."
        },
        {
            "title": "Keywords",
            "content": "Generative Recommender Model, User Preference Learning, Large Language Models ACM Reference Format: Lu Wang, Di Zhang, Fangkai Yang, Pu Zhao, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qingwei Lin, Weiwei Deng, Dongmei Zhang, Feng Sun, and Qi Zhang. 2025. LettinGo: Explore User Profile Generation for Recommendation System. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining V.2 (KDD 25), August 37, 2025, Toronto, ON, Canada. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/ 3711896.3737024. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. KDD 25, Toronto, ON, Canada. 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1454-2/25/08 https://doi.org/10.1145/3711896.3737024."
        },
        {
            "title": "1 Introduction",
            "content": "User profiling is core element for modern recommendation systems, providing structured representations of user preferences and behaviors that drive personalized recommendations. By summarizing user interaction histories, profiles distill essential behavioral patterns from noisy raw data, enabling recommendation systems to deliver more accurate and relevant recommendations [40]. This capability is critical in domains such as e-commerce, streaming KDD 25, August 37, 2025, Toronto, ON, Canada. Lu Wang et al. be dynamic and adaptable to better reflect the multi-dimensional nature of user preferences. To address these issues, profiles should be adaptive, allowing for unconstrained representations that can incorporate real-time feedback from the recommendation task [22, 25, 26]. Existing supervised fine-tuning (SFT) methods impose further restrictions on profile formats [38, 45], which hinders flexibility. There is growing need to explore profile generation approaches that are less constrained by predefined formats. Reinforcement learning (RL)-based strategies, such as DeepSeek-R1 [11], offer promising solution by enabling dynamic profile optimization and allowing the system to learn profiles directly from task performance without rigid format constraints. Figure 1: Impact of Different Historical Textual Behavior Lengths on LLM (LLaMA 8B) Recommendation Performance for Amazon, Yelp, and MovieLens datasets. The performance improves initially with increased length but declines after certain point. services, and social media, where nuanced understanding of user interests directly enhances satisfaction and engagement [30, 42]. Traditional recommendation systems have primarily relied on embedding-based representations, encoding user preferences as numerical vectors in latent space [3, 35, 39]. While effective for similarity-based matching, these embeddings face notable challenges. They lack interpretability, making it difficult to understand the specific preferences being modeled, and they are difficult to update dynamically, especially in scenarios with sparse data or new users (commonly known as the cold-start problem) [9, 33, 37, 50? ]. Furthermore, these representations often fail to capture essential contextual signals, such as sequential patterns and temporal dependencies, which are critical for modeling complex user behavior [5, 15, 18, 44]. Recent advancements in Large Language Models (LLMs) have demonstrated strong potential in recommendation tasks, providing new avenue for user profiling by leveraging natural language understanding to create semantically rich, interpretable, and adaptable profiles [17, 30, 49]. However, directly using users historical behavior as input to LLMs has proven insufficient for optimal performance. Token length constraints often lead to noise inclusion and the omission of critical information, limiting the effectiveness of recommendations. As shown in Figure 1, the recommendation performance improves initially with increased input length but declines beyond certain point due to these limitations. These performance issues highlight the need for more structured and effective approaches to profile generation within LLMs. Existing methods typically prompt LLMs with rigid, predefined profile formats [43, 46], aiming to condense user information into manageable input sizes. While such profiles may improve efficiency in certain scenarios, they often fail to capture the complexity and variability of real-world user behavior. Consequently, static profiles struggle to generalize across diverse contexts and evolving task requirements. Defining what constitutes good profile is inherently difficult, as its effectiveness is best measured by its impact on downstream recommendation performance. Profiles need to In this paper, we introduce LettinGo , novel approach for exploring and generating adaptive user profiles to enhance downstream recommendation systems. It integrates exploration and feedback-driven alignment to produce profiles that more accurately reflect user behaviors and preferences. LettinGo proceeds in three key stages: (1) Profile Exploration: We begin by collecting diverse set of user profiles generated by various LLMs, including both closedsource models such as GPT-4o-mini [29] and open-source models that can be fine-tuned as profile generators. This stage aims to explore user profiles from diverse and complementary set of profiles, while avoiding overfitting to specific profiles favored by LLMs to be fine-tuned as the profile generator. (2) Task-Driven Evaluation: The generated profiles, combined with recent user interaction histories, are then integrated into downstream recommendation system. This stage provides direct evaluation of profile quality based on recommendation performance. (3) Profile Preference Alignment: Finally, we leverage the evaluated profiles to construct pairwise preference data, which is used to fine-tune the LLMs. Compared with SFT methods that impose rigid constraints in profile formats, LettinGo directly uses preference alignment to maintain flexibility in the generated profiles, letting go of the profile format and exploring good profiles for enhanced recommendation accuracy. Our work focuses on advancing profile generation within LLMbased recommendation systems. Our key contribution lies in the development of an integrated framework that combines diverse profile exploration with task-driven optimization to learn profile generator, which aims to generate adaptive and high-quality profiles. Experimental results demonstrate that LettinGo significantly improves recommendation accuracy, adaptability, and contextual awareness. This work highlights the importance of enhancing user profile generation techniques for the next generation of recommendation systems."
        },
        {
            "title": "2 Related works\n2.1 Profile in recommendation",
            "content": "User profiles have long been key element in improving the effectiveness of recommendation systems by tailoring suggestions to individual preferences and characteristics. Early approaches, such as CRESDUP [3], utilize client-side Dynamic User Profile (DUP) LettinGo: Explore User Profile Generation for Recommendation System to deliver privacy-preserving personalized recommendations. Similarly, UPCSim [39] leverages user profile attributessuch as age, gender, occupation, and locationto compute correlation coefficients, thereby enhancing recommendation accuracy. More recently, the emergence of LLMs has sparked growing interest in their application for personalized recommendations through the generation and refinement of user profiles. For instance, RLMRec [32] employs GPT-3.5-turbo to create both user and item profiles, which are then aligned with collaborative filtering (CF) embeddings to mitigate feature noise. Likewise, KAR [43] capitalizes on LLMs reasoning abilities to produce user and item profiles, projecting them as additional features to boost performance in downstream recommendation tasks. The GPG method [49] proposes promptbased strategy, utilizing LLMs to generate profiles and incorporate contextual information for personalization. PALR [46] constructs natural language user profiles based on interaction history, directly integrating these profiles into prompts for recommendation models. However, these works have not fully explored the potential of LLM-generated profiles or their impact on recommendation tasks. Recent studies have revealed that while LLMs demonstrate promising capabilities in various domains, their direct application to recommendation tasks often yields suboptimal performance compared to specialized recommendation algorithms. This performance gap underscores the critical importance of domain-specific knowledge and collaborative filtering signals in recommendation systems [21]. To address this limitation, researchers have explored various approaches to integrate recommendation-specific collaborative signals into LLMs through parameter-efficient fine-tuning methods [24]. Notable attempts in this direction include TALLRec [2], which employs the Low-Rank Adaptation (LoRA) [14] architecture to fine-tune the LLaMA-7B model [34] on recommendation data. Similarly, Chen et al. [12] investigated the fine-tuning of an OpenAI ada model for recommendation tasks. However, their findings indicate that directly fine-tuning LLMs for recommendation tasks still underperforms compared to alternative approaches, such as utilizing LLM embeddings for similarity matching or as initialization parameters for specialized recommendation models."
        },
        {
            "title": "2.2 LLM as Recommender itself",
            "content": "The emergence of pre-trained large language models (LLMs) has brought significant success to the field of natural language processing (NLP), and LLMs have also shown immense potential in other domains, such as recommendation systems [41]. While other works have already explored the use of natural language to explain the predictions of conventional recommendation models [20, 32, 43], promising direction is to leverage LLMs directly for recommendation tasks [7, 10, 13, 19, 36, 48].Since fine-tuning LLMs requires substantial computational resources, zero-shot learning and incontext learning have gained widespread attention. Several studies have explored the use of LLMs as recommenders, yielding some initial successes [10, 13, 16, 23, 36].For instance, ChatRec [10] combines user profiles and interaction history to prompt the LLM for recommendations. Kang et al. [16] conducted comprehensive comparison between LLMs and strong collaborative filtering (CF) methods. Their analysis revealed that, with fine-tuning, LLMs can achieve comparable or even superior performance with only small KDD 25, August 37, 2025, Toronto, ON, Canada. portion of the training data, demonstrating the potential of LLMs in terms of data efficiency. Wang and Lim [36] evaluate the zero-shot next-item recommendations performance of GPT-3, highlight the potential of using LLMs in zero-shot recommendation"
        },
        {
            "title": "3 Method\n3.1 Preliminary\nTask Description. The task is to leverage userâ€™s long interaction\nhistory to generate a profile, and then combined with userâ€™s short\ninteraction history as the input to a recommendation system to\ndecide if the user is interested in a given item.\nFormulation. To be more specific, given a user ğ‘¢, their inter-\naction history with ğ¿ interaction behaviors is defined as Hğ‘¢ =\n[â„ğ‘¢,1, â„ğ‘¢,2, . . . , â„ğ‘¢,ğ¿], where smaller subscription means more re-\ncent interaction behavior, i.e., â„ğ‘¢,ğ‘™ is more recent than â„ğ‘¢,ğ‘™+1. This\ninteraction history is divided into two components:",
            "content": "Recent History hğ‘¢ = [â„ğ‘¢,1, â„ğ‘¢,2, . . . , â„ğ‘¢,ğ¾ ], which contains ğ¾ most recent interactions utilized for the downstream prediction task. Long History Hğ‘¢ = [â„ğ‘¢,ğ¾+1, â„ğ‘¢,ğ¾+2, . . . , â„ğ‘¢,ğ¿], which contains ğ¿ ğ¾ historical interactions. This is used to explore and generate diverse user profiles in various formats. The user profile pğ‘¢ , representing dynamic and flexible understanding of the users long-term preferences, is generated as: pğ‘¢ = ğ‘“LLM (Hğ‘¢ ), (1) where ğ‘“LLM denotes the profile generation process conducted by an LLM. Unlike traditional fixed formats, the profile pğ‘¢ is unconstrained and can take various forms depending on the exploration step, enabling adaptability and expressiveness. To evaluate the quality of the generated profiles, downstream recommendation task is used. The task employs the generated profile pğ‘¢ , the short history hğ‘¢ , and target item ğ‘¡. These inputs are structured into natural language prompt, forming the input to the downstream recommendation system: Ë†ğ‘¦ğ‘¢ = ğ‘“Rec (pğ‘¢, hğ‘¢, ğ‘¡), (2) where ğ‘“Rec represents the downstream recommendation system and Ë†ğ‘¦ğ‘¢ is the predicted relevance score for the target item ğ‘¡. The feedback from the downstream task serves as an indirect measure of profile quality. By optimizing the profile generator based on this feedback, the model learns to align profile generation with downstream performance, ensuring higher-quality and contextaware profiles."
        },
        {
            "title": "3.2 The Design of LettinGo",
            "content": "We propose novel method LettinGo to explore and learn to generate good user profile for recommendation systems. It aims to create flexible, diverse, and adaptive representations of user behaviors and preferences, that help the downstream recommendation system to recommend appropriate items that suit the users interests. As shown in Figure 2, we first leverage various LLMs to explosively generate diverse user profiles in 1 Profile Exploration from user interaction and behavior history, capturing long-term behaviors and accommodating different contexts. Then, the generated user profile combined with the users recent interaction history is fed KDD 25, August 37, 2025, Toronto, ON, Canada. Lu Wang et al. Figure 2: An overview of LettinGo, which consists of three stages. It begins with Profile Exploration, where diverse user profiles are generated from existing LLMs. These profiles are then utilized in downstream recommendation systems for Task-Driven Evaluation. Finally, the evaluated profiles are constructed as pairwise preference data to train the profile generator in Profile Preference Alignment. into recommendation system which evaluates the quality of the generated profile as 2 Task-Driven Evaluation. The evaluated user profiles form pairwise preference data that are used in 3 Profile Preference Alignment to train good user profile generator. We discuss the details of each stage in the following sections. 3.2.1 Profile Exploration. Traditional fixed-format user profiles limits their adaptability to different tasks. To address this, we aim to explore diverse user profiles that reflect user preferences. By sampling profiles from various existing LLMs, we ensure wide variety of user preference representations. To systematically explore diverse profile formats, We first design simple, general-purpose prompt to instruct LLMs to generate user profiles: You will serve as an assistant to help me generate user profile based on this users sentiments history to better understand this users interest and thus predict his/her sentiment about target item. will provide you with some behavior history of the user in this format: [item attributes and sentiment] .The user profile you generate should contain as much useful content as possible to help predict the users sentiment towards new business. USER HISTORY: [user history]. PROFILE YOU GENERATE: Profile generation is performed using both closed-source models such as GPT-4o-mini [29] and open-source models such as LLaMA 3 [8], where open-source models can be fine-tuned as profile generators. LLMs are typically pre-trained on large and diverse datasets, and they can offer robust, general-purpose representations of user preferences. Collecting profiles from diverse LLMs could avoid overfitting to specific profiles that preferred by the LLMs used as the base model to train as the profile generator. Each LLM generates ğ‘ = 10 profiles for given user ğ‘¢ based on their long history Hğ‘¢ . To encourage diversity in the profiles, we set high sampling temperature (temp = 1.0). The output is collection of diverse profiles from all LLMs. 3.2.2 Task-Driven Profile Evaluation. Evaluating the quality of generated user profiles directly is challenging, as there is no predefined ground truth for good profile. Instead, we use the performance of downstream recommendation task as an indirect measure of profile quality. This allows us to assess profiles in task-driven context, ensuring their relevance and effectiveness for personalized recommendations. To evaluate profiles, we adopt pairwise approach to construct labeled training data. For each user ğ‘¢, the exploration phase genğ‘¢, . . . , pğ‘ erates multiple profiles {p1 ğ‘¢ }. We combine each profile with the users recent interaction history hğ‘¢ and target item ğ‘¡, constructing structured prompt for the downstream recommendation task: ğ‘¢, p2 Given users past sentiments towards other items (sorted by time,from earliest to latest) in the format: [item attributes and sentiment] , and user profile which depict the users interest about items, your task is helping me predict users possible sentiment about target item based on these information in one word. The sentiment has three categories: like, neutral, and dislike. Remember, your output should only contain one word (like, neutral or dislike, in lowercase) that represent user sentiment you predict, without any additional content. USER HISTORY: [user history]. USER PROFILE: [user profile]. The candidate item is: [item]. LettinGo: Explore User Profile Generation for Recommendation System The downstream recommendation model ğ‘“Rec evaluates each profile1 pğ‘¢ and produces relevance score Ë†ğ‘¦ğ‘¢ for the target item. Profiles are then ranked based on the recommendation systems success in accurately predicting relevant items. We construct each pairwise training data by selecting two profiles for each user: positive profile p+ ğ‘¢ , which contributes to successful recommendation, and negative profile ğ‘¢ , which leads to failed recommendation. The resulting pairwise dataset {(p+ ğ‘¢ )} provides task-driven signal for profile quality, laying the foundation for the preference alignment stage. ğ‘¢, 3.2.3 Profile Preference Alignment. primary goal of our method is to explore high-quality user profiles without fixed formats. We avoid limiting the format of user profiles to capture the diversity and complexity of user behaviors, ensuring adaptability to various downstream tasks and recommendation needs. This flexibility allows LLMs to fully utilize their expressive capabilities, generating nuanced and context-aware profiles that improve system performance and generalizability. Standard SFT methods inherently constrain the profile generator to specific formats seen during training, reducing flexibility. In addition, It is inherently difficult to define ground truth profile for user, as the optimal profile depends on how well it performs in the downstream recommendation task. This dependency necessitates feedback-driven approach, aligning profile generation with the specific needs of the downstream model. To ensure that the generated profiles remain flexible and taskspecific, we train the profile generator using feedback-driven alignment approach. Instead of relying on SFT approaches, which impose rigid constraints on profile format, we align the generator with task performance using the pairwise data from the task-driven evaluation stage. This approach optimizes the generator to produce high-quality profiles while maintaining the flexibility of the format. We train the profile generator ğ‘“LLM using Direct Preference Optimization (DPO) with triple data (Hğ‘¢, p+ ğ‘¢ ), where Hğ‘¢ is the users long interaction history. The generators objective is to assign higher preference scores to positive profiles than to negative ones, aligning the profile generation process with downstream task performance. ğ‘¢, The DPO [31] loss is defined as: ğ‘¢,p ğ‘¢ ) (cid:2)log ğœ (cid:0)ğ‘“LLM (p+ LDPO = E(Hğ‘¢,p+ ğ‘¢ Hğ‘¢ )(cid:1) (cid:3) , (3) where ğœ (ğ‘¥) = 1/(1 + ğ‘’ ğ‘¥ ) is the sigmoid function. This loss encourages the generator to produce profiles that maximize downstream recommendation performance. ğ‘¢ Hğ‘¢ ) ğ‘“LLM (p The output of this stage is fine-tuned profile generator ğ‘“LLM that dynamically generates user profiles optimized for downstream recommendation tasks, balancing flexibility and recommendation effectiveness."
        },
        {
            "title": "3.3 Data Collection Algorithm",
            "content": "Algorithm 1 presents our optimized data collection pipeline for preference-based fine-tuning. The goal is to construct training dataset that allows the model to learn to distinguish high-quality 1We omit ğ‘– in pğ‘– ğ‘¢ which represents the ğ‘–ğ‘¡â„ profile of user ğ‘¢ for simple notation. KDD 25, August 37, 2025, Toronto, ON, Canada. user profiles from suboptimal ones, thereby aligning profile generation with downstream task objectives. For each user ğ‘¢ in the dataset, we first generate multiple candidate profiles using diverse set of LLM-based profilers {ğ‘“ (ğ‘š) }ğ‘€ ğ‘š=1, LLM each sampled ğ‘ times with specified temperature ğ‘‡ to ensure variability. Each generated profile is then evaluated using downstream predictor ğ‘“Rec, which predicts user labels Ë†y based on the profile and interaction history Hğ‘¢ . Profiles that yield correct predictions (Ë†y = yğ‘¢ ) are labeled as positive examples P+ ğ‘¢ , while incorrect ones are considered negative ğ‘¢ . If both positive and negative profiles are available for user, we construct pairwise preference data by forming all possible (p+ ğ‘¢, ğ‘¢ ) pairs. These comparison tuples, consisting of the users history, the preferred and dispreferred profiles, and the ground-truth label, are added to the training dataset DDPO. This procedure ensures that the collected training data captures fine-grained distinctions between good and bad profiles, as judged by their downstream utility. The resulting dataset is well-suited for Direct Preference Optimization (DPO), enabling the profile generator to improve by learning from task-relevant, model-informed feedback rather than human-annotated preferences. Algorithm 1 Optimized Data Collection Require: User set U; Profilers {ğ‘“ (ğ‘š) }ğ‘€ ğ‘š=1; LLM Downstream predictor ğ‘“Rec; User interaction history Hğ‘¢ ; Ground-truth labels yğ‘¢ for each user ğ‘¢; Sampling temperature ğ‘‡ ; Profiles per model ğ‘ . Ensure: Training dataset D. 1: DDPO 2: for each user ğ‘¢ do 3: ğ‘¢ , {Good/bad profiles for ğ‘¢} ğ‘¢ , P+ for each model ğ‘š = 1, . . . , ğ‘€ do (Hğ‘¢ ;ğ‘‡ ) {Generate profile} for ğ‘› = 1, . . . , ğ‘ do ğ‘“ (ğ‘š) LLM Ë†y ğ‘“Rec (p, Hğ‘¢ ) {Predict} if Ë†y = yğ‘¢ then ğ‘¢ P+ P+ else ğ‘¢ {p} ğ‘¢ {p} ğ‘¢ end if end for end for if P+ ğ‘¢ then ğ‘¢ and ğ‘¢ P+ for each p+ for each ğ‘¢ do ğ‘¢ ğ‘¢ do DDPO DDPO {(Hğ‘¢, p+ ğ‘¢, ğ‘¢ , yğ‘¢ )} 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end for end for end if 21: 22: end for 23: return KDD 25, August 37, 2025, Toronto, ON, Canada. Lu Wang et al. Table 1: Statistics of the experimental datasets. Datasets Movielens-10M Amazon-Books Yelp #Users 71567 1,850,187 19,800 #Items 10681 483,579 22,"
        },
        {
            "title": "4 Evaluation",
            "content": "Features #train #test 15,637 13,212 14,427 2,000 Title, Genre 2,000 Title, Category 2,000 Business name, Category In this section, we present the experimental evaluation results of our framework across multiple datasets, addressing the following research questions: RQ1: How does LettinGo perform compared to traditional fixed-format profiles in recommendation systems? RQ2: How does the feedback-driven alignment strategy through DPO contribute to profile generation quality? RQ3: What is the impact of historical interaction length on profile generation quality and downstream recommendation performance? RQ4: How does LettinGo enhance recommendation interpretability and capture diverse user preferences?"
        },
        {
            "title": "4.1 Experiments Settings\n4.1.1 Datasets. We conducted experiments on three widely-used\nrecommendation datasets: Movielens-10M [27], Amazon Books [1,\n28], and Yelp [47], all of which consist of user ratings and reviews\nfor items. To ensure meaningful analysis with sufficient interaction\nhistory, we filtered the datasets to retain only users with more than\n70 historical interactions. For the test set, following the methodol-\nogy of Kang et al., we selected the last interacted item from users\nwith more than 70 historical interactions and randomly sampled\n2,000 users to construct the test set. When evaluating on the train-\ning set, the length of historical records used to generate profiles\ncan be explicitly controlled (e.g., 30, 50, or 70 interactions). Due to\ncomputational constraints, we further sampled 3,000 users from\nthe remaining data to construct the training set for profile genera-\ntion. To ensure diversity in the training data and enable the profile\ngeneration model to effectively handle varying lengths of historical\ninteractions, we divided the sampled users into three groups of\n1,000 each. For each group, we used interaction histories of lengths\n30, 50, and 70, respectively, as input to sample profiles. This en-\nsured that the final training data included profiles generated from\ninteraction histories of various lengths.",
            "content": "For each user, we generate 10 profiles, which were labeled and paired according to the methodology outlined in Section 3.2.2. Specifically, the profiles were paired based on whether they resulted in accurate or inaccurate downstream recommendation. The statistics of the training and test sets are presented in Table 1. 4.1.2 Evaluation. Similar to GPG [49] and the approach in [16], we directly leverage LLMs and textual data to perform zero-shot user preference prediction. Following the experimental paradigm of Multi-Behaviour Recommendation [4], we evaluate the effectiveness of generated user profiles by framing it as classification task. For each user, we input the following components into the downstream LLM recommendation system:Users recent history, generated profile, and candidate item. The system predicts the label for the candidate item, classifying it into one of three categories: dislike, neutral, or like. The objective is to assess how well the input profile supports accurate predictions. If the system correctly classifies the item, the profile is validated as effective. If the prediction is incorrect, the profile is considered less representative of the users preferences. We employ two widely-adopted classification metrics: Accuracy, weighted-F1 score. Given that our task involves threeway classification (like, neutral, and dislike), these metrics provide comprehensive evaluation of the models performance across all sentiment categories. The use of macro-averaging ensures that each class contributes equally to the overall evaluation, regardless of class imbalance. 4.1.3 Baselines. To enable comprehensive comparison and to demonstrate the effectiveness of our proposed approach, we implement the following baseline methods. These include the most basic method, which predicts based solely on the most recent 10 interaction records (10H), as well as other approaches that leverage natural language prompts to assist the model in generating user profiles using 30 long interaction history (30P) for recommendation tasks: Prediction with 10 Recent History (10H). Following the experimental setup of prior research [16], we adopt the use of the most recent 10 interaction records for direct prediction as baseline. It is widely acknowledged that recent interaction records are the most indicative of users short-term preferences, making the most recent 10 interactions particularly significant for capturing immediate user intent. KAR [43]. As mentioned in our introduction, increasing the number of interaction records can improve recommendation performance to some extent. However, excessively long interaction histories may introduce noise and lead to overly lengthy contexts, which can hinder model effectiveness. Compared to our method of summarizing user profiles, this approach directly utilizes the same number of interaction records for prediction. Both RLMRec [32] and PALR [46] are designed to prompt LLMs to generate user profiles for recommendation systems by combining recent interactions with long-term behavioral data. While the underlying approach of both methods is similar, they differ in the specific prompts used to guide the LLM in profile generation. To fairly compare the impact of profile generation on recommendation performance, we implement both baselines using only their respective prompts while keeping other components constant. The profile generation prompts of the baselines are in Appendix A. 4.1.4 Implementation Details. In our experiments, we primarily employ LLaMA [8, 34] as both the recommendation model and the profile generation model. Specifically, LLaMA3 8B Instruct is utilized as the downstream prediction model, while LLaMA3 8B Instruct [8] and LLaMA2 14B Chat [34] are used for generating user profiles. Additionally, we leverage GPT-4o-mini and Claude [6] to generate diverse profiles. For the item recommendation, the model temperature is set to 0 to ensure deterministic outputs. In contrast, during profile sampling for training data collection, the temperature is increased to 1.0 to enhance diversity. The training process is implemented using the LLaMA-Factory framework [51]. LettinGo: Explore User Profile Generation for Recommendation System KDD 25, August 37, 2025, Toronto, ON, Canada. Key hyperparameters, such as batch size and learning rate, are determined through grid search to achieve optimal performance."
        },
        {
            "title": "4.2 Performance Comparison\n4.2.1\nImprovement over baselines (RQ1). Table 2 presents the com-\nparative results of our proposed methods against the baseline across\nthree widely-used datasets: Movielens-10M, Amazon Books, and\nYelp. We utilize LLama3-8B-Instruct as both the profile generator\nand the downstream recommendation model; during prediction,\nwe keep the temperature at 0. To validate the effectiveness of our\nframework, we compare it against three state-of-the-art methods\nthat enhance recommendation performance through profile gener-\nation. The experimental results are reported in Table 2. Specifically,\nwe use the prompts from these methods to generate profiles and\ninclude a baseline that predicts recommendations solely based on\nthe most recent 10 interaction records. The experimental results\ndemonstrate that our proposed method achieves significant per-\nformance improvements across all datasets. On the LLaMA3 8B\nInstruct model, our approach improves the accuracy by an aver-\nage of 20 percentage points across the three datasets compared\nto the baseline that uses only historical interactions. Furthermore,\ncompared to other profile-based approaches, our method exhibits\na clear advantage, particularly on the Amazon dataset, where it\nachieves an accuracy of 66.30% and an F1 score of 69.04%, outper-\nforming all baseline methods. This performance superiority can be\nattributed to the following key factors: (i) Structured and informa-\ntive profiles: By leveraging carefully designed prompt templates,\nLettinGo generates more structured and information-rich user\nprofiles, effectively capturing the core preference characteristics\nof users. (ii) Dynamic and balanced preference modeling: By limit-\ning the historical interactions to the most recent 10 records while\nincorporating the generated profile information, our method bal-\nances short-term and long-term user interests. This allows it to\nretain usersâ€™ latest dynamic preferences while leveraging the stable,\nlong-term preferences distilled in the profiles. (iii) Task-aligned\nprofile optimization: Through downstream task-driven evaluation,\nwe select and align profile generation with the data most beneficial\nto downstream tasks. This alignment training enables the profile\ngenerator to consistently produce more effective and task-relevant\nprofiles.",
            "content": "The experimental results also reveal differences in performance improvements across datasets, which may be attributed to the characteristics of each dataset. For instance, the relative improvement is the largest on the MovieLens dataset, indicating that in domains such as movie recommendation, users long-term interests and preference characteristics have more significant impact on recommendation performance."
        },
        {
            "title": "4.3 Ablation Study\n4.3.1 Effectiveness of optimization/alignment (RQ2). To investigate\nthe impact of profile alignment on the final performance, we con-\nducted ablation experiments on three datasets using the same ex-\nperimental settings described in Section 4.1 on the LLama3 model.\nSpecifically, we compared the performance before and after opti-\nmizing with DPO (Direct Preference Optimization). This analysis",
            "content": "Figure 3: Average token count comparison between profile and history on LLaMA3 helps us understand how alignment contributes to generating more task-relevant and high-quality profiles. As shown in Table 3, our DPO-based profile alignment method yields consistent and substantial improvements in accuracy across all three datasets. On the MovieLens-10M dataset, it improves accuracy by 2.1% over the SFT baseline. The effect is even more pronounced on the Yelp dataset, with 4.2% improvement, and reaches its peak on the Amazon Books dataset, where accuracy increases by 6.7%. These results demonstrate that profile alignment using preferencebased optimization effectively leverages feedback from downstream tasks to guide the profile generation model. By learning to produce profile formats that are better aligned with task-specific objectives, the method significantly enhances the quality of generated profiles, more accurately summarizes user preferences, and ultimately improves downstream recommendation performance. 4.3.2 Impact of historical interaction length on profile quality (RQ3). We conducted another set of experiments to evaluate the impact of historical interaction length on the quality of the generated profiles. Specifically, we used different lengths of user interaction histories (e.g., the most recent 30, 50, and 70 interactions) to generate profiles, following the experimental settings described in Section 4.1. The results of these experiments are summarized in Table 2. Interestingly, the results did not align with our initial expectation that longer histories would always produce better profiles. Instead, the performance varied depending on the dataset and experimental conditions. This phenomenon can be attributed to the following factors: Dataset Scale and Sparsity: In datasets with sparse user interactions, shorter histories are often sufficient to capture the primary user preferences. In contrast, longer histories may introduce noise, reducing the quality of the profiles. On the other hand, in datasets with dense interactions, longer histories can provide richer semantic information, which benefits profile generation. User Interest Dynamics: User preferences often evolve over time. Shorter histories can better reflect recent preferences, which are typically more relevant for downstream tasks. However, longer histories may include outdated or irrelevant interactions, diluting the signal and potentially degrading the performance. Model Capacity and Robustness to Noise: The recommendation models ability to process input data and handle noise also plays critical role. For example, models with higher capacity can better extract meaningful signals from longer histories, while KDD 25, August 37, 2025, Toronto, ON, Canada. Lu Wang et al. Table 2: Performance on three dataset using LLaMA 3 (8B) and LLaMA 2 (13B). Method LLaMA 3 (8B) LLaMA 2 (13B) Movie-lens F1 Acc Yelp Acc F1 Amazon Books Movie-lens F1 Acc Acc F1 Yelp Acc F1 Amazon Books Acc F1 10H KAR[43] RLMRec[32] PALR[46] Ours 10H+30P 10H+50P 10H+70P 44.95 51.00 48.00 48.10 51.80 52.80 53.00 41.08 49.91 47.07 47.26 51.05 51.16 51. 37.45 67.90 61.30 64.10 70.70 70.65 70.40 42.92 61.60 58.96 59.47 62.91 63.17 62.94 48.15 67.40 63.60 66.40 70.55 70.75 70. 55.93 69.84 67.42 69.29 71.79 71.71 71.27 44.95 48.00 45.65 49.10 50.45 51.05 51.09 41.08 46.51 45.26 47.94 49.25 49.59 57. 37.45 65.70 62.60 63.10 66.35 65.65 66.00 42.92 60.93 58.90 58.85 60.47 60.31 60.14 48.15 60.05 59.85 60.10 64.75 65.75 63. 55.93 65.28 65.05 65.47 68.34 69.01 67.71 Figure 4: The open-format profiles generated by LettinGo vary across different datasets. models with limited capacity may struggle with excessive input data, becoming more sensitive to noise. These findings suggest that the choice of historical interaction length should be adapted dynamically, taking into account the characteristics of the dataset and the capacity of the recommendation model. This adaptive approach could help optimize the trade-off between capturing sufficient user preferences and avoiding noise, ultimately improving both profile quality and downstream recommendation performance."
        },
        {
            "title": "4.4 Case Study\n4.4.1 Effectiveness of LettinGo profile (RQ4). Figure 3 presents\nthe average token length of profiles generated by Llama3 using 30\ninteraction records from the test set, as well as the average token\nlength of the same historical interaction records. The comparison\ndemonstrates that, compared to directly feeding long historical\nrecords into the large language model, the generated profiles ef-\nfectively reduce the context length. This reduction alleviates the\nchallenges faced by the model in capturing information within",
            "content": "LettinGo: Explore User Profile Generation for Recommendation System Table 3: Performance comparison of different profile generation methods (in accuracy). Dataset Without DPO SFT With DPO MovieLens Yelp Amazon Books 50.9 64.9 59. 51.1 66.2 63.7 53.0 70.4 70.4 Figure 5: sample where the prediction fails before using the profile but succeeds after using it. long-context scenarios. As shown in Figure 5, this is an example where the model makes incorrect predictions when using only 10 historical records, but successfully predicts when additional profile information is added. The parts with the same background color in USER HISTORY and USER PROFILE indicate the user preferences correctly summarized by the profile. For example, this user is very interested in movies of the action, adventure, and fantasy genres and also enjoys light and humorous comedies. However, they are not particularly fond of movies whose primary theme is horror or thriller, unless they contain strong elements of science fiction and adventure. In the prediction for the TARGET MOVIE, the profile also contributes useful information about the users preference for science fiction, adventure, and comedy genres, which helps the model make successful prediction. In different domain-specific datasets, the user profiles generated by our method exhibit significant variations in format, reflecting the distinct demands of each domain and the diverse application scenarios of user profiling. As shown in Figure 4, the MovieLens user profile primarily adopts narrative style, emphasizing emotional engagement and thematic preferences. This format is well-suited for analyzing subjective experiences and psychological inclinations. In contrast, the Yelp user profile employs highly structured format, explicitly categorizing aspects such as interests, preferred cuisines, and sentiment analysis, making it more suitable for applications in machine learning and recommendation systems. Meanwhile, the Amazon Books user profile strikes balance between narrative and structured formats, capturing users genre preferences and brand loyalty while maintaining degree of readability and analytical flexibility. These differences in format highlight the need for domain-specific customization of user profiles, tailored to the KDD 25, August 37, 2025, Toronto, ON, Canada. unique characteristics of the dataset and its intended use cases. Our profiling approach, along with its corresponding optimizations, effectively addresses this requirement by providing flexible framework that caters to both human interpretation and machine-driven analysis."
        },
        {
            "title": "Transferability",
            "content": "To assess the competitiveness of our DPO-based profile generation method against closed-source models, we conducted comparison with GPT-4o. On the MovieLens dataset, we used GPT-4o to generate user profiles and evaluated them with LLaMA 3 8B Instruct predictor. Our method (DPO with 10H+70P) achieved 53.00% accuracy and 51.69 F1, outperforming GPT-4o-generated profiles, which reached 52.80% accuracy and 51.30 F1. These results indicate that our DPO-based generator produces more effective and consistent profiles in larger-profile settings. We further evaluated the transferability of our approach by using Qwen2.5 7B Instruct for both profile generation and downstream prediction. As shown in Table 4, our method generalizes well across different model backbones, demonstrating consistent improvements as the profile size increases. Table 4: Performance of profile generation with Qwen2.5 7B Instruct on MovieLens. Method Accuracy F1 Score 10H 10H+30P 10H+50P 10H+70P 52.50 58.30 56.64 57. 50.23 56.87 55.16 56."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present LettinGo, novel framework for flexible user profile generation that leverages large language models to generate flexible user profiles from long-term interaction histories, thereby enhancing recommendation system performance. Our framework consists of three key components: (1) Profile Exploration, which leverages both internal and external LLMs to generate diverse profile representations without format constraints; (2) Task-Driven Profile Evaluation, which assesses profile quality through downstream recommendation performance; and (3) Profile Preference Alignment, which optimizes profile generation through feedback-driven training using DPO. This design enables our framework to generate adaptive and high-quality profiles while maintaining flexibility in profile representation.Experimental results demonstrate that our method significantly outperforms existing baseline approaches across multiple datasets, validating the substantial value of flexible and adaptive user profiles in enhancing recommendation performance. KDD 25, August 37, 2025, Toronto, ON, Canada. Lu Wang et al. References [1] Amazon. 2025. Amazon Books. https://amazon-reviews-2023.github.io/. Accessed: 2025-01-15. [2] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems. 10071014. [3] Ting Chen, Wei-Li Han, Hai-Dong Wang, Yi-Xun Zhou, Bin Xu, and Bin-Yu Zang. 2007. Content recommendation system based on private dynamic user profile. In 2007 International conference on machine learning and cybernetics, Vol. 4. IEEE, 21122118. [4] Xiaoqing Chen, Zhitao Li, Weike Pan, and Zhong Ming. 2023. Survey on Multi-Behavior Sequential Recommendation. arXiv preprint arXiv:2308.15701 (2023). [5] Yi-Cheng Chen, Tipajin Thaipisutikul, and Timothy Shih. 2020. learningbased POI recommendation with spatiotemporal context awareness. IEEE Transactions on Cybernetics 52, 4 (2020), 24532466. [6] Claude. 2025. Claude. https://claude.ai/. Accessed: 2025-01-15. [7] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering chatgpts capabilities in recommender systems. In Proceedings of the 17th ACM Conference on Recommender Systems. 11261132. [8] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [9] Li Gao, Haiping Fan, Yi Liu, and Jinyang Zhao. 2024. RSEIU: recommendation algorithm for shopping based on explicit and implicit feedback of user. (2024). [10] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524 (2023). [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [12] Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023. Leveraging large language models for sequential recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems. 10961102. [13] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2024. Large language models are zero-shot rankers for recommender systems. In European Conference on Information Retrieval. Springer, 364381. [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021). [15] Xiaowen Huang, Shengsheng Qian, Quan Fang, Jitao Sang, and Changsheng Xu. 2018. Csan: Contextual self-attention network for user sequential recommendation. In Proceedings of the 26th ACM international conference on Multimedia. 447455. [16] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do llms understand user preferences? evaluating llms on user rating prediction. arXiv preprint arXiv:2305.06474 (2023). [17] Kirill Kronhardt, Sebastian Hoffmann, Fabian Adelt, and Jens Gerken. 2024. PERSONÃ†R-Transparency Enhancing Tool for LLM-Generated User Personas from Live Website Visits. In Proceedings of the International Conference on Mobile and Ubiquitous Multimedia. 527531. [18] Saurabh Kulkarni and Sunil Rodd. 2020. Context Aware Recommendation Systems: review of the state of the art techniques. Computer Science Review 37 (2020), 100255. [19] Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023. Large language models for generative recommendation: survey and visionary discussions. arXiv preprint arXiv:2309.01157 (2023). [20] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023. CTRL: Connect Collaborative and Language Model for CTR Prediction. arXiv preprint arXiv:2306.02841 (2023). [21] Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Hao Zhang, Yong Liu, Chuhan Wu, Xiangyang Li, Chenxu Zhu, et al. 2023. How can recommender systems benefit from large language models: survey. arXiv preprint arXiv:2306.05817 (2023). [22] CanYi Liu, Wei Li, Hui Li, Rongrong Ji, et al. 2024. Beyond Inter-Item Relations: Dynamic Adaption for Enhancing LLM-Based Sequential Recommendation. arXiv preprint arXiv:2408.07427 (2024). [23] Junling Liu, Chao Liu, Peilin Zhou, Renjie Lv, Kang Zhou, and Yan Zhang. Is chatgpt good recommender? preliminary study. arXiv preprint 2023. arXiv:2304.10149 (2023). [24] Peng Liu, Lemei Zhang, and Jon Atle Gulla. 2023. Pre-train, Prompt, and Recommendation: Comprehensive Survey of Language Modeling Paradigm Adaptations in Recommender Systems. Transactions of the Association for Computational Linguistics 11 (2023), 15531571. [25] Rafael Mendoza, Isabella Cruz, Richard Liu, Aarav Deshmukh, David Williams, Jesscia Peng, and Rohan Iyer. 2024. Adaptive Self-Supervised Learning Strategies for Dynamic On-Device LLM Personalization. arXiv preprint arXiv:2409.16973 (2024). [26] Laia Lluch Molins and Elena Cano GarcÃ­a. 2023. How to embed SRL in online learning settings? Design through learning analytics and personalized learning design in moodle. Journal of New Approaches in Educational Research 12, 1 (2023), 120138. [27] Movielens. 2025. Movielens-10M. https://grouplens.org/datasets/movielens/10m/. Accessed: 2025-01-15. [28] Jianmo Ni, Jiacheng Li, and Julian McAuley. 2019. Justifying recommendations using distantly-labeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). 188197. [29] OpenAI. 2025. GPT-4o-mini. https://openai.com/index/gpt-4o-mini-advancingcost-efficient-intelligence/. Accessed: 2025-01-15. [30] Erasmo Purificato, Ludovico Boratto, and Ernesto William De Luca. 2024. User Modeling and User Profiling: Comprehensive Survey. arXiv preprint arXiv:2402.09660 (2024). [31] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems 36 (2023), 5372853741. [32] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Representation learning with large language models for recommendation. In Proceedings of the ACM on Web Conference 2024. 3464 3475. [33] Dohyoung Rim, Sirojiddin Nuriev, and Younggi Hong. 2025. Cyclic Training of Dual Deep Neural Networks for Discovering User and Item Latent Traits in Recommendation Systems. IEEE Access (2025). [34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [35] Vamosi, Reutterer, and Platzer. 2022. deep recurrent neural network approach to learn sequence similarities for user-identification. Decision Support Systems (2022). [36] Lei Wang and Ee-Peng Lim. 2023. Zero-shot next-item recommendation using large pretrained language models. arXiv preprint arXiv:2304.03153 (2023). [37] Xin Wang, Lixin Han, Jingxian Li, and Hong Yan. 2025. tensor recommendation method based on HMM network and meta-path. Information Sciences 689 (2025), 121412. [38] Zihan Wang, Xiaocui Yang, Yongkang Liu, Shi Feng, Daling Wang, and Yifei Zhang. 2024. Muse: Multimodal Conversational Recommendation Dataset with Scenario-Grounded User Profiles. arXiv preprint arXiv:2412.18416 (2024). [39] Triyanna Widiyaningtyas, Indriana Hidayah, and Teguh Adji. 2021. User profile correlation-based similarity (UPCSim) algorithm in movie recommendation system. Journal of Big Data 8, 1 (2021), 52. [40] J. Wu. 2023. Computational Understanding of User Interfaces. (2023). [41] Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2024. survey on large language models for recommendation. World Wide Web 27, 5 (2024), 60. [42] S. Wu, Y. Zhu, L. Wang, and X. Xie. 2019. Session-based recommendation with graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence. [43] Yunjia Xi, Weiwen Liu, Jianghao Lin, Xiaoling Cai, Hong Zhu, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, and Yong Yu. 2024. Towards open-world recommendation with knowledge augmentation from large language models. In Proceedings of the 18th ACM Conference on Recommender Systems. 1222. [44] Lianghao Xia, Chao Huang, Yong Xu, and Jian Pei. 2022. Multi-behavior sequential recommendation with temporal graph transformer. IEEE Transactions on Knowledge and Data Engineering 35, 6 (2022), 60996112. [45] Diji Yang, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang, Jie Yang, and Yi Zhang. 2024. Im-rag: Multi-round retrieval-augmented generation through learning inner monologues. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 730740. [46] Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, and Yanbin Lu. 2023. Palr: Personalization aware llms for recommendation. arXiv preprint arXiv:2305.07622 (2023). [47] Yelp. 2025. Yelp. https://www.yelp.com/dataset/. Accessed: 2025-01-15. [48] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2023. Self-supervised learning for recommender systems: survey. IEEE Transactions on Knowledge and Data Engineering 36, 1 (2023), 335355. [49] Jiarui Zhang. 2024. Guided Profile Generation Improves Personalization with LLMs. arXiv preprint arXiv:2409.13093 (2024). LettinGo: Explore User Profile Generation for Recommendation System KDD 25, August 37, 2025, Toronto, ON, Canada. [50] Fuzhe Zhao, Chaoge Huang, Han Xu, Wen Yang, and Wenlin Han. 2024. RGMeta: Enhancing Cold-Start Recommendations with Residual Graph Meta-Embedding Model. Electronics 13, 17 (2024), 3473. [51] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning of 100+ language models. arXiv preprint arXiv:2403.13372 (2024)."
        },
        {
            "title": "A Baseline Prompts\nKAR Prompt",
            "content": "Given the user's business reviewing history with sentiments over time, listed below: {user_history}, analyze the user's preferences, taking into account factors such as business name and categories. Provide clear explanations based on the details from the user's reviewing history and other pertinent factors. PALR Prompt Your task is to use keywords to summarize user's preference based on history interations. The Output is an itemized list based on importance. The output template is {{1.KEY_WORD_1:\"HISTORY_BUSINESS_1\",\"HISTORY_BUSIN ESS_2\"; 2.KEY_WORD_2:\"HISTORY_BUSINESS_3\"}} The history businessed and their keywords and user' semtiment are: {user_history} RLMRec Prompt You will serve as an assistant to help me determine which types of businesses specific user is likely to enjoy. will provide you with information about businesses that the user has visited, as well as his or her sentiments of those businesses. Here are the instructions: 1. Each visited businesse will be described in the format with the following attributes: Title:the name of the business, Categories:the categories of the business, Sentiment:user semtiment toward business. 2. The information will give you: INTERATION ITEMS: list of JSON strings describing the items that the user has visited. Requirements: 1. Please provide your decision in JSON format, following this structure: {{ \"summarization\": \"A summarization of what types of businesses this user is likely to enjoy\" (if you are unable to summarize it, please set this value to \"None\") \"reasoning\": \"briefly explain your reasoning for the summarization\" }} 2. Please ensure that the \"summarization\" is no longer than 100 words. 3. The \"reasoning\" has no word limits. 4. Do not provided any other text outside the JSON string. {user_history}"
        }
    ],
    "affiliations": [
        "Microsoft Corporation Beijing, China",
        "Peking University Beijing, China"
    ]
}