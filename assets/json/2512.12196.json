{
    "paper_title": "AutoMV: An Automatic Multi-Agent System for Music Video Generation",
    "authors": [
        "Xiaoxuan Tang",
        "Xinping Lei",
        "Chaoran Zhu",
        "Shiyun Chen",
        "Ruibin Yuan",
        "Yizhi Li",
        "Changjae Oh",
        "Ge Zhang",
        "Wenhao Huang",
        "Emmanouil Benetos",
        "Yang Liu",
        "Jiaheng Liu",
        "Yinghao Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for \"story\" or \"singer\" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work."
        },
        {
            "title": "Start",
            "content": "AutoMV: An Automatic Multi-Agent System for Music Video Generation M-A-P Xiaoxuan Tang1*, Xinping Lei1,2*, Chaoran Zhu3*, Shiyun Chen , Ruibin Yuan ,4, Yizhi Li ,5, Changjae Oh3, Ge Zhang , Wenhao Huang , Emmanouil Benetos3, Yang Liu1, Jiaheng Liu ,2, Yinghao Ma ,3* m-a-p.ai 1Beijing University of Posts and Telecommunications 2Nanjing University 3Queen Mary University of London 4Hong Kong University of Science and Technology 5University of Manchester GitHub: https://github.com/multimodal-art-projection/AutoMV Website: https://m-a-p.ai/AutoMV/"
        },
        {
            "title": "Abstract",
            "content": "Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, multi-agent system that generates full music videos (MVs) directly from song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for story or singer scenes. Verifier Agent evaluates their output, enabling multi-agent collaboration to produce coherent longform MV. To evaluate M2V generation, we further propose benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work. 5 2 0 2 3 1 ]"
        },
        {
            "title": "M\nM",
            "content": ". [ 1 6 9 1 2 1 . 2 1 5 2 : r *These authors contributed equally to this work. Co-corresponding authors. Figure 1. AutoMV video generation results. We introduce AutoMV, multi-agent pipeline which produces coherent, music-synchronised full-length music videos guided by beat, structure, and lyric cues. Our pipeline generates music videos, which maintain consistent person identity, contain diverse camera shots and visual effects, and align with the corresponding music audio and lyrics."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Related Work 2.1 Video generation . . . . 2.2 Audio-visual alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Methodology 3.1 Preprocessing on music informatics . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Screenwriter and director . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Video generation . 3.4 Gemini Verifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experiment 4.1 Evaluation metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 5 5 6 7 8 8 9 9 9 4.2 Experiment setup and bselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4.3 Ablation study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5 Results and Discussion 5.1 Main results . . . 5.2 Ablation results . 5.3 Case study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 6 Conclusion Demo Comparison Detailed Protocol of LLM and Human Experts Scoring Details of Baseline Detail Results of LLM-score Per-categary Failure Case Study and Future Work Human eval v.s. Gemini eval v.s. Rule-based eval 3 19 19 22 23 25 1. Introduction Recent advances in AI-generated content (AIGC) algorithms have demonstrated remarkable success in wide range of fields, from visual contents like image generation [Ramesh et al., 2021, Gong et al., 2025, Zhang et al., 2025a, Chen et al., 2024, Li et al., 2024a, Kim et al., 2024] and video synthesis [Gupta et al., 2024, Bar-Tal et al., 2024, Gao et al., 2025, Henschel et al., 2025, Gal et al., 2024, Wang et al., 2024b], to music composition and sound effect design based on text input [Copet et al., 2023, Evans et al., 2025, Qu et al., Yuan et al., 2025, Wang et al., 2025, Schneider et al., 2024] or video input Li et al. [2024b], Su et al. [2024], Kang et al. [2024], Tian et al. [2025], Dong et al. [2024], cultivating multiple industry applications [Wang et al., 2024a, Ren et al., 2025, Ma et al., 2024, Yu et al., 2024]. Yet, cross-modal systems that jointly produce model audio and video remain relatively limited, especially when tasked with producing coherent, minutelong outputs rather than short, aesthetic clips. In this work, we study music-to-video (M2V) generation: given full song, automatically create temporally aligned, visually consistent MV (MV), aiming is to automatically generate high-quality, long-form visual narratives aligned with music would democratize content creation for artists and media. Despite its potential impact to the entertainment industry, M2V has received little attention compared to other AIGC tasks such as text-to-video and video-to-music. Yet, the necessity of automatic M2V pipeline is substantial: despite recent advances in production technologies, independent music producers still rely heavily on collaboration with specialized professionals with pipeline shown in fig. 2, often incurring more than 10 personal expenses that can exceed $10k per track [Deshmane and Mart√≠nez-de Alb√©niz, 2023]. Moreover, producing single track can require anywhere from 40 to 120 studio hours [Burgess, 2013], further underscoring the intensive effort and coordination involved. Therefore, scalable M2V system could fundamentally reshape the content creation landscape, empowering independent musicians and low-budget creators who currently face prohibitive costs for professional video production to further promote their work. Such impact extends directly to the dominant paradigm of short-form video platforms, where user engagement is demonstrably linked to the tight synchronization and narrative alignment between visual and audio content. More broadly, an effective M2V solution would lower the barrier to high-quality visual storytelling, effectively decentralizing production in industry. The challenges in full-song M2V generation are manifold. First, most current video generation methods are \"shot-based,\" producing only brief, disjointed clips, fundamentally lacking the long-term temporal modeling required to maintain coherence across multi-minute song. Second, though one can include lyrics in the text-to-video prompts, generated visuals are often music-irrelevant, failing to align with critical audio elements such as beats, musical structure, and overall aesthetic concept. Third, generated videos of recent approaches typically suffer from temporal inconsistencies such as varying character identities or incoherent scene progression, and sometimes lack basic physical feasibility. Finally, evaluating such complex, long-form artistic video outputs is non-trivial; existing metrics fail to capture music-video alignment, musicality and creativity. To address these limitations, we propose AutoMV, an automatic multi-agent system designed to generate coherent, story-driven content directly from audio. Our system first leverages music information retrieval (MIR) tools to construct the context and decompose the audio into its core components: beats, structural segments, along with time-stamped lyrics and captions. Gemini Director agent then interprets this multi-modal information to generate comprehensive, time-aligned visual script with key characters detained descriptions and video generation prompts, ensuring coherent narrative that respects the songs structure and lyrics. The designed characters are stored in shared external bank for other agents to ensure identity consistency. The script guides generation module that utilizes specialized APIs for visual Figure 2. Motivation for AutoMV. Current music video production requires extensive human labour, time and expense. Our AutoMV workflow saves large amount of effort while maintaining satisfactory quality. The cost of time and expense are from Burgess [2013], Deshmane and Mart√≠nez-de Alb√©niz [2023] and the quality score is from our subjective evaluation. content creation and lip-syncing. Moreover, verifier agents assess the generated clips for script instruction alignment, and physical feasibility, iterating to select the optimal output. Lastly, to validate our system, we introduce new benchmark for full-song M2V generation. Recognizing the evaluation gap, we propose comprehensive LLM judger that assesses outputs across 12 distinct artistic and technical criteria and analyzes their correlates with human judgment. Our contributions are threefold: 1. AutoMV, the first open-source, multi-agent workflow for generating coherent, full-length music videos. 2. The first M2V evaluation benchmark and LLM judgers for scalable evaluation of long-form music video. 3. comprehensive ablation study quantifying the critical impact of the MIR tools, character bank, and the Verifier agent, validating our system design. 2. Related Work 2.1. Video generation Recent advances in text-to-video focus on long-form consistency, compositional control, and photorealism. StreamingT2V introduces chunked autoregression with short/long-term memories for seamless multi-minute generation [Henschel et al., 2025]. Open-Sora Plan provides an open, scalable pipeline (wavelet VAE, joint imagevideo denoiser, and controllers) for high-resolution, long-duration synthesis [Lin et al., 2024]. VideoTetris enables spatio-temporal compositional diffusion to follow complex prompts with multiple objects and dynamics [Tian et al., 2024]. Lumieres space-time U-Net generates the full temporal span in one pass to improve global temporal coherence [Bar-Tal et al., 2024]. On data and architecture, OpenVid-1M supplies high-quality captioned pairs and an MVDiT backbone [Nan et al., 2024], while W.A.L.T achieves state-of-the-art fidelity with diffusion transformer and unified latent codec [Gupta et al., 2024]. Recent advances in text-to-video generation have enabled increasingly coherent short video clips, but producing long-form, story-driven videos remains challenging. Several large-scale models can now generate high-fidelity videos from text prompts. PixelDance and SeedDance model support multi-shot narrative consistency within single 10-second clip [Zeng et al., 2024, Gao et al., 2025]. Googles Veo 3.1 similarly produces up to 8-second, 1080p videos with stunning realism and even native audio . Wan-2.2 suite offers powerful text-to-video generation by combine LLMs with video diffusion allowing text-, image-, and speech-to-video synthesis at high resolution on consumer GPUs with lip-synchronization capability as talking face [Wan et al., 2025]. However, these systems typically focus on single-scene or short events typically lacking temporal consistency and thematic continuity required for full-length. Recent works explore multi-agent and LLM-guided frameworks for long-form video generation. GenMAC introduces an iterative multi-agent pipeline for compositional text-to-video, where specialized LLM-based agents for verification, suggestion, etc., collaborate to refine complex scenes [Huang et al., 2024]. ViMax employs an Agentic pipeline with Director, Screenwriter, Producer, etc. to automate multi-shot video creation while maintaining character and scene consistency across shots VideoDirectorGPT leverages GPT-4 as high-level planner to expand user prompt into structured multi-scene video plan (detailed scene descriptions, entity layouts, backgrounds, and consistency groupings), which then guides layout-conditioned video generator to ensure temporal consistency of characters and settings across scenes [Lin et al., 2023]. HollywoodTown adopt hierarchical multi-agent orchestration inspired by real film production pipelines, allowing agents to share context and iterate with feedback for minute-scale videos [Wei et al., 2025]. Unlike text-driven systems, our AutoMV targets M2V: it aligns visuals to audio structure, beats, and lyrics, tailored to long, song-level generation. 2.2. Audio-visual alignment In the context of s, prior systems have begun addressing alignment of visuals with musical structure [Li et al., 2025]. MV-Crafter introduced pipeline for automatic music-video creation, with modules for script generation, video assembly, and beat synchronization [Chen et al., 2025]. [Mao et al., 2025] propose MV dataset without training generative models. However, generating visuals directly from audio remains challenging, and most produced only short, disconnected clips. Conversely, researchers have explored the inverse problem of generating music to suit given video, which also demands tight cross-modal coordination. Diff-BGM uses diffusion model to create background music for video by controlling musical attributes with video features dynamic visual cues drive the musics rhythm while semantic cues guide melody and mood [Li et al., 2024b]. VidMuse framework produces high-fidelity soundtracks aligned to video content by incorporating both local and global visual cues [Tian et al., 2025]. V2Meow trains multi-stage autoregressive model on music-video pairs to align signatures between video and music, enabling high-quality musical outputs conditioned on visual features [Su et al., 2024]. Meanwhile, dedicated multi-agent strategies have been applied to audio generation as well. AudioGenie is training-free multi-agent system that orchestrates expert models to produce diverse audio (sound effects, speech, music) from multimodal inputs [Rong et al., 2025]. Likewise, LVAS-Agent focuses on long-form video dubbing with collaborative agent workflow: it decomposes the task into scene segmentation, script generation, sound design and audio synthesis, with discussioncorrection mechanism and retrieval loop to maintain The Google Veo3.1 API document GitHub page - ViMax: Agentic Video Generation 6 temporal coherence across scenes [Zhang et al., 2025b]. Across both directions, the trend is to integrating domain-specific analysis (beats, scenes, emotions) with collaborative generation modules or LLM-based planners agent leads to more synchronous and coherent audio-visual content, which sets the stage for systems like AutoMV to advance full-length generation. 3. Methodology As illustrated in fig. 3, we propose AutoMV, training-free multi-agent pipeline that turns full song into coherent, long-form . It comprises four stages: (1) Music-aware preprocessing to structure, vocal track and lyrics with timecodes; (2) Director agent that plans shot boundaries and writes time-aligned script; (3) Renderer that selects between text and image to video or lip-sync APIs per shot; and (4) Verifier that scores alignment and physical feasibility, for re-generation and fallback when needed. Figure 3. Overview of AutoMV: multi-agent pipeline that analyzes music, plans shot-level scripts, generates video clips with adaptive backends, and verifies alignment and realism before assembling coherent full-length music video. S2V refers to speech-to-video model. 3.1. Preprocessing on music informatics We utilize following MIR tools to extract music informatics to natural language for further process. Music captioning. Qwen2.5-Omni [Xu et al., 2025] produces high-level caption (genre, mood, instrumentation) and infers vocalist attributes (e.g., gender), which later guide casting for image generation, as it demonstrate better results on music compared to audio LLMs [Li et al., 2024c, Ma et al., 2025]. Structure analysis. We employ SongFormer [Hao et al., 2025] to segment the song into 7 intro/verse/chorus/bridge, enabling semantically meaningful shot grouping and narrative arcs. Source separation. With htdemucs [Rouard et al., 2023], we isolate vocals and accompaniment. The vocal stem supports robust automatic lyrics transcription and drives lip-sync. Lyrics transcription. We utilize Whisper [Radford et al., 2023] to transcribe the lyrics from the separated vocal tracks; Gemini then furhter search the lyrics on the Internet and refines lyrics and timestamps and provide more reasonable result. 3.2. Screenwriter and director We use Gemini[Comanici et al., 2025] as screenwriter to compose the video scripts and the design of all characters, and further utilise cheaper but faster Doubao API as director for video description generation and image generation, to be used as the starting key frame of the video. Shot segmentation and narrative scenario. Given the music structure and timestampaligned lyrics, the screenwriter first constructs set of temporally contiguous segments that serve as the basic units for subsequent visual generation. This segmentation is derived from lyric boundaries and section transitions, with user-controllable duration constraints (315 seconds). All clip start/end times respect 1/24 quantization, ensuring audiovideo length parity to eliminate accumulation drift. Once the temporal layout is fixed, the screenwriter generates the description associated with each segment. For every segment, the model interprets the enclosed lyric content, extracting its semantic meaningsuch as thematic cues, emotional tone, and implied imagery, and converts this understanding into description of the intended scene, atmosphere, and character actions. The description serves as the high-level conditioning signals that guide the director in shot specification and keyframe prompt generation. Character bank. The screenwriter instantiates character label set (detailed appearance, nationality, age, gender) to maintain identity across shots. Prompts of script. For every shot, Doubao will play the role of direct and outputs (i) detailed shot-level video script (setting, actions, camera hints), and (ii) an image prompt describing the keyframe. For characters mentioned in the image prompts, the director retrieves the closest match from the pre-defined character bank and injects detailed description (hair, face, skin color, outfit, age, gender etc.) to stabilize identity. Keyframe images. The director requests Doubao image generation from the enriched prompts to seed video; for subsequent shots, the last frame of the previous clip is reused when continuity is desired. For example, 15-second shot segmentation designed by Gemini is devided into 2 parts. The keyframe of the first slot is generated from the image caption written by Doubao, and the keyframe of the second half of the shot is the endframe of the first video clip generated in next step. 3.3. Video generation Backend selection. Per shot, the Director chooses between: For each shot, the director will chose either Doubao video generation API, or Qwen-wan2.2 bsaed on the screenwriter scripts and feedback of verifier, then generate the video based on the keyframe and video description. Doubao for general cinematic rendering. We use Doubao to generate video subclip whose default length is 38 s; up-to-15-second videos is then produced by merging 13 subclips while preserving frame alignment. Image inputs are either Director-generated keyframes for the first 8 subshot in segment, or the previous clips last frame. Qwen-Wan-2.2 for lip-sync. We use Qwen-wan2.2 for lines where Gemini screenwriter think mouth articulation is salient or important such as section onsets andchorus entries. In order to enhance the robustness on ambiguous lyrics (e.g., strong sibilants or stylized delivery), we route the htdemucs vocal stem to the lip-sync backend for the video generation, and combine the generated video with original mixed song track. 3.4. Gemini Verifier We design image verifiers and video verifier for text-visual alignment and physical feasibility using Gemini 2.5 Pro. Image verification. For each keyframe candidate (up to 3), the Verifier checks: Physical realism (plausible poses, lighting, perspective) and instruction adherence (match to script/prompt on the action and the ccharacter description). For each rendered clip (up to 3 candidates), the Verifier first check the physical feasibility - artifact-free motion, plausible kinematics. And if it pass the check, Gemini further score the textimage alignment between shot actions and script/lyrics/music caption, along with the identity continuity, i.e. if it align with the specific characters in the bank. If realism passes, the highest-scoring candidate is accepted. Video verification. Most of the video shot has the character consistency to the keyframe and it has decent capability on following the instruction of motion or actions. So we score the physical feasibility instead. 4. Experiment 4.1. Evaluation metrics Objective evaluation. We adopt ImageBind score (IB) to assess MV quality quantitatively. It measures cross-modal consistency by computing the embedding similarity between the generated video frames and corresponding audio, indicating how well visuals reflect audio content [Girdhar et al., 2023]. LLM as judger. To assess long-form musictovideo generation beyond traditional metrics, we introduce an LLM-based scoring framework powered by multimodal large model (e.g., Gemini-2.5-Pro, Gemini-2.5-Flash, etc.). The model receives the full generated video along with its corresponding song audio and rates it on 12 carefully defined criteria covering four categories: Technical, Post-Production, Content, and Art. Each criterion is scored on 15 scale, and category scores are obtained by averaging their respective sub-questions. The final LLM-based Score is weighted combination of these four category scores: 1. Technical contributes 20%, including character consistency, physical authenticity, lip sync accuracy and visual style harmony, each contribute 5%; 2. Post-Production 20%: shot continuity and audio-visual correlation, each 10%; 3. Content 30%: musical theme relevance, story-telling, and emotional expression, each 10%; and 4. Art 30%: visual composition & quality, creativety, and AI novelty, each 10. This provides holistic evaluation of synchronization accuracy, storytelling quality, visual consistency, and artistic expressiondimensions that are critical for music-video generation but difficult for conventional metrics to capture. For more detailed description of the evaluation criteria, please refer to the supplementary material. 9 Table 1. Overall cost and evaluation on the 30-song benchmark. Cost and runtime are normalized per song (lower is better). Higher is better for others. LLM scores, and human expert scores are in [1, 5]. Gemini-2.5-Pro ratings for Technical, Post-Production, Music Content, and Art are denoted Teùê∫, Poùê∫, Coùê∫, Arùê∫; human scores are Teùêª, Poùêª, Coùêª, Arùêª. Best results are in bold, second-best are underlined. 1.06 1.45 2. 1.81 1.84 2.08 Method Revid.ai-base OpenArt-story AutoMV (full) Cost($) Time IB(%) Teùê∫ Poùê∫ Coùê∫ Arùê∫ Teùêª Poùêª Coùêª Arùêª Expert 10 5-10 min 19.9 3.28 4.28 4.20 4.26 1.25 1.03 1.01 1.00 20-40 10-20 min 18.5 4.23 4.35 4.09 4.24 2.42 1.45 1.10 1.15 24.4 4.30 4.55 4.59 3.61 2.94 2.05 2.62 2.12 10-20 30 min AutoMV (w/o lyrics info) AutoMV (w/o character bank) AutoMV (w/o verifier) 22.8 24.3 23.5 2.64 1.57 1.83 1.40 2.29 1.57 2.15 1.41 2.24 2.11 2.39 1.63 Human (experts) 10k Weeks 24.1 4.74 4.70 4.56 3.20 3.82 2.99 2.95 2.17 2.90 Human expert evaluation We conduct human subjective study using the same 12-criteria rubric described previously. We recruit independent musicians and experienced music-industry professionals, including long-time record-label practitioners, music-product managers, and MV directors, all of whom have extensive expertise in rhythm, emotional expression, and musicvisual storytellingdimensions that typical viewers may overlook. Participants watch each full generated together with its corresponding audio track and assign 15 scores for every sub-criterion. These scores are aggregated in the same manner as the LLM evaluation to obtain Technical, Post-Production, Content, and Art scores, followed by weighted final score. This provides human-grounded benchmark that reflects expert perception of musicality, synchronization and artistic quality, enabling comparison between LLM judgments human evaluations. 4.2. Experiment setup and bselines We evaluate AutoMV on curated set of 30 professionally released from YouTube, spanning English, Chinese, Japanese, and Korean songs. For each song, we extract the full audio track and process it through our music-aware preprocessing pipeline (beat/structure analysis, source separation, lyrics transcription, and captioning), followed by the full AutoMV generation pipeline described in Section 3. This ensures that all systems operate on the same audio input and comparable metadata. All local inference is performed on server equipped with eight NVIDIA A800 80GB GPUs with CUDA 12.8, offering large-memory GPU compute suitable for long-form video generation. To contextualize AutoMVs performance, we compare against two commercial, closedsource video generation platforms: OpenArt narrtive mode and Review.ai basewhich represent strong proprietary baselines for textand audio-conditioned video creation. We prohibit human editing when using such products ensure generating is only based on music. In addition, we include the original ground-truths, created by professional experts team, as an upper bound for human-quality production. All methods are evaluated using the same objective metrics and the 12-criterion LLM/human scoring described previously. Narrative video model of OpenArt MV. revidl.ai 10 4.3. Ablation study In this section, we introduce following ablation experiments to verify the importance of each component in AutoMV. Without lyrics and timestamps. To evaluate the importance of music-informed guidance, we remove the Whisper-based lyric transcription and corresponding timestamp alignment. In this setting, the Gemini creenwriter no longer receives semantic or rhythmic cues from the lyrics, relying solely on Songformer structure analysis. Without character bank. To investigate the impact of structured character conditioning, we disable the character bank and remove all appearance-related constraints. This configuration tests whether the system can maintain consistent character identity and visual coherence across multiple shots without explicit feature guidance. Without Gemini Verifier. To assess the role of quality control in the generation pipeline, we remove the Gemini Verifier agents that evaluate both images and videos. The system accepts unfiltered first-round generation resultseven content that is physically unrealistic or inconsistent with the script requirements will be adopted. 5. Results and Discussion 5.1. Main results Table 1 shows that AutoMV achieves the strongest overall performance among all automatic systems, outperforming both commercial baselines in semantic audiovisual alignment (IB), LLM-based quality scores, and expert human ratings. Compared to Revid.ai-base and OpenArtstory, AutoMV delivers substantially higher ImageBind scores, indicating tighter correspondence between the generated visuals and the underlying music. Human evaluators similarly prefer AutoMV compared to baselines by large margin. In terms of efficiency and cost, AutoMV maintains competitive cost of 1020 USD per song, comparable to Revid.ai-base and cheaper than OpenArt story mode, (notably cheaper than other modes). Although our current implementation requires around 30 minutes or more per song, this reflects the limitations of our available hardware and single-node inference setup. With more parallelism, faster GPUs, and production-grade infrastructure team, the end-to-end runtime could be reduced substantially. Even under these constrained conditions, AutoMV produces far more consistent and musically grounded results than commercial instantgeneration systems. Using Gemini-2.5-Pro, we evaluate MVs produced by human experts, OpenArt-story, Revid.ai-base, and AutoMV across four dimensions: Music Content, Technical, Post-production, and Art. The results demonstrate that LLMs rankings human expert judgments. In Technical and Post-production, the human-expert-directed MVs received the highest scores, followed by the AutoMV generated results, with Revid.ai-base ranks the last. For Music Content category, AutoMV achieves comparable scores to human expert-directed MVs, while OpenArt-story and Revid.ai-base scored significantly lower. Interestingly, commercial baselines systems receive the highest LLM score in Art; however, this comes mainly from the AI Feature Showcase subcategory, where its strong AI-styled artifacts are interpreted as artistic novelty by the LLM. Finally, the gap between AutoMV and professionally produced ground-truth MVs persists, but narrows meaningfully in metrics sensitive to music alignment (IB) and high-level creative coherence (human score). This indicates that structured music-aware reasoningrather than raw model scale aloneis crucial for long-form MV generation. 11 Table 2 reports expert ratings across all 12 sub-criteria in all 4 aspects. We invited senior practitioners from leading music companies, including independent musicians, experienced MV directors or MV product managers, to evaluate the MVs according to strict rubrics for each sub-question (please refer to the supplementary for details). In particular, criteria related to innovation and AI-specific artistry are scored conservatively, since many commercial MVs are formulaic and not clearly better than simple baselines, leading to relatively low human (expert) score in table 1. Overall, AutoMV (full) substantially outperforms commercial generators. Compared with Revid.ai-base and OpenArt-story in all 4 categories. AutoMV achieves much higher Technical scores, driven by improvements in Character Consistency (CC), Physical Authenticity (PA), Lip Sync (LS), and Visual Harmony (VH). In Post-Production, AutoMV improves Shot Continuity (SC) and AudioVisual Correlation (AC), reflecting better rhythm-aware editing. In Content, AutoMV closes large part of the gap to human-produced MVs, with notably stronger Musical Theme relevance (MT), Storytelling (ST), and Emotional Expression (EM) than commercial systems. For Art, AutoMV achieves higher Visual Quality (VQ) and the best AI Novelty among all automatic methods, though all methodsincluding human MVsreceive relatively low Creativity (CR) scores under our very strict, innovation-focused rubric. Human (expert) MVs still set the upper bound, especially in Tech (3.82) and Cont (2.95). Nevertheless, the proximity of AutoMVs scores to experts on several axes (e.g., CC, VQ) indicates training-free music-aware pipeline can approach professional quality on long-form MVs. Detailed component-wise gains are analyzed in our ablation studies in the following subsections. 5.2. Ablation results Table 2. Detailed human (expert) score across 12 criteria and 4 category scores. Abbr. of 12 sub-categaries: CC = Character Consistency, PA = Physical Authenticity, LS = Lip Sync, VH = Visual Harmony; SC = Shot Continuity, AC = AudioVisual Correlation; MT = Musical Theme, ST = Storytelling, EM = Emotional Expression; VQ = Visual Quality, CR = Creativity, and AN = AI Novelty. All scores are in [1, 5] (higher is better). Best results are in bold, second-best are underlined. Weighted average of all criteria is the expert score in table 1. Method Revid.ai-base OpenArt-story AtoMV (full) CC PA LS VH SC AC MT ST EM VQ CR AN 1.00 2.00 1.00 1.00 1.03 1.03 1.03 1.00 1.00 1.00 1.00 1.00 2.95 2.69 1.55 2.48 1.68 1.23 1.16 1.11 1.04 1.29 1.05 1.10 3.07 2.95 2.67 3.07 2.00 2.10 3.08 2.18 2.60 3.28 1.23 1.83 AutoMV (w/o lyrics info) 2.00 2.93 2.64 3.00 2.00 1.14 2.11 1.29 2.11 2.07 1.07 1.07 AutoMV (w/o figure bank) 1.22 2.37 2.70 2.85 2.00 1.15 2.54 1.33 2.57 2.04 1.11 1.07 2.33 2.30 2.33 2.00 2.00 2.22 2.52 2.11 2.54 2.61 1.06 1.22 AutoMV (w/o verifier) Human (experts) 3.79 4.14 3.48 3.86 2.95 3.02 3.17 2.71 2.98 3.40 2.06 1.05 Without lyrics & timestamps. Removing lyric and timestamp guidance primarily affects Content, Post-Production, and Art, while leaving Technical quality largely unchanged. Contentrelated scores drop most sharplyMT, ST, and EMshowing that the system loses its ability to follow the songs meaning and emotional trajectory. In Post-Production, AC declines significantly, indicating weaker alignment between visual events and musical phrasing, even though the internal consistency of shot transitions remains similar. Art scores (VQ, CR, AN) also decrease, suggesting that lyric information provides creative cues beyond what music 12 captioning alone can supply. In contrast, Technical dimensions such as PA, LS, and VH remain relatively stable, confirming that lyrics do not strongly influence physical realism, lip-sync accuracy, or basic visual harmony quality. Without character bank. Removing the character bank severely disrupts identity coherence and stylistic stability. CC drops from 3.07 to 1.22, and this loss of consistent characters also impacts perceived PA. Although LS remains high and EM stays competitive, experts highlight mismatched faces, clothing changes, and even character swaps across shots. Commercial baselines typically rely on fixed, single-character library, but real songs often express emotions toward multiple people, making multi-character consistency essential. Without structured character conditioning, viewers struggle to identify stable protagonist, leading to lower scores in MT and ST. These show that long-form MVs need more than good per-shot renderingpersistent, well-defined character design is crucial for narrative coherence. Without Gemini verifiers. Removing the Gemini verifiers leads to more subtle but still meaningful degradation. The w/o verifier variant retains relatively strong Post-Production, with comparable SC and even slightly higher AC, but suffers in several Artistic and Technical dimensions. PA and VH are lower, and VQ drops from 3.28 to 2.61, indicating more artifacts and less polished visuals. AN also decreases, as many interesting but flawed generations are no longer filtered out. Experts report more physically implausible motion and off-script details, confirming that verifier-driven rejection and re-generation are important for enforcing realism and maintaining the higher-end quality plateau achieved by full AutoMV. 5.3. Case study We further illustrate the advantages of AutoMV over baseline methods through representative example, as shown in fig. 4. Compared with OpenArt-story, which exhibits limited character actions, few distinct roles, and minimal interaction with the scene, and Revid.ai-base, which generates mostly static images lacking expressive motion, our method produces visually rich and dynamic content. AutoMV maintains identity consistency across shots, preserving character appearance and attire under varying poses and camera angles (Fig. 4a). Moreover, it generates diverse and semantically coherent visual narratives, including singing, dancing, and interactions with the environment (Fig. 4b). All examples are produced from the same input music, highlighting AutoMVs ability to generate coherent, expressive, and contextually engaging music videos while preserving both character integrity and content variety. 6. Conclusion We presented AutoMV, training-free, multi-agent system for generating coherent, full-length music videos directly from audio. By combining music-aware preprocessing, script-driven planning, character design, and verifier-guided refinement, AutoMV addresses key limitations of existing methods, such as musical alignment, narrative structure, and long-term visual consistency. Experiments on 30 songs show that AutoMV consistently outperforms strong commercial systems across semantic, technical, editorial, and artistic dimensions, significantly narrowing the gap to professional MVs. detailed human study with industry experts confirms these gains, while LLM-based evaluation provides scalable complementary metric. Despite this progress, challenges remain. Human-directed MVs still achieve higher overall quality, and current multimodal LLMs cannot fully replace expert judgment. Future work includes improving character consistency over longer sequences, enhancing creative reasoning, and exploring more efficient inference pipelines. Overall, AutoMV demonstrates that music-aware reasoning and structured multi-agent coordination are essential for producing long-form, storydriven MVs, opening possibilities for scalable music video production."
        },
        {
            "title": "Acknowledgement",
            "content": "Chaoran Zhu Yinghao Ma is research student at the UKRI Centre for Doctoral Training in Artificial Intelligence and Music, supported by UK Research and Innovation [grant number EP/S022694/1]. Yinghao Ma also acknowledges the support of Google PhD Fellowship. Dr. Yang Liu Dr. Jiaheng Liu We would like to thank musicians who participate the subjective evaluation."
        },
        {
            "title": "Ethics and Social Impact",
            "content": "AutoMV is designed to support creative production while minimizing potential ethical, legal, and societal risks. First, to prevent misuse and confusion between synthetic and authentic media, we strongly advocate that all generated music videos be clearly labeled with AI-generated tags and metadata, ensuring transparent distinction from human-produced works. As generative quality improves, such labeling becomes increasingly critical for public trust and accountability. Second, copyright protection is strictly respected. The benchmark does not redistribute any commercial audio. Instead, we release only YouTube URLs of original music videos, enabling future researchers to evaluate their systems under the same protocol using large multimodal judges (e.g., Gemini-3-Pro, Gemini-2.5-Pro), without violating music copyright law. Third, we recognize the long-standing issue of under-representation in music, video, and language resources, including minority music styles, visual identities, and low-resource lyric languages. We encourage future dataset curation and system design to actively address representational imbalance and cultural diversity. Fourth, the human expert evaluation involving musicians and industry professionals follows established institutional ethics review requirements, with informed consent and voluntary participation. Fifth, the MIT open-source license lowers the technical barrier for independent musicians and small creators, enabling low-cost MV production and potentially increasing visibility and income on platforms such as YouTube. However, our results still show clear quality gap compared with large commercial productions. AutoMV cannot replace human creators in the short term, and long-term labor and creative displacement risks require continuous monitoring. Sixth, as visual generation realism improves, there is an increasing risk of portrait rights and identity misuse. Generated content that resembles real individuals may raise legal and ethical concerns, especially without consent. To mitigate these risks, we advocate several safeguards for future work: (1) integrating imperceptible audio watermarks into generated content for traceability, (2) establishing clear terms of use and content attribution guidelines in downstream deployment, and (3) encouraging community-driven audits of generated outputs for misuse detection. We believe that proactive governancetogether with transparency, open research practices, and stakeholder engagementcan help balance creative empowerment with ethical responsibility. Finally, we confirm that no international personal data transfer occurs between the UK and mainland China. The system uses no personal or sensitive data: inputs consist only of copyrightsafe music excerpts, public YouTube links, and synthetic or stock imagery. No political content or human-subject data is involved. All experiments are conducted on M-A-P infrastructure, 14 and the released code follows the MIT license. The project has no defense, dual-use, or criticalinfrastructure relevance and complies with export-control regulations and national security laws in both regions."
        },
        {
            "title": "References",
            "content": "O. Bar-Tal, H. Chefer, O. Tov, C. Herrmann, R. Paiss, S. Zada, A. Ephrat, J. Hur, G. Liu, A. Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. R. J. Burgess. The art of music production: The theory and practice. Oxford University Press, 2013. C. Chen, S. Dang, Y. Liu, N. Zhao, Y. Shi, and N. Cao. Mv-crafter: An intelligent system for music-guided video generation. ACM Transactions on Interactive Intelligent Systems, 15(3):127, 2025. J. Chen et al. Pixart-Œ¥: Fast and controllable image generation with latent consistency models. arXiv preprint arXiv:2401.05252, 2024. G. Comanici, E. Bieber, M. Schaekermann, I. Pasupat, N. Sachdeva, I. Dhillon, M. Blistein, O. Ram, D. Zhang, E. Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y. Adi, and A. D√©fossez. Simple and controllable music generation. Advances in Neural Information Processing Systems, 36:4770447720, 2023. A. Deshmane and V. Mart√≠nez-de Alb√©niz. Come together, right now? an empirical study of collaborations in the music industry. Management Science, 69(12):72177235, 2023. Z. Dong, X. Liu, B. Chen, P. Polak, and P. Zhang. Musechat: conversational music recommendation system for videos. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1277512785, 2024. Z. Evans, J. D. Parker, C. Carr, Z. Zukowski, J. Taylor, and J. Pons. Stable audio open. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. R. Gal et al. Breathing life into sketches using text-to-video priors. CVPR, 2024. Y. Gao, H. Guo, T. Hoang, W. Huang, L. Jiang, F. Kong, H. Li, J. Li, L. Li, X. Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin, and I. Misra. Imagebind: One embedding space to bind them all, 2023. URL https://arxiv.org/abs/2305.05665. L. Gong, X. Hou, F. Li, L. Li, X. Lian, F. Liu, L. Liu, W. Liu, W. Lu, Y. Shi, et al. Seedream 2.0: native chinese-english bilingual image generation foundation model. arXiv preprint arXiv:2503.07703, 2025. A. Gupta, L. Yu, K. Sohn, X. Gu, M. Hahn, F.-F. Li, I. Essa, L. Jiang, and J. Lezama. Photorealistic video generation with diffusion models. In European Conference on Computer Vision, pages 393411. Springer, 2024. C. Hao, R. Yuan, J. Yao, Q. Deng, X. Bai, W. Xue, and L. Xie. Songformer: Scaling music structure analysis with heterogeneous supervision. arXiv preprint arXiv:2510.02797, 2025. R. Henschel, L. Khachatryan, H. Poghosyan, D. Hayrapetyan, V. Tadevosyan, Z. Wang, S. Navasardyan, and H. Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25682577, 2025. K. Huang, Y. Huang, X. Ning, Z. Lin, Y. Wang, and X. Liu. Genmac: compositional text-to-video generation with multi-agent collaboration. arXiv preprint arXiv:2412.04440, 2024. 15 J. Kang, S. Poria, and D. Herremans. Video2music: Suitable music generation from videos using an affective multimodal transformer model. Expert Systems with Applications, 249:123640, 2024. J. Kim et al. Arbitrary-scale image generation and upsampling using latent diffusion model. CVPR, 2024. C. Li, Y. Chen, Y. Ji, J. Xu, Z. Cui, S. Li, Y. Zhang, J. Tang, Z. Song, D. Zhang, Y. He, H. Liu, Y. Wang, Q. Wang, Z. Wu, J. Luo, Z. Pan, W. Xie, C. Zhang, Z. Wang, J. Tian, Y. Wang, Z. Cao, M. Dai, K. Wang, R. Wen, Y. Ma, Y. Pan, S. Chang, T. Taheri, H. Xia, C. Plachouras, E. Benetos, Y. Li, G. Zhang, J. Yang, T. Peng, Z. Wang, M. Liu, J. Peng, Z. Zhang, and J. Liu. Omnivideobench: Towards audio-visual understanding evaluation for omni mllms, 2025. URL https://arxiv.org/abs/2510.10689. H. Li et al. Self-discovering interpretable diffusion latent directions for responsible text-to-image generation. CVPR, 2024a. S. Li, Y. Qin, M. Zheng, X. Jin, and Y. Liu. Diff-bgm: diffusion model for video background music generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2734827357, 2024b. Y. Li, G. Zhang, Y. Ma, R. Yuan, K. Zhu, H. Guo, Y. Liang, J. Liu, Z. Wang, J. Yang, et al. Omnibench: Towards the future of universal omni-language models. arXiv preprint arXiv:2409.15272, 2024c. B. Lin, Y. Ge, X. Cheng, Z. Li, B. Zhu, S. Wang, X. He, Y. Ye, S. Yuan, L. Chen, et al. Open-sora plan: Open-source large video generation model. arXiv preprint arXiv:2412.00131, 2024. H. Lin, A. Zala, J. Cho, and M. Bansal. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning. arXiv preprint arXiv:2309.15091, 2023. Y. Ma, A. √òland, A. Ragni, B. M. Del Sette, C. Saitis, C. Donahue, C. Lin, C. Plachouras, E. Benetos, E. Shatri, et al. Foundation models for music: survey. arXiv preprint arXiv:2408.14340, 2024. Z. Ma, Y. Ma, Y. Zhu, C. Yang, Y.-W. Chao, R. Xu, W. Chen, Y. Chen, Z. Chen, J. Cong, et al. Mmar: challenging benchmark for deep reasoning in speech, audio, music, and their mix. arXiv preprint arXiv:2505.13032, 2025. Z. Mao, M. Zhao, Q. Wu, Z. Zhong, W.-H. Liao, H. Wakaki, and Y. Mitsufuji. Cross-modal learning for music-to-music-video description generation. arXiv preprint arXiv:2503.11190, 2025. K. Nan, R. Xie, P. Zhou, T. Fan, Z. Yang, Z. Chen, X. Li, J. Yang, and Y. Tai. Openvid-1m: large-scale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371, 2024. X. Qu, Y. Ma, Z. Zhou, K. M. Lo, J. Liu, R. Yuan, L. Min, X. Liu, T. Zhang, X. Du, et al. Mupt: generative symbolic music pretrained transformer. In The Thirteenth International Conference on Learning Representations. A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. L. Ren, H. Wang, J. Li, Y. Tang, and C. Yang. Aigc for industrial time series: From deep-generative models to large-generative models. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 2025. Y. Rong, J. Wang, G. Lei, S. Yang, and L. Liu. Audiogenie: training-free multi-agent framework for diverse multimodality-to-multiaudio generation. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 88728881, 2025. S. Rouard, F. Massa, and A. D√©fossez. Hybrid transformers for music source separation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. F. Schneider, O. Kamal, Z. Jin, and B. Sch√∂lkopf. Mo√ªsai: Efficient text-to-music diffusion models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 80508068, 2024. K. Su, J. Y. Li, Q. Huang, D. Kuzmin, J. Lee, C. Donahue, F. Sha, A. Jansen, Y. Wang, M. Verzetti, et al. V2meow: Meowing to the visual beat via video-to-music generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 49524960, 2024. Y. Tian, L. Yang, H. Yang, Y. Gao, Y. Deng, J. Chen, X. Wang, Z. Yu, X. Tao, P. Wan, et al. Videotetris: Towards compositional text-to-video generation. Advances in Neural Information Processing Systems, 37: 2948929513, 2024. Z. Tian, Z. Liu, R. Yuan, J. Pan, Q. Liu, X. Tan, Q. Chen, W. Xue, and Y. Guo. Vidmuse: simple video-to-music generation framework with long-short-term modeling. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1878218793, 2025. T. Wan, A. Wang, B. Ai, B. Wen, C. Mao, C.-W. Xie, D. Chen, F. Yu, H. Zhao, J. Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. X. Wang, Y. Hong, and X. He. Exploring artificial intelligence generated content (aigc) applications in the metaverse: Challenges, solutions, and future directions. IET Blockchain, 4(4):365378, 2024a. X. Wang et al. recipe for scaling up text-to-video generation with text-free videos. CVPR, 2024b. Y. Wang, S. Wu, J. Hu, X. Du, Y. Peng, Y. Huang, S. Fan, X. Li, F. Yu, and M. Sun. Notagen: Advancing musicality in symbolic music generation with large language model training paradigms. In Proceedings of the Thirty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2025, Montreal, Canada, August 16-22, 2025, pages 1020710215. ijcai.org, 2025. doi: 10.24963/IJCAI.2025/1134. URL https://doi.org/10.24963/ijcai.2025/1134. Z. Wei, M. Li, Z. Zhang, R. Yuan, P. Hui, H. Qu, J. Evans, M. Agrawala, and A. Rao. Hollywood town: Long-video generation via cross-modal multi-agent orchestration. arXiv preprint arXiv:2510.22431, 2025. J. Xu, Z. Guo, J. He, H. Hu, T. He, S. Bai, K. Chen, J. Wang, Y. Fan, K. Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. T. Yu, W. Yang, J. Xu, and Y. Pan. Barriers to industry adoption of ai video generation tools: study based on the perspectives of video production professionals in china. Applied Sciences, 14(13):5770, 2024. R. Yuan, H. Lin, S. Guo, G. Zhang, J. Pan, Y. Zang, H. Liu, Y. Liang, W. Ma, X. Du, et al. Yue: Scaling open foundation models for long-form music generation. arXiv preprint arXiv:2503.08638, 2025. Y. Zeng, G. Wei, J. Zheng, J. Zou, Y. Wei, Y. Zhang, and H. Li. Make pixels dance: High-dynamic video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88508860, 2024. J. Zhang et al. Diffusion-4k: Ultra-high-resolution image synthesis with latent diffusion models. CVPR, 2025a. Y. Zhang, X. Xu, X. Xu, L. Liu, and Y. Chen. Long-video audio synthesis with multi-agent collaboration. arXiv preprint arXiv:2503.10719, 2025b. 17 Figure 4. Visualised comparisons with baselines. Our method performs better in identity consistency (a) and content diversity (b), including singing and dancing, which are crucial for music videos. The examples are generated from the same input music. A. Demo Comparison We kindly invite reviewers to view the attached video demos, which compare the generated full-length music videos (MVs). Compared with other baselines, our pipeline achieves better identity consistency, content diversity, story flow, and alignment between the music, lyrics, and visual content, while maintaining lower cost. In addition, we observe that MVs generated by Revid.ai-base show good lyricsvisual alignment but have almost no narrative structure. Conversely, OpenArt-Story performs well in maintaining character identity and producing coherent storylines, yet its results consist mainly of simple, static shots that cannot be mixed with singing and dancing scenes, and its lyricsimage correspondence is relatively weaker. B. Detailed Protocol of LLM and Human Experts Scoring Gemini 2.5 Pro/Flash, Gemini 3 Pro, Qwen2.5-Omni, and Qwen3-Omni are prompted with the full-length generated music video and the original audio track. It evaluates 12 sub-criteria, each defined by five-level rubric describing failure modes and ideal behavior. All ratings use 15 integer scale. This rubric is also adopted as the guideline for human evaluation. Four Evaluation Categories Technical (20%) Average of four sub-points (each weighted 5% in final score): 1. Character Consistency: Stability of appearance across scenes. 1 point: Character appearance changes frequently, facial features, clothing, and body type show obvious inconsistencies, making it impossible for viewers to confirm it is the same character. 2 points: Character appearance has more than 3 obvious inconsistencies, such as sudden changes in facial features or hairstyle, that significantly affect the viewing experience. 3 points: Character appearance is basically consistent, with 12 minor inconsistencies that do not affect the overall viewing experience. 4 points: Character appearance maintains high consistency, and details (such as makeup or accessories) transition naturally between different scenes. 5 points: Character appearance remains perfectly consistent throughout, and even in complex lighting and posture changes, details remain precise. 2. Physical Authenticity: Adherence to real-world physics and natural motion. 1 point: Serious violation of physical rules throughout, movements are stiff, and objects or characters move against basic physical laws, making it clearly inconsistent with real-world logic. 2 points: Multiple (more than 5) physical inconsistencies are present, such as floating sensation, model penetration, incorrect object interaction, discontinuous movement, or gravity anomalies. 3 points: Generally follows physical rules, simple movements appear natural, but complex interactions (multi-object collisions, fluids, or fabrics) have obvious flaws. 4 points: Physics effects approach realism, and various object interactions (character movements, environmental elements, or special effects) generally conform to physical rules with only minor details lacking. 5 points: Perfect physical performance, and all element interactions appear as if filmed in reality, including complex movements, environmental physics (water, fire, or smoke), and minute details conforming to natural laws. 3. Lip Sync Accuracy: Alignment between mouth movement and lyrics. 19 1 point: Lip movements completely mismatched with lyrics throughout, and they are obviously misaligned or static. 2 points: Over 30% of lyrical segments have mismatched lip movements, and high notes or special pronunciations have obvious errors. 3 points: Main lyrics are synchronized, but details (such as consonants or prolonged sounds) lack precision, making it approximately 1020% asynchronous. 4 points: Over 95% perfect lip synchronization, including fast-paced segments, with only occasional complex pronunciations showing slight deviations. 5 points: Professional-grade lip synchronization including all syllables, breathing, and emotional changes, which is indistinguishable from professional singers live performance. 4. Visual Style Harmony: Color, rendering, and aesthetic consistency. 1 point: Chaotic visual style, and color tone, texture, and rendering style are completely inconsistent between different scenes. 2 points: Obvious style discontinuities are present, such as realistic scenes suddenly switching to cartoon style without transition design. 3 points: The style is basically unified, and individual scenes (such as special effects or dream sequences) have stylistic differences but with clear intention. 4 points: The visual style is highly unified throughout, and scene transitions maintain consistent aesthetics and coordinated color scheme. 5 points: Perfect visual consistency while achieving rich visual layers within unified style, forming unique visual identity. Post-Production (20%) Average of two sub-points (each weighted 10%): 1. Shot Continuity: Pacing, transitions, and temporal/spatial coherence. 1 point: Abrupt shot connections, numerous jump cuts, and chaotic spatial or temporal logic, making it difficult for viewers to follow. 2 points: Lacking basic editing techniques, transitions are crude, rhythm is missing, and there are multiple discontinuous images. 3 points: Uses conventional editing techniques, shot connections are basically smooth, and the rhythm generally matches the music. 4 points: Sophisticated editing techniques, rich shot language, and creative transitions are used, with clear spatial logic and precise rhythm control. 5 points: Master-level editing standard, perfect shot narrative, and every cut has well-considered artistic purpose, forming unique editing style. 2. AudioVisual Correlation: Alignment between visuals and musical rhythm/emotion. 1 point: Images are almost unrelated to music, and rhythm, mood, and highlight segments are all asynchronous. 2 points: Basic audio-visual synchronization (such as drum beats corresponding to image cuts) is present, but lacking deeper connection. 3 points: Important musical nodes (chorus or climax) have clear visual correspondence, and basic emotional matching is present. 4 points: Precise audio-visual synchronization, including rhythm, emotional layers, and musical details with visual counterparts. 5 points: Music and images achieve symbiotic relationship, and visual elements become extensions of the music, forming unique \"audiovisual language.\" Content (30%) Average of three sub-points (each weighted 10%): 20 1. Musical Theme Relevance: Alignment of the visuals with the songs meaning. 1 point: Video content is completely unrelated to the song theme, and musical emotions contradict visual emotions. 2 points: Superficial relevance (such as literal lyric presentation) is present, but lacking understanding and expression of deeper musical meaning. 3 points: Accurately grasps the core musical theme, and the visual narrative basically matches the songs emotional progression. 4 points: Deeply explores the songs connotations, and enriches musical expression through visual metaphors or symbolic techniques. 5 points: Video becomes an inseparable part of the music, expands musical meaning through unique perspective, and mutually enhances artistic heights. 2. Storytelling: Narrative structure and character coherence. 1 point: No clear narrative structure, random image compilation, and viewers are unable to understand the story or theme. 2 points: Has basic narrative elements but confused logic, character motivations are unclear, and plot development is discontinuous. 3 points: Complete narrative structure (beginning-development-climax-conclusion), and basic plot logic is clear. 4 points: Carefully designed narrative framework, character portrayals are wellrounded, plot twists are meaningful, and themes are clear. 5 points: Innovative narrative techniques, multi-layered story structure, and leaves room for thought while maintaining emotional resonance, achieving short film-level narrative standard. 3. Emotional Expression: Clarity and depth of emotional portrayal. 1 point: Emotional expression is flat or fake, and character expressions/movements lack emotional conviction, meaning viewers cannot engage. 2 points: Emotional expression is one-dimensional, lacks layered changes, and fails to touch viewers emotional resonance points. 3 points: Appropriate basic emotional expression, and natural character mood changes can elicit basic empathy from viewers. 4 points: Rich, multi-layered emotional expression, and nuanced emotional changes can trigger deep emotional resonance from viewers. 5 points: Emotional expression achieves artistic sublimation, creates profound emotional experience through exquisite audiovisual language, and produces lasting emotional impact on viewers. Art (30%) Average of three sub-points (each weighted 10%): 1. Visual Composition & Quality: Lighting, framing, and overall fidelity. 1 point: Image quality is rough, resolution is low, composition is random, and lighting is absent, making the quality similar to early amateur productions. 2 points: Basic image quality is clear, but composition is flat, lighting is simple, and quality is equivalent to an ordinary internet video level. 3 points: Professional standard image quality, standard composition, and basic lighting design, making the quality approaching commercial MV standards. 4 points: High-quality visual presentation, composition and lighting are carefully designed, and color aesthetics are harmonious, resulting in excellent quality. 5 points: Cinematic-level visual quality, every frame is meticulously constructed, and lighting and color achieve artistic standards, allowing it to be appreciated as visual art independently. 21 2. Creativity: Novelty of concepts, scenes, and transitions. 1 point: No innovation throughout, and it completely adopts existing MV templates, making the concept highly similar to works from the past three years. 2 points: Only 12 common creative points (such as conventional transitions or basic effects) are present, and the core concept lacks uniqueness. 3 points: Thematic innovation is clear (such as narrative structure reorganization), but execution references existing cases. 4 points: Breakthrough visual symbols (such as new camera movement devices) are used, and it achieves conceptual innovation in at least 3 scenes. 5 points: Original worldview throughout, and at least two scenes or camera movements overturn traditional MV design paradigms. 3. AI Novelty: Meaningful integration of AI-native aesthetics. 1 point: Completely fails to utilize AI technology characteristics, or deliberately imitates traditional filming effects while concealing AI features. 2 points: Passively displays AI characteristics (such as occasional deformation or style breaks), but does not incorporate them into creative design. 3 points: Consciously showcases AI aesthetics (such as style fusion or surreal transformation), but remains at technical demonstration level. 4 points: Creatively uses AI characteristics as expressive means, forms unique visual language, and serves narrative or emotional purposes. 5 points: Elevates AI characteristics to artistic language, creates visual \"spectacles\" impossible with traditional photography while maintaining artistic integrity. This LLM-based framework captures semantic, narrative, and artistic qualities that traditional metrics cannot evaluate. It aligns with human judgment for long-form music videos and provides reproducible, low-variance evaluation signal suited for future M2V research, while human evaluation serves as parallel assessment track that directly reflects expert perception. For expermental results of LLM-judger, please refer to section D. For relation between human expert score and LLM judger, please refer to section F. C. Details of Baseline Currently, no open-source music-video generation pipeline is publicly available. Therefore, we selected two proprietary and closed-source piplines as baseline models: OpenArt-story mode** and Revid.ai-base . OpenArt is widely used platform for music-to-video generation and offers multiple generation modes. We adopt its narrative mode because it aligns most closely with the goal of designing story-driven music videos. Although OpenArt also provides singer mode\" that focuses on singer-centric clips, we do not include it in our comparison because it lacks narrative structure and primarily stitches together singing Close-up shot video clips. In addition, the characters in OpenArt outputs must be selected from predefined character bank, which cannot be adapted to the input music. The OpenArt-story pipeline first generates images and then produces video clips before final composition, using several proprietary image and video generation models. For our experiments, we selected Doubao for image generation, identical to our pipeline, and the Hailuo model for video generation to reduce the cost of API usage. The current cost of OpenArt MV generation is around $20 per song, still higher than our proposed method. The video generation quality may be better if we select better video **Narrative video model of OpenArt MV. revid.ai generation backbone from OpenArt, though the story script and the characters typically remain unchanged. Revid.ai-base is the second pipeline included for comparison. We only evaluate the base version, which animates AI-generated images. We exclude the Pro version because its videogeneration cost is excessive, being more than three times higher than that of our pipeline. The internal models used by Revid.ai are not publicly disclosed. For both OpenArt and Revid.ai, we disable all manual editing options to ensure that the generated results are conditioned solely on the input music. D. Detail Results of LLM-score Per-categary Tables 3, 4, 5, 6, and 7 present the detailed LLM scores for the five models evaluated: Gemini3-Pro-Preview, Gemini-2.5-Pro, Gemini-2.5-Flash, Qwen-Omni-3, and Qwen-Omni-2.5. It is important to note that for the Qwen-Omni-3 and Qwen-Omni-2.5 models, we only processed 30-second clip from the middle of each video, containing approximately 4-6 shots. This was necessary due to their input limitation of 20,000 visual tokens and 30 second video clips with audio track and text prompts include 18k tokens already. Table 3. Detailed Gemini-2.5-Flash evaluation across 12 sub-criteria and 4 category scores. Abbr. of 12 sub-categaries: CC = Character Consistency, PA = Physical Authenticity, LS = Lip Sync, VH = Visual Harmony; SC = Shot Continuity, AC = AudioVisual Correlation; MT = Musical Theme, ST = Storytelling, EM = Emotional Expression; VQ = Visual Quality, CR = Creativity, and AN = AI Novelty. Abbr. of categaries: Tech = Technical, Post = Post-Production, and Cont = Content. All scores are in [1, 5] (higher is better). Method CC PA LS VH SC AC MT ST EM VQ CR AN Tech Post Cont Art OpenArt-story AutoMV (full) 4.77 3.93 3.00 4.63 4.07 4.43 4.60 3.43 4.10 4.63 3.90 4.10 4.19 4.25 4.04 4.21 4.13 4.33 4.13 4.43 4.10 4.83 4.87 4.00 4.80 4.73 3.77 2.67 4.26 4.47 4.56 3.72 Human (experts) 4.87 4.53 4.37 4.53 4.07 4.90 5.00 3.97 4.67 4.50 4.03 1.27 4.58 4.49 4.55 3. Table 4. Detailed Gemini-2.5-Pro evaluation across 12 sub-criteria and 4 category scores. Abbr. of 12 sub-categaries: CC = Character Consistency, PA = Physical Authenticity, LS = Lip Sync, VH = Visual Harmony; SC = Shot Continuity, AC = AudioVisual Correlation; MT = Musical Theme, ST = Storytelling, EM = Emotional Expression; VQ = Visual Quality, CR = Creativity, and AN = AI Novelty. Abbr. of categaries: Tech = Technical, Post = Post-Production, and Cont = Content. All scores are in [1, 5] (higher is better). Method CC PA LS VH SC AC MT ST EM VQ CR AN Tech Post Cont Art OpenArt-story AutoMV (full) 4.60 4.10 3.33 4.90 4.10 4.60 4.70 3.47 4.10 4.83 3.47 4.43 4.23 4.35 4.09 4.24 4.07 4.30 4.37 4.47 4.30 4.80 4.90 4.13 4.73 4.70 3.27 2.87 4.30 4.55 4.59 3.61 Human (experts) 4.93 4.63 4.67 4.83 4.50 4.90 5.00 3.87 4.70 4.70 3.90 1.00 4.77 4.70 4.52 3. Overall, we observe that Gemini-2.5 / Gemini-3 Pro series produce structured, discriminative scores that broadly follow human preferences, while Qwen-Omni models tend to compress the score range and often saturate near the upper bound, though much cheaper. Gemini-2.5-Flash and 2.5-Pro. table 34 show that both Flash and Pro clearly separate OpenArt-story, AutoMV, and Human across all four categories. In Technical and PostProduction, human MVs receive the highest scores, AutoMV is consistently second, and Ope23 Table 5. Detailed Gemini-3-pro-preview evaluation across 12 sub-criteria and 4 category scores. Abbr. of 12 sub-categaries: CC = Character Consistency, PA = Physical Authenticity, LS = Lip Sync, VH = Visual Harmony; SC = Shot Continuity, AC = AudioVisual Correlation; MT = Musical Theme, ST = Storytelling, EM = Emotional Expression; VQ = Visual Quality, CR = Creativity, and AN = AI Novelty. Abbr. of categaries: Tech = Technical, Post = Post-Production, and Cont = Content. All scores are in [1, 5] (higher is better). Method CC PA LS VH SC AC MT ST EM VQ CR AN Tech Post Cont Art OpenArt-story AutoMV (full) 3.40 2.90 2.26 4.13 3.30 3.73 3.90 2.60 3.00 4.17 2.37 3.23 3.17 3.52 3.17 3.26 3.47 3.10 2.47 4.20 3.73 4.03 4.27 3.33 3.73 4.23 2.76 3.43 3.31 3.88 3.78 3.47 Human (experts) 4.87 4.73 3.93 4.80 4.47 4.57 4.73 3.50 4.33 4.60 3.40 1.40 4.58 4.52 4.19 3.13 Table 6. Detailed Qwen-Omni-3 evaluation across 12 sub-criteria and 4 category scores. Abbr. of 12 sub-categaries: CC = Character Consistency, PA = Physical Authenticity, LS = Lip Sync, VH = Visual Harmony; SC = Shot Continuity, AC = AudioVisual Correlation; MT = Musical Theme, ST = Storytelling, EM = Emotional Expression; VQ = Visual Quality, CR = Creativity, and AN = AI Novelty. Abbr. of categaries: Tech = Technical, Post = Post-Production, and Cont = Content. All scores are in [1, 5] (higher is better). Method CC PA LS VH SC AC MT ST EM VQ CR AN Tech Post Cont Art OpenArt-story AutoMV (full) 4.00 4.00 4.00 4.00 3.00 3.33 4.00 4.00 4.00 4.00 4.00 4.00 4.00 3.17 4.00 4.00 3.93 3.93 3.93 4.00 3.00 3.07 3.63 4.00 4.00 4.00 3.97 4.00 3.95 3.04 3.88 3.99 Human (experts) 4.00 3.80 4.00 4.00 3.00 3.13 3.90 4.07 4.07 4.07 4.00 4.00 3.95 3.07 4.01 4.02 nArt is lastmatching our expert study. In Content, AutoMV often approaches or slightly surpasses human scores, reflecting its strong music-aware planning. In Art, both Gemini variants appear sensitive to AI-styled visuals: OpenArt sometimes scores higher due to strong AI effects in the AI Novelty (AN) dimension, while AutoMV remains closer to human videos in more classical criteria such as **VQ** and EM. Overall, Gemini-2.5-Pro is slightly more conservative and consistent with human rankings than Flash. Gemini-3-Pro-Preview. table 5 shows similar ranking pattern but with generally lower absolute scores and narrower spread. Human MVs still dominate, AutoMV remains clearly better than OpenArt in Content and Post-Production, and the relative ordering across subcriteria is mostly preserved. This suggests that Gemini-3-Pro-Preview may be somewhat more cautious and less separative than Gemini-2.5-Pro in this preliminary version, but section demonstrate it has better sample level correlation to human experts compared to Gemini-2.5. Qwen-Omni-3 (30-second clips). In table 6, scores for OpenArt, AutoMV, and Human are very close (often within 0.10.3), with many criteria hovering around 3.84.1. Because Qwen-Omni-3 only sees 30-second mid-song clip (due to token limits), it mostly reflects the local quality of few shots rather than full-song structure. As result, it struggles to capture long-term narrative, rhythm-aware editing, or identity consistency, and becomes much less discriminative than Gemini models. Qwen-Omni-2.5 (30-second clips). Table 7 shows an even stronger saturation effect: almost all methods, including baselines, receive scores between 4.7 and 5.0 across most criteria. This makes relative ranking unreliable, even though the ordering (Human AutoMV OpenArt) is roughly reasonable. These results highlight that, under current prompting and context constraints, Qwen-Omni-2.5 behaves more like generous pass/fail checker than finegrained evaluator. 24 Table 7. Detailed Qwen-Omni-2.5 evaluation across 12 sub-criteria and 4 category scores. Abbr. of 12 sub-categaries: CC = Character Consistency, PA = Physical Authenticity, LS = Lip Sync, VH = Visual Harmony; SC = Shot Continuity, AC = AudioVisual Correlation; MT = Musical Theme, ST = Storytelling, EM = Emotional Expression; VQ = Visual Quality, CR = Creativity, and AN = AI Novelty. Abbr. of categaries: Tech = Technical, Post = Post-Production, and Cont = Content. All scores are in [1, 5] (higher is better). Method CC PA LS VH SC AC MT ST EM VQ CR AN Tech Post Cont Art OpenArt-story AutoMV (full) 4.80 4.73 4.80 4.93 4.57 4.70 5.00 5.00 5.00 5.00 4.16 4.20 4.82 4.64 5.00 4.45 4.83 4.83 4.90 4.97 4.67 4.77 5.00 5.00 5.00 5.00 4.07 4.23 4.88 4.72 5.00 4.43 Human (experts) 4.77 4.70 4.77 4.70 4.20 4.07 4.87 4.97 4.87 5.00 4.03 4.20 4.74 4.14 4.90 4.41 Across all five tables, we conclude that Gemini-2.5/3-Pro are the most aligned and discriminative LLM-judger, closely tracking expert assessments while preserving meaningful gaps between methods. Qwen-based judges remain useful as sanity checks, but their limited temporal context and score saturation make them less suitable as primary evaluators for long-form music video generation. E. Failure Case Study and Future Work This section analyzes characteristic failure cases in AutoMV. Although our pipeline successfully automates music video generation, the output quality remains inconsistent due to limitations from foundation models (e.g., video generation APIs, ASR modules) . We categorize the observed issues into four groups: physical implausibility, off-beat dance movements, textual inconsistencies, and lip-sync mismatch. Figure 5. Examples of physically implausible generations Physically Implausible. Despite leveraging the Gemini-based verification module, the pipeline occasionally generates unrealistic outputs. Typical failure cases include abnormal hand-object contacts, glowing eyes, and anatomically implausible human poses. AutoMV allows for manual regeneration of unsatisfactory images/videos, enabling users to proactively circumvent such issues through human intervention. Off-Beat Dance Movements. Although AutoMV produces visually dynamic dance sequences, precise synchronization between motion trajectories and musical beats remains challenging. Existing beat-tracking and rhythm-alignment methods are not sufficiently robust 25 across diverse music styles and temporal patterns, and thus are not employed as module. We treat beat-aligned motion generation as separate and complex research problem that requires further exploration. Figure 6. Handwritten letter close-up with inconsistent glyph shapes (the second line in the image) and temporal progression. Textual Inconsistencies. The video generation model exhibits limitations in producing coherent and accurate on-screen text. This issue becomes pronounced when the Gemini director introduces textual elements as part of the visual narrative (e.g., close-up shots of handwritten letters). Current video generation models struggle to render consistent glyph shapes and temporal progression of characters. Additionally, the ASR component (Whisper) is not fully robust, and transcription errors may propagate across downstream modules. Consequently, the final video may contain subtitles or embedded text that appear visually distorted, illegible, or semantically misaligned with the audio content. Figure 7. (a) without source separation(b) source separation Lip-Sync mismatch without source separation. The lip-sync model processes mixed au26 dio tracks, causing visible misalignment between mouth movements and lyrical syllables, which is particularly pronounced in Jay Chous songs as shown in fig. 7. Lip-syncing with source separation significantly outperforms the non-separation approach in both accuracy and naturalness.Thus, source separation is employed in our main experiments. F. Human eval v.s. Gemini eval v.s. Rule-based eval Figure 8. Pearson correlation coefficients between model-generated or objective scores and human ratings across 17 evaluation metrics. The heatmap displays correlations for six models (Gemini-3, Gemini-2.5-Pro, Gemini-2.5-Flash, Qwen-Omni-3, Qwen-Omni-2.5) across 12 subcriteria (Character Consistency(CC), Physical Authenticity(PA), Lip Sync Accuracy(LS), Visual Harmony(VH), Shot Continuity(SC), Audio-Visual Correlation(AC), Musical Theme Fit(MT), Storytelling(ST), Emotional Expression(EM), Visual Quality(VQ), Creativity(CR), AI Novelty(AN)), 4 category scores (Technical, Post-Production, Content, Artistic), and the Weighted Total score. Darker shades of blue indicate stronger correlation, while value of 0 signifies no correlation or that the evaluation method is not applicable to the metric. fig. 8 illustrates the relationship between human expert judgments and Model-based evaluations. The results indicate that an LLMs performance in evaluating music videos is directly correlated with its video understanding capabilities. Models with superior video understanding, such as the Gemini series (Gemini-3-Pro-Preview, Gemini-2.5-Pro, and Gemini-2.5-Flash), demonstrate higher correlation with human judgments on the Technical, Post-Production, and Content categories compared to the Qwen-Omni series. Notably, the most capable model, Gemini-3-Pro-Preview, shows significant alignment with human preferences on \"Physical Authenticity,\" \"Shot Continuity,\" and \"Emotional Expression.\" These findings suggest that employing models with strong video understanding for music video evaluation is reliable approach. Furthermore, ImageBind, compact multimodal model capable of jointly encoding visual, auditory, and textual data, also exhibits moderate correlation (0.274) with the overall human-rated scores, despite providing only single holistic score."
        }
    ],
    "affiliations": [
        "Beijing University of Posts and Telecommunications",
        "Hong Kong University of Science and Technology",
        "Nanjing University",
        "Queen Mary University of London",
        "University of Manchester"
    ]
}