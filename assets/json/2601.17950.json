{
    "paper_title": "UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders",
    "authors": [
        "Matthew Walmer",
        "Saksham Suri",
        "Anirud Aggarwal",
        "Abhinav Shrivastava"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The space of task-agnostic feature upsampling has emerged as a promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as a shortcut to achieve dense features for a fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with state-of-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers a versatile and efficient approach to creating denser features."
        },
        {
            "title": "Start",
            "content": "UPLiFT: Efficient Pixel-Dense Feature Upsampling with Local Attenders Matthew Walmer1 Saksham Suri2 Anirud Aggarwal1 Abhinav Shrivastava1 1University of Maryland, College Park 2Meta 6 2 0 J 5 2 ] . [ 1 0 5 9 7 1 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The space of task-agnostic feature upsampling has emerged as promising area of research to efficiently create denser features from pre-trained visual backbones. These methods act as shortcut to achieve dense features for fraction of the cost by learning to map low-resolution features to high-resolution versions. While early works in this space used iterative upsampling approaches, more recent works have switched to cross-attention-based methods, which risk falling into the same efficiency scaling problems of the backbones they are upsampling. In this work, we demonstrate that iterative upsampling methods can still compete with cross-attention-based methods; moreover, they can achieve state-of-the-art performance with lower inference costs. We propose UPLiFT, an architecture for Universal Pixel-dense Lightweight Feature Transforms. We also propose an efficient Local Attender operator to overcome the limitations of prior iterative feature upsampling methods. This operator uses an alternative attentional pooling formulation defined fully locally. We show that our Local Attender allows UPLiFT to maintain stable features throughout upsampling, enabling state-of-the-art performance with lower inference costs than existing pixel-dense feature upsamplers. In addition, we apply UPLiFT to generative downstream tasks and show that it achieves competitive performance with stateof-the-art Coupled Flow Matching models for VAE feature upsampling. Altogether, UPLiFT offers versatile and efficient approach to creating denser features. 1. Introduction With the increasing spread of deep learning networks in various real-world applications, efficiency for visual information processing continues to be of key importance. General-purpose, pre-trained visual backbones, such as DINO [4, 34, 42], can be driving force for efficiency, as these models provide strong starting point for various visual applications, and reduce the total training cost needed to develop systems for new tasks. However, DINO Code: https://github.com/mwalmer-umd/UPLiFT/ Web: https://www.cs.umd.edu/mwalmer/uplift/ Figure 1. UPLiFT time-scaling and dense features. We present UPLiFT, an efficient feature-upsampler that leverages our new Local Attender to extract semantically-stable, pixel-dense features. (Top) UPLiFTs inference time and memory scales linearly with the number of visual tokens, while other recent SOTA methods face quadratic scaling. (Bottom) PCA visualization of lowresolution DINOv2 features and pixel-dense UPLiFT features. and other Vision Transformer (ViT) [13] backbones face fundamental problem: they must down-sample the spatial dimension to make visual tokens, which limits the final feature map density. For many applications, denser features are necessity for effective performance, and while it is possible to increase the token density of ViTs to yield denser features [2], the cost of self-attention operations scale quadratically with the number of visual tokens in the image. Thus, we face fundamental trade-off between the benefits of denser features and the computational cost they incur. For this reason, feature super-resolution methods have recently grown in popularity [10, 15, 18, 43, 51]. These methods act as task-agnostic add-ons to existing visual fea1 ture extracting backbones to directly transform their coarse, low-resolution features into high-resolution ones that preserve the original semantics. Such approaches have the potential to combine the benefits of dense features with the strength of pre-trained visual backbones, while avoiding the quadratic scaling cost of self-attention. Early approaches [15, 43] used multiple steps of iterative 2 feature upsampling to push the coarse backbone features to pixel-scaledensity. More recent approaches have switched to using cross-attention-based feature pooling to directly upsample to any desired output size. While these approaches offer strength and flexibility, they risk falling back into the pitfall of quadratic time scaling. In this work, we examine the recent advances in taskagnostic feature super-resolution methods, and propose new, efficient upsampler architecture called UPLiFT (Universal Pixel-dense Lightweight Feature Transforms). We draw inspiration from LiFT [43], and we demonstrate that iterative upsampling methods are still strong competitor with more recent cross-attention-based approaches. Not only does our UPLiFT model achieve state-of-the-art performance on several dense prediction tasks, it does so with lower inference costs than comparable recent methods. To achieve this, we propose new visual operator based on the observations of [48], which showed that some ViT attention heads consistently learn to compute offset local attentional operations. As such, we propose new Local Attender operator, an alternative formulation for attention which eschews the now standardized Query-Key-Value formulation and instead defines all operations based on local relative positions. We show that this localized attentional pooling mechanism achieves the same benefits of consistent feature semantics while avoiding the scaling costs of cross-attention. As shown in Figure 1, our UPLiFT method maintains roughly linear time scaling with respect to visual token count, while recent state-of-the-art cross-attentionbased methods display quadratic time and high memory use. Finally, we apply our UPLiFT model to range of dense downstream tasks, including both predictive and generative tasks. In addition to pre-trained ViT features, we apply UPLiFT to upsample the latent features of Variational Autoencoders (VAE) [21], enabling applications in image superresolution and efficient image generation. We show that in this domain, UPLiFT achieves competitive performance with state-of-the-art Coupled Flow-Matching (CFM) models [41], while running with much lower inference cost. Overall, our contributions include: UPLiFT, an efficient latent feature upsampler that can be applied to create high-quality, pixel-dense features. Local Attender operator that reformulates attentional pooling into an efficient, locally-defined operation. We show that our Local Attender enables UPLiFT to maintaining consistent feature semantics, while avoiding the Figure 2. UPLiFT Tasks. We demonstrate our UPLiFT feature upsampler for applications in both predictive and generative tasks. This includes semantic segmentation, monocular depth estimation, image super-resolution, and efficient text-to-image generation. higher costs of comparable recent approaches. State-of-the-art performance on several dense predictive tasks, surpassing prior feature upsamplers while having faster inference speeds. An extended study of feature upsampling in the realm of generative tasks, showing the strength and efficiency of UPLiFT in comparison to more computationally expensive Coupled Flow Matching method. 2. Related Works 2.1. Feature Upsampling for Visual Backbones Higher-resolution visual features can be beneficial for many tasks, but extracting denser features comes with larger computational cost. Feature-upsampling methods are shortcut to create denser features from pre-trained, generalpurpose visual backbones like DINO [4, 34, 42], MoCo [5, 6, 16], MAE [17], CLIP [37], and SigLip [45, 53]. In the past, many works have been proposed to design learnable feature adapters which are trained with supervision from particular downstream task [29, 31, 49, 55]. More recently, there has been an increasing number of methods proposed to output denser features in task-agnostic way [10, 15, 18, 43, 51]. These methods are typically selfsupervised, and are usually trained to upsample the features of particular pre-trained backbone. Such upsamplers can streamline development for dense tasks, by providing semantically rich and dense features directly out of the box. Early works in this area [15, 43] used iterative upsampling with simple modules to create denser features. FeatUp [15] proposed two feature upsampling methods, the first us2 ing an improved Joint Bilinear Upsampler (JBU) [22] and the second using per-image Implicit Networks [8]. The first method provides high-speed upsampling but lower upsampling quality, while the latter provides incredibly sharp and high-quality upsampled features, but has high inference cost as it requires training an implicit model for each image. LiFT [43] proposed Lightweight Feature Transforming network with simple self-supervised learning process to create 2 upsampled features, with the potential for larger upsampling through iterative application of the same module. While this approach is simple and efficient, it has been shown that LiFTs iterative upsampling can lead to semantic drift and degraded features [10]. In this work, we propose new iterative feature upsampler, UPLiFT, which overcomes the limitations of prior similar approaches. More recent works [10, 18, 51] have shifted to using cross-attention-based feature resampling. LoftUp [18] and JAFAR [10] both propose query-key-value (QKV) cross-attention [47] approach that combines high-resolution queries derived from an input image, with low-resolution keys and values derived from backbone features to perform flexible and direct upsampling. Moreover, this approach acts as an effective regularizer, ensuring the output features maintain similar distribution to the input features. AnyUp [51] further adapts the JAFAR architecture [10] into backbone-agnostic model, with special adapter layer that allows it to generalize to other feature extractors at inference time. These approaches show strong results, but their use of QKV cross-attention makes them less efficient, giving them quadratic time-scaling with respect to the number of visual tokens. In contrast, our proposed UPLiFT architecture enjoys the same benefits of stable features, while also remaining linear in time with respect to the number of image patches. We also recognize that UPLiFTs use of fixed-step upsamplers makes it slightly less flexible than cross-attention-based methods; however, given that most approaches simply aim to upsample to pixel-dense features, we believe this is not significant detriment. ily end-to-end generative architectures for super-resolution [7, 11, 24, 33, 40]. However, for this work, we specifically wish to study methods which upsample VAE latent features, with image-super-resolution being possible application of said features. Meanwhile, extensive research has also been devoted to adapting pre-trained visual diffusion models to generate images at resolutions larger than their original training scale [19, 25, 27, 36, 44, 52]. These methods may achieve similar effect: adapting pre-trained generator to create larger images, but they involve modifying the generator module/process itself, not the generated latents. These approaches also typically come with significantly increased generation costs, which feature upsampling could avoid. Moreover, recent generator architectures like [23] are designed to flexibly output features at range of latent code sizes, but they still face the trade-off that visual feature extractors face: larger features come with larger extraction/generation cost. For this work, we aim to compare with methods that specifically learn to upsample VAE latent features. The state-of-the-art method in said space is Coupled FlowMatching (CFM) [41], which uses powerful FlowMatching model [1, 28, 30] conditioned on low-resolution VAE features to predict high-resolution VAE features. CFM is trained using learning objective very similar to LiFT and UPLiFT, and it can be applied to upsample VAE latent codes either from encoded real images or from generator like Stable Diffusion [38]. Unlike UPLiFT, CFM performs upscaling by first decoding the latent representation into pixel space, applying bilinear upscaling therein, and then re-encoding the result into latent space. The CFM module then refines the high-resolution features; however, the overall effect is still to produce high resolution latents from low resolution ones. In this work, we show that UPLiFT can also learn to effectively upsample VAE features that produce output images with similar visual fidelity to CFM. Moreover, UPLiFT achieves this with fraction of the parameters, training data, and inference cost of CFM. 2.2. Feature Upsampling for Generative Tasks 3. UPLiFT Approach The majority of prior works in task-agnostic feature upsampling have focused on visual feature extractors typically designed for predictive downstream tasks. Meanwhile, relatively little attention has been given to designing upsamplers for features for generative tasks. For example, the feature-spaces of Variational Autoencoders [21] are now commonly used in Latent Diffusion models [39] to provided compressed space for image generation. Because VAEs have decoders, upsampled VAE features can easily be decoded to enable tasks like image super-resolution and efficient text-to-image generation. We note that both of these tasks are areas of significant research in their own right. For example, many methods exist that train primarIn this work, we propose an iteratively-growing feature upsampling approach inspired by LiFT [43]. While LiFT achieved effective 2 feature upsampling, its approach to iterative upsampling for pixel-dense features could lead to semantic drift and degraded performance in downstream tasks. In this work, we present UPLiFT, new upsampler architecture that overcomes these issues to efficiently produce pixel-dense features while maintaining consistency in the feature distribution. Thanks to our Local Attender, UPLiFTs speed and memory requirements scale linearly with respect to the number of input visual patches/tokens, which leads to better speed and scaling properties than recent cross-attention-based methods. Figure 3. UPLiFT Inference. At inference time, our UPLiFT Encoder (EUPLiFT) produces shallow but dense features to guide all subIterative application of the UPLiFT Decoder (DUPLiFT) upsamples the low-resolution backbone features to sequent upsampling steps. pixel-density. Our proposed Local Attender module is integrated with the UPLiFT Decoder to maintain iterative feature consistency. 3.1. Architecture Overview We first illustrate our UPLiFT architecture in its inferencetime configuration, as shown in Figure 3. UPLiFT has two primary modules: the UPLiFT encoder, denoted as EUPLiFT, and the UPLiFT decoder, denoted as DUPLiFT. DUPLiFT does the primary work of predicting upsampled features. Both are simple convolutional modules, which use strided convolution for downsampling or transpose convolution for upsampling. Note that we train single compact DUPLiFT that performs 2 upsampling, and that same module is applied multiple times to achieve pixel-dense upsampling. Meanwhile, this decoder is also guided by additional highresolution features extracted by EUPLiFT from the input image. We follow the same intuition as LiFT: the input image already is strong source of high-resolution information to guide feature upsampling. However, unlike LiFT, which required re-running its image encoder on every step with bilinearly upsampled input images, we instead design EUPLiFT such that it only runs once with the base input image. To do this, we make EUPLiFT into shallow but dense encoder which preserves the original pixel density in its output features. These dense encoder features are downsampled with nearest-neighbors downsampling to the appropriate size to guide each upsampling step with DUPLiFT. These design choices make UPLiFT both efficient and compact. However, as shown by [10], this process of iterative upsampling risks introducing semantic drift, which we address in the following section with our proposed Local Attender. 3.2. Local Attender Preserving semantic consistency in the upsampled features is key to maintaining the strength of their representations. Recent works like [18] and [10] use cross attention from high-resolution queries to low-resolution keys and values to perform upsampling. This approach implicitly acts as strong form of feature regularization, as it enforces constraint that the upsampled features must be linear combination of the input features. However, these cross-attentionbased approaches lead to quadratic-scaling costs. Moreover, we hypothesize that full cross attention over all features is not necessary, and information about only local features should be sufficient to upsample the features in given image region. We note that the recent work AnyUp [51] shares this hypothesis, and modifies JAFAR by introducing windowed attention. However, as shown by Figure 1, AnyUps windowed attention still suffers from quadratic time scaling. Instead, we propose new formulation for local attention operator, which achieves the same featureregularization benefits of cross-attention while maintaining efficiency and linear-time-scaling. We draw inspiration from [48], which showed that self-attention heads in ViTs often learn to attend to local positions with fixed directional offset. Based on this, we propose Local Attender, which instead operates over pre-set local neighborhood defined by collection of fixed directional offsets. Under our Local Attender design, all attentional operations are defined relative to the current tokens position, removing the need for positional embeddings. We illustrate our Local Attender operator in Figure 4, and define it as follows. First, we assume two input feature maps, Guide Feature and Value Feature . The Local Attender operator uses to guide linear resampling of to produce new feature map output. Specifically, is used to predict an attention map over local neighborhood around each token in . For an initial case, assume and have the same spatial dimensions, HW , but different channel depths, CG and CV . Next, we define fixed neighborhood N, which is set of 2D directional integer offsets, (i, j). For given visual token Vx,y in , it can only attend to positions Vx+i,y+j for (i, j) N. In this way, the neighborhood defines the range and size of the local attentional operations computed by the Local Attender. Moreover, the size and pattern of can be flexibly defined. We present experiments with several neighborhood designs in the Supplemental Material. In general, let = n. Next, we apply 11 convolutional layer to to con4 Figure 4. Local Attender Operator. We propose streamlined and efficient local attention operator, which gathers features over set neighborhood defined by fixed direction offsets. vert it to shape HW n. This convolutional layer is the only learnable element of the Local Attender. We then apply position-wise softmax to this feature, creating the Attender Map A. For given (x, y) position in A, the value Ax,y,k represents the attentional weight to apply to the feature at Vx+ik,y+jk where (ik, jk) is the kth offset in N. In practical terms, we compute set of offset feature maps by applying the offsets of to Value Feature with replication padding. We then multiply these maps by the Attender Map and we sum along the newly created neighborhooddimension to produce the final locally-attended features of shape HW CV . Let equal the number of spatial token in G. Then the memory and compute cost of the Local Attender scales as O(nT ), or linearly in as is constant. Finally, we relax an initial assumption to allow Local Instead, asAttender to act as an upsampling operator. sume has shape HW CV and guide has shape cHcW CG, where Z. We first group the tokens of into an HW grid of cells, each cell being of shape cc. For all tokens in given cell Cx,y with (x, y) HW , the neighborhood they can attend to is defined around the corresponding Value token at Vx,y. As such, the size of the neighborhoods remains the same, but each token in now corresponds to cc cell in G. Once again, the output features are linear sums of features in , ensuring feature consistency with respect to . The output map size is determined by G, with the final output being cHcW CV . We integrate our Local Attender operator as the final step of our UPLiFT decoder, with the initial decoder output as and the original backbone feature as . We show that the Local Attender ensures the preservation of the backbone feature distribution, which allows UPLiFT to efficiently create semantically-strong, pixel-dense features. In the following section, we show that this leads to state-of-the-art performance with faster inference speeds than recent methods. 3.3. UPLiFT Training We train UPLiFT with low-to-high-res self-supervised feature prediction objective based on LiFT, though we adapt Figure 5. UPLiFT Training. UPLiFT uses multi-step training strategy where the feature reconstruction loss is applied at all intermediate steps. All lift decoders (DUPLiFT) share the same weights. our training to explicitly include multiple upsampling steps during training. We illustrate our UPLiFT training process in Figure 5. Let represent the frozen, pre-trained visual backbone. Let EU and DU denote the UPLiFT Encoder and Decoder, which are trained jointly in this process. Let HW denote the maximum size an image is loaded at. We refer to this high-resolution image as I. We also specify training depth d, which describes the scale of downsampling applied to the images that are input to UPLiFT. For given depth d, the ground-truth image will be downsampled by factor of 2d to size (H/2d)(W/2d). We denote this low-resolution image as . Next, we pass both and through to extract low-resolution and high-resolution features = B(I) and = B(I ). Additionally, is passed through EU to produce features which are used to guide upsampling. Note that with our dense UPLiFT enE has the same spatial resolution as , while coder, has lower-resolution with size (H/2dp)(W/2dp), where is the patch size. Next, we perform feature upsampling. To start, DU receives the low resolution feature and guiding feature which is down-sampled to 2 the scale of . DU predicts upsampled features with resolution (H/2d1p)(W/2d1p), which are then provided to the Local Attender to upsample . The result is 2, which is two times larger than the original low-resolution features. This process is repeated times, until we produce 2d with size (H/p)(W/p), the same as . We then compute the reconstruction loss as the L2 distance between the upsampled feature and the high-resolution feature: Lsimple = DL2(F 2d, ) (1) This simple learning objective explicitly incorporates multiple applications of DU during training, which helps UPLiFT learn to create pixel-dense features at inference time. However, this training does not necessarily need to be ap5 Table 1. Segmentation and Depth Estimation. UPLiFT surpassed all prior upsampling methods in semantic segmentation on four datasets. For depth estimation, it is second best for δ1 and tied for best in RMSE. UPLiFT achieves these scores with faster inference speeds than recent SOTA methods. Best results are marked in bold and second best are underlined. Semantic Segmentation Depth Estimation Upsampler COCO VOC ADE20K Cityscapes COCO Method Params (M) Time (ms) mIoU Acc mIoU Acc mIoU Acc mIoU Acc δ1 RMSE Nearest Bilinear LiFT-2 LiFT FeatUp LoftUp JAFAR AnyUp UPLiFT 1.2 1.2 0.2 4.3 0.7 0.9 0.8 0.6 2. 3.8 51.9 109.6 223.5 111.7 146.7 79.4 56.41 59.41 58.28 57.42 61.77 62.19 61.71 62.08 62.55 77.11 79. 78.97 78.46 80.99 81.35 81.01 81.31 81.57 78.29 81.62 82.46 80.97 83.52 84.63 84.38 84.33 85.21 94.37 95. 95.73 95.37 96.06 96.33 96.22 96.23 96.51 37.86 40.43 39.74 38.95 42.07 42.16 41.96 42.25 42.97 72.17 74. 73.79 73.34 75.52 75.79 75.43 75.80 76.00 54.87 59.71 61.07 61.98 60.50 62.09 61.89 61.33 65.38 90.56 92. 93.09 93.61 93.12 93.55 93.52 93.44 56.66 58.83 57.17 55.07 60.01 58.69 60.59 61.32 94.41 61.16 0.73 0. 0.70 0.73 0.66 0.68 0.65 0.63 0.63 plied with = log2(p), as shallower depths can be sufficient to train UPLiFT. In addition, for each DU upsampling step, an additional intermediate feature map can be extracted to derive an additional loss term. Denote the full set of upsampled feature maps as: = {F = {F 2, 1/2d1, 4, ... , 2d1, 1/2d2 , ... , 2d} 1/2, 1/1} (2) We also compute intermediate ground truth feature maps by downsampling to intermediate resolutions, with F1/2k = B(I1/2k ), to give the set of target feature maps: = {F1/2d1 , F1/2d2 , ... , F1/2, F1/1} (3) Then we can compute the total loss for depth with all intermediate feature maps as: Ld = (cid:88) k=1 DL2(F 1/2dk , F1/2dk ) (4) In practice, we find UPLiFT achieves the best performance when multiple depths are used during training. For our primary results, we use := {1, 2, 3} for each training step. We present ablations with additional configurations in Appendix C. Overall, our final UPLiFT learning objective can be denoted as: LUPLiFT = (cid:88) dD Ld (5) 4. UPLiFT for Predictive Tasks Experimental Methods. We start with experiments focused on dense predictive tasks: semantic segmentation and monocular depth estimation. We compare with other 6 task-agnostic feature-upsampling methods LiFT [43], Featup [15], LoftUp [18], JAFAR [10], and AnyUp [51]. We also compare with simple baselines of bilinear and nearestneighbor upsampling. To compare with prior works, we select DINOv2-S/14 [34] as our primary standard backbone for experiments. For all tasks in this section, we use an UPLiFT model trained for one epoch on the ImageNet-1K dataset [12], with maximum ground truth image size of 448 and maximum input image size of 224. We train using our multi-step loss (Section 3.3) with 3 depth levels. We build on the experimental protocols of [10] and train linear probes models on top of the upsampled DINOv2S/14 features produced by each method. In addition, following the example of [51], we make small correction to the learning rate schedule of the official JAFAR evaluation suite. For this reason, we have recomputed all the baseline methods to provide fair comparison. All methods are run in pixel-dense upsampling mode. Following the example of [10], we also run LiFT in 2 upsampling mode followed by bilinear upsampling to pixel-dense features, which we denote as LiFT-2. All evaluations use 448448 input images and 448448 upsampled features. For each method, we also report the number of parameters and the average upsampling time for single images feature as measured on one NVIDIA A5000 GPU. Please see the Appendix for additional details. Segmentation. We present semantic segmentation results in Table 1 for four datasets: COCO-Stuff [26], Pascal VOC [14], ADE20k [54], and Cityscapes [9]. First, our results confirm the observations of [10], that iteratively running LiFT to produce pixel-dense features leads to degraded performance in this task. We find that single LiFT upsampling iteration (LiFT-2) yields better performance than iterative upsampling to pixel-dense features. Next, we see that UPLiFT achieves state-of-the-art performance, with the highest mIoU and Accuracy scores for all four datasets, Figure 6. 20482048 Image Generation Comparison with CFM. We show that UPLiFT achieves comparable visual upsampling quality to CFM [15], while using only 1/6th the network parameters, 1/200th the training data, and only 2 iterative upsampling steps. Table 2. Zero-shot COCO-5k 5121024. UPLiFT achieves superior FID to CFM for COCO text-to-image upsampling, while running with significantly faster inference speeds. Like [41], we measure speed for this task with batch size 4. CFM is run with 20 steps. Row adapted from [41] Diffusion Model Upsampler Performance Model Steps Method Params (M) Resolution s/img FID Table 3. Zero-shot reLAION-400M-5k 5121024. UPLiFT demonstrates strong upsampling for text-to-image, surpassing CFM in all three metrics, with faster inference. CFM is run with 4 or 40 steps. As CFM does not report diffusion steps, we include two options for comparison. Speed measured with batch size 1 as in [41]. Rows adapted from [41]. Diffusion Model Upsampler Performance Steps Method Params (M) Resolution s/img CLIP FID p-FID SD1.5 SD1.5 SD1.5 SDXL 25 25 50 CFM-20 UPLiFT 306 53 512 3.96 24.88 512 1024 512 8.79 5.15 28.81 24.23 1024 39.84 24.07 LCM SDXL 4 25 40 CFM-4 * CFM-40 * 25 UPLiFT 40 UPLiFT 306 306 53 512 512 5121024 5121024 5121024 5121024 1024 1.27 1.96 2.72 3.16 1.56 2. 0.72 31.67 31.87 26.16 26.14 31.00 31.17 22.32 21.90 21.61 21.67 21.17 20.73 15.83 15.96 16.10 15.49 31.06 28.05 24.13 Model SD1.5 SD1. SD1.5 SD1.5 SD1.5 SD1.5 even surpassing recent methods that use cross-attention feature pooling. This result demonstrates that our Local Attender approach is an effective method for feature pooling and upsampling. Finally, we see that UPLiFT has faster inference speed than all high-performing pixel-dense upsamplers. While other baselines like Nearest, Bilinear, LiFT2, and LiFT achieve faster speeds, they also have inferior performance, meaning UPLiFT provides an optimal combination of speed and feature quality. Depth Estimation. We next apply UPLiFT to monocular depth estimation on COCO-Stuff. Like [10], we report the δ1 score, which is thresholded accuracy score, and the Root Mean Square Error (RMSE). Predicting visual depth from monocular cues requires broader understanding of the entire image. In theory, the cross-attention-based methods, LoftUp, JAFAR, and AnyUp, should have an advantage in this task, as their feature pooling approach can gather broader information from wider image regions. However, we find that UPLiFT ties with AnyUp for the lowest RMSE score, and is second only to AnyUp for δ1 score. These results demonstrate that our UPLiFT features, which are upsampled using only local information through our Local Attender, can still derive sufficient global information from the backbone features to achieve competitive performance in depth estimation. Efficiency and Scaling of UPLiFT. As shown by Table 1, UPLiFT achieves faster inference speeds than recent stateof-the-art methods using cross-attention pooling. Note that those speeds are measured for an image size of 448448 with patch size of 14, which yields 1024 visual tokens. As recent methods have moved to using cross-attention for feature scaling, they now face quadratic time and memory scaling with the number of input tokens. Meanwhile, our UPLiFT method maintains linear scaling, leading to even faster performance for larger image sizes. This is essential, as producing high-resolution image features can easily become memory-intensive process. To demonstrate this, we test UPLiFT, LoftUp, JAFAR, and AnyUp for gradually increasing image sizes, until they hit memory limit of 24 GB on an A5000 GPU. We assume patch size of 16 and run Table 4. Zero-shot Image Super Resolution 2561024 on 5k samples of FacesHQ and LHQ. UPLiFT achieves superior scores in SSIM and PNSR for LHQ, and better SSIM for FacesHQ. We note that UPLiFT achieves these results with single general-purpose module, while CFM uses dataset-specific trained modules with significantly more parameters and iterations. Row adapted from [41] Upsampler FacesHQ LHQ Method Iters Params (M) SSIM PSNR FID p-FID Params (M) SSIM PSNR FID p-FID Bilinear Nearest CFM UPLiFT 1 1 50 113 53 0.72 0.66 0.82 0.84 21.74 20.48 30.40 29.93 108.12 193.08 1.36 5.20 176.90 332.28 1.62 6.62 306 0.63 0.60 0.69 0.73 22.09 21.08 25.69 26.70 147.65 241.55 2.27 5.29 224.70 288.66 2.38 7.23 all methods in pixel-dense upsampling configuration. We report the average inference speed against the number of visual tokens in Figure 1. We find that all baseline methods run out of memory at roughly 1500 visual tokens, which corresponds to an image size of 624624. At this image size, UPLiFT has inference speeds 2.5-5 faster than the baseline methods. UPLiFT can upsample images with up to 2601 visual tokens without hitting our memory limit. 5. UPLiFT for Generative Tasks Experimental Methods. In this section, we extend UPLiFT to generative tasks by applying it to VAE latent features, enabling efficient image generation and superresolution. To preserve the feature distribution for generative tasks, we find that larger UPLiFT model is necessity, but our larger model still only has one half or one sixth the parameters of the comparable CFM models. We train UPLiFT for 5 epochs on Unsplash-Lite [46], which contains 25k high-quality images. For comparison, CFMs generalpurpose model is trained on the Unsplash dataset with over 5 Million images. Using maximum ground-truth image size of 1024, we train across 4 depth levels using the multistep iterative loss introduced in Section 3.3. Both methods use the Stable Diffusion 1.5 [38] VAE to perform encoding and decoding between pixel and latent space. We follow the protocols of [41] across two generatext-to-image diffusion upscaling and image tive tasks: super-resolution. We first use 5k random samples from COCO 2014 and reLAION-400M to evaluate UPLiFTs ability to upscale diffusion features. This task effectively increases Stable Diffusion 1.5s output size from 512512 to 10241024. We compare with CFM, as well as SDXL and LCM-LoRA SDXL, which natively generate at 10241024, to provide an additional point of reference for visual quality from high-resolution latents. We also evaluate UPLiFT for image super-resolution on the FacesHQ and LHQ datasets, again following the protocols of [41]. Additionally, we compare against two lightweight baselines: direct bilinear and nearest neighbors upsampling in latent space. Reported latency is measured on single NVIDIA A100 GPU. Please see Appendix for addition details on the model, training, and tasks. Efficient High-Resolution Image Generation. On COCO (Tab. 2), UPLiFT achieves lower FID than CFM in single step while reducing latency by 41%. UPLiFT is also close to matching the FID of SDXL, which natively generatives 1024 images at much slower speed. On reLAION (Tab. 3), UPLiFT achieves strong performance and faster speeds than CFM, with our 40-step configuration producing the highest CLIP, best FID and patch-FID, while maintaining significantly lower latency. These results demonstrate UPLiFT as strong choice for efficient generative feature upscaling. Image Super-Resolution. We evaluate UPLiFT for 4 image super-resolution via latent-space-upsampling, comparing against two lightweight baselines: bilinear and nearest neighbors. Note that the baseline CFM models for this task are trained specifically for the datasets tested, while UPLiFT uses the same general-purpose model from the previous section. Despite this, UPLiFT achieves competitive visual quality using just two iterative steps, with an inference time of only 271 milliseconds per image. Remarkably, UPLiFT is just 13% slower than bilinear upsampling in latent space, yet delivers an order-of-magnitude improvement in super-resolution quality. These results highlight UPLiFTs efficiency and generalization capabilities. 6. Conclusion In this work, we have presented UPLiFT, an efficient feature-upsampling method to create high-resolution feature maps from low-resolution features of pre-trained visual backbones. We show that UPLiFT produces highperforming pixel-dense features, and it does so with lower inference costs than existing comparable methods. UPLiFT achieves this through an iterative, convolutional upsampling architecture, without need for expensive crossattention. Instead, we propose an efficient Local Attender operator, which succeeds in maintaining semantic consistency of features for minimal extra computational cost. Finally, we show that UPLiFT achieves state-of-the-art performance in range of tasks, including both predictive and generative domains. Our code and UPLiFT models will be released at time of publication. We hope that UPLiFT will help to improve the efficiency and practicality of deep models for dense visual tasks. 8 Acknowledgments. This work was partially supported by NSF CAREER Award (#2238769) to AS. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. The authors acknowledge UMDs supercomputing resources made available for conducting this research. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of NSF or the U.S. Government."
        },
        {
            "title": "References",
            "content": "[1] Michael Albergo, Nicholas Boffi, and Eric VandenEijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. 3 [2] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. arXiv preprint arXiv:2112.05814, 2(3):4, 2021. 1 [3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 13 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 1, 2 [5] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020. 2 [6] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96409649, 2021. [7] Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. Fsrnet: End-to-end learning face super-resolution with In Proceedings of the IEEE conference on facial priors. computer vision and pattern recognition, pages 24922501, 2018. 3 [8] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image In Proceedings of the IEEE/CVF conference on function. computer vision and pattern recognition, pages 86288638, 2021. 3 [9] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 32133223, 2016. 6 [10] Paul Couairon, Loick Chambon, Louis Serrano, JeanEmmanuel Haugeard, Matthieu Cord, and Nicolas Thome. Jafar: Jack up any feature at any resolution. arXiv preprint arXiv:2506.11136, 2025. 1, 2, 3, 4, 6, 7, 12, 16 [11] Ryan Dahl, Mohammad Norouzi, and Jonathon Shlens. Pixel recursive super resolution. In Proceedings of the IEEE international conference on computer vision, pages 54395448, 2017. [12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 6 [13] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1 [14] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge: retrospective. International journal of computer vision, 111(1):98136, 2015. 6 [15] Stephanie Fu, Mark Hamilton, Laura Brandt, Axel Feldman, Zhoutong Zhang, and William Freeman. Featup: modelagnostic framework for features at any resolution. arXiv preprint arXiv:2403.10516, 2024. 1, 2, 6, 7, 12, 16, 18 [16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 2 [17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 2 [18] Haiwen Huang, Anpei Chen, Volodymyr Havrylov, Andreas Geiger, and Dan Zhang. Loftup: Learning coordinatebased feature upsampler for vision foundation models. arXiv preprint arXiv:2504.14032, 2025. 1, 2, 3, 4, 6, 12, 13, 16 [19] Juno Hwang, Yong-Hyun Park, and Junghyo Jo. Upsample guidance: Scale up diffusion models without training. arXiv preprint arXiv:2404.01709, 2024. 3 [20] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448456. pmlr, 2015. [21] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3 [22] Johannes Kopf, Michael Cohen, Dani Lischinski, and Matt Uyttendaele. Joint bilateral upsampling. ACM Transactions on Graphics (ToG), 26(3):96es, 2007. 3, 12 [23] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 3 [24] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photorealistic single image super-resolution using generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 46814690, 2017. 3 9 [25] Yuming Li, Peidong Jia, Daiwei Hong, Yueru Jia, Qi She, Rui Zhao, Ming Lu, and Shanghang Zhang. Asgdiffusion: Parallel high-resolution generation with asynchronous structure guidance. arXiv preprint arXiv:2412.06163, 2024. 3 [26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. 6 [27] Zhihang Lin, Mingbao Lin, Wengyi Zhan, and Rongrong Ji. Accdiffusion v2: Towards more accurate higher-resolution diffusion extrapolation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [28] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 3 [29] Wenze Liu, Hao Lu, Hongtao Fu, and Zhiguo Cao. LearnIn Proceedings of ing to upsample by learning to sample. the IEEE/CVF international conference on computer vision, pages 60276037, 2023. 2 [30] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. 3 [31] Hao Lu, Wenze Liu, Zixuan Ye, Hongtao Fu, Yuliang Liu, and Zhiguo Cao. Sapa: Similarity-aware point affiliation for feature upsampling. Advances in Neural Information Processing Systems, 35:2088920901, 2022. 2 [32] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick Von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. 14 [33] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised photo upsampling via latent space exploration of generative models. In Proceedings of the ieee/cvf conference on computer vision and pattern recognition, pages 24372445, 2020. [34] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 1, 2, 6, 12, 13, 16 [35] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 14 [36] Haonan Qiu, Shiwei Zhang, Yujie Wei, Ruihang Chu, Hangjie Yuan, Xiang Wang, Yingya Zhang, and Ziwei Liu. Freescale: Unleashing the resolution of diffusion models via In Proceedings of the IEEE/CVF tuning-free scale fusion. International Conference on Computer Vision, pages 16893 16903, 2025. 3 [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 2 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 3, 8, 13 [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [40] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image superIEEE transactions on resolution via iterative refinement. pattern analysis and machine intelligence, 45(4):47134726, 2022. 3 [41] Johannes Schusterbauer, Ming Gui, Pingchuan Ma, Nick Stracke, Stefan Andreas Baumann, Vincent Tao Hu, and Bjorn Ommer. Fmboost: Boosting latent diffusion with flow In European Conference on Computer Vision, matching. pages 338355. Springer, 2024. 2, 3, 7, 8, 13, 14, 15, 23 [42] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 1, 2, 12, 16 [43] Saksham Suri, Matthew Walmer, Kamal Gupta, and Abhinav Shrivastava. Lift: surprisingly simple lightweight feature transform for dense vit descriptors. In European Conference on Computer Vision, pages 110128. Springer, 2024. 1, 2, 3, 6, 12, 16 [44] Athanasios Tragakis, Marco Aversa, Chaitanya Kaul, Roderick Murray-Smith, and Daniele Faccio. Is one gpu enough? pushing image generation at higher-resolutions with foundation models. arXiv preprint arXiv:2406.07251, 2(3):5, 2024. 3 [45] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 2 [46] Unsplash. Unsplash Full, Lite Dataset 1.3.0, 2025. Accessed: 14 November 2025. 8 [47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 3 [48] Matthew Walmer, Saksham Suri, Kamal Gupta, and Abhinav Shrivastava. Teaching matters: Investigating the role In Proceedings of of supervision in vision transformers. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74867496, 2023. 2, 4 [49] Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, and Dahua Lin. Carafe: Content-aware reassembly of features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 30073016, 2019. 2 [50] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to 10 structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [51] Thomas Wimmer, Prune Truong, Marie-Julie Rakotosaona, Michael Oechsle, Federico Tombari, Bernt Schiele, and Jan Eric Lenssen. Anyup: Universal feature upsampling. arXiv preprint arXiv:2510.12764, 2025. 1, 2, 3, 4, 6, 12, 16 [52] Zhen Yang, Guibao Shen, Liang Hou, Mushui Liu, Luozhou Wang, Xin Tao, Pengfei Wan, Di Zhang, and Ying-Cong Chen. Rectifiedhr: Enable efficient high-resolution image generation via energy rectification. arXiv e-prints, pages arXiv2503, 2025. 3 [53] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 2 [54] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127(3):302321, 2019. 6 [55] Minghao Zhou, Hong Wang, Yefeng Zheng, and Deyu Meng. refreshed similarity-based upsampler for diarXiv preprint rect high-ratio feature upsampling. arXiv:2407.02283, 2024. 2 11 A. Additional Details for Predictive Tasks In this work, we focus on comparing UPLiFT with other task-agnostic feature upsamplers, which have grown in popularity in recent years. Such methods take existing pretrained feature extractors, and provide an upsampler add-on to provide dense, powerful features directly out of the box. We compare with the following methods: FeatUp [15]: We compare with the JBU FeatUp variant, which is an iterative upsampler that performs 16 upsampling with stack of four modified Joint Bilinear Upsamplers [22]. While the implicit variant of FeatUp produces impressive results, it has been shown by works like [18] that this approach is too slow to be practical for large-scale evaluations, with functional inference speeds over 500 slower than comparable methods. We use the official FeatUp JBU model distributed by [15] trained for DINOv2-S/14 without backbone normalization. LiFT [43]: LiFT is also an iterative upsampling method, which in its base form performs 2 upsampling. The same LiFT model can also be applied iteratively four times to perform 16 upsampling. However, [10] has shown that this iterative upsampling can lead to semantic drift and degraded features. For this reason, we follow the example of [10] and present LiFT in two configurations: LiFT runs the model four times for 16 upsampling and LiFT-2 runs the model only once, then follows this with bilinear upsampling to pixel scale. As there is not an official LiFT model for DINOv2-S/14, we train one for this work. LoftUp [18]: LoftUp uses cross-attention to directly upsample low-resolution backbone features to pixel-scale features, with 14 upsampling step. We compare with the official LoftUp for DINOv2-S/14 model distributed by [18]. We note that this LoftUp model was originally trained with the DINOv2-S/14 backbone distributed by [34], while other recent methods (ours included) use the version distributed by Hugging Face1. While these DINOv2 models are fundamentally the same model trained on the LVD-142M dataset and should produce similar results, we choose to conduct an additional test to determine if this has any impact on the performance of LoftUp. We perform evaluation twice with LoftUp, once with each version of DINOv2-S/14, and report the results in Table 5. We find that LoftUps performance is nearly identical for both backbones, with only minor variations seen in the mIoU scores for VOC and Cityscapes. For this reason, we conclude that these minor variations in backbone distributions are not significant confounding factor in our results, and we thus choose to present results for LoftUp with its original backbone for sake of fair representation. 1https://huggingface.co/timm/vit_small_patch14_ dinov2.lvd142m JAFAR [10]: JAFAR also uses cross-attention-based approach to perform direct 14 feature upsampling. We build on the evaluation protocols of JAFAR for segmentation and depth estimation, and we compare with the official JAFAR for DINOv2-S/14 model from [10]. AnyUp [51]: AnyUp proposes modified version of the JAFAR architecture which is designed with an additional feature-agnostic layer that enables the model to generalize to different backbones at inference time. Note that AnyUp is still initially trained with features from particular backbone for upsampling training. Moreover, the results of [51] show that performing inference with different backbone than was used in training leads to poorer downstream performance than using the same backbone for both training and evaluation, suggesting that training modelspecific upsamplers is still preferable for performance. For this study, we compare with the official AnyUp model distributed by [51]. Upsampling Rates. For our main evaluation, we use DINOv2-S/14 as the standard backbone, and run all methods in configuration to produce pixel-dense features, which requires 14 upsampling. For methods that perform 16 upsampling, which includes our UPLiFT, we follow the example of [10] and simply over-upsample the features, and then downsample to pixel-resolution. We note that this puts UPLiFT at computational efficiency disadvantage compared to cross-attention-based methods, but despite this, we still achieve faster inference than the crossattention baselines. Moreover, we note that more recent self-supervised backbones, like DINOv3 [42], have moved back to using patch size 16, which makes 16 upsampling the likely desired option for future research. B. Additional Details for Generative Tasks B.1. UPLiFT for VAE Features We describe additional details of our UPLiFT model designed for VAE feature upsampling and generative tasks. UPLiFT size for VAE. We empirically find that small parameter count upsampling models, like those demonstrated to be effective for predictive downstream tasks, are insufficient for generative downstream tasks. We illustrate this in Figure 7, where we present comparison with small UPLiFT model, which has roughly 2.8M parameters and is trained using the same training protocols as our main model. In this comparison, we see that the resulting model produces blurry, low quality upsampling, and is not able to capture high-frequency information. We theorize that the necessary model capacity to train an effective feature upsampler for generative tasks is intrinsically larger than the necessary size for predictive tasks. We base this theory on the intuition that predictive tasks are about narrowing information down 12 Table 5. Evaluating the impact of minor backbone variations. We present comparison of baseline LoftUp [18] running either with its original DINOv2-S/14 backbone from [34], or with an alternate distribution of DINOv2-S/14 provided by Hugging Face, which is commonly used in other upsampling works. Note that both models are theoretically the same model and only represent different distribution platforms. We find that there is minimal variation in the resulting performance, with the largest changes being in VOC mIoU and Cityscapes mIoU. Our UPLiFT surpasses LoftUps performance for either backbone. We report results for LoftUp with its original backbone in the main work to provide fair representation of the method. Upsampler COCO VOC ADE20K Cityscapes COCO Method Params (M) Time (ms) mIoU Acc mIoU Acc mIoU Acc mIoU Acc δ1 RMSE LoftUp + Orig Backbone LoftUp + HF Backbone UPLiFT 4.3 4.3 0.8 223.5 223. 79.4 62.19 62.20 62.55 81.35 81.35 81.57 84.63 84. 85.21 96.33 96.30 96.51 42.16 42.16 42.97 75.79 75. 76.00 62.09 62.17 65.38 93.55 93.56 58.69 58.70 94. 61.16 0.68 0.68 0.63 Semantic Segmentation Depth Estimation into compressed understanding, which can be represented more compactly with smaller network, while generative tasks require high level of detail over diverse range of visual textures and patterns to achieve high quality. For this reason, we train larger UPLiFT model for generative tasks, increasing the number of layers in both the encoder and decoder module and increasing the channels per layer. This larger size UPLiFT has 53.5M parameters, which still makes it significantly smaller than the compared CFM [41] models, which have 113M or 306M parameters. Refiner Block. We find that it is beneficial to introduce an additional Refiner Block after the Local Attender module, which is designed to realign the output features with the distribution expected by the VAE decoder. We demonstrate the importance of the refiner block by ablating it in Figure 7 (Right). Without the refiner block, the upsampled images have significant blocky artifacts, which are likely the result of the strict, linear-combination feature upsampling approach. We note that our Local Attender module, and the cross-attention modules used by recent works, act as form of strong feature regularization, which ensures that the features output by the upsampler maintain similar distribution to the original input distribution, which, in theory should also match the distribution expected by the VAE decoder. However, through testing, we find that this design may be too restrictive for generative context. For this reason, we introduce post-attender Refiner Block which gives UPLiFT an opportunity to improve the final features. Noise Layers and Augmentation. We follow the example of [41] and concatenate additional gaussian random noise channel inputs at several points in the UPLiFT Encoder module when upsampling VAE features. These additional noise channels are provided to help seed the generation of high resolution details. We also apply gaussian noise augmentation to the input latent representation, following [41]. Color Correction. When training UPLiFT for VAE, all our loss terms are computed in the latent feature space, which improves the efficiency of training by removing the need to decode high-resolution images. Our UPLiFT training is effective at minimizing the L2 distance of upsampled features in latent space, however, we find that small perturbations in latent space can lead to slight color shifts after decoding. While this could likely be addressed through the use of pixel-space loss terms, this would greatly increase training costs. Instead, we introduce simple color correction module after the VAE Decoder at inference time. This module computes the per-color-channel means for the low-resolution input image and the high-resolution output image, and subtracts the difference vector from the output image to re-align the color mean. We find that this simple module is sufficient to remove any minor color shifts. Layer Normalization. Finally, for our larger VAE UPLiFT model, we find it is beneficial to replace the Batch Norm [20] layers with Layer Norm [3] instead, as it provides better numerical stability for the deeper architecture. When using Batch Norm, we sometimes observe color blob artifacts in the upsampled images, which are consequence of numerical instability in the model. After replacing the Batch Norm operations with Layer Norm, these artifacts no longer occur. B.2. Experimental Methods for Generative Tasks We summarize additional key details of our experimental protocols for generative tasks. Diffusion Upsampling. We evaluate UPLiFTs ability to upscale diffusion features on COCO 2014 and reLAION400M following the protocols of [41]. On COCO, we randomly sample 5k caption-image pairs. On reLAION, we randomly sample 5k images with minimum resolution of 10241024. While [41] used the now deprecated LAION dataset, our sample is sufficiently similar, and our computed FID with LCM-LoRA SDXL matches that of CFM. During inference, we generate latents with Stable Diffusion 1.5 [38] for each sampled caption, and then decode them to generate 5k 512512 images. We apply UPLiFT to produce 10241024 images. Note that UPLiFT leverages the image created from the decoded low-resolution latents to guide 13 Figure 7. Visual ablation of design choices for VAE UPLiFT. (Left) UPLiFT achieves high quality 5121024 upsampling with larger parameter count model. (Middle) smaller-scale UPLiFT has insufficient capacity to upsample all high-frequency information and produces blurry results. (Right) Ablation of the Refiner Block leads to blocky artifacts in upsampled images. feature upsampling, similar to CFM, which must decode the image as part of the Pixel-Space Upsampling (PSU) method. However, UPLiFT does not require re-encoding bilinearly upsampled image like CFM does, which gives an additional efficiency gain to UPLiFT. The default precision of float32 is used for all diffusion processes, and each model utilizes its default scheduler with the specified number of steps. We also compare with SDXL [35] and LCM-LoRA SDXL [32], which natively generate at 10241024, to provide an additional point of reference for visual quality from high-resolution latents. SDXL can be viewed as an upper-bound oracle for native megapixel generation, while LCM-LoRA SDXL provides point of reference of low-latency distilled model at megapixel native generation. Please note that SD 1.5 cannot natively generate latents for images at resolutions different from 512512 without significant quality degradation or alterations. [41] includes experiments upscaling 256256 to 10241024 by specially fine-tuning Stable Diffusion 1.5 model at lower resolution. For our tests, we choose to prioritize experiments using only the official Stable Diffusion 1.5 model generating latents for 512512 images. Utilizing the official code of [41], we compute FID and patch-FID, which splits the image into four random patches sized 512512. For CLIP score, we use the Python package clip score with clip-vit-base-patch32. Super-Resolution. Following CFM [41], we perform super-resolution evaluations on two datasets: FacesHQ and LHQ. We randomly sample 5k images from the LHQ dataset and the joint combination of CelebA-HQ and FFHQ datasets, called FacesHQ. For each image, we perform 10241024 center crop, followed by bilinear downsample to 256256. Finally, we apply UPLiFT to recover the full sized image. To do so, we first use Stable Diffusion 1.5s VAE to encode the image into latent space. Then we apply the UPLiFT module with 2 iterations, resulting in 4 latent, which is decoded using the same VAEs decoder back into pixel space. Performance is measured using the evaluation scripts of [41], which track SSIM [50], PSNR, FID, and patch-FID. Latency Timing. To ensure fair comparison with [41], we measure latency on single NVIDIA A100-SXM480GB across both experiments. We use batch size of 1 for experiments on reLAION, and 4 for experiments on COCO to be consistent with [41]. All experiments utilize the torch.compile module, and measurements are conducted with 3 warm-up batches. The final latency is averaged across 10 subsequent batches. C. Ablations of UPLiFT Design Choices C.1. Training Depth As discussed in Section 3.3, we train UPLiFT with multidepth, multi-step feature reconstruction training objective, where the level of downsampling and the number of upsampling steps depends on the training depth, d. Here we present an ablation over d, where we train our UPLiFT model with different training depth configurations, including individual depths, or multiple concurrent depths. We use semantic segmentation on COCO and VOC with pixeldense features as our evaluation tasks for this ablation. The results are summarized in Table 6. When training with only one depth level (rows 14), better performance is achieved with shallower depths. VOC has the best performance with d=1, and COCO has slightly better performance with d=2. We believe this occurs be14 Figure 8. Local Attender neighborhood designs. We visualize the neighborhood designs tested with our Local Attender module. The center token (orange) is always included in the neighborhood, and the blue tokens represent the offset relative local positions that are included in feature pooling through local attention. Table 6. Ablation of training depth configurations. We test COCO and VOC segmentation for different training depth levels, including individual and concurrent depth configurations. Table 7. Ablation of Local Attender neighborhood sizes. We compare UPLiFT performance with Local Attender modules with different neighborhood patterns, or without the Local Attender. Depth = 1 = 2 = 3 = 4 {1, 2} {1, 2, 3} {1, 2, 3, 4} COCO VOC COCO VOC mIoU Acc mIoU Acc Neighbors mIoU Acc mIoU Acc 62.26 62.34 62.24 60.82 62.46 62.55 62.36 81.34 81.43 81.34 80. 81.50 81.57 81.45 85.03 84.89 84.67 81.42 85.04 85.21 84.97 96.42 96.40 96.33 95.42 96.44 96.51 96.42 No LA = 5 = 9 = 13 = 17 = 25 47.52 62.15 62.33 62.51 62.55 62.46 73.06 81.30 81.42 81.53 81.57 81.50 64. 84.80 84.99 85.10 85.21 85.22 91.37 84.80 96.44 96.47 96.51 96.48 cause shallower depths give the UPLiFT Encoder large input image, which allows it to learn more about highfrequency image information. For d=3, we see slight drop in performance and for d=4 we see major drop in performance. Under d=4, the 448448 image is downsampled to 2424 which yields only 22 token grid. It seems this level of downsampling is too severe for effective learning. Next, we find that training with multiple depths concurrently can enhance performance. As shown by row 5, training with {1, 2} concurrently leads to superior performance on both COCO and VOC. This effect is further enhanced by adding in d=3. However, the addition of d=4 becomes detrimental for multi-depth training, which again indicates that downsampling the input too severely hinders learning. From this, we find that training with multi-depth configuration with {1, 2, 3} yeilds the best results, so we use this training configuration in our predictive task experiments. Note that for our VAE UPLiFT model, we train with maximum resolution of 10241024 in following [41]. For this larger image size, we find that {1, 2, 3, 4} works to good effect. C.2. Local Attender Neighborhood We propose new Local Attender module, which uses relatively-defined, variably-sized local neighborhood to perform local attention pooling in linear time. In this section, we present additional experiments with different neighborhood sizes and shapes. We limit the neighborhood size to maximum of 55. We present an illustration of the neighborhood sizes and shapes tested in Figure 8. In addition, we present results for an UPLiFT model with the Local Attender module ablated. All models are trained for one epoch on ImageNet-1k, with 448448 max image size, like the main work. Results are summarized in Table 7. First, from row 1, we see that the removal of the Local Attender module significantly harms performance, which indicates that our Local Attender is essential for maintaining effective feature upsampling through multiple steps. Next, in row 2, we test n=5 with what we consider to be the smallest viable neighborhood, which consists of only the current token and its immediately touching neighbors. Already for n=5 we see strong performance in both datasets, though this performance continues to improve as we expand the neighborhood to 9, 13, and 17 neighbors. The n=25 configuration, which compared to n=17 adds on several additional neighbors along the second layer of offsets, only leads to roughly equal performance in VOC and slightly lower performance in COCO. This result indicates that the addition of further neighbors may not be beneficial if they are added in intermediate places between existing neighbors. Overall, our best performance is achieved with the star-shaped n=17 neighborhood design, so we use this as our Local Attender neighborhood for all our experiments in the main work. 15 Table 8. Semantic Segmentation with DINOv3 and UPLiFT. We measure segmentation performance on COCO and VOC for DINOv2S/14 vs. DINOv3-S+/16. We compare against JAFAR and AnyUp, which either report results with this DINOv3 backbone or have provided an official model for it [10]. We find UPLiFT gives the best performance for all metrics, datasets, and backbones. Row adapted from [51]. DINOv2-S/ DINOv3-S+/16 Upsampler COCO VOC COCO VOC Method JAFAR AnyUp UPLiFT Params (M) mIoU 0.7 0. 0.8 61.71 62.16 62.55 Acc 81.01 81.37 81. mIoU 84.38 84.00 85.21 Acc 96.22 96.19 96. mIoU 62.47 62.99 63.74 Acc 81.50 81.84 82. mIoU 83.05 84.72 Acc 95.99 96. D. Additional Results and Visualizations D.1. UPLiFT for DINOv3 In our primary results in Section 4, we follow the example of recent feature upsampling works and use DINOv2S/14 [34] as our primary backbone for assessing UPLiFT and comparing it with baseline works. However, as of writing, DINOv3 [42] has recently been published, and we expect future work in feature upsampling will likely shift to focus on this family of backbones. For this reason, we present additional results for Semantic Segmentation using DINOv3-S+/16. We compare with JAFAR [10], which has released an official model for this backbone on their repository, and with AnyUp [51], which has published results for DINOv3 on COCO in their work. While we would additionally like to compare with LoftUp [18], no official DINOv3 LoftUp upsampler has been published as of writing, nor has official training code been made publicly available. We present results for COCO and VOC in Table 8, alongside results for DINOv2-S/14 for comparison. We see that the DINOv3 features lead to stronger performance in COCO, with improved results for DINOv3 with UPLiFT. However, DINOv3 does not necessarily yield stronger performance in VOC, as is seen for both UPLiFT and JAFAR. However, for both the DINOv2 and DINOv3 backbones, UPLiFT achieves the best semantic segmentation performance for both metrics and datasets. D.2. Additional Efficiency Comparisons In Section 4, we assessed the efficiency of UPLiFT in comparison to recent state-of-the-art cross-attention-based feature upsamplers [10, 18, 51]. We present additional efficiency results here. Along with reporting inference time, we also report the inference GPU memory usage, and we also present these results for two different GPU types: an NVIDIA A5000 with 24 GB of memory, and an NVIDIA A6000 with 48 GB of memory. We plot these results in Figure 9. First, we see that the cross-attention-based methods clearly show quadratic scaling in their memory usage. All three methods reach the A5000s 24 GB memory limit at around 1500 visual tokens, while UPLiFT can reach over 2500 tokens. We note that at the lower range of image sizes, UPLiFT uses slightly more memory than the baseline methods, but by 1000 tokens, UPLiFTs linear scaling makes it more efficient. Moreover, the improved efficience of UPLiFT is even more apparent when tested on an A6000. With twice the GPU memory, UPLiFT can process twice as many visual tokens on an A6000, while the other methods rapidly fill the extra memory. The improved speed of UPLiFT for larger images is also apparent on the A6000, with it being 24 faster than the baseline methods at 2000 tokens. D.3. Semantic Stability with UPLiFT critical issue with LiFT [43] is the occurrence of semantic drift through iterative feature upsampling. We present visual comparison of semantic drift in LiFT compared with the semantic stability achieved by UPLiFT. Following [15], we visualize the features of both methods using Principle Component Analysis (PCA) with 3 components. We extract the backbone features and all intermediate feature upsampling steps from both methods, and then perform joint-PCA over the full collection. This means that consistent colors from one image to the next means consistent features too. We visualize the results in Figure 10. Examining the PCA images for LiFT (row 1), we see that iterative upsampling leads to increasing degradation of the latents. The features become faded, murkier, and distorted through each step. Local image regions with consistent semantic content, like the dogs ear and nose region (row 3), do not maintain consistent and similar features. In comparison, UPLiFT (row 2) maintains consistent features through each upsampling step, without signs of internal distortions. In addition, the edges of objects are more distinctly defined than LiFT, and they match well to the original image, which enables better performance in tasks like semantic segmentation. Overall, these results demonstrate that our UPLiFT approach, through the use of Local Attender, is able to maintain consistent feature semantics through iterative upsampling steps to produce high-quality, pixel-dense features. 16 Figure 9. Speed and Memory Usage Comparison. We compare UPLiFT with recent cross-attention-based feature upsampling methods when running with gradually increasing image sizes. We report the average inference time and memory usage against the visual token count for two different GPU types: an NVIDIA A5000 with 24 GB of memory and an NVIDIA A6000 with 48 GB. UPLiFT maintains linear time and memory scaling with respect to the number of tokens, while the baseline methods show quadratic scaling. This gives UPLiFT far faster performance as the token count increases, and allows us to run pixel-dense feature upsampling on larger images. D.4. Additional Visualization for Generative Tasks We provide further visualizations for UPLiFT applied to generative tasks. Figure 11 highlights UPLiFTs strong and efficient performance in 4 super-resolution from 512512 to 20482048. We find that UPLiFT adds only 8.47% latency compared to bilinear upsampling in latent space, and yields far superior image quality to the naive upsampling method. Figure 12 includes 20482048 images, upscaled from Stable Diffusion 1.5, and Figure 13 shows the same at 10241024. For the super-resolution task, we include samples from the evaluation datasets FacesHQ and LHQ using 2 iterative upsampling twice, from 256256 to 10241024 in Figure 14 and Figure 15 respectively. 17 Figure 10. Comparison of semantic drift in LiFT and semantic stability in UPLiFT. We visualize intermediate feature upsampling steps through PCA, following [15]. LiFT shows signs of feature drift, with local features becoming murkier and more distorted in deeper steps. This drift can lead to poor performance in downstream tasks, as the strength of the original backbone representation is lost. UPLiFT maintains consistent feature semantics thanks to our Local Attender, and local object regions maintain consistent features (coloration) across all upsampling stages. 18 Figure 11. UPLiFT vs. Latent Space Bilinear 4 upsampling for image super-resolution. We compare the latency of UPLiFT versus applying bilinear upsampling in latent space for image super-resolution to demonstrate UPLiFTs state-of-the-art efficiency. While only 8.47% slower end-to-end, UPLiFT produces significantly better visual fidelity, as shown by the zoomed-in views (bottom) which display the source low-resolution image compared with UPLiFTs super-resolution image. The low-resolution image is selected from the FacesHQ dataset and bilinearly downscaled from 10241024 to 512512 before UPLiFT is applied. Best viewed zoomed in. Figure 12. UPLiFT 512512 20482048 upsampled images using Stable Diffusion 1.5. We apply our VAE UPLiFT model to this task in 4 upsampling configuration. UPLiFT upsamples latents corresponding to 512512 images generated using 50 diffusion steps on Stable Diffusion 1.5. Best viewed zoomed in. 20 Figure 13. UPLiFT 512512 10241024 upsampled images using Stable Diffusion 1.5. We apply our VAE UPLiFT model to this task in 2 upsampling configuration. UPLiFT upsamples latents corresponding to 512512 images generated using 50 diffusion steps on Stable Diffusion 1.5, and the end-to-end latency is 2.75 seconds on an NVIDIA A100 GPU. The UPLiFT model itself takes only 104 milliseconds of this time. Best viewed zoomed in. 21 Figure 14. UPLiFT 256256 10241024 super-resolution samples from FacesHQ. We use our VAE UPLiFT model that is not fine-tuned for image super-resolution and is only trained in latent space. Our end-to-end upsampling time is only 270.9 milliseconds on an NVIDIA A100 GPU. Best viewed zoomed in. Figure 15. UPLiFT 256256 10241024 super-resolution samples from LHQ. We use our VAE UPLiFT model, which is trained as generalist model and is not specifically fine-tuned for this dataset. The LHQ dataset presents greater challenge than FacesHQ, based on the diversity of visual textures present. Despite this challenge, we see good performance with our generalist UPLiFT. In comparison, [41] uses fine-tuned model with 3 the parameter count for evaluations on LHQ versus FacesHQ. Best viewed zoomed in."
        }
    ],
    "affiliations": [
        "Meta",
        "University of Maryland, College Park"
    ]
}