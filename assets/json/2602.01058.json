{
    "paper_title": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning",
    "authors": [
        "Dylan Zhang",
        "Yufeng Xu",
        "Haojin Wang",
        "Qingzhi Chen",
        "Hao Peng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone. We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts. We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected. We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation."
        },
        {
            "title": "Start",
            "content": "Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning Dylan Zhang 1 Yufeng (Felix) Xu 1 2 Haojin Wang 1 Qingzhi Chen 1 Hao Peng"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 6 2 0 2 1 ] . [ 1 8 5 0 1 0 . 2 0 6 2 : r Post-training of reasoning LLMs is holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone. We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to mismatch typical in current SFTRL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts. We propose PEAR (Policy Evaluationinspired Algorithm for Offline Learning Loss Reweighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected. We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen2.5/3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass@8 gains up to 14.6% on AIME-2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation. 1University of Illinois Urbana-Champaign 2New York University (Shanghai). Work done during internship at UIUC. Correspondence to: Dylan Zhang <shizhuo2@illinois.edu>, Hao Peng <haopeng@illinois.edu>. Preprint. February 3, 2026. 1 Post-training of reasoning language models typically follows two-stage paradigm: an offline supervised fine-tuning (SFT) phase produces an initial checkpoint, which is then used to initialize an online reinforcement learning (RL) phase that further enhances the model (Shao et al., 2024; Guo et al., 2025; Yang et al., 2024). Both areas have become active research fronts. In particular, growing body of work has proposed offline learning objectives to improve SFT, often by reweighting or regularizing next-token likelihood (Qin & Springenberg, 2025; Zhu et al., 2025d; Wu et al., 2025; Lin et al., 2025; Li et al., 2025a). From practical perspective, the performance of interest is usually the models final accuracy after completing both SFT and downstream RL. However, it is common that these existing techniques optimize for SFT-stage performance in isolation, often with the implicit assumption that gains in offline performance will translate to improved performance after RL. Kang et al. (2025) show that repetition and data homogeneity boost SFT but may reduce RL headroom. This motivates us to investigate if offline gains of an objective could also be misleading proxy for its effectiveness as an RL initialization. We empirically show the gains of stronger offline checkpoint over weaker one can shrink, disappear, or even reverse after both undergo identical RL training. Therefore, optimizing for offline performance alone may be counterproductive when the goal is strong final performance after RL (Fig. 1 in 2). We contend that the goal of an offline stage is not merely strong offline accuracy, but an initialization that facilitates improvement under the online RL. This requires addressing distribution mismatch between offline and online stages: Typically, during SFT, the model learns from data sampled from different distribution, often dubbed the behavior policy (Sutton & Barto, 2018; Precup et al., 2000; Uehara et al., 2022). In contrast, during online RL, the target of learning (thereby the target policy), learns from roll-outs generated by itself. There is clear distribution mismatch that needs to be corrected between them (Zhao et al., 2022; Lee et al., 2021; Zu et al., 2025) in order for an effective offline-to-online transition. It is therefore crucial to quantify and correct this distribution mismatch. Inspired by off-policy evaluation (OPE) (Precup Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning Figure 1. Offline v.s. Online pass@1 on SynLogic Games on total of 19 Models. It exhibits significant ranking changes indicating offline performance will not entail online performance. In addtion, our proposed approach remains the most effective initialization for online RL. et al., 2000; Thomas & Brunskill, 2016; Jiang & Li, 2016; Levine et al., 2020), we address this by reweighting offline data using importance weights, i.e., the likelihood ratio between the target policy and the behavior policy (3). Intuitively, this reweighting scales each tokens loss according to how likely the target policy would generate the same continuation relative to the behavior policy, so that offline training better reflects the trajectories that online RL will actually revisit. We present sequence-level (Rowland et al., 2020) and token-wise reweighting based on suffix-ratios (Precup et al., 2000). We also present variants that improve stability by block-wise weighing and leveraging negative data. We evaluate PEAR and its variants on reasoning games and math benchmarks across 6 different models of various sizes: Qwen3-Base-0.6B; 1.7B; 4B and 8B (Yang et al., 2025), Qwen2.5-1.5B-Math (Yang et al., 2024) and DeepSeekDistill-Qwen-1.5B (Guo et al., 2025). Using an SFTRL pipeline that varies only the SFT-stage objectives, PEAR and its variants consistently improve post-RL performance over strong baselines. Comparing checkpoints finetuned from Qwen3-1.7B-Base using PEAR versus canonical SFT on the same data, the former outperform the latter by by 40% absolute accuracy on logic games, and achieve 14% Pass@8 gain on AIME-25 (Balunovic et al., 2026) (See 4.3). Moreover, our analysis shows that PEAR-initialized models undergo less parameter drift during RL 4.6. 2. Offline Performance May Not Entail Online There are various techniques to improve supervised finetuning (SFT) for reasoning, typically targeting stronger offline performance or reduced forgetting. Recent reasoning LM post-training pipeline typically applies an online RL stage to further improve performance after SFT (Shao et al., 2024; Guo et al., 2025; Yang et al., 2024). In this setting, the offline stage provides the initialization for RL, and prior study (Kang et al., 2025) has identified that dataset construction and hyper-parameter afName Per-token objective / weight SFT (NLL) SFT+KL ℓ(p) = log ℓ(p) = log + β KL GeneralFamily-α ℓα(p) = (α 0 log p) TopP-q BottomP-q TopLogP-q BottomLogP-q TALR SFT+KL 1 pα α (1 p) 1[p q] (1 p) 1[p q] log(p) 1[p q] log(p) 1[p q] wt exp(ℓt/τ ) = p1/τ log(p) + βDKL Table 1. Compared objectives at the per-token level.1 fects SFT and RL performances differently in this pipeline. This naturally leads us to question: Will the advantage of an offline learning objective carry over to post-RL performance? We experiment with wide spectrum of objectives covering the span the standard SFT loss-strength spectrum: drift control (KL), smooth probability-shaped reweighting and hard masking toward high-/low-confidence tokens. Li et al. (2025a) presents generalized view of SFT loss by studying series of probability-based objectives  (Table 1)  , where one could control how strongly training emphasizes lowvs. high-probability tokens by altering the transformation of probability, and learning selectively from easy / difficult tokens. TALR similarly modifies SFT via adaptive token-wise reweighting  (Table 1)  . We follow their recommended hyperparameters; see Appendix E.2 and E.1. In addition, we consider standard negative log-likelihood (cid:2) (NLL) loss; and KL-regularized NLL L(θ) = E(x,y)D log πθ(y x)(cid:3) + β ExD[KL(πθ( x) πref ( x))] . We alter β {0.03, 0.1, 0.3, 1} for KL-regularized variant. We perform controlled, contamination-free experiment by applying each of these offline objectives followed by online RL on synthetic logic puzzles from (Liu et al., 2025). 2 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning 2.1. Offline = Online Figure 1 visualizes offline versus online performance. While several objectives indeed outperform SFT offline on Pass@1, these gains do not reliably translate to stronger post-RL models: some checkpoints are simply harder for subsequent RL to improve and ultimately lose their offline advantage. One may be tempted to pick TopLogP for Qwen3-1.7B-Base because of the best offline scores, yet this choice leads to worstamong-all post-RL performance, even under-performing SFT initialized model. We inspect other descriptors for sampling that potentially relates to RL: offline pass@K with large (Yue et al., 2025) and majority voting accuracy (Kang et al., 2025) may correlate with the RL performance, we show (in Figure 1) that the ranking is not always well-preserved when comparing the effectiveness of different techniques either. 2.2. Why Uniform Loss Is Misaligned Standard SFT and KL-distillation apply uniform token-level supervision under prefixes induced by the behavior (datagenerating) policy πβ, while online RL samples and optimizes rollouts from the evolving target policy πθ. This behaviortarget occupancy mismatchwell known in offlineto-online RLcan hurt the subsequent online phase (Huang et al., 2025; Zu et al., 2025; Lee et al., 2021; Zhao et al., 2022). In auto-regressive generation, small early mismatches shift the prefix distribution and propagate forward, compounding over long horizons (Ross et al., 2011; Mehta et al., 2024; Liu et al., 2019; Ross & Bagnell, 2014; Sun et al., 2017). This is especially acute for long-form reasoning (Guo et al., 2025), where traces often involve implicit search (trial, backtracking, self-correction): πβ may over-represent continuations that are effectively dead-ends under πθ, so uniformly training on all logged tokens can reinforce transitions that RL will rarely revisit  (Fig. 3)  . 2.3. Off-Policy Evaluation To reason about the offline-to-online mismatch, we adopt an off-policy evaluation (OPE) lens: we have logged trajectories from behavior policy πβ, while the subsequent online RL stage generates rollouts from (changing) target policy πθ. Classical OPE corrects this behaviortarget shift via change of measure with likelihood ratios (Precup et al., 2000; Jiang & Li, 2016; Uehara et al., 2022; Levine et al., 2020): Eτ πθ [f (τ )] = Eτ πβ (cid:104) πθ(τ ) (cid:105) πβ (τ ) (τ ) . OPE comprises family of estimators that correct for behaviortarget mismatch using likelihood ratio, including variants that compute likelihood ratio across the entire trajectory = (cid:81) πθ(atst) πβ (atst) (Thomas & Brunskill, 2016) t=1 Figure 2. sketch of our weighing intuition. Red numbers are probabilities under target policy, Blue numbers are probabilities under behavior policy. After token A, the behavior (data-generating) policy often continues with (e.g., 0.9 0.8), but this continuation is highly unlikely for the policy we ultimately want to optimize. As result, the offline data over-represents C, which can push the model to associate with an implausible continuation. During online RL, once the model generates A, it will rarely follow with and C, so learning from these offline continuations provides little useful signal. We therefore down-weight token to avoid visiting it. and uniformly apply to all actions for simplicity (Rowland et al., 2020) and those that compute suffix ratios from certain time step wn = (cid:81) πθ(atst) πβ (atst) for each decision (Precup et al., 2000). This naturally suggests using likelihood-ratio-based sequence or continuation weights as compatibility signals between the logged data and the current policy to correct for the mismatch mentioned in 2.2 with different granularity. t=n+1 TAKEAWAYS Offline = online: better algorithm in offline scores need not yield better post-RL performance. 3. Method To address the offline-to-online mismatch identified in 2, we introduce PEAR (Policy Evaluationinspired Algorithm for Offline Learning Loss Reweighting), reweighting scheme for offline fine-tuning that produces stronger initialization for subsequent online RL on verifiable reasoning. PEAR keeps the underlying objective unchanged (SFT or KL-based distillation) and modifies only how each tokens loss is weighted. Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning Figure 3. An illustration of how PEARs 3 variant works. APPROACH SUMMARY 3.3. Sequence-Level Weighting Step 1. Compute token log-likelihood ratios on tokens from offline dataset. Step 2. Aggregate into weights using either one of 3 variants and stabilize it. Step 3. Weigh the loss for each token. See Figure 3 for illustration. 3.1. Problem setup and notation We consider standard offline fine-tuning from dataset of promptresponse pairs. = {(x, y)}, where is prompt and is token sequence produced by known data-generating policy πβ. We train target model πθ on D. Let πθ(yt x, y<t) denote the model probability at position t, and let ℓθ(x, y<t, yt) be per-token training loss. 3.2. PEAR-weighted Offline Training Objective PEAR comes as simple weighting approach for standard offline loss on each token. Given an example (x, y) from the dataset, we first compute the weight Gt following either one of the 3 weighting strategies we will present below. We will apply numerical stabilization techniques discussed in subsequent subsection 3.7 and denote the numerically-stabilized version as ˆGt. PEAR keeps the underlying per-token loss ℓθ(x, y<t, yt) unchanged (SFT/NLL or KD/forward-KL), and only reweighs it: LPEAR(θ) E(x,y)D (cid:34) (cid:88) t=1 (cid:105) (cid:104) (cid:98)Gt sg (cid:35) ℓθ(x, y<t, yt) , where sg[] stops gradients through the weights (we treat (cid:98)Gk as fixed coefficient, not an additional differentiable path). We start by presenting the simplest form of PEAR weighting: sequence-level importance weighting. For each token, let us define πθ(ytx,y<t) πβ (ytx,y<t) , to denote the probability ratio between policy we want to train πθ and the behavior (data-generating) policy πβ. Then, the resulting sequencelevel importance ratio is w1:T πθ(yx) t=1 can be used to represent sequences relative likelihood under πθ to πβ. This allows us to estimate the loss under (cid:2)ℓθ(x, y)(cid:3) = the target policys distribution: Eyπθ(x) Eyπβ (x)[w1:T ℓθ(x, y)] . We therefore use this weight Gi w1:T = 1, 2, ..T to equally weigh each token in the trajectory. In our experiments, we show that this simple weighting mechanism can yield strong performance. πβ (yx) = (cid:81)T 3.4. Token-level Weighting Based on Continuation We then take more granular view on the sequence. Sequence-level PEAR equally applies the global importance score to each token, yet the weights may not be the same across positions, sequence may become unlikely because of implausible regions. For token yt, we evaluate whether the continuation from dataset y>t remains plausible under the model conditioned on taking yt. If the relative plausibility is small, it means gradients at time primarily encourage tokens that lead into regions that πθ is unlikely to revisit when sampling from itself. We therefore down-weight the loss on such tokens, focusing offline updates on prefixes whose continuations from the dataset are compatible with the current target policy. To this effect, we introduce tokenlevel importance weighting based on the suffix importance ratio, where Gt = γT (cid:81) j=t+1j where γ (0, 1] is 4 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning Algorithm 1 PEAR Require: One example (x, y) with = (y1, . . . , yT ); model πθ; behavior policy πβ. Require: Block size B; mode uniform or suffix; discount γ (0, 1]. Require: Token loss ℓθ(x, y<t, yt); clip bounds [ℓ, u], [Gmin, Gmax]. Ensure: Weighted loss L(θ). 1 Partition {1, . . . , } into = /B contiguous blocks token-level weighting: = 1 {Ik}K k=1; let ek = max Ik. 2 Per-token quantities. 3 δt clip(log πθ(yt x, y<t) log πβ(yt x, y<t), ℓ, u) , [T ] clipped log-ratios for numerical stability 4 ℓt ℓθ(x, y<t, yt), [T ] token loss under the base objective (SFT/KD) 5 Blockwise reductions. 6 ρk (cid:80) δt, tIk [K] 7 bk (cid:80) block log-ratio (log of within-block product of πθ/πβ) ℓt, [K] tIk aggregate loss for this block 8 if uniform then (cid:16) exp(cid:0) (cid:80)K (cid:98)GT clip k=1 ρk (cid:1), Gmin, Gmax (cid:17) 10 11 (cid:98)Gt (cid:98)GT , (cid:88) return sg [T ] (cid:105) (cid:104) (cid:98)GT bk equivalently (cid:80)T t=1 sg k=1 12 end if 13 Suffix mode: continuation weights (single backward scan). 14 0; 0 m=k+1 ρm 15 for = downto 1 do (cid:1) 16 (cid:98)Gek clip(cid:0)exp(cid:0)(T ek) log γ + u(cid:1), Gmin, Gmax weight by how plausible the remaining continuation is under πθ tracks future log-ratio: (cid:80)K (cid:104) Ik (cid:105) bk (cid:98)Gek same Gt within block 17 18 (cid:98)Gt (cid:98)Gek , + sg + ρk 19 20 end for 21 return Let Sk (cid:81)T j=ek+1 denote the importance ratio of the suffix after block (i.e., how likely the remaining continuation is under πθ relative to πβ). Equivalently, Sk can be computed block-wise as Sk = (cid:81)K We assign every token in block the same discounted continuation weight Gt = Gblk γT ek Sk, Ik. m=k+1 blk . Note that when = 1, we recover the token-level PEAR introduced in 3.4. 3.6. Optionally Incorporating Negative Examples When contains verified failures, we optionally add repulsive term that discourages imitating negative trajectories in policy-consistent way. Let = {x, y} denote failures. We can still compute sequence-level weights and apply repulsive term on those data points: Lneg(θ) E(x,y)D λ sg (cid:34) (cid:105) (cid:88) (cid:104) (cid:100)G (cid:35) <t, ) ℓθ(x, , t=1 (cid:105) (cid:104) (cid:98)Gt ℓt where (cid:100)G is sequence level weight on negative trajectories. Here, we perform gradient ascent to push the model away from the negative response with trajectory-level weight. 3.7. Numerical Stabilization For numerical stability, we compute importance weights in log-space to avoid products of ratios over long sequences. We apply clip on both per-decision ratios and final weights ˆGt as described in Algorithm 1. 4. Experiments We present careful controlled study under clean set-ups to study the effectiveness of various PEAR-based weighting, that in turn proves the insights in 2. discount factor to control variance in long horizon (Sutton & Barto, 2018; Jiang & Li, 2016). 4.1. Tasks and data 4.1.1. LOGIC GAMES 3.5. Block-Level Weighting to Improve Stability product over long horizons inevitably introduce large variance (Bossens & Thomas, 2024; Liu et al., 2018; 2020). To reduce the effective length of the multiplicative importanceweight for better stability, we present block-level variant that trades granularity for stability. We partition positions {1, . . . , } into = /B contiguous blocks {Ik}K k=1, each of length at most B. Let ek max Ik denote the last index of block k. For each block k, define the product of token-level ratios within the block as blk (cid:81) t. tIk Task Sources. We use synthetic, verifiable puzzles from SynLogic (Liu et al., 2025) and Enigmata (Chen et al., 2025b) as primary testbed. Both of them are synthetic reasoning environments that procedurally generate verifiable puzzle instances from diverse environments to allow noisefree data collation, training and evaluation. This allows for minimally confounded evaluation setting, with reduced exposure to knowledge dependence and contamination. Offline buffer construction. We generate synthetic games using the rule-based generator and de-duplicate prompts across train/test and remove any train instances that overlap with evaluation prompts. We sample responses 5 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning (a) Pass@1 comparison across initializations for SynLogic Games. PEARB=1 significantly improve upon SFT initialization, and incorporating negative gradients can further improve Pass@1. (b) PEARB=1applied to KL based knowledge distillation. Figure 4. Results on SynLogic dataset. We demonstrate that PEAR consistently improves post-RL performance.The bars reflect Pass@1 and dots mark Pass@8. In Figure 4a, Single Step IS is baseline that corrects each token only based on the probability ratio of the token itself. See 4.3. with Qwen3-8B (Yang et al., 2025) and verify final answers. The resulting offline buffer contains roughly 100,000 correct trajectories. Evaluation. We measure Pass@{1, 8} on held-out set of puzzles with the original verifiers. We evaluate using samples from SynLogics evaluation set. 4.1.2. MATH REASONING Data. For offline training, we use the subset of all math problems in SYNTHETIC-2 dataset (Prime Intellect, 2025) total of 33,400 unique instructions. We sample responses from Qwen3-8B and verify with final answer, forming dataset of 100,000 question-response pairs. For online RL, we use DAPO-17k dataset (Yu et al., 2025). Evaluation. Following common practice, we evaluate on MATH-500 (Hendrycks et al., 2021), MINERVA (Lewkowycz et al., 2022), AIME-2024, AIME-2025 and AMC-2023 (Balunovic et al., 2026). We report average accuracy across 64 samples to reduce variance and pass@K. 4.2. Training details Offline training Unless stated otherwise, we train for 1 epoch with learning rate 3105 on games and 1105 on math. For PEAR, we use γ = 0.999, clip log ˆGt to [10, 5], and clip per-decision log to [0.08, 0.3]. 4.3. Results PEARB=1 stands for token-level weighting ( 3.3), our default form of PEAR in the evaluation below that directly reflects the key intuition. PEAR improves post-RL performance under fixed RL budget. demonstrates the gain from initializing model with the token-wise form of PEAR over standard SFT across different model sizes. We show clear improvement on pass@1 across model sizes. Moreover, PEAR outperforms all previously mentioned techniques in 2. Notably, PEARs performance does not surface in terms of its out-of-the-box offline performance, and even may not beat SFT, since PEAR is not designed to boost offline scores in isolation, but to better shape the prior for online RL. In addition, we show in Table 2 that PEAR-initialized model can achieve higher overall performance across common math reasoning benchmarks for multiple model families. In Figure 4b show that PEARs weighing scheme can also work with KL-based knowledge distillation (ℓθ = KL(πβ( x, y<t) πθ( x, y<t))) and further improve upon that by computing the score using already-computed information during KD, adding minimal overhead to the KLbased KD baseline. PEARB=1 uses the exact same weighting as we apply it with NLL loss (i.e. suffix likelihood ratio). This underscores PEAR generality as plug-in reweighting approach for both commonly used offline objectives. Importantly, it proves our central hypothesis that offline stage should correct for distribution mismatch between behavior and target policies. Online RL Starting from each offline checkpoint π0, we run the same online RL procedure to obtain πRL. We use GRPO (Shao et al., 2024) with learning rate 106, batch size 128, and KL coefficient 0.01. In Figure 6, we compare different modes of PEAR: Sequence-Level(3.3), Token-Level(3.4) and BlockLevel(3.5). All these variants out-perform standard SFT. Additionally, we observe that sequence-level weighing turns 6 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning Base Model BENCHMARK AIME24 AIME25 AMC23 Olympiad MATH500 PASS@ AVG. 64 8 64 AVG. 64 8 64 AVG. 64 8 64 AVG. 64 8 AVG. 64 8 QWEN2.5-1.5B-MATH SFT +GRPO 5% 19% 37% 3% 14% 30% 42% 80% 98% 29% 55% 63% 89% 28% 51% PEARB=1 +GRPO 8% (+3) 23% (+4) 47% (+10) 8% (+5) 24% (+10) 40% (+10) 47% (+5) 78% (-2) 98% (0) 34% (+5) 57% (+2) 70% (+7) 91% (+2) 33% (+5) 55% (+4) DS-QWEN-1.5B SFT +GRPO 1% 2% 7% 1% 5% 17% 20% 50% 63% 13% 34% 36% 68% 14% 32% PEARB=1 +GRPO 14% (+13) 38% (+36) 57% (+50) 14% (+13) 35% (+30) 53% (+36) 56% (+36) 91% (+41) 98% (+35) 41% (+28) 66% (+32) 72% (+36) 94% (+26) 39% (+25) 65% (+33) QWEN3-4B-BASE SFT PEARB=1 +GRPO +GRPO 15% (+2) 13% 40% (+15) 25% 60% (+17) 43% 15% (+8) 7% 35% (+14) 21% 57% (+17) 40% 59% (+5) 54% 88% (+5) 83% 95% (+2) 93% 46% (+5) 41% 66% (+6) 60% 80% (+2) 78% 93% (0) 93% 43% (+4) 39% 65% (+9) 56% QWEN3-8B-BASE SFT PEARB=1 +GRPO +GRPO 19% (+4) 15% 41% (+6) 35% 67% (+7) 60% 18% (+1) 17% 35% (0) 35% 53% (0) 53% 55% (+10) 45% 85% (+10) 75% 98% (+3) 95% 40% (+8) 32% 65% (+5) 60% 74% (+8) 66% 93% (+3) 90% 41% (+6) 35% 64% (+5) 59% Pass@1 Avg. Pass@8 Avg. Table 2. After-GRPO math results with models initialized by standard SFT v.s. vanilla PEAR-weighted NLL. QWEN3-BASE 0.6B 1.7B 4B 8B 9% 23% 39% 36% SFT +GRPO SINGLE STEP +GRPO PEARB=1 +GRPO 10% (+1) 12% (+3) 18% (-5) 26% (+3) 38% (-1) 44% (+5) 30% (-6) 41% (+5) Table 3. Average accuracy across math benchmarks: parentheses denote changes over SFT+GRPO (%). Negative gains are shown in red. out highly effective despite its simplicity. You need to weigh the future, not single action. Concurrent works (Wu et al., 2025; Zhang et al., 2025a; Zhu et al., 2025d;a) propose several action- = level stabilization to SFT of E(x,y,R)D , with depending only on the prefix and the current action. We experiment with w(x, y<t, yt) = πθ(ytx,y<t) πβ (ytx,y<t) which is generic form of one-step weighting, computed and stabilized the same way as in PEAR. (cid:105) t=1 w(x, y<t, yt)ℓθ(x, y<t, yt) the form L(θ) (cid:104)(cid:80)T This is myopic objective that up-weights single actions that the target policy finds plausible, not taking into account the long-term effect. As shown in Figure 4a and Table 3, singlestep weighting less effective, since what matters for online RL readiness is whether the logged successful continuation is compatible with the current policy over the remaining horizon (Jiang & Li, 2016; Metelli et al., 2020; Nachum et al., 2019; Ross et al., 2011). PEARB=1 +GRPO 13.1% (+4.7) QWEN3-0.6B-BASE 38.3% (+25.2) QWEN3-1.7B-BASE 59.8% (+10.3) QWEN3-4B-BASE QWEN3-8B-BASE 61.7% (+8.4) Table 4. PEAR can transfer to different RL task distribution. SFT +GRPO 8.4% 13.1% 49.5% 53.3% DIRECT +GRPO 2.8% 2.8% 8.0% 15.0% (a) Qwen3-1.7B-Base (b) Qwen3-4B-Base Figure 5. PEAR-to-base KL divergence across weight levels.y-axis is the weight (clipped). Token distribution is more heavily driven on important tokens that drives success probability. distribution. Concretely, we initialize RL from the PEAR checkpoint and run online training on the subset of 12.8K problems from Enigmata training set, then evaluate on held-out set of Enigmata tasks after removing any nearduplicates across splits. Figure 4a As shown in Table 4, despite the difference between the offline training domain and the online RL domain, PEAR consistently provides stronger initialization than standard SFT: it achieves better post-RL performance under the same RL recipe and roll-out budget. The benefit of PEAR is not overfit to the offline domain; it transfers better to shifted online RL distribution under identical RL compute. 4.4. PEAR Transfers to different RL task distributions. 4.5. Incorporating Negative Examples We next test whether the capability induced by PEAR during offline training transfers to online RL on different task We experiment with the variant presented in 3.6, which pushes down the likelihood of entire negative sequences Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning correcting for those mis-alignments. 5. Related Works Learning Dynamics of Post-Training There is growing interest in understanding the learning characteristics of different post-training approaches (SFT v.s. RL). growing line of work studies why reinforcement-learning (RL) can behave qualitatively differently from supervised fine-tuning (SFT). They find that SFT more readily overfits and degrades out-of-distribution (OOD) performance, whereas on-policy RL more often improves generalization across distribution shifts and can partially undo SFT-induced drift (Chu et al., 2025; Jin et al., 2025b). Recent analyses further connect RLs reduced catastrophic forgetting to its on-policy sampling bias,(Shenfeld et al., 2025; Chen et al., 2025a; Jin et al., 2025a). Beyond behavior-level metrics, recent analyses probe parameter-space dynamics: Zhu et al. characterize (2025b) RLVR updates as structured off-principal learning that preserves spectral structure relative to SFT, while Zhao et show (2025) RL post-training can amplify patterns already in pretraining, present often concentrating probability mass onto al. Figure 7. Mean principal angle between offline and online GRPO gradients. dominant output mode. Offline RL For Language Models There is line of work in LM post training that treat responses as logged decision trajectories (Lanchantin et al., 2025; Wang et al., 2024; Snell et al., 2023; Baheti et al., 2024; Richemond et al., 2024; Mukherjee et al., 2025a) and apply policyoptimization techniques to improve performance. Others seek to improve online RL by introducing offline / semioffline mechanisms (Lanchantin et al., 2025; Zhang et al., 2025b; Li et al., 2025b). Differently, our focus is on better bridging offline and offline stages in common SFT+RL posttraining pipeline. Figure 6. Perforamnce of different variants of PEAR. The bars reflect Pass@1 and dots mark Pass@8. while avoiding token-/suffix-level signed ratio products that can be particularly unstable on long horizons. We subsampled 50K positive data and included 50K negative data from the same behavior policy on the same set of instructions. Figure 4a shows that under the same offline data budget, mixing negative trajectories for stabilization can bring significant additional gains to RL over positive-only PEAR initialization. 4.6. Analysis RQ1: What Positions Does PEAR Concentrate On? By design, PEARs learning concentrates on the tokens that evaluate to larger weights. To analyze this effect, we compute per-token weight ( 3.4) ˆGts and KL(πtrained(atst)πinit(atst)). We see the high-value tokens are distributionally more steered away from the base policy πinit, showing that the behavior of the trained model is systematically more updated on those important locations steering the suffix. RQ2: How Does PEARs Learning Interact with Online RL? Figure 7 shows the average principal angle between PEARs gradients and those for GRPO is smaller than those of SFT and variants, suggesting that PEARs correction can indeed make the offline updates more consistent with the online GRPO learning direction. We also observe that applying stronger KL constraints could create greater mismatch between offline and online gradients, although it better preserves closeness to base model. Thus, we observe online RL training after PEAR smallest drift measured by average NSS2 in Figure 8-b between online and offline checkpoints compared with other initializations, whereas the heavy-lifting happened in the offline stage (Figure 8-a). It shows PEAR suffers the least from offline-to-online mismatch and spent less parameter updates 2NSS measures the relative drift of singular-value spectrum after training (Zhu et al., 2025b) Modifications To SFT Some works modify the SFT loss itself to reduce overfitting and capability losse.g., 8 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning up-weights those that remain plausible. Empirically, PEAR consistently improves post-RL accuracy across verifiable reasoning games and math benchmarks, yielding up to 14.6% pass@8 gain on AIME-2025 after online RL. More broadly, our results suggest practical principle: the offline stage should prioritize reproducible successes under the target policy that will be optimized online."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here. Figure 8. Parameter drift of different approaches. (a) is the NSS score between offline and base model. (b) is the NSS score between online and offline model."
        },
        {
            "title": "Contribution Statement",
            "content": "probability-based objectives beyond NLL (Li et al., 2025a), entropy-regularized distribution matching (Diao et al., 2026), and token/sample-wise reweighting or gating to suppress destructive gradients. (Sanyal et al., 2025; Lin et al., 2025).There are also various importance-weighted / stabilized SFT methods like iw-SFT (Qin & Springenberg, 2025), DFT (Wu et al., 2025), SFT (Zhu et al., 2025a), Proximal-SFT (Zhu et al., 2025d), OPCSFT (Zhang et al., 2025a) that use probability-ratio or trustregion style weights primarily to mitigate off-policy instability, suppress the influence of low-probability tokens, and constrain KL/entropy drift so that supervised fine-tuning remains well-behaved under distribution shift. In contrast, our approach is not introduced as stabilization or trustregion heuristic for SFT but mechanism to better initialize models for subsequent online RL. 6. Conclusion Reasoning LLM post-training typically follows an offline SFT online RL pipeline, so offline objectives should be judged by how well they initialize RL, not just by SFT accuracy. Across extensive experiments, we find that stronger offline performance is an unreliable proxy for post-RL performance: objectives that dominate after SFT can be overtaken after identical RL, producing substantial rank reversals. We attribute this gap to offline-to-online policy mismatch. Offline SFT imitates logged continuations from logged prefixes, whereas online RL updates the model on trajectories sampled from its current policy, concentrating learning on prefixes it actually reaches. To reduce this mismatch, we propose PEAR (Policy Evaluationinspired Algorithm for Offline Learning Loss Reweighting), an OPE-inspired loss reweighting scheme that down-weights logged continuations that are implausible under the current policy and 9 DZ: Lead. Idea and implementation; paper writing; experimentation. FX: Implementation, experimentation. HW: Experimentation. QC: Experimentation. HP: Advising."
        },
        {
            "title": "References",
            "content": "Baheti, A., Lu, X., Brahman, F., Bras, R. L., Sap, M., and Riedl, M. Leftover lunch: Advantage-based offline reinforcement learning for language models, 2024. URL https://arxiv.org/abs/2305.14718. Balunovic, M., Dekoninck, J., Petrov, I., Jovanovic, N., and Vechev, M. Matharena: Evaluating llms on uncontaminated math competitions, 2026. URL https: //arxiv.org/abs/2505.23281. Bossens, D. M. and Thomas, P. S. Low variance off-policy evaluation with state-based importance sampling, 2024. URL https://arxiv.org/abs/2212.03932. Chen, H., Razin, N., Narasimhan, K., and Chen, D. Retaining by doing: The role of on-policy data in mitigating forgetting, 2025a. URL https://arxiv.org/abs/ 2510.18874. Chen, J., He, Q., Yuan, S., Chen, A., Cai, Z., Dai, W., Yu, H., Yu, Q., Li, X., Chen, J., Zhou, H., and Wang, M. Enigmata: Scaling logical reasoning in large language models with synthetic verifiable puzzles, 2025b. URL https://arxiv.org/abs/2505.19914. Chu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans, D., Le, Q. V., Levine, S., and Ma, Y. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. URL https://arxiv. org/abs/2501.17161. Diao, M., Yang, L., Gong, W., Zhang, Y., Yan, Z., Han, Y., Liang, K., Xu, W., and Ma, Z. Entropy-adaptive Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning fine-tuning: Resolving confident conflicts to mitigate forgetting, 2026. URL https://arxiv.org/abs/ 2601.02151. Precup, D., and Hamdaqa, M. Rl fine-tuning heals ood forgetting in sft, 2025a. URL https://arxiv.org/ abs/2509.12235. Guo, D., Yang, D., Zhang, H., Song, J., Wang, P., Zhu, Q., Xu, R., Zhang, R., Ma, S., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., Liu, A., Xue, B., Wang, B., Wu, B., Feng, B., Lu, C., Zhao, C., Deng, C., Ruan, C., Dai, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., Luo, F., Hao, G., Chen, G., Li, G., Zhang, H., Xu, H., Ding, H., Gao, H., Qu, H., Li, H., Guo, J., Li, J., Chen, J., Yuan, J., Tu, J., Qiu, J., Li, J., Cai, J. L., Ni, J., Liang, J., Chen, J., Dong, K., Hu, K., You, K., Gao, K., Guan, K., Huang, K., Yu, K., Wang, L., Zhang, L., Zhao, L., Wang, L., Zhang, L., Xu, L., Xia, L., Zhang, M., Zhang, M., Tang, M., Zhou, M., Li, M., Wang, M., Li, M., Tian, N., Huang, P., Zhang, P., Wang, Q., Chen, Q., Du, Q., Ge, R., Zhang, R., Pan, R., Wang, R., Chen, R. J., Jin, R. L., Chen, R., Lu, S., Zhou, S., Chen, S., Ye, S., Wang, S., Yu, S., Zhou, S., Pan, S., Li, S. S., Zhou, S., Wu, S., Yun, T., Pei, T., Sun, T., Wang, T., Zeng, W., Liu, W., Liang, W., Gao, W., Yu, W., Zhang, W., Xiao, W. L., An, W., Liu, X., Wang, X., Chen, X., Nie, X., Cheng, X., Liu, X., Xie, X., Liu, X., Yang, X., Li, X., Su, X., Lin, X., Li, X. Q., Jin, X., Shen, X., Chen, X., Sun, X., Wang, X., Song, X., Zhou, X., Wang, X., Shan, X., Li, Y. K., Wang, Y. Q., Wei, Y. X., Zhang, Y., Xu, Y., Li, Y., Zhao, Y., Sun, Y., Wang, Y., Yu, Y., Zhang, Y., Shi, Y., Xiong, Y., He, Y., Piao, Y., Wang, Y., Tan, Y., Ma, Y., Liu, Y., Guo, Y., Ou, Y., Wang, Y., Gong, Y., Zou, Y., He, Y., Xiong, Y., Luo, Y., You, Y., Liu, Y., Zhou, Y., Zhu, Y. X., Huang, Y., Li, Y., Zheng, Y., Zhu, Y., Ma, Y., Tang, Y., Zha, Y., Yan, Y., Ren, Z. Z., Ren, Z., Sha, Z., Fu, Z., Xu, Z., Xie, Z., Zhang, Z., Hao, Z., Ma, Z., Yan, Z., Wu, Z., Gu, Z., Zhu, Z., Liu, Z., Li, Z., Xie, Z., Song, Z., Pan, Z., Huang, Z., Xu, Z., Zhang, Z., and Zhang, Z. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, September 2025. ISSN 1476-4687. doi: 10.1038/s41586-025-09422-z. URL http://dx. doi.org/10.1038/s41586-025-09422-z. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. Huang, X., Liu, X., Zhang, E., Yu, T., and Li, S. Offline-toonline reinforcement learning with classifier-free diffusion generation, 2025. URL https://arxiv.org/ abs/2508.06806. Jiang, N. and Li, L. Doubly robust off-policy value evaluation for reinforcement learning, 2016. URL https: //arxiv.org/abs/1511.03722. Jin, H., Luan, S., Lyu, S., Rabusseau, G., Rabbany, R., Jin, H., Lv, S., Wu, S., and Hamdaqa, M. Rl is neither panacea nor mirage: Understanding supervised vs. reinforcement learning fine-tuning for llms, 2025b. URL https://arxiv.org/abs/2508.16546. Kang, F., Kuchnik, M., Padthe, K., Vlastelica, M., Jia, R., Wu, C.-J., and Ardalani, N. Quagmires in sft-rl post-training: When high sft scores mislead and what to use instead, 2025. URL https://arxiv.org/ abs/2510.01624. Lanchantin, J., Chen, A., Lan, J., Li, X., Saha, S., Wang, T., Xu, J., Yu, P., Yuan, W., Weston, J. E., Sukhbaatar, S., and Kulikov, I. Bridging offline and online reinforcement learning for llms, 2025. URL https://arxiv.org/ abs/2506.21495. Lee, S., Seo, Y., Lee, K., Abbeel, P., and Shin, J. Offlineto-online reinforcement learning via balanced replay and pessimistic q-ensemble, 2021. URL https://arxiv. org/abs/2107.00591. Levine, S., Kumar, A., Tucker, G., and Fu, J. Offline reinforcement learning: Tutorial, review, and perspectives on open problems, 2020. URL https://arxiv.org/ abs/2005.01643. Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving quantitative reasoning problems with language models, 2022. URL https://arxiv.org/abs/2206.14858. Li, G., Qiu, R., Chen, X., Ji, H., and Tong, H. Beyond log likelihood: Probability-based objectives for supervised fine-tuning across the model capability continuum, 2025a. URL https://arxiv.org/abs/2510.00526. Li, S., Zhou, Z., Lam, W., Yang, C., and Lu, C. Repo: Replay-enhanced policy optimization, 2025b. URL https://arxiv.org/abs/2506.09340. Lin, J., Wang, Z., Qian, K., Wang, T., Srinivasan, A., Zeng, H., Jiao, R., Zhou, X., Gesi, J., Wang, D., Guo, Y., Zhong, K., Zhang, W., Sanghavi, S., Chen, C., Yun, H., and Li, L. Sft doesnt always hurt general capabilities: Revisiting domain-specific fine-tuning in llms, 2025. URL https: //arxiv.org/abs/2509.20758. Liu, J., Fan, Y., Jiang, Z., Ding, H., Hu, Y., Zhang, C., Shi, Y., Weng, S., Chen, A., Chen, S., Huang, Y., Zhang, M., Zhao, P., Yan, J., and He, J. Synlogic: Synthesizing verifiable reasoning data at scale for learning 10 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning logical reasoning and beyond, 2025. URL https: //arxiv.org/abs/2505.19641. Liu, Q., Li, L., Tang, Z., and Zhou, D. Breaking the curse of horizon: Infinite-horizon off-policy estimation, 2018. URL https://arxiv.org/abs/1810.12429. Liu, Y., Swaminathan, A., Agarwal, A., and Brunskill, E. Off-policy policy gradient with state distribution correction, 2019. URL https://arxiv.org/abs/1904. 08473. Liu, Y., Bacon, P.-L., and Brunskill, E. Understanding the curse of horizon in off-policy evaluation via conditional importance sampling, 2020. URL https://arxiv. org/abs/1910.06508. Mehta, S. A., Ciftci, Y. U., Ramachandran, B., Bansal, S., and Losey, D. P. Stable-bc: Controlling covariate shift with stable behavior cloning, 2024. URL https: //arxiv.org/abs/2408.06246. Metelli, A. M., Papini, M., Montali, N., and Restelli, M. Importance sampling techniques for policy optimization. Journal of Machine Learning Research, 21(141):1 75, 2020. URL http://jmlr.org/papers/v21/ 20-124.html. Mukherjee, S., Lai, V. D., Addanki, R., Rossi, R., Yoon, S., Bui, T., Rao, A., Subramanian, J., and Kveton, B. Offline rl by reward-weighted fine-tuning for conversation optimization, 2025a. URL https://arxiv.org/ abs/2506.06964. Qin, C. and Springenberg, J. T. Supervised fine tuning on curated data is reinforcement learning (and can be improved), 2025. URL https://arxiv.org/abs/ 2507.12856. Richemond, P. H., Tang, Y., Guo, D., Calandriello, D., Azar, M. G., Rafailov, R., Pires, B. A., Tarassov, E., Spangher, L., Ellsworth, W., Severyn, A., Mallinson, J., Shani, L., Shamir, G., Joshi, R., Liu, T., Munos, R., and Piot, B. Offline regularised reinforcement learning for large language models alignment, 2024. URL https://arxiv.org/abs/2405.19107. Ross, S. and Bagnell, J. A. Reinforcement and imitation learning via interactive no-regret learning, 2014. URL https://arxiv.org/abs/1406.5979. Ross, S., Gordon, G. J., and Bagnell, J. A. reduction of imitation learning and structured prediction to no-regret online learning, 2011. URL https://arxiv.org/ abs/1011.0686. Rowland, M., Harutyunyan, A., van Hasselt, H., Borsa, D., Schaul, T., Munos, R., and Dabney, W. Conditional importance sampling for off-policy learning. In Chiappa, S. and Calandra, R. (eds.), Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research, pp. 4555. PMLR, 2628 Aug 2020. URL https://proceedings.mlr.press/ v108/rowland20b.html. Mukherjee, S., Yuan, L., Hakkani-Tur, D., and Peng, H. Reinforcement learning finetunes small subnetworks in large language models, 2025b. URL https://arxiv. org/abs/2505.11711. Sanyal, S., Prairie, H., Das, R., Kavis, A., and Sanghavi, S. Upweighting easy samples in fine-tuning mitigates forgetting, 2025. URL https://arxiv.org/abs/ 2502.02797. Nachum, O., Chow, Y., Dai, B., and Li, L. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections, 2019. URL https://arxiv. org/abs/1906.04733. Precup, D., Sutton, R. S., and Singh, S. P. Eligibility traces for off-policy policy evaluation. In Proceedings of the Seventeenth International Conference on Machine Learning, ICML 00, pp. 759766, San Francisco, CA, USA, 2000. Morgan Kaufmann Publishers Inc. ISBN 1558607072. Prime Intellect. SYNTHETIC-2, 2025. URL https://huggingface.co/datasets/ PrimeIntellect/SYNTHETIC-2. Oct 7, 2025. Accessed Jan 18, 2026. Updated Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Shenfeld, I., Pari, J., and Agrawal, P. Rls razor: Why online reinforcement learning forgets less, 2025. URL https://arxiv.org/abs/2509.04259. Snell, C., Kostrikov, I., Su, Y., Yang, M., and Levine, S. Offline rl for natural language generation with implicit language learning, 2023. URL https://arxiv. org/abs/2206.11871. Qian, C., Acikgoz, E. C., He, Q., Wang, H., Chen, X., Hakkani-Tür, D., Tur, G., and Ji, H. Toolrl: Reward is all tool learning needs, 2025. URL https://arxiv. org/abs/2504.13958. Sun, S., Cai, M., He, H., Chen, B., Bao, S., Yang, Y., Wu, H., and Wang, H. Distributional clarity: The hidden driver of rl-friendliness in large language models, 2026. URL https://arxiv.org/abs/2601.06911. Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning Sun, W., Venkatraman, A., Gordon, G. J., Boots, B., and Bagnell, J. A. Deeply aggrevated: Differentiable imitation learning for sequential prediction, 2017. URL https://arxiv.org/abs/1703.01030. Sutton, R. S. and Barto, A. G. Reinforcement Learning: An Introduction. Bradford Book, Cambridge, MA, USA, 2018. ISBN 0262039249. Thomas, P. S. and Brunskill, E. Data-efficient off-policy policy evaluation for reinforcement learning, 2016. URL https://arxiv.org/abs/1604.00923. Uehara, M., Shi, C., and Kallus, N. review of off-policy evaluation in reinforcement learning, 2022. URL https: //arxiv.org/abs/2212.06355. Wang, H., Hao, S., Dong, H., Zhang, S., Bao, Y., Yang, Z., and Wu, Y. Offline reinforcement learning for llm multistep reasoning, 2024. URL https://arxiv.org/ abs/2412.16145. Wei, Y., Duchenne, O., Copet, J., Carbonneaux, Q., Zhang, L., Fried, D., Synnaeve, G., Singh, R., and Wang, S. I. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution, 2025. URL https: //arxiv.org/abs/2502.18449. Wu, Y., Zhou, Y., Ziheng, Z., Peng, Y., Ye, X., Hu, X., Zhu, W., Qi, L., Yang, M.-H., and Yang, X. On the generalization of sft: reinforcement learning perspective with reward rectification, 2025. URL https: //arxiv.org/abs/2508.05629. Yang, A., Zhang, B., Hui, B., Gao, B., Yu, B., Li, C., Liu, D., Tu, J., Zhou, J., Lin, J., Lu, K., Xue, M., Lin, R., Liu, T., Ren, X., and Zhang, Z. Qwen2.5-math technical report: Toward mathematical expert model via selfimprovement, 2024. URL https://arxiv.org/ abs/2409.12122. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Zhou, J., Lin, J., Dang, K., Bao, K., Yang, K., Yu, L., Deng, L., Li, M., Xue, M., Li, M., Zhang, P., Wang, P., Zhu, Q., Men, R., Gao, R., Liu, S., Luo, S., Li, T., Tang, T., Yin, W., Ren, X., Wang, X., Zhang, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Zhang, Y., Wan, Y., Liu, Y., Wang, Z., Cui, Z., Zhang, Z., Zhou, Z., and Qiu, Z. Qwen3 technical report, 2025. URL https: //arxiv.org/abs/2505.09388. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., Liu, X., Lin, H., Lin, Z., Ma, B., Sheng, G., Tong, Y., Zhang, C., Zhang, M., Zhang, W., Zhu, H., Zhu, J., Chen, J., Chen, J., Wang, C., Yu, H., Song, Y., Wei, X., Zhou, H., Liu, J., Ma, W.- Y., Zhang, Y.-Q., Yan, L., Qiao, M., Wu, Y., and Wang, M. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/ abs/2503.14476. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Yue, Y., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model?, 2025. URL https://arxiv.org/ abs/2504.13837. Zhang, W., Xie, Y., Sun, Y., Chen, Y., Wang, G., Li, Y., Ding, B., and Zhou, J. On-policy rl meets off-policy experts: Harmonizing supervised fine-tuning and reinforcement learning via dynamic weighting, 2025a. URL https: //arxiv.org/abs/2508.11408. Zhang, Z., Feng, G., Guan, J., He, D., and Wu, W. Beyond online sampling: Bridging offline-to-online alignment via dynamic data transformation for LLMs. In Christodoulopoulos, C., Chakraborty, T., Rose, C., and Peng, V. (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 2709727109, Suzhou, China, November 2025b. Association for Computational Linguistics. ISBN 9798-89176-332-6. doi: 10.18653/v1/2025.emnlp-main. 1378. URL https://aclanthology.org/2025. emnlp-main.1378/. Zhao, R., Meterez, A., Kakade, S., Pehlevan, C., Jelassi, S., and Malach, E. Echo chamber: Rl post-training amplifies behaviors learned in pretraining, 2025. URL https: //arxiv.org/abs/2504.07912. Zhao, Y., Boney, R., Ilin, A., Kannala, J., and Pajarinen, J. Adaptive behavior cloning regularization for stable offline-to-online reinforcement learning, 2022. URL https://arxiv.org/abs/2210.13846. Zhu, H., Su, J., Lai, P., Ma, R., Zhang, W., Yang, L., and Chen, G. Anchored supervised fine-tuning, 2025a. URL https://arxiv.org/abs/2509.23753. Zhu, H., Zhang, Z., Huang, H., Su, D., Liu, Z., Zhao, J., Fedorov, I., Pirsiavash, H., Sha, Z., Lee, J., Pan, D. Z., Wang, Z., Tian, Y., and Tai, K. S. The path not taken: Rlvr provably learns off the principals, 2025b. URL https://arxiv.org/abs/2511.08567. Zhu, H., Zhang, Z., Huang, H., Su, D., Liu, Z., Zhao, J., Fedorov, I., Pirsiavash, H., Sha, Z., Lee, J., Pan, D. Z., Wang, Z., Tian, Y., and Tai, K. S. The path not taken: Rlvr provably learns off the principals, 2025c. URL https://arxiv.org/abs/2511.08567. 12 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning Zhu, W., Xie, R., Wang, R., Sun, X., Wang, D., and Liu, P. Proximal supervised fine-tuning, 2025d. URL https: //arxiv.org/abs/2508.17784. Zu, L., Zhou, H., and Zhang, X. Behavior-adaptive qlearning: unifying framework for offline-to-online rl, 2025. URL https://arxiv.org/abs/2511. 03695. 13 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning A. Computation Of Metrics RLVR for LLMs has become increasingly expensive since the model needs to rollout on the training set and get updates at the same time. This gets even worse when the RLVR environment involves tool calling or code execution (Wei et al., 2025; Qian et al., 2025), which can takes minutes to hPEAR to finish. Therefore, understanding the signals that can suggest models potential after RLVR can save huge amount of compute, and has received great attention in the LLM community (Sun et al., 2026). In this section, we discuss the implementation details of the signal netrics we evaluated. A.1. KL We compute the forward KL between the base model and the model trained on offline data to understand how much the trained models distribution diverge from the original one, and whether big or small divergence result in superior performance. Specifically, the metric we evaluated is defined as follows: Forward KL = KL(PbasePtrained) = ΣPbase log (Pbase/Ptrained) To evaluate the forward KL, we first collect calibration set of 269 question-answer pairs in SynLogic games, and forward the data through the base and tuned model to compute the distribution and corresponding KL. The reported forward KL is computed by taking macro average over all sequences in the calibration set. A.2. Sparsity Mukherjee et al. (2025b); Zhu et al. (2025c) observed the different sparsity patterns in SFT and RLVR training, we try to understand if update patterns will have different sparsities under different offline learning objectives. Given linear module Wbase from the base model and the corresponding module Wtrained in the trained model, ϵ be the sparsity threshold, the sparsity of the module is calculated as: sparsity(W ) = 1(Wij < ϵ) , = Wtrained Wbase, 1 (cid:88) i,j models sparsity is calculated by taking macro average across all its linear modules. A.3. Normalized Spectrum Shift Normalized Spectrum Shift (NSS) (Zhu et al., 2025c) is metric to measure the drift of the tuned model in the parameter space. For each module, we first perform singular value decomposition on the weight matrices to obtain the singular values, then measure the normalized distance between the singular value spaces. NSS(W ) = σ(W+) σ(W0)2/σ(W0)2 A.4. Gradient Rotation We compute the rotation of gradients during the offline and online stage to understand if the offline and online training stages update the model in similar directions. Specifically, given the gradient if module in the base model offlineW in the offline stage and the gradient of the same module in the online stage onlineW , we first perform SVD on the two matrices to obtain Uoffline and Uonline, then we evaluate the subspace rotation between the two matrices as follows: cosθi(U ) := σi(Uoffline,kUonline,k), = 1, . . . , where UK denotes and top-k subspace of and is equal to 128 in our case. To estimate the rotation of module we simply take the average of the θi, . . . , θk. To estimate the rotation of model, we simply take macro average over all the linear modules. In order to compute the gradients, we take model that has been trained on offline data, and use another calibration set to run pilot offline training on the model for 10 steps and collect the gradients. For the online gradient, we simply run GRPO on the modle for 10 steps and collect the gradients. We take the means of the offline and online gradients across the steps and use the means to compute the module-wise rotations. Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning Figure 9. The comparison between different metrics versus SynLogic online pass@1. (a) offline model forward KL divergence against the base model. (b) offline model update sparsity against the base model. (c) average spectrum drift of different linear modules in the base and offline model. (d) average spectrum drift of different linear modules in the offline and online model. 15 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning A.5. Results The results of metrics vs online pass@1 for Qwen3-1.7B-Base are visualized in Figure 9. From the figure, we can see that IS-SFT results in comparable forward KL divergence and much lower sparsity compared to SFT methods. Additionally, our method results in greater update during the offline stage, causing greater spectrum drift against the base model compared to SFT methods. In contrast, IS-SFT causes smaller updates during the online stage. B. Suffix Change-of-Measure Let be prompt and y1:T token trajectory. For any {1, . . . , }, define the prefix st := (x, y<t) and the suffix yt:T . For an autoregressive policy π, π(yt:T st) = (cid:89) k=t π(yk x, y<k)."
        },
        {
            "title": "Define the suffix likelihood ratio",
            "content": "ρt:T (x, y) := πθ(yt:T st) πβ(yt:T st) = (cid:89) k=t πθ(yk x, y<k) πβ(yk x, y<k) . Then for any measurable function φ of the continuation (and the fixed prefix), Eyt:T πθ(st) (cid:2)φ(st, yt:T )(cid:3) = = (cid:88) yt:T (cid:88) yt:T πθ(yt:T st) φ(st, yt:T ) πβ(yt:T st) πθ(yt:T st) πβ(yt:T st) (cid:2)ρt:T (x, y) φ(st, yt:T )(cid:3), φ(st, yt:T ) = Eyt:T πβ (st) assuming πβ(yt:T st) > 0 whenever πθ(yt:T st) > 0. Intuition. Conditioning on the same prefix st, the two policies induce different distributions over the remaining continuation yt:T . The ratio ρt:T is exactly the RadonNikodym derivative that reweights πβ-suffix samples into unbiased expectations under πθ: suffixes that are more likely under πθ than πβ receive larger weight, and vice versa. C. An Alternative Intuition: Suffix ratios as an off-policy estimate of return / value. Recall the (discounted) terminal-feedback action-value under the target policy: Qπθ γ (st, at) Eat+1:T πθ(st+1:T ) (cid:2)γ R(τ ) (cid:12) (cid:12) st, at (cid:3) , (1) where τ = (s1, a1, . . . , sT , aT ) and R(τ ) is observed only at . Change of measure on the continuation. Condition on the same prefix decision (st, at) and rewrite the πθ-continuation expectation as an expectation over continuations sampled from the logging policy πβ: (cid:88) Qπθ γ (st, at) = πθ(at+1:T st, at) γT tR(τ ) at+1:T (cid:88) at+1:T = πβ(at+1:T st, at) πθ(at+1:T st, at) πβ(at+1:T st, at) (cid:125) (cid:123)(cid:122) (cid:124) wt+1:T (τ ) (cid:3) , (cid:12) st, at γT tR(τ ) (2) with the suffix importance ratio = Eτ πβ (cid:2)γ R(τ ) wt+1:T (τ ) (cid:12) j=t+1 Intuition: wt+1:T translates logged suffixes into what πθ would typically see. If the logged continuation is unlikely under πθ, it should contribute little to πθs expected return-to-go from (st, at). wt+1:T (τ ) (cid:89) πθ(aj sj) πβ(aj sj) . 16 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning From to token-level return estimator. Given one logged trajectory τ D, single-sample plug-in estimator of (2) is exactly your per-token credit weight Gt(τ ) γ tR(τ ) wt+1:T (τ ), so that Gt(τ ) Qπθ γ (st, at). (3) Intuition: uniform SFT corresponds to replacing by constant (every token gets equal credit), while PEAR replaces it by an outcome-aware return estimate that (i) propagates terminal feedback back to earlier decisions (via γT tR) and (ii) discounts suffixes that πθ would not actually realize during on-policy rollouts (via wt+1:T ). D. Stabilization E. Details On Baselines E.1. Token-Adaptive Loss Reweighting (TALR) TALR reweights token-level negative log-likelihood (NLL) by an exponential function of token difficulty. Given token probability pt for the supervised token at position t, define token NLL TALR assigns an adaptive weight ℓt = log pt. (cid:18) wt = exp (cid:19) , ℓt τ wt = max(cid:0)sg( wt), wmin (cid:1), where sg() denotes stop-gradient (weights treated as constants in backprop). The reweighted batch loss is the (token-)mean: LTALR = 1 (cid:88) t=1 wt ( log pt), with the number of supervised tokens in the batch. Key TALR hyperparameters. Weight floor wmin: fixed to 0.01 in all experiments (prevents vanishing weights on very hard tokens). Temperature τ : selected dynamically each step as the median of the average sequence loss within the batch. E.2. Beyond Log Likelihood Here we summarize the hyper-parameter settings reported in Li et al. (2025a). The base learning rate is 5 105. Objective hyperparameters. Their core parametric family is fα(p) = 1pα (with α 0 recovering NLL). In the main math results, they instantiate several concrete choices, including: (i) NLL log p, (ii) (equivalently the α = 1 member up to an additive constant), and (iii) hard-thresholded NLL of the form log(p) I[p 0.2]. They also discuss higher-power prior-leaning variants (e.g., α = 10). α F. Additional Results And Visualizations G. Discussion: Should Offline Training match Online RL characteristics In The Two-Stage Process? While it is tempting to treat common stabilization signalssmaller KL to the base (Shenfeld et al., 2025) policy, sparser (Mukherjee et al., 2025b) (lower-magnitude) updates, or smaller rotation (Zhu et al., 2025b) away from the base representation, these quantities primarily measure conservatism, not actually an accurate mismatch correction. 17 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning Name Per-token loss (p) Hyperparameters / mask Key original NLL (standard SFT) GeneralFamily-α Probability family OnlyTopP-q OnlyBottomP-q OnlyTopLogP-q OnlyBottomLogP-q Plain-p objective Top-thresholded (plain-p) Bottom-thresholded (plain-p) Top-thresholded (NLL) Bottom-thresholded (NLL) log 1 pα α 1 (1 p) 1[p q] (1 p) 1[p q] log(p) 1[p q] log(p) 1[p q] None α (with α 0 recovering log p) None (equiv. to GeneralFamily-1 up to constants) [0, 1] [0, 1] [0, 1] [0, 1] Paper-only (used for analysis/ablations; not exposed as repo keys) Table 5. Objectives in Beyond Log Likelihood. Here := pθ(yt y<t, x) denotes the model probability of the ground-truth token at step t, and the sequence loss is (cid:80) (pt). (a) KL-to-base and update sparsity of offline updates versus Pass@1. (b) KL-to-base and update sparsity of offline updates versus Pass@8. Figure 10. Offline update strength measured by KL to base model and sparsity of parameter updates. Figure 8-a shows that KL penalties ensure smaller drift yet does not boost the performance compared with vanilla SFT, and while PEAR leads to more aggressive KL drifts and denser updates 8-b, it performs better. In fact, for downstream RL the goal is not to minimize movement per se, but to move in the right directions: toward behaviors that improve expected return under on-policy rollouts, even if that requires nontrivial deviation from the base. Consequently, checkpoint that looks stable by these proxies may still yield weak post-RL gains (or learn slowly) because it has not acquired the right inductive biases, coverage, or credit-assignment structure that makes subsequent RL compute-efficient. H. Table For Offline vs Online Metrics (Pass@1 and Pass@8) One surprising finding we made is that stronger offline performance does not necessary lead post-RL performance. The results are visualized in Figure 11. We can see that while SFT This section shows the detailed offline and online performances of Qwen3-1.7B-Base and Qwen3-4B-Base trained with different learning objectives. From the figures we can clearly see the line segments intesect with each other, and that offline-online performances do not have consistent ranking. Detailed evaluation results of Qwen3-1.7B-Base and Qwen3-4B-Base can be found in Table 6 and 7, respectively. 18 Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning Figure 11. Visualization of offline vs online performance. (a): Qwen3-1.7B-Base-Base offline pass@1 versus online pass@1. (b): Qwen34B-Base offline pass@1 versus online pass@1. (c): Qwen3-1.7B-Base offline pass@128 versus online pass@1. (d): Qwen3-1.7B-Base offline majority vote versus online pass@1. objective Offline Online Pass@1 Pass@8 Pass@128 maj vote Pass@1 Pass@ PEAR PEAR SFT SFT TALR SFT-KL(β = 0.03) SFT-KL(β = 0.3) SFT-KL(β = 1.0) TopP TopLogP BottomP BottomLogP (1 p)α/α 32.50% 37.50% 20.00% 23.75% 22.50% 27.50% 8.75% 35.00% 10.00% 35.00% 5.00% 40.00% 5.00% 28.75% 13.75% 33.75% 20.00% 37.50% 3.75% 30.00% 10.00% 28.75% 8.75% 27.50% 31.25% 32.50% 20.00% 55.00% - - - 56.25% 56.25% 56.25% 47.50% 60.00% 1.25% 1.25% 0.00% 6.25% - - - 22.50% 56.25% 45.00% 70.00% 30.00% 57.50% 13.75% 48.75% 42.75% 67.50% 22.50% 40.00% 8.75% 13.75% 21.25% 18.75% 37.50% 18.75% 10.00% 37.50% 13.75% 16.25% 55.00% 1.25% 10.00% 37.50% 17.50% 21.25% 48.75% Table 6. Offline and online pass rates of Qwen3-1.7B-Base with different learning objectives. All results are evaluated on SynLogic. objective PEAR SFT SFT TALR TopP TopLogP BottomP BottomLogP (1 p)α/α offline online Pass@1 Pass@ Pass@1 Pass@8 32.50% 37.50% 63.75% 76.25% 10.00% 26.25% 42.50% 71.25% 13.75% 45.00% 36.40% 68.80% 37.50% 45.00% 42.50% 63.75% 33.75% 52.50% 58.76% 72.50% 32.50% 60.00% 38.75% 71.25% 10.00% 16.25% 26.25% 42.50% 32.50% 47.50% 33.75% 63.75% Table 7. Offline and online pass rates of Qwen3-4B-Base with different learning objectives. All results are evaluated on SynLogic."
        }
    ],
    "affiliations": [
        "New York University (Shanghai)",
        "University of Illinois Urbana-Champaign"
    ]
}