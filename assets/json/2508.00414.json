{
    "paper_title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training",
    "authors": [
        "Tianqing Fang",
        "Zhisong Zhang",
        "Xiaoyang Wang",
        "Rui Wang",
        "Can Qin",
        "Yuxuan Wan",
        "Jun-Yu Ma",
        "Ce Zhang",
        "Jiaqi Chen",
        "Xiyun Li",
        "Hongming Zhang",
        "Haitao Mi",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on a variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-of-the-art results among open-source and free agents. Notably, our 8B-parameter open-source model surpasses previous leading systems such as WebDancer and WebSailor, establishing a new performance standard for accessible, high-capability AI agents. Code is available at https://github.com/Tencent/CognitiveKernel-Pro"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 4 1 4 0 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Technical Report",
            "content": "Cognitive Kernel-Pro: Framework for Deep Research Agents and Agent Foundation Models Training Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, Dong Yu"
        },
        {
            "title": "Tencent AI Lab",
            "content": "https://github.com/Tencent/CognitiveKernel-Pro Figure 1: (a) Performance comparison on the full GAIA development set (number of examples n=165). The left panel presents results from our open-source Cognitive Kernel-Pro framework, utilizing our Qwen3-8B SFT model and Claude-3.7 as foundation models with exclusively free tools. The right panel displays Pass@1 scores for proprietary agents and open-source systems employing paid tools. (b) Performance on the text-only GAIA subset (n=103), demonstrating our 8B models superiority over 7B models in the WebDancer/WebSailor family (2% higher Pass@1, over 10% higher Pass@3). Abstract General AI Agents are increasingly recognized as foundational frameworks for the next generation of artificial intelligence, enabling complex reasoning, web interaction, coding, and autonomous research capabilities. However, current agent systems are either closed-source or heavily reliant on variety of paid APIs and proprietary tools, limiting accessibility and reproducibility for the research community. In this work, we present Cognitive Kernel-Pro, fully open-source and (to the maximum extent) free multi-module agent framework designed to democratize the development and evaluation of advanced AI agents. Within Cognitive Kernel-Pro, we systematically investigate the curation of high-quality training data for Agent Foundation Models, focusing on the construction of queries, trajectories, and verifiable answers across four key domains: web, file, code, and general reasoning. Furthermore, we explore novel strategies for agent test-time reflection and voting to enhance agent robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving state-ofthe-art results among open-source and free agents. Notably, our 8B-parameter opensource model surpasses previous leading systems such as WebDancer and WebSailor, establishing new performance standard for accessible, high-capability AI agents. Note: The term Cognitive Kernel (Zhang et al., 2024) refers to the core computational framework of the agent, designed to emulate the cognitive processes of the human mind. Equal Core Contributors"
        },
        {
            "title": "Technical Report",
            "content": "Figure 2: Technical roadmap showcasing prior innovations from Tencent AI Lab (Cognitive Kernel; Zhang et al., 2024, WebVoyager; He et al., 2024a, etc) and their integration to Cognitive KernelPro via three core components, agent framework development, agent data construction, and agent foundation model training. Yellow blocks highlight novel contributions in this work and the corresponding section numbers."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of Deep Research Agents (Monica.Im, 2025; OpenAI, 2025) has transformed the landscape of automated knowledge discovery and problem-solving. These agents, powered by large language models (LLMs) and vision-language models (VLMs), excel in tasks such as coding, web navigation, file processing, and complex reasoning. However, efforts toward fully open-source agent frameworks (Roucher et al., 2025; Wu et al., 2025a; Li et al., 2025a) remain limited. Existing open-source implementations (Zhu et al., 2025; Hu et al., 2025) often rely on proprietary tools like Jina Reader, FireCrawl, or Chunkr to achieve competitive performance, creating barriers to accessibility and reproducibility, or lack of multimodal or general agentic abilities (Wu et al., 2025a; Li et al., 2025a). This dependency on paid tools underscores the need for robust, fully opensource framework that maximizes the inherent capabilities of LLMs and VLMs without external dependencies. To address this gap, we propose Cognitive Kernel-Pro, multi-module, hierarchical agent framework designed to facilitate fully open-source agent development. Cognitive Kernel-Pro leverages Python code as its action space, harnessing the full reasoning and code-generation potential of modern LLMs. The framework adopts modular architecture, featuring main agent that orchestrates specialized sub-agents for web navigation, file handling, and tool invocation. Each module operates independently, ensuring modularity and extensibility while simplifying the collection of task-specific training data. By minimizing reliance on proprietary tools, Cognitive Kernel-Pro emphasizes the intrinsic capabilities of Agent Foundation Models. In addition to the framework, we introduce comprehensive training recipe tailored for Cognitive Kernel-Pro, covering diverse domains such as web navigation, file processing, code generation, and reasoning. Our approach includes the construction of verifiable agent query-answer pairs, ensuring high-quality training data. To enhance data collection, we incorporate intermediate process hints and employ hint-based rejection sampling, which significantly improves the quality and relevance of the collected data. This structured training methodology enables Cognitive Kernel-Pro to achieve robust performance across diverse tasks while maintaining full open-source compatibility."
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: Overview of the Cognitive Kernel-Pro Agent Framework. The left panel illustrates the functionality of agent class, where the main agent, web agent, and file agent inherit from the common base class. The planner maintains state dictionary containing completed_list, todo_list, experience, and information (2.1). The action generator produces Python code as code agent or invokes predefined functions of sub-agents, such as the web agent, as well as other built-in tools. The right panel illustrates the hierarchical structure of Cognitive Kernel-Pro, listing all functions defined by each agent. Additionally, standalone reflection module is included to assess task completion; if the task is incomplete, the agent will retry (2.2). The agent foundation model behind each module/sub-agent is the same. Furthermore, we explore inference-time optimization techniques to address the inherent randomness in tasks like web browsing. To mitigate variability, we propose pipeline that integrates retry mechanisms and ensemble-based multi-run strategies. This approach enhances the reliability and consistency of Cognitive Kernel-Pros performance, particularly in dynamic and unpredictable environments. By combining modular framework, robust training recipe, and optimized inference strategies, Cognitive Kernel-Pro sets new standard for open-source agent development, paving the way for accessible and reproducible advancements in agent-based research."
        },
        {
            "title": "2 Cognitive Kernel-Pro Framework",
            "content": "We present an overview of the Cognitive Kernel-Pro framework in Figure 3."
        },
        {
            "title": "2.1 Architecture",
            "content": "We adopt two-tier multi-module framework in our agent implementation. This framework consists of main agent, responsible for task decomposition, sub-task delegation, and information aggregation, tool calling, code generation, as well as several sub-agents, whose objective is to solve the sub-tasks assigned by the main agent. Both the main agent and sub-agents inherit from the same base class, where the input is task string, the output is response string, and intermediate actions are executed as Python code. Main-Agent. The main agent directly manages the problem-solving process towards achieving the overall goal. It decomposes the original complex tasks into manageable sub-tasks and assigns these to sub-agents as needed. Upon receiving responses from the sub-agents, the main agent aggregates the information and continues with the main procedure. Notably, the main agent does not possess specialized skills such as web browsing or file processing; only the sub-agents (e.g., the web agent and the file agent) are equipped with such capabilities. Nevertheless, the main agent is aware of the functionalities of the sub-agents and is in charge of delegating appropriate sub-tasks accordingly."
        },
        {
            "title": "Technical Report",
            "content": "Agent Deep Research OWL OAgents WebDancer WebSailor Open Framework Open-source Model No Proprietary Tool (excl. Google) Agent Ability File Web Code Cognitive Kernel-Pro Table 1: Feature Comparison of AI Agent Frameworks. Google Search API (which, can be easily switced to free APIs such as DuckDuckGo if needed) is excluded when comparing proprietary tools because its must in search-related tasks. Note: WebDancer and WebSailor support PDF fetching but lack general file agent capabilities Sub-Agents. The sub-agents are equipped with specialized skills that are essential for generalpurpose task-solving agent system. Each sub-agent follows multi-step procedure similar to that of the main agent but is enhanced with specialized actions that enable direct interaction with specific resources. In our system, we primarily include the following two sub-agents: Web Agent. The web agent is equipped with browser and can navigate live web pages to collect relevant and time-sensitive information. We implement an autonomous web browser using playwright, which provides both the accessibility tree and the screenshot of the current web page. The web agent makes decisions based on the current web pages observations. We adopt typical web agent actions, including click, type, scroll, wait, goback, restart, goto, save, stop, screenshot. Here, save refers to explicitly saving web file to local path for the file agent to process, while stop denotes terminating the navigation process due to task completion or unrecoverable errors. screenshot is special function to turn on screenshot mode to call multimodal language model to process the image. If this function is not called, the default input to the agent foundation model is the text-only accessibility tree of the current webpage. File Agent. The file agent is designed to process variety of file types, such as PDF files (.pdf), spreadsheet files (.xlsx, .xls, .csv), and image files (.png, .jpg, .gif, etc.). Inspired by the web agent, we adopt similar task-solving process. To manage potentially large files, we split each file into pages (using specialized tools for each file type) and allow the file agent to read subset of pages at time. Correspondingly, the action space includes load_file, read_text, read_screenshot, search and stop. Here, the file agent can decide whether to read the screenshot of certain pages or only the text, with the screenshot mode being essential for image-based tasks. While we do not have standalone Code Agent in the system, every sub-agent is code agent since the output action of every agent is essentially python code. For example, every agent can generate Python code to perform calculation or other reasoning tasks that can be solved by code generation and execution. In addition to these sub-agents, our framework is flexible and can be extended to support more sub-agents with specialized skills. The design of this two-tier multi-module framework enables the decoupling of the main agents task-solving procedure from the detailed sub-task execution of the sub-agents, providing flexible and adaptable system capable of supporting wide range of scenarios. Tool Calling Our system utilizes minimum paid tools. We use Google Search API to return search results, wrapped as function simple_web_search. Besides, we only use ask_llm as an additional function to directly let the base agent foundation model to answer question. Depiste Google Search API is not free but it is required by most information seeking agents. Other than that, we do not use any proprietary tools, as illustrated in Table 1."
        },
        {
            "title": "Technical Report",
            "content": "Type Web Data Name Data Type #Query #Steps OpenWebVoyager (He et al., 2024b) Web Browsering Multihop URLQA (3.2) AgentWebQA (w/ hint) (3.3) PersonaHub-Aug (3.4) WebWalkerQA (Wu et al., 2025b) Web Information Seeking Web Information Seeking (No Ground Answer) Web Information Seeking 1,259 4,225 2,721 1,000 1,904 300 1,000 616 225 179 1,400 9,098 25,589 32,231 2,088 18, 1,566 9,482 524 730 165 1,400 File DocBench (Zou et al., 2024) TableBench (Wu et al., 2025c) .pdf .csv, .xlsx Reasoning NumiaMath (Beeching et al., 2024) BAAI/TACO (Li et al., 2023) RiddleSense (Lin et al., 2021) LogiCoT (Liu et al., 2023) Math Reasoning Code/Puzzle Riddle/Puzzle Logical Reasoning Table 2: Summary of the training recipe. The detailed implementations are presented in the Appendix A. 2.2 Inference-time Optimization We introduce two inference-time optimization processesreflection and votingdesigned to enable the agent to evaluate and refine its own trajectories, contributing to more robust and accurate performance of the framework. Reflection (Critic) The reflection process enables the agent to review and evaluate its previous actions. After completing each task, the agents reflection module generates summary of the entire action trajectory, presenting it in an action-observation format (i.e., Action 1: ..., Observation 1: ...; Action 2: ..., Observation 2: ...). The agent then assesses both the trajectory and the predicted answer according to the following rubrics: Non-Empty: The answer must be non-empty string, ensuring that the output is neither null nor blank. Reasonable: The answer should be appropriate for the task at hand. For instance, if the task requests location name, the response should be plausible location name, free from irrelevant information or extraneous text. Successful: The sequence of actions should be executed without errors or failures, such as being unable to open required files or access necessary websites. Reliable: The agents reasoning and references should be based on trustworthy sources and sound logic. If the agent identifies any violations of these criteria, it will attempt the task again, repeating this process until satisfactory answer is produced or predefined retry limit is reached. Voting The voting process enables the agent to aggregate multiple trajectories, enhancing its decision-making and increasing the likelihood of achieving optimal outcomes. In practice, the agent attempts the same task several times, summarizes all resulting trajectories, and then selects the trajectory that best adheres to the guidelines established in the reflection process as the final output. Unlike reflection, which evaluates each attempt in isolation, the voting process allows the agent to compare and contrast information across multiple trajectories. This comparative approach helps the agent identify higher-quality solutions by leveraging differences among the attempts. For example, when the agent is asked to find singers earliest album, one attempt might return an album from the 2000s while another finds one from the 1990s. By comparing these results, the voting module can recognize that the album from the 1990s is the more accurate answer, as it is earlier."
        },
        {
            "title": "Technical Report",
            "content": "Figure 4: Illustration of information aggregation in the creation of URLQA."
        },
        {
            "title": "3.1 Overall Data Recipe",
            "content": "The overall training data recipe is presented in Table 2. We divide the ability of deep research agent into three types, Web, File, and Reasoning. For each of the category, we either convert the existing benchmarks to the format that we need or construct new deep research queries (3.2, 3.3, and 3.4)."
        },
        {
            "title": "3.2 Multi-hop Web Search Data Construction",
            "content": "The data synthesis procedure here aims to create diverse and complex multi-hop information-seeking QA pairs grounded in web pages. We expect the constructed questions requiring information cannot be obtained without retrieval process. To cover multiple domains, we first collect seed URL set by searching for topic-diverse texts from several datasets using the commercial API of Google. Then, an agent traverses and browses web pages starting from these seed URLs with the designed prompt and examples, gathering information and composing questions accordingly. Additionally, to simulate varied task intents, we add the principals and several examples in the prompt, constraining that the answer must be derived through information aggregation operations, as shown in Figure 4. The composition rules are specially designed for different forms of information sources, such as math calculation for numbers, sorting for candidate sets, data analysis for tables."
        },
        {
            "title": "3.3 Agent Exploration-based Data Construction",
            "content": "The data synthesizing process can be viewed as specialized task, for which we can re-use our existing agent framework. In this context, all sub-agents remain unchanged, while the main agent is adapted specifically for data synthesis. The overall procedures and mechanisms are largely consistent with those of our general-purpose framework. We adopt several modifications to further tailor the framework for the data synthesizing process. Prompt Adjustments. The data synthesizing process operates in way that is essentially the reverse of the ordinary task-solving procedure. In ordinary task-solving, the agent is provided with question and aims to search for the answer. In contrast, data synthesizing requires the agent to construct the query itself by integrating pieces of information gathered throughout the exploration process. To accommodate this use case, we retain the core mechanisms of the main agent but revise its prompts to address the unique requirements of data synthesis. In particular, we instruct the agent to construct complex queries by combining information from multiple verifiable sources. Topic Sampling. To allow diverse set of interesting queries that are synthesized, before the agent-based data synthesizing procedure, we first generate an overall topic for each query to be synthesized. Using self-instruct based method, we use an LLM to generate broad and interesting topics with verifiable sources of truth. We provide several seed examplse and let the LLMs to"
        },
        {
            "title": "Technical Report",
            "content": "generate more. After generation, we adopt diversity-based sampling procedure to sample diverse subset of topics for our actual query synthesizing process. Hint-based Training Trajectory Sampling. The query synthesizing procedure yields not only the constructed queries but also all associated intermediate and final results. We observe that, during trajectory sampling for training, providing these intermediate results as hints to the task-solving agent significantly improves the success rate for training data collection. To enhance this, we augment the queries with additional textual hints. It is important to note that this augmentation is employed only during training data collection, where we can assume that answers are available. Once the trajectories are obtained, all such hints are removed from the model inputs and outputs prior to the actual training process. The hints are wrapped between <secret> and </secret> during sampling, and will be removed during training. The prompt of Data Synthesizing Requirements, Topic Sampling, and Hint-based Training Trajectory Sampling are presented in Appendix B."
        },
        {
            "title": "3.4 Persona Hub-based Data Augmentation",
            "content": "Persona-triggered Query Synthesis. PersonaHub (Ge et al., 2024) provides an effective strategy to synthesize large-scale diverse queries for various LLM tasks like math, logic reasoning, instruction following, etc. In this work, we explore to utilize Persona Hub to synthesize training queries for the deep research agent task. Seeded with manually crafted deep research question and its corresponding persona as the in-context example, we utilize an LLM to generate synthetic deep research query given synthetic persona from Persona Hub. Though with limited number of manually crafted deep research questions, we can easily scale up the synthesis of diverse deep research queries by using more personas from Persona Hub as triggers. Trajectory Sampling and Validation. The main challenge of Persona Hub-based data augmentation for deep research agent task is the lack of ground truth answer for the synthetic query. To tackle this challenge, we conduct cross-validation of the trajectory outcomes from different agent systems and include 1k synthetic queries with their trajectories from Cognitive Kernel-Pro agent system into our training set, as detailed in Table 2. Our ablation study suggests small number of training trajectories from Persona Hub-based data augmentation can effectively improve performance. On the other hand, manual validation and response annotation of these synthetic queries are considerable but not yet included in this work."
        },
        {
            "title": "3.5 Reasoning Data Construction",
            "content": "We refine several existing reasoning datasets relevant to general agents, including NumiaMath (applied mathematical reasoning), LogiCoT (logical reasoning), TACO (code reasoning), and RiddleSense (puzzles and riddles). For TACO, we extract input/output pairs from the task descriptions and construct code agent queries by concatenating the task description with the input case, using the corresponding output as the expected gold answer. For the other datasets, we directly transform the question-answer pairs into the input-output format compatible with the ask_llm function."
        },
        {
            "title": "3.6 Trajectory Sampling",
            "content": "For all constructed query-answer pairs, we utilize gpt-4.1 as the foundational backbone model within our Cognitive Kernel-Pro framework to generate agent trajectories. Subsequently, we apply rejection sampling using similarity-based matching, facilitated by the cot_qa of LangChain, with gpt-4.1 as the backbone model. For hint-based sampling, we exclude all hints enclosed within <secret> and </secret> tags to prevent information leakage. Each query is sampled up to three times until successful completion."
        },
        {
            "title": "Technical Report",
            "content": "Framework Agent Model Paid Tools Avg. Level 1 Level Level 3 Closed-source Agent Frameworks TraseAgent Deep Research h2oGPTe Desearch Manus Claude Unknown Claude-3.5 GPT-4o Claude, etc. Unknown Unknown Unknown Unknown Unknown Open-source Agent Frameworks w/ Paid Tools OWLRoleplaying OWLWorkforce OWLWorkforce* OAgent -Pass@3 w/o Paid Tools TapeAgents AutoAgent Magnetic-1 Smolagents Smolagents* - Voting - Pass@3 Cognitive Kernel-Pro -Voting -Pass@3 Cognitive Kernel-Pro -Voting -Pass@ 4o & o3-mini Claude-3-7 Claude-3-7 Claude-3-7 Chunkr, FireCrawl, Whisper, o3-mini w/o whisper Jina Reader, Whisper, Baidu & Bing API Claude-3-7 Claude-3-5 OpenAI o1 Openai o1 Claude-3-7 Claude-3-7 CK-Pro-8B 70.30 67.36 63.64 56.97 73.3 58.18 69.09 60.61 66.67 73.93 55.76 55.15 46.06 49.70 52.10 53.99 63.64 57.58 63.64 70.91 32.73 34.54 38. 83.02 74.29 67.92 71.70 86.5 81.14 84.91 73.58 77.36 83.02 71.70 71.70 56.60 54.72 60.38 66.04 75.47 77.36 69.81 83.02 43.40 47.17 50.94 69.77 69.06 67.44 58.14 70.1 54.65 67.44 62.79 66.28 74. 53.49 53.40 46.51 53.49 54.65 52.33 61.63 54.65 65.12 68.60 32.56 33.72 38.37 46.15 47.60 42.31 23.08 57.7 23.08 42.31 26.92 46.15 53.85 30.77 26.92 23.08 26.92 26.92 33.33 46.15 26.92 46.15 53.85 11.54 11.54 11. Table 3: Performance of various agent frameworks on GAIA dev set (n=165). * after agent names indicate our reproduced results. We boldface the best pass@3 performance and underline the best pass@1 performance of open-source agent frameworks without paid tools (except for Google Search)."
        },
        {
            "title": "4.1 Setup",
            "content": "Baselines Based on the open-source code of OWL, we reproduced OWLs performance using Claude-3.7-Sonnet in our own environment. All experimental settings followed the default configurations provided by OWL, including the use of corresponding LLM APIs for each agent and the integration of certain paid tools, such as Chunkr and FireCrawl. All agents adopted greedy decoding during their inference, and the maximum number of replanning tries was set to the default value of 2. It should be noted that we did not use the Whisper API, and our network environment was different from that of the original experiments. These factors may have contributed to the reproduced performance being lower than the original results reported by OWL. As for the implementation of the SmolAgents, our experiment utilizes most of the tools provided by the Open Deep-Research version of SmolAgents and follows its configuration, except that we enhance the web browsing tool with DOM tree parsing to display web structure, enable element clicking, and text input. Cognitive Kernel-Pro We only use one paid tool, Google Search API, which is must for almost all agent frameworks. Claude-3.7 is used as the backbone for supporting the agent framework. We also use our fine-tuned CK-Pro-8B (based on Qwen-3-8B) as the agent foundation model."
        },
        {
            "title": "Technical Report",
            "content": "Framework Base Model Avg. Level 1 Level 2 Level WebThinker-Base WebThinker-RL Search-o1 WebDancer WebDancer WebSailor WebSailor WebShaper WebShaper Search-o1 R1-Searcher WebDancer -Pass@ 3 WebSailor QwQ-32B QwQ-32B Qwen-2.5-32B Qwen-2.5-32B QwQ-32B Qwen-2.5-32B Qwen-2.5-72B QwQ-32B QwQ-2.5-72B Qwen-2.5-7B Qwen-2.5-7B Qwen-2.5-7B Qwen-2.5-7B Qwen-2.5-7B Cognitive Kernel-Pro Qwen-3-8B Qwen-3-8B -Voting Qwen-3-8B -Pass@3 44.7 48.5 28.2 40.7 51.5 53.2 55.4 53.3 60. 17.5 20.4 31.0 34.0 37.9 40.3 41.1 49.3 53.8 56.4 33.3 46.1 61.5 - - 69.2 69.2 23.1 28.2 41.0 - - 51.3 53.8 61.5 44.2 50.0 25.0 44.2 50.0 - - 50.0 63. 17.3 19.2 30.7 - - 36.5 34.6 44.2 16.7 16.7 0.0 8.3 25.0 - - 16.6 16.6 0.0 8.3 0.0 - - 8.33 16.7 16.7 Table 4: Performance of open-source agent frameworks on the text-only subset of GAIA (n=103). We boldface the best Pass@3 performance and underline the best pass@1 performance for models with size 7 or 8B. Datasets We use the GAIA dataset (Mialon et al., 2024) as the evaluation benchmark, comprehensive suite designed to assess the general intelligence and multi-step reasoning capabilities of AI agents across diverse tasks, including web navigation, question answering, file manipulation, and multimodal processing, making it ideal for evaluating the performance of our Cognitive Kernel-Pro framework."
        },
        {
            "title": "4.2 Results",
            "content": "Full dev set of GAIA Table 3 shows the performance of various agent frameworks on the complete GAIA dataset, differentiating between closed-source and open-source systems, with the latter grouped by their use of paid tools, and featuring our reproduced results marked with an asterisk (*). Cognitive Kernel-Pro, utilizing Claude-3.7, surpasses Smolagents by 5% in Pass@1 and 7% in Pass@3 under identical experimental conditions (e.g., LLM and Search APIs, Internet connectivity), demonstrating its efficacy. Its performance also rivals OWL, which relies on proprietary tools like Chunkr for file processing and FireCrawl for web browsing, underscoring its significant potential. Additionally, we present results from fine-tuning Qwen-3-8B model on the trajectories outlined in Section 3, supported by GPT-4.1 for multimodal functions, achieving Pass@3 score of 38.18%a 30% gap from the state-of-the-art Claude-3.7 modelsuggesting considerable scope for future enhancements. Text-only Subset of GAIA We present the performance comparisons on the text-only subset of GAIA in Table 4. The major baseline is the 7B version of WebDancer and WebSailor. In addition, we list the performance of 32B and 72B models as reference in the upper half of the table. We also include the performance of Search-o1 (Li et al., 2025b), R1-Searcher (Song et al., 2025), and WebThinker (Li et al., 2025c) in the table. Cognitive Kernel-Pro under CK-Pro-8B model yield the best pass@1 and pass@3 performance across all levels of GAIA. Ablation Study of Reflection We present an ablation study of the effect of the reflection module in Table 5. Using an open-source model Qwen-3-32B is already good enough, counterparting GPT-4.1. However, if we use our trained CK-Pro-8B model, without being finetuned with reflection ability,"
        },
        {
            "title": "Technical Report",
            "content": "Inference-time Alg. w/o Reflection Reflection Reflection Reflection Inference-time Model CK-Pro-8B Qwen-3-32B GPT-4.1 Avg. Level Level 2 Level 3 27.0 28.5 31.5 32.7 35.8 37.9 41.5 43.4 27.9 29.4 32.5 32.6 7.7 7.7 7.7 11. Table 5: Ablations on different backbone LLM used for reflection and voting. Base Agent Model MLLM Avg. Level 1 Level 2 Level CK-Pro-8B pass@1 Qwen-2.5-VL-72B CK-Pro-8B pass@1 GPT-4.1 CK-Pro-8B pass@3 Qwen-2.5-VL-72B CK-Pro-8B pass@3 GPT-4.1 33.94 32.67 37.56 38. 43.40 43.40 49.06 50.94 34.88 32.56 38.64 38.37 11.54 11.54 11.54 11. Table 6: Performance of using CK-Pro-8B as the base agent foundation model and variances of multimodal language model we use. We underline the best results on pass@1 and boldface the best performance on pass@3 there is only marginal improvement. This indicates future direction of involving the ability of reflection to the training of agent foundation models. Ablation Study of the Multimodal Language Model Table 6 presents the impact of using different backbones for our multimodal language model. Our results show that replacing Qwen-2.5-VL-72B with GPT-4.1 yields only marginal performance improvements. This suggests that the observed performance gains are not solely because of the use of more advanced multimodal model like GPT-4.1, as Qwen-2.5-VL-72B achieves comparable results. Future work will be developing fully multimodal language model as the backbone, designed to seamlessly support both text and multimodal inputs."
        },
        {
            "title": "5 Related Work",
            "content": "The field of deep research agents has rapidly evolved, driven by the need for autonomous systems capable of conducting complex, multi-step research tasks. These agents leverage large language models (LLMs) and vision-language models (VLMs) to perform tasks such as web navigation, data analysis, code generation, and report synthesis. Below we introduce both close-source and open-source deep research agents."
        },
        {
            "title": "5.1 Proprietary Deep Research Agents",
            "content": "Proprietary systems have set high standard for deep research agents by demonstrating robust performance in autonomous task execution. OpenAIs Deep Research (OpenAI, 2025) integrates most advanced OpenAI models to autonomously browse the web, analyze data, and generate comprehensive reports. Powered by specialized version of the o3 model, it achieves strong performance on benchmarks like GAIA (67.36% average pass@1 accuracy, 72.57% cons@64 accuracy) and Humanitys Last Exam (26.6% accuracy), significantly outperforming other models. Googles Gemini Deep Research (Google DeepMind, 2025) was part of the Gemini 2.5 suite, it autonomously searches hundreds of websites, reasons iteratively, and produces detailed reports, emphasizing real-time adaptability and multimodal processing. Perplexitys Deep Research (Perplexity AI, 2025) excels in domains like finance, marketing, and technology, achieving 21.1% accuracy on Humanitys Last Exam and 93.9% on SimpleQA. It iteratively searches, reads documents, and refines"
        },
        {
            "title": "Technical Report",
            "content": "research plans. More recent work, Kimi-Researcher (Moonshot AI, 2025a;b), an advanced feature of Moonshot AIs Kimi platform, excels in delivering precise research outputs for complex queries across diverse domains."
        },
        {
            "title": "5.2 Open-Source Deep Research Frameworks",
            "content": "Open-source frameworks have made significant strides in democratizing deep research agents, with notable contributions including Hugging Faces SmolAgents (Roucher et al., 2025), lightweight Python library that supports various LLMs for web search and data processing but may lack optimization for complex, multi-step research tasks; Alibaba Tongyis WebAgent Framework, comprising WebDancer (Wu et al., 2025a), WebSailor (Li et al., 2025a), and WebShaper (Tao et al., 2025), which excels in super-human reasoning for web-based tasks like BrowseComp, GAIA, and WebWalkerQA (Wu et al., 2025b). OWL (Optimized Workforce Learning; Hu et al., 2025), hierarchical multi-agent system that leads open-source frameworks with 69.09% average score on the GAIA benchmark, supporting tools for online search, multimodal processing, browser automation, document parsing, and code execution. TapeAgent (Bahdanau et al., 2024), from ServiceNow, uses tape log to streamline LLM agent development, matching GPT-4o in tasks like form-filling with cost efficiency. AutoAgent (Tang et al., 2025) enables non-technical users to create LLM agents via natural language, achieving 55.15% GAIA accuracy and excelling in multi-agent tasks (Tang et al., 2025). OAgent (Zhu et al., 2025), an open-source platform, supports modular agent building for reasoning and automation but may rely on proprietary tools. In all, open-source frameworks for deep research agents lag behind proprietary systems in performance and accessibility. Although some open-source agents demonstrate competitive results, they often depend on proprietary tools, limiting their reproducibility. Furthermore, research on open-source agent foundation models remains underexplored, as most efforts rely on prompting external APIs. In this work, we address these gaps by developing fully open-source framework and model, leveraging (to the maximum extend) freely available tools to enhance accessibility and performance."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce Cognitive Kernel-Pro, fully open-source generalist agent framework that maximizes the use of free tools, achieving state-of-the-art performance on the GAIA benchmark among open-source, free-tool agents while remaining competitive with frameworks relying on proprietary tools. Additionally, we explore the training of an open-source agent foundation model within this framework, developing an 8B-based model that surpasses previous counterparts such as WebDancer and WebSailor. Future efforts will concentrate on advancing more capable, multimodal agent foundation models to address increasingly complex tasks."
        },
        {
            "title": "References",
            "content": "Dzmitry Bahdanau, Nicolas Gontier, Gabriel Huang, Ehsan Kamalloo, Rafael Pardinas, Alex Piché, Torsten Scholak, Oleh Shliazhko, Jordan Prince Tremblay, Karam Ghanem, Soham Parikh, Mitul Tiwari, and Quaizar Vohra. Tapeagents: holistic framework for agent development and optimization. arXiv preprint arXiv:2412.08445, 2024. Published: 2024-12-11, Accessed: 2025-07-25. Edward Beeching, Shengyi Costa Huang, Albert Jiang, Jia Li, Benjamin Lipkin, Zihan Qina, Kashif Rasul, Ziju Shen, Roman Soletskyi, and Lewis Tunstall. Numinamath 7b tir. https://huggingf ace.co/AI-MO/NuminaMath-7B-TIR, 2024. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024. Google DeepMind. Gemini deep research your personal research assistant. https://gemini.goo gle.com, 2025. Accessed: 2025-07-25."
        },
        {
            "title": "Technical Report",
            "content": "Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 68646890. Association for Computational Linguistics, 2024a. doi: 10.18653/V1/2024.ACL-LONG.371. URL https://doi.org/10.18653/v1/2024.acl-long.371. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Hongming Zhang, Tianqing Fang, Zhenzhong Lan, and Dong Yu. Openwebvoyager: Building multimodal web agents via iterative real-world exploration, feedback and optimization. CoRR, abs/2410.19609, 2024b. doi: 10.48550/ARXIV.241 0.19609. URL https://doi.org/10.48550/arXiv.2410.19609. Mengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan Jin, Yingru Li, Qiguang Chen, Zeyu Zhang, Yifeng Wang, Qianshuo Ye, Bernard Ghanem, Ping Luo, and Guohao Li. Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation, 2025. URL https://arxiv.org/abs/2505.23885. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. Websailor: Navigating super-human reasoning for web agent, 2025a. URL https://arxiv.org/abs/2507.02592. Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. CoRR, abs/2501.05366, 2025b. doi: 10.48550/ARXIV.2501.05366. URL https://arxiv.org/abs/ 2501.05366. Accessed: 2025-07-26. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability, 2025c. URL https://arxiv.org/abs/2504.21776. Bill Yuchen Lin, Ziyi Wu, Yichi Yang, Dong-Ho Lee, and Xiang Ren. Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 15041515, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.131. URL https://aclanthology.org/2021.fi ndings-acl.131. Hanmeng Liu, Zhiyang Teng, Leyang Cui, Chaoli Zhang, Qiji Zhou, and Yue Zhang. LogiCoT: Logical chain-of-thought instruction tuning. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 29082921, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emn lp.191. URL https://aclanthology.org/2023.findings-emnlp.191/. Grégoire Mialon, Clémentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. GAIA: benchmark for general AI assistants. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=fibxvahvs3. Monica.Im. Manus ai. Technical report, Monica.Im, 2025. URL https://manus.im/. Moonshot AI. Kimi-k2. https://github.com/MoonshotAI/Kimi-K2, 2025a. Published: 2025-07-11, Accessed: 2025-07-25. Moonshot AI. Kimi-researcher: End-to-end rl training for emerging agentic capabilities. https: //moonshotai.github.io, 2025b. Published: 2025-06-20, Accessed: 2025-07-25."
        },
        {
            "title": "Technical Report",
            "content": "OpenAI. Introducing deep research. Technical report, OpenAI, 2025. URL https://openai.com/i ndex/introducing-deep-research/. Perplexity AI. Introducing perplexity deep research. https://www.perplexity.ai/hub/blog/int roducing-perplexity-deep-research, 2025. Published: 2025-02-14, Accessed: 2025-07-25. Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismäki. smolagents: smol library to build great agentic systems. https://github.com/hugging face/smolagents, 2025. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2503.05592. Jiabin Tang, Tianyu Fan, and Chao Huang. Autoagent: fully-automated and zero-code framework for llm agents. arXiv preprint arXiv:2502.05957, 2025. Published: 2025-02-18, Accessed: 2025-07-25. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webshaper: Agentically data synthesizing via information-seeking formalization. https://arxiv.org/abs/ 2507.15061, 2025. Published: 2025-07-20, Accessed: 2025-07-25. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webdancer: Towards autonomous information seeking agency, 2025a. URL https://arxiv.org/abs/2505.22648. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal. CoRR, abs/2501.07572, 2025b. doi: 10.48550/ARXIV.2501.07572. URL https: //doi.org/10.48550/arXiv.2501.07572. Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xinrun Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, Guanglin Niu, Tongliang Li, and Zhoujun Li. Tablebench: comprehensive and complex benchmark for table question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2549725506, 2025c. Preprint available at arXiv:2408.09174. Hongming Zhang, Xiaoman Pan, Hongwei Wang, Kaixin Ma, Wenhao Yu, and Dong Yu. Cognitive kernel: An open-source agent system towards generalist autopilots. CoRR, abs/2409.10277, 2024. doi: 10.48550/ARXIV.2409.10277. URL https://doi.org/10.48550/arXiv.2409.10277. He Zhu, Tianrui Qin, King Zhu, Heyuan Huang, Yeyi Guan, Jinxiang Xia, Yi Yao, Hanhao Li, Ningning Wang, Pai Liu, Tianhao Peng, Xin Gui, Xiaowan Li, Yuhui Liu, Yuchen Eleanor Jiang, Jun Wang, Changwang Zhang, Xiangru Tang, Ge Zhang, Jian Yang, Minghao Liu, Xitong Gao, Jiaheng Liu, and Wangchunshu Zhou. Oagents: An empirical study of building effective agents, 2025. URL https://arxiv.org/abs/2506.15741. Anni Zou, Wenhao Yu, Hongming Zhang, Kaixin Ma, Deng Cai, Zhuosheng Zhang, Hai Zhao, and Dong Yu. Docbench: benchmark for evaluating llm-based document reading systems, 2024. URL https://arxiv.org/abs/2407.10701."
        },
        {
            "title": "Technical Report",
            "content": "A Technical Details of Cognitive Kernel-Pro Framework Code-based Action and Tool-using. Both the main agent and the sub-agents employ similar multi-step workflow for their problem-solving process. We utilize code-based actions: all actions, including sub-agent and tool invocations, are defined as Python functions. The agents generate code to call these functions, which are then executed directly to perform the corresponding actions (using Pythons built-in exec function). To capture the outputs from action execution, we use Pythons built-in print function; these outputs are subsequently fed into later steps to provide intermediate results. Here are the instructions with regard to the action output format:"
        },
        {
            "title": "ACTION OUTPUT FORMAT",
            "content": "## Output Please generate your response, your reply should strictly follow the format: Thought: Provide an explanation for your action in one line. review of the previous steps to provide context. or relevant information obtained since the last step. reasoning and the rationale behind your current output or decision. Code: wrap the code with markdown python code marks and print your output. Output your python code blob for the next action to execute. Begin with concise Next, describe any new observations Finally, clearly explain your Remember to State-enhanced Problem-solving Workflow. In addition to code-based actions, our workflow incorporates explicit planning and state management. Specifically, before each action decision, the agent adopts planning step, formulating plans based on previous steps and the latest observations. crucial mechanism in this process is the maintenance of progress state, which records summaries of important information from previous steps, including intermediate results and lessons learned from earlier attempts. This progress state offers concise historical context and guides subsequent actions. The following instructions detail the structure of the progress state for the main agent:"
        },
        {
            "title": "PROGRESS STATE",
            "content": "A list of completed steps and gathered information list of planned future steps; aim to plan multiple steps ## Progress State The progress state is crucial for tracking the tasks advancement and includes: completed_list (List[str]): essential for achieving the final goal. todo_list (List[str]): ahead when possible. experience (List[str]): attempts or special tips, to inform future actions. information (List[str]): steps. counting (to avoid redundancy). Here is an example progress state for task to locate and download specific paper for analysis: { These records serve as the memory and are important for tasks such as Summaries of past experiences and notes, such as failed list of collected important information from previous completed_list: [Located and downloaded the paper (as paper.pdf) using the web agent., Analyze the paper with the document agent.], todo_list: [Perform web search with the key words identified from the paper.], experience: [], information: [The required key words from the paper are AI and NLP.] } Unified Multi-module Communication. key aspect of our multi-module system design is the specification of communication between the main agent and the sub-agents. To ensure simple and robust communication, we adopt unified and minimal text-based interface for all sub-agent calling. Each sub-agent is implemented as callable function following the protocol below:"
        },
        {
            "title": "Technical Report",
            "content": "Input: The sub-agent accepts an input argument of task, which is plain string describing the sub-task assigned to it. Optionally, the sub-agent may accept additional arguments specific to its functionality (e.g., file paths for the file agent). Output: The sub-agent returns dictionary with two fields: output, string containing the well-formatted answer that strictly adheres to any specified output format; and log, string providing supplementary notes, such as steps taken, issues encountered, or relevant context. Definition: To enable the main agent to understand the utilities and use cases of each sub-agent, all sub-agents provide Python docstring-style definition, which is provided to the main agent. For example, the definition of the web agent is as follows: WEB-AGENT DEFINITION def web_agent(task: str) dict: \"\"\" Employs web browser to navigate and interact with web pages to accomplish specific task. Args: task (str): detailed description of the task to perform. This may 1) The target website(s) to visit (include valid URLs); 2) Specific include: output formatting requirements; 3) Instructions to download files (specify desired output path if needed). Returns: dict: dictionary with the following structure: output (str): The well-formatted answer, strictly following any specified output format; log(str): Additional notes, such as steps taken, issues encountered, or relevant context. Notes: - If the task specifies an output format, ensure the output field matches it exactly. - The web agent can download files, but cannot process or analyze them. If file analysis is required, save the file to local path and return control to an external planner or file agent for further processing. Example: >>> answer = web_agent(task=What is the current club of Messi? (Format your output directly as club_name.)) >>> print(answer) \"\"\" With these unified input/output definitions, our system can flexibly manage interactions and collaboration between the main agent and sub-agents, facilitating extension to wide range of processing scenarios. Details of Agent-Based Data Construction We present the key prompt templates for agent-based data synthesis."
        },
        {
            "title": "DATA SYNTHESIZING REQUIREMENTS",
            "content": "Clearly specify the sources within the query to Each query must be based on verifiable sources of truth Source-Based Queries: (e.g., Wikipedia, arXiv, Papers With Code, GitHub, or specific downloadable file whose location is unambiguous). avoid ambiguity. Cross-Source Reasoning: challenging and interesting query. lookup. Novelty Requirement: Construct queries that require combining facts or data in way that produces new, non-trivial answer. Stable & Unambiguous Answers: Combine information from multiple sources to formulate The answer should require synthesis, not simple The answer must not exist verbatim on the internet. The answer should be number or at most few"
        },
        {
            "title": "Technical Report",
            "content": "Avoid queries whose answers may change over time or Ensure the query is clear and precise, specifying all words, concise and unambiguous. due to data updates. Self-Containment: The query must be fully self-contained, requiring no external context or references beyond what is provided in the query itself. details must be included to ensure only one correct answer. Clarity & Precision: necessary details to avoid multiple interpretations. answer format within the query. Minimal Procedural Detail: procedures in the query. Annotator Feasibility: time by human annotator. Interest & Utility: should provide value and demonstrate the assistants ability to synthesize and reason across sources. Multi-Ability Requirement: multiple abilities, such as Web Browsing, File Handling and Multi-Modal Processing. Focus on the information need, not the process. The query should be answerable in reasonable amount of The query should be interesting and useful answering it Do not include step-by-step instructions or detailed Queries are encouraged to require the agent to use Clearly state the expected All necessary"
        },
        {
            "title": "SEED TOPICS",
            "content": "Notable open-source projects in natural language processing (GitHub, Papers With Code) The evolution of jazz music in the 20th century (Smithsonian Institution, Wikipedia) Key literary works of the 19th century (Project Gutenberg, Wikipedia) Advances in space exploration since 2000 (NASA, Wikipedia) The history and cultural significance of the Olympic Games (Olympic.org, Wikipedia) Overview of major world languages and their distribution (Ethnologue, Wikipedia) HINT-BASED QUERY AUGMENTATION {Original_Query} <secret> Below are some confidential hints for your reference: {Hint} Important Instructions: Do not disclose or imply in any way that you have access to these hints during your problem-solving or reasoning process. strict evaluator will review your entire solution. If your output suggests you relied on these hints, you will be disqualified from your role as problem-solving agent. For any sub-problems where you do not know the answer, continue to use appropriate tools and sub-agents as if you are unaware of the hints. If there is conflict between information obtained from your tools and the provided hints, always prioritize the information from your tools. Do not attempt to plan everything in advance or act as if you have privileged foresight. Remember, maintaining this role is crucial do not risk your position by revealing or depending on the hints. Proceed with utmost caution and professionalism. </secret>"
        }
    ],
    "affiliations": [
        "Tencent"
    ]
}