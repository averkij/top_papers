{
    "paper_title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models",
    "authors": [
        "Mateusz Pach",
        "Shyamgopal Karthik",
        "Quentin Bouniot",
        "Serge Belongie",
        "Zeynep Akata"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce a comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on a CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs."
        },
        {
            "title": "Start",
            "content": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models Mateusz Pach1,2,3,4 Shyamgopal Karthik1,2,3,4,5 Quentin Bouniot1,2,3,4 Zeynep Akata1,2,3,4 1Technical University of Munich 2Helmholtz Munich 3Munich Center of Machine Learning 5 University of Tubingen 6University of Copenhagen Serge Belongie6 4 Munich Data Science Institute 5 2 0 2 3 ] . [ 1 1 2 8 2 0 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Sparse Autoencoders (SAEs) have recently been shown to enhance interpretability and steerability in Large Language Models (LLMs). In this work, we extend the application of SAEs to Vision-Language Models (VLMs), such as CLIP, and introduce comprehensive framework for evaluating monosemanticity in vision representations. Our experimental results reveal that SAEs trained on VLMs significantly enhance the monosemanticity of individual neurons while also exhibiting hierarchical representations that align well with expert-defined structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that applying SAEs to intervene on CLIP vision encoder, directly steer output from multimodal LLMs (e.g., LLaVA) without any modifications to the underlying model. These findings emphasize the practicality and efficacy of SAEs as an unsupervised approach for enhancing both the interpretability and control of VLMs. 1. Introduction Vision-Language Models (VLMs) such as CLIP [31] and SigLIP [41] have become ubiquitous over the past few years due to their ability to reason across images and text simultaneously. However, there has been limited understanding of the inner workings of these models [26, 33, 37]. Sparse AutoEncoders (SAEs) [23] discover concepts (abstract features shared between data points) efficiently due to their simple architecture learned through post-hoc reconstruction task. Although popular for Large Language Models (LLMs) [3, 16], the analysis of pretrained VLMs with SAEs has been limited to interpretable classification [21, 33], or discovering shared concepts across models [37]. Several SAEs [35, 16, 25, 32] in the literature either propose new activation functions (e.g. Batch-TopK [4], JumpReLU [32]) or different regularization strategies and architectures. For instance, Matryoshka SAEs [5, 25] are claimed to have better separated concepts with hierarchical structuring. The intuition behind SAEs relies on reconstructing activations in higher-dimensional space to disentangle distinct Figure 1. Training Sparse Autoencoders (SAEs) over VLMs (e.g. CLIP): The highly activating images of neuron in given layer of pretrained VLM are polysemantic (top), and neuron in an SAE trained to reconstruct the same layer is more monosemantic (bottom, with higher Monosemanticity Score (MS)) concepts from their overlapping representations in neural activations [3]. Neurons at different layers within deep neural networks are known to be naturally polysemantic [27], meaning that they fire for multiple unrelated concepts such as cars and airplanes. One popular explanation for this behavior is the superposition hypothesis [1, 12], stating that concepts are encoded as linear combination of neurons. SAEs explicitly attempts to resolve this issue by separating the entangled concepts into distinct representations. In this work, we quantitatively evaluate SAEs for VLMs through their monosemanticity, defined as the similarity between inputs firing neuron. We propose Monosemanticity Score (MS) for vision tasks that measures the pair1 wise similarity of images weighted by the activations for given neuron. Unlike natural language where individual words require surrounding context (such as complete sentences) to clearly identify their meanings, individual images can directly activate neurons without additional context. We define highly activating images as images that strongly fire particular neuron. The more similar those images, the more focused on concept the neuron is, and thus the more monosemantic it would be. Using our MS score, we observe and validate that neurons within SAEs are significantly more monosemantic (see Figure 1, bottom) than original neurons (see Figure 1, top). The neuron of the original VLM typically has low MS, since it fires for wide range of vehicles, from scooter to planes. On the other hand, neurons within the SAE are more focused on single concept, e.g. red strawberries, obtaining higher MS. This observation remains valid even for SAE with the same width as the original layer, implying that the sparse reconstruction objective inherently improves the concept separability. Our proposed score also highlights the hierarchical structure of concepts discovered by Matryoshka SAEs [5, 25], that is aligned with expert-defined taxonomy of iNaturalist [39]. Finally, we illustrate applicability of vision SAEs by transfering CLIP-based SAE onto Multimodal LLMs (MLLMs), e.g. LLaVA [22]. Intervening on single SAE neuron in the vision encoder, while keeping the LLM untouched, allows steering the overall MLLM generated output towards the concept encoded in the selected SAE neuron. We summarize our contributions as follows: We propose Monosemanticity Score (MS) for SAEs in vision tasks, that computes neuron-wise activationweighted pairwise similarity of image embeddings. We quantitatively compare MS between SAEs, and across their neurons. We find that wider and sparser latents lead to better scores. Additionally, we evaluate Matryoshka SAEs [5, 25], achieving superior MS, and highlight the natural hierarchy arising within their discovered concepts. We leverage the well-separability of concepts in SAE layers to intervene on neuron activations and steer outputs of MLLMs towards any discovered concept. 2. Related Work In this work, we are interested in evaluating the capabilities of SAEs as tool to interpret the latent space of VLMs. Sparse Autoencoders. Recent works have focused on repurposing traditional dictionary learning methods for providing interpretability of LLMs and VLMs [2, 30]. Specifically, there have been lot of success in interpreting and steering LLMs using the features learned by sparse autoencoders [11, 36]. With respect to the underlying SAE mechanisms, there have been several improvements proposed from the original setup, including novel activation functions such as Batch TopK [4] or JumpReLU [32], along with taking inspiration from Matryoshka representation learning [20] for SAEs [5, 25]. In this work, we provide comprehensive analysis and evaluation of the monosemanticity of SAEs in VLMs and their downstream applications. Vision-Language Models. Since the introduction of Contrastive Language-Image Pretraining (CLIP) [24, 31, 41], several models have been introduced that either map images and text to shared embedding space, or develop models that are able to generate text conditioned on image inputs [8, 22]. While these models have achieved strong results on several benchmarks and use-cases over the past few years, interpretability of these models has become an important concern, from scientific standpoint, but also to allow for their safe usage and scalable control. Therefore, there have been investigations into the representations of these models [14, 15, 26] which have found interpretable neurons [10]. We show that SAEs are more suitable tool to interpret the model as opposed to the raw features. SAEs for VLMs. Based on the success of SAEs to interpret features in LLMs, there have been several efforts in applying them to vision and vision-language models. Specifically, popular direction has been to train SAEs on top of the CLIP model activations [13, 21, 33, 34] as well as other vision encoders (e.g. DINOv2 [29]). Additionally, there has also been interest in interpreting the denoising diffusion models using SAEs [7, 17, 18, 35], discovering common concepts across different vision encoders [37], and applying them on multimodal LLMs [42]. Differently, in this work, we focus on rigorous evaluation framework for SAEs in VLM apart from utilizing SAEs on vision encoders to steer multimodal LLMs. Concurrent to our work, Yan et al. [40] also investigate monosemanticity of multimodal models, although their study is more focused on the difference between modalities. Furthermore their metrics are more limited, as they compare random samples without taking into account activation values. 3. Sparse Autoencoders for VLMs We first describe the SAEs, defining the notations used in this study, and then introduce our proposed metric for evaluating the monosemanticity of representations induced by SAEs in VLMs, and steering outputs of MLLMs. 3.1. Background and Formulation of SAEs SAEs implement form of sparse dictionary learning, where the goal is to learn sparse decomposition of signal into an overcomplete dictionary of atoms [28]. More specifically, an SAE consists of linear layers Wenc Rdω and Wdec Rωd as encoder and decoder, with non-linear activation function σ : Rω Rω. Both layers share bias term Rd subtracted from the encoders input and later added to the decoders output. The width ω of the latent SAE layer is chosen as factor of the original dimension, 2 Figure 2. Illustration of the computation of our Monosemanticity Score (MS). (A) We extract embeddings and activations from given set of images, (B) then compute the pairwise embedding similarities and pairwise neuron activations. (C) MS is equal to the average of embedding similarities weighted by the neuron activations. such that ω := ε, where ε is called the expansion factor. In general, SAEs are applied on embeddings Rd of given layer of the model to explain : Y, such that fl : Rd represents the composition of the first layers and Rdi is the space of input images. given input image is first transformed into the corresponding embedding vector := fl(x), before being decomposed by the SAE into vector of activations ϕ(v) Rω, and its reconstruction vector ˆv Rd is obtained by: ϕ(v) := σ(W ψ(v) := decv + b, enc(v b)), (1) Matryoshka SAEs [5, 25] group neuron activations ϕi(v) into different levels of sizes M, to obtain nested dictionary trained with multiple reconstruction objectives: R(v) := (cid:88) mM W decϕ1:m(v)2, (4) where ϕ1:m corresponds to keeping only the first neuron activations, and setting the others to zero. It is important to note that Matryoshka SAEs can be combined with any SAE variant, e.g. with BatchTopK [5] or ReLU [25], as only the reconstruction objective is modified. ˆv := ψ(ϕ(v)). 3.2. Monosemanticity Score The linear layers Wenc and Wdec composing the SAE are learned through reconstruction objective and sparsity regularization S, to minimize the following loss: L(v) := R(v) + λS(v), (2) where λ is hyperparameter governing the overall sparsity of the decomposition. The most simple instantiation [3, 33] uses ReLU activation, an L2 reconstruction objective and an L1 sparsity penalty, such that σ() := ReLU(), R(v) := ˆv2, S(v) := ϕ(v)1. (3) The (Batch) TopK SAEs [4, 16, 23] use TopK activation function governing the sparsity directly through K. Finally, neurons interpretability increases as its representation becomes disentangled into single, clear concept. Therefore, quantifying the monosemanticity of individual neurons helps identify the most interpretable ones, while aggregating these scores across an entire layer allows assessing the overall semantic clarity and quality of the representations learned by the SAE. We propose measuring monosemanticity by computing pairwise similarities between images that strongly activate given neuron, where high similarity indicates these images likely represent the same concept. These similarities can be efficiently approximated using deep embeddings from pretrained image encoder E. Since selecting fixed number of top-activating images is challenging due to varying levels of specialization across neurons, we instead evaluate monosemanticity over large, diverse set of unseen images, weighting each image by its activation strength for the neuron. 3 We formally describe our proposed Monosemanticity Score (MS) below, with an illustration given in Figure 2. This metric can be computed for each of the ω neurons extracted from the SAE. Given diverse set of images = {xn X}N n=1, and pretrained image encoder E, we first extract embeddings to obtain pairwise similarity matrix = [snm]n,m [1, 1]N , which captures semantic similarity between each pair of images. The similarity snm of the pair (xn, xm) is computed as the cosine similarity between the corresponding pair of embedding vectors: snm := E(xn) E(xm) E(xn)E(xm) . (5) We then collect activation vectors {ak = [ak k=1 across all ω neurons, for all images in the dataset I. Specifically, for each image xn, the activation of the k-th neuron: n]n RN }ω vn := fl(xn), := ϕk(vn), ak (6) where represents the layer at which the SAE is applied, fl is the composition of the first layers of the explained model, and ϕk is the k-th neuron of ϕ(vn) (or of vn when evaluating neurons of the original layer of ). To ensure consistent activation scale, we apply min-max normalization to each ak, yielding ak := [ak n]n [0, 1]N , where ak = minn ak ak maxn ak minn ak . (7) Using these normalized activations, we compute relevance matrix Rk = [rk nm]n,m [0, 1]N for each one of the ω neurons, which quantifies the shared neuron activation of each image pair: nak m. nm := ak rk (8) Finally, our proposed score MSk [1, 1] for the k-th neuron is computed as the average pairwise similarity weighted by the relevance, without considering same image pairs (xn, xn). This is formally defined as MSk := 1 (N 1) (cid:88) (cid:88) n=1 m=1 m=n rk nmsnm. (9) 3.3. Steering MLLMs with Vision SAEs We first describe LLaVA [22], an example MLLM architecture. After that we demonstrate how to steer it using an SAE attached to its vision encoder. The LLaVA model : expects pair of image and text (x, t) and outputs text answer o, where Rdt is the word embedding space. Internally, it converts the image into tx token embeddings {vi}tx i=1 obtained from vision encoder fl : RdtX composed of the first layers of CLIP [31]. These embeddings are then projected into visual tokens Hx Rdttx in the word embedding space, and are finally fed along with tokenized text Ht Rdttt into the pretrained LLM (e.g. LLaMA [38] or Vicuna [6]) to obtain the output text o. We modify this architecture by injecting pretrained SAE (ϕ, ψ) of width ω at the token-level after the vision encoder fl. For all token embeddings vi Rd, {1, . . . , tx}, we first extract the SAE decomposition into activation ai := ϕ(vi) Rω across all neurons. After identifying the neuron {1, . . . , ω} representing the targeted concept, to steer the overall model towards this concept, we manipulate the SAE activations of all token embeddings for the neuron to obtain {ˆai Rω}tX i=1, such that {1, . . . , ω}, ˆaj = (cid:40) α, aj , = = (10) where α is the intervention value we want to apply to the activation of neuron k. Finally, we decode the manipulated activation vectors for each token ˆai back into manipulated token embedding ˆvi = ψ(ˆai) Rd with the SAE decoder. Token embeddings are then processed as usual to generate the steered LLaVAs response. We include an illustration of the overall process in the Appendix. The SAEs let us induce controllable semantic biases in the response of the overall model without modifying the underlying model parameters, and without touching any textual part. In other words, we steer the model into seeing the targeted concept in the image, even if it is not present. 4. Experiments In this section, we first introduce our experimental details, before performing our analysis of the monosemanticity and the hierarchical structure being learnt by the SAEs. Finally, we also demonstrate the efficacy of steering the LLaVA [22] models using the CLIP SAE. 4.1. Experimental Settings We train SAEs to explain fixed and pretrained CLIPViT L/14-336px [31] model. In our experiments we use ImageNet [9] and iNaturalist 2021 [39] datasets. Specifically, to train and validate SAEs, we pre-extract activation vectors from the explained model obtained on the images of the given dataset. The activation vectors are extracted from the classification (CLS) tokens in the residual stream after layers {11, 17, 22, 23}, or from the output of the (last) projection layer, except for steering experiments, in which we train SAEs from 2 random token embeddings per image extracted after layer = 22. In the following sections, we are interested in both BatchTopK [4] and Matryoshka BatchTopK SAEs [5] variants. If not stated otherwise, we set the groups of Matryoshka SAEs Figure 3. Qualitative examples of highest activating images for different neurons from high (left) to low (right) Monosemanticity Scores (MS). As the score gets higher, images become more similar, illustrating the correlation with monosemanticity. as = {0.0625ω, 0.1875ω, 0.4375ω, ω}, which roughly corresponds to doubling the size of the number of neurons added with each level down. For the BatchTopK activation, we fix the maximum number of non-zero latent neurons to = 20. Both SAE types are compared across wide range of expansion factors ε {1, 2, 4, 8, 16, 64}. All SAEs are optimized for 105 steps with minibatches of size 4096 using Adam optimizer [19], with the learning rate initialized at ω following previous work [16]. To measure the quality of SAE reconstruction, we report the Fraction of xs Variance Explained by ˆx (FVE) and the L0-norm of ϕ(v) for sparsity of activations. We also quantify MS of neurons using CLIP-ViT B/32 [31] as image encoder E. 16 125 4.2. Evaluating Interpretability of VLM Neurons We train SAEs on Imagenet and use the validation set for evaluation and present in Figure 3 the 16 highest activating images for neurons with decreasing MS from left to right. We observe that the highest scoring neurons (with MS = 0.9, on the far left) are firing for images representing the same object, i.e. close-up pictures of koala (on the top) and red fish (on the bottom). As the score decreases, the corresponding neurons fire for less similar or even completely different objects or scenes. This illustrates that MS correlates well with the underlying monosemanticity. In Table 1, we report MSbest / MSworst for the best (in green) and the worst (in red) scoring neurons of two SAE types (BatchTopK [4] and Matryoshka BatchTopK [5]) trained at different layers with various expansion factors ε. We also include results for original neurons of the corresponding layer decomposed by SAEs (No SAE). We ob5 serve that SAEs neurons consistently have higher MS for their best neuron, and lower MS for their worst, when compared to original ones, implying that SAEs are better separating and disentangling concepts between their neurons. Interestingly, while MSbest and MSworst are respectively increasing and decreasing with higher expansion factor ε, i.e. with increased width ω of the SAE layer, this holds true already for expansion factor ε = 1, meaning that the disentanglement of concepts is also linked to the sparse dictionary learning and not only to the increased dimensionality. Finally, comparing SAE variants, we observe that the Matryoshka reconstruction objective improves the concept separation at same expansion factor. However, comparing FVE, Matryoshka SAEs achieve about 2 or 3 points lower for the same expansion factors (more details in Appendix). To analyze monosemanticity across all neurons, we plot in Figure 4 the scores of both the original neurons and the Matryoshka SAE neurons from the last layer of model . We ordered neurons by decreasing scores and normalized neuron indices to the [0, 1] interval to better compare SAEs with different widths. These results confirm our analysis above, and demonstrate that the vast majority of neurons within SAEs have improved MS compared to the original neurons. Even when comparing with ε = 1, i.e. with same width between the SAE and original layers, we can see that about 90% of the neurons within the SAE have better scores than the original neurons, proving once again that the sparse decomposition objective inherently induces better separation of concepts between neurons. Furthermore, MS share similar relative trend for increased expansion factors, implying that the absolute number of neurons increase for each Table 1. Comparison of the best / worst monosemanticity of neurons as measured by our proposed metric, for two recent SAE methods and different widths, applied on different layers of CLIP ViT-Large [31] model. SAE type Layer No SAE Expansion factor BatchTopK [4] Matryoshka [5, 25] 1 4 8 16 64 11 17 22 23 last 11 17 22 23 last 0.50 / 0.47 0.50 / 0.47 0.50 / 0.47 0.50 / 0.47 0.50 / 0.47 0.50 / 0.47 0.50 / 0.47 0.50 / 0.47 0.50 / 0.47 0.50 / 0.47 0.80 / 0.41 0.84 / 0.37 0.82 / 0.39 0.81 / 0.41 0.80 / 0.40 0.90 / 0.39 0.94 / 0.33 0.88 / 0.40 0.85 / 0.40 0.85 / 0.41 0.87 / 0.38 0.87 / 0.33 0.85 / 0.38 0.84 / 0.40 0.84 / 0.40 0.95 / 0.31 0.93 / 0.35 0.87 / 0.33 0.86 / 0.35 0.88 / 0. 0.90 / 0.28 0.94 / 0.35 0.89 / 0.37 0.89 / 0.35 0.87 / 0.36 0.97 / 0.23 0.96 / 0.29 0.89 / 0.29 0.90 / 0.35 0.89 / 0.31 0.91 / 0.27 0.94 / 0.28 0.93 / 0.29 0.91 / 0.27 0.87 / 0.31 1.00 / 0.22 0.96 / 0.22 0.94 / 0.23 0.91 / 0.19 0.91 / 0.26 0.95 / 0.24 0.96 / 0.24 0.93 / 0.15 0.93 / 0.24 0.89 / 0.25 0.94 / 0.18 0.97 / 0.14 1.00 / 0.15 0.93 / 0.17 0.92 / 0. 1.00 / 0.20 1.00 / 0.14 1.00 / 0.15 1.00 / 0.08 1.00 / 0.17 1.00 / 0.19 1.00 / 0.11 1.00 / 0.06 1.00 / 0.14 1.00 / 0.09 Figure 4. Monosemanticity Scores in decreasing order across neurons, normalized by width. Results are shown for the last layer of the model, without SAE (No SAE, in black dashed line), and with SAE using different expansion factors (in straight lines, for ε = 1, for ε = 4 and for ε = 16). value. We refer the reader to Appendix for MS with raw (unnormalized) neuron indices. Finally, although the worst score decreases with increased width, it is worth noting that only relatively few neurons reach such low scores. The relationship between the sparsity level used when training Matryoshka SAEs and the scores of the learned neurons is illustrated in Figure 5. We observe that stricter sparsity constraint decomposes the representation into more monosemantic features. The average MS peaks at 0.70 for the optimal setting of = 1, whereas it drops to 0.53 for = 50. However, this does not imply that the highest sparsity (K = 1) is always the best choice, as improvements in MS come at the cost of reduced reconstruction quality. In the same setup, the FVE varies from 31.3% at the lowest = 1 to 74.9% at the highest = 50. To balance the inFigure 5. Impact of sparsity factor on Monosemanticity Scores across neurons extracted from SAEs trained on the last layer of the model with expansion factor ε = 1. Results are shown for different sparsity levels, with straight lines for = 1, for = 10, for = 20, and for = 50. Scores of the original neurons (No SAE, in black dashed lines) are added for comparison. terpretability and the reconstruction quality, we set = 20 as our default, for which the FVE remains at reasonable 66.8%. Detailed results are in the Appendix. 4.3. Matryoshka Hierarchies We train and evaluate the SAE on embeddings extracted from iNaturalist [39] dataset using an expansion factor ε = 2 and groups of size = {3, 16, 69, 359, 1536}. These group sizes correspond to the numbers of nodes of the first 5 levels of the species taxonomy tree of the dataset, i.e. the respective number of kingdoms, phylums, classes, orders, and families. To measure the granularity of the concepts, we map each neuron to the most fitting depth in the iNaturalist taxon6 Figure 6. We steer the outputs of LLaVA by clamping the activation values of chosen neuron, i.e. Neuron #39 = pencil neuron, in the CLIP SAE. We observe that while initially the poem follows the instruction (the prompt + white image) strongly, the outputs become more and more influenced by the concept that this neuron represents with increasing intervention weight α, talking about attributes of pencil first, and then just the concept pencil. This shows that our interventions enable new capabilities for the unsupervised steering of these models. Table 2. Average LCA depth and monosemanticity (MS) scores across neurons at each level in the Matryoshka nested dictionary. Level Depth MS Avg. Max. Min. 3.33 0.67 0.69 0.66 1 2.92 0.68 0.75 0.64 3.85 0.70 0.78 0.63 3 3.86 0.73 0.85 0.61 4.06 0.74 0.89 0.43 omy tree to compare the hierarchy of concepts within the Matryoshka SAE with human-defined ones. To obtain this neuron-to-depth mapping, we select the top-16 activating images per neuron, and compute the average depth of the Lowest Common Ancestors (LCA) in the taxonomy tree for each pair of images. For instance, given neuron with an average LCA depth of 2, we can assume that images activating this neuron are associated to species from multiple classes of the same phylum. We report the average assigned LCA depth of neurons across the Matryoshka group level in Table 2. We notice that average LCA depths are correlated with the level, suggesting that the Matryoshka hierarchy can be aligned with human-defined hierarchy. We additionally aggregate statistics of MS of neurons for each level. Average and maximum MS also correlates with the level, confirming that the most specialized neurons are found in the lowest levels. Table 3. Average similarity between neurons highly activating images and output word, with and without steering, using white input image or random images from ImageNet. Upperbound score with correct image and class name text pairs is 0.283 0.034, while lower-bound with random image and class name text pair is 0.185 0.028. Steering White Image 0.259 0.036 0.212 0. ImageNet 0.263 0.037 0.211 0.028 4.4. Steering Multimodal LLMs We train the Matryoshka BatchTopK SAE [5] with expansion factor ε = 64 on random token embeddings from layer = 22 of the CLIP vision encoder obtained for the ImageNet training data. The trained SAE is plugged after the vision encoder of LLaVA-1.5-7b [22] (uses Vicuna [6] LLM). We illustrate in both Figure 6 and Figure 7 the effectiveness of steering the output of LLaVA, in two different contexts, when manipulating single neuron of the SAE. In Figure 6, we prompt the model with the instruction Write me short love poem, along with white image. By intervening on an SAE neuron associated to the pencil concept and increasing the corresponding activation value, we observe the impact on the generated output text. While the initial output mentions the white color and focuses on the textual instruction, i.e. love poem, the output becomes 7 Figure 7. Effects of neuron interventions on MLLM-generated scientific article titles. Steering magnitudes are categorized as 0, medium, and high based on the intervention strength. The neurons are visualized with the highest activating images from which we deduce their associated concepts: polka dots, shipwreck, and rainbow. more and more focused on pencil attributes as we manually increase the intervention value α (most highly activating images for the selected neuron is in appendix) until it only mentions pencils. In Figure 7, we show different input prompt (Generate scientific article title) given with the same white image, and results obtained when intervening on three different neurons, for three different intervention strengths. We also include images highly activating each neuron, that can be respectively associated to the concepts of polka dots, shipwreck and rainbow. No matter which neuron is selected, as expected, with the same prompt the output is the same for when α = 0. In each case, the generated output includes more and more references to the concept that fires the neuron the most as we increase the intervention value. Hence, intervening on SAE neurons in the vision encoder of an MLLM steers the visual information fed to the LLM towards the concepts encoded in the selected neurons. To further confirm the steering capabilities, we prompt the model to answer What is shown on the image? Use exactly one word. and compare its original output with the one generated after fixing specific SAE neuron activation value at α = 100, one neuron after the other. More specifically, we measure embedding similarity in CLIP space between the output word and the top-16 images activating the selected neuron. In the first setup, we input the same white image as above, and intervene on the first 1000 neurons. In the second setup, we use 1000 random images from ImageNet as inputs and steer only the 10 first neurons. We report results for the two setups in Table 3. In both setups, steering SAE neurons induces higher similarity between images highly activating the imputted neuron and the modified text outputs, compared to the original text outputs. To put these results in perspective, we additionally measure the average similarity scores between one representative image of each ImageNet class with the class name text as an upper-bound value, and the average scores between randomly selected pairs of image and class name text as lower-bound value. For the lower-bound, we obtain an average score of 0.185 0.028 and 0.283 0.034 for the upper-bound. These results illustrate the significance of the relative improvement we obtain from steering, i.e. relative improvement of about 22% when considering the range between the possible lowerand upper-bounds. 5. Conclusion We introduced the Monosemanticity Score (MS), quantitative metric for evaluating monosemanticity at the neuron level in SAEs trained on VLMs. Our analysis revealed that SAEs primarily increased monosemanticity through sparsity, and we demonstrated that hierarchical structuring of concepts improved representations, as exemplified by the superior performance of Matryoshka SAEs. Leveraging the clear separation of concepts encoded in SAEs, we explored their effectiveness for unsupervised, concept-based steering of multimodal LLMs, highlighting promising direction for future research. Potential extensions of this work include adapting our metric to text representations and investigating the interplay between specialized (low-level) and broad (high-level) concepts within learned representations."
        },
        {
            "title": "Acknowledgements",
            "content": "This work was partially funded by the ERC (853489 - DEXIM) and the Alfried Krupp von Bohlen und Halbach Foundation, which we thank for their generous support. We are also grateful for partial support from the Pioneer Centre for AI, DNRF grant number P1. Shyamgopal Karthik thanks the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support. Mateusz Pach would like to thank the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program for support."
        },
        {
            "title": "References",
            "content": "[1] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of word senses, with applications to polysemy. Transactions of the Association for Computational Linguistics, 6:483495, 2018. 1 [2] Usha Bhalla, Alex Oesterling, Suraj Srinivas, Flavio Calmon, and Himabindu Lakkaraju. Interpreting clip with sparse linear concept embeddings (splice). arXiv preprint arXiv:2402.10376, 2024. 2 [3] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2, 2023. 1, 3 [4] Bart Bussmann, Patrick Leask, and Neel Nanda. Batchtopk sparse autoencoders. arXiv preprint arXiv:2412.06410, 2024. 1, 2, 3, 4, 5, 6 [5] Bart Bussmann, Patrick Leask, and Neel Nanda. Learning multi-level features with matryoshka saes. AI Alignment Forum, 2024. 1, 2, 3, 4, 5, 6, 7 [6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 4, 7 [7] Bartosz Cywinski and Kamil Deja. Saeuron: Interpretable concept unlearning in diffusion models with sparse autoencoders. arXiv preprint arXiv:2501.18052, 2025. [8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. 2 [9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 4 [10] Amil Dravid, Yossi Gandelsman, Alexei A. Efros, and Assaf Shocher. Rosetta neurons: Mining the common units in model zoo. In ICCV, 2023. 2 [11] Esin Durmus, Alex Tamkin, Jack Clark, Jerry Wei, Jonathan Marcus, Joshua Batson, Kunal Handa, Liane Lovitt, Meg Tong, Miles McCain, Oliver Rausch, Saffron Huang, Sam Bowman, Stuart Ritchie, Tom Henighan, and Deep Ganguli. Evaluating feature steering: case study in mitigating social biases, 2024. 2 [12] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac HatfieldDodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. arXiv preprint arXiv:2209.10652, 2022. [13] Hugo Fry. Towards multimodal interpretability: Learning sparse interpretable features in vision transformers, 2024. 2 [14] Yossi Gandelsman, Alexei Efros, and Jacob Steinhardt. Interpreting clips image representation via text-based decomposition. In ICLR, 2024. 2 [15] Yossi Gandelsman, Alexei Efros, and Jacob Steinhardt. Interpreting the second-order effects of neurons in clip. In ICLR, 2025. 2 [16] Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. In The Thirteenth International Conference on Learning Representations, 2025. 1, 3, 5 [17] Dahye Kim and Deepti Ghadiyaram. Concept steerers: Leveraging k-sparse autoencoders for controllable generations. arXiv preprint arXiv:2501.19066, 2025. 2 [18] Dahye Kim, Xavier Thomas, and Deepti Ghadiyaram. Revelio: Interpreting and leveraging semantic information in diffusion models. arXiv preprint arXiv:2411.16725, 2024. 2 [19] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. 5 [20] Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et al. Matryoshka representation learning. Advances in Neural Information Processing Systems, 35:3023330249, 2022. 2 [21] Hyesu Lim, Jinho Choi, Jaegul Choo, and Steffen SchneiSparse autoencoders reveal selective remapping arXiv preprint der. of visual concepts during adaptation. arXiv:2412.05276, 2024. 1, 2 [22] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 2, 4, 7 [23] Alireza Makhzani and Brendan Frey. K-sparse autoencoders. arXiv preprint arXiv:1312.5663, 2013. 1, 3 [24] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image pretraining. In ECCV, 2022. 2 [25] Noa Nabeshima. Matryoshka sparse autoencoders. AI Alignment Forum, 2024. 1, 2, 3, 6 [26] Tuomas Oikarinen and Tsui-Wei Weng. CLIP-dissect: Automatic description of neuron representations in deep vision networks. In ICLR, 2023. 1, 2 [27] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature visualization. Distill, 2(11):e7, 2017. 1 [28] Bruno Olshausen and David Field. Sparse coding with an overcomplete basis set: strategy employed by v1? Vision research, 37(23):33113325, 1997. 2 9 manticity in multimodal representations. arXiv:2502.14888, 2025. 2 arXiv preprint [41] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 1, [42] Kaichen Zhang, Yifei Shen, Bo Li, and Ziwei Liu. Large multi-modal models can interpret features in large multimodal models. arXiv preprint arXiv:2411.14982, 2024. 2 [29] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2 [30] Jayneel Parekh, Pegah Khayatan, Mustafa Shukor, Alasdair Newson, and Matthieu Cord. concept-based explainability framework for large multimodal models. NeurIPS, 2024. 2 [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 1, 2, 4, 5, 6 [32] Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, Janos Kramar, and Neel Nanda. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders. arXiv preprint arXiv:2407.14435, 2024. 1, 2 [33] Sukrut Rao, Sweta Mahajan, Moritz Bohle, and Bernt Schiele. Discover-then-name: Task-agnostic concept bottlenecks via automated concept discovery. In European Conference on Computer Vision, pages 444461. Springer, 2024. 1, 2, 3 [34] Samuel Stevens, Wei-Lun Chao, Tanya Berger-Wolf, and Yu Su. Sparse autoencoders for scientifically rigorous interpretation of vision models. arXiv preprint arXiv:2502.06755, 2025. [35] Viacheslav Surkov, Chris Wendler, Mikhail Terekhov, Justin Deschenaux, Robert West, and Caglar Gulcehre. Unpacking sdxl turbo: Interpreting text-to-image models with sparse autoencoders. arXiv preprint arXiv:2410.22366, 2024. 2 [36] Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread, 2024. 2 [37] Harrish Thasarathan, Julian Forsyth, Thomas Fel, Matthew Kowal, and Konstantinos Derpanis. Universal sparse autoencoders: Interpretable cross-model concept alignment. arXiv preprint arXiv:2502.03714, 2025. 1, 2 [38] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 4 [39] Grant Van Horn, Elijah Cole, Sara Beery, Kimberly Wilber, Serge Belongie, and Oisin Mac Aodha. Benchmarking representation learning for natural world image collections. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1288412893, 2021. 2, 4, 6 [40] Hanqi Yan, Xiangxiang Cui, Lu Yin, Paul Pu Liang, YuThe multi-faceted monoselan He, and Yifei Wang. 10 Figure A1. LLaVA-like models can be steered towards seeing concept (e.g. panda) not present in the input image x. By attaching SAE after vision encoder and intervening on its neuron representing that concept, we effectively manipulate the LLMs response. Such flexible and precise steering is possible thanks to the extensive concept dictionary identified through the SAE. A. More details on steering We illustrate in Figure A1 how we steer LLaVA-like models. We separately train SAEs on top of the pretrained CLIP vision encoder to reconstruct the token embeddings vi , and then attach it back after the vision encoder during inference. Intervening on neuron within the SAE layer steers the reconstructed tokens ˆvi towards the activated concept, which then steers the LLMs generated output. Figure A2. MS in decreasing order across neurons. Results are shown for layer without SAE (No SAE), and with SAE using different expansion factors (1, 4 and 16). B. Additional results on monosemanticity Monosemanticity scores across all neurons, without normalized index, are shown in Figure A2. We observe that neurons cover wider range of scores as we increase the width of the SAE layer. Furthermore, for given threshold of monosemanticity, the number of neurons having score higher than this threshold is also increasing with the width. We report in Table A1, the average ( std) monosemanticity scores across neurons for the two SAE variants, attached at different layers and for increasing expansion factors. Although average scores remain similar when increasing expansion factor, we observe high increase between the original layer and an SAE with expansion factor ε = 1. Tables A2 and A4 show average, best and worst MS computed using DINOv2 ViT-B as the vision encoder E. Even more distinctively than in previous setup, the neurons from SAEs score better compared to the ones originally found in the model. In Tables A3 and A5, we report MS statistics for SAEs trained for SigLIP SoViT-400m model. The results highly resemble the ones for CLIP ViT-L model. Finally, in Figure A3 we plot MS across single neurons. We consider setups in which (a) neurons of CLIP ViT-L are evaluated with CLIP ViT-B as the image encoder E, (b) neurons of CLIP ViT-L are evaluated with DINOv2 ViT-B as E, and (c) neurons of SigLIP SoViT-400m are evaluated with CLIP ViT-B as E. In all three cases SAE neurons are more monosemantic compared to the original neurons of the models. It shows that MS results are consistent across different architectures being both explained and used as E. 11 Table A1. The average MS of neurons in CLIP ViT-L model. CLIP ViT-B is used as the image encoder E. SAE type Layer No SAE Expansion factor x1 x2 x8 x16 x64 BatchTopK Matryoshka 11 17 22 23 last 11 17 22 23 last 0.4837 0.0067 0.4840 0.0079 0.4816 0.0053 0.4814 0.0045 0.4812 0.0042 0.4837 0.0067 0.4840 0.0079 0.4816 0.0053 0.4814 0.0045 0.4812 0.0042 0.52 0.05 0.55 0.07 0.60 0.09 0.60 0.09 0.59 0.08 0.54 0.08 0.57 0.09 0.61 0.09 0.60 0.09 0.59 0.09 0.53 0.06 0.56 0.08 0.61 0.09 0.61 0.10 0.60 0. 0.55 0.08 0.58 0.09 0.62 0.09 0.62 0.10 0.61 0.10 0.53 0.05 0.57 0.08 0.62 0.09 0.62 0.10 0.61 0.10 0.55 0.08 0.58 0.10 0.63 0.10 0.62 0.10 0.62 0.11 0.53 0.05 0.56 0.05 0.63 0.09 0.62 0.10 0.61 0.10 0.54 0.08 0.58 0.10 0.62 0.11 0.61 0.11 0.61 0.11 0.53 0.05 0.56 0.08 0.62 0.10 0.61 0.10 0.59 0. 0.53 0.07 0.57 0.10 0.62 0.11 0.60 0.11 0.59 0.12 0.53 0.06 0.56 0.09 0.60 0.11 0.59 0.12 0.56 0.10 0.52 0.06 0.54 0.09 0.59 0.12 0.54 0.11 0.54 0.12 Table A2. The average MS of neurons in CLIP ViT-L model. DINOv2 ViT-B is used as the image encoder E. SAE type Layer No SAE Expansion factor x1 x2 x4 x16 x64 BatchTopK Matryoshka 11 17 22 23 last 11 17 22 23 last 0.0135 0.0003 0.0135 0.0004 0.0135 0.0003 0.0135 0.0003 0.0135 0.0002 0.0135 0.0003 0.0135 0.0004 0.0135 0.0003 0.0135 0.0003 0.0135 0.0002 0.03 0.06 0.05 0.07 0.14 0.12 0.15 0.13 0.12 0.11 0.05 0.10 0.09 0.14 0.17 0.17 0.17 0.16 0.16 0.17 0.04 0.06 0.07 0.09 0.18 0.15 0.18 0.16 0.17 0.15 0.06 0.10 0.10 0.15 0.21 0.18 0.21 0.19 0.20 0. 0.04 0.06 0.08 0.11 0.20 0.17 0.20 0.17 0.19 0.17 0.05 0.09 0.11 0.16 0.23 0.19 0.22 0.18 0.23 0.19 0.03 0.05 0.07 0.10 0.21 0.17 0.21 0.17 0.19 0.16 0.05 0.09 0.11 0.15 0.23 0.19 0.22 0.18 0.22 0.19 0.03 0.05 0.07 0.10 0.21 0.18 0.20 0.18 0.16 0.16 0.04 0.08 0.10 0.15 0.23 0.19 0.20 0.18 0.19 0. 0.03 0.05 0.06 0.10 0.17 0.18 0.17 0.18 0.13 0.15 0.03 0.06 0.06 0.10 0.18 0.19 0.12 0.16 0.13 0.16 Table A3. The average MS of neurons in SigLIP SoViT-400m model. CLIP ViT-B is used as the image encoder E. SAE type Layer No SAE Expansion factor x1 x2 x4 x8 x64 BatchTopK Matryoshka 11 16 21 last 11 16 21 last 0.4805 0.0014 0.4809 0.0024 0.4810 0.0052 0.4811 0. 0.4805 0.0014 0.4809 0.0024 0.4810 0.0052 0.4811 0.0048 0.50 0.03 0.51 0.04 0.52 0.05 0.61 0.09 0.50 0.03 0.51 0.05 0.52 0.05 0.61 0.09 0.51 0.04 0.52 0.05 0.53 0.06 0.61 0.09 0.50 0.05 0.52 0.06 0.53 0.06 0.62 0.10 0.51 0.05 0.52 0.06 0.53 0.06 0.62 0. 0.50 0.05 0.52 0.07 0.53 0.06 0.62 0.10 0.51 0.06 0.53 0.07 0.53 0.07 0.62 0.09 0.50 0.06 0.52 0.07 0.53 0.07 0.62 0.10 0.52 0.06 0.53 0.07 0.54 0.08 0.62 0.10 0.51 0.07 0.52 0.07 0.52 0.07 0.60 0.11 0.52 0.07 0.53 0.08 0.53 0.08 0.60 0. 0.51 0.07 0.51 0.07 0.51 0.07 0.58 0.11 12 Table A4. Comparison of the best / worst MS of neurons in CLIP ViT-L model. DINOv2 ViT-B is used as the image encoder E. SAE type Layer No SAE Expansion factor 1 2 4 8 64 BatchTopK Matryoshka 11 17 22 23 last 11 17 22 23 last 0.01 / 0.01 0.01 / 0.01 0.01 / 0.01 0.01 / 0.01 0.01 / 0. 0.01 / 0.01 0.01 / 0.01 0.01 / 0.01 0.01 / 0.01 0.01 / 0.01 0.61 / -0.02 0.65 / 0.01 0.66 / 0.01 0.73 / 0.01 0.57 / 0.01 0.84 / -0.06 0.86 / -0.04 0.83 / 0.01 0.82 / 0.01 0.82 / 0.01 0.73 / -0.08 0.79 / -0.02 0.79 / 0.01 0.72 / 0.01 0.78 / 0.01 0.90 / -0.07 0.84 / -0.05 0.83 / 0.01 0.84 / 0.01 0.91 / 0.01 0.71 / -0.06 0.86 / -0.07 0.80 / 0.01 0.83 / 0.01 0.78 / 0. 0.95 / -0.08 0.93 / -0.07 0.87 / -0.02 0.89 / -0.04 0.89 / -0.03 0.87 / -0.07 0.86 / -0.08 0.88 / -0.08 0.89 / -0.02 0.81 / -0.01 1.00 / -0.11 0.94 / -0.08 0.94 / -0.06 0.93 / -0.04 0.93 / -0.05 0.90 / -0.10 0.93 / -0.08 0.92 / -0.06 0.93 / -0.06 0.85 / -0.04 0.89 / -0.10 0.96 / -0.08 1.00 / -0.11 0.96 / -0.06 0.91 / -0.07 1.00 / -0.11 1.00 / -0.12 1.00 / -0.11 1.00 / -0.10 1.00 / -0. 1.00 / -0.10 1.00 / -0.14 1.00 / -0.11 1.00 / -0.11 1.00 / -0.12 Table A5. Comparison of the best / worst MS of neurons in SigLIP SoViT-400m model. CLIP ViT-L is used as the image encoder E. SAE type Layer No SAE Expansion factor 1 2 4 8 16 BatchTopK Matryoshka 11 16 21 last 11 16 21 last 0.49 / 0.48 0.53 / 0.47 0.54 / 0.47 0.50 / 0.47 0.49 / 0.48 0.53 / 0.47 0.54 / 0.47 0.50 / 0. 0.61 / 0.41 0.74 / 0.38 0.76 / 0.38 0.83 / 0.41 0.70 / 0.40 0.78 / 0.40 0.85 / 0.39 0.87 / 0.40 0.83 / 0.29 0.75 / 0.34 0.77 / 0.35 0.86 / 0.40 0.93 / 0.29 0.84 / 0.29 0.81 / 0.37 0.87 / 0.38 0.88 / 0.27 0.93 / 0.25 0.83 / 0.25 0.88 / 0.37 0.77 / 0.27 0.91 / 0.19 0.83 / 0.25 0.89 / 0. 0.90 / 0.23 0.94 / 0.20 0.89 / 0.17 0.92 / 0.33 0.93 / 0.18 0.93 / 0.18 0.93 / 0.24 0.91 / 0.25 1.00 / 0.12 0.93 / 0.22 0.95 / 0.20 0.93 / 0.20 0.91 / 0.22 1.00 / 0.19 0.94 / 0.21 0.94 / 0.15 1.00 / 0.15 1.00 / 0.18 1.00 / 0.11 1.00 / 0.11 1.00 / 0.16 1.00 / 0.16 1.00 / 0.15 1.00 / 0. (a) Neurons of CLIP ViT-L evaluated with CLIP ViT-B as the image encoder (b) Neurons of CLIP ViT-L evaluated with DINOv2 ViT-B as the image encoder (c) Neurons of SigLIP SoViT-400m evaluated with CLIP ViT-B as the image encoder Figure A3. MS in decreasing order across neurons. Results are shown for the last layers of two different models, without SAE (black dashed line), and with SAE being trained with expansion factor 1 (green solid line). MS is computed with distinct image encoders E. 13 C. Reconstruction of SAEs In Table A6 and Table A7, we report respectively FVE and sparsity (L0), for the two SAE variants we compare in Section 4.2. As BatchTopK activation enforces sparsity on batch-level, during test-time it is replaced with ReLU(xγ), with is the input and γ is vector of thresholds estimated for each neuron, as the average of the minimum positive activation values across number of batches. For this reason the test-time sparsity may slightly differ from fixed at the value of 20 in our case. We report in Table A8 the detailed metrics (FVE, L0 and statistics of MS) obtained for SAEs trained with different values considered in Section 4.2. Table A6. Comparison of Fraction of Variance Explained (FVE) by different SAEs trained with = 20 for CLIP ViT-L model. SAE type Layer No SAE BatchTopK Matryoshka 11 17 22 23 last 11 17 22 23 last 100 100 100 100 100 100 100 100 100 100 Expansion factor x1 x2 x4 x8 x16 74.7 70.4 68.7 67.2 70.1 72.8 67.3 65.5 63.9 66.8 75.0 71.9 72.6 71.5 74.6 73.9 69.5 69.6 68.5 71.6 75.1 72.6 74.9 74.0 77.1 74.5 70.7 71.5 71.0 74. 75.0 72.9 76.0 75.3 78.2 75.1 71.8 74.0 73.1 76.0 74.7 72.9 76.8 76.0 78.6 75.2 72.6 75.4 74.8 77.6 73.5 72.5 77.4 76.8 79.1 74.5 72.7 76.6 74.6 78. Table A7. Comparison of true sparsity measured by L0-norm for different SAEs trained with = 20 for CLIP ViT-L model. SAE type Layer No SAE BatchTopK Matryoshka 11 17 22 23 last 11 17 22 23 last 1024 1024 1024 1024 768 1024 1024 1024 1024 768 Expansion factor x1 x4 x8 x16 x64 19.7 19.4 19.6 19.8 19.9 19.4 19.3 19.7 19.7 20. 19.5 19.4 19.7 19.8 19.9 19.5 19.3 19.7 19.8 19.9 19.4 19.2 19.7 19.9 19.9 19.4 19.3 19.6 19.8 19.8 19.6 19.6 19.8 20.1 20.1 19.6 19.4 19.8 19.9 19. 20.0 19.5 20.3 20.3 20.2 19.8 19.5 19.9 20.6 20.2 22.9 22.3 23.0 22.2 22.2 21.3 20.5 22.0 25.1 22.5 Table A8. Statistics for SAEs trained with different sparsity constraint on activations of the last layer with expansion factor 16. No SAE row contains results for raw activations before attaching the SAE. 1 10 20 50 0.9 9.9 20.0 50.1 L0 FVE (%) MS Min Max Mean 0.32 0.40 0.41 0.41 0.47 0.90 0.85 0.85 0.81 0.50 0.70 0.08 0.61 0.09 0.59 0.09 0.53 0. 0.48 0.00 31.3 60.6 66.8 74.9 No SAE Figure A4. Images highly activating the neuron we intervene on in Figure 6, which we manually labeled as Pencil Neuron. D. Uniqueness of concepts The sparse reconstruction objective regularizes the SAE activations to focus on different concepts. To confirm it in practice, we collect top-16 highest activating images for each neuron of SAE and compute Jaccard Index between every pair of neurons. The images come from training set. We exclude 10 out of 12288 neurons for which we found less than 16 activating images and use Matryoshka SAE trained on the last layer with expansion factor of 16. We find that > 0 for 16000 out of 75368503 pairs (> 0.03%) and > 0.5 for only 20 pairs, which shows very high uniqueness of learned concepts. E. Additional qualitative results We illustrate in Figure A4 the highly activating images for the Pencil neuron, which we used for steering in Figure 6. In Figure A5 we provide more randomly selected examples of neurons for which we computed MS. Figure A5. Qualitative examples of highest activating images for different neurons from high (left) to low (right) monosemanticity score. As the metric gets higher, highest activating images are more similar, illustrating the correlation with monosemanticity."
        }
    ],
    "affiliations": [
        "Helmholtz Munich",
        "Munich Center of Machine Learning",
        "Munich Data Science Institute",
        "Technical University of Munich",
        "University of Copenhagen",
        "University of Tubingen"
    ]
}