{
    "paper_title": "KITAB-Bench: A Comprehensive Multi-Domain Benchmark for Arabic OCR and Document Understanding",
    "authors": [
        "Ahmed Heakl",
        "Abdullah Sohail",
        "Mukul Ranjan",
        "Rania Hossam",
        "Ghazi Ahmed",
        "Mohamed El-Geish",
        "Omar Maher",
        "Zhiqiang Shen",
        "Fahad Khan",
        "Salman Khan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the growing adoption of Retrieval-Augmented Generation (RAG) in document processing, robust text recognition has become increasingly critical for knowledge extraction. While OCR (Optical Character Recognition) for English and other languages benefits from large datasets and well-established benchmarks, Arabic OCR faces unique challenges due to its cursive script, right-to-left text flow, and complex typographic and calligraphic features. We present KITAB-Bench, a comprehensive Arabic OCR benchmark that fills the gaps in current evaluation systems. Our benchmark comprises 8,809 samples across 9 major domains and 36 sub-domains, encompassing diverse document types including handwritten text, structured tables, and specialized coverage of 21 chart types for business intelligence. Our findings show that modern vision-language models (such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like EasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate (CER). Furthermore, we highlight significant limitations of current Arabic OCR models, particularly in PDF-to-Markdown conversion, where the best model Gemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in accurately recognizing Arabic text, including issues with complex fonts, numeral recognition errors, word elongation, and table structure detection. This work establishes a rigorous evaluation framework that can drive improvements in Arabic document analysis methods and bridge the performance gap with English OCR technologies."
        },
        {
            "title": "Start",
            "content": "KITAB-Bench: Comprehensive Multi-Domain Benchmark for"
        },
        {
            "title": "Arabic OCR and Document Understanding",
            "content": "Ahmed Heakl* , Abdullah Sohail* , Mukul Ranjan* , Rania Hossam* , Ghazi Ahmed Mohamed El-Geish , Omar Maher , Zhiqiang Shen , Fahad Khan , Salman Khan MBZUAI Monta AI Australian National University {ahmed.heakl,mabdullah.sohail,mukul.ranjan,salman.khan}@mbzuai.ac.ae {geish,omar}@monta.ai Link√∂ping University https://mbzuai-oryx.github.io/KITAB-Bench/ 5 2 0 2 0 2 ] . [ 1 9 4 9 4 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "With the growing adoption of RetrievalAugmented Generation (RAG) in document processing, robust text recognition has become increasingly critical for knowledge extraction. While OCR (Optical Character Recognition) for English and other languages benefits from large datasets and well-established benchmarks, Arabic OCR faces unique challenges due to its cursive script, right-to-left text flow, and complex typographic and calligraphic features. We present KITAB-Bench, comprehensive Arabic OCR benchmark that fills the gaps in current evaluation systems. Our benchmark comprises 8,809 samples across 9 major domains and 36 sub-domains, encompassing diverse document types including handwritten text, structured tables, and specialized coverage of 21 chart types for business intelligence. Our findings show that modern vision-language models (such as GPT-4, Gemini, and Qwen) outperform traditional OCR approaches (like EasyOCR, PaddleOCR, and Surya) by an average of 60% in Character Error Rate (CER). Furthermore, we highlight significant limitations of current Arabic OCR models, particularly in PDF-to-Markdown conversion, where the best model Gemini-2.0-Flash achieves only 65% accuracy. This underscores the challenges in accurately recognizing Arabic text, including issues with complex fonts, numeral recognition errors, word elongation, and table structure detection. This work establishes rigorous evaluation framework that can drive improvements in Arabic document analysis methods and bridge the performance gap with English OCR technologies."
        },
        {
            "title": "Introduction",
            "content": "With the upsurge in adoption of RetrievalAugmented Generation (RAG) based systems for document processing, the quality of document ingestion pipelines has become increasingly critical. * Equal Contributions Figure 1: Overview of the core domains and subdomains in KITAB-Bench. Our benchmark spans nine major domains (e.g., OCR, charts to JSON, table recognition) and 36 sub-domains (e.g., scanned text, handwritten text, various chart types), providing comprehensive evaluation framework for modern Arabic document processing and analysis. Optical Character Recognition (OCR) plays crucial role in this pipeline, enabling the conversion of physical documents into machine-readable text and databases for enabling effective knowledge retrieval. Although significant progress has been made in the multilingual OCR (JaidedAI, 2020; Fu et al., 2024; Wei et al., 2024; Smith, 2007), with comprehensive datasets like PubLayNet (Zhong et al., 2019b), DocBank (Li et al., 2020), M6Doc (Cheng et al., 2023), and DocLayNet (Pfitzmann et al., 2022), Arabic OCR continues to lag behind. This gap is largely due to the unique challenges of the Arabic script, including its cursive nature, complex typography, and right-to-left text orientation.  (Table 1)  , Existing Arabic OCR datasets like KHATT (Mahmoud et al., 2014) and IFN/ENIT (Pechwitz et al., 2002) focus mainly on handwritten text, whereas APTI (Slimane et al., Figure 2: Overview of different tasks in our benchmark: Eight key components illustrating the task inputs and outputs for table recognition, chart understanding, text recognition, diagram analysis, VQA, line detection, layout analysis, and PDF-to-Markdown conversion, complete with input/output examples for each task. Domain/ Characteristics PDF to Markdown Layout Detection Line Detection Line Recognition Table Recognition Image to Text Charts to JSON Diagram to Code VQA Handwritten Samples Open Source Total Samples (#) EXAMS-V CamelMIDAD KHATT 823 Bench 3,004 29,435 5, KITABBench (Ours) 8,809 Table 1: Comparison of Arabic OCR Benchmarks Across Different Domains. Benchmarks compared: LaraBench (Abdelali et al., 2023), CamelBench (Ghaboura et al., 2024), MIDAD (Bhatia et al., 2024), KHATT (Mahmoud et al., 2014), and KITAB-Bench (Ours). (: Only the Arabic samples are considered.) (: The test set of the dataset is considered.) 2009) covers only specific aspects of printed text. These efforts fail to address advanced document processing challenges such as table parsing, font detection, and numeral recognition. Arabic benchmarks like CAMEL-Bench (Ghaboura et al., 2024) and LAraBench (Abdelali et al., 2023) evaluate large multimodal and language models, but they give limited attention to document understanding tasks. Consequently, there remains need for more comprehensive framework to systematically evaluate and compare Arabic OCR solutions. Our benchmark addresses these gaps by offering diverse document types and evaluation tasks to facilitate in-depth assessments of modern OCR systems. We present KITAB-Bench, comprehensive Arabic OCR benchmark spanning 9 domains and 36 sub-domains. Our framework evaluates layout detection (text blocks, tables, figures), multi-format recognition (printed/handwritten text, charts, diagrams), and structured output generation (HTML tables, DataFrame charts, markdown). This enables rigorous assessment of both basic OCR capabilities and advanced document understanding tasks. The contributions of this work include (1) comprehensive Arabic OCR benchmark covering multiple document types and recognition tasks. (2) Detailed evaluation metrics for assessing performance across different document understanding challenges. We also propose CharTeX and CODM metric to evaluate chart extraction and diagram extraction respectively. (3) Baseline results for popular OCR systems and Vision Language Models (VLMs), highlighting current limitations and areas for improvement. (4) standardized framework for comparing Arabic OCR systems, facilitating future research and development."
        },
        {
            "title": "2 Related Work",
            "content": "The development of robust Optical Character Recognition (OCR) systems has been extensively studied across document layout analysis (Zhao et al., 2024; Shen et al., 2021; Paruchuri, 2024b; JaidedAI, 2020; Auer et al., 2024; Li et al., 2020), table detection (Li et al., 2019; Paliwal et al., 2019; Nassar et al., 2022; Li et al., 2021; Schreiber et al., 2017), and document understanding (Staar et al., 2018; Weber et al., 2023; Livathinos et al., 2021). While English OCR benefits from rich Figure 3: Comparison of model performance across four document understanding tasks (Table Recognition, Image to Text, Diagram to JSON, and Layout Detection) showing successful and failed cases for different models including Ground Truth, EasyOCR, GPT-4, Qwen, Surya, Tesseract, Yolo, and DETR on Arabic document benchmark data."
        },
        {
            "title": "Total Samples",
            "content": "PDF to Markdown Layout Line Detection Line Recognition Table Recognition Image to Text Charts to DataFrame Diagram to Json VQA"
        },
        {
            "title": "Total",
            "content": "33 2,100 378 378 456 3,760 576 226 902 8,809 Table 2: Distribution of samples across different domains in our dataset. more detailed count for different sub-domains and data sources is in Appendix A. datasets like PubLayNet (Zhong et al., 2019b), DocBank (Li et al., 2020), M6Doc (Cheng et al., 2023), and DocLayNet (Pfitzmann et al., 2022), Arabic lacks standardized benchmarks for diverse fonts and layouts. Recent efforts like MIDAD (Bhatia et al., 2024) curates extensive training data for Arabic OCR and handwriting recognition, while Peacock (Alwajih et al., 2024) introduces culturally-aware Arabic multimodal models. Existing resources such as CAMEL-Bench (Ghaboura et al., 2024), LAraBench (Abdelali et al., 2023), MADAR (Bouamor et al., 2018), OSACT (Mubarak et al., 2022), and Tashkeela (Zerrouki and Balla, 2017) focus on language modeling or specific tasks rather than full-page OCR evaluation. Handwriting datasets including HistoryAr (Pantke et al., 2014), IFN/ENIT (Pechwitz et al., 2002), KHATT (Mahmoud et al., 2014), APTI (Slimane et al., 2009), and Muharaf (Saeed et al., 2024) emphasize word/line recognition over document structure analysis. Arabic table recognition faces challenges from merged cells and RTL formatting (Pantke et al., 2014). While methods like GTE (Zheng et al., 2021), GFTE (Li et al., 2021), CascadeTabNet (Prasad et al., 2020), TableNet (Paliwal et al., 2019), and TableFormer (Nassar et al., 2022) advance Latin table detection, their effectiveness on Arabic documents remains unproven. Document conversion pipelines (CCS (Staar et al., 2018), Tesseract (Smith, 2007), Docling (Auer et al., 2024), Surya (Paruchuri, 2024b), Marker (Paruchuri, 2024a), MinerU (Wang et al., 2024a), PaddleOCR (Du et al., 2020)) lack Arabic-specific optimizations for segmentation and diacritic handling (Mahmoud et al., 2018; Kiessling et al., 2019). This highlights the critical need for comprehensive Arabic OCR benchmarks addressing text recognition, table detection, and layout parsing."
        },
        {
            "title": "3 KITAB-Bench",
            "content": "Our methodology offers novel approach to benchmarking Arabic OCR systems via comprehensive data collection strategy and systematic evaluation framework. We gather curated samples from existing Arabic document datasets, manually collected and annotated PDFs, and employ five-phase LLMassisted human-in-the-loop pipeline (Figure 4) to generate diverse supplementary content. Our evaluation framework spans nine specialized tasks, enabling thorough assessment of OCR performance across various document processing challenges and providing robust benchmark for Arabic document understanding tasks."
        },
        {
            "title": "3.1 PDF Data Collection",
            "content": "We curated 33 diverse PDFs from online sources in academia, medicine, law, and literature. To ensure challenging cases, we selected documents featuring richly formatted tables with extensive color usage, merged cells, Arabic numerals, historical texts, watermarks, and handwritten annotations. Each PDF averaged three pages, and we then manually annotated them. This dataset comprehensively captures real-world complexities, making it valuable benchmark for PDF-to-Markdown conversion. Figure 4: Synthetic Data Generation Pipeline: 5-stage process using LLMs to generate topics, create raw data, produce visualization code, render charts, and perform human evaluation for quality control."
        },
        {
            "title": "3.3 Dataset Statistics",
            "content": "To generate data for charts, diagrams and tables, we implemented five-phase LLM-assisted generation pipeline with human validation at critical stages, as illustrated in Figure 4. In Phase (Topic Generation), our system employs an LLM to generate diverse topic names across multiple domains. This phase incorporates various personas (academic, legal, medical, technical) to ensure broad coverage of document types. Phase II (Data Generation) transforms the validated topics into structured raw data. The LLM generates content following Arabic linguistic and formatting conventions across various domains. In Phase III (Code Generation), the system converts the validated raw data into plotting code, with special attention to Arabic text rendering requirements and RTL content management. Phase IV (Image Rendering) utilizes specialized rendering engines (Mermaid, Plotly, Vegalite, HTML) to create visual representations while maintaining Arabic text integrity. The final phase (Human Evaluation) implements rigorous quality control through expert validation. Evaluators filter charts, tables and diagrams based on detected anomalies and ensure adherence to Arabic-specific document conventions. This phase is crucial for maintaining the high quality of our benchmark dataset. Our benchmark dataset comprises over 8,809 samples across 9 major domains and 36 sub-domains, representing comprehensive collection of Arabic document types for OCR evaluation. As detailed in Table 8, the dataset combines carefully curated samples from established datasets, manually annotation PDFs, and synthetically generated content created through our LLM-assisted pipeline (Figure 4). The Image-to-Text portion (3,760 samples) includes data from historical documents (HistoryAr (Pantke et al., 2014)), handwritten text collections (Khatt (Mahmoud et al., 2014), ADAB (Boubaker et al., 2021), Muharaf (Saeed et al., 2024)), and scene text (EvAREST (Hassan et al., 2021)), while layout detection comprises 2,100 samples from BCE-Arabic-v1 (Saad et al., 2016) and DocLayNet (Pfitzmann et al., 2022). For layout analysis, we incorporated 1,700 samples from BCE-Arabic-v1 dataset (Saad et al., 2016), 400 samples from DocLayNet dataset (Pfitzmann et al., 2022) focusing on financial, academic, legal, and patent documents. The line detection and recognition tasks contains 378 samples each from self-developed dataset. We further enriched the dataset with 500 samples from PATS-A01 (ElMuhtaseb, 2010) benchmark to ensure diverse representation. For handwritten text recognition, we assembled comprehensive collection of 1,000 Task Metric Surya Tesseract EasyOCR Detection 79.67 mAP@50 mAP@0.5:0.95 27.40 Recognition WER CER 1.01 0.87 46.39 14. 1.00 0.66 68.02 32.74 0.53 0.20 Table 3: Performance of different models on Line Detection and Line Recognition Task on our Benchmark samples combining datasets from Khatt (Mahmoud et al., 2014) (both paragraph and line-level annotations), Adab (Boubaker et al., 2021), Muharaf (Saeed et al., 2024), and OnlineKhatt (Mahmoud et al., 2018). The benchmark also includes specialized content from ISI-PPT (Wu and Natarajan, 2017) (500 samples), and Hindawi (Elfilali, 2023) (200 samples) for various document types. Scene text understanding is supported by 800 samples from EvArest (Hassan et al., 2021), providing realworld context diversity. detailed table showing all the dataset is provided in the Appendix A. significant portion of our dataset consists of synthetically generated content, including 576 samples for Charts-to-DataFrame (spanning 16 different chart types), 422 samples for Diagram-to-Code (covering sequence diagrams, flowcharts, and tree maps), 456 samples for Tables-to-CSV/HTML, and 902 samples for VQA tasks. These synthetic samples were generated through our five-phase LLMassisted human-in-the-loop pipeline (Figure 4). Every sample in our dataset - whether from existing sources or newly generated - underwent validation by native Arabic speakers before inclusion in the final benchmark. This rigorous validation, reinforced by expert review and automated checks, ensures high quality and authenticity across all domains. detailed analysis is in Appendix C."
        },
        {
            "title": "4 Experiments",
            "content": "Our experimental evaluation comprehensively assesses the capabilities of current OCR systems and state-of-the-art vision-language models (VLMs) across different Arabic and multilingual document understanding tasks. Figure 2 illustrates the nine distinct tasks in our evaluation framework. We evaluate three categories of systems: VLMs, traditional OCR systems, and specialized document processing tools. For VLMs, we include both closed-source models like gpt-4o-2024-08-06, gpt-4o-mini-2024-07-18 (Hurst et al., 2024; Achiam et al., 2023), and gemini-2.0-flash (Georgiev et al., 2024; Google DeepMind, as well as open-source alternatives 2025), such as Qwen2-VL-7B (Wang et al., 2024b), Qwen2.5-VL-7B (Team, 2025), and the AIN-7B (Heakl et al., 2025). Traditional OCR approaches in our evaluation include Surya (Paruchuri, 2024b), Tesseract (Smith, 2007), EasyOCR (JaidedAI, 2020), and PaddleOCR (Li et al., 2022; Du et al., 2021). For specialized document processing tasks, we employ systems like Docling (Auer et al., 2024), and Marker (Paruchuri, 2024a). Layout detection capabilities are evaluated using methods implemented in Surya-layout (Paruchuri, 2024b), Yolo-doclaynet (Zhao et al., 2024) from MinerU (Wang et al., 2024a), and RT-DETR (Zhao et al., 2023) based method in Docling (Auer et al., 2024)."
        },
        {
            "title": "4.1 Evaluation Frameworks and Metrics",
            "content": "Our evaluation framework comprises nine specialized tasks designed to assess different aspects of Arabic OCR systems, as demonstrated in Figure 2. Each task addresses specific challenges in Arabic document processing. For this reason, we employ task-specific metrics to evaluate different aspects of document understanding. PDF-to-Markdown: It evaluates the conversion of Arabic PDFs to structured markdown while preserving the text and table structure. Since both table and text structure are important, for evaluating PDF to Markdown conversion quality, we propose MARS (Markdown Recognition Score), which combines chrF (Popovic, 2015) with TreeEdit-Distance-based Similarity (TEDS) (Zhong et al., 2020) : MARS = Œ± chrF3 + (1 Œ±) TEDS(Ta, Tb) (1) where Œ± (0 Œ± 1) is the weight. Ta represent"
        },
        {
            "title": "Metric",
            "content": "Surya Yolo-doc-"
        },
        {
            "title": "DocLayNet",
            "content": "mAP@0.5 0.506 mAP@0.5:0.95 0.381 0.751 Precision 0.593 Recall 0.635 F1 Score 0.675 mAP@0.5 mAP@0.5:0.95 0.469 0.782 Precision 0.856 Recall 0.799 F1 Score laynet 0.470 0.369 0.608 0.592 0.585 0.404 0.335 0.527 0.503 0.499 Detr (docling) 0.750 0.566 0.626 0.725 0.654 0.758 0.541 0.635 0.770 0.670 Table 4: Performance comparison of layout detection models using different evaluation metrics predicted table structure and Tb the ground truth structure. Table Recognition: We evaluate table extraction using both HTML and CSV formats, where HTML format (evaluated using TEDS (Zhong et al., 2020)) preserves rich structural information including cell spans and hierarchical relationships crucial for complex Arabic tables, while CSV format (evaluated using Jaccard Index 2) focuses on raw data extraction optimized for machine processing and data analysis pipelines. This dual-format evaluation ensures systems can both maintain complex table structures for human readability and provide clean, structured data for automated processing, specifically important for RAG based systems. J(P, G) = G G = G + P (2) where G represents the number of exact matching cells between predicted and ground truth tables, and G represents the total number of unique cells across both tables. Chart-to-Dataframe: This task evaluates extracting structured data from Arabic charts into machine-readable dataframes. Systems must accurately parse numerical values, text labels, and preserve data relationships across chart types (bar, line, pie). We use the Structuring Chart-oriented Representation Metric (SCRM) (Xia et al., 2024)which combines type recognition, topic understanding, and structural numerical fidelity (see Appendix D)and also propose our own CharTeX (Chart Extraction Score) metric. CharTeX combines the chrF scores for chart type and topic with the jaccord index for the dataframe, using fuzzy matching (80% threshold) when columns do not exactly align. Metric = Œ±Jtype +Œ≤Jtopic +(1Œ±Œ≤)Jdata (3) Here, Jtype and Jtopic denote the chrF scores between the predicted and ground-truth chart type and topic, while Jdata measures the structural similarity of the predicted and ground-truth JSON data. Diagram-to-JSON: This task evaluates the conversion of Arabic flowcharts and technical diagrams into JSON while preserving semantic relationships and technical specifications. We propose CODM (Code-Oriented Diagram Metric), extending SCRM (Xia et al., 2024), with the same fomulation as in Eq 3. More detail about this metric is provided in Appendix E. Image-to-Text: This task assess the basic text recognition capabilities across different Arabic fonts and styles, including the handling of cursive script connections, diacritical marks, and various text orientations. We use we use Character Error Rate (CER) and Word Error Rate (WER). For predicted text sequence ÀÜy and ground truth sequence y, CER is computed as: CER = L(y,ÀÜy) , where L(y, ÀÜy) is the Levenshtein distance between character sequences and is the ground truth length. WER is calculated the same way with words as the unit of error. Visual Question Answering: Tests the ability of models to understand and reason about Arabic document content, we evaluate using standard accuracy for MCQ questions and exact word match. Line Detection: Focuses on the accurate identification and processing of individual text lines in Arabic documents. We evaluate using mean Average Precision (mAP) at different Intersection over Union (IoU) thresholds: mAP@0.5 and mAP@0.5:0.95, which assess the localization accuracy of detected text lines. Layout Detection: Assesses document structure analysis capabilities, including the identification of headers, paragraphs, and complex layout elements in Arabic documents. Performance is measured using mAP@0.5 and mAP@0.5:0.95 for localization accuracy, complemented by Precision, Recall, and F1 scores to evaluate the overall detection quality across different layout components. All metrics are computed on our diverse benchmark dataset, which encompasses various document types and complexity levels in both Arabic and multilingual contexts. Table 10 provides detailed mapping of tasks, metrics, and evaluated systems."
        },
        {
            "title": "4.2 Experimental Setup",
            "content": "We implement our evaluation pipeline with careful consideration of hyperparameters for different metrics. All experiments use NVIDIA A100 GPUs. For VLMs, we use their official implementations or API endpoints. Traditional OCR systems are evaluated using pre-trained models provided by the frameworks. For PDF-to-Markdown evaluation metric MARS 1, we choose Œ± = 0.5 and Œ± = 0.5 and Œ≤ = 0.2 for Diagram-to-JSON evaluation metric CODM. We average the results over multiple runs, with performance comparisons shown in different tables [3, 6, 5, 7, and 4]."
        },
        {
            "title": "Table Extraction",
            "content": "End-to-End PDF"
        },
        {
            "title": "Model Group Models",
            "content": "TEDS (HTML) Jaccard (CSV) CHrF (Text) TEDS (Table) MARS Closed Open GPT-4o GPT-4o-mini Gemini-2.0-Flash Qwen2-VL-7B Qwen2.5-VL-7B AIN-7B 85.76 69.32 83. 57.83 59.31 75.94 66.36 49.50 65.55 40.20 59.58 64.83 69.62 56.59 75.75 40.30 69.21 56.52 60.61 52.69 55. 2.54 11.65 49.32 65.12 54.64 65.65 21.42 40.43 52.92 Framework Tesseract EasyOCR Surya DDocling (Auer et al., 2024) pipeline 59.91D 28.23D 38.64I 49.10D 39.09I 58.38M 50.15M Img2Table (Cattan, 2021) pipeline Marker (Paruchuri, 2024a) pipeline 14.85D 16.04I 23.83D 17.88I 70.42M 44.29M 51.34M 45.44D 51.12D 54.29D 52.68D 57.46D Table 5: Performance comparison of different models for table extraction and end-to-end PDF to markdown conversion tasks on our benchmark. Group Models CHrF CER WER Closed Open Framework GPT-4o GPT-4o-mini Gemini-2.0-Flash Qwen2VL-7B Qwen2.5VL-7B AIN-7B Tesseract EasyOCR Paddle Surya 61.01 47.21 77.95 33.94 49.23 78.33 39.62 45.47 16.73 20. 0.31 0.43 0.13 1.48 1.20 0.20 0.54 0.58 0.79 4.95 0.55 0.71 0.32 1.55 1.41 0.28 0.84 0.89 1.02 5. Table 6: Performance comparison of models for OCR (image to text) tasks on our benchmark. detailed performance comparison among different open-source dataset is available in Appendix B"
        },
        {
            "title": "5 Results and Discussion",
            "content": "In this section, we present comprehensive evaluation of different models across different tasks of our framework. The results provide clear distinction between the performance of closed-source models, open-source models, and framework-based solutions, revealing both their strengths and limitations. We observe very clear performance gap between closed and open-source solutions. While closedsource models like Gemini-2.0-Flash consistently outperform other models almost all the tasks."
        },
        {
            "title": "5.1 Charts, Diagrams, and VQA",
            "content": "Table [7] presents model performance across different chart and diagram understanding tasks, evaluated using SCRM and CharTeX (for charts), and VQA-based accuracy metrics. Among closedsource models, Gemini-2.0 achieves the highest performance on chart understanding metrics, scoring 71.4% on SCRM and 56.28% on CharTeX. The performance gap between Gemini-2.0 and GPT-4o is particularly pronounced in CharTeX evaluation (10.33%) compared to SCRM (2.8%). Open-source models shows significant limitation in complex chart understanding. While their SCRM scores remain competitive, both Qwen variants score below 23% on CharTeX evaluation. The visual questionanswering results reveal an important exception to the general closed-source advantage. AIN achieves 87% on PATDVQA, surpassing Gemini-2.0 by 11.5%. AIN also shows competitive performance on MTVQA (31.50%), which is similar to GPT4o and 4% better than GPT-4o-mini. This shows that open-source models can be competitive with closed-source alternatives."
        },
        {
            "title": "5.2 Layout and Lines: Document Structure",
            "content": "Our evaluation of document structure understanding reveals distinct performance patterns across layout detection and line processing tasks. In layout detection  (Table 4)  , RT-DETR (Zhao et al., 2023) achieves superior overall performance with mAP@0.5 scores of 0.750 and 0.758 on BCE (arabic only) and DocLayNet (english) datset respectively. However, Surya (Paruchuri, 2024b) demonstrates higher precision (0.782 on DocLayNet, 0.751 on BCE), despite lower recall rates. This trade-off suggests that different architectures optimize for different aspects of layout detection. The line processing results  (Table 3)  highlight clear contrast between detection and recognition capabilities. While Surya excels in detection with mAP@0.50 of 79.67%, EasyOCR demonstrates superior recognition performance (WER: 0.53, CER: Group Model Chart Diagram Visual QA SCRM CharTeX CODM MTVQAO ChartsVQAM DiagramsVQAM PATDVQAM Average Closed Open GPT-4o GPT-4o-mini Gemini-2.0-Flash Qwen2-VL-7B Qwen2.5-VL-7B AIN-7B 68.6 67.2 71. 56.6 36.2 66.6 45.95 43.33 56.28 21.59 22.08 34.61 61.6 61.4 71.8 63.0 59.2 66.40 32.00 26.80 35. 19.60 23.00 31.50 77.00 58.00 72.00 59.00 74.00 75.00 85.29 83.33 88.24 82.35 79.41 85.29 82.50 80.00 75. 77.50 74.50 87.00 69.19 62.03 67.68 59.61 62.72 69.69 Table 7: Model Performance on Chart Understanding, Diagram Parsing, and Visual Question Answering Tasks. For VQA tasks, denotes open-ended question type from MTVQA (Tang et al., 2024) dataset and denotes MCQ type questions. 0.20). This inverse relationship between detection and recognition performance across models indicates fundamental challenge in optimizing both capabilities simultaneously. Notably, Tesseract shows consistent but lower performance across both metrics, suggesting that newer architectures have made significant improvements over traditional approaches. We also observe that no single model excels at both detection and recognition, which requires for hybrid solutions."
        },
        {
            "title": "5.3 Tables, OCR, and PDF-to-Markdown",
            "content": "Across table extraction tasks  (Table 5)  , closedsource models maintain clear advantage, with GPT-4o achieving 85.76% TEDS and 66.36% Jaccard scores. Among open-source models, AIN (75.94% TEDS) significantly outperforms Qwen variants, while specialized frameworks like Surya achieve competitive results (70.42% Jaccard) through targeted pipelines. In OCR evaluation  (Table 6)  , Gemini-2.0-Flash leads with the lowest error rates (CER: 0.13, WER: 0.32). Notably, AIN matches this performance level (WER: 0.28), while traditional OCR frameworks like EasyOCR and Tesseract show moderate performance (CER: 0.58, 0.54). The significant performance drop in Paddle (CER: 0.79) and Surya (CER: 4.95) highlights the challenges in developing robust OCR systems. End-to-end document processing  (Table 5)  reveals the largest gaps between approaches. Closedsource models maintain consistent performance (GPT-4o: 65.12% MARS, Gemini-2.0: 65.65% MARS), while open-source models show substantial degradation (Qwen2-VL-7B: 21.42% MARS). Framework approaches achieve better stability, with Tesseract and EasyOCR scoring above 50% MARS, suggesting that specialized pipelines can partially bridge the gap with larger models in complete document processing tasks. Our comprehensive evaluation demonstrates that while closed-source models maintain superior performance over open-source models across most tasks, specialized frameworks like Surya, RTDETR Layout, and EasyOCR achieve competitive performance in targeted scenarios like table extraction, layout detection, and text recognition respectively. However, this framework advantage significantly diminishes in end-to-end pdf-to-markdown tasks where the integration capabilities of large models prove crucial, as evidenced by the performance gaps between commercial VLMs and traditional systems like EasyOCR, Surya and Tesseract in End-to-End PDF task  (Table 5)  ."
        },
        {
            "title": "6 Conclusion",
            "content": "We introduce comprehensive benchmark for Arabic OCR that fills the gap in standardized evaluation frameworks for Arabic document processing. Our dataset of 8,809 samples across nine major domains is the most diverse collection assembled for OCR evaluation, incorporating handwritten, scanned, synthetic, and scene text, as well as complex tables, charts, and end-to-end pdf-tomarkdown. This framework extends beyond simple text recognition to include structural document analysis and enables systematic assessment of OCR performance across various fonts, styles, and layouts."
        },
        {
            "title": "7 Limitations and Future Directions",
            "content": "Despite its contributions, this benchmark has limitations. While it covers diverse Arabic document types, it lacks full representation of historical manuscripts and low-resource dialects. Future work should expand to include these, along with scanned records from government, academic, and financial institutions. Another key limitation is in table and chart recognition, where OCR models struggle with structure preservation, header detection, and merged cell parsing. Though our benchmark introduces challenges in these areas, further refinements are needed for robust multimodal OCR capable of jointly processing text, tables, and figures. Future advancements should focus on dataset expansion, novel evaluation metrics, deep learning refinements, and cross-lingual OCR innovations to enhance Arabic VLMs."
        },
        {
            "title": "References",
            "content": "Ahmed Abdelali, Hamdy Mubarak, Shammur Absar Chowdhury, Maram Hasanain, Basel Mousi, Sabri Boughorbel, Yassine El Kheir, Daniel Izham, Fahim Dalvi, Majd Hawasly, et al. 2023. Larabench: Benchmarking arabic ai with large language models. arXiv preprint arXiv:2305.14982. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Fakhraddin Alwajih, El Moatez Billah Nagoudi, Gagan Bhatia, Abdelrahman Mohamed, and Muhammad Abdul-Mageed. 2024. Peacock: family of arabic multimodal large language models and benchmarks. arXiv preprint arXiv:2403.01031. Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Nikolaos Livathinos, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Fabian Lindlbauer, Kasper Dinkla, et al. 2024. Docling technical report. arXiv preprint arXiv:2408.09869. Gagan Bhatia, El Moatez Billah Nagoudi, Fakhraddin Alwajih, and Muhammad Abdul-Mageed. 2024. Qalam: multimodal llm for arabic optical character and handwriting recognition. arXiv preprint arXiv:2407.13559. Houda Bouamor, Nizar Habash, Mohammad Salameh, Wajdi Zaghouani, Owen Rambow, Dana Abdulrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani, Alexander Erdmann, et al. 2018. The madar arabic dialect corpus and lexicon. In Proceedings of the eleventh international conference on language resources and evaluation (LREC 2018). Houcine Boubaker, Abdelkarim Elbaati, Najiba Tagougui, Haikal El Abed, Monji Kherallah, Volker M√§rgner, and Adel M. Alimi. 2021. Adab database. Hassina Bouressace and Janos Csirik. 2019. Printed arabic text database for automatic recognition systems. In Proceedings of the 2019 5th International Conference on Computer and Technology Applications, pages 107 111. Xavier Cattan. 2021. images and scanned pdfs. xavctn/img2table. Accessed: 2025-02-14. img2table: Extract tables from https://github.com/ H. Cheng, P. Zhang, S. Wu, et al. 2023. M6doc: large-scale multi-format, multi-type, multi-layout, multi-language, multi-annotation category dataset for modern document layout analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. 2024. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146. Yuning Du, Chenxia Li, Ruoyu Guo, Cheng Cui, Weiwei Liu, Jun Zhou, Bin Lu, Yehua Yang, Qiwen Liu, Xiaoguang Hu, et al. 2021. Pp-ocrv2: Bag of tricks for ultra lightweight ocr system. arXiv preprint arXiv:2109.03144. Yuning Du, Chenxia Li, Ruoyu Guo, Xiaoting Yin, Weiwei Liu, Jun Zhou, Yifan Bai, Zilin Yu, Yehua Yang, Qingqing Dang, et al. 2020. Pp-ocr: pracarXiv preprint tical ultra lightweight ocr system. arXiv:2009.09941. Husni A. El-Muhtaseb. 2010. Pats-a01 - an arabic text database. https://faculty.kfupm.edu.sa/ ics/muhtaseb/ArabicOCR/PATS-A01.htm. Database for Arabic Text Recognition Research. Ali Elfilali. 2023. https://huggingface.co/datasets/Ali-C137/ Hindawi-Books-dataset. Dataset. Hindawi books dataset. Ling Fu, Biao Yang, Zhebin Kuang, Jiajun Song, Yuzhe Li, Linghao Zhu, Qidi Luo, Xinyu Wang, Hao Lu, Mingxin Huang, et al. 2024. Ocrbench v2: An improved benchmark for evaluating large multimodal models on visual text localization and reasoning. arXiv preprint arXiv:2501.00321. Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Sara Ghaboura, Ahmed Heakl, Omkar Thawakar, Ali Alharthi, Ines Riahi, Abduljalil Saif, Jorma Laaksonen, Fahad Khan, Salman Khan, and Rao Anwer. 2024. Camel-bench: comprehensive arabic lmm benchmark. arXiv preprint arXiv:2410.18976. Google DeepMind. 2025. Gemini Model Updates - February 2025. Accessed: 2025-02-14. Heba Hassan, Ahmed El-Mahdy, and Mohamed Hussein. 2021. Arabic scene text recognition in the deep learning era: Analysis on novel dataset. IEEE Access, 9:107046107058. Ahmed Heakl, Sara Ghaboura, Omkar Thawkar, Fahad Shahbaz Khan, Hisham Cholakkal, Rao Muhammad Anwer, and Salman Khan. 2025. Ain: The arabic inclusive large multimodal model. arXiv preprint arXiv:2502.00094. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. JaidedAI. 2020. Easyocr: Ready-to-use optical character recognition with multi-language support. https: //github.com/JaidedAI/EasyOCR. Accessed: 202502-14. Benjamin Kiessling, Daniel St√∂kl Ben Ezra, and Matthew Thomas Miller. 2019. Badam: public dataset for baseline detection in arabic-script manuscripts. In Proceedings of the 5th International Workshop on Historical Document Imaging and Processing, page 1318, New York, NY, USA. Association for Computing Machinery. Chenxia Li, Weiwei Liu, Ruoyu Guo, Xiaoting Yin, Kaitao Jiang, Yongkun Du, Yuning Du, Lingfeng Zhu, Baohua Lai, Xiaoguang Hu, et al. 2022. Pp-ocrv3: More attempts for the improvement of ultra lightweight ocr system. arXiv preprint arXiv:2206.03001. Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, Ming Zhou, and Zhoujun Li. 2019. Tablebank: benchmark dataset for table detection and recognition. arXiv preprint arXiv:1903.01949. Minghao Li, Yiheng Xu, Leyang Cui, Shaohan Huang, Furu Wei, and Zhoujun Li. 2020. Docbank: bencharXiv mark dataset for document layout analysis. preprint arXiv:2006.01038. Yiren Li, Zheng Huang, Junchi Yan, Yi Zhou, Fan Ye, and Xianhui Liu. 2021. Gfte: graph-based financial table extraction. In Pattern Recognition. ICPR International Workshops and Challenges: Virtual Event, January 1015, 2021, Proceedings, Part II, pages 644658. Springer. Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed Nassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter Staar. 2021. Robust pdf document conversion using recurrent neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 1513715145. Sabri Mahmoud, Irfan Ahmad, Wasfi Al-Khatib, Mohammad Alshayeb, Mohammad Tanvir Parvez, Volker M√§rgner, and Gernot Fink. 2014. Khatt: An open arabic offline handwritten text database. Pattern Recognition, 47(3):10961112. Sabri Mahmoud, Hamzah Luqman, Baligh AlHelali, Galal BinMakhashen, and Mohammad Tanvir Parvez. 2018. Online-khatt: an open-vocabulary database for arabic online-text processing. The Open Cybernetics & Systemics Journal, 12(1). Hamdy Mubarak, Hend Al-Khalifa, and AbdulMohsen Al-Thubaity. 2022. Overview of osact5 shared task on arabic offensive language and hate speech detection. In Proceedinsg of the 5th Workshop on Open-Source Arabic Corpora and Processing Tools with Shared Tasks on Quran QA and Fine-Grained Hate Speech Detection, pages 162166. Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, and Peter Staar. 2022. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 46144623. Shubham Singh Paliwal, Vishwanath, Rohit Rahul, Monika Sharma, and Lovekesh Vig. 2019. Tablenet: Deep learning model for end-to-end table detection and tabular data extraction from scanned document images. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 128133. IEEE. Werner Pantke, Martin Dennhardt, Daniel Fecker, Volker M√§rgner, and Tim Fingscheidt. 2014. An historical handwritten arabic dataset for segmentation-free word spotting-hadara80p. In 2014 14th International Conference on Frontiers in Handwriting Recognition, pages 1520. IEEE. Vik Paruchuri. 2024a. Marker: Convert pdf to markdown and other formats. https://github.com/ VikParuchuri/marker. Vik Paruchuri. 2024b. Surya: Accurate line-by-line text detection and recognition in complex documents. https://github.com/VikParuchuri/surya. Mario Pechwitz, Snoussi Maddouri, Volker M√§rgner, Noureddine Ellouze, Hamid Amiri, et al. 2002. Ifn/enitIn Proc. of database of handwritten arabic words. CIFED, volume 2, pages 127136. Citeseer. Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed Nassar, and Peter Staar. 2022. Doclaynet: large human-annotated dataset for document-layout analysis. arXiv preprint arXiv:2206.01062. Maja Popovic. 2015. chrf: character n-gram f-score for automatic mt evaluation. In Proceedings of the tenth workshop on statistical machine translation, pages 392 395. Devashish Prasad, Ayan Gadpal, Kshitij Kapadni, Manish Visave, and Kavita Sultanpure. 2020. Cascadetabnet: An approach for end to end table detection and structure recognition from image-based documents. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 572573. Rana SM Saad, Randa Elanwar, NS Abdel Kader, Samia Mashali, and Margrit Betke. 2016. Bce-arabic-v1 dataset: Towards interpreting arabic document images for people with visual impairments. In Proceedings of the 9th ACM International Conference on PErvasive Technologies Related to Assistive Environments, pages 18. M. Saeed, A. Chan, A. Mijar, and J. Moukarzel. 2024. Muharaf: Manuscripts of handwritten arabic dataset for cursive text recognition. arXiv preprint arXiv:2406.09630. Sebastian Schreiber, Stefan Agne, Ivo Wolf, Andreas Dengel, and Sheraz Ahmed. 2017. Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), volume 1, pages 11621167. IEEE. Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin Charles Germain Lee, Jacob Carlson, and Weining Li. 2021. Layoutparser: unified toolkit for deep learning based document image analysis. In Document Analysis and RecognitionICDAR 2021: 16th International Conference, Lausanne, Switzerland, September 510, 2021, Proceedings, Part 16, pages 131146. Springer. Fouad Slimane, Rolf Ingold, Slim Kanoun, Adel Alimi, and Jean Hennebert. 2009. new arabic printed text image database and evaluation protocols. In 2009 10th international conference on document analysis and recognition, pages 946950. IEEE. Ray Smith. 2007. An overview of the tesseract ocr In Ninth international conference on docuengine. ment analysis and recognition (ICDAR 2007), volume 2, pages 629633. IEEE. Peter WJ Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. 2018. Corpus conversion service: machine learning platform to ingest documents at scale. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 774782. Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, Yanjie Wang, Yuliang Liu, Hao Liu, Xiang Bai, and Can Huang. 2024. Mtvqa: Benchmarking multilingual text-centric visual question answering. Preprint, arXiv:2405.11985. Qwen Team. 2025. Qwen2.5-vl. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. 2024a. Mineru: An open-source solution for precise document content extraction. arXiv preprint arXiv:2409.18839. Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. 2024. General ocr theory: Towards ocr-2.0 via unified end-to-end model. arXiv preprint arXiv:2409.01704. Yue Wu and Prem Natarajan. 2017. Self-organized text detection with minimal post-processing via border In International Conference on Computer learning. Vision. Renqiu Xia, Bo Zhang, Haoyang Peng, Hancheng Ye, Xiangchao Yan, Peng Ye, Botian Shi, Yu Qiao, and Junchi Yan. 2023. Structchart: Perception, structuring, reasoning for visual chart understanding. arXiv preprint arXiv:2309.11268. Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, et al. 2024. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185. Taha Zerrouki and Amar Balla. 2017. Tashkeela: Novel corpus of arabic vocalized texts, data for autodiacritization systems. Data in brief, 11:147. Zhao, Lv, Xu, Wei, Wang, Dang, Liu, and Chen. 2023. Detrs beat yolos on realtime object detection. arxiv e-prints. arXiv preprint arXiv:2304.08069. Z. Zhao, H. Kang, B. Wang, and C. He. 2024. Doclayout-yolo: Enhancing document layout analysis through diverse synthetic data and global-to-local adaptive perception. arXiv preprint arXiv:2410.12628. Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. 2021. Global table extractor (gte): framework for joint table identification and cell structure recognition using visual context. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 697706. Zhong, ShafieiBavani, and Jimeno-Yepes. 2019a. Image-based table recognition: data, model, and evaluation. corr abs/1911.10683. arXiv preprint arXiv:1911.10683. Xu Zhong, Elaheh ShafieiBavani, and Antonio Jimeno Yepes. 2020. Image-based table recognition: data, model, and evaluation. In European conference on computer vision, pages 564580. Springer. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024b. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191. Xu Zhong, Jianbin Tang, and Antonio Jimeno Yepes. 2019b. Publaynet: largest dataset ever for document layout analysis. In 2019 International conference on document analysis and recognition (ICDAR), pages 1015 1022. IEEE. Maurice Weber, Carlo Siebenschuh, Rory Butler, Anton Alexandrov, Valdemar Thanner, Georgios Tsolakis, Haris Jabbar, Ian Foster, Bo Li, Rick Stevens, et al. 2023. Wordscape: pipeline to extract multilingual, visually rich documents with layout annotations from web crawl data. Advances in Neural Information Processing Systems, 36:2604826068."
        },
        {
            "title": "Collection",
            "content": "Our benchmark integrates diverse data sources to ensure comprehensive coverage of Arabic document types. As detailed in Table 2, the dataset Document Types: 36 sub-domains including financial reports and technical manuals Structural Complexity: 43% of tables contain merged cells; 29% of charts use dual-axis configurations"
        },
        {
            "title": "D Tasks Models and Metrics",
            "content": "Table 10 maps evaluation tasks to corresponding models and metrics. The framework evaluates nine core capabilities: Structural Understanding: Layout detection (mAP), line detection (IoU) Content Extraction: Text recognition (CER), table parsing (TEDS) Semantic Reasoning: VQA accuracy, chartto-dataframe conversion (SCRM) Specialized metrics like MARS ( Œ±=0.5) address the dual requirements of text fidelity and structural preservation in PDF-to-Markdown conversion."
        },
        {
            "title": "E SCRM and CODM",
            "content": "The Structuring Chart-oriented Representation Metric (SCRM) evaluates chart understanding through three components: SCRM = 0.4Jtype + 0.3Jtopic + 0.3Jdata (4) where Jtype, and Jtopic are chrF scores, and Jdata measures JSON structural similarity. The Code-Oriented Diagram Metric (CODM) extends SCRM for flowcharts and technical diagrams: CODM = 0.5Jtopology + 0.5Jsemantics (5) assessing both node-edge relationships and semantic labels. As shown in Figure 5 and 6, domainspecific prompts guided model responses for metric calculation. For instance, sequence diagrams required strict adherence to Arabic UML notation standards during evaluation. combines manually curated samples, synthetic data generated through our LLM-assisted pipeline (Figure 4), and existing publicly available datasets. Key sources include: Handwritten Text: KHATT (paragraph and line-level annotations), ADAB, Muharaf, and OnlineKhatt. Historical Documents: HistoryAr and HistoricalBooks. Scene Text: EvAREST for real-world context diversity. Layout Analysis: BCE-Arabic-v1 and DocLayNet. Synthetic Content: 576 chart samples (16 types) and 422 diagram samples generated via our five-phase pipeline (Section 3.2). The dataset emphasizes domain diversity, covering academic, medical, legal, financial, and technical documents. All samples underwent rigorous validation by native Arabic speakers to ensure linguistic and structural accuracy."
        },
        {
            "title": "B Detailed Performance Comparison",
            "content": "Table 9 provides granular performance metrics for VLMs and OCR frameworks across 12 Arabic text recognition datasets. Gemini-2.0-Flash demonstrates exceptional robustness on synthetic datasets (CER: 0.01 on PATS), while AIN-7B excels in historical manuscript recognition (CER: 0.26 on HistoryAr). Traditional OCR systems like Tesseract show limitations in handwritten text (CER: 1.26 on HistoryAr), highlighting the need for script-specific optimizations."
        },
        {
            "title": "C Data Analysis",
            "content": "Our data generation pipeline (Figure 4) enabled the creation of 1,502 synthetic samples (576 charts, 422 diagrams, 456 tables). The pipelines human validation phase rejected 18% of initial outputs due to RTL formatting errors or semantic inconsistencies. As shown in Figure 5 and 6, domainspecific prompts ensured adherence to Arabic linguistic conventions during LLM-assisted generation. The final dataset exhibits balanced representation across: Font Styles: 21 Arabic calligraphic styles"
        },
        {
            "title": "Domain\nPDF to Markdown\nLayout Detection",
            "content": "Sub-Domain General Docs Line Detection Line Recognition Table Recognition Image to Text"
        },
        {
            "title": "Total Dataset Size",
            "content": "Docs Docs Financial Synthetic Historical Hand. Paragraph Hand. Word Hand. Line"
        },
        {
            "title": "Scene\nBar\nLine\nPie\nBox\nViolin\nArea\nSunBurst\nDot\nDual Axis\nDensity Curve\nBubble\nGrouped Bar\nStacked Bar\nHistogram\nHeatMap\nScatter\nSequence\nFunnel\nClass\nNetwork\nVenn\nFlowChart\nTreeMap\nDiagrams\nCharts\nNews Letter\nScene",
            "content": "Dataset Source Manual BCE-Arabic-v1 (Saad et al., 2016) DocLayNet (Pfitzmann et al., 2022) Manual Manual Pixmo (Deitke et al., 2024) PATS (El-Muhtaseb, 2010) SythenAR HistoryAr (Pantke et al., 2014) HistoricalBooks Khatt (Mahmoud et al., 2014) ADAB (Boubaker et al., 2021) Muharaf (Saeed et al., 2024) OnlineKhatt (Mahmoud et al., 2018) Khatt (Mahmoud et al., 2014) ISI-PPT (Wu and Natarajan, 2017) ArabicOCR Hindawi (Elfilali, 2023) EvAREST (Hassan et al., 2021) Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Synthetic Manual Manual PATD (Bouressace and Csirik, 2019) MTVQA Original 33 1.9k 80k 375 375 490 21.6k 39.1k 1.5k 40 2.72k 15k 24.5k 8.5k 13.4k 86.5k 20.3k 79k 5.59k 100 100 100 100 100 50 30 30 20 10 20 50 50 100 10 100 50 20 20 20 20 100 100 102 105 2.42k 818 Selected 33 1,700 400 378 378 456 500 500 200 10 200 200 200 200 200 500 50 200 800 61 43 56 31 36 29 15 15 26 5 13 60 82 70 11 23 46 52 30 18 7 112 157 102 100 200 500 Total 33 2,100 378 378 3,760 576 226 902 8,809 Table 8: Dataset Distribution Across Different Domains, sub-domains and Data Source"
        },
        {
            "title": "Size",
            "content": "GPT-4o GPT-4o-mini Gemini-2.0-Flash CER WER CER WER CER WER Qwen2-VL CER WER PATS SythenAR HistoryAr HistoricalBooks Khatt Adab Muharaf OnlineKhatt ISI-PPT ArabicOCR Hindawi EvArest 500 500 200 10 200 200 200 200 500 50 200 800 0.23 0.09 0.51 0.41 0.45 0.30 0.56 0.29 0.08 0.06 0.34 0.20 3,760 0.31 0.30 0.20 0.82 0.76 0.74 0.73 0.90 0.63 0.18 0.26 0.56 0. 0.55 0.53 0.14 0.67 0.59 0.64 0.35 0.63 0.41 0.15 0.16 0.48 0.25 0.43 0.71 0.32 0.96 0.88 0.91 0.83 0.94 0.76 0.31 0.46 0.71 0.51 0.71 0.01 0.07 0.28 0.05 0.19 0.19 0.33 0.17 0.06 0.00 0.01 0. 0.13 0.02 0.17 0.64 0.22 0.45 0.56 0.69 0.44 0.15 0.02 0.04 0.36 0.32 1.02 0.59 3.46 1.90 1.12 0.63 3.57 1.30 1.03 1.25 1.82 0.41 1.48 1.02 1.13 2.86 2.16 5.04 1.08 2.87 2.01 1.06 1.50 2.05 0. 1."
        },
        {
            "title": "Size",
            "content": "Qwen2.5-VL CER WER CER WER"
        },
        {
            "title": "Surya\nCER WER",
            "content": "PATS SythenAR HistoryAr HistoricalBooks Khatt Adab Muharaf OnlineKhatt ISI-PPT ArabicOCR Hindawi EvArest 500 500 200 10 200 200 200 200 500 50 200 800 0.26 0.21 0.47 0.33 0.07 0.00 0.61 0.36 0.36 1.00 1.00 0.19 3,760 0.28 0.36 0.40 0.83 0.72 0.22 0.01 0.96 0.70 0.54 1.00 1.00 0. 0.54 0.00 0.04 0.26 0.84 0.61 1.00 0.38 0.03 0.52 0.01 0.11 0.30 0.20 0.00 0.16 0.54 0.88 1.12 1.00 0.54 0.12 0.53 0.01 0.15 0.32 0.58 0.14 0.31 0.72 0.74 0.67 1.00 0.77 0.59 0.31 0.01 0.31 0. 0.89 0.28 0.72 1.26 0.99 1.06 1.14 1.22 1.20 0.64 0.01 0.72 1.02 0.79 4.66 4.82 10.32 6.81 4.25 7.28 6.19 6.71 4.25 2.75 0.15 5.91 4.67 7.90 12.78 6.30 3.77 8.71 7.48 6.95 3.77 3.58 0.20 3.86 4. 5.61 Table 9: Performance comparison of Large Vision-Language Models on KITAB-Bench (lower is better). Task Metrics Document Understanding Tasks PDF to Markdown chrF + TEDS Layout Detection Line Detection mAP@0.5 mAP@0.5:0.95 Precision Recall F1 mAP@0.5 mAP@0.5:0.95 Line Recognition WER, CER Table Understanding Tasks Tables Recognition (HTML) TEDS (Zhong et al., 2019a) Tables Recognition (CSV) Jaccard Index Visual Understanding Tasks Image to Text CER, WER chrF, BLEU METEOR Charts DataFrame to SCRM (Xia et al., 2024, 2023) Diagram to Json SCRM VQA Accuracy + Word Match Score Open LLMs Closed LLMs OCR Systems Qwen2-VL Qwen2.5-VL AIN PaliGemma Qwen2-VL Qwen2.5-VL AIN PaliGemma Qwen2-VL Qwen2.5-VL AIN-7B PaliGemma Qwen2-VL Qwen2.5-VL AIN PaliGemma Qwen2-VL Qwen2.5-VL AIN-7B PaliGemma Qwen2-VL Qwen2.5-VL AIN-7b PaliGemma GPT-4o GPT-4o-mini Gemini-2.0-Flash GPT-4o GPT-4o-mini Gemini-2.0-Flash GPT-4o GPT-4o-mini Gemini-2.0-Flash GPT-4o GPT-4o-mini Gemini-2.0-Flash GPT-4o GPT-4o-mini Gemini-2.0-Flash GPT-4o GPT-4o-mini Gemini-2.0-Flash Docling Marker MinerU PDF-Extract-Kit Surya Yolo-doclaynet (MinerU) Detr (docling) Surya Tesseract EasyOCR Surya Tesseract EasyOCR Docling[EasyOCR] Docling[Tesseract] Marker Img2Table[EasyOCR] Img2Table[Tesseract] Docling[EasyOCR] Docling[Tesseract] Marker Img2Table[EasyOCR] Img2Table[Tesseract] Docling[EasyOCR] Docling[Tesseract] Marker Img2Table[EasyOCR] Img2Table[Tesseract] Table 10: Comprehensive evaluation metrics and models for document understanding tasks. The table is organized into three main categories: document understanding, table understanding, and visual understanding tasks. Each task is evaluated using specific metrics and implemented across various models and OCR systems. Figure 5: Prompts for Different Task Categories. Figure 6: Prompts for Diagrams and Tables."
        }
    ],
    "affiliations": [
        "Australian National University",
        "Link√∂ping University",
        "MBZUAI",
        "Monta AI"
    ]
}