{
    "paper_title": "FRESA:Feedforward Reconstruction of Personalized Skinned Avatars from Few Images",
    "authors": [
        "Rong Wang",
        "Fabian Prada",
        "Ziyan Wang",
        "Zhongshi Jiang",
        "Chengxiang Yin",
        "Junxuan Li",
        "Shunsuke Saito",
        "Igor Santesteban",
        "Javier Romero",
        "Rohan Joshi",
        "Hongdong Li",
        "Jason Saragih",
        "Yaser Sheikh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a novel method for reconstructing personalized 3D human avatars with realistic animation from only a few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their practical applications. In contrast, we learn a universal prior from over a thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design a 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose a multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse a plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on a large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-the-arts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at https://github.com/rongakowang/FRESA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 7 0 2 9 1 . 3 0 5 2 : r FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images Rong Wang1,2 Junxuan Li2 Fabian Prada2 Shunsuke Saito2 Hongdong Li1 Ziyan Wang2 Igor Santesteban2 Jason Saragih2 Zhongshi Jiang2 Javier Romero2 Yaser Sheikh Chengxiang Yin2 Rohan Joshi2 1Australian National University 2Meta Reality Labs Research Figure 1. FRESA. We present novel method to reconstruct personalized skinned avatars with realistic pose-dependent animation all in feed-forward approach, which generalizes to causally taken phone photos without any fine-tuning. We visualize the predicted skinning weights associated with the most important joints in (b) and colormaps of per-vertex displacement magnitudes1during animation in (c)."
        },
        {
            "title": "Abstract",
            "content": "We present novel method for reconstructing personalized 3D human avatars with realistic animation from only few images. Due to the large variations in body shapes, poses, and cloth types, existing methods mostly require hours of per-subject optimization during inference, which limits their In contrast, we learn universal practical applications. prior from over thousand clothed humans to achieve instant feedforward generation and zero-shot generalization. Specifically, instead of rigging the avatar with shared skinning weights, we jointly infer personalized avatar shape, skinning weights, and pose-dependent deformations, which effectively improves overall geometric fidelity and reduces deformation artifacts. Moreover, to normalize pose variations and resolve coupled ambiguity between canonical shapes and skinning weights, we design 3D canonicalization process to produce pixel-aligned initial conditions, which helps to reconstruct fine-grained geometric details. We then propose multi-frame feature aggregation to robustly reduce artifacts introduced in canonicalization and fuse plausible avatar preserving person-specific identities. Finally, we train the model in an end-to-end framework on large-scale capture dataset, which contains diverse human subjects paired with high-quality 3D scans. Extensive experiments show that our method generates more authentic reconstruction and animation than state-of-thearts, and can be directly generalized to inputs from casually taken phone photos. Project page and code is available at https://github.com/rongakowang/FRESA. 1. Introduction 3D digitization for clothed humans is highly demanded in many vision and graphics applications, such as extended reality (XR), virtual try-on, and telepresence [27]. Since humans vary largely in body shapes and cloth types, the reconstructed avatars need to faithfully preserve these personalized details, and can be realistically animated robust to the underlying topology. To this end, traditional pipelines often require laborious artist designs [14, 54] or controlled environments [16, 39], which limits their use. In this work, we aim to automatically reconstruct animatable avatars from easily accessible sources, e.g. casually taken phone photos, to better facilitate downstream applications. Existing works in 3D clothed human reconstructions [47, 48, 59, 60, 66, 67] have demonstrated impressive quality and generalizability. While they mostly focus on reconstructing static frame for given pose, [17, 20, 31] further extend to produce avatars more suitable for animation. This is achieved by reconstructing the avatar in canonical *Work done while Rong Wang was an intern at Reality Labs Research. 1Scales normalized across all vertices to highlight large deformation. space associated with rigged body template, and then animating it by target joint motions via the linear blend skinning (LBS) [37] method. However, all these works skin the avatar by querying the skinning weights from nearest template body vertices, which inevitably produces deformation artifacts in challenging poses and extreme body shapes. Recently, [12, 49, 56, 62] propose to jointly optimize personalized canonical shapes and skinning weights from posed 3D scans or images. However, due to the lack of unified prior from different body shapes and cloth types [55], they are restricted to per-subject setting and require hours of testtime optimization, which is computationally expensive. In this paper, we present novel method to reconstruct personalized skinned avatars for realistic animation in an efficient feed-forward approach. The key to our method is universal clothed human model, which is learned from large collection of human subjects while capable of jointly inferring personalized canonical avatar shapes, skinning weights, and pose-dependent deformation in animation. Specifically, to normalize pose variations in input images and ensure model generalizability, we first develop 3D canonicalization process to produce pixel-aligned initial conditions for geometry and semantics references, which helps to reconstruct canonical avatars with fine-grained geometric details. While the canonicalization can produce artifacts due to noisy unposing, we propose to robustly reduce the artifacts and generate plausible avatar by aggregating from multi-frame references. In this way, the fused feature can preserve information intrinsic to the person, e.g. body shapes and cloth types, to better represent the person identity. Finally, to resolve the coupled ambiguity between the canonical geometry and skinning weights, i.e. to avoid generating incorrect components that can be accidentally warped to correct posed shape [32], we adopt multistage training process to jointly supervise the model with posed-space ground truths and canonical-space regularization. Leveraging the rich prior from the universal model, we achieve feed-forward reconstruction of avatars and animation without requiring expensive per-subject optimization. Our key contributions can be summarized as follows. (i) We present novel method FRESA, which enables instant feed-forward reconstruction of 3D human avatars with personalized canonical shapes, skinning weights, and posedependent animation via universal clothed human model, while achieving zero-shot generalization to inputs of only few casually taken photos. (ii) We propose novel pipeline with explicit canonicalization and multi-frame aggregation, which robustly improves the fidelity and realism of resulting avatars. (iii) We develop large-scale clothed human dataset with diverse subjects and high-quality 3D scans to facilitate learning an effective universal prior. Extensive experiments show that our method outperforms state-of-thearts in both reconstruction fidelity and animation quality. 2. Related Works 3D Clothed Human Reconstruction. Early works of 3D human reconstruction often utilize parametric human models [34, 42] as prior for the underlying body shapes. For instance, [5, 36] adopt SMPL+D model to express cloth geometry as per-vertex displacements of the SMPL [34] body template, while [3, 26] use UV maps to further improve the mesh resolutions. However, these approaches assume fixed topology and can not model loose clothes with large shape variation to the body template. In contrast, [4, 47, 48, 59, 60] propose to leverage neural implicit fields [38, 40] for continuous surface representation, which can flexibly model diverse body shapes and clothes types. Such representation also supports fast mesh extraction via Marching Cubes [35] to enable explicit geometry supervision and real-time rendering on mobile devices, which are difficult to achieve with other representations such as neural radiance field [18, 57, 68] or 3D primitives [25, 30, 69] as they require more time-demanding rendering pipelines. However, above methods mostly focus on reconstructing static humans for the given poses, while lacking dedicated canonicalization and skinning for reconstructed avatars, thus are not suitable for animation-based applications [56]. Animatable Avatar Reconstruction. ARCH [56] pioneers the work to produce avatars that are more suitable for animation purposes. This is achieved by directly reconstructing avatars in canonical space associated with rigged template. Since it utilizes hand-crafted spatial encoding for geometric features, it tends to produce low-fidelity results with less accuracy and photorealism. ARCH++ [17] addresses this issue by learning geometry features from PointNet++ [45] encoder, but only uses posed body template and thus loses fine-grained details. [31] further refines the pipeline by sampling features from estimated normal images followed by post-processing for geometry refinement. Unlike these feed-forward approaches, [61, 63] rely on pretrained diffusion models [33, 46] to hallucinate canonical avatar shapes from unconstrained photo collections, but require time-consuming per-subject optimization. Despite the improvement in avatar geometry, all these methods simply rig the avatar using skinning weights from nearest vertices of the rigged template for animation, which are only reliable for limited body shapes and cloth types. As consequence, they often generate undesired deformation artifacts in challenging poses and extreme body shapes. Recently, several works [13, 29, 49, 55, 56, 62] propose to jointly optimize personalized canonical shapes and skinning weights with pose-dependent deformation to reduce artifacts and improve animation quality. Specifically, [12, 56] optimize correspondence between the canonical and posed space via an iterative root-finding algorithm [12]. [49] assumes 3D scans as inputs and introduces an optimizable skinning field supervised by cycle consistency loss. HowFigure 2. Method Overview. We propose novel method to feed-forwardly reconstruct personalized skinned avatars via universal clothed human model. Specifically, given frames of posed human images {Ii} from front and back views, we first estimate their normal and segmentation images, and then unpose them for each frame and view to produce pixel-aligned initial conditions in 3D canonicalization process (Section 3.1). Next, we propose to aggregate mult-frame references and produce single bi-plane feature as the representation of the subject identity. By sampling from this feature, we jointly decode personalized canonical avatar mesh M, skinning weights and pose-dependent vertex displacement (Section 3.2) from canonical tetrahedral grid. Finally, we adopt multi-stage training process to train the model with posed-space ground truth and canonical-space regularization (Section 3.3). ever, due to the lack of universal prior for different body shapes and cloth types, these methods are restricted to per-subject fitting approach and require computationally expensive test-time optimization to produce an avatar. [54] attempts to estimate personalized skinning weights and appear deformation on stylized characters, but requires known canonical character topologies. [52] proposes to directly estimate subject-specific skinning maps from input images via dual-encoder-decoder model. However, its canonicalization requires hand-crafted correction and thus is not endto-end differentiable. In contrast to all above methods, we present novel method that can jointly infer personalized geometry, skinning weights, and pose-dependent deformation all in an end-to-end learning framework. 3. Method Ii} { pi} { Problem Definition. Given frames of clothed human images i=1, we i=1 and estimated 3D body poses aim to produce an animatable 3D human avatar with personalized canonical geometry, skinning weights, and posedependent deformation for animation. Specifically, to ensure coverage of sufficient areas for the subject, we assume two images If RHW 3 from front and back views2for each frame i, where and denote the image height and width. In addition, we assume the pose vector and Ib RV 3 and faces RJ3 and person-specific pi consists of joint angles θi RJ , both compatible with pre-defined bone scales rigged body template with joints. For the output, we define the avatar geometry representation as mesh with RF 3 in the canonical vertices space, i.e. aligned with the template. The vertices are associated with the joints as defined by skinning weight matrix RV , which allows the avatar to be animated by the LBS method. When animating the avatar with target pose pt, we further predict pose-dependent vertex displacement RV 3 to correct LBS artifacts and improve animaVt tion quality. The method overview is shown in Figure 2 and we will introduce each module in the following sections. 3.1. 3D Canonicalization Since posed human images can vary largely in body poses, scales and positions relative to the camera, directly extracting pixel features from raw images can be challenging and thus often leads to overly smooth geometry. To tackle this issue, we propose to normalize the variations in an explicit 3D canonicalization process with the following steps. 3D Lifting. We first leverage an off-the-shelf human foundation model [23] to independently estimate normal images 2to ensure feasibility in phone photo setting, we allow each view to be taken at different time and thus do not need to be multi-view consistent. where ˆu, j=1wjTj)[u; 1] , RHW 3 for each frame and view v. Next, we folNv low [60] to lift all normal images and produce front and back surface meshes via normal integration [9]. In addition, we also use [23] to separately predict segmentation RHW S, where denotes the number of images Sv class labels for each pixel. The segmentation images provide semantic guidance and help to produce locally consistent skinning weights. Finally, we back-project pixel segmentation labels onto lifted surface meshes as registered vertex attributes. We refer readers to more details of the lifting process in the supplementary materials. Mesh Unposing. Once we have extracted posed surface meshes from input images, we align them in the shared canonical space via unposing, which is defined in Eq.(2) as the inverse of the LBS operation as: [ ˆu; 1] = LBS(u, w, T) = ((cid:80)J [v; 1] = LBS1( ˆu, w, T) = ((cid:80)J (1) j=1wjTj)1[ ˆu; 1] , (2) R3 are pair of posed and unposed ver- ; 1] denotes homogeneous vector, tices of surface mesh, [ RJ44 is the joint transformation computed from the RJ is the pose vector (including the bone scales), and associated vertex skinning weights. However, key challenge for unposing is that the optimal skinning weight is unknown, as the personalized skinning weight is not available at this stage. While it is possible to first optimize it via cyclic consistency loss [49], such optimization can take hours to converge and thus is not desirable. Instead, we propose to use an arbitrary skinning weight, i.e. from the nearest posed template vertices, and perform deterministic unposing. The intuition is that while such unposing can produce noisy results, its output is only used an initial condition and the artifact patterns are consistent and thus can be later corrected by the universal model. We compare the effects of unposing in Figure 5 in the ablation study. Canonical Rendering. For each unposed surface mesh, we use fixed orthogonal camera cv to render its unposed norR W 3, mal and segmentation images as Nv where and are the height and width of the unposed images. Compared to the original posed images, body parts in unposed images are pixel-aligned within each view (illustrated in Figure 2), since we also normalize the bone scales in Eq.(2). In this way, it significantly reduces the difficulty of feature extraction as features for each body vertex can be consistently queried from corresponding pixel locations, thus helps to generate canonical geometry with fine-grained In addition, projecting 3D surfaces onto 2D imdetails. age space allows us to leverage powerful image models as shown in [30], which further facilitates the feature extraction process. With the unposed normal and segmentation images as inputs, we then develop universal clothed human model for reconstructing personalized skinned avatars as will be introduced in the following section. and Sv 3.2. Universal Clothed Human Model To tackle the complexity of diverse body shapes and cloth types, we develop universal model to learn prior from large collection of clothed human subjects. The model consists of an encoder that aggregates multi-frame initial conditions to refine canonicalization results and fuse single feature representing the subject identity, followed by decoder that jointly decodes personalized canonical shapes, skinning weights, and pose-dependent deformations. Multi-Frame Encoder. For challenging poses and cloth types, canonicalization artifacts can be challenging to correct due to the large deviation from the actual canonical shape. To tackle this issue, we propose to aggregate multiple posed references to improve model robustness and fuse more plausible canonical avatar in multi-frame encoder ). Specifically, we first stack unposed normal and segfe( mentation images to align geometry and semantic references, and then follow [53] to separately extract highresolution feature Hv for fine-grained geometric details, and low-resolution feature Lv for global identity representation of the subject as: Hv = fh( Nv Sv ), Lv = fl( Nv Sv ) , (3) ) is shallow convolutional neural network where fh( (CNN) and fl( denotes ) is DeepLabV3 [11] backbone, channel-wise concatenation. Unlike [53], we adopt more light-weight feature encoders and do not use vision transformers [58], thanks to the fact that the initial conditions are pixel-aligned. Finally, we aggregate features of all frames W 2C, with deinto single bi-plane feature noting the number of feature channels for each plane as: = (Bf Bb), Bv = 1 (cid:80)N i=1fb(Hv Lv ) , (4) where fb( ) is CNN encoder. The feature aggregation is achieved by averaging features of each frame, which has several advantages. First, it flexibly supports an arbitrary number of input frames and is agnostic to the frame order. In addition, the averaged features favor details that are intrinsic to the subject, i.e. body shapes and cloth types, while effectively disentangling pose-specific wrinkles and artifacts that do not commonly appear. We show in Figure 5 in the ablation study that the aggregated bi-plane feature allows the decoder to generate more plausible canonical geometry. Canonical Geometry Decoder. To generate avatar meshes with arbitrary topology and fine-grained geometric details, we adopt hybrid 3D representation DMTet [50]. Specifically, we first follow [19] to construct tetrahedral grid encapsulated by an outer shell of the body template, as illustrated in Figure 2. Such grid initialization allocates grid vertices mostly near the body template, thus helps to imR3, prove the geometry resolution. For each grid vertex we orthogonally project it onto each plane in the bi-plane feature using the camera cv, and accordingly sample pixel RC. While ϕv encodes the geometric and features as ϕv semantic information, we further incorporate the 3D position of the grid vertex as an inductive bias to encourage surface continuity and reduce floaters. Finally, we forward all ) to predict the features to MLP geometry decoder fg( R3 for SDF value each grid vertex as: and vertex displacement (s, g) = fg(ϕf ϕb g) . (5) The canonical mesh can then be differentiably extracted by Marching Tetrahedra (MT) [50] with the outputs. Skinning Weight Decoder. To ensure smooth animation for challenging poses and extreme body shapes, we propose to decode personalized skinning weights in separate MLP decoder fs( ), in contrast to previous methods that rig the avatar with fixed skinning weights. Thanks to the decoded canonical avatar mesh, we can explicitly assign the skinning weight for each canonical vertex v. Similarly to the geometry decoder, we sample each canonical vertex from the RC. bi-plane feature to obtain the skinning feature ψv To encourage neighboring vertices to have similar skinning weights, we also include the canonical vertex position as the input feature as: = fs(φf φb v) , (6) where the last layer in fs( ) is Softmax [7] layer to ensure the validity of the output skinning weights. Pose-dependent Deformation. Since animation driven solely by LBS often leads to deformation artifacts, we propose to improve the animation quality by further including pose-dependent deformation module as shape correction. Specifically, we first follow [30] to render front W 3 from the target and back position map Pv pose vector pt as the pose condition. We then combine it with the avatar identity condition, i.e. the rendered decoded canonical mesh to obtain deformation bi-plane feature ˆBt = fd(Pv ˆBt = ( ˆBf ) has the same architecture as the encoder where fd( ) is renderer function to render normal im- ), and ( fe( ages of canonical mesh with camera cv. Finally, we sample RC in similar way as above and decode the features ψv per-vertex displacement R W 2C as: ˆBb (M, cv)) , R3: ˆBv t), (7) vt = fc(ψf ψb ) , (8) where fc( ) is MLP decoder. With all above components, we can animate the avatar with any target joint transformation ˆT to produce the posed mesh ˆMt as: [ˆvt; 1] = LBS(v + vt, w, ˆT) , (9) where ˆvt is vertex of the posed vertices ˆVt RV 3. 3.3. Training Process Due to the coupling between the canonical geometry and skinning weights, supervising only with the ground truth in the posed space can generate avatars with incorrect components but can be accidentally warped to the correct target [32]. To address this issue, we propose multi-stage training process to co-supervise the model with both posedspace ground truth and canonical-space regularization. Canonical-Space Stage. We first train with only the biplane encoder fe( ) as an initial- ) geometry decoder fg( ization step. Since ground truth canonical meshes do not exist, we construct pseudo ground truths by unposing 3D scans of each frame in an optimization process (details are in the supplementary materials). Note the unposing here produces less artifacts but takes 20 minutes for each frame, thus is only applicable for data preparation. Next, we use differentiable renderer [43] to render the canonical mesh in random set of views to produce the predicted RP HW 3 and canonical normal and depth images RP HW 3 respectively, and impose an l1 loss as: (10) Lc = and 1 + 1 , where N are the rendered normal and depth images of the pseudo ground truth canonical mesh in the same set of views. During training, we randomly select frame in the input frame set for supervision, encouraging the geometry decoder to preserve details commonly appear in all frames, while discard artifacts that are frame-varying. Posed-Space Stage. Once the geometry decoder is sufficiently initialized by Eq.(10), we then jointly train all modules by supervising in the posed space as: La( ˆ Nt, ˆ ˆ Dt ˆ ˆ ˆ 1 + 1 + Lp = Nt Nt and ˆ where ˆ Dt are rendered images of predicted posed and ˆ mesh, ˆ are corresponding rendered images from ground truth posed scans, La is perceptual loss [65] to recover sharper wrinkles in the predicted posed mesh. In addition, we regularize the skinning weights prediction as: ) , (11) Ls = 1 , where is the nearest skinning weights queried from the body template. Finally, we impose an edge loss to penalize deformation artifacts of over-stretched triangles as: (12) Le = 1 (cid:80) {i,j}E max(0, (ˆvi ˆvj)2 t) , (13) where ˆvi, ˆvj is pair of vertex in the predicted posed mesh connected by an edge defined in denotes the total number of edges in the mesh, and is constant threshold. The overall training objectives for the posed space training stage is thus weighted sum of individual loss as , = λpLp + λsLs + λeLe. 4. Experiments 4.1. Dataset & Metrics To train the universal model, we construct large-scale dome capture dataset with 1100 clothed human subjects, with each subject performing up to 100 different poses. We leverage surface reconstruction method [8] to extract 3D posed scans for each frame as the posed-space ground truths. More details are included in the supplementary material. We reserve 35 unseen subjects in our dataset (Dome Data) for testing. To evaluate model generalizability, we further render synthetic images from 130 subjects in RenderPeople [2] and directly test our pretrained models on it. We follow [17] to evaluate the geometric quality of reconstruction and animation results using 3 metrics: average Point-to-Surface distance (P2S) and Chamfer distance (CD) in centimeters, as well as L2 re-projected normal error (Normal) between the predicted and ground truth posed scans. 4.2. Implementation Details We implement our models in PyTorch [41] and perform all experiments on single NVIDIA A100 GPU. We render all images with 512 512 pixels. We train the model using the Adam [24] optimizer for 10K iterations in the canonicalspace stage, and 100K iterations in the posed-space stage, 104. The total training proboth with learning rate of 1 cess takes 2 days to converge, and we report the inference time comparison in Table 1. For the training losses, we set λp = 1, λs = 0.1, λe = 100 respectively, and use an edge 104. More details and network length threshold of = 1 architecture are included in the appendix. 4.3. Results and Comparisons (i) learningWe compare with three types of baselines: based method [17] that produces animatable avatars in feed-forward approach, (ii) optimization-based method [15] that optimizes canonical avatar shapes from multi-view videos, and (iii) diffusion-based method [61] that generates avatar with SDS loss [44]. As all baselines do not predict personalized skinning weights, we follow their implementation to animate the avatar with fixed skinning weight (queried from nearest template vertices). For [15, 61] and our method, we reconstruct the canonical avatar from the same set of posed images, and test the quality of animation given an unseen target pose. For fair comparison, we adapt both baselines to use the same rigged template as us and provide them with ground truth pose vectors. Since [17] use only single input image and do not have training code, we are unable to fairly compare with it and thus only test its reconstruction quality using the target frame with its own rigged template. In Figure 6, we implement similar sampling strategy with it and compare with our canonicalization approach instead. More comparison with other reconstruction and animation methods are included in the appendix. Table 1. Quantitative comparison with existing methods. Our method achieves superior geometry quality than existing methods [17, 22, 61], and requires significantly less inference time. Methods ARCH++ [17] Vid2Avatar [15] PuzzleAvatar [61] Ours (LBS Only) Ours (Full Model) Dome Data RenderPeople Normal 0.338 0.072 0.104 0.030 0.026 P2S 4.52 0.98 1.47 0.43 0.37 CD 5.07 1.12 1.63 0.49 0.43 Normal 0.195 - 0.132 0.026 0. P2S 2.44 - 1.79 0.33 0.30 CD 2.60 - 1.91 0.38 0.34 Time 26s 8h 3h 18s 18s As compared in Table 1, our method outperforms baseline methods by large margin in all geometry metrics, demonstrating results with superior geometric quality. Moreover, in contrast to [15, 61] that require hours of testtime optimization, we can produce personalized skinned avatars in less than 20 seconds (time details in the supplementary materials) thanks to the feed-forward reconstruction. We further compare qualitative results with baselines in Figure 3. We observe the fixed skinning strategy inevitably leads to deformation artifacts, e.g. over-stretched triangles, for challenging poses and body shapes. In comparison, we generate more smooth and realistic animation and effectively reduces deformation artifacts for even loose garments like skirts. We also observe that diffusion-based method [61] can fail to preserve facial identity, while our method faithfully reconstructs personalized details. Moreover, we show that our pose-dependent deformation module produces plausible wrinkle patterns, while [15] simply bakes wrinkles in canonical shapes without further deformations. This module also helps to reconstruct more accurate geometric details as compared in Table 1. Finally, we show in Figure 4 that the universal prior directly applies to phone photos or synthetic characters. 4.4. Ablation Study Effects of Canonicalization. We show the effects of canonicalization by comparing the results of reconstructed canonical geometry using one posed reference in Figure 6. Since input subjects vary largely in poses and shapes, directly sampling from posed inputs, i.e. by warping query vertices via forward LBS similar to [17], retrieves misaligned features and thus leads to overly smoothed geometry (second column). In contrast, the canonicalization can produce pixel-aligned initial conditions. However, due to the inaccurate skinning, the initial unposing results often contain artifacts, e.g. stretched triangles, that are not suitable for use. To tackle this, we refine them in the universal model to effectively refine more plausible canonical shape, while preserving fine-grained geometric details. Effects of Multi-Frame Aggregation. We compare the reconstructed canonical shapes with different numbers of reference frames in Figure 5. Due to the lack of ground truth canonical shapes, refinement for canonicalization using Figure 3. Qualitative Comparison. Our method produces superior animation quality when reposed to an unseen pose for challenging poses, body shapes and cloth types, which reduces deformation artifacts, e.g. stretched triangles, and generates plausible wrinkles. Figure 4. Method Generalizability. We show the pretrained universal model can directly apply to causally taken photos and synthetic images from Renderpeople [2], which demonstrates its practical applications. When applied to phone photos, we do not require perfect alignment of front and back views and use estimated poses from monocular images for canonicalization. More details are in appendix. single frame can be challenging for loose garments and long hairs when input poses are not ideal. In comparison, by including more frames, the multi-frame encoder learns to preserve intrinsic features that commonly appear across different frames, thus can fuse more plausible avatar. In practice, we observe the fusion results saturate in 5 frames to produce sufficiently plausible canonical shapes. Effects of Personalized Skinning Weights. We show the benefits of predicting personalized skinning weights in Figure 7. In particular, skinning the avatar using nearest template body vertices can produce over-stretched triangles, e.g. under armpit, while personalized skinning weights effectively reduces such deformation artifact. Furthermore, we show that training with multiple input and target frames (N = 5) produces more robust skinning weights than only learn to repose to the same frame as input (N = 1). Effects of Pose-dependent Deformation. As shown in Figure 8, we observe three benefits on including posedependent deformation. First, it corrects LBS artifacts, e.g. when bending elbow and wrist, which improves animation Figure 5. Effects of multi-frame aggregation. Given set of unposed normal frames from different poses in the left, we show results of fused canonical shapes using the first frames at each column in the right. we observe that aggregation from multiple frames produces more plausible canonical shapes by correcting unposing artifacts, e.g. on skirts and hairs, while preserving person-specific details. Figure 6. Effects of canonicalization. By taking canonicalization results as inputs, the universal model can learn to reduce unposing artifacts and preserve fine-grained details, compared to directly sampling features from posed inputs via forward warping as [17]. Figure 8. Effects of Pose-dependent Deformation. The deformation module can correct LBS artifacts (wrist and arm bending in first row) and generate plausible garment dynamics (sleeves draping in second row), which improves animation realism. 5. Discussion Limitation. Although our method achieves superior reconstruction results, the geometric fidelity of the avatars are bounded by the resolution of the tetrahedral grids, thus may lead to missing details such as tiny accessories. Moreover, we only consider deformations dictated by pose, omitting complex dynamics such as body-cloth interactions, and motions of long hairs or extremely loose garments whose deformations are not entirely pose-dependent. Conclusion. In this paper, we present FRESA, novel method for feed-forward reconstruction of personalized skinned avatars using only few images. The core to our method is universal clothed human model learned from diverse human subjects, which achieves joint inferring of personalized canonical shapes, skinning weights, and posedependent deformations. Moreover, we introduce 3D canonicalization process with multi-frame aggregation to further improve the model robustness. Thanks to these innovations, our method achieves instant generation and zeroshot generalization with superior animation quality. Figure 7. Effects of personalized skinning weights. We show personalized skinning weights reduce deformation artifacts, e.g. under armpit, and can be more robustly estimated when trained with multiple input and target frames. Note we show results deformed by LBS only, i.e. without pose-dependent deformation. realism. Second, it can generate plausible garment dynamics such as sleeves draping when raising arms. Finally, we observe an overall refinement of fidelity in wrinkles details. A. Data Acquisition Data Capture. We visualize our dome capture data in Figure 9. For each subject, we capture 128 images from different view points using group of fixed cameras, and adopt SuperNormal [8] to reconstruct the 3D scans for geometry supervision. We further estimate 3D joints for each frame from multi-view images, and adopt an incremental pose encoder [10] to obtain the pose vector p. With the diverse posed clothed humans and high-quality scans, we can learn an effective universal prior that well generalizes to phone photos. ( ˆ = arg min ization for canonical-space stage training. Specifically, we adopt FlexiCubes [51] as the 3D geometry representation and build cubic grid with = 2563 vertices near the rigged body template. We then initialize the SDF value of each grid vertex as the signed distance to the template, and optimize canonical vertices parameterized by the Flexicube parameters s, α, β, γ [51] such that: Rn( V) Rd( V) Lr) , (14) ) are renderer functions for normal and depth rendering, is the posed vertices obtained by forward LBS (where skinning weights queried from nearest Lr is the regularization term in [51], and template vertices), ˆ are ground truth depth and normal images for posed scans. We empirically observe that such unposing strategy reduces artifacts of over-stretched triangles as shown in Figure 10. However, since such unposing requires complete optimization process and takes 20 minutes to converge, it is only suitable for data preparation. ) and Rn( , ˆ 1 + 1 + Rd( where ˆ Figure 10. Unposing Comparison. We compare the results between naive unposing (used in the inference pipeline) and pseudo GT via optimization (used for data preparation). The second approach produces more plausible results but requires significantly more time. Note we filter edges with length larger than 1 104 to reduce noises. Approval of Usage. All participants involved in dome capture data and phone photos have signed consent form that authorize the usage of their images for model development and academic publications. B. Canonicalization Details Pose Tracking for Photos. We use an artists designed rigged body mesh as the template, which contains = 67 joints. For each front and back view photo, we estimate its 2D joint positions using [23], and optimize the pose vector to minimize the 2D projection loss similar to [6]. We further determine the absolute scale of the subject based on pretrained statistical prior model using PCA coefficients. The overall optimization process takes about 1 minute per Figure 9. Samples of dome data. Our dataset contains diverse posed clothed humans paired with high-quality 3D scans as ground truths, which facilitates learning an effective universal prior. Pseudo-GT Canonical Meshes. To resolve the coupled ambiguity between canonical shapes and skinning weights, we construct pseudo-GT3 canonical meshes as the regular3ideal GT meshes should be obtained by unposing scans with personalized skinning weights, which is not available in canonical-space stage. and pose tracking process work mostly robust in these two views. However, our method can still produce plausible side view geometry by learning across diverse subjects. C. More Implementation Details Figure 13. Visualization in Four Views. By only taking inputs of front and back views, our method can infer plausible side-view geometry and produce consistent boundary. frame. For fair comparison, in the main paper, we report inference time for all methods excluding the pose tracking time as we assume known poses in our pipeline. Note that to ensure practical use of our method, we do not require perfect alignment between front and back views for causally taken photos, i.e. we do not require known camera poses, and photos do not need to be synchronized in time, as illustrated in Figure 11. Such casual inputs can be robustly handled with the universal clothed human prior and multi-frame aggregation, as shown in main paper. In addition, since there are no GT body poses for phone photos, we use an off-the-shelf pose estimator to estimate body poses for each view. Demo results in the main paper show that our method can robustly generate plausible avatars under this imperfect unposing, ensuring practical use of our method. Moreover, the proposed multi-frame aggregation approach can further improve robustness against inaccurate pose estimation in individual frame. Figure 11. Illustration of settings for photos. We use estimated body poses and do not require perfect alignment between views. 3D Lifting. We follow [60] to use d-BiNI method to obtain the lifted front and back surface meshes for each frame. The surface depth is initialized based on the tracked poses, i.e. the surface depth of posed body template. We visualize the resulting unposed surface meshes in Figure 12. Figure 12. Illustration of Lifted Surface Meshes. Note we removed the over-stretched edges after unposing. The lifting process produces two unposed surface meshes but can not be perfectly aligned in boundary. In contrast to [60] that attempts to directly complete the lifted meshes in the 3D space, we re-render them into 2D images as initial conditions, and infer the canonical shape from scratch for two reasons: (i) it produces more plausible boundaries by jointly refining geometry in both visible and invisible parts, as shown in Figure 13. (ii) it can learn personalized body shape instead of fixed shape bounded by the initial depth, as shown in Figure 14. Finally, we choose to use two views in the paper as the lifting Figure 14. Results of Inferred Body Shape. Our method can produce personalized body shapes based on input conditions and is not restricted to the template shape. We train our model using = 5 input frames, which achieves the best balance between plausibility and fidelity. In inference, our model can be applied to an arbitrary number of input frames based on availability. To increase the model generalizability, we apply data augmentation by mixing unposing results from both d-BiNI and 3D scans when training the model. Finally, we use [1] to tetrahedralize the volume near the canonical template (with distance of 0.2m), resulting in grid of resolution 256. More details about our network architecture are included in Appendix D. Inference Time Details. We report the inference time for 960. Specifically, one input photo with image size (i) the segmentation and normal estimation for [23] takes 4.90s, (ii) the d-BiNI time for both views takes 9.91s, (iii) the unposing (including finding the nearest template vertices) takes 1.54s, (iv) the canonical rendering takes 0.06s, and (v) the overall model inference takes 1.64s, thus the total inference time is 18.05s. All time are reported with single NVIDIA A100 GPU. Baseline Implementation. For [17], we use the test code and pretrained models provided by the author. Since the method only takes single image as input, we test its reconstruction quality by using the target posed image as input. For [15, 61], we modify its code to use our rigged template and canonical pose instead of SMPL-X [42] template. For [15], we also follow their implementation to use nearest template vertices skinning weights, weighted by the pointto-point distances in deformed space, while [61] only uses template mesh to initialize the DMTet grid. D. Network Architecture R5125123 follow [30] and conback position maps Pt catenate with the rendered front and back images of the inferred canonical mesh, and forward it to fd( ) (with the )) to produce residual biplane same architecture as fe( ˆBt R96 R51251296. We then sample pixel feature ψt as the feature for each vertex in the canonical mesh. The output channels for each Linear block is [64, 64, 64, 64, 3]. E. More Animation Comparison In this section, we compare with SCANimate [49], which optimizes personalized skinning weights and canonical shapes jointly in an implicit field. As shown in Figure 15, while such approach produces smooth deformation, the use of implicit field results in low geometry resolution and thus missing fine-grained details. Moreover, [49] rely on timeconsuming per-subject fitting and 3D posed meshes as inputs, whereas our method can achieve instant feed-forward reconstruction from few images. 64 96. In fh( Multi-Frame Encoder. We show the architecture for fe( ) in Figure 19. For simplicity of notation, we refer to all input features as these after concatenating the front and back views, e.g. R25125123, and thus discarding the view dependency in the superscript and assume all features below have batch size of 2. In fl( ), the DeepLabV3 [11] 256, backbone produce feature map of shape 64 and the output channels for the Conv2d are [128, 128, 96] respectively. All upsampling blocks are implemented bilinear interpolation, thus the dimension for Li is as 2 ), the output channels for the Conv2d 256 256 are [64, 96, 96, 96, 96] respectively. Note we follow [53] to include positional encoding before the first convolution block, thus its input channel is 6 instead of 8. Except that ) has stride 3, all other the first convolution block in fh( blocks have stride 1. The final dimension for Hi is the same as Li. In fb( ) the output channels for the Conv2d are [256, 128, 96], and the biplane feature Bi has shape of 512 Canonical Geometry Decoder. We show the architecture ) in Figure 20. For each grid vertex g, we sample for fg( R96. The output the feature on Bi to obtain the feature ϕ channels for each Linear block is [64, 64, 64, 64, 4]. Note here we include BatchNorm (BN) [21] and treat each vertex as batch sample. We observe this module can be used to replace geometric initialization [64] to ensure valid mesh at initial steps, i.e. avoid situations where the network predicts all positive or negative SDF values. Skinning Weight Decoder. We show the architecture for ) in Figure 20. Except that the last linear layer has an fs( output dimension of 161, all other modules follow the same ). architecture as fg( Pose-dependent Deformation Decoder. We show the ar- ) in Figure 20. We first render front and chitecture for fc( 512 96. Figure 15. Animation comparison with SCANimate. For [49], we use FRESA reconstructions as reference posed meshes. Note that hand motions are missing as it is SMPL-based."
        },
        {
            "title": "Comparison with Reconstruction Methods",
            "content": "In this work, we aim to generate personalized avatars that can be realistically animated driven by novel poses. In contrast, other baselines like [28, 47, 48, 59, 60, 67] are often characterized as single-image reconstruction method, which focuses on recovering the geometry for the input pose only, and animates posed avatars using fixed skinning weights. Hence they do not study avatar animation and thus are not closely related to our work. Moreover, in the experiments we evaluate the animation quality on unseen poses and predict pose-dependent deformation to recover fine-grained details like wrinkles, thus fair comparison is difficult to perform with the reconstruction methods. For completeness, we show in Figure 16 that our method can produce highquality geometry details comparable to [28, 60, 67], thanks to the effective prior learned from diverse subjects. Considering fairness of evaluation, we do not quantitatively benchmarking reconstruction quality on our dataset. F. Texture Reconstruction Our method can be extended to generate texture for the reIn this section we proconstructed personalized meshes. Figure 16. Qualitative comparison with single image reconstruction methods. Our method produces high-quality geometry details comparable to ECON [60], SIFU [67], and PSHuman [28] on both dome data and phone photos. vide one sample implementation for texture reconstruction. Specifically, we first unpose lifted surface meshes with back-projected vertex color (refer to Figure 12 as an example) and render the RGB images as input. We then encode the RGB images into separate bi-plane feature (us- )), and pose ing encoder with the same architecture as fe( the canonical avatar by the target pose vector pt. For each rendered pixel of the posed avatar mesh, we use the corresponding 3D position on the canonical mesh to sample the bi-plane feature, which is forwarded to MLP decoder )) to predict the RGB (with the same architecture as fc( color for that pixel. We show in Figure 17 that this approach produces realistic rendering results. G. Failure Cases In our method, the deformation module is only conditioned on skeletal pose vector, which is deficient to model complex dynamics such as motions of hair or extremely loose garments like long dress. We show failure cases in Figure 18, where the results posed deformation do not match the real dynamics. Future works are encouraged to explore more comprehensive pose conditions or physics-inspired models to tackle this issue. Figure 17. Results of Textured Meshes. Our method can be extended to produce high-resolution texture for realistic rendering. Figure 18. Failure Cases. With only the pose vector as condition, our method fails to produce complex hair motions and dynamics of extremely loose garments."
        },
        {
            "title": "References",
            "content": "[1] quartet. https://github.com/crawforddoran/quartet. 2 [2] Renderpeople. https://renderpeople.com/3d-people/. 6, 7 [3] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, and Marcus Magnor. Tex2shape: Detailed full human body geometry from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22932303, 2019. 2 [4] Thiemo Alldieck, Mihai Zanfir, and Cristian Sminchisescu. Photorealistic monocular 3d reconstruction of humans wearing clothing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1506 1515, 2022. 2 Figure 19. Model Architecture for multi-frame encoder fe(). Note we stack two views together and omit the superscript v. The final bi-plane feature is obtained by summing the feature for each frame Bi. denotes channel-wise concatenation. Figure 20. Model Architecture for canonical geometry decoder fg(). Figure 21. Model Architecture for skinning weight decoder fs(). Figure 22. Model Architecture for pose-dependent vertex displacement decoder fc(). [5] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll. Multi-garment net: Learning to dress In Proceedings of the IEEE/CVF 3d people from images. international conference on computer vision, pages 5420 5430, 2019. 2 [6] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael Black. Keep it smpl: Automatic estimation of 3d human pose and shape from single image. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 561578. Springer, 2016. 1 [7] John Bridle. Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Neurocomputing: Algorithms, architectures and applications, pages 227236. Springer, 1990. 5 [8] Xu Cao and Takafumi Taketomi. Supernormal: Neural surface reconstruction via multi-view normal integration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2058120590, 2024. 6, 1 [9] Xu Cao, Hiroaki Santo, Boxin Shi, Fumio Okura, and Yasuyuki Matsushita. Bilateral normal integration. In European Conference on Computer Vision, pages 552567. Springer, 2022. [10] Joao Carreira, Pulkit Agrawal, Katerina Fragkiadaki, and Jitendra Malik. Human pose estimation with iterative erIn Proceedings of the IEEE conference on ror feedback. computer vision and pattern recognition, pages 47334742, 2016. 1 [11] Liang-Chieh Chen. Rethinking atrous semantic image segmentation. for arXiv:1706.05587, 2017. 4, 3 convolution arXiv preprint [12] Xu Chen, Yufeng Zheng, Michael Black, Otmar Hilliges, and Andreas Geiger. Snarf: Differentiable forward skinning for animating non-rigid neural implicit shapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1159411604, 2021. 2 [13] Yushuo Chen, Zerong Zheng, Zhe Li, Chao Xu, and Yebin Liu. Meshavatar: Learning high-quality triangular human avatars from multi-view videos. arXiv preprint arXiv:2407.08414, 2024. 2 [14] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett, Dennis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk, and Steve Sullivan. High-quality streamable free-viewpoint video. ACM Transactions on Graphics (ToG), 34(4):113, 2015. 1 [15] Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. Vid2avatar: 3d avatar reconstruction from videos in the wild via self-supervised scene decomposition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1285812868, 2023. 6, 3 [16] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen, Geoff Harvey, Sergio OrtsEscolano, Rohit Pandey, Jason Dourgarian, et al. The relightables: Volumetric performance capture of humans with realistic relighting. ACM Transactions on Graphics (ToG), 38(6):119, 2019. 1 [17] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. Arch++: Animation-ready clothed human reconstruction revisited. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1104611056, 2021. 1, 2, 6, 8, [18] Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei Yang, and Ziwei Liu. Sherf: Generalizable human nerf from In Proceedings of the IEEE/CVF Internaa single image. tional Conference on Computer Vision, pages 93529364, 2023. 2 [19] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Jiaxiang Tang, Deng Cai, and Justus Thies. Tech: Text-guided reconstruction of lifelike clothed humans. In 2024 International Conference on 3D Vision (3DV), pages 15311542. IEEE, 2024. 4 [20] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. Arch: Animatable reconstruction of clothed humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 30933102, 2020. 1 [21] Sergey Ioffe. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 3 [22] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Selfrecon: Self reconstruction your digital avatar from monocIn Proceedings of the IEEE/CVF Conference ular video. on Computer Vision and Pattern Recognition, pages 5605 5615, 2022. 6 [23] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision modIn European Conference on Computer Vision, pages els. 206228. Springer, 2025. 3, 4, [24] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. 2014. 6 arXiv preprint arXiv:1412.6980, [25] Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, and Anurag Ranjan. Hugs: Human gaussian splats. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 505515, 2024. 2 [26] Zorah Lahner, Daniel Cremers, and Tony Tung. Deepwrinkles: Accurate and realistic clothing modeling. In Proceedings of the European conference on computer vision (ECCV), pages 667684, 2018. 2 [27] Jason Lawrence, Ye Pan, Dan Goldman, Rachel McDonnell, Julie Robillard, Carol OSullivan, Yaser Sheikh, Michael Zollhoefer, and Jason Saragih. State of the art in telepresence. In ACM SIGGRAPH 2022 Courses, pages 1 74. 2022. [28] Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi, Mengfei Li, Xiaowei Chi, Siyu Xia, Wei Xue, et al. Pshuman: Photorealistic single-view human reconstruction using cross-scale diffusion. arXiv preprint arXiv:2409.10141, 2024. 3, 4 [29] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhofer, Jurgen Gall, Angjoo Kanazawa, and Christoph Lassner. In EuTava: Template-free animatable volumetric actors. ropean Conference on Computer Vision, pages 419436. Springer, 2022. 2 [30] Zhe Li, Zerong Zheng, Lizhen Wang, and Yebin Liu. Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1971119722, 2024. 2, 4, 5, 3 [31] Tingting Liao, Xiaomei Zhang, Yuliang Xiu, Hongwei Yi, Xudong Liu, Guo-Jun Qi, Yong Zhang, Xuan Wang, Xiangyu Zhu, and Zhen Lei. High-fidelity clothed avatar In Proceedings of reconstruction from single image. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86628672, 2023. 1, 2 [32] Siyou Lin, Hongwen Zhang, Zerong Zheng, Ruizhi Shao, and Yebin Liu. Learning implicit templates for point-based clothed human modeling. In European Conference on Computer Vision, pages 210228. Springer, 2022. 2, 5 [33] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object, 2023. [34] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. 2023. 2 [35] William Lorensen and Harvey Cline. Marching cubes: high resolution 3d surface construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages 347353. 1998. 2 [36] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael Black. Learning to dress 3d people in generative clothing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64696478, 2020. 2 [37] Thalmann Magnenat, Richard Laperri`ere, and Daniel Thalmann. Joint-dependent local deformations for hand animation and object grasping. In Proceedings of Graphics Interface88, pages 2633. Canadian Inf. Process. Soc, 1988. 2 [38] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44604470, 2019. 2 [39] Mathias Parger, Chengcheng Tang, Yuanlu Xu, Christopher Twigg, Lingling Tao, Yijing Li, Robert Wang, and Markus Steinberger. Unoc: Understanding occlusion for emIEEE Transactions on bodied presence in virtual reality. Visualization and Computer Graphics, 28(12):42404251, 2021. [40] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165174, 2019. 2 [41] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. 6 [42] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Expressive body capture: 3d hands, Michael Black. In Proceedings of face, and body from single image. the IEEE/CVF conference on computer vision and pattern recognition, pages 1097510985, 2019. 2, 3 [43] Stanislav Pidhorskyi, Tomas Simon, Gabriel Schwartz, He Wen, Yaser Sheikh, and Jason Saragih. Rasterized edge arXiv gradients: Handling discontinuities differentiably. preprint arXiv:2405.02508, 2024. 5 [44] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 6 [45] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems, 30, 2017. 2 [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [47] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 23042314, 2019. 1, 2, 3 [48] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit function for In Proceedings of high-resolution 3d human digitization. the IEEE/CVF conference on computer vision and pattern recognition, pages 8493, 2020. 1, 2, 3 In Proceedings of the IEEE/CVF clothed avatar networks. Conference on Computer Vision and Pattern Recognition, pages 28862897, 2021. 2, 4, 3 [50] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. Advances in Neural Information Processing Systems, 34:60876101, 2021. 4, 5 [51] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization. ACM Trans. Graph., 42(4):371, 2023. 1 [52] Jisu Shin, Junmyeong Lee, Seongmin Lee, Min-Gyu Park, Ju-Mi Kang, Ju Hong Yoon, and Hae-Gon Jeon. Canonicalfusion: Generating drivable 3d human avatars from multiple images. arXiv preprint arXiv:2407.04345, 2024. 3 [53] Alex Trevithick, Matthew Chan, Michael Stengel, Eric Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Ravi Ramamoorthi, and Koki Nagano. Real-time radiance fields for single-image portrait view synthesis. 2023. 4, [54] Rong Wang, Wei Mao, Changsheng Lu, and Hongdong Li. Towards high-quality 3d motion transfer with realistic apparel animation. In European Conference on Computer Vision, pages 3551. Springer, 2025. 1, 3 [55] Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, and Siyu Tang. Metaavatar: Learning animatable clothed human models from few depth images. Advances in Neural Information Processing Systems, 34:28102822, 2021. 2 [56] Shaofei Wang, Katja Schwarz, Andreas Geiger, and Siyu Tang. Arah: Animatable volume rendering of articulated human sdfs. In European conference on computer vision, pages 119. Springer, 2022. 2 [57] Chung-Yi Weng, Brian Curless, Pratul Srinivasan, Jonathan Barron, and Ira Kemelmacher-Shlizerman. Humannerf: Free-viewpoint rendering of moving people from In Proceedings of the IEEE/CVF conmonocular video. ference on computer vision and pattern Recognition, pages 1621016220, 2022. 2 [58] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in neural information processing systems, 34: 1207712090, 2021. 4 [59] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael Icon: Implicit clothed humans obtained from norBlack. In 2022 IEEE/CVF Conference on Computer Vimals. sion and Pattern Recognition (CVPR), pages 1328613296. IEEE, 2022. 1, 2, [60] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael Black. Econ: Explicit clothed humans optimized via normal integration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 512523, 2023. 1, 2, 4, 3 [49] Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael Black. Scanimate: Weakly supervised learning of skinned [61] Yuliang Xiu, Yufei Ye, Zhen Liu, Dimitrios Tzionas, and Michael Black. Puzzleavatar: Assembling 3d avatars from personal albums. arXiv preprint arXiv:2405.14869, 2024. 2, 6, 3 [62] Yuxuan Xue, Bharat Lal Bhatnagar, Riccardo Marin, Nikolaos Sarafianos, Yuanlu Xu, Gerard Pons-Moll, and Tony Tung. Nsf: Neural surface fields for human modeling from monocular depth. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1504915060, 2023. 2 [63] Xihe Yang, Xingyu Chen, Daiheng Gao, Shaohui Wang, Xiaoguang Han, and Baoyuan Wang. Have-fun: Human avatar reconstruction from few-shot unconstrained images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 742752, 2024. [64] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. Advances in Neural Information Processing Systems, 34:48054815, 2021. 3 [65] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 5 [66] Zechuan Zhang, Li Sun, Zongxin Yang, Ling Chen, and Yi Yang. Global-correlated 3d-decoupling transformer for clothed avatar reconstruction. Advances in Neural Information Processing Systems, 36, 2024. 1 [67] Zechuan Zhang, Zongxin Yang, and Yi Yang. Sifu: Side-view conditioned implicit function for real-world usIn Proceedings of the able clothed human reconstruction. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99369947, 2024. 1, 3, 4 [68] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently generated human radiance field from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77437753, 2022. 2 [69] Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, and Yebin Liu. Gpsgaussian: Generalizable pixel-wise 3d gaussian splatting for In Proceedings of real-time human novel view synthesis. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1968019690, 2024."
        }
    ],
    "affiliations": [
        "Australian National University",
        "Meta Reality Labs Research"
    ]
}