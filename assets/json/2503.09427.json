{
    "paper_title": "Multimodal Language Modeling for High-Accuracy Single Cell Transcriptomics Analysis and Generation",
    "authors": [
        "Yaorui Shi",
        "Jiaqi Yang",
        "Sihang Li",
        "Junfeng Fang",
        "Xiang Wang",
        "Zhiyuan Liu",
        "Yang Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate single-modal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), a unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-the-art cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells -- the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84\\% relative improvement of textual discrepancy for cell description generation, 20.5\\% higher accuracy for cell type annotation, and 4\\% improvement in $k$-NN accuracy for text-conditioned pseudo-cell generation, outperforming baselines."
        },
        {
            "title": "Start",
            "content": "Multimodal Language Modeling for High-Accuracy Single Cell Transcriptomics Analysis and Generation Yaorui Shi1*, Jiaqi Yang1*, Sihang Li1, Junfeng Fang2, Xiang Wang1, Zhiyuan Liu2, Yang Zhang2 1 University of Science and Technology of China 2 National University of Singapore {yaoruishi, sihang0520, xiangwang1223, acharkq}@gmail.com, {yangjiaqi20, fjf}@mail.ustc.edu.cn, zhang@nus.edu.sg 5 2 0 2 2 ] . [ 1 7 2 4 9 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Pre-trained language models (PLMs) have revolutionized scientific research, yet their application to single-cell analysis remains limited. Text PLMs cannot process single-cell RNA sequencing data, while cell PLMs lack the ability to handle free text, restricting their use in multimodal tasks. Existing efforts to bridge these modalities often suffer from information loss or inadequate singlemodal pre-training, leading to suboptimal performances. To address these challenges, we propose Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), unified PLM for joint cell and text modeling. scMMGPT effectively integrates the state-of-theart cell and text PLMs, facilitating cross-modal knowledge sharing for improved performance. To bridge the text-cell modality gap, scMMGPT leverages dedicated cross-modal projectors, and undergoes extensive pre-training on 27 million cells the largest dataset for multimodal cell-text PLMs to date. This large-scale pre-training enables scMMGPT to excel in joint cell-text tasks, achieving an 84% relative improvement of textual discrepancy for cell description generation, 20.5% higher accuracy for cell type annotation, and 4% improvement in kNN accuracy for text-conditioned pseudo-cell generation, outperforming baselines. Our code is available at https://github.com/syr-cn/ scMMGPT."
        },
        {
            "title": "Introduction",
            "content": "Pre-trained language models (PLMs) are transforming scientific research (Touvron et al., 2023a; OpenAI, 2023; Hurst et al., 2024; Li et al., 2024b; Shi et al., 2024). Their ability to recall scientific knowledge, analyze data, and perform mathematical reasoning helps to reduce manual efforts and lower the research barrier in many tasks. Notably, *Equal contribution. Corresponding authors: Z. Liu and Y. Zhang. Figure 1: Comparison between the scRNA-seq results and the textual descriptions of cell. The inherent disparities between these two data modalities make it difficult to jointly model them. PLMs are opening new avenues for single-cell analysis, which explores the molecular and functional characteristics of individual cells. Previous studies have successfully employed PLMs for cell annotation (Li et al., 2024a) and retrieval (Xu et al., 2023a; Lan et al., 2024), benefiting from the extensive cellular knowledge embedded in the PLMs training corpus. Beyond the text-based PLMs above, cell PLMs are also explored for single-cell analysis (Ji et al., 2021a; Abdolhosseini et al., 2019). As Figure 1 shows, cell can be represented as an array of gene expression levels, providing insights into its biological properties. These arrays are generated through single-cell RNA sequencing (scRNA-seq) (Saliba et al., 2014; Shalek et al., 2014) technology. Pretrained on scRNA-seq data, cell PLMs have been applied for batch effect correction, and pseudo-cell generation (Yang et al., 2022; Hao et al., 2024; Cui et al., 2024). However, cell PLMs are inherently limited by their inability to process free text, preventing them from integrating the rich single-cell knowledge in textual corpora and restricting their (a) Cell type annotation Figure 2: Cell type annotation results with different cell representation methods. accuracies on the full dataset and test set. Using cell sentences as cell representation leads to significant accuracy degradation. (b-d) UMAP visualization of classification results and the ground truth. Classification using cell sentences yields lower accuracy score and exhibits poorer recognition capabilities in certain cell clusters. ability to perform text-guided cell generation and cell description generation. To resolve this limitation, we want to develop unified multimodal PLM unifying cell and text data for comprehensive single-cell analysis. While this area has been explored, we identify two common limitations of previous works: The Information Loss of Cell Sentences. Previous cell-text PLMs primarily represent single cells as cell sentences (Hou and Ji, 2024; Levine et al., 2024; Choi et al., 2024), where genes are ranked by expression level, and only the top 30-100 genes are retained as the cells representation. This method captures less than 1% of the total gene annotations in modern databases (Program et al., 2025; Cao et al., 2017), which record over 10,000 genes. Additionally, cell sentences discard crucial information about gene expression values. As Figure 2 shows, this representation leads to significant performance degradation in cell type annotation compared to the original expression values representation, highlighting crucial information loss. Limited Single-Modal Pre-training. Some previous works (Choi et al., 2024; Hou and Ji, 2024) are built on text PLMs without sufficient pretraining on scRNA-seq data, limiting their capacity for comprehensive single-cell analysis. While notably Levine et al. (2024) performs large-scale cell pre-training on text PLMs checkpoint, it suffers from catastrophic forgetting, compromising its text processing ability (cf. Section 4.2). In this work, we propose the Single-Cell MultiModal Generative Pre-trained Transformer (scMMGPT), novel PLM designed for the multimodal analysis and generation between cell and text. scMMGPT builds on the scGPT (Cui et al., 2024), an extensively pre-trained cell PLM capable of encoding the full scRNA-seq data. To address scGPTs inability to process text, we implement cell-to-text projector that projects scGPTs representations to the text space, leveraging Llama2 (Touvron et al., 2023b), powerful text PLM, for text generation. For cell generation, we implement text-to-cell projector to map Llama-2s representations to scGPTs cell space, providing textual guidance. These cross-modal projectors between cell and text enable effective information exchange between the two PLMs, leveraging their respective domain knowledge. Given the advanced PLMs in the cell and text modalities, scMMGPT focuses on bridging their modality gaps by pre-training the two cross-modal projectors. To this end, we pre-train scMMGPT on 27 million cells from CELLxGENE (Program et al., 2025), which is the largest pre-training dataset for multimodal cell-text PLMs. Using this dataset, we pre-train the text-to-cell projector for text-conditioned cell generation, enabling it to map the textual embeddings to space that the cell PLM can understand. Similarly, the cell-to-text projector is pre-trained for cell description generation. Before this step, we warmup the cell-to-text projectors weights with the additional pre-training of cell-text contrasting learning and matching. This step follows (Li et al., 2023), aiming to obtain model for cell type annotation tasks. Our tailor-made architecture and extensive pretraining equips scMMGPT with superior performance on various downstream tasks. It achieves an 84% relative improvement of textual discrepancy for cell description generation, 4% k-NN accuracy improvement for text-conditioned pseudo-cell generation, and 20.5% higher accuracy for cell type annotation than baselines. Ablation studies further validate the effectiveness of the key components."
        },
        {
            "title": "2 Related Works",
            "content": "Single-Cell PLMs. Single-cell sequencing technologies provide diverse biological features that facilitate the interpretation of cellular structures and functions (Heumos et al., 2023; Cao and Gao, 2022). Advances in scRNA-seq have generated massive, high-precision transcriptomic datasets, driving the development of cell PLMs (Ziegenhain et al., 2017). This technique quantifies the mRNA molecule abundance, producing gene expression matrices that record expression values of individual genes across cells (Ji et al., 2021b). Previous works have developed transformer-based foundation models on scRNA-seq data, pre-training with masked learning objectives on millions of cells (Zhao et al., 2023; Theodoris et al., 2023; Yang et al., 2022). Subsequent works improve the learning process by incorporating cell labels, such as batch effects (Cui et al., 2024). After fine-tuning, these PLMs have proven useful in practical downstream tasks including cell-type annotation, perturbation response prediction, and pathway activity inference. Cell-Text Modeling. Textual descriptions of cells and scRNA-seq data capture complementary aspects of cellular systems. To jointly leverage this complementary information, prior research has explored enhancing cellular representation learning using biological text descriptions (Chen and Zou, 2023; Zhao et al., 2024). Inspired by multimodal PLMs in other scientific domains (Liu et al., 2023b; Edwards et al., 2022), cell-to-text translation is also explored (Xu et al., 2023a). Notably, the cell sentence representation (Levine et al., 2024) is introduced by transforming scRNA-seq data into textual token sequences, which are widely used in subsequent studies (Hou and Ji, 2024; Choi et al., 2024). However, cell sentences have substantial information loss, constraining the models capacity to perceive fine-grained cellular transcriptomics. Scientific Multimodal PLMs. Multimodal PLMs show remarkable potential for integrating data from various modalities (Li et al., 2023; Alayrac et al., 2022; Zhang et al., 2024), inspiring research for scientific modalities. Existing works have constructed multimodal PLMs for small molecules (Liu et al., 2023b; Fang et al., 2024; Liu et al., 2024a) and proteins (Xu et al., 2023b; Liu et al., 2024b) to tackle cross-modal scientific problems, such as description generation, molecular property prediction and text-conditioned de novo design (Li et al., 2024c; Edwards et al., 2022; Liu et al., 2023c; Cao et al., 2025; Liu et al., 2025; Luo et al., 2024b). While single-cell analysis presents similar scientific significance, existing works struggle to maintain information fidelity when integrating single-cell transcriptomics with textual knowledge. Unlike previous methods, scMMGPT employs PLMs on both modalities to precisely model the scRNA-seq data and textual tokens without information loss."
        },
        {
            "title": "3 Methods",
            "content": "scMMGPT employs two PLMs for both cell and text modalities, facilitating the understanding and generation of cell and text through effective information sharing via cell-to-text and text-to-cell projectors. Figure 3 illustrates the model architecture of scMMGPT. In this section, we delve into the construction process of scMMGPT, including detailed data collection and encoding (3.1), the multimodal PLMs of scMMGPT (3.2), and the complete pre-training scheme (3.3). 3.1 Data Preprocessing Single-cell RNA sequencing (scRNA-seq) data records the gene expression levels across individual cells at transcriptomic resolution. These data can be represented as cell-gene expression matrix NN , where Xij (1 N, 1 ) denotes the RNA abundance of gene in cell i. To accurately characterize the transcriptional state of cell, we represent each cell using the list of genes profiled during sequencing and their corresponding expression values. Formally, for each cell i, we represent the list of genes as g(i) = [g(i) 1 , g(i) ], where each g(i) is gene token from pre-defined gene vocabj ulary (Cui et al., 2024). The corresponding RNA abundance information from the gene expression matrix is denoted as an expression value vector x(i) = [x(i) ] RM . To mitigate the influence of sequencing depths (Zhang et al., 2020), the expression value vectors then undergo normalization step followed by log1p transformation: 2 , . . . , x(i) 2 , . . . , g(i) 1 , x(i) (cid:101)x(i) = log(1 + x(i) k=1 x(i) (cid:80)M ). (1) Figure 3: Overview of scMMGPT. scMMGPT utilizes cell PLM and text PLM to process corresponding modalities, undergoing large-scale pre-training on three primary tasks: (1) Cell-Text Representation Alignment: scMMGPT receives inputs from both modalities and calculates relevance score based on the output features of both PLMs. (2) Cell Generation: scMMGPT uses the text PLM to extract embeddings from the textual descriptions, which are then passed through projector and cross-attention layer to the cellular PLM for cell generation. (3) Text Generation: scMMGPT uses the cell PLM to parse the genes and expression values from scRNA-seq results. After projector, these cellular embeddings are fed to the text PLM to decode as textual description of the cell. We leverage the scRNA-seq data from the CellxGene Database (Program et al., 2025), and collect the metadata and textual descriptions for each cell via their online explorer1. We further augment these descriptions with textual knowledge from the Open Biomedical Ontologies Foundry (OBO Foundry) (Smith et al., 2007) and Wikipedia2. These textual descriptions are then processed using text tokenizer into textual tokens sequence t(i) = [t(i) 1 , t(i) ]. The dataset details are in Appendix A. 2 , . . . , t(i) et al., 2024). Text PLM. To facilitate high-quality text generation, we utilize decoder-only generative transformer, Llama-2 (Touvron et al., 2023b), as our text PLM. This model is pre-trained on 2 trillion tokens of publicly available web data, incorporating extensive human knowledge across diverse domains. Cross-Modal Projectors. We employ cell-totext and text-to-cell projector to achieve representation transformation between the cellular and textual PLMs to bridge their modality gap: 3.2 Model Architecture As shown in Figure 3, scMMGPT consists of three trainable components: (1) cell PLM for the understanding and generation of cells, (2) text PLM for the understanding and generation of textual data, and (3) the cross-modal projectors that facilitate information sharing between different modalities. Cell PLM. We utilize transformer-based cell PLM, scGPT (Cui et al., 2024), to process the gene tokens and expression values unique to scRNAseq data. scGPT undergoes generative pre-training on over 33 million single-cell samples (Program et al., 2025), with training objectives including gene expression prediction and cell generation. This model encapsulates rich knowledge in the domain of single-cell analysis and has been validated for downstream tasks of cell type annotation and text-conditioned pseudo-cell generation (Levine 1https://cellxgene.cziscience.com/ 2https://www.wikipedia.org/ The Cell-to-Text Projector is implemented as Querying-Transformer (Q-Former) (Li et al., 2023) to map the cell representations generated by the cell PLM into the input space of the text PLM. Q-Former maintains set of trainable query tokens that interact with the output embeddings of the cell PLM through crossattention mechanism. The parameters of the QFormer are initialized using BiomedBERT (Gu et al., 2021), BERT encoder trained with biomedical scientific abstracts and literature from PubMed (Canese and Weis, 2013). The Text-to-Cell Projector is implemented by cross-attention layers (Vaswani, 2017) to map the textual representations produced by the text PLM into the feature space of the cell PLM. These textual features then serve as soft-prompt (Li and Liang, 2021) to the cell PLM, providing conditions for downstream tasks such as textconditioned pseudo-cell generation. 3.3 Training Objectives The training objective of scMMGPT is to bridge the embedding spaces of scRNA-seq and text data, thereby enabling it to perform cross-modal alignment and translation. Cell-Text Representation Alignment. The celltext representation alignment objective aims to establish shared embedding space between cellular data and the text descriptions. The Cell-Text Contrastive (CTC) objective aims to map cells and text into shared feature space. Using the matching cell and text as the positive examples, we use other cells within the same batch as negative examples, and construct the loss function based on InfoNCE (Oord et al., 2018): LCTC = 1 (cid:88) i=1 )/τ (cid:80)B (log ecos (z(i) ,z(i) j=1 ecos (z(i) ecos (z(i) ,z(i) )/τ j=1 ecos (z(i) ,z(j) )/τ ), ,z(j) )/τ (2) + log (cid:80)B and z(i) where z(i) represent the textual and cellular embeddings, τ is the temperature parameter, and indicates batch size. The Cell-Text Matching (CTM) objective is designed as classification task, where the model learns to predict whether given cell and text are matched or not. We let the textual representations interact with cell representations through crossattention layers in our Q-Former projector. Formally, the cell-text matching loss for batch can be expressed as: LCTM = 1 EjU(1,B) (cid:88) k= log ρ(c(i), t(i)) (3) + log ρ(c(i), t(j)), where U() is the uniform distribution and ρ(c(i), t(i)) denotes the models predicted probability of (c(i), t(i)) being matched. The overall objective for cell-text representation alignment training combines LCTC and LCTM: LAlign = LCTC + LCTM. (4) Cell-Text Translation. To enable the crossmodal cell-text translation ability of scMMGPT, we design generative training objectives for the unified cell-text and text-cell translation, including cell description generation and pseudo-cell generation. The Cell Description Generation (CDGen) objective aims to generate the corresponding text descriptions for given cell. The cells are first mapped to the embedding space of text tokens using cellto-text projector. Then, the decoder-only text PLM performs autoregressive next token prediction starting from these cell tokens to generate description of the cell. Formally, the objective of this task is to minimize the loss function of the autoregressive language modeling: LCDGen = log p(t(i)c(i)) = (cid:88) l= log p(t(i) 1 , . . . , t(i) t(i) l1, c(i)). (5) Another generative task in cell-text translation is Pseudo-Cell Generation (PCGen), in which the model performs conditional cell generation based on textual descriptions. We append dummy cell tokens at the end of each cell description and use the text PLM to autoregressively generate the embeddings of these dummy tokens based on the text sequence ahead. These features are then fed into the cell PLM via cross-attention mechanism to generate pseudo-cells x(i) = [x(i) ]. Formally, we use Mean Squared Error (MSE) loss as the training objective for PCGen: 1 , . . . , x(i) LPCGen = (cid:88) j=1 MSE(x(i) , (cid:101)x(i) ). (6) During the cross-modal generative training, we jointly optimize LCDGen and LPCGen through linear combination of the two loss functions: LGen = LCDGen + LPCGen. (7) 3.4 Training Pipeline We conduct two-stage pre-training of scMMGPT on the 27 million pre-training corpus described in Section 4.1 to establish cross-modal understanding and generation capabilities. We then evaluate the model on downstream tasks after task-specific finetuning. For zero-shot evaluations, we directly test the model using pre-trained checkpoints without fine-tuning. Pre-training Stage 1: The first pre-training stage aims to establish basic alignment between cellular data and textual representations. Considering the intrinsic variability of cellular data, this stage focuses on aligning cellular features with the text embedding space. We optimize the parameters Figure 4: Summary of the cellular data used in the two-stage pre-training of scMMGPT. The dataset includes 27 million (M) single-cell transcriptomic profiles from diverse range of human organs and tissues of the cell PLM and the cell-to-text projector using LAlign. The model from this stage is directly used in cell type annotation. Pre-training Stage 2: Building on stage 1, we freeze the parameters of the cell PLM and cellto-text projector and train scMMGPT to develop cross-modal generation capabilities using the same corpus with LGen. In this phase, we optimize the text-to-cell projector via full fine-tuning, and the text PLM via LoRA (Hu et al., 2022) tuning. The resulting model supports cell description generation and text-conditional pseudo-cell generation tasks. Task-Specific Fine-Tuning: For each task, we fine-tune scMMGPT on the corresponding downstream training set, and use the fine-tuned checkpoints for evaluation. On cell description generation and text-conditional pseudo-cell generation tasks, we use LCDGen and LPCGen as optimization objectives respectively. For cell type annotation, we use LAlign as the objective."
        },
        {
            "title": "4 Experiments",
            "content": "We empirically evaluate scMMGPT on three downstream tasks: cell description generation, textconditioned pseudo-cell generation, and cell type annotation. Furthermore, we perform ablation studies to illustrate the impacts of different input formats and model architectures. Additionally, we visualize experimental outcomes and training details to provide better understanding of the model. 4.1 Experiment Setup Unless otherwise mentioned, we use the wholehuman checkpoint of scGPT to initialize the cell PLM, and Llama-2 7B (Touvron et al., 2023b) for the text PLM. The model is then pre-trained and fine-tuned according to Section 3.4. The pretraining stage 1 last for 5 epochs and the stage 2 last for 1 epoch. Unless otherwise specified, the text PLM is pre-trained and fine-tuned with LoRA (Hu et al., 2022) adapter, while the cell PLM and the projectors undergo full-parameter training. More implementation details are provided in Appendix B. Pre-training Dataset. We collect 60 million single-cell data from the CellxGene (Program et al., 2025) database for the pre-training of scMMGPT, including scRNA-seq matrices and corresponding metadata annotations. After data filtering and deduplication, we obtain dataset comprising 27 million single-cell transcriptomics that span diverse human tissues and organs, as illustrated in Figure 4. The textual descriptions of each cell are generated with cellular metadata and the OBO Foundry (Smith et al., 2007). We exclude the test sets of the downstream datasets and reserve 1,000 samples for validation, resulting in approximately 26.9 million cells for pre-training. Further details about data distribution and preprocessing protocols are recorded in Appendix A. Model GPT-2 Small GPT-2 Large Mistral-8X7B-Instruct Mistral-7B-Instruct GPT-3.5 Cell2Sentence Small Cell2Sentence Large MMD () 1.045 0.009 0.939 0.006 0.639 0.016 0.754 0.010 0.298 0.004 0.198 0.004 0.198 0.004 EMD () 0.752 0.004 0.701 0.016 0.544 0.005 0.584 0.004 0.490 0.008 0.414 0.006 0.413 0.002 T-Test () 1.31, = 0.896 -1.44, = 0.885 -1.20, = 0.233 -8.64, = 0.384 1.23, = 0.220 2.96, = 0.003* 2.85, = 0.004* KS-Test () 1.52, = 0.783 1.81, = 0.581 0.24, = 0.246 0.23, = 0.299 0.21, = 0.392 0.35, = 0.023* 0.36, = 0.014* scMMGPT 0.031 0.00284.34% 0.011 0.00097.34% 29.57, = 0.000*+937.54% 0.62, = 0.000*+72.22% Table 1: Results of cell description generation on the immune tissue (Domínguez Conde et al., 2022) dataset. Asterisks (*) denotes statistical significance (p 0.05). Baseline results are borrowed from (Levine et al., 2024). k-NN Accuracy Model scGEN scVI scDiffusion scGPT Cell2Sentence scMMGPT = = 5 = 10 = 25 0.2376 0.0112 0.2436 0.0062 0.2335 0.0125 0.1838 0.0086 0.2588 0.0061 0.2330 0.0093 0.2400 0.0064 0.2288 0.0111 0.1788 0.0169 0.2565 0.0060 0.2377 0.0053 0.2425 0.0034 0.2368 0.0067 0.1811 0.0149 0.2746 0. 0.2335 0.0041 0.2348 0.0032 0.2306 0.0049 0.1882 0.0071 0.2715 0.0070 0.2996 0.0065+0.04 0.2992 0.0055+0.04 0.2986 0.0038+0.02 0.2981 0.0051+0.03 Table 2: Results of text-conditioned pseudo-cell generation on the immune tissue dataset. The baseline results are borrowed from (Levine et al., 2024). 4.2 Cell Description Generation 4.3 Text-guided Pseudo-cell Generation The cell description generation task evaluates models ability to generate accurate and meaningful textual descriptions of cells provided their scRNAseq data. We perform fine-tuning and evaluation on the immune tissue (Domínguez Conde et al., 2022) dataset using baseline methods including GPT-2 (Radford et al., 2019), Mistral 7B (Jiang et al., 2023), Mixtral 8x7B (Jiang et al., 2024), GPT-3.5, and Cell2Sentence (Levine et al., 2024). To evaluate the generation quality, we compute the Maximum Mean Discrepancy (MMD) and Earth Movers Distance (EMD) between the textual embedding (Xiao et al., 2024) between predicted and ground truth descriptions. Additionally, we conduct T-test and Kolmogorov-Smirnov test (KStest) to statistically assess and confirm whether the generated descriptions are significantly closer to the original annotations compared to those from unrelated cell descriptions. The performances are shown in Table 1. Our model significantly outperforms all baselines, achieving an 84% reduction in MMD (0.031) and 97% reduction in EMD (0.011) compared to the best baseline model. scMMGPT also demonstrates lower standard deviations in both MMD and EMD compared to the baselines, suggesting greater robustness and consistency in its performance. The T-test and KS-test results further reveal highly significant p-values (p 0.05), indicating strong alignment between the generated and original descriptions. These results demonstrate scMMGPTs superior capability in understanding cellular states. We conduct cell generation experiments on the immune tissue (Domínguez Conde et al., 2022) dataset. We select several generative single-cell models as baselines, including scGen (Lotfollahi et al., 2019), scVI (Lopez et al., 2018), scDiffusion (Luo et al., 2024a), scGPT (Cui et al., 2024), and Cell2Sentence (Levine et al., 2024). Inspired by previous studies, we train simple k-Nearest Neighbors (k-NN) classifier on the test set to distinguish the generated cells. The classification accuracies under different values are reported to reflect the quality of the generated cells. The results are presented in Table 2. scMMGPT achieves state-of-the-art performance in text-conditioned pseudo-cell generation, significantly outperforming all baseline models across all k-NN accuracies (k=3,5,10,25). The consistently high accuracy and low standard deviations of scMMGPT demonstrate its robustness and effectiveness in bridging cellular and textual data. 4.4 Cell Type Annotation In the cell type annotation task, we evaluate the models ability to classify cells based on their scRNA-seq data and textual descriptions of specific cell types. We compare our model against two baseline methods BioTranslator (Xu et al., 2023a) and LangCell (Zhao et al., 2024) on the Tabula Sapiens (Consortium* et al., 2022) dataset. This dataset comprises 161 distinct human cell types, most of which are absent from our pre-training corpus. We report the classification accuracies under varying fine-tuning conditions, where the model is Zero-Shot Fine-tuned on 10% Types Fine-tuned on 20% Types Fine-tuned on 30% Types Model Acc@1 Acc@5 Acc@10 Acc@1 Acc@5 Acc@10 Acc@1 Acc@5 Acc@10 Acc@1 Acc@5 Acc@10 Random BioTranlator LangCell 0.6 - 28.6 3.1 - 69. 6.2 - 82.9 0.6 3.5 30.5 3.1 33.6 71.0 6.2 45.4 83.7 0.6 13.4 35.0 3.1 48.2 74. 6.2 63.5 86.4 0.6 13.7 38.2 3.1 50.6 83.0 6.2 68.6 92.1 scMMGPT 49.1+20.5 83.1+13.8 91.1+8. 55.7+25.2 89.2+18.2 96.0+12.3 59.7+24.8 90.4+15.8 96.8+10.4 60.9+22.7 93.6+10.6 98.4+6.4 Table 3: Results of cell type annotation (%) on the Tabula Sapiens (Consortium* et al., 2022) dataset. The models are fine-tuned on certain proportion of test cell types to evaluate their generalization performance. Acc@N denotes top-N accuracy. Model Cell Representation MMD () EMD () BLEU-2 () ROUGE-2 () scMMGPT (TinyLlama1.1B, w/o scGPT) scMMGPT (TinyLlama1.1B) scMMGPT (Llama-27B) Cell Sentence Expression Values Expression Values 0.104 0.074 0.031 0.023 0.021 0.011 45.79% 48.77% 77.32% 40.11% 42.03% 72.49% Table 4: Results of ablation studies on cell description generation task. We compare different cell representation methods (cell sentence v.s. expression values) and different backbone text LMs. fine-tuned on different proportions of test cell types (10%, 20%, and 30%). The experimental results are summarized in Table 3. In the zero-shot setting, scMMGPT achieves an Acc@1 of 49.1% and an Acc@5 of 83.1%, surpassing all the baseline models even in fine-tuning setting. As shown in the table, the accuracy of scMMGPT under zero-shot setting is even higher than many of the fine-tuned baseline results. As the proportion of fine-tuning cell types increases, scMMGPT consistently improves its performance across all metrics, reaching maximum Acc@1 of 60.9% when fine-tuned on 30% cell types, almost doubling the accuracy of the state-of-the-art models. These results demonstrate that scMMGPTs pre-trained knowledge of both cellular and textual data enables strong generalization to unseen cell types without additional fine-tuning. 4.5 Ablation Studies To systematically evaluate the impact of the cell PLM and different text PLM backbones on our models performance, we conduct comprehensive ablation studies in this section. Impact of the Cell PLM. To validate the effectiveness of the cell PLM within the scMMGPT framework, we conduct comparative experiment between two model configurations: (1) the full scMMGPT, which integrates both the cell PLM and the text model, and (2) text-only variant of scMMGPT, which excludes the cell PLM and use the text PLM only. Both models are trained using the same settings and evaluated on the task of cell description generation. As shown in Table 4, the full scMMGPT model significantly outperforms the text-only variant across multiple metrics, including BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). Specifically, the inclusion of the cell PLM improves BLEU-2 and ROUGE-2 scores by 3% and 2%, respectively, while reducing text distances between generated and ground truth descriptions. These results highlight the critical role of the cell PLM in capturing and leveraging detailed transcriptomic information. Impact of Different Text PLM Backbones. To investigate the influence of model size, we perform experiments with two different text LMs: (1) TinyLlama 1.1B and (2) Llama2-7B. As shown in Table 4, though using smaller LM causes performance drops of scMMGPT, its MMD and EMD scores still surpass those of the best baseline model, Cell2Sentence Large (see Table 1). 4.6 Visualization Influence of Different Cell Representation Methods. To further quantify the information loss in cell sentences, we conduct visualization experiment comparing cell sentence inputs with original expression values. Specifically, we train two separate MLPs with identical hyperparameters for cell type annotation on the PBMC10K3 dataset. As shown in Figure 2, the cell sentence representation leads to significant increase in error rate, particularly when distinguishing morphologically similar cell types such as dendritic cells and FCGR3A+ monocytes. Influence of Gene Set Size. We explore the effect of number of genes used for cell representation by selecting the top-k most highly expressed genes and their associated expression levels as input to scMMGPT. The relationship between scRNA-seq length and captioning performance is illustrated 3https://support.10xgenomics.com/ single-cell-multiome-atac-gex/datasets/1.0.0/ pbmc_granulocyte_sorted_10k Figure 5: UMAP visualization of scMMGPT embeddings for cells from different experimental batches. The result demonstrates the models ability to capture cell type distinctions while effectively mitigating batch effects. achieved by integrating cell PLM with text PLM through cross-modal projectors. Pre-trained on 27 million cells from the CELLxGENE dataset, scMMGPT demonstrates superior performances across various single-cell analysis tasks. Looking forward, we will expand scMMGPT to incorporate more species, and integrate other cell modalities, like scATAC-seq and CITE-seq. This expansion will enable scMMGPT to tackle more challenges of multi-omic integration (Lotfollahi et al., 2022), cross-omic translation (Liu et al., 2023a), and novel cell type discovery (Yang et al., 2022), further enhancing its utility in single-cell research."
        },
        {
            "title": "Limitations",
            "content": "One significant limitation of scMMGPT is that its pre-training data primarily sourced from the CELLxGENE (Program et al., 2025) dataset, which predominantly covers human tissues. This focus restricts scMMGPTs ability to incorporate knowledge about cells from non-human species, such as those from the widely-used mouse data (Franzén et al., 2019). Another major limitation is that scMMGPT exclusively explores transcriptomic information from cells, lacking integration with other single-cell sequencing modalities, such as scATAC-seq and CITE-seq (Liu et al., 2023a; Lin et al., 2022). This constraint limits the model to analyzing RNA abundance alone, omitting critical perspectives on chromatin accessibility and protein expression within cells. Incorporating these additional modalities could provide more comprehensive understanding of cellular states and functions. Figure 6: Performance of cell description generation across different numbers of genes used for cell representation. The x-axis represents the number of genes (log scale). Solid lines represent BLEU and ROUGE scores corresponding to the left axis. Dashed lines represent the text distances corresponding to the right axis. in Figure 6, where ranges from 25 to 211. The results show consistent improvement in performance as the input length increases, confirming that more detailed transcriptomic information positively impacts model predictions. Batch Effect Mitigation in scMMGPT Embeddings. In wet lab experiments, it is challenging to maintain identical experimental conditions across different batches, which can lead to variations in the measured scRNA-seq data. We analyze two sets of immune tissue samples from different experimental batches, each containing ten randomly selected cell types. We compute scMMGPT embeddings for these samples and visualize them using UMAP, as shown in Figure 5. The results demonstrate that cell embeddings from scMMGPT effectively capture cell type differences while minimizing the influence of batch effects."
        },
        {
            "title": "5 Conclusion and Future Works",
            "content": "In this work, we propose scMMGPT, novel multimodal framework for single-cell analysis. scMMGPT bridges scRNA-seq data and text to support tasks of cell description generation, text-guided cell generation, and cell type annotation. This is"
        },
        {
            "title": "References",
            "content": "Farzad Abdolhosseini, Behrooz Azarkhalili, Abbas Maazallahi, Aryan Kamal, Seyed Abolfazl Motahari, Ali Sharifi-Zarchi, and Hamidreza Chitsaz. 2019. Cell identity codes: understanding cell identity from gene expression profiles using deep neural networks. Scientific reports, 9(1):2342. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736. Kathi Canese and Sarah Weis. 2013. Pubmed: the bibliographic database. The NCBI handbook, 2(1). He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. 2025. Instructmol: Multi-modal integration for building versatile and reliable molecular assistant in drug discovery. In Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025, pages 354379. Yuan Cao, Junjie Zhu, Peilin Jia, and Zhongming Zhao. 2017. scrnaseqdb: database for rna-seq based gene expression profiles in human single cells. Genes, 8(12):368. Zhi-Jie Cao and Ge Gao. 2022. Multi-omics singlecell data integration and regulatory inference with graph-linked embedding. Nature Biotechnology, 40(10):14581466. Yiqun Chen and James Zou. 2023. Genept: simple but effective foundation model for genes and cells built from chatgpt. bioRxiv, pages 202310. Hongyoon Choi, Jeongbin Park, Sumin Kim, Jiwon Kim, Dongjoo Lee, Sungwoo Bae, Haenara Shin, and Daeseung Lee. 2024. Cellama: Foundation model for single cell and spatial transcriptomics by cell embedding leveraging language model abilities. bioRxiv, pages 202405. The Tabula Sapiens Consortium*, Robert Jones, Jim Karkanias, Mark Krasnow, Angela Oliveira Pisco, Stephen Quake, Julia Salzman, Nir Yosef, Bryan Bulthaup, Phillip Brown, et al. 2022. The tabula sapiens: multiple-organ, single-cell transcriptomic atlas of humans. Science, 376(6594):eabl4896. Haotian Cui, Chloe Wang, Hassaan Maan, Kuan Pang, Fengning Luo, Nan Duan, and Bo Wang. 2024. scgpt: toward building foundation model for single-cell multi-omics using generative ai. Nature Methods, pages 111. Domínguez Conde, Xu, LB Jarvis, DB Rainbow, SB Wells, Gomes, SK Howlett, Suchanek, Polanski, HW King, et al. 2022. Cross-tissue immune cell analysis reveals tissue-specific features in humans. Science, 376(6594):eabl5197. Carl Edwards, Tuan Manh Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. 2022. Translation between molecules and natural language. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 375413. Junfeng Fang, Shuai Zhang, Chang Wu, Zhengyi Yang, Zhiyuan Liu, Sihang Li, Kun Wang, Wenjie Du, and Xiang Wang. 2024. Moltc: Towards molecular relational modeling in language models. arXiv preprint arXiv:2402.03781. Oscar Franzén, Li-Ming Gan, and Johan LM Björkegren. 2019. Panglaodb: web server for exploration of mouse and human single-cell rna sequencing data. Database, 2019:baz046. Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):123. Minsheng Hao, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi Cheng, Taifeng Wang, Jianzhu Ma, Xuegong Zhang, and Le Song. 2024. Largescale foundation model on single-cell transcriptomics. Nature Methods, pages 111. Lukas Heumos, Anna Schaar, Christopher Lance, Anastasia Litinetskaya, Felix Drost, Luke Zappia, Malte Lücken, Daniel Strobl, Juan Henao, Fabiola Curion, et al. 2023. Best practices for single-cell analysis across modalities. Nature Reviews Genetics, 24(8):550572. Wenpin Hou and Zhicheng Ji. 2024. Assessing gpt-4 for cell type annotation in single-cell rna-seq analysis. Nature Methods, pages 14. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana Davuluri. 2021a. Dnabert: pre-trained bidirectional encoder representations from transformers model for dna-language in genome. Bioinformatics, 37(15):21122120. Yuge Ji, Mohammad Lotfollahi, Alexander Wolf, and Fabian Theis. 2021b. Machine learning for perturbational single-cell omics. Cell Systems, 12(6):522 537. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088. Wei Lan, Guohang He, Mingyang Liu, Qingfeng Chen, Junyue Cao, and Wei Peng. 2024. Transformer-based single-cell language model: survey. Big Data Mining and Analytics, 7(4):11691186. Daniel Levine, Syed Rizvi, Sacha Lévy, Nazreen Pallikkavaliyaveetil, David Zhang, Xingyu Chen, Sina Ghadermarzi, Ruiming Wu, Zihe Zheng, Ivan Vrkic, et al. 2024. Cell2sentence: Teaching large language models the language of biology. In Forty-first International Conference on Machine Learning. Cong Li, Qingqing Long, Yuanchun Zhou, and Meng Xiao. 2024a. screader: Prompting large language models to interpret scrna-seq data. arXiv preprint arXiv:2412.18156. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. In International conference on machine learning, pages 1973019742. Sihang Li, Jin Huang, Jiaxi Zhuang, Yaorui Shi, Xiaochen Cai, Mingjun Xu, Xiang Wang, Linfeng Zhang, Guolin Ke, and Hengxing Cai. 2024b. Scilitllm: How to adapt llms for scientific literature understanding. arXiv preprint arXiv:2408.15545. Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, and Qi Tian. 2024c. 3d-molm: Towards 3d molecule-text interpretation in language models. In ICLR. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP (1), pages 45824597. Association for Computational Linguistics. Chin-Yew Lin. 2004. Rouge: package for automatic In Text summarization evaluation of summaries. branches out, pages 7481. Yingxin Lin, Tung-Yu Wu, Sheng Wan, Jean YH Yang, Wing Wong, and YX Rachel Wang. 2022. scjoint integrates atlas-scale single-cell rna-seq and atac-seq data with transfer learning. Nature biotechnology, 40(5):703710. Linjing Liu, Wei Li, Ka-Chun Wong, Fan Yang, and Jianhua Yao. 2023a. pre-trained large generative model for translating single-cell transcriptome to proteome. bioRxiv, pages 202307. Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. 2023b. Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1562315638. Zhiyuan Liu, Yanchen Luo, Han Huang, Enzhi Zhang, Sihang Li, Junfeng Fang, Yaorui Shi, Xiang Wang, Kenji Kawaguchi, and Tat-Seng Chua. 2025. NEXTMOL: 3d diffusion meets 1d language modeling for 3d molecule generation. In The Thirteenth International Conference on Learning Representations. Zhiyuan Liu, Yaorui Shi, An Zhang, Sihang Li, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, and Tat-Seng Chua. 2024a. Reactxt: Understanding molecular \"reaction-ship\" via reaction-contextualized moleculeIn Findings of the Association text pretraining. for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 53535377. Zhiyuan Liu, Yaorui Shi, An Zhang, Enzhi Zhang, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. 2023c. Rethinking tokenizer and decoder in masked graph modeling for molecules. In NeurIPS. Zhiyuan Liu, An Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, and Tat-Seng Chua. 2024b. Prott3: Protein-to-text generation for text-based protein understanding. In ACL (1), pages 59495966. Association for Computational Linguistics. Romain Lopez, Jeffrey Regier, Michael Cole, Michael Jordan, and Nir Yosef. 2018. Deep generative modeling for single-cell transcriptomics. Nature methods, 15(12):10531058. Mohammad Lotfollahi, Anastasia Litinetskaya, and Fabian Theis. 2022. Multigrate: single-cell multiomic data integration. BioRxiv, pages 202203. Mohammad Lotfollahi, Alexander Wolf, and Fabian Theis. 2019. scgen predicts single-cell perturbation responses. Nature methods, 16(8):715721. Erpai Luo, Minsheng Hao, Lei Wei, and Xuegong Zhang. 2024a. scdiffusion: conditional generation of high-quality single-cell data using diffusion model. Bioinformatics, 40(9):btae518. Yanchen Luo, Junfeng Fang, Sihang Li, Zhiyuan Liu, Jiancan Wu, An Zhang, Wenjie Du, and Xiang Wang. 2024b. Text-guided small molecule generation via diffusion model. iScience, 27(11). Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748. OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th international ACM SIGIR conference on research and development in information retrieval, pages 641649. Hanwen Xu, Addie Woicik, Hoifung Poon, Russ Altman, and Sheng Wang. 2023a. Multilingual translation for zero-shot biomedical classification using biotranslator. Nature Communications, 14(1):738. Minghao Xu, Xinyu Yuan, Santiago Miret, and Jian Tang. 2023b. Protst: Multi-modality learning of In Interprotein sequences and biomedical texts. national Conference on Machine Learning, pages 3874938767. Fan Yang, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lu, and Jianhua Yao. 2022. scbert as large-scale pretrained deep language model for cell type annotation of single-cell rna-seq data. Nat. Mac. Intell., 4(10):852866. Duzhen Zhang, Yahan Yu, Jiahua Dong, Chenxing Li, Dan Su, Chenhui Chu, and Dong Yu. 2024. Mmllms: Recent advances in multimodal large language models. arXiv preprint arXiv:2401.13601. Martin Jinye Zhang, Vasilis Ntranos, and David Tse. 2020. Determining sequencing depth in singlecell rna-seq experiment. Nature communications, 11(1):774. Suyuan Zhao, Jiahuan Zhang, and Zaiqing Nie. 2023. Large-scale cell representation learning via divideand-conquer contrastive learning. arXiv preprint arXiv:2306.04371. Suyuan Zhao, Jiahuan Zhang, Yushuai Wu, Yizhen Luo, and Zaiqing Nie. 2024. Langcell: Language-cell pretraining for cell identity understanding. In Forty-first International Conference on Machine Learning. Christoph Ziegenhain, Beate Vieth, Swati Parekh, Björn Reinius, Amy Guillaumet-Adkins, Martha Smets, Heinrich Leonhardt, Holger Heyn, Ines Hellmann, and Wolfgang Enard. 2017. Comparative analysis of single-cell rna sequencing methods. Molecular cell, 65(4):631643. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318. CZI Cell Science Program, Shibla Abdulla, Brian Aevermann, Pedro Assis, Seve Badajoz, Sidney Bell, Emanuele Bezzi, Batuhan Cakir, Jim Chaffer, Signe Chambers, et al. 2025. Cz cellxgene discover: single-cell data platform for scalable exploration, analysis and modeling of aggregated data. Nucleic Acids Research, 53(D1):D886D900. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Antoine-Emmanuel Saliba, Alexander Westermann, Stanislaw Gorski, and Jörg Vogel. 2014. Singlecell rna-seq: advances and future challenges. Nucleic acids research, 42(14):88458860. Alex Shalek, Rahul Satija, Joe Shuga, John Trombetta, Dave Gennert, Diana Lu, Peilin Chen, Rona Gertner, Jellert Gaublomme, Nir Yosef, et al. 2014. Single-cell rna-seq reveals dynamic paracrine control of cellular variation. Nature, 510(7505):363369. Yaorui Shi, Sihang Li, Taiyan Zhang, Xi Fang, Jiankun Wang, Zhiyuan Liu, Guojiang Zhao, Zhengdan Zhu, Zhifeng Gao, Renxin Zhong, et al. 2024. Intelligent system for automated molecular patent infringement assessment. arXiv preprint arXiv:2412.07819. Barry Smith, Michael Ashburner, Cornelius Rosse, Jonathan Bard, William Bug, Werner Ceusters, Louis Goldberg, Karen Eilbeck, Amelia Ireland, Christopher Mungall, et al. 2007. The obo foundry: coordinated evolution of ontologies to support biomedical data integration. Nature biotechnology, 25(11):12511255. Christina Theodoris, Ling Xiao, Anant Chopra, Mark Chaffin, Zeina Al Sayed, Matthew Hill, Helene Mantineo, Elizabeth Brydon, Zexian Zeng, Shirley Liu, et al. 2023. Transfer learning enables predictions in network biology. Nature, 618(7965):616624. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288."
        },
        {
            "title": "A Details of Datasets",
            "content": "A.1 Collection of the Pre-training Dataset A.1.1 Cell Transcriptomics Collection The pre-training dataset for scMMGPT is constructed using publicly available data from the CellxGene database (Program et al., 2025), with snapshot taken on July 1, 2024. The dataset undergoes series of filtering steps to ensure quality and consistency: and English Wikipedia. For each cell in the pretraining dataset, we first identify its biological classification (e.g., \"Tendon Cell\"). These classifications are then mapped to formal definitions in OBO Foundrys Cell Ontology, which provides machinereadable terms for cell types. Additionally, we supplement these definitions with detailed explanations extracted from relevant Wikipedia entries, enriching the textual descriptions with accessible and comprehensive context. We retain only human single-cell RNA sequencing (scRNA-seq) data, excluding entries from other species. We focus on data generated using the 10X Genomics platform, as its standardized outputs minimize technical variability across datasets. We deduplicate the dataset by keeping only one copy of each unique cell. To prevent information leakage, we remove all cells that appear in the test sets of downstream evaluation datasets. After these filtering steps, the final dataset comprises approximately 27 million cells from 344 categories and 60697 different genes spanning diverse human tissues, including brain, lung, heart, blood, pancreas, kidney, pan-cancer, and others. Table 5 shows the statistics of the dataset before and after the filtering. Table 5: Dataset statistics before and after data filtering. Tissue/Category Pre-filtering Post-filtering Brain Lung Pancreas Pan-cancer Kidney Heart Blood Others Total 22 3.3 0.22 4.4 1.0 2.2 5.4 22 60.5 7.5 1.2 0.08 2.6 0.35 0.7 4.2 10.3 26.9 A.1.2 Textual Description Collection To ensure consistent and accurate cell-type annotations, we integrate standardized descriptions from two key resources: the Open Biomedical Ontologies Foundry (OBO Foundry) (Smith et al., 2007) Example Cell Description from the Open Biomedical Ontologies Foundry. Tendon Cell: An elongated fibrocyte that is part of tendon. the cytoplasm is stretched between the they have central collagen fibres of the tendon. cell nucleus with prominent nucleolus. tendon cells have well-developed rough endoplasmic reticulum and they are responsible for synthesis and turnover of tendon fibres and ground substance. Example Cell Description from Wikipedia. Tendon Cell: Tendon cells, or tenocytes, are elongated fibroblast type cells. The cytoplasm is stretched between the collagen fibres of the tendon. They have central cell nucleus with prominent nucleolus. Tendon cells have well-developed rough endoplasmic reticulum and they are responsible for synthesis and turnover of tendon fibres and ground substance. Tendon cells form connecting epithelial layer between the muscle and shell in molluscs. In gastropods, for example, the retractor muscles connect to the shell via tendon cells. Muscle cells are attached to the collagenous myo-tendon space via hemidesmosomes. The myo-tendon space is then attached to the base of the tendon cells via basal hemidesmosomes, while apical hemidesmosomes, which sit atop microvilli, attach the tendon cells to thin layer of collagen. This is in turn attached to the shell via organic fibres which insert into the shell. Molluscan tendon cells appear columnar and contain large basal cell nucleus. The cytoplasm is filled with granular endoplasmic reticulum and sparse golgi. Dense bundles of microfilaments run the length of the cell connecting the basal to the apical hemidesmosomes. A.2 Collection of downstream Dataset We collected multiple benchmark datasets to evaluate the performance of the scMMGPT model in various downstream tasks. Immune Tissue (Domínguez Conde et al., 2022): This comprehensive reference dataset profiles 360,000 human immune cells through single-cell RNA sequencing (scRNA-seq), systematically annotated with 35 distinct cell subtypes. Derived from 16 tissue types across 12 adult donors, it provides cross-tissue characterization of lymphocyte, myeloid, and stromal cell populations, establishing baseline for immunological studies. 3,346 captures PBMC10K4: Integrating two independent scRNA-seq studies of healthy human this peripheral blood mononuclear cells, resource exactively pressed genes across 9 defined cell types: cells, CD4+/CD8+ lymphocytes, CD14+/FCGR3A+ monocytes, dendritic cells, natural killer cells, megakaryocytes, and rare populations. The dataset serves as standardized benchmark for methodological validation in immunogenomics. Tabula Sapiens (Consortium* et al., 2022): Spanning 24 human organs with 483,152 single-cell profiles, this pan-tissue atlas identifies 161 rigorously validated cell types across epithelial, immune, endothelial, and stromal lineages. Incorporating demographic diversity through multi-ethnic donors, it establishes transcriptional baselines from bladder mucosa to vascular endothelial using unified scRNAseq protocols."
        },
        {
            "title": "B Experimental Details",
            "content": "Parameter Gene vocab size Gene padding function Gene padding max len QFormer BERT hidden dim QFormer num_query_token QFormer cross_attention_freq Gene embed dim Cell projector dim Text projector dim Language model hidden size LM output max length Cell decoder attention layer Cell decoder attention head Value 60,697 High value 2,048 768 32 2 512 256 256 2,048 128 1 4 Table 6: Model Architecture Specifications B.1 Pre-Training Details The scMMGPT model employs multimodal pretraining framework that integrates gene expres4https://support.10xgenomics.com/ single-cell-multiome-atac-gex/datasets/1.0.0/ pbmc_granulocyte_sorted_10k Parameter Value Similarity function Cosine similarity AdamW Optimizer Linear Scheduler 1e-05 Max learning rate 1000 Warm up steps 0.001 Weight decay 12 Batch size Table 7: pre-train Experiment Configurations sion data with textual information. Inheriting scGPTs (Cui et al., 2024) architecture, the cell encoder utilizes gene vocabulary of 60,697 entries. For cellular input representation, we implement top-value alignment strategy that selects the 2,048 highest-expressed genes along with their expression values. Cross-modal alignment is achieved through Q-Former (Li et al., 2023) module with 32 query tokens, where the cross-attention mechanisms are activated every two layers. Pre-training was executed on eight NVIDIA 4090D GPUs over five epochs (1.4 million total steps), requiring approximately five days for completion. The optimization process employed AdamW with weight decay of 0.001 and peak learning rate of 105, modulated through linear warmup (1,000 steps from 106 minimum learning rate) followed by linear decay. We select 2 negative samples for each sample to calculate the InfoNCE (Oord et al., 2018) loss described by Formula 2. B.2 Downstream Training Details For the fine-tuning of downstream tasks, we conduct single-epoch training with constrained batch size of 4, preserving the AdamW optimizer configuration in the pre-training stage. Language model adaptation employs Low-Rank Adaptation (LoRA) (Hu et al., 2022) with rankdecomposition dimension of 8, scaling factor α of 32, and dropout ratio of 0.1 for stochastic regularization during weight adaptation. For each downstream analysis dataset, we perform quality control by removing the ambiguous categories (e.g., \"Other\", \"Unknown\"). We establish symmetrical training pairs with strict 1:1 allocation between cellular generation and textual synthesis objectives. This balanced design promotes bidirectional cross-modal alignment while mitigating task dominance. Visualization of scRNA-seq Data To facilitate better understanding of scRNA-seq matrices, we select subset of cells from the Tabula Sapiens dataset for visualization. In wet-lab singlecell sequencing experiments, researchers measure the expression levels of predefined set of genes across individual cells. Each value in the matrix represents the expression level of corresponding gene within single cell. The colors in the heatmap indicate the log1p-transformed expression levels. Figure 7: Visualization of single-cell RNA sequencing matrix. Rows represent individual cells, and columns represent genes. The color intensity corresponds to the log1p-transformed expression levels, with darker shades indicating higher expression."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "University of Science and Technology of China"
    ]
}