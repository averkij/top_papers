{
    "paper_title": "Simplifying, Stabilizing and Scaling Continuous-Time Consistency Models",
    "authors": [
        "Cheng Lu",
        "Yang Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Consistency models (CMs) are a powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose a simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512x512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 64x64, and 1.88 on ImageNet 512x512, narrowing the gap in FID scores with the best existing diffusion models to within 10%."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 1 ] . [ 1 1 8 0 1 1 . 0 1 4 2 : r SIMPLIFYING, STABILIZING & SCALING CONTINUOUSTIME CONSISTENCY MODELS Cheng Lu & Yang Song OpenAI"
        },
        {
            "title": "ABSTRACT",
            "content": "Consistency models (CMs) are powerful class of diffusion-based generative models optimized for fast sampling. Most existing CMs are trained using discretized timesteps, which introduce additional hyperparameters and are prone to discretization errors. While continuous-time formulations can mitigate these issues, their success has been limited by training instability. To address this, we propose simplified theoretical framework that unifies previous parameterizations of diffusion models and CMs, identifying the root causes of instability. Based on this analysis, we introduce key improvements in diffusion process parameterization, network architecture, and training objectives. These changes enable us to train continuous-time CMs at an unprecedented scale, reaching 1.5B parameters on ImageNet 512512. Our proposed training algorithm, using only two sampling steps, achieves FID scores of 2.06 on CIFAR-10, 1.48 on ImageNet 6464, and 1.88 on ImageNet 512512, narrowing the gap in FID scores with the best existing diffusion models to within 10%."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021b) have revolutionized generative AI, achieving remarkable results in image (Rombach et al., 2022; Ramesh et al., 2022; Ho et al., 2022), 3D (Poole et al., 2022; Wang et al., 2024; Liu et al., 2023b), audio (Liu et al., 2023a; Evans et al., 2024), and video generation (Blattmann et al., 2023; Brooks et al., 2024). Despite their success, significant drawback is their slow sampling speed, often requiring dozens to hundreds of steps to generate single sample. Various diffusion distillation techniques have been proposed, including direct distillation (Luhman & Luhman, 2021; Zheng et al., 2023b), adversarial distillation (Wang et al., 2022; Sauer et al., 2023), progressive distillation (Salimans & Ho, 2022), and variational score distillation (VSD) (Wang et al., 2024; Yin et al., 2024b;a; Luo et al., 2024; Xie et al., 2024b; Salimans et al., 2024). However, these methods come with challenges: direct distillation incurs extensive computational cost due to the need for numerous diffusion model samples; adversarial distillation introduces complexities associated with GAN training; progressive distillation requires multiple training stages and is less effective for one or two-step generation; and VSD can produce overly smooth samples with limited diversity and struggles at high guidance levels. Figure 1: Sample quality vs. effective sampling compute (billion parameters number of function evaluations during sampling). We compare the sample quality of different models on ImageNet 512512, measured by FID (). Our 2-step sCM achieves sample quality comparable to the best previous generative models while using less than 10% of the effective sampling compute. Consistency models (CMs) (Song et al., 2023; Song & Dhariwal, 2023) offer significant advantages in addressing these issues. They eliminate the need for supervision from diffusion model samples, avoiding the computational cost of generating synthetic datasets. CMs also bypass adversarial training, sidestepping its inherent difficulties. Aside from distillation, CMs can be trained from scratch with consistency training (CT), without relying on pre-trained diffusion models. Previous work (Song & Dhariwal, 2023; Geng et al., 2024; Luo et al., 2023; Xie et al., 2024a) has demonstrated the 1 Figure 2: Selected 2-step samples from continuous-time consistency model trained on ImageNet 512512. effectiveness of CMs in few-step generation, especially in one or two steps. However, these results are all based on discrete-time CMs, which introduces discretization errors and requires careful scheduling of the timestep grid, potentially leading to suboptimal sample quality. In contrast, continuous-time CMs avoid these issues but have faced challenges with training instability (Song et al., 2023; Song & Dhariwal, 2023; Geng et al., 2024). In this work, we introduce techniques to simplify, stabilize, and scale up the training of continuoustime CMs. Our first contribution is TrigFlow, new formulation that unifies EDM (Karras et al., 2022; 2024) and Flow Matching (Peluchetti, 2022; Lipman et al., 2022; Liu et al., 2022; Albergo et al., 2023; Heitz et al., 2023), significantly simplifying the formulation of diffusion models, the associated probability flow ODE and CMs. Building on this foundation, we analyze the root causes of instability in CM training and propose complete recipe for mitigation. Our approach includes improved time-conditioning and adaptive group normalization within the network architecture. Additionally, we re-formulate the training objective for continuous-time CMs, incorporating adaptive weighting and normalization of key terms, and progressive annealing for stable and scalable training. With these improvements, we elevate the performance of consistency models in both consistency training and distillation, achieving comparable or better results compared to previous discrete-time formulations. Our models, referred to as sCMs, demonstrate success across various datasets and model sizes. We train sCMs on CIFAR-10, ImageNet 6464, and ImageNet 512512, reaching an unprecedented scale with 1.5 billion parametersthe largest CMs trained to date (samples in Figure 2). We show that sCMs scale effectively with increased compute, achieving better sample 2 quality in predictable way. Moreover, when measured against state-of-the-art diffusion models, which require significantly more sampling compute, sCMs narrow the FID gap to within 10% using two-step generation. In addition, we provide rigorous justification for the advantages of continuoustime CMs over discrete-time variants by demonstrating that sample quality improves as the gap between adjacent timesteps narrows to approach the continuous-time limit. Furthermore, we examine the differences between sCMs and VSD, finding that sCMs produce more diverse samples and are more compatible with guidance, whereas VSD tends to struggle at higher guidance levels."
        },
        {
            "title": "2.1 DIFFUSION MODELS",
            "content": "Given training dataset, let pd denote its underlying data distribution and σd its standard deviation. Diffusion models generate samples by learning to reverse noising process that progressively perturbs data sample x0 pd into noisy version xt = αtx0 + σtzt, where zt (0, I) is standard Gaussian noise. This perturbation increases with [0, ], where larger indicates greater noise. We consider two recent formulations for diffusion models. EDM (Karras et al., 2022; 2024). The noising process simply sets αt = 1 and σt = t, with the (cid:105) (cid:104) w(t) fθ(xt, t) x02 training objective given by Ex0,z,t , where w(t) is weighting function. 2 The diffusion model is parameterized as fθ(xt, t) = cskip(t)xt + cout(t)Fθ(cin(t)xt, cnoise(t)), where Fθ is neural network with parameters θ, and cskip, cout, cin, and cnoise are manually designed coefficients that ensure the training objective has the unit variance across timesteps at initialization. For sampling, EDM solves the probability flow ODE (PF-ODE) (Song et al., 2021b), defined by dxt dt = [xt fθ(xt, t)]/t, starting from xT (0, 2I) and stopping at x0. Flow Matching. The noising process uses differentiable coefficients αt and σt, with time derivatives denoted by α and σ (typically, αt = 1 and σt = t). The training objective is given by (cid:104) Ex0,z,t w(t) Fθ(xt, t) (α , where w(t) is weighting function and Fθ is neural network parameterized by θ. The sampling procedure begins at = 1 with x1 (0, I) and solves the probability flow ODE (PF-ODE), defined by dxt dt = Fθ(xt, t), from = 1 to = 0. tx0 + σ tz) (cid:105) 2 2.2 CONSISTENCY MODELS consistency model (CM) (Song et al., 2023; Song & Dhariwal, 2023) is neural network fθ(xt, t) trained to map the noisy input xt directly to the corresponding clean data x0 in one step, by following the sampling trajectory of the PF-ODE starting at xt. valid fθ must satisfy the boundary condition, fθ(x, 0) x. One way to meet this condition is to parameterize the consistency model as fθ(xt, t) = cskip(t)xt + cout(t)Fθ(cin(t)xt, cnoise(t)) with cskip(0) = 1 and cout(0) = 0. CMs are trained to have consistent outputs at adjacent time steps. Depending on how nearby time steps are selected, there are two categories of consistency models, as described below. Discrete-time CMs. The training objective is defined at two adjacent time steps with finite distance: Ext,t [w(t)d(fθ(xt, t), fθ (xtt, t))] , (1) where θ denotes stopgrad(θ), w(t) is the weighting function, > 0 is the distance between adjacent time steps, and d(, ) is metric function; common choices are ℓ2 loss d(x, y) = y2 2, Pseudo-Huber loss d(x, y) = (cid:112)x y2 2 + c2 for > 0 (Song & Dhariwal, 2023), and LPIPS loss (Zhang et al., 2018). Discretetime CMs are sensitive to the choice of t, and therefore 3 0 Figure 3: Discrete-time CMs (top & middle) vs. continuous-time CMs (bottom). Discretetime CMs suffer from discretization errors from numerical ODE solvers, causing imprecise predictions during training. In contrast, continuous-time CMs stay on the ODE trajectory by following its tangent direction with infinitesimal steps. require manually designed annealing schedules (Song & Dhariwal, 2023; Geng et al., 2024) for fast convergence. The noisy sample xtt at the preceding time step is often obtained from xt by solving the PF-ODE with numerical ODE solvers using step size t, which can cause additional discretization errors. Continuous-time CMs. When using d(x, y) = y2 (2023) show that the gradient of Eq. (1) with respect to θ converges to 2 and taking the limit 0, Song et al. θExt,t (cid:20) w(t)f θ (xt, t) dfθ (xt, t) dt (cid:21) , (2) dt = xtfθ (xt, t) dxt where dfθ (xt,t) dt + tfθ (xt, t) is the tangent of fθ at (xt, t) along the trajectory of the PF-ODE dxt dt . Notably, continuous-time CMs do not rely on ODE solvers, which avoids discretization errors and offers more accurate supervision signals during training. However, previous work (Song et al., 2023; Geng et al., 2024) found that training continuous-time CMs, or even discrete-time CMs with an extremely small t, suffers from severe instability in optimization. This greatly limits the empirical performance and adoption of continuous-time CMs. Consistency Distillation and Consistency Training. Both discrete-time and continuous-time CMs can be trained using either consistency distillation (CD) or consistency training (CT). In consistency distillation, CM is trained by distilling knowledge from pretrained diffusion model. This diffusion model provides the PF-ODE, which can be directly plugged into Eq. (2) for training continuous-time CMs. Furthermore, by numerically solving the PF-ODE to obtain xtt from xt, one can also train discrete-time CMs via Eq. (1). Consistency training (CT), by contrast, trains CMs from scratch without the need for pretrained diffusion models, which establishes CMs as standalone family of generative models in their own right. Specifically, CT approximates xtt in discrete-time CMs as xtt = αttx0 +σttz, reusing the same data x0 and noise when sampling xt = αtx0 +σtz. In the continuous-time limit, as 0, this approach yields an unbiased estimate of the PF-ODE dxt dt α tz, leading to an unbiased estimate of Eq. (2) for training continuous-time CMs. tx0 + σ"
        },
        {
            "title": "3 SIMPLIFYING CONTINUOUS-TIME CONSISTENCY MODELS",
            "content": "Previous consistency models (CMs) adopt the model parameterization and diffusion process formulation in EDM (Karras et al., 2022). Specifically, the CM is parameterized as fθ(xt, t) = cskip(t)xt + cout(t)Fθ(cin(t)xt, cnoise(t)), where Fθ is neural network with parameters θ. The coefficients cskip(t), cout(t), cin(t) are fixed to ensure that the variance of the diffusion objective is equalized across all time steps at initialization, and cnoise(t) is transformation of for better time conditioning. Since EDM diffusion process is variance-exploding (Song et al., 2021b), meaning that xt = x0 + tzt, we can derive that cskip(t) = σ2 d/(t2 + σ2 + t2, and cin(t) = 1/(cid:112)t2 + σ2 (see Appendix B.6 in Karras et al. (2022)). Although these coefficients are important for training efficiency, their complex arithmetic relationships with and σd complicate theoretical analyses of CMs. d), cout(t) = σd t/(cid:112)σ2 To simplify EDM and subsequently CMs, we propose TrigFlow, formulation of diffusion models that keep the EDM properties but satisfy cskip(t) = cos(t), cout(t) = sin(t), and cin(t) 1/σd (proof in Appendix B). TrigFlow is special case of flow matching (also known as stochastic interpolants or rectified flows) and v-prediction parameterization (Salimans & Ho, 2022). It closely resembles the trigonometric interpolant proposed by Albergo & Vanden-Eijnden (2023); Albergo et al. (2023); Ma et al. (2024), but is modified to account for σd, the standard deviation of the data distribution pd. Since TrigFlow is special case of flow matching and simultaneously satisfies EDM principles, it combines the advantages of both formulations while allowing the diffusion process, diffusion model parameterization, the PF-ODE, the diffusion training objective, and the CM parameterization to all have simple expressions, as provided below. Diffusion Process. Given x0 pd(x0) and (0, σ2 xt = cos(t)x0 + sin(t)z for [0, π dI), the noisy sample is defined as 2 ]. As special case, the prior sample π (0, σ dI). 2 Diffusion Models and PF-ODE. We parameterize the diffusion model as fθ(xt, t) = Fθ(xt/σd, cnoise(t)), where Fθ is neural network with parameters θ, and cnoise(t) is transfor4 mation of to facilitate time conditioning. The corresponding PF-ODE is given by dxt dt = σdFθ (cid:18) xt σd (cid:19) , cnoise(t) . Diffusion Objective. In TrigFlow, the diffusion model is trained by minimizing LDiff(θ) = Ex0,z,t (cid:34)(cid:13) (cid:13) (cid:13) (cid:13) σdFθ (cid:18) xt σd (cid:19) , cnoise (t) vt (cid:35) , (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (3) (4) where vt = cos(t)z sin(t)x0 is the training target. Consistency Models. As mentioned in Sec. 2.2, valid CM must satisfy the boundary condition fθ(x, 0) x. To enforce this condition, we parameterize the CM as the single-step solution of the PF-ODE in Eq. (3) using the first-order ODE solver (see Appendix B.1 for derivations). Specifically, CMs in TrigFlow take the form of fθ(xt, t) = cos(t)xt sin(t)σdFθ (cid:18) xt σd (cid:19) , cnoise(t) , (5) where cnoise(t) is time transformation for which we defer the discussion to Sec. 4.1."
        },
        {
            "title": "4 STABILIZING CONTINUOUS-TIME CONSISTENCY MODELS",
            "content": "Training continuous-time CMs has been highly unstable (Song et al., 2023; Geng et al., 2024). As result, they perform significantly worse compared to discrete-time CMs in prior works. To address this issue, we build upon the TrigFlow framework and introduce several theoretically motivated improvements to stabilize continuous-time CMs, with focus on parameterization, network architecture, and training objectives. 4.1 PARAMETERIZATION AND NETWORK ARCHITECTURE Key to the training of continuous-time CMs is Eq. (2), which depends on the tangent function dfθ (xt,t) dt . Under the TrigFlow formulation, this tangent function is given by dfθ(xt, t) dt (cid:18) = cos(t) σdFθ (cid:19) , (cid:18) xt σd (cid:19) dxt dt sin(t) xt + σd dFθ (cid:16) xt σd dt (cid:17) , , (6) where dxt dt represents the PF-ODE, which is either estimated using pretrained diffusion model in consistency distillation, or using an unbiased estimator calculated from noise and clean samples in consistency training. To stabilize training, it is necessary to ensure the tangent function in Eq. (6) is stable across different time steps. Empirically, we found that σdFθ , the PF-ODE dxt dt , and the noisy sample xt are all relatively stable. The only term left in the tangent function now is sin(t) dFθ dxt dt + dxt sin(t)tFθ. After further analysis, we found xtFθ dt is typically well-conditioned, so instability originates from the time-derivative sin(t)tFθ , which can be decomposed according to dt = sin(t)xtFθ sin(t)tFθ = sin(t) cnoise(t) emb(cnoise) cnoise Fθ emb(cnoise) , (7) where emb() refers to the time embeddings, typically in the form of either positional embeddings (Ho et al., 2020; Vaswani, 2017) or Fourier embeddings (Song et al., 2021b; Tancik et al., 2020) in the literature of diffusion models and CMs. Below we describe improvements to stabilize each component from Eq. (7) in turns. Identity Time Transformation (cnoise(t) = t). Most existing CMs use the EDM formulation, which can be directly translated to the TrigFlow formulation as described in Appendix B.2. In 5 EDM, Fourier scale = 16.0 EDM, positional embedding TrigFlow, positional embedding Figure 4: Stability of different formulations. We show the norms of both terms in dt + tfθ for diffusion models trained with the EDM (cnoise(t) = log(σd tan(t))) and TrigFlow (cnoise(t) = t) formulations using different time embeddings. We observe that large Fourier scales in Fourier embeddings cause instabilities. In addition, the EDM formulation suffers from numerical issues when π 2 , while TrigFlow (using positional embeddings) has stable partial derivatives for both xt and t. dt = xfθ dxt θ df (a) Tangent Normalization (b) Adaptive Weighting (c) Discrete vs. Continuous Figure 5: Comparing different training objectives for consistency distillation. The diffusion models are EDM2 (Karras et al., 2024) pretrained on ImageNet 512512. (a) 1-step and 2-step sampling of continuous- , 1, 1) and normalized tangents time CMs trained by using raw tangents df df θ θ ( dt + 0.1). (b) Quality of 1-step and 2-step samples from continuous-time CMs trained w/ and dt )/( w/o adaptive weighting, both are w/ tangent normalization. (c) Quality of 1-step samples from continuous-time CMs vs. discrete-time CMs using varying number of time steps (N ), trained using all techniques in Sec. 4. , clipped tangents clip( θ dt θ dt df df particular, the time transformation becomes cnoise(t) = log(σd tan t). Straightforward derivation shows that with this cnoise(t), sin(t) tcnoise(t) = 1/ cos(t) blows up whenever π 2 . To mitigate numerical instability, we propose to use cnoise(t) = as the default time transformation. Positional Time Embeddings. For general time embeddings in the form of emb(c) = sin(s 2πω + ϕ), we have cemb(c) = 2πω cos(s 2πω + ϕ). With larger Fourier scale s, this derivative has greater magnitudes and oscillates more vibrantly, causing worse instability. To avoid this, we use positional embeddings, which amounts to 0.02 in Fourier embeddings. This analysis provides principled explanation for the observations in Song & Dhariwal (2023). Adaptive Double Normalization. Song & Dhariwal (2023) found that the AdaGN layer (Dhariwal & Nichol, 2021), defined as = norm(x) s(t) + b(t), negatively impacts CM training. Our modification is adaptive double normalization, defined as = norm(x)pnorm(s(t))+pnorm(b(t)), where pnorm() denotes pixel normalization (Karras, 2017). Empirically we find it retains the expressive power of AdaGN for diffusion training but removes its instability in CM training. As shown in Figure 4, we visualize how our techniques stabilize the time-derivates for CMs trained on CIFAR-10. Empirically, we find that these improvements help stabilize the training dynamics of CMs without hurting diffusion model training (see Appendix G). 4.2 TRAINING OBJECTIVES Using the TrigFlow formulation in Sec. 3 and techniques proposed in Sec. 4.1, the gradient of continuous-time CM training in Eq. (2) becomes (cid:20) θExt,t w(t)σd sin(t)F θ (cid:18) xt σd (cid:19) dfθ (xt, t) dt , (cid:21) . 6 Below we propose additional techniques to explicitly control this gradient for improved stability. Tangent Normalization. As discussed in Sec. 4.1, most gradient variance in CM training comes from the tangent function dfθ . We propose to explicitly normalize the tangent function by replacing dt dt with dfθ dfθ dt + c), where we empirically set = 0.1. Alternatively, we can clip the tangent within [1, 1], which also caps its variance. Our results in Figure 5(a) demonstrate that either normalization or clipping leads to substantial improvements for the training of continuous-time CMs. dt /( dfθ Adaptive Weighting. Previous works (Song & Dhariwal, 2023; Geng et al., 2024) design weighting functions w(t) manually for CM training, which can be suboptimal for different data distributions and network architectures. Following EDM2 (Karras et al., 2024), we propose to train an adaptive weighting function alongside the CM, which not only eases the burden of hyperparameter tuning but also outperforms manually designed weighting functions with better empirical performance and 2 θE[Fθ negligible training overhead. Key to our approach is the observation that θE[F Fθ + y2 2], where is an arbitrary vector independent of θ. When training continuous-time CMs using Eq. (2), we have = w(t)σd sin(t) dfθ . This observation allows us to convert Eq. (2) into dt the gradient of an MSE objective. We can therefore use the same approach in Karras et al. (2024) to train an adaptive weighting function that minimizes the variance of MSE losses across time steps 1 (details in Appendix D). In practice, we find that integrating prior weighting w(t) = σd tan(t) further reduces training variance. By incorporating the prior weighting, we train both the network Fθ and the adaptive weighting function wϕ(t) by minimizing θ y] = 1 LsCM(θ, ϕ) := Ext,t (cid:34) ewϕ(t) (cid:13) (cid:13) (cid:13) (cid:13) Fθ (cid:18) xt σd (cid:19) , Fθ (cid:18) xt σd (cid:19) , cos(t) dfθ(xt, t) dt (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:35) wϕ(t) , (8) std) (details in Appendix G). where is the dimensionality of x0, and we sample tan(t) from log-Normal proposal distribution (Karras et al., 2022), that is, eσd tan(t) (Pmean, 2 Diffusion Finetuning and Tangent Warmup. For consistency distillation, we find that finetuning the CM from pretrained diffusion model can speed up convergence, which is consistent with Song et al. (2023); Geng et al. (2024). Recall that in Eq. (6), the tangent dfθ can be decomposed dt into two parts: the first term cos(t)(σdFθ dxt dt ) is relatively stable, whereas the second term dFθ sin(t)(xt + σd dt ) causes instability. To alleviate this problem, we gradually warm up the second term by replacing the coefficient sin(t) with sin(t), where linearly increases from 0 to 1 over the first 10k training iterations. With all techniques in place, the stability of both discrete-time and continuous-time CM training substantially improves. We provide detailed algorithms for discrete-time CMs in Appendix E, and train continuous-time CMs and discrete-time CMs with the same setting. As demonstrated in Figure 5(c), increasing the number of discretization steps in discrete-time CMs improves sample quality by reducing discretization errors, but degrades once becomes too large (after > 1024) to suffer from numerical precision issues. By contrast, continuous-time CMs significantly outperform discrete-time CMs across all which provides strong justification for choosing continuous-time CMs over discrete-time counterparts. We call our model sCM (s for simple, stable, and scalable), and provide detailed pseudo-code for sCM training in Appendix A."
        },
        {
            "title": "5 SCALING UP CONTINUOUS-TIME CONSISTENCY MODELS",
            "content": "Below we test all the improvements proposed in previous sections by training large-scale sCMs on variety of challenging datasets. 5.1 TANGENT COMPUTATION IN LARGE-SCALE MODELS The common setting for training large-scale diffusion models includes using half-precision (FP16) and Flash Attention (Dao et al., 2022; Dao, 2023). As training continuous-time CMs requires computing the tangent dfθ accurately, we need to improve numerical precision and also support dt memory-efficient attention computation, as detailed below. 7 ImageNet 64 64 ImageNet 512 512 (a) FID () as function of single forward flops. ImageNet 64 64 ImageNet 512 512 (b) FID ratio () as function of single forward flops. Figure 6: sCD scales commensurately with teacher diffusion models. We plot the (a) FID and (b) FID ratio against the teacher diffusion model (at the same model size) on ImageNet 6464 and 512512. sCD scales better than sCT, and has constant offset in the FID ratio across all model sizes, implying that sCD has the same scaling property of the teacher diffusion model. Furthermore, the offset diminishes with more sampling steps. (a) Precision score () (b) Recall score () (c) FID score () Figure 7: sCD has higher diversity compared to VSD: Sample quality comparison of the EDM2 (Karras et al., 2024) diffusion model, VSD (Wang et al., 2024; Yin et al., 2024b), sCD, and the combination of VSD and sCD, across varying guidance scales. All models are of EDM2-M size and trained on ImageNet 512512. involves calculating dFθ JVP Rearrangement. Computing dfθ dt + tFθ , dt which can be efficiently obtained via the Jacobian-vector product (JVP) for Fθ ( , ) with the input σd vector (xt, t) and the tangent vector ( dxt dt , 1). However, we empirically find that the tangent may overflow in intermediate layers when is near 0 or π 2 . To improve numerical precision, we propose to rearrange the computation of the tangent. Specifically, since the objective in Eq. (8) contains cos(t) dfθ dt is proportional to sin(t) dFθ dt dt = xtFθ dxt , we can compute the JVP as: and dfθ dt cos(t) sin(t) dFθ dt (cid:16) = xt σd Fθ (cid:17) (cid:18) cos(t) sin(t) (cid:19) dxt dt + tFθ (cos(t) sin(t)σd), which is the JVP for Fθ (, ) with the input ( xt , t) and the tangent (cos(t) sin(t) dxt dt , σd cos(t) sin(t)σd). This rearrangement greatly alleviates the overflow issues in the intermediate layers, resulting in more stable training in FP16. JVP of Flash Attention. Flash Attention (Dao et al., 2022; Dao, 2023) is widely used for attention computation in large-scale model training, providing both GPU memory savings and faster training. However, Flash Attention does not compute the Jacobian-vector product (JVP). To fill this gap, we propose similar algorithm (detailed in Appendix F) that efficiently computes both softmax selfattention and its JVP in single forward pass in the style of Flash Attention, significantly reducing GPU memory usage for JVP computation in attention layers. 5.2 EXPERIMENTS To test our improvements, we employ both consistency training (referred to as sCT) and consistency distillation (referred to as sCD) to train and scale continuous-time CMs on CIFAR-10 (Krizhevsky, 2009), ImageNet 6464 and ImageNet 512512 (Deng et al., 2009). We benchmark the sample quality using FID (Heusel et al., 2017). We follow the settings of Score SDE (Song et al., 2021b) on CIFAR10 and EDM2 (Karras et al., 2024) on both ImageNet 6464 and ImageNet 512512, while changing the parameterization and architecture according to Section 4.1. We adopt the method proposed by Song et al. (2023) for two-step sampling of both sCT and sCD, using fixed intermediate 8 Table 1: Sample quality on unconditional CIFAR-10 and class-conditional ImageNet 64 64. Unconditional CIFAR-10 METHOD Diffusion models & Fast Samplers Score SDE (deep) (Song et al., 2021b) EDM (Karras et al., 2022) Flow Matching (Lipman et al., 2022) DPM-Solver (Lu et al., 2022a) DPM-Solver++ (Lu et al., 2022b) DPM-Solver-v3 (Zheng et al., 2023c) Joint Training Diffusion GAN (Xiao et al., 2022) Diffusion StyleGAN (Wang et al., 2022) StyleGAN-XL (Sauer et al., 2022) CTM (Kim et al., 2023) Diff-Instruct (Luo et al., 2024) DMD (Yin et al., 2024b) SiD (Zhou et al., 2024) Diffusion Distillation DFNO (LPIPS) (Zheng et al., 2023b) 2-Rectified Flow (Liu et al., 2022) PID (LPIPS) (Tee et al., 2024) BOOT (LPIPS) (Gu et al., 2023) Consistency-FM (Yang et al., 2024) PD (Salimans & Ho, 2022) TRACT (Berthelot et al., 2023) CD (LPIPS) (Song et al., 2023) sCD (ours) Consistency Training iCT (Song & Dhariwal, 2023) iCT-deep (Song & Dhariwal, 2023) ECT (Geng et al., 2024) sCT (ours) Class-Conditional ImageNet 6464 NFE () FID () METHOD NFE () FID () 2000 35 142 10 10 4 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 2.20 2.01 6.35 4.70 2.91 2.51 3.75 3.19 1.52 1.87 4.53 3.77 1.92 3.78 4.85 3.92 4.38 5.34 8.34 5.58 3.78 3.32 3.55 2.93 3.66 2. 2.83 2.46 2.51 2.24 3.60 2.11 2.97 2.06 Diffusion models & Fast Samplers ADM (Dhariwal & Nichol, 2021) RIN (Jabri et al., 2022) DPM-Solver (Lu et al., 2022a) EDM (Heun) (Karras et al., 2022) EDM2 (Heun) (Karras et al., 2024) Joint Training StyleGAN-XL (Sauer et al., 2022) Diff-Instruct (Luo et al., 2024) EMD (Xie et al., 2024b) DMD (Yin et al., 2024b) DMD2 (Yin et al., 2024a) SiD (Zhou et al., 2024) CTM (Kim et al., 2023) Moment Matching (Salimans et al., 2024) Diffusion Distillation DFNO (LPIPS) (Zheng et al., 2023b) PID (LPIPS) (Tee et al., 2024) TRACT (Berthelot et al., 2023) PD (Salimans & Ho, 2022) (reimpl. from Heek et al. (2024)) CD (LPIPS) (Song et al., 2023) MultiStep-CD (Heek et al., 2024) sCD (ours) Consistency Training iCT (Song & Dhariwal, 2023) iCT-deep (Song & Dhariwal, 2023) ECT (Geng et al., 2024) sCT (ours) 250 1000 20 79 63 1 1 1 1 1 1 1 2 1 2 1 1 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 2.07 1.23 3.42 2.44 1.33 1.52 5.57 2.20 2.62 1.28 1.52 1.92 1.73 3.00 3. 7.83 9.49 7.43 4.97 10.70 4.70 6.20 4.70 3.20 1.90 2.44 1.66 4.02 3.20 3.25 2.77 2.49 1.67 2.04 1.48 time step = 1.1. For sCD models on ImageNet 512512, since the teacher diffusion model relies on classifier-free guidance (CFG) (Ho & Salimans, 2021), we incorporate an additional input into the model Fθ to represent the guidance scale (Meng et al., 2023). We train the model with sCD by uniformly sampling [1, 2] and applying the corresponding CFG to the teacher model during distillation (more details are provided in Appendix G). For sCT models, we do not test CFG since it is incompatible with consistency training. Training compute of sCM. We use the same batch size as the teacher diffusion model across all datasets. The effective compute per training iteration of sCD is approximately twice that of the teacher model. We observe that the quality of two-step samples from sCD converges rapidly, achieving results comparable to the teacher diffusion model using less than 20% of the teacher training compute. In practice, we can obtain high-quality samples after only 20k finetuning iterations with sCD. Benchmarks. In Tables 1 and 2, we compare our results with previous methods by benchmarking the FIDs and the number of function evaluations (NFEs). First, sCM outperforms all previous few-step methods that do not rely on joint training with another network and is on par with, or even exceeds, the best results achieved with adversarial training. Notably, the 1-step FID of sCD-XXL on ImageNet 512512 surpasses that of StyleGAN-XL (Sauer et al., 2022) and VAR (Tian et al., 2024a). Furthermore, the two-step FID of sCD-XXL outperforms all generative models except diffusion and is comparable with the best diffusion models that require 63 sequential steps. Second, the two-step sCM model significantly narrows the FID gap with the teacher diffusion model to within 10%, achieving FIDs of 2.06 on CIFAR-10 (compared to the teacher FID of 2.01), 1.48 on ImageNet 6464 (teacher FID of 1.33), and 1.88 on ImageNet 512512 (teacher FID of 1.73). Additionally, we observe that sCT is more effective at smaller scales but suffers from increased variance at larger scales, while sCD shows consistent performance across both small and large scales. Table 2: Sample quality on class-conditional ImageNet 512 512. Our reimplemented teacher diffusion model based on EDM2 (Karras et al., 2024) but with modifications in Sec. 4.1. METHOD Diffusion models ADM-G (Dhariwal & Nichol, 2021) RIN (Jabri et al., 2022) U-ViT-H/4 (Bao et al., 2023) DiT-XL/2 (Peebles & Xie, 2023) SimDiff (Hoogeboom et al., 2023) VDM++ (Kingma & Gao, 2024) DiffiT (Hatamizadeh et al., 2023) DiMR-XL/3R (Liu et al., 2024) DIFFUSSM-XL (Yan et al., 2024) DiM-H (Teng et al., 2024) U-DiT (Tian et al., 2024b) SiT-XL (Ma et al., 2024) Large-DiT (Alpha-VLLM, 2024) MaskDiT (Zheng et al., 2023a) DiS-H/2 (Fei et al., 2024a) DRWKV-H/2 (Fei et al., 2024b) EDM2-S (Karras et al., 2024) EDM2-M (Karras et al., 2024) EDM2-L (Karras et al., 2024) EDM2-XL (Karras et al., 2024) EDM2-XXL (Karras et al., 2024) GANs & Masked Models BigGAN (Brock, 2018) StyleGAN-XL (Sauer et al., 2022) VQGAN (Esser et al., 2021) MaskGIT (Chang et al., 2022) MAGVIT-v2 (Yu et al., 2023) MAR (Li et al., 2024) VAR-d36-s (Tian et al., 2024a) NFE () FID () #Params METHOD NFE () FID () #Params 2502 1000 2502 2502 5122 5122 2502 2502 2502 2502 250 2502 2502 792 2502 2502 632 632 632 632 63 1 12 1024 12 642 642 102 7.72 3.95 4.05 3.04 3.02 2.65 2.67 2.89 3.41 3.78 15.39 2.62 2.52 2.50 2.88 2.95 2.23 2.01 1.88 1.85 1.81 8.43 2.41 26.52 7.32 1.91 1.73 2.63 559M 320M 501M 675M 2B 2B 561M 525M 673M 860M 204M 675M 3B 736M 900M 779M 280M 498M 778M 1.1B 1.5B 160M 168M 227M 227M 307M 481M 2.3B Teacher Diffusion Model EDM2-S (Karras et al., 2024) EDM2-M (Karras et al., 2024) EDM2-L (Karras et al., 2024) EDM2-XL (Karras et al., 2024) EDM2-XXL (Karras et al., 2024) 632 632 632 632 632 Consistency Training (sCT, ours) sCT-S (ours) sCT-M (ours) sCT-L (ours) sCT-XL (ours) sCT-XXL (ours) Consistency Distillation (sCD, ours) sCD-S sCD-M sCD-L sCD-XL sCD-XXL 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 2.29 2.00 1.87 1.80 1.73 10.13 9.86 5.84 5.53 5.15 4.65 4.33 3.73 4.29 3. 3.07 2.50 2.75 2.26 2.55 2.04 2.40 1.93 2.28 1.88 280M 498M 778M 1.1B 1.5B 280M 280M 498M 498M 778M 778M 1.1B 1.1B 1.5B 1.5B 280M 280M 498M 498M 778M 778M 1.1B 1.1B 1.5B 1.5B Scaling study. Based on our improved training techniques, we successfully scale continuous-time CMs without training instability. We train various sizes of sCMs using EDM2 configurations (S, M, L, XL, XXL) on ImageNet 6464 and 512512, and evaluate FID under optimal guidance scales, as shown in Fig. 6. First, as model FLOPs increase, both sCT and sCD show improved sample quality, showing that both methods benefit from scaling. Second, compared to sCD, sCT is more compute efficient at smaller resolutions but less efficient at larger resolutions. Third, sCD scales predictably for given dataset, maintaining consistent relative difference in FIDs across model sizes. This suggests that the FID of sCD decreases at the same rate as the teacher diffusion model, and therefore sCD is as scalable as the teacher diffusion model. As the FID of the teacher diffusion model decreases with scaling, the absolute difference in FID between sCD and the teacher model also diminishes. Finally, the relative difference in FIDs decreases with more sampling steps, and the sample quality of the two-step sCD becomes on par with that of the teacher diffusion model. Comparison with VSD. Variational score distillation (VSD) (Wang et al., 2024; Yin et al., 2024b) and its multi-step generalization (Xie et al., 2024b; Salimans et al., 2024) represent another diffusion distillation technique that has demonstrated scalability on high-resolution images (Yin et al., 2024a). We apply one-step VSD from time to 0 to finetune teacher diffusion model using the EDM2-M configuration and tune both the weighting functions and proposal distributions for fair comparisons. As shown in Figure 7, we compare sCD, VSD, combination of sCD and VSD (by simply adding the two losses), and the teacher diffusion model by sweeping over the guidance scale. We observe that VSD has artifacts similar to those from applying large guidance scales in diffusion models: it increases fidelity (as evidenced by higher precision scores) while decreasing diversity (as shown by lower recall scores). This effect becomes more pronounced with increased guidance scales, ultimately causing severe mode collapse. In contrast, the precision and recall scores from two-step sCD are comparable with those of the teacher diffusion model, resulting in better FID scores than VSD."
        },
        {
            "title": "6 CONCLUSION",
            "content": "Our improved formulations, architectures, and training objectives have simplified and stabilized the training of continuous-time consistency models, enabling smooth scaling up to 1.5 billion parameters 10 on ImageNet 512512. We ablated the impact of TrigFlow formulation, tangent normalization, and adaptive weighting, confirming their effectiveness. Combining these improvements, our method demonstrated predictable scalability across datasets and model sizes, outperforming other few-step sampling approaches at large scales. Notably, we narrowed the FID gap with the teacher model to within 10% using two-step generation, compared to state-of-the-art diffusion models that require significantly more sampling steps."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We would like to thank Allan Jabri, Aaron Lou, Alex Nichol, Huiwen Chang, Heewoo Jun, and Ishaan Gulrajani for technical discussions, and Mingyu Ye for assistance with plots and diagrams. We also appreciate the support from Mark Chen and Prafulla Dhariwal for this research project."
        },
        {
            "title": "REFERENCES",
            "content": "Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=li7qeBbCR1t. Alpha-VLLM. Large-DiT-ImageNet. https://github.com/Alpha-VLLM/ LLaMA2-Accessory/tree/f7fe19834b23e38f333403b91bb0330afe19f79e/ Large-DiT-ImageNet, 2024. Commit f7fe198. Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2266922679, 2023. David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, and Eric Gu. Tract: Denoising diffusion models with transitive closure time-distillation. arXiv preprint arXiv:2303.04248, 2023. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Andrew Brock. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/ video-generation-models-as-world-simulators. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning (2023). arXiv preprint arXiv:2307.08691, 2023. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: largescale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255. IEEE, 2009. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, volume 34, pp. 87808794, 2021. 11 Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Zach Evans, CJ Carr, Josiah Taylor, Scott Hawley, and Jordi Pons. Fast timing-conditioned latent audio diffusion. arXiv preprint arXiv:2402.04825, 2024. Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024a. Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024b. Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua Susskind. Boot: Data-free distillation of denoising diffusion models with bootstrapping. In ICML 2023 Workshop on Structured Probabilistic Inference & Generative Modeling, 2023. Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. Diffit: Diffusion vision transformers for image generation. arXiv preprint arXiv:2312.02139, 2023. Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models. arXiv preprint arXiv:2403.06807, 2024. Eric Heitz, Laurent Belcour, and Thomas Chambon. Iterative α-(de) blending: minimalist deterministic diffusion model. In ACM SIGGRAPH 2023 Conference Proceedings, pp. 18, 2023. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by two time-scale update rule converge to local Nash equilibrium. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems, volume 30, pp. 66266637, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pp. 68406851, 2020. Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In International Conference on Machine Learning, pp. 1321313232. PMLR, 2023. Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive computation for iterative generation. arXiv preprint arXiv:2212.11972, 2022. Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. In International Conference on Machine Learning, 2022. Tero Karras. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. arXiv preprint arXiv:2206.00364, 2022. Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing In Proceedings of the IEEE/CVF and improving the training dynamics of diffusion models. Conference on Computer Vision and Pattern Recognition, pp. 2417424184, 2024. Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36, 2024. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. In Advances in Neural Information Processing Systems, 2021. Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and sample steps are flawed. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 54045411, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023a. Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019. Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. arXiv preprint arXiv:2406.09416, 2024. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 92989309, 2023b. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022a. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 13 Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1429714306, 2023. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pp. 81628171. PMLR, 2021. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Stefano Peluchetti. Non-denoising forward-time diffusions, 2022. URL https://openreview. net/forum?id=oVfIKuhqfC. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with CLIP latents. arXiv preprint arXiv:2204.06125, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. Tim Salimans, Thomas Mensink, Jonathan Heek, and Emiel Hoogeboom. Multistep distillation of diffusion models via moment matching. arXiv preprint arXiv:2406.04103, 2024. Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances in neural information processing systems, 31, 2018. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 22562265. PMLR, 2015. Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, volume 32, pp. 1189511907, 2019. Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. In Advances in Neural Information Processing Systems, volume 34, pp. 14151428, 2021a. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021b. 14 Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In International Conference on Machine Learning, pp. 3221132252. PMLR, 2023. Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. Advances in neural information processing systems, 33:75377547, 2020. Joshua Tian Jin Tee, Kang Zhang, Hee Suk Yoon, Dhananjaya Nagaraja Gowda, Chanwoo Kim, and Chang Yoo. Physics informed distillation for diffusion models. Transactions on Machine Learning Research, 2024. Yao Teng, Yue Wu, Han Shi, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Dim: Diffusion mamba for efficient high-resolution image synthesis. arXiv preprint arXiv:2405.14224, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024a. Yuchuan Tian, Zhijun Tu, Hanting Chen, Jie Hu, Chao Xu, and Yunhe Wang. U-dits: Downsample tokens in u-shaped diffusion transformers. arXiv preprint arXiv:2405.02730, 2024b. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024. Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion GANs. In International Conference on Learning Representations, 2022. Qingsong Xie, Zhenyi Liao, Zhijie Deng, Shixiang Tang, Haonan Lu, et al. Mlcm: Multistep consistency distillation of latent diffusion model. arXiv preprint arXiv:2406.05768, 2024a. Sirui Xie, Zhisheng Xiao, Diederik Kingma, Tingbo Hou, Ying Nian Wu, Kevin Patrick Murphy, Tim Salimans, Ben Poole, and Ruiqi Gao. EM distillation for one-step diffusion models. arXiv preprint arXiv:2405.16852, 2024b. Jing Nathan Yan, Jiatao Gu, and Alexander Rush. Diffusion models without attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82398249, 2024. Ling Yang, Zixiang Zhang, Zhilong Zhang, Xingchao Liu, Minkai Xu, Wentao Zhang, Chenlin Meng, Stefano Ermon, and Bin Cui. Consistency flow matching: Defining straight flows with velocity consistency. arXiv preprint arXiv:2407.02398, 2024. Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024a. Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 66136623, 2024b. Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. 15 Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023a. Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast In International conference on machine sampling of diffusion models via operator learning. learning, pp. 4239042402. PMLR, 2023b. Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics. Advances in Neural Information Processing Systems, 36: 5550255542, 2023c. Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024."
        },
        {
            "title": "APPENDIX",
            "content": "We include additional derivations, experimental details, and results in the appendix. The detailed training algorithm for sCM, covering both sCT and sCD, is provided in Appendix A. We present comprehensive discussion of the TrigFlow framework in Appendix B, including detailed derivations (Appendix B.1) and its connections with other parameterization (Appendix B.2). We introduce new algorithm called adaptive variational score distillation in Appendix C, which eliminates the need for manually designed training weighting. Furthermore, we elaborate on general framework for adaptive training weighting in Appendix D, applicable to diffusion models, consistency models, and variational score distillation. As our improvements discussed in Sec. 4 are also applicable for discrete-time consistency models, we provide detailed derivations and the training algorithm for discrete-time consistency models in Appendix E, incorporating all the improved techniques of sCM. We also provide complete description of the Jacobian-vector product algorithm for Flash Attention in Appendix F. Finally, all experimental settings and evaluation results are listed in Appendix G, along with additional samples generated by our sCD-XXL model trained on ImageNet at 512512 resolution in Appendix H."
        },
        {
            "title": "A TRAINING ALGORITHM OF SCM",
            "content": "We provide the detailed algorithm of sCM in Algorithm 1, where we refer to consistency training of sCM as sCT and consistency distillation of sCM as sCD. Algorithm 1 Simplified and Stabilized Continuous-time Consistency Models (sCM). Input: dataset with std. σd, pretrained diffusion model Fpretrain with parameter θpretrain, model Fθ, weighting wϕ, learning rate η, proposal (Pmean, Pstd), constant c, warmup iteration H. Init: θ θpretrain, Iters 0. repeat std), arctan( eτ σd dI), τ (Pmean, 2 x0 D, (0, σ2 dt cos(t)z sin(t)x0 if consistency training else dxt dxt min(1, Iters/H) cos2(t)(σdFθ dxt g/(g + c) L(θ, ϕ) ewϕ (t) (θ, ϕ) (θ, ϕ) ηθ,ϕL(θ, ϕ) Iters Iters + dt ) cos(t) sin(t)(xt + σd , t) Fθ ( xt σd Fθ( xt σd 2 wϕ(t) , t) g2 ), xt cos(t)x0 + sin(t)z , t) dt σdFpretrain( xt σd Tangent warmup JVP rearrangement Tangent normalization Adaptive weighting dFθ dt ) until convergence TRIGFLOW: SIMPLE FRAMEWORK UNIFYING EDM, FLOW MATCHING AND VELOCITY PREDICTION B.1 DERIVATIONS Denote the standard deviation of the data distribution pd as σd. We consider general forward diffusion process at time [0, ] with xt = αtx0 + σtz for the data sample x0 pd and the noise sample (0, σ2 dI) (note that the variance of is the same as that of the data x0)1, where αt > 0, σt > 0 are noise schedules such that αt/σt is monotonically decreasing w.r.t. t, with α0 = 1, σ0 = 0. The general training loss for diffusion model can always be rewritten as LDiff(θ) = Ex0,z,t (cid:104) w(t) Dθ(xt, t) x02 2 (cid:105) , (9) where different diffusion model formulation contains four different parts: 1For any diffusion process with xt = α tx0 + σ it to xt = α does not result in any loss of generality. tx0 + σ σd (σdϵ) and let := σdϵ, αt := α tϵ where ϵ (0, I), we can always equivalently convert dI) . So the assumption for (0, σ2 t, σt := σ σd 17 1. Parameterization of Dθ, such as score function (Song & Ermon, 2019; Song et al., 2021b), noise prediction model (Song & Ermon, 2019; Song et al., 2021b; Ho et al., 2020), data prediction model (Ho et al., 2020; Kingma et al., 2021; Salimans & Ho, 2022), velocity prediction model (Salimans & Ho, 2022), EDM (Karras et al., 2022) and flow matching (Lipman et al., 2022; Liu et al., 2022; Albergo et al., 2023). 2. Noise schedule for αt and σt, such as variance preserving process (Ho et al., 2020; Song et al., 2021b), variance exploding process (Song et al., 2021b; Karras et al., 2022), cosine schedule (Nichol & Dhariwal, 2021), and conditional optimal transport path (Lipman et al., 2022). 3. Weighting function for w(t), such as uniform weighting (Ho et al., 2020; Nichol & Dhariwal, 2021; Karras et al., 2022), weighting by functions of signal-to-noise-ratio (SNR) (Salimans & Ho, 2022), monotonic weighting (Kingma & Gao, 2024) and adaptive weighting (Karras et al., 2024). 4. Proposal distribution for t, such as uniform distribution within [0, ] (Ho et al., 2020; Song et al., 2021b), log-normal distribution (Karras et al., 2022), SNR sampler (Esser et al., 2024), and adaptive importance sampler (Song et al., 2021a; Kingma et al., 2021). Below we show that, under the unit variance principle proposed in EDM (Karras et al., 2022), we can obtain general but simple framework for all the above four parts, which can equivalently reproduce all previous diffusion models. Step 1: General EDM parameterization. We consider the parameterization for Dθ as the same principle in EDM (Karras et al., 2022) by Dθ(xt, t) = cskip(t)xt + cout(t)Fθ(cin(t)xt, cnoise(t)), (10) and thus the training objective becomes LDiff = Ex0,z,t (cid:34) w(t)c2 out(t) (cid:13) (cid:13) (cid:13) (cid:13) Fθ(cin(t)xt, cnoise(t)) cskip(t)αtx0 (1 cskip(t)σt)z cout(t) (cid:35) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 . (11) To ensure the input data of Fθ has unit variance, we should ensure Var[cin(t)xt] = 1 by letting 1 (cid:112)α2 + σ2 To ensure the training target of Fθ has unit variance, we have out(t) = σ2 c2 skip(t)α2 cin(t) = dc2 σd . + (cskip(t)σt 1)2. (12) (13) To reduce the error amplification from Fθ to Dθ, we should ensure cout(t) to be as small as possible, which means we should take cskip(t) by letting cout cskip = 0, which results in cskip(t) = σt + σ2 α2 , cout(t) = σdαt + σ2 (cid:112)α2 . (14) Though equivalent, we choose cout(t) = σdαt α2 +σ2 which can simplify some derivations below. In summary, the parameterization and objective for the general diffusion noise schedule are Dθ(xt, t) = xt σt + σ2 α2 LDiff = Ex0,z,t w(t) αt (cid:112)α2 + σ2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) α2 + σ2 α2 (cid:32) σdFθ xt (cid:112)α2 + σ2 σd (cid:33) , cnoise(t) , σdFθ (cid:32) xt (cid:112)α2 + σ2 σd (cid:33) , cnoise(t) αtz σtx0 (cid:112)α2 + σ2 . (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2 (15) (16) Step 2: All noise schedules can be equivalently transformed. One nice property of the unit variance principle is that the αt, σt in both the parameterization and the objective are homogenous, which means we can always assume α2 = 1 without loss of generality. To see this, we can apply + σ2 simple change-of-variable of ˆαt = thus we have αt α2 +σ2 , ˆσt = σt α +σ2 and ˆxt = xt α2 +σ2 = ˆαtx0 + ˆσtz, Dθ(xt, t) = ˆσt ˆxt ˆαtσdFθ (cid:18) ˆxt σd (cid:19) , cnoise(t) , (cid:34) LDiff = Ex0,z,t w(t)ˆα2 (cid:13) (cid:13) (cid:13) (cid:13) σdFθ (cid:18) ˆxt σd (cid:19) , cnoise(t) (ˆαtz ˆσtx0) (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:35) . (17) (18) As for the sampling procedure, according to DPM-Solver++ (Lu et al., 2022b), the exact solution of diffusion ODE from time to time satisfies xt = σt σs xs + σt (cid:90) λt λs eλDθ(xλ, λ)dλ, (19) where λt = log αt σt = ˆxt the fact that xt ˆσt σt , so the sampling procedure is also homogenous for αt, σt. To see this, we can use and λt = log ˆαt ˆσt := ˆλt, thus the above equation is equivalent to ˆxt = ˆσt ˆσs ˆxs + ˆσt (cid:90) ˆλt ˆλs ˆλDθ( ˆxˆλ, ˆλ)dˆλ, (20) which is exactly the sampling procedure of the diffusion process ˆxt, which means noise schedules of diffusion models wont affect the performance of sampling. In other words, for any diffusion process (αt, σt, xt) at time t, we can always divide them by (cid:112)α2 to obtain the diffusion process (ˆαt, ˆσt, ˆxt) with ˆα2 = 1 and all the parameterization, training objective and sampling procedure can be equivalently transformed. The only difference is the corresponding training weighting w(t)σ2 in Eq. (18), which we will discuss in the next step. + ˆσ2 + σ ˆα2 straightforward corollary is that the optimal transport path (Lipman et al., 2022) in flow matching with αt = 1 t, σt = can be equivalently converted to other noise schedules. The reason of its better empirical performance is essentially due to the different weighting during training and the lack of advanced diffusion sampler such as DPM-Solver series (Lu et al., 2022a;b) during sampling, not the straight path (Lipman et al., 2022) itself. Step 3: Unified framework by TrigFlow. As we showed in the previous step, we can always assume ˆα2 = 1. An equivalent change-of-variable of such constraint is to define + ˆσ2 ˆt := arctan (cid:19) (cid:18) ˆσt ˆαt = arctan (cid:19) , (cid:18) σt αt (21) so ˆt [0, π 2 ] is monotonically increasing function of [0, ], thus there exists one-one mapping between and ˆt to convert the proposal distribution p(t) to the distribution of ˆt, denoted as (cid:0)ˆt(cid:1). As ˆαt = cos (cid:0)ˆt(cid:1) , ˆσt = sin (cid:0)ˆt(cid:1), the training objective in Eq. (18) is equivalent to LDiff = Ex0,z (cid:90) π 0 (cid:0)ˆt(cid:1) (cid:0)ˆt(cid:1) cos2 (cid:0)ˆt(cid:1) (cid:123)(cid:122) (cid:125) (cid:124) training weighting (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:124) (cid:13) σdFθ (cid:18) ˆxˆt σd , cnoise (cid:19) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cos (cid:0)ˆt(cid:1) sin (cid:0)ˆt(cid:1) x0) (cid:13) (cid:13) (cid:13) (cid:13) (cid:125) (cid:123)(cid:122) (cid:13) independent from αt and σt 2 (cid:0)ˆt(cid:1) dˆt . (22) Therefore, we can always put the influence of different noise schedules into the training weighting of the integral for ˆt from 0 to π 2 , while the ℓ2 norm loss at each ˆt is independent from the choices of αt and σt. As we equivalently convert the noise schedules by trigonometric functions, we name such framework for diffusion models as TrigFlow. For simplicity and with slight abuse of notation, we omit the ˆt and denote the whole training weighting as single w(t), we summarize the diffusion process, parameterization, training objective and samplers of TrigFlow as follows. Diffusion Process. x0 pd(x0), (0, σ2 dI), xt = cos(t)x0 + sin(t)z for [0, π 2 ]. 19 Parameterization. Dθ(xt, t) = cos(t)xt sin(t)σdFθ (cid:18) xt σd (cid:19) , cnoise(t) , (23) where cnoise(t) is the conditioning input of the noise levels for Fθ, which can be arbitrary one-one mapping of t. Moreover, the parameterized diffusion ODE is defined by dxt dt = σdFθ (cid:18) xt σd (cid:19) , cnoise(t) . Training Objective. LDiff(θ) = Ex0,z (cid:34)(cid:90) π 2 0 w(t) (cid:13) (cid:13) (cid:13) (cid:13) σdFθ (cid:18) xt σd (cid:19) , cnoise (t) (cos(t)z sin(t)x0) (cid:35) dt , (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (24) (25) where w(t) is the training weighting, which we will discuss in details in Appendix D. As for the sampling procedure, although we can directly solve the diffusion ODE in Eq. (24) by Eulers or Heuns solvers as in flow matching (Lipman et al., 2022), the parameterization for σdFθ may be not the optimal parameterization for reducing the discreteization errors. As proved in DPMSolver-v3 (Zheng et al., 2023c), the optimal parameterization should cancel all the linearity of the ODE, and the data prediction model Dθ is an effective approximation of such parameterization. Thus, we can also apply DDIM, DPM-Solver and DPM-Solver++ for TrigFlow by rewriting the coefficients into the TrigFlow notation, as listed below. 1st-order Sampler by DDIM. Starting from xs at time s, the solution xt at time is xt = cos(s t)xt sin(s t)σdFθ (cid:18) xs σd (cid:19) , cnoise(s) (26) One good property of TrigFlow is that the 1st-order sampler can naturally support zero-SNR sampling (Lin et al., 2024) by letting = π 2 without any numerical issues. 2nd-order Sampler by DPM-Solver. Starting from xs at time s, by reusing previous solution xs at time s, the solution xt at time is xt = cos(s t)xs sin(s t)σdFθ (cid:18) xs σd (cid:19) , cnoise(s) where ϵθ(xt, t) = sin(t)xt + cos(t)σdFθ log tan(s)log tan(s) log tan(s)log tan(t) . (cid:16) xt σd sin(s t) 2rs cos(s) (cid:17) (ϵθ(xs, s) ϵθ(xs, s)) , (27) , cnoise(t) is the noise prediction model, and rs = 2nd-order Sampler by DPM-Solver++. Starting from xs at time s, by reusing previous solution xs at time s, the solution xt at time is (cid:18) xs σd (Dθ(xs, s) D(xs, s)) , (28) xt = cos(st)xs sin(st)σdFθ sin(s t) 2rs sin(s) , cnoise(s) + (cid:19) where rs = log tan(s)log tan(s) log tan(s)log tan(t) . B.2 RELATIONSHIP WITH OTHER PARAMETERIZATION As previous diffusion models define the forward process with xt = αtx0 +σtϵ = αtx0 + σt σd for ϵ (0, I), we can obtain the relationship between and TrigFlow time steps [0, π 2 ] by (cid:19) (σdϵ) = arctan , xt = xt. (29) (cid:18) σt σdαt σd + σ2 tσ2 (cid:112)α2 Thus, we can always translate the notation from previous noise schedules to TrigFlow notations. Moreover, below we show that TrigFlow unifies different current frameworks for training diffusion models, including EDM, flow matching and velocity prediction. EDM. As our derivations closely follow the unit variance principle proposed in EDM (Karras et al., 2022), our parameterization can be equivalently converted to EDM notations. Specifically, the transformation between TrigFlow (xt, t) and EDM (xσ, σ) is = arctan (cid:19) (cid:18) σ σd , xt = cos(t)xσ. (30) The reason why TrigFlow notation is much simpler than EDM is just because we define the end point of the diffusion process as (0, σ2 dI) with the same variance as the data distribution. Thus, the unit variance principle can ensure that all the intermediate xt does not need to multiply other coefficients as in EDM. Flow Matching. Flow matching (Lipman et al., 2022; Liu et al., 2022; Albergo et al., 2023) defines stochastic path between two samples x0 from data distribution and from tractable distribution which is usually some Gaussian distribution. For general path xt = αtx0 + σtz with α0 = 1, αT = 0, σ0 = 0, σT = 1, the conditional probability path is vt = dαt dt x0 + dσt dt z, and it learns parameterized model vθ(xt, t) by minimizing Ex0,z,t (cid:104) w(t) vθ(xt, t) vt2 2 (cid:105) , and the final probability flow ODE is defined by dxt dt = vθ(xt, t). (31) (32) (33) As TrigFlow uses αt = cos(t) and σt = sin(t), it is easy to see that the training objective and the diffusion ODE of TrigFlow are also the same as flow matching with vθ(xt, t) = σdFθ( xt , cnoise(t)). σd To the best of our knowledge, TrigFlow is the first framework that unifies EDM and flow matching for training diffusion models. Velocity Prediction. The velocity prediction parameterization (Salimans & Ho, 2022) trains parameterization network with the target αtz σtx0. As TrigFlow uses αt = cos(t), σt = sin(t), it is easy to see that the training target in TrigFlow is also the velocity. Discussions on SNR. Another good property of TrigFlow is that it can define data-varianceinvariant SNR. Specifically, previous diffusion models define the SNR at time as SNR(t) = α2 for σ2 xt = αtx0 + σtϵ with ϵ (0, I). However, such definition ignores the influence of the variance of x0: if we rescale the data x0 by constant, then such SNR doesnt get rescaled correspondingly, which is not reasonable in practice. Instead, in TrigFlow we can define the SNR by ˆSNR(t) = α2 σ2 σ2 = 1 tan2(t) , (34) which is data-variance-invariant and also simple."
        },
        {
            "title": "FRAMEWORK",
            "content": "In this section, we propose the detailed derivation for variational score distillation (VSD) in TrigFlow framework and an improved objective with adaptive weighting. C.1 DERIVATIONS Assume we have samples x0 RD from data distribution pd with standard deviation σd, and define corresponding forward diffusion process {pt}T t=0 starting at p0 = pd and ending at pT (0, ˆσ2I), with pt0(xtx0) = (xtαtx0, σ2 I). Variational score distillation (VSD) (Wang et al., 2024; Yin 21 et al., 2024b;a) trains generator gθ : RD RD aiming to map noise samples (0, ˆσ2I) to the data distribution, by minimizing Et (cid:2)w(t)DKL (cid:0)qθ pt (cid:1)(cid:3) = Et,z,ϵ (cid:2)w(t) (cid:0)log qθ (αtgθ(z) + σtϵ) log pt(αtgθ(z) + σtϵ)(cid:1)(cid:3) , min θ where ϵ (0, I), qθ is the diffused distribution at time with the same forward diffusion process as pt while starting at qθ 0 as the distribution of gθ(z), w(t) is an ad-hoc training weighting (Poole et al., 2022; Wang et al., 2024; Yin et al., 2024b), and follows proposal distribution such as uniform distribution. It is proved that the optimum of qθ satisfies q0 = pd (Wang et al., 2024) and thus the distribution of the generator matches the data distribution. Moreover, by denoting xθ := αtgθ(z) + σtϵ and taking the gradient w.r.t. θ, we have (cid:0)qθ pt )(cid:1)(cid:3) (cid:0)log qθ (xθ ) log pt(xθ (xt) + (cid:0)xt log qθ (cid:2)w(t)DKL (cid:2)w(t)θ (cid:18) (cid:20) θEt = Et,z,ϵ θ log qθ = Et,z,ϵ w(t) (cid:19)(cid:21) (cid:1)(cid:3) (xt) xt log pt(xt)(cid:1) xθ θ (xt) xt log pt(xt)(cid:1) αtgθ(z) (cid:20) w(t) (cid:0)xt log qθ (cid:21) = Et,xt (cid:124) (cid:2)w(t)θ log qθ (cid:123)(cid:122) =0 (xt)(cid:3) (cid:125) +Et,z,ϵ θ = Et,z,ϵ (cid:20) αtw(t) (cid:0)xt log qθ (xt) xt log pt(xt)(cid:1) gθ(z) θ (cid:21) . Therefore, we need to approximate the score functions xt log qθ (xt) for the generator and xt log pt(xt) for the data distribution. VSD trains diffusion model for samples from gθ(z) to approximate xt log qθ (xt) and uses pretrained diffusion model to approximate xt log pt(xt). In this work, we train the diffusion model in TrigFlow framework, with αt = cos(t), σt = σd sin(t), ˆσ = σd, = π 2 . Specifically, assume we have pretrained diffusion model Fpretrain parameterized by TrigFlow, and we train another diffusion model Fϕ to approximate the diffused generator distribution, by (cid:34) Ez,z,t w(t) min ϕ (cid:13) (cid:13) (cid:13) (cid:13) σdFϕ (cid:19) , vt (cid:18) xt σd (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:35) , where xt = cos(t)x0 + sin(t)z, vt = cos(t)z sin(t)x0, (0, σ2 (0, σ2 and the score function xt log pt(xt) is dI), x0 = gθ(z) with dI). Moreover, the relationship between the ground truth diffusion model FDiff(xt, t) σdFDiff(xt, t) = E[vtxt] = 1 tan(t) xt Ex0xt [x0] , xt log pt(xt) = Ex0xt (cid:20) xt cos(t)x0 sin2(t) σ2 cos(t)σdFDiff + sin(t)xt σ2 sin(t) . 1 sin(t) (cid:21) = Thus, we train the generator gθ by the following gradient w.r.t. θ: Et,z,z (cid:20) cos2(t) σd sin(t) (cid:18) w(t) Fpretrain (cid:19) , Fϕ (cid:18) xt σd (cid:18) xt σd which is equivalent to the gradient of the following objective: (cid:19)(cid:19) gθ(z) (cid:21) , , θ (cid:34) Et,z,z cos2(t) σd sin(t) w(t) (cid:13) (cid:13) gθ(z) gθ (z) + Fpretrain (cid:13) (cid:13) (cid:18) xt σd (cid:19) , Fϕ (cid:18) xt σd , (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 (cid:35) , where gθ(z) is the same as gθ(z) but stops the gradient for θ. Note that the weighting functions used in previous works (Wang et al., 2024; Yin et al., 2024b) is proportional to sin2(t) cos(t) , thus the prior weighting is proportional to sin(t) cos(t), which has U-shape similar to the log-normal distribution used in Karras et al. (2022). Thus, we can instead use log-normal proposal distribution and apply the adaptive weighting by training another weighting network wψ(t). We refer to Appendix for detailed discussions about the learnable adaptive weighting. Thus we can obtain the training objective, as listed below. C.2 TRAINING OBJECTIVE Training Objective of Adaptive Variational Score Distillation (aVSD). LDiff(ϕ) := Ez,z,t w(t) min ϕ (cid:34) (cid:13) (cid:13) (cid:13) (cid:13) σdFϕ (cid:19) , vt (cid:18) xt σd (cid:35) , (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 LVSD(θ, ψ) := Et,z,z min θ,ψ (cid:34) ewψ(t) (cid:13) (cid:13) gθ(z) gθ(z) + Fpretrain (cid:13) (cid:13) (cid:18) xt σd (cid:19) , Fϕ (cid:18) xt σd (cid:35) (cid:19)(cid:13) 2 (cid:13) wψ(t) (cid:13) (cid:13) 2 , . And we also choose proportional distribution of for estimating LVSD(θ, ψ) by log(tan(t)σd) (Pmean, 2 std) and tune these two hyperparameters (note that they may be different from the proposal distribution for training LDiff(ϕ), as detailed in Appendix G. In addition, for consistency models fθ(xt, t), we choose (0, σ2 σdFθ( , π 2 ), and thus the corresponding objective is σd (cid:34) (cid:19) (cid:19) dI) and gθ(z) := fθ(z, π 2 ) = (cid:19) (cid:35) Et,z,z min θ,ψ ewψ(t) (cid:13) (cid:13) (cid:13) (cid:13) σdFθ (cid:18) σd , π 2 (cid:18) σd , π 2 σdFθ Fpretrain , +Fϕ wψ(t) . (cid:18) xt σd (cid:18) xt σd , (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13) 2 ADAPTIVE WEIGHTING FOR DIFFUSION MODELS, CONSISTENCY MODELS AND VARIATIONAL SCORE DISTILLATION We first list the objectives of diffusion models, consistency models and variational score distillation (VSD). For diffusion models, as shown in Eq. (25), the gradient of the objective is θLDiff(θ) = θEt,x0,zw(t) (cid:2)σdFθ vt2 (cid:3) = θEt,x0,z 2 where Fθ is the same as Fθ but stops the gradient w.r.t. θ. For VSD, the gradient of the objective is (cid:2)w(t)σdF θ (σdFθ vt)(cid:3) , θLDiff(θ) = θEt,z,z (cid:2)w(t)gθ(z)(Fpretrain Fϕ)(cid:3) . And for continuous-time CMs parameterized by TrigFlow, the objective is LCM(θ) = θEt,x0,z (cid:20) w(t) sin(t)f θ (cid:21) , dfθ dt where fθ is the same as fθ but stops the gradient w.r.t. θ. Interestingly, all these objectives can be rewritten into form of inner product between neural network and target function which has the same dimension (denoted as D) as the output of the neural network. Specifically, assume the neural network is Fθ parameterized by θ, we study the following objective: Et (cid:2)F θ y(cid:3) , min θ where we do not compute the gradients w.r.t. θ for y. In such case, the gradient will be equivalent to (cid:104) θEt Fθ Fθ + y2 (cid:105) , where Fθ is the same as Fθ but stops the gradient w.r.t. θ. In such case, we can balance the gradient variance w.r.t. by training an adaptive weighting network wϕ(t) to estimate the loss norm, i.e., minimizing Et min ϕ (cid:20) ewϕ(t) Fθ Fθ + y2 2 wϕ(t) . (cid:21) This is the adaptive weighting proposed by EDM2 (Karras et al., 2024), which balances the loss variance across different time steps, inspired by the uncertainty estimation of Sener & Koltun (2018). By taking the partial derivative w.r.t. in the above equation, it is easy to verify that the optimal w(t) satisfies (cid:2)Fθ Fθ + y2 Therefore, the adaptive weighting reduces the loss variance across different time steps. In such case, all we need to do is to choose (cid:3) 1. 2 ew(t) 23 1. prior weighting λ(t) for y, which may be helpful for further reducing the variance of y."
        },
        {
            "title": "Then the objective becomes",
            "content": "Et min θ,ϕ (cid:20) ewϕ(t) Fθ Fθ + λ(t)y2 (cid:21) 2 wϕ(t) . 1 e.g., for diffusion models and VSD, since the target is either = vt or = Fpretrain Fϕ which are stable across different time steps, we can simply choose λ(t) = 1; while for consistency models, the target = sin(t) df dt may have huge variance, we choose λ(t) = σd tan(t) to reduce the variance of λ(t)y, which empirically is critical for better performance. 2. proposal distribution for sampling the training t, which determines which part of we should focus on more. For diffusion models, we generally need to focus on the intermediate time steps since both the clean data and pure noise cannot provide precise training signals. Thus, the common choice is to choose normal distribution over the log-SNR of time steps, which is proposed by Karras et al. (2022) and also known as log-normal distribution. In this way, we do not need to manually choose the weighting functions, significantly reducing the tuning complexity of training diffusion models, CMs and VSD. DISCRETE-TIME CONSISTENCY MODELS WITH IMPROVED TRAINING"
        },
        {
            "title": "OBJECTIVES",
            "content": "Note that the improvements proposed in Sec. 4 can also be applied to discrete-time consistency models (CMs). In this section, we discuss the improved version of discrete-time CMs for consistency distillation. E.1 PARAMETERIZATION AND TRAINING OBJECTIVE Parameterization. We also parameterize the CM by TrigFlow: fθ(xt, t) = cos(t)xt σd sin(t)Fθ (cid:19) , . (cid:18) xt σd And we denote the pretrained teacher diffusion model as dxt dt = Fpretrain( xt σd Reference sample by DDIM. Assume we sample x0 pd, (0, σ2 dI), and xt = cos(t)x0 + sin(t)z, we need reference sample xt at time < to guide the training of the CM, which can be obtained by one-step DDIM from to t: , t). xt = cos(t t)xt σd sin(t t)Fpretrain (cid:19) , . (cid:18) xt σd Thus, the output of the consistency model at time is fθ(xt, t) = cos(t) cos(tt)xtσd cos(t) sin(tt)Fpretrain (cid:19) , (cid:18) xt σd σd sin(t)Fθ (cid:18) xt σd (cid:19) . , (35) Original objective of discrete-time CMs. The consistency model at time can be rewritten into fθ (xt, t) = cos(t)xt σd sin(t)Fθ (cid:19) , (cid:18) xt σd = (cos(t) cos(t t) sin(t) sin(t t))xt (36) σd(sin(t t) cos(t) + cos(t t) sin(t))Fθ (cid:19) , (cid:18) xt σd 24 Therefore, by computing the difference between Eq. (35) and Eq. (36), we define θ (xt, t, t) := fθ (xt, t) fθ (xt, t) sin(t t) (cid:16) (cid:19) (cid:18) xt σd = cos(t) σdFθ , σdFpretrain (cid:18) xt σd (cid:19) , (cid:17) (cid:125) (cid:123)(cid:122) dxt dt (cid:124) (cid:16) sin(t) xt + σd cos(t t)Fθ (cid:16) xt σd (cid:17) , σdFθ (cid:16) xt σd , t(cid:17) (cid:124) sin(t t) (cid:123)(cid:122) dF σd θ dt (cid:125) (37) (cid:17) . Comparing to Eq. (6), it is easy to see that limtt θ (xt, t, t) = dfθ using d(x, y) = y2 becomes dt (xt, t). Moreover, when 2, and = t, the training objective of discrete-time CMs in Eq. (1) Ext,t (cid:2)w(t)fθ(xt, t) fθ (xtt, t)2 (cid:3) , 2 which has the gradient of Ext,t = θExt,t = θExt,t (cid:2)w(t)θf θ (xt, t) (fθ (xt, t) fθ (xtt, t))(cid:3) θ (xt, t)θ(xt, t, t)(cid:3) (cid:2)w(t) sin(t t)f (cid:2)w(t) sin(t t) sin(t)F θ (xt, t)θ(xt, t, t)(cid:3) (38) Adaptive weighting for discrete-time CMs. Inspired by the continuous-time consistency models, we can also apply the adaptive weighting technique into discrete-time training objectives in Eq. (38). Specifically, since θ(xt, t, t) is first-order approximation of dfθ dt (xt, t), we can directly replace the tangent in Eq. (8) with θ (xt, t, t), and obtain the improved objective of discrete-time CMs by: LsCM(θ, ϕ) := Ext,t (cid:34) ewϕ(t) (cid:13) (cid:13) (cid:13) (cid:13) Fθ (cid:18) xt σd (cid:19) , Fθ (cid:19) , (cid:18) xt σd (cid:13) 2 (cid:13) cos(t)θ (xt, t, t) (cid:13) (cid:13) 2 (cid:35) wϕ(t) , (39) where wϕ(t) is the adaptive weighting network. Tangent normalization for discrete-time CMs. We apply the simliar tangent normalization method as continuous-time CMs by defining gθ (xt, t, t) := cos(t)θ (xt, t, t) cos(t)θ (xt, t, t) + , where > 0 is hyperparameter, and then the objective in Eq. (39) becomes LsCM(θ, ϕ) := Ext,t (cid:34) ewϕ(t) (cid:13) (cid:13) (cid:13) (cid:13) Fθ (cid:18) xt σd (cid:19) , Fθ (cid:19) , (cid:18) xt σd gθ (xt, t, t) (cid:35) wϕ(t) , (cid:13) 2 (cid:13) (cid:13) (cid:13) 2 Tangent warmup for discrete-time CMs. We replace the θ (xt, t, t) with the warmup version: (cid:19) (cid:19)(cid:19) (cid:18) θ(xt, t, t, r) = cos(t) σdFθ , σdFpretrain (cid:18) xt σd σd cos(t t)Fθ (cid:18) xt σd (cid:17) , (cid:16) xt σd , sin(t) xt + σdFθ (cid:16) xt σd , t(cid:17) , sin(t t) where linearly increases from 0 to 1 over the first 10k training iterations. We provide the detailed algorithm of discrete-time sCM (dsCM) in Algorithm 2, where we refer to consistency distillation of discrete-time sCM as dsCD. 25 Algorithm 2 Simplified and Stabilized Discrete-time Consistency Distillation (dsCD). Input: dataset with std. σd, pretrained diffusion model Fpretrain with parameter θpretrain, model Fθ, weighting wϕ, learning rate η, proposal (Pmean, Pstd), constant c, warmup iteration H. Init: θ θpretrain, Iters 0. repeat (cid:16) xt σd std), arctan( eτ σd (cid:17) x0 D, (0, σ2 dI), τ (Pmean, 2 xt cos(t t)xt σd sin(t t)Fpretrain min(1, Iters/H) cos2(t)(σdFθ σdFpretrain) cos(t) sin(t)(xt + σd g/(g + c) L(θ, ϕ) ewϕ (t) (θ, ϕ) (θ, ϕ) ηθ,ϕL(θ, ϕ) Iters Iters + 1 , t) Fθ ( xt σd Fθ( xt σd 2 wϕ(t) , t) g2 , ), xt cos(t)x0 + sin(t)z Tangent warmup dFθ dt ) JVP rearrangement Tangent normalization Adaptive weighting until convergence E.2 EXPERIMENTS OF DISCRETE-TIME SCM We use the algorithm in Algorithm 2 to train discrete-time sCM, where we split [0, π 2 ] into intervals by EDM sampling spacing. Specifically, we first obtain the EDM time step by σi = (σ1/ρ min + (σ1/ρ min ))ρ with ρ = 7, σmin = 0.002 and σmax = 80, and then obtain ti = arctan(σi/σd) and set t0 = 0. During training, we sample with discrete categorical distribution that splits the log-normal proposal distribution as used in continuous-time sCM, similar to Song & Dhariwal (2023). max σ1/ρ As demonstrated in Figure 5(c), increasing the number of discretization steps in discrete-time CMs improves sample quality by reducing discretization errors, but obviously degrades once becomes too large (after > 1024) to suffer from numerical precision issues. By contrast, continuous-time CMs significantly outperform discrete-time CMs across all which provides strong justification for choosing continuous-time CMs over discrete-time counterparts. JACOBIAN-VECTOR PRODUCT OF FLASH ATTENTION The attention operator (Vaswani, 2017) needs to compute = softmax(x)V where R1L, RLD, R1D. Flash Attention (Dao et al., 2022; Dao, 2023) computes the output by maintaining three variables m(x) R, ℓ(x) R, and (x) with the same dimension as x. The computation is done recursively: for each block, we have m(x) = max(ex), ℓ(x) = (cid:88) ex(i)m(x), (x) = exm(x)V , and for combining two blocks = [x(a), x(b)], we merge their corresponding m, ℓ, by m(x) = max (cid:16) x(a), x(b)(cid:17) , ℓ(x) = em(x(a))m(x)ℓ(x(a)) + em(x(b))m(x)ℓ(x(b)), (x) = (cid:104) (cid:105) em(x(a))m(x)f (x(a)), em(x(b))m(x)f (x(b)) , = (x) ℓ(x) . However, to the best of knowledge, there does not exist an algorithm for computing the JacobianVector product of the attention operator in the Flash Attention style for faster computation and memory saving. We propose recursive algorithm for the JVP computation of Flash Attention below. Denote := softmax(x). Denote the tangent vector for R1L, R1L, RLD, R1D as tx R1L, tp R1L, tV RLD, ty R1D, correspondingly. The JVP for attention is computing (x, tx), (V , tV ) (y, ty), which is ty = tpV + ptV (cid:124)(cid:123)(cid:122)(cid:125) softmax(x)tV , where tpV = (p tx) (cid:124) (cid:123)(cid:122) (cid:125) 1L (pt ) (cid:124) (cid:123)(cid:122) (cid:125) 11 . (pV ) (cid:124) (cid:123)(cid:122) (cid:125) 26 Notably, the computation for both ptV and pV can be done by the standard Flash Attention with the value matrices and tV . Thus, to compute ty, we only need to maintain vector g(x) := (ptx)V and scalar µ(x) := pt during the Flash Attention computation loop. Moreover, since we do not know during the loop, we can reuse the intermediate m, ℓ, in Flash Attention. Specifically, for each block, (cid:16) (cid:17) g(x) = exm(x) tx , µ(x) = (cid:88) ex(i)m(x)t(i) , and for combining two blocks = [x(a), x(b)], we merge their corresponding and µ by g(x) = (cid:104) (cid:105) em(x(a))m(x)g(x(a)), em(x(b))m(x)g(x(b)) , µ(x) = em(x(a))m(x)µ(x(a)) + em(x(b))m(x)µ(x(b)), and after obtaining m, ℓ, , g, µ for the row vector x, the final result of tpV is tpV = g(x) ℓ(x) µ(x) ℓ(x) y. Therefore, we can use single loop to obtain both the output and the JVP output ty, which accesses the memory for the attention matrices only once and avoids saving the intermediate activations, thus saving the GPU memory."
        },
        {
            "title": "G EXPERIMENT SETTINGS AND RESULTS",
            "content": "G.1 TRIGFLOW FOR DIFFUSION MODELS We train the teacher diffusion models on CIFAR-10, ImageNet 6464 and ImageNet 512512 with the proposed improvements of parameterization and architecture, including TrigFlow parameterization, positional time embedding and adaptive double normalization layer. We list the detailed settings below. CIFAR-10. Our architecture is based on the Score SDE (Song et al., 2021b) architecture (DDPM++). We use the same settings of EDM (Karras et al., 2022): dropout rate is 0.13, batch size is 512, number of training iterations is 400k, learning rate is 0.001, Adam ϵ = 108, β1 = 0.9, β2 = 0.999. We use 2nd-order single-step DPM-Solver (Lu et al., 2022a) (DPM-Solver-2S) with Heuns intermediate time step with 18 steps (NFE=35), which is exactly equivalent to EDM Heuns sampler. We obtain FID of 2.15 for the teacher model. ImageNet 6464. We preprocess the ImageNet dataset following Dhariwal & Nichol (2021) by 1. Resize the shorter width / height to 64 64 resolution with bicubic interpolation. 2. Center crop the image. 3. Disable data augmentation such as horizontal flipping. Except for the TrigFlow parameterization, positional time embedding and adaptive double normalization layer, we follow exactly the same setting in EDM2 config (Karras et al., 2024) to train models with sizes of S, M, L, and XL, while the only difference is that we use Adam ϵ = 1011. ImageNet 512512. We preprocess the ImageNet dataset following Dhariwal & Nichol (2021) and Karras et al. (2024) by 1. Resize the shorter width / height to 512 512 resolution with bicubic interpolation. 2. Center crop the image. 3. Disable data augmentation such as horizontal flipping. 4. Encode the images into latents by stable diffusion VAE2 (Rombach et al., 2022; Janner et al., 2022), and rescale the latents by channel mean µc = [1.56, 0.695, 0.483, 0.729] and channel std σc = [5.27, 5.91, 4.21, 4.31]. We keep the σd = 0.5 as in EDM2 (Karras et al., 2024), so for each latent we substract µc and multiply it by σd/σc. 2https://huggingface.co/stabilityai/sd-vae-ft-mse 27 When sampling from the model, we redo the scaling of the generated latents and then run the VAE decoder. Notably, our channel mean and channel std are different from those in EDM2 (Karras et al., 2024). It is because when training the VAE, the images are normalized to [1, 1] before passing to the encoder. However, the channel mean and std used in EDM2 assumes the input images are in [0, 1] range, which mismatches the training phase of the VAE. We empirically find that it is hard to distinguish the reconstructed samples by human eyes of these two different normalization, while it has non-ignorable influence for training diffusion models evaluated by FID. After fixing this mismatch, our diffusion model slightly outperforms the results of EDM2 at larger scales (XL and XXL). More results are provided in Table 5. Except for the TrigFlow parameterization, positional time embedding and adaptive double normalization layer, we follow exactly the same setting in EDM2 config (Karras et al., 2024) to train models with sizes of S, M, L, XL and XXL, while the only difference is that we use Adam ϵ = 1011. We enable label dropout with rate 0.1 to support classifier-free guidance. We use 2nd-order single-step DPM-Solver (Lu et al., 2022a) (DPM-Solver-2S) with Heuns intermediate time step with 32 steps (NFE=63), which is exactly equivalent to EDM Heuns sampler. We find that the optimal guidance scale for classifier-free guidance and the optimal EMA rate are also the same as EDM2 for all model sizes. G.2 CONTINUOUS-TIME CONSISTENCY MODELS In all experiments, we use = 0.1 for tangent normalization, and use = 10000 for tangent warmup. We always use the same batch size as the teacher diffusion training, which is different from Song & Dhariwal (2023). During sampling, we start at tmax = arctan with σmax = 80 such that it matches the starting time of EDM (Karras et al., 2022) and EDM2 (Karras et al., 2024). For 2-step sampling, we use the algorithm in Song et al. (2023) with an intermediate = 1.1 for all the experiments. We always initialize the CM from the EMA parameters of the teacher diffusion model. For sCD, we always use the Fpretrain of the teacher diffusion model with its EMA parameters during distillation. (cid:16) σmax σd (cid:17) We empirically find that the proposal distribution should have small Pmean, i.e. close to the clean data, to ensure the training stability and improve the final performance. Intuitively, this is because the training signal of CMs only come from the clean data, so we need to reduce the training error for near to 0 to further reduce the accumulation errors. CIFAR-10. For both sCT and sCD, we initialize from the teacher diffusion model trained with the settings in Appendix G.1, and use RAdam optimizer (Liu et al., 2019) with learning rate of 0.0001, β1 = 0.9, β2 = 0.99, ϵ = 108, and without learning rate schedulers. proposal distribution of Pmean = 1.0, Pstd = 1.4. For the attention layers, we use the implementation in (Karras et al., 2022) which naturally supports JVP by PyTorch (Paszke et al., 2019) auto-grad. We use EMA half-life of 0.5 Mimg (Karras et al., 2022). We use dropout rate of 0.20 for sCT and disable dropout for sCD. ImageNet 6464. We only enable dropout at the resolutions equal to or less than 16, following Simple Diffusion (Hoogeboom et al., 2023) and iCT (Song & Dhariwal, 2023). We multiply the learning rate of the teacher diffusion model by 0.01 for both sCT and sCD. We train the model with half precision (FP16), and use the flash attention jvp proposed in Appendix for computing the tangents of flash attention layers. Other training settings are the same as the teacher diffusion models. More details of training and sampling are provided in Table 3 and Table 7. During sampling, we always use EMA length σrel = 0.05 for sampling from CMs. ImageNet 512512. We only enable dropout at the resolutions equal to or less than 16, following Simple Diffusion (Hoogeboom et al., 2023) and iCT (Song & Dhariwal, 2023). We multiply the learning rate of the teacher diffusion model by 0.01 for both sCT and sCD. We train the model with half precision (FP16), and use the flash attention jvp proposed in Appendix for computing the tangents of flash attention layers. Other training settings are the same as the teacher diffusion models. More details of training and sampling are provided in Table 4 and Table 5. During sampling, we always use EMA length σrel = 0.05 for sampling from CMs. We add an additional input in Fθ( xt , t, s) where represents the CFG guidance scale of the teacher σd model, where is embedded by positioinal embedding layer and an additional linear layer, and the embedding is added to the embedding of t, similar to the label conditioning. During training, we 28 Table 3: Training settings of all models and training algorithms on ImageNet 6464 dataset. Model Size M XL 2048 192 positional 35000 0.9 0.99 1.0e-11 280.2 Model details Batch size Channel multiplier Time embedding layer noise conditioning cnoise(t) adaptive double normalization Learning rate decay (tref) Adam β1 Adam β2 Adam ϵ Model capacity (Mparams) Training details of diffusion models (TrigFlow) Training iterations Learning rate max (αref) Dropout probability Proposal Pmean Proposal Pstd. Shared details of consistency models Learning rate max (αref) Proposal Pmean Proposal Pstd. Tangent normalization constant (c) Tangent warm up iterations EMA length (σrel) of pretrained diffusion Training details of sCT Training iterations Dropout probability for resolution 16 Training details of sCD Training iterations Dropout probability for resolution 16 1048k 1.0e-2 0% -0.8 1. 1.0e-4 -1.0 1.6 0.1 10k 0.075 400k 45% 400k 0% 2048 256 positional 35000 0.9 0.99 1.0e-11 497.8 2048 320 positional 35000 0.9 0.99 1.0e-11 777.6 2048 384 positional 35000 0.9 0.99 1.0e-11 1119. 1486k 9.0e-3 10% -0.8 1.6 9.0e-5 -1.0 1.6 0.1 10k 0.06 400k 45% 400k 0% 761k 8.0e-3 10% -0.8 1.6 8.0e-5 -1.0 1.6 0.1 10k 0. 400k 45% 400k 0% 540k 7.0e-3 10% -0.8 1.6 7.0e-5 -1.0 1.6 0.1 10k 0.04 400k 45% 400k 0% uniformly sample [1, 2] and apply CFG with guidance scale to the teacher diffusion model to get Fpretrain. VSD experiments. We do not use EMA for Fϕ in VSD, instead we always use the original model for Fϕ for stabilizing the training. The learning rate of Fϕ is the same as the learning rate of CMs. More details and results are provided in Tables 4 to 6. 29 Table 4: Training settings of all models and training algorithms on ImageNet 512512 dataset. 2048 192 positional 70000 0.9 0.99 1.0e-11 280.2 1048k 1.0e-2 0% -0.4 1.0 1.0e-4 -0.8 1.6 0.1 10k 0.025 100k 25% 200k 0% 2.0 Model details Batch size Channel multiplier Time embedding layer noise conditioning cnoise(t) adaptive double normalization Learning rate decay (tref) Adam β1 Adam β2 Adam ϵ Model capacity (Mparams) Training details of diffusion models (TrigFlow) Training iterations Learning rate max (αref) Dropout probability Proposal Pmean Proposal Pstd. Shared details of consistency models Learning rate max (αref) Proposal Pmean Proposal Pstd. Tangent normalization constant (c) Tangent warm up iterations EMA length (σrel) of pretrained diffusion Training details of sCT Training iterations Dropout probability for resolution 16 Training details of sCD Training iterations Dropout probability for resolution 16 Maximum of CFG scale Training details of sCD with adaptive VSD Training iterations Learning rate max (αref) for Fϕ Dropout probability for Fϕ Proposal Pmean for LDiff(ϕ) Proposal Pstd. for LDiff(ϕ) Number of updating of ϕ per updating of θ One-step sampling starting time tmax Proposal Pmean for LVSD(θ) Proposal Pstd. for LVSD(θ) Loss weighting λVSD for LVSD 20k 1.0e-4 0% -0.8 1.6 1 arctan( 80 σd 0.4 2.0 1.0 Model Size 2048 320 positional 70000 0.9 0.99 1.0e-11 777.6 XL XXL 2048 384 positional 70000 0.9 0.99 1.0e-11 1119. 2048 448 positional 70000 0.9 0.99 1.0e-11 1523.4 696k 8.0e-3 10% -0.4 1.0 8.0e-5 -0.8 1.6 0.1 10k 0.015 100k 35% 200k 10% 2.0 598k 7.0e-3 10% -0.4 1. 7.0e-5 -0.8 1.6 0.1 10k 0.02 100k 35% 200k 10% 2.0 376k 6.5e-3 10% -0.4 1.0 6.5e-5 -0.8 1.6 0.1 10k 0.015 100k 35% 200k 10% 2.0 2048 256 positional 70000 0.9 0.99 1.0e-11 497.8 1048k 9.0e-3 10% -0.4 1.0 9.0e-5 -0.8 1.6 0.1 10k 0.03 100k 35% 200k 10% 2. 20k 9.0e-5 10% -0.8 1.6 1 arctan( 80 σd 0.4 2.0 1.0 ) 20k 8.0e-5 10% -0.8 1.6 1 arctan( 80 σd 0.4 2.0 1.0 ) 20k 7.0e-5 10% -0.8 1.6 1 arctan( 80 σd 0.4 2.0 1.0 ) 20k 6.5e-5 10% -0.8 1.6 1 arctan( 80 σd 0.4 2.0 1.0 ) ) 30 Table 5: Evaluation of sample quality of different models on ImageNet 512512 dataset. Results of EDM2 (Karras et al., 2024) are with EDM parameterization and the original AdaGN layer. The FDDINOv2in EDM2 are obtained by tuned EMA rate, which is different from our EMA rates that are tuned for FID scores. M Model Size XL XXL 1.1 5.84 5.53 192.13 160.66 1.1 10.13 9.86 278.35 244. 0.030 1.2 1.8 2.00 2.01 43.33 41.98 0.025 1.4 2.0 2.29 2.23 52.08 52.32 Sampling by diffusion models (NFE = 126) EMA length (σrel) Guidance scale for FID Guidance scale for FDDINOv2 FID (TrigFlow) FID (EDM2) FDDINOv2(TrigFlow) FDDINOv2(EDM2) with σrel for FDDINOv2 Sampling by consistency models trained with sCT Intermediate time tmid in 2-step sampling 1-step FID 2-step FID 1-step FDDINOv2 2-step FDDINOv2 Sampling by consistency models trained with sCD Intermediate time tmid in 2-step sampling Guidance scale for FID, 1-step sampling Guidance scale for FID, 2-step sampling Guidance scale for FDDINOv2, 1-step sampling Guidance scale for FDDINOv2, 2-step sampling 1-step FID 2-step FID 1-step FDDINOv2 2-step FDDINOv2 Sampling by consistency models trained with multistep sCD 1.2 Guidance scale for FID 2.0 Guidance scale for FDDINOv2 FID, = 2 2.51 FID, = 4 2.46 FID, = 8 2.24 FID, = 16 2.18 FDDINOv2, = 2 60.47 FDDINOv2, = 4 56.38 FDDINOv2, = 8 49.46 FDDINOv2, = 16 46.94 Sampling by consistency models trained with sCD + adaptive VSD Intermediate time tmid in 2-step sampling 1.1 1.0 Guidance scale for FID, 1-step sampling 1.0 Guidance scale for FID, 2-step sampling 1.5 Guidance scale for FDDINOv2, 1-step sampling 1.5 Guidance scale for FDDINOv2, 2-step sampling 2.67 1-step FID 2.29 2-step FID 54.81 1-step FDDINOv2 53.53 2-step FDDINOv2 1.1 1.5 1.4 2.0 2.0 3.07 2.50 104.22 71.15 1.4 2.0 2.79 2.78 2.49 2.34 76.29 72.01 60.13 55.89 1.1 1.3 1.2 2.0 2.0 2.75 2.26 83.78 55. 1.1 1.2 1.2 1.7 1.7 3.37 2.70 72.12 69.00 0.015 1.2 1.8 1.87 1.88 39.23 38.20 0.020 1.2 1.8 1.80 1.85 36.73 35.67 0.015 1.2 1.8 1.73 1.81 35.93 33.09 1.1 5.15 4.65 169.98 135.80 1.1 4.33 3.73 147.06 114. 1.1 4.29 3.76 146.31 112.69 1.1 1.3 1.2 2.0 1.9 2.55 2.04 76.10 50.63 1.2 2.0 2.32 2.28 2.04 1.99 54.91 50.99 44.87 42.55 1.1 1.0 1.0 1.6 1.6 2.26 1.99 50.46 48.54 1.1 1.3 1.2 2.0 1.9 2.40 1.93 70.30 46.66 1.15 1.9 2.29 2.22 2.02 1.90 51.91 47.61 41.26 39. 1.1 1.0 1.0 1.5 1.5 2.39 2.01 48.11 46.61 1.1 1.3 1.2 2.0 1.9 2.28 1.88 67.80 44.97 1.15 1.9 2.16 2.10 1.90 1.82 50.70 46.78 40.56 38.55 1.1 1.0 1.0 1.5 1.5 2.16 1.89 45.54 43.93 31 Table 6: Ablation of adaptive VSD and sCD on ImageNet 512512 dataset with model size M. EMA length (σrel) Guidance scale for FID, 1-step sampling Guidance scale for FID, 2-step sampling Guidance scale for FDDINOv2, 1-step sampling Guidance scale for FDDINOv2, 2-step sampling 1-step FID 2-step FID 1-step FDDINOv2 2-step FDDINOv2 Method sCD 0.05 1.3 1.2 2.0 2.0 2.75 2.26 83.78 55.70 sCD + VSD 0.05 1.0 1.0 1.5 1.5 2.67 2.29 54.81 53.53 VSD 0.05 1.1 1.4 3.02 57.19 Table 7: Evaluation of sample quality of different models on ImageNet 6464 dataset. Sampling by diffusion models (NFE=63) EMA length (σrel) FID (TrigFlow) Sampling by consistency models trained with sCT Intermediate time tmid in 2-step sampling 1-step FID 2-step FID Sampling by consistency models trained with sCD Intermediate time tmid in 2-step sampling 1-step FID 2-step FID Model Size 0.06 1.55 1.1 2.25 1.81 1.1 2.79 1. 0.04 1.44 1.1 2.08 1.57 1.1 2.43 1.70 0.075 1.70 1.1 3.23 2. 1.1 2.97 2.07 XL 0.04 1.38 1.1 2.04 1.48 1.1 2.44 1."
        },
        {
            "title": "H ADDITIONAL SAMPLES",
            "content": ". 9 1 a g , ) o ( 5 1 l 9 . n u , ) l ( 9 2 l 9 . 1 a g , ) h g ( 3 3 l 9 . 1 a g , ) c ( 8 8 l Figure 8: Uncurated 1-step samples generated by our sCD-XXL trained on ImageNet 512512. 9 1 . a g , ) o ( 5 1 l 9 . 1 a g , ) l ( 9 a 9 . 1 a g , ) h g ( 3 3 l 9 . 1 a g , ) a ( 8 8 l Figure 9: Uncurated 2-step samples generated by our sCD-XXL trained on ImageNet 512512. 34 9 . 1 a g , ) t i ( 7 2 1 l . 1 a g , ) a ( 3 2 3 l 9 . 1 a g , ) a s ( 7 8 3 l 9 . 1 a g , ) a a ( 8 8 3 l Figure 10: Uncurated 1-step samples generated by our sCD-XXL trained on ImageNet 512512. 9 . 1 a g , ) t i ( 7 2 1 l 9 . n u , ) a ( 3 2 3 l 9 . 1 a g , ) a s ( 7 8 3 l 9 . 1 a g , ) a a ( 8 8 3 l Figure 11: Uncurated 2-step samples generated by our sCD-XXL trained on ImageNet 512512. 36 9 1 . n u , ) l ( 7 1 4 l . 9 1 a g , ) b ( 5 2 a 9 . 1 a g , ) r s c ( 3 3 9 l 9 . n u , ) r o ( 3 7 9 l Figure 12: Uncurated 1-step samples generated by our sCD-XXL trained on ImageNet 512512. 37 9 1 . a g , ) l ( 7 1 a . 9 1 a g , ) b ( 5 2 4 l 9 . 1 a g , ) r s c ( 3 3 9 l 9 . 1 a g , ) r o ( 3 7 9 l Figure 13: Uncurated 2-step samples generated by our sCD-XXL trained on ImageNet 512512."
        }
    ],
    "affiliations": [
        "OpenAI"
    ]
}