{
    "paper_title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation",
    "authors": [
        "Yuming Jiang",
        "Siteng Huang",
        "Shengke Xue",
        "Yaxi Zhao",
        "Jun Cen",
        "Sicong Leng",
        "Kehan Li",
        "Jiayan Guo",
        "Kexiang Wang",
        "Mingxiu Chen",
        "Fan Wang",
        "Deli Zhao",
        "Xin Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 2 1 2 5 1 . 9 0 5 2 : r RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation Yuming Jiang1, Siteng Huang1,2, Shengke Xue1,2, Yaxi Zhao1,2, Jun Cen1,2, Sicong Leng1, Kehan Li1,2, Jiayan Guo1, Kexiang Wang1, Mingxiu Chen1,2, Fan Wang1, Deli Zhao1,2, Xin Li1, 1DAMO Academy, Alibaba Group 2Hupan Lab This paper presents RynnVLA-001, vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides more effective initialization for VLA models. GitHub: https://github.com/alibaba-damo-academy/RynnVLA-001 Date: September 19,"
        },
        {
            "title": "1 Introduction",
            "content": "The past few years have witnessed rapid progress in large language models (Comanici et al., 2025; Anthropic; OpenAI, 2025; Grattafiori et al., 2024; Guo et al., 2025a; Yang et al., 2025), large multimodal models (OpenAI, 2024; Bai et al., 2025; Zhu et al., 2025; Guo et al., 2025b), vision-based recognition models (He et al., 2022; Assran et al., 2023; Oquab et al., 2023; Ravi et al., 2024; Tschannen et al., 2025), and generative models (Peebles and Xie, 2023; Esser et al., 2024; Tian et al., 2024; Baldridge et al., 2024). The success in these fields is attributed to the availability of large-scale datasets. For instance, large language models benefit from abundant training data readily accessible from web sources. In contrast, progress in Vision-Language-Action (VLA) models is constrained by the scarcity of large-scale robot manipulation data. Collecting such data typically relies on human teleoperation on physical robots to record manipulation trajectories, making large-scale dataset construction both labor-intensive and costly. There have been some early attempts to address the challenges of data scarcity. On the one hand, some methods propose to build large-scale robot manipulation datasets (ONeill et al., 2024; Liu et al., 2024b; Khazatsky et al., 2024; Bu et al., 2025), which collect manipulation data under diverse environments and even with different embodiments. However, the size of these datasets still remains far smaller than those used in LLMs, VLMs, and generative models (Abadji et al., 2022; Schuhmann et al., 2022; Chen et al., 2024; Li et al., 2024a). Another line of studies works on exploiting massive prior knowledge from pretrained generative models (Cheang et al., 2024; Hu et al., 2024) or VLMs (Zitkovich et al., 2023; Kim et al., 2024; Li et al., 2024b; Black et al., 2024; Bjorck et al., 2025b; Kim et al., 2025; Liu et al., 2024b) to alleviate the data scarcity problem. In this work, we propose RynnVLA-001, VLA model enhanced by video generation pretraining. The key insight of RynnVLA-001 is to implicitly transfer the manipulation skills learned from human demonstrations in ego-centric videos to robot manipulation. The overall training pipeline is shown in Fig. 1. In the first stage, Ego-Centric Video Generative Pretraining, we train an Image-to-Video (I2V) model that takes single"
        },
        {
            "title": "INTRODUCTION",
            "content": "Figure 1 Training data pipeline of RynnVLA-001. Our framework leverages three types of training data: (1) Ego-Centric Video Generative Pretraining uses millions of ego-centric human manipulation videos for future frame prediction. (2) Human-Centric Trajectory-Aware Video Modeling trains on videos with human keypoint annotations, enabling joint prediction of frames and trajectories. (3) Robot-Centric Vision-Language-Action Modeling employs robot datasets paired with language instructions to learn mappings from visual observations and language to robotic actions. image and language instruction as inputs and predicts subsequent frames. To capture general manipulation dynamics, this stage relies on ego-centric human manipulation videos, which emphasize first-person hand operations. We design dedicated data curation pipeline to filter out 12M ego-centric manipulation videos from existing web sources. Trained with ego-centric videos, the model is capable of predicting manipulations at the visual level. However, gap remains between the high-level visual observations and the low-level action spaces required to control real robots. To bridge the gap, we introduce another stage of Human-Centric Trajectory-Aware Video Modeling, where we further train the I2V model on ego-centric videos paired with human keypoint annotations. In this stage, in addition to future frames, the model is also trained to predict the keypoint trajectories in future frames conditioned on current observations and language instructions. These keypoint-based patterns share similarities with robot actions, thereby facilitating the transfer from visual dynamics to robot manipulation with low-level actions. Following previous pretraining stages, we further adapt the model using self-collected robot datasets. In this phase, the model is trained to predict action chunks rather than single-step action, conditioned on RGB observations and language instructions. To ensure the smoothness and temporal coherence of predicted actions, we propose ActionVAE, variational autoencoder that encodes action chunks into compact embeddings. Once trained, the ActionVAE is fixed and employed to extract latent representations of future actions. The model is then optimized to predict these action embeddings alongside future visual observations. During inference, given an observation and language instruction, the model outputs single action embedding, which is subsequently decoded by ActionVAE into sequence of executable robot actions. Our proposed RynnVLA-001 enables robot arm to execute complex pick-and-place and long-horizon tasks by accurately following high-level language instructions. To evaluate the effectiveness of the pretraining weights of RynnVLA-001, we compare our proposed RynnVLA-001 with state-of-the-art models, including GR00T N1.5 (Bjorck et al., 2025a) and Pi0 (Black et al., 2024), by finetuning on the same robot manipulation data. Our proposed RynnVLA-001 consistently achieves higher success rates, demonstrating that the proposed pretraining framework provides more effective initialization for VLA modeling."
        },
        {
            "title": "2.1 Vision-Language-Action Models",
            "content": "The recent development of robotic agents relies on harnessing the sophisticated capabilities of pretrained Vision-Language Models (VLMs), enabling embodied systems to effectively perceive, reason, and act within complex environments. RT-2 (Zitkovich et al., 2023) pioneered the concept of vision-language-action models (VLAs), which co-finetuned VLMs with both robotic trajectory data and large-scale Internet vision-language tasks. key strategy in this line of work was the discretization of continuous actions into categorical bins, making them compatible with standard LLM backbones. This approach was further advanced by models like OpenVLA (Kim et al., 2024), which improved generalization through extensive pretraining, and FAST (Pertsch et al., 2025), which explored more efficient tokenization schemes. However, action discretization often leads to precision loss in action representations. VQ-VLA (Wang et al., 2025) proposed to train VQVAE to discretize the action chunks, which improved the accuracy of action representation. Another line of approaches proposed to integrate policy heads specifically designed for continuous action generation. LCB (Shentu et al., 2024) pioneered such dual-system architecture by leveraging pretrained VLM (Liu et al., 2023) for high-level reasoning and scene understanding, while employing separate and specialized policy head to generate the continuous and low-level actions (Ke et al., 2024). Numerous subsequent studies have built upon this dual-system paradigm, exploring various refinements such as optimizing interaction frequencies, employing different policy head models like diffusion transformers (Peebles and Xie, 2023), and incorporating diverse training strategies across multiple embodiments (Zhang et al., 2024; Li et al., 2024b; Wen et al., 2025; Lin et al., 2025; Cui et al., 2025; Shukor et al., 2025; Cheang et al., 2025). Notably, recent efforts have matured this concept into more general frameworks. For instance, Pi0 (Black et al., 2024) integrated PaliGemma (Beyer et al., 2024) with conditional flow matching (Lipman et al., 2023) for continuous control. Similarly, the GR00T project (Bjorck et al., 2025b,a) has delivered an end-to-end trained and open-source dual-system on complex hardware like humanoid robots. These frameworks represent significant step towards advanced VLM-based VLA models."
        },
        {
            "title": "2.2 Future Prediction for Robot Learning",
            "content": "Existing VLM-based policies are often limited by inadequate modeling of visual dynamics, as they typically rely on one or two static images from the current observation. To address this, significant stream of research has focused on incorporating future prediction to explicitly model physical dynamics and thereby improve policy learning. These efforts can be broadly categorized into three paradigms. The first paradigm conditions the policy on an explicitly generated future state. Early works like SuSIE (Black et al., 2023) and UniPi (Du et al., 2023) leveraged image editing or generation models to produce single future keyframe, which then served as visual goal for the action policy. This concept was later advanced by methods such as GEVRM (Zhang et al., 2025), which utilized powerful video foundation models like Open-Sora (Zheng et al., 2024) to generate more expressive future goal sequences. DREAMGEN (Jang et al., 2025) employed video world models to generate large-scale synthetic robot data for offline policy training. more integrated paradigm unifies future prediction and action generation within single architecture. Some approaches (Wu et al., 2024; Tian et al., 2025; Zhao et al., 2025) enforced sequential dependency to generate future subgoal image before predicting the action sequence. PAD (Guo et al., 2024) employed diffusion model to jointly forecast future images and actions, while WorldVLA (Cen et al., 2025) developed an autoregressive framework where multi-step future state prediction and action generation mutually inform and benefit each other. The third paradigm leverages future video prediction primarily as powerful pretraining objective for representation learning, rather than as direct component during inference. For instance, GR-2 (Cheang et al., 2024) pretrained model on vast internet video datasets before finetuning for action prediction, while VPP (Hu et al., 2024) successfully leveraged representations from pre-existing video foundation models for robotic control. While these methods validate the potential of video pretraining, our work advances this paradigm by introducing multi-stage curriculum that progressively connects visual generation with robot control. We first pretrain video generation model on large-scale ego-centric videos. Next, we introduce an intermediate stage that incorporates human trajectory prediction to bridge the gap between purely visual prediction and action generation. Finally, this trajectory-aware model is finetuned on robot-specific data."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "Figure 2 Model architecture and training stages of RynnVLA-001. The training consists of three stages: (1) Ego-Centric Video Generative Pretraining trains transformer-based Image-to-Video (I2V) model for future frame prediction. (2) Human-Centric Trajectory-Aware Video Modeling extends the I2V model with action (trajectory) prediction heads, incorporating both visual and state embeddings (blue blocks). (3) Robot-Centric Vision-Language-Action Modeling transfers pretrained weights to robot data, where the model generates action embeddings decoded by ActionVAE into executable actions."
        },
        {
            "title": "3 Methodology",
            "content": "In this work, we propose RynnVLA-001, Vision-Language-Action (VLA) model built upon large-scale video generation pretraining. We first train an ego-centric Image-to-Video (I2V) model, then finetune it into trajectory-aware ego-centric video generation model. Finally, we adapt the model into VLA model by inheriting the pretrained weights. The model is progressively trained through three stages, as illustrated in Fig. 2: 1) Ego-Centric Video Generative Pretraining: An ego-centric Image-to-Video (I2V) model is trained on ego-centric human manipulation videos. This stage enables the model to predict future frames. 2) Human-Centric Trajectory-Aware Video Modeling: The pretrained I2V model is further finetuned to jointly predict future frames and human keypoint trajectories. This stage bridges the gap between purely visual frame prediction and action-oriented modeling. 3) Robot-Centric Vision-Language-Action Modeling: The VLA model inherits the weights from the previous stages and is trained on robot data using language instructions and current observations (including two-view observations and joint states) as inputs. It is optimized to predict both future frames and action embeddings, with the latter decoded by the pretrained ActionVAE into executable robot actions."
        },
        {
            "title": "3.1 Ego-Centric Video Generative Pretraining",
            "content": "A key challenge in scaling VLA models is the scarcity of large-scale paired training data. To address this, we transfer priors from human demonstrations through large-scale video pretraining, beginning with Ego-Centric Video Generative Pretraining. The objective of this stage is to train an I2V model that closely mimics the inference process of VLA model. In typical VLA setting, actions are predicted conditioned on current observations (e.g., visual inputs and robot states) and language instruction. Accordingly, our I2V model is trained to predict future video frames based on an initial visual observation and corresponding language description. This pretraining task forces the model to learn the physical dynamics of object manipulation from an ego-centric perspective."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "For the network architecture, we adopt an autoregressive (AR) Transformer. Due to the limited availability of AR-based video generation frameworks, we extend powerful AR image generation model, Chameleon (Team, 2024; Liu et al., 2024a), to perform the I2V task. To ensure the learned priors are directly relevant for robotic manipulation, we curate 12M ego-centric human manipulation videos for training. These videos contain first-view human operations and focus on hand manipulations that are analogous to robot gripper movements and operations (Details of the data curation pipeline will be provided in Sec. 4.). Besides, we also filter 244K robotic manipulation videos from open-source datasets (Brohan et al., 2022; Walke et al., 2023; ONeill et al., 2024; Khazatsky et al., 2024). As shown in Fig. 2, instead of providing the instruction only once, language tokens are interleaved with visual tokens to better align with VLA inference, where each action prediction is conditioned on both modalities. The input sequence is structured as [language tokens, visual tokenst, language tokens, visual tokenst+1, ...]. The training is supervised by the cross-entropy loss over discrete visual tokens and language tokens."
        },
        {
            "title": "3.2 Human-Centric Trajectory-Aware Video Modeling",
            "content": "While the I2V model from Stage 1 learns to predict dynamics at the visual level, it lacks an explicit understanding of actions, which is essential for the final VLA modeling. To bridge the gap between purely visual prediction and action generation, Stage 2 refines the pretrained model into trajectory-aware framework. The key idea of this stage is to finetune the pretrained model with multi-task objective: to concurrently predict future visual frames and the corresponding human keypoint trajectories. Human trajectories can be regarded as another form of actions. By learning to associate visual changes with their underlying motion trajectories, the model develops more holistic understanding of manipulation dynamics. To this end, we adopt the EgoDex dataset (Hoque et al., 2025), which provides trajectories of all upper-body joints captured via Apple Vision Pro devices. Among these, we selectively use only the wrist keypoints, as they approximate end-effector positions. Crucially, rather than predicting raw coordinates, the model is trained to predict compact and continuous embeddings of trajectory chunks generated by pretrained ActionVAE (detailed in Sec. 3.3). This design yields dense representation of motion while reducing prediction complexity. To provide the model with proprioceptive information, we introduce state embeddings (blue blocks in Fig. 2). These embeddings represent the current keypoint positions of the human wrists and are fed into the model at each timestep. These are projected into the transformers input dimension via linear layer and interleaved with other tokens. The input sequence is now structured as: [language, visual tokenst, state embeddingt, <ACTION_PLACEHOLDER>, ...], where <ACTION_PLACEHOLDER> is the signal to generate continuous action embeddings. This sequence explicitly provides the model with three crucial pieces of information: the high-level goal (language), the current visual scene (visual tokens), and the current physical configuration of the wrists (state embeddings). The architecture from Stage 1 is extended to handle the new and continuous prediction target. lightweight action head (a single linear layer) is introduced. The transformers main output remains discrete visual tokens, while the action head maps the last hidden state to the continuous latent space of action embeddings. The training of the action head is supervised by L1 loss, which is computed exclusively for the outputs at token positions of <ACTION_PLACEHOLDER>. The prediction of visual tokens remains the same as Stage 1."
        },
        {
            "title": "3.3 ActionVAE: Action Representaton via VAE",
            "content": "In VLA models, predicting action chunks (i.e., short sequences of actions) is more effective than predicting single and step-by-step actions (Zhao et al., 2023; Kim et al., 2025). This design choice is motivated by two key factors: 1) Avoiding repetitive predictions: Single-step action execution often results in negligible visual changes, which can cause the model to repeatedly output the same action and become stuck. Predicting chunk encourages more substantial visual changes. 2) Efficiency: Generating multiple actions in single forward pass reduces computational overhead and inference latency."
        },
        {
            "title": "3 METHODOLOGY",
            "content": "To facilitate this chunk-level prediction while ensuring the generated actions are smooth and coherent, we introduce an Action Variational Autoencoder (ActionVAE) in Stage 2&3. As illustrated in Fig. 2, the ActionVAE consists of an encoder that compresses an action chunk into compact and continuous latent embedding, and decoder that reconstructs the original action sequence from this embedding. Since our training pipeline involves both human demonstrations and robot executions, and their kinematic spaces differ, we train two domain-specific ActionVAEs: one for compressing human trajectories (used in Stage 2) and another for compressing robot actions (used in Stage 3). This ensures that each domain has tailored and accurate action representation. Importantly, the ActionVAE is embodiment-specific. As it encodes chunk-level actions that often correspond to atomic motion primitives, well-trained model can be directly used to extract action embeddings from new data on the same embodiment without retraining1."
        },
        {
            "title": "3.4 Robot-Centric Vision-Language Action Modeling",
            "content": "In the final stage, we adapt the pretrained trajectory-aware model into VLA model for robot control. This is achieved by integrating the robot-specific ActionVAE representations and finetuning the model on robot-centric data. The models primary objective is to predict the embedding of the next robot action chunk, which is then decoded by the ActionVAE into an executable action sequence. The architecture largely inherits the framework from Stage 2 but is now adapted for the robot domain. key modification lies in the action head. Since human hand trajectories differ substantially from robot arm kinematics, the action head pretrained in Stage 2 is discarded. Instead, new lightweight action head (a single linear layer) is initialized to predict robot action embeddings. The input sequence for the VLA model is structured to mirror the real-world robotic deployment scenario, using the same placeholder-based design as Stage 2: [language, visual tokenst, state embeddingt, <ACTION_PLACEHOLDER>, ...], where the visual tokens now comprise two camera views (a front view and wrist view), as opposed to the single-view input in Stage 2. This sequence explicitly provides four components: (1) the high-level goal (language), (2) robot-centric visual observations (front and wrist views), (3) the current robot state, and (4) signal to generate the next action (<ACTION_PLACEHOLDER>). During training, the model is optimized with two concurrent objectives: 1) Robot Action Prediction: The hidden state corresponding to the output of the <ACTION_PLACEHOLDER> token is fed into the newly initialized action head. This head regresses the hidden state to continuous embedding representing the next robot action chunk. The training of this head is supervised by an L1 loss between the predicted embedding and the ground-truth embedding from the robot-specific ActionVAE. 2) Future Visual Prediction: The model continues the autoregressive prediction of visual tokens for the next frame, supervised by cross-entropy loss. This auxiliary task regularizes training and preserves the models understanding of world dynamics."
        },
        {
            "title": "3.5 Inference",
            "content": "During inference, the VLA model operates within closed-loop control cycle to perform tasks. At each step of the cycle, the model receives the language instruction, the current RGB observation from the robots cameras and current robot states as inputs. Crucially, to optimize for efficiency, we make modification at inference time. The model only predict the action embedding, and discards the generation of future vision tokens. While predicting future frames serves as valuable auxiliary task for regularization during training, it is computationally expensive and unnecessary for control. Discarding this process significantly increases inference speed, making the model more practical for real-time applications. 1Our pretrained ActionVAE can extract action embeddings from unseen data. Our demo video demonstrates this: although the ActionVAE was never trained on tasks such as pick up the small cube and put it on the big cube, pick up the holder and place it straight then pick up the pen and place it in the holder, Tower of Hanoi with two disks, and put the strawberry and solid glue on the palm, the embeddings it extracts can be used to train the downstream VLA model."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Figure 3 Illustration of Evaluation Tasks. We evaluate the performance of VLA models on three tasks: (1) pick up and place green blocks, (2) pick up and place strawberries, and (3) grab pen and put it into holder. Each task is evaluated under three settings: single-target manipulation, multi-target manipulation (first three images), and instruction-following with distractors (rightmost image). The predicted action embedding is then immediately passed to the decoder of the ActionVAE. The decoder reconstructs coherent sequence (or chunk) of low-level robot actions from this single embedding. The robot executes this entire action chunk. Upon completion, new observation is captured and fed back into the model along with the language instruction, initiating the next cycle. This process repeats until the task is successfully completed."
        },
        {
            "title": "4 Ego-Centric Video Data Curation Pipeline",
            "content": "To pretrain our model on relevant human manipulation demonstrations, we construct large-scale dataset of ego-centric manipulation videos from web sources (Wang et al., 2024b; Grauman et al., 2022; Miech et al., 2019; Damen et al., 2022, 2018, 2021; Goyal et al., 2017; Mahdisoltani et al., 2018). Since raw video data is noisy and highly diverse, we design multi-stage data curation pipeline to filter and annotate videos suited for our pretraining stage. The pipeline consists of the following steps: Keypoint Detection. For each frame in the video, we apply pose estimation model (Yang et al., 2023) to extract human keypoints, including facial landmarks, torso joints, and hand-related keypoints (e.g., wrists, elbows, fingers). Ego-centric Filtering. We apply two key filtering criteria to retain only high-quality ego-centric manipulation videos: 1) No facial keypoints: Videos containing facial landmarks are discarded. The appearance of human face strongly suggests third-person perspective, which is not suitable for ego-centric modeling. 2) Presence of hand keypoints: We retain only frames where keypoints corresponding to the wrists and hands are visible. The presence of hands near the camera is strong indicator of ego-centric human manipulation, which is essential for modeling transferable robotic behavior. Text Description Annotation. For each curated video, we use the Qwen2-VL-7B (Wang et al., 2024a) to generate concise textual descriptions. These descriptions are intentionally kept short to mimic natural language instructions typically used in robot learning (e.g., put the bottle in the box, open the drawer). The resulting text-video pairs serve as effective supervision signals for vision-action alignment."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Dataset. To train and evaluate our proposed RynnVLA-001 model, we collect new real-world manipulation dataset using LeRobot SO100 robotic arm (Cadene et al., 2024). The dataset comprises expert demonstrations collected through human teleoperation. To ensure our dataset covers basic manipulation skills, we design and collect data for three representative tasks as shown in Fig. 3: 1) Pick up and place green blocks: This task focuses on fundamental object recognition and grasping abilities. We collect 248 demonstrations. 2) Pick"
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Table 1 Performance comparison on three manipulation tasks. We report task-specific success rates, average success rate over three tasks, and SR@1. Each number represents the average SR across the three evaluation settings in Tab. 2. Method Task-Specific Success Rate (%) Average (%) SR@1 (%) Pick up and place green blocks Pick up and place strawberries Grab pen and put it into holder GR00T N1.5 Pi0 RynnVLA-001 (Ours) 65. 75.6 90.0 53.3 71.1 91.7 48. 64.4 90.0 55.6 70.4 90.6 37. 56.3 56.7 Table 2 Performance comparison on three different evaluation settings. Single-target manipulation refers to the setting only single target object present on the desktop. Multi-target manipulation means multiple target objects present on the desktop. Instruction-following with distractors refers to the test cases where target objects and distractor objects are present on the desktop at the same time. Method Single-Target Manipulation Multi-Target Manipulation Instruction-following with Distractors GR00T N1.5 Pi0 RynnVLA-001 (Ours) 63.3 80.0 93. 46.7 71.1 86.7 56.7 60.0 91. up and place strawberries: This task requires precise localization and grasping point estimation, focusing on the models fine-grained perception capabilities. We collect 249 demonstrations. 3) Grab pen and put it into holder: This task demands advanced 3D spatial reasoning, specifically the ability to infer object orientation and height for precise insertion action. We collect 301 demonstrations. To enhance the richness and complexity of the data, the scenes of manipulation are set to vary from containing only target objects to more complex arrangements that include other irrelevant, distractor objects. During teleoperation, the human operators goal is to move all target objects to their destination. Furthermore, the data is collected using three different SO100 arms in various environments with different lighting conditions. Baselines. We compare our model with two strong open-source baselines, namely GR00T N1.5 (Bjorck et al., 2025a) and Pi0 (Black et al., 2024). We initialize these models from the corresponding pretrained weights and then finetune the model with the same SO100 data as our model. We use the official code of GR00T N1.5 and Pi0 and strictly follow the instructions to finetune the model. Evaluation. We consider three different scenarios in our evaluation protocol: 1) Single-target Manipulation, where only single target object is on the desktop, 2) Multi-target Manipulation, where multiple target objects are on the desktop, 3) Instruction-following with Distractors, where both target objects and distractor objects appear on the desktop. For all scenarios, trial is considered success if the model correctly places at least one target object in its designated location within predefined time limit. trial is marked as failure under any of the following conditions: 1) The time limit is exceeded. 2) The model makes more than five consecutive failed attempts to grasp target object. 3) Specifically, for the Instruction-Following with Distractors scenario, the model attempts to manipulate any distractor objects. We also report the Success Rate@1 (SR@1) metric, defined as the percentage of tasks successfully completed within single trial. To evaluate generalization, each task is evaluated on multiple robotic arms, each operating in unique physical environment."
        },
        {
            "title": "5.2 Comparison with SoTA methods",
            "content": "Table 1 presents detailed comparison of task-specific and average success rates. Our model, RynnVLA-001, demonstrates substantially higher overall performance, outperforming both GR00T N1.5 and Pi0 across all three tasks. As for the SR@1 metric, the performance of our proposed RynnVLA-001 is comparable to Pi0. The relatively low SR@1 values for all three models suggest that there is still large zoom for improving object localization accuracy to achieve reliable single-trial success."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Table 3 Effectiveness of Pretrained Weight on three evaluation settings. We train four variants of RynnVLA-001 with four different initialized weights. We report the success rates on three tasks, average success rate and SR@1. Method Task-Specific Success Rate (%) Average (%) SR@1 (%) Pick up and place green blocks Pick up and place strawberries Grab pen and put it into holder RynnVLA-001-Scratch2 RynnVLA-001-Chameleon RynnVLA-001-Video RynnVLA-001 (Full) 0 56.6 81.7 90.0 6.7 50. 86.7 91.7 6.7 43.3 85.0 90. 4.4 50.0 84.4 90.6 0 22. 49.4 56.7 Table 4 Effectiveness of Pretrained Weight. We train four variants of RynnVLA-001 with four different initialized weights. We report the success rates in three different evaluation settings. Method Single-Target Manipulation Multi-Target Manipulation Instruction-Following with Distractors RynnVLA-001-Scratch RynnVLA-001-Chameleon RynnVLA-001-Video RynnVLA-001 (Full) 0 55. 93.3 93.3 6.7 53.3 78.3 86. 6.7 41.7 81.7 91.7 In Table 2, we report success rates in three different evaluation settings. The task becomes more challenging when more objects appear in the scene. For GR00T N1.5, the success rates of multi-target manipulation and instruction-following with distractor objects become lower than those of single-target manipulation. For Pi0, when distractor objects appear on the desk, the success rates drop significantly, indicating its limited instruction-following capability. In contrast, the performance of our proposed RynnVLA-001 remains stable."
        },
        {
            "title": "5.3 Effectiveness of Pretraining",
            "content": "In RynnVLA-001, we propose two pretraining stages: 1) ego-centric video generative pretraining and 2) human-centric trajectory-aware video modeling. To investigate the effectiveness of our proposed two-stage pretraining pipeline, we conduct comprehensive ablation study, with results presented in Table 3 and Table 4. First, we evaluate the impact of Stage 1: Ego-centric Video Generative Pretraining. We compare three initialization strategies for the final VLA model: 1) RynnVLA-001-Scratch: baseline initialized from random weights, skipping all pretraining. 2) RynnVLA-001-Chameleon: stronger baseline initialized directly from the pretrained weights of the Chameleon Text-to-Image (T2I) model (Team, 2024), bypassing our video pretraining stage. 3) RynnVLA-001-Video: Our model after the training of Stage 1, which starts from Chameleon weights but is further pretrained on ego-centric videos. The results clearly demonstrate the importance of video-centric pretraining. The RynnVLA-001-Scratch model is incapable of correlating language instructions with meaningful actions, resulting in an extremely low success rate. Benefiting from the pretrained T2I checkpoint, the RynnVLA-001-Chameleon model achieves reasonable results on simple grasping. However, it exhibits limited localization capability, capping its performance at success rate of 50.0%. In contrast, RynnVLA-001-Video achieves significant performance improvement, indicating that priors learned from ego-centric videos are effective for VLA adaptation. Next, we build upon this video-pretrained model to evaluate the contribution of Stage 2: Human-centric Trajectory-Aware Video Modeling. By incorporating this second pretraining stage where the model learns to predict human trajectories, our full model, RynnVLA-001, achieves the best performance among all variants. This final improvement shows the benefit of explicitly bridging the gap between visual prediction and action generation by pretraining the model to predict human trajectories. 2Given the low performance of the RynnVLA-001-Scratch model, its evaluation is limited to 5 trials per task and setting, and conducted on single robot arm. In contrast, all other models are evaluated with 60 trials per task. These trials are distributed evenly across two robotic arms, with each arm conducting 10 trials for each of the three scenarios (totaling 30 trials per arm)."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Table 5 Ablation Study of VLA Components on the Calvin Benchmark. All models are trained with reduced epochs for efficiency; scores are for relative comparison. Method Task Task Success Rate (%) Avg. Len. 2 3 4 5 256 256 Task ABC -> 92. Raw Actions Prediction Task ABC -> 93.8 Deeper Action Head Task ABC -> 90.2 83.7 86.5 77. 73.5 80.4 65.3 62.1 74.2 54. 53.2 67.0 44.3 3.652 4.019 3. Full Model Task ABC -> 95.4 88.2 82.2 78.2 72. 4."
        },
        {
            "title": "5.4 Ablation Study on Model Designs",
            "content": "To systematically evaluate the impact of the developed key components, we conduct series of ablation studies on the Calvin Benchmark (Mees et al., 2022). For experimental efficiency, our full model and the ablated variants are trained from the pretraining weights of RynnVLA-001-Video but for reduced number of epochs. 3 Consequently, the results presented in this section are intended for comparative analysis to demonstrate the relative importance of each component. Image Resolution. In this study, we investigate the impact of image resolution on RynnVLA-001-Video. As shown in Tab. 5, substantial performance drop is observed when the resolution decreases from our proposed 384384 to 256256. This degradation is attributed to the resolution mismatch with the VQGAN component, which is pretrained exclusively on 512 512 images. At lower resolution of 256 256, the VQGANs reconstruction quality degrades, the VQGAN fails to generate high-fidelity reconstructions, resulting in imprecise visual tokens that cannot faithfully represent the source content. Consequently, VLA model trained on these imprecise tokens exhibits reduced performance. Furthermore, our choice of 384 384 strikes balance: 1) it maintains high reconstruction fidelity by using the resolution closer to the VQGANs native resolution; 2) it offers significant reduction in computational overhead compared to the 512 512 resolution, making it more practical choice for deployment. Action Representations. In this work, we propose to use Variational Autoencoder (VAE) to compress action chunks into compact latent embeddings. This approach contrasts with prior methods (Kim et al., 2024; Black et al., 2024; Bjorck et al., 2025b) that directly predict raw action sequences. To evaluate the effectiveness of the component, we conduct an ablation study on the Calvin ABC->D benchmark, comparing the performance of predicting VAE embeddings against predicting raw actions. As shown in Tab. 5, predicting actions in the VAEs latent space outperforms the direct prediction of raw actions. The performance gains stem from two key advantages provided by the VAE: 1) it provides an efficient, compressed representation of complex action sequences, and 2) the inherent structure of its latent space improves temporal consistency, yielding smoother predicted actions. Size of Action Head. Our action prediction module utilizes simple action head, i.e., single linear layer that projects the transformers final hidden state into the action embedding space. To assess the impact of head complexity, we perform an ablation study comparing this design with deeper five-layer MLP head on the Calvin Task ABC->D benchmark. As shown in Tab. 5, increasing the depth of action head is surprisingly detrimental to performance, causing the evaluation score to decrease substantially from 4.019 to 3.323. This result indicates that the transformers output representation is already highly effective for the task. direct linear mapping is sufficient for decoding, while the additional complexity of deeper head appears to introduce noise or overfitting, ultimately impairing performance. This underscores the value of architectural simplicity in the decoding stage of our model. 3We modify the evaluation for the place in slider task because the original prompt, store the grasped block in the sliding cabinet, results in extremely low performance. To better assess action prediction capabilities, we revise it to place the grasped object in the sliding cabinet."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Figure 4 Visualization of Video Generative Pretraining. Given an input image and text prompt, an I2V model is trained to predict the next 7 frames. Our pretrained video generation model is capable of generating plausible motions while maintaining the consistency with the input image."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Figure 5 Analysis on the front cameras function for coarse localization. (a) Under normal dual-camera settings, the robot successfully picks the strawberries. (b) The front camera is masked, leaving only the wrist camera functional. (c) The robot can still complete the task if the target is within the wrist cameras initial field of view. However, task success rate drops from 80% (4/5) to 0% when the target is outside the wrist cameras view (on the left side), demonstrating that the front camera is essential for guiding the robot to the targets coarse location."
        },
        {
            "title": "5.5 Further Analysis",
            "content": "Visualization of Video Pretraining Model. The first stage of our proposed RynnVLA-001 involves pretraining an ego-centric Image-to-Video (I2V) model. This I2V paradigm is chosen to align with the typical input for VLA models: an initial image observation and text-based instruction. As illustrated in Fig. 4, the pretrained model can generate video frames with plausible motion and consistent content from given image and text prompt. Although the model is prone to generate subtle visual changes between frames, we find it sufficient for its role as pretrained backbone for the subsequent VLA training. Improving Instruction-Following Capabilities by Increasing Scene Complexity in Training Data. Our evaluation protocol for instruction-following capabilities involves placing distractor objects in the desktop to test the models robustness against visual ambiguity. We hypothesize that training exclusively on data with isolated target objects leads to simplistic and vision-driven policy, where the model learns to grasp any objects without performing actions that follow the provided language instruction. To validate this hypothesis, we perform an ablation study. variant of RynnVLA-001 is trained solely on data without distractor objects. When evaluated on the task pick up the strawberry in scene cluttered with pens and green blocks, this ablated model demonstrates 0% success rate over 10 trials. total of 5 failure cases of the 10 trials consistently select distractor object. In contrast, our full RynnVLA-001 model, trained on our comprehensive dataset including distractors, achieves 90% success rate (9/10) on this task. These results quantitatively underscore the critical importance of diverse data collection with distractors for developing reliable language-conditioned VLA models. Functional Analysis of Front and Wrist Cameras We also investigate the distinct functionalities of the front and wrist cameras on our LeRobot SO100 arm. We hypothesize that the front camera provides coarse object localization and 3D projective context, while the wrist camera is responsible for precise local adjustments. 1) To validate the front cameras role in coarse localization, we conduct an experiment where the front camera is disabled. Under normal conditions (Figure 5(a)), the robot successfully completes the task. When the front camera is masked (Figure 5(b)), we observe that the model could still succeed as long as the target is within the wrist cameras initial field of view (Figure 5(c)). However, if the target (e.g., strawberries on the left) is outside the wrist cameras view, the robot fails to initiate any action. Quantitative results confirm this: for targets on the right, the success rate drops slightly from 100% (5/5) to 80% (4/5) after masking. For the targets on the left, the success rate decreases from 80% (4/5) to 0%. These findings strongly suggest that the front cameras primary function is to guide the end-effector to roughly approach the target. 2) Furthermore, we explore the front cameras function in providing 3D information for tasks requiring depth perception, such as inserting pen into holder. As shown in Figure 6(a), the robot succeeds with the normal camera setup. However, when we elevate the front camera, altering the scenes projective geometry, the model fails to insert"
        },
        {
            "title": "REFERENCES",
            "content": "Figure 6 The front camera provides critical 3D projective information for precise manipulation. (a) With the standard camera configuration, the robot successfully inserts the pen into the holder. (b) When the front camera is elevated, the altered projective geometry of the scene causes the model to fail the task. This highlights the models reliance on the specific 3D perspective provided by the front camera for spatial reasoning. the pen (Figure 6(b)). This demonstrates that the front camera provides critical 3D projective information that the model relies on for spatial reasoning and manipulation."
        },
        {
            "title": "6 Discussion and Conclusion",
            "content": "In this work, we propose RynnVLA-001, VLA model enhanced by human demonstrations. We introduce human demonstrations in two pretraining stages: Ego-Centric Video Generative Pretraining and HumanCentric Trajectory-Aware Video Modeling. The first stage trains an I2V model by learning dynamics through predicting next frames. The second stage bridges the gaps between I2V models and VLA models by learning to predict keypoint trajectories of human. In VLA model, we propose ActionVAE to embed action chunks into compacted embeddings. Owing to our dedicated designs, our proposed RynnVLA-001 outperforms state-of-the-art models such as GR00T N1.5 and Pi0. Limitation. In this work, we validate the performance of RynnVLA-001 on the LeRobot SO100 robot arm. However, the scope of our current evaluation presents several limitations that we plan to address in future work. Our experiments are limited to single robot embodiment and an evaluation environment that closely mirrored the training setup. Furthermore, the front-facing camera is mounted in fixed position. To rigorously assess and enhance the models generalization capabilities, future efforts will focus on: (1) extending the evaluation to more diverse range of robot arms; (2) testing the model in more varied and unstructured environments; and (3) diversifying camera viewpoints."
        },
        {
            "title": "Acknowledgement",
            "content": "We thank our colleagues from the control team and the testing team for their valuable support throughout this work. Our implementation is based on code from the open-source repositories https://github.com/ Alpha-VLLM/Lumina-mGPT and https://github.com/facebookresearch/chameleon, which we adapted for training. The LaTeX template is built upon Metas original template."
        },
        {
            "title": "References",
            "content": "Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoît Sagot. Towards cleaner document-oriented multilingual crawled corpus. arXiv preprint arXiv:2201.06642, 2022."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. System card: Claude Opus 4 and Claude Sonnet 4. https://www.anthropic.com/news/claude-4. Mahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski, Pascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas. Self-supervised learning from images with joint-embedding predictive architecture. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1561915629, 2023. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. Jason Baldridge, Jakob Bauer, Mukul Bhutani, Nicole Brichtova, Andrew Bunner, Lluis Castrejon, Kelvin Chan, Yichang Chen, Sander Dieleman, Yuqing Du, et al. Imagen 3. arXiv preprint arXiv:2408.07009, 2024. Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey A. Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bosnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier J. Hénaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. PaliGemma: versatile 3B VLM for transfer. arXiv preprint arXiv:2407.07726, 2024. Johan Bjorck, Valts Blukis, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi \"Jim\" Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Xiaowei Jiang, Kaushil Kundalia, Jan Kautz, Zhiqi Li, Kevin Lin, Zongyu Lin, Loic Magne, Yunze Man, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Jing Wang, Qi Wang, Shihao Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Seonghyeon Ye, Zhiding Yu, Yizhou Zhao, Zhe Zhang, Ruijie Zheng, and Yuke Zhu. GR00T N1.5: An improved open foundation model for generalist humanoid robots. https://research.nvidia.com/labs/gear/gr00t-n1_5/, 2025a. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, Joel Jang, Zhenyu Jiang, Jan Kautz, Kaushil Kundalia, Lawrence Lao, Zhiqi Li, Zongyu Lin, Kevin Lin, Guilin Liu, Edith LLontop, Loic Magne, Ajay Mandlekar, Avnish Narayan, Soroush Nasiriany, Scott Reed, You Liang Tan, Guanzhi Wang, Zu Wang, Jing Wang, Qi Wang, Jiannan Xiang, Yuqi Xie, Yinzhen Xu, Zhenjia Xu, Seonghyeon Ye, Zhiding Yu, Ao Zhang, Hao Zhang, Yizhou Zhao, Ruijie Zheng, and Yuke Zhu. GR00T N1: an open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025b. Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey Levine. Zero-shot robotic manipulation with pretrained image-editing diffusion models. arXiv preprint arXiv:2310.10639, 2023. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. AgiBot World Colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. Remi Cadene, Simon Alibert, Alexander Soare, Quentin Gallouedec, Adil Zouitine, Steven Palma, Pepijn Kooijmans, Michel Aractingi, Mustafa Shukor, Dana Aubakirova, Martino Russi, Francesco Capuano, Caroline Pascal, Jade Choghari, Jess Moss, and Thomas Wolf. LeRobot: State-of-the-art machine learning for real-world robotics in Pytorch. https://github.com/huggingface/lerobot, 2024. Jun Cen, Chaohui Yu, Hangjie Yuan, Yuming Jiang, Siteng Huang, Jiayan Guo, Xin Li, Yibing Song, Hao Luo, Fan Wang, Deli Zhao, and Hao Chen. WorldVLA: Towards autoregressive action world model. arXiv preprint arXiv:2506.21539, 2025. Chilam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, and Minzhao Zhu. GR-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, Hao Niu, Wenxuan Ou, Wanli Peng, Zeyu Ren, Haixin Shi, Jiawen Tian, Hongtao Wu, Xin Xiao, Yuyang Xiao, Jiafeng Xu, and Yichu Yang. GR-3 technical report. arXiv preprint arXiv:2507.15493, 2025. Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70M: Captioning 70M videos with multiple cross-modality teachers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Can Cui, Pengxiang Ding, Wenxuan Song, Shuanghao Bai, Xinyang Tong, Zirui Ge, Runze Suo, Wanqi Zhou, Yang Liu, Bofang Jia, Han Zhao, Siteng Huang, and Donglin Wang. OpenHelix: short survey, empirical analysis, and open-source dual-system VLA model for robotic manipulation. arXiv preprint arXiv:2505.03912, 2025. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The EPICKITCHENS dataset. In European Conference on Computer Vision, 2018. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. The EPIC-KITCHENS dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(11):41254141, 2021. Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, pipeline and challenges for EPIC-KITCHENS-100. International Journal of Computer Vision, 130:3355, 2022. Yilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans, and Pieter Abbeel. Learning universal policies via text-guided video generation. In Advances in Neural Information Processing Systems, 2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The \"Something Something\" video database for learning and evaluating visual common sense. In IEEE/CVF International Conference on Computer Vision, pages 58425850, 2017. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4D: Around the world in 3,000 hours of egocentric video. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1899519012, 2022. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025a. Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1.5-VL technical report. arXiv preprint arXiv:2505.07062, 2025b. Yanjiang Guo, Yucheng Hu, Jianke Zhang, Yen-Jen Wang, Xiaoyu Chen, Chaochao Lu, and Jianyu Chen. Prediction with action: Visual policy learning via joint denoising process. In Advances in Neural Information Processing Systems, 2024. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1600016009, 2022. Ryan Hoque, Peide Huang, David J. Yoon, Mouli Sivapurapu, and Jian Zhang. EgoDex: Learning dexterous manipulation from large-scale egocentric video. arXiv preprint arXiv:2505.11709, 2025."
        },
        {
            "title": "REFERENCES",
            "content": "Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, and Jianyu Chen. Video prediction policy: generalist robot policy with predictive visual representations. arXiv preprint arXiv:2412.14803, 2024. Joel Jang, Seonghyeon Ye, Zongyu Lin, Jiannan Xiang, Johan Bjorck, Yu Fang, Fengyuan Hu, Spencer Huang, Kaushil Kundalia, Yen-Chen Lin, Loic Magne, Ajay Mandlekar, Avnish Narayan, You Liang Tan, Guanzhi Wang, Jing Wang, Qi Wang, Yinzhen Xu, Xi Zeng, Kaiyuan Zheng, Ruijie Zheng, Ming-Yu Liu, Luke S. Zettlemoyer, Dieter Fox, Jan Kautz, Scott Reed, Yuke Zhu, and Linxi Fan. Dreamgen: Unlocking generalization in robot learning through neural trajectories. arXiv preprint arXiv:2505.12705, 2025. Tsung-Wei Ke, Nikolaos Gkanatsios, and Katerina Fragkiadaki. 3D Diffuser Actor: policy diffusion with 3D scene representations. In Conference on Robot Learning, pages 19491974, 2024. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, et al. DROID: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2403.12945, 2024. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Paul Foster, Pannag R. Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. OpenVLA: an open-source vision-language-action model. In Conference on Robot Learning, pages 26792713, 2024. Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. DataComp-LM: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024a. Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, and Baining Guo. CogACT: foundational vision-language-action model for synergizing cognition and action in robotic manipulation. arXiv preprint arXiv:2411.19650, 2024b. Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, and Yang Gao. OneTwoVLA: unified vision-language-action model with adaptive reasoning. arXiv preprint arXiv:2505.11917, 2025. Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In International Conference on Learning Representations, 2023. Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mGPT: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024a. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In Advances in Neural Information Processing Systems, pages 3489234916, 2023. Songming Liu, Lingxuan Wu, Bangguo Li, Hengkai Tan, Huayu Chen, Zhengyi Wang, Ke Xu, Hang Su, and Jun Zhu. RDT-1B: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024b. Farzaneh Mahdisoltani, Guillaume Berger, Waseem Gharbieh, David Fleet, and Roland Memisevic. On the effectiveness of task granularity for transfer learning. arXiv preprint arXiv:1804.09235, 2018. Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. CALVIN: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):73277334, 2022. Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In IEEE/CVF International Conference on Computer Vision, 2019. Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alexander Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew E. Wang, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan,"
        },
        {
            "title": "REFERENCES",
            "content": "Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Paul Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Haoshu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I. Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi Jim Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J. Joshi, Niko Sünderhauf, Ning Liu, Norman Di Palo, Nur Muhammad (Mahi) Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R. Sanketi, Patrick Tree Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Martín-Martín, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Liangwei Xu, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, and Zipeng Lin. Open X-Embodiment: robotic learning datasets and RT-X models : Open X-Embodiment collaboration. In IEEE International Conference on Robotics and Automation, pages 68926903, 2024. OpenAI. GPT-4o system card, 2024. https://openai.com/index/hello-gpt-4o/. OpenAI. Introducing GPT-4.1 in the API, 2025. https://openai.com/index/gpt-4-1/. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, pages 41724182, 2023. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. FAST: efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. SAM 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, pages 2527825294, 2022. Yide Shentu, Philipp Wu, Aravind Rajeswaran, and Pieter Abbeel. From LLMs to actions: Latent codes as bridges in hierarchical robot control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 85398546, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andrés Marafioti, Simon Alibert, Matthieu Cord, Thomas Wolf, and Rémi Cadène. SmolVLA: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In Advances in Neural Information Processing Systems, pages 8483984865, 2024. Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. In International Conference on Learning Representations, 2025. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. SigLIP 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe HansenEstruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. BridgeData V2: dataset for robot learning at scale. In Conference on Robot Learning, 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Xiaofeng Wang, Kang Zhao, Feng Liu, Jiayu Wang, Guosheng Zhao, Xiaoyi Bao, Zheng Zhu, Yingya Zhang, and Xingang Wang. EgoVid-5M: large-scale video-action dataset for egocentric video generation. arXiv preprint arXiv:2411.08380, 2024b. Yating Wang, Haoyi Zhu, Mingyu Liu, Jiange Yang, Hao-Shu Fang, and Tong He. VQ-VLA: Improving vision-languageaction models via scaling vector-quantized action tokenizers. arXiv preprint arXiv:2507.01016, 2025. Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. DexVLA: vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025. Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In International Conference on Learning Representations, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 42104220, 2023. Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, and Donglin Wang. GEVRM: goal-expressive video generation model for robust visual manipulation. In International Conference on Learning Representations, 2025. Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, and Jianyu Chen. HiRT: enhancing robotic control with hierarchical robot transformers. In Conference on Robot Learning, pages 933946, 2024. Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Tsung-Yi Lin, Gordon Wetzstein, Ming-Yu Liu, and Donglai Xiang. CoT-VLA: Visual chain-of-thought reasoning for vision-language-action models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17021713, 2025. Tony Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-Sora: democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024."
        },
        {
            "title": "REFERENCES",
            "content": "Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. InternVL3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong T. Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. RT-2: vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183, 2023."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab"
    ]
}