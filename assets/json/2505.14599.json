{
    "paper_title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models",
    "authors": [
        "Guangzhi Xiong",
        "Eric Xie",
        "Corey Williams",
        "Myles Kim",
        "Amir Hassan Shariatmadari",
        "Sikun Guo",
        "Stefan Bekiranov",
        "Aidong Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 9 9 5 4 1 . 5 0 5 2 : r Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models Guangzhi Xiong1 , Eric Xie1 , Corey Williams1 , Myles Kim1 , Amir Hassan Shariatmadari1 , Sikun Guo1 , Stefan Bekiranov1 and Aidong Zhang1 1University of Virginia {hhu4zu, jrg4wx, cmw6pa, mbt8hz, ahs5ce, qkm6sq, sb3de, aidong}@virginia.edu"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo."
        },
        {
            "title": "1 Introduction\nLarge language models (LLMs) have transformed the land-\nscape of artificial intelligence, demonstrating remarkable ca-\npabilities across diverse applications, from natural language\nunderstanding to creative content generation [Karanikolas et\nal., 2023; Franceschelli and Musolesi, 2024; Raiaan et al.,\n2024]. These models, trained on extensive corpora of text,\ndemonstrate an ability to analyze, summarize, and generate\nhuman-like text, enabling advancements across diverse do-\nmains. Recently, there has been a growing interest in leverag-\ning LLMs for scientific discovery [Zhong et al., 2023; Yang\net al., 2023; Kumar et al., 2023; Liu et al., 2024; Baek et\nal., 2024; Guo et al., 2024]. Their capacity to process and",
            "content": "synthesize vast amounts of scientific literature positions them as valuable tools in aiding researchers, particularly for tasks such as literature reviews, summarization, and even generating new hypotheses [Qi et al., 2023; Zhou et al., 2024; M. Bran et al., 2024; Wright et al., 2022; Zeng et al., 2023; DArcy et al., 2024; Ifargan et al., 2025; Yang et al., 2025]. One particularly promising application of LLMs is their use in biomedical hypothesis generation, where they can assist in identifying promising research directions [Park et al., 2024; Si et al., 2024; Guo et al., 2025]. By analyzing extensive biomedical literature, LLMs can uncover gaps in existing knowledge and propose novel hypotheses that may not be immediately apparent to human researchers. For instance, LLMs have been successfully applied to propose novel drug combinations for breast cancer treatment, some of which were later validated in laboratory experiments, showcasing their potential to accelerate biomedical discoveries [AbdelRehim et al., 2024]. Despite these advancements, there are substantial challenges that limit the practical utility of LLMs in biomedical hypothesis generation. critical concern is the inability to evaluate the truthfulness of generated hypotheses. While LLMs can generate hypotheses that seem plausible, it remains uncertain whether these hypotheses are valid and grounded in existing knowledge or merely hallucinated and scientifically invalid. This issue is further exacerbated by the well-documented hallucination problem, where LLMs confidently produce information that is factually inaccurate or unsupported, posing challenges to their reliability in biomedical contexts [Jin et al., 2024]. While current research has largely focused on improving the novelty and diversity of LLM-generated hypotheses, their truthfulness and grounding in established knowledge remain underexplored [Baek et al., 2024; Hu et al., 2024; Si et al., 2024]. To address these challenges, we introduce TruthHypo, comprehensive benchmark for evaluating the ability of LLMs to generate truthful biomedical hypotheses, and KnowHD, knowledge-based hallucination detection framework designed to assess the groundedness of these hypotheses. TruthHypo, built on biomedical knowledge graph along with domain-specific corpus, provides controlled environment to evaluate how well LLM-generated hypotheses align with established biomedical knowledge. KnowHD focuses on analyzing the reasoning processes of LLMs to identify hypotheFigure 1: Overview of the TruthHypo benchmark, including dataset construction, task formulation, and truthfulness evaluation. ses that are likely hallucinated or untruthful. Our findings reveal that LLMs face significant challenges in generating truthful hypotheses. By analyzing hallucinations in the reasoning processes behind generated hypotheses, we demonstrate that groundedness scores from KnowHD serve as an effective signal for identifying truthful hypotheses from the diverse outputs of LLMs. Human evaluations on open-ended hypothesis generation tasks further confirm the utility of KnowHD in identifying scientifically valid hypotheses. Our main contributions are summarized as follows: We introduce TruthHypo, comprehensive benchmark designed to evaluate the ability of LLMs to generate truthful biomedical hypotheses. We propose KnowHD, knowledge-based hallucination detection framework that assesses the groundedness of LLM-generated hypotheses and identifies hallucinated claims by analyzing the rationale behind the hypothesis generation. We provide an extensive analysis of existing LLMs on TruthHypo, highlighting their limitations and challenges in generating truthful hypotheses. Our evaluation further reveals the connection between hallucination and truthfulness of generated hypotheses, showing the effectiveness of using KnowHD to select truthful and grounded hypotheses."
        },
        {
            "title": "Benchmark",
            "content": "To systematically evaluate the ability of large language models (LLMs) to generate truthful scientific hypotheses, we introduce TruthHypo, benchmark tailored for biomedical hypothesis generation. TruthHypo is designed to simulate realworld conditions by employing rigorous dataset construction, task formulation, and truthfulness evaluation metrics. An overview of the dataset construction, task formulation, and evaluation framework is depicted in Figure 1."
        },
        {
            "title": "2.1 Dataset Construction",
            "content": "The dataset for TruthHypo is derived from PubTator 3.0 [Wei et al., 2024], comprehensive biomedical knowledge graph that includes annotated relations (also called edges) extracted from scientific articles. To simulate the temporal progression of scientific discovery, we partitioned the graph into seen and unseen subsets based on the publication years of the corresponding articles. Relations in the seen subset were extracted from papers published before 2023, identified by PMIDs 366000001. The unseen subset, designed to represent new discoveries, comprises relations extracted from papers published after 2024, identified by PMIDs 38200000. To ensure no overlap between the two subsets, we removed the edges in the unseen subset that shared head and tail entities with those in the seen subset. In addition, to maintain quality and validity, only relations discovered by multiple articles in the test data were retained. This filtering process guarantees that the unseen subset exclusively contains knowledge unavailable before 2024, simulating the conditions of future scientific research. In building the dataset, we focused on three key relation types: Chemical & Gene, Disease & Gene, and Gene & Gene. These relation types were chosen for their complementary nature, detailed annotations, and potential for objective evaluation. To construct comprehensive classification tasks for evaluating different LLMs, we augment the dataset with negative test cases to assess whether LLMs tend to make false-positive predictions on entity pairs that lack direct relationship in the existing knowledge base. The number of negative samples (labeled as no relation) for each relation type is controlled to align with the average number of instances across other labels of the same relation type. The final dataset has 1209 instances for the Chemical & Gene task, 268 instances for the Disease & Gene task, and 547 1PMID is the unique identifier of the paper where the edge was extracted. instances for the Gene & Gene task. summary of the dataset statistics is presented in Table 1. Task Label # Instance Chemical & Gene Disease & Gene Gene & Gene positive correlate negative correlate no relation stimulate inhibit no relation positive correlate negative correlate no relation 328 478 403 104 75 89 247 118 Table 1: Statistics of various tasks in the TruthHypo benchmark."
        },
        {
            "title": "2.2 Task Formulation\nThe TruthHypo benchmark includes three tasks, correspond-\ning to the selected relation types: “Chemical & Gene”, “Dis-\nease & Gene”, and “Gene & Gene”. For each task, the input\nis a hypothesis generation query with two entities (see Figure\n5 in Appendix D for the template), and the LLM is required\nto hypothesize the potential relationship between them based\non available knowledge and reasoning.",
            "content": "To comprehensively assess LLM performance, we evaluate their ability to generate hypotheses under various knowledge augmentation settings. In the first setting, LLMs rely solely on their parametric knowledge information encoded in their parameters during pretraining on large corpora. This evaluates the models intrinsic understanding and reasoning capabilities. To enhance hypothesis generation, we introduce second setting in which LLMs are augmented with structured knowledge from the seen knowledge graph. In this approach, key entities from the input are mapped to nodes in the graph, and multi-hop link chains connecting these nodes are explored. These chains, representing relevant relationships, are transformed into textual descriptions and provided as context for the model to use during hypothesis generation. Another setting leverages information from biomedical literature using retrieval-augmented generation (RAG) pipeline. Relevant documents are retrieved from the PubMed corpus2 using BM25 [Robertson et al., 2009]. To maintain consistency with the knowledge graphs temporal split, only articles with PMIDs 36600000 are included in the retrieval. This simulates the process of generating hypotheses based on literature available at given point in time. Finally, we consider combined setting, where both structured knowledge from the graph and unstructured information from retrieved literature are used to support hypothesis generation. This comprehensive approach provides more holistic context, enabling the model to reason across both sources. The LLM prompt templates we used to combine the external information with the original user instructions can be found in Figures 6, 8, 7, and 9 (Appendix D). 2https://pubmed.ncbi.nlm.nih.gov/"
        },
        {
            "title": "2.3 Evaluation Metrics\nTo evaluate the quality of generated scientific hypotheses, we\nemploy a set of complementary metrics tailored to different\naspects of hypothesis generation. These metrics assess the\nperformance of LLMs in identifying valid connections be-\ntween entities (link-level evaluation) and predicting specific\nrelations (relation-level evaluation).",
            "content": "For link-level evaluation, we focus on precision, recall, and F1 score. Precision measures the proportion of correctly identified connections among all hypothesized connections, emphasizing the reduction of false positives. Recall evaluates the models ability to comprehensively identify all valid connections, capturing its sensitivity to true positives. The F1 score, as the harmonic mean of precision and recall, provides balanced measure of performance, combining both the accuracy of predictions and the coverage of valid connections. These link-level metrics are critical for assessing the LLMs ability to hypothesize plausible relationships between entities, regardless of the specific relation type. For relation-level evaluation, we employ accuracy to measure how often the generated hypotheses match the correct relation labels in the ground truth. Accuracy captures the overall correctness of hypotheses by considering both the existence of connection and the predicted relation type. While precision, recall, and F1 focus on identifying potential connections, accuracy provides finer-grained assessment of the models capability to generate accurate relation labels. By combining link-level and relation-level evaluations, the TruthHypo benchmark comprehensively measures the truthfulness of LLM-generated hypotheses, assessing the ability of LLMs to produce scientifically valid outputs."
        },
        {
            "title": "3 Knowledge-based Hallucination Detection\nAs discussed earlier, a critical concern regarding the truthful-\nness of LLM-generated hypotheses is the occurrence of hallu-\ncinations, where models generate plausible-sounding but un-\nsupported claims. To address this, we introduce KnowHD,\na knowledge-based hallucination detection framework that\nevaluates the groundedness of LLM-generated hypotheses by\nanalyzing the rationale behind their generation. KnowHD op-\nerates using scientific literature, knowledge graphs, or a com-\nbination of both as the knowledge base. An overview of the\nframework is presented in Figure 2.",
            "content": "To evaluate groundedness, each hypothesis and its reasoning chain are first decomposed into set of atomic claims. This step is critical because hypotheses often consist of compound reasoning steps, some of which may be supported by existing knowledge while others may not. Parsing these into atomic claims allows more granular evaluation of groundedness and isolates unsupported components. This step is implemented by prompting LLMs with the template shown in Figure 10 (Appendix D). When using scientific literature as the knowledge base, relevant documents for each atomic claim are retrieved from the PubMed corpus, limited to articles published before 2023 (PMID 36600000). BM25 is employed to rank documents based on their relevance to the claim. To ensure computational efficiency and focus on the most relevant information, claims enables detailed assessment of the groundedness of hypotheses, identifying unsupported components and improving the reliability of LLM-generated outputs."
        },
        {
            "title": "4 Benchmark Analysis on TruthHypo\n4.1 Experiment Settings\nTo assess the ability of existing LLMs to generate truthful\nscientific hypotheses, we selected a diverse range of mod-\nels varying in type and size. The Llama-3 family [Dubey\net al., 2024] represents open-source LLMs, while the GPT-4\nfamily [Achiam et al., 2023] exemplifies proprietary models.\nFrom each family, we evaluated two LLMs of different sizes\n(Llama-3.1-8B & Llama-3.1-70B, GPT-4o-mini & GPT-4o)\nto investigate size-related differences in performance. All\nLLMs were trained on the knowledge available before 2024,\npreventing recall of the exact knowledge for hypothesis gen-\neration. More implementation details are in Appendix A.",
            "content": "The TruthHypo benchmark evaluates LLMs across four distinct settings: (1) parametric knowledge only, (2) parametric knowledge with knowledge graphs (KG), (3) parametric knowledge with literature (Lit.), and (4) parametric knowledge with both KG and literature. These settings allow us to explore the impact of external knowledge sources on hypothesis generation. The F1 and accuracy scores of different models are reported in this section. More detailed results on the precision and recall can be found in Appendix C."
        },
        {
            "title": "Generation",
            "content": "Table 2 presents the evaluation results for different LLMs and knowledge settings on TruthHypo. Across all tasks, the results indicate that most LLMs struggle to generate truthful scientific hypotheses, with only GPT-4o achieving mean accuracies exceeding 60%. Additionally, we can observe that link-level F1 scores are higher than relation-level accuracy scores, which indicates that LLMs can identify potential connections between entities but often fail to accurately predict the specific relationships. For models from the same family with different sizes, larger LLMs tend to generate scientific hypotheses more likely to be truthful. This can be attributed to two main factors. First, larger LLMs generally perform better because they can store and leverage more knowledge in their parameters, as shown by the results of parametric knowledge-only setting. Second, LLMs of different sizes have diverse capabilities to process external knowledge for hypothesis generation. For example, GPT-4o-mini shows modest 1.14% accuracy improvement when augmented with KG and literature, whereas GPT-4o achieves more substantial 5.14% increase under the same conditions. This suggests that larger LLMs can better utilize additional context to reason about truthful scientific hypotheses. Similar trends are observed when comparing Llama-3.1-8B and Llama-3.1-70B. Interestingly, smaller models, such as Llama-3.1-8B, sometimes experience decreased performance when information from KG and literature is introduced. This degradation may stem from challenges in effectively integrating internal and external information, which can disrupt the models reasoning processes. Figure 2: Overview of the KnowHD hallucination detection framework. Hypotheses are parsed into atomic claims, which are then evaluated for groundedness using knowledge graph, scientific literature or both as knowledge sources. only the top-k documents are retained. The context retrieved from the literature corpus for claim is defined as: contextD(p) = {d1, d2, , dk di D, BM25(p; di) τ, rank(di) k}, (1) where di represents document in the corpus, BM25(p; di) is the relevance score assigned to the document for the claim p. τ is threshold ensuring relevance, and rank(di) denotes the rank of di in the BM25-retrieved list. When using knowledge graph as the knowledge base, the context for claim is derived from the graph structure. For claim p, relevant knowledge is extracted as: contextG(p) = {(eh, r, et) {eh, et} V(p) } , (2) where (eh, r, et) represents an edge in the knowledge graph with head entity eh, tail entity et, and relation r. The set V(p) contains all entities mentioned in the claim p. The groundedness of claim is determined based on whether the given context information (contextD, contextG, or contextD contextG) can fully support the claim, which is implemented by prompting LLMs to provide judgment using the template in Figure 12 (Appendix D). If the concatenated context collectively entails the claim, it is considered grounded. The overall groundedness of hypothesis is computed as: groudedness(h) = 1 C(h) (cid:88) pC(h) 1[context(p) = p], (3) where C(h) represents the set of atomic claims for hypothesis h, and 1[x = y] returns 1 if entails and 0 otherwise. The context(p) can be contextD(p), contextG(p), or contextD(p) contextG(p). By offering both literature-based and graph-based contexts, KnowHD provides robust framework for hallucination detection, offering flexibility to adapt to the available knowledge sources. This systematic evaluation of atomic Chemical & Gene Disease & Gene Gene & Gene Average Knowledge LLM F1 Parametric [Wei et al., 2022] Parametric + KG [Baek et al., 2024] Parametric + Lit. [Lewis et al., 2020] Parametric + KG + Literature Llama-3.1-8B 80.16 Llama-3.1-70B 81.36 83.31 GPT-4o-mini 80.74 GPT-4o 81.37 Llama-3.1-8B Llama-3.1-70B 87.85 86.42 GPT-4o-mini 88.66 GPT-4o 80.78 Llama-3.1-8B Llama-3.1-70B 82.56 85.28 GPT-4o-mini 79.52 GPT-4o Llama-3.1-8B 75.98 Llama-3.1-70B 84.80 88.34 GPT-4o-mini 89.71 GPT-4o Acc 42.43 52.44 61.29 66. 40.61 62.86 57.65 63.85 46.07 56.74 59.80 65.92 36.48 59.31 60.96 69.31 F1 79.37 83.29 81.84 75.38 79.59 67.62 74.17 79. 80.46 84.16 85.71 75.84 77.58 77.64 84.47 82.86 Acc 41.04 54.48 59.33 54.85 48.13 52.24 55.60 56.72 43.28 52.99 53.73 55. 41.42 56.34 58.21 62.31 F1 79.19 76.66 79.32 71.56 79.61 78.29 81.65 82.73 79.91 79.18 81.50 64.69 79.19 81.24 84.17 85. Acc 46.07 49.91 53.02 55.58 48.45 58.14 62.34 61.06 42.60 51.55 51.19 51.92 45.70 55.76 58.50 63.99 66.90 71.54 75.49 73.17 70.65 79.10 79.40 81.62 68.58 73.37 77.08 71.84 65.37 77.37 81.42 83.55 Acc 43.23 52.03 58.79 61. 43.73 60.18 58.65 62.15 44.76 54.84 56.67 60.82 39.62 57.95 59.93 66.95 Table 2: Performance comparison of different LLMs on the TruthHypo benchmark across various knowledge settings. The metrics reported are link-level F1 and relation-level accuracy (Acc) for each task (Chemical & Gene, Disease & Gene, Gene & Gene), as well as their averages. Param. denotes parametric knowledge, while KG and Lit. refer to knowledge graphs and literature, respectively. All scores are percentages (%). Performance differences are also observed across the three relation types: Chemical & Gene, Disease & Gene and Gene & Gene. Notably, all larger models, including GPT4o, GPT-4o-mini, and Llama-3.1-70B, tend to perform better on Chemical & Gene tasks than on the other two types. This trend suggests that the Chemical & Gene task may be more aligned with the pre-trained knowledge or reasoning capabilities of these models. In contrast, the smaller Llama-3.1-8B shows more inconsistent pattern, with performance varying across tasks and settings, likely reflecting its more limited parametric capacity and reasoning abilities. These variations in performance across relation types may be attributed to differences in training data distributions or the complexity of the relation types themselves. The relatively stronger performance on the Chemical & Gene task highlights potential domain-specific biases or strengths in the LLMs, offering insights into their suitability for targeted applications in real-world scientific discovery."
        },
        {
            "title": "Hypotheses",
            "content": "To assess the groundedness of the generated hypotheses, we evaluated their rationales using KnowHD under various knowledge settings. KnowHD measures how well hypothesis is supported by structured knowledge (KG), unstructured knowledge (literature), or both combined. The groundedness evaluation results for hypotheses generated by GPT-4o-mini are presented in Table 3. The results demonstrate distinct contributions of KG and literature to grounding hypotheses. For example, KnowHD with the literature as the support knowledge base can verify 76.30% claims in the rationales of literature-augmented Chemical & Gene hypotheses. However, the hallucination Task Knowledge KnowHD KG Lit. KG + Lit. Chemical & Gene Disease & Gene Gene & Gene Parametric + KG + Lit. + KG + Lit. Parametric + KG + Lit. + KG + Lit. Parametric + KG + Lit. + KG + Lit. 44.77 49.93 47.19 50.57 45.44 57.07 49.34 51.11 42.94 58.07 44.49 54.03 67.34 51.08 76.30 65. 71.56 60.70 78.65 75.26 67.81 56.41 76.43 67.96 74.49 73.03 83.20 78.90 78.91 79.81 85.32 86.68 76.16 79.64 84.48 82.87 Table 3: KnownHD (KG, Lit., and KG + Lit.) groudedness scores of hypotheses generated by GPT-4o-mini under different knowledge settings. All scores are percentages (%). detector can hardly verify the rationale generated based on adding KG information to parametric knowledge with only 51.08% of the claims being grounded. Combining KG and literature yields the highest groundedness scores, effectively leveraging the complementary strengths of both sources to identify grounded claims and detect hallucinations. To further explore the relationship between hallucination and truthfulness, Figure 3 examines mean accuracy as function of groundedness scores. Hypotheses were grouped based on their groundedness scores, and the average accuracy for each group was calculated. The figure reveals positive correlation between groundedness scores and hypothesis truthfulness. As groundedness scores increase, the likelihood Figure 3: Mean accuracy corresponding to different levels of groundedness. Hypotheses are grouped based on their groundedness scores provided by KnowHD (KG + Literature). Only groups with no less than 10 hypotheses are shown in the plots. The dot size reflects the number of samples in each level of groundedness. of the hypothesis being truthful also increases. For example, GPT-4o-mini achieves mean accuracy of 60.96% on Chemical & Gene tasks under the combined KG + Literature setting, but this rises to 72.77% for hypotheses with groundedness scores above 80%. These findings underscore the potential of KnowHD to identify hypotheses with higher probability of being truthful, particularly in contexts enriched with external knowledge. 4."
        },
        {
            "title": "Improving Generation of Truthful Hypotheses\nwith KnowHD",
            "content": "To validate the utility of KnowHD on enhancing hypothesis generation, we prompted LLMs to generate five candidate hypotheses for each input and selected the one with the highest groundedness score as the final output. This approach was compared to two baselines: the greedy search method, where the hypothesis is generated using greedy next-token selection by the LLM, and the self-consistency method [Wang et al., 2022], which selects hypotheses based on majority voting across multiple predictions. As shown in Figure 4, groundedness-based hypothesis selection generally outperforms both the greedy search and majority-voting methods across most knowledge settings. In the parametric knowledge-only setting, the majority-voting method achieves slightly higher accuracy (61.86%) compared to groundedness-based selection (59.83%). However, as external knowledge is introduced, groundedness-based selection demonstrates consistent improvements over both baselines. For example, in the combined parametric + KG + Literature setting, GPT-4o-mini achieves an average accuracy of 63.44% when groundedness-based selection is used, approaching the performance of the larger GPT-4o model. These results highlight the effectiveness of groundedness scores in scenarios where external knowledge is incorporated, as they help identify hypotheses that are more likely to be truthful. By detecting hallucinations in reasoning steps and focusing on grounded hypotheses, KnowHD provides robust mechanism for enhancing the reliability and truthfulness of LLM-generated scientific hypotheses. Figure 4: Accuracy improvements of GPT-4o-mini using KnowHD (KG + Lit.) groundedness scores for hypothesis selection. Param., KG and Lit. denote parametric knowledge, knowledge graphs, and literature, respectively."
        },
        {
            "title": "5 Human Study on Open-ended Tasks\nTo further assess the generalizability of KnowHD’s effective-\nness in selecting truthful hypotheses, we conducted exper-\niments on open-ended hypothesis generation tasks. These\ntasks were designed to evaluate whether KnowHD could reli-\nably identify hypotheses with a higher likelihood of truthful-\nness across broader and less structured generation scenarios.\nFor this analysis, we utilized the publicly available hypothesis\ngeneration dataset introduced by Qi et al. [2024], which in-\nvolves generating free-form hypotheses based on given back-\nground information. We selected GPT-4o-mini as the tested\nLLM and enhanced its hypothesis generation process by in-\ncorporating external knowledge from scientific literature and\nknowledge graphs (KG). The model was prompted to gener-\nate five distinct scientific hypotheses for each input. These\nhypotheses were then evaluated by KnowHD, which assessed\ntheir groundedness based on their alignment with both struc-\ntured (KG) and unstructured (literature) knowledge sources.\nTo analyze the relationship between groundedness scores\nand hypothesis truthfulness, we filtered generated hypotheses\nto create pairs with contrasting groundedness levels. For each\ninput, we identified one hypothesis with the highest ground-",
            "content": "Group Groudedness GPT-4o Human highly-grounded lowly-grounded p-value 83.94 40.61 7.84 1011 61.11 38.89 1.05 102 59.26 40.74 2.71 102 Table 4: Results of analysis on open-ended hypothesis generation tasks. GPT and Human denote the selection ratios by GPT-4o and human experts, respectively. All scores are percentages (%). pvalues were calculated using Wilcoxon signed-rank test and Z-test. edness score and another with the lowest. We retained pairs where the higher groundedness score was at 30% greater than the lower score. This filtering resulted in 54 pairs of hypotheses with significant differences in groundedness levels. To validate KnowHDs effectiveness, we involved two domain experts to annotate each pair (80% agreement), selecting the hypothesis they deemed more likely truthful based on the given information. Additionally, GPT-4o was prompted to analyze the same pairs and provide its judgment. Results of this annotation study, summarized in Table 4, report the selection ratio for each group, defined as the proportion of hypotheses in each group identified as more truthful. The results demonstrate significant relationship between groundedness scores and the perceived truthfulness of hypotheses. Hypotheses with higher groundedness scores were consistently more likely to be selected as truthful by both human experts and GPT-4o, as indicated by the substantial differences in selection ratios. These findings highlight the utility of KnowHD in distinguishing truthful hypotheses, even in unstructured, open-ended generation tasks. By effectively leveraging groundedness as criterion, KnowHD provides robust mechanism for improving the reliability of LLMgenerated hypotheses, reinforcing its potential for facilitating real-world scientific discovery processes."
        },
        {
            "title": "7 Conclusion\nWe presented TruthHypo, a benchmark for evaluating the\nability of LLMs to generate truthful biomedical hypothe-\nses, and KnowHD, a framework for detecting hallucinations\nby assessing groundedness in reasoning. Through extensive\nevaluation, we highlighted the limitations of existing LLMs\nand demonstrated that selecting highly grounded hypotheses\nimproves truthfulness. These contributions offer valuable in-\nsights for improving the reliability and utility of LLMs in sci-\nentific discovery.",
            "content": "Acknowledgements This work is supported in part by the US National Science Foundation under grants 2217071, 2213700, 2106913, 2008208, and NIH grant 1R01LM014012. References Abbi Abdel-Rehim, Hector Zenil, Oghenejokpeme Orhobor, Marie Fisher, Ross Collins, Elizabeth Bourne, Gareth Fearnley, Emma Tate, Holly Smith, Larisa Soldatova, et al. Scientific hypothesis generation by large language model: Laboratory validation in breast cancer treatment. arXiv preprint arXiv:2405.12258, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738, 2024. Ioana Ciuca, Yuan-Sen Ting, Sandor Kruk, and Kartheik Iyer. Harnessing the power of adversarial prompting and large language models for robust hypothesis generation in astronomy. arXiv preprint arXiv:2306.11648, 2023. Mike DArcy, Tom Hope, Larry Birnbaum, and Doug Downey. Marg: Multi-agent review generation for scientific papers. arXiv preprint arXiv:2401.04259, 2024. Finale Doshi-Velez and Been Kim. Towards rigorous sciarXiv preprint ence of interpretable machine learning. arXiv:1702.08608, 2017. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Giorgio Franceschelli and Mirco Musolesi. On the creativity of large language models. AI & SOCIETY, pages 111, 2024. Xuemei Gu and Mario Krenn. Forecasting high-impact research topics via machine learning on evolving knowledge graphs. arXiv preprint arXiv:2402.08640, 2024. Sikun Guo, Amir Hassan Shariatmadari, Guangzhi Xiong, and Aidong Zhang. Embracing foundation models for In 2024 IEEE Internaadvancing scientific discovery. tional Conference on Big Data (BigData), pages 1746 1755. IEEE, 2024. Sikun Guo, Amir Hassan Shariatmadari, Guangzhi Xiong, Albert Huang, Myles Kim, Corey M. Williams, Stefan Bekiranov, and Aidong Zhang. Ideabench: Benchmarking large language models for research idea generation. In 31st SIGKDD Conference on Knowledge Discovery and Data Mining - Datasets and Benchmarks Track, 2025. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, and Zhenzhong Lan. Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. arXiv preprint arXiv:2410.14255, 2024. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232, 2023. Tal Ifargan, Lukas Hafner, Maor Kern, Ori Alcalay, and Roy Kishony. Autonomous llm-driven researchfrom NEJM AI, data to human-verifiable research papers. 2(1):AIoa2400555, 2025. Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Yu Philip. survey on knowledge graphs: Representation, acquisition, and applications. IEEE transactions on neural networks and learning systems, 33(2):494514, 2021. Qiao Jin, Nicholas Wan, Robert Leaman, Shubo Tian, Zhizheng Wang, Yifan Yang, Zifeng Wang, Guangzhi Xiong, Po-Ting Lai, Qingqing Zhu, et al. Demystifying arXiv large language models for medicine: primer. preprint arXiv:2410.18856, 2024. Nikitas Karanikolas, Eirini Manga, Nikoletta Samaridi, Eleni Tousidou, and Michael Vassilakopoulos. Large language models versus natural language understanding and generation. In Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics, pages 278 290, 2023. Mario Krenn, Lorenzo Buffoni, Bruno Coutinho, Sagi Eppel, Jacob Gates Foster, Andrew Gritsevskiy, Harlin Lee, Yichao Lu, Joao Moutinho, Nima Sanjabi, et al. Forecasting the future of artificial intelligence with machine learning-based link prediction in an exponentially growing knowledge network. Nature Machine Intelligence, 5(11):13261335, 2023. Varun Kumar, Leonard Gleyzer, Adar Kahana, Khemraj Shukla, and George Em Karniadakis. Mycrunchgpt: chatgpt assisted framework for scientific machine learning. arXiv preprint arXiv:2306.15551, 2023. Jakub Lala, Odhran ODonoghue, Aleksandar Shtedritski, Sam Cox, Samuel Rodriques, and Andrew White. Paperqa: Retrieval-augmented generative agent for scientific research. arXiv preprint arXiv:2312.07559, 2023. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, et al. Chain of ideas: Revolutionizing research via novel idea development with llm agents. arXiv preprint arXiv:2410.13185, 2024. Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation embeddings for knowledge graph completion. In Proceedings of the AAAI conference on artificial intelligence, volume 29, 2015. Xingyu Liu, Juan Chen, and Quan Wen. survey on graph arXiv classification and link prediction based on gnn. preprint arXiv:2307.00865, 2023. Shengchao Liu, Jiongxiao Wang, Yijin Yang, Chengpeng Wang, Ling Liu, Hongyu Guo, and Chaowei Xiao. Conversational drug editing using retrieval and domain feedback. In The Twelfth International Conference on Learning Representations, 2024. Hui Wen Loh, Chui Ping Ooi, Silvia Seoni, Prabal Datta Barua, Filippo Molinari, and Rajendra Acharya. Application of explainable artificial intelligence for healthcare: systematic review of the last decade (20112022). Computer methods and programs in biomedicine, 226:107161, 2022. Man Luo, Arindam Mitra, Tejas Gokhale, and Chitta Baral. Improving biomedical information retrieval with neural retrievers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 1103811046, 2022. Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew White, and Philippe Schwaller. Augmenting large language models with chemistry tools. Nature Machine Intelligence, pages 111, 2024. Shengjie Ma, Chengjin Xu, Xuhui Jiang, Muzhi Li, Huaren Qu, and Jian Guo. Think-on-graph 2.0: Deep and interpretable large language model reasoning with knowledge graph-guided retrieval. arXiv e-prints, pages arXiv2407, 2024. Tim Miller. Explainable ai is dead, long live explainable ai! hypothesis-driven decision support using evaluative ai. In Proceedings of the 2023 ACM conference on fairness, accountability, and transparency, pages 333342, 2023. Sai Munikoti, Anurag Acharya, Sridevi Wagle, and Sameera Horawalavithana. Evaluating the effectiveness of retrievalaugmented large language models in scientific document reasoning. arXiv preprint arXiv:2311.04348, 2023. Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1):1133, 2015. Yang Jeong Park, Daniel Kaplan, Zhichu Ren, Chia-Wei Hsu, Changhao Li, Haowei Xu, Sipei Li, and Ju Li. Can chatgpt be used to generate scientific hypotheses? Journal of Materiomics, 10(3):578584, 2024. Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, and Siliang Tang. Graph retrieval-augmented generation: survey. arXiv preprint arXiv:2408.08921, 2024. generators: comprehensive evaluation. In First Conference on Language Modeling, 2024. Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, and Daniel Weld. Scideator: Humanllm scientific idea generation grounded in research-paper arXiv preprint arXiv:2409.14634, facet recombination. 2024. Mohaimenul Azam Khan Raiaan, Md Saddam Hossain Mukta, Kaniz Fatema, Nur Mohammad Fahad, Sadman Sakib, Most Marufatul Jannat Mim, Jubaer Ahmad, Mohammed Eunus Ali, and Sami Azam. review on large language models: Architectures, applications, taxonomies, open issues and challenges. IEEE Access, 2024. Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends in Information Retrieval, 3(4):333389, 2009. Dong Shu, Tianle Chen, Mingyu Jin, Chong Zhang, Mengnan Du, and Yongfeng Zhang. Knowledge graph large language model (kg-llm) for link prediction. arXiv preprint arXiv:2403.07311, 2024. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can large-scale huarXiv preprint llms generate novel research ideas? man study with 100+ nlp researchers. arXiv:2409.04109, 2024. Sanchit Sinha, Guangzhi Xiong, and Aidong Zhang. Colidr: Concept learning using aggregated disentangled represenIn Proceedings of the 30th ACM SIGKDD Contations. ference on Knowledge Discovery and Data Mining, pages 26992710, 2024. Sanchit Sinha, Guangzhi Xiong, and Aidong Zhang. selfexplaining neural architecture for generalizable concept learning. In Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, pages 503511, 2024. Michael Skarlinski, Sam Cox, Jon Laurent, James Braza, Michaela Hinks, Michael Hammerling, Manvitha Ponnapati, Samuel Rodriques, and Andrew White. Language agents achieve superhuman synthesis of scientific knowledge. arXiv preprint arXiv:2409.13740, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Self-consistency improves chain of Denny Zhou. arXiv preprint thought reasoning in language models. arXiv:2203.11171, 2022. Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. Scimon: Scientific inspiration machines optimized for novelty. arXiv preprint arXiv:2305.14259, 2023. Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, and Bowen Zhou. Large language models are zero shot hypothesis proposers. arXiv preprint arXiv:2311.05965, 2023. Shijie Wang, Wenqi Fan, Yue Feng, Xinyu Ma, Shuaiqiang Knowledge graph retrievalllm-based recommendation. Wang, and Dawei Yin. augmented generation for arXiv preprint arXiv:2501.02226, 2025. Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Hu Jinfang, and Bowen Zhou. Large language models as biomedical hypothesis Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Chih-Hsuan Wei, Alexis Allot, Po-Ting Lai, Robert Leaman, Shubo Tian, Ling Luo, Qiao Jin, Zhizheng Wang, Qingyu Chen, and Zhiyong Lu. Pubtator 3.0: an ai-powered literature resource for unlocking biomedical knowledge. Nucleic Acids Research, page gkae235, 2024. Dustin Wright, David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Isabelle Augenstein, and Lucy Lu Wang. Generating scientific claims for zero-shot scientific fact checking. arXiv preprint arXiv:2203.12990, 2022. Chaokai Wu, Yansong Wang, and Tao Jia. Dynamic link prediction using graph representation learning with enhanced In 2023 26th Interstructure and temporal information. national Conference on Computer Supported Cooperative Work in Design (CSCWD), pages 279284. IEEE, 2023. Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. Benchmarking retrieval-augmented generation for In Findings of the Association for Computamedicine. tional Linguistics: ACL 2024, 2024. Guangzhi Xiong, Qiao Jin, Xiao Wang, Minjia Zhang, Zhiyong Lu, and Aidong Zhang. Improving retrievalaugmented generation in medicine with iterative follow-up questions. In Biocomputing 2025: Proceedings of the Pacific Symposium, pages 199214. World Scientific, 2024. Guangzhi Xiong, Eric Xie, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, and Aidong Zhang. Improving scientific hypothesis generation with knowlarXiv preprint edge grounded large language models. arXiv:2411.02382, 2024. Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and Erik Cambria. Large language models for automated open-domain scientific hypotheses discovery. arXiv preprint arXiv:2309.02726, 2023. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, and Dongzhan Zhou. Large language models for rediscovIn 2nd ering unseen chemistry scientific hypotheses. AI4Research Workshop: Towards Knowledge-grounded Scientific Research Lifecycle, 2025. Qi Zeng, Mankeerat Sidhu, Hou Pong Chan, Lu Wang, and Heng Ji. Meta-review generation with checklist-guided iterative introspection. arXiv preprint arXiv:2305.14647, 2023. Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. Advances in neural information processing systems, 31, 2018. Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein, and Jacob Steinhardt. Goal driven discovery of distributional differences via language descriptions. Advances in Neural Information Processing Systems, 36:4020440237, 2023. Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan. Hypothesis generation with large language models. arXiv preprint arXiv:2404.04326, 2024. Implementation Details For the retrieval of external knowledge from scientific literature, we implemented the information retrieval system by adopting the BM25 retriever [Robertson et al., 2009] for processed PubMed chunks provided by the MedRAG toolkit [Xiong et al., 2024a,b]. BM25 (Best Matching 25) is probabilistic retrieval model that ranks documents based on term frequency, document length normalization, and the specificity of terms through inverse document frequency (IDF). We selected BM25 as our text retriever because it is particularly effective for the biomedical domain, where dense retrievers often struggle to encode the nuanced semantics of biomedical terms such as gene names [Luo et al., 2022]. BM25s reliance on exact term matching with statistical weighting makes it well-suited for capturing term-specific relevance in structured biomedical text. In our experiments, τ in Equation (1) is set as 0.0. The number of retrieved documents is set as = 32 for hypothesis generation, and = 8 for claim verification. To identify biomedical entities in given claim, we used two-step process. In the first step, we prompted LLMs to extract the entity mentions directly from the claim (Figure 11). This step focused on identifying relevant biomedical terms, such as gene names, proteins, or diseases, without additional processing or complex workflows. The extracted entity mentions were then used in the second step, where each mention was matched to its unified representation in the PubTator 3.0 knowledge graph. This matching was implemented using BM25 retriever. For constructing the BM25 index, each piece of text, or chunk, was designed by concatenating all possible mentions of given entity stored in PubTator 3.0. By leveraging BM25s ranking capabilities, we retrieved the most relevant chunk corresponding to each entity mention, ensuring accurate alignment with PubTators unified entities. Computational Cost Table 5 shows the number of all tokens used in experiments for Table 2. It shows that the additional knowledge from either the knowledge graph (KG) or literature (Lit.) will significantly increase the number of input tokens. In particular, the literature brings more tokens than KG, as the knowledge in KG is always structured and summarized. While input lengths vary across different settings, output lengths are relatively stable, consistent pattern shown in different LLMs. LLM Type Setting Param. +KG +Lit. +KG+Lit. Llama -3.1-8B Llama -3.1-70B GPT-4o -mini GPT-4o Input Output Input Output Input Output Input Output 295.8k 811.1k 295.8k 813.7k 295.8k 751.6k 295.8k 909.9k 1.7M 25.8M 1.0M 782.5k 1.7M 25.8M 777.6k 881.2k 1.7M 25.8M 787.2k 684.0k 1.7M 25.8M 891.5k 839.1k 27.2M 1.2M 27.2M 767.8k 27.2M 707.1k 27.2M 875.3k Table 5: Summary of #tokens used for all experiments in Table 2."
        },
        {
            "title": "TruthHypo",
            "content": "Table 2 presents the F1 score of various LLMs on the TruthHypo benchmark, evaluating their ability to identify the existence of new relation given current knowledge. To provide more granular analysis, Table 6 breaks down the results into precision and recall for different tasks, offering insights into the strengths and weaknesses of each model and knowledge augmentation setting. From the results in Table 6, we observe that smaller LLMs, such as Llama-3.1-8B, tend to achieve higher recall scores across all tasks, along with relatively lower precision. This indicates that while these models can generate comprehensive set of hypotheses, they are prone to high false positive rate, which could pose challenges in real-world applications, such as scientific hypothesis generation, where precision is often critical. High false positive rates could result in wasted time and resources when pursuing hypotheses that are unlikely to hold upon experimental validation. Given that validating new biomedical hypotheses often requires months or even years of research, ensuring high precision in hypothesis generation is of paramount importance. Among the tested models, GPT-4o with external knowledge from the literature achieved the highest precision across all tasks, demonstrating its ability to generate hypotheses with fewer false positives. However, this precision came at the expense of lower recall, especially when compared to GPT4o with knowledge augmentations from both literature and knowledge graphs (KG). This trade-off highlights the importance of balancing precision and recall based on the specific requirements of given application. When comparing different knowledge settings, we found that the improvements provided by external knowledge sources varied across tasks and models. For example, knowledge graph (KG) information significantly enhanced the precision of all LLMs on tasks involving Disease & Gene and Gene & Gene relations, but it did not notably improve the In precision of GPT-4o on the Chemical & Gene task. contrast, the literature knowledge augmentation slightly improved the precision of all LLMs except GPT-4o-mini. Interestingly, the setting that combined both knowledge sources provided more balanced precision improvement, offering middle ground between the individual benefits of KG and literature-based augmentations. Additionally, Table 6 reveals that larger models such as GPT-4o consistently outperformed smaller models in precision, regardless of the knowledge setting, reflecting their ability to integrate complex external information effectively. This highlights the potential of larger models to better utilize structured and unstructured knowledge sources for hypothesis generation. However, smaller models, with their higher recall, may still serve as useful tools for exploratory or broad hypothesis generation tasks where exhaustive coverage is prioritized over precision. Overall, the analysis demonstrates that the choice of LLM and knowledge augmentation strategy should be guided by the specific trade-offs between precision and recall that align with the requirements of the downstream task. For biomedical applications, where precision is often paramount, leveraging models like GPT-4o with literature-based augmentations appears to be the most effective approach. To further understand the limitations of hypothesis generation with high groundedness scores, we conducted an indepth analysis of the error patterns. We identified two representative types of errors: (1) cases where the LLM incorrectly infers that there is no association between the given entities, despite supporting evidence; and (2) cases where the model simply echoes or paraphrases the provided context without engaging in substantive reasoning or hypothesis formation. These findings highlight the need for more robust mechanisms to ensure both accurate association detection and genuine reasoning in hypothesis generation, enhancing the interpretability and trustworthiness of the overall system [DoshiVelez and Kim, 2017; Loh et al., 2022; Miller, 2023; Sinha et al., 2024a,b]."
        },
        {
            "title": "Experiments",
            "content": "Figure 5 shows the template we used to construct hypothesis generation query given two different entities. The prompt templates for the use of LLMs in the Parametric, Parametric + KG, Parametric + Lit., and Parametric + KG + Lit. settings are presented in Figures 6, 7, 8, 9, respectively. These templates were designed to guide the LLMs in effectively leveraging various sources of knowledge while maintaining pre-determined structure in the model output to facilitate consistent parsing and downstream analysis. Figure 10 shows the template for LLMs to extract scientific claims from the entire rationale. For the identification of biomedical entities and the use of LLMs for claim verification, we employed the templates shown in Figures 11 and 12, respectively. The entity identification templates (Figure 11) were crafted to enable the LLMs to extract precise mentions of biomedical entities such as genes or diseases from textual claims. These prompts were carefully designed to minimize ambiguity, ensuring that entities sharing the same mention could be properly distinguished using their unique IDs. Knowledge LLM Chemical & Gene Disease & Gene Gene & Gene Average Prec Recall Prec Recall Prec Recall Prec Recall Parametric Wei et al. [2022] Parametric + KG Baek et al. [2024] Parametric + Lit. Lewis et al. [2020] Parametric + KG + Literature Llama-3.1-8B 67.57 Llama-3.1-70B 74.69 83.00 GPT-4o-mini 90.59 GPT-4o Llama-3.1-8B 71.42 Llama-3.1-70B 90.63 86.37 GPT-4o-mini 86.27 GPT-4o Llama-3.1-8B 68.82 Llama-3.1-70B 74.92 78.18 GPT-4o-mini 92.73 GPT-4o Llama-3.1-8B 68.21 Llama-3.1-70B 84.13 82.61 GPT-4o-mini 86.61 GPT-4o 98.51 89.33 83.62 72.83 94.54 85.24 86.48 91. 97.77 91.94 93.80 69.60 85.73 85.48 94.91 93.05 66.79 75.23 79.47 82.67 74.04 93.14 91.06 91.30 68.36 75.56 80.10 83.78 70.64 87.41 82.45 84. 97.77 93.30 84.36 69.27 86.03 53.07 62.57 70.39 97.77 94.97 92.18 69.27 86.03 69.83 86.59 81.01 66.92 73.13 75.50 83.27 72.16 88.58 92.39 87. 68.49 74.58 74.94 89.37 69.96 80.05 83.16 87.50 96.99 80.55 83.56 62.74 88.77 70.14 73.15 78.08 95.89 84.38 89.32 50.68 91.23 82.47 85.21 84. 67.29 74.37 80.37 87.60 71.93 90.34 88.27 87.21 68.67 74.92 77.55 90.62 69.01 83.33 82.73 86.61 98.00 87.48 83.70 69.63 91.85 76.89 79.70 84. 97.26 90.30 92.37 64.44 87.26 82.59 91.19 89.11 Table 6: Performance comparison of different LLMs on the TruthHypo benchmark across various knowledge settings, with precision and recall as the evaluation metrics. Prec denotes the link-level precision, while Recall represents the link-level recall. Prompt template for constructing user input with given entities Can we hypothesize the potential relation between {{entity type 1}} {{entity name 1}} ({{entity ID 1}}) and {{entity type 2}} {{entity name 2}} ({{entity ID 2}})? [{{relation label 1}}, {{relation label 2}}, The final hypothesis can be one of no relation]. Figure 5: Prompt template for constructing user input with given entities. Prompt template for hypothesis generation with parametric knowledge only You are scientist. Your task is to generate scientific hypothesis following given instructions. ### User Input {{input}} Your output must include two sections: 1. **### Step-by-step Reasoning**: - Think step-by-step to derive the hypothesis. 2. **### Structured Output**: - Present your proposed hypothesis in the following JSON format: json { proposed hypothesis: Statement of the proposed hypothesis } Figure 6: Prompt template for hypothesis generation with parametric knowledge only. Prompt template for hypothesis generation with knowledge from parameters and KG You are scientist. Your task is to generate scientific hypothesis following given instructions. ### Relevant Knowledge {{knowledge}} ### User Input {{input}} Your output must include two sections: 1. **### Step-by-step Reasoning**: - Think step-by-step to derive the hypothesis. 2. **### Structured Output**: - Present your proposed hypothesis in the following JSON format: json { proposed hypothesis: Statement of the proposed hypothesis } Figure 7: Prompt template for hypothesis generation with knowledge from parameters and knowledge graphs (KGs). Prompt template for hypothesis generation with knowledge from parameters and literature You are scientist. Your task is to generate scientific hypothesis following given instructions. ### Relevant Documents {{document}} ### User Input {{input}} Your output must include two sections: 1. **### Step-by-step Reasoning**: - Think step-by-step to derive the hypothesis. 2. **### Structured Output**: - Present your proposed hypothesis in the following JSON format: json { proposed hypothesis: Statement of the proposed hypothesis } Figure 8: Prompt template for hypothesis generation with knowledge from parameters and literature. Prompt template for hypothesis generation with knowledge from parameters and KG and literature You are scientist. Your task is to generate scientific hypothesis following given instructions. ### Relevant Documents {{document}} ### Relevant Knowledge {{knowledge}} ### User Input {{input}} Your output must include two sections: 1. **### Step-by-step Reasoning**: - Think step-by-step to derive the hypothesis. 2. **### Structured Output**: - Present your proposed hypothesis in the following JSON format: json { proposed hypothesis: Statement of the proposed hypothesis } Figure 9: Prompt template for hypothesis generation with knowledge from parameters, KG, and literature. Prompt template for claim identification ### Statement {{statement}} Summarize the statement as list of claims which will be further verified by external resources. Output the summarized claims in the JSON format: json{claims: [claim1, ...]} Figure 10: Prompt template for claim identification. Prompt template for entity recognition ### Background {{background}} Extract key entities from the background statement that will be used to search for relevant information in an external knowledge graph. Each entity should be extracted as entity type (e.g., Disease/Chemical/- Gene/Mutation) entity name (entity id if presented). Output the extracted entities in the JSON format: json{entities: [entity1, ...]} Figure 11: Prompt template for entity recognition. Prompt template for claim verification You are scientist. Your task is to verify if the relevant documents and knowledge (if applicable) can support the given claim. ### Relevant Documents {{document}} ### Relevant Knowledge {{knowledge}} ### Claim {{claim}} Judge if the given information supports the claim. Output {groundedness: 1} if the materials support the claim else {groundedness: 0}. Figure 12: Prompt template for claim verification."
        }
    ],
    "affiliations": [
        "University of Virginia"
    ]
}