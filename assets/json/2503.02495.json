{
    "paper_title": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer",
    "authors": [
        "Yujiao Yang",
        "Jing Lian",
        "Linhui Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement selective routing on input data and experts. Our approach advances MoE design with four key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch-wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE's routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the UoE model surpass Full Attention, state-of-art MoEs and efficient transformers (including the model architecture of recently proposed DeepSeek-V3) in several tasks across image and natural language domains. In language modeling tasks, we achieve an average reduction of 2.38 in perplexity compared to the best-performed MoE method with an average of 76% FLOPs. In Long Range Arena benchmark, we recorded an average score that is at least 0.68% higher than all comparison models including Full Attention, MoEs, and transformer variants, with only 50% FLOPs of the best MoE method. In image classification, our model yielded an average accuracy improvement of 1.75% than the best model while maintaining comparable FLOPs. The source codes are available at https://github.com/YujiaoYang-work/UoE."
        },
        {
            "title": "Start",
            "content": "Union of Experts: Adapting Hierarchical Routing to Equivalently Decomposed Transformer Yujiao Yang, Jing Lian, Member, IEEE, Linhui Li, Member, IEEE (MoE) AbstractMixture-of-Experts enhances model performance while maintaining computational efficiency, making it well-suited for large-scale applications. However, expert in exist MoE paradigm works as an individual, thereby lacking high-quality expert interactions. Moreover, they have not been effectively extended to attention block, which constrains further efficiency improvements. To tackle these issues, we propose Union-of-Experts (UoE), which decomposes transformer into an equitant group of experts, and then implement selective routing on input data and experts. Our approach advances MoE design with four key innovations: (1) We conducted equitant expert decomposition on both MLP blocks and attention blocks based on matrix partition in tensor parallelism. (2) We developed two routing paradigms: patch-wise data selection and expert selection, to apply routing across different levels. (3) We design the architecture of UoE model, including Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoEs routing and computation operation, and optimize efficiency based on the hardware processing analysis. The experiments demonstrate that the UoE model surpass Full Attention, state-of-art MoEs and efficient transformers (including the model architecture of recently proposed DeepSeek-V3) in several tasks across image and natural language domains. In language modeling tasks, we achieve an average reduction of 2.38 in perplexity compared to the best-performed MoE method with an average of 76% FLOPs. In Long Range Arena benchmark, we recorded an average score that is at least 0.68% higher than all comparison models including Full Attention, MoEs, and transformer variants, with only 50% FLOPs of the best MoE method. In image classification, our model yielded an average accuracy improvement of 1.75% than the best model while maintaining comparable FLOPs. The source codes are available at https://github.com/YujiaoYangwork/UoE. Index TermsTransformer, Mixture of Experts, Attention mechanisms, Parallel processing, Deep learning I. INTRODUCTION ixture of Experts (MoE) [1] is an advanced deep learning framework that effectively enhances model efficiency and strengthens its predictive capabilities. This architecture features sophisticated gating mechanism that orchestrates constellation of finely tuned expert networks, each excelling with distinct data subsets. With the dynamic This work was supported by the National Science and Technology Major Project (2022ZD0115502), the National Natural Science Foundation of China (Nos. 52172382, 52472426). (Corresponding author: Linhui Li.) Yujiao Yang, Jing Lian and Linhui Li are with the School of Mechanical Engineering, Dalian University of Technology, Dalian 116024, China. (e-mail: yjyang@mail.dlut.edu.cn; lilinhui@dlut.edu.cn; lianjing@dlut.edu.cn ). allocation mechanism, it can enhance the model's computational efficiency while maintaining an optimal performance. Existing MoE methods are confronted with certain challenges. In terms of performance, they are fundamentally grounded in the concept of ensemble learning [2] . In this manner, dense model is essentially isolated into multiple sub-models. Each sub-model contains an independent subspace, which allows model to learn multiple different representations in parallel across multiple subspaces [3] . However, in this approach, the sub-models can only achieve indirect and limited interaction through downstream aggregator, resulting in relatively insufficient global information exchange capability. From an efficiency perspective, exist MoE methods are primarily applied to MLP blocks. They have not been effectively extended to attention efficiency improvements. Moreover, they seldom succeed in achieving parallel computing, nor in optimizing efficiency based on cuda computing process. block, which constrains further To address the aforementioned issues, we propose Union-ofExperts (UoE). The idea of UoE is inspired and influenced by Megatron-LM [4] which implements an efficient intra-layer model parallelism. We apply this mechanism on MoE method, to decompose both MLPs and attention blocks into several experts while maintaining its intrinsic nature. In this paradigm, each expert evolves to become one part of whole model rather than an individual. The enhanced collaboration promotes collective intelligence and knowledge sharing, which in turn enhances model performance. Fig. 1 illustrates comparison between model parallelism, MoE and our proposed UoE. tensor, thereby effectively exploit We also develop group of routing mechanism, which contains selection strategies for input data and experts. In data selection strategy, we split each input sample into patches, and select no more than patches for each expert as input. In this configuration, expert only receive one part of the sample as locality of input information while maintaining fine-grained data routing strategy. In expert selection strategy, we route each input sample to top of experts. This strategy is conducive to enhancing model stability as activated experts can observe the input sequence in comprehensive view. Combining the two mechanisms allows for dynamically and precisely removing the parts of samples and experts that are not beneficial to final results, which helps to optimize computational efficiency while preserving and even enhancing model performance. the We applied the proposed routing mechanism to equivalent decomposed attention experts and MLP experts to build the fundamental architecture of UoE. UoEs architecture consists of Fig. 1. The comparison between Model Parallelism, Mixture-of-Experts and our proposed Union-of-Experts. Model parallelism partitions the model equivalent modules for distributed computation. Mixture-of-Experts employs multiple independent experts and selectively activates subset for the output. Union-of-Experts integrates the equivalent decomposition strategy of model parallelism into MoE framework, making the activated experts as union equivalent to single model of the same scale. Selective Multi-Head Attention (SMHA) and Union-of-MLPExperts (UoME). SMHA maintains the multi-head design of Multi-Head Attention mechanism, which achieves selective routing mechanism while enabling the model to learn diverse representations across multiple subspaces. UoME incorporates the selective routing mechanism into decomposed MLP model, integrating the activated experts into union similar to larger-scale dense model. Its worth nothing that the experts in both components are formulated by decomposing MHA and MLP of transformer model in accordance with the equivalent decomposition principle. Therefore, they are unified within the UoE architecture. To further improve our models efficiency, we implement parallel multi-expert computing at the algorithmic level. This approach completely resolving the issues in parallelization of classical MoE methods. We also conduct an in-depth analysis to identify the inefficiency factors in computation. Specifically, we visualize the time cost of each operation in the complete training phase and excavated several potential inefficiency steps. By adopting the above methods, our method reduces Floating Point Operations (FLOPs) by at least 30% in comparison with the original version, and achieves an average 2.26 speedup and 2.68 memory efficiency over the state-of-the-art MoE method. It can be anticipated that applying our model to largerscale language models with more sparser activation setting will lead to more than 10x increase in efficiency. We evaluate UoE on three typical tasks: language modeling, long range sequence modeling and image classification. Experimental results demonstrate that our model consistently outperforms several methods, including standard transformer [5] , state-of-the-art MoE (including the latest DeepSeek-V3 work) and transformer variants. Specifically, with significantly reduced computational overhead, it achieves 2.87 optimization in perplexity compared to the state-of-the-art MoE models on WikiText-103 benchmark, and 1.89 decrease on One-Billion-Word language modeling benchmark. On the wellestablished Long Range Arena (LRA) benchmark, the average precision outperforms the state-of-the-art MoE model by 0.68%, as well as the advanced efficient transformer model by 1.9%. For the ViT-based image classification task, the average precision shows an average improvement of 1.75% over the state-of-the-art MoE model. These results strongly emphasize our model's robust performance and adaptability in various domains. II. RELATED WORK A. Variants of Transformer in leverages Transformer intelligence, bringing [5] architecture artificial represents significant forth advancement unparalleled capabilities alongside the challenge of resourceintensive training and serving processes. Significant enthusiasm has been ignited for actively enhancing the efficiency of transformer architectures. preponderance of the works concentrate on reducing memory usage and improving computational efficiency. Sparse Transformer [6] introduces sparse attention mechanisms to selectively attend to relevant tokens within sequence. Linformer [7] and Lightninglinear-complexity self-attention Attention[8, 9] mechanisms, catering to large-scale data and lengthy sequences. Reformer [10] mitigates memory constraints by employing reversible layers and locality-sensitive hashing techniques. Flash Attention [11] reduces memory access costs through tiling, while its subsequent version [12] further enhances performance by optimizing memory access and computation fusion. DeepSeek-V2 [13] introduces the Multi-Head Latent Attention (MLA) mechanism, which employs low-rank joint compression to enhance training efficiency and reduce the KV cache size during inference. Tensor Product Attention (TPA) [14] dynamically constructs QKV as context-dependent decomposed tensors, enabling adaptive adjustments and facilitating seamless integration with effective Rotary Position Embedding. The other works aims to enhance the modeling capabilities of long sequences. Transformer-XL [15] introducing segmentlevel recurrence to enhance sequence modeling beyond the scope of the original Transformer. Sinkhorn Transformer [16] fuses Sinkhorn algorithm with self-attention mechanisms to improve sequence modeling accuracy. Long-Short-Term Memory Transformer [17] combines Transformer with Long Short-Term Memory (LSTM) mechanisms and thus enhance the modeling capability for long sequences. SeerAttention [18] integrates learnable gating mechanism into standard attention mechanism, enabling adaptive selection of salient blocks within the attention map. Although These innovations have notably improved the performance of the transformer frameworks in their respective domains, they are typically model-specific and may not be universally applicable. By comparison, our work provides model-free method. By simply adding selection mechanism to existing model parallel transformers, we can train model more efficiently while maintaining or even improving its performance. We also notice that the recent NSA [19] from DeepSeek and the MoBA [20] from Moonshot AI published on February 18, 2025 shares similarities with the Selective Multi-Head Attention (SMHA) mechanism in Innovation Point 3 of the abstract. However, this similarity does not diminish the significance of our work. Firstly, the core innovation of our work, which was developed independently in close temporal proximity, is characterized by the first application of equivalent decomposition to the partition of MoE experts (Innovation Point 1). This is pivotal in enhancing the overall capability of expert group and is distinct from the aforementioned work. Next, the SMHA essentially represents the application of UoE's equivalent decomposition and selective routing in Multi-Head Attention. Actually, SMHA is multi-head condition in UoE. We introduced not only SMHA for multi-head model but also Union-of-MLP-Experts (UoME) for dense model. Moreover, routing in SMHA also exhibits significant differences mechanism and model architecture compared the to aforementioned work. e.g., we developed two independent selection paradigm (Innovation Point 2), data selection and expert selection, and designed and parallelized expert computation framework. (Innovation Point 4). Refer section 3B and 3C for more implementation details. B. Mix of Experts Mixture-of-Experts (MoE) enhances model capacity while maintaining the computation overhead, thereby attaining superior performance relative to dense models across various domains. In classical MoE layers, group of collaborative experts works in unison to address complex tasks, while each can be simple Feed-Forward Network or fully independent submodel [21]. To effectively manage this ensemble of experts, MoE introduces routing layers that decide which experts to activate based on the input, followed by aggregation layer that combine their outputs into unified response. Extensive works have emerged in the development of MoE architecture. The first successful deep learning-based MoE [1] inserted an routing layer between two LSTM layers to select sparse combination of activated experts, reaching the state-ofthe-art performance in machine translation. Despite this success, however, follow-on research was relatively dormant with greater emphasis on directly studying the Transformer [5]. This changed with the release of GShard [22] and Switch Transformers [23] both of which replaced the feed-forward layers in transformer architecture with expert layers. While the the dominant experts-as-a-layer approach has become paradigm, more recent works revisit the concept of experts as fully independent models [24, 25], which confers benefit of modularity and composability. Skywork-MoE [26] introduces innovative gated logit normalization and adaptive auxiliary loss coefficients to enhance expert diversity and training efficiency. DeepSeek-MoE fine-grained common experts and shared experts, which enhance expert specialization across different tasks. [27] employs Although the existing works have made significant strides, they are constrained by the organizational form of experts. In MoE frameworks, each expert works as an independent this pattern has been individual. The effectiveness of empirically validated, showcasing its reliability in practical scenarios. However, such design undermines collaboration ability of experts, which could potentially impair the model's ability to capture complex patterns. Moreover, it may introduce additional gather layers, leading to increased computational demands. In contrast, our approach set one part of whole model as an expert, enabling highly collaborative of experts from global perspective, without adding any fusion structure. It is worth nothing that our approach is not aimed at replacing all groups of experts with union models, but rather at introducing selection mechanism without damaging models original organizational form, no matter it was originally dense model or mixture of sub-models or heads (such as Multi-Head Attention layer). C. Model Parallelism Model parallelism is computational strategy designed to distribute the workload of large neural network models across multiple processing units [28]. There are generally two types of parallelism: tensor parallelism and pipeline parallelism. Pipeline parallelism offers an alternative by dividing the model into distinct stages, each handled by different device. This approach was notably advanced by the GPipe framework [29], which implemented synchronous mini-batch pipelining to train models with billions of parameters. On the other hand, Tensor parallelism partitions tensor operation (such as matrix-matrix multiplication) across multiple devices to accelerate computation or increase model size. In the field, Megatron-LM [4] demonstrated tensor parallelism in scaling transformer models. By splitting the weight matrices and activations across GPUs, it achieved substantial improvements in training large language models. We build our approach upon the work of Megatron-LM. Specifically, we apply its lossless weight splitting algorithm to equivalently decompose whole model into group of UoE experts. Compared to Megatron-LM, our approach only needs to activate subset of weights, thus can dynamically eliminate unnecessary computation overhead. the potential of 2 Fig. 2. The overall architecture of UoE. We equivalently decompose MLP block and attention block of transformer into independent computation branch (with = in the figure), and regard each branch as an expert, as shown in panel and b. We selectively route input batch to each expert for independent calculation, and aggregate the results of experts based on the routing indices. Panel and illustrate the mechanisms of selection function and aggregation function, where ijX can be either patch or sample, corresponding to data selection and expert selection respectively. III. IMPLEMENTING UNION-OF-EXPERTS A. Lossless Decomposition of Transformer Fig. 3 illustrates the workflow of our methods. We first detail the implementation of splitting transformer model into an equivalent set of experts. classical transformer can be comprised of two main components: Multi-Head Attention block and Multi-Layer Perceptron (MLP) block. We will illustrate the decomposition mechanism of both components separately. 1) MLP Block We start by detailing the MLP block. We referenced the method introduced by Shoeybi et al. [4] to decompose = . standard two-layer MLP into experts. WLOG, let 2 two-layer MLP is combination of two linear layers where each layers output is calculated through multiplication of input batch and transformation matrix followed by an activation function . We can make an equivalent decomposition on linear layer through two methods. One is to split along its column dimension, i.e., . In this pattern, activation function can be applied to the output of each partitioned matrix multiplication independently. The result can be calculated as follows: ] ) XAœï= ( ) [ 1 2 , 1 2 ( )œï XA 2 XA 1 (1) œï [ ( œï ( œï ( )] ), = = = : [ ] ,"
        },
        {
            "title": "The  equation  can  be  further  categorized  into  a  broadcast",
            "content": "operation and an element-wise multiplication with activation: ] , , 1 = = [ [ ] 2 (2) 2 ] [ ) = X , 1 ) ( œï 1 1 ] ( œï , Where denotes hadamard product. Another method is to split along its row dimension and along its column dimension, as shown in Equation 4: [ 2 2 , 1 2 ) ( œï = (3) = [ , 1 2 ], = 1 (4) With this pattern, we compute the result as follows: 1 2 Similarly, it can be decomposed into an element-wise + A 2 œï , 1 ( œï = (5) 1 = [ ] ) 2 multiplication and reduce operation with activation: , 1 = [ [ ] [ A , 2 1 1 2 ] , 1 2 [ ] , = 1 ] = = œï 2 = = 1 ( œï + 2 ) Note that due to the nonlinearity activation function ( œï . When incorporating the two ( œï ( œï + 2 1 ) 1 2 + ) ) decomposition procedures into our method, we employ an index select function in place of the broadcast operation in Equation 2, and replace the sum function in Equation 7 with the index add function. Please refer to section and for more details. Within one expert, the computation process should operate (6) (7) ( )œï , independentlywhich means that synchronization operations such as broadcast and reduce cannot be performed during computation. Inspired by Shoeybi et al. [4], we split the MLPs first layer in column dimension, and partition the second one in row dimension, as shown in Fig. 2(a). In this way, the synchronizations only occur at the beginning and ending of the MLPs calculation, thus we can take each independent part as an expert. 2) MHA Block Then we turn to more complex scenario: the attention block. An attention block can be decomposed into four steps: (8) , , XW= ) ( KŒ∏= SV= OW= (9) the input and output projection matrix. and denote the query, key and value. denote (10) (11) Where and denote the input and output batch. , OW ( )Œ∏ represents nonlinear transformation function. Similar to the IW , MLP situation, we apply expert column partition on OW . Thus, we acquire and corresponding row partition on group of independent experts which as whole is equivalent to head Attention block, as shown in Fig. 2(b). B. Dynamic Routing Strategy of Experts IW and We then apply unique routing method to the well-crafted attention and MLP experts. This is facilitated by applying index selection at the beginning of expert processing, and index addition in the end. Fig. 2(c) and Fig. 2(d) presents an overview of the preprocessing and postprocessing procedure. Generally, our proposed routing method comprises two modes, which perform division-selection mechanism separately on samples and models. The two modes can be implemented separately in different blocks, as well as simultaneously enabled within one block, although the latter may incur additional computation costs. 1) Data Selection Mode At each propagation step, the UoE takes an input batch . We can improve the efficiency by removing input fragments that are predicted to have negligible impact on each experts computation. Let denotes the length of token dimension, within batch of For single sample samples, we split them into patches with length along pl = FFN (FFN ( ) )T (13) We implement softmax activation to calculate the gating jp with respect to the i-th expert jg of the j-th patch ,i value (14) iE : , = , ) exp( = exp( , ) 0 For each expert, we select portion of patches as the input. The strategy that simply select the top-k patches for each expert is unable to dynamically adjust the amounts of valid patches based on input type and task complexity, leading to an inappropriate discarding of beneficial patches. Regarding this matter, we propose two stages routing method to achieve dynamic allocation of input patches. First, we route each patch to experts with the highest probabilities. The indexes of with respect to the j-th patches can be experts obtained by implementing topk function: = , We then calculate the maximum counts of patches Topk({ }, ) (15) id id g received by every expert from each sample: = max( = 1 m ( = 1 id )) (16) Where denotes the indicator function. = if 1 id , and 0 otherwise. We select the patches with id ) ( is in highest gating value as each experts input. the index of the that routing to the i-th expert is calculated id patches as follows: id = Topk({ }, ) , We sort the indices to restore their original relative position. Based on the indices, we extract the patches each expert required. This step is implemented through an index selection function sf : (17) = , id ( ) (18) l Where . In this manner, each expert receives copy of group of patches as input, as shown in Fig. 2(c). The above processing maintains the relative positions of patches while altering their absolute positions. However, in our method, the Rotary Position Embedding (RoPE) positional is added at the beginning of each attention block, so this change of absolute positions will not result in the loss of location information, see the next section for more details. ] , the sequence dimension: [ = 0 pm l , l = 0 ( 1 x ml 1 1) , , p Then we process each input with the i-th expert iE , which is implemented in parallel through batch matrix operations:"
        },
        {
            "title": "Note that each expert",
            "content": "= ) ( (19) iE here refers to one part of whole (12) x"
        },
        {
            "title": "Where",
            "content": ". We route each patch to the best determined top-k experts, selected from set of experts. The gating score is streamlined by two-layer feed-forward network (FFN): n d . This kind of union can not only be like union, dense model, but also be like multi-head one. Specifically, if iE is linear, we get union like dense model, otherwise, the union will be like multi-head one. In order to derive the final Fig. 3. The processing workflow of UoEs routing mechanism. Initially, we split each input sample into patches, and decompose whole model into experts. In data selection paradigm, we select no more than patches for each expert as input. In expert selection strategy, we route each original or selected input to the top experts. We compute the output of experts in parallel, and aggregate the results to the correct positions of the final output. output, we apply an index add function to gather into jy , the original input sample , For the j-th output patch procedure is described as follows: = + i = 1 = 0 , id ( , = ) (20) Where pm reshape and id denotes the indicator function. we add outputs of selected patches to its corresponding location in input tensor. In other words, we apply selective residual connection to form output tensor. This approach not only realize fine-grained routing, but also preserves the locality within patches, thereby effectively improving UoEs performance. 2) Expert Selection Mode In expert selection mode, we improve efficiency by limiting the number of activated experts. Note that compared to data selection, this method employs batch rather than sample as the processing subject. To initiate this process, we take batch of samples as input, . The gating value can be calculated as follows: ( reshape X ) = softmax(FFN( )) (21) We route the j-th sample in batch to the optimal of of the j-th sample id i experts, the routing indices can be obtained through topk function: = Topk({ id 1 j , }, ) (22) The sample indices tuple id belong to the i-th experts can be denoted as follows: id { id = ( ) {1, 2,... }, id } (23) With the sample indices, we apply an index selection to get the i-th experts input samples from function sf original input batch : f id ( (24) We enable experts to process their respective received ) , ( ) = ( ) samples: ( ) = ( Finally, we gather the outputs into original input through an index add function to obtain the final output . jY , the procedure is described as For each output sample (25) ) ( ) follows: = + ic = 1 = ( ) id ( ( ) = ) (26) ic denotes the cardinality of the i-th set of Where tuple. In comparison with data selection, the input sample in expert selection mode maintains its integrity and continuity across the sequence dimension, which contributes to the robustness enhancement. We claim that UoEs routing mechanism is essentially superposition of data selection and expert selection. Fig. 3 presents this processing workflow. It is evident that both selection modes are special cases of this paradigm. If both modes are activated within one block, only the selected patches routed to the activated experts will be processed, which enables fine-grained and flexible routing progress. Although it shows considerable potential, its relatively intricate structure imposes extra time costs under current parallel framework. In practice, we primarily employ single data selection or expert selection mechanism to construct the model. This contributes to comprehensive optimization of performance and efficiency. C. Transformer Based UoE Implementation We apply the aforementioned mechanisms to build our Union-of-Experts transformer. The overall architecture of the model is depicted in Fig. 2. We will briefly review the details of UoEs attention and MLP block in this section. To facilitate explanation without loss of generality, we set the patch length to 1, and apply data selection on both attention block and pl MLP block of the model. 1) Selective Multi-Head Attention denote the sequence length, In our UoE transformer, the attention block is developed based on standard Multi-Head-Attention (MHA) block. Let denote the number of attention heads, which is equal to an the number of experts, al denote the maximum length of processed inputs , which is equal to maximum patch counts multiplied by number of patches in one head, hd denote the dimension per head or expert, and denote the input sample at UoE attention block. We apply on the input . The data data selection function processing procedures can be formulated as follows: , id denote the embedding dimension, (27) ( ) DSf ùëëùëë = h DS While DSf represents series of operations defined in Equations 12 to 18. n h id a denote the routing indices, . We apply three column parallel linear functions to , to calculate query , key , and value : ] = = ,..., , 1 2 [ (28) (29) (30) (31) (32) [ , 1 [ 2, ,..., 2 ,..., = [ ] , ,... W W 1 2 2 1 ] = = ] = = d a h , Where , ability, we matrix, extrapolation Embedding (RoPE) into query and key: . To v , , a denotes the projection improve the model's integrated Rotary Position PPL = exp( 1 = 1 log ( i 1: 1 )) = [ i , RoPE( )] ="
        },
        {
            "title": "Where",
            "content": "c k , r , qc , a kc denotes the constant part in query and key, a qr , a kr the part where RoPE will be applied, . We combine the attention queries , keys , 1 and values to calculate the attention output: qc qr kc kr d + = denotes = , + = Softmax( i ) v= (33) (34) = where oW h 2 ,..., n , W 1 1 = denotes the output projection matrix. defined in (35) ( ) Ultimately, we apply index add function , Equation 20 on to yield the final output : IA = + al = 0 2) Union-of-MLP-Experts = , id ( , = ) (36) In the design of the UoE style multilayer perceptron block, we incorporated lossless model decomposition and expert mn denote the routing methods into two-layer MLP. Let denote the maximum length of number of MLP experts, ml processed inputs , ed denote the project dimension of experts. We start by applying preprocess to the input data. Given sample in an input batch , the process can be formulated as follows: inpl , id = DS ( ) (37) While DSf is defined in Equations 12 to 18, id m m . As illustrated in denote the routing indices, section A, we implement column partition on the first MLP layer, and row partition on the second one. We can outline the expert processing procedure in the following manner: ( = = œï ( œï = 1 1 2, 1 ) 2 2 ( œï ) = ( œï , ( œï ) ,... A v , 1 2 ,... (38) (39) ,... = ) ) [ ] [ ] d IA e denote Where the down-projection matrix, denote the up-projection matrix, OA ( )œï denotes the SiLU activation function. Finally, we use the to add each element in sequence index add function to the specified position in input sequence to obtain the output : IAf = IA ( , ) (40) 3) Attention Mask p"
        },
        {
            "title": "When",
            "content": "integrating data selection into mask attention mechanism, it is necessary to select elements from the original attention mask to form new group of masks that match with , we the group of selected data. For padding mask use the indices derived from Equation 17 to perform indexing along sequence dimension: = , (41) im denotes the padding mask corresponding to the i-th experts input. With regard to casual mask , It is required to perform indexing operations along sm and the two dimensions for an times. Considering that are comparable in size, this will result input sample in non-negligible computational overhead. Here we introduce an efficient and equivalent solution. As illustrated in id S"
        },
        {
            "title": "Where",
            "content": "p l ) ( al i Fig. 4. The processing of casual mask in data selection paradigm. Each expert requires the computation of casual mask based on its corresponding routing lower triangle indices. In self-attention mechanism, we apply the same indices on both queries and keys. As result, we obtain an easily producible a matrix independent of the indices, which is applicable to all experts. Fig. 4, because of the symmetry between the two indexing operations, the generated mask is essentially independent of its order leading principal position. Thus, we take the submatrix of the original mask as the shared sequence mask sm inherit the autoregressive capability of the transformer. 4) Load Balancing Loss for all experts inputs, enabling the model to -thal a Prior research indicates that the imbalanced expert load in MoE models may result in routing collapse and computational inefficiency in expert parallelism scenarios [30]. To mitigate excessive imbalance within the given sequence, we employ an auxiliary sequence-wise load balancing loss similar to Dai et al. [27]: Bal = PŒ± = 1 ( t , Topk ( { , 1 } , ) ) = n kl = 1 = , 1 = 1 where Œ± denotes hyper-parameter named balance factor, denotes the number of experts in attention or MLP block, denotes the number of activated experts, and denotes the gating value. The auxiliary load balancing loss promotes expert load distribution for each sequence to be balanced. D. Efficiency Optimization UoEs efficiency lies in its capacity to scale up the model size without proportional increase in computational expense. For certain reasons such as non-parallel implementation, they typically encounter challenges in achieving the expected acceleration. To improve UoEs efficiency, we conducted an inefficiencies and analytical implemented corresponding refinements, which we will discuss phase by phase. 1) Preprocessing & Postprocessing Phase the potential review of In preprocessing phase, we calculate the gating value to (42) (43) (44) dynamically route data subsets into several experts. In postprocessing phase, we use to route expert outputs to its correct location. Existing methods ordinarily employ routing procedure serially through n-step for loop. With additional memory copy operation such as Memcpy DtoD, it exacerbates the time complexity. To tackle this issue, we uniformly encode all data indexes, and retrieve all experts inputs within one index selection operation. Similarly, we apply only one scatter add operation to achieve the inverse process of retrieve operation. In this way, only one memory copy operation is needed in each phase, leading to significant reduction in time costs. 2) Propagation Phase In propagation phase, matrix operation accounts for the majority of the computational overhead. Existing MoE methods have difficulties in parallel computing, leading to significant reduction in computational efficiency. For instance, we observed that the non-parallelized implementation of MoEs sometimes even exhibited higher time cost than dense model with same configuration in scenario that only 50% parameters of MoEs are activated. From another perspective, matrix operation processes involve three procedures: operand result writing back. transmission, computation, Performance profiles of various MoE implementations indicate that each of these factors has the potential to become performance bottleneck. and In light of the aforementioned considerations, we optimized the matrix operation process of UoE. Specifically, we transformed inputs and weights into tensors with matching dimensions, and then perform batch matrix operations. Specifically, we conduct this procedure using the baddbmm function, as it effectively reduces computational overhead through three factors: 1. The function exhibits reduced time costs when transferring learnable parameters to cuda cores. 2. The function implements an operation that integrates matrix multiplication and bias addition, with execution times comparable to single multiplication. 3. The function performs results, the computed an in-place operation which directly updating the bias terms the with computational overhead associated with secondary write operation. Overall, by introducing the above optimizations, our model achieves speedup of over 30% compared to the original version. thereby eliminating IV. EXPERIMENTS In this section, we assess the performance of our proposed UoE method through series of experiments conducted in both language and vision domains. In detail, we designed three experiments: range sequence modeling and image classification, each focusing on specific challenge. A. Natural Language Modeling language modeling, long Benchmark Description. In natural language modeling experiments, we train and evaluate our model on two datasets: wikitext-103 and One Billion Word. WikiText-103 is large-scale language modeling dataset derived from Wikipedia, consisting of 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of models to capture long-term dependencies and contextual relationships within extended text sequences. One Billion Word, on the other hand, is corpus containing approximately 1 billion words of diverse text sourced from news articles, serving as robust benchmark for large-scale language modeling with focus on handling diverse and noisy real-world data. for efficiency optimization. Given Comparison Baseline. We compare our method with stateof-the-art MoE methods, including DeepSeek-MoE [27], DeepSeek-V3 [31], Skywork-MoE [26] and XMoE [32]. DeepSeek-MoE employs fine-grained expert specialization for task allocation within specialized domains and utilizes shared experts to enhance knowledge transfer across different tasks. Its successor version DeepSeek-V2 [13] and DeepSeek-V3 introduce new Multi-Head Latent Attention (MLA) mechanism, which compresses key-value pairs into latent that representations DeepSeek-V2 and V3 only exhibit subtle differences in routing method (i.e., the V3 replaces softmax activation with the tanh activation), we select the latest V3 version as representative comparison model. Skywork-MoE refines expert activation selection through gating logit normalization and employs adaptive auxiliary to dynamically regulate regularization in response to expert load balance. XMoE employs fine-grained experts and thresholdbased router to enhance model adaptability and promote sparsity incorporate large-scale models. We also Transformer [5, 33] as fully activated baseline model to facilitate more thorough evaluation of efficiency. loss coefficients in Evaluation Metrics. To evaluate the performance of our model on language modeling task, we adopt Perplexity (PPL) and Floating Point Operations (FLOPs) as the primary evaluation metrics. Perplexity is metric that measures how well language model predicts sequence of words 1 PPL = exp( log ( i 1: 1 )) (45) = 1 where is the total number of words in the dataset, is the predicted probability of the i-th word given ( ) w 1: 1 the previous context . 1: 1 The FLOPs provides hardware-agnostic estimate of computational complexity, which is particularly relevant for comparing models in terms of efficiency. As the FLOPs calculation methods differ across algorithms, we utilized calflops [34] to calculate FLOPs automatically. the adopt Implementation Details. We developed experimental framework with reference to code implementation of Dai et al. [15]. The parameters of each model were configured based on the principle of approximate model size and floating-point operations. Specifically, we transformer configuration from [15] as the base configuration, while the remaining baseline models share the same common parameters (e.g., Number Layers, Attention Heads and dimensions). For the parameters unique to the baseline model, we refer to the original code implementation for configuration. With regard to each MoE model, we activate half of the MLP experts to process each input. This configuration ensures sufficient expert participation while preventing overfitting, which helps to balance loads and enhance efficiency. Similarly, for DeepSeek-V2, which apply Multi-Head Latent Attention mechanism, we set its lora rank equal to half of embedding dimension. All experiments employ the same train setting, such as batch size, sequence length and training steps. In terms of UoEs configuration, we apply data selection mode to attention blocks and expert selection mode to MLP blocks. we set number of MLP experts equal to that in baselines and activate half for each input sample. In MultiHead Attention block, we set the number of experts equal to the number of heads and assign each expert 50% of the input sequence. In this configuration, the theoretical FLOPs required by our model is roughly 50% of the count associated with transformer baseline. Experimental results are depicted in Table I. Results. Table illustrates the comparative results. The performance metrics indicate that our model noticeably outperforms all state-of-the-art baselines, which highlights the advantages of overall expert collaboration and fine-grained collaborative routing. we achieved test perplexity of 24.09/ 24.52 in wikitext103/One Billion Word datasets, which is that superior to Transformer (24.23/24.70) and all competing MoE-based works (best: 26.96/26.41 from DeepSeek-V3). Prevalent MoE methods are derivative models that replace the MLP block in transformer with Mixture-of-Experts architecture. Among them, some works focus on introducing fine-grained routing (i.e., DeepSeek-MoE and XMoE). Other research efforts emphasize enhance routing precision and balance (i.e., Skywork-MoE). By comparison, our approach not only possesses these characteristics, but also enhances expert mixing methods in-depth expert collaboration, leading to significantly improved performance to promote TABLE PERFORMANCE COMPARISON BETWEEN UOE AND BASELINES ON WIKITEXT-103 AND ONE BILLION WORD BENCHMARKS. Model Transformer[5] XMoE[32] DeepSeek-MoE[27] Skywork-MoE[26] DeepSeek-V3[31] Ours Wikitext-103 One Billion Word PPL 24.23 30.60 28.30 29.10 26.96 24.09 TFLOPs 2.67 2.67 2.30 2.42 2.65 1.74 PPL 24.70 29.01 28.83 28.65 26.41 24.52 TFLOPs 6.27 4.69 5.46 5.54 5.26 4.53 than above methods. Furthermore, there is class of MoE methods that incorporates an optimization scheme particularly for attention block. E.g., the Multi-Head Latent Attention mechanism introduced in DeepSeek-V3 and our UoE attention block. Experimental results demonstrate that under similar FLOPs metrics, our approach achieves superior performance compared to DeepSeek-V3 (24.09/24.52 vs 26.96/26.41), which underscores the efficacy of our approach, especially with to our Union-of-Experts based attention mechanism. respect We also analyzed the FLOPs of experimental models. As shown in Table I, the FLOPs of our model in Wikitext103/One Billion Word (1.74/4.53) is better than all competing models, including DeepSeek-V3 (2.65/5.26), Skywork-MoE (2.42/5.54), DeepSeek-MoE (2.30/5.46), XMoE (2.67/4.69) and dense transformer (2.67/6.27). The model's FLOPs are dependent on its architectural design and parameter configuration. In our experiment, MLP blocks of each MoE methods activate only half of their parameters. With regard to attention blocks, unlike other works (e.g., DeepSeek-MoE) that utilize softmax attention, our UoE design partially activated attention mechanism, while DeepSeek-V3 employs low-rank adaptive one, both contribute to FLOPs reducing. On the other hand, in parallel implementation of selective activation, the imbalance in expert workloads will result in additional costs, which leads to higher actual FLOPs than the theoretical value (i.e., 50% FLOPs of dense transformer). Nonetheless, our UoE still maintains FLOPs superior than leading methods. B. Context Modeling on Long Range Arena Benchmark Benchmark Description. To evaluate UoEs long text modeling capability, we trained our models on the widely recognized Long Range Arena (LRA) benchmark [35]. This benchmark comprises several sub-tasks, which evaluate model's performance from various perspectives, including compositionality, hierarchical structure, and spatial reasoning. ListOps. The Long ListOps task tests the model's ability to handle hierarchically structured data within extended contexts, using sequences of up to 2K in length. Text. The Byte-Level Text Classification task evaluates the model's capacity to classify documents from byte-level data, simulating real-world tasks like spam detection. Retrieval. The Byte-level Document Retrieval task assesses models ability in encoding and retrieving documents based on similarity scores. Image. The Image Classification on Sequences of Pixels task involves classifying images represented as 1D sequences, requiring models to capture spatial dependencies. Pathfinder. The Pathfinder task tests long-range spatial dependencies in visual data, requiring models to determine whether two points are connected by path. following transformer variants, the same protocol as Comparison Baseline. Following the same protocol as the previous experiment, we compare our method with Transformer and state-of-the-art MoE methods, which contains DeepSeek-MoE, DeepSeek-V3, Skywork-MoE and XMoE, in previous experimental design. Considering that our model can be regarded as variant of the transformer architecture, we also including select several efficient Reformer [10], Linformer [7], Performer [36], Linear Attention [37], Nystr√∂mformer [38] and Flash-Attention [11], as our baseline models. Reformer introduces locality-sensitive hashing to approximate attention mechanisms, which helps to reduce time complexity from quadratic to linear. Linformer employs low-rank projections to approximate the full attention matrix. Performer utilizes kernel methods to approximate softmax attention. Linear Attention replaces the softmax-based transformation. attention mechanism with Nystr√∂mformer applies Nystr√∂m approximation the to attention mechanism to reduce computation complexity. Flash Attention leverages optimized memory access, block-wise computation, and efficient parallelization, achieving faster execution and reduced memory usage. linear in Evaluation Metrics. We adopt widely used Accuracy and FLOPs as evaluation criteria for the classification task. Accuracy measures the proportion of correctly classified instances the dataset, providing straightforward assessment of the model's performance. The FLOPs is employed to test the computational efficiency of the model by measuring the total number of floating-point operations needed. In this experiment, we provide the average ratio of tested model FLOPs to standard transformer FLOPs in each subtask of LRA benchmark. In general, the metrics provide balanced evaluation of the model's performance and efficiency. Implementation Details. We designed and trained our models based on the frameworks established by Tay et al. [35] For fair comparison, we follow the experimental setting for efficient transformers as used in Xiong et al. [38] and Zhu et al. [39]. For some works not covered, we provide the best results from Dao et al. [11]. In terms of MoE comparison model, we adopted similar parameter configuration to that used in the previous experiment. (e.g., set activate ratio to 0.5 and MLAs lora rank equal to half of embedding dimension). For consistency, the same training parameters are applied in all experiments. In terms of UoEs setup, considering that in LRA datasets, the input sequences are notably long, and the information within these long sequences is distributed sparsely. We apply data selection mode on both attention blocks and MLP blocks of our model to effectively extract the valuable information TABLE II THE EXPERIMENTAL RESULTS ON LONG RANGE ARENA (LRA) BENCHMARK Task ListOps Text Retrieval Image Pathfinder Average FLOPs Transformer [5] Reformer [10] Linformer [7] Performer [36] Linear Attention [37] Nystr√∂mformer [38] Flash Attention [11] XMoE [32] DeepSeek-MoE [27] SkyworkMoE [26] DeepSeek-V3 [31] UoE (ours) 37.13 36.44 37.38 36. 38.80 37.34 37.60 38.00 37.80 37. 38.26 38.91 Efficient Transformers 82.30 78.64 79. 82.20 80.70 81.29 81.40 Mix-of-Experts Methods 81. 81.35 82.25 81.77 82.82 42.44 43. 38.56 42.10 42.60 41.58 43.50 42. 43.55 45.24 45.15 46.09 65.35 64. 56.12 63.60 63.20 65.75 63.90 63. 65.12 65.18 65.50 65.61 74.16 69. 76.34 69.90 72.50 70.94 72.70 73.65 74. 74.11 74.52 75.17 60.28 58.52 57. 58.92 59.56 59.38 59.82 59.70 60. 60.93 61.04 61.72 1.00 0.19 0.23 0.23 0.18 0.32 0.17 0.99 0.99 0.99 0.74 0.37 The results of Performer, Linear Attention and Flash Attention are from Dao et al. [11]. embedded at various locations within sequences. Similar to the previous configuration, we set the number of MLP experts equal to that in baselines, while in Multi-Head Attention Block, it is equal to attention head count. For all experts in data selection paradigm, the activation ratio is set to 0.5. Experimental results are depicted in Table II. task the ListOps in Table II. In Results. Comparisons with models on LRA benchmark are that models shown hierarchically structured data, our UoE achieve significant accuracy of 38.91, outperforming all competing efficient transformer models (best: 38.80 from Linear Attention), as well as all MoE methods (best: 38.26 from DeepSeek-V3), showing its capacity of understanding recursive patterns and operating over long-context sequences. In the Retrieval task that evaluates the models encoding and storage of compressed representations, our model achieved an accuracy of 82.82, ranking among the top contenders of transformer variants (best: 82.20 from Performer) and MoEs (best: 82.25 from Skywork-MoE). Our model also highlights its proficiency in image processing. The performance of our model in the Image task was measured at 46.09, exceeding the performance of all competing models, including the previous highest score of 45.24 reported by Skywork-MoE. The remaining tasks further demonstrate the superiority of our model. Achieving an accuracy of 65.61 on the Text task, and 75.17 on the Pathfinder task, our model significantly outperformed all MoE methods, with the highest MoE accuracy being 65.50/74.52 from DeepSeek-V3. It is also competitive with advanced efficient transformers. In Text task, the best score is 65.75 achieved by Nystr√∂mformer, while in Pathfinder task, it is 76.34 attained by Linformer. Overall, our model achieves an average score of 61.72 with 37% FLOPs of the standard transformer model. In comparison to the latest state-of-the-art DeepSeek-V3 method, our model achieves an average score improvement of 0.68% while requiring only 50% of its computational costs. This performance is also on par with the advanced Flash Attention, which achieves an average score of 59.82 with 17% FLOPs. Note that as variant of MoE, UoEs complexity determined by the application context. When constructed with linear complexity attention mechanism such as Flash Attention, it can achieve significantly greater efficiency improvements. The results presented above demonstrate the superiority of our proposed method. By introducing the unique Union-ofExperts mechanism in the entire transformer, our model overcomes the inherent limitations of traditional MoE methods and transformer architecture, surpassing existing state-of-theart MoE methods. Moreover, even without an explicit efficient architecture design, our model's performance is competitive with the state-of-the-art efficient transformer model. In reality, our model exhibits natural advantage in modeling long sequences. On the one hand, each expert in our model only receives portion of patches or tokens from the sequence as input. The expert may be subjected to finite set of inputs that are positioned at relatively distant intervals along the sequence, which long-range dependencies. On the other hand, the expert is able to focus on specific input region, thereby facilitating the effective processing of local information. By applying this paradigm to attention block and MLP block, we have implemented the extraction and specialization of useful knowledge from the sequence, significantly enhancing the model's ability to handle long-range dependencies. In addition, the FLOPS of XMoE, DeepSeek-MoE and Skywork-MoE show no significant differences from standard transformer. This can be attributed to the quadratic complexity of Multi-Head Attention. For long its efficiency in handling indicate sequence inputs, the computational overhead of Multi-Head Attention constitutes the major portion. As these models primarily optimize MLP, their efficiency remains largely unaffected. C. Image Classification Dataset Description. We perform the image classification experiments on the CIFAR-10 and CIFAR-100 datasets. CIFAR-10 consists of 60,000 32x32 color images categorized into 10 classes, with 6,000 images per class, split into 50,000 training images and 10,000 test images. Each image in CIFAR-10 is labeled with single class, representing common objects such as airplanes, automobiles, etc. CIFAR-100 is an extension of CIFAR-10, contains 60,000 32x32 color images divided into 100 fine-grained classes, which are grouped into 20 coarser super-classes. Each class in CIFAR-100 contains 600 images, with 500 used for training and 100 for testing. This dataset is more challenging for its larger number of classes, hierarchical labeling with finegrained and coarse labels, and smaller amount of training data. Comparison Baseline. Analogous to language modeling experiments, we compare our method with Transformer and state-of-the-art MoE methods, which contains DeepSeek-MoE, DeepSeek-V2, Skywork-MoE and XMoE. Evaluation Metrics. We use Accuracy and FLOPs as evaluation criteria for the classification task. Implementation Details. We construct the experimental models based on Vision Transformer (ViT) [40]. We follow most of the setup and training configurations of ViT, with specific modifications such as integrating our UoE mechanism to transformer blocks, or replace them with baseline MoE models. We apply data selection mode to attention blocks and expert selection mode to MLP blocks with similar expert number and activation ratio settings. We designate this experimental group as UoE or UoE-DE. To further explore the influence of selection mechanism on model performance, we also provide control group in which data selection is applied to both attention block and MLP block, which we denoted as UoE-DD. Results. The experimental results are illustrated in Table III. Since the two tasks share the same model, they exhibit same FLOPs values. The results show that our model significantly surpasses all state-of-the-art baselines. By introducing Rotary Position Embedding (RoPE) to transformer model design, our UoE achieves significant accuracy improvement of 8.25% in CIFAR-10 and 12.87% than standard transformer while maintaining the same parameter count and only 64.77% of the FLOPs. In comparison with MoE methods, our model the well-performing DeepSeekV3/Skywork-MoE model by 1.19%/1.83% in CIFAR-10 and 1.93%/6.22% in CIFAR-100, which can be attribute to our indepth expert collaboration and fine-grained dynamic selection on both attention block and MLP block. in CIFAR-100 surpasses On the other hand, Skywork-MoE, DeepSeek-MoE and XMoE primarily concentrate on improving routing algorithm of MLP experts. However, Skywork-MoE applies the RoPE to TABLE III EXPERIMENTAL RESULTS ON VIT-BASED IMAGE CLASSIFICATION TASK Model Transformer [5] XMoE [32] DeepSeek-MoE [27] SkyworkMoE [26] DeepSeek-V3 [31] UoE-DD (ours) UoE (ours) Accuracy Cifar-10 Cifar-100 74.45 70.90 72.31 80.87 81.51 81.81 82.70 42.60 41.27 40.82 49.25 53.16 55.09 55. GFLOPs 340.21 264.99 245.08 281.83 210.38 220.99 220.34 information relative positional effectively capturing in sequences, while the remaining two models do not conduct significant refinement on the attention block. With RoPE and selective attention optimization, our model outperforms DeepSeek-MoE/XMoE by 10.39%/11.80% on Cifar-10 and 14.65%/14.20% on Cifar-100. In terms of computation efficiency, our model achieves 220.34 GFLOPs, matching the efficiency of the most efficient DeepSeek model (210.38 GFLOPs), while significantly surpassing other competitors (best: 245.08 GFLOPs from DeepSeek-MoE). This clearly demonstrates the efficiency-enhancing effect of selective mechanism on our model. We also provide the results of the control group UoE-DD. The control group attained accuracy of 81.81 and 55.09 for Cifar-10 and Cifar-100, respectively, which is slightly lower than that of the experimental group but remained significantly superior to all MoE comparison models. We posit that this may be attributed to the complementary effects of the two mechanisms at local and global levels: In data selection paradigm, attention experts take subset of the input sequences and provides lower-level, localized analysis. While in expert selection paradigm, MLP experts integrate all local results from attention experts and provide the final outcome from global perspective. In this process, the information within inputs was thoroughly integrated and comprehensively reviewed, leading to enhanced performance. D. Ablation experiments In this section, we conduct an ablation study with two parts: 1) an internal ablation study that investigates how variations in the models configuration affect its performance, and 2) an external ablation study that compares each UoEs component with advanced competing counterparts. Internal Ablation Study. We first implement ablation study on model configuration, which is depended on two additional parameters introduced in UoE: number of experts . The experiments are and activation ratio of data or experts ùëõùëõ performed on Listops dataset, following the experimental design used in section 4B. We conduct experiments with various parameter combinations to evaluate their impact on the two basic components: Selective Multi-Head Attention (SMHA) block, which symbolizes UoE in dense condition, and Union-of-MLP-Experts (UoME) block, which represents that in multi-head condition. Specifically, we config two experimental models: the model adopts the SMHA block and ùëüùëü Fig. 5. Ablation study on number of experts and activation ratio . standard two-layer MLP block, denoted as SMHA, and the model uses standard Multi-Head Attention block and UoME block, denoted as UoME. Fig. 5 illustrates the results of our experiment. = , we set showcase comprehensively accuracy, which"
        },
        {
            "title": "To evaluate the effects of activation ratio",
            "content": "2, 4,8 and regulate the parameter . As shown in Fig. 5, the variations in activation ratio have no regular pattern of impact on the characteristics of MoE based methods. In MoE frameworks, expert roles are highly differentiated, which means that only few experts are typically required to effectively process an individual instance taken from wide-ranging distribution. In the model exhibits our experiments, when suboptimal performance in some conditions. when , both types of SMHA and UoME models exhibited sufficient fitting capability. Therefore, additional increments in action ration may introduce redundancy and contribute little to enhancing performance. Actually, when increasing expert count or the number of patches processed by each expert (i.e., 0.25 = 0.5 , increase r), an improvement in training accuracy was observed, indicating slight overfitting in the models. By increasing dataset size, redundant experts can be transformed into useful experts, which may in turn enhancing model performance. Then we explore the effects of the number of experts . UoEs attention block integrates the outputs of activated individual attention heads into comprehensive final outcome. Empirical studies have demonstrated that increasing the number of attention heads facilitates the simultaneous learning of input sequences within distinct subspaces. On the other hand, it may limit the size of heads, potentially undermining the ability of individual heads to discern subtle details. As , the data selection illustrated in Table IV, when version gets an average accuracy of 38.44/38.30. reflecting the balance between two interacting factors. However, when increasing head counts to 8, the average accuracy decreases to 37.89, indicating that the limited experts scale has led to considerable degradation in performance. By contrast, the expert selection version achieved more stable average score = 2, 4 ùëõùëõ TABLE IV THE AVERAGE PERFORMANCE OF UOES COMPONENTS UNDER DIFFERENT ACTIVATION RATIO FOR N=2, 4 AND 8 TABLE COMPARATIVE EXPERIMENTS OF UOES SMHA BLOCK AND UOME BLOCK WITH CORRESPONDING COMPONENTS IN COMPARISON MODELS Expert Counts n=2 n=4 n=8 Avg. SMHA UoME DS. 38.44 38.30 37.89 38. ES. 37.65 37.80 37.56 37.67 DS. 38.37 38.23 38.13 38.24 ES. 38.21 37.88 37.94 38. of 37.65/37.80/37.56, indicating that it is less affected by the number and size of attention experts, thus demonstrating better robustness. In UoE style MLP block, activated experts function as an integral part of dense MLP model, which ensuring the models robustness against the reduction in expert size. As presented in Table IV, when is varied to 2, 4 and 8, the performance of data/expert selection UoME model under different activation ratio remains relatively stable, with average values of 38.37/38.21, 38.23/37.88, and 38.13/37.94. 1r < , increasing expert counts leads to fine-grained When and highly specialized expert operation, which is potentially improvement. However, as beneficial illustrated in section 3D, it may induce an increasement of computation and memory cost because the expert is too small to load in cuda core, thereby wasting unused computation space. In general, we recommend setting of MLP expert counts that is larger while keeping the expert dimension larger than the size of cuda core. to performance conducted them with performance, we External Ablation Study. Our previous experiments indicate that UoE surpasses state-of-the-art competing models in variety of tasks. To further verify the positive contribution of UoEs attention and MLP components to our models statecomparative of-the-art the corresponding experiments on both of components in advanced competing models. The experiments are performed on Listops and Wikitext-103 datasets, following the experimental design in section 4A and 4B. For model setup, we adopt the SMHA and UoME in data selection paradigm with the configuration of , and utilize the MHA block and the two-layer MLP block in standard transformer as the basic configuration. For attention block, we select the MLA block, as implemented in DeepSeek-V2/V3, to serve as our comparison model. For MLP block, we choose the MoE components of Skywork-MoE, DeepSeek-MoE and XMoE as the comparison model. The computational complexity of the comparison model is on par with that of the corresponding UoE components. The remaining components of experimental models are configured as the matching components of the standard transformer. The experimental results are shown in Table V. 0.5 4, = = r Our results demonstrate that UoEs attention and MLP blocks significantly outperform the corresponding components of existing state-of-the-art models. With minimal computation expenses, SMHA achieved an accuracy of 38.41 in Listops task, which is higher than MLA's 38.16. In Wikitext-103 task, Model MLA [31] SMHA (ours) XMoE [32] DeepSeek-MoE [27] Skywork-MoE [26] UoME (ours) Listops WikitextAcc. 38.16 38.41 38.00 37.80 37.85 38.56 GFLOPs 5.48 2.99 7.31 7.21 7.25 7.10 PPL 24.29 22.68 30.60 28.30 29.10 24. TFLOPs 3.01 2.55 2.67 2.31 2.42 1.95 it takes the best-performing perplexity of 22.68. We posit that significant reason for MLA's effectiveness lies in the indirect increase of model depth facilitated by the introduction of the LoRA mechanism. In UoE, by incorporating selection and routing mechanism into attention heads, we are able to take advantage of task related experts while mitigating the impact thereby achieving competitive of unhelpful experts, performance in an alternative manner. in MLP components. Given The remaining results further demonstrate the superiority of the UoE architecture. Compared to Skywork-MoE, DeepSeekMoE and XMoE, our UoME model only exhibits significant difference reduced computational cost, the accuracy/perplexity of UoME in Listops/Wikitext-103 is markedly superior to that of Skywork-MoE (37.85/29.10), DeepSeekMoE (37.80/28.30), and XMoE (38.00/32.60). Within the MLP block of UoE or UoME, the activated experts function as cohesive whole. The enhanced interaction in experts contribute to additional performance benefits, which emerges as critical factor in surpassing existing MoE methods. tasks, at 38.56/24.26, the Finally, we vary sequence length and measure runtime and memory usage of UoE against various comparison models on one A100 GPU with 80 GB HBM. To facilitate consistent comparison, we maintain the configuration of the context modeling experiment in section 4B, with the only modification being the substitution of the input tensor with randomly generated tensors of varying sequence lengths, similar in format to the original data. We utilize the pytorch profiler to measure the total time cost on both forward and backward propagation, and the nvtop visualization tool to monitor the peak GPU memory usage during model execution. The maximum sequence length was configured to 4096, as further increases to 8192 would cause memory overflow in all models except for UoE. Runtime. Fig. 6 (left) illustrate the runtime in milliseconds of the forward + backward pass of UoE compared to DeepSeek-V3, Skywork-MoE, DeepSeek-MoE and XMoE. The Runtime exhibits quadratic growth with respect to the sequence from more efficient implementation, Our UoE model runs significantly faster than all comparison models, except in cases where the input sequence length is less than 1024, the speed is marginally slower than XMoE. We posit that XMoE naturally inherits the optimization strategies from Megatron-LM, thereby reducing length. Benefiting Fig. 6. Runtime and memory usage of the experimental model. the additional overhead caused by non-matrix operations. These optimizations are also applicable to other models including ours. As sequence increases, matrix operations gradually become the dominant factor in total time cost. In this scenario, our model outperforms all others by virtue of its lower computational complexity. Especially, when increasing the length to 4096, we achieve an average 2.26 speedup related to comparison models. length We also note that the models' actual runtime does not strictly correlate with the theoretical FLOPs. Specifically, DeepSeek-V3 and Skywork-MoE performs rather mediocrely in this experiment. Note that our focus is limited to the algorithm itself and does not extend to hardware optimizations. In terms of algorithm implementation, The MoE mechanism in DeepSeek Series, Skywork-MoE and XMoE is serial, leading to lower computational efficiency than our parallel-optimized UoE model. On the other hand, the specific structures of these comparison models may potentially impact their efficiency. The Multi-Head Latent Attention in DeepSeek-V3 employs the compression mechanism from LoRA, which potentially increases the number of matrix operations. moreover, when LoRA rank is relatively small, efficient parallel computation may be compromised. The MoE component of Skywork-MoE conducts serial indexing operations on expert weights, resulting in additional time overhead. By comparison, our model does not exhibit these issues, thereby achieving superior runtime performance. Memory Footprint. Fig. 6 (right) shows the memory footprint of UoE compared to DeepSeek-V3, Skywork-MoE, DeepSeek-MoE and XMoE. In general, the memory footprint grows quadratically with sequence length. In comparison with competing Models, our UoE achieve an average 2.68 memory efficiency improvement. We declare that UoE is inherently superior in terms of memory efficiency. When attention block and MLP block both activate 50% parameters, the theoretical memory efficiency can reach up to 4 greater than that of fully activated condition, showing significant advantage over comparison models. In the experiment, the comparison model employed MoE components with configuration similar to UoEs MLP block. The MLP experts in DeepSeek Series models consist of three linear layers, which entails higher computational complexity. Moreover, XMoE has relatively complex overall structure. As result, DeepSeek-MoE and XMoE exhibit the highest memory usage in the figure. The memory usage of SkyworkMoEs attention block is comparable to that of standard MultiHead Attention. DeepSeek-V3 adopts efficient Multi-Head Latent Attention mechanism, which helps to mitigate the memory efficiency loss from the complex MLP expert. As result, it demonstrates memory footprint analogous to Skywork-MoE. It is worth mentioning that in this experiment, attention block has greater impact on memory efficiency than MLP block. With the Selective Multi-Head Attention mechanism, our UoE achieves more remarkable efficiency improvement. transformers In summary, we conducted comprehensive and meticulous experiments across multiple representative datasets from both the image and natural language domains. The experimental results have demonstrated that our UoE model surpass Full Attention models and group of state-of-art MoEs and efficient tasks while attaining in several computational complexity that matches or surpasses state-ofthe-art models. In addition, the recent experimental results in the released works NSA [19] and MoBA [20] partially and indirectly validate the effectiveness of UoEs SMHA mechanism. Overall, this experiment has provided valuable insights into the effectiveness and efficiency of our UoE model. Further research could focus on the application of UoE in the training and inference of large language models. V. CONCLUSION We propose the Union-of-Experts, novel and effective MoE type framework. We leverage lossless decomposition to convert whole transformer model into group of experts. In fundamental architecture of dense models, preventing the performance this manner, our method retains the dual data-expert selection mechanism degradation from architectural adjustments. Based on the architecture of decomposed transformer, we devise finegrained that dynamically select useful input parts and allocate them to the activated attention or MLP experts. Our experiments on the challenging benchmarks demonstrate that our UoE surpasses state-of-art MoE and transformer variants with lower level of computational cost, indicating its superior efficiency and effectiveness in various tasks across image and language domains. Considering the extensive applications of MoE and transformers within Large Language Models, our model has promising development and application prospects. For research purposes, we release all the source code of UoE to the public, we aspire for this work to provide valuable insights in performance and efficiency enhancement of large-scale language models, and hope that it will facilitate further research and development. REFERENCES [1] N. Shazeer et al., \"Outrageously large neural networks: The sparselygated mixture-of-experts layer,\" arXiv preprint arXiv:1701.06538, 2017. [2] Z.-H. Zhou, Ensemble methods: foundations and algorithms. CRC press, 2012. [3] W. Li, Y. Peng, M. Zhang, L. Ding, H. Hu, and L. Shen, \"Deep model fusion: survey,\" arXiv preprint arXiv:2309.15698, 2023. [4] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro, \"Megatron-lm: Training multi-billion parameter language models using model parallelism,\" arXiv preprint arXiv:1909.08053, 2019. [5] A. Vaswani, \"Attention is all you need,\" Advances in Neural Information Processing Systems, 2017. [6] R. Child, S. Gray, A. Radford, and I. Sutskever, \"Generating long sequences with sparse transformers,\" arXiv preprint arXiv:1904.10509, 2019. [7] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, \"Linformer: Selfattention with linear complexity,\" arXiv preprint arXiv:2006.04768, 2020. [8] Z. Qin et al., \"Scaling transnormer to 175 billion parameters,\" arXiv preprint arXiv:2307.14995, 2023. [9] Z. Qin, W. Sun, D. Li, X. Shen, W. Sun, and Y. Zhong, \"Lightning attention-2: free lunch for handling unlimited sequence lengths in large language models,\" arXiv preprint arXiv:2401.04658, 2024. [10] N. Kitaev, ≈Å. Kaiser, and A. Levskaya, \"Reformer: The efficient transformer,\" arXiv preprint arXiv:2001.04451, 2020. [11] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R√©, \"Flashattention: Fast and memory-efficient exact attention with io-awareness,\" Advances in neural information processing systems, vol. 35, pp. 16344-16359, 2022. [12] T. Dao, \"Flashattention-2: Faster attention with better parallelism and work partitioning,\" arXiv preprint arXiv:2307.08691, 2023. [13] A. Liu et al., \"Deepseek-v2: strong, economical, and efficient mixture-of-experts language model,\" arXiv preprint arXiv:2405.04434, 2024. [14] Y. Zhang et al., \"Tensor Product Attention Is All You Need,\" arXiv preprint arXiv:2501.06425, 2025. [15] Z. Dai, \"Transformer-xl: Attentive language models beyond fixedlength context,\" arXiv preprint arXiv:1901.02860, 2019. [16] Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan, \"Sparse sinkhorn attention,\" in International Conference on Machine Learning, 2020: PMLR, pp. 9438-9447. [17] W. Luo, Y. Liu, B. Li, W. Hu, Y. Miao, and Y. Li, \"Long-Short Term in Compressed Domain for Few-Shot Video Cross-Transformer Classification,\" in IJCAI, 2022, pp. 1247-1253. [18] Y. Gao et al., \"Seerattention: Learning intrinsic sparse attention in your llms,\" arXiv preprint arXiv:2410.13276, 2024. [19] J. Yuan et al., \"Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention,\" arXiv preprint arXiv:2502.11089, 2025. [20] E. Lu et al., \"MoBA: Mixture of Block Attention for Long-Context LLMs,\" arXiv preprint arXiv:2502.13189, 2025. [21] W. Fedus, J. Dean, and B. Zoph, \"A review of sparse expert models in deep learning,\" arXiv preprint arXiv:2209.01667, 2022. [22] D. Lepikhin et al., \"Gshard: Scaling giant models with conditional computation and automatic sharding,\" arXiv preprint arXiv:2006.16668, 2020. [23] W. Fedus, B. Zoph, and N. Shazeer, \"Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity,\" Journal of Machine Learning Research, vol. 23, no. 120, pp. 1-39, 2022. [24] S. Gururangan, M. Lewis, A. Holtzman, N. A. Smith, and L. Zettlemoyer, \"Demix layers: Disentangling domains for modular language modeling,\" arXiv preprint arXiv:2108.05036, 2021. [25] J. Wang, J. Wang, B. Athiwaratkun, C. Zhang, and J. Zou, \"Mixture-ofAgents Enhances Large Language Model Capabilities,\" arXiv preprint arXiv:2406.04692, 2024. [26] T. Wei et al., \"Skywork-moe: deep dive into training techniques for mixture-of-experts language models,\" arXiv preprint arXiv:2406.06563, 2024. [27] D. Dai et al., \"Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models,\" arXiv preprint arXiv:2401.06066, 2024. [28] S. Moreno-Alvarez, J. M. Haut, M. E. Paoletti, and J. A. Rico-Gallego, for deep neural networks,\" \"Heterogeneous model parallelism Neurocomputing, vol. 441, pp. 1-12, 2021. [29] Y. Huang et al., \"Gpipe: Efficient training of giant neural networks using pipeline parallelism,\" Advances in neural information processing systems, vol. 32, 2019. [30] L. Wang, H. Gao, C. Zhao, X. Sun, and D. Dai, \"Auxiliary-loss-free load preprint for mixture-of-experts,\" strategy arXiv balancing arXiv:2408.15664, 2024. [31] A. Liu et al., \"Deepseek-v3 technical report,\" arXiv preprint arXiv:2412.19437, 2024. [32] Y. Yang, S. Qi, W. Gu, C. Wang, C. Gao, and Z. Xu, \"XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection,\" in Findings of the Association for Computational Linguistics ACL 2024, 2024, pp. 11664-11674. [33] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones, \"Characterlevel language modeling with deeper self-attention,\" in Proceedings of the AAAI conference on artificial intelligence, 2019, vol. 33, no. 01, pp. 3159-3166. [34] Y. Ju. \"calflops: FLOPs and Params calculate tool for neural networks.\" https://github.com/MrYxJ/calculate-flops.pytorch (accessed Jul. 24, 2024). [35] Y. Tay et al., \"Long range arena: benchmark for efficient transformers,\" arXiv preprint arXiv:2011.04006, 2020. [36] K. Choromanski et al., \"Rethinking attention with performers,\" arXiv preprint arXiv:2009.14794, 2020. [37] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, \"Transformers are rnns: Fast autoregressive transformers with linear attention,\" in International conference on machine learning, 2020: PMLR, pp. 51565165. [38] Y. Xiong et al., \"Nystr√∂mformer: nystr√∂m-based algorithm for approximating self-attention,\" in Proceedings of the AAAI Conference on Artificial Intelligence, 2021, vol. 35, no. 16, pp. 14138-14148. [39] C. Zhu et al., \"Long-short transformer: Efficient transformers for language and vision,\" Advances in neural information processing systems, vol. 34, pp. 17723-17736, 2021. [40] A. Dosovitskiy, \"An image is worth 16x16 words: Transformers for image recognition at scale,\" arXiv preprint arXiv:2010.11929, 2020. Yujiao Yang received the B.E. degree in vehicle engineering from Nanjing Tech University, Nanjing, China. He is currently pursuing the M.E. degree with the Dalian University of Technology, interests Dalian, China. His research include foundation models, large-scale models for autonomous driving, and generative models. Jing Lian (Member, IEEE) received the Ph.D. degree in communication and information system from Jilin University, Jilin, China, in 2008. She is currently an Associate Professor and the Deputy Director of the Automotive Electronic Institute, Dalian University of Technology and Judicial expert in the vehicle performance. She is the leader of more than 20 research projects. She is the author of over 70 publications. Her research interests include environmental perception, automotive electronics, trajectory prediction, and control of intelligent vehicle. in 2008. He Linhui Li (Member, IEEE) received the Ph.D. degree in vehicle operation engineering from Jilin University, Jilin, China, is an associate at Dalian University of professor Technology. He was visiting scholar at The Ohio State University from 2017 to 2018, and is the author of over 40 publications. His main research interests include intelligent vehicle trajectory prediction, vision based environmental perception of intelligent vehicle, and navigation control."
        }
    ],
    "affiliations": [
        "School of Mechanical Engineering, Dalian University of Technology, Dalian 116024, China"
    ]
}