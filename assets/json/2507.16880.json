{
    "paper_title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed",
    "authors": [
        "Antoni Kowalczuk",
        "Dominik Hintersdorf",
        "Lukas Struppek",
        "Kristian Kersting",
        "Adam Dziedzic",
        "Franziska Boenisch"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As a first step in this direction, we introduce a novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and a foundation for building more trustworthy and compliant generative AI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 0 8 8 6 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Finding Dori",
            "content": ": Memorization in Text-to-Image"
        },
        {
            "title": "Diffusion Models Is Less Local Than Assumed",
            "content": "Antoni Kowalczuk1 Dominik Hintersdorf 2,3 Lukas Struppek 2,3 Kristian Kersting2,3,4,5 Adam Dziedzic1 Franziska Boenisch1 1CISPA Helmholtz Center for Information Security 2German Research Center for Artificial Intelligence (DFKI) 3Computer Science Department, Technical University of Darmstadt 4Hessian Center for AI (Hessian.AI) 5Centre for Cognitive Science, Technical University of Darmstadt"
        },
        {
            "title": "Abstract",
            "content": "Text-to-image diffusion models (DMs) have achieved remarkable success in image generation. However, concerns about data privacy and intellectual property remain due to their potential to inadvertently memorize and replicate training data. Recent mitigation efforts have focused on identifying and pruning weights responsible for triggering replication, based on the assumption that memorization can be localized. Our research assesses the robustness of these pruning-based approaches. We demonstrate that even after pruning, minor adjustments to text embeddings of input prompts are sufficient to re-trigger data replication, highlighting the fragility of these defenses. Furthermore, we challenge the fundamental assumption of memorization locality, by showing that replication can be triggered from diverse locations within the text embedding space, and follows different paths in the model. Our findings indicate that existing mitigation strategies are insufficient and underscore the need for methods that truly remove memorized content, rather than attempting to suppress its retrieval. As first step in this direction, we introduce novel adversarial fine-tuning method that iteratively searches for replication triggers and updates the model to increase robustness. Through our research, we provide fresh insights into the nature of memorization in text-to-image DMs and foundation for building more trustworthy and compliant generative AI."
        },
        {
            "title": "Introduction",
            "content": "Generating high-quality images with diffusion models (DMs) enjoys great popularity. However, undesired memorization of training data in text-to-image DMs [2, 37] poses significant risks to privacy and intellectual property, as it can favor the unintended replication of sensitive or copyrighted content during inference. In response, various detection and mitigation strategies have been proposed [32, 37, 43, 45]. Most existing mitigation techniques either aim to identify and filter out highly memorized samples during training [32, 37] or modify inputs at inference time [32, 37, 45] to reduce memorization-induced data replication. While the training-based methods require computationally expensive retraining, the inference-time methods are limited to models behind APIs, as users of open-source models can easily disable these mechanisms by altering the source code. To overcome both limitations, recent approaches [3, 14] observe that the text prompts of memorized images elicit distinct activation patterns in the DMs. Based on these activations, the methods prune equal contribution, corresponding authors: antoni.kowalczuk@cispa.de, {dominik.hintersdorf, lukas.struppek}@dfki.de Preprint. Under review. Figure 1: Left: 1 Without mitigation, the DM closely replicates the training sample. 2 Mitigation strategies, such as pruning memorization neurons with NeMo [14] or Wanda [3], prevent replication for the memorized prompt, thereby suggesting successful removal. Yet, 3 adversarial embeddings still trigger replication. Right: While pruning alters the generation trajectory for the original memorized prompt (blue), adversarial embeddings steer denoising along alternative paths (red) that still lead to the memorized content, unaffected by the pruning-based mitigation. small set of weights, effectively reducing the risk of verbatim data replication, while preserving overall image quality. However, since these methods work with single prompt per memorized image, it remains an open question whether they prevent the replication of memorized images through other inputs. We search for Diffusion Memorization (Dori ) beyond the prompt space by crafting adversarial embeddingstext embeddings different from the memorized promptsthat trigger memorized image generations. Adversarial embeddings allow us to recover supposedly removed memorized data after pruning (see Fig. 1, left), revealing that pruning merely conceals memorization. Initial effectiveness of pruning-based methods for mitigating memorization is attributed to locality phenomenon, property of the model to store memorized data in small (local) set of memorization weights. Intuitively, if this property holds, then pruning memorization weights would prevent generation of memorized data. In this work, we challenge the locality phenomenon and question whether locality is real or misinterpretation of the early success of pruning-based methods. We investigate the input space and the models weights and find little support for it. Memorization seems to be spread out, as there exist multiple adversarial embeddings that cause replication of the same data point, with the DM following different paths during generation, see Fig. 1 (right). Similarly, the activation patterns and memorization weights identified for the same memorized image vary across different inputs that trigger its replication, further undermining the notion of locality. Accordingly, robust memorization removal method should avoid the pitfall of assuming locality. To this end, we build on adversarial embeddings and develop novel adversarial fine-tuning approach to completely erase memorized samples from text-to-image DMs. Our approach is inspired by adversarial training [10, 26, 40], which iteratively generates adversarial examples to train robust models. While prior methods have focused on parameter pruning or input adjustments, our approach directly modifies the models parameters to eliminate memorization. In contrast to pruning-based mitigation techniques, our method achieves reliable removal and remains robust against adversarial embeddings designed to circumvent mitigation. In summary, we make the following contributions: 1. We reveal that existing weight-pruning methods merely conceal memorization in text-to-image DMs rather than truly erase memorized content from model. 2. We challenge the assumption that memorization is local, demonstrating that it fails to hold across both the input space and the model parameters. 3. As more robust defense, we propose novel adversarial fine-tuning scheme that permanently mitigates memorization in already trained DMs."
        },
        {
            "title": "2 Background and Related Work",
            "content": "In this section, we explore the core principles of text-to-image generation using DMs and examine research focused on the critical issue of unintended memorization within this context. 2 2.1 Text-to-Image Generation with Diffusion Models Diffusion models [15, 38] (DMs) are class of generative models trained by gradually corrupting training images by adding Gaussian noise and training model ϵθ to predict the noise that has been added. Once trained, DMs generate new images by starting from pure noise xT (0, I) and progressively denoising it. At each time step = T, . . . , 1, the model ϵθ predicts the noise ϵθ(xt, t, y) needed for the denoising step. In the domain of text-to-image generation, the denoising process is guided by text prompt p, which is transformed into text embedding by text encoder. During training, time step U(1, ) and noise vector ϵ (0, I) are randomly sampled to 1 αtϵ based on the training image x0. The amount of noise create noisy image xt = added is controlled by noise scheduler αt, for which there are multiple choices [19, 20, 27, 38]. The training objective for the noise predictor ϵθ is then to predict the noise ϵ that has been added: αtx0 + LDM (x0, ϵ, y, t, θ) = ϵ ϵθ (xt, t, y) 2 2. (1) Training and generating samples with DMs can be computationally expensive. The latent DM framework [33] reduces this burden by operating in lower-dimensional latent space instead of the pixel space. This latent space is learned by separately trained variational autoencoder [22, 41] that encodes images into compact representations and decodes generated latents back to the image space. 2.2 Memorization in Diffusion Models Definition. In the context of generative models, memorization [8, 9] can manifest as the model reproducing portions of its training data, such as closely replicating particular sample. Specifically, verbatim memorization (VM) describes cases when training image is reliably generated by the model with almost pixel-perfect match. Template memorization (TM) is more relaxed notion, in which we require that only parts of the image are closely replicated, such as the background of an image or specific object [43]. Memorization in DMs. Recent work has demonstrated that DMsespecially text-to-image models [33, 35]are also prone to such unintended memorization [2, 6, 7, 12, 17, 18, 25, 37, 46], raising concerns around privacy and intellectual property. Since then, multiple methods have been developed to detect data replication [23, 32, 43, 45]. While many of these techniques rely on the availability of training prompts to identify memorized content, another line of research detects memorization even in the absence of training prompts, focusing instead on identifying specific memorized images [16, 25]. Mitigation. Memorization in DMs can either be prevented during training or by intervening in the generation process at inference time. Existing training-time mitigation techniques either adjust the training data by removing duplicates [2, 37] or reject training samples for which the model indicates signs of memorization [5, 32, 45]. However, since re-training large DMs is expensive, inference-time mitigation strategies are crucial for already trained models. These mitigation strategies adjust the input tokens [37], update the text embeddings [45], change the cross-attention scores [32], or guide the noise prediction away from memorized content [4]. However, these methods offer no permanent mitigation, increase the inference time, and can easily be turned off for locally deployed models. Local Pruning-Based Mitigation. More permanent solutions have focused on identifying and removing the weights responsible for triggering data replication. Hintersdorf et al. [14] developed NeMo, localization algorithm to detect memorization neurons within the cross-attention value layers of DMs, of which all weights are pruned. More specifically, NeMo first conducts an out-ofdistribution detection to identify neurons with high absolute activations under memorized prompts and reduces the set of identified neurons by checking their influence on data replication individually. Similarly, Chavhan et al. [3] applied Wanda [39]a pruning technique originally developed for large language modelsto locate and prune individual weights in the output fully-connected layers of cross-attention modules responsible for memorization. Wanda identifies weights by their weight importance, computed as the product between the weights and the activation norm. The method then prunes the top k% of weights with the highest importance scores compared to scores computed on null string. While both methods successfully avoid data replication triggered by memorized prompts, it remains an open question whether the memorized content is successfully erased from the model."
        },
        {
            "title": "3 Breaking Pruning-Based Mitigation Methods",
            "content": "This section introduces our method for finding triggers of memorized content in text embeddings, which we use to critically assess the effectiveness of the two pruning-based mitigation methods NeMo and Wanda. Our analysis demonstrates that while both methods successfully eliminate data replication when faced with memorized prompts, they are highly vulnerable to adversarial embeddings. 3.1 Finding Dori With Adversarial Text Embeddings Let xmem be memorized training image and ymem the text embedding of the prompt associated with xmem . After applying weight-pruning using NeMo or Wanda, the DM conditioned on ymem no longer replicates xmem , which suggests the memorized image was successfully removed from the model. We examine whether pruning-based methods truly mitigate memorization or merely conceal it. To this end, we optimize an adversarial text embedding yadv to trigger the generation of xmem an approach inspired by adversarial evaluation techniques in the domain of concept unlearning [47]. Specifically, we update the adversarial embedding y(i) adv, starting from the original text embedding y(0) adv :=ymem , using learning rate η and the standard diffusion loss defined in Eq. (1): adv = y(i) y(i+1) adv ηy(i) adv LDM (xmem , ϵ, y(i) adv , t, θNeMo/Wanda ), (2) where θNeMo/Wanda are parameters of the DM after applying NeMo or Wanda pruning to mitigate replication of xmem . Our goal is to find an adversarial embedding yadv that consistently triggers xmem , regardless of the initial noise. For this reason, we do not fix the timestep and the noise ϵ, but re-sample them at each optimization step from U(1, ) and (0, I), respectively. detailed formulation of the optimization procedure is provided in Alg. 1 in Appx. D. 3.2 Experimental Setup We begin by defining the experimental setup used in this and the subsequent sections. Models and Datasets: We focus our investigation on Stable Diffusion v1.4 [33] and set of 500 memorized prompts [43, 45] from the LAION-5B [36] training dataset, in line with previous research [3, 14, 32, 45] on memorization in text-to-image DMs. More recent DMs are trained on more carefully curated and deduplicated datasets, which reduces the amount of memorization, as discussed in previous work [37, 44]. While results for TM prompts are included in Appx. E, the main paper focuses on VM prompts, addressing this arguably more critical form of memorization. Metrics: Following prior work [14, 45], we employ SSCD [30], feature extractor commonly employed to detect and quantify copying behavior in DMs. To measure similarity between two images, we compute the cosine similarity between their SSCD feature embeddings. Higher values indicate higher degree of content replication. All metrics are computed as the median of the maximum scores across ten generated images per memorized prompt or adversarial embedding. We vary the seeds for image generation and adversarial embedding optimization to avoid overfitting. To evaluate replication, we define SSCDOrig as the cosine similarity between generated images and their associated training image. Values above 0.7 indicate that the memorized image is successfully generated [45]. Additionally, we use the similarity between images generated before and after mitigation techniques are applied, denoted by SSCDGen, to assess the effects of mitigation. We expect lower SSCDGen scores when mitigation is successful. Typically, memorized images are consistently replicated, regardless of the choice of the initial noise ϵ. Conversely, for any other input, images generated by DM are diverse under varying initial noise. We quantify diversity as the average pairwise cosine similarity DSSCD between SSCD embeddings of images generated from the same input but different initial noise. Images generated after mitigation should exhibit greater diversity, indicated by lower DSSCD values. To assess image quality, we measure prompt alignment using CLIP [31] similarity ACLIP, comparing each generated image with its corresponding textual prompt. Higher ACLIP scores indicate stronger semantic alignment with the input prompt. We also compute the Fréchet Inception Distance (FID) [13] and Kernel Inception Distance (KID, reported in the Appendix) [1]. Both are evaluated on 30k prompts of the COCO dataset [24]a standard benchmark for image generation [29]. Lower scores indicate improved image quality. 4 Table 1: Pruning-based mitigation of memorization is vulnerable to adversarial embeddings. Without any mitigation technique applied (1st row), the generated images clearly indicate data replication. Searching for adversarial embeddings on non-memorized prompts (2nd row) does not lead to clear replication. After localizing and pruning weights with NeMo (3rd row) or Wanda (4th row), data replication appears effectively prevented. However, identifying adversarial embeddings with Doriindicated by reveals that embeddings capable of triggering data replication may persist, even after pruning. Finally, our adversarial fine-tuning (5th row) successfully removes memorized content and prevents data replication. N/A denotes not applicable. Setting No Mitigation Non-Memorized Prompts Non-Memorized Prompts + NeMo [14] NeMo + Wanda [3] Wanda + Adv. Fine-Tuning (Ours) Adv. Fine-Tuning (Ours) + Memorization Type Verbatim None None Verbatim Verbatim Verbatim Verbatim Verbatim Verbatim SSCDOrig 0.90 0. 0.17 0.05 0.48 0.06 SSCDGen N/A DSSCD 1.00 0.00 ACLIP 0.33 0.01 N/A N/A 0.35 0.06 0.67 0. 0.35 0.02 0.32 0.02 0.33 0.18 0.91 0.03 0.40 0.21 0.97 0.02 0.46 0.13 1.00 0.00 0.34 0.02 0.33 0.02 0.20 0.08 0.76 0. 0.21 0.09 0.82 0.05 0.37 0.07 0.96 0.02 0.34 0.02 0.32 0.01 0.15 0.07 0.36 0.14 0.15 0.07 0.54 0.10 0.35 0.08 0.54 0. 0.33 0.01 0.30 0.02 FID 14.44 14.44 15.16 16. 13.61 Mitigation Methods: We base our research on pruning-based mitigation methods, specifically NeMo [14] and Wanda [3], which are currently the only existing mitigation methods of this category. For both methods, we use the default hyperparameters specified in the respective papers. For NeMo, we set the threshold used to identify memorization neurons to τmem = 0.428 (mean neuron activations plus one standard deviation). For Wanda, we use weight sparsity of 1%. Following the original evaluation settings, we identify and evaluate mitigation sample-wise for NeMo, whereas Wanda identifies the set of weights to be pruned for all memorized images at once using the first 10 timesteps. Adversarial Embedding Optimization: We initialize the adversarial embedding optimization with the text embeddings of the prompts of the memorized images. We then optimize each embedding for 50 steps with learning rate of 0.1 using Adam [21] and batch size of 8. 3.3 Pruning-Based Mitigation Conceals but Does Not Erase Memorization We begin by showing that NeMo and Wanda prevent data replication only in the text spacewhere users input promptsbut do not fully remove the memorized content from the DM. As shown in the first row of Tab. 1, verbatim memorized prompts trigger replication in the unmodified DM. In contrast, image generations for non-memorized training prompts (2nd row) show no signs of memorization under the SSCD score. Even when applying our adversarial embedding optimization, indicated by in the table, the resulting metrics suggest no close data replication. This finding is particularly important for the validity of our investigations, as it confirms that our adversarial embedding optimization method specifically targets memorized content and does not falsely indicate memorization for non-memorized data in the current setting. We explore adversarial embeddings in the context of non-memorized content more closely in Appx. D.1. Applying NeMo (third row) or Wanda (fourth row), which identify and prune weights associated with memorization in the cross-attention value and output fully-connected layers, respectively, substantially reduces data replication as reflected by low SSCD scores. At first glance, both methods appear effective at mitigating data replication, as also visualized in Fig. 1 ( 2 ). However, blocking replication from the original prompts does not imply that the memorized content has been removed from the model. To probe this, we employ adversarial embedding optimization (Sec. 3.1) to explore the neighborhood around memorized prompt embeddings. Despite pruning, we find that adversarial embeddings can still trigger replication, as shown in rows marked with and Fig. 1 ( 3 ), particularly for VM prompts. These results suggest that pruning-based methods like NeMo and Wanda primarily conceal memorization rather than eliminate it, preventing replication via the text space but leaving the memorized content internally intact. We also conduct sensitivity analysis (Appx. E.2) on the steps required to yield adversarial embeddings, finding that in most cases, significantly fewer than 50 steps are already sufficient to identify embeddings that circumvent the mitigation methods. For TM results, also reported in Appx. E.2, we observe increased 5 replication, yet SSCD-based metrics fail to correctly quantify this type of replication due to their non-semantic variations in generated images. Based on the presented results, one might argue that the number of pruned weights simply needs to be increased to fully mitigate memorization. However, we show that this is not the case. For Wanda, we find that increasing the strength of pruning, i.e., removing more weights, successfully defends against adversarial embeddings, but at cost of significant damage to generation quality, as shown in Appx. E.7. Specifically, to completely remove single image, one needs to prune about 10% of the weights in the output fully-connected layers with Wanda. At this scale, the DM loses its capability to reliably generate concepts related to the memorized image, even from non-memorized prompts. Since NeMo lacks parameter to directly control the pruning budget, we instead adopt an iterative approach. In Appx. E.6, we combine NeMos pruning method with adversarial embedding search, yielding more robust and lasting defense. Specifically, we first identify memorization weights with NeMo for original memorized text embeddings, and prune them from the model. We then perform our adversarial embedding optimization to obtain new embedding that elicits data replication, identify new set of weights for this adversarial embedding, and prune them, while keeping the previously pruned weights inactive. This process is repeated, expanding the pruned set with each iteration. Despite this effort, we find that data replication persists, and the models utility degrades substantially as more weights are removed. Given the fragility of pruning-based approaches, we modify our adversarial search technique (Sec. 3.1) by initializing the optimized embeddings by sampling from (0, I), instead of using the memorized embeddings as initialization, and repeating the experiments from this section. The results, presented in Appx. E.3, closely match the results in Tab. 1, indicating that memorization triggers are not local. We explore this phenomenon in greater depth in the next section."
        },
        {
            "title": "4 The Illusion of Memorization Locality",
            "content": "Our finding that pruning only conceals memorization (Sec. 3.3), requiring severe degradation for full removal, contradicts the locality assumption, i.e., that only small set of weights is responsible for memorizing specific training sample. This motivates us to investigate its validity more closely. We examine the text embedding space and internal DM activations, revealing that evenly distributed adversarial triggers elicit distinct internal DM activation patterns, resulting in low overlap among weights identified by pruning. This evidence suggests the locality assumption to be incorrect. 4.1 Data Replication Triggers are Not Localized in Text Embedding Space Existing memorization mitigation techniques operate solely on the text embedding of the memorized prompt, assuming that disentangling the generation process from the prompts embedding is sufficient to prevent replication. In the previous section, we focused our adversarial embedding search on local subspaces around the embedding of the memorized training prompt. Assuming that all adversarial embeddings for specific memorized image indeed share the same local subspace, natural mitigation would be to construct region around each memorized embedding, e.g., an ϵ-ball, and map all points within this region back to its center. However, our findings contradict this assumption: adversarial embeddings for single memorized image can be found evenly distributed in the text embedding space. To demonstrate this fact, we craft set of 100 adversarial embeddings, denoted by Yadv, for the same memorized image. Instead of initializing the embeddings with the memorized prompt, initialization is performed at random positions, i.e., for each yadv Yadv we have y(0) After optimizing each embedding for 50 steps using the procedure described in Sec. 3.1, we generate the corresponding images and assess their similarity to the memorized training sample. Across all runs, the generated images consistently yield SSCDOrig scores above 0.7, strongly indicating successful replication. Yet, the optimized embeddings do not collapse to single point but are scattered in the embedding space, as visualized by the t-SNE [42] plot in Fig. 2. We repeat the experiment by initializing y(0) adv with embeddings of 100 randomly selected, non-memorized prompts. Results from this experiment, presented in Fig. 6 (Appx. G.1), draw similar picture of evenly distributed replication triggers. Both results clearly demonstrate that data replication can be triggered virtually from all over the embedding space, taking away the illusion of local memorization triggers. adv (0, I). We continue by providing more quantitative support for our findings by computing the pairwise distances in the set of randomly initialized embeddings and repeating the same for the set of optimized adversarial embeddings. Our findings, also visualized in Fig. 7 (Appx. G.1), show that the optimized embeddings of Yadv are even more spread out than their random initializations. This result highlights that the adversarial embeddings are widely scattered and do not converge to single region. In the embedding space, we find no traces of memorization locality in the sense that only local regions can trigger replication of specific memorized imagean assumption implicitly made by existing pruning-based mitigation methods. Successful mitigation of memorization should, therefore, drop this locality assumption and account for significantly broader range of adversarial embeddings beyond the space around the memorized training prompt embedding. Figure 2: Data replication triggers are scattered in the embedding space. We show that the adversarial embeddings are distributed similarly to the randomly initialized embeddings, thus refuting the input space locality of data replication triggers. 4.2 Images are Not Memorized in Subset of Weights Building on our observation of high diversity among adversarial embeddings, we now turn our attention to internal model activations. For fixed input noise, we anticipate these activations will vary depending on the text embedding provided to the DM. This analysis is crucial, because pruning methods like Wanda and NeMo rely heavily on internal activationstreated as per-weight metricsto identify weights contributing to data replication as candidates for pruning. Consequently, if different adversarial embeddings leading to the same output yield distinct activation patterns, we would expect Wanda and NeMo to prune different sets of weights. Inconsistency in the resulting pruned sets would suggest lack of empirical support for the locality assumption regarding memorization. We quantify activation variability using discrepancy, defined as the mean pairwise L2 distance between activations of specific layer across different input embeddings during the first denoising step. To eliminate randomness due to the noise sampling, we fix the initial noise and only replace the adversarial embeddings used to guide the generation process. The precise formulation of the discrepancy metric is provided in Eq. (4) in Appx. G.2. Evaluation of the discrepancy is done on two sets of text embeddings: the set Yadv of 100 adversarial embeddings crafted for single image, which is identical to the set used in Sec. 4.1, and set of an additional 100 randomly selected memorized embeddings, denoted by Ymem , where each embedding corresponds to different memorized image. Since replicating distinct images should require more varied activations than replicating the same one, we expect higher discrepancy for Ymem and lower, more consistent values for Yadv. The activations are collected from the layers where NeMo and Wanda operate: the cross-attention modules value layer for NeMo and their output fully-connected layers for Wanda. We compute activations for seven cross-attention modules, indexed from 1 to 7, spanning the three Down blocks (indices 1 to 6, each block has two modules) and the Mid block of the U-Net [34], following the setup of NeMo. Surprisingly, the results in Fig. 3 (left) show that activation patterns for adversarial embeddings Yadv exhibit discrepancies across layers that are comparable to those observed for different memorized embeddings Ymem . This suggests that different adversarial embeddings, even when used to trigger the same image generation, activate distinct parts of the modelcontradicting the intuition that embeddings producing the same output should yield similar activations. While for Wanda the discrepancy appears slightly smaller for Yadv, it remains substantial, indicating that each adversarial embedding yadv Yadv induces unique activation pattern within the model. 7 Figure 3: Locality through the lens of activations and memorization weights. Although adversarial embeddings trigger the same image, their activations (left) exhibit high discrepancycomparable to that of embeddings causing replication of different images. Since memorization weight identification relies on activations, this large discrepancy results in low weight agreement (right), which undermines the idea that weights responsible for replicating memorized image can be pinpointed and pruned. While high discrepancy scores suggest that different weights may contribute to data replication, we investigate this further by analyzing the consistency of pruning-based mitigation methods across different adversarial embeddings for the same image. Since NeMo and Wanda rely on activations to select weights for pruning, we expect these sets to vary across different adversarial embeddings. To validate this expectation, we define the weight agreement between two adversarial embeddings as the overlap of identified weights from NeMo or Wanda, normalized by the total number of identified weights, i.e., the intersection over union. The final agreement metric is computed as the average pairwise weight agreement across set of adversarial embeddings. The precise formulation of this metric is provided in Eq. (5) in Appx. G.2. If no weights are identified for pair in given layer, we set the agreement to 1. As with the discrepancy metric, we report the mean agreement per layer. In Fig. 3 (right), we confirm the inconsistency of identified weights. We observe that Wandas agreement remains below 0.6 for most layers, whereas NeMo appears more precise, with agreement exceeding 0.8 for all layers except the first one. In that layer, agreement drops to approximately 0.6, which is comparable to the agreement observed for the set of memorized prompts Ymem . The high weight agreement in deeper layers is largely due to NeMo identifying memorization-related weights primarily in layer 1, fact we also visualize in Fig. 8 (left) in Appx. G.2. Although NeMo appears more stable, iterative pruning experiments (Appx. E.6) reveal it still identifies different weights for different embeddings once earlier weights are pruned. Importantly, the agreement for Yadv is similar to that of Ymem , which indicates that both methods fail to accurately localize the weights responsible for content replication, challenging the locality assumption. Low weight agreement, high activation discrepancy, and the abundance of adversarial embeddings capable of replicating any memorized image suggest that memorization in DMs is more complex phenomenon than previously assumed, and that local interventions are insufficient to address it. However, we refrain from claiming that our results definitively disprove the existence of locality. Rather, we emphasize the inherent difficulty in reliably identifying the specific weights responsible for memorization, assuming such weights exist. Since current methods rely on activation patterns to detect memorization-related weights, they fail when adversarial embeddings induce divergent activations, leading to inconsistent weight selection. Moreover, in an attempt to remove memorized content, these methods often need to prune so many weights that the model becomes virtually unusable. This behavior suggests that effectively eliminating memorized content from DM may require interventions that affect all weights, rather than targeting only small subset through pruning."
        },
        {
            "title": "5 Robust Mitigation via Adversarial Fine-Tuning",
            "content": "To address the limitations of pruning-based approaches, we propose adversarial fine-tuning as robust method for permanently removing memorized content from DMs. 8 5.1 Adversarial Fine-Tuning Building on our earlier findings that pruning-based mitigation methods conceal rather than eliminate memorization, we propose fine-tuning-based approach designed to permanently remove memorized content from the model. Our method takes inspiration from the concept of adversarial training of image classifiers [10, 26, 40], which iteratively crafts adversarial examples and updates the classifier to resist them. Our fine-tuning is inspired by the adversarial perspective in Sec. 3, in which an adversary iteratively tries to trigger undesired data replication of memorized content. Before fine-tuning, we first generate set of images conditioned on the memorized prompt while applying mitigation method that suppresses data replication to collect set of surrogate samples. We refer to individual generated surrogate images as (cid:101)x0, with their corresponding noisy versions at diffusion time step = T, . . . , 1 as (cid:101)xt. Since the generation of these surrogate images is independent of the fine-tuning process, images can be generated beforehand. During each fine-tuning step, we first sample memorized image and search for corresponding trigger embedding yadv, following our procedure described in Sec. 3.1. We switch between initializing the embedding y(0) adv with the memorized prompts embedding and drawing from random Gaussian distribution to increase robustness to adversarial embeddings crafted from different initializations. Based on these trigger embeddings, we fine-tune the noise predictor ϵθ to disrupt trajectories that would otherwise lead to data replication by steering it toward surrogate trajectories. For each batch, we sample multiple time steps and use the corresponding surrogate noisy images (cid:101)xt to update the model. We fine-tune the model using the standard diffusion training objective, conditioned on the adversarial embedding: LAdv((cid:101)x0, ϵ, yadv, t, θ) = ϵ ϵθ ((cid:101)xt, t, yadv) 2 2. Fine-tuning exclusively on adversarial embeddings and generated images risks degrading the models general utility. To preserve performance on non-memorized data, we train the model on additional non-memorized image-captions pairs using the standard diffusion loss LDM defined in Eq. (1). In practice, we perform single forward pass per update step by concatenating the embeddings and corresponding images used for both the adversarial and standard loss components to speed up the training. An algorithmic description of our adversarial fine-tuning method is provided in Appx. F. (3) 5.2 Experimental Results of Adversarial Fine-tuning As before, we rely on Stable Diffusion v1.4. In this section, we focus on removing VM samples from the model, as the impact on this type of memorization can be clearly quantified using SSCDbased metrics. We use subset of the LAION-Aesthetics [36] dataset as non-memorized samples to compute the second loss. Throughout our experiments, we use NeMo to generate 25 surrogate samples per memorized prompt. However, we emphasize that any effective mitigation method whether through pruning model components or modifying inputs at inference timeis suitable. As the goal is to preserve semantic alignment while removing memorization, the surrogate samples can also be generated from separate diffusion model, which does not replicate the target example. The DMs U-Net is fine-tuned for up to 50 epochs using the Adam optimizer with learning rate of 1e-5. For simplicity, we use the same batch size of 4 for both surrogate and non-memorized samples, keeping 1:1 ratio. After crafting an adversarial embedding, we perform three consecutive update steps with the same embedding but different image batches to reduce computational cost. Adversarial embeddings are generated following the hyperparameters described in Sec. 3.2. We find that our adversarial fine-tuning procedure quickly removes memorized content. Tab. 1 (bottom row) presents the evaluation results after fine-tuning for five epochs. The results show that adversarial embeddings can no longer trigger data replication, indicating permanent mitigation compared to pruning-based methods. At the same time, the models utility is preserved: the FID score improves from 14.44 to 13.61 after fine-tuning, suggesting no harm to the image quality. We conduct an extensive analysis of how the models robustness evolves during fine-tuning and find that it remains robust after five epochs. While longer training continues to improve evaluation metrics, in practice, five epochs are sufficient to erase memorized content. This result also holds when increasing the number of optimization steps during the search for adversarial embeddings at evaluation time. Additional sensitivity analysis results are reported in Appx. F. Overall, our novel 9 adversarial fine-tuning method is the first to achieve permanent and robust mitigation of undesired memorization in DMs after their initial training."
        },
        {
            "title": "6 Conclusions",
            "content": "We demonstrate that memorization in text-to-image DMs is not strictly local phenomenon. While pruning-based methods such as NeMo and Wanda can suppress the generation of memorized training image when prompted with its original caption, they do not remove the underlying memorization from the model. As result, the same image can still be regenerated by optimizing the prompt embedding. In effect, pruning alters the generation trajectory for the original prompt, but adversarial embeddings can steer the denoising process along alternative paths that nonetheless reproduce the memorized content. Building on this insight, we propose novel fine-tuning strategy that leverages adversarial embeddings to fully erase memorized samples from the model. Our work represents concrete step toward the responsible deployment of generative models by enabling deeper understanding of memorization and offering reliable mechanism for its removal."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), Project number 550224287. This work has been financially supported by the German Research Center for Artificial Intelligence (DFKI) project SAINT. Responsibility for the content of this publication lies with the authors."
        },
        {
            "title": "References",
            "content": "[1] Mikolaj Binkowski, Danica J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD gans. In International Conference on Learning Representations (ICLR), 2018. [2] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pages 52535270, 2023. [3] Ruchika Chavhan, Ondrej Bohdal, Yongshuo Zong, Da Li, and Timothy Hospedales. Memorization is localized within small subspace in diffusion models. International Conference on Machine Learning (ICML) - Workshop on Generative AI and Law, 2024. [4] Chen Chen, Daochang Liu, and Chang Xu. Towards memorization-free diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 84258434, 2024. [5] Chen Chen, Daochang Liu, Mubarak Shah, and Chang Xu. Enhancing privacy-utility trade-offs to mitigate memorization in diffusion models. arXiv preprint arXiv:2504.18032, 2025. [6] Yunhao Chen, Xingjun Ma, Difan Zou, and Yu-Gang Jiang. Extracting training data from unconditional diffusion models. arXiv preprint arXiv:2406.12752, 2024. [7] Salman Ul Hassan Dar, Arman Ghanaat, Jannik Kahmann, Isabelle Ayx, Theano Papavassiliu, Stefan Schoenberg, and Sandy Engelhardt. Investigating data memorization in 3d latent diffusion models for medical image synthesis. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 5665. Springer, 2023. [8] Vitaly Feldman. Does learning require memorization? short tale about long tail. In ACM SIGACT Symposium on Theory of Computing, pages 954959, 2020. [9] Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. Conference on Neural Information Processing Systems (NeurIPS), 33:28812891, 2020. [10] Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 10 [11] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [12] Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, and Ye Wang. On Memorization in Diffusion Models. arXiv preprint, arXiv:2310.02664, 2023. [13] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In Conference on Neural Information Processing Systems (NeurIPS), page 66296640, 2017. [14] Dominik Hintersdorf, Lukas Struppek, Kristian Kersting, Adam Dziedzic, and Franziska Boenisch. Finding nemo: Localizing neurons responsible for memorization in diffusion models. In Conference on Neural Information Processing Systems (NeurIPS), volume 37, pages 88236 88278, 2024. [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In Conference on Neural Information Processing Systems (NeurIPS), pages 68406851, 2020. [16] Yue Jiang, Haokun Lin, Yang Bai, Bo Peng, Zhili Liu, Yueming Lyu, Yong Yang, Jing Dong, In Image-level memorization detection via inversion-based inference perturbation. et al. International Conference on Learning Representations (ICLR), 2025. [17] Zahra Kadkhodaie, Florentin Guth, Eero Simoncelli, and Stephane Mallat. Generalization in diffusion models arises from geometry-adaptive harmonic representation. In International Conference on Learning Representations (ICLR), 2024. [18] Zahra Kadkhodaie, Florentin Guth, Eero P. Simoncelli, and Stéphane Mallat. Generalization in diffusion models arises from geometry-adaptive harmonic representation. In International Conference on Learning Representations (ICLR), 2024. [19] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. Conference on Neural Information Processing Systems (NeurIPS), 35:2656526577, 2022. [20] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Conference on Neural Information Processing Systems (NeurIPS), 34:2169621707, 2021. [21] Diederik P. Kingma and Jimmy Ba. Adam: Method for Stochastic Optimization. In International Conference on Learning Representations (ICLR), 2015. [22] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. [23] Nicky Kriplani, Minh Pham, Gowthami Somepalli, Chinmay Hegde, and Niv Cohen. Solidmark: Evaluating image memorization in generative models. arXiv preprint arXiv:2503.00592, 2025. [24] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), pages 740755, 2014. [25] Zhe Ma, Xuhong Zhang, Qingming Li, Tianyu Du, Wenzhi Chen, Zonghui Wang, and Shouling Ji. Could it be generated? towards practical analysis of memorization in text-to-image diffusion models. arXiv preprint arXiv:2405.05846, 2024. [26] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018. [27] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning (ICML), pages 81628171, 2021. 11 [28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Conference on Neural Information Processing Systems (NeurIPS), pages 80248035, 2019. [29] I. Pavlov, A. Ivanov, and S. Stafievskiy. Text-to-Image Benchmark: benchmark for generative models. https://github.com/boomb0om/text2image-benchmark, 2023. [30] Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. self-supervised descriptor for image copy detection. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 1451214522, 2022. [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pages 87488763, 2021. [32] Jie Ren, Yaxin Li, Shenglai Zeng, Han Xu, Lingjuan Lyu, Yue Xing, and Jiliang Tang. Unveiling and mitigating memorization in text-to-image diffusion models through cross attention. In European Conference on Computer Vision (ECCV), pages 340356. Springer, 2024. [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. [34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention (MICCAI), pages 234241, 2015. [35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In Conference on Neural Information Processing Systems (NeurIPS), 2022. [36] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In Conference on Neural Information Processing Systems (NeurIPS), 2022. [37] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mitigating copying in diffusion models. Conference on Neural Information Processing Systems (NeurIPS), 36:4778347803, 2023. [38] Yang Song and Stefano Ermon. Improved Techniques for Training Score-Based Generative Models. In Conference on Neural Information Processing Systems (NeurIPS), pages 12438 12448, 2020. [39] Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. simple and effective pruning approach for large language models. In International Conference on Learning Representations (ICLR), 2024. [40] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014. [41] Aaron Van Den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. Conference on Neural Information Processing Systems (NeurIPS), 30, 2017. [42] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(86):25792605, 2008. 12 [43] Ryan Webster. reproducible extraction of training images from diffusion models. arXiv preprint, arXiv:2305.08694, 2023. [44] Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b. arXiv preprint arXiv:2303.12733, 2023. [45] Yuxin Wen, Yuchen Liu, Chen Chen, and Lingjuan Lyu. Detecting, Explaining, and Mitigating Memorization in Diffusion Models. In International Conference on Learning Representations (ICLR), 2024. [46] Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Peng Wang, Liyue Shen, and Qing Qu. The emergence of reproducibility and consistency in diffusion models. In International Conference on Machine Learning (ICML), 2024. [47] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. To generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images ... for now. In European Conference on Computer Vision (ECCV), pages 385403, 2025."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Background and Related Work"
        },
        {
            "title": "2.2 Memorization in Diffusion Models . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3 Breaking Pruning-Based Mitigation Methods"
        },
        {
            "title": "3.1 Finding Dori With Adversarial Text Embeddings",
            "content": ". . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.2 Experimental Setup .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.3 Pruning-Based Mitigation Conceals but Does Not Erase Memorization . . . . . . .",
            "content": "4 The Illusion of Memorization Locality 4.1 Data Replication Triggers are Not Localized in Text Embedding Space . . . . . . . 4.2 Images are Not Memorized in Subset of Weights . . . . . . . . . . . . . . . . . . 5 Robust Mitigation via Adversarial Fine-Tuning 5.1 Adversarial Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Experimental Results of Adversarial Fine-tuning . . . . . . . . . . . . . . . . . . 6 Conclusions Limitations Hardand Software Details Model and Dataset Details Additional Details and Experiments on Adversarial Embedding Optimization D.1 Can We Make DM to Output Any Image With Adversarial Embeddings? . . . . . D.2 Comparing Behavioral Differences Between Sets . . . . . . . . . . . . . . . . . . D.3 Can the Embeddings Themselves Be Constrained? . . . . . . . . . . . . . . . . . Additional Experiments on Pruning-Based Mitigation E.1 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Sensitivity Analysis of Adversarial Embedding Optimization . . . . . . . . . . . . E.3 Starting Embedding Optimization from Random Embeddings . . . . . . . . . . . . E.4 Additional Hyperparameter Evaluation for Wanda . . . . . . . . . . . . . . . . . . E.5 Increasing Sparsity for Wanda . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.6 Iterative Application of NeMo . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.7 Cost of Successful Memorization Removal with Wanda . . . . . . . . . . . . . . . Additional Details and Experiments on Adversarial Fine-Tuning 14 1 2 3 3 4 4 5 6 6 8 9 9 10 16 16 17 17 19 20 21 21 23 24 25 27 28 F.1 Algorithmic Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Sensitivity Analysis . F.3 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Details and Experiments on Locality G.1 Locality in the text embedding space . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Locality in the models weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.3 NeMo and Wanda identify memorization weights in different layers . . . . . . . . Qualitative Results H.1 Qualitative Results for Wanda Pruning . . . . . . . . . . . . . . . . . . . . . . . . H.2 Qualitative Results for NeMo Pruning . . . . . . . . . . . . . . . . . . . . . . . . H.3 Adversarial Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.4 Wanda with 10% Sparsity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . H.5 Qualitative Results for Iterative NeMo Pruning . . . . . . . . . . . . . . . . . . . 28 30 32 34 34 35 37 37 38 39"
        },
        {
            "title": "A Limitations",
            "content": "Our analysis of the locality of memorization within model parameters focuses on selected subset of layers. While it is possible that signs of locality may also be present in other componentssuch as self-attention mechanisms or convolutional layerswe chose to concentrate on the layers where existing mitigation methods are typically applied and where initial success in suppressing replication has been observed. This targeted approach allows us to provide concrete and meaningful insights into the locality hypothesis. Notably, to our knowledge, no current methods explicitly aim to identify memorization-related weights outside the studied layers. Furthermore, supporting evidence from the NeMo paper (Appendix C.9) indicates that pruning convolutional layers does not effectively reduce memorization, suggesting that our chosen focus captures the most relevant regions for intervention. Additionally, we recognize that our adversarial fine-tuning method for removing memorized content involves higher computational cost compared to pruning-based approaches. This is due to the need for generating adversarial inputs, creating surrogate samples, and extending the fine-tuning dataset with non-memorized data to preserve utility. We see this as valuable trade-off, as our method offers the first reliable and permanent mitigation that is robust to adversarial embedding attacks. Nonetheless, we believe there is substantial potential for future work to build on our findings and develop more efficient mitigation strategies that retain our methods effectiveness while reducing computational overhead. Hardand Software Details We conducted all experiments on NVIDIA DGX systems running NVIDIA DGX Server Version 5.2.0 and Ubuntu 20.04.6 LTS. The machines are equipped with 2 TB of RAM and feature NVIDIA A100-SXM4-40GB GPUs. The respective CPUs are AMD EPYC 7742 64-core. Our experiments utilized CUDA 12.2, Python 3.10.13, and PyTorch 2.2.0 with Torchvision 0.17.0 [28]. Notably, all experiments are conducted on single GPUs. All models used in our experiments are publicly available on Hugging Face. We accessed them using the Hugging Face diffusers library (version 0.27.1). To facilitate reproducibility, we provide Dockerfile along with our code. Additionally, all hyperparameters required to reproduce the results presented in this paper are included."
        },
        {
            "title": "C Model and Dataset Details",
            "content": "Our experiments primarily use Stable Diffusion v1-4 [33], which is publicly available at https: //huggingface.co/CompVis/stable-diffusion-v1-4. Comprehensive information about the data, training parameters, limitations, and environmental impact can be found at that URL. The model is released under the CreativeML OpenRAIL license. The memorized prompts analyzed in our study originate from the LAION2B-en [36] dataset, which was used to train the DM. We use set of memorized prompts provided by Wen et al. [45]2, who identified them using the extraction tool developed by Webster [43]. The LAION dataset is licensed under the Creative Commons CC-BY 4.0. As the images in the dataset may be subject to copyright, we do not include them in our codebase; instead, we provide URLs that allow users to retrieve the images directly from their original sources. For performing our fine-tuning-based mitigation method, we furthermore downloaded 100k images from the LAION aesthetics dataset, subset of LAION5B. 2Available at https://github.com/YuxinWenRick/diffusion_memorization."
        },
        {
            "title": "Optimization",
            "content": "In the following, we elaborate on the design of the adversarial optimization Eq. (2) used to obtain yadv to trigger generation of xmem . First, we provide the algorithm in Alg. 1. Then we showcase that naive unconstrained optimization would yield False Positives (yadv that are capable of forcing the DM to generate arbitrary images), see Appx. D.1. Motivated by this finding, we experiment with the varying strength of the optimization, and arrive at the final constraint of 50 optimization steps, which allows us to successfully craft yadv if the optimization target (image) is memorized, and fail to provide yadv for all other (non-memorized) targets. We evaluate constraining schemes that work in the embedding space in Appx. D.3, and show that they are unsuccessful at preventing False Positives. Algorithm 1 Finding Dori with Adversarial Embeddings optionally after pruning-based mitigation applied Input: Diffusion model ϵθ Memorized training image xmem Memorized training prompt pmem Number of optimization steps Learning rate η Output: Adversarial embedding yadv y(0) adv encode_text(pmem) for {1, . . . , } do ϵ (0, I) Uniform({1, . . . , }) xt add_noise(xmem, ϵ, t) xt, t, y(i1) ˆϵ ϵθ adv y(i1) y(i) end for return y(N ) adv adv adv η y(i1) (cid:16) (cid:17) adv alternatively, initialize y(0) adv (0, I) sample discrete timestep from noise schedule adding noise using the training noise scheduler ϵ ˆϵ2 2 update adv. embedding with gradient descent D.1 Can We Make DM to Output Any Image With Adversarial Embeddings? To assess whether Doris ability to replicate memorized images is truly due to memorization, we also test whether adversarial text embeddings can be used to generate an arbitrary (non-memorized) image, as described in Sec. 3.1. Intuitively, we expect that we can force an 800M parameter model to produce specific output vector (latent representation of an image) of size 16,384, given we perform an unconstrained gradient-based optimization of an input vector (text embedding) of size 59,136. In effect, if the model can be forced to produce arbitrary, non-memorized images using these embeddings, it would suggest that Dori is not exploiting memorization, but rather steering the model toward designated outputsregardless of whether the content was memorized. To generate non-memorized images with Dori, we sample 100 images from the COCO2014 training set and run the optimization for 1000 steps for each sampled image. Using the resulting adversarial embeddings, we generate 5 images per embedding with SD-v1.4 and compute the SSCD scores between the generated and original images. The SSCD scores for all examples exceed the memorization threshold of 0.7, and qualitatively, Fig. 4 shows that the images are replicated almost perfectly. While this initially might seem as if Dori is not only replicating memorized samples, we demonstrate in Appx. D.2 that there is, in fact, difference between triggering generation of memorized versus non-memorized content. 17 Figure 4: Arbitrary image replication. We find that when pushed to the extreme, Dori search yields generations (columns from two to six from the left) of non-memorized data (first from the left). 18 D.2 Comparing Behavioral Differences Between Sets Our findings from Appx. D.1 undermine our adversarial-based memorization identification. In effect, it may seem that our results regarding NeMo and Wanda (Sec. 3.3) locality in the embedding space (Sec. 4.1). and locality in the models weights (Sec. 4.2) become invalid. Indeed, if we are able to trigger generation of any image, then we should not claim that NeMo and Wanda only conceal the memorization instead of fully removing it, and the findings regarding localization would be false, as the obtained adversarial embeddings yadv yield little information about how the model (and the embedding space) behaves when faced with memorized data. To ensure correctness of our methodology, andin effectthe findings, we investigate if there is any difference between the optimization process for memorized and other (non-memorized) images. We compare how the L2 norm of text embeddings progress during optimization, as well as how early we cross the 0.7 SSCD threshold. We analyze two sets of memorized images (100 VM samples and 100 TM samples), and set of 100 images from COCO2014 train. Moreover, we analyze two sets of generated images from SD-v1.4: generated using 100 captions from COCO2014 train, and using 100 prompts of images that have been subject of template memorization. The latter generated set addresses limitations of our detection metricSSCDOrigwhich relies on all semantic and compositional parts of two compared images to match closely to cross the memorization threshold of 0.7. In case of template memorization, the model replicates only part of training image, e.g., the background, specific objects, or replicates the semantic contents of the image, while varying features of low importance, like textures. We note that generated images and memorized template images will differ when it comes to the low importance features, effectively lowering SSCD score, however, the semantic composition of the generated images will match the one of memorized. We add generated images from 100 non-training prompts (COCO2014) to test the worst-case False Positive scenario of our method. If the model is already able to generate an image from some input, the optimization should converge the fastest for these images, even though they are neither part of the training data, nor memorized. In Fig. 5 (right) we show how the SSCD score progresses with the optimization. We note that for verbatim memorization (and generated template memorization) we need only handful of optimization steps to obtain yadv that reliably triggers generation of the images. For non-memorized data we reach SSCD above 0.7 after approximately 500 steps. Notably, we require as much as 200 steps to craft yadv that reliably produces generated (non-memorized) images. These results show that the optimization process is indeed different for memorized images than for other images. Building on these findings we allow the optimization procedure to only perform 50 update stepsa value that guarantees generation of memorized images (if present in the model), while preventing False Positives, i.e., generation of non-memorized images. This constraint ensures methodological correctness of our adversarial-based approach, and proves our results in Secs. 3.3 and 4 are valid. Figure 5: Finding Dori with more optimization steps. We note that our method starts producing False Positives, i.e., replicating non-memorized data, only after 500 optimization steps (left). Notably, to achieve non-training data replication, the norm of the optimized embedding raises drastically (right). 19 D.3 Can the Embeddings Themselves Be Constrained? The findings in Appx. D.2 show that unconstrained optimization can lead to \"replication\" of an arbitrary image. In our work we default to restricting the number of optimization steps to prevent that false replication. We validate the soundness of that limit empirically, showing in Tab. 1 (second row) that we alleviate the problem. An alternative approach to prevent triggering arbitrary data would be to investigate the embedding space itself, and based on its characteristics, define meaningful constraints on the optimization. We focus on L2 norms of the embeddings, as in Fig. 5 (right) we observe that the norms stay low for memorized content, while to replicate non-memorized content, the embeddings have to have their norm significantly increased. To get glimpse at the embedding space, we embed 400k prompts from COCO 2014 train dataset, and note that the distribution of the norms is centered around 250, with standard deviation of around 2. Additionally, we perform gradient optimization of tokens to obtain minimal and maximal possible L2 norm of text embedding. We find discrete inputs that lower L2 norm down to 220, and inputs that are able to increase the norm to 280. adv 2 > 280, we project it back to norm ball of 280 by y(i) With the limits of the embedding space established, we constrain our optimization to craft adversarial embeddings with L2 norm below the maximum possible value: 280. To this end, at each optimization step i, if y(i) , an approach inspired by Projected Gradient Descent [26]. Interestingly, even with such constraint, we are able obtain adversarial embeddings that trigger generation of non-memorized content, however, it requires more optimization steps, 2000 instead of 500. Next, we constrain the optimization even further, and expect embeddings to have norms smaller than 220the lower boundary. Notably, memorized data is still replicated after merely 50 optimization steps even after pruning, while to replicate non-memorized data we need 10,000 steps. adv y(i) 280 y(i) adv 2 2 adv We conclude that constraining adversarial embeddings might be futile strategy to prevent nonmemorized data replication, as even under heavy constraints, we are able to find embeddings that trigger generation of arbitrary images. Thus, we suggest limiting the number of update steps, and initializing optimization at fixed point in text embedding space, to alleviate the issue with False Positives. 20 Additional Experiments on Pruning-Based Mitigation We find that close data replication is primarily triggered by VM prompts, while TM prompts lead to lower apparent replication. However, because TM prompts tend to produce partial replications that differ in non-semantic aspects of image composition, like the pattern on phone case, SSCD-based metrics are less informative in this case than for VM prompts. E.1 Hyperparameters We followed the default hyperparameters for NeMo and Wanda reported in the respective publications. NeMo: We set the memorization score threshold to τmem = 0.428, corresponding to the mean SSIM score plus one standard deviation, as measured on holdout set of 50,000 LAION prompts. For the stronger variant of NeMo, reported in Tab. 2, we lowered the threshold to τmem = 0.288, which corresponds to the mean SSIM score minus one standard deviation. While we follow the original evaluation procedure by individually identifying and disabling neurons for each memorized prompt, we compute the FID and KID metrics by simultaneously deactivating all neurons identified for VM and TM prompts, respectively. This approach provides more consistent estimate of the prunings overall impact on model utility. Wanda: For Wanda, we follow the experimental setup of Chavhan et al. [3]. Specifically, we use all 500 memorized prompts to identify weights in the second fully connected layer of the cross-attention mechanism. As in Chavhan et al. [3], we select the top 1% of weights with the highest Wanda scores. These weights are then aggregated across the first 10 time steps and pruned to mitigate memorization. Additional results for identifying weights using Wanda per memorized prompt, for 10 and for all 500 memorized prompts, can be seen in Tab. 4. Results for different number of time steps and different values of sparsity can be found in Tab. 5 and Tab. 6, respectively. E.2 Sensitivity Analysis of Adversarial Embedding Optimization In Tab. 2, we compare the results of NeMo and Wanda with and without adversarial embedding optimization. Application of adversarial embeddings is denoted by . Additionally, we repeat the experiments using different numbers of adversarial optimization steps, denoted by Adv. Steps in the table. All optimizations are initialized from the memorized training embedding. Notably, single optimization step is already sufficient to circumvent the mitigation introduced by NeMo. In the case of Wanda, approximately 25 optimization steps are required before clear replication is triggered. In addition to the main paper, we also report results for TM prompts. While the SSCD scores are substantially lower than those for VM prompts, we note that replication of memorized content is still possible. However, the SSCD score fails to adequately capture TM memorization due to the semantic variations in the generated images. At the bottom of the table, we also report results for adversarial embedding optimization applied to non-memorized training images, to evaluate whether replication can be triggered for non-memorized content. However, even after 150 optimization steps, SSCD scores remain below the memorization threshold of 0.7. Table 2: Comparison of different numbers of adversarial embedding optimization steps. Embeddings are initialized with their corresponding training prompt embeddings. denotes the application of adversarial embeddings. Adv. Steps Memorization Type Setting No Mitigation NeMo [14] Wanda [3] NeMo + NeMo + NeMo + NeMo + NeMo + NeMo + NeMo (strong, τmem = 0.288) + Wanda + Wanda + Wanda + Wanda + Wanda + Wanda + Non-Memorized Images Non-Memorized Images + Non-Memorized Images + Non-Memorized Images + Non-Memorized Images + Non-Memorized Images + Non-Memorized Images + 10 25 50 100 150 1 10 25 50 100 1 10 25 50 150 Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template None None None None None None None SSCDOrig 0.90 0.04 0.17 0.09 0.33 0.18 0.23 0.08 0.20 0.08 0.17 0.05 0.86 0.07 0.28 0.11 0.81 0.06 0.42 0.13 0.88 0.04 0.50 0. 0.91 0.03 0.55 0.12 0.93 0.02 0.60 0.14 0.92 0.02 0.65 0.16 SSCDGen N/A N/A 0.40 0.21 0.54 0.28 0.21 0.09 0.18 0. 0.94 0.04 0.51 0.28 0.88 0.05 0.21 0.15 0.95 0.03 0.20 0.15 0.97 0.02 0.17 0.12 0.96 0.02 0.17 0.12 0.96 0.02 0.17 0. DSSCD 1.00 0.00 0.90 0.08 0.46 0.13 0.55 0.10 0.37 0.07 0.38 0.09 1.00 0.00 0.62 0.21 0.99 0.01 0.72 0.13 1.00 0.00 0.75 0. 1.00 0.00 0.79 0.11 1.00 0.00 0.86 0.09 1.00 0.00 0.93 0.06 ACLIP 0.33 0.01 0.33 0.02 0.34 0.02 0.34 0.03 0.34 0.02 0.34 0. 0.32 0.01 0.33 0.02 0.32 0.01 0.32 0.02 0.32 0.01 0.32 0.02 0.33 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0. 0.91 0.03 0.55 0.12 0.96 0.02 0.19 0.12 1.00 0.00 0.79 0.10 0.33 0.02 0.32 0.02 0.11 0.06 0.12 0.05 0.64 0.11 0.43 0. 0.77 0.05 0.65 0.07 0.82 0.05 0.73 0.05 0.85 0.04 0.75 0.05 0.85 0.04 0.76 0.05 0.58 0.08 0.54 0.09 0.76 0.14 0.53 0. 0.90 0.07 0.78 0.10 0.96 0.02 0.88 0.06 0.98 0.01 0.94 0.04 0.99 0.01 0.95 0.03 0.24 0.04 0.31 0.06 0.31 0.02 0.31 0. 0.32 0.02 0.32 0.02 0.32 0.01 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.31 0.02 FID KID 14. 0.0061 15.16 18.97 16.86 17.51 15.16 18.97 15.16 18.97 15.16 18. 15.16 18.97 15.16 18.97 15.16 18.97 14.92 18.85 16.86 17.51 16.86 17. 16.86 17.51 16.86 17.51 16.86 17.51 16.86 17.51 0.0061 0.0048 0.0065 0. 0.0061 0.0048 0.0061 0.0048 0.0061 0.0048 0.0061 0.0048 0.0061 0.0048 0.0061 0. 0.0064 0.0042 0.0065 0.0070 0.0065 0.0070 0.0065 0.0070 0.0065 0.0070 0.0065 0. 0.0065 0.0070 N/A N/A N/A N/A N/A N/A N/A 0.35 0.06 0.35 0.02 14.44 0. 0.34 0.06 0.34 0.02 14.44 0.0061 0.48 0.06 0.32 0. 14.44 0.0061 0.58 0.06 0.32 0.02 14.44 0. 0.67 0.07 0.32 0.02 14.44 0.0061 0.79 0.07 0.32 0. 14.44 0.0061 0.88 0.06 0.32 0.02 14.44 0. 0.11 0.05 0.11 0.05 0.58 0.11 0.12 0.05 0.69 0.07 0.12 0.05 0.76 0.05 0.10 0.06 0.80 0.05 0.09 0.06 0.81 0.04 0.09 0. 0.17 0.05 0.17 0.04 0.28 0.05 0.39 0.06 0.48 0.06 0.58 0. 0.65 0.06 22 E.3 Starting Embedding Optimization from Random Embeddings We repeat the experiments on adversarial embedding optimization, but instead of initializing from the memorized training prompt embedding, we start each optimization from random Gaussian noise. Remarkably, the results closely match those obtained when initializing from the memorized prompt, indicating that data replication can be triggered from various positions in the embedding space. Table 3: Comparison of different numbers of adversarial embedding optimization steps. Embeddings are initialized randomly. Adv. Steps Memorization Type denotes the application of adversarial embeddings. SSCDOrig 0.07 0.02 0.10 0.03 SSCDGen 0.06 0.02 0.10 0.03 Verbatim Template DSSCD 0.31 0.06 0.26 0.05 Setting NeMo + NeMo + NeMo + NeMo + NeMo + NeMo + Wanda + Wanda + Wanda + Wanda + Wanda + Wanda + 10 25 50 100 150 10 25 50 100 150 ACLIP 0.21 0.02 0.28 0. 0.32 0.01 0.32 0.02 0.32 0.01 0.32 0.02 0.33 0.02 0.32 0.02 0.33 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.21 0.02 0.22 0. 0.29 0.03 0.28 0.03 0.31 0.02 0.31 0.02 0.32 0.02 0.31 0.02 0.32 0.02 0.32 0.02 0.32 0.01 0.32 0.02 Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template 0.81 0.06 0.42 0.13 0.88 0.04 0.50 0. 0.91 0.03 0.55 0.12 0.93 0.02 0.60 0.14 0.92 0.02 0.64 0.15 0.07 0.02 0.08 0.03 0.28 0.10 0.10 0.04 0.68 0.10 0.10 0. 0.80 0.06 0.09 0.05 0.85 0.05 0.09 0.06 0.86 0.04 0.08 0.07 0.89 0.05 0.21 0.15 0.95 0.03 0.20 0.15 0.97 0.02 0.18 0. 0.96 0.02 0.17 0.12 0.96 0.02 0.16 0.12 0.07 0.02 0.08 0.02 0.28 0.13 0.21 0.10 0.70 0.10 0.68 0.06 0.84 0.06 0.78 0. 0.87 0.05 0.82 0.05 0.87 0.04 0.82 0.04 0.99 0.01 0.72 0.14 1.00 0.00 0.75 0.11 1.00 0.00 0.79 0.10 1.00 0.00 0.87 0. 1.00 0.00 0.93 0.06 0.38 0.10 0.45 0.12 0.42 0.07 0.37 0.09 0.84 0.13 0.83 0.08 0.97 0.02 0.93 0.04 0.99 0.01 0.98 0. 0.99 0.00 0.99 0.01 23 E.4 Additional Hyperparameter Evaluation for Wanda Table 4: As shown, applying Wanda across all prompts is less effective at mitigating memorization compared to applying it individually per prompt. However, as discussed in Appx. E.7, applying Wanda per prompt and aggregating the found neurons over all 500 prompts comes at the high cost of reduced overall performance because of so many weights being pruned. In the setting with 10 prompts, we randomly sample 10 prompts across 5 different seeds and report the average results. This setup proves less effective at mitigating memorization than using the full set of 500 prompts. Setting Memorization Type Wanda [3] per Prompt Wanda [3] 10 Prompts Wanda [3] all Prompts Wanda per Prompt + Wanda 10 Prompts + Wanda all Prompts + Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template SSCDOrig 0.11 0.03 0.14 0.04 SSCDGen 0.12 0.03 0.13 0.04 DSSCD 0.27 0.06 0.35 0.10 ACLIP 0.32 0.02 0.32 0.03 0.22 0.10 0.19 0.06 0.24 0.11 0.24 0. 0.41 0.09 0.42 0.10 0.34 0.02 0.34 0.03 0.20 0.08 0.17 0.05 0.21 0.09 0.18 0.08 0.37 0.07 0.38 0.09 0.34 0.02 0.34 0. 0.69 0.07 0.10 0.06 0.75 0.06 0.10 0.06 0.80 0.06 0.09 0.05 0.76 0.06 0.69 0.06 0.81 0.05 0.72 0.05 0.84 0.06 0.78 0. 0.91 0.05 0.86 0.08 0.97 0.02 0.88 0.06 0.97 0.02 0.93 0.04 0.32 0.02 0.32 0.02 0.32 0.01 0.32 0.02 0.32 0.02 0.31 0. Table 5: Even when applying Wanda [3] with higher number of time steps it is still possible to break it using Dori. SSCDOrig 0.22 0.08 0.18 0.05 0.20 0.08 0.17 0.05 0.20 0.08 0.17 0.04 0.20 0.08 0.18 0.05 0.20 0.08 0.18 0. 0.20 0.07 0.17 0.04 0.77 0.05 0.10 0.06 0.76 0.05 0.10 0.06 0.76 0.05 0.11 0.06 0.74 0.05 0.10 0.05 0.74 0.05 0.10 0. 0.73 0.06 0.11 0.05 SSCDGen 0.24 0.09 0.20 0.08 0.21 0.09 0.18 0.08 0.21 0.07 0.17 0.06 0.20 0.07 0.17 0.06 0.21 0.07 0.17 0. 0.20 0.07 0.17 0.06 0.82 0.05 0.73 0.05 0.82 0.05 0.72 0.05 0.82 0.05 0.73 0.05 0.80 0.05 0.73 0.05 0.81 0.05 0.73 0. 0.79 0.05 0.72 0.05 DSSCD 0.38 0.08 0.39 0.09 0.37 0.07 0.38 0.09 0.39 0.08 0.39 0.09 0.37 0.07 0.39 0.10 0.38 0.07 0.39 0. 0.38 0.07 0.41 0.11 0.97 0.01 0.88 0.06 0.96 0.02 0.88 0.06 0.96 0.02 0.87 0.07 0.96 0.02 0.88 0.07 0.96 0.02 0.88 0. 0.96 0.02 0.88 0.06 ACLIP 0.34 0.02 0.33 0.02 0.34 0.02 0.34 0.03 0.34 0.03 0.33 0.03 0.34 0.02 0.33 0.03 0.34 0.02 0.33 0. 0.34 0.02 0.33 0.03 0.32 0.01 0.32 0.02 0.32 0.01 0.32 0.02 0.32 0.01 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.01 0.32 0. 0.32 0.02 0.32 0.02 Setting Number of Timesteps Memorization Type Wanda Wanda + 10 20 30 40 50 10 20 30 40 50 Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template E.5 Increasing Sparsity for Wanda Table 6: Applying Wanda [3] with higher sparsity does not change the fact that the method seems to only conceal memorization instead of completely removing it from the model. Increasing the sparsity also comes at the cost of reduced image quality, as the FID and the KID values suggest. Setting Sparsity Memorization Type Wanda Wanda + 1% 2% 3% 4% 5% 10% 1% 2% 3% 4% 5% 10% Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template Verbatim Template SSCDOrig 0.20 0.08 0.17 0.05 0.19 0.07 0.17 0.05 0.17 0.07 0.16 0.05 0.17 0.06 0.14 0.05 0.15 0.05 0.13 0.05 0.12 0.03 0.11 0. 0.76 0.06 0.10 0.06 0.71 0.07 0.10 0.05 0.65 0.08 0.10 0.06 0.62 0.08 0.10 0.05 0.56 0.10 0.10 0.05 0.40 0.13 0.10 0. SSCDGen 0.21 0.09 0.18 0.08 0.20 0.07 0.16 0.07 0.17 0.06 0.15 0.06 0.17 0.05 0.14 0.06 0.16 0.05 0.14 0.05 0.13 0.04 0.13 0. 0.82 0.05 0.73 0.05 0.76 0.06 0.67 0.06 0.73 0.06 0.61 0.08 0.66 0.08 0.58 0.08 0.62 0.09 0.52 0.10 0.45 0.14 0.35 0. DSSCD 0.37 0.07 0.38 0.09 0.36 0.07 0.38 0.08 0.34 0.06 0.38 0.09 0.34 0.06 0.37 0.09 0.32 0.05 0.39 0.10 0.33 0.06 0.39 0. 0.96 0.02 0.88 0.06 0.90 0.05 0.83 0.08 0.87 0.06 0.76 0.09 0.81 0.08 0.71 0.10 0.77 0.10 0.66 0.10 0.67 0.10 0.54 0. ACLIP 0.34 0.02 0.34 0.03 0.33 0.02 0.33 0.03 0.33 0.02 0.32 0.03 0.33 0.02 0.32 0.03 0.32 0.02 0.32 0.03 0.31 0.02 0.30 0. 0.32 0.01 0.32 0.02 0.32 0.02 0.32 0.02 0.31 0.02 0.31 0.02 0.31 0.02 0.31 0.02 0.31 0.02 0.31 0.02 0.30 0.02 0.30 0. FID KID 16.86 17.51 18.17 19.55 20.37 22.40 23.07 24. 25.53 26.61 37.34 36.69 16.86 17.51 18.17 19.55 20.37 22.40 23.07 24. 25.53 26.61 37.34 36.69 0.0067 0.0068 0.0066 0.0073 0.0075 0.0086 0.0088 0. 0.0102 0.0106 0.0168 0.0166 0.0067 0.0068 0.0066 0.0073 0.0075 0.0086 0.0088 0. 0.0102 0.0106 0.0168 0.0166 25 E.6 Iterative Application of NeMo Table 7: We apply NeMo [14] iteratively such that after each round of pruning, we search for new adversarial embeddings that can still trigger memorization, and then apply NeMo again to prune the newly identified weights. Despite multiple iterations, this process does not completely eliminate memorization, as adversarial embeddings can still uncover residual memorized content. Due to the high computational cost of repeated NeMo applications and searching for adversarial embeddings, we focus our analysis on prompts known to be verbatim memorized. In some cases, after several iterations, NeMo no longer detects any memorization. When this happens, we analyze the outputs generated from the adversarial embeddings that NeMo failed to flag. Method Iterations NeMo [14] Adv. Images NeMo [14] Mitigated Images 1 2 3 4 5 1 2 3 4 5 SSCDOrig 0.92 0.03 0.92 0.03 0.92 0.03 0.92 0.03 0.92 0. 0.35 0.19 0.23 0.09 0.21 0.09 0.20 0.07 0.20 0.07 SSCDGen 0.94 0.03 0.94 0.03 0.95 0.03 0.95 0.03 0.95 0.03 0.34 0.22 0.23 0.11 0.20 0.09 0.19 0.08 0.19 0.08 DSSCD 1.00 0.00 1.00 0.00 1.00 0.00 1.00 0.00 1.00 0.00 0.480.14 0.410.09 0.390.08 0.390.08 0.370.08 ACLIP 0.330.02 0.320.02 0.330.01 0.330.02 0.320. 0.340.02 0.350.02 0.350.02 0.340.02 0.340.02 26 E.7 Cost of Successful Memorization Removal with Wanda The results in Tab. 6 indicate that Wanda might be successful in removing memorized content from the model (low SSCDOrig at sparsity 10%) with limited harm to the alignment between the prompt and the generated images for benign input (high ACLIP). However, FID scores appear to increase significantly with pruning (increase from 16.68 to 37.34 for VM samples). We investigate the harm that Wanda with 10% weights pruned causes to the model. The weights pruned by Wanda not only correspond to the memorized image, but also partially encode concepts present in the memorized content. For example, the memorized image in Fig. 1 ( 1 ) consists also of concept of woman. We show that in order to fully remove the image from the model, weights responsible for benign concepts, present in memorized data, are negatively affected. Preserve every tag and keyword.\", The format should be [prompt1, prompt2, We verify that idea by generating 100 images from set of 10 prompts, which are paraphrases of the memorized prompts. Then, we compute CLIP similarity between the paraphrases and generated images, AConcepts), to capture how the alignment changes with high pruning. The paraphrases are obtained by prompting LLama-3.1-8B-Instruct [11], with the system prompt the \"You are paraphrasing engine. user prompt \"Write 10 alternative phrasings of:CAPTION. Return only the paraphrasings, no other text. ...]\". For example, for the prompt \"Living in the Light with Ann Graham Lotz\" we obtain \"Embracing Life in the Radiance of God with Ann Graham Lotz\", \"A Life of Radiant Faith with Ann Graham Lotz\", \"Living Life in the Illumination of God with Ann Graham Lotz\", \"In the Presence of Gods Radiant Light with Ann Graham Lotz\", \"Faith in the Light of God with Ann Graham Lotz\", \"Radiant Living with Ann Graham Lotz\", \"In Gods Illuminating Light with Ann Graham Lotz\", \"Ann Graham Lotz on Living in Gods Radiant Presence\", \"Radiant Faith Living with Ann Graham Lotz\", \"Living Life in Gods Illuminating Light with Ann Graham Lotz\". Additionally, we quantify image quality of the concepts by computing FID score (denoted FIDConcept) between 10,000 images generated from the prompts before and after pruning for VM and TM samples. and The results in Tab. 8 show that the concepts associated with the memorized images suffer after mitigation with Wanda. We observe significant drop from ACLIP of around 0.37 to 0.33 for VM, which suggests that the generated images no longer follow the prompt. Moreover, the quality of the generated images degrades. FIDConcepts above 80 for VM and above 90 for TM samples indicates significant harm to the model, corroborated by the last row of Tab. 6. Additionally, we provide qualitative results of damage to concepts in Appx. H.4. Table 8: Successful memorization removal with Wanda requires significant damage to the model. While 10% sparsity ratio for Wanda mitigates memorization even under Dori, we observe sharp drop in the generation quality (FIDConcept) and alignment between the prompt and generated images (AConcept) for paraphrases of prompts used to remove memorization. Setting Memorization No Mitigation Wanda + 10% pruned + Verbatim Template Verbatim Template SSCDOrig 0.90 0.04 0.17 0.09 AConcepts 0.37 0.02 0.36 0. FIDConcepts N/A N/A 0.40 0.13 0.10 0.04 0.33 0.02 0.33 0.02 80.70 92.46 27 Additional Details and Experiments on Adversarial Fine-Tuning F.1 Algorithmic Description Alg. 2 provides an algorithmic overview of our adversarial fine-tuning mitigation method. As described in the main paper, we begin by generating images from memorized prompts using mitigation technique that preserves alignment with the prompt while avoiding replication of training data. Alternatively, these images can be generated using separate diffusion model that has not been trained on the corresponding samples. To preserve the models general utility, second set of non-memorized samples is incorporated during fine-tuning. In the algorithmic description, surrogate and non-memorized samples are processed in separate batches to clearly illustrate the fine-tuning steps. However, in practice, embeddings and samples from both batches are concatenated to avoid redundant forward passes and to accelerate optimization. For each surrogate sample, fixed adversarial embedding is used throughout an epoch. These embeddings are either initialized from the memorized prompt embedding or from random Gaussian noise. The adversarial fine-tuning loss, Ladv, is computed exclusively on surrogate samples and their corresponding adversarial embeddings to reduce memorization. In parallel, model utility is preserved through utility loss, Lnon-mem, which is computed solely on the non-memorized samples. In our experiments, surrogate and non-memorized batches are of equal size by default; however, increasing the size of the non-memorized batch places greater emphasis on utility preservation. 28 Algorithm 2 Fine-Tuning Diffusion Model to Mitigate Memorization Input: Diffusion model ϵθ Non-memorized images and corresponding prompts Dnon-mem Memorized images and corresponding prompts Dmem Surrogate images Dsurrogate Learning rate η, epochs E, steps per image Images generated with active mitigation Output: Fine-tuned model ϵθ for epoch {1, . . . , E} do for (xmem, pmem) Dmem do if epoch mod 2 == 1 then yadv find_adv_embedding(xmem, pmem) Start from text embedding else yadv find_adv_embedding(xmem, random) Start from random embedding end if for {1, . . . , S} do Sample surrogate image xsurr Dsurrogate Sample non-memorized (xnon-mem, pnon-mem) Dnon-mem ϵsurr (0, I) Uniform(1, ) x(t) surr add_noise(xsurr, ϵsurr, t) ˆϵsurr ϵθ( x(t) surr, t, yadv) Ladv ϵsurr ˆϵsurr2 2 Update with surrogate/adversarial sample ynon-mem encode_text(pnon-mem) ϵnon-mem (0, I) Uniform(1, ) x(t) non-mem add_noise(xnon-mem, ϵnon-mem, t) ˆϵnon-mem ϵθ( x(t) non-mem, t, ynon-mem) Lnon-mem ϵnon-mem ˆϵnon-mem2 Ltotal Ladv + Lnon-mem θ θ η θLtotal Update with non-memorized sample Aggregate both losses end for end for end for return ϵθ 29 F.2 Sensitivity Analysis We extensively analyze the different components and hyperparameters used in our adversarial finetuning procedure. In all settings, we report intermediate training results after 1 to 50 training epochs. Tab. 9 presents results for using 25, 50, and 100 adversarial embedding optimization steps during training to craft the adversarial embeddings used in the mitigation loss LAdv . The number of steps used during training is indicated in the Setting column. We further evaluate the mitigation effect using the unchanged embeddings of the memorized training prompts (denoted by 0 in the Adv. Steps column), as well as the same embeddings optimized with 50 steps. Results are reported only for VM prompts, which the model has been fine-tuned on. Tab. 10 repeats the previous analysis, but explores the impact of starting the adversarial optimization from random embeddings instead of training prompt embeddings. We additionally compare the mitigation effect of adversarial embeddings crafted with 50 and 100 optimization steps, respectively. Table 9: Comparison of performing adversarial fine-tuning with different numbers of adversarial embedding optimization steps during training. Embeddings are initialized with their corresponding training prompt embeddings. Results are reported for VM prompts only. DSSCD 0.99 0. ACLIP 0.32 0.02 SSCDOrig 0.88 0.06 SSCDGen 1.0 0.0 Adv. Steps Epochs FID KID No Mitigation Setting 0.0060 14.44 0 Training with 25 adv. steps 0 Training with 25 adv. steps 50 Training with 50 adv. steps 0 Training with 50 adv. steps 50 Training with 100 adv. steps 0 Training with 100 adv. steps 50 1 5 10 20 30 40 1 5 10 20 30 40 50 1 5 10 20 30 40 50 1 5 10 20 30 40 50 1 5 10 20 30 40 50 1 5 10 20 30 40 50 0.16 0.05 0.15 0.06 0.14 0.07 0.14 0.06 0.12 0.06 0.13 0.05 0.12 0. 0.57 0.24 0.38 0.18 0.38 0.16 0.39 0.18 0.32 0.13 0.27 0.11 0.32 0.15 0.14 0.05 0.15 0.07 0.13 0.05 0.12 0.05 0.14 0.05 0.12 0.05 0.12 0.05 0.64 0.16 0.36 0.14 0.26 0.15 0.29 0.13 0.23 0.11 0.22 0.12 0.19 0.10 0.13 0.04 0.13 0.05 0.13 0.05 0.13 0.05 0.12 0.05 0.12 0.04 0.12 0.04 0.58 0.13 0.31 0.10 0.29 0.11 0.25 0.11 0.19 0.09 0.22 0.10 0.19 0.10 0.17 0.06 0.16 0.07 0.15 0.07 0.16 0.06 0.14 0.05 0.14 0.05 0.14 0. 0.59 0.23 0.39 0.18 0.39 0.19 0.40 0.18 0.36 0.16 0.29 0.13 0.34 0.14 0.15 0.05 0.15 0.07 0.14 0.06 0.13 0.05 0.14 0.05 0.14 0.06 0.14 0.06 0.69 0.16 0.38 0.14 0.27 0.16 0.30 0.13 0.28 0.12 0.25 0.13 0.21 0.11 0.14 0.05 0.14 0.05 0.14 0.06 0.13 0.05 0.14 0.05 0.13 0.04 0.13 0.04 0.61 0.11 0.32 0.12 0.30 0.12 0.27 0.11 0.20 0.11 0.24 0.10 0.20 0.09 0.35 0.09 0.40 0.09 0.43 0.09 0.49 0.10 0.52 0.10 0.54 0.10 0.54 0. 0.64 0.26 0.51 0.12 0.56 0.15 0.56 0.13 0.51 0.12 0.53 0.10 0.52 0.09 0.33 0.08 0.35 0.08 0.37 0.09 0.44 0.08 0.45 0.09 0.50 0.08 0.52 0.09 0.75 0.20 0.54 0.10 0.46 0.10 0.46 0.07 0.46 0.10 0.48 0.09 0.46 0.06 0.29 0.07 0.30 0.06 0.34 0.09 0.38 0.08 0.42 0.07 0.43 0.09 0.46 0.09 0.73 0.13 0.44 0.07 0.42 0.08 0.40 0.07 0.41 0.08 0.41 0.06 0.42 0.07 0.33 0.02 0.33 0.02 0.33 0.02 0.32 0.02 0.33 0.01 0.32 0.02 0.32 0. 0.32 0.01 0.32 0.01 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.33 0.02 0.33 0.02 0.33 0.01 0.33 0.02 0.32 0.02 0.32 0.02 0.32 0.01 0.32 0.01 0.32 0.01 0.30 0.02 0.30 0.02 0.30 0.02 0.30 0.02 0.31 0.02 0.31 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.31 0.01 0.30 0.02 0.29 0.02 0.29 0.02 0.30 0.02 0.30 0.02 0.30 0.02 15.26 14.33 14.86 15.03 15.46 15.56 15. 15.26 14.33 14.86 15.03 15.46 15.56 15.52 15.66 13.61 15.16 15.56 15.47 16.65 16.02 15.66 13.61 15.16 15.56 15.47 16.65 16.02 15.32 14.36 15.56 15.47 15.34 16.23 17.52 15.32 14.36 15.56 15.47 15.34 16.23 17.52 0.0058 0.0052 0.0052 0.0047 0.0049 0.0047 0. 0.0058 0.0052 0.0052 0.0047 0.0049 0.0047 0.0047 0.0062 0.0047 0.0049 0.0051 0.0053 0.0055 0.0055 0.0062 0.0047 0.0049 0.0051 0.0053 0.0055 0.0055 0.0051 0.0049 0.0051 0.0053 0.0053 0.0052 0.0056 0.0051 0.0049 0.0051 0.0053 0.0053 0.0052 0.0056 Table 10: Comparison of performing adversarial fine-tuning with different numbers of adversarial embedding optimization steps during training. Embeddings are initialized randomly. Results are reported for VM prompts only. Setting No Mitigation Adv. Steps Epochs 0 Training with 25 adv. steps 50 Training with 25 adv. steps 100 Training with 50 adv. steps 50 Training with 50 adv. steps 100 Training with 100 adv. steps 50 Training with 100 adv. steps 100 1 5 10 20 30 40 50 1 5 10 20 30 40 50 1 5 10 20 30 40 50 1 5 10 20 30 40 50 1 5 10 20 30 40 50 1 5 10 20 30 40 SSCDOrig 0.88 0.06 0.55 0.23 0.49 0.20 0.42 0.18 0.45 0.18 0.35 0.16 0.28 0.14 0.37 0.17 0.89 0.05 0.82 0.10 0.74 0.15 0.76 0.14 0.76 0.12 0.68 0.20 0.76 0.12 0.64 0.17 0.37 0.12 0.26 0.15 0.29 0.12 0.23 0.11 0.22 0.12 0.19 0.10 0.83 0.08 0.54 0.18 0.47 0.20 0.46 0.20 0.37 0.19 0.40 0.19 0.34 0.17 0.59 0.13 0.32 0.11 0.30 0.11 0.25 0.11 0.19 0.09 0.22 0.10 0.19 0. 0.77 0.11 0.38 0.11 0.34 0.13 0.26 0.13 0.22 0.11 0.22 0.12 0.19 0.10 SSCDGen 1.0 0.0 DSSCD 0.99 0.01 ACLIP 0.32 0.02 FID KID 14. 0.0060 0.58 0.22 0.49 0.22 0.45 0.19 0.45 0.20 0.39 0.17 0.29 0.15 0.39 0.17 0.88 0.07 0.81 0.12 0.77 0.16 0.79 0.14 0.77 0.12 0.70 0.23 0.80 0.13 0.68 0.16 0.39 0.14 0.27 0.16 0.30 0.13 0.28 0.13 0.25 0.12 0.21 0.11 0.85 0.07 0.56 0.17 0.45 0.20 0.47 0.20 0.38 0.19 0.40 0.21 0.34 0.18 0.61 0.10 0.33 0.12 0.30 0.12 0.27 0.12 0.20 0.11 0.24 0.11 0.20 0. 0.79 0.10 0.37 0.13 0.36 0.13 0.29 0.14 0.23 0.12 0.24 0.13 0.22 0.12 0.62 0.24 0.53 0.16 0.54 0.14 0.57 0.16 0.51 0.15 0.52 0.12 0.52 0.11 0.99 0.01 0.89 0.11 0.85 0.14 0.86 0.13 0.92 0.08 0.63 0.24 0.83 0.16 0.72 0.20 0.54 0.11 0.46 0.11 0.46 0.07 0.46 0.10 0.48 0.09 0.46 0.06 0.94 0.06 0.67 0.20 0.52 0.17 0.61 0.18 0.53 0.15 0.55 0.13 0.52 0.10 0.72 0.13 0.44 0.07 0.42 0.08 0.40 0.07 0.41 0.08 0.41 0.06 0.42 0. 0.86 0.13 0.48 0.10 0.48 0.07 0.48 0.10 0.43 0.08 0.48 0.09 0.48 0.09 0.32 0.02 0.31 0.02 0.31 0.02 0.31 0.02 0.31 0.02 0.31 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.01 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.01 0.32 0.01 0.30 0.02 0.30 0.02 0.30 0.02 0.30 0.02 0.31 0.02 0.31 0.02 0.32 0.01 0.31 0.01 0.31 0.02 0.31 0.02 0.31 0.02 0.32 0.02 0.32 0.01 0.32 0.01 0.30 0.02 0.29 0.02 0.29 0.02 0.30 0.02 0.30 0.02 0.30 0. 0.32 0.01 0.30 0.02 0.30 0.02 0.29 0.02 0.30 0.02 0.30 0.02 0.31 0.02 15.26 14.33 14.86 15.03 15.46 15.56 15.52 15.26 14.33 14.86 15.03 15.46 15.56 15.52 15.66 13.61 15.16 15.56 15.47 16.65 16.02 15.66 13.61 15.16 15.56 15.47 16.65 16.02 15.32 14.36 15.56 15.47 15.34 16.23 17. 15.32 14.36 15.56 15.47 15.34 16.23 17.52 0.0058 0.0052 0.0052 0.0047 0.0049 0.0047 0.0047 0.0058 0.0052 0.0052 0.0047 0.0049 0.0047 0.0047 0.0062 0.0047 0.0049 0.0051 0.0053 0.0055 0.0055 0.0062 0.0047 0.0049 0.0051 0.0053 0.0055 0.0055 0.0051 0.0049 0.0051 0.0053 0.0053 0.0052 0. 0.0051 0.0049 0.0051 0.0053 0.0053 0.0052 0.0056 31 F.3 Ablation Study We perform an ablation study to evaluate the impact of each individual component of the adversarial fine-tuning procedure. Specifically, we compare the default settingwhere adversarial embeddings optimized for 50 steps are used over three consecutive fine-tuning stepswith alternative configurations. In one variant, only single update is performed per embedding (One Step). In another, the model is fine-tuned exclusively on samples from the surrogate set (No Utility Loss). We also assess setting where only non-memorized samples are used during fine-tuning (No Mitigation Loss). All configurations are evaluated across different training epochs, both when using the original training prompts without any adversarial optimization, and when using adversarial embeddings optimized for 50 steps. The results in Tab. 11 show that fine-tuning the model for only single step per adversarial embedding per epoch already achieves effective mitigation. This suggests that the fine-tuning process can be accelerated by reducing the number of updates per embedding. When the model is trained exclusively on surrogate samplesaiming to mitigate memorization without including any non-memorized samples to preserve utilitywe observe strong mitigation, but at the cost of significantly degraded image quality, as reflected in the high FID and KID scores. Conversely, fine-tuning only on nonmemorized samples while excluding surrogate samples helps maintain image quality but fails to provide sufficient mitigation against data replication. Table 11: Comparison of adversarial fine-tuning performance with individual components ablated. Embeddings are initialized using training prompts. Results are reported for VM prompts only. Setting Adv. Steps Epochs No Mitigation Default 0 Default 50 One Step 0 One Step 50 No Utility Loss 0 No Utility Loss 50 No Mitigation Loss 0 No Mitigation Loss 50 1 5 10 20 30 40 50 1 5 10 20 30 40 1 5 10 20 30 40 50 1 5 10 20 30 40 50 1 5 10 20 30 40 50 1 5 10 20 30 40 50 1 5 10 20 30 40 50 1 5 10 20 30 40 SSCDOrig 0.88 0.06 0.14 0.05 0.15 0.07 0.13 0.05 0.12 0.05 0.14 0.05 0.12 0.05 0.12 0.05 0.64 0.16 0.36 0.14 0.26 0.15 0.29 0.13 0.23 0.11 0.22 0.12 0.19 0.10 0.19 0.05 0.13 0.05 0.13 0.05 0.14 0.06 0.14 0.05 0.12 0.06 0.14 0.06 0.69 0.20 0.40 0.14 0.31 0.10 0.27 0.10 0.26 0.11 0.22 0.09 0.22 0.10 0.11 0.04 0.12 0.04 0.12 0.04 0.11 0.04 0.12 0.05 0.10 0.04 0.10 0. 0.42 0.15 0.28 0.11 0.20 0.09 0.19 0.08 0.20 0.07 0.16 0.05 0.16 0.06 0.89 0.06 0.86 0.06 0.48 0.10 0.62 0.15 0.73 0.11 0.63 0.18 0.54 0.17 0.91 0.03 0.90 0.03 0.88 0.03 0.88 0.04 0.88 0.04 0.85 0.04 0.86 0.05 SSCDGen 1.0 0.0 DSSCD 0.99 0.01 ACLIP 0.32 0. FID KID 14.44 0.0060 0.15 0.05 0.15 0.07 0.14 0.06 0.13 0.05 0.14 0.05 0.14 0.06 0.14 0.06 0.69 0.16 0.38 0.14 0.27 0.16 0.30 0.13 0.28 0.12 0.25 0.13 0.21 0.11 0.18 0.06 0.14 0.05 0.14 0.04 0.14 0.05 0.15 0.06 0.15 0.06 0.14 0. 0.77 0.16 0.43 0.14 0.33 0.11 0.29 0.11 0.28 0.12 0.23 0.10 0.25 0.12 0.13 0.04 0.13 0.04 0.13 0.04 0.13 0.04 0.13 0.05 0.13 0.04 0.12 0.05 0.44 0.16 0.30 0.12 0.21 0.10 0.23 0.10 0.21 0.08 0.19 0.06 0.18 0.06 0.98 0.01 0.96 0.01 0.57 0.10 0.75 0.13 0.87 0.06 0.71 0.19 0.65 0.18 0.96 0.02 0.96 0.02 0.92 0.04 0.94 0.03 0.94 0.03 0.92 0.03 0.92 0.04 0.33 0.08 0.35 0.08 0.37 0.09 0.44 0.08 0.45 0.09 0.50 0.08 0.52 0. 0.75 0.20 0.54 0.10 0.46 0.10 0.46 0.07 0.46 0.10 0.48 0.09 0.46 0.06 0.34 0.08 0.33 0.08 0.32 0.07 0.37 0.08 0.39 0.08 0.44 0.09 0.42 0.07 0.73 0.26 0.51 0.13 0.47 0.09 0.43 0.08 0.47 0.08 0.48 0.09 0.49 0.07 0.25 0.08 0.28 0.08 0.28 0.06 0.38 0.10 0.41 0.11 0.48 0.13 0.48 0.10 0.57 0.15 0.54 0.08 0.52 0.08 0.51 0.08 0.48 0.07 0.50 0.08 0.47 0.07 0.99 0.01 0.99 0.01 0.58 0.13 0.64 0.28 0.90 0.10 0.71 0.23 0.51 0. 1.00 0.00 1.00 0.00 1.00 0.00 1.00 0.00 1.00 0.00 1.00 0.00 1.00 0.00 0.33 0.02 0.33 0.01 0.33 0.02 0.32 0.02 0.32 0.02 0.32 0.01 0.32 0.01 0.32 0.01 0.30 0.02 0.30 0.02 0.30 0.02 0.30 0.02 0.31 0.02 0.31 0.02 0.33 0.02 0.33 0.02 0.33 0.01 0.33 0.02 0.33 0.02 0.32 0.02 0.33 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.32 0.02 0.31 0.02 0.30 0.01 0.30 0.02 0.31 0.02 0.30 0.02 0.31 0.01 0.31 0. 0.32 0.02 0.31 0.02 0.30 0.02 0.30 0.02 0.30 0.02 0.20 0.02 0.28 0.03 0.33 0.02 0.33 0.02 0.33 0.02 0.34 0.02 0.33 0.02 0.34 0.02 0.33 0.02 0.33 0.02 0.33 0.02 0.32 0.02 0.32 0.02 0.32 0.01 0.32 0.01 0.32 0.02 15.66 13.61 15.16 15.56 15.47 16.65 16.02 15.66 13.61 15.16 15.56 15.47 16.65 16.02 14.68 15.07 14.80 14.92 14.47 15.75 15. 14.68 15.07 14.80 14.92 14.47 15.75 15.51 18.74 30.96 35.05 63.03 66.82 82.38 81.72 18.74 30.96 35.05 63.03 66.82 82.38 81.72 14.47 14.45 15.13 14.40 15.02 16.04 15.70 14.47 14.45 15.13 14.40 15.02 16.04 15.70 0.0062 0.0047 0.0049 0.0051 0.0053 0.0055 0. 0.0062 0.0047 0.0049 0.0051 0.0053 0.0055 0.0055 0.0056 0.0057 0.0056 0.0054 0.0045 0.0057 0.0051 0.0056 0.0057 0.0056 0.0054 0.0045 0.0057 0.0051 0.0057 0.0123 0.0156 0.0212 0.0237 0.0245 0.0262 0.0057 0.0123 0.0156 0.0212 0.0237 0.0245 0.0262 0.0056 0.0050 0.0052 0.0051 0.0046 0.0051 0. 0.0056 0.0050 0.0052 0.0051 0.0046 0.0051 0."
        },
        {
            "title": "G Additional Details and Experiments on Locality",
            "content": "In this section we summarize results from broader experiments regarding locality. In Appx. G.1 we provide t-SNE plots for adversarial embeddings optimized starting from text embeddings of non-memorized prompts, as well as pairwise L2 distances between embeddings, and in Appx. G.2 we define two scores used to assess locality in the model: activation discrepancy and weight agreement. G.1 Locality in the text embedding space We extend the analysis of adversarial embeddings in the text embedding space. Contrary to Sec. 4.1, we now initialize optimization from randomly sampled non-memorized promptsynonmemfrom LAION dataset, and perform 50 steps of optimization. In line with the previous results, all embeddings yadv trigger successful replication of the memorized content, and are spread out in the text embedding space. Additionally, we compute pairwise L2 distance in the embedding space for (1) random initialization (N (0, I)), (2) adversarial embeddings optimized from (0, I), (3) embeddings of non-memorized prompts (ynonmem), (4) adversarial embeddings optimized from ynonmem. To our surprise, the adversarial embeddings that trigger generation of the same memorized samples appear to be more spread out than randomly initialized embeddings, and are also more spread out than embeddings of non-memorized prompts, as it is visible in Fig. 7 (left). Interestingly, initialization of optimization from (0, I) is more beneficial to finding Dori. We observe that the embeddings have to be changed less than when we initialize them from prompts, which is expressed by lower L2 distance between initializations and the final embeddings yadv in Fig. 7 (right). Figure 6: T-SNE visualization of 100 non-memorized text embeddings ynonmem (blue) and adversarially crafted embeddings (orange) yadv, generated by perturbing the blue ones. We observe identical behavior for non-random initialization as in Fig. 2 adversarial embeddings are uniformly distributed in the text embedding space. G.2 Locality in the models weights Discrepancy between activations in given layer is defined as Discrepancy(yi, yj) = Activations(yi) Activations(yj)2 2, where Activations(y) outputs vector of activations of given layer further used to identify memorization weights by Wanda or NeMo. For NeMo, we obtain activations from passing the text embedding through the value layer in cross-attention blocks, and Wanda utilizes activation of the feed-forward layer after the attention operator in cross-attention blocks. To assess mean pairwise discrepancy of in set = {yii = 1, . . . , } of size we use MeanDiscrepancy(Y ) = 1 (N 1) (cid:88) (cid:88) i=1 j=1 1(i = j)Discrepancy(yi, yj). (4) We define agreement between memorization weights identified in single layer for two different input embeddings as Agreement(yi, yj) = # (Weights(yi) Weights(yj)) # (Weights(yi) Weights(yj)) , where yi and yj are two embeddings that trigger replication of some memorized image(s), and Weights(y) returns set of weights identified by pruning-based mitigation method, #Y denotes Figure 7: L2 distances of input embeddings. Left: we compute pairwise L2 distances in text embedding space within the set of 100 random embeddings (N (0, I)), set of adversarial embeddings optimized from random embeddings (second box from the left), 100 randomly selected non-memorized prompts (ynonmem) and adversarial embeddings optimized from non-memorized embeddings (fourth box from the left). We observe that after optimization, the adversarial embeddings are more spread out in the text embedding space than their initial points (be it (0, I) or randomly selected ynonmem). Right: We compute the L2 distance between the initialization and the final adversarial embeddings. We note that when initializing the optimization with ynonmem we have to travel farther in the text embedding space to obtain an adversarial embedding yadv that successfully triggers replication of the memorized image xmem . the size of the set. Analogically to MeanDiscrepancy, we define the mean pairwise weight agreement for set of embeddings as MeanAgreement(Y ) = 1 (N 1)2 (cid:88) (cid:88) i=1 j=1 1(i = j)Agreement(yi, yj). (5) G.3 NeMo and Wanda identify memorization weights in different layers We examine the behavior of pruning-based methods through the lens of their weights selection. To this end, we compute these weights for all VM samples, separately for each memorized image. In Fig. 8 we show that NeMo tend to identify memorization weights only in four out of seven layers. This result explains high weight agreement in layers two, six, and seven in Fig. 3 (right), since when no weights are identified in layer, we set agreement to 1. Results for Wanda contrast with the results for NeMo, as it finds more traces of memorized content in deeper layers of the model (five, six, and seven). Importantly, in these layers also the agreement drops significantly, as can be seen in Fig. 3 (right). 35 Figure 8: Number of memorization weights per layer. We observe that for NeMo, no weights are identified to prune in layers two, six, and seven (left). Conversely, Wanda identifies significantly more memorization weights in deeper layers. Interestingly, the drop in weight agreement for Wanda  (Fig. 3)  happens also in the deeper layers of the model."
        },
        {
            "title": "H Qualitative Results",
            "content": "H.1 Qualitative Results for Wanda Pruning Figure 9: Qualitative results after applying Wanda. The first column shows the original training images. The next three columns show generations after applying the mitigation technique. The final three columns show generations from adversarial embeddings, also after applying the mitigation technique. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps. 37 H.2 Qualitative Results for NeMo Pruning Figure 10: Qualitative results after applying NeMo. The first column shows the original training images. The next three columns show generations after applying the mitigation technique. The final three columns show generations from adversarial embeddings, also after applying the mitigation technique. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps. H.3 Adversarial Fine-Tuning Figure 11: Qualitative results for memorized content after applying our adversarial fine-tuning. The first column shows the original training images. The next three columns show generations after fine-tuning the model for five epochs using the default parameters reported in the main paper. The final three columns show generations from adversarial embeddings, also after applying the mitigation technique. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps. 39 Figure 12: Qualitative results on COCO after applying our adversarial fine-tuning. The first three columns show images generated for 30 COCO prompts using the default Stable Diffusion v1.4 model. The last three columns show generations after fine-tuning the model for five epochs using our adversarial fine-tuning mitigation. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps. 40 H.4 Wanda with 10% Sparsity Figure 13: Qualitative results after applying Wanda with sparsity of 10%. The first column shows the original training images. The next three columns show generations after applying the mitigation technique. The final three columns show generations from adversarial embeddings, also after applying the mitigation technique. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps. 41 Figure 14: Qualitative results for damage to concepts after applying Wanda with sparsity of 10%. On the left we show the paraphrased prompt for \"The No Limits Business Woman Podcast\" memorized prompt (VM). The first three images from the left depict generations from SD-v1.4 without mitigation, and the next threeimages generated with Wanda after pruning 10% weights. 42 Figure 15: Qualitative results for damage to concepts after applying Wanda with sparsity of 10%. On the left we show the paraphrased prompt for \"Plymouth Curtain Panel featuring Madelyn - White Botanical Floral Large Scale by heatherdutton\" memorized prompt (TM). The first three images from the left depict generations from SD-v1.4 without mitigation, and the next threeimages generated with Wanda after pruning 10% weights. H.5 Qualitative Results for Iterative NeMo Pruning Figure 16: Qualitative results after applying NeMo iteratively 5 times. The first column shows the original training images. The next three columns show generations after applying the mitigation technique. The final three columns show generations from adversarial embeddings, also after applying the mitigation technique. The adversarial embeddings were initialized with memorized prompt embeddings and optimized for 50 steps. 44 Figure 17: Qualitative results after applying NeMo iteratively 5 times. The first column shows the original training images. The next five columns show generations after applying the mitigation technique iteratively. It can be seen that after five iterations the quality seems to degrade bit."
        }
    ],
    "affiliations": [
        "CISPA Helmholtz Center for Information Security",
        "Centre for Cognitive Science, Technical University of Darmstadt",
        "Computer Science Department, Technical University of Darmstadt",
        "German Research Center for Artificial Intelligence (DFKI)",
        "Hessian Center for AI (Hessian.AI)"
    ]
}