{
    "paper_title": "RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild",
    "authors": [
        "Jingkai Zhou",
        "Yifan Wu",
        "Shikai Li",
        "Min Wei",
        "Chao Fan",
        "Weihua Chen",
        "Wei Jiang",
        "Fan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Controllable character animation remains a challenging problem, particularly in handling rare poses, stylized characters, character-object interactions, complex illumination, and dynamic scenes. To tackle these issues, prior work has largely focused on injecting pose and appearance guidance via elaborate bypass networks, but often struggles to generalize to open-world scenarios. In this paper, we propose a new perspective that, as long as the foundation model is powerful enough, straightforward model modifications with flexible fine-tuning strategies can largely address the above challenges, taking a step towards controllable character animation in the wild. Specifically, we introduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our sufficient analysis reveals that the widely adopted Reference Net design is suboptimal for large-scale DiT models. Instead, we demonstrate that minimal modifications to the foundation model architecture yield a surprisingly strong baseline. We further propose the low-noise warmup and \"large batches and small iterations\" strategies to accelerate model convergence during fine-tuning while maximally preserving the priors of the foundation model. In addition, we introduce a new test dataset that captures diverse real-world challenges, complementing existing benchmarks such as TikTok dataset and UBC fashion video dataset, to comprehensively evaluate the proposed method. Extensive experiments show that RealisDance-DiT outperforms existing methods by a large margin."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 7 7 9 4 1 . 4 0 5 2 : r RealisDance-DiT: Simple yet Strong Baseline towards Controllable Character Animation in the Wild Jingkai Zhou1,2,3, Yifan Wu4, Shikai Li1,3, Min Wei1,3, Chao Fan5, Weihua Chen1,3, Wei Jiang2, Fan Wang1,3 1DAMO Academy, Alibaba Group, 2Zhejiang University, 3Hupan Lab, 4Southern University of Science and Technology, 5Shenzhen University Equal contribution. Corresponding author. zhoujingkai.zjk@alibaba-inc.com, kugang.cwh@alibaba-inc.com Controllable character animation remains challenging problem, particularly in handling rare poses, stylized characters, character-object interactions, complex illumination, and dynamic scenes. To tackle these issues, prior work has largely focused on injecting pose and appearance guidance via elaborate bypass networks, but often struggles to generalize to open-world scenarios. In this paper, we propose new perspective that, as long as the foundation model is powerful enough, straightforward model modifications with flexible fine-tuning strategies can largely address the above challenges, taking step towards controllable character animation in the wild. Specifically, we introduce RealisDance-DiT, built upon the Wan-2.1 video foundation model. Our sufficient analysis reveals that the widely adopted Reference Net design is suboptimal for large-scale DiT models. Instead, we demonstrate that minimal modifications to the foundation model architecture yield surprisingly strong baseline. We further propose the low-noise warmup and large batches and small iterations strategies to accelerate model convergence during fine-tuning while maximally preserving the priors of the foundation model. In addition, we introduce new test dataset that captures diverse real-world challenges, complementing existing benchmarks such as TikTok dataset and UBC fashion video dataset, to comprehensively evaluate the proposed method. Extensive experiments show that RealisDance-DiT outperforms existing methods by large margin. The project page is at this linka. Date: April 22, 2025 ahttps://thefoxofsky.github.io/project_pages/RealisDance-DiT/index"
        },
        {
            "title": "1 Introduction",
            "content": "Controllable character animation can be widely applied in film production, virtual digital humans, and e-commerce promotions. This task has recently garnered significant attention from both academia [27, 30, 9, 10, 36, 18, 24, 28] and industry [25, 1, 35, 2], due to advances in generative models and increasing demand for personalized content creation. Existing methods [9, 30, 10, 36, 18, 24, 28] employ the Reference Net to inject the reference character ID, achieving significant advances in character consistency. However, their performance in open scenes remains unsatisfactory, as they struggle to address challenges such as rare poses, stylized characters, interactions between characters and objects, complex lighting conditions, and scene changes. See Figure 2 for example. Existing methods struggle with complex lighting conditions and generate face artifact in the silhouette frame. Also, it is difficult for existing methods to produce character-object interactions, the generated results leave dumbbells suspended in the air when the woman squats down. In cases of rare poses, existing methods tend to introduce artifacts in body parts where the model lacks adequate understanding. Moreover, when facing stylized characters, existing methods tend to generate incorrect body parts, such as producing realistic face for the comic character. Intuitively, all of these issues can be attributed to three possible reasons: 1) the Reference Net is not well designed and not robust enough, 2) the main model is not powerful enough to handle these challenges, and 3) the overall fine-tuning is insufficient in terms of both data and iterations. In this paper, we propose new perspective that, as long as the foundation model is powerful enough, 1 Figure 1 Results of RealisDance-DiT. Left: Frames generated by RealisDance-DiT. Right: Evaluation on the proposed RealisDance-Val dataset using VBench-I2V metrics. Zoom in for better visibility. Please refer to the project page for more videos. simple model modifications with flexible fine-tuning strategies can unlock the potential for controllable character animation in the wild. Specifically, we propose RealisDance-DiT, which is built upon the strong video foundation model Wan-2.1 [29]. We only make few simple adjustments to Wan-2.1, including adding condition input layers and modifying the RoPE position encoding, which yields superior baseline. Our analysis reveals that applying Reference Net does not help with such large DiT foundation models, and even brings negative effects beyond heavy additional overhead. This is because the existing large video foundation model itself possesses the capability to achieve controllable character animation in the wild. The key lies in guiding the model to unlock this capability, rather than adding additional complex structures. Given that we utilize powerful video foundation model, the fine-tuning strategies should prioritize preserving prior knowledge within the model. Therefore, we propose two flexible and effective fine-tuning strategies to accelerate the convergence process while maximally preserving the priors in the foundation model. The first one is the low-noise warmup strategy. Our experiments demonstrate that reducing the amount of noise added during the early stages of finetuning can speed up convergence. Samples with low added noise are easier for the model to process than those with high added noise. Using simpler samples for warm-up fine-tuning helps stabilize the adaptation to the new task, rather than starting with difficult samples that may push the model away from the initial local optimum established during pre-training. The second strategy is called large batches and small iterations. We suggest distributing the data in larger batches and fewer iterations. Larger batch sizes enable the model to benefit from more informative yet smooth gradients per update, allowing it to focus on important factors in the downstream task, rather than being hindered by noise in the data. Fewer iterations help the model to keep the pre-trained priors, reducing the risk of overfitting on downstream datasets. Together, these two strategies facilitate faster convergence while preserving rich priors, which is crucial for fine-tuning powerful video foundation models. In addition, we curated test dataset named RealisDance-Val, comprising 100 videos with corresponding conditions. This dataset features diverse and challenging scenarios, including rare poses, stylized characters, dynamic scenes, complex lighting conditions, and interactions between characters and objects. It is specifically designed to evaluate the performance of generative models in open scenes. The proposed method is evaluated on the TikTok dataset [12], UBC fashion video dataset [32], and RealisDanceVal datasets. Experimental results demonstrate that RealisDance-DiT performs favorably against existing methods. Figure 1 exhibits several generated results of RealisDance-DiT. In short, we make the following contributions: Challenged the traditional view that requires heavy reference networks to inject character ID. Instead, we emphasize the powerful video foundation model that naturally processes the capability of character consistency. We only need to bring it out via straightforward modifications. Proposed two flexible fine-tuning strategies to accelerate convergence while maximally preserving 2 method tends to fail when dealing with stylized characters, complex gestures, and large camera motions. Animate-X [24] introduces pose indicator approach tailored to stylized characters. RealisDance [35] integrates the 3D hand condition (HaMeR [17]) to improve hand fidelity. HumanVid [28] attempts to address camera motions in realistic settings. Despite these advances, existing methods perform unsatisfactorily in open scenes, especially when encountering rare poses, complex lighting conditions, characterobject interactions, and scene changes. We point out that this is attributed to the weak image-tovideo main network used by existing methods. There is an urgent demand for applying powerful native video foundational models to handle challenges in open scenes. We note that several concurrent studies [15, 5, 13] are exploring this direction. We go beyond them by proposing simple yet effective model modifications and two practical fine-tuning strategies. The proposed RealisDance-DiT is structurally simple, empirically robust, and experimentally excellent baseline, which pushes the boundaries of controllable animation in the wild."
        },
        {
            "title": "3.1 Simple model modifications",
            "content": "We build RealisDance-DiT based on Wan-2.1. Figure 3 shows several architecture modifications we explored. Initially, we attempted to directly transfer Reference Net to Wan-2.1. However, this results in an overly large network that is difficult to finetune given limited GPU resources. We had to remove some blocks from the Reference Net to reduce the fine-tunable parameters. However, the downscaled Reference Net converges slowly and yields only mediocre performance. Then, we experimented with directly concatenating the reference latent to the noise latent and fine-tuning the entire network. Surprisingly, this simple design converges much quicker, and the fine-tuned model adapts to the downstream controllable character animation tasks excellently. This indicates that the large native video foundation model inherently possesses the capability to generalize to downstream tasks. The key is not to modify the model architecture but to bring out its inherent abilities. Based on this goal, we further try to only fine-tune the newly introduced condition patchifiers, the zero projection layer, and the self-attention blocks. Compared to full fine-tuning, fine-tuning with fewer parameters does not slow down convergence or degrade the final performance, which confirms our hypothesis. Figure 2 Failure cases of existing methods. Existing methods sometimes generate face in the silhouette frame, leave dumbbells suspended in the air when the woman squats down, generate artifacts when producing the yoga pose, and generate realistic face for the comic character. the built-in priors of the powerful video foundation model. Collected an open-scene test dataset and provided the field with structurally simple, empirically robust, and experimentally strong baseline model, which is expected to inspire future work."
        },
        {
            "title": "2 Related work",
            "content": "Controllable character animation has drawn significant attention since the GAN-based methods [21, 22, 6, 3]. Recently, with the advantage of Diffusion models, several methods [27, 9, 36, 24] have made considerable progress in photorealistic generation. Specifically, DisCo [27], an early diffusion-based method, leverages ControlNet [33] to incorporate both background and pose guidance and adopts the motion module to enhance cross-frame consistency, enabling animation generation from static reference images. However, it injects character ID through the CLIP [19] feature, which loses lots of detailed information. As result, DisCo struggles to maintain character consistency. Animate Anyone [9] injects character ID via the UNet-based Reference Net. While this effectively preserves identity in many cases, the 3 (a) Original Wan-2.1 (b) Reference Net variant (c) Full fine-tune (d) Self-attn fine-tune Figure 3 Illustration of architecture modifications and fine-tunable model parameters. The proposed RealisDance-DiT is fine-tuned under the final setting. There are several important details to highlight. We use the same three pose conditions as those used in RealisDance [35], i.e., HaMeR [17], DWPose [31], and SMPL-CS [35]. All pose conditions and the reference image are encoded using the original Wan-2.1 VAE. The encoded pose latents are concatenated along the channel dimension. Then, the concatenated pose latent and the reference latent are fed into the pose and reference patchifiers, respectively. The pose patchifier is initialized randomly, while the reference patchifier is initialized using the weights from the noise patchifier. Finally, the pose latent is added to the noise latent, and the reference latent is concatenated with the noise latent along the sequence length, before being sent to the subsequent DiT blocks. We also replace the Rotary Position Embedding (RoPE) [23] used in self-attention with the shifted RoPE. As illustrated in Figure 4, the reference latent does not share RoPE with the noise latent. It employs the spatially shifted RoPE at the first frame, where the shifting is according to the height and width of the noise latent. Furthermore, our simple modifications can be seamlessly extended to the Wan-2.1 I2V model. In fact, the final version of RealisDance-DiT is developed based on Wan-2.1 I2V, where we omit the first frame, apply an all-zero mask to the noise latent, and input the reference image to the CLIP model instead of the first frame."
        },
        {
            "title": "3.2 Fine-tuning strategies",
            "content": "We further propose the low-noise warmup and large batches and small iterations strategies for fast convergence while preserving rich priors in Wan-2.1. Low-noise warmup strategy. The timestep sampling strategy is critical for the training stability and final performance of the diffusion model. Although several methods [4, 26, 7, 14] have explored various types of sampling distributions, such as uniform distribution and logit-normal distribution, all of them utilize fixed distribution throughout the entire training / fine-tuning period. We argue that the timestep sampling distribution should be dynamic to fit different phases of fine-tuning. For example, at the beginning of the fine-tuning process, it is ideal to sample more small timesteps to reduce the level of added noise. Samples with lower added noise are easier for the model to process, thus helping to stabilize finetuning in the early stages. During the middle stage of fine-tuning, the probability of sampling intermediate or larger timesteps should gradually increase, as this helps the model adapt to various timesteps in the downstream task. Based on this idea, we propose the low-noise warmup strategy, which is implemented by dynamic sampling distribution. Given the widely used uniform sampling strategy, its probability density function can be expressed in the following formula (x) = (cid:40) 1 Tmax if 0 Tmax otherwise , (1) where Tmax is the maximum of the timestep, which is Figure 5 Illustration of low-noise warmup strategy. details. We also observed that, with more iterations, the controllability is slightly enhanced, but the diversity of generated results is greatly reduced, and the frequency of artifacts in the generated results increases. This means that the prior gradually disappears with the increase of fine-tuning iterations, while the prior of the foundation model is crucial for controllable character animation in open scenes."
        },
        {
            "title": "3.3 Inference strategies",
            "content": "During fine-tuning, we randomly drop reference images and text prompts at rate of 5%, allowing the model to set the CFG [8] scale to 2 at inference. To handle reference characters with diverse body shapes, we also employ an optimization-based approach to refine and replace shape parameters β of the SMPLX model [16]. Starting with an initial estimate of βinit and pose obtained by GVHMR [20], we utilize SMPLify-X to optimize β guided by estimated 2D keypoints and human silhouettes. The objective is to minimize the discrepancy between the projected 3D silhouette and the reference silhouette while maintaining accurate alignment of the 2D keypoints. Finally, the refined shape parameters are used to replace the default shape parameters of the driving pose, enabling shape alignment between the reference characters and the pose sequence."
        },
        {
            "title": "4 Experiments",
            "content": "We compared RealisDance-DiT with several open source methods, including MooreAA [1], AnimateX [24], ControlNeXt [18], MimicMotion [34], and MusePose [25]. The concurrent work OmniHuman [15] and HumanDiT [5] have not yet released their code and models, so we cannot make comparisons. The concurrent work VACE [13] made its code public few days before submission, which does not give us enough time to evaluate their methods. are Experiments conducted on the TikTok dataset [12], UBC fashion video dataset [32], and RealisDance-Val datasets, where RealisDance-Val contains 100 videos collected from the Internet, covering various characters, rare poses, lighting conditions, and character-object interactions. For TikTok dataset and UBC fashion video dataset, scenes, Figure 4 Illustration of spatially shifted RoPE for the reference latent. typically set to 1000. The proposed low-noise warmup strategy introduces an iteration-relevant component to the above probability density function, facilitating dynamic timestep sampling. Its probability density function can be expressed as below (cid:40) (x) = (i)x + Tmax 0 2 (i) + 1 Tmax if 0 Tmax otherwise , (2) where (i) can be any function related to fine-tuning iteration i, with value range in [0, ]. Here we simply use linear function, for example, which can be expressed as 2 2 max (i) = (cid:40) α 2(τ i) τ 2 max if 0 τ otherwise , (3) where α [0, 1] is hyperparameter that controls the maximum value of (i), τ notes the threshold of maximum warmup step. Figure 5 illustrates the lownoise warmup strategy. When iteration is smaller than the maximum warmup threshold τ , there is greater probability of sampling small timesteps. As the iteration increases, the probability of sampling large timesteps rises. Once exceeds τ , the sampling distribution degrades to uniform sampling. Large batches and small iterations. We recommend fine-tuning the model with larger batches and fewer iterations. This strategy has two benefits for finetuning. On the one hand, by utilizing larger batch size, the model can be updated with more informative gradients, allowing it to focus on key factors relevant to the downstream task, without being affected by noise in the data. Therefore, the model converges faster and more stably. On the other hand, fewer iterations reduce the risk of overfitting the downstream dataset. We found that after the model adapted key factors relevant to the downstream task, it began to fit inductive biases in the downstream data. In other words, as fine-tuning continues, the loss reduction comes more from the model learning the inductive biases in the downstream data, such as high-frequency 5 Figure 6 Visualization of frames generated by RealisDance-DiT. The images with orange borders are reference images. Zoom in for better visibility. Please refer to the project page for more videos. we follow the evaluation settings in HumanVid [28]. Specifically, the method predicts frames within the range [1,72) with stride of 3, obtains sequence of 24 frames. The reference image is selected as the middle frame within the prediction range. The prediction resolution is set to 512896. For RealisDance-Val, the method predicts the first 5 seconds of the video at resolution of 5761024 or 1024576 according to the original aspect ratio. We manually select the most informative frame as the reference image. We train two versions of RealisDance-DiT: one for ablation experiments and the other as the final version. The RealisDance-DiT used for the ablation experiments is based on the Wan-2.1 T2V 1.3B model and trained on dataset containing 204.8k high-quality videos bought from vendors. The final version of RealisDance-DiT is based on the Wan-2.1 I2V 14B model and trained on dataset comprising 1M highquality videos bought from vendors. Both training datasets exclude the three test datasets mentioned above to ensure that no test data is used during training. During training, we employ AdamW as the optimizer and set the learning rate to 1e-5."
        },
        {
            "title": "4.1 Overall comparisons",
            "content": "Qualitative evaluation and comparisons. Figure 6 visualizes several video frames generated by RealisDanceDiT. As can be seen from the first two rows of the figure, RealisDance-DiT effectively handles the interactions between characters and objects. For example, in the first row, the paddle moves naturally as the character rows the boat, and in the second row, the broom moves naturally as the character sweeps the floor. The third row shows the generation capabilities of RealisDance-DiT in complex lighting scenarios, where it accurately renders light and shadow on the character according to physical principles. The fourth row evaluates the performance of RealisDanceDiT under rare poses. When the pose estimation is accurate, the model generates smooth, physically consistent videos. The fifth row demonstrates that RealisDance-DiT generalizes to characters of various body shapes and styles. Finally, the last row shows the potential of RealiDance-DiT for application in multiple character scenarios. Figure 7 qualitatively compares RealisDance-DiT In the first with existing methods and products. 6 Figure 7 Qualitative comparisons between RealisDance-DiT and other methods. Zoom in for better visibility. Please refer to the project page for more videos. row of Figure 7, the reference image depicts man playing basketball. ControlNeXt [18] effectively generates the mans movements, but the basketball disappears from the scene. In contrast, RealisDance-DiT not only successfully generates the basketball, but also ensures that the basketball bounces realistically after landing during dribbling. In the second row, Animate-X [24] fails to generate legs when facing complex yoga poses. In the third row, when dealing with animated characters, the human body parts generated by MooreAA exhibit numerous artifacts. For instance, in the third row, the model fails to accurately represent what the arms should look like in different poses. In contrast, the proposed method generalizes well to anime characters, thanks to the priors inherited from Wan-2.1. In the fourth row, MimicMotion fails to generate the dynamic scene caused by camera motion, while RealisDance-DiT can effectively handle camera motion and maintain detail consistency with the reference image. In the fifth row, when addressing complex lighting conditions, the shadows produced by MusePose do not align with the poses of the character. In contrast, RealisDance-DiT generates more accurate shadows that correspond to the dance movements. In the last row, we compare RealisDance-DiT with commercial product from ViggleAI. As can be seen, the proposed method effectively handles the interaction between the character and the dog when the character spins. However, in the results generated by ViggleAI, the dog is always suspended in the air, and the character disappears when she turns back. Quantitative comparisons. For all quantitative evaluations, we select the same frames across different methods to calculate FID and FVD, ensuring fairness in the comparisons. Table 2 shows the quantitative evaluation on the TikTok dataset. It is worth noting that, as we follow the setting in HumanVid [5], we use the last 40 videos from the TikTok dataset out of total of 340 videos as the test set. Therefore, for several methods like MooreAA, these data could be included in their training data. Even under such an unfair comparison, RealisDance-DiT achieves the best FVD and FID values among the compared methods. Table 3 presents the comparison results on the UBC fashion video dataset. Since the UBC fashion video dataset is not utilized as training data by any other methods, this comparison is more equitable. RealisDance-DiT achieves competitive results, ranking either first or second across all evaluation metrics. The above two datasets are too naïve to effectively evaluate the model capabilities in real-world scenarios. Therefore, we have collected new test dataset, RealisDance-Val, to evaluate existing methods. Unlike the previous settings, we do not use low-level metrics such as SSIM, PSNR, and LPIPS, because our test set encompasses wider range of open scenes, where the background of some test videos may change. Consequently, the model must generate new content 7 Method Animate-X ControlNeXt MimicMotion MooreAA MusePose I2V Temporal Motion Dynamic Aesthetic Subject BG Consist Consist Flicker Smooth Degree Quality I2V Subject BG FVD FID 96.06 92.91 92.79 92.33 92.24 96.59 93.92 93.80 93.35 93. 93.83 91.41 91.10 93.12 93.88 94.83 93.57 93.20 93.77 94.88 97.40 96.91 96.78 95.20 97.88 97.76 98.52 98.05 98.20 96.74 98.57 98. 53 63 59 68 57 66 55.22 55.57 53.31 56.08 56.28 855.87 34.32 810.82 37.24 783.55 40.19 867.48 35.50 1049.06 42.02 57.93 563.28 24. RealisDance-DiT 95.97 96.57 93.91 95.83 Table 1 Quantitative Results on the RealisDance-Val. RealisDance-DiT ranks either first or second across all evaluation metrics. Especially for FVD and FID, RealisDance-DiT outperforms all compared methods by large margin. Method SSIM PSNR LPIPS FVD FID Ref. Net Light Ref. Net Full Ft. Part Ft. Animate-X ControlNeXt MimicMotion MooreAA MusePose 0.2854 508.63 32.77 0.7427 16.71 0.7282 16.31 0.2958 548.01 33.48 0.7507 19.30 0.2203 472.51 34.88 0.2296 501.22 37.28 0.7636 18.62 0.2484 532.75 41.99 0.7566 18.20 RealisDance-DiT 0.7170 17.55 0.2613 458.81 30. Table 2 Quantitative Results on the TikTok dataset. Method SSIM PSNR LPIPS FVD FID Animate-X ControlNeXt MimicMotion MooreAA MusePose 0.0691 70.47 10.11 0.8931 22.15 0.8530 18.48 0.1320 143.02 13.82 0.9126 23.80 0.0605 80.89 15.40 0.0929 149.66 21.74 0.8795 20.83 0.0665 96.17 14.95 0.8955 22.20 RealisDance-DiT 0.9083 23.33 0.0526 72.94 10. Table 3 Quantitative Results on the UBC fashion video dataset. based on the background of the reference images. However, since the background occupies the majority of the frame, the difference between the newly generated background and the ground truth will result in low values for those low-level metrics, even if the new content is very realistic. Therefore, we utilize Vbench-I2V [11] as the metric for the RealisDanceVal dataset. Table 1 exhibits the comparison results on the RealisDance-Val dataset. RealisDance-DiT ranks either first or second across all evaluation metrics. Especially for FVD and FID, RealisDance-DiT outperforms all compared methods by large margin."
        },
        {
            "title": "4.2 Ablation studies",
            "content": "Model designs. We conducted ablation experiments to investigate the architectural designs and fine-tunable parameters of RealisDance-DiT, which is built on the Wan-2.1 I2V 14B model. Table 4 compares four settings: Reference Net variant, lightweight Reference Net variant, simple modifications with full OOM FID FVD OOM 31.01 678.98 25.58 519.22 24.79 563.28 Table 4 Comparisons between different fine-tuning designs. fine-tuning, and simple modifications with part finetuning. The Reference Net variant is too heavy to be fine-tuned using limited GPU resources. Therefore, we pruned some blocks from the Reference Net. Specifically, each block in the Reference Net corresponds to every five blocks in the main network. We observed that such lightweight Reference Net variant converges slowly, even with extra fine-tunable parameters, which achieves only 31.01 in FID and 678.98 in FVD. While simple modifications, whether through full fine-tuning or part fine-tuning, can lead to stronger baselines. This is because the current large video foundation models are already powerful enough, which can easily adapt to downstream tasks using their built-in priors. We should make as few modifications as possible and bring out its capabilities for downstream tasks. Furthermore, as demonstrated, part fine-tuning will not degrade the final performance. This is because the foundation model is large enough that fine-tuning subset of the parameters is adequate to adapt to downstream tasks. Low-noise warmup strategy. We compare the lownoise warmup strategy against fixed uniform timestep sampling and high-noise warmup counterpart. For this comparison, we train RealisDance-DiT based on the Wan-2.1 T2V 1.3B model. Figure 8 illustrates the smoothed training loss curves. As shown, the low-noise warmup strategy accelerates convergence compared to the standard fixed uniform timestep sampling. Additionally, Figure 8 demonstrates that the high-noise warmup counterpart slows down convergence, further confirming our hypothesis that lownoise samples are crucial for stabilizing the early stages of fine-tuning. 8 Figure 8 Visualization of smoothed training loss curves. Figure 9 Visualization of different batch configurations on the RealisDance-Val dataset. # denotes the batch size. Zoom in for better visibility. Large batches and small iterations. We evaluate various batch and iteration settings for RealisDance-DiT, utilizing the Wan-2.1 T2V 1.3B model as the foundation for this experiment. This evaluation aims to determine how different batch configurations impact convergence speed. Due to limited GPU resources, we implement large batches using gradient accumulation. Figure 9 visualizes the generated frames corresponding to different batch configurations at various iterations. We can see that larger batches get faster convergence speed, even with the same learning rate. Furthermore, we observed that as iterations increase, the conditional controllability slightly improves, however, the diversity of the generated results significantly decreases, and numerous artifacts are introduced. As shown in the figure 9, the model fine-tuned with batch size of 8 loses the ability to preserve the background at iteration 36k, causing the background to completely transform into artifacts. This is because fine-tuning with too many iterations will make the model overfit the downstream data and lose its original prior. Therefore, we suggest using large batch size along with small number of iterations for fine-tuning, which facilitates rapid convergence while maximally preserving the prior knowledge of the foundation model. Figure 10 Illustration of limitation cases. RealisDance-DiT tends to generate static background when the character and the camera are relatively stationary."
        },
        {
            "title": "5 Limitations",
            "content": "There are two cases where RealisDance-DiT may not produce satisfactory results. The first case is when all three estimated poses are incorrect for extremely complex poses. In this case, RealisDance-DiT tends to generate random poses and introduces artifacts. The second case is when the character and the camera are relatively stationary, for example, the character is skiing with GoPro or riding motorcycle towards In the camera, see the illustration in Figure 10. this case, RealisDance-DiT tends to generate static background instead of background that gradually recedes. These issues need to be addressed in future work."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we present RealisDance-DiT, simple yet powerful baseline that makes progress towards controllable character animation in the wild. We emphasize that as long as the foundation model is powerful enough, straightforward model modifications with flexible fine-tuning strategies can yield superior baseline. This is because such large native video foundation model inherently can generalize to downstream tasks. Furthermore, we propose two fine-tuning strategies that speed up the model convergence while maximally preserving the built-in priors. Experiments are conducted on the TikTok dataset, the UBC fashion video dataset, and the proposed RealisDance-Val datasets. Both qualitative and quantitative experimental results demonstrate that RealisDance-Val significantly outperforms the other compared methods, qualifying it as solid baseline for future research."
        },
        {
            "title": "References",
            "content": "[1] Moore-animateanyone. https://github.com/MooreThreads/Moore9 AnimateAnyone, 2024. [2] Viggle ai. https://viggleai.io/, 2024. [3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In 7th International Conference on Learning Representations, 2019. [4] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [5] Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, and Jianke Zhu. Humandit: Poseguided diffusion transformer for long-form human motion video generation. arXiv preprint arXiv:2502.04847, 2025. [6] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems, volume 27, pages 26722680, 2014. [7] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, pages 74077417, 2023. [8] Jonathan Ho and Tim Salimans. ClassifierarXiv preprint free diffusion guidance. arXiv:2207.12598, 2022. [9] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. [10] Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. Animate anyone 2: High-fidelity character image animation with environment affordance. arXiv preprint arXiv:2502.06145, 2025. [11] Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, Yaohui Wang, Xinyuan Chen, Ying-Cong Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. [12] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1275312762, 2021. [13] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-inone video creation and editing. arXiv preprint arXiv:2503.07598, 2025. [14] Yiheng Li, Heyang Jiang, Akio Kodaira, Masayoshi Tomizuka, Kurt Keutzer, and Chenfeng Xu. Immiscible diffusion: Accelerating diffusion training with noise assignment. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, 2024. [15] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. [16] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019. [17] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3d with transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, pages 98269836, 2024. [18] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. [19] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine 10 Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139, pages 87488763, 2021. [20] Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In SIGGRAPH Asia Conference Proceedings, 2024. [21] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. In Advances in Neural Information Processing Systems, volume 32, pages 71357145, 2019. [22] Aliaksandr Siarohin, Oliver J. Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1365313662, 2021. [23] Jianlin Su, Murtadha H. M. Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [24] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. arXiv preprint arXiv:2410.10306, 2024. [25] Zhengyan Tong, Chao Li, Zhaokang Chen, Bin Wu, and Wenjiang Zhou. Musepose: posedriven image-to-video framework for virtual human generation. arxiv, 2024. [26] Kai Wang, Yukun Zhou, Mingjia Shi, Zhihang Yuan, Yuzhang Shang, Xiaojiang Peng, Hanwang Zhang, and Yang You. closer look at time steps is worthy of triple speed-up for diffusion model training. arXiv preprint arXiv:2405.17403, 2024. [27] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93269336, 2024. [28] Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, and Dahua Lin. Humanvid: Demystifying training data for camera-controllable human image animation. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [29] WanTeam, :, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [30] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. [31] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In IEEE/CVF International Conference on Computer Vision, pages 42124222, 2023. [32] Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network for pose-guided human video generation. In 30th British Machine Vision Conference, page 51, 2019. [33] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In IEEE/CVF International Conference on Computer Vision, pages 38133824, 2023. [34] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024. 11 [35] Jingkai Zhou, Benzhi Wang, Weihua Chen, Jingqi Bai, Dongyang Li, Aixi Zhang, Hao Xu, Mingyang Yang, and Fan Wang. Realisdance: Equip controllable character animation with realistic hands. arXiv preprint arXiv:2409.06202, 2024. [36] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, volume 15113, pages 145162, 2024."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Hupan Lab",
        "Shenzhen University",
        "Southern University of Science and Technology",
        "Zhejiang University"
    ]
}