{
    "paper_title": "Subject-Consistent and Pose-Diverse Text-to-Image Generation",
    "authors": [
        "Zhanxin Gao",
        "Beier Zhu",
        "Liang Yao",
        "Jian Yang",
        "Ying Tai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Subject-consistent generation (SCG)-aiming to maintain a consistent subject identity across diverse scenes-remains a challenge for text-to-image (T2I) models. Existing training-free SCG methods often achieve consistency at the cost of layout and pose diversity, hindering expressive visual storytelling. To address the limitation, we propose subject-Consistent and pose-Diverse T2I framework, dubbed as CoDi, that enables consistent subject generation with diverse pose and layout. Motivated by the progressive nature of diffusion, where coarse structures emerge early and fine details are refined later, CoDi adopts a two-stage strategy: Identity Transport (IT) and Identity Refinement (IR). IT operates in the early denoising steps, using optimal transport to transfer identity features to each target image in a pose-aware manner. This promotes subject consistency while preserving pose diversity. IR is applied in the later denoising steps, selecting the most salient identity features to further refine subject details. Extensive qualitative and quantitative results on subject consistency, pose diversity, and prompt fidelity demonstrate that CoDi achieves both better visual perception and stronger performance across all metrics. The code is provided in https://github.com/NJU-PCALab/CoDi."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 6 9 3 8 0 . 7 0 5 2 : r Subject-Consistent and Pose-Diverse Text-to-Image Generation Zhanxin Gao1 Beier Zhu2 Liang Yao3 Jian Yang1 Ying Tai1 1Nanjing University, 2Nanyang Technological University 3Vipshop yingtai@nju.edu.cn July 14, Abstract Subject-consistent generation (SCG)aiming to maintain consistent subject identity across diverse scenesremains challenge for text-to-image (T2I) models. Existing training-free SCG methods often achieve consistency at the cost of layout and pose diversity, hindering expressive visual storytelling. To address the limitation, we propose subject-Consistent and pose-Diverse T2I framework, dubbed as CoDi, that enables consistent subject generation with diverse pose and layout. Motivated by the progressive nature of diffusion, where coarse structures emerge early and fine details are refined later, CoDi adopts two-stage strategy: Identity Transport (IT) and Identity Refinement (IR). IT operates in the early denoising steps, using optimal transport to transfer identity features to each target image in pose-aware manner. This promotes subject consistency while preserving pose diversity. IR is applied in the later denoising steps, selecting the most salient identity features to further refine subject details. Extensive qualitative and quantitative results on subject consistency, pose diversity, and prompt fidelity demonstrate that CoDi achieves both better visual perception and stronger performance across all metrics. The code is provided in https://github.com/NJU-PCALab/CoDi."
        },
        {
            "title": "Introduction",
            "content": "While text-to-image (T2I) [28, 33, 30, 3] models excel in high-quality image generation [30, 22], they struggle to maintain subject consistency across multiple scenes. Subject-consistent generation (SCG) aims to synthesize images of the same subject across diverse contextual prompts with three key objectives: (1) ensuring subject consistency across generated instances, (2) promoting layout and pose diversity across different instances to avoid repetitive or overly similar compositions, and (3) maintaining prompt fidelity to accurately reflect the semantics of each prompt. The capability enables numerous practical applications including multi-scene narrative for visual storytelling, customizable character design for animation and gaming, and coherent illustration sequences for graphic novels. Current SCG methods [11, 41] primarily rely on training-intensive optimization [1] or mapping networks [32, 7] to bind subjects to latent representations. These approaches often require computationally expensive fine-tuning per subject or depend on domain-specific encoders, limiting scalability and generalizability. Training-free methods [35, 44, 18] have gained significant attention due to their elimination of parameter tuning, strong generalization capabilities, and broad compatibility with diverse diffusion architectures. Current training-free methodsConsiStory [35] and StoryDiffusion [44]enhance subject consistency by sharing self-attention keys and values across generated images. However, as noted in their limitations [35, 9] and evident in Fig. 1, these methods indicates corresponding author. 1 Figure 1: Comparison of subject-consistent generation methods: Vanilla SDXL [26], ConsiStory [35], StoryDiffusion [44] and CoDi (ours). (a&b) Existing methods sacrifice pose diversity for subject consistency, e.g., ConsiStory produces similar poses in Figure 1(a); and the lower right with hands placed in front in Figure 1(b). In contrast, CoDi generates consistent subjects, while matching the pose diversity of Vanilla SDXL. (c) We report two metrics to assess pose quality: 1) fidelity, measuring the distance to the pose of SDXL, and 2) diversity (see Sec.4.1 for details). Our CoDi attains the best performance on both metrics. often achieve high consistency at the cost of severely reduced layout and pose diversity, making it challenging to balance all three objectives. To better balance the three objectives, we propose training-free frameworksubject-Consistent and pose-Diverse generation, dubbed CoDithat achieves strong subject consistency while preserving diverse poses. Motivated by the progressive nature of diffusion models [42]which shows that low-frequency attributes like pose and layout are formed in early denoising steps, while high-frequency details such as facial features emerge laterour CoDi adopts two-stage strategy: Identity Transport (IT) and Identity Refinement (IR). During the early denoising steps, IT uses optimal transport to align each target images features with the reference identity features. Intuitively, this resembles mosaicking: assembling the subject using visual pieces from the reference image, rearranged to match the target posethus naturally preserving identity and keeping the original pose. In the later denoising steps, IR further refines subject consistency by guiding each target image to attend to the most salient identity attributes via cross-attention. As shown in Figure 1, our CoDi achieves superior visual results in both subject consistency and pose diversity. We further evaluate pose quality using two metrics: (1) pose fidelity, measuring the distance to the pose of Vanilla SDXL, and (2) pose diversity, quantifying pose variance. CoDi achieves the best performance on both. We evaluate our method on the existing subject-consistent T2I generation benchmark ConsiStory+ [18]. Compared to other training-free approaches, both quantitative and qualitative results validate that our framework achieves better subject consistency while preserving richer layout and pose diversity. It demonstrates superior trade-off among subject consistency, pose diversity, and prompt fidelity. 2 Further analysis is also provided to demonstrate CoDis advantages in pose diversity."
        },
        {
            "title": "2 Related Work",
            "content": "To steer text-to-image generation with diffusion models [30, 26, 4], various methods have been proposed to incorporate control signals such as depth maps, edge maps, and segmentation [20, 43, 40, 13]. Among them, subject consistency (a.k.a identity preservation) has attracted growing attention, aiming to generate set of images conditioned on specified subject. Existing subject-consistent generation (SCG) methods can be broadly categorized into two groups: training-based and training-free. Training-based SCG. Training-based methods require either (1) fine-tuning on additional training data [39, 15, 14, 2, 17] or (2) test-time optimization using reference images [29, 7, 12, 37]. The first line of work, represented by StoryDALL-E [19] and Make-A-Story [27], incorporates additional modules to capture subject information, followed by fine-tuning on large datasets to enable direct control over the subject given reference image. The second line of work, exemplified by DreamBooth [31] and Textual Inversion [6], optimizes model parameters or token embeddings on the given test images to inject subject identity. Despite their success in maintaining subject consistency, training-based methods suffer from high training costs or significant test-time latency. In contrast, our CoDi is training-free and introduces only mild additional latency. Training-free SCG. Training-free methods circumvent the need for iterative tuning of model parameters. For instance, 1-Prompt-1-Story [18] improves consistency by aligning prompt embeddings across generations. However, textual embedding control alone does not suffice to enforce consistency, often resulting in subject drift. The current leading methods, ConsiStory [35] and StoryDiffusion [44], adopt attention-based mechanisms to promote subject consistency by sharing self-attention keys and values across generated images. However, as noted in their limitation discussions [35, 9], applying attention across set of images reduces pose diversity. To address this issue, our CoDi explicitly preserves diversity and promotes consistency by aligning early-stage features between the target and reference images via optimal transport."
        },
        {
            "title": "3 Method",
            "content": "Our CoDi consists of two stages: Identity Transport (IT) and Identity Refinement (IR). Our IT operates in the early denoising stage to transport identity features from the reference image while preserving the pose and background of the target images. IR is applied in later denoising stages to refine subject consistency in fine-grained details. This two-stage design is inspired by [42], which reveals that low-frequency attributes such as pose and layout are determined early in the denoising timesteps, whereas high-frequency components like facial details emerge in later steps. We begin with the setup of subject-consistent generation (SCG), review of attention-based SCG methods and brief introduction of optimal transport."
        },
        {
            "title": "3.1 Preliminaries",
            "content": "Setup. SCG aims to synthesize batch of images that share the same subject identity across diverse scenes. Formally, given set of textual prompts {tn}N n=1, where each prompt is composed of shared identity prompt tid and unique attribute prompt an, i.e., tn = [tid, an]. For instance, given t1 =a hyper-realistic digital painting of fairy giggling in grove of enchanted crystals and t2 =A hyper-realistic digital painting of fairy lost in maze of giant sunflowers, the identity prompt is tid =a hyper-realistic digital painting of fairy, and the attribute prompts are a1 = giggling in grove of enchanted crystals and a2 = lost in maze of giant sunflowers. We refer to the image generated 3 from the identity prompt tid as the reference image, denoted as xid. The objective is to generate target images {xn}N n=1 that depict visually consistent subject with xid, while capturing the scene-specific attributes described in an. See Figure 2 for concrete example. Review of cross-image attention SCG. The current leading training-free SCG methods, ConsiStory [35] and StoryDiffusion [44], adopt attention-based strategies that extend the standard self-attention to cross-image attention mechanism. Formally, let {Xn}N n=1 denote the features of the target images {xn}N n=1. For generating i-th image, standard self-attention first projects Xi to queries Qi, keys Ki, and values Vi, then compute Zi = Attn(Qi, Ki, Vi) = softmax (cid:19) (cid:18) QiK Vi, (1) where is the feature dimension. Let denote matrix concatenation. We compute the concatenated keys and values as K1:N = [K1 ... KN ] and V1:N = [V1 ... VN ], respectively. To enhance consistency, cross-image attention mechanism allows the feature of the i-th image, Xi, to attend to the values V1:N of other images using their corresponding keys K1:N . (cid:18) QiK 1:N Zi = Attn(Qi, K1:N , V1:N ) = softmax V1:N . (cid:19) (2) While both SCG methods adopt cross-image attention, they differ slightly in implementation: ConsiStory [35] limits attention to masked subject regions, whereas StoryDiffusion [44] randomly samples tokens from all regions without subject constraints. As discussed in their limitations [35, 9], attention-based methods significantly reduce layout diversity. We conjecture that attending to shared pool of keys and values entangles feature updates across images, implicitly aligning spatial layouts and poses. To mitigate this, prior work [35] introduces components such as attention dropout and query blending. However, these additions increase computational overhead and still fail to recover pose diversity (as shown in Figure 1). In this paper, we draw inspiration from structural learning to simultaneously preserve subject consistency and pose diversity by transporting identity features via optimal transport. Optimal transport. Optimal transport (OT) [36, 21, 45] provides framework for measuring the distance between two distributions. Specifically, given two sets of support features {vm}M m=1 and {un}N n=1, we define two discrete distributions and as: P(x) = (cid:88) m=1 amδ(vm x), Q(x) = (cid:88) n= bnδ(un x) (3) where δ() denotes the Dirac function, and am, bn denote the associated probabilities that sum to 1, respectively. Let = [a1, ..., aM ] and = [b1, ..., bN ]. Given cost matrix RM , where each entry C(m, n) denotes the transport cost between vm and un (typically defined by their similarity), the OT distance between and is defined as: dOT(P, Q; C) = min 0 T, C, s.t. 1M = a, 1N = b, (4) where RM is the transport plan, with (m, n) 0 representing the amount of mass moved from vm to un, , denotes the Frobenius inner product, 1M is -dimensional all-one vector. 3."
        },
        {
            "title": "Identity Transport",
            "content": "Our IT operates in the early denoising steps (e.g., the first 10 of 50 total steps) to independently transport identity features from the reference image xid to each target image xn for all [1, ]. Our IT begins by extracting subject features from masked regions. 1We slightly abuse the notations and , which here do not refer to an image or the number of target images. 4 Figure 2: Illustration of our CoDi. (a) Extract subject masks (Mid and Mn) by averaging the image-text cross-attention at the final denoising timestep for subject-related tokens (e.g., fairy). (b) Compute the OT plan Tn using the cost matrix and the probability masses and (detailed in Sec. 3.2). (c) Identity transport (IT) operates in the early denoising steps to transfer reference subject features to targe images in pose-aware manner. (d) Identity refinement (IR) operates in the late denoising steps to refine subject details using selective cross-image attention mechanism. Extract subject features. Masking out background regions offers two benefits for subject consistency: it reduces background interference and computational cost by focusing on the subject alone. We adopt similar strategy to that of previous methods [8, 35], using image-text cross-attention to extract subject masks. Specifically, let Xid denote the features of the reference image xid generated from the identity prompt tid. When generating xid, we average the cross-attention maps at the final denoising timestep for subject-related tokens (e.g., fairy), followed by applying Otsus method [25] to produce binary mask Mid. This mask highlights the subject-relevant regions, from which we extract the subject features as: Sid = Xid Mid Rsidd (5) where applies the binary mask to retain subject features, sid denotes the number of ones in the binary mask Mid, and is the feature dimension. Similarly, for each target image xn, we extract subject features as Sn = Xn Mn Rsnd. The process is visualized in Figure 2(a). id, ..., ssid id ] and Sn = Transport between Sid and Sn. Given the subject feature pairs Sid = [s1 ], we first derive an optimal transport plan that aligns the reference features set [s1 {si i=1 (See Figure 2(b)). Using this plan , we compose the target subject features by transporting features from the reference image. Intuitively, this process resembles mosaicking: we assemble the target subject using pieces from the reference image, rearranged to match the target pose. Since the visual pieces originate from the reference image, subject identity is naturally preserved. To solve the OT problem in Eq. (4), we first define the cost i=1 with the target features {si n, ..., ssn id}sid n}sn matrix and the associated probability masses and b. Definition of the cost matrix C. The cost matrix is typically defined based on the pairwise distances between features: smaller distances imply lower transport costs. For pair si from final denoising step (where features contain minimal noise), the cost is defined as: id and sj C(i, j) = 1 cos(si id, sj n) = 1 sj si id id2sj n2 si . (6) Definition of the probability masses and b. Intuitively, = [a1, . . . , asid ] represents the importance weights of the subject features, where larger ai indicates that feature si id is more relevant to the subject tid. We reuse the average cross-attention maps for generating the subject-relevant mask as the feature importance and apply softmax function to ensure the sum (cid:80) ai equals to 1. The importance weights for the target subject features Sn are derived analogously. With the cost matrix and the probability masses and b, we solve the OT plan Tn in Eq. (4) using network simplex algorithm [24]. With the derived Tn, the subject target features composed by reference subject features are computed as = SOT Sid. (7) To form the final representation OT with the non-subject features (masked out by Mn) from Xn. The representation is then passed through the diffusion network to produce the output. The IT process is illustrated in Figure 2(c). , we combine SOT 3."
        },
        {
            "title": "Identity Refinement",
            "content": "The motivation behind this stage is that the IT module performs coarse transport between Sid and Sn. However, since the binary subject masks are imprecise and the foreground of target images evolves during denoisingwhile our transport plan Tn remains fixedfurther refinement of subject details becomes necessary. Our IR operates in the later denoising steps (e.g., the last 40 of 50 total steps) to reinforce subject details in the target images. IR resembles cross-image attention-based SCG methods, except that each target image attend only to the most relevant reference features to avoid entangled feature update across target images. Specifically, to generate the n-th image, we first construct the concatenated keys and values as Kn,id = [Kn Kid] and Vn,id = [Vn Vid], respectively. The cross-image attention scores are compute as An = softmax . (8) (cid:33) (cid:32) QnK n,id For each query, we retain only the top-α attention scores of the reference tokens (i.e., Kid). Specifically, for each row Ai of A, we define the top-α index set Ii (see Appendix for details) and zero out all other entries of Kid: (cid:40) Aij = Aij, 0, if Ii otherwise and ˆAi = Ai (cid:80) jIi The final cross-attention output is then computed as: Attnα(Qn, Kn,id, Vn,id) = ˆAVn,id . Aij (9) (10) This filtering mechanism ensures that only the most relevant identity features from the reference image contribute to the attention update. The IR process is demonstrated in Figure 2(d). 6 Figure 3: Qualitative comparison among Vanilla SDXL [26], ConsiStory [35], StoryDiffusion [44], and 1-Prompt-1-Story [18]. ConsiStory and StoryDiffusion generate similar poses across examples, while 1-Prompt-1-Story preserves pose diversity but struggles with subject consistency. In contrast, our CoDi achieves both."
        },
        {
            "title": "4.1 Setup",
            "content": "Benchmark. We evaluate our CoDi on the standard subject-consistent benchmark, ConsiStory+[18], which comprises nearly 200 prompt sets and supports the generation of over 1,100 images. Each prompt set includes subject described in specific style, with multiple frame-specific descriptions. Baselines and implementation details. We compare our CoDi with SoTA training-free SCG methods, including ConsiStory [35], StoryDiffusion [44] and 1-Prompt-1-Story [18]. We reproduce all baselines using their officially released code. All methods are implemented using the same backbone model, Stable Diffusion XL 1.0 [26], with an image resolution of 10241024, except for StoryDiffusion, which is evaluated at 768 768 due to its high memory consumption, following its original setting. To ensure fairness, identical noise seeds are used for all methods. We set the hyperparameter α in Eq. (10) to select the top 50% of reference features. Evaluation metrics. Our evaluation framework assesses the quality of generated images from three aspects: (1) subject consistency, (2) pose diversity, and (3) prompt fidelity. Subject consistency is evaluated by computing the average pairwise cosine similarity (or distance) between image embeddings 7 Table 1: Quantitative comparison of subject consistency, pose diversity and prompt fidelity. Best results are marked in bold. Method Subject Consistency Pose CLIP-I () DINO-v2 () DreamSim () Diversity () Prompt Fidelity () Vanilla SDXL [26] 1-Prompt-1-Story [18] ConsiStory [35] StoryDiffusion [44] 0.8417 0.8627 0.8751 0. 0.8010 0.8233 0.8428 0.8471 0.3139 0.2959 0.2336 0.2356 CoDi (ours) 0.8809 0.8514 0. 0.0772 0.0662 0.0621 0.0605 0.0758 0.9082 0.8814 0.9148 0.9038 0.9041 Table 2: Main component analysis (quantitative) on identity transport (IT) and identity refinement (IR). IT enhances subject consistency and preserves pose diversity. IR enhances subject consistency while reduces pose diversity. Their combination yields the best consistency and preserves diversity. IT IR Subject Consistency CLIP-I () DINO-v2 () DreamSim () Pose Diversity () Prompt Fidelity () 0.8417 0.8576 0.8859 0.8809 0.8010 0.8207 0.8618 0. 0.3139 0.2707 0.2044 0.2136 0.0772 0.0800 0.0675 0.0758 0.9082 0.9090 0.8975 0.9041 within each target image set. We use three image encoders for this evaluation: CLIP-I [10], DINOv2 [23], and DreamSim [5]. To evaluate pose diversity, we extract 2D human joint coordinates using ViTPoses pose estimation model [38]. To eliminate global variations in translation, rotation, and scale, we align poses using Procrustes analysis [34], inspired by standard practices in face alignment [16]. The pose diversity score is then computed as the average Euclidean distance between corresponding keypoints across aligned image pairs. higher score indicates greater pose diversity. For prompt fidelity, we use CLIP-Score [10] to measure the cosine similarity between each image embedding and its corresponding textual prompt embedding. See Appendix for more details."
        },
        {
            "title": "4.2 Experimental Results",
            "content": "Qualitative comparison. As shown in Figure 3, our CoDi achieves superior visual quality in terms of pose diversity, subject consistency, and prompt fidelity. Our CoDi preserves the pose diversity of Vanilla SDXL [26] while overcoming its limitation in subject consistency. In comparison, ConsiStory [35] and StoryDiffusion [44] achieve subject consistency at the cost of pose diversity. For example, in the scientist scenario, the man exhibits nearly identical body poses. Although 1-Prompt1-Story [18] maintains strong layout and pose diversity in both cases, its subject consistency remains limited. Quantitative comparison. Table 1 presents quantitative comparison. (1) Across all three subject consistency metricsCLIP-I [10], DINO-v2 [23], and DreamSim [5]our CoDi achieves the best performance, demonstrating superior identity preservation across instances. In particular, our method obtains the lowest DreamSim score (0.2136), indicating closer alignment with human perceptual similarity than competing methods. (2) In terms of pose diversity, CoDi achieves the highest score (0.0758), closely matching Vanilla SDXL (0.0772). This demonstrates its ability to preserve the inherent pose diversity of the diffusion model while maintaining subject consistency. (3) For prompt fidelity, CoDi performs competitivelyranking second only to ConsiStory and comparable to Vanilla SDXL. These results demonstrate CoDis ability to achieve subject consistency without compromising pose diversity or prompt alignment. 8 Figure 4: Main component analysis (qualitative) on identity transport (IT) and identity refinement (IR). IT enhances subject consistency in the coarse-grained level and preserves pose diversity. IR enhances subject consistency in the fine-grained level reduces pose diversity. Their combination yields the best consistency and preserves diversity."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "Main component analysis. The contribution of each module (IT and IR) to subject consistency and pose diversity are evaluated through quantitative and qualitative ablations, as shown in Table 2 and Figure 4. Table 2 shows that both IT and IR improve subject consistency, while IT also enhances pose diversity. However, using IR alone reduces pose diversityfor example, the score drops from 0.0772 to 0.0675 compared to the SDXL baseline. When both modules are applied, subject consistency further improves due to their synergistic effect, while pose diversity is preserved. Figure 4 visualizes the effective of each module. Compared to Vanilla SDXL, applying IT preserves the original pose and improves subject consistency, but some details, such as facial identity, remain suboptimal. In contrast, IR alone enhances fine-grained consistency, but leads to nearly identical poses across images, resulting in substantial loss of diversity. As shown in the bottom row, combining IT and IR improves both coarse and fine-grained consistency without compromising pose diversity. Study on stage transition point. Our CoDi adopts two-stage strategy: identity transport (IT) in the early denoising steps and identity refinement (IR) in the later ones. By default, we set the stage transition point at step = 10 out of total of 50 denoising steps (IT is applied when 10, and IR afterward). In this study, we investigate how the choice of transition point affects generation quality. As shown in Figure 5 (a), we vary from 2 to 30 and evaluate subject consistency (DINO-v2) and pose diversity. We find that our default choice = 10 achieves favorable trade-off between consistency and diversity. Effect of α. In the IR stage, we select the top-α percent of reference features to inject into the target subject features. In this study, we examine how varying α affects subject-consistent generation. 9 Figure 5: Ablation studies on (a) stage transition point, and (b) the effect of α. Table 3: Inference Time and Memory Usage. We report the inference time (in seconds) and peak GPU memory usage on single A6000 GPU for generating set of five images from prompt set with resolution of 1024 1024. StoryDiffusion [44] is excluded due to excessive GPU memory consumption beyond the A6000s limit. Method Inference time (s) GPU memory (GB) Vanilla SDXL [26] 1-Prompt-1-Story [18] ConsiStory [35] CoDi (ours) 77.67 115.85 113.88 154.89 35.58 17.13 46.60 45. Specifically, we vary α from 30% to 70% and report subject consistency (DINO-v2) and pose diversity in Figure 5(b). We observe that increasing α improves subject consistency but reduces pose diversity. Setting α = 50% provides favorable trade-off. Inference time and memory usage. We measure the inference time and memory usage of different SCG methods on single A6000 GPU, as shown in Table 3. We report the wall-clock time for generating set of five images from prompt set (since the baseline method ConsiStory [35] performs cross-image attention across batch of images) at resolution of 1024 1024. Based on Table 3, our method CoDi exhibits slightly higher inference time (154.89s) and comparable GPU memory usage (45.20GB) relative to ConsiStory. While 1-Prompt-1-Story [18] is the most memory-efficient, it compromises subject consistency. Note that we exclude StoryDiffusion [44] due to excessive GPU memory usage beyond the A6000s limit at 1024 1024 resolution (its original setting uses 768 768)."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose CoDi, novel training-free framework that addresses the trade-off between subject consistency and pose diversity. CoDi comprises two key components: identity transport (IT) and identity refinement (IR). During early denoising steps, IT aligns features across instances by optimally transporting the identity subjects features to each, while preserving pose diversity. IR further refines subject consistency by aligning instance features with the salient attributes of the identity subject in the later denoising steps. The effectiveness of our CoDi is demonstrated by its state-of-the-art performance in achieving subject consistency and maintaining pose diversity."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. The chosen one: Consistent characters in text-to-image diffusion models. In ACM SIGGRAPH, 2024. [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. [4] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [5] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. In NeurIPS, 2023. [6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In ICLR, 2023. [7] Rinon Gal, Moab Arar, Yuval Atzmon, Amit Bermano, Gal Chechik, and Daniel CohenOr. Designing an encoder for fast personalization of text-to-image models. arXiv preprint arXiv:2302.12228, 2(3), 2023. [8] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. In ICLR, 2023. [9] Amir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared attention. In CVPR, 2024. [10] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In EMNLP, 2021. [11] Dawid Kopiczko, Tijmen Blankevoort, and Yuki Asano. Vera: Vector-based random matrix adaptation. In ICLR, 2024. [12] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multiconcept customization of text-to-image diffusion. In CVPR, 2023. [13] Mingkun Lei, Xue Song, Beier Zhu, Hao Wang, and Chi Zhang. Stylestudio: Text-driven style transfer with selective control of style elements. In CVPR, 2025. [14] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin Wu, Lawrence Carin, David Carlson, and Jianfeng Gao. Storygan: sequential conditional gan for story visualization. In CVPR, 2019. [15] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In CVPR, 2024. [16] Chunze Lin, Beier Zhu, Quan Wang, Renjie Liao, Chen Qian, Jiwen Lu, and Jie Zhou. Structurecoherent deep feature learning for robust face alignment. IEEE Transactions on Image Processing, 30:53135326, 2021. [17] Chang Liu, Haoning Wu, Yujie Zhong, Xiaoyun Zhang, Yanfeng Wang, and Weidi Xie. Intelligent grimm-open-ended visual storytelling via latent diffusion models. In CVPR, 2024. [18] Tao Liu, Kai Wang, Senmao Li, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, Jian Yang, and Ming-Ming Cheng. One-prompt-one-story: Free-lunch consistent text-to-image generation using single prompt. In ICLR, 2025. [19] Adyasha Maharana, Darryl Hannan, and Mohit Bansal. Storydall-e: Adapting pretrained text-to-image transformers for story continuation. In ECCV. Springer, 2022. [20] Kangfu Mei, Hossein Talebi, Mojtaba Ardakani, Vishal Patel, Peyman Milanfar, and Mauricio Delbracio. The power of context: How multimodality improves image super-resolution. In CVPR, 2025. [21] Gaspard Monge. Mémoire sur la théorie des déblais et des remblais. Mem. Math. Phys. Acad. Royale Sci., pages 666704, 1781. [22] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In AAAI, 2024. [23] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [24] James Orlin. polynomial time primal network simplex algorithm for minimum cost flows. Mathematical Programming, 78:109129, 1997. [25] Nobuyuki Otsu et al. threshold selection method from gray-level histograms. Automatica, 11(285-296):2327, 1975. [26] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [27] Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, and Leonid Sigal. Make-a-story: Visual memory conditioned consistent story generation. In CVPR, 2023. [28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [29] Daniel Roich, Ron Mokady, Amit Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based editing of real images. ACM Transactions on graphics (TOG), 42(1):113, 2022. [30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. [32] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In CVPR, 2024. [33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. In NeurIPS, 2022. 12 [34] Peter Schönemann. generalized solution of the orthogonal procrustes problem. Psychometrika, 31(1):110, 1966. [35] Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, and Yuval Atzmon. Training-free consistent text-to-image generation. SIGGRAPH, 2024. [36] Cédric Villani et al. Optimal transport: old and new, volume 338. Springer, 2008. [37] Guangxuan Xiao, Tianwei Yin, William Freeman, Frédo Durand, and Song Han. Fastcomposer: Tuning-free multi-subject image generation with localized attention. IJCV, 2024. [38] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. In NeurIPS, 2022. [39] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal long story generation with large language model. arXiv preprint arXiv:2407.08683, 2024. [40] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In CVPR, 2023. [41] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [42] Zhongqi Yue, Jiankun Wang, Qianru Sun, Lei Ji, Eric Chang, Hanwang Zhang, et al. Exploring diffusion time-steps for unsupervised representation learning. In ICLR, 2024. [43] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In CVPR, 2023. [44] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Feng, and Qibin Hou. Storydiffusion: Consistent self-attention for long-range image and video generation. In NeurIPS, 2024. [45] Xingyu Zhu, Shuo Wang, Beier Zhu, Miaoge Li, Yunfan Li, Junfeng Fang, Zhicai Wang, Dongsheng Wang, and Hanwang Zhang. Dynamic multimodal prototype learning in visionlanguage models. In ICCV, 2025."
        },
        {
            "title": "A Additional Implementation Details",
            "content": "Extracting subject masks. We extract subject masks (Mid and Mn) by averaging the image-text cross-attention maps over all layers at the final denoising timestep, focusing specifically on subjectrelated tokens. Let Qimg denote the keys of image features and Ksub the keys of the subject-related tokens. For each cross-attention layer l, the unnormalized attention weights are computed as: Wl = QimgK sub , where is the feature dimension. We then average the attention weights across all layers: ="
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) l=1 Wl. We apply Otsus thresholding [25] to obtain the binary subject mask : = Otsu(W ). (11) (12) (13) Selection of the most salient identity features. Our IR refines target images using the most salient reference features, which are determined by the OT plan. Specifically, the saliency score of the i-th identity feature is computed as: sOT = (cid:88) n= Tn(i, :), 1 C(i, :) , (14) The top-α index set Ii in Eq. 9 contains indices with the α highest saliency scores."
        },
        {
            "title": "B Additional Evaluation Details",
            "content": "Unified evaluation protocol. We adopt unified evaluation protocol across all metrics. Specifically, for each target image set with generated images {xn}N n=1, we compute the average pairwise evaluation score as follows: uk = 2 (N 1) 1 (cid:88) (cid:88) n= j=n+1 (xn, xj), (15) where (, ) denotes the metric-specific similarity or distance function between two images, depending on the evaluation objective. The final evaluation score is then obtained by averaging uk over all target image sets:2 = 1 K (cid:88) k=1 uk. (16) Pose diversity score. We begin by extracting normalized 2D human keypoints and their confidence scores from each target image using ViTPose [38], SoTA transformer-based model known for its high accuracy and robustness in human pose estimation. Each image is represented by set of keypoint locations and their confidences β. = [(px 1 , py K, py K)] RH2, β = [β1, . . . , βK] RK (17) where each keypoint pi = (px ) is normalized by the image width and height and βi [0, 1] denotes its confidence score. To ensure robustness, we discard keypoints with confidence scores below 1), . . . , (px , py 2We slightly abuse the notations K, which here do not refer to keys in transformer. 14 Figure 6: Pose diversity scores across different confidence thresholds τ . Our CoDi consistently outperforms other SCG methods and performs comparably to Vanilla SDXL [26]. threshold τ . For pair of target images xi and xj, we retain only the indices of keypoints that are valid in both images. We then perform Procrustes method [34] to remove global variations in translation, rotation, and scale by aligning pi to pj. Specifically, we first compute the centroids of the keypoints which are denoted as µi and µj. We then center both keypoint sets by subtracting their respective centroids and normalize their ℓ2 norm: pi = pi µi pi µi , pj = pj µj pj µj2 Next, we compute the optimal rotation matrix using singular value decomposition (SVD): U, Σ, = SVD(p pj), = VU. (18) (19) The resulting is an orthogonal rotation matrix that minimizes the Frobenius norm between the aligned keypoint sets, ensuring the best rigid alignment in the least-squares sense. The optimal scaling factor is given by: γ = pj2 pi2 tr(Σ). (20) The aligned keypoints are then obtained by applying the computed scale, rotation, and translation: ˆpi = γ piR + µj. (21) The pose diversity score between pair of images xi and xj is computed as the average Euclidean distance between ˆpi and pj. To analyze pose diversity under different confidence thresholds τ , we compare the pose diversity scores of various methods across range of τ values. As shown in the Fig. 6, our CoDi consistently outperforms other SCG methods under all τ settings, and achieves performance comparable to Vanilla SDXL [26]. We use τ = 0.7 in our experiments to balance keypoint reliability and coverage."
        },
        {
            "title": "C Limitations",
            "content": "Similar to prior subject-mask-based methods such as ConsiStory [35], our CoDi framework relies on cross-attention scores to extract subject masks and estimate image token importance in the OT Plan. Occasionally, the pre-trained diffusion model assigns higher attention to background regions than to the subject, as shown in Fig. 7, which hinders the effective transport of identity features Xid from the reference image xid to target image xn, resulting in subject inconsistency. However,such failures are rare in practice (under 5%) and can be solved by simply changing the seed. 15 Figure 7: Limitations. Our method relies on the quality of cross-attention from the pre-trained diffusion model to accurately localize the subject."
        },
        {
            "title": "D Additional Results",
            "content": "We present additional qualitative comparisons in Fig. 8, along with more results generated by our CoDi in Fig. 9. These examples further demonstrate that our method achieves state-of-the-art performance in subject consistency, pose diversity, and prompt fidelity. In contrast, existing SCG methods remain limited, often excelling in only one or two of these aspectstypically at the expense of pose diversity or subject consistency. Long story generation. As each target image xn relies solely on reference image xid for subject identity, our CoDi enables extended visual storytelling. As demonstrated in Fig. 10, it maintains subject consistency across diverse prompt semantics, supporting the generation of varied layouts and poses. This makes CoDi effective for long-form generation, where both prompt fidelity and visual diversity are essential. 16 Figure 8: Additional qualitative comparisons. Our CoDi achieves the best trade-off among subject consistency, pose diversity, and prompt fidelity. 17 Figure 9: Additional qualitative results generated by our CoDi demonstrate strong subject consistency and pose diversity. 18 Figure 10: Long Story Generation. CoDi supports extended visual storytelling by generating diverse scene compositions while consistently preserving subject identity throughout the sequence."
        }
    ],
    "affiliations": [
        "Nanjing University",
        "Nanyang Technological University",
        "Vipshop"
    ]
}