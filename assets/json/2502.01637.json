{
    "paper_title": "Scaling Embedding Layers in Language Models",
    "authors": [
        "Da Yu",
        "Edith Cohen",
        "Badih Ghazi",
        "Yangsibo Huang",
        "Pritish Kamath",
        "Ravi Kumar",
        "Daogao Liu",
        "Chiyuan Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose SCONE ($\\textbf{S}$calable, $\\textbf{C}$ontextualized, $\\textbf{O}$ffloaded, $\\textbf{N}$-gram $\\textbf{E}$mbedding), a method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent $n$-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. SCONE enables two new scaling strategies: increasing the number of cached $n$-gram embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows SCONE to outperform a 1.9B parameter baseline across diverse corpora, while using only half the inference-time FLOPS."
        },
        {
            "title": "Start",
            "content": "Da Yu Edith Cohen Badih Ghazi Yangsibo Huang Pritish Kamath Ravi Kumar Daogao Liu Chiyuan Zhang 5 2 0 2 3 ] . [ 1 7 3 6 1 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We propose SCONE (Scalable, Contextualized, Offloaded, N-gram Embedding), method for extending input embedding layers to enhance language model performance as layer size scales. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for set of frequent n-grams. These embeddings provide contextualized representation for each input token and are learned with separate model during training. During inference, they are precomputed and stored in off-accelerator memory with minimal impact on inference speed. SCONE enables two new scaling strategies: increasing the number of cached n-gram embeddings and scaling the model used to learn them, all while maintaining fixed inference-time FLOPS. We show that scaling both aspects allows SCONE to outperform 1.9B parameter baseline across diverse corpora, while using only half the inferencetime FLOPS. 1. Introduction Embedding layers in language models map discrete tokens to continuous vector representations (Mikolov, 2013; Sennrich et al., 2016). These layers can be implemented as lookup tables, enabling efficient retrieval of embeddings using hashor tree-based data structures. This allows embedding layers to be offloaded to main memory or even secondary storage (e.g., disk) with minimal impact on inference speed. This is desirable, as main memory and secondary storage are significantly more cost-effective than accelerators (e.g., GPUs and TPUs (McCallum, 2024)). These advantages drive our exploration of methods for scaling up embedding layers. However, scaling the embedding layer by simply increasing the vocabulary size has limited benefits. The first issue is the coupling between the input embedding layer and the output Google. Correspondence to: {dayuwork,edco,pritishk,chiyuan}@google.com. Figure 1. (Top) Perplexity (lower is better) on the OLMo (Groeneveld et al., 2024) evaluation mixture. Inference-time FLOPS refer to the forward pass computation cost for four model sizes (0.7B, 1B, 1.3B, and 1.9B). With 10M f-grams, the 1.3B model matches the 1.9B baseline, while with 1B f-grams, the 1B model surpasses it. (Bottom) End-to-end token generation speed on single A100 using vLLM (Kwon et al., 2023). Storing f-gram embeddings in main memory introduces negligible latency, while NVMe storage slows generation slightly but does not create bottleneck. (logits) layer: (i) It is common to share weights between the input embedding and the output layer. In this case, the weights already reside in the accelerator memory as they are needed for logits computation, which eliminates the benefits of offloading the input embedding. (ii) Even when the weight parameters are not shared, the weight shapes are tied to the vocabulary size and embedding dimension. When scaling the input embedding by increasing the vocabulary"
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "Figure 2. Illustration of SCONE (with maximum n-gram length of 3). The term f-grams refers to the set of frequent n-grams (Section 3). size, as explored in prior studies (Wang et al., 2019; Zheng et al., 2021; Liang et al., 2023; Tao et al., 2024), the inference cost also increases due to the size growth of the output embedding that is used to compute logits (Dagan et al., 2024). This scaling becomes computationally impractical beyond vocabulary size of few hundred thousands. The second issue is that the benefits of scaling the vocabulary diminish, even when computational costs can be mitigated by advances in accelerators or smarter algorithms (Joulin et al., 2017; Shim et al., 2017): Scaling leads to large number of tail tokens, that have low frequency in the training corpus. The training of their (input and output) embeddings receives very few updates which results in representations that are of lower quality (Liao et al., 2021; Dou et al., 2024). Our experiments with GPT-2 models (Radford et al., 2019) pre-trained on WebText (Peterson et al., 2019) confirm these limitations: Only 7.3% of embedding vectors in 2M vocabulary receive more than 100 updates over 100M training tokens, compared to 97.6% for 32K vocabulary. Additionally, we observe performance degradation and linear increase in accelerator memory usage when the vocabulary size exceeds 1M (Appendix C). Our contributions. In this paper we propose novel approach to disentangle the input and output embeddings, bypassing those issues and enabling the effective input embedding scaling with minimal additional inference cost. Instead of increasing the size of the vocabulary, we augment each token in the base vocabulary with n-gram contextualized variants. The n-grams are selected from predefined set of frequently occurring n-grams, that we refer to as f-grams. Those contextualized tokens are only used in input embedding computation, allowing us to build an augmented input embedding table with billions of entries without impacting the computation cost of the output layer. Furthermore, the embeddings for those contextualized tokens are generated from an embedding transformer model, referred to as f-gram model, that is jointly trained (see Figure 2 for an illustration). This allows us to obtain rich contextualized representations without being subjected to the sparse tail phenomenon of naively increasing the vocabulary size. With this novel design, we introduce two new directions for improving model performance: (i) increasing the number of cached f-gram embeddings and (ii) scaling up the f-gram model for learning those embeddings. Notably, both approaches preserve inference-time FLOPs. These directions enable us to fully leverage the precomputation and offloading of contextualized embeddings. We demonstrate that 1B parameter model with SCONE outperforms baseline model requiring 2 more inference-time FLOPs. Figure 1 presents representative results from Section 4.2. To summarize, our contributions are as follows: We propose SCONE, scalable approach to expand the embedding layer in language models (Section 3). We conduct extensive experiments to evaluate design choices and validate our method in large-scale pre-training setups (Section 4). 2. Preliminaries We focus on pre-training decoder-only language models using the causal language modeling objective, standard recipe to train modern language models (Radford et al., 2019; Brown et al., 2020). This involves predicting the next token based solely on preceding tokens, enabling these models to handle diverse text generation tasks. We introduce formal notations to describe such model architectures. Following Phuong & Hutter (2022), we prioritize"
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "clarity and omit details that are not essential for describing our method. We assume that vocabulary Vtoken of tokens has already been defined. The token embedding layer is parameterized by function : Vtoken Rd that maps any token in Vtoken to an embedding vector in Rd, where is the embedding dimension. We abstractly view transformer model as : (Rd)Nmax Rd, which maps sequence of vectors in Rd of length at most Nmax to single embedding vector1 in Rd. The size of transformer model refers to the number of parameters in the model. prediction head : Rd Vtoken maps the embedding vector to probability distribution over tokens in the vocabulary Vtoken (where Vtoken denotes the probability simplex). Collectively these pieces yield basic next-word prediction model MT ,A,D, which, given an input sequence (σ1, . . . , σm) token of tokens, produces distribution over the next token ˆσm+1, which can be used for auto-regressive sequence prediction. For completeness, we present the pesudocode of the basic next-word prediction model in Algorithm 3 (Appendix B). Remarks on Token Embedding Layers. The number of parameters in token embedding layer : Vtoken Rd is the product of the vocabulary size Vtoken, which ranges from few dozen to hundreds of thousands in current models (Radford et al., 2019; Dubey et al., 2024), and the embedding dimension d, which is typically ranges in few thousands (Brown et al., 2020; Touvron et al., 2023). Often, the prediction head : Rd Vtoken computes the similarity between the models output embedding and all vocabulary embeddings to predict the next token, thereby sharing the embeddings in the token embedding layer. Embedding layers admit highly efficient implementations. The key-value pairs can be organized with hashor treebased data structures, which allow the embedding layer to operate with access cost that is either constant or logarithmic in the number of embedding vectors. These low cost methods, in principle, allow the embedding layer to be offloaded from accelerators with minimal latency impact (see Section 3.3). Most implementations, however, store the token embedding layer in accelerator memory, since the token embeddings are also required for applying the prediction head. 3. SCONE Architecture We introduce the SCONE architecture that uses new embedding layer for frequently occurring n-grams of tokens. Figure 2 shows the high-level approach. First we construct set Vf-gram [2,n] token := (cid:83)n k=2 token 1The output embedding vector is typically the last token embedding, which is commonly used for next-token prediction. In this work, for the f-gram model, we use the last token embedding as the contextualized embedding of an input token. 3 Algorithm 1 Constructing set of f-grams Vf-gram. Parameters: S: desired size of Vf-gram. Input: {(σ1, . . . , σNmax)(i)} : training set, where each sequence is from Nmax token . Output: Vf-gram [2,n] for = 2, . . . , do token: set of f-grams of size S. token sequences from for ω := (σ 1, . . . , σ k) token do Cω the number of times ω appears in all sequences {(σ1, . . . , σNmax )(i)}. Let ω1, ω2, . . . be list of elements of (cid:83)n such that Cω1 Cω2 , breaking ties arbitrarily. return {ω1, . . . , ωS}: set of f-grams of size token, sorted k=2 Algorithm 2 SCONE method FT ,Vf-gram,Af-gramF . Parameters: : Vtoken Rd: token embedding layer, Vf-gram TRAINING: f-gram transformer model Af-gram : (Rd)n Rd, INFERENCE: f-gram embedding layer token: set of f-grams, : Vf-gram Rd. Input: sequence of tokens (σ1, . . . , σm) Output: Embeddings (e1, . . . , em) (Rd)m. for = 1, 2, . . . , do token. smallest < s.t. (σj, . . . , σi) Vf-gram if such exists, otherwise i. if = then ei (σi). else ei (cid:40) Af-gram(T (σj), . . . , (σi)) F(σj, . . . , σi) at training at inference return (e1, . . . , em) of frequently occurring k-grams of length at most n, that we term f-grams; throughout this paper, we use to denote the maximum length of f-grams considered. Algorithm 1 illustrates this process. That said, Algorithm 1 is for illustration purposes only; in practice, we use more efficient implementation requiring only (n 1) linear scans over the training corpus, as detailed in Section 3.1. Notably, the counting and ranking process resembles continuing the training of BPE tokenizer (Sennrich et al., 2016) with the existing vocabulary. Next, we define the SCONE method, which maps given sequence of tokens to sequence of embedding vectors. SCONE method behaves differently at training and at inference, as described in Algorithm 2. At training, it is parameterized by an f-gram transformer model Af-gram. However, at inference, it is parameterized instead by an f-gram embedding layer : Vf-gram Rd that maps the set of f-grams obtained above to embedding vectors. This embedding layer"
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "is implemented by caching the outputs of Af-gram for all f-grams in Vf-gram and storing them in hashor tree-based data structure for efficient retrieval. The embeddings produced by SCONE are then passed to standard transformer model Amain : (Rd)Nmax Rd, referred to as the main model, followed by prediction head : Rd Vtoken . Together, these components form the end-to-end process for next-word prediction with SCONE. We present the corresponding pseudocode in Algorithm 4 in Appendix B. In the rest of this section, we will discuss the motivations behind these design decisions and provide further implementation details. 3.1. BPE-Style Discovery of f-grams The construction of Vf-gram, as defined in Algorithm 1, can be implemented efficiently with (n1) linear scans over the training corpus. We perform one scan for each [2, n], starting with 2-grams. In subsequent scans, we impose minimum frequency threshold of 5 to reduce memory usage. At the (k + 1)th scan, the set of k-grams from the previous scan allows us to skip any (k + 1)-gram candidates that cannot meet the threshold. Specifically, if an (k + 1)-gram surpasses the threshold, its k-suffix or prefix must appear at least as many times. Figure 3 shows how the number of unique k-grams (up to 6-grams) grows as the training corpus scales from few billion to one trillion tokens. Finally, all found k-grams (for [2, n]) are ranked by frequency, and the top are selected as keys, where = Vf-gram is the target number of f-grams. Our procedure for counting and ranking the n-grams is analogous to continuing BPE tokenizers training on an existing vocabulary. In each BPE iteration (Gage, 1994; Sennrich et al., 2016), the frequencies of all token pairs (2-grams) are counted and the most frequent pair is merged to form new token, expanding the vocabulary by one. However, merging and recounting pairs repeatedly is prohibitively expensive for large corpora. Instead, we simply collect and sort all n-grams up to small n. 3.2. Learning f-gram Embeddings with Af -gram We motivate our use of Af-gram by considering the alternative of directly backpropagating gradients to large embedding table. The issue with the alternative approach is that it does not exploit the dependencies between n-grams and therefore suffers from fewer updates per embedding. We explored this by pre-training GPT-2 models with token vocabulary sizes ranging from 32K to 2M. As vocabulary size increases, token embedding updates become sparser, which eventually degrades performance. For example, when training over 100M tokens, 97.6% of the tokens in 32K 4 Figure 3. Number of unique 2to 6-grams that appear at least five times. We vary the size of the corpus by uniformly sampling sequences from the OLMo tokenized training corpus (Soldaini et al., 2024). vocabulary receive more than 100 updates. In contrast, with vocabulary of 2M, only 7.3% of the tokens reach this threshold. See Appendix for more details. The sparsity makes it extremely challenging to learn an embedding table by directly backpropagating gradients to the embeddings. SCONE solve this problem by parameterizing the embeddings with f-gram transformer Af-gram, avoiding the sparse update issue. SCONE jointly trains the Af-gram model with the main model Amain and the token embedding layer . This overcomes the sparse updates issue but also introduces additional compute costs. For each ω Vf-gram, the computation is the same as that of processing short sequence of length ω through standard transformer. Since ω is small (ω 5 for most of our experiments), the primary overhead comes from the feed-forward layer. During inference, the f-gram embedding layer can be precomputed and stored in lookup table, offloaded to CPU memory or secondary storage for efficient retrieval. Meanwhile, the token embedding layer remains on the accelerator for decoding. This design leverages the low complexity of embedding layers to enrich token representations without increasing decoding costs. Importantly, SCONE introduces novel approach to improving performance under fixed inference-time FLOPS budget. Prior work shows that increasing training compute beyond compute-optimal threshold yields diminishing returns for fixed model size (Hoffmann et al., 2022). common strategy to utilize extra training compute is scaling up both model size and compute, but this typically raises inference-time FLOPS as well. In contrast, our method allows the Af-gram model to benefit from greater training compute without increasing inference-time FLOPS."
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "3.3. Space Usage and Query Latency We evaluate the space usage and query latency of the f-gram embedding layer under various configurations. We show that latency is not bottleneck for language model inference and the space costs are low due to the use of relatively inexpensive system memory or secondary storage. Using the setup described in Section 4.2, we set the maximum n-gram length to 5 and experiment with Vf-gram being 10M, 100M, and 1B with embedding dimension of = 2048 with 16-bit precision per floating point value. Experiments were conducted on workstation with 64 Intel Xeon CPU cores and 512 GB of memory. Space and latency were measured for both in-memory and on-disk storage. In memory, embeddings are stored as single matrix with hash dictionary mapping f-grams to indices, while ondisk storage uses the Lightning Memory-Mapped Database (Chu, 2011) to directly store f-gram and embedding pairs on NVMe solid-state drives. Table 1. Space usage of the f-gram embedding layer F, along with cost for memory and NVMe solid-state drives (McCallum, 2024). # of n-grams"
        },
        {
            "title": "System memory",
            "content": "Solid-state drive 107"
        },
        {
            "title": "766.8 GB",
            "content": "(does not fit)"
        },
        {
            "title": "7665.4 GB",
            "content": "Price (per GB) 2 USD 0.1 USD Table 1 summarizes the space usage for both storage methods. In both cases, the space required increases linearly with the number of embedding vectors. The 10M and 100M f-gram embedding layers are able to fit within CPU memory, with the 10M layer requiring 41.4 GB. For on-disk storage, there is additional overhead as the same 10M layer occupies 77.3 GB storage. Figure 4. Amortized per-token query latency (ms), averaged over 100,000 batches. The latency spike from batch size 1 to 2 when reading from system memory is due to batch operator overhead, which is less pronounced for solid-state drives. Figure 5. Effect of the maximum f-gram length in Vf-gram, on perplexity and matched length. The left y-axis shows perplexity (averaged over three seeds), where the leftmost star indicates baseline performance. The right y-axis shows the average length of matched f-grams. The perplexity decreases as we increase the maximum length from 2 to 4, but then plateaus with some fluctuation. Similarly, the average matched length initially rises but stabilizes after size 4. Figure 4 shows the latency of retrieving embeddings with different batch sizes. Latency is measured as the end-toend time from loading batch of tokens to the f-gram embeddings (Algorithm 2) being ready on GPU. CPU cache is cleared before each test, and up to 4 queries are made per token to identify the longest matching n-gram (with maximum length of 5). For in-memory storage, sequential queries suffice as they are not the bottleneck. In contrast, for on-disk storage, we make parallel queries to the database. At batch size of 1, the latency for 10M f-gram embedding layer on the NVMe drive is 1.1ms, increasing to 2.3ms for 1B f-gram embedding layer. This is well below the latency threshold for LLM inference, as typical commercial APIs offer generation speed of 100 tokens per second, corresponding to 10ms per token (ArtificialAnlys, 2025). Larger batch sizes further improve efficiency, with batch size of 16 reducing the amortized per-token latency to 0.5ms. In-memory access is much faster: for 100M f-gram embedding layer and batch size of 16, the amortized per-token latency is only 0.017ms. 4. Experimental Evaluation 4.1. Design Choices We analyze three key hyperparameters: (i) the maximum f-gram length in Vf-gram, (ii) the number of f-grams used Vf-gram, and (iii) the Af-gram model size. We use the released GPT-2 tokenizer, which has Vtoken = 50,257, and train on the WebText dataset (Peterson et al., 2019). The tokenized corpus contains 9B training tokens, from which"
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "Figure 6. Evaluation perplexity as function of Vf-gram. Model sizes in the legend correspond to the main model sizes, including the token embedding layer. The dashed lines and leftmost stars indicate baseline performance. Perplexity decreases overall with increasing sizes of Vf-gram. we extract f-grams using the method in Section 3.1. We consider three main model sizes with 76M, 340M, and 510M non-embedding parameters. Including the token embedding layer, the total parameter count increases to 128M, 419M, and 589M, respectively. The embedding dimensions for these models are 1024, 1536, and 1536, respectively. These models are either trained using only the token embedding layer as baselines or with an additional Af-gram when SCONE is applied. Following Radford et al. (2019), we use batch size of 512 and sequence length of 1024. Since Radford et al. (2019) do not specify the number of training steps, we train all models for 80B tokens, roughly twice the number of training tokens in Radford et al. (2018). For evaluation, we use the validation split of WebText and WikiText-103 (Merity et al., 2016), one of the largest downstream datasets in Radford et al. (2019). Additional implementation details are in Appendix E.1. 4.1.1. VARYING THE MAXIMUM F-GRAM LENGTH We explore the effect of varying the maximum length of f-grams in Vf-gram. We vary from 2 to 8 while fixing the total number of f-grams to Vf-gram =20M. This means that we obtain frequency cutoff (the minimum frequency of an n-gram in Vf-gram) that increases with n: This value was 7 for = 2 and 108 for = 8. We then measure for each (i) evaluation perplexity and (ii) the average length of matched f-grams on Wikitext-103. Our findings are reported in Figure 5. We observe that evaluation perplexity increases for between 2 and 4 and then plateaus with some fluctuations. similar trend is observed for the average match length, which is the average length of f-gram found in SCONE method (Algorithm 2) for each token in the evaluation set: it rises from 2 to 4 before stabilizing. This is likely because longer f-grams occur less frequently than shorter ones after ranking. Even with higher maximum n-gram length, most selected entries remain short. Additionally, longer f-grams from the training corpus are less likely to match downstream data. Findings on the validation split of WebText (Appendix D.1) exhibit similar pattern, though the average matched length plateaus later, at length 6. Considering these findings, for the experiments in the remainder of this paper, we set the maximum f-gram length to = 5 unless stated otherwise. 4.1.2. VARYING THE NUMBER OF F-GRAMS We observe consistent improvements in language modeling performance as we scale up Vf-gram. To implement the f-gram model Af-gram, we replicate the baseline model architecture but remove the token embedding layer. This results in the size of Af-gram matches the baseline models non-embedding parameters. Figure 6 shows the evaluation perplexity as Vf-gram, the number of f-gram embeddings, increases from 512K to 100M. On the WebText validation split, the perplexity decreases consistently as the number of f-gram embeddings increases. Similarly, on WikiText-103, the perplexity generally decreases with more f-gram embeddings, though minor fluctuations are observed. In Figure 6, we include three additional baselines where the non-embedding parameters of the three main models are doubled, resulting in models with 204M, 759M, and 1099M parameters for the original 128M, 419M, and 589M models, respectively. This ensures that the total parameter"
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "count of each baseline matches the training-time parameter count when SCONE is applied. With 100M f-gram embeddings, the 419M and 589M models trained with SCONE match or surpass the performance of the 759M and 1099M baselines, respectively, despite using only half as many nonembedding parameters during inference. rectly scaling up Amain. However, the latter also increases inference-time FLOPS, whereas scaling Af-gram does not, as it is replaced with an off-accelerator lookup table during inference. This highlights our method as novel way to leverage additional training compute while maintaining fixed inference-time compute. 4.1.3. VARYING THE SIZE OF THE Af -gram MODEL 4.2. Scaling Up the Training Corpus We demonstrate that the two new scaling aspects of SCONE, namely size of Vf-gram and parameters in Af-gram, apply to large-scale pre-training. The cutoff frequencies are 21,956 for 10M f-grams and 70 for 1B f-grams. We use base architecture with 18 decoder blocks and 1B parameters. Following the OLMo-1B architecture, we set context length Nmax = 2048 and embedding dimension = 2048. To explore parameter scaling, we vary the number of layers, creating four model variants: 0.7B, 1.0B, 1.3B, and 1.9B. SCONE is evaluated on the first three, while the 1.9B model serves solely as baseline. We test SCONE with two sizes of Af-gram, 0.6B and 1.8B, matching the nonembedding parameters of the 0.7B and 1.9B variants. All models are trained on 200B tokens, with sequences of tokens sampled uniformly from the corpus. While Hoffmann et al. (2022) suggests 20B tokens for compute-optimal 1B model, we use larger corpus to approach convergence. Training loss curves are provided in Appendix D.2, with implementation details in Appendix E.2. Figure 1 presents the perplexity on the OLMo evaluation mixture2, which comprises 11 diverse corpora, including web crawl data, literature, online forums, scientific writing, coding, and more. Table 2 presents the performance breakdown for the 1B, 1.3B, and 1.9B variants. In Figure 1, the X-axis represents inference-time forward FLOPS per sequence, computed following the breakdown in Hoffmann et al. (2022). Results indicate that increasing both Vf-gram and the size of Af-gram consistently improves performance across all evaluation corpora. Additionally, in Figure 1 we report end-to-end token generation speed using the vLLM framework (Kwon et al., 2023) with batch size of 1, showing that even for large values of Vf-gram, embedding retrieval is not bottleneck. For representative finding, in the 1B model variant, the baseline achieves an average perplexity of 16.082. Setting Vf-gram to 10M improves the perplexity to 15.459 with 0.6B Af-gram model and to 15.125 with 1.8B Af-gram model, the later outperforming the 1.3B baseline (15.284). Increasing Vf-gram to 1B further improves perplexity to 15.187 and 14.581 for the 0.6B and 1.8B Af-gram models, respectively, surpassing the 1.9B baseline (14.598) despite 2https://github.com/allenai/OLMo/blob/v0.4. 0/configs/official/OLMo-1B.yaml#L90 Figure 7. Evaluation perplexity on Wikitext-103 as function of the size of Af-gram. Model sizes in the legend correspond to the main model sizes, including the token embedding layer. Dashed lines and stars on the left represent baseline performance. The perplexity improves as the size of Af-gram grows. We observe that, for fixed Vf-gram, scaling up the Af-gram model size provides new way to improve language modeling performance. We vary the model size by changing the number of layers in the main model architecture. For each Amain model size, we evaluate four Af-gram model sizes: 0.5x, 1x, 2x, and 3x the non-embedding parameters of the main model. We set Vf-gram to be 100M. Figure 7 presents the evaluation perplexity on Wikitext-103. The observations on WebText validation split are similar, and we present the results in Appendix D.1. The results in Figure 7 show that the perplexity generally decreases as the Af-gram model size increases, although the improvements become smaller as the model size grows larger. For instance, with the 419M main model, 170M Af-gram model improves the perplexity from 26.1 to 23.4, outperforming the 589M baseline (24.7) by clear margin. Further scaling of the Af-gram model to 1020M (resulting in 1439M total parameters during training) lowers the perplexity to 22.1, which is slightly higher than the 1099M baseline (21.9). This suggests that scaling up the Af-gram model size initially provides better scaling curve, but beyond certain size, it yields less optimal scaling curve compared to di-"
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "Table 2. Perplexity (lower is better) on the OLMo evaluation mixture. All models are trained for 200B tokens. SCONE consistently improves language modeling performance across all evaluation corpora. With 10M Vf-gram, 1.3B model matches the performance of the 1.9B baseline. Similarly, with 1B Vf-gram, 1B model matches the 1.9B baseline. Model size c4-en books common-crawl pes2o reddit stack wiki ice m2de-s2orc pile wikitext-103 Average 1B baseline 16.813 21.570 +10M Vf-gram (0.6B Af-gram) 16.087 20.963 +10M Vf-gram (1.8B Af-gram) 15.727 20.429 +1B Vf-gram (0.6B Af-gram) 15.846 20.593 +1B Vf-gram (1.8B Af-gram) 15.158 19.680 1.3B baseline 15.994 20.157 +10M Vf-gram (0.6B Af-gram) 15.509 19.816 +10M Vf-gram (1.8B Af-gram) 15.193 19.587 +1B Vf-gram (0.6B Af-gram) 15.270 19.510 +1B Vf-gram (1.8B Af-gram) 14.803 18.996 1.9B baseline 15.270 19.017 16. 16.039 15.473 15.684 14.857 15.921 15. 14.995 15.106 14.541 15.184 11.682 22. 3.360 14.453 15.281 11.270 21.797 3. 13.777 14.979 11.124 21.388 3.231 13. 14.709 11.071 21.411 3.213 13.543 14. 10.761 20.757 3.133 12.964 14.220 11. 21.634 3.248 13.721 14.651 10.887 21. 3.192 13.260 14.372 10.795 20.735 3. 13.071 14.272 10.707 20.763 3.139 13. 14.177 10.502 20.296 3.085 12.637 13. 27.900 26.361 25.785 26.026 24.958 26. 25.450 25.258 25.009 24.533 10.719 20. 3.163 13.119 14.095 25.461 10.429 10. 9.956 9.889 9.553 9.927 9.757 9. 9.546 9.357 9.570 16.053 15.371 15. 15.077 14.354 15.143 14.616 14.438 14. 13.971 16.082 15.459 15.125 15.187 14. 15.284 14.844 14.654 14.609 14.245 14. 14.598 requiring only half the inference-time FLOPS. 5. Related Work We discuss additional related work, including Mixture of Experts and Memory Layers, two established methods for scaling language models under fixed FLOPS budget, in Appendix A. Contextualized Word Embeddings. Words can have different meanings depending on context. Prior work has incorporated context into word embeddings, either from the entire sequence (McCann et al., 2017; Peters et al., 2018) or short n-grams (Gupta et al., 2019), before applying them to downstream tasks. Modern language models inherently use contextualized token embeddings, leveraging attention mechanisms. In this study, we extend the embedding layer to include contextualized f-gram embeddings for each token. key novelty is that our approach allows embeddings to be precomputed and offloaded from accelerators, providing contextual embeddings for each token without increasing inference-time FLOPS. Scaling of Vocabulary Size. Tao et al. (2024) show that larger models benefit from larger vocabularies, aligning with the trend of advanced language models whose vocabulary sizes have increased from few dozen thousand (Devlin et al., 2019; Radford et al., 2019) to few hundred thousand (Google, 2024; Adler et al., 2024; Dubey et al., 2024; DeepSeek, 2024). However, the optimal vocabulary size predicted by Tao et al. (2024) is still much smaller than the model size, e.g., vocabulary size of 216K for 70B parameter model. These findings motivate us to extend the embedding layer without changing the vocabulary size, fully exploiting the lookup nature of the input embedding layer. Tokenization in Language Models. Our method assumes predefined vocabulary from trained tokenizer. Several popular algorithms exist for training tokenizers (Sennrich et al., 2016; Wu, 2016; Kudo, 2018b;a). In this work, we use BPE tokenizer, following prior seminal works (Radford et al., 2019; Touvron et al., 2023). However, our method is not tied to any specific tokenization algorithm and can be applied seamlessly to others. Tokenization-free language models have also been widely explored (Kim et al., 2016; Choe et al., 2019; Xue et al., 2022; Yu et al., 2023b; Wang et al., 2024; Deiseroth et al., 2024; Meta, 2024; Pagnoni et al., 2024). While we have not tested our method on tokenization-free models, we believe our core ideaintroducing an off-accelerator embedding layer by precomputing embeddings for frequent input patternsremains applicable. Concurrent Work. We learned about concurrent study by Huang et al. (2025) upon the release of our work. They also explore scaling the embedding layer in language models. Both works propose decoupling the input embedding layer size from the output layer by introducing an additional embedding layer at input, and observe similar performance gains as the size of the additional embedding layer increases. However, the method in Huang et al. (2025) differs significantly from ours, introducing trade-offs that we discuss below. Huang et al. (2025) instantiate the additional embedding layer during training. Similar to our work, they also leverage short preceding n-gram to determine the embedding for each input token. key distinction is that they use all n-grams without differentiating by frequency. Since the number of n-grams is typically much larger than the number of embedding vectors, they hash each n-gram to obtain an index in the additional embedding layer, leading to multiple n-grams sharing the same embedding vector. We speculate"
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "that this also mitigates the sparse update issue discussed in Appendix C. The method proposed in Huang et al. (2025) has both advantages and drawbacks compared to ours. On the positive side, it directly backpropagates gradients to an instantiated embedding layer, eliminating the need for separate embedding model. However, instantiating large embedding layer during training introduces other challenges. At training, transformer models process all tokens in parallel, generating high volume of read and write queries to the additional embedding layer. This makes offloading the layer from the accelerator less practical, thereby poses significant memory challenges for accelerators. While Huang et al. (2025) mitigate this by sharding the embedding layer across accelerators via tensor parallelism, this still limits the number of embeddings, with the largest tested configuration in Huang et al. (2025) being 12.8M. In contrast, our method avoids instantiating embeddings during training, allowing us to scale to 1B embeddings. Additionally, our design of Af-gram introduces new axis for leveraging extra training compute while keeping both the additional embedding layer size and inference-time FLOPS fixed. 6. Conclusion We introduce SCONE, scalable approach for generating ngram contextualized embeddings for each input token without increasing inference-time FLOPS. These embeddings are learned during training and cached in off-accelerator storage for inference. SCONE enables two new aspects for scaling language models: (1) scaling the number of cached contextualized embeddings and (2) scaling the model size for learning them, both while maintaining fixed inferencetime FLOPS. This is especially useful for latency-sensitive applications (Jones, 2021; Snell et al., 2024) and reducing serving costs. Future work could explore scaling embedding layers for other modalities; for instance, recent research underscores the importance of vocabulary size in visual modeling (Yu et al., 2023a)."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors would like to thank Andrew Tomkins for his helpful feedback on an early draft."
        },
        {
            "title": "References",
            "content": "Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya, P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Cohen, J., et al. Nemotron-4 340b technical report. arXiv:2406.11704, 2024. and Gosh, G. Memory layers at scale. arXiv:2412.09764, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. NIPS, 2020. Choe, D., Al-Rfou, R., Guo, M., Lee, H., and Constant, N. Bridging the gap for tokenizer-free language models. arXiv:1908.10322, 2019. Chu, H. Lightning memory-mapped database, 2011. URL http: //www.lmdb.tech/doc/. Accessed: 2025-01-23. Dagan, G., Synnaeve, G., and Roziere, B. Getting the most out of your tokenizer for pre-training and domain adaptation. In ICML, 2024. DeepSeek. Deepseek-v3 technical report. arXiv:2412.19437, 2024. DeepSpeed. Deepspeed, 2024. URL https://github.com/ microsoft/DeepSpeed. Accessed: 2025-01-19. Deiseroth, B., Brack, M., Schramowski, P., Kersting, K., and Weinbach, S. T-FREE: Subword tokenizer-free generative LLMs via sparse representations for memory-efficient embeddings. In EMNLP, 2024. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pretraining of deep bidirectional transformers for language understanding. arXiv:1810.04805, 2019. Dou, L., Liu, Q., Zeng, G., Guo, J., Zhou, J., Mao, X., Jin, Z., Lu, W., and Lin, M. Sailor: Open language models for south-East Asia. In EMNLP, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv:2407.21783, 2024. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. JMLR, 2022. Gage, P. new algorithm for data compression. The Users Journal, 1994. Google. Gemma 2: Improving open language models at practical size. arXiv:2408.00118, 2024. Groeneveld, D., Beltagy, I., Walsh, P., Bhagia, A., Kinney, R., Tafjord, O., Jha, A., Ivison, H., Magnusson, I., Wang, Y., Arora, S., Atkinson, D., Authur, R., Chandu, K. R., Cohan, A., Dumas, J., Elazar, Y., Gu, Y., Hessel, J., Khot, T., Merrill, W., Morrison, J. D., Muennighoff, N., Naik, A., Nam, C., Peters, M. E., Pyatkin, V., Ravichander, A., Schwenk, D., Shah, S., Smith, W., Strubell, E., Subramani, N., Wortsman, M., Dasigi, P., Lambert, N., Richardson, K., Zettlemoyer, L., Dodge, J., Lo, K., Soldaini, L., Smith, N. A., and Hajishirzi, H. Olmo: Accelerating the science of language models. arXiv:2402.00838, 2024. ArtificialAnlys. Independent analysis of ai models and api providers, 2025. URL https://artificialanalysis. ai/. Accessed: 2025-01-07. Gupta, P., Pagliardini, M., and Jaggi, M. Better word embeddings by disentangling contextual n-gram information. arXiv:1904.05033, 2019. Berges, V.-P., Oguz, B., Haziza, D., Yih, W.-t., Zettlemoyer, L., He, X. O. Mixture of million experts. arXiv:2407.04153, 2024."
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., et al. Training compute-optimal large language models. arXiv:2203.15556, 2022. Huang, H., Zhu, D., Wu, B., Zeng, Y., Wang, Y., Min, Q., and Zhou, X. Over-tokenized transformer: Vocabulary is generally worth scaling. arXiv preprint arXiv:2501.16975, 2025. McCann, B., Bradbury, J., Xiong, C., and Socher, R. Learned in translation: Contextualized word vectors. NIPS, 2017. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models. arXiv:1609.07843, 2016. Meta. Large concept models: Language modeling in sentence representation space. arXiv:2412.08821, 2024. Huang, Y., Zhang, J., Shan, Z., and He, J. Compression represents intelligence linearly. In COLM, 2024. Mikolov, T. Efficient estimation of word representations in vector space. arXiv:1301.3781, 3781, 2013. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F., et al. Mixtral of experts. arXiv:2401.04088, 2024. Johnson, J., Douze, M., and Jegou, H. Billion-scale similarity search with GPUs. IEEE Trans. Big Data, 2019. Jones, A. L. Scaling scaling laws with board games. arXiv:2104.03113, 2021. Joulin, A., Cisse, M., Grangier, D., Jegou, H., et al. Efficient In ICML, pp. 13021310, softmax approximation for gpus. 2017. Pagnoni, A., Pasunuru, R., Rodriguez, P., Nguyen, J., Muller, B., Li, M., Zhou, C., Yu, L., Weston, J., Zettlemoyer, L., et al. Byte latent transformer: Patches scale better than tokens. arXiv:2412.09871, 2024. Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and Zettlemoyer, L. Deep contextualized word representations. In NAACL-HLT, 2018. Peterson, J., Meylan, S., and Bourgin, D. Openwebtext, 2019. URL https://github.com/jcpeterson/ openwebtext. Accessed: 2025-01-19. Phuong, M. and Hutter, M. Formal algorithms for transformers. Kim, Y., Jernite, Y., Sontag, D., and Rush, A. Character-aware arXiv:2207.09238, 2022. neural language models. In AAAI, 2016. Kudo, T. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv:1808.06226, 2018a. Kudo, T. Subword regularization: Improving neural network translation models with multiple subword candidates. arXiv:1804.10959, 2018b. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In SOSP, 2023. Lample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., and Jegou, H. Large memory layers with product keys. Advances in Neural Information Processing Systems, 2019. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. {GS}hard: Scaling giant models with conditional computation and automatic sharding. In ICLR, 2021. Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving language understanding with unsupervised learning. Technical report, OpenAI, 2018. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. In ACL, 2016. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In ICLR, 2017. Shim, K., Lee, M., Choi, I., Boo, Y., and Sung, W. Svd-softmax: Fast softmax approximation on large vocabulary neural networks. NIPS, 30, 2017. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv:2408.03314, 2024. Liang, D., Gonen, H., Mao, Y., Hou, R., Goyal, N., Ghazvininejad, M., Zettlemoyer, L., and Khabsa, M. XLM-V: Overcoming the vocabulary bottleneck in multilingual masked language models. In EMNLP, 2023. Liao, X., Huang, Y., Wei, C., Zhang, C., Deng, Y., and Yi, K. Efficient estimate of low-frequency words embeddings based on the dictionary: case study on chinese. Applied Sciences, 2021. Soldaini, L., Kinney, R., Bhagia, A., Schwenk, D., Atkinson, D., Authur, R., Bogin, B., Chandu, K., Dumas, J., Elazar, Y., Hofmann, V., Jha, A., Kumar, S., Lucy, L., Lyu, X., Lambert, N., Magnusson, I., Morrison, J., Muennighoff, N., Naik, A., Nam, C., Peters, M., Ravichander, A., Richardson, K., Shen, Z., Strubell, E., Subramani, N., Tafjord, O., Walsh, E., Zettlemoyer, L., Smith, N., Hajishirzi, H., Beltagy, I., Groeneveld, D., Dodge, J., and Lo, K. Dolma: an open corpus of three trillion tokens for language model pretraining research. In ACL, 2024. Loshchilov, I. arXiv:1711.05101, 2017. Decoupled weight decay regularization. Sukhbaatar, S., Weston, J., Fergus, R., et al. End-to-end memory networks. NIPS, 2015. McCallum, J. C. Price and performance changes of computer technology with time, 2024. URL https://jcmit.net/ index.htm. Accessed: 2025-01-20. Tao, C., Liu, Q., Dou, L., Muennighoff, N., Wan, Z., Luo, P., Lin, M., and Wong, N. Scaling laws with vocabulary: Larger models deserve larger vocabularies. In NeurIPS, 2024."
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. Wang, H., Yu, D., Sun, K., Chen, J., and Yu, D. Improving pre-trained multilingual model with vocabulary expansion. In CoNLL, 2019. Wang, J., Gangavarapu, T., Yan, J. N., and Rush, A. M. MamIn COLM, babyte: Token-free selective state space model. 2024. Weston, J., Chopra, S., and Bordes, A. Memory networks. NIPS, 2015. Wu, Y. Googles neural machine translation system: Bridging the gap between human and machine translation. arXiv:1609.08144, 2016. Xue, L., Barua, A., Constant, N., Al-Rfou, R., Narang, S., Kale, M., Roberts, A., and Raffel, C. Byt5: Towards token-free future with pre-trained byte-to-byte models. TACL, 2022. Yu, L., Lezama, J., Gundavarapu, N. B., Versari, L., Sohn, K., Minnen, D., Cheng, Y., Birodkar, V., Gupta, A., Gu, X., et al. Language model beats diffusiontokenizer is key to visual generation. arXiv:2310.05737, 2023a. Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M. Megabyte: Predicting million-byte sequences with multiscale transformers. NeurIPS, 2023b. Zheng, B., Dong, L., Huang, S., Singhal, S., Che, W., Liu, T., Song, X., and Wei, F. Allocating large vocabulary capacity for cross-lingual language model pre-training. In EMNLP, 2021."
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "Algorithm 3 Basic Next-Word Prediction Model MT ,A,D. Parameters: : Vtoken Rd: token embedding layer, : (Rd)Nmax Rd: transformer model, : Rd Vtoken : prediction head. Input: (σ1, . . . , σm) Output: Probability distribution over next token ˆσm+1. for = 1, . . . , do token for Nmax. ei (σi) : Input embedding per token eout A(e1, . . . , em): Output embedding. return D(eout). A. Additional Related Work Mixture of Experts (MoE) and Memory Layers are two well-established methods for scaling language models within fixed FLOPS budget, as discussed below. Mixture of Experts MoE replaces traditional feedforward layers with parallel expert layers, activating only one (or few) per token via lightweight router (Shazeer et al., 2017; Lepikhin et al., 2021; Fedus et al., 2022; Jiang et al., 2024; He, 2024). This allows scaling by increasing the number of experts without increasing active experts per token. However, all experts must reside on the accelerator, leading to higher memory usage. Memory Layers Memory layers store large sets of embeddings (continuous vectors) and retrieve nearest-neighbor embeddings during the forward pass via (approximate) similarity search (Weston et al., 2015; Sukhbaatar et al., 2015; Lample et al., 2019; Berges et al., 2024). These retrieved embeddings contribute to computations without adding to FLOPS. Despite advancements in similarity search (Lample et al., 2019; Johnson et al., 2019), memory layers still need to reside on accelerators, which increases memory demands to impractical levels at larger scales (Berges et al., 2024). Moreover, the embeddings in memory layers are typically updated through backpropagation, which also introduces sparse update challenges as memory scales. Our approach focuses on input embedding layers, which we demonstrate can be efficiently offloaded from accelerators. This ensures constant memory usage and fixed FLOPS on the accelerator during inference. Additionally, by modifying only the input embedding layer, our method integrates seamlessly with both MoE and memory layer techniques. B. Additional Algorithms Algorithm 4 Next-word prediction with SCONE MT ,Vf-gram,Af-gramF ,Amain,D Parameters: : Vtoken Rd: token embedding layer, Vf-gram [2,n] TRAINING: f-gram transformer model Af-gram : (Rd)n Rd, INFERENCE: f-gram embedding layer token: set of f-grams, : Vf-gram Rd. Amain : (Rd)Nmax Rd: main transformer model : Rd Vtoken : Prediction head. Input: (σ1, . . . , σm) Output: Probability distribution over next token ˆσm+1. (e1, . . . , em) FT ,Vf-gram,Af-gramF (σ1, . . . , σm) (Algorithm 2) eout Amain(e1, . . . , em) return D(eout). token for Nmax. In Section 2, we discuss simple next-word prediction model, MT ,A,D, consisting of token embedding layer ,"
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "transformer model A, and prediction head D. This model takes token sequence (σ1, . . . , σm), with each token from the token vocabulary Vtoken, and produces probability distribution for the next token. We provide the pseudocode for MT ,A,D in Algorithm 3. In Algorithm 2 in Section 3, we present the pseudocode for SCONEs process of generating contextualized f-gram embeddings. Next, we describe the end-to-end next-word prediction process using SCONE (Algorithm 4). Specifically, the process, denoted as MT ,Vf-gram,Af-gramF ,Amain,D, takes an input sequence (σ1, . . . , σm) token and produces distribution over the next token ˆσm+1. Note that in Algorithm 4, f-gram embeddings are generated with Af-gram during training and retrieved from lookup table during inference. C. Challenges of Scaling Vocabulary Size in Embedding Layers Scaling the vocabulary size is the most straightforward way to enlarge an embedding layer, but we find that larger vocabularies degrade performance beyond certain threshold and significantly increase accelerator usage during decoding. We pre-train GPT-2 models (Radford et al., 2019) with three sizes of non-embedding parameters: 85M (small), 302M (medium), and 708M (large) on the WebText dataset (Peterson et al., 2019), testing six vocabulary sizes ranging from 32,768 to 2,097,152. The tokenizers are trained using the BPE algorithm (Gage, 1994; Sennrich et al., 2016). We follow the implementation in Tao et al. (2024), which allows token merges across word boundaries. Each model is trained on 80 billion tokens. Since larger vocabularies produce fewer tokens for the same dataset, they effectively enable models to process more data. Additional implementation details are provided in Appendix E.1. Figure 8. BPC of three model sizes on the validation set (lower is better). For all three model sizes, BPC initially improves as vocabulary size increases but eventually deteriorates. Figure 8 presents the average bits per character (BPC) on the WebText validation set. We report BPC instead of cross-entropy loss because the latter is sensitive to vocabulary size, with larger vocabularies typically producing higher losses. BPC, by contrast, is common vocabulary-insensitive metric for comparing models trained with different tokenizers (Huang et al., 2024). We observe that BPC for all three models initially improves with larger vocabulary sizes but eventually deteriorates. Figure 9 shows the percentages of tokens that receive more than given number of updates over 100 million training tokens. In standard embedding layers, gradients are directly backpropagated to the embedding vectors. With fixed number of training tokens, larger vocabularies lead to fewer updates per token. For vocabulary size of 2,097,152, only 7.3% of tokens receive more than 100 updates, compared to 97.6% for vocabulary size of 32,768. This suggests that the performance drop for larger vocabularies may stem from sparse updates to per-token embedding vectors. In addition to performance degradation, increasing the vocabulary size significantly raises accelerator usage during the inference stage. This is because predicting the next token involves running linear layer and softmax operation across the entire vocabulary to identify the closest embedding. Figure 10 illustrates that both the number of embedding layer parameters stored on the GPU and the GPU memory cost increase linearly with vocabulary size. These costs are measured using batch size of 1, sequence length of 1024, and 16-bit precision."
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "Figure 9. Percentages of tokens (y-axis) that receive more than given number of updates (x-axis), measured over 100 million training tokens. As the vocabulary size increases, tokens receive increasingly sparse updates. Figure 10. Number of embedding layer parameters on the GPU and corresponding GPU memory usage. Computational costs increase linearly with vocabulary size. D. Additional Experiments D.1. More Results for Training on WebText Varying Maximum f-gram Length. In Section 4.1.1, we discuss the impact of varying the maximum f-gram length in Vf-gram and present results on Wikitext-103. We observe that relatively small maximum length is sufficient, as long as it is not too small, otherwise, the number of available n-grams for ranking becomes too limited. Here, in Figure 11, we show the corresponding results on WebText, which exhibit similar trends. The left y-axis represents the evaluation loss (averaged over three seeds), with the leftmost star indicating baseline performance. The right y-axis shows the average length of matched f-grams. As the maximum size increases, the loss initially decreases but then plateaus with some fluctuations. Meanwhile, the matched length rises initially before stabilizing for larger values. Varying Af -gram Model Size. In Section 4.1.3, we discuss the impact of varying the size of Af-gram on evaluation perplexity for Wikitext-103. We find that increasing the model size leads to further performance improvements for fixed Vf-gram. In Figure 12, we present the results on WebText, which show similar trend. Model sizes in the legend correspond to inference-time sizes on accelerators. Dashed lines and stars on the left represent baseline performance. The evaluation perplexity improves as the size of Af-gram grows."
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "Figure 11. Effect of the maximum f-gram length in Vf-gram, evaluated on the WebText validation split. Figure 12. Evaluation perplexity on WebText as function of the size of Af-gram. Figure 13. Average perplexity on the OLMo evaluation mixture throughout training. Models with SCONE enabled converge later, indicating stronger capacity, and achieve better perplexity. D.2. More Results for Training on OLMo Corpus Loss Curves over Training. In Section 4.2, we pre-train four model variants with sizes of 0.7B, 1B, 1.3B, and 1.9B, evaluating SCONE on the first three. Each model is trained for 200B tokens, uniformly sampled from the OLMo-tokenized training corpus. The training token count is roughly 10 times more than the compute-optimal token count for 1B model"
        },
        {
            "title": "Scaling Embedding Layers in Language Models",
            "content": "suggested by Hoffmann et al. (2022), to ensure near-convergence. Figure 13 shows the evaluation loss throughout training. The loss curves indicate that models trained with SCONE converge later, suggesting that it effectively expands model capacity. E. Implementation Details Here, we provide additional implementation details. While f-gram lookup is efficient for inference, it creates bottleneck during training since at training time transformer models process all token positions in parallel. This leads to GPU idle time when fetching the longest matching f-gram on the fly. To remove this bottleneck, after we construct the set of f-grams (Vf-gram), we pre-scan the training sequences to tag the longest matching length for each token. During training, we can then directly retrieve the corresponding f-gram for forward computation with the Af-gram model. For the Af-gram model, we use an absolute position embedding layer where the maximum position equals the longest n-gram in Vf-gram. Within each batch, all f-grams are padded to the longest n-gram length in that batch. We train all models with the bfloat16 precision. E.1. WebText Parameters (million) model ffw size layers 128 204 491 759 589 1099 1024 1024 1536 1536 1536 1536 4096 4096 6144 6144 6144 6 12 12 24 18 36 Table 3. Baseline model configurations for pre-training on WebText. For constructing the f-gram model (Af-gram), we vary the number of layers in the 128M, 491M, and 589M variants and discard the token embedding layer. For pre-training on WebText (Peterson et al., 2019), we follow Radford et al. (2019) and set the batch size and sequence length to 512 and 1024, respectively. Radford et al. (2019) do not specify the number of training tokens or optimizer details. We train the models for 80B tokens, roughly doubling the count in Radford et al. (2018). For optimization, we use AdamW (Loshchilov, 2017) with weight decay of 0.1. Following Hoffmann et al. (2022), we set the maximum learning rate to 2 104 and apply cosine learning rate scheduler. We list the model configurations in Table 3. E.2. OLMo Tokenized Training Corpus Parameters (million) model ffw size layers 711 1014 1316 1920 2048 2048 2048 2048 8192 8192 8192 8192 12 18 24 Table 4. Baseline model configurations for pre-training on OLMo corpus. For constructing the f-gram model (Af-gram), we use the 711M and 1920M configurations and discard the token embedding layers. For pre-training on the OLMo tokenized training corpus, we follow the optimizer settings for the 1B variant in Groeneveld et al. (2024) 3. All models use sequence length of 2048 and are trained on 200B tokens from sequences uniformly sampled from the corpus. We use DeepSpeed (DeepSpeed, 2024) with ZeRO stage 1 to reduce GPU memory usage. ZeRO stage 1 partitions the optimizer state across GPUs. Our hardware supports training models up to training-time size of 3B parameters. We list the model configurations in Table 4. 3https://github.com/allenai/OLMo/blob/v0.4.0/configs/official/OLMo-1B.yaml#L"
        }
    ],
    "affiliations": [
        "Google"
    ]
}