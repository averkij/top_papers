{
    "paper_title": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion",
    "authors": [
        "Hau-Shiang Shiu",
        "Chin-Yang Lin",
        "Zhixiang Wang",
        "Chi-Wei Hsiao",
        "Po-Fan Yu",
        "Yu-Chih Chen",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/"
        },
        {
            "title": "Start",
            "content": "Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion Hau-Shiang Shiu1 Chin-Yang Lin1 Zhixiang Wang2 Chi-Wei Hsiao3 1National Yang Ming Chiao Tung University Po-Fan Yu1 Yu-Chih Chen1 Yu-Lun Liu1 2Shanda AI Research Tokyo 3MediaTek Inc. 5 2 0 2 9 2 ] . [ 1 9 0 7 3 2 . 2 1 5 2 : r Figure 1. Comparison of visual quality and inference speed across various categories of VSR methods. Stream-DiffVSR achieves superior perceptual quality (lower LPIPS) and maintains comparable runtime to CNNand Transformer-based online models, while also demonstrating significantly reduced inference latency compared to existing offline approaches. Best and second-best results are marked in red and green."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose StreamDiffVSR, causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and lightweight temporal-aware decoder with Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP [99], it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130. StreamDiffVSR achieves the lowest latency reported for diffusionbased VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/streamdiffvsr-project-page/ Video super-resolution (VSR) aims to reconstruct highresolution (HR) videos from low-resolution (LR) inputs and is vital in applications such as surveillance, live broadcasting, video conferencing, autonomous driving, and drone imaging. It is increasingly important in low-latency rendering workflows, including neural rendering and resolution upscaling in game engines and AR/VR systems, where latency-aware processing is crucial for visual continuity. Specifically, latency-sensitive processing involves two key aspects: per-frame inference time (throughput) and end-to-end system latency (delay between receiving an input frame and producing its output). Existing VSR methods often struggle with this trade-off. While CNNand Transformer-based models offer balance between efficiency and quality, they fall short in perceptual detail. Diffusionbased models excel in perceptual quality due to strong generative priors, but suffer from high computational cost and reliance on future frames, making them impractical for timesensitive video applications. In this paper, we propose Stream-DiffVSR, diffusionbased method specifically tailored to online video superresolution, effectively bridging the gap between high-quality but slow diffusion methods and fast but lower quality CNNor Transformer-based methods. Unlike previous diffusionbased VSR approaches (e.g., StableVSR [58] and MGLDVSR [89]) that typically require 50 or more denoising steps and bidirectional temporal information, our method leverages diffusion model distillation to significantly accelerate inference by reducing denoising steps to just four. Additionally, we introduce an Auto-regressive Temporal Guidance mechanism and an Auto-regressive Temporal-aware Decoder to effectively exploit temporal information from previous frames, significantly enhancing temporal consistency and perceptual fidelity. Fig. 1 illustrates the core advantage of our approach by comparing visual quality and runtime across various categories of video super-resolution methods. Our StreamDiffVSR achieves superior perceptual quality (measured by LPIPS [96]) and temporal consistency, outperforming existing unidirectional CNNand Transformer-based methods (e.g., MIA-VSR [105], RealViformer [98], TMP [99]). Notably, Stream-DiffVSR offers significantly faster per-frame inference than prior diffusion-based approaches (e.g., StableVSR [58], MGLD-VSR [89]), attributed to our use of distilled 4-step denoising process and lightweight temporalaware decoder. In addition, existing diffusion-based methods, such as StableVSR [58] typically rely on bidirectional or future-frame information, resulting in prohibitively high processing latency that is not suitable for online scenarios. Specifically, for 100-frame video, StableVSR (46.2 s/frame) would incur an initial latency exceeding 4600 seconds on an RTX 4090 GPU, as it requires processing the entire sequence before generating even the first output frame. In contrast, our Stream-DiffVSR operates in strictly causal, autoregressive manner, conditioning only on the immediately preceding frame. Consequently, the initial frame latency of StreamDiffVSR corresponds to single frames inference time (0.328 s/frame), reducing the latency by more than three orders of magnitude compared to StableVSR. This significant latency reduction demonstrates that Stream-DiffVSR effectively unlocks the potential of diffusion models for practical, low-latency online video super-resolution. To summarize, the main contributions of this paper are: We introduce the first diffusion-based framework explicitly designed for online, low-latency video super-resolution, achieving efficient inference through distillation from 50 denoising steps down to 4 steps. We propose novel Auto-regressive Temporal Guidance mechanism and Temporal-aware Decoder to effectively leverage temporal information only from past frames, significantly enhancing perceptual quality and temporal consistency. Extensive experiments demonstrate that our approach outperforms existing methods across key perceptual and temporal consistency metrics while achieving practical inferTable 1. Comparison of diffusion-based VSR methods. We report online capability, inference steps, runtime (FPS on 720p, RTX 4090), maximum end-to-end latency (sec), and whether each method uses distillation, temporal modeling, or offline future frames. OOM denotes out-of-memory, and - indicates missing public inference results. Notably, Stream-DiffVSR is the only diffusion-based method that runs in strictly online, past-only setting with the lowest latency. Method FPS Online Steps @720p latency Distill Max # of StableVSR [58] MGLD-VSR [89] Upscale-A-Video [104] DiffVSR [34] VEnhancer [21] Stream-DiffVSR (ours) 50 50 30 15 4 0.02 0.02 OOM OOM 3.05 4620 218 - - - 0.328 Temporal Input Future/Bi-dir Future/Bi-dir Future/Bi-dir Future/Bi-dir Future/Bi-dir Past-only Temporal Decoder ence speeds, thereby making diffusion-based VSR applicable for real-world online scenarios. To contextualize our contributions, Table 1 compares recent diffusion-based VSR methods in terms of online inference capability, runtime efficiency, and temporal modeling. Our method uniquely achieves online low-latency inference while preserving high visual quality and temporal stability. This substantial latency reduction of over three orders of magnitude compared to prior diffusion-based VSR models demonstrates that Stream-DiffVSR is uniquely suited for low-latency online applications such as video conferencing and AR/VR. 2. Related Work Video Super-resolution. VSR methods reconstruct highresolution videos from low-resolution inputs through CNNbased approaches [4, 5, 68, 70, 79, 87], deformable convolutions [12, 70, 107], online processing [99], recurrent architectures [15, 25, 33, 61, 91], flow-guided methods [19, 43, 92], and Transformer-based models [36, 37, 63, 73, 105]. Despite advances, low-latency online processing remains challenging. Real-world Video Super-resolution. Real-world VSR addresses unknown degradations [6, 88] through pre-cleaning modules [6, 18, 44, 80], online approaches [98], kernel estimation [28, 54], synthetic degradations [7, 27, 65, 97], new benchmarks [11, 102], real-time systems [3], advanced GANs [9, 72], and Transformer restorers [2, 35, 93]. Warp error-aware consistency [31] emphasizes temporal error regularisation. Diffusion-based Image and Video Restoration. Diffusion models provide powerful generative priors [8, 14, 55] for single-image SR [24, 32, 60], inpainting [40, 47, 71, 81], and quality enhancement [16, 23, 77]. Video diffusion methods include StableVSR [58], MGLD-VSR [89], DC-VSR [20], DOVE [10], UltraVSR [42], Upscale-A-Video [104], DiffVSR [34], DiffIR2VR-Zero [90], VideoGigaGAN [86], 2 VEnhancer [21], temporal coherence [76], AVID [100], and SeedVR2 [78]. Auto-regressive approaches [39, 67, 84, 101] show promise. Acceleration techniques include consistency models [17, 48], advanced solvers [45, 46, 103], flow-based methods [29, 41], distillation [50, 62, 85, 106, 108], and efficient architectures [1]. Theoretical advances [74, 75] and recent image/offline distillation methods [66, 82, 83, 94] exist, but our Stream-DiffVSR uniquely applies distillation in strict online settings with causal temporal modeling for real-time VSR. 3. Method We propose Stream-DiffVSR, streamable auto-regressive diffusion framework for efficient video super-resolution (VSR). Its core innovation lies in an auto-regressive formulation that improves both temporal consistency and inference speed. The framework comprises: (1) distilled few-step U-Net for accelerated diffusion inference, (2) Autoregressive Temporal Guidance that conditions latent denoising on previously warped high-quality frames, and (3) an Auto-regressive Temporal-aware Decoder that explicitly incorporates temporal cues. Together, these components enable Stream-DiffVSR to produce stable and perceptually coherent videos. 3.1. Diffusion Models Preliminaries Diffusion Models [22] transform complex data distributions into simpler Gaussian distributions via forward diffusion process and reconstruct the original data using learned reverse denoising process. The forward process gradually adds Gaussian noise to the initial data x0, forming 1 βt xt1, βtI(cid:1) Markov chain: q(xt xt1) = N(cid:0)xt; for = 1, . . . , , where βt denotes predefined noise schedule. At timestep t, the noised data xt can be directly sampled from the clean data x0 as: xt = αt x0 + 1 αt ϵ, where ϵ (0, I) and αt = (cid:81)t i=1(1 βi), where αt = (cid:81)t i=1(1 βi). The reverse process progressively removes noise from xT , reconstructing the original data x0 through learned denoising operation modeled as Markov chain, i.e., pθ(x0, . . . , xT 1 xT ) = (cid:81)T t=1 pθ(xt1 xt). Each individual step is parameterized by neural network-based denoising function pθ(xt1 xt) = N(cid:0)xt1; µθ(xt, t), Σθ(t)I(cid:1). Typically, the network predicts the noise component ϵθ(xt, t), from which the denoising mean is estimated as µθ(xt, t) = 1 xt αt (cid:16) 1αt 1αt ϵθ(xt, t) . Latent Diffusion Models (LDMs) [57] further reduce computational complexity by projecting data into lower-dimensional latent space using Variational Autoencoders (VAEs), significantly accelerating inference without sacrificing generative quality. (cid:17) Figure 2. Overview of Auto-regressive Temporal-aware Decoder. Given the denoised latent and warped previous frame, our decoder enhances temporal consistency using temporal processor modules. This module aligns and fuses these features via interpolation, convolution, and weighted fusion, effectively stabilizing detail reconstruction when decoding into the final RGB frame. 3.2. U-Net Rollout Distillation We distill pre-trained Stable Diffusion (SD) 4 Upscaler [56, 57], originally designed for 50-step inference, into 4-step variant that balances speed and perceptual quality. To mitigate the traininginference gap of timestep-sampling distillation, we adopt rollout distillation, where the U-Net performs the full 4-step denoising each iteration to obtain clean latent. Detailed algorithms and implementation are provided in the supplementary material due to page limits. Unlike conventional distillation that supervises random intermediate timesteps, our method applies loss only on the final denoised latent, ensuring the training trajectory mirrors inference and improving stability and alignment. Our distillation requires no architectural changes. We train the U-Net by optimizing latent reconstruction with loss that balances spatial accuracy, perceptual fidelity, and realism: Ldistill = zden zgt2 2 + λLPIPS LPIPS (D(zden), xgt) + λGAN LGAN (D(zden)) , (1) where zden and zgt are the denoised and ground-truth latent representations. The decoder D() maps latent features back to RGB space for perceptual (LPIPS) and adversarial (GAN) loss calculations, encouraging visually realistic outputs. 3.3. Auto-regressive Temporal Guidance Leveraging temporal information is essential for capturing dynamics and ensuring frame continuity in video superresolution. However, extensive temporal reasoning often incurs significant computational overhead, increasing perframe inference time and system latency. Thus, efficient online VSR requires carefully balancing temporal utilization and computational cost to support low-latency processing. To this end, we propose Auto-regressive Temporal Guidance (ARTG), which enforces temporal coherence during 3 Figure 3. Training pipeline of Stream-DiffVSR. The training process consists of three sequential stages: (1) Distilling the denoising U-Net to reduce diffusion steps while maintaining perceptual quality with training objective (1); (2) Training the Temporal Processor Module (TPM) within the decoder to enhance temporal consistency at the RGB level with training objective (3); (3) Training the Auto-Regressive Temporal Guidance (ARTG) module to leverage previously restored high-quality frames for improved temporal coherence with training objective (6). Each module is trained separately before integrating them into the final framework. latent denoising. At each timestep t, the U-Net takes both the current noised latent zt and the warped RGB frame from the previous output, ˆxwarp t1, ftt1), where ftt1 is the optical flow from frame t1 to t. The denoising prediction is then formulated as: t1 = Warp(xSR ˆϵθ = UNet(zt, t, ˆxwarp t1 ), (2) where the warped image ˆxwarp ing input to guide the denoising process. t1 serves as temporal conditionWe train the ARTG module independently using consecutive pairs of low-quality and high-quality frames. The denoising U-Net and decoder are kept fixed during this stage, and the training objective focuses on reconstructing the target latent representation while preserving perceptual quality and visual realism. The total loss function is defined as: LARTG = zden zgt2 + λLPIPS LPIPS(D(zden), xgt) + λGAN LGAN(D(zden)), (3) where zden denotes the denoised latent from DDIM updates with predicted noise ˆϵθ, and zgt is the ground-truth latent. The decoder D() maps latents to RGB, producing D(zden) for comparison with the ground-truth image xgt. The latent ℓ2 loss enforces alignment, the perceptual loss preserves visual fidelity, and the adversarial loss promotes realism. This design leverages only past frames to propagate temporal context, improving consistency without additional latency. 3.4. Auto-regressive Temporal-aware Decoder Although the Auto-regressive Temporal Guidance (ARTG) improves temporal consistency in the latent space, the features produced by the Stable Diffusion 4 Upscaler remain at one-quarter of the target resolution. This mismatch may introduce decoding artifacts or misalignment in dynamic scenes. To address this issue, we propose an Auto-regressive Temporal-aware Decoder that incorporates temporal context into decoding to enhance spatial fidelity and temporal consistency. At timestep t, the decoder takes the denoised and the aligned feature ˆft1 derived from the latent zden previous super-resolved frame. Specifically, we compute: ˆxwarp t1 = Warp(xSR t1, ftt1), ˆft1 = Enc(ˆxwarp t1 ), (4) where xSR t1 is the previously generated RGB output, ftt1 is the optical flow from frame 1 to t, and Enc() is frozen encoder that projects the warped image into the latent feature space. The decoder then synthesizes the current frame using: = Decoder(zden xSR , ˆft1). (5) We adopt multi-scale fusion strategy inside the decoder to combine current spatial information and prior temporal features across multiple resolution levels, as illustrated in Fig. 2. This design helps reinforce temporal coherence while recovering fine spatial details. Temporal Processor Module (TPM). We integrate TPM after each spatial convolutional layer in the decoder to explicitly inject temporal coherence, enhancing stability and continuity of reconstructed frames. These modules utilize latent features from the current frame and warped features from the previous frame, optimizing temporal consistency independently from spatial reconstruction. Our training objective for the TPM is defined as: + λflow , xGT LTPM = Lrec(xrec ) (cid:13) (cid:13)OF(xrec (cid:13) + λGANLGAN(xrec ) + λLPIPSLPIPS(xrec , xGT ), , xrec t1) OF(xGT , xGT t1) (cid:13) (cid:13) (cid:13) 2 (6) Figure 4. Overview of our pipeline. Given low-quality (LQ) input frame, we first initialize its latent representation and employ an autoregressive diffusion model composed of distilled denoising U-Net, autoregressive temporal Guidance, and an autoregressive temporal Decoder. Temporal guidance utilizes flow-warped high-quality (HQ) results from the previous frame to condition the current frames latent denoising and decoding processes, significantly improving perceptual quality and temporal consistency in an efficient, online manner. Table 2. Quantitative comparison against bidirectional/offline methods on the REDS4 dataset. We compare CNN-, Transformer-, and diffusion-based methods on REDS4. Stream-DiffVSR achieves superior perceptual and temporal quality with high stability across sequences. indicates higher is better; indicates lower is better. Dir. denotes temporal direction: for bidirectional/offline, for unidirectional/online. Runtime is measured per 720p frame on an RTX 4090. Latency-max denotes the maximum end-to-end latency measured over 100-frame video sequences, providing fair comparison with offline methods whose initial delay scales with sequence length. tLP and tOF are scaled by 100 and 10. Best and second-best results are marked in red and blue. Dir. Method PSNR SSIM LPIPS DISTS NIQE NRQM BRISQUE tLP tOF Runtime (s) latency-max (s) CNN-based Methods - Bicubic 25.501 32.386 BasicVSR++ RealBasicVSR 27.042 0.712 0.907 0.778 0.460 0.132 0. 0.187 0.069 0.065 7.360 3.850 2.530 3.459 6.363 6.769 60.256 38.641 18.046 21.603 4.241 9.017 2.490 6.422 4.759 - 0.098 0. Transformer-based Methods RVRT MIA-VSR 32.701 32.790 0.911 0.912 0.130 0.123 0.067 0. 3.793 3.742 6.366 6.451 38.038 37.099 9.133 2.421 8.870 2.354 0.498 0.768 StableVSR MGLD-VSR Ours Diffusion-based Methods 27.928 26.53 27.256 0.793 0.749 0.766 0.102 0.151 0. 0.047 0.065 0.062 2.713 2.972 3.114 6.960 6.701 7.055 16.249 15.291 17.717 5.755 2.742 18.139 5.910 4.198 3.638 46.2 43.6 0. - 9.8 6.4 2.49 76.8 4620 218 0.328 R3HW is the predicted frame at time t, where xSR and xGT is the ground-truth frame. The reconstruction loss Lrec = SmoothL1(xrec ) enforces spatial fidelity, the adversarial loss LGAN improves realism, and the opticalflow term OF(, ) reduces temporal discrepancies, yielding consistent and perceptually faithful outputs. , xGT 3.5. Training and Inference Stages Our training pipeline consists of three independent stages  (Fig. 3)  , while our inference process and the Auto-Regressive Diffusion-based VSR algorithm are illustrated in Fig. 4 and detailed in the appendix due to page constraints, respectively. 5 Distilling the Denoising U-Net. We first distill the denoising U-Net using pairs of low-quality (LQ) and high-quality (HQ) frames to optimize per-frame super-resolution and latentspace consistency. Training the Temporal Processor Module (TPM). In parallel, we train the Temporal Processor Module (TPM) in the decoder using ground-truth frames, keeping all other weights fixed. This enhances the decoders capability to incorporate temporal information into the final RGB reconstruction. Training Auto-regressive Temporal Guidance. After training and freezing the U-Net and decoder, we train the ARTG, Table 3. Quantitative comparison against unidirectional/online methods on the REDS4 dataset. Dir. Method PSNR SSIM LPIPS DISTS MUSIQ NIQE NRQM BRISQUE tLP tOF Runtime (s) latency-max (s) CNN-based Methods - Bicubic TMP 25.501 30.672 0.712 0.871 0.460 0.194 0.187 0. 27.362 63.818 7.360 4.378 3.459 5.796 60.256 43.394 21.603 4.241 10.424 2.480 - 0. - 0.041 RealViformer 26.763 0.761 0.129 0. 64.585 2.731 7.028 17.272 11.261 4.037 0. 9.9 Transformer-based Methods Diffusion-based Methods StableVSR* Ours 27.174 27. 0.763 0.766 0.111 0.099 0.051 0.062 66.428 65.595 2.572 3.114 6.944 7. 15.805 17.117 11.107 3.925 4.198 3.638 46.2 0.328 4620 0.328 Table 4. Quantitative comparison against bidirectional/offline methods on the Vimeo-90K-T dataset. Stream-DiffVSR surpasses other bidirectional methods in perceptual quality, temporal consistency, and runtime. Runtime is the average per-frame inference time (seconds) on 448256 videos using an RTX 4090. Best and second-best results are shown in red and blue. Dir. Method PSNR SSIM LPIPS DISTS MUSIQ NIQE NRQM BRISQUE tLP tOF Runtime (s) latency-max (s) CNN-based Methods - 29.282 Bicubic BasicVSR++ 37.479 RealBasicVSR 29. 0.864 0.956 0.857 0.297 0.098 0.156 0.209 0.117 0.149 23.433 51.940 56.986 8.735 7.077 5.069 3.588 5.509 7. 61.714 47.792 23.822 11.606 2.49 4.691 1.57 10.947 3.46 - 0.012 0.008 Transformer-based Methods RVRT MIA-VSR 37.815 37. 0.955 0.957 0.093 0.086 0.105 0.101 49.937 51.402 7.205 7.116 5.393 5. 48.352 47.865 4.873 1.429 4.696 1.419 0.061 0.096 StableVSR MGLD-VSR Ours Diffusion-based Methods 31.823 29.651 32.593 0.878 0.865 0.900 0.095 0.151 0.056 0.111 0.137 0.105 54.582 57.788 52. 4.745 5.340 4.403 7.265 7.217 7.672 20.039 20.761 29.297 26.224 3.108 12.550 4.661 4.307 2.689 5.749 5.426 0.041 - 0.084 0. 0.305 0.672 40.243 27.130 0.041 which leverages flow-aligned previous outputs to enhance temporal coherence without degrading spatial quality. This staged training strategy progressively refines spatial fidelity, latent consistency, and temporal smoothness in decoupled manner. Inference. Given sequence of low-quality (LQ) frames, our method auto-regressively generates high-quality (HQ) outputs. For each frame t, denoising is conditioned on the previous output HQt1, warped via optical flow to capture temporal motion. To balance quality and efficiency, we employ 4-step DDIM scheme using distilled U-Net. By combining motion alignment with reduced denoising steps, our inference pipeline achieves efficient and stable temporal consistency. 4. Experiment Due to space limitations, we provide the experimental setup in the appendix. We quantitatively compare Stream-DiffVSR with stateof-the-art VSR methods on REDS4, Vimeo-90K-T, VideoLQ, and Vid4, covering diverse scene content and motion characteristics. Tabs. 2 and 4 report results across CNN-, Transformer-, and diffusion-based approaches under both bidirectional (offline) and unidirectional (online) settings. On REDS4, Stream-DiffVSR achieves superior perceptual quality (LPIPS=0.099) over CNN (BasicVSR++, RealBasicVSR), Transformer (RVRT), and diffusion-based methods (StableVSR, MGLD-VSR), while also delivering competitive temporal consistency (tLP=4.198, tOF=3.638). Notably, it attains these gains with substantially lower runtime (0.328s/frame vs. 4346s/frame for diffusion models). On Vimeo-90K-T, Stream-DiffVSR likewise attains leading perceptual performance (LPIPS=0.056, DISTS=0.105) and improved temporal consistency (tLP=4.307, tOF=2.689) with competitive runtime of 0.041s/frame, highlighting its suitability for online deployment. In addition to speed, Stream-DiffVSR achieves markedly lower memory footprint. As shown in Tab. 6, prior diffusion-based VSR methods such as DOVE, SeedVR2, and Upscale-A-Video either require over 42 GB of GPU memory or fail with out-of-memory errors on an NVIDIA A6000. In contrast, Stream-DiffVSR operates within 20.8 GB while running more than 2.5 faster, underscoring its efficiency and deployability. Results on VideoLQ and Vid4 further confirm strong perceptual and temporal performance, demonstrating robust generalization across the entire evaluation dataset. 6 Table 5. Quantitative comparison against unidirectional/online methods on the Vimeo-90K-T dataset. Dir. Method PSNR SSIM LPIPS DISTS MUSIQ NIQE NRQM BRISQUE tLP tOF Runtime (s) latency-max (s) CNN-based Methods Bicubic - TMP 29.282 36.482 0.864 0.946 0.297 0. 0.209 0.118 23.433 48.374 8.735 7.368 3.588 5.096 61.714 49.192 11.606 2.49 4.870 1. - 0.006 - 0.006 Transformer-based Methods RealViformer 30.291 0.877 0. 0.140 53.107 5.515 6.711 24.628 8.232 2. 0.013 0.091 Diffusion-based Methods StableVSR* Ours 31.729 32.593 0.875 0. 0.072 0.056 0.113 0.105 54.447 52.755 4.698 4.403 7.280 7.672 19.836 29. 30.858 3.144 4.307 2.689 5.749 0.041 40.243 0.041 Figure 5. Qualitative comparison on REDS4 and Vimeo-90K-T datasets. Our method demonstrates superior visual quality with sharper details compared to unidirectional methods (TMP [99], RealViformer [98]) and competitive performance against bidirectional methods (StableVSR [58], MGLD-VSR [89], RVRT [37], BasicVSR++[5], RealBasicVSR[6]). Improvements include reduced artifacts and enhanced temporal stability (see zoomed patches). Table 6. Memory and inference speed comparison on NVIDIA A6000. OOM = out of memory. Our method achieves the lowest memory footprint, fastest runtime, and lowest latency. Method Peak Memory Per Frame Runtime (s) Latency-max (s) DOVE SeedVR2 Upscale-A-Video Ours 42.208 GB OOM OOM 20.8 GB 1.74 0.67 173.56 0.67 4.1. Qualitative Comparisons We provide qualitative comparisons in Fig. 5, where StreamDiffVSR generates sharper details and fewer artifacts than prior methods. Additional visualizations of temporal consistency and flow coherence are included in the supplemental material. qualitative comparison with Upscale-A-Video Table 7. Ablation study of temporal modules in StreamDiffVSR. Component LPIPS DISTS MUSIQ NIQE NRQM BRISQUE tLP tOF WarpErr 0.099 Per-frame 0.117 w/o ARTG w/o TPM 0.116 TPM (unwarped) 0.122 0.099 Ours 0.071 0.070 0.078 0.082 0. 65.981 63.347 67.110 63.849 65.586 3.249 3.194 3.197 3.201 3.111 6.969 6.980 7.007 7.159 7.256 21.655 19.027 20.279 14.063 17.667 7.261 4.201 6.132 3.910 12.847 4.639 12.846 5.689 4.265 3.620 25.668 16.598 21.990 17.143 14. Table 8. Ablation study on training strategy. Stage combination PSNR SSIM LPIPS DISTS MUSIQ tLP tOF WarpErr stage 1 and 2 stage 1 and 3 stage 2 and 3 All stage jointly Sperate (Ours) 25.442 0.702 26.307 0.753 26.906 0.758 26.135 0.736 27.256 0.766 0.156 0.121 0.132 0.124 0.099 0.100 0.077 0.077 0.073 0. 27.307 67.528 21.781 6.37 64.902 13.094 4.09 21.689 64.751 10.510 4.225 15.726 17.816 4.596 24.298 67.35 4.265 3.620 14.909 65.586 (UAV) [104] is included in the appendix. 7 Figure 7. Ablation study on inference steps. The 4-step model yields the best qualityefficiency trade-off, validating our distillation strategy. Figure 6. Ablation study on the Temporal Processor Module (TPM). Integrating TPM improves motion stability and reduces temporal artifacts by leveraging warped previous-frame features, enhancing temporal consistency in video super-resolution. Table 9. Ablation study on denoising step count within StreamDiffVSR. We evaluate 50, 10, 1, and 4 steps. Our 4-step design achieves favorable balance between perceptual quality and runtime. Step(s) LPIPS DISTS MUSIQ NIQE NRQM BRISQUE tLP tOF Runtime (s) 0.102 50 0.122 10 1 0.138 4 (Ours) 0.099 0.068 0.072 0.076 0.062 66.061 64.900 63.915 65.586 2.804 2.869 3.843 3.111 7.026 6.917 6.984 7. 9.925 12.461 29.552 17.667 18.798 3.826 9.990 3.625 9.899 3.882 4.265 3.620 3.460 0.718 0.106 0.328 Table 10. Ablation study on Rollout Training. Comparison of random timestep distillation vs. rollout training across fidelity and perceptual metrics. Method PSNR SSIM LPIPS DISTS MUSIQ GPU Hours Random Timestep Selection 26.27 26.36 Rollout Distillation 0.743 0.753 0.099 0.095 0.071 0.075 65.981 66.391 60.5 4.2. Ablation Study We ablate key components of Stream-DiffVSR including denoising-step reduction, ARTG, TPM, timestep selection, and training-stage combinations on REDS4 to ensure consistent evaluation of perceptual quality and temporal stability. We perform ablation studies on training strategies in Tab. 10 and Tab. 8. For stage-wise training, partial or joint training yields inferior results, while our separate stage-wise scheme achieves the best trade-off across fidelity, perceptual, and temporal metrics. For distillation, rollout training outperforms random timestep selection in both quality and efficiency, reducing training cost from 60.5 to 21 GPU hours on 4A6000 GPUs. We assess the runtimequality trade-off by varying DDIM inference steps while keeping model weights fixed. As Figure 8. Ablation study on Auto-regressive Temporal Guidance (ARTG). ARTG enhances temporal consistency and perceptual quality by leveraging warped previous frames, reducing flickering, and improving structural coherence. shown in Tab. 9 and Fig. 7, fewer steps increase efficiency but reduce perceptual quality, whereas more steps improve fidelity with higher latency. 4-step setting provides the best balance. Tab. 7 and Fig. 8 show the effectiveness of ARTG and TPM. The per-frame baseline uses only the distilled U-Net with both ARTG and TPM disabled. In the ablation labels, w/o indicates that module is fully removed; for instance, TPM (unwarp) feeds TPM the previous HR frame without flow-based warping, removing motion alignment. ARTG improves perceptual quality (LPIPS 0.1170.099) and temporal consistency (tLP100 6.1324.265). TPM further enhances temporal coherence through temporal-feature warping and fusion, yielding additional gains in tLP100. These results highlight the complementary roles of latent-space guidance and decoder-side temporal modeling. 5. Conclusion We propose Stream-DiffVSR, an efficient online video superresolution framework using diffusion models. By integrating distilled U-Net, Auto-Regressive Temporal Guidance, and Temporal-aware Decoder, Stream-DiffVSR achieves superior perceptual quality, temporal consistency, and practical inference speed for low-latency applications. Limitations. Stream-DiffVSR remains heavier than CNN and Transformer models, and its use of optical flow can introduce fast-motion artifacts. Its auto-regressive design also weakens initial frames, indicating need for better initialization. Improving robustness to real-world degradations remains important. 8 Acknowledgements. This research was funded by the National Science and Technology Council, Taiwan, under Grants NSTC 112-2222-E-A49-004-MY2 and 113-2628E-A49-023-. The authors are grateful to Google, NVIDIA, and MediaTek Inc. for their generous donations. Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MOE in Taiwan."
        },
        {
            "title": "References",
            "content": "[1] Weimin Bai, Suzhe Xu, Yiwei Ren, Jinhua Hao, Ming Sun, Wenzheng Chen, and He Sun. Instantvir: Real-time video inverse problem solver with distilled diffusion prior. arXiv preprint arXiv:2511.14208, 2025. 3 [2] Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 62286237, 2018. 2 [3] Yanpeng Cao, Chengcheng Wang, Changjun Song, Yongming Tang, and He Li. Real-time super-resolution system of 4k-video based on deep learning. In 2021 IEEE 32nd International Conference on Application-specific Systems, Architectures and Processors (ASAP), pages 6976. IEEE, 2021. 2 [4] Kelvin CK Chan, Xintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Basicvsr: The search for essential components in video super-resolution and beyond. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 49474956, 2021. 2 [5] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and Chen Change Loy. Basicvsr++: Improving video superresolution with enhanced propagation and alignment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 59725981, 2022. 2, 7, 1 [6] Kelvin CK Chan, Shangchen Zhou, Xiangyu Xu, and Investigating tradeoffs in real-world Chen Change Loy. video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 59625971, 2022. 2, 7, [7] Ke-Chi Chang, Ren Wang, Hung-Jin Lin, Yu-Lun Liu, ChiaPing Chen, Yu-Lin Chang, and Hwann-Tzong Chen. Learning camera-aware noise models. In European Conference on Computer Vision, pages 343358. Springer, 2020. 2 [8] Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen Lo, Chia-Che Chang, Yu-Lun Liu, Yu-Lin Chang, ChiaPing Chen, and Chun-Yi Lee. Denoising likelihood score matching for conditional score-based data generation. arXiv preprint arXiv:2203.14206, 2022. 2 [9] Rui Chen, Yang Mu, and Yan Zhang. High-order relational generative adversarial network for video super-resolution. Pattern Recognition, 146:110059, 2024. 2 [10] Zheng Chen, Zichen Zou, Kewei Zhang, Xiongfei Su, Xin Yuan, Yong Guo, and Yulun Zhang. Dove: Efficient one-step diffusion model for real-world video super-resolution. arXiv preprint arXiv:2505.16239, 2025. 2 Linyan Jiang, Haibo Lei, et al. Aim 2024 challenge on efficient video super-resolution for av1 compressed content. In European Conference on Computer Vision, pages 304 325. Springer, 2024. 2 [12] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international conference on computer vision (ICCV), pages 764773, 2017. 2 [13] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture similarity. IEEE transactions on pattern analysis and machine intelligence, 44(5):25672581, 2020. 1 [14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 2 [15] Dario Fuoli, Shuhang Gu, and Radu Timofte. Efficient video super-resolution through recurrent latent space propagation. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pages 34763485. IEEE, 2019. [16] Sicheng Gao, Xuhui Liu, Bohan Zeng, Sheng Xu, Yanjing Li, Xiaoyan Luo, Jianzhuang Liu, Xiantong Zhen, and Baochang Zhang. Implicit diffusion models for continuous super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1002110030, 2023. 2 [17] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and Zico Kolter. Consistency models made easy. arXiv preprint arXiv:2406.14548, 2024. 3 [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 2 [19] Zujin Guo, Wei Li, and Chen Change Loy. Generalizable implicit motion modeling for video frame interpolation. Advances in Neural Information Processing Systems, 37:63747 63770, 2024. 2 [20] Janghyeok Han, Gyujin Sim, Geonung Kim, Hyun-Seung Lee, Kyuha Choi, Youngseok Han, and Sunghyun Cho. Dc-vsr: Spatially and temporally consistent video superresolution with video diffusion prior. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. 2 [21] Jingwen He, Tianfan Xue, Dongyang Liu, Xinqi Lin, Peng Gao, Dahua Lin, Yu Qiao, Wanli Ouyang, and Ziwei Liu. Venhancer: Generative space-time enhancement for video generation. arXiv preprint arXiv:2407.07667, 2024. 2, [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [23] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. 2 [11] Marcos Conde, Zhijun Lei, Wen Li, Christos Bampis, Ioannis Katsavounidis, Radu Timofte, Qing Luo, Jie Song, [24] Chi-Wei Hsiao, Yu-Lun Liu, Cheng-Kun Yang, Sheng-Po Kuo, Kevin Jou, and Chia-Ping Chen. Ref-ldm: latent 9 diffusion model for reference-based face image restoration. Advances in Neural Information Processing Systems, 37: 7484074867, 2024. [25] Takashi Isobe, Xu Jia, Shuhang Gu, Songjiang Li, Shengjin Wang, and Qi Tian. Video super-resolution with recurrent structure-detail network. arXiv preprint arXiv:2008.00455, 2020. 2 [26] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11251134, 2017. 2 [27] Mehran Jeelani, Noshaba Cheema, Klaus Illgner-Fehns, Philipp Slusallek, Sunil Jaiswal, et al. Expanding synthetic real-world degradations for blind video super resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11991208, 2023. 2 [28] Xiaozhong Ji, Yun Cao, Ying Tai, Chengjie Wang, Jilin Li, and Feiyue Huang. Real-world super-resolution via In proceedings of kernel estimation and noise injection. the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 466467, 2020. 2 [29] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954, 2024. 3 [30] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. Musiq: Multi-scale image quality transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 51485157, 2021. 1 [31] Chenyang Lei, Yazhou Xing, and Qifeng Chen. Blind video temporal consistency via deep video prior. Advances in Neural Information Processing Systems, 33:10831093, 2020. [32] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single image super-resolution with diffusion probabilistic models. Neurocomputing, 479:4759, 2022. 2 [33] Wenbo Li, Xin Tao, Taian Guo, Lu Qi, Jiangbo Lu, and Jiaya Jia. Mucan: Multi-correspondence aggregation network for video super-resolution. arXiv preprint arXiv:2007.11803, 2020. 2 [34] Xiaohui Li, Yihao Liu, Shuo Cao, Ziyan Chen, Shaobin Zhuang, Xiangyu Chen, Yinan He, Yi Wang, and Yu Qiao. Diffvsr: Enhancing real-world video super-resolution with diffusion models for advanced visual quality and temporal consistency. arXiv preprint arXiv:2501.10110, 2025. 2 [35] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1833 1844, 2021. 2 [36] Jingyun Liang, Jiezhang Cao, Yuchen Fan, Kai Zhang, Rakesh Ranjan, Yawei Li, Radu Timofte, and Luc Van Gool. arXiv preprint Vrt: video restoration transformer. arXiv:2201.12288, 2022. 2 [37] Jingyun Liang, Yuchen Fan, Xiaoyu Xiang, Rakesh Ranjan, Eddy Ilg, Simon Green, Jiezhang Cao, Kai Zhang, Radu Timofte, and Luc Gool. Recurrent video restoration transformer with guided deformable attention. Advances in Neural Information Processing Systems, 35:378393, 2022. 2, 7, 1 [38] Ce Liu and Deqing Sun. On bayesian adaptive video super resolution. IEEE transactions on pattern analysis and machine intelligence, 36(2):346360, 2013. [39] Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan Perez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. 3 [40] Kuan-Hung Liu, Cheng-Kun Yang, Min-Hung Chen, YuLun Liu, and Yen-Yu Lin. Corrfill: Enhancing faithfulness in reference-based inpainting with correspondence guidance in diffusion models. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 1618 1627. IEEE, 2025. 2 [41] Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusionbased text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. 3 [42] Yong Liu, Jinshan Pan, Yinchuan Li, Qingji Dong, Chao Zhu, Yu Guo, and Fei Wang. Ultravsr: Achieving ultrarealistic video super-resolution with efficient one-step diffusion space. arXiv preprint arXiv:2505.19958, 2025. 2 [43] Yu-Lun Liu, Yi-Tung Liao, Yen-Yu Lin, and Yung-Yu Chuang. Deep video frame interpolation using cyclic frame generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 87948802, 2019. 2 [44] Yu-Lun Liu, Wei-Sheng Lai, Ming-Hsuan Yang, Yung-Yu Chuang, and Jia-Bin Huang. Learning to see through obstructions with layered decomposition. IEEE transactions on pattern analysis and machine intelligence, 44(11):8387 8402, 2021. 2 [45] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. Machine Intelligence Research, pages 122, 2025. [46] Cheng Lu et al. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. NeurIPS, 35:57755787, 2022. 3 [47] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1146111471, 2022. 2 [48] Simian Luo et al. Latent consistency models: Synthesizing high-resolution images with few-step inference. arXiv preprint arXiv:2310.04378, 2023. 3 [49] Chao Ma, Chih-Yuan Yang, Xiaokang Yang, and MingHsuan Yang. Learning no-reference quality metric for single-image super-resolution. Computer Vision and Image Understanding, 158:116, 2017. 1 10 [50] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1429714306, 2023. [51] Anish Mittal, Anush Krishna Moorthy, and Alan Conrad Bovik. No-reference image quality assessment in the spatial domain. IEEE Transactions on image processing, 21(12): 46954708, 2012. 1 [52] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and superresolution: Dataset and study. In CVPR Workshops, 2019. 2 [53] Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and superresolution: Dataset and study. In CVPRW, 2019. 1 [54] Jinshan Pan, Haoran Bai, Jiangxin Dong, Jiawei Zhang, and Jinhui Tang. Deep blind video super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 48114820, 2021. 2 [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 2 [56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 3, 1 [57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, [58] Claudio Rota, Marco Buzzelli, and Joost van de Weijer. Enhancing perceptual quality in video super-resolution through temporally-consistent detail synthesis using diffusion models. In European Conference on Computer Vision, pages 3653. Springer, 2024. 2, 7, 1 [59] Michele Saad and Alan Bovik. Blind quality assessment of videos using model of natural scene statistics and motion coherency. In 2012 Conference Record of the Forty Sixth Asilomar Conference on Signals, Systems and Computers (ASILOMAR), pages 332336. IEEE, 2012. 1 [60] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David Fleet, and Mohammad Norouzi. Image superresolution via iterative refinement. IEEE transactions on pattern analysis and machine intelligence, 45(4):47134726, 2022. 2 [61] Mehdi SM Sajjadi, Raviteja Vemulapalli, and Matthew Brown. Frame-recurrent video super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 66266634, 2018. 2 [62] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. [63] Shuwei Shi, Jinjin Gu, Liangbin Xie, Xintao Wang, Yujiu Yang, and Chao Dong. Rethinking alignment in video superresolution transformers. arXiv preprint arXiv:2207.08494, 2022. 2 [64] Jiaming Song, Chenlin Meng, and Stefano Ermon. arXiv preprint Denoising diffusion implicit models. arXiv:2010.02502, 2020. 2 [65] Yexing Song, Meilin Wang, Zhijing Yang, Xiaoyu Xian, and Yukai Shi. Negvsr: Augmenting negatives for generalized noise modeling in real-world video super-resolution. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1070510713, 2024. 2 [66] Lingchen Sun, Rongyuan Wu, Zhiyuan Ma, Shuaizheng Liu, Qiaosi Yi, and Lei Zhang. Pixel-level and semantic-level adjustable super-resolution: dual-lora approach. 2025. 3 [67] Mingzhen Sun, Weining Wang, Gen Li, Jiawei Liu, Jiahui Sun, Wanquan Feng, Shanshan Lao, SiYu Zhou, Qian He, and Jing Liu. Ar-diffusion: Asynchronous video generation with auto-regressive diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 73647373, 2025. 3 [68] Yang-Che Sun, Cheng Yu Yeo, Ernie Chu, Jun-Cheng Chen, and Yu-Lun Liu. Fiper: Factorized features for robust image super-resolution and compression. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [69] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 2 [70] Yapeng Tian, Yulun Zhang, Yun Fu, and Chenliang Xu. Tdan: Temporally-deformable alignment network for video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 33603369, 2020. 2 [71] Shr-Ruei Tsai, Wei-Cheng Chang, Jie-Ying Lee, Chih-Hai Su, and Yu-Lun Liu. Lightsout: Diffusion-based outpainting In Proceedings of the for enhanced lens flare removal. IEEE/CVF International Conference on Computer Vision, pages 63536363, 2025. 2 [72] Yu-Ju Tsai, Yu-Lun Liu, Lu Qi, Kelvin CK Chan, and MingHsuan Yang. Dual associated encoder for face restoration. arXiv preprint arXiv:2308.07314, 2023. 2 [73] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 2 [74] Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. Advances in neural information processing systems, 37:8395184009, 2024. [75] Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. 3 11 [76] Hengkang Wang, Yang Liu, Huidong Liu, Chien-Chih Wang, Yanhui Guo, Hongdong Li, Bryan Wang, and Ju Sun. Temporal-consistent video restoration with pre-trained diffusion models. arXiv preprint arXiv:2503.14863, 2025. 3 [77] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change Loy. Exploiting diffusion prior for real-world image super-resolution. International Journal of Computer Vision, 132(12):59295949, 2024. 2 [78] Jianyi Wang, Shanchuan Lin, Zhijie Lin, Yuxi Ren, Meng Wei, Zongsheng Yue, Shangchen Zhou, Hao Chen, Yang Zhao, Ceyuan Yang, et al. Seedvr2: One-step video restoration via diffusion adversarial post-training. arXiv preprint arXiv:2506.05301, 2025. 3 [79] Xintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and Chen Change Loy. Edvr: Video restoration with enhanced deformable convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 00, 2019. 2 [80] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In International Conference on Computer Vision Workshops (ICCVW), 2021. [81] Shuchen Weng, Haojie Zheng, Peixuan Zhan, Yuchen Hong, Han Jiang, Si Li, and Boxin Shi. Vires: Video instance repainting with sketch and text guidance. arXiv preprint arXiv:2411.16199, 2024. 2 [82] Rongyuan Wu, Lingchen Sun, Zhiyuan Ma, and Lei Zhang. One-step effective diffusion network for real-world image super-resolution. arXiv preprint arXiv:2406.08177, 2024. 3 [83] Rongyuan Wu, Tao Yang, Lingchen Sun, Zhengqiang Zhang, Shuai Li, and Lei Zhang. Seesr: Towards semantics-aware real-world image super-resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2545625467, 2024. 3 [84] Desai Xie, Zhan Xu, Yicong Hong, Hao Tan, Difan Liu, Feng Liu, Arie Kaufman, and Yang Zhou. Progressive autoregressive video diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 63226332, 2025. 3 [85] Sirui Xie, Zhisheng Xiao, Diederik Kingma, Tingbo Hou, Ying Nian Wu, Kevin Murphy, Tim Salimans, Ben Poole, and Ruiqi Gao. Em distillation for one-step diffusion models. Advances in Neural Information Processing Systems, 37: 4507345104, 2024. 3 [86] Yiran Xu, Taesung Park, Richard Zhang, Yang Zhou, Eli Shechtman, Feng Liu, Jia-Bin Huang, and Difan Liu. Videogigagan: Towards detail-rich video super-resolution. 2024. 2 [87] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and William Freeman. Video enhancement with task-oriented International Journal of Computer Vision, 127(8): flow. 11061125, 2019. 2, [88] Xi Yang, Wangmeng Xiang, Hui Zeng, and Lei Zhang. Realworld video super-resolution: benchmark dataset and decomposition based learning scheme. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 47814790, 2021. 2 [89] Xi Yang, Chenhang He, Jianqi Ma, and Lei Zhang. Motionguided latent diffusion for temporally consistent real-world video super-resolution. In European Conference on Computer Vision, pages 224242. Springer, 2024. 2, 7, 1 [90] Chang-Han Yeh, Chin-Yang Lin, Zhixiang Wang, ChiWei Hsiao, Ting-Hsuan Chen, Hau-Shiang Shiu, and YuLun Liu. Diffir2vr-zero: Zero-shot video restoration with diffusion-based image restoration models. arXiv preprint arXiv:2407.01519, 2024. 2 [91] Peng Yi, Zhongyuan Wang, Kui Jiang, Junjun Jiang, and Jiayi Ma. Progressive fusion video super-resolution network via exploiting non-local spatio-temporal correlations. In Proceedings of the IEEE/CVF international conference on computer vision, pages 31063115, 2019. 2 [92] Geunhyuk Youk, Jihyong Oh, and Munchurl Kim. Fma-net: Flow-guided dynamic filtering and iterative feature refinement with multi-attention for joint video super-resolution and deblurring. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4455, 2024. 2 [93] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 57285739, 2022. 2 [94] Aiping Zhang, Zongsheng Yue, Renjing Pei, Wenqi Ren, and Xiaochun Cao. Degradation-guided one-step image super-resolution with diffusion priors, 2024. [95] Kai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing practical degradation model for deep blind image super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4791 4800, 2021. 1 [96] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 2, 1 [97] Ruofan Zhang, Jinjin Gu, Haoyu Chen, Chao Dong, Yulun Zhang, and Wenming Yang. Crafting training degradation distribution for the accuracy-generalization trade-off in realworld super-resolution. 2023. 2 [98] Yuehan Zhang and Angela Yao. Realviformer: Investigating attention for real-world video super-resolution. In European Conference on Computer Vision, pages 412428. Springer, 2024. 2, 7, 1 [99] Zhengqiang Zhang, Ruihuang Li, Shi Guo, Yang Cao, and Lei Zhang. Tmp: Temporal motion propagation for online video super-resolution. IEEE Transactions on Image Processing, 2024. 1, 2, 7 [100] Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, and Licheng Yu. Avid: Any-length video inpainting with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71627172, 2024. 12 [101] Ziqing Zhang, Kai Liu, Zheng Chen, Xi Li, Yucong Chen, Bingnan Duan, Linghe Kong, and Yulun Zhang. Infvsr: Breaking length limits of generic video super-resolution. arXiv preprint arXiv:2510.00948, 2025. 3 [102] Weisong Zhao, Jingkai Zhou, Xiangyu Zhu, Weihua Chen, Xiao-Yu Zhang, Zhen Lei, and Fan Wang. Realisvsr: Detailenhanced diffusion for real-world 4k video super-resolution. arXiv preprint arXiv:2507.19138, 2025. 2 [103] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpmsolver-v3: Improved diffusion ode solver with empirical model statistics. Advances in Neural Information Processing Systems, 36:5550255542, 2023. 3 [104] Shangchen Zhou, Peiqing Yang, Jianyi Wang, Yihang Luo, and Chen Change Loy. Upscale-a-video: Temporalconsistent diffusion model for real-world video superresolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2535 2545, 2024. 2, 7 [105] Xingyu Zhou, Leheng Zhang, Xiaorui Zhao, Keze Wang, Leida Li, and Shuhang Gu. Video super-resolution transformer with masked inter&intra-frame attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2539925408, 2024. 2, 1 [106] Zhenyu Zhou, Defang Chen, Can Wang, Chun Chen, and Siwei Lyu. Simple and fast distillation of diffusion models. Advances in Neural Information Processing Systems, 37: 4083140860, 2024. [107] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, better results. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 93089316, 2019. 2 [108] Junhao Zhuang, Shi Guo, Xin Cai, Xiaohui Li, Yihao Liu, Chun Yuan, and Tianfan Xue. Flashvsr: Towards real-time diffusion-based streaming video super-resolution. arXiv preprint arXiv:2510.12747, 2025. 3 13 Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion"
        },
        {
            "title": "Overview",
            "content": "This supplementary material provides additional details and results to support the main paper. We first describe the complete experimental setup, including training procedures, datasets, evaluation metrics, and baseline configurations. We then present extended implementation details and threestage breakdown of our training pipeline, covering U-Net distillation, temporal-aware decoder training, and the Autoregressive Temporal Guidance module. Next, we report additional quantitative comparisons on multiple benchmarks under both bidirectional and unidirectional settings, followed by extensive qualitative visualizations illustrating perceptual quality and temporal consistency. We also include representative failure cases to highlight current limitations. A. Experimental Setup A.1. Training and Evaluation Setup Stream-DiffVSR is trained in three sequential stages to ensure stable optimization and modular control over temporal components. All evaluation experiments are conducted on an NVIDIA RTX 4090 GPU with TensorRT acceleration. Details of the stage-wise training procedure and configurations are provided in the supplementary. A.2. Datasets We evaluate our method using widely-recognized benchmarks: REDS [53] and Vimeo-90K [87]. REDS consists of 300 video sequences (1280720 resolution, 100 frames each); sequences 000, 011, 015, and 020 (REDS4) are used for testing. Vimeo-90K-T contains 91,701 clips (448256 resolution), with 64,612 for training and 7,824 (Vimeo-90K) for evaluation, offering diverse real-world content for training. For testing under real-world degradation, we also evaluate on two additional benchmarks: VideoLQ [95], noreference video quality dataset curated from real Internet content, and Vid4 [38], classical benchmark with 4 videos commonly used for VSR evaluation. The evaluation results are provided in supplementary. A.3. Evaluation metrics We assess the effectiveness of our approach using comprehensive set of perceptual and temporal metrics across multiple aspects. Reference-based Perceptual Quality: LPIPS [96] and DISTS [13]. No-reference Perceptual Quality: MUSIQ [30], NIQE [59], NRQM [49], BRISQUE [51]. Temporal Consistency: Temporal Learned Perceptual Similarity (tLP), and Temporal Optical Flow difference (tOF). Inference Speed: Per-frame runtime, latency measured on an NVIDIA RTX 4090 GPU to evaluate lowlatency applicability. Note that while we report PSNR and SSIM results (REDS4: 27.256 / 0.768) for completeness, we do not rely on these distortion-based metrics in our main analysis, as they often fail to reflect perceptual quality and temporal coherence, especially in generative VSR settings. This has also been observed in prior work [96]. Our qualitative results demonstrate superior perceptual and temporal quality, as we prioritize low-latency stability and consistency over overfitting to any single metric. A.4. Baseline methods We evaluate our method against leading CNN-based, Transformer-based, and Diffusion-based models. Specifically, we include bidirectional (offline) methods such as BasicVSR++[5], RealBasicVSR[6], RVRT [37], StableVSR [58], MGLD-VSR [89], and unidirectional (online) methods including MIA-VSR [105], TMP [99], RealViformer [98], and StableVSR [58], comprehensively comparing runtime, perceptual quality, and temporal consistency. B. Additional Implementation Details B.1. Implementation Details Our UNet backbone is initialized from the StableVSR [58] released UNet checkpoint, which is trained for imagebased super-resolution from Stable Diffusion (SD) x4 Upscaler [56, 57]. We then perform 4-step distillation to adapt this UNet for efficient video SR. ARTG, in contrast, is built upon our distilled UNet encoder and computes temporal residuals from previous high-resolution outputs using convolutional and transformer blocks. These residuals are injected into the decoder during upsampling, enhancing temporal consistency without modifying the encoder or increasing diffusion steps. Our decoder is initialized from AutoEncoderTiny and extended with Temporal Processor Module (TPM) to incorporate multi-scale temporal fusion during final reconstruction. 0StableVSR [58] is originally bidirectional model. We implement unidirectional variant (StableVSR) that only uses forward optical flow for fair comparison under the online setting. 1 C. Additional Training Detials C.1. Stage 1: U-Net Distillation We initialize the denoising U-Net from the 50-step diffusion model released by StableVSR [58], which was trained on REDS [52] dataset. To accelerate inference, we distill the 50-step U-Net into 4-step variant using deterministic DDIM [64] scheduler. During training, our rollout distillation always starts from the noisiest latent at timestep 999 and executes the full sequence of four denoising steps {999, 749, 499, 249}. Supervision is applied only to the final denoised latent at = 0, ensuring that training strictly mirrors the inference trajectory and reducing the gap between training and inference. We use batch size of 16, learning rate of 5e-5 with constant, and AdamW optimizer (β1 = 0.9, β2 = 0.999, weight decay 0.01). Training is conducted for 600K iterations with patch size of 512 512.The distillation loss consists of MSE loss in latent space, LPIPS [96] loss, and adversarial loss using PatchGAN discriminator [26] in pixel level, with weights of 1.0, 0.5, and 0.025 respectively. Adversarial loss are envolved after 20k iteration for training stabilization. C.2. Stage 2: Temporal-aware Decoder Training The decoder receives both the encoded ground truth latent features and temporally aligned context features (via flowwarped previous frames). The encoder used to extract temporal features is frozen.We use batch size of 16, learning rate of 5e-5 with constant, and AdamW optimizer (β1 = 0.9, β2 = 0.999, weight decay 0.01). Training is conducted for 600K iterations with patch size of 512 512. Loss consists of smooth L1 reconstruction loss, LPIPS [96] loss, flow loss using RAFT [69] and adversarial loss using PatchGAN discriminator [26] in pixel level for training, with weights of 1.0, 0.3, 0.1 and 0.025 respectively. Flow loss and adversarial loss are envolved after 20k iteration for training stabilization. C.3. Stage 3: Auto-regressive Temporal Guidance We train the ARTG module while freezing both the U-Net and decoder. Optical flow is computed between adjacent frames using RAFT [69], and the warped previous superresolved frame is injected into the denoising U-Net and decoder. The loss formulation is identical to Stage 1, conducted with 60K iterations. This guides ARTG to enhance temporal coherence while maintaining alignment with the original perceptual objectives. D. Additional Quantitative comparison. We provide extended quantitative results across multiple datasets and settings. Specifically, we report both bidirectional and unidirectional performance with mean and standard deviation on REDS4 (Tabs. 11 and 12) and Vimeo-90K 2 Algorithm 1: Training procedure for U-Net rollout distillation. Input: Dataset = {( I, I)}; pre-trained VAE; 4-step noise scheduler; student U-Net with parameters θ; discriminator D(). for epoch = 1 to do for each batch ( I, I) do z0 VAE.encode(I); Sample ϵ (0, I); zT αT z0 + 1 αT ϵ ; maximum timestep // Add noise at // --- Rollout 4-step denoising --- ˆzT [ zT , I]; for step = T, . . . , 1 do ˆϵ UNet(ˆzs, s); ˆzs1 Scheduler.step(ˆϵ, s, ˆzs); ˆI VAE.decode(ˆz0); LL2 ˆI I2 2; LLPIPS LPIPS( ˆI, I); LGAN softplus(cid:0)D( ˆI)(cid:1); λL2 LL2 + λLPIPS LLPIPS + λGAN LGAN; Update parameters: θ θ ηθL; Algorithm 2: Auto-Regressive Diffusion VSR. i=1, flows {fi1}N Notation: { Ii}: Input LR frames, { ˆIi}: Enhanced frames, FlowWarp: Warping w.r.t. flow, VAE: Auto-regressive VAE, UNet: Distilled diffusion U-Net, ARTG: Auto-Regressive Temporal guidance, PrepareLatents: Create latent input, timesteps: {t1, . . . , t4} Input: { Ii}N Output: { ˆIi}N i=1. for = 1 to do LQi Ii zi PrepareLatents(LQi, t) if > 1 then ˆI i1 FlowWarp( ˆIi1, fi1) Ei1 VAE.encode( ˆI i=2, VAE, UNet, ARTG. i1) for timesteps do if > 1 then zi ARTG(zi, ˆI i1) ˆϵ UNet(zi, t) zi DiffusionUpdate(ˆϵ, t, zi) if > 1 then ˆIi VAE.Decode(z, Ei1) else ˆIi VAE.Decode(z) return { ˆIi} (Tabs. 13 and 14), while additional bidirectional results are provided on VideoLQ (Tab. 15) and Vid4 (Tabs. 16 and 17). These supplementary results further validate the robustness of our approach under diverse benchmarks and temporal settings. Table 11. Quantitative comparison against bidirectional/offline methods on the REDS4 dataset. We compare CNN-, Transformer-, and diffusion-based approaches. Stream-DiffVSR shows superior perceptual quality, temporal consistency, and stability. All values are reported as mean std over 4 videos. / denote higher/lower is better. Dir.: = bidirectional/offline, = unidirectional/online. Runtime is measured per 720p frame on an RTX 4090. Latency-first and Latency-avg measure first-frame and average latency; tLP and tOF are scaled by 100 and 10. Best and second-best values are marked in red and blue. For space reasons, the main paper reports the mean-only version; the full meanstd statistics are shown here. Dir. Method PSNR SSIM LPIPS DISTS MUSIQ NIQE NRQM BRISQUE tLP tOF Runtime (s) latency-first (s) latency-avg (s) CNN-based Methods - B U Bicubic BasicVSR++ RealBasicVSR 27.042 1.865 25.501 1.516 3.459 0.177 60.256 1.828 0.460 0.042 0.187 0.013 27.362 2.239 7.360 0.120 32.386 2.415 0.907 0.029 0.132 0.023 0.069 0.012 67.002 4.291 3.850 0.439 6.363 0.330 38.641 5.224 6.769 0.242 18.046 4.185 0.134 0.016 0.060 0.006 67.033 4.283 2.530 0.452 0.712 0.062 0.778 0. 21.603 5.817 9.017 4.384 6.422 4.726 4.241 5.765 2.490 4.440 4.759 7.722 - 0.098 0.064 RVRT MIA-VSR 32.701 2.487 0.911 0.027 0.130 0.022 0.067 0.011 67.251 4.372 3.793 0.463 6.366 0.339 38.038 5.779 32.790 2.535 0.912 0.028 0.123 0.022 0.064 0.011 68.140 3.964 3.742 0.472 6.451 0.304 37.099 5.668 9.133 4.408 8.870 4. 2.421 4.316 2.354 4.026 0.498 0.768 Transformer-based Methods Diffusion-based Methods StableVSR MGLD-VSR Ours 27.928 2.411 26.53 1.939 27.256 2. 0.793 0.063 0.749 0.062 0.766 0.062 0.102 0.015 0.047 0.006 67.058 3.797 2.713 0.456 0.151 0.019 0.065 0.006 66.081 4.027 2.972 0.386 0.099 0.013 0.062 0.007 65.595 3.982 3.114 0.186 6.960 0.211 16.249 4.133 6.701 0.202 15.291 4.463 7.055 0.257 17.117 1.836 5.755 4.618 18.139 8.772 4.198 3.795 2.742 4.741 5.910 6.888 3.638 4.855 46.2 43.6 0. - 9.8 6.4 49.8 0.768 4620 218 0.328 - 4.9 3.2 24.9 0.768 2310 109 0. Dir. Method Table 12. Quantitative comparison against unidirectional/online methods on the REDS4 dataset. PSNR Runtime (s) BRISQUE MUSIQ NRQM DISTS LPIPS NIQE SSIM tOF tLP latency-first (s) latency-avg (s) - Bicubic TMP 25.501 1.516 30.672 2.317 0.712 0.062 0.871 0. 0.460 0.042 0.194 0.039 0.187 0.013 0.090 0.010 27.362 2.239 63.818 4.129 7.360 0.120 4.378 0.333 3.459 0.177 5.796 0.312 60.256 1.828 43.394 4. 21.603 5.817 10.424 5.654 4.241 5.765 2.480 3.852 - 0.041 - 0.041 - 0.041 Transformer-based Methods RealViformer 26.763 1.898 0.761 0.062 0.129 0.062 0.065 0.004 64.585 5. 2.731 0.454 6.356 0.079 17.272 4.546 11.261 5.613 11.782 3.762 0. 9.9 4.95 Diffusion-based Methods StableVSR* Ours 27.174 2.449 27.256 2. 0.763 0.069 0.766 0.062 0.111 0.017 0.099 0.013 0.051 0.006 0.062 0.007 66.428 4.040 65.595 3.982 2.572 0.356 3.114 0.186 6.944 0.211 7.055 0. 15.805 4.626 17.117 1.836 11.107 8.293 4.198 3.795 3.925 4.561 3.638 4.855 46.2 0.328 4620 0.328 2310 0. CNN-based Methods Table 13. Quantitative comparison on the Vimeo-90K-T dataset (bidirectional/offline). Our Stream-DiffVSR achieves superior perceptual quality, temporal consistency, and substantially lower runtime. Results are reported as mean std across the dataset, with runtime measured on 448256 videos using an RTX 4090 GPU. Best and second-best results are shown in red and blue. For space reasons, the main paper presents the mean-only version; the full meanstd statistics are provided here. Dir. Method PSNR SSIM LPIPS DISTS MUSIQ NIQE NRQM BRISQUE tLP tOF Runtime (s) latency-first (s) latency-avg (s) CNN-based Methods - B U 29.282 3.647 Bicubic 37.479 4.724 BasicVSR++ RealBasicVSR 29.388 2.692 0.864 0.061 0.956 0.033 0.857 0.059 0.297 0.105 0.098 0.04 0.156 0.113 0.209 0.044 0.117 0.024 0.149 0. 23.433 5.633 51.940 6.169 56.986 4.418 8.735 0.397 7.077 1.111 5.069 0.464 3.588 0.43 5.509 3.514 7.413 0.66 61.714 4.599 47.792 12.514 23.822 10.19 11.606 7.674 4.691 5.013 10.947 14.292 2.49 1.645 1.57 0.974 3.46 2. RVRT MIA-VSR 37.815 5.049 37.598 4.724 0.955 0.033 0.957 0.032 0.093 0.05 0.086 0.039 0.105 0.023 0.101 0.025 49.937 6.509 51.402 6. 7.205 1.005 7.116 1.158 5.393 0.992 5.569 1.249 48.352 12.147 47.865 13.17 4.873 6.486 4.696 5.874 1.429 1.079 1.419 0.997 Transformer-based Methods StableVSR MGLD-VSR Ours 31.823 3.686 29.651 2.354 32.593 3.82 0.878 0.058 0.865 0.057 0.900 0.060 0.095 0.044 0.151 0.076 0.056 0.035 0.111 0.025 0.137 0.032 0.105 0.017 54.582 6.111 57.788 3.876 52.755 6. 4.745 0.857 5.340 0.798 4.403 1.02 7.265 1.427 7.217 0.814 7.672 1.476 20.039 6.398 20.761 8.394 29.297 10.007 26.224 9.042 12.550 10.504 4.307 4.359 3.108 2.794 4.661 3.449 2.689 1.619 Diffusion-based Methods - 0.012 0.008 0.061 0.096 5.749 5.426 0.041 - 0.084 0.056 0.427 0.096 40.243 27.130 0. - 0.042 0.028 0.213 0.096 20.121 13.560 0.041 Dir. Method PSNR Table 14. Quantitative comparison on the Vimeo-90K-T dataset(unidirectional/online). Runtime (s) BRISQUE MUSIQ NRQM DISTS LPIPS NIQE SSIM tOF tLP latency-first (s) latency-avg (s) - U Bicubic TMP 29.282 3.647 36.482 4.672 0.864 0.061 0.946 0.039 0.297 0.105 0.109 0. 0.209 0.044 0.118 0.027 23.433 5.633 48.374 6.31 8.735 0.397 7.368 0.909 3.588 0.43 5.096 0.891 61.714 4.599 49.192 11.55 11.606 7.674 4.870 5. 2.49 1.645 1.603 1.011 - 0.006 - 0.006 - 0.006 RealViformer 30.291 2. 0.877 0.055 0.130 0.061 0.140 0.03 53.107 3.65 5.515 0.486 6.711 0. 24.628 7.933 8.232 6.864 2.769 1.909 0.013 0.091 0. Transformer-based Methods StableVSR* Ours 31.729 3.698 32.593 3.82 0.875 0.061 0.900 0.060 0.098 0.049 0.056 0.035 0.113 0.026 0.105 0. 54.447 6.008 52.755 6.017 4.698 0.853 4.403 1.02 7.280 1.444 7.672 1.476 19.836 6.131 29.297 10.007 30.858 13.166 4.307 4.359 3.144 2.845 2.689 1. 5.749 0.041 40.243 0.041 20.121 0.041 Diffusion-based Methods CNN-based Methods Table 15. Quantitative comparison on the VideoLQ dataset. Left: Against bidirectional/offline methods ; Right: unidirectional/online methods. (a) Bidirectional/Offline Dir. Method NIQE NRQM BRISQUE Bicubic BasicVSR++ RealBasicVSR 3.151 3.745 6. CNN-based Methods 7.945 5.909 3.973 Transformer-based Methods 3.493 6.939 3.810 5.860 Diffusion-based Methods 6.154 3.973 5.761 4.163 6.140 3.929 RVRT MIA-VSR StableVSR MGLD-VSR Ours - B U 57.944 56.800 30.158 60.557 58.513 22.973 29.497 23.176 (b) Unidirectional/Online Dir. Method NIQE NRQM BRISQUE CNN-based Methods - U Bicubic TMP 7.945 6.751 3.151 3.511 57.944 59.841 Transformer-based Methods RealViformer 4. 6.066 28.266 Diffusion-based Methods StableVSR* Ours 3. 3.929 6.122 6.140 23.814 23.176 Table 16. Quantitative comparison against bidirectional/offline methods on the Vid4 dataset. Dir. Method PSNR SSIM LPIPS NRQM BRISQUE tLP tOF latency-max (s) CNN-based Methods - B"
        },
        {
            "title": "B\nB",
            "content": "U Bicubic BasicVSR++ RealBasicVSR 21.719 26.230 21.963 0.582 0.828 0.597 0.512 0.193 0.210 3.429 6.481 7. 58.680 38.409 21.804 27.819 15.029 6.630 1.145 0.507 0.9 Transformer-based Methods RVRT MIA-VSR 26.377 26. 0.826 0.826 0.229 0.174 6.006 6.619 44.667 38.509 17.146 14.297 0.507 0. Diffusion-based Methods StableVSR MGLD-VSR"
        },
        {
            "title": "Ours",
            "content": "22.541 21.983 22.725 0.644 0.605 0.652 0.194 0.243 0. 7.224 7.129 7.346 13.254 16.525 15.260 48.585 31.744 8. 0.957 3.152 0.962 - 6.86 4.48 1.743 53.76 3234 152.6 0. Table 17. Quantitative comparison against unidirectional/online methods on the Vid4 dataset. Dir. Method PSNR SSIM LPIPS NRQM BRISQUE tLP tOF latency-max (s) CNN-based Methods - U U"
        },
        {
            "title": "Bicubic\nTMP",
            "content": "21.719 25.579 0.582 0.797 0.512 0.256 3.429 5.698 58.680 46.257 27.819 14. 1.145 0.566 - 0.029 Transformer-based Methods"
        },
        {
            "title": "RealViformer",
            "content": "21.963 0.597 0.257 7.604 21.804 11. 1.107 6.93 Diffusion-based Methods StableVSR*"
        },
        {
            "title": "Ours",
            "content": "22.213 22.725 0.623 0.652 0.203 0. 7.233 7.346 11.966 15.260 59.594 8. 1.036 0.962 3234 0.229 4 Figure 9. Qualitative comparison with Upscale-A-Video (UAV). Due to GPU memory limitations (OOM on an RTX 4090), we use UAV results extracted from its official project video for qualitative comparison. Despite this constraint, our Stream-DiffVSR exhibits superior visual fidelity and temporal consistency across frames. Figure 10. Additional visual results. E. Additional Visual Result Figs. 10 to 12 presents qualitative results on challenging real-world sequences. Compared with CNN-based (TMP, BasicVSR++) and Transformer-based (RealViFormer) approaches, as well as the diffusion-based MGLD-VSR, our method produces sharper structures and more faithful textures while effectively reducing temporal flickering. These visual comparisons further demonstrate the effectiveness of our design in maintaining perceptual quality and temporal consistency across diverse scenes. Temporal consistency comparison. As shown in the consecutive-frame comparisons Fig. 13, Stream-DiffVSR alleviates flickering artifacts and preserves stable textures over time, yielding noticeably stronger temporal coherence Figure 11. Additional visual results. Figure 12. Additional visual results. 6 Figure 13. Temporal consistency comparison. Qualitative comparison of temporal consistency across consecutive frames. Our proposed Stream-DiffVSR effectively mitigates flickering artifacts and maintains stable texture reconstruction, demonstrating superior temporal coherence compared to existing VSR methods. Figure 14. Optical flow visualization comparison. Visualization of optical flow consistency across different VSR methods. Our proposed Stream-DiffVSR produces smoother and more temporally coherent flow fields, indicating improved motion consistency and reduced temporal artifacts compared to competing approaches. than prior VSR methods. Optical flow visualization comparison. The optical flow consistency visualizations Fig. 14 further highlight our advantages: Stream-DiffVSR generates smoother and more temporally coherent flow fields, reflecting improved motion stability and reduced temporal artifacts. We also provide qualitative comparisons with Upscale-AVideo [104] in Fig. 9. Owing to GPU memory constraints, the official model cannot be executed locally, so we rely on frames extracted from its project video. Despite this limitation, Stream-DiffVSR demonstrates superior fine-detail reconstruction and notably improved temporal stability in UAV scenarios. F. Failure cases Fig. 15 illustrates limitation of our approach on the first frame of video sequence. Since no past frames are available for temporal guidance, the model may produce blurrier details or less stable structures compared to subsequent frames. This issue is inherent to all online VSR settings, where temporal information cannot be exploited at the sequence start. 7 Figure 15. Limitation on the first frame without temporal context. Our method may underperform on the first frame of video sequence due to the absence of prior temporal information. This limitation is inherent to online VSR settings, where no past frames are available for guidance. As shown in later frames, once temporal context becomes available, our method quickly stabilizes and reconstructs high-fidelity details."
        }
    ],
    "affiliations": [
        "MediaTek Inc.",
        "National Yang Ming Chiao Tung University",
        "Shanda AI Research Tokyo"
    ]
}