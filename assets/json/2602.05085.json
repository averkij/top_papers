{
    "paper_title": "Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories",
    "authors": [
        "Sidi Lu",
        "Zhenwen Liang",
        "Dongyang Ma",
        "Yan Wang",
        "Haitao Mi",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 5 8 0 5 0 . 2 0 6 2 : r Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories Sidi Lu, Zhenwen Liang, Dongyang Ma, Yan Wang, Haitao Mi, Dong Yu Tencent AI Lab Correspondence to: sidilu@global.tencent.com In this paper, we aim to bridge test-time-training with new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with conventional two-layer MLP design that has clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memoriesperformed in principled way by reusing model parameters, activations and/or gradients is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining much smaller context window. In addition, we also test the models general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the models existing internal knowledge. Date: February 6,"
        },
        {
            "title": "1 Introduction",
            "content": "Test-time adaptation of language models has long been central problem for reliable deployment. Generally, existing approaches fall under two paradigms: non-parametric and parametric mechanisms. The dominant non-parametric paradigm is in-context learning (ICL) [2]: the model conditions on prompts that contain demonstrations, tool outputs [26] and additional information from retrieval-based augmentation (RAG) [17], then produces task-appropriate behavior without updating weights. This approach is appealing, as it is stable and deployment-friendly, yet its learning is mediated entirely through attention over the prompt. As result, the adaptation is bounded by context length [18] and can be overly restricted to certain formatting, ordering and distractors [28, 39], which can manifest as controllability failures, including susceptibility to prompt injection and related jailbreak-style attacks [20, 40]. In contrast, test-time training (TTT) methods [31] perform online parametric updates using self-supervised signals available at inference. While these techniques can in principle internalize new information beyond the prompt, they often incur additional optimization cost (typically multiple gradient-based iterations per token step) [35], and they require careful objective design to avoid catastrophic distribution shift while still enabling useful adaptivity. In this paper, we try to answer the following question: Can we significantly improve the parameter and compute efficiency of test-time training through principled initialization of the memory module? We argue that proper initialization can dramatically accelerate convergence and reduce the number of parameters required for effective memorization [7]. We propose Locas, locally-supported parametric memory that achieves both parameter and compute efficiency through principled initialization. In contrast to previous TTT methods that operate through applying gradient to all or part of the model parameters where the introduced parts adopt random initialization [35], Locas adaptively uses the backbone models behavior to guide the initialization of the newly introduced memory module. We discuss two general variants of Locas, including the more theoretically sound one called LocasMLP with two-layer MLP structure, and the more practically flexible one called Locas-GLU that shares the GLU-FFN structure with SOTA LLMs [27]. To maintain compact parametric memory with minimized information loss, we also develop an efficient compression algorithm for such memory. We generalize the traditional SVD algorithm [6], which only works on linear projections, to the two-layer non-linear case, and propose the Non-Linear SVD (NL-SVD) algorithm to efficiently compute the best compressed form with lower latent dimension. Contributions. We propose Locas, Locally-supported parametric memory that utilizes the backbone model it relies on to perform principled initialization, achieving boosted computation and parameter efficiency, fast convergence and improved generalization. We discuss the expansion and compression of such parametric memory w.r.t. the growth of longer context. Specifically: For Locas-MLP, we show that the proposed initialization is step-wise optimal (both time-step and gradient-update-step) under certain assumptions; for Locas-GLU, we show that activation-guided parameter cloning anchors the memory initialization within the principal subspace of the original FFN parameters. We propose the Non-Linear SVD for compression of the Locas-MLP variant (see Appendix A.1), while noting its practical limitations compared to standard backpropagation. We evaluate the proposed Locas against existing methods including the full attention baseline, contexttruncation baseline and the TempLoRA baseline on PG-19 whole-book language modeling and Locomo long-context dialogue QA tasks [21, 24, 35], demonstrating significant parameter and compute efficiency advantages."
        },
        {
            "title": "2 Methodology",
            "content": "We hereby introduce Locas, locally-supported parametric memory framework that achieves parameter and compute efficiency through principled initialization. We first revisit the Feed-Forward Network (FFN) in transformers, and reinterpret it as soft-lookup table or internal attention mechanism."
        },
        {
            "title": "2.1 Notation: FFNs are Soft Look-up Table Memories",
            "content": "We begin by introducing notation and making explicit the interpretation of feed-forward networks (FFNs) as parametric memories. Consider classical transformer layer indexed by i, with hidden dimensionality and FFN intermediate dimensionality m. Let Ai Rd denote the input activation to the FFN at token position and layer i, after the attention sublayer and layer normalization. classical two-layer FFN with element-wise nonlinearity ϕ() computes FFN(Ai t) = ϕ(cid:0)K Ai (cid:1) , (1) where Rdm is the first-layer weight matrix (which we call \"key matrix\" later on) and Rmd is the second-layer weight matrix (which we call \"value matrix\" later on) . We omit bias terms for clarity, as they do not affect the arguments that follow. Let kj Rd denote the j-th column of and vj Rd denote the j-th row of . The FFN output can then be written as sum over intermediate dimensions: FFN(Ai t) = (cid:88) j=1 ϕ(cid:0)Ai t, kj(cid:1) vj. (2) Figure 1 Illustration of typical dense transformer layer with FFN interpreted as soft look-up table memory, in comparison with the attention mechanism, which is contextual soft look-up table mechanism. The GLU variant follows similar structure but with an additional gating mechanism. This decomposition admits direct interpretation as soft look-up table or internal attention mechanism. Each pair (kj, vj) defines memory slot: the key vector kj determines when the slot is activated via its inner product with the input, while the value vector vj determines what content is retrieved. The nonlinearity ϕ() acts as an activation gate that sparsifies or modulates the contribution of each memory slot. Under this view, the FFN implements an unnormalized linear attention mechanism [16] with fixed, parametric set of keyvalue pairs: FFN(Ai t) = (cid:88) j=1 αj(Ai t) vj, αj(Ai t) . = ϕ(cid:0)Ai t, kj(cid:1) . (3) Unlike self-attention, these keys and values are not derived from the input sequence but are instead stored directly in the model parameters. As result, the FFN can be viewed as persistent, content-addressable memory whose capacity is proportional to its intermediate dimensionality m. This reinterpretation highlights critical observation: increasing is equivalent to increasing the number of memory slots available to the model, while modifying (kj, vj) corresponds to writing new entries into this parametric memory. In modern large transformers, FFNs typically dominate parameter count, suggesting that they are the primary locus of memorization capacity. In the remainder of this section, we exploit this perspective to construct an explicit mechanism for writing new keyvalue pairs into FFNs at test time with principled initialization, thereby enabling efficient parametric memory expansion with fast convergence. Figure 2 shows how the proposed parametric memory works in transformer layers."
        },
        {
            "title": "2.2 Two Variants: Locas-MLP and Locas-GLU",
            "content": "Before discussing the initialization strategies, we first introduce two variants of Locas that cater to different model architectures and use cases. Locas-MLP This variant adopts conventional two-layer MLP structure with ReLU activation: Locas-MLP(A) = ReLU(K A) Figure 2 Architecture of the proposed Locas parametric memory integrated as sideway FFN module in transformer layers. The memory module operates in parallel with the backbone FFN, with its output scaled and added to the main pathway. This design enables genuine model capacity expansion at test time while preserving the backbone models pretrained representations. where Rdr is the key matrix and Rrd is the value matrix, with being the latent dimensionality of the memory. This variant has clearer theoretical guarantees as both its key and value matrices admit step-wise optimal closed-form solutions. However, it has compatibility issues with models that were not trained with two-layer MLP-style FFN blocks - specifically, the piecewise-linear separability of representations at the MLP input is often poor for such models. Locas-GLU This variant shares the GLU-FFN structure with state-of-the-art LLMs (e.g., LLaMA, Qwen, Mistral): Locas-GLU(A) = (σ(GA) A) where Rdr is the gate matrix, Rdr is the up-projection (key) matrix, Rrd is the downprojection (value) matrix, and σ() is the SiLU activation function. This variant can be seamlessly attached to existing GLU-based models for both parameter-efficient and computation-efficient continual learning."
        },
        {
            "title": "Optimal Initialization",
            "content": "Given the memorized token xt with context x<t, for layer i, we denote its hidden output as Hi, and the concatenated hidden output of all layers as H. The next-token prediction log-likelihood log p(xtx<t) can be written as function of model parameter θ and the concatenated hidden output H: log p(xtx<t) = (θ; H; xt) We hereby consider how to efficiently utilize the backpropagation process without actually performing parameter-space updates. We denote the gradient of the likelihood function w.r.t. hidden output as = log p(xtx<t). By definition of gradients, we have: η > 0, H+ = + η s.t. (θ; H+; xt) > (θ; H; xt) We hereby give the following step-wise optimal initialization for the key vector km and value vector vm of the new dimension in the parametric memory for layer i: Normalize(Ai ki t) ϵ GlobalNormalize(Gi vi t) , where GlobalNormalize() means normalizing the gradient across both the hidden size and layer number axes. In such cases, for each layer, the impact from the introduced dimension in the parametric memory can be written as: Hi = ϕ(Ai t, ki m) vi = GlobalNormalize(Gi t) Gi , where is constant number that can be arbitrarily controlled via different normalization and rescaling strategies."
        },
        {
            "title": "2.4 Memory Initialization for Locas-GLU: Activation-Guided Parameter Cloning",
            "content": "For Locas-GLU, we adopt different initialization strategy that leverages the backbone models own parameters. Unlike Locas-MLP which uses activations and gradients to directly initialize the memory, Locas-GLU analyzes the activation patterns of the models native FFN during the memorization pass to determine which basis vectors to clone. Activation-Based Basis Selection Given an input chunk x<T , we first perform forward pass through the backbone model. At each layer i, we compute the intermediate activation of the GLU-FFN: Mi = σ(W GAi t) (W KAi t) where i, and is the intermediate dimensionality. G, Rmd are the gate and up-projection weight matrices of the backbone models FFN at layer We then compute the activation importance for each intermediate dimension by averaging the absolute activation values across all tokens in the chunk: αi = 1 (cid:88) t=1 Mi t,j Top-K Selection as Nonlinear PCA in Activation Space After sorting the intermediate dimensions by in descending order, we select the top-r dimensions (those with largest average their activation importance αi activation) to construct our memory. Let denote the set of indices corresponding to these most-activated dimensions at layer i. Parameter Cloning for Key and Gate Matrices The key (up-projection) and gate matrices of Locas-GLU are initialized by cloning the corresponding rows from the backbone models FFN: Normalize([W K]jS Gi Normalize([W G]jS ) ) where []jS denotes selecting rows indexed by . Zero Initialization for Value Matrix To maximize behavioral consistency at the initialization point, the value (down-projection) matrix is initialized to zero: 0 This ensures that immediately after initialization, the Locas-GLU module contributes zero to the models output, similar to the initialization strategy in LoRA. The memory then gradually learns to store contextspecific information through gradient-based updates. Interpretation The effectiveness of selecting most-activated bases can be understood from the perspective of local support: this strategy essentially selects the top-K principal components in the models pretrained feature decomposition space to approximate the support manifold of the current context. This approach effectively balances the tension between local support and generalizable features - the selected dimensions are both highly relevant to the current context and carry well-learned representations from pretraining. Furthermore, since we introduce sideway FFN rather than learning modification in the original parameter space as in LoRA, we achieve genuine model capacity expansion at test time. Weight Norm Clipping for Implicit KL Constraint To bound the influence that the memory module can exert on the backbone model at each forward step, we apply weight norm clipping to the key, gate, and value matrices during inference. Specifically, for each row (or column) vector in these matrices, we enforce: max(w2, 1) This operation resembles classical Weight Normalization [25], but with crucial difference: we only clip vectors whose norm exceeds 1.0, leaving smaller vectors unchanged. This asymmetric treatment ensures that the memorys per-step contribution is bounded within fixed-radius ball in the output space, effectively imposing an implicit KL divergence constraint on the models behavior shift, but at much lower computational cost than explicit KL regularization. Output Scaling Factor The output of the Locas-GLU memory module is not added to the backbone FFN output with equal weight. Instead, we introduce scaling factor τ that modulates the memorys contribution: Hout = FFN(A) + τ Locas-GLU(A) In practice, we set τ to be the average row norm of the backbone FFNs down-projection weight matrix divided by the memory width r: τ = 1 1 (cid:88) j= Wdown[j, :]2 This adaptive scaling ensures that the memorys output magnitude is calibrated to match the typical contribution scale of the backbone FFN, while also accounting for the memory capacity. The division by normalizes the aggregated contribution from all memory slots, providing principled initialization point that neither overwhelms the backbone nor is too weak to have effect. Combined with weight norm clipping, this provides double safeguard against catastrophic forgetting during test-time adaptation."
        },
        {
            "title": "2.5 Memory Accumulation over Context",
            "content": "In practice, we update the Locas memory via standard backpropagation as the model processes streaming context. The memory is initialized using the principled strategies described above, and then updated with gradient descent on the language modeling objective. This approach is simple, efficient, and compatible with modern mixed-precision training pipelines. For Locas-MLP, the layer-wise parametric memory grows linearly with the number of memorized tokens. While standard backpropagation provides an efficient way to update and consolidate this memory, we also develop theoretically principled compression algorithm called Non-Linear SVD (NL-SVD) that can reduce the latent dimensionality while preserving the dominant activation behavior. However, empirical results (see Table 1) show that NL-SVD does not outperform simple BP updates in terms of final performance, while incurring substantially higher computational cost due to numerical precision requirements and lack of optimized GPU kernels. We therefore recommend using BP for memory updates in practice. The complete NL-SVD algorithm and its theoretical analysis are provided in Appendix A.1 for readers interested in the theoretical aspects. Importance of Initialization key insight of Locas is that proper initialization of the memory module is crucial for both convergence speed and generalization. Random initialization requires many more gradient steps to reach comparable performance, while our activation-guided initialization provides strong starting point that is already aligned with the models internal representations. This dramatically reduces the number of update steps required, contributing to both parameter efficiency (fewer dimensions needed) and compute efficiency (fewer gradient updates). We provide detailed ablation studies on initialization strategies and memory width in Section 3.3."
        },
        {
            "title": "3 Experiments",
            "content": "We evaluate the proposed Locas framework along two axes: long-context extrapolation and rapid domain adaptation. We highlight both parameter efficiency and compute efficiency of Locas compared to existing methods."
        },
        {
            "title": "3.1 Experimental Setup",
            "content": "Datasets We conduct experiments on two major benchmarks: the online whole-book level language modeling task on PG-19 long text dataset, and the LoCoMo long dialogue question answering task. PG-19 is book-level language modeling benchmark containing long-form narrative text with document lengths significantly exceeding typical pretraining contexts. We use this benchmark to evaluate the models ability to incrementally accumulate and utilize domain-specific information over very long spans. LoCoMo (Long-Context Conversational Memory) [21] is benchmark designed to test long-context memory in conversational settings. It consists of synthetic multi-turn dialogues spanning tens of thousands of tokens, with questions requiring recall of specific facts (names, dates, preferences) mentioned earlier in the conversation. The benchmark includes both single-hop (direct fact retrieval) and multi-hop (combining multiple facts) questions, making it ideal for evaluating parametric memory mechanisms. Models For whole-book language modeling experiments, we use pretrained Qwen3-0.6B-Base, Qwen3-1.7BBase, Qwen3-1.7B models. Since most recently published SOTA LLMs use GLU-FFN and can be less compatible with Locas-MLP, we pretrain our own 0.6B decoder-only transformer using two-layer MLP FFNs, on roughly 150B tokens from the SlimPajama [29] dataset. Baselines We compare Locas against the following baselines: Context Truncation: Inputs longer than the training context are truncated to the last 2K or 4K tokens. This baseline represents the naive approach of discarding past context beyond the window size, serving as lower bound for methods that attempt to retain long-range information. Long Context Attention: The model is evaluated with its native attention mechanism up to 16K tokens. This baseline leverages the models pretrained long-context capabilities (if available) without any test-time adaptation, representing what can be achieved purely through in-context learning. TempLoRA [35]: test-time training method that introduces low-rank adapters (LoRA) to all linear projections (attention Q, K, , and FFN gate, up, down) and updates them via gradient descent during inference. TempLoRA represents the state-of-the-art parametric approach for test-time adaptation, but incurs significant parameter and computational overhead. All test-time-training baselines adopt context truncation to 2K tokens."
        },
        {
            "title": "3.2 Whole-Book Online Language Modeling on PG-19",
            "content": "We next evaluate Locas as test-time domain adaptation mechanism on PG-19. Pretrained Qwen3 models are evaluated on PG-19. For each document, parametric memory is built incrementally as the model processes the text, with principled initialization enabling fast convergence. Memory compression is applied after fixed intervals to maintain bounded latent size. Compared to direct evaluation with full attention, Locas achieves comparable or lower perplexity with significantly fewer parameters and lower computational cost, particularly for longer documents where attention computation becomes expensive. Relative to context truncation, Locas yields substantial improvements, indicating successful accumulation and reuse of domain-specific information. Notably, Locas achieves comparable performance to TempLoRA while using only 25% of the additional parameters and 38% of the computational overhead. Table 1 PPL on PG-19 Whole Book online language modeling experiment. Locas-GLU achieves comparable or better performance than TempLoRA while using only 17% of the additional parameters and 38% of the computational overhead. PPL @ Cxt. Len. Extra #Params Relative Time 50K 100K 150K 200K 0.6B MLP-FFN LM (Ours) Cxt. Trunc. (2K) Cxt. Trunc. (4K) TTT w/ TempLoRA TTT w/ Locas-MLP + NL-SVD (Ours) TTT w/ Locas-MLP + BP (Ours) 19.35 18.98 18.94 18.87 18.89 19.03 18.64 18.47 18.46 18. 18.92 18.54 18.25 18.23 18.22 18.72 18.31 17.93 17.89 17.90 Qwen3-0.6B-Base Cxt. Trunc. (2K) Long Cxt. Attn. (16K) TTT w/ TempLoRA 27.49 25.24 26.71 27.13 24.79 25. 26.94 24.55 25.42 27.28 24.57 25.22 TTT w/ Locas-GLU (Ours) 26.58 25.76 25. 25.00 Qwen3-1.7B-Base Cxt. Trunc. (2K) Long Cxt. Attn. (16K) TTT w/ TempLoRA 20.60 18.98 20.03 20.38 18.70 19.55 20.25 18.53 19. 20.50 18.58 19.13 TTT w/ Locas-GLU (Ours) 20.00 19.49 19.12 19. Qwen3-1.7B(-Instruct) Cxt. Trunc. (2K) Long Cxt. Attn. (16K) TTT w/ TempLoRA 31.59 31.45 23.74 31.27 31.18 22.57 31.09 30.97 21.92 31.49 31.03 21. TTT w/ Locas-GLU (Ours) 23.65 22.55 21.92 21.65 0 0 41.9M 4.2M 4.2M 0 0 36.7M 5.5M 0 0 73.4M 11.0M 0 0 73.4M 11.0M 1x 1.4x 4.7x 12.7x 1.7x 1x 13.9x 4.3x 1.4x 1x 18.6x 4.7x 1.8x 1x 18.6x 4.7x 1.8x Discussion on Locas-MLP and NL-SVD As shown in Table 1, we also evaluate Locas-MLP with the proposed Non-Linear SVD compression algorithm on our custom 0.6B MLP-FFN language model. While NL-SVD provides theoretically principled approach to memory compression (see Appendix A.1 for details), it does not demonstrate clear advantage over BP in terms of final perplexity (17.89 vs. 17.90 at 200K context length), while incurring substantially higher computational cost (12.7 relative time vs. 1.7 for BP). This is primarily due to numerical precision requirements and lack of optimized GPU kernels for SVD operations. Given these considerations, we recommend using BP for memory updates in practice. See Appendix A.1.7 for detailed discussion of practical limitations."
        },
        {
            "title": "3.3 Ablation Studies",
            "content": "We conduct ablation studies to analyze the key design choices in Locas, focusing on (1) memory initialization strategies and (2) the effect of memory width (latent dimension). 3.3.1 Effect of Initialization Strategy key insight of Locas is that proper initialization of the memory module is crucial for both convergence speed and generalization. We compare different basis selection criteria for Locas-GLU: selecting the most-activated dimensions (Top-K), selecting the least-activated dimensions (Bottom-K), and random selection. We also compare against simple Gaussian random initialization and the normalized activation initialization strategy from Locas-MLP. Results in Table 2 demonstrate that our Top-K activation-guided initialization consistently outperforms Table 2 Ablation study on memory initialization strategies for Locas-GLU. We compare different basis selection criteria (selecting the most-activated dimensions (Top-K), selecting the least-activated dimensions (Bottom-K), and random selection), in comparison to simple random initialization baseline, and the normalized activation initialization as in Locas-MLP. Results show that Top-K consistently outperforms other strategies. Note that, for normalized activation initialization, we only find it possible for the model to converge using LR = 1e-6. For all other cases, we take K=64 and LR = 4e-3. PPL@50K PPL@100K PPL@200K Qwen3-1.7B-Base + Locas-GLU No Memory Gaussian Random Init. Norm. Acti. Init. Random Selection Cloning Bottom-K (Least Activated) Top-K (Most Activated) 20. 20.44 20.58 20.02 20.04 20.00 20.38 20.09 20. 19.51 19.53 19.49 20.50 19.90 19.93 19.06 19.08 19.04 all other strategies. The improvement over random initialization is substantial, validating our hypothesis that leveraging the backbone models activation patterns provides strong inductive bias for memory initialization. Interestingly, random selection cloning (cloning random dimensions from the backbone) also performs reasonably well, suggesting that the pretrained representations carry useful information regardless of which dimensions are selected. However, Top-K selection provides consistent edge by focusing on the most task-relevant dimensions. 3.3.2 Effect of Memory Width key advantage of our activation-guided initialization is its ability to concentrate information into small number of dimensions, analogous to how Principal Component Analysis (PCA) captures most variance in the top few principal components. Since our Top-K selection strategy essentially performs nonlinear PCA in the models activation space, we hypothesize that Locas should maintain strong performance even with very low latent dimensionality r, as the selected bases already capture the most important activation directions for the current context. To validate this hypothesis, we conduct systematic study comparing the effect of memory width (latent dimension r) on model performance for both Locas-GLU and LoRA. For fair comparison, we analyze the parameter counts of both methods: Locas-GLU with latent dimension introduces 3 parameters per layer (for Rdr, Rdr, and Rrd matrices). In contrast, LoRA with rank introduces two low-rank matrices Rrdin and Rdoutr for each linear layer, resulting in (din + dout) parameters per layer. When applied to all attention projections (Q, K, , O, each d) and FFN projections (gate, up: d; down: m, where is the FFN intermediate dimension), LoRA introduces 8Ldr + 3Lr(d + m) total parameters. Since 3d in modern GLU-based architectures, LoRAs parameter count is approximately 8Ldr + 12Ldr = 20Ldr, roughly 6.7 that of Locas-GLU at the same rank r. Observations Table 3 reveals several important findings that validate our theoretical analysis. First, LocasGLU achieves strong performance even at very low latent dimensions. With only = 16 (2.8M parameters), Locas-GLU achieves PPL of 19.14 at 200K context length, which is already competitive with TempLoRA at = 64 (73.4M parameters, PPL 19.13). This represents 26 reduction in parameter count with comparable performance, demonstrating the effectiveness of our activation-guided initialization. Second, performance saturation occurs at lower dimensions for Locas-GLU than for TempLoRA. Locas-GLU shows diminishing returns beyond = 64: the improvement from = 64 to = 128 is only 0.02 PPL (19.04 19.02). In contrast, TempLoRA continues to benefit from higher ranks, with 0.03 PPL improvement from = 64 to = 128 (19.13 19.10). This saturation behavior is consistent with our PCA interpretation: if Top-K selection effectively identifies the principal activation directions, then most Table 3 Effect of memory width on PG-19 language modeling performance. We compare Locas-GLU and TempLoRA at various latent dimensions / ranks. Results demonstrate that Locas-GLU maintains strong performance even at very low dimensions, consistent with the PCA interpretation that top activated bases capture the most important information. Latent Dim #Params / Rank (M) PPL @ Context Length 50K 100K 150K 200K Qwen3-1.7B-Base Context Truncation (2K) - 0 20.60 20.38 20. 20.50 TempLoRA (All Projections) = 16 = 32 = 64 = 128 Locas-GLU (FFN, Ours) = 16 = 32 = 64 = 128 16 32 64 16 32 64 128 18.4 36.7 73.4 146.9 2.8 5.5 11.0 22.0 20.19 20.09 20.03 20.04 19.75 19.64 19.55 19.55 19.43 19.30 19.20 19. 19.39 19.24 19.13 19.10 20.01 20.01 20.00 19.97 19.52 19.51 19.49 19.46 19.19 19.16 19.12 19.12 19.14 19.10 19.04 19.02 task-relevant information is captured by the first few selected bases, with subsequent bases providing only marginal additional capacity. Third, the performance-vs-parameter curve strongly favors Locas-GLU. At equivalent latent dimensions, Locas-GLU consistently outperforms TempLoRA despite using 6.7 fewer parameters per rank. For example, at = 32, Locas-GLU (5.5M params) achieves PPL 19.10, while TempLoRA (36.7M params) achieves PPL 19.24. This suggests that concentrating adaptation capacity in the FFN pathway with principled initialization is more effective than distributing low-rank updates across all projection matrices. 3.3.3 Impact on General Capabilities: MMLU Evaluation critical concern for any test-time adaptation method is whether memorizing domain-specific information causes catastrophic forgetting of the models general capabilities. To evaluate this, we test each methods impact on MMLU performance after memorizing an entire book from PG-19. Table 4 MMLU accuracy (%) on Qwen3-1.7B-Base after memorizing complete PG-19 book. We compare the degradation of general capabilities across different test-time training methods. Lower degradation indicates better preservation of the models pretrained knowledge. We use the MMLU test split for evaluation. Note that our baseline results differ slightly from the Qwen3 technical report; we verified our evaluation protocol using EleutherAIs lm-eval harness, which produced consistent results with our custom script. MMLU Acc. from Extra (%) Baseline #Params Relative Time Qwen3-1.7B-Base No Memorization (Baseline) TTT w/ TempLoRA TTT w/ TempLoRA (r=512) TTT w/ Locas-GLU (Ours) TTT w/ Locas-GLU (Ours, r=512) 60.4 59.8 59.2 60.2 60.3 0 -0.6 -1.2 -0.2 -0.1 0 73.4M 587M 11.0M 88.1M 1x 4.7x 11.2x 1.8x 3.7x Observations Table 4 demonstrates that Locas-GLU exhibits significantly less catastrophic forgetting compared to TempLoRA. After memorizing complete PG-19 book, Locas-GLU causes only 0.2% MMLU degradation (60.4% 60.2%), while TempLoRA results in 0.6% degradation (60.4% 59.8%). When scaling up the memory capacity to = 512, the gap widens further: Locas-GLU maintains near-baseline performance with only 0.1% degradation, whereas TempLoRA suffers 1.2% degradation. This result validates our hypothesis that the sideway architecture of Locas is inherently more resistant to catastrophic forgetting. Unlike TempLoRA, which modifies the models existing weight matrices through low-rank updates and thus directly interferes with pretrained representations, Locas-GLU adds parallel pathway that leaves the backbone parameters entirely untouched. The memory modules contribution is strictly additive and controlled by weight norm clipping and output scaling, ensuring that the backbone models original behavior is preserved as safe baseline that can always be recovered by zeroing out the memory output. Interestingly, we observe positive correlation between parameter count and forgetting for TempLoRA (0.6% at = 64 vs. 1.2% at = 512), but this correlation is nearly absent for Locas-GLU (0.2% at = 64 vs. 0.1% at = 512). This suggests that as more information is memorized, TempLoRAs modifications increasingly conflict with the models pretrained knowledge, while Locas-GLUs parallel architecture naturally accommodates additional capacity without interference."
        },
        {
            "title": "3.4 Long-Context Dialogue QA on LoCoMo",
            "content": "To evaluate Locas beyond language modeling, we test on the LoCoMo (Long-Context Conversational Memory) benchmark [21], which assesses models ability to answer questions about information scattered across long multi-turn dialogues. This task requires the model to memorize and retrieve facts mentioned in previous conversation turns, making it an ideal testbed for memory mechanisms. Task Description LoCoMo consists of synthetic multi-turn dialogues where each conversation spans tens of thousands of tokens. Questions are posed at the end of each dialogue, requiring the model to recall specific details (e.g., names, dates, preferences) mentioned earlier in the conversation. The benchmark evaluates five question categories: (1) Single-Hop questions requiring direct fact retrieval; (2) Multi-Hop questions requiring reasoning over multiple facts; (3) Open-Domain questions about general knowledge mentioned in conversations; (4) Temporal questions requiring temporal reasoning about events; and (5) Adversarial questions designed to test robustness against misleading context. Evaluation Protocol We evaluate models using the same test-time training protocol as in PG-19 experiments. For each dialogue, the model first processes the conversation history with Locas memory accumulation enabled, then answers the questions using the accumulated parametric memory, with or without the complete dialogue context. We compare against context truncation and TempLoRA baselines under identical settings. Observations Table 5 reveals several important findings about Locas-GLUs effectiveness in dialogue memory tasks. Locas-GLU consistently outperforms both baselines across most question types. On Qwen31.7B-Base with full attention context, Locas-GLU achieves 41.6% F1 on single-hop questions compared to 37.3% for vanilla full attention and 37.7% for TempLoRA, representing relative improvement of 11.5% and 10.3% respectively. Similar improvements are observed for multi-hop questions (25.2% vs. 23.8% and 23.1%), indicating that Locas effectively memorizes facts and supports compositional reasoning over them. The gains are even more pronounced on the larger Qwen3-4B-Base model, where Locas-GLU achieves 47.6% on single-hop questions (+15.5% over full attention baseline). Temporal reasoning benefits significantly from parametric memory. On Qwen3-1.7B-Base, LocasGLU achieves 34.1% F1 on temporal questions compared to 29.1% for TempLoRA, suggesting that the sideway memory architecture better preserves the sequential structure of events. The improvement is even more substantial on Qwen3-4B-Base (18.1% vs. 17.2% for TempLoRA and 13.9% for full attention), demonstrating that larger models can more effectively leverage the memorized temporal information. Adversarial robustness improves with Locas-GLU. While all methods exhibit some susceptibility to adversarial questions (negative F1 scores indicate that models sometimes produce the trap answer), Locas-GLU Table 5 Results on LoCoMo long-context dialogue QA benchmark. We report F1 scores (%) across five question categories. For adversarial questions, we report the negative F1 score with the adversarial (trap) answer. Single Multi -Hop -Hop Domain Open Temporal Adv. Qwen3-1.7B-Base Full Attention No Cxt. Full Attn. + TTT w/ TempLoRA No Cxt. + TTT w/ TempLoRA Full Attn. + TTT w/ Locas-GLU (Ours) No Cxt. + TTT w/ Locas-GLU (Ours) Qwen3-4B-Base Full Attention No Cxt. Full Attn. + TTT w/ TempLoRA No Cxt. + TTT w/ TempLoRA Full Attn. + TTT w/ Locas-GLU (Ours) No Cxt. + TTT w/ Locas-GLU (Ours) 37.3 5.6 37.7 5.2 41.6 6.8 41.2 3.8 41.5 6.3 47.6 8.2 23.8 4.6 23.1 4.5 25.2 8. 22.6 3.7 24.3 6.4 28.1 9.1 14.0 10.8 13.3 9.3 14.1 10.6 6.7 9.5 7.9 6.7 7.1 8. 33.5 1.3 29.1 1.3 34.1 4.3 13.9 1.6 17.2 1.6 18.1 5.5 -33.0 -5.2 -31.8 -5.4 -28.7 -5. -25.4 -3.2 -24.3 -5.6 -19.8 -7.6 shows improved robustness. On Qwen3-4B-Base, Locas-GLU achieves -19.8% adversarial F1 compared to -25.4% for full attention and -24.3% for TempLoRA, indicating that the parametric memory helps anchor the model to factually correct information rather than misleading context. No-context evaluation reveals parametric memory retention. When evaluated without any dialogue context (No Cxt. rows), Locas-GLU consistently outperforms TempLoRA, particularly on multi-hop questions (8.4% vs. 4.5% on Qwen3-1.7B-Base, 9.1% vs. 6.4% on Qwen3-4B-Base). This suggests that Locas-GLU more effectively internalizes dialogue facts into persistent parametric memory, enabling recall even without access to the original context. The substantial gap on temporal questions (4.3% vs. 1.3% on Qwen3-1.7B-Base) further validates that temporal relationships are better captured by our sideway architecture."
        },
        {
            "title": "4 Related Work",
            "content": "Test-Time Training and Adaptation Test-time training (TTT) methods update model parameters during inference using self-supervised signals from the test input. Early work by Sun et al. [31] demonstrated that auxiliary self-supervised tasks can improve model robustness to distribution shifts. More recently, Sun et al. [32] proposed TTT layers that replace the fixed hidden state in RNNs with machine learning model updated via gradient descent at test time, achieving linear complexity while matching Transformer performance on long sequences. This approach establishes fundamental connection between sequence modeling and online learning. In the LLM domain, TempLoRA [35] introduces temporary low-rank adapters that are progressively trained on generated text chunks during inference, achieving significant perplexity reductions on long-text generation tasks. Building on this, recent work has explored more sophisticated sample selection and update strategies. VDS-TTT [22] employs verifier model to select high-confidence pseudo-labels for test-time training, achieving up to 32% improvement on mathematical reasoning benchmarks. TLM [13] proposes test-time learning via perplexity minimization, adapting LLMs to target domains using only unlabeled test data while preserving pretrained knowledge through LoRA. SPINE [38] introduces token-selective test-time reinforcement learning with entropy-band regularization, focusing updates on high-entropy forking tokens to maintain reasoning stability. Our work differs from these approaches in two key aspects. First, we focus on principled initialization of the memory module rather than relying on random initialization followed by extensive gradient updates. Second, our sideway FFN architecture enables genuine model capacity expansion rather than modifying existing parameters, providing stronger guarantees against catastrophic forgetting. Parameter-Efficient Fine-Tuning Parameter-efficient fine-tuning (PEFT) methods aim to adapt large pretrained models with minimal additional parameters. LoRA [12] introduced low-rank decomposition of weight updates, reducing trainable parameters by orders of magnitude while matching full fine-tuning performance. Subsequent work has proposed numerous improvements: QLoRA [5] combines quantization with LoRA for memory efficiency, while DoRA [19] decomposes weight updates into magnitude and direction components. Recent advances have further refined PEFT methodology. GraLoRA [15] partitions weight matrices into subblocks with independent low-rank adapters, mitigating gradient entanglement and achieving 8.5% improvement on code generation tasks. DiffoRA [14] introduces Differential Adaptation Matrix that dynamically identifies optimal modules for fine-tuning, improving accuracy by 2.25% over standard LoRA. CTR-LoRA [36] integrates curvature-aware trust-region methods with LoRA, using second-order information to guide rank allocation and training stability. Locas shares LoRAs goal of parameter efficiency but pursues fundamentally different approach. Rather than modifying existing weight matrices through low-rank updates, we introduce parallel memory pathway that preserves the backbone models representations entirely. Our activation-guided initialization strategy can be viewed as form of warm start that leverages the backbone models internal structure, analogous to how Top-K selection performs nonlinear PCA in the models activation space. FFN Interpretation and Knowledge Editing The interpretation of transformer FFN layers as key-value memories was established by Geva et al. [8], who demonstrated that FFN neurons act as pattern detectors (keys) associated with output contributions (values). This view has profound implications for understanding how factual knowledge is stored and retrieved in language models. Recent work has extended these insights to lifelong model editing. WISE [33] introduces dual-parametric memory architecture with main memory (pretrained knowledge) and side memory (edited knowledge), using trained router to dynamically select between them. This architecture is strikingly similar to our Locas-GLU design, though WISE focuses on discrete knowledge edits while we target continuous context memorization. LoKI [34] proposes low-damage knowledge implanting by aligning fine-tuning with mechanistic knowledge storage patterns. Our interpretation of FFNs as soft lookup tables directly builds on Geva et al. [8], and our sideway FFN design shares conceptual similarities with WISEs side memory. However, we focus on online memory accumulation from streaming context rather than discrete edits, and our activation-guided initialization provides principled approach to memory slot selection that these methods lack. Long-Context Modeling and State Space Models Efficient long-context modeling has been central challenge in language model research. Attention-based approaches include sparse attention patterns [1, 4], linear attention approximations [16], and position extrapolation techniques [23, 30]. State space models (SSMs) offer an alternative paradigm with linear complexity, with Mamba [11] achieving competitive performance through selective state spaces. Locas represents complementary approach to these architectural innovations. Rather than modifying the attention or recurrence mechanism, we introduce parametric memory that operates alongside the existing architecture. This modularity allows Locas to be combined with any of these long-context methods, potentially offering additive benefits. Memory-Augmented Neural Networks Memory-augmented neural networks have rich history, from Neural Turing Machines [9] and Memory Networks [37] to differentiable neural computers [10]. These approaches typically maintain an explicit external memory accessed via attention-like mechanisms, enabling models to store and retrieve information beyond their parametric capacity. In the LLM era, memory augmentation has taken new forms. Retrieval-augmented generation (RAG) [17] uses external document stores accessed via semantic retrieval. More recent work has explored parametric alternatives: Mem0 [3] introduces scalable memory architecture that dynamically extracts, consolidates, and retrieves conversational information, with graph-based variant (Mem0g) representing memory as directed labeled graph for multi-hop reasoning. On the LOCOMO benchmark, Mem0 achieves 26% relative improvement over baseline methods while reducing latency by 91% and token costs by 90%. Locas can be viewed as form of parametric memory augmentation that operates within the model architecture rather than externally. Our FFN-based memory naturally integrates with the models forward pass, avoiding the retrieval latency and integration challenges of external memory systems. The principled initialization strategy further distinguishes our approach by enabling rapid memory formation with minimal gradient updates."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented Locas, locally-supported parametric memory framework that bridges test-time training with efficient continual learning. Our key insight is that the backbone model itself provides principled initialization for sideway FFN-style memory modules, enabling both parameter efficiency and compute efficiency that significantly surpass existing approaches. We introduced two variants: Locas-MLP with theoretical guarantees and the Non-Linear SVD compression algorithm, and Locas-GLU that seamlessly integrates with modern GLU-based LLMs through activationguided parameter cloning. Our Top-K selection strategy effectively performs nonlinear PCA in the models activation space, concentrating task-relevant information into compact latent basis. Extensive experiments on PG-19 whole-book language modeling and LoCoMo long-context dialogue QA demonstrate that Locas-GLU achieves comparable or superior performance to TempLoRA while using only 15% of the additional parameters and 38% of the computational overhead. Crucially, ablation studies on MMLU reveal that Locass sideway architecture exhibits minimal catastrophic forgetting (0.1-0.2% degradation) compared to methods that directly modify model weights, validating the benefit of genuine capacity expansion over parameter interference. Our work opens several promising directions for future research, including dynamic memory allocation strategies, hierarchical memory architectures for multi-scale temporal dependencies, and integration with retrieval-augmented generation systems for hybrid parametric-nonparametric memory."
        },
        {
            "title": "References",
            "content": "[1] Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [3] Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, and Deshraj Yadav. Mem0: Building productionready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. [4] Rewon Child. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [5] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115, 2023. [6] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3): 211218, 1936. [7] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 11261135. PMLR, 2017. [8] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 54845495, 2021. [9] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. [10] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using neural network with dynamic external memory. Nature, 538(7626):471476, 2016. [11] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. In First conference on language modeling, 2024. [12] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [13] Jinwu Hu, Zhitian Zhang, Guohao Chen, Xutao Wen, Chao Shuai, Wei Luo, Bin Xiao, Yuanqing Li, and Mingkui Tan. Test-time learning for large language models. arXiv preprint arXiv:2505.20633, 2025. [14] Tangyu Jiang, Haodi Wang, and Chun Yuan. Diffora: Enabling parameter-efficient fine-tuning via differential module selection. arXiv preprint arXiv:2502.08905, 2025. [15] Yeonjoon Jung, Daehyun Ahn, Hyungjun Kim, Taesu Kim, and Eunhyeok Park. Gralora: Granular low-rank adaptation for parameter-efficient fine-tuning. arXiv preprint arXiv:2505.20355, 2025. [16] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. [17] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [18] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. doi: 10.1162/tacl_a_00638. URL https://aclanthology.org/2024.tacl-1.9/. [19] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In Forty-first International Conference on Machine Learning, 2024. [20] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, et al. Prompt injection attack against llm-integrated applications. arXiv preprint arXiv:2306.05499, 2023. [21] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753, 2024. [22] Mohammad Mahdi Moradi, Hossam Amer, Sudhir Mudur, Weiwei Zhang, Yang Liu, and Walid Ahmed. Continuous self-improvement of large language models by test-time training with verifier-driven sample selection. arXiv preprint arXiv:2505.19475, 2025. [23] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021. [24] Jack Rae, Anna Potapenko, Siddhant Jayakumar, Chloe Hillier, and Timothy Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911.05507. [25] Tim Salimans and Durk Kingma. Weight normalization: simple reparameterization to accelerate training of deep neural networks. Advances in neural information processing systems, 29, 2016. [26] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. [27] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. [28] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. In International Conference on Machine Learning, pp. 3121031227. PMLR, 2023. [29] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob Steeves, Joel Hestness, and Nolan Dey. SlimPajama: 627B token cleaned and deduplicated version of RedPajama. https://cerebras.ai/ blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https:// huggingface.co/datasets/cerebras/SlimPajama-627B. [30] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [31] Yu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In International conference on machine learning, pp. 92299248. PMLR, 2020. [32] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. [33] Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, and Huajun Chen. Wise: Rethinking the knowledge memory for lifelong model editing of large language models. Advances in Neural Information Processing Systems, 37:5376453797, 2024. [34] Runyu Wang, Peng Ping, Zhengyu Guo, Xiaoye Zhang, Quan Shi, Liting Zhou, and Tianbo Ji. Loki: Low-damage knowledge implanting of large language models. arXiv preprint arXiv:2505.22120, 2025. [35] Yan Wang, Dongyang Ma, and Deng Cai. With greater text comes greater necessity: Inference-time training helps long text generation. arXiv preprint arXiv:2401.11504, 2024. [36] Zhuxuanzi Wang, Mingqiao Mo, Xi Xiao, Chen Liu, Chenrui Ma, Yunbei Zhang, Xiao Wang, Smita Krishnaswamy, and Tianyang Wang. Ctr-lora: Curvature-aware and trust-region guided low-rank adaptation for large language models. arXiv preprint arXiv:2510.15962, 2025. [37] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014. [38] Jianghao Wu, Yasmeen George, Jin Ye, Yicheng Wu, Daniel Schmidt, and Jianfei Cai. Spine: Token-selective test-time reinforcement learning with entropy-band regularization. arXiv preprint arXiv:2511.17938, 2025. [39] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International conference on machine learning, pp. 1269712706. PMLR, 2021. [40] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043, 2023."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Non-Linear SVD for Memory Compression of Locas-MLP In this section, we describe the Non-Linear SVD (NL-SVD) algorithm for compressing the Locas-MLP parametric memory. While this algorithm provides theoretical guarantees on functional equivalence within the retained activation subspace, empirical results suggest that standard backpropagation achieves comparable or better performance with significantly lower computational overhead. A.1.1 Motivation and Theory Any standard two-layer ReLU-activated perceptron with finite intermediate dimensionality can be viewed as sparsely activated two-layer ReLU-activated perceptron with infinite intermediate dimensionality. Specifically, for those virtual dimensions that are hereby conceptually introduced without impacting the behavior of the network, if they have non-linear keys, its trivial to prove that they must have zero vectors as their value vectors. This motivates us to rethink of the effective latent dimensionality of the FFN modules. As matter of fact, we can always rescale the key and value vectors of non-linear dimension, by shrinking particular key vector with certain scale, and multiply the corresponding value vector with the respective value. For activation functions like ReLU which are linear/piecewise linear against the only breakpoint = 0, its trivial to rigorously prove that such rescaling does not alter the behavior of the function parametrized by such FFNs. In practice, we found that this also approximately generalizes to other activation functions like GeLU, which share most functional behavior with ReLU and only differ minorly in certain ranges. We take the following intuition to build our proposed Non-Linear SVD algorithm for memory compression: the activation pattern of 2-layer perceptron, including both whether and how strong the FFN is activated, is dominated by the key matrix and the product of the vector norms from the two matrices. A.1.2 Key Matrix Dimension Reduction We first perform the following process to isolate the dominant activation structure carried by the key matrix. We begin by row-normalizing and column-normalizing . Denote the row norms of by {αi} and the column norms of by {βi}. We then form composed scaling coefficient si = αiβi for each intermediate dimension i. These coefficients are used to rescale the key matrix as Ki = siKi, which captures both the activation direction and its effective magnitude contributed by the corresponding value vectors. Intuitively, summarizes how strongly each key dimension participates in the input-dependent activation of the FFN. We then perform singular value decomposition on the rescaled key matrix, = ΣW ,and retain only the top singular directions. Let Un Rdn denote the matrix formed by the first columns of . This choice minimizes the reconstruction error of under rank-n approximation, and therefore preserves the dominant activation subspace of the original FFN. The reduced key matrix is constructed by row-normalizing , yielding set of orthogonal probe vectors {pj} with unit norm. These probe vectors serve as canonical activation directions that span the effective latent subspace of the FFN. A.1.3 Value Matrix Reconstruction To construct the corresponding reduced value matrix, we leverage the functional equivalence property of the two-layer perceptron. Each probe vector pj is fed as an input to the original FFN, and we record the resulting output contribution prior to the output projection: We use vj as the j-th column of the new value matrix Vn. vj = (pj) Rd By construction, for any probe vector pj, the reduced FFN produces exactly the same output as the original FFN. Since the probe vectors form an orthogonal basis of the retained activation subspace, any input whose activation lies within this subspace is represented without loss. For inputs with components outside this subspace, the approximation error is governed by the discarded singular values of K. A.1.4 Functional Equivalence It is straightforward to show that the resulting reduced FFN defines function that is identical to the original one on the span of pj. This follows directly from the linearity of the second layer and the fact that the value vectors are obtained by querying the original network itself. Therefore, the proposed procedure yields an efficient compression of the parametric memory, with reduced latent dimensionality n, while preserving the dominant non-linear activation behavior of the original FFN. A.1.5 Algorithm The complete compression algorithm is described as follows: Algorithm 1 Non-Linear SVD for Two-Layer FFN Memory Compression Require: Key matrix Rdm, value matrix Rmd Require: target rank n; (Optional) dimension retaining threshold ϵ Ensure: Reduced key matrix Rdn and reduced value matrix Rnd 1: Normalize each column of K, extract row norms {αi}m i=1 2: Normalize each column of , extract column norms {βi}m i=1 3: Compute composed scalars si = αiβi for all 4: Form weighted key matrix ˆK by scaling row of normalized by si 5: Perform EVD on ˆK: ˆK ˆK = ΣU 6: Select top-n left singular vectors Un 7: Form reduced key matrix using rows of Un 8: Normalize each row of (discarding those with norm less than threshold ϵ) 9: for each row vector kj in do 10: 11: 12: end for 13: Form reduced value matrix by stacking { vj}n 14: return K, Feed kj as input to the original FFN Collect output vector vj as columns j=1 A.1.6 Expansion-Compression Cycle The memory expansion and NL-SVD compression procedures can be composed into single, coherent expansion-compression cycle that operates over memorized text span. Given text span x<T , the model processes tokens sequentially. At each token step t, the expansion rule is applied independently at each layer. m) is instantiated using the normalized hidden activation Concretely, for layer i, new key-value pair (ki . This operation monotonically increases the intermediate Ai dimensionality of the FFN. m, vi and the globally normalized gradient signal Gi After processing contiguous span of Ncapacity tokens, the accumulated parametric memory may become over-complete. The NL-SVD compression step replaces the oversized FFN with reduced one whose latent dimensionality is fixed to target rank (typically = Ncapacity/2), while preserving the dominant activation geometry. From temporal perspective, the expansion-compression cycle can be applied at flexible granularities. In the extreme case, compression can be invoked after every token, resulting in fixed-size parametric memory that is continuously refreshed. Alternatively, expansion can be accumulated over longer spans before triggering compression. A.1.7 Practical Limitations Despite its theoretical elegance, NL-SVD exhibits notable practical limitations: Numerical Precision Requirements: NL-SVD requires at least single-precision (float32) floating-point arithmetic to ensure numerical stability during the SVD decomposition step. This precludes seamless integration with mixed-precision training schemes (e.g., bfloat16) that are standard practice for efficient LLM training and inference. Computational Overhead: The SVD operation and related linear algebra routines are not as welloptimized for modern GPU architectures compared to standard gradient-based primitives, resulting in significant computational overhead (12.7 relative time vs. 1.7 for BP, as shown in Table 1). Limited Applicability: The theoretical guarantees of NL-SVD do not perfectly transfer to Locas-GLU due to its more complex gating mechanism. Since Locas-GLU offers broader compatibility with modern architectures, the practical scope of NL-SVD is limited. Empirical results in Table 1 show that NL-SVD does not demonstrate clear advantage over BP in terms of final perplexity (17.89 vs. 17.90 at 200K context length), while incurring substantially higher computational cost. Given these considerations, we recommend using BP for memory updates in practice."
        }
    ],
    "affiliations": [
        "Tencent AI Lab"
    ]
}