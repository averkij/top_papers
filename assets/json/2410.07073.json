{
    "paper_title": "Pixtral 12B",
    "authors": [
        "Pravesh Agrawal",
        "Szymon Antoniak",
        "Emma Bou Hanna",
        "Baptiste Bout",
        "Devendra Chaplot",
        "Jessica Chudnovsky",
        "Diogo Costa",
        "Baudouin De Monicault",
        "Saurabh Garg",
        "Theophile Gervet",
        "Soham Ghosh",
        "Amélie Héliou",
        "Paul Jacob",
        "Albert Q. Jiang",
        "Kartik Khandelwal",
        "Timothée Lacroix",
        "Guillaume Lample",
        "Diego Las Casas",
        "Thibaut Lavril",
        "Teven Le Scao",
        "Andy Lo",
        "William Marshall",
        "Louis Martin",
        "Arthur Mensch",
        "Pavankumar Muddireddy",
        "Valera Nemychnikova",
        "Marie Pellat",
        "Patrick Von Platen",
        "Nikhil Raghuraman",
        "Baptiste Rozière",
        "Alexandre Sablayrolles",
        "Lucile Saulnier",
        "Romain Sauvestre",
        "Wendy Shang",
        "Roman Soletskyi",
        "Lawrence Stewart",
        "Pierre Stock",
        "Joachim Studnia",
        "Sandeep Subramanian",
        "Sagar Vaze",
        "Thomas Wang",
        "Sophia Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Pixtral-12B, a 12--billion-parameter multimodal language model. Pixtral-12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \\& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral-12B is released under Apache 2.0 license."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 0 1 ] . [ 2 3 7 0 7 0 . 0 1 4 2 : r Pixtral 12B"
        },
        {
            "title": "Abstract",
            "content": "We introduce Pixtral 12B, 12billion-parameter multimodal language model. Pixtral 12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing number of larger models. Unlike many open-source models, Pixtral is also cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B & Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral 12B is released under Apache 2.0 license. Webpage: https://mistral.ai/news/pixtral-12b/ Inference code: https://github.com/mistralai/mistral-inference/ Evaluation code: https://github.com/mistralai/mistral-evals/"
        },
        {
            "title": "Introduction",
            "content": "This paper describes Pixtral 12B, multimodal language model trained to understand both images and text, released with open weights under an Apache 2.0 license. Pixtral is an instruction tuned model which is pretrained on large scale interleaved image and text documents, and hence is capable of multi-turn, multi-image conversation. Pixtral comes with new vision encoder which is trained with novel ROPE-2D implementation, allowing it to process images at their native resolution and aspect ratio. In this way, the model can flexibly process images at low resolution in latency-constrained settings, while processing images at high resolution when fine-grained reasoning is required. When compared against models of similar size in the same evaluation setting, we find that Pixtral delivers strong multimodal reasoning capabilities without sacrificing text-only reasoning performance. Figure 1: Pixtral Performance. Pixtral outperforms all open-models within its weight class on multimodal tasks by substantial margin. Left: Performance on MM-MT-Bench, new multimodal, multiturn, instruction following benchmark designed to reflect real world usage of multimodal language models. Right: Performance on the public LMSys leaderboard (Vision arena, October 2024). For instance, our model matches or exceeds the performance of models like Qwen2-VL 7B [23] and Llama-3.2 11B [6] on popular multimodal benchmarks like MMMU [24] and MathVista [14], while outperforming most open-source models on popular text-only tasks like MATH [7] and HumanEval [26]. Pixtral even outperforms much larger models like Llama-3.2 90B [6], as well as closed models such as Claude-3 Haiku [1] and Gemini-1.5 Flash 8B [18], on multimodal benchmarks. During evaluation of Pixtral and the baselines, we found that evaluation protocols for multimodal language models is not standardized, and that small changes in the setup can dramatically change the performance of some models. We provide thorough analysis of our experience in re-evaluating vision-language models under common evaluation protocol. Specifically, we identify two issues with evaluation: Prompts: Several benchmarks have default prompts which are under-specified, and dramatically reduce the performance of leading closed source models [16, 1] compared to reported figures. Evaluation Metrics: The official metrics typically require exact match, which score model generations as correct only if they exactly match the reference answer. However, this metric penalizes answers which are substantively correct but in slightly different format (e.g., \"6.0\" vs \"6\"). To alleviate these issues, we propose Explicit prompts that explicitly specify the format required by the reference answer. We further analyze the impact of flexible parsing for various models, releasing the evaluation code and prompts in an effort to establish fair and standardized evaluation protocols1. Moreover, while current multimodal benchmarks mostly evaluate short-form or multiple-choice question answering given an input image, they do not fully capture models utility for practical use cases (e.g. in multi-turn, long-form assistant setting). To address this, we open-source novel multimodal, multi-turn evaluation: MM-MT-Bench2. We find that performance on MM-MT-Bench correlates highly with ELO rankings on the LMSys Vision Leaderboard. Pixtral excels at multimodal instruction following, surpassing comparable open-source models on the MM-MT-Bench benchmark (see Figure 1). Based on human preferences on the LMSys Vision Leaderboard, Pixtral 12B is currently the highest ranked Apache 2.0 model, substantially outperforming other open-models such Llama-3.2 11B [6] and Qwen2-VL 7B [23]. It even ranks higher than several closed models such as Claude-3 Opus & Claude-3 Sonnet [1], and several larger models such as Llama-3.2 90B [6]. 1https://github.com/mistralai/mistral-evals/ 2https://huggingface.co/datasets/mistralai/MM-MT-Bench 2 Figure 2: Pixtral Vision Encoder. Pixtral uses new vision encoder, which is trained from scratch to natively support variable image sizes and aspect ratios. Block-diagonal attention masks enable sequence packing for batching, while ROPE-2D encodings facilitate variable image sizes. Note that the attention mask and position encodings are fed to the vision transformer as additional input, and utilized only in the self-attention layers."
        },
        {
            "title": "2 Architectural details",
            "content": "Pixtral 12B is based on the transformer architecture [22], and consists of multimodal decoder to perform highlevel reasoning, and vision encoder to allow the model to ingest images. The main parameters of the model are summarized in Table 1."
        },
        {
            "title": "2.1 Multimodal Decoder",
            "content": "Pixtral 12B is built on top of Mistral Nemo 12B [15], 12-billion parameter decoder-only language model that achieves strong performance across range of knowledge and reasoning tasks."
        },
        {
            "title": "2.2 Vision Encoder",
            "content": "Parameters Decoder Encoder dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size patch_size 5120 40 128 14336 32 8 131072 131072 - 1024 24 64 4096 16 16 4096 - 16 Table 1: Decoder and encoder parameters. In order for Pixtral 12B to ingest images, we train new vision encoder from scratch, named PixtralViT. Here, our goal is to instantiate simple architecture which is capable of processing images across wide range of resolutions and aspect ratios. To do this, we build 400 million parameter vision transformer [5] (see Table 1) and make four key changes over the standard architectures [17]: Break tokens: In order to assist the model in distinguishing between images with the same number of patches (same area) but different aspect ratios, we include [IMAGE BREAK] tokens between image rows [2]. We further include an [IMAGE END] token at the end of an image sequence. Gating in FFN: Instead of standard feedforward layer in the attention block, we use gating in the hidden layer [19]. Sequence packing: In order to efficiently process images within single batch, we flatten the images along the sequence dimension and concatenate them [3]. We construct block-diagonal mask to ensure no attention leakage between patches from different images. RoPE-2D: We replace traditional learned and absolute position embeddings for image patches with relative, rotary position encodings [11, 20] in the self-attention layers. While learned position embeddings must be interpolated to deal with new image sizes (often at the cost of performance), relative position encodings lend themselves naturally to variable image sizes. Figure 3: Complete Pixtral Architecture. Pixtral has two components: vision encoder, which tokenizes images, and multimodal decoder, which predicts the next text token given sequence of text and images. Pixtral can take an arbitrary number of images as input, provided they fit within its 128K context window. Particularly, let be d-dimensional patch vector (either key or query feature). We denote this feature as x(i,j) when it appears at position (i, j) in the image. Then, the ROPE-2D transform of x(i,j) is expressed as: where (i,j) Θ = ROPE-2D (cid:16) (cid:17) x(i,j), Θ cos iθ1 sin iθ1 cos iθ1 sin iθ1 0 0 0 0 ... ... 0 0 0 (1) = (i,j) Θ x(i,j) , 0 0 0 0 cos jθ2 sin jθ2 cos jθ2 sin jθ2 ... ... 0 0 0 0 . . . 0 0 0 0 ... cos jθ 2 sin jθ 0 0 0 0 ... sin jθ 2 cos jθ 2 . Here, sub-matrices (i,j) Θ [k : + 2, : + 2] capture the height position of the feature (i) for odd values of dimension k, and capture the width position (j) for even values of (1-based indexing). Furthermore, Θ = [θ1 . . . θd/2] is vector of frequencies for the various dimensions of x, where θm is defined following standard practice for ROPE-1D [20]. Critically, our simple implementation of the ROPE-2D transform satisfies the relative property: that inner products between two vectors are dependent only on their relative difference in height and width position, rather than their absolute position (see more details in Appendix B). Discussion: Our vision encoder is specifically designed for multimodal modeling. Traditional encoders are typically optimized for ImageNet performance at resolution of, for example, 224 224 or 336 336 pixels. When incorporated into multimodal language models which flexibly perform tasks from standard classification to optical character recognition prior works typically break an image into smaller (square) tiles before independently feeding tiles to the vision encoder. Instead, our vision encoder can naturally adapt to both high and low resolution images at their native aspect ratio, providing substantially improved performance for multi-modal tasks (see Section 4.4)."
        },
        {
            "title": "2.3 Complete architecture",
            "content": "The Pixtral vision encoder is linked to the multimodal decoder via two-layer fully connected network. This network transforms the output of the vision encoder into the input embedding size required by the decoder via an intermediate hidden layer of the same size, employing the GeLU activation [8]. The image tokens are treated identically to the text tokens by the multimodal decoder, including RoPE-1D [20] positional encodings for all tokens. Particularly, our decoder uses causal self-attention mechanism, smoothly facilitating capabilities such as multi-image conversations. The architecture is illustrated in Figure 3. 4 Figure 4: MM-MT-Bench: We open-source new instruction following benchmark for multimodal models, which correlates highly with LMSys ELO ratings. Given an input image, reference answer and model response, an independent LLM judge is instructed to grade the models response on scale of 1 through 10."
        },
        {
            "title": "3 MM-MT-Bench: A benchmark for multi-modal instruction following",
            "content": "Most existing multimodal benchmarks measure the ability of model to perform some form of multiple-choice question answering given an input image. While this is useful signal for the models ability to understand the image, it does not capture the extent of the models utility to user (for instance as multimodal assistant or chatbot). In order to measure this quality, instruction-tuned text-only models are typically evaluated on MT-Bench [25], wherein an independent LLM judge grades models output with respect to reference answer. We construct and release new benchmark named Multimodal MT-Bench (MM-MT-Bench) in similar vein to the text-only variant, to evaluate the performance of instruction-tuned multimodal models. Design. MM-MT-Bench contains 92 conversations in total. It covers breadth of practical use cases, covering five categories of images: charts (21), tables (19), PDF pages (24) diagrams (20) and miscellaneous (8). There are 69 single-turn conversations, 18 conversations with 2 turns, 4 of them with 3 turns and 1 conversation with 4 turns. To evaluate model, we query the model in parallel over all turns of conversation, providing reference answers for the past turns as history. Each turn is rated independently by the judge with the entire conversation history provided. The judge is prompted to rate the conversation on scale of 1 to 10 based on correctness (i.e. was the extracted information correct) and completeness (i.e. does the model answer cover all the points raised in the reference). The evaluation process is illustrated in Figure 4. The judge prompt is provided in Appendix A.5. The results shown in Table 2 show that MM-MT-Bench has 0.91 Pearson Correlation Coefficient with LMSys-Vision ELO ratings. Examples. MM-MT-Bench was designed to mimic real world usage of vision-language models, for extraction, summarization and reasoning over the contents of an image. Representative images from each category are provided in Figure 12 and an example of rated model responses from visionlanguage models are provided in Figure 11. We manually curated the images, prompts and answers and verified the answers from second group of labelers. We ensure that all prompts require reference to the image input to be answered correctly."
        },
        {
            "title": "4 Results",
            "content": "In this section, we provide evaluations of Pixtral 12B against closed and open-source models across range of model sizes, re-evaluating all models through the same evaluation harness. Particularly, for each dataset, we design the prompt such that we can reproduce the results of leading multimodal models (GPT-4o [16] and Claude-3.5 Sonnet [1]). These prompts are Explicit and fully specify the output format (see Section 4.2), allowing models which follow the prompt instructions to be marked accurately at test-time. All models were evaluated with the same prompts, which are specified in Appendix A. We provide additional analysis on re-evaluating models under various prompts and metrics in Sections 4.2 and 4.3, as well as in Appendices and E. 5 Pixtral 12B Qwen-2-VL 7B [23] w/ Flexible Parsing Llama-3.2 11B [6] w/ Flexible Parsing Molmo-D 7B [4] LLaVA-OneVision 7B [9] Claude-3 Haiku [1] Gemini-1.5-Flash 8B(0827) [18] Molmo 72B [4] LLaVA-OneVision 72B [9] Qwen-2-VL 72B [23] Llama-3.2 90B [6] GPT-4o (0513) [16] Claude-3.5 Sonnet [1] Mathvista MMMU ChartQA DocVQA CoT 58.3 53.7 55.2 24.3 47.9 12.3 36.1 44.8 56.9 52.2 57.2 68.2 49.1 64.6 64.4 CoT 52.0 48.1 48.7 23.0 45.3 24.3 45. 50.4 50.7 52.7 54.4 60.3 53.7 68.6 68.0 CoT 81.8 41.2 77.5 14.8 78.5 27.0 67.2 69.6 78. 75.6 66.9 66.6 33.8 85.1 87.6 ANLS 90.7 94.5 91.1 72.2 90.5 74.6 79.5 86.5 91.6 96.3 85. 88.9 90.3 VQAv2 MM-MT-Bench LMSys-Vision VQA Match GPT-4o Judge (Oct 24) 78.6 75.9 67.1 57.1 78.4 68.4 65. 75.2 83.8 81.6 67.0 77.8 70.7 6.05 5.45 4.79 3.72 4.12 5.46 5.93 3.51 4.95 6.59 5.50 7.72 7. 1076 1040 1032 1000 1111 992 1104 1071 1208 1189 Table 2: Multimodal Benchmarks. Pixtral substantially outperforms open models of similar size, as well as several closed-source models. We re-evaluate all models with the same prompt and evaluation metric (see Section 4.2). For transparent comparison against Qwen2-VL 7B [23] and Llama-3.2 11B [6], we additionally report their performance under relaxed evaluation constraints in (gray) (see Section 4.3). To further investigate the gap with reported figures for some open-source models, we provide analysis in Section E. MT-Bench MMLU Math HumanEval 5-shot Maj@1 Pass@1 Pixtral 12B LLaVA-OneVision 7B [9] Molmo-D 7B [4] Qwen-2-VL 7B [23] Llama-3.2 11B [6] 7. 6.94 4.53 6.41 7.51 69.2 67.9 61.2 68.5 68.5 48.1 38.6 10.2 27.9 48.3 72. 65.9 3.7 62.2 62.8 Table 3: Language benchmarks. Pixtral 12B consistently outperforms open-source models of comparable size on text-only benchmarks, making it drop-in multimodal replacement for existing text-only deployments."
        },
        {
            "title": "4.1 Main Results",
            "content": "Multimodal performance: Table 2 shows that Pixtral substantially outperforms all open models around its scale on multimodal benchmarks, as well as closed source models such as Claude-3 Haiku [1] and Gemini-1.5 Flash 8B [18]. Particularly, Pixtral outperforms all models of comparable size on MM-MT-Bench, which targets real world use cases, finding corroborated by strong performance on LMSys Vision Arena. On this public leaderboard, Pixtral 12B approaches the performance of the largest open-weights models, such as Qwen2-VL 72B [23] and Llama-3.2 90B [6]. We highlight that, with our Explicit prompts, the performance of some open-source models is substantially lower than their reported figures. For the closest open-source models Qwen2-VL 7B [23] and Llama-3.2 11B [6] this is mainly due to models not following instructions on answer formatting (e.g. generating \"The answer is 6.\" instead of \"Final answer: 6\"). For transparent comparison against these models, we further report their evaluations using relaxed metrics, with more flexible parsing, in gray (see Section 4.3). We analyze the performance of these models under various prompts in Appendix D. In Appendix E, we customize the evaluation to each model in turn, describing the changes required to bridge the gaps to reported performance. Language performance: Table 3 evaluates Pixtral 12B against open-source models of comparable size on common text-only benchmarks (again, with common prompting and evaluation protocols). Pixtral does not compromise text understanding in pursuit of multimodal capabilities, making it suitable drop-in replacement for both text and vision tasks. 6 Figure 5: Effect of Naive vs. Explicit prompts on leading models. Leading models benefit greatly from Explicit prompts which provide details about the output format. This makes sense, as otherwise substantively correct responses are marked as incorrect during evaluation (top row, right). VQAv ChartQA MMMU Prompt Naive Explicit Naive Explicit Naive Explicit GPT-4o (0513) [16] Sonnet-3.5 [1] Qwen-2-VL 7B [23] Llama-3.2 11B [21] Llama-3.2 90B [21] Pixtral 12B 64.2 50.2 82.1 29.5 52.6 78.9 77.8 70. 75.9 67.1 67.0 78.6 58.0 39.6 83.4 0.0 3.9 84.3 85.1 87.6 41.2 14.8 33.8 81.8 55.0 48. 46.7 20.7 27.0 45.8 68.6 68.0 48.1 23.0 53.7 52.0 Table 4: Prompt ablations. Leading models require prompts which explicitly specify the output format to perform well. Pixtral 12B performs well with both Explicit and Naive prompts, with only minor regression on ChartQA."
        },
        {
            "title": "4.2 Prompt selection",
            "content": "Here we discuss our methodology for designing the evaluation prompts. In our evaluation harness, we choose prompts which allow for reproduction of the reported results of leading closed-source models: GPT-4o [16] and Claude-3.5-Sonnet [1]. These prompts are provided in Appendix A, and we report results averaged over 10 prompts in Appendix D. We find that commonly used prompts do not properly specify the output format. For instance, for multiple choice question, we find open-source prompts include vague instructions like \"Select the correct answer from the options above\". In this case, it is impossible for models to know whether answers should be presented as an index (\"Option A\", \"Option B\" etc.) or with natural language response. Models are then penalized for incorrect formatting. As such, leading models require prompts which explicitly specify the required output format. We illustrate this with real example from MMMU in Figure 5. In Table 4, we demonstrate that our Explicit prompts substantially improve the performance of leading models over Naive prompts. We also note that in number of cases, the performance of smaller models reduces with the Explicit prompt format, perhaps due to discrepancy with the prompt-style in the training set of these benchmarks. Pixtral 12B generally performs better with Explicit prompts, with only minor regression on ChartQA."
        },
        {
            "title": "4.3 Sensitivity to evaluation metrics",
            "content": "In Section 4.2, we discuss the importance of prompts which properly specify the output format. However, during evaluations, we find that even with Explicit prompts, many models still provide outputs in various formats, which are then penalized by metrics which require responses to match the reference answers exactly. To investigate this, we take models generations and evaluate them under progressively looser parsing constraints. For instance, if the correct answer is \"6\", flexible metrics do not penalize answers such as \"6.0\" or \"The answer is 6\". We provide the details of these parsing settings in Appendix C, but 7 Llama-3.2 11B [21] Llama-3.2 90B [21] Qwen2-VL 7B [23] Pixtral 12B Mathvista Baseline Flexible level 1 Flexible level 2 Flexible level 3 MMMU Baseline Flexible level 1 Flexible level 2 Flexible level 3 ChartQA Baseline Flexible level 1 Flexible level 2 Flexible level 24.3 25.9 40.2 47.9 23.0 23.4 41.0 45.3 14.8 20.4 29.9 78.5 49.1 50.3 54.7 57.3 53.7 53.7 55.7 56.7 33.8 33.9 35.6 79. 53.7 54.3 54.3 55.2 48.1 48.1 48.1 48.7 41.2 73.8 73.8 77.5 58.3 58.3 58.3 58.5 52.0 52.0 52.0 52.0 81.8 81.9 81.9 82. Table 5: Flexible parsing ablations. We evaluate models under progressively looser parsing constraints (see Appendix for details). Under loose parsing constraints, the performance of some models dramatically improves. Pixtral 12B performance is stable under all parsing conditions, and continues to lead even when flexible parsing is accounted for. Flexible Level 3 is included for illustration only, as it allows some incorrect answers to be marked as correct. Figure 6: Vision encoder ablations: When leveraged for visual instruction tuning, our encoder substantially outperforms strong CLIPA [10] baseline for tasks requiring fine-grained document understanding, while maintaining parity for natural images. here note that Flexible Level 3 marks response as correct if the reference answer occurs anywhere in the generation. This is an overly generous metric which is included only to illustrate an upper bound, as it permits answers like \"6000\" for reference answer of \"6\". We provide the results of our analysis in Table 5. We find that the performance of some models dramatically improves with more flexible parsing metrics, indicating that the lower scores can be attributed to the inability of models to properly follow prompt instructions. We further note that Pixtral 12B benefits very little from flexible parsing (substantiating its ability to follow instructions), and furthermore can generally outperform other models even after flexible metrics are used."
        },
        {
            "title": "4.4 Vision Encoder Ablations",
            "content": "In order to verify the design choices for our vision encoder, we conduct small-scale ablations with Visual Instruction Tuning [13]. We conduct short-horizon multimodal instruction-tuning runs, both with our vision encoder (Pixtral-ViT), as well as CLIPA [10] backbone as baseline. For both vision encoders, we use Mistral-Nemo 12B-Instruct [15] to initialize the multimodal decoder. 8 Like many open-source vision encoders, CLIPA is trained at fixed resolution of 224 224 pixels. In order to upscale the resolution in vision-language models, existing methods [12] construct several tiled crops from the image, and pass each crop independently through the vision encoder at its pretraining resolution. We conduct two ablations with CLIPA: (a) we resize the entire image to 224 224; (b) we construct 25 crops of the input image, for total resolution of 1120 1120. These models are also evaluated at 224 pixels and 1120 pixels respectively, while our flexible encoder is evaluated at variable image resolutions, with maximum resolution of 1024 pixels. In Figure 6, we find that our model substantially outperforms CLIPA in settings which require finegrained understanding, such as chart and document understanding, while matching its performance on natural language benchmarks such as VQAv2."
        },
        {
            "title": "5 Qualitative examples",
            "content": "We discuss real world application of Pixtral by looking at some qualitative examples. Specifically, Pixtral can be used for reasoning over complex figures (eg. Fig. 7), multi-image instruction following (eg. Fig. 8), chart understanding and analysis (eg. Fig. 9) and converting image to code (eg. Fig. 10). In Fig. 11, we compare Pixtral 12B to QwenVL-7B and Gemini-1.5 Flash-8B (0827) on an example from MM-MT-Bench. The example consists of complex chart on job jitters in the US with an instruction requiring accurate understanding, reasoning and analysis of the chart. Pixtrals response is complete and accurate, hence getting rating of 8, while Gemini-Flash-8B extracts wrong information, and QwenVL does not elaborate on trends."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduced Pixtral 12B, state-of-the-art multimodal model that excels in both text-only and multimodal tasks. With novel architecture featuring 400M-parameter vision encoder and 12B-parameter multimodal decoder, Pixtral 12B demonstrates strong performance across various benchmarks, outperforming other open models and matching larger models. Its superior instruction following abilities, support for variable image sizes, and long context window make it highly versatile for complex multimodal applications. Pixtral 12B is released under the Apache 2.0 license."
        },
        {
            "title": "7 Contributors",
            "content": "Mistral AI Science team (listed in alphabetical order by last name): Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, Albert Q. Jiang, Kartik Khandelwal, Timothée Lacroix, Guillaume Lample, Diego Las Casas, Thibaut Lavril, Teven Le Scao, Andy Lo, William Marshall, Louis Martin, Arthur Mensch, Pavankumar Muddireddy, Valera Nemychnikova, Marie Pellat, Patrick Von Platen, Nikhil Raghuraman, Baptiste Rozière, Alexandre Sablayrolles, Lucile Saulnier, Romain Sauvestre, Wendy Shang, Roman Soletskyi, Lawrence Stewart, Pierre Stock, Joachim Studnia, Sandeep Subramanian, Sagar Vaze, Thomas Wang, Sophia Yang."
        },
        {
            "title": "Acknowledgements",
            "content": "We extend our thanks to the LMSys team for their assistance in deploying our model in the LLM arena, and the vLLM team for their help in integrating Pixtral 12B into their inference library. 9 Figure 7: Reasoning over complex figures. An example showcasing Pixtrals capabilities to understand and reason over complex figures. Pixtral correctly identifies that the green boxes represent the European countries and then reads and sorts the GDP of all the European countries to list the top 5 with accurate GDP numbers. 10 Figure 8: Multi-image instruction following. Pixtral can process arbitrary number of images in its context window. The example shows that Pixtral can successfully combine the information from both images into single markdown table. Figure 9: Chart Understanding and Analysis. Pixtral demonstrates the capability to interpret and analyze intricate charts with high accuracy. In this instance, Pixtral correctly identifies that \"dark-dragon\" corresponds to the red line. Furthermore, it recognizes that the training loss is expected to decrease smoothly and notes that the training run became unstable around the 10K step mark due to significant spike in loss. 12 Figure 10: Image to Code. This demonstration illustrates Pixtrals capability to convert hand-drawn website interfaces into executable HTML code, bringing hand-drawn designs to life as fully functional websites. 13 Figure 11: Examples of model responses from Pixral-12B, QwenVL-7B and Gemini-1.5 Flash-8B (0827) LLM-as-a-judge scores. Pixtrals response is complete and accurate, hence getting rating of 8, while GeminiFlash-8B extracts wrong information, and QwenVL does not elaborate on trends. Figure 12: Example images from MM-MT-Bench"
        },
        {
            "title": "References",
            "content": "[1] Anthropic (2024). The Claude 3 Model Family: Opus, Sonnet, Haiku. https: //www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_ Card_Claude_3.pdf. [2] Bavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena, A., Somani, A., and Tasırlar, S. (2023). Fuyu-8b: multimodal architecture for ai agents. [3] Dehghani, M., Mustafa, B., Djolonga, J., Heek, J., Minderer, M., Caron, M., Steiner, A., Puigcerver, J., Geirhos, R., Alabdulmohsin, I. M., et al. (2024). Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36. [4] Deitke, M., Clark, C., Lee, S., Tripathi, R., Yang, Y., Park, J. S., Salehi, M., Muennighoff, N., Lo, K., Soldaini, L., et al. (2024). Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146. [5] Dosovitskiy, A. (2020). An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929. [6] Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. (2024). The llama 3 herd of models. arXiv preprint arXiv:2407.21783. [7] Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. (2021). Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874. [8] Hendrycks, D. and Gimpel, K. (2016). Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415. [9] Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Li, Y., Liu, Z., and Li, C. (2024). Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326. [10] Li, X., Wang, Z., and Xie, C. (2023). An inverse scaling law for clip training. In NeurIPS. [11] Li, Y. and Harada, T. (2022). Lepard: Learning partial point cloud matching in rigid and deformable scenes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55545564. [12] Liu, H., Li, C., Li, Y., and Lee, Y. J. (2024a). Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306. [13] Liu, H., Li, C., Wu, Q., and Lee, Y. J. (2024b). Visual instruction tuning. Advances in neural information processing systems, 36. [14] Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. (2023). Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255. [15] MistralAI (2024). Mistral NeMo 12B. https://mistral.ai/news/mistral-nemo/. [16] OpenAI, R. et al. (2023). Gpt-4 technical report. ArXiv, 2303:08774. [17] Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. (2021). Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. [18] Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., et al. (2024). Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. [19] Shazeer, N. (2020). Glu variants improve transformer. arXiv preprint arXiv:2002.05202. 16 [20] Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. (2024). Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063. [21] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. [22] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. [23] Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and Lin, J. (2024). Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. [24] Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., et al. (2023). Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arxiv. [25] Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. [26] Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., Wang, Y., Saied, A., Chen, W., and Duan, N. (2023). Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364."
        },
        {
            "title": "Table of Contents",
            "content": "A Prompts . A.1 MMMU and Mathvista . . . . A.2 ChartQA . . . . A.3 VQAv2 . . A.4 DocVQA . . . . A.5 MM-MT-Bench Judge Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Relative Position Encoding Property of ROPE-2D Flexible Parsing Settings Robustness to prompting D.1 Llama-Specific Prompts . . D.2 Average performance across prompts . . . . . . . . . . . . . . . . . . . . . . . . . Reproducing Reported Numbers . . . . . . . . . . . . . . . E.1 Summary . . . E.2 Closed models: Claude-3 Haiku and Gemini-Flash-8B . . . . E.3 Qwen2-VL 7B . . . . . E.4 Llama-3.2 . . . E.5 Llava-OneVision 72B . . . . E.6 Molmo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 19 19 19 19 19 20 21 21 22 22 22 22 22 23 23"
        },
        {
            "title": "A Prompts",
            "content": "Here we open-source the prompts used for evaluations in the main paper. As discussed in Section 4.2, prompts are selected to reproduce reported performance of GPT-4o [16] and Claude-3.5 Sonnet [1]. A.1 MMMU and Mathvista Analyze the image and question carefully, using step-by-step reasoning. First, describe any image provided in detail. Then, present your reasoning. And finally your final answer in this format: Final Answer: <answer> where <answer> is: - The single correct letter choice A, B, C, D, E, F, etc. when options are provided. Only include the letter. - Your direct answer if no options are given, as single phrase or number. - If your answer is number, only include the number without any unit. - If your answer is word or phrase, do not paraphrase or reformat the text you see in the image. - You cannot answer that the question is unanswerable. You must either pick an option or provide direct answer. IMPORTANT: Remember, to end your answer with Final Answer: <answer>. A.2 ChartQA Analyze the image and question carefully, using step-by-step reasoning. First, describe any image provided in detail. Then, present your reasoning. And finally your final answer in this format: Final Answer: <answer> where <answer> follows the following instructions: - <answer> should be single phrase or number. - <answer> should not paraphrase or reformat the text in the image. - If <answer> is ratio, it should be decimal value like 0.25 instead of 1:4. - If the question is Yes/No question, <answer> should be Yes/No. - If <answer> is number, it should not contain any units. - If <answer> is percentage, it should include % sign. - If <answer> is an entity, it should include the full label from the graph. IMPORTANT: Remember, to end your answer with Final Answer: <answer>. A.3 VQAv - Answer the question using single word, number, or short phrase. Use as few words as possible. - If the answer is number, report it as number, i.e. 2, not Two, and only include the number without any unit. - If the question is Yes/No, answer with Yes/No, and nothing else (no likely, unknown, etc.). - You cannot answer that the question is unanswerable. You must answer. A.4 DocVQA Answer the question using single word or phrase. A.5 MM-MT-Bench Judge Prompt SYSTEM: Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the most recent question given the previous conversation as context. Your evaluation should consider correctness and helpfulness. You will be given reference answer and the assistants answer. Begin your evaluation by comparing the assistants answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on scale of 1 to 10 by strictly following this format: \"[[rating]]\", for example: \"Rating: [[5]]\". <The Start of Conversation with User> 19 ### User: <image> Analyze this image. ### Reference answer: The image consists of ... ### Assistant: This is an image of... <The End of Conversation with User>nnn The history of the conversation is passed to the judge with reference answers as assistant answer (teacher-forcing). Relative Position Encoding Property of ROPE-2D In this section, we show the relative position encoding property of ROPE-2D. The goal is prove that: ROPE-2D(x(p,q), Θ), ROPE-2D(y(r,s), Θ) = ROPE-2D(x(pr,qs), Θ), ROPE-2D(y(0,0), Θ) for any feature x, Rd for all positions p, {0 . . . H} and q, {0 . . . }. To keep the discussion simple, we will illustrate this property for = 4 (the extension to higher dimension is straightforward). ROPE-2D (cid:16) x(p,q), Θ (cid:17) = cos pθ1 sin pθ1 cos pθ1 sin pθ1 0 0 0 0 0 0 0 cos qθ2 sin qθ2 cos qθ2 sin qθ2 x1 x2 x3 x4 ROPE-2D (cid:16) y(r,s), Θ (cid:17) = cos rθ1 sin rθ1 cos rθ1 sin rθ1 0 0 0 0 0 0 0 cos sθ2 sin sθ2 cos sθ2 sin sθ2 y1 y2 y3 y4 Now, we compute ROPE-2D(x(p,q), Θ), ROPE-2D(y(r,s), Θ) = (x1 x2) + (x3 x4) = (x1 x2) sin pθ1 (cid:18)cos pθ1 sin pθ1 cos pθ1 (cid:18)cos qθ2 sin qθ2 cos qθ2 sin qθ (cid:19)T (cid:18)cos rθ1 sin rθ1 cos rθ1 sin rθ1 (cid:19)T (cid:18)cos sθ2 sin sθ2 cos sθ2 (cid:19) (cid:18)y1 y2 (cid:19) (cid:19) (cid:19) (cid:18)y3 y4 sin sθ2 (cid:18)cos pθ1 cos rθ1 + sin pθ1 sin rθ1 sin rθ1 cos pθ1 + sin pθ1 cos rθ1 cos pθ1 cos rθ1 + sin pθ1 sin rθ1 sin rθ1 cos pθ1 sin pθ1 cos rθ1 + (x3 x4) (cid:18)cos qθ2 cos sθ2 + sin qθ2 sin sθ2 sin qθ2 cos sθ2 + sin qθ2 cos sθ2 cos qθ2 cos sθ2 + sin qθ2 sin sθ2 sin qθ2 cos sθ2 sin qθ2 cos sθ = (x1 x2) (cid:18)cos ((p r) θ1) sin ((p r) θ1) cos ((p r) θ1) sin ((p r) θ1) (cid:19)T (cid:19) (cid:18)y1 + (x3 x4) (cid:18)cos ((q s) θ2) sin ((q s) θ2) cos ((q s) θ2) sin ((q s) θ2) = ROPE-2D(y(pr,qs), Θ), ROPE-2D(y(0,0), Θ) (cid:19)T (cid:19) (cid:18)y3 y4 20 (cid:19)T (cid:19) (cid:18)y1 y2 (cid:18)y3 y4 (cid:19) (cid:19)"
        },
        {
            "title": "C Flexible Parsing Settings",
            "content": "In Section 4.3, we introduce three parsing levels which evaluate models under progressively looser constraints. While common evaluation metrics reward only exactly the answer format in the ground truth annotation, we seek to relax these requirements and investigate how model performance varies. Baseline: This setting requires exact following of prompt instructions, with model responses ending in the string \"Final Answer: <ANSWER>\". Flexible Parsing Level 1: This setting also catches cases where the model ends responses with \"Answer: <ANSWER>\". Flexible Parsing Level 2: Here we additionally catch cases where the model has added extra markdown formatting. We strip markdown such as: \"**Answer**\", \"**Answer:**\", \"*Answer: <ANSWER>*\". We find such formatting to be particularly prevalent in Llama-3.2 models [6]. Flexible Parsing Level 3: This is the most generous evaluation setting. Here we mark response as correct if the ground truth answer appears anywhere in the models response. For single letter answers, we search the response for \"is <A>\", \"are <A>\", \"<A>\". For single number responses, we search the response for the number both with and without commas. We highlight that Flexible Parsing Level 3 is intended to serve as an upper bound, as it may mark incorrect answers as correct."
        },
        {
            "title": "D Robustness to prompting",
            "content": "D.1 Llama-Specific Prompts In Section 4.1, we evaluate all models with common prompt, which allowed us to reproduce the reported figures of GPT-4o [16] and Claude-3.5 Sonnet [1]. This prompt requires models to end responses with \"Final Answer: <ANSWER>\" (see Appendix for full prompts). However, when evaluating Llama-3.2 models [6], we found that this model family defaults to responding with \"**Answer:** <ANSWER>\" (i.e., with markdown formatting and omission of Final, despite the explicit instruction). While the performance degradation due to regex mismatches is mitigated through our flexible parsing strategy (see Section 4.3), we found that Llama-3.2 models performed substantially better when the prompt specifically asks for \"**Answer:** <ANSWER>\" (i.e., respecting its default output format). In Table 6, we show the results for models both with the default prompts from Appendix A, and with the Llama-specific prompts (all evaluated under the Exact Match metric). We show that the Llama-specific prompt substantially improves the performance of Llama-3.2 models, particularly for the 11B variant, with over 15% jumps on both Mathvista and MMMU. We further note that Pixtral performance is stable across prompts, and leads the 11B variant by substantial margin. Mathvista MMMU ChartQA Exact Match Exact Match Exact Match Llama-3.2 11B [6] Llama-3.2 90B [6] Qwen2-VL 7B [23] Pixtral 12B Default prompt Llama-specific prompt Default prompt Llama-specific prompt Default prompt Llama-specific prompt Default prompt Llama-specific prompt 24.3 41.6 49.1 57.6 53.7 52.6 58.3 57.7 23.0 41.9 53.7 58. 48.1 47.4 52.0 50.8 14.8 33.7 33.8 34.8 41.2 74.0 81.8 83. Table 6: Evaluation with Llama-specific prompts. We re-evaluate models with prompt tailored towards the Llama-3.2 model family [6]. We find that this substantially improves the performance of the 11B variant of the model. Pixtral 12B reports stable performance across both prompts, and maintains substantial lead over Llama-3.2 11B and Qwen2-VL 7B. 21 D.2 Average performance across prompts Here we report average results across number of prompts. We task Mistral Large v2 with creating 10 versions of the prompt used in the main paper (see Appendix A), with varied wording while keeping instructions explicit. As prior works suffer under stricter parsing constraints, all models are evaluated under Flexible Parsing Level 3 for this experiment (see Section 4.3 and Appendix C). We find that the trends follow those from the main paper, with Pixtral outperforming models of comparable size, and surpassing Llama-3.2 90B [6] on Mathvista and ChartQA. Pixtral also typically displays lower variance in performance between prompts (shown in gray). Mathvista MMMU ChartQA Flexible Level 3 Flexible Level 3 Flexible Level 3 Llama-3.2 11B [6] Llama-3.2 90B [6] Qwen2-VL 7B [23] Pixtral 12B 42.1 (1.9) 56.0 (1.5) 53.7 (2.1) 56.4 (1.0) 45.3 (1.0) 56.7 (0.5) 46.9 (1.9) 49.5 (1.5) 77.2 (0.8) 80.1 (0.5) 77.0 (0.8) 83.8 (0.4) Table 7: Average multimodal performance across prompts. We evaluate models with 10 different prompts, reporting the mean performance, and standard deviations in gray. The trends follow those in the main paper, with Pixtral outperforming open-source models of comparable size. All models are evaluated with Flexible Level 3 parsing (see Section 4.3)"
        },
        {
            "title": "E Reproducing Reported Numbers",
            "content": "In Section 4.1 we re-evaluate all models under common and rigorous protocol. All models are evaluated under the same evaluation metric and with the same prompt, in such way that frontier models achieve their reported performance. Under this common protocol, we found some models substantially underperformed their reported figures. Here, we document the steps required to recover the reported figures of open models, by tuning the evaluation prompt and metric to each model in turn. All results are shown in Table 8. E.1 Summary Our analysis indicates that frontier models, and even smaller closed-source models, are able to recover or exceed their reported figures under the common protocol discussed in Section 4.1. This is achieved through precise following of instructions in the Explicit prompts (see Appendix A). Smaller, open-source models typically require some degree of prompt tuning and/or adjustment of the evaluation metric, targeted towards the model, to recover reported performance. With such interventions, we are generally able to recover or exceed reported figures. Pixtal 12B, like closed and leading models, is able to follow prompt instructions to report strong performance without targeted interventions. This is substantiated in robust performance across prompts (see Appendix D), as well as strong performance in both LMSys Vision Arena and MM-MTBench (see Section 4.1). E.2 Closed models: Claude-3 Haiku and Gemini-Flash-8B We find we the standardized evaluation protocol roughly matches or exceeds reported figures, with small gain achieved through flexible parsing. The only exception is for Claude Haiku [1] on ChartQA, where Flexible Parsing Level 3 is required to approach reported performance. E.3 Qwen2-VL 7B We first simplify the prompt into one-line instruction, similar to the training set of ChartQA. Next, we provide different prompts depending on the answer format expected. For instance, if the answer is floating point number, we specify \"Answer with two decimal place floating point\", 22 with analogous prompts for integer and multiple-choice questions. We found that providing single, unified prompt with all format specifications (as in the prompts in Appendix A) reduces performance. E.4 Llama-3.2 We find that these models default to responses with markdown formatting such as: \"**Answer**\", <ANSWER>*\". We find substantial improvement by changing the \"**Answer:**\", \"*Answer: Explicit prompt to request this format (see Appendix D). These models then recover their reported performance after evaluating with Flexible Level 3. When evaluating Llama-3.2 90B on DocVQA , many generations are of the form The answer is <ANSWER>, which is penalized by the ANLS metric. We strip such prefixes, and this improves DocVQA by +4.8. E.5 Llava-OneVision 72B Similarly to Qwen2-7B [23], we first simplify the prompt into one-line instruction and provide different prompts depending on the answer format expected. We found that providing single, unified prompt with all format specifications reduces performance. E.6 Molmo Similarly to Qwen2-7B [23] and Llava-Onevision 7B [9], we first simplify the prompt into one-line instruction, and provide different prompts depending on the answer format expected. Furthermore, similarly to the intervention for Llama-3.2 [6], we reformat the prompt and relax the evaluation metrics. Molmo models default to ending long responses with nn<ANSWER>. In long-answer cases, we adjust the evaluation metric to capture this. For VQAv2, we apply custom post-processing filters, such as remapping textual output of numerical answers to the integer digits (e.g. Two to 2). 23 Mathvista MMMU ChartQA DocVQA Pixtral 12B Qwen-2-VL 7B [23] Measured (Exact Match) Measured (Custom evaluation, see Section E.3) Reported Llama-3.2 11B [6] Measured (Exact Match) Measured (Custom evaluation, see Section E.4) Reported Molmo-D 7B [4] Measured (Exact Match) Measured (Custom evaluation, see Section E.6) Reported LLaVA-OneVision 7B [9] Measured (Exact Match) Measured (Custom evaluation, see Section E.5) Reported Molmo 72B [4] Measured (Exact Match) Measured (Custom evaluation, see Section E.6) Reported Llama-3.2 90B [6] Measured (Exact Match) Measured (Custom evaluation, see Section E.4) Reported Claude-3 Haiku [1] Measured (Exact Match) Measured (Custom evaluation, see Section E.2) Reported Gemini-1.5-Flash 8B(0827) [18] Measured (Exact Match) Measured (Custom evaluation, see Section E.2) Reported CoT 58.3 53.7 63.7 58. 24.3 47.9 51.5 12.3 43.2 51.6 36.1 63.1 63.2 52.2 61.3 58.6 49.1 57.5 57.3 44.8 44.8 46. 56.9 57.1 - CoT 52.0 48.1 50.6 54.1 23.0 46.6 50.7 24.3 47.0 45. 45.1 48.1 48.8 52.7 52.9 54.1 53.7 60.2 60.3 50.4 51.3 50.2 50.7 50.7 50.3 CoT 81.8 41.2 83.4 83.0 14.8 78.5 83.4 27.0 76.7 84.1 67.2 80.2 80.0 75.6 82.3 87. 33.8 91.7 85.5 69.6 79.8 81.7 78.0 78.2 - ANLS 90.7 94.5 94.5 94. 91.1 91.1 88.4 72.2 72.2 92.2 90.5 90.5 87.5 86.5 86.5 93.5 85.7 91.5 90.1 74.6 74.6 88. 79.5 79.5 73.6 VQAv2 MM-MT-Bench LMSys-Vision VQA Match GPT-4o Judge (Oct 24) 78.6 6. 1076 75.9 82.1 - 67.1 67.1 75.2 57.1 70.0 85.6 78.4 83.7 - 75.2 75.5 86. 67.0 67.0 78.1 68.4 68.4 - 65.5 69.2 - 5.45 - - 4.79 - - 3.72 - - 4.12 - - 3.51 - - 5.50 - - 5.46 - - 5.93 - - 1032 1000 1111 Table 8: Reproducing the reported performance of prior models. In Table 2 we conduct fair re-evaluation of all models through the same evaluation harness, with the same prompt and metric. Here, we endeavour to recover the reported performance of all models by tuning evaluation settings towards individual models. We highlight that Pixtral 12B, like strong closed-source models (e.g. Gemini-1.5-Flash 8B [18] and Claude-3 Haiku [1]) is able reports strong performance without such interventions."
        }
    ],
    "affiliations": []
}