{
    "paper_title": "DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation",
    "authors": [
        "Junhao Chen",
        "Mingjin Chen",
        "Jianjin Xu",
        "Xiang Li",
        "Junting Dong",
        "Mingze Sun",
        "Puhua Jiang",
        "Hongxiang Li",
        "Yuhang Yang",
        "Hao Zhao",
        "Xiaoxiao Long",
        "Ruqi Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Controllable video generation (CVG) has advanced rapidly, yet current systems falter when more than one actor must move, interact, and exchange positions under noisy control signals. We address this gap with DanceTogether, the first end-to-end diffusion framework that turns a single reference image plus independent pose-mask streams into long, photorealistic videos while strictly preserving every identity. A novel MaskPoseAdapter binds \"who\" and \"how\" at every denoising step by fusing robust tracking masks with semantically rich-but noisy-pose heat-maps, eliminating the identity drift and appearance bleeding that plague frame-wise pipelines. To train and evaluate at scale, we introduce (i) PairFS-4K, 26 hours of dual-skater footage with 7,000+ distinct IDs, (ii) HumanRob-300, a one-hour humanoid-robot interaction set for rapid cross-domain transfer, and (iii) TogetherVideoBench, a three-track benchmark centered on the DanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure skating. On TogetherVideoBench, DanceTogether outperforms the prior arts by a significant margin. Moreover, we show that a one-hour fine-tune yields convincing human-robot videos, underscoring broad generalization to embodied-AI and HRI tasks. Extensive ablations confirm that persistent identity-action binding is critical to these gains. Together, our model, datasets, and benchmark lift CVG from single-subject choreography to compositionally controllable, multi-actor interaction, opening new avenues for digital production, simulation, and embodied intelligence. Our video demos and code are available at https://DanceTog.github.io/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 8 7 0 8 1 . 5 0 5 2 : r DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation Junhao Chen1 Mingjin Chen2 Jianjin Xu3 Xiang Li4 Puhua Jiang1 Hongxiang Li4 Yuhang Yang6 Junting Dong5 Mingze Sun1 Hao Zhao1 Xiaoxiao Long7 Ruqi Huang1 1Tsinghua University 3Carnegie Mellon University 4Peking University 6University of Science & Technology of China 2Beijing NormalHong Kong Baptist University 5Shanghai AI Laboratory 7Nanjing University Project Page: https://DanceTog.github.io/ Figure 1: DanceTogether generates complex two-person interaction videos with interactive details and consistent identity preservation from single reference image (see the left-most of each row), using independent multi-person pose and mask sequences as control signals."
        },
        {
            "title": "Abstract",
            "content": "Controllable video generation (CVG) has advanced rapidly, yet current systems falter when more than one actor must move, interact, and exchange positions under noisy control signals. We address this gap with DanceTogether, the first end-to-end diffusion framework that turns single reference image plus independent posemask streams into long, photorealistic videos while strictly preserving every identity. novel MaskPoseAdapter binds who and how at every denoising step by fusing robust tracking masks with semantically rich but noisypose heat-maps, eliminating the identity drift and appearance bleeding that plague frame-wise pipelines. To train and evaluate at scale, we introduce (i) PairFS-4K, 26 of dual-skater footage with 7,000+ distinct IDs, (ii) Corresponding Author. HumanRob-300, one-hour humanoidrobot interaction set for rapid cross-domain transfer, and (iii) TogetherVideoBench, three-track benchmark centred on the DanceTogEval-100 test suite covering dance, boxing, wrestling, yoga, and figure skating. On TogetherVideoBench, DanceTogether outperforms the prior arts by significant margin. Moreover, we show that one-hour fine-tune yields convincing humanrobot videos, underscoring broad generalization to embodied-AI and HRI tasks. Extensive ablations confirm that persistent identityaction binding is critical to these gains. Together, our model, datasets, and benchmark lift CVG from singlesubject choreography to compositionally controllable, multi-actor interaction, opening new avenues for digital production, simulation, and embodied intelligence. Our video demos and code are available at https://DanceTog.github.io/."
        },
        {
            "title": "Introduction",
            "content": "Controllable video generation (CVG) [98, 93, 56, 39, 62] seeks to translate explicit control signalse.g. per-frame human poses, body masks, or trajectory commandsinto photorealistic human-motion videos. Compared to AI generation tasks that use single conditioning (reference images or text)[63, 5, 61, 35], some controllable generation tasks typically combine multi-modal conditions as input [114, 62, 68, 29, 106, 15, 12, 56]. Such tasks using multi-modal control signals have broad and important applications in film production [7, 70, 30], digital human interaction [78, 95, 13, 101, 74, 40, 89], and embodied AI [4, 82, 8, 116, 38, 25, 65, 98, 20, 105, 2]. In particular, we investigate the task of CVG with multi-person interactions, which is highly challenging as it simultaneously requires (i) preserve the identities of multiple actors over hundreds of frames, (ii) maintain the spatio-temporal coherence of complex interactions such as hand-holding, lifts, position exchanges, and synchronous choreography, and (iii) faithfully obey noisy control signals in the presence of occlusion, motion blur, and rapid viewpoint changes. Most existing systems adopt frame-wise synthesis followed by temporal smoothing paradigm: each image is generated independently from pose or text conditions and then stitched into video via interpolation, optical-flow warping, or temporal convolutions [18, 56, 122, 10, 102, 55]. Nearly all of these models are trained solely on single-person dance datasets [121, 96, 29, 56, 87, 115, 86, 34, 85, 41, 62]. handful of works incorporate multi-person footage [91, 112, 99], but they exhibit pronounced identity drift and appearance bleeding when the actors exchange positions. In general, state-of-the-art methods struggle with identity inconsistency, cross-subject contamination, and missing interaction detailsissues that rapidly worsen once more than one performer is involved. We present DanceTogether, the first end-to-end diffusion framework expressly tailored for controllable multi-person interaction video generation. Our guiding hypothesis is that robust multi-actor synthesis requires an explicit, persistent binding between identity and motion throughout the diffusion process. To this end, we deliberately disentangle identity from action and then re-couple them: instead of relying solely on fragile pose estimates, we fuse stable tracking masks with semantically rich pose cues. This fusion is realised by novel conditional adapter, MaskPoseAdapter, which combines the reliable, easy-to-obtain body masks with the informative yet noisy poses into bimodal control signal. By integrating each subjects mask and pose into unified representation, the adapter enforces precise identity-to-action alignment at every generative step. Our framework operationalizes the identityaction binding principle through three tightly coupled modules. (i) MultiFace Encoder distills compact set of identity tokens from single image and injects them into every cross-attention layer, ensuring subject appearance is held constant throughout the sequence. (ii) MaskPoseAdapter fuses robust per-person tracking masks with semantically richbut noisypose maps to deliver bimodal conditional signal that aligns who and how at every diffusion step, thereby safeguarding both identity integrity and motion fidelity. (iii) Video Diffusion Backbone leverages these aligned signals to synthesize high-resolution clips whose multiactor motions remain coherent, physically plausible, and free of inter-subject drift. Extensive evaluation on the new TogetherVideoBenchbuilt around our 100-clip DanceTogEval-100 setshows that DanceTogether decisively advances controllable multi-person video generation. Across the three core tracks (Identity-Consistency, Interaction-Coherence, Video Quality) it raises the bar over the strongest prior (StableAnimator [79] +swing dance data [58] finetune) by +12.6 HOTA, +7.1 IDF1, +5.9 MOTA, trims MPJPE2D by 69 % (1555 492 px), and boosts OKS/PoseSSIM to 0.83/0.93. Visual fidelity also improved accordingly: human mask region FVD/FID decreased from 29.0/66.7 to 17.1/48.0, without sacrificing CLIP alignment effect. Fine-tuning on our proposed one-hour HumanRob-300 dataset can generate convincing human-robot interaction videos, which highlights the frameworks broad generalization capability and prospects in embodied AI research. To summarize, our main contributions include: 1. DanceTogether framework. We present the first end-to-end diffusion framework for controllable multi-person interaction video generation. Our novel MaskPoseAdapter fuses stable tracking masks with pose cues to enforce identity-action binding throughout the generation process. 2. Data curation pipeline and datasets. We develop monocular-RGB pipeline for extracting tracking-aware human poses and masks. Using this, we curate PairFS-4K (26h dual-person figure skating) and HumanRob-300 (1h robot interaction) datasets. 3. TogetherVideoBench benchmark. We introduce comprehensive evaluation benchmark with three tracks (Identity-Consistency, Interaction-Coherence, Video Quality) and DanceTogEval-100 containing 100 dual-actor clips across diverse activities. 4. Superior performance and generalization. Our method achieves significant improvements: +12.6 HOTA, +7.1 IDF1, +5.9 MOTA over the strongest baseline, 69% reduction in pose error, and enhanced visual fidelity (FVD: 29.017.1). Cross-domain fine-tuning demonstrates strong generalization to human-robot scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Diffusion Models for Video Generation In recent years, diffusion models have achieved great achievements in the field of video generation [64, 45, 37, 44, 44, 83, 84, 14, 47, 24]. In the technical solution of video generation model, early work mainly used 3D-Unet to achieve consistent fusion of time and space [69, 28]. On this basis, [6] introduces the time dimension into the latent spatial diffusion model to convert the image generator into video generator; further, [28] uses the basic video generation model and series of interleaved [94] is spatial and temporal video super-resolution models to generate high-definition videos; based on end-to-end video generation and editing of the diffusion model, and uses spatiotemporal consistency modeling and multimodal condition control to support video generation under multimodal conditions such as text, images, and video. [5] Based on the potential diffusion model transformation of 2D image synthesis training, good time insertion strategy for managing video data is proposed. Although large-scale commercial pre-trained models such as [37, 1, 61] have good time consistency and high resolution, they still cannot meet the video generation task using fine human motion control signal input. 2.2 Controllable Human Video Generation The integration of diffusion models [67, 5, 56, 99, 112, 85, 97, 29, 42, 41, 34, 21, 86, 79, 121, 62] has greatly advanced controllable human video generation, with most methods building on pretrained Stable Diffusion and incorporating action or pose guidance for continuous video synthesis. Pose conditions are commonly represented by keypoints or skeletons, as in ControlNet [114] and ReferenceNet [29], and are used as conditional inputs during denoising. For instance, Disco [86] separates background and pose control via dedicated modules, strategy extended by later works [29, 97] to improve video continuity. Other approaches [85, 49, 115] introduce geometric priors, using rendered images from 3D models (e.g., depth, normal, semantic maps) as pose conditions, while methods like [121, 41] employ SMPL models or 2D keypoints, but are mostly limited to single-person or simple multi-person scenarios. Despite these advances, most methods focus on single-person generation and struggle with complex multi-person interactions and identity consistency. To address identity preservation, recent works [91, 87, 115, 54, 92] explore pose-guided identity maintenance, such as using identity encodings or masks [111], but these are often limited to short or simple videos. Tevet et al. [77] generate high-quality action sequences but lack robust identity modeling for long, complex videos. Some video-oriented methods [85, 115, 92] use local masks or attention to reduce identity confusion, but still lack explicit identity-action binding, leading to drift in long sequences. 3 Figure 2: DanceTogether pipeline overview: single reference image and per-person pose/mask sequences enter the system; the MaskPoseAdapter fuses these control signals, the MultiFace Encoder injects identity tokens, and the video-diffusion backbone synthesizes an interaction video that preserves consistent identities for all actors."
        },
        {
            "title": "3 Method",
            "content": "3.1 Overview: DanceTogether Pipeline Given reference image Iref and per-person control signals {Pi, Mi}N i=1 (pose maps and tracking masks for individuals across frames), DanceTogether synthesizes video ˆV RT 3HW while (i) preserving each identity, (ii) respecting the spatio-temporal interaction encoded in the poses, and (iii) remaining consistent with the poses and masks. The pipeline  (Fig. 2)  contains three key learnable modules including Video Diffusion Backbone (Sec. 3.2), MaskPoseAdapter (Sec. 3.3) and MultiFace Encoder (Sec. 3.4). 3.2 Video Diffusion Backbone Starting point StableAnimator. Our backbone follows the StableAnimator architecture [79]: 16-frame latent UNet fθ derived from Stable Video Diffusion (SVD). For every training clip we take as input (cid:0)Iref , P1:T , M1:T (cid:1) where Iref R3HW is reference image, and Pt / Mt are the pose map and tracking mask at frame t. Three conditioning streams. The UNet is conditioned by three streams, each of which begins with frozen pretrained encoder and is then refined by trainable adapters (see Fig. 2): Latent image stream. frozen SVD VAE encoder maps both the reference image Iref and each input video frame to their respective latent representations. The reference latent zref RC6464 is tiled along the temporal axis and concatenated with the per-frame latents zgt. This concatenated tensor is then fused with the trainable MaskPoseAdapters condition latents via element-wise addition, producing the final latent input to the UNet. CLIP image embeddings. frozen ViT-H/14 encoder ϕCLIP produces eclip R1024. These embeddings serve as keys/values in every trainable cross-attention block. Refined face embeddings. frozen ArcFace model ϕID outputs eid R512, which is then refined by the trainable MultiFaceEncoder gψ: Eface = gψ (cid:0)eid, eclip(cid:1) RKd, (1) 4 implemented as four Perceiver-IO layers (K = 4, = 768). The resulting identity tokens modulate the same trainable cross-attention layers. Distribution-aware ID Adapter. To prevent feature-distribution shift when injecting identity tokens, StableAnimator inserts an ID Adapter before each temporal block. Given input features h, we first apply spatial self-attention and two cross-attention steps, then align and fuse the face branch to the image branch in single fused update: ˆh = SA(h), himg = CA(ˆh, eclip), hface = CA(ˆh, Eface), hface = hface µface σface σimg + µimg, hout = himg + hface. (2) Here SA/CA denote self-/cross-attention, (µ, σ) are the per-token mean and standard deviation, and Eface the set of identity tokens. By matching the first and second moments of the face and image features, this adapter preserves identity information consistently across all frames. Human-tracking masked reconstruction loss. Building upon StableAnimators face-focused loss, we incorporate per-person binary masks for face and body regions. Original 512 512 masks are downsampled via nearest-neighbor interpolation to the latent resolution 64 64. Given individuals with binary masks body {0, 1}16464, we optimize face, Lrec = (cid:88) i=1 EϵN (0,1) (cid:13) (cid:13)(zgt zϵ) (cid:0)1 + (cid:13) body + 2 face (cid:1)(cid:13) 2 (cid:13) (cid:13) 2 . (3) Here body masks have weight 1 and face masks weight 2, encouraging the model to focus capacity on identity-critical regions while preserving overall reconstruction fidelity. 3.3 MaskPoseAdapter Relying solely on pose keypoints (pose maps) makes it difficult to distinguish different individuals in multi-person scenarios; directly treating binary tracking masks as additional channels would compromise the translational invariance of the pose encoder. We therefore propose MaskPoseAdapter: first performing lightweight transformations on masks in the pose feature space, then injecting them into pose latents using gated-weighting strategy, and finally applying cross-person soft-attention to reorder per-person importance. Fig. 2 illustrates MaskPoseAdapter, which fuses independent pose streams and masks into single posemask latent RBC6464. Per-person Pose Encoding. For each person i, an independent PoseNet0401 processes the RGB pose map Pi R3512512. PoseNet consists of eight convolutional layers, expanding the channels from 3 to 128, followed by 1 1 convolution, with weights shared across all persons. The output pose features are then scaled by learnable factor s. The final output can be expressed as: pose = Conv11 (cid:0)PoseNet(Pi)(cid:1) RC6464, = 320, (4) Light Mask Processor. Binary human/facial masks Mi {0, 1}1512512 are processed through two 3 3 convolutional layers to produce 3-channel feature map: which preserves contour information while avoiding mask features from dominating the pose features. mask = ψ(Mi) R36464, (5) Gate-based Fusion. We apply two per-pixel gates to control how much of the pose and mask features to trust. These gates are implemented as convolutional layers followed by Sigmoid activations. The gate outputs are: )(cid:1), wmask (6) where γ and η are each ConvSiLUConvSigmoid sequence. The gated features are then combined with learnable weight λ 0.8 as: = σ(cid:0)η(f mask = σ(cid:0)γ(f pose wpose )(cid:1), i pose fi = λ wpose (cid:125) (cid:123)(cid:122) (cid:124) ID-dominant + (1 λ) wmask (cid:123)(cid:122) fine mask (cid:124) 5 mask , (cid:125) (7) where pose fusion, where the coefficient αres controls the strength of the residual term: is the pose feature reduced to 3 channels for gating. residual link is added to refine the fi = fi + αres (cid:0)(1 λ) wmask mask (cid:1), αres = 0.5. (8) Pose Enhancer. The fusion output is passed through lightweight PoseEnhancer module consisting of 3 3 convolution, followed by SiLU activation and BatchNorm, and 1 1 convolution: hi = PoseEnhancer(fi). (9) To further refine the pose features, scaling factor sp = 1.5 is applied to the raw features before final integration: fi = sp fi + (1 αres) hi. (10) LayerNorm and Attention. Each of the enhanced pose features fi is normalized per-channel using LayerNorm, resulting in fi. The normalized features are concatenated along the channel dimension and processed through lightweight attention mechanism consisting of three 1 1 convolution layers, each followed by BatchNorm and ReLU. This generates attention logits ℓi for each person: ℓ = ϕ(cid:2)LayerNorm(f1), . . . , LayerNorm(fN )(cid:3) RN 6464. These logits are normalized across the person dimension using temperature-scaled softmax function: (11) αatt = SoftmaxWithTempτ (ℓ), SoftmaxWithTempτ (x) = softmax(x/τ ), (12) where τ is learnable temperature parameter. Cross-Person Integration. We integrate the normalized features using both attention weighting and concatenation. First, we compute an attention-weighted sum of the features: = (cid:88) i=1 αatt,i fi. (13) Then, we pass the weighted sum through 1 1 integration convolution to fuse the multi-person features into final representation: Finally Fint = Conv11(S). = 0.95 Fint + 0.05 1 N (cid:88) i=1 fi, (14) (15) where RC6464 is reshaped to (B, T, C, 64, 64) and injected into the UNet. 3.4 MultiFace Encoder For every mini-batch we receive Eid RN BDid with Did = 512 and Dclip = 1024, where the first axis enumerates the identities and the second the samples in the batch. Each sample also carries length-1 CLIP embedding eclip RB1Dclip, which is used as key/value memory in all cross-attention steps. Stage Per-identity token projection. For identity {1, . . . , } and sample we i,b with two-layer MLP (Linear(512,1024) GELU transform the ArcFace vector eid Linear(1024,KD)) and reshape it into = 4 learnable tokens of width = 768: xi,b = MLP2GELU(eid i,b = LN(cid:0)reshapeKD(xi,b)(cid:1) RKD. t(0) i,b) RKD, (16) (17) Stage II FacePerceiver refinement. The latent tokens t(0) with depth Lp = 4: i,b query lightweight FacePerceiver i,b = t(ℓ) t(ℓ+1) (cid:16) i,b + FFN i,b + CrossAttn(cid:0)t(ℓ) t(ℓ) i,b , eclip (cid:1)(cid:17) , ℓ = 0, . . . , 3. (18) 6 Queries originate from the latent tokens, whereas keys/values are the concatenation of the projected CLIP embedding and the tokens (cf. PerceiverAttention in the code). residual shortcut controlled by the flags shortcut, scale (λ) reproduces the exact behaviour of MultiFace Encoder: ti,b = (cid:40)t(4) i,b , t(0) i,b + λ t(4) shortcut = 0, i,b , shortcut = 1. (19) Stage III Multi-person concatenation. After processing all identities with shared weights, the refined tokens are stacked along the sequence axis: Tb = (cid:2)t1,b; t2,b; . . . ; tN,b RBN KD for the batch. (cid:3) R(N K)D, (20) The UNets cross-attention layers can therefore read directly, gaining NK extra tokens without any architectural change. 3.5 Data Curation Pipeline To address the lack of two-person interaction datasets with diverse identities, static backgrounds, and fixed cameras, we propose comprehensive data curation pipeline that recovers poses and mask annotations from monocular RGB videos. As shown in Fig. 3, our pipeline segments videos into scenes, detects and tracks individuals using YOLOv8x [33] and OSNet-based ReID [118, 119], and selects primary subjects based on coverage and consistency. We then generate high-quality per-person masks and 133-point pose annotations using SAMURAI [103], DWPose [107], and MatAnyone [104], followed by automatic and manual filtering to ensure data quality. We aggregate wide range of singleand two-person motion datasetsincluding TikTokDataset [32], Champ [121], DisPose [41], HumanVid [91], Swing Dance [58], Harmony4D [36], CHI3D [22], Beyond Talking [75], and our newly collected PairFS-4Kto maximize identity diversity and interaction types. PairFS-4K, comprising 4.8K figure skating segments and over 7,000 unique identities, is the first large-scale two-person figure skating video dataset. All datasets are summarized in Tab. 1, providing rich foundation for controllable human interaction video generation in real-world scenarios. More details of the Data Curation Pipeline can be found in Sec. D. For specifics on the collection and processing of PairFS-4K, please refer to Sec. E. Figure 3: Data Curation Pipeline Overview. Our pipeline processes raw videos through human tracking, mask generation with SAMURAI [81], pose estimation with DW-Pose [107], and alpha matting to produce per-person annotations. 3.6 TogetherVideoBench Benchmark We introduce TogetherVideoBench, comprehensive benchmark for controllable multi-person video generation, which systematically evaluates three orthogonal tracks: Identity-Consistency, Interaction-Coherence, and Video Quality. Please refer to details Sec. in the Appendix. 7 Table 1: Summary of datasets used in DanceTogether training. Static competition background; Static laboratory background; Multi-view setup. Dataset TikTokDataset [32] Champ [121] DisPose [41] HumanVid [91] Hi4D [110] Harmony4D [36] CHI3D [22] Swing Dance [58] HoCo [75] Type Single Single Single Single Double Double Double Double Double Action Dance Dance Dance Dance Interact Interact Interact Dance Talking Head PairFS-4K Double HumanRob-300 Single DanceTogEval-100 Double Figure Skating Robot Interact Interact & Dance"
        },
        {
            "title": "4 Results",
            "content": "4.1 Experimental Setup IDs Total Avg. Scene Camera 332 832 8,636 16,310 40 24 6 1,356 26 7,273 336 200 1.03 hrs 9.73 hrs 38.12 hrs 89.89 hrs 0.10 hrs 0.58 hrs 1.75 hrs 23.36 hrs 45 hrs 26.87 hrs 0.83 hrs 0.54 hrs 11 42 11 17 3.6 12 4 122 7 20 9 20 Static Static Static Fixed Fixed Fixed Dynamic Moving Fixed Static Fixed Static Static Fixed Static Moving Static Fixed Static Moving Dynamic Moving Fixed Static We collect several publicly available video datasets, as detailed in Section D.1. We utilize DWPose [107] and ArcFace [17] to extract skeletal poses and facial embeddings/masks. To evaluate the robustness of our model, we conduct experiments on DanceTogEval-100, curated set of 100 previously unseen two-person interaction videos from the internet. Following recent advances in animation generation [79], we initialize our U-Net, PoseNet, and Face Encoder with the pre-trained weights from StableAnimator [5], then further train them on large-scale single-person datasets [79, 41, 121, 91]. We subsequently transfer the pre-trained weights to our proposed MaskPoseAdapter and MultiFace Encoder, and perform full fine-tuning using multi-person datasetsincluding our proposed PairFS dataset [58, 75, 36, 22]. Our model is trained for 20 epochs on 8 NVIDIA A100 80G GPUs, with batch size of 1 per GPU and learning rate set to 1e 5. For ablation study, please refer to Sec. C. 4.2 Baselines We compare our approach with state-of-the-art pose-conditioned human video generation models, including Animate Anyone [29], Champ [121], MimicMotion [115], HumanVid [91], UniAnimate [87], UniAnimate-DiT [88], DisPose [41], and StableAnimator [79]. In particular, we fine-tune StableAnimator for 40 epochs on the dual-person dancing subset from the Swing Dance dataset [58], and include this fine-tuned variant as new baseline in our evaluation. Fig. 4 compares our proposed DanceTogether with four strong baselines Animate Anyone [29], HumanVid [91], UniAnimate [87], and StableAnimator [79] all of which achieve relatively high scores in the quantitative evaluation. Additional comparisons, including more baselines and dual-person interaction examples, are provided in the appendix Sec. G. 4.3 Quantitative Results Track 1: IdentityConsistency. Table 2 reports multiple-object-tracking (MOT) scores on DanceTogEval-100. Across all eight published baselines, StableAnimator fine-tuned on SwingDance (StableAnimator +Dataswing) is the previous best performer, reaching 71.35 HOTA and 82.53 IDF1. DanceTogether markedly exceeds this strong baseline on every single metric: with full training data it lifts HOTA from 71.35 to 81.79 (+10.44 %) and IDF1 from 82.53 to 87.73 (+6.3 %), while pushing AssA to 86.69. Adding the proposed PairFS-4K dataset provides further gain, culminating in 83.94 HOTA, 89.59 IDF1, and 79.80 MOTA. These results establish new state of the art for long-range identity preservation under frequent occlusions and position exchanges. Track 2: InteractionCoherence. Table 3 evaluates how faithfully each method follows the target motion and how smoothly the interaction unfolds. Our model slashes MPJPE2D by 68 % relative to the top baseline (from 1555 px to 492 px) and attains the highest OKS (0.83) and 8 Figure 4: The RGB image in the Ref Image row is the input reference frame, and the two pose maps in that row correspond to the inference results shown immediately below. All baselines exhibit severe identity drift, loss of interaction details, or even missing subjects when dealing with position exchanges and complex interactive poses. For additional qualitative results, please refer to Appendix Fig. 11 and Fig. 12. Table 2: Multiple Object Tracking results on TogetherVideoBench. Negative values occur because the sum of false positives (FP) and false negatives (FN) exceeds the number of ground truth objects. This happens when the frames only contain single person. Method Animate Anyone [29] Champ [121] MimicMotion [115] HumanVid [91] UniAnimate [87] UniAnimate-DiT [88] DisPose [41] StableAnimator [79] StableAnimator w. Dataswing DanceTog w. Dataswing DanceTog w. Dataf ull DanceTog w. Dataf ull + DataP airF Venue CVPR 2024 ECCV 2024 Arxiv 2024 NeurIPS 2024 SCIS 2025 Arxiv 2025 ICLR 2025 CVPR 2025 CVPR 2025 HOTA family CLEAR/MOTA family Identity HOTA DetA AssA MOTA MOTP IDF1 41.26 19.32 21.14 56.12 48.43 35.02 20.68 67.75 71.35 80.26 81.79 83.94 39.99 14.78 16.06 58.89 46.71 31.15 15.91 67.91 70.91 74.44 77.19 79. 43.21 26.32 30.50 53.69 50.69 40.65 29.47 67.70 71.89 86.57 86.69 88.68 26.67 -19.54 -55.77 58.86 42.33 10.66 -52.49 69.62 73.89 73.68 77.04 79.80 75.73 67.92 62.13 84.20 80.74 77.99 62.00 87.67 88.22 95.45 95.69 95. 51.54 17.84 15.24 68.84 59.58 39.51 15.42 79.37 82.53 86.28 87.73 89.59 PoseSSIM (0.93). At the same time, DanceTogether records the lowest motion-discontinuity scoresSmoothRMS 0.83 106 and TimeDynRMSE 1.59 104indicating physically plausible, temporally consistent choreography. Champ achieves high scores on SmoothRMS and TimeDynRMSE due to its use of estimated SMPL as guidance, which incorporates smoothing methods in the process of generating SMPL sequences. These two metrics only compare the motion continuity of each individual person without applying weights to the pair. Champs inference results typically contain only single person; for qualitative comparison results, please refer to Sec. G. The FVMD is halved compared with StableAnimator (0.54 vs. 1.00), further corroborating superior interaction quality. Track 3: Video Quality. Tables 4 and 5 present full-frame and mask-aware appearance metrics. Benefiting from dense identityaction binding and the high-diversity PairFS-4K corpus, DanceTogether delivers the best perceptual fidelity in both settings. In full-frame evaluation it attains the lowest FVD (76.3) and FID (75.1), alongside the highest CLIP score (0.95) and ST-SSIM (0.70). Within the human-masked regionsthe areas most sensitive to identity driftmask-aware FVD plunges from 29.0 to 17.1, and C-FID shrinks from 12.5 to 7.9, highlighting crisp texture reproduction and identity 9 Table 3: Comparison of models across interaction coherence metrics. Method MPJPE2D OKS PoseSSIM Animate Anyone Champ MimicMotion HumanVid UniAnimate UniAnimate-DiT DisPose StableAnimator StableAnimator w. Dataswing DanceTog w. Dataswing DanceTog w. Dataf ull DanceTog w. Dataf ull + DataP airF 3255.07 4117.88 5542.99 3480.74 2286.26 2184.81 2791.60 1571.50 1555.16 858.99 557.60 492. 0.27 0.06 0.09 0.48 0.37 0.22 0.08 0.63 0.70 0.75 0.81 0.83 0.67 0.78 0.74 0.78 0.72 0.71 0.73 0.82 0.84 0.88 0.92 0.93 SmoothRMS TimeDynRM SE (106) (104) 1.26 0.78 1.02 1.09 1.24 1.53 1.07 0.96 0.89 0.84 0.85 0.83 2.43 1.60 1.94 2.10 2.36 2.92 2.04 1.84 1.72 1.62 1.64 1.59 FVMD (105) 1.87 0.90 1.15 1.11 2.13 3.72 1.36 1.00 0.77 0.51 0.66 0.54 accuracy. Notably, these improvements are achieved without sacrificing low-level reconstruction fidelity: L1 and LPIPS fall in tandem, while PSNR and SSIM increase. Table 4: Comparison of models using Full Frame evaluation metrics. Method L1 PSNR SSIM LPIPS DISTS CLIP ST-SSIM GMSD-T FVD FID C-FID AnimateAnyone Champ MimicMotion HumanVid UniAnimate UniAnimate-DiT DisPose StableAnimator StableAnimator w. Dataswing DanceTog w. Dataswing DanceTog w. Dataf ull DanceTog w. Dataf ull + DataP airF 37.32 43.70 52.08 38.93 37.95 43.11 42.52 33.44 30.31 32.62 29.94 29.52 13.23 11.93 11.04 13.67 13.62 12.34 12.28 14.60 15.27 15.12 15.81 15.85 0.49 0.49 0.47 0.52 0.55 0.50 0.54 0.57 0.60 0.59 0.61 0. 0.56 0.56 0.58 0.50 0.53 0.53 0.54 0.44 0.42 0.44 0.42 0.42 0.27 0.29 0.32 0.26 0.29 0.28 0.31 0.24 0.22 0.23 0.22 0.22 0.91 0.91 0.91 0.93 0.89 0.92 0.91 0.94 0.94 0.94 0.95 0. 0.54 0.39 0.37 0.53 0.61 0.45 0.41 0.66 0.69 0.68 0.70 0.70 0.42 0.36 0.39 0.35 0.42 0.42 0.39 0.40 0.42 0.38 0.39 0.39 108.2 125.7 121.0 97.2 132.0 111.9 127.4 85.7 78.8 79.3 76.9 76. 118.1 114.6 116.6 90.2 151.2 100.3 127.9 84.1 79.3 82.1 77.6 75.1 27.7 25.6 26.2 18.6 42.8 20.8 31.0 18.1 16.1 14.7 13.1 12.6 Table 5: Comparison of models using Human Masked Region evaluation metrics. Method L1 PSNR SSIM LPIPS DISTS CLIP ST-SSIM GMSD-T FVD FID C-FID AnimateAnyone Champ MimicMotion HumanVid UniAnimate UniAnimate-DiT DisPose StableAnimator StableAnimator w. Dataswing DanceTog w. Dataswing DanceTog w. Dataf ull DanceTog w. Dataf ull + DataP airF 59.92 83.35 77.02 47.97 56.34 64.48 76.75 48.51 41.41 34.49 32.80 30.14 10.45 8.36 8.75 12.13 11.05 9.89 8.93 12.00 13.06 14.76 15.15 15.82 0.92 0.92 0.92 0.93 0.92 0.91 0.92 0.93 0. 0.94 0.94 0.94 0.06 0.07 0.07 0.05 0.06 0.06 0.07 0.05 0.04 0.03 0.03 0.03 0.13 0.17 0.16 0.12 0.13 0.14 0.16 0.12 0.11 0.09 0.09 0.08 0.92 0.90 0.90 0.93 0.92 0.90 0.90 0.93 0. 0.94 0.94 0.95 0.70 0.58 0.56 0.76 0.70 0.68 0.60 0.75 0.80 0.85 0.85 0.87 0.18 0.17 0.17 0.15 0.17 0.18 0.17 0.15 0.14 0.14 0.14 0.14 44.8 69.2 65.5 34.9 45.0 51.4 64.7 38.4 29. 21.5 20.6 17.1 101.4 178.7 180.9 72.4 109.8 119.6 196.0 71.8 66.7 57.5 56.1 48.0 19.2 34.2 33.9 14.2 21.4 21.5 36.4 15.7 12.5 9.5 8.9 7."
        },
        {
            "title": "5 Conclusion",
            "content": "We present DanceTogether, the first end-to-end diffusion framework for generating long, photorealistic multi-actor videos from single reference image and independent posemask streams, while strictly preserving each identity. Our method integrates novel MaskPoseAdapter for persistent identityaction alignment and MultiFace Encoder for compact appearance encoding. Trained on our newly curated multi-actor datasets and evaluated on comprehensive benchmark, DanceTogether outperforms all existing pose-conditioned video generation models by significant margin. It generalizes well across domains, as demonstrated by convincing humanrobot interactions after minimal adaptation. This work marks step forward toward compositionally controllable, identity-aware video synthesis, laying foundation for future advances in digital content creation, simulation, and embodied AI."
        },
        {
            "title": "References",
            "content": "[1] Kling AI. Kling ai: Next-generation ai creative studio. 10 [2] Mert Albaba, Chenhao Li, Markos Diomataris, Omid Taheri, Andreas Krause, and Michael Black. Nil: Nodata imitation learning by leveraging pre-trained video diffusion models. arXiv preprint arXiv:2503.10626, 2025. [3] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear mot metrics. EURASIP Journal on Image and Video Processing, 2008:110, 2008. [4] Homanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, and Sean Kirmani. Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation. arXiv preprint arXiv:2409.16283, 2024. [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22563 22575, 2023. [7] Emanuele Bugliarello, Anurag Arnab, Roni Paiss, Pieter-Jan Kindermans, and Cordelia Schmid. What are you doing? closer look at controllable human video generation. arXiv preprint arXiv:2503.04666, 2025. [8] Haoye Cai, Chunyan Bai, Yu-Wing Tai, and Chi-Keung Tang. Deep video generation, prediction and completion of human action sequences. In Proceedings of the European conference on computer vision (ECCV), pages 366382, 2018. [9] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In CVPR, 2017. [10] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei Efros. Everybody dance now. In Proceedings of the IEEE/CVF international conference on computer vision, pages 59335942, 2019. [11] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. survey on evaluation of large language models. ACM transactions on intelligent systems and technology, 15(3):145, 2024. [12] Junhao Chen, Xiang Li, Xiaojun Ye, Chao Li, Zhaoxin Fan, and Hao Zhao. Idea23d: Collaborative lmm agents enable 3d model generation from interleaved multimodal inputs. In Proceedings of the 31st International Conference on Computational Linguistics, pages 41494166, 2025. [13] Mingjin Chen, Junhao Chen, Huan-ang Gao, Xiaoxue Chen, Zhaoxin Fan, and Hao Zhao. Ultraman: Ultra-fast and high-resolution texture generation for 3d human reconstruction from single image. 2025. [14] Qihua Chen, Yue Ma, Hongfa Wang, Junkun Yuan, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, and Wei Liu. Follow-your-canvas: Higher-resolution video outpainting with extensive content generation. arXiv preprint arXiv:2409.01055, 2024. [15] Yang Chen, Yingwei Pan, Yehao Li, Ting Yao, and Tao Mei. Control3d: Towards controllable text-to-3d generation. In Proceedings of the 31st ACM International Conference on Multimedia, pages 11481156, 2023. [16] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and benchmarks for real-world 3d object understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2112621136, 2022. [17] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2019. [18] Yichun Shi Di Chang, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Xiao Yang, and Mohammad Soleymani. Magicdance: Realistic human dance video generation with motions & facial expressions transfer. arXiv preprint arXiv:2311.12052, 2(3):4, 2023. [19] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture similarity. IEEE transactions on pattern analysis and machine intelligence, 44(5):25672581, 2020. [20] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. survey of embodied ai: From simulators to research tasks. IEEE Transactions on Emerging Topics in Computational Intelligence, 6(2):230244, 2022. [21] Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui, Xiefan Guo, Xianhui Lin, Haolan Xue, Chen Shi, Xiaowen Li, et al. Dreamoving: human video generation framework based on diffusion models. arXiv preprint arXiv:2312.05107, 2023. [22] Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Vlad Olaru, and Cristian Sminchisescu. Reconstructing three-dimensional models of interacting humans. arXiv preprint arXiv:2308.01854, 2023. [23] Hongcheng Guo, Wei Zhang, Junhao Chen, Yaonan Gu, Jian Yang, Junjia Du, Binyuan Hui, Tianyu Liu, Jianxin Ma, Chang Zhou, and Zhoujun Li. Iw-bench: Evaluating large multimodal models for converting image-to-web. 2024. [24] Haonan Han, Rui Yang, Huan Liao, Jiankai Xing, Zunnan Xu, Xiaoming Yu, Junwei Zha, Xiu Li, and Wanhua Li. Reparo: Compositional 3d assets generation with differentiable 3d layout alignment. arXiv preprint arXiv:2405.18525, 2024. 11 [25] Zekun Hao, Xun Huang, and Serge Belongie. Controllable video generation with sparse trajectories. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 78547863, 2018. [26] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 75147528, 2021. [27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [29] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. [30] Yaosi Hu, Chong Luo, and Zhenzhong Chen. Make it move: controllable image-to-video generation with text descriptions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1821918228, 2022. [31] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [32] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1275312762, 2021. [33] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Yolo by ultralytics. https://github.com/ ultralytics/ultralytics, 2023. [34] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2262322633. IEEE, 2023. [35] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. [36] Rawal Khirodkar, Jyun-Ting Song, Jinkun Cao, Zhengyi Luo, and Kris Kitani. Harmony4d: video dataset for in-the-wild close human interactions. Advances in Neural Information Processing Systems, 37:107270107285, 2024. [37] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [38] Jonas Krumme and Christoph Zetzsche. World knowledge from ai image generation for robot control. arXiv preprint arXiv:2503.16579, 2025. [39] Zhengfei Kuang, Shengqu Cai, Hao He, Yinghao Xu, Hongsheng Li, Leonidas Guibas, and Gordon Wetzstein. Collaborative video diffusion: Consistent multi-video generation with camera control. Advances in Neural Information Processing Systems, 37:1624016271, 2024. [40] Jehee Lee, Jinxiang Chai, Paul SA Reitsma, Jessica Hodgins, and Nancy Pollard. Interactive control of avatars animated with human motion data. In Proceedings of the 29th annual conference on Computer graphics and interactive techniques, pages 491500, 2002. [41] Hongxiang Li, Yaowei Li, Yuhang Yang, Junjie Cao, Zhihong Zhu, Xuxin Cheng, and Long Chen. Dispose: Disentangling pose guidance for controllable human image animation. In The Thirteenth International Conference on Learning Representations, 2025. [42] Quanhao Li, Zhen Xing, Rui Wang, Hui Zhang, Qi Dai, and Zuxuan Wu. Magicmotion: Controllable video generation with dense-to-sparse trajectory guidance. arXiv preprint arXiv:2503.16421, 2025. [43] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1340113412, 2021. [44] Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, and Xiu Li. Lodge: coarse to fine diffusion network for long dance generation guided by the characteristic dance primitives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15241534, 2024. [45] Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su, Zeping Ren, Han Zhang, Yansong Tang, and Xiu Li. Finedance: fine-grained choreography dataset for 3d full body dance generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1023410243, 2023. [46] Mingxiang Liao, Qixiang Ye, Wangmeng Zuo, Fang Wan, Tianyu Wang, Yuzhong Zhao, Jingdong Wang, Xinyu Zhang, et al. Evaluation of text-to-video generation models: dynamics perspective. Advances in Neural Information Processing Systems, 37:109790109816, 2024. [47] Bin Lin, Yunyang Ge, Xinhua Cheng, Zongjian Li, Bin Zhu, Shaodong Wang, Xianyi He, Yang Ye, Shenghai Yuan, Liuhan Chen, et al. Open-sora plan: Open-source large video generation model. arXiv 12 preprint arXiv:2412.00131, 2024. [48] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision (ECCV), pages 740755, 2014. [49] Yukang Lin, Ronghui Li, Kedi Lyu, Yachao Zhang, and Xiu Li. Rich: Robust implicit clothed humans reconstruction from multi-scale spatial cues. In Chinese Conference on Pattern Recognition and Computer Vision (PRCV), pages 193206. Springer, 2023. [50] Jiahe Liu, Youran Qu, Qi Yan, Xiaohui Zeng, Lele Wang, and Renjie Liao. Frechet video motion distance: metric for evaluating motion consistency in videos. arXiv preprint arXiv:2407.16124, 2024. [51] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2213922149, 2024. [52] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. Fetv: benchmark for fine-grained evaluation of open-domain text-to-video generation. Advances in Neural Information Processing Systems, 36:6235262387, 2023. [53] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Laura Leal-Taixé, Daniel Cremers, Ian Reid, Stefan Roth, Simone Milani, Alexander Kirillov, and Paul Voigtlaender. Hota: higher order metric for evaluating multi-object tracking. International Journal of Computer Vision, 129(2):548578, 2021. [54] Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, and Yongming Zhu. Dreamactor-m1: Holistic, expressive and robust human image animation with hybrid guidance. arXiv preprint arXiv:2504.01724, 2025. [55] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tieniu Tan. Notice of removal: Videofusion: Decomposed diffusion models for high-quality video generation. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1020910218. IEEE, 2023. [56] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Pose-guided text-to-video generation using pose-free videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 41174125, 2024. [57] Neelu Madan, Andreas Møgelmose, Rajat Modi, Yogesh Rawat, and Thomas Moeslund. Foundation models for video understanding: survey. Authorea Preprints, 2024. [58] Vongani Maluleke, Lea Müller, Jathushan Rajasegaran, Georgios Pavlakos, Shiry Ginosar, Angjoo Kanazawa, and Jitendra Malik. Synergy and synchrony in couple dances. arXiv preprint arXiv:2409.04440, 2024. [59] Thomas Melistas, Nikos Spyrou, Nefeli Gkouti, Pedro Sanchez, Athanasios Vlontzos, Yannis Panagakis, Giorgos Papanastasiou, and Sotirios Tsaftaris. Benchmarking counterfactual image generation. Advances in Neural Information Processing Systems, 37:133207133230, 2024. [60] Anush Moorthy and Alan Bovik. Efficient motion weighted spatio-temporal video ssim index. In Human Vision and Electronic Imaging XV, volume 7527, pages 440448. SPIE, 2010. [61] OpenAI. Sora: Creating video from text. https://openai.com/sora, 2024. Accessed: 2024-06-01. [62] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. [63] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, 2024. [64] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [65] Yiran Qin, Zhelun Shi, Jiwen Yu, Xijun Wang, Enshen Zhou, Lijun Li, Zhenfei Yin, Xihui Liu, Lu Sheng, Jing Shao, et al. Worldsimbench: Towards video generation models as world simulators. arXiv preprint arXiv:2410.18072, 2024. [66] Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and data set for multi-target, multi-camera tracking. In European Conference on Computer Vision (ECCV), pages 1735, 2016. [67] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [68] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-tovideo generation with explicit motion modeling. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [69] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [70] Kunpeng Song, Tingbo Hou, Zecheng He, Haoyu Ma, Jialiang Wang, Animesh Sinha, Sam Tsai, Yaqiao Luo, Xiaoliang Dai, Li Chen, et al. Directorllm for human-centric video generation. arXiv preprint arXiv:2412.14484, 2024. [71] Tomás Soucek and Jakub Lokoc. Transnet v2: An effective deep network architecture for fast shot transition detection. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 1121811221, 2024. [72] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. arXiv preprint arXiv:2407.14505, 2024. [73] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. Advances in neural information processing systems, 36:4965949678, 2023. [74] Mingze Sun, Junhao Chen, Junting Dong, Yurun Chen, Xinyu Jiang, Shiwei Mao, Puhua Jiang, Jingbo Wang, Bo Dai, and Ruqi Huang. Drive: Diffusion-based rigging empowers generation of versatile and expressive characters. arXiv preprint arXiv:2411.17423, 2024. [75] Mingze Sun, Chao Xu, Xinyu Jiang, Yang Liu, Baigui Sun, and Ruqi Huang. Beyond talkinggenerating International Journal of Computer Vision, holistic 3d human dyadic motion for communication. 133(5):29102926, 2025. [76] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with large language models: survey. IEEE Transactions on Circuits and Systems for Video Technology, 2025. [77] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit Bermano. Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022. [78] Linrui Tian, Siqi Hu, Qi Wang, Bang Zhang, and Liefeng Bo. Emo2: End-effector guided audio-driven avatar video generation. arXiv preprint arXiv:2501.10687, 2025. [79] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: High-quality identity-preserving human image animation. arXiv preprint arXiv:2411.17697, 2024. [80] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. [81] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan Popovic. Articulated mesh animation from multi-view silhouettes. In Acm Siggraph 2008 papers, pages 19. 2008. [82] Boyang Wang, Nikhil Sridhar, Chao Feng, Mark Van der Merwe, Adam Fishman, Nima Fazeli, and Jeong Joon Park. This&that: Language-gesture controlled video generation for robot planning. arXiv preprint arXiv:2407.05530, 2024. [83] Jiangshan Wang, Yue Ma, Jiayi Guo, Yicheng Xiao, Gao Huang, and Xiu Li. Cove: Unleashing the diffusion feature correspondence for consistent video editing. arXiv preprint arXiv:2406.08850, 2024. [84] Jiangshan Wang, Junfu Pu, Zhongang Qi, Jiayi Guo, Yue Ma, Nisha Huang, Yuxin Chen, Xiu Li, and Ying Shan. Taming rectified flow for inversion and editing. arXiv preprint arXiv:2411.04746, 2024. [85] Qilin Wang, Zhengkai Jiang, Chengming Xu, Jiangning Zhang, Yabiao Wang, Xinyi Zhang, Yun Cao, Weijian Cao, Chengjie Wang, and Yanwei Fu. Vividpose: Advancing stable video diffusion for realistic human image animation. arXiv preprint arXiv:2405.18156, 2024. [86] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 93269336, June 2024. [87] Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, and Nong Sang. Unianimate: Taming unified video diffusion models for consistent human image animation. Science China Information Sciences, 2025. [88] Xiang Wang, Shiwei Zhang, Longxiang Tang, Yingya Zhang, Changxin Gao, Yuehuan Wang, and Nong Sang. Unianimate-dit: Human image animation with large-scale video diffusion transformer. arXiv preprint arXiv:2504.11289, 2025. [89] Yuchi Wang, Junliang Guo, Jianhong Bai, Runyi Yu, Tianyu He, Xu Tan, Xu Sun, and Jiang Bian. Instructavatar: Text-guided emotion and motion control for avatar generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 81328140, 2025. [90] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. [91] Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Youqing Fang, Yuwei Guo, Wenran Liu, Jing Tan, Kai Chen, Tianfan Xue, Bo Dai, et al. Humanvid: Demystifying training data for camera-controllable human image animation. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [92] Zhenzhi Wang, Yixuan Li, Yanhong Zeng, Yuwei Guo, Dahua Lin, Tianfan Xue, and Bo Dai. Multiidentity human image animation with structural video diffusion. arXiv preprint arXiv:2504.04126, 2025. [93] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. arXiv preprint arXiv:2312.03641, 2023. [94] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024. [95] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Jiashi Feng, and Mike Zheng Shou. Xagen: 3d expressive human avatars generation. Advances in Neural Information Processing Systems, 36, 2024. 14 [96] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [97] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. [98] Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, et al. Human motion video generation: survey. Authorea Preprints, 2024. [99] Jingyun Xue, Hongfa Wang, Qi Tian, Yue Ma, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, et al. Follow-your-pose v2: Multiple-condition guided character image animation for stable pose control. arXiv preprint arXiv:2406.03035, 2024. [100] Peng Yan, Xuanqin Mou, and Wufeng Xue. Video quality assessment via gradient magnitude similarity deviation of spatial and spatiotemporal slices. In Mobile Devices and Multimedia: Enabling Technologies, Algorithms, and Applications 2015, volume 9411, pages 182191. SPIE, 2015. [101] Yichao Yan, Zanwei Zhou, Zi Wang, Jingnan Gao, and Xiaokang Yang. Dialoguenerf: Towards realistic avatar face-to-face conversation video generation. Visual Intelligence, 2(1):24, 2024. [102] Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, Jianping Shi, and Dahua Lin. Pose guided human video generation. In Proceedings of the European conference on computer vision (ECCV), pages 201216, 2018. [103] Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, and Jenq-Neng Hwang. Samurai: Adapting segment anything model for zero-shot visual tracking with motion-aware memory. arXiv preprint arXiv:2411.11922, 2024. [104] Peiqing Yang, Shangchen Zhou, Jixin Zhao, Qingyi Tao, and Chen Change Loy. MatAnyone: Stable video matting with consistent memory propagation. In CVPR, 2025. [105] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli VanderBilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay Krishna, Lingjie Liu, et al. Holodeck: Language guided generation of 3d embodied ai environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1622716237, 2024. [106] Zhengyuan Yang, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. Idea2img: Iterative self-refinement with gpt-4v for automatic image design and generation. In European Conference on Computer Vision, pages 167184. Springer, 2024. [107] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with twostages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023. [108] Xiaojun Ye, Junhao Chen, Xiang Li, Haidong Xin, Chao Li, Sheng Zhou, and Jiajun Bu. Mmad: Multi-modal movie audio description. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 11415 11428, 2024. [109] Wanqi Yin, Zhongang Cai, Ruisi Wang, Fanzhou Wang, Chen Wei, Haiyi Mei, Weiye Xiao, Zhitao Yang, Qingping Sun, Atsushi Yamashita, Lei Yang, and Ziwei Liu. Whac: World-grounded humans and cameras. In European Conference on Computer Vision, pages 2037. Springer, 2024. [110] Yifei Yin, Chen Guo, Manuel Kaufmann, Juan Jose Zarate, Jie Song, and Otmar Hilliges. Hi4d: 4d instance segmentation of close human interaction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1701617027, 2023. [111] Jae Shin Yoon, Lingjie Liu, Vladislav Golyanik, Kripasindhu Sarkar, Hyun Soo Park, and Christian Theobalt. Pose-guided human animation from single image in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1503915048, 2021. [112] Beiyuan Zhang, Yue Ma, Chunlei Fu, Xinyang Song, Zhenan Sun, and Ziqiang Li. Follow-your-multipose: Tuning-free multi-character text-to-video generation via pose guidance. arXiv preprint arXiv:2412.16495, 2024. [113] Baoli Zhang, Haining Xie, Pengfan Du, Junhao Chen, Pengfei Cao, Yubo Chen, Shengping Liu, Kang Liu, and Jun Zhao. Zhujiu: multi-dimensional, multi-faceted chinese benchmark for large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 479494, 2023. [114] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [115] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024. [116] Hongxiang Zhao, Xingchen Liu, Mutian Xu, Yiming Hao, Weikai Chen, and Xiaoguang Han. Tasterob: Advancing video generation of task-oriented hand-object interaction for generalizable robotic manipulation. arXiv preprint arXiv:2503.11423, 2025. 15 [117] Kaiyang Zhou and Tao Xiang. Torchreid: library for deep learning person re-identification in pytorch. arXiv preprint arXiv:1910.10093, 2019. [118] Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and Tao Xiang. Omni-scale feature learning for person re-identification. In ICCV, 2019. [119] Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, and Tao Xiang. Learning generalisable omni-scale representations for person re-identification. TPAMI, 2021. [120] Mingjian Zhu, Hanting Chen, Qiangyu Yan, Xudong Huang, Guanyu Lin, Wei Li, Zhijun Tu, Hailin Hu, Jie Hu, and Yunhe Wang. Genimage: million-scale benchmark for detecting ai-generated image. Advances in Neural Information Processing Systems, 36:7777177782, 2023. [121] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, pages 145162. Springer, 2024. [122] Tianyi Zhu, Dongwei Ren, Qilong Wang, Xiaohe Wu, and Wangmeng Zuo. Generative inbetweening through frame-wise conditions-driven video generation. arXiv preprint arXiv:2412.11755, 2024."
        },
        {
            "title": "A Limitations",
            "content": "While DanceTogether achieves state-of-the-art performance on two-person interaction benchmarks, it has several limitations. First, our framework is optimized for up to two actors; extending it to handle larger groups would incur substantial computational and memory overhead and may require hierarchical or factorized conditioning mechanisms. Second, the quality of generated videos depends heavily on the accuracy of the input pose and mask sequencessevere occlusions, fast motion blur, or failures in the underlying detectors (e.g., DWPose [107], SAMURAI [103]) can degrade identity preservation and interaction fidelity. Third, we assume mostly static camera and relatively simple backgrounds; dynamic camera motion or highly cluttered scenes may introduce artifacts or identity confusion. Fourth, like most diffusion-based methods, DanceTogether is computationally intensive and incurs non-trivial latency, limiting real-time applications."
        },
        {
            "title": "B Broader impacts",
            "content": "DanceTogether opens new possibilities for creative content production, digital avatar animation, and embodied-AI simulation by enabling controllable, identity-preserving multi-person video generation. It can accelerate workflows in film, game, and VR/AR industries, and provide high-fidelity training data for human-robot interaction research. However, the ability to generate realistic multi-person videos also raises potential misuse riskssuch as deepfake creation, identity impersonation, and privacy infringements. Our large-scale datasets (PairFS-4K, HumanRob-300) may inadvertently encode demographic biases; we therefore recommend careful curation and bias analysis before deployment. To mitigate misuse, we plan to release public checkpoints with visible watermarks and to accompany the code and models with clear ethical guidelines and usage licenses. We believe that, with appropriate safeguards, DanceTogether can serve as responsible tool for advancing both research and creative industries."
        },
        {
            "title": "C Ablation Study",
            "content": "C.1 Dataset ablation study Ablation study on the datasets have been compared in the main text in Tabs. 2, 3, 4, and 5. StableAnimator [79] fine-tuned for 40 epochs on the swing dance dataset [58] (StableAnimator w. Dataswing) shows significant improvement over the original pre-trained weights provided by the authors, but still performs noticeably worse than DanceTog trained for 20 epochs on the same Swing dance dataset (DanceTog w. Dataswing). Using all training data except PairFS-4K (DanceTog w. Dataf ull) clearly performs better than the model trained only on the swing dance dataset (DanceTog w. Dataswing), but still underperforms compared to DanceTog trained on all data including PairFS4K (denoted as DanceTog w. Dataf ull + DataP airF S). C.2 Ablation study on sub-modules and inputs Tab. 6 provides ablation results for the new model and multi-input approaches proposed in DanceTog. Where, w/o mask input means not using separate mask input during the input process. w/o pose 16 input means not using separate pose input during the input process. w/o MaskPoseAdapter means using the original PoseNet, i.e., using the poses of all people as condition inputs to the model. w/o MultiFaceEncoder means using the original FaceEncoder, i.e., using the embedding of the largest face detected in the reference image as condition input to the model. Model Variant Track 1: IdentityConsistency Track 2: InteractionCoherence Track 3: Video Quality HOTA MOTA IDF1 MPJPE2D OKS PoseSSIM FVMD105 PSNR FVD FID C-FID w/o mask input w/o pose input w/o MaskPoseAdapter w/o MultiFaceEncoder 33.63 81.48 48.95 83.31 15.48 74.23 40.93 78.81 DanceTog 83.94 79. 42.49 86.38 62.02 88.55 89.59 1625.04 1292.33 1692.55 893.32 492.24 0.28 0.46 0.48 0.74 0. 0.85 0.85 0.79 0.89 0.93 2.97 4.91 3.80 1.26 0.54 11.02 14.98 11.19 15.67 40.4 19.7 41.3 17. 73.1 58.1 72.0 49.2 15.82 17.1 48.0 14.7 9.4 14.2 8.4 7. Table 6: Module ablation study. Fig. 5 and Fig. 6 present qualitative comparisons of our ablation studies. DanceTogether is compatible with StableAnimators Inference with HJB-based Face Optimization [79]. Since our task and test samples focus on full-body two-person interaction video generation rather than large-area facemask talking heads or single-person half-body dance sequences, the benefit of HJB-based Face Optimization is less pronounced. In our tests, inference without HJB-based Face Optimization runs at approximately 0.8 s/iteration, whereas enabling HJB-based Face Optimization reduces throughput to about 15 s/iteration. Furthermore, our ablation study indicates that applying HJB-based Face Optimization does not significantly impact the quality of two-person interaction video generation. Consequently, all experiments reported in the main text for StableAnimator and DanceTog were performed without HJB-based Face Optimization. Figure 5: Ablation study animation results (1/2). C.3 Comparison between PoseNet and MaskPoseAdapter Fig. 7 shows the feature maps obtained by the original PoseNet and our proposed MaskPoseAdapter from consecutive frames with the same input. It can be clearly observed that the output of MaskPoseAdapter strongly binds pose and mask information, enabling clear identification of which ID each pose corresponds to, and still providing sufficient mask information even when input poses Figure 6: Ablation study animation results (2/2). are missing in some occluded frames. In contrast, the original PoseNets output makes it difficult to distinguish each individual pose, and pose features may be lost in occluded frames. C.4 Experiments on residual alpha and mask processor Fig. 8 and Fig. 9 illustrate the influence of various Light mask processors and the parameter αres on the feature maps generated by MaskPoseAdapter. Through extensive experimentation, we determined the optimal number of output channels for the Light mask processor and the value of α that effectively balances the pose and mask features in the output feature maps of MaskPoseAdapter. In practice, when training on the full dataset, we set αres = 0.5 and employ Light mask processor with 3-channel output."
        },
        {
            "title": "D Data Curation Pipeline",
            "content": "Due to the limitations of existing two-person interaction datasets [58, 75], which fail to simultaneously provide identity diversity, static backgrounds, and fixed camera positions, we propose novel data processing pipeline that recovers tracked human pose estimations from monocular RGB videos. Our pipeline extracts independent pose sequences, human silhouette masks, and facial masks for distinct individuals. We collected over 170 hours of paired figure skating videos from the internet and curated more than 26 hours of high-quality two-person figure skating segments, providing tracking masks, pose estimations, and facial masks for each individual subject ID. Additionally, we compiled 1-hour humanoid robot dataset for fine-tuning our model to support controllable video generation tasks involving humanoid robots. D.1 Dataset Collection We collected various single-person motion videos from existing research to enrich identity information, including TikTokDataset [32], Champ [121], DisPose [41] and HumanVid [91]. Additionally, we gathered two-person interaction videos from existing research, including partner dancing, dual talking heads, and laboratory-recorded interactions from Swing Dance [58], Harmony4D [36], 18 Figure 7: Comparison of PoseNet and MaskPoseAdapter outputs under identical frame inputs. Figure 8: The effect of residual alpha on MaskPoseAdapter output. Figure 9: MaskPoseAdapter output. The effect of different output channel numbers in the Light mask processor on 20 HI4D [110], CHI3D [22], and Beyond Talking [75]. While synthetic data has been used for video generation training in prior work [109, 91], our method focuses on controllable human interaction video generation in real-world scenarios, so we did not use any synthetic data during training. D.2 Human Tracking and Subject Selection We first segment raw videos into scenes using TransNetV2 [71] and detect humans using YOLOv8x [33]. For each person crop pt using pre-trained OSNet [117, 118, 119]. Our enhanced tracking algorithm combines spatial proximity with ReID similarity to maintain consistent identities across frames. From all tracked identities, we select the two main subjects based on coverage (appearance frequency 40%), consistency, and quality score Qi = 0.7 Coveragei + 0.3 Consistencyi. i, we extract 512-dimensional identity features D.3 Annotation Generation Starting from the bounding boxes of key frames, SAMURAI [103] bidirectionally propagates masks throughout the video sequence. We extract pose information of 133 keypoints using DWPose [107] and assign each pose to independent subject IDs via an IOU matching approach utilizing the masks generated by SAMURAI. MatAnyone [104] produces high-quality alpha mattes from SAMURAI masks, providing data for tasks requiring background replacement [75]. The complete data processing pipeline is illustrated in Fig. 3, where our Data Curation Pipeline generates four outputs from RGB video input: independent mask sequences, pose sequences, and facial mask sequences for each individual, as well as alpha mask sequences for all subjects. D.4 Data Filtering Clips are automatically filtered based on: bbox overlap (max IoU < 0.1), size validation (2% < bbox area < 80% of frame), exact 2 primary subjects with 40% coverage, and temporal consistency (> 90% successful tracking). For PairFS-4K, we additionally perform manual curation to ensure high-quality two-person interactions with clear visibility and balanced representation of skating movements. PairFS-4K Dataset Preparation Process We collected 932 figure skating videos from the internet, including numerous Olympic figure skating compilation videos with multiple shots. Using TransNetV2 [71], we developed an automatic segmentation script and employed HumanReID and Yolox for identification and tracking of the main subjects. After manually filtering out segments that did not conform to single-person or pair figure skating criteria, we obtained 4.8K figure skating segments with total duration of approximately 26 hours, and an average segment length of about 20 seconds. We train our model on TikTokDataset [32], Champ [121], DisPose [41], HumanVid [91], Swing Dance [58], Harmony4D [36], CHI3D [22], Beyond Talking [75], and PairFS-4K, using resolutions of 512 512. Due to the limited number of unique identities in HI4D, we exclude it from our training set. detailed summary of all datasets is provided in Table 1. PairFS-4K is the first two-person figure skating video dataset with over 7,000 unique identities."
        },
        {
            "title": "F TogetherVideoBench Benchmark",
            "content": "F.1 Video Generation Benchmark Overview There have been many benchmarks for evaluating large generative models [52, 113, 72, 51, 16, 73, 120, 59, 31, 23, 11]. Recently, some video understanding methods have also been used to evaluate the quality of generated videos [76, 46, 51, 108, 57]. Despite this, the field of controllable video generation has lacked reliable evaluation benchmark. Recent controllable video benchmarks (AIST++ [43], TikTok-Eval [32]) have mainly focused on single-person dance or static portrait animations, overlooking the three key challenges faced by realistic multi-person generation: multiidentity consistency (avoiding identity confusion in long sequences), interaction coherence (ensuring physically reasonable and temporally smooth interactions), and strict conditional fidelity (precisely 21 following pose, mask, or text control inputs). To systematically evaluate these dimensions, we propose TogetherVideoBench, featuring three orthogonal tracksIdentity-Consistency, InteractionCoherence, and Video Qualitysupported by unified, automated parsing pipeline that extracts perperson pose, mask, face-crop, and bounding-box representations for fair and reproducible assessment. Identity-Consistency: To evaluate the ability of models to maintain consistent appearance and identity for each individual across long video sequences, we adopt standard multi-object tracking metrics, including HOTA [53], MOTA [3], and IDF1 [66]. These metrics comprehensively assess detection accuracy, association accuracy, and identity preservation, and are computed using the TrackEval toolkit [53]. This track is crucial for ensuring that generated videos do not suffer from identity switches or appearance confusion, especially in multi-person scenarios. Interaction-Coherence: This track focuses on the temporal smoothness and physical plausibility of interactions between multiple humans, as well as the adherence to external control signals. We employ pose adherence (MPJPE-2D) [9], object keypoint similarity (OKS) [48], and the following metrics: pose structure similarity (PoseSSIM), motion smoothness (SmoothRMS), temporal dynamics error (TimeDynRMSE), and Fréchet Video Motion Distance (FVMD) [50] to comprehensively evaluate the quality of human motion and interaction. Video Quality: To assess the overall visual fidelity and semantic consistency of generated videos, we use suite of widely adopted metrics, including SSIM [90], FVD [80], FID [27], CLIP [26], and the following metrics: LPIPS, L1, PSNR, DISTS [19], ST-SSIM [60], GMSD-T [100]. These metrics collectively measure both the perceptual quality and the alignment of generated content with the intended conditions. We calculate the metrics for both the overall frame and the human mask region of each frame separately, as shown in Fig. 10. Since the backgrounds of some evaluation data exhibit slight jittering, we believe that the quantitative evaluation of the human mask region is more indicative of human ID consistency and video quality in the generated videos than the quantitative evaluation of the full frame. All tracks share unified Data Curation Pipeline that automatically extracts per-person pose, mask, face-crop, and bounding box for both ground truth and generated videos, ensuring reproducibility and fair comparison. For each video, we compute the relevant metrics for every individual and report the average across all videos in each group. F.2 Evaluation Dataset While laboratory-recorded datasets such as Harmony4D [36], HI4D [110], and CHI3D [22] provide precise annotations, their videos are typically limited to 312 seconds, feature single scenes, and involve minimal position exchanges between subjects. As result, they are insufficient for evaluating long-duration, multi-position, and realistic human interactions. To address this gap, we have manually curated and edited 100 high-quality two-person interaction videos from public competitions, films, documentaries, and social media, forming the core evaluation set of TogetherVideoBench. These videos encompass wide range of real-world interaction patterns, including exchange-intensive swing and Lindy-Hop routines, Latin ballroom duets, pair figure skating, boxing, wrestling and combat sequences, partner acrobatics and acro-yoga throws, everyday social gestures (such as handshakes and hugs), and two-person conversations. Each clip features exactly two performers, with nearly static cameras and backgrounds. Frequent occlusions, position exchanges, and physical contact between subjects introduce long-range motion, viewpoint changes, and identity-switching challengesfactors, making it suitable testbed. F.3 Metrics Below are the evaluation metrics and computation procedures used in the three tracks of TogetherVideoBench. To ensure reproducibility, both ground-truth and generated videos are first processed by our Data Curation Pipeline (Sec. D), which yields for each subject: Pose sequences: 133 keypoints per frame via DWPose [107]. Human masks: per-frame human masks via SAMURAI [103]. Bounding boxes: tight boxes around each human mask (for MOT eval [53]). Track 1 Identity-Consistency 22 Figure 10: We use individual human masks for each person to conduct quantitative evaluation. The error map shown in the figure is the L1 Loss error map, which calculates the pixel-level absolute difference between the GT and predicted images. IDF1 : After framelevel association with the Hungarian algorithm, let IDTP, IDFP and IDFN be identitytrue positives, false positives and false negatives. IDF1 = 2 IDTP 2 IDTP + IDFP + IDFN . (21) It is the harmonic mean of identity precision and recall and therefore measures how often the correct ID label is maintained. IDP / IDR : Precision and recall components of IDF1. IDP = IDTP IDTP + IDFP , IDR = IDTP IDTP + IDFN . (22) HOTA : Higher-Order Tracking Accuracy [53] decomposes into DetA (detection accuracy), AssA (association accuracy) and LocA (localisation accuracy): HOTA = DetA AssA, LocA = 1 1 TP (cid:88) (cid:0)1 IoU(b)(cid:1). (23) bTP 23 MOTA / MOTP : CLEAR-MOT summary: MOTA = 1 FP + FN + IDSW GT dets , MOTP = 1 (cid:80) TP (cid:0)1 IoU(cid:1) TP . (24) IDSW , FP , FN : Absolute counts of identity switches, false positives and false negatives. Track 2 Interaction-Coherence All keypoints are first temporally aligned and isotropically scaleshift aligned via similarity transform. MPJPE-2D : Let ˆxtpj and xtpj be the predicted and ground-truth pixel coordinates of joint of person at frame t, after SIM3 alignment; T, P, denote total frames, persons, and joints. Then MPJPE-2D ="
        },
        {
            "title": "1\nT P J",
            "content": "T (cid:88) (cid:88) (cid:88) (cid:13) (cid:13)ˆxtpj xtpj (cid:13) (cid:13)2. t= p=1 j=1 (25) OKS : For each frame t, flatten over valid keypoints. Let dk be the Euclidean error of the kth keypoint, σk its COCO standard deviation, and the estimated person area. Then OKSt = 1 (cid:88) k=1 (cid:16) exp d2 k(A + 106) 2 σ2 (cid:17) , OKS = 1 (cid:88) t=1 OKSt. (26) Pose-Heat SSIM : Rasterise the set of keypoints at each frame into Gaussian heatmap H() of size with σ = 4 px, then PoseHeatSSIM = 1 (cid:88) t=1 SSIM(cid:0)H( ˆXt), H(Xt)(cid:1), (27) where ˆXt, Xt RP J2 are the keypoint arrays. SmoothRMS : Compute the third-order temporal derivative (jerk) of each trajectory, scaled by frame rate : Then ... tpj = d3 dt3 xtpj 3. SmoothRMS = (cid:118) (cid:117) (cid:117) (cid:116) 1 T (cid:88) (cid:88) (cid:88) (cid:13) (cid:13) ... tpj (cid:13) 2 2. (cid:13) t=1 p=1 j=1 Time-Dyn RMSE : With the second-order derivative (acceleration) xtpj = d2 dt2 xtpj 2, TimeDynRMSE = (cid:118) (cid:117) (cid:117) (cid:116) 1 J (cid:88) (cid:88) (cid:88) t=1 p=1 j= (cid:13) (cid:13)xtpj (cid:13) 2 2. (cid:13) define FVMD : (28) (29) (30) (31) Model the velocity vectors of all keypoints as 2D Gaussians (µp, Σp) for prediction and (µg, Σg) for ground truth, where µ = E[ x] and Σ = Cov[ x]. Then FVMD = (cid:13) (cid:13)µp µg (cid:13) 2 + Tr(cid:0)Σp + Σg 2 (Σp Σg) 2 (cid:13) 1 2 (cid:1). (32) 24 Track 3 Video Quality L1 : Let It(x, y, c) and ˆIt(x, y, c) be the ground-truth and predicted RGB pixel values at frame t, spatial location (x, y) and channel c, over frames of size and = 3 channels. Then L1 ="
        },
        {
            "title": "1\nT H W C",
            "content": "T (cid:88) (cid:88) (cid:88) (cid:88) (cid:12)It(x, y, c) ˆIt(x, y, c)(cid:12) (cid:12) (cid:12). t=1 x= y=1 c=1 PSNR : Compute the per-frame mean squared error MSE ="
        },
        {
            "title": "1\nH W C",
            "content": "W (cid:88) (cid:88) (cid:88) x=1 y=1 c= (cid:0)It(x, y, c) ˆIt(x, y, c)(cid:1)2 , then SSIM : PSNR = 20 log10 (cid:16) 255 (cid:17) ."
        },
        {
            "title": "MSE",
            "content": "For each frame and each channel c, compute then average: LPIPS : SSIMc = SSIM(cid:0)It(, , c), ˆIt(, , c)(cid:1), SSIM = 1 (cid:88) (cid:88) t=1 c=1 SSIMc . On 256 256 crop, let ϕℓ() be the ℓ-th layer feature map and wℓ learned weights. Then LPIPS = 1 (cid:88) ℓ=1 1 HℓWℓ (cid:13) wℓ (cid:0)ϕℓ(I) ϕℓ( ˆI)(cid:1)(cid:13) (cid:13) (cid:13)1. (33) (34) (35) (36) (37) (38) DISTS : Let fℓ() be VGG16 feature maps, fℓ their normalized versions, and G() the Gram matrix. Define structureℓ = fℓ(I), fℓ( ˆI) fℓ(I) fℓ( ˆI) , textureℓ = MSE(cid:0)G(fℓ(I)), G(fℓ( ˆI))(cid:1). (39) Then CLIPScore : DISTS = 1 L (cid:88) (cid:16) ℓ=1 0.5 structureℓ + 0.5 (cid:0)1 textureℓ (cid:1)(cid:17) . (40) We encode each frame from the ground-truth and generated videos into CLIP image embeddings vt, ˆvt Rd, normalize them to unit vectors, and compute the frame-wise cosine similarity: The final CLIPScore is obtained by averaging over all frames: st = ˆvt vt ˆvt . CLIPScore = 1 (cid:88) t=1 st. ST-SSIM : With window length = 3, define for each spatio-temporal block SSIM3D = SSIM(cid:0)It:t+w1, ˆIt:t+w1 (cid:1), then ST-SSIM = 1 + 1 w+1 (cid:88) t=1 SSIM3D. 25 (41) (42) (43) (44) GMSD-Temporal : For each = 2, . . . , , let gt(x, y) = (cid:13) (cid:13)It(x, y)(cid:13) (cid:13)2, ˆgt(x, y) = (cid:13) (cid:13) ˆIt(x, y)(cid:13) (cid:13)2, GMSt(x, y) = 2 gt ˆgt + ε + ˆg2 g2 + ε . GMSD-Temporal = (cid:118) (cid:117) (cid:117) (cid:116) 1 (T 1) (cid:88) t=2 Varx,y (cid:0)GMSt(x, y)(cid:1). (45) (46) (47) and"
        },
        {
            "title": "Then",
            "content": "FVD : Extract I3D features for each non-overlapping 16-frame clip, compute means µr, µf and covariances Σr, Σf , then FVD = (cid:13) (cid:13)µr µf (cid:13) 2 + Tr(cid:0)Σr + Σf 2(ΣrΣf )1/2(cid:1). 2 (cid:13) FID : On all frames, extract Inception-V3 features, form (µr, Σr) and (µf , Σf ), and use FID = (cid:13) (cid:13)µr µf (cid:13) 2 + Tr(cid:0)Σr + Σf 2(ΣrΣf )1/2(cid:1). 2 (cid:13) CLIP-FID : Identical to FID but using CLIP embeddings instead of Inception features: CLIPFID = (cid:13) (cid:13)µr µf (cid:13) 2 + Tr(cid:0)Σr + Σf 2(ΣrΣf )1/2(cid:1). 2 (cid:13) (48) (49) (50) All Track-3 metrics are reported both on the full frame and on each human mask (per-person); the final masked score is the arithmetic mean over the two performers."
        },
        {
            "title": "G More Results",
            "content": "Fig. 11, Fig. 12, Fig. 13, and Fig. 14 present qualitative comparisons across consecutive frames for different cases. The top row in each figure shows the input reference image and the corresponding pose sequence. The pose sequence is estimated from ground truth video, and the first frame is used as the reference image input for each baseline. Our proposed DanceTog method consistently outperforms all baselines in generating video frames with rich interaction details. Notably, it preserves individual identity features even when the two subjects exchange positions. For qualitative video comparisons, please refer to the supplementary webpage. Figs. 1520 show qualitative comparisons of all baselines. We extracted consecutive frames where position swapping occurs. The leftmost column is the GT video. We used the first frame of the GT video as the reference image (not the first frame shown in the figures), and the dwpose results estimated from the GT video as the pose condition input for each baseline (corresponding to the GT images in the first column). Due to file size limitations, the images below are compressed. Please refer to the webpage in the supplementary materials for the original videos. Applications: HumanRobot Interaction Video Generation After fine-tuning on our HumanRob-300 humanoid-robot video dataset, DanceTogether can generate realistic interaction videos between humanoid robot and human (see Fig. 21). This demonstrates the effectiveness and generalization ability of DanceTogether, offering new insights for embodied-AI and humanrobot interaction research. After the robot and the human exchange positions, both agents retain their original identities. The method also handles fine-grained interactionssuch as handshakes and sparringremarkably well. This part of the video results can be found on the supplementary webpage. 26 Figure 11: Additional animation results (1/4). The image with red borders is the reference images. Figure 12: Additional animation results (2/4). The image with red borders is the reference images. 27 Figure 13: Additional animation results (3/4). The image with red borders is the reference images. Figure 14: Additional animation results (4/4). The image with red borders is the reference images. 28 Figure 15: Additional animation results (4/18). 29 Figure 16: Additional animation results (5/18). 30 Figure 17: Additional animation results (8/18). 31 Figure 18: Additional animation results (10/18). 32 Figure 19: Additional animation results (11/18). 33 Figure 20: Additional animation results (12/18). 34 Figure 21: Using the first frame as the reference image, we perform inference on humanrobot interaction sequences conditioned on independent pose maps and human masks."
        }
    ],
    "affiliations": [
        "Beijing Normal University",
        "Carnegie Mellon University",
        "Hong Kong Baptist University",
        "Nanjing University",
        "Peking University",
        "Shanghai AI Laboratory",
        "Tsinghua University",
        "University of Science & Technology of China"
    ]
}