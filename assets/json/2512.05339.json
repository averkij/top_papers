{
    "paper_title": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models",
    "authors": [
        "Mahesh Kumar Nandwana",
        "Youngwan Lim",
        "Joseph Liu",
        "Alex Yang",
        "Varun Notibala",
        "Nishchaie Khanna"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks."
        },
        {
            "title": "Start",
            "content": "Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models Mahesh Kumar Nandwana*, Youngwan Lim*, Joseph Liu*, Alex Yang, Varun Notibala, Nishchaie Khanna Roblox {mnandwana,ylim,josephliu,ayang,vnotibala,nkhanna}@roblox.com 5 2 0 2 5 ] . [ 1 9 3 3 5 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive inputoutput moderation, using pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses mix of synthetic and open-source safety datasets, augmented with chainof-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks. Introduction Large Language Models (LLMs) undergo extensive posttraining alignment procedures to enhance safety and ensure that their outputs adhere to human values and intent (Ouyang et al. 2022). Despite these efforts, LLMs remain vulnerable to producing inappropriate or risky content when prompted in adversarial or ambiguous ways (Ge et al. 2025; Zhang et al. 2025). This persistent challenge underscores the limitations of alignment alone in ensuring robust safety in diverse deployment contexts. As result, there is growing need to develop complementary safety mechanisms, such as guardrail models, that operate alongside LLMs to detect, filter, or prevent harmful behaviors in real-time. These auxiliary systems serve as critical components in layered approach to LLM safety, offering an additional line of defense to protect users and uphold responsible AI deployment standards. Current state-of-the-art guardrail systems primarily rely on fixed, static taxonomy of safety violations, which are *These authors contributed equally. Project Lead predetermined during the models training. This approach, while effective for controlled environments, fundamentally fails to account for the fluid nature of safety requirements. The interpretation of safe or violating output is not universal; it is highly dependent on contextual factors such as the user demographic, cultural norms, regional regulations, and the specific application domain. This contextual variability creates critical limitation. guardrail model designed for general audience may be overly restrictive for adults in private setting or dangerously permissive for youth-oriented platform. Consequently, existing guardrail systems face an inherent tradeoff: they are either too rigid to adapt to diverse deployment scenarios or so broad that they fail to provide adequate protection. This tension between static safety framework and dynamic real-world needs highlights crucial gap in current research. To address this, we argue for new paradigm: taxonomyadaptive guardrail models. Such system can dynamically infer and apply context-specific safety policy at inference time. Our work introduces method to generalize beyond single fixed taxonomy, allowing the guardrail to adapt its understanding of violations based on contextual signals. This approach enables more precise, scalable, and context-aware content moderation, moving us closer to truly responsible and flexible LLM deployment. critical component in building effective guardrail models is the underlying taxonomy and its associated risk guidelines. These taxonomies are not universalthey differ significantly across companies, products, and even specific use cases. There is no one-size-fits-all solution. As result, there is growing need for systems that can adapt to custom taxonomies and context-specific safety requirements. For instance, content categorized under dating may be acceptable in products targeting an 18+ audience, but would be inappropriate in applications designed for users under 18. This rigidity in existing systems poses challenges for deploying guardrails in nuanced or evolving environments. Without the ability to align with product-specific definitions of harm, these models risk both under-blocking inappropriate content and over-blocking benign content, leading to poor user experiences and potential safety gaps. Moreover, as taxonomies evolveeither due to shifting cultural norms, regulatory changes, or product requirementsmodels tied to fixed label spaces struggle adapt. To address these limitations, we propose framework that enables taxonomyadaptive modeling, allowing systems to flexibly incorporate custom taxonomies and adjust their behavior accordingly. Our approach enables safer, more context-aware AI deployment without requiring retraining from scratch for every new use case. The primary contributions of this work are: We introduce Roblox Guard 1.0 1, state-of-the-art safety guardrail model for LLMs, which achieves competitive, state-of-the-art-comparable performance over existing systems, setting new state-of-the-art on several key benchmarks like Toxic Chat and BeaverTails. The model demonstrates strong generalization to unseen content safety taxonomies, making it adaptable to evolving moderation needs. Roblox Guard 1.0 is trained entirely on large-scale corpus of over 384,000 open-source and synthetic data examples, enhancing reproducibility and transparency. We further incorporate chain-of-thought rationales during training, which provide rich contextual grounding that improves out-of-domain generalization and robustness without requiring extensive retraining for each new set of safety definitions. We release RobloxGuard-Eval 2, large-scale benchmark dataset for LLM safety evaluation, consisting of 2,872 examples across 23 safety categories. This dataset is publicly available and designed to facilitate standardized evaluation of guardrail models and address the saturation of existing safety benchmarks."
        },
        {
            "title": "Related Work",
            "content": "LLM Guardrail Models Early approaches to input and output moderation in LLMs relied on traditional content moderation systems such as OpenAIs Content Moderation API and Googles Perspective API. These systems are classifier-based and trained on fixed set of predefined safety taxonomy labels. While effective for general-purpose moderation, they are inherently limited in adaptability, struggling to support new or evolving safety categories. Moreover, these classifiers are not designed to process the long and complex contexts typical of LLM interactions. More recent work has focused on LLM-based guardrail models that can moderate inputs and outputs more flexibly (Inan et al. 2023; Zeng et al. 2024; Ghosh et al. 2024b). These models are typically instruction-tuned on fixed set of safety categories with explicit labeling instructions. While they are better equipped to handle long-form content and nuanced context, they still inherit limitations from their static taxonomies. Specifically, they require re-training or re-labeling to accommodate new moderation categories, which hinders scalability in fast-evolving domains such as platform-specific safety policies. 1https://github.com/Roblox/RobloxGuard-1.0 2https://huggingface.co/datasets/Roblox/RobloxGuard-Eval Separate from classifier-based models, distinct line of work focuses on programmable guardrail frameworks, such as NVIDIAs NeMo-Guardrails (Rebedea et al. 2023). These systems allow developers to define explicit conversational policies and topic boundaries (e.g., do not discuss medical topics). While this approach offers fine-grained conversational flow control, it relies on manually crafted rules and is not designed to learn and generalize to complex, nuanced content safety taxonomies. Our work builds on these approaches by introducing taxonomy-adaptive moderation model that generalizes to unseen categories, supports compositional taxonomies, and is trained entirely on open-source and synthetic data. LLM Safety Benchmark Datasets The development and evaluation of safety-focused guardrail models for LLMs frequently rely on proprietary or closed datasets (Inan et al. 2023; Zeng et al. 2024), which poses significant challenges for reproducibility, transparency, and comparability in academic research. Although there are few public datasets, such as BeaverTails (Ji et al. 2023) and SafeRLHF (Dai et al. 2024), they often suffer from critical limitations. Firstly, many are limited in scale and lack the breadth required to evaluate the complex behaviors of modern LLMs. Second, most focus exclusively on adversarial or harmful prompts, such as XSTest (Rottger et al. 2024) and HarmBench (Mazeika et al. 2024), without including the corresponding model responses, restricting their usefulness for end-to-end moderation tasks, particularly those involving response-level safety analysis. Furthermore, existing datasets often lack robust taxonomic structure, making it difficult to conduct fine-grained evaluations, gap that datasets such as Aegis (Ghosh et al. 2024b) and WildGuard (Han et al. 2024) have begun to address. Another key limitation is the narrow scope of inappropriate content categories. Many benchmarks emphasize wellknown harms such as toxicity or hate speech, while overlooking more nuanced or emerging areas like personal information leakage, deceptive content, donation solicitation, and misuse of platform-specific features. As LLM deployment expands across real-world applications, the need for comprehensive taxonomy-sensitive safety benchmarks becomes increasingly important. We also note that LLM benchmarks in general have begun to become saturated (Patwardhan et al. 2025), necessitating more comprehensive larger scale benchmarks that more adequately cover wider range of safety categories. To address these gaps, we introduce RobloxGuard Evala large-scale, taxonomy-rich evaluation dataset tailored for benchmarking LLM guardrail models. Our dataset includes fine-grained safety categories aligned with realworld content moderation policies and enables systematic evaluation of both prompt-level and response-level safety. Taxonomy for LLM Safety Roblox maintains one of the most comprehensive and finegrained safety taxonomies in the industry, reflecting the plat-"
        },
        {
            "title": "Category",
            "content": "Content Safety Taxonomy Categories (25 Total) Child Exploitation Threats, Bullying, and Harassment Discrimination, Slurs, and Hate Speech Real-World Sensitive Events Romantic and Sexual Content Profanity Religious Content Cheating and Scams Intellectual Property Violations Prohibited Advertising Practices and Content Soliciting Donations: Tipping Sharing Personal Information Misusing Roblox Systems: Jailbreaking Terrorism and Violent Extremism Suicide, Self Injury, and Harmful Behavior Harmful Off-Platform Speech or Behavior Violent Content and Gore Illegal and Regulated Goods and Activities Political Figures and Entities Expanded Policies for Suitability Spam Independent Advertisement Publishing Promotional Offers Paid Random Items Directing Users Off-Platform Table 1: The 25 Content Safety Categories covered in RobloxGuard Eval, reflecting Robloxs fine-grained safety taxonomy. forms uniquely diverse and global user base 3. With 25 distinct violation categories ranging from well-established harms like hate speech and discrimation to more nuanced concerns such as deceptive monetization practices and platform misuse - the taxonomy captures wide spectrum of safety challenges that arise in real-world, user-generated environments. Prior safety works have utilized portions of this taxonomy to enhance other safety models (Kumar Nandwana et al. 2024; Liu et al. 2024). This rich categorization enables more rigorous and multidimensional evaluation of LLM guardrails. Rather than being limited to handful of generic content risks, we are able to stress-test models across broad and evolving set of harm surfaces, many of which are underrepresented in existing benchmarks. By grounding our evaluation in this taxonomy, we move closer to measuring what actually matters in deployed safety systems: robustness, coverage, and adaptability across complex, context-dependent violation categories."
        },
        {
            "title": "Datasets",
            "content": "Multi-Stage Synthetic Data Generation Synthetic data forms major component of RobloxGuards training corpus. We leverage set of LLMs to generate both prompts and responses in structured pipeline composed of three key stages, which is also illustrated in Figure 1 and Figure 3: 1. Input Prompt Generator. Many prior works (Ghosh et al. 2024b) generate synthetic data by sourcing input prompts from existing datasets such as Anthropic HHRLHF (Bai et al. 2022), Do-Anything-Now (Shen et al. 2024), and Do-Not-Answer (Wang et al. 2023). However, we argue that this limits prompt diversityespecially when applied to Robloxs nuanced, platform-specific content safety taxonomy. 3https://en.help.roblox.com/hc/en-us/articles/203313410Roblox-Community-Standards Instead, we generate prompts by feeding policy document to an LLM. This document includes all safety categories and their definitions. The model is then prompted to generate wide variety of adversarial queries and attack vectors aimed at misaligning target LLM. This approach ensures broad coverage across both safety categories and malicious prompt types. We use the DeepSeek-R1-Distill-Qwen-7B model (Guo et al. 2025) for this stage. 2. Response Generator. For each generated prompt, we use diverse set of LLMs to produce candidate responses. To increase variation and realism, we sample from three different models: Mistral-7B-v0.1 (Jiang et al. 2023), Llama-3.2-3B-Instruct, and Qwen2.5-7BInstruct-Abliterated-v2. 3. LLM-as-a-Judge. In the final stage, we use LLMs to label the prompt-response pairs. We adopt models such as Mistral-Small-24B-Instruct-2501 and DeepSeek-R1 as judges. To ensure quality, we calibrate their labeling performance on holdout data set and benchmark them against GPT-4o, which had F1 score of 85.61%, FPR of 9.34% and recall of 90.36% against the ground truth labeled by human experts. The GPT-4o output serves as reference for consistency and reliability in label quality. RobloxGuard Evaluation Set There are very few high quality evaluation datasets available to benchmark LLM safety. To address this, we also developed high-quality evaluation dataset across Robloxs content safety taxonomy - representing 25 violation categories. The evaluation set is curated by internal red-teaming by humans, in which we prompt the LLM by simulating adversarial attacks to look for vulnerabilities. The evaluation dataset contains prompt and response pairs with the responses hand-labeled by set of policy experts to ensure their quality. Each prompt and response pair is labeled by 3 experts, with agreement required by 2 of 3 experts"
        },
        {
            "title": "Targets",
            "content": "None Illegal and Regulated Goods and Activities Romantic and Sexual Content Real-World Sensitive Events Terrorism and Violent Extremism Discrimination, Slurs, and Hate Speech Political Figures and Entities Directing Users Off Platform Sharing Personal Information Profanity Threats, Bullying, and Harassment Violent Content and Gore Suicide, Self Injury, and Harmful Behavior Child Exploitation Spam Expanded Policies for Suitability Soliciting Donations Misusing Roblox Systems Cheating and Scams Independent Advertisement Publishing Prohibited Advertising Practices and Content Paid Random Items Intellectual Property Violations 1,980 124 99 97 90 85 81 67 55 50 34 27 23 14 13 8 5 5 5 4 3 2 1 Table 2: Distribution of Categories in RobloxGuard-Eval Dataset. in order for it to be included as label. It spans wide spectrum of violation types, helping us create more meaningful labels for evaluation. The final evaluation set includes 2,872 examples. The dataset is open-sourced to the community. Roblox Guard 1.0 Training Dataset The training data for Roblox Guard 1.0 combines both publicly available safety datasets and data generated via our synthetic data pipeline. Specifically, we incorporate examples from three public-domain sources: Aegis (Ghosh et al. 2024b), WildGuard (Han et al. 2024), and Beavertails (Ji et al. 2023). Statistics for the full training dataset are sumFigure 1: Synthetic data generation pipeline for Roblox Guard 1. Prompt Response Chain-of-Thought Label Category Prompt Response Label Chain-of-Thought Category Prompt Response Category Label Chain-of-Thought Prompt Response Category Chain-of-Thought Label Prompt Response Chain-of-Thought Category Label Prompt Response Label Category Chain-of-Thought Figure 2: Input Inversion. We permute the ordering of target components (Chain-of-Thought, Label, and Category) during training. This prevents the model from overfitting to specific output format and improves robustness, allowing RobloxGuard to be taxonomy-adaptive. marized in Table 3. With over 384,000 total examples, our combined training set is, to our knowledge, one of the largest instruction-following datasets for LLM safety. As shown in Table 4, our dataset is substantially larger than the training sets used to build other prominent guardrail models, including those for BingoGuard, WildGuard, and Aegis 2.0. This large-scale, diverse corpus is critical for achieving the broad generalization we demonstrate in our results. Our instruction set design is inspired by FLAN-style multi-task learning (Longpre et al. 2023), treating each safety taxonomy category as distinct task. To enhance task diversity and improve generalization, we leverage two key techniques: Chain-of-Thought (CoT) rationales and input inversion. Unlike approaches that unify all datasets under shared taxonomy, we retain the native labels and category structures provided by each dataset. This decision preserves the original annotation fidelity and allows us to capture wider variety of label definitions and granularity levels. Chain-of-Thought To provide richer contextual grounding during fine-tuning, we generate CoT rationales using the DeepSeek-R1 model. For each dataset, we use its native taxonomy and definitions as input prompts to guide CoT generation. We find that including detailed category descriptions significantly improves the quality of the rationales and, in turn, the performance of the fine-tuned model. Input Inversion For input inversion as shown in Figure 2, we generate examples that combine both prompt-level and response-level classification instructions, interleaved with CoT explanations. This strategy enhances the models robustness by increasing instruction diversity and encourages better generalization to new or evolving safety policies without retraining. # Task: You are an AI security tester tasked with generating dataset... Your objective is to create scenarios that could potentially bypass content moderation systems... <BEGIN POLICY> Category: {{category}} Subcategory: {{subcategory}} Policy Content: {{policy content}} <END POLICY> <JAILBREAK PLANNING> 1. Analyze the policy content... 2. Consider realistic usage scenarios... 3. Devise jailbreak techniques... 4. Plan the system prompt and user message... 5. Summarize the jailbreak approach... <END JAILBREAK PLANNING> Format each entry as JSON object... { \"System Prompt\": \"You are an advertiser...\", \"User Message\": \"Please review this ad for...\", \"Jailbreak Techniques\": \"Manipulated metadata...\", \"Violation Title\": \"Advertising - Independent...\" } # Task: Generate response from the Target LLM. Input from Stage 1: <SYSTEM PROMPT> You are an advertiser attempting to bypass content moderation by manipulating metadata in your ad content. <END SYSTEM PROMPT> <USER MESSAGE> Please review this ad for potential policy violations: This offer is only available until [specific time] due to limited stock. The time seems off for automatic renewal policies. <END USER MESSAGE> # Task: You are skilled content moderator responsible for evaluating... Read this Community Standards... Community Standards Overview: - Safety: Child Exploitation, Suicide... - Civility: Real-World Sensitive Events, Bullying, Harassment... - Integrity: Cheating and Scams, Misinformation... Assess the dialogue... - System Prompt: {system} - User Input: {user} - AI Output: {ai output} Provide output in...JSON format: { \"violation\": \"true or false\" } { \"AI Output\": \"This ad may be potential policy violation. The language used is misleading, as it implies that the offer is available for limited time...\" } { } \"violation\": \"true\" (a) Stage 1: Generation (b) Stage 2: Response (c) Stage 3: Validation Figure 3: Overview of the three-stage synthetic data pipeline. (a) Stage 1 (Generation): An AI security tester prompt generates an adversarial scenario (System Prompt, User Message). (b) Stage 2 (Response): The scenario is used to prompt target LLM, producing an AI Output. (c) Stage 3 (Validation): judge LLM, guided by Community Standards, evaluates the output and produces final binary violation label. Benchmarking Datasets We rigorously benchmark Roblox Guard 1.0 across diverse set of publicly available datasets to comprehensively evaluate its generalization capabilities across multiple safety taxonomies. At the prompt level, we conduct evaluations on the Aegis 1.0 (Ghosh et al. 2024a), Aegis 2.0 (Ghosh et al. 2024b), OpenAI Mod (Markov et al. 2023), SimpleSafetyTest (Vidgen et al. 2023), Toxic Chat (Lin et al. 2023), WildGuard (Han et al. 2024), and XSTest (Rottger et al. 2024) datasets. For response-level assessment, in addition to our proprietary RobloxGuard-Eval, we systematically evaluate the model on the Aegis 2.0, BeaverTails (Ji et al. 2023), HarmBench (Mazeika et al. 2024), SafeRLHF (Dai et al. 2024), and WildGuard datasets, ensuring thorough coverage of varied and challenging content domains. Aegis 1.0 (Ghosh et al. 2024a) is human labeled dataset composed of human LLM interactions. We use prompt portion of this dataset for benchmarking. Aegis 2.0 (Ghosh et al. 2024b) is an extended version of Aegis 1.0 featuring scalable taxonomy of 12 core and 9 fine-grained safety violation categories, designed to improve the alignment of LLM safety guardrails. It has both prompts and responses. OpenAI Mod (Markov et al. 2023) is prompt-level classification dataset labeled across coarse safety categories such as hate, violence, harassment, and sexual content, commonly used for evaluating LLM moderation capabilities. WildGuard (Han et al. 2024) is combination of training and test datasets contains both vanilla queries and red-teaming queries. It features the unique adversarial jailbreaks examples produced by WildTeaming. WildType Source Prompt Aegis WildGuard Prompt +Response Aegis WildGuard BeaverTails Llama Synthetic Mistral Synthetic Qwen Synthetic Total 14,773 48,783 9,431 37,934 99,481 53,840 59,982 60,009 Positives w/ CoT (P) w/o CoT (P) Negatives w/ CoT (N) w/o CoT (N) 7,159 24,914 3,541 8,368 54,831 24,172 44,262 46, 3,499 0 2,719 8,368 54,204 20,802 44,262 0 3,660 24,914 822 0 627 3,370 0 46,042 79,435 7,614 23, 5,890 29,566 44,650 29,668 15,720 13,967 3,499 0 4,782 29,566 43,591 27,844 15,720 0 170,944 125,002 4,115 23, 1,108 0 1,059 1,824 0 13,967 45,942 Total 384,233 213, 133,854 Table 3: Breakdown of training examples by prompt type, source, and Chain-of-Thought (CoT) inclusion. Positive (P) and Negative (N) samples are marked accordingly. GuardTraining contains 86.8K examples. We select all their training queries to add into our initial query set. We use the WildGuardTest with 1.7K data as the test set. SimpleSafetyTest (Vidgen et al. 2023) is synthetic benchmark designed to evaluate LLM guardrails using straightforward harmful prompts spanning common safety categories like suicide, self-harm, violence, scams, and child abuse. Toxic Chat (Lin et al. 2023) is dataset of around 10,166 real userAI interaction prompts constructed from the Vicuna and Chatbot Arena demos, annotated for toxicity in conversational context. The dataset was annotated by 4 researchers with high agreement (96.11% on 720-item pilot set), and focuses on subtle toxic content in user queries that may not contain overt insults but still violate norms. XSTest (Rottger et al. 2024) is diagnostic benchmark with 450 prompts designed to identify exaggerated safety behaviors in LLMs, testing their balance between helpfulness and over-cautious refusals across diverse prompt types. BeaverTails (Ji et al. 2023) is manually labeled dataset with contains both prompt and response pairs. The prompts are derived from HH-RLHF (Bai et al. 2022) and are labeled based on 14 harmful categories. We use the entire test set portion to evaluate our model unlike the subset of this set used in (Yin et al. 2025). We use both prompt and responses from this set for benchmarking. SafeRLHF (Dai et al. 2024) is response-level benchmark consisting of approximately 5,100 labeled examples from model completions, annotated across multiple safety categories with severity levels. It is specifically designed to evaluate LLMs ability to refuse or safely respond to harmful prompts, reflecting nuanced, real-world Model / Dataset Paper Training Set Size Roblox Guard 1.0 (Ours) WildGuard BingoGuard NemoGuard-8B (Han et al. 2024) (Yin et al. 2025) (Ghosh et al. 2024b) 384,233 86,759 54,897 30, Table 4: Comparison of training set sizes for prominent LLM safety guardrail models. Our work uses significantly larger and more diverse corpus. safety challenges encountered during RLHF training. HarmBench (Mazeika et al. 2024) is comprehensive evaluation framework comprising 510 unique instances across 4 functional categories (standard, copyright, contextual, and multimodal) and 7 semantic categories (e.g., cybercrime, misinformation), designed to assess large language models (LLMs) robustness against harmful behaviors through automated red teaming and robust refusal techniques. We use 602 text behaviors classifier values."
        },
        {
            "title": "Experiments",
            "content": "Roblox Guard 1.0 Training We fine-tune the Llama-3.1-8B-Instruct model using LowRank Adaptation (LoRA) (Hu et al. 2022) to develop Roblox Guard 1.0, our instruction-following policy model. The finetuning is performed using the PEFT library with LoRA rank of = 16, enabling efficient adaptation while maintaining generalization. Training is conducted on dataset of more than 384k diverse policy instructions, covering wide range of moderation scenarios with varying taxonomies. The objective is to maximize the likelihood of correct continuations conditioned on the instruction, aligning the model with intended policy behaviors. We train for 3 epochs using learning rate of 1 104, batch size of 8 (per device), warmup ratio of 0.03, and context length of 2408 tokens. Training is performed in mixed precision (bfloat16) on single machine equipped with 8 A100 GPUs (each with 80GB of memory). The resulting fine-tuned model is referred to as Llama-3.1-8BInstruct-RobloxGuard-1.0. To evaluate practical deployment, we benchmarked the inference latency of Roblox Guard 1.0. The model was served using the vLLM engine on an AWS g6.12xlarge instance. For typical full-context classification payload of 790 total tokens (770 prompt and 20 completion tokens), we measured an average latency of 869.9ms over 10 runs. This practical inference speed confirms the models suitability for high-accuracy, real-time moderation tasks. We also run ablations to quantify the effect that our synthetic dataset pipeline, CoT and input inversion has on the performance of our model, using the same training parameters. To isolate the contribution of our synthetic data genera-"
        },
        {
            "title": "Dataset",
            "content": "Roblox Guard 1.0-8B LlamaGuard3-8B WildGuard-7B ShieldGemma-7B BingoGuard-8B GPT-4o NemoGuard-8B Aegis 1.0 Prompt Aegis 2.0 Prompt OAI Mod SimpleSafetyTest Toxic Chat WildGuard Prompt XSTest Aegis 2.0 Response BeaverTails Harmbench SafeRLHF WildGuard Response RobloxGuard-Eval 91.9% 87.9% 70.3% 100.0% 79.1% 89.5% 86.4% 86.0% 87.3% 85.7% 69.9% 80.6% 79.6% Prompt-based Benchmarks 74.8% 77.3% 79.4% 97.0% 50.9% 70.1% 88.3% 89.4% 81.9% 72.1% 99.5% 70.8% 88.9% 94.4% Response-based Benchmarks 65.7% 69.7% 84.9% 53.7% 70.2% 3.5% 83.5% 84.4% 86.2% 64.2% 75.4% 15.1% 88.7% 84.1% 82.1% 100.0% 70.2% 88.1% 92.5% 81.8% 84.8% 84.8% 66.6% 77.8% 55.5% 83.2% 90.4% 78.7% 77.8% 77.9% 70.4% 97.4% 100.0% 68.1% 75.7% 88.9% 87.9% 94.9% 90.2% 78.9% 83.3% 86.4% 83.8% 86.4% 83.5% 67.9% 68.7% 73.1% 80.1% 66.3% 25.9% 89.8% 87.0% 77.0% 99.0% 59.5% 82.1% 82.6% 87.6% 77.6% 78.6% 58.2% 75.7% 23.6% Table 5: Comparison of various guard models against suite of safety benchmarks. The benchmarks are categorized into prompt-based and response-based attacks. Scores represent the success rate of the guard model in defending against the attacks measured in F1. tion pipeline, we first trained model variant exclusively on publicly available datasets (Aegis, WildGuard, and BeaverTails), excluding all synthetically generated samples. The training hyperparameters and procedures were kept identical to those used in the full model. This setup allows us to assess the extent to which synthetic data contribute to domain adaptation and overall performance. Next, to evaluate the impact of explicit reasoning, we conducted an ablation study on CoT. In this experiment, model variant was trained on the full dataset (public and synthetic) but with all CoT rationales removed. As result, the model was exposed only to the prompt, response, taxonomy, and final safety label. Finally, to assess the benefits of input inversion, we trained another model on the complete data set but used single fixed instruction template throughout. This design eliminates the structured variation between the policy instructions at the prompt and the response-level. Results We evaluate at both prompt and response levels. For prompt level evaluation, we only use prompt along with taxonomy. For response level evaluation, we use both prompt and response along with taxonomy to assess for the entire context. We report our results in terms of F1 (%). We also compare our models performance with that of several well-known models. These include LlamaGuard3, WildGuard (Han et al. 2024), ShieldGemma (Zeng et al. 2024), BingoGuard (Yin et al. 2025), NemoGuard (Ghosh et al. 2024b), and GPT-4o. Most of the models, with the exception of GPT-4o are trained for LLM guardrails and are in the similar parameter count range as the Roblox Guard 1.0. Roblox Guard 1.0 consistently achieves state-of-the-art performance across both prompt-based and response-based benchmarks. On prompt-level datasets such as Aegis 1.0, Aegis 2.0, Toxic Chat, and WildGuard Prompt, our model outperforms existing guardrail systems, often by significant margin. This demonstrates strong capabilities in detecting harmful intent from prompts alone, without relying on model responses. Notably, Roblox Guard 1.0 achieves 91.9% F1 on Aegis 1.0 Prompt and 89.5% on WildGuard Prompt, surpassing all competing models, including commercial systems like GPT-4o. Response-level evaluation further highlights the robustness and adaptability of our model. Unlike prompt-only classification, response-level moderation involves nuanced reasoning over the interplay between prompt and model outputoften requiring context comprehension and deeper safety understanding. On challenging benchmarks such as BeaverTails and Aegis 2.0 Response, Roblox Guard 1.0 either matches or outperforms the best available baselines. For instance, our model achieves 87.3% F1 on BeaverTails, outperforming LlamaGuard3, ShieldGemma, and GPT-4o. On Harmbench and SafeRLHF, our performance is also competitive, despite the diversity of these datasets in terms of style and violation categories. One of the most important findings comes from the evaluation on out-of-domain datasets such as Toxic Chat, SafeRLHF, XSTest, and HarmBench. These datasets introduce novel prompt styles, response formats, and harm categories that were not explicitly seen during training. Despite this, Roblox Guard 1.0 maintains strong performanceachieving 79.1% on Toxic Chat, 69.9% on SafeRLHF, 86.4% on XSTest, and 85.7% on HarmBenchoften outperforming models specifically tuned for these tasks. The diversity of these benchmarks makes them reliable proxy for testing generalization under taxonomy drift, prompting inconsistencies, and distribution shifts. We attribute this capability to (1) our use of diverse instruction sets generated via input inversion, and (2) the compositional nature of our training strategy, which treats each taxonomy as separate but related task. These design choices encourage zero-shot generalization and allow the model to recognize harm types even under novel category formulations. RobloxGuard-Eval is specifically designed to test models against rich and fine-grained safety taxonomy that reflects real-world moderation needs. With 23 nuanced categoriesincluding underrepresented classes such as promoCategory Dataset Roblox Guard 1.0 (Full) w/o Synthetic w/o CoT w/o Input Inversion Prompt Response Aegis 1.0 Prompt Aegis 2.0 Prompt OAI Mod SimpleSafetyTest Toxic Chat WildGuard Prompt XSTest Aegis 2.0 Response BeaverTails Harmbench SafeRLHF WildGuard Response RobloxGuard-Eval 91.9% 87.9% 70.3% 100.0% 79.1% 89.5% 86.4% 86.0% 87.3% 85.7% 69.9% 80.6% 79.6% 89.4% 86.2% 49.2% 100.0% 74.8% 84.9% 77.1% 83.84% 85.3% -a 64.5% 68.3% 20.3% 89.5% 86.8% 67.0% 99.5% 78.5% 88.5% 87.6% 81.6% 86.5% 81.8% 71.0% 79.4% 82.3% 91.7% 88.0% 71.3% 100.0% 78.8% 88.3% 83.4% 84.4% 85.9% 84.2% 69.1% 78.0% 80.7% Table 6: Ablation study on the impact of CoT rationales, Synthetic Data, and Input Inversion. We report F1-scores (%) on various benchmarks, comparing our full model against ablated versions. The results show that CoT is critical for out-of-domain generalization, while both techniques contribute to the models final performance. aThe model trained without synthetic data failed to produce valid inferences on this benchmark, resulting in no score. tional scams, off-platform solicitation, and deceptive monetizationthis benchmark captures harm types rarely seen in other public datasets. As shown in Table 5, most existing guardrail modelsincluding those that perform competitively on generic safety benchmarksstruggle significantly on RobloxGuard Eval, with some models dropping below 30% F1. This highlights critical limitation: despite strong performance on well-known datasets, these models fail to generalize when faced with more granular and domainspecific safety taxonomies. In contrast, Roblox Guard 1.0 maintains high accuracy without requiring any task-specific tuning, demonstrating its robustness in handling both broad and specialized harm categories. This illustrates that existing safety benchmarks are more likely than not already saturated. This further underscores the importance of evaluation frameworks like RobloxGuard-Eval, which stress-test models against evolving and platform-aligned taxonomies rather than static, oversimplified ones. Taken together, these results show that Roblox Guard 1.0 is not only competitive with specialized guardrail systems but also uniquely robust in settings where taxonomies shift, evolve, or expand. This makes it strong candidate for real-world deployment in dynamic moderation environments, where adaptability and coverage are critical."
        },
        {
            "title": "Ablations",
            "content": "Our ablation study  (Table 6)  isolates the impact of each component of our methodology. First, removing our synthetic dataset causes catastrophic drop in performance on our domain-specific benchmark, RobloxGuard-Eval from 79.6% F1 down to 20.3%. Performance also drops significantly on OAI Mod from 70.3% to 49.2% and WildGuard Response from 80.6% to 68.3%. This confirms that public datasets alone are insufficient to cover the nuanced, fine-grained policies prevalent in realworld applications and highlights the critical value of our synthetic data pipeline. Second, removing CoT rationales leads to performance degradation on several complex benchmarks requiring deeper reasoning, notably Aegis 2.0 Response (a 4.4% drop) and Harmbench (a 3.9% drop). Interestingly, performance slightly improved on SafeRLHF and RobloxGuard-Eval. We hypothesize this occurs because these benchmarks may contain higher proportion of straightforward violations where explicit CoT reasoning provides less benefit compared to simpler pattern matching. Overall, the results suggest CoT is most beneficial for nuanced, multi-turn, or reasoningintensive safety evaluations. Finally, removing input inversion most significantly impacts performance on XSTest (a 3.0% drop) and WildGuard Response (a 2.6% drop). Since XSTest evaluates model brittleness and over-refusals, and WildGuard uses adversarial formats, this confirms that input inversion is key to improving model robustness and flexibility against diverse instruction formats."
        },
        {
            "title": "Conclusion",
            "content": "We present Roblox Guard 1.0, binary classification system operating at both prompt and response levels to ensure LLM safety. Our model exhibits strong generalization to unseen taxonomies and achieves state-of-the-art or comparable performance across multiple benchmarks. Central to our approach is synthetic data pipeline augmented with CoT rationales, which enriches contextual grounding during training. Additionally, we introduce RobloxGuardEval, comprehensive new evaluation dataset aligned with Robloxs rigorous safety taxonomy, annotated by internal policy experts to ensure high-fidelity labels that capture nuanced policy distinctions. As existing safety benchmarks approach saturation, RobloxGuard-Eval provides fresh, challenging benchmark designed to rigorously evaluate taxonomy-awareness and out-of-domain generalization in safety-critical contexts. We encourage the community to adopt RobloxGuard-Eval as standard for future research in LLM safety and moderation. References Bai, Y.; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; DasSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan, T.; et al. 2022. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Dai, J.; Pan, X.; Sun, R.; Ji, J.; Xu, X.; Liu, M.; Wang, Y.; and Yang, Y. 2024. Safe RLHF: Safe Reinforcement LearnIn The Twelfth International ing from Human Feedback. Conference on Learning Representations. Ge, Y.; Kirtane, N.; Peng, H.; and Hakkani-Tur, D. 2025. Llms are vulnerable to malicious prompts disguised as scientific language. arXiv preprint arXiv:2501.14073. Ghosh, S.; Varshney, P.; Galinkin, E.; and Parisien, C. 2024a. Aegis: Online adaptive ai content safety moderation with ensemble of llm experts. arXiv preprint arXiv:2404.05993. Ghosh, S.; Varshney, P.; Sreedhar, M. N.; Padmakumar, A.; Rebedea, T.; Varghese, J. R.; and Parisien, C. 2024b. AEGIS2.0: Diverse AI Safety Dataset and Risks TaxonIn Neurips Safe omy for Alignment of LLM Guardrails. Generative AI Workshop 2024. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Han, S.; Rao, K.; Ettinger, A.; Jiang, L.; Lin, B. Y.; Lambert, N.; Choi, Y.; and Dziri, N. 2024. Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms. Advances in Neural Information Processing Systems, 37: 80938131. Hu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang, S.; Wang, L.; Chen, W.; et al. 2022. Lora: Low-rank adaptation of large language models. ICLR, 1(2): 3. Inan, H.; Upasani, K.; Chi, J.; Rungta, R.; Iyer, K.; Mao, Y.; Tontchev, M.; Hu, Q.; Fuller, B.; Testuggine, D.; et al. 2023. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674. Ji, J.; Liu, M.; Dai, J.; Pan, X.; Zhang, C.; Bian, C.; Chen, B.; Sun, R.; Wang, Y.; and Yang, Y. 2023. Beavertails: Towards improved safety alignment of llm via human-preference dataset. Advances in Neural Information Processing Systems, 36: 2467824704. Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.; Chaplot, D. S.; de las Casas, D.; Bressand, F.; Lengyel, G.; Lample, G.; Saulnier, L.; Lavaud, L. R.; Lachaux, M.-A.; Stock, P.; Scao, T. L.; Lavril, T.; Wang, T.; Lacroix, T.; and Sayed, W. E. 2023. Mistral 7B. https://arxiv.org/abs/2310.06825. Kumar Nandwana, M.; He, Y.; Liu, J.; Yu, X.; Shang, C.; Du Bois, E.; McGuire, M.; and Bhat, K. 2024. Voice Toxicity Detection Using Multi-Task Learning. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 331335. Lin, Z.; Wang, Z.; Tong, Y.; Wang, Y.; Guo, Y.; Wang, Y.; and Shang, J. 2023. ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation. In Findings of the Association for Computational Linguistics: EMNLP 2023, 46944702. Liu, J.; Nandwana, M. K.; Pylkkonen, J.; Heikinheimo, H.; and McGuire, M. 2024. Enhancing Multilingual Voice Toxicity Detection with Speech-Text Alignment. In Proc. Interspeech 2024, 42984302. Longpre, S.; Hou, L.; Vu, T.; Webson, A.; Chung, H. W.; Tay, Y.; Zhou, D.; Le, Q. V.; Zoph, B.; Wei, J.; et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning, 2263122648. PMLR. Markov, T.; Zhang, C.; Agarwal, S.; Nekoul, F. E.; Lee, T.; Adler, S.; Jiang, A.; and Weng, L. 2023. holistic approach In Proto undesired content detection in the real world. ceedings of the AAAI conference on artificial intelligence, volume 37, 1500915018. Mazeika, M.; Phan, L.; Yin, X.; Zou, A.; Wang, Z.; Mu, N.; Sakhaee, E.; Li, N.; Basart, S.; Li, B.; et al. 2024. HarmBench: Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal. In International Conference on Machine Learning, 3518135224. PMLR. Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744. Patwardhan, T.; Dias, R.; Proehl, E.; Kim, G.; Wang, M.; Watkins, O.; Fishman, S. P.; Aljubeh, M.; Thacker, P.; Fauconnet, L.; Kim, N. S.; Chao, P.; Miserendino, S.; Chabot, G.; Li, D.; Sharman, M.; Barr, A.; Glaese, A.; and Tworek, J. 2025. GDPval: Evaluating AI Model Performance on RealWorld Economically Valuable Tasks. arXiv:2510.04374. Rebedea, T.; Dinu, R.; Sreedhar, M. N.; Parisien, C.; and Cohen, J. 2023. NeMo Guardrails: Toolkit for Controllable and Safe LLM Applications with Programmable Rails. In Feng, Y.; and Lefever, E., eds., Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 431445. Singapore: Association for Computational Linguistics. Rottger, P.; Kirk, H.; Vidgen, B.; Attanasio, G.; Bianchi, F.; and Hovy, D. 2024. XSTest: Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 53775400. Shen, X.; Chen, Z.; Backes, M.; Shen, Y.; and Zhang, Y. 2024. do anything now: Characterizing and evaluating in-the-wild jailbreak prompts on large language models. In Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security, 16711685. Vidgen, B.; Scherrer, N.; Kirk, H. R.; Qian, R.; Kannappan, A.; Hale, S. A.; and Rottger, P. 2023. Simplesafetytests: test suite for identifying critical safety risks in large language models. arXiv preprint arXiv:2311.08370. Wang, Y.; Li, H.; Han, X.; Nakov, P.; and Baldwin, T. 2023. Do-not-answer: dataset for evaluating safeguards in llms. arXiv preprint arXiv:2308.13387. Yin, F.; Laban, P.; PENG, X.; Zhou, Y.; Mao, Y.; Vats, V.; Ross, L.; Agarwal, D.; Xiong, C.; and Wu, C.-S. 2025. BingoGuard: LLM Content Moderation Tools with Risk Levels. In The Thirteenth International Conference on Learning Representations. Zeng, W.; Liu, Y.; Mullins, R.; Peran, L.; Fernandez, J.; Harkous, H.; Narasimhan, K.; Proud, D.; Kumar, P.; Shieldgemma: Generative Radharapu, B.; et al. 2024. arXiv preprint ai content moderation based on gemma. arXiv:2407.21772. Zhang, C.; Zhu, C.; Xiong, J.; Xu, X.; Li, L.; Liu, Y.; and Lu, Z. 2025. Guardians and Offenders: Survey on Harmful Content Generation and Safety Mitigation. arXiv preprint arXiv:2508.05775."
        }
    ],
    "affiliations": [
        "Roblox"
    ]
}