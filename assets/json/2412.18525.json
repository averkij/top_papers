{
    "paper_title": "Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization",
    "authors": [
        "Yang Shen",
        "Xiu-Shen Wei",
        "Yifan Sun",
        "Yuxin Song",
        "Tao Yuan",
        "Jian Jin",
        "Heyang Xu",
        "Yazhou Yao",
        "Errui Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Computer Vision (CV) has yet to fully achieve the zero-shot task generalization observed in Natural Language Processing (NLP), despite following many of the milestones established in NLP, such as large transformer models, extensive pre-training, and the auto-regression paradigm, among others. In this paper, we explore the idea that CV adopts discrete and terminological task definitions (\\eg, ``image segmentation''), which may be a key barrier to zero-shot task generalization. Our hypothesis is that without truly understanding previously-seen tasks--due to these terminological definitions--deep models struggle to generalize to novel tasks. To verify this, we introduce Explanatory Instructions, which provide an intuitive way to define CV task objectives through detailed linguistic transformations from input images to outputs. We create a large-scale dataset comprising 12 million ``image input $\\to$ explanatory instruction $\\to$ output'' triplets, and train an auto-regressive-based vision-language model (AR-based VLM) that takes both images and explanatory instructions as input. By learning to follow these instructions, the AR-based VLM achieves instruction-level zero-shot capabilities for previously-seen tasks and demonstrates strong zero-shot generalization for unseen CV tasks. Code and dataset will be openly available on our GitHub repository."
        },
        {
            "title": "Start",
            "content": "Explanatory Instructions: Towards Unified Vision Tasks Understanding and Zero-shot Generalization Yang Shen1 Xiu-Shen Wei2 Yifan Sun3 Yuxin Song3 Tao Yuan Jian Jin1 He-Yang Xu2 Yazhou Yao1 Errui Ding3 1Nanjing University of Science and Technology 2Southeast University 3Baidu 4 2 0 2 5 2 ] . [ 2 5 2 5 8 1 . 2 1 4 2 : r Figure 1. (a) Early CV models are designed to handle discrete vision tasks. (b) Recent VLMs use terminological instructions (i.e., terminological task definitions), e.g., semantic segmentation and pose map. (c) We propose Explanatory Instructions to explain CV tasks objective and construct the dataset of Explanatory CV Tasks. We train the model via this dataset. (d) The trained model showcases instruction-level zero-shot capabilities. (e) By omitting certain human-defined vision tasks in the training dataset (cf. Sec. 4.2), we demonstrate promising vision task-level zero-shot capabilities. *This work is done when the first author is an intern in Baidu. The first author is under the supervision of X.-S. Wei. Corresponding author."
        },
        {
            "title": "Abstract",
            "content": "Computer Vision (CV) has yet to fully achieve the zero-shot task generalization observed in Natural Language Processing (NLP), despite following many of the milestones established in NLP, such as large transformer models, extensive pre-training, and the auto-regression paradigm, among others. In this paper, we rethink the reality that CV adopts discrete and terminological task definitions (e.g., image segmentation), and conjecture it is key barrier that hampers zero-shot task generalization. Our hypothesis is that without truly understanding previously-seen tasksdue to these terminological definitionsdeep models struggle to generalize to novel tasks. To verify this, we introduce Explanatory Instructions, which provide an intuitive way to define CV task objectives through detailed linguistic transformations from input images to outputs. We create large-scale dataset comprising 12 million image input explanatory instruction output triplets, and train an auto-regressive-based vision-language model (AR-based VLM) that takes both images and explanatory instructions as input. By learning to follow these instructions, the AR-based VLM achieves instruction-level zero-shot capabilities for previously-seen tasks and demonstrates strong zero-shot generalization for unseen CV tasks. Code and dataset will be openly available on our GitHub repository. 1. Introduction Natural Language Processing (NLP) has achieved significant advancements in task-level zero-shot generalization, driven by key milestones such as transformer models, large-scale pretraining, and the auto-regression paradigm [8, 61, 66]. This impressive adaptability stems from the language models capacity to interpret flexible, open-ended but semantically rich instructions, enabling them to generalize across language tasks without additional task-specific fine-tuning. In contrast, Computer Vision (CV) has not yet unlocked the task-level zero-shot generalization capabilities. Early CV models largely depend on discrete vision task boundaries (cf. Fig. 1 (a)), which restricts their flexibility and generalization capabilities. While recent efforts, e.g., Lumina-mGPT [35], OmniGen [71], and PixWizard [34], have attempted to incorporate wide range of vision tasks via fixed or open-style terminological instructions (cf. Fig. 1 (b)), their zero-shot capabilities remain confined within seen vision tasks or rely on few-shot prompting for guidance. Our intuition strongly implies that this limitation may stem from the constrained scope of instructions and existing vision-language models (VLMs) do not truly understand the previously-seen vision tasks. To be specific, these terminological instructions are consisted of discrete and symbolic task definitions that do not convey the objective behind each task, which can be replaced with any other symbols (e.g., changing Semantic segmentation within these instructions into Task or Task djjjvau) without affecting the vision task results. To enable deep models to understand vision tasks genuinely, we introduce Explanatory Instructions that characterize vision tasks by providing linguistic descriptions of the task objective, i.e., the detailed transformations from input images to outputs, cf. Fig. 1 (c). To provide context, we first revisit the concept of vision tasks. Generally, vision task refers to specific objective or operation within the visual domain that humans or machines need to accomplish [42]. However, current vision tasks are constrained by rigid, human-defined boundaries, e.g., controllable generation tasks such as Depth-to-Image, Canny-to-Image, Pose-toImage, dense image prediction tasks such as Segmentation, Pose Estimation, Surface Normal Estimation. In practice, researchers present vision tasks to models via discrete and terminological definitions, limiting both the articulation of task objectives and the expressive flexibility of task descriptions. For instance, when describing segmentation task, nonexperts often use natural, descriptive phrases that, more importantly, carry clear and specific meanings, e.g., overlay the stream area with bright blue color or paint the stream in blue and mark the rocks within it in red, rather than narrowly defined terminological instructions such as semantic segmentation or close variants. Through this form of instruction, we clarify the true objectives of various vision tasks, moving them beyond the constraints of terminological task categories. We further help enhance the diversity of task expressions within the vision domain, while also strengthening the alignment between text and vision modalities at the task level. To operationalize this idea, we construct the Dataset of Explanatory Computer Vison Tasks (DECVT) that contains 12 million image input explanatory instruction output triplets. Notably, for the same image inputs, DECVT provides different explanatory instructions and corresponding output images for varying task objectives. Furthermore, DECVT adopts explanatory instructions to represent transformations in both directions, e.g., from original images to segmentation outputs and from segmentation outputs back to original images (cf., the segmentation example in Fig.1 (c)), capturing richer diversity of task expressions. By omitting certain terminological-based vision tasks, e.g., Canny-to-Image and Image-to-Canny, Depth-to-Image and Image-to-Depth, we conduct instruction-driven supervised fine-tuning on an autoregressive-based vision-language model (AR-based VLM), utilizing approximately 1.5 million bidirectional pair of image explanatory instructions image triplets (i.e., input image, output image, explanatory instructions from input to output and from output to input). Results show that the fine-tuned model exhibits both instruction-level zero-shot ca2 pabilities (cf. Fig. 1 (d)) and promising task-level zero-shot capabilities (cf. Fig. 1 (e)), demonstrating its potential to generalize effectively across diverse vision tasks. The main contributions of this work are as follows: We propose Explanatory Instructions that intuitively explain task objectives by characterizing diverse vision tasks through linguistic descriptions of the transformations between paired images. We further construct Dataset of Explanatory CV Tasks, which, to the best of our knowledge, is the first dataset to integrate multiple vision tasks while providing instructions that authentically describe the task objectives. We conduct experiments to demonstrate that, after learning to follow the proposed Explanatory Instructions, the AR-based VLM model exhibits zero-shot generalization not only at the instruction level but also at the vision task level. Besides, fine-tuning on the constructed dataset further enhances the models versatility, enabling it to handle any combination of tasks, rather than being restricted to single task that relies on specific terminological instructions. This advancement represents significant step toward more flexible and unified understanding of vision tasks. 2. Dataset of Explanatory CV Tasks We construct Dataset of Explanatory CV Tasks (DECVT) based on our proposed Explanatory Instructions fashion, where vision tasks are represented exclusively through textual descriptions of transformations between images, effectively clarifying each tasks objective without relying on predefined terminological categories. During the construction of DECVT, we consider the discrete nature of the established concept of computer vision tasks and divide the dataset into two components, including Terminological-based Vision Tasks and Explanatory-based Vision Tasks. The first component primarily includes tasks whose objectives have been abstracted into standardized terminologies, while the second component encompasses tasks that require explanatory descriptions to differentiate them. These two components contain approximately 6 million bidirectional pairs of image explanatory instructions image triplets (i.e., around 12 million individual input image explanatory instruction output triplets). Below, we provide an introduction along with example entries for each component. Please refer to Appendix for specifics on the dataset construction. 2.1. Terminological-based Vision Tasks This component primarily encompasses low-level vision tasks, controllable generation tasks, dense image prediction tasks, image grounding tasks and their inverse counterparts. While these tasks have been refined and succinctly abstracted through human summarization, their inherent discreteness limits the models ability to comprehend these vision tasks fully. Therefore, we aim to reduce the rigidity by providing Caption of image A: close-up of traditional mooncake with an intricate, embossed design on its golden-brown crust, placed on decorative white doily. Caption of image B: simplified version of the mooncake where only the main shapes and contours of the design are visible, showing stark contrast between the outlines and the background, with all detailed textures removed. the visual detailed Explanatory instruction from to B: Reduce elements, focusing only on the essential shapes and contours of the design. Strip away color and texture, retaining only the prominent outlines to emphasize the primary form and structure against plain background. Explanatory instruction from to A: Add layers of color, texture, and depth to the outlined shapes, recreating the intricate details and embossed patterns. Introduce warm, golden-brown hue to bring out the richness of the surface, detailed complete restoring its the appearance decorative base. the of object with and Figure 2. Examples of terminological-based vision tasks, e.g., holistically nested edge detection. detailed explanatory instructions that explicitly articulate the underlying objectives of each task. To further explore task-level zero-shot generalization capabilities, this component includes only subset of terminological-based vision tasks. Specifically, we incorporate low-level data from opensource datasets including AllWeather [64], NYU-Rain [33], D-HAZE [2], NH-HAZE [3], UIEB [29], Adobe-5k [9], covering restoration tasks of Image Restoration, Deraining, Dehazing and Desnowing. We also collect Object Detection data from the LVIS [22] dataset, Style Transfer data from CSGO1[73] and prepare data for dense image prediction tasks including Depth Estimation, Surface Normal Estimation, Pose Estimation, and Semantic Segmentation from ADE20K [87], Depth in the Wild [11] and randomly selected data from the next two components. Additionally, for controllable generation tasks, we incorporate HolisticallyNested Edge (HED) Boundary to Image and inverse dense image prediction tasks (e.g., Pose-to-Image, Depth-to-Image, Segmentation-to-Image). Examples for this component are shown in Fig. 1 (c) and Fig. 2. Details and additional exam1As CSGO has not been open-sourced, we only collect data from the paper. 3 Caption of image A: glass of iced latte with light creamy color sits on wooden surface. The drink has visible ice cubes floating at the top, creating refreshing appearance. The background is softly blurred, focusing the attention on the drink and enhancing its warm, cozy aesthetic. Caption of image B: glass of iced coffee with distinct layers, where dark espresso swirls into the lighter creamy base. The drink is set on hexagonal coaster placed on textured glass surface, with blurred, minimalistic background. The presentation emphasizes the richness of the espresso and gives modern, refined feel to the scene. the the Adjust wooden beneath surface effect. Explanatory instruction from to B: Change chilled drink presentation to glossy, textured platform that refracts light. Alter the arrangement and appearance of the beverage to have swirl of ingredients visible from the side, creating smoother gradient lighting to soften shadows and introduce more ambient illumination across the scene, highlighting the layers and making the glasss ridges appear more prominent. Explanatory instruction from to A: Replace the sleek, reflective surface beneath the clear beverage vessel with warm, rustic wooden texture that enhances the natural, earthy tone of the scene. layers Modify to appearance with focus on single color. Adjust lighting sharper contrasts and distinct edges, making the transparent container stand out against the background. the achieve conditions beverages blended create visual more to Figure 3. Examples of explanatory-based vision tasks. ples are provided in Appendix A.2 and Appendix A.3.2. 2.2. Explanatory-based Vision Tasks Compared to language tasks, vision tasks inherently require more intricate forms of expression. While leveraging terminological-based vision tasks offers solid foundation, their predefined boundaries can limit the diversity and complexity of task objectives. To address this, we introduce explanatory-based vision tasks, which utilize descriptive instructions to articulate the transformations between images, enabling broader and more diverse range of vision task 4 objectives. We first collect data related to instruction-based image editing, where diverse editing instructions enable users to perform straightforward edits using natural language commands. However, each editing instruction typically involves only single operation of modification, such as adding, removing, or modifying the background or specific objects within an image, while preserving the overall structure and contextual integrity of the unaltered regions. Specifically, we directly incorporate publicly available image editing datasets including MagicBrush (only involving the real world and multiturn subsets) [82], SEED [20], HQ-Edit [25], HIVE [84], InstructPix2Pix [7] and PromptFix [80]. Afterward, we seek to transcend established vision task paradigms by incorporating more visual-related image pairs, thereby expanding the scope of task objectives. The transformations between these image pairs involve sophisticated variations which cannot be adequately described with straightforward instructions that typically focus on individual changes, e.g., combinations of camera angles with intricate lighting compositions, nuanced adjustments to scene composition and visual ambiance, or macro-level visual content changes but with similar features. These complex dynamics extend beyond the capabilities of conventional terminological-based vision tasks and require more intricate language instructions. We collect images to expand this component from search engines, with examples shown in Fig. 1 (c) and Fig. 3. Details on constructing these explanatory instructions and additional examples are provided in Appendix A.1 and Appendix A.3.1. 3. Methodology The primary objective of this work is to demonstrate that VLMs can achieve both instruction-level and vision tasklevel zero-shot capabilities using the proposed Explanatory Instructions, which characterize vision tasks through linguistic descriptions of their objectives, rather than to provide state-of-the-art solution. Consequently, we adopt straightforward token-based VLM that integrates an image tokenizer/de-tokenizer with general mixed-modal autoregressive language model. The overall framework is illustrated in Fig. 4. Architecture To create unified token sequence, both text and images should be tokenized into discrete spaces. Following Chameleon [60] and Lumina-mGPT [35], we employ byte pair encoding tokenizer [54] for the text modality and use quantization-based tokenization method to convert continuous image patches into discrete tokens for the image modality [17, 51, 52, 65]. After projecting text and image inputs into unified sequence by concatenating the text tokens and the flattened 1D image tokens, we employ decoderonly autoregressive transformer utilizing the standard dense Figure 4. Framework of our vanilla token-based VLM method. transformer architecture to simplify the generative modeling. Following LLaMa-2 [62], we adopt adaptations including RMSNorm [81] for normalization, SwiGLU [55] for activation and rotary positional embeddings (RoPE) [58] for positional encoding. To enhance training stability, PreLayerNorm [74], Post-LayerNorm [16] and Query-Key Normalization (QK-Norm) [23] are incorporated into each transformer block. Unambiguous Image Representation (UniRep) [35] are added to enable image generation at flexible resolution and aspect ratio. IB by instruction α and then to image IC by instruction β, we treat this as single sequence where image IA is edited into image IC by instruction α + β), applying the loss function exclusively to output tokens. For all the experiments, we employ the AdamW [39] optimizer with weight decay of 0.01 and betas set to (0.9, 0.95). The learning rate is configured at 4 105, and the z-loss is applied with weight of 105. To increase training throughput, all the data are pre-tokenized before training and clustered based on the number of tokens. Initialization The core contribution of this work lies in redefining vision tasks through Explanatory Instructions and investigating their potential for generalization, shifting the focus away from pretraining decoder-only autoregressive transformer from scratch. To achieve this, we leverage multimodal generative model pretrained on large-scale imagetext datasets, which provides flexible image generation capabilities. This approach eliminates the need for random initialization or language-only model initialization, conserving substantial computational resources otherwise required for establishing foundational image generation capabilities. Consequently, we can concentrate on exploring zero-shot generalization at both the instruction level and the vision task level within vanilla token-based VLM. Supervised Fine-tuning During training, the decoder-only auto-regressive transformer models the conditional probability p(xtx1, x2, ..., xt1) of multimodal sequences using the standard next-token prediction objective. Following previous works [14, 68], we apply z-loss to prevent the models output logits from becoming overly confident, thereby enhancing generalization. To streamline the supervised fine-tuning process, we organize all the data into single-dialog formats (e.g., for multi-turn editing data, if image IA is edited to image Inference During inference, any description associated with the input image can be provided as an explanatory instruction, guiding the fine-tuned token-based VLM to generate the corresponding output image based on both the image and the instruction. Furthermore, the sampling strategy for auto-regressive models involves various hyper-parameters that significantly affect the results. For example, while the top-k value is typically set to 5 for text generation in LLMs, we recommend increasing the top-k to 2048 for the image generation stage.2 4. Experiments In this section, we evaluate the models zero-shot capabilities on unseen instructions and unseen vision tasks through different experimental settings. Quantitative experiments that demonstrate the models zero-shot capabilities at both the instruction and task levels can refer to Appendix B. 4.1. Zero-shot Capabilities on Unseen Instructions Settings We fine-tune 7B AR-based VLM by utilizing all the DECVT training data. The pre-trained model used to initialize is Lumina-mGPT-7B-768-Omni. Training is 2More discussions are provided in Appendix 5 Input Image Input Image Explanatory Instruction: Transform the water elements into fire elements, using burning flame as the main theme. The hair becomes flame-like, in orange and red tones, rising upwards with subtle sparks. The dress is redesigned with flowing flame textures, resembling burning silk, creating dynamic contrasts between light and shadow. The background shifts to resemble molten lava, with lively fire flicker. Explanatory Instruction: Dim the scenes brightness, reduce the intensity of sunlight, and adjust the environment to rainy, overcast setting. Add sense of light drizzle and mist to the scene, creating soft yet slightly melancholic atmosphere. Gradually increase the vibrancy of the pink tones around the petals, but give them faintly faded and blurred appearance, and introduce sense of decay. Explanatory Instruction: Change the color of the car to red while try to keep all other objects in the scene unchanged. Add layer of snow to the surfaces of the road, the car, and the trees to create the atmosphere of winter morning. Pay close attention to details such as shadows and lighting adjustments. Output Image Output Image Input Image Output Image Figure 5. Examples of instruction-level zero-shot capabilities. conducted on 64 A100 GPUs, with batch size for each GPU set as 8 over 2 epochs (around 47k iterations), totaling 5,400 GPU hours. Image resolution equals 448 448. Zero-shot Instruction-level Capabilities Certain terminological-based tasks in NLP, e.g., Machine Translation, Emotion Detection, Named Entity Recognition, and Relation Extraction, have clearly defined task boundaries (i.e., their objectives are specific). However, many other language tasks, such as Dialogue and Question Answering, lack such clear task objective boundaries. Similarly, we argue that the set of terminological-based vision tasks represents only small subset of possible vision-related tasks. Many vision-related tasks are likely analogous to Dialogue in NLP, lacking clear task definitions or boundaries. As result, it is difficult to determine whether certain vision tasks are explicitly included during dataset construction (just as dataset for LLMs often include Dialogue or Question Answering data). However, it is easy to calculate textual similarity to verify whether the instructions associated with given image are part of the training set. Therefore, we refer to the models ability to generalize on such instructions and images as instruction-level zero-shot capabilities. Examples in Fig. 5 provide evidence that the AR-based VLM fine-tuned with the proposed explanatory instructions exhibits promising potential to become versatile vision generalist. Note that, in addition to image editing examples, our proposal also performs well on traditional discrete vision tasks, such as Semantic Segmentation, Surface Normal Estimation, and more, as detailed in Appendix C. 4.2. Zero-shot Capabilities on Unseen Vision Tasks Settings To directly assess the task-level zero-shot capabilities of the decoder-only autoregressive transformer finetuned on the constructed DECVT, we exclude certain tasks from the DECVT training data. Specifically, we remove data corresponding to Image Restoration, Depth Estimation, Surface Normal Estimation, Depth-to-Image, Surface Normal-to-Image, HED-to-Image and HED Boundary Detection tasks from the Terminological-based Vision Tasks component. For quick validation, we utilize subset of the DECVT training data: 30% data from the Explanatory-based Vision Tasks component (50% image editing data while the remaining 50% data from more visualrelated image pairs) and 20% data from the remaining portion of the Terminological-based Vision Tasks component, with each subset containing approximately 0.5 million bidirectional pair ofimage explanatory instruction image triplets (totally 1.5 million bidirectional pair of triplets). The pre-trained model used to initialize is Lumina-mGPT-7B768. Training for the 7B AR-based VLM model is conducted on 8 A100 GPUs with batch size set as 128 over 2 epochs (around 46k iterations), totaling 1,340 GPU hours. Image resolution equals 448 448. Task-level Zero-shot Capabilities We evaluate the finetuned models task-level zero-shot generalization capabilities on three previously unseen terminological-based vision tasks, i.e., HED-to-Image, Canny-to-Image, and Depth-to-Image, as directly testing on these unseen vision tasks provides the most intuitive demonstration of the models task-level zeroshot potential. For each task, we provided linguistic descrip-"
        },
        {
            "title": "Output Images",
            "content": "Figure 6. Examples of task-level zero-shot capabilities (HED-to-Image). Resolution: 448448. Explanatory Instruction: Gradually restore the scenes natural colors, filling in each region with realistic gradients and textures that represent natural elements. Reintroduce details such as color gradients in the sky, reflections in the water, and varied shades across fields and trees. Add depth by applying soft shading to convey lighting and shadow, ensuring the scene captures the natural flow of colors, reflections, and atmospheric lighting typical of landscape under daylight."
        },
        {
            "title": "Output Images",
            "content": "Figure 7. Examples of task-level zero-shot capabilities (Canny-to-Image). Resolution: 448448. Explanatory Instruction: Fill in all the empty outlines with rich colors that reflect vibrant tones, while redefining the shapes with smooth textures. Add layers of depth to the flat contours by enhancing brightness gradients in the sky, shadowing in the mountains, and intricate shades among the flowers. Reintroduce the sensation of open space and dimension by contrasting sharp objects with muted backgrounds and crisp details in the foreground."
        },
        {
            "title": "Output Images",
            "content": "Figure 8. Examples of task-level zero-shot capabilities (Depth-to-Image). Resolution: 448448. Explanatory Instruction: Create realistic coastal scene based on the image. Begin by interpreting the gradient colors to identify the relative distance of landscape elements. Use this information to construct rocky coastline with prominent headland and isolated rock. Add natural colors for sea and sky, and introduce texture and shading to the rocky surfaces, aiming to replicate dusk setting with soft lighting and gentle waves. tions, termed as Explanatory Instructions, which detailed the desired transformations for the input images. These instructions, input along with the source images, guided the model in generating outputs that closely aligned with the specified transformations, as shown in Fig. 6, Fig. 7 and Fig. 8. These successful interpretation and generation processes, without prior exposure to these tasks, highlights the potential and feasibility of the proposed explanatory instructions in promoting task-level zero-shot learning within the vision domain. For additional task-level zero-shot examples, please refer to Appendix C. 7 5. Limitations and Discussions In this section, we primarily focus on the most critical aspecttask-level zero-shot capabilities. For more comprehensive discussion on limitations and additional insights (e.g., dataset constraints and model stability), please refer to Appendix D. While the experiments in Sec. 4.2 and Appendix demonstrate that models fine-tuned on datasets constructed with the proposed explanatory instructions exhibit task-level zero-shot capabilities, it is important to note that these capabilities are observed primarily in generation tasks (e.g., HED-to-Image, Canny-to-Image, Depth-to-Image, Outpainting and Inpainting) and low-level vision tasks (e.g., Low-light Enhancement and Deblurring). However, it fails to exhibit task-level zero-shot capabilities in vision tasks like Image-to-Canny or Image-to-Depth. Despite these limitations, it is important to highlight that the task-level zero-shot capabilities we observe are fundamentally different from the fixed-instruction forms commonly used in controllable generation tasks. In fixed-instruction scenarios, models not only know the task objectives but are also given the captions for the images to be generated. In contrast, by providing varied explanatory instructions, we guide the model to understand the specific task objective, thereby expanding its ability to handle diverse range of tasks. Regarding the scope of task-level zero-shot capabilities, we hypothesize that the primary reason for this limitation is the lack of alignment between the image tokenizer (e.g., VQ-VAE or VQ-GAN) and the text modality during pretraining. If the pretrained model used for initialization lacks the ability to generate images resembling Canny edges or depth maps, the fine-tuned model struggles to generalize to these tasks based solely on linguistic descriptions. In contrast, for controllable generation tasks such as Canny-to-Image, the text concepts (e.g., sky) have already been aligned with image tokens in the pre-trained model through paired text-image data. As result, it is less affected by the lack of alignment between the image tokenizer and text modality, enabling task-level zero-shot capabilities through our proposed explanatory instructions. 6. Related Work Vision Task Understanding With the groundbreaking success of LLMs [8, 61] in the field of NLP which have unified language tasks, researchers are no longer satisfied with earlier proprietary vision models (where each model handles specific vision task). Instead, they seek to leverage the language modality to better understand the vision modality and associated vision tasks. Advancements in these VLMs [1, 12, 15, 31, 36, 89] have enabled substantial progress on vision-language tasks, achieving outstanding performance across diverse applications. However, early VLMs primarily produce textual outputs, which constrains their capacity to represent and manipulate visual information. To push beyond these limitations, researchers have explored integrating task-specific extensions such as expert models and downstream tools to enhance VLMs ability to handle diverse downstream vision tasks [18, 19, 21, 24, 37, 38, 69, 70]. However, such simple extensions can not enable models to generalize across computer vision tasks. As the pursuit of Artificial General Intelligence (AGI) continues to grow, there is an increasing demand for unified foundational models that can tackle diverse vision tasks and achieve zero-shot generalization at vision task level. Vision Task Generalization Although early VLMs have demonstrated some degree of generalization ability, it largely limited to visual question answering, where the question answering capability inherently possessed by LLMs. In the ongoing exploration to find unified approach for handling diverse vision tasks and enabling vision task-level generalization, Chameleon [60] is among the first to apply the same transformer architecture to sequences of both image and text tokens, enabling the generation of both text descriptions and image representations within the LLM paradigm. ShowO [72] also demonstrate unified models by combining text and image generation capabilities, while these models are limited to specific generation tasks. Lumina-mGPT [35], OmniGen [71], and PixWizard [34] then leverage fixed or open-style terminological instructions, employing either LLMs or flow-based Diffusion Transformers (DiT) [41] as foundational architectures to integrate wide range of vision tasks. Despite these advancements, these so-called vision generalist models still fail to exhibit vision task-level generalization. We infer the primary reason is that these models still rely on terminological instructions such as Semantic segmentation, which confine the model to perform only the specific tasks defined by these terms or closely related tasks (e.g., different forms of Image Restoration). In addition, the scope of these terminological instructions remain constrained and do not explain the true objective of vision tasks, far from the flexibility and richness of task expressions and objectives in the field of NLP. In this work, we move beyond the traditional notion of computer vision tasks and introduce explanatory instructions to explicitly articulate the objectives underlying different visual changes, forming an expansive set of unconstrained vision tasks that enable the model to achieve vision task-level generalization. 7. Conclusion significant body of research [8, 50, 61] has demonstrated that, with sufficient training data, models can exhibit zeroshot generalization capabilities. Building on these findings, this work goes beyond the traditional notion of vision tasks by introducing explanatory instructions to reveal the true objectives underlying various vision tasks. In constructing the 8 Dataset of Explanatory CV Tasks, we effectively captured and described diverse range of non-terminological vision tasks. Through straightforward fine-tuning experiments on vanilla token-based VLM, we showed that this approach enables the model to achieve both instruction-level and vision task-level zero-shot capabilities. Although the model still faces certain limitations, we believe this work represents step toward achieving broader zero-shot generalization in computer vision by leveraging explanatory instructions. This method overcomes the constraints of conventional vision task definitions and paves the way for more flexible and universally applicable unified generative models."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors would like to extend their sincere thanks to Xuhao Sun for his valuable discussions during the early stages of this work. Special thanks are also due to Mengxi Zhang and Chuyang Zhao for their contributions in reviewing and verifying the code. Additionally, we express our gratitude to the Chameleon team and the Lumina-mGPT team for their contributions to the open-source community."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikoł aj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. Flamingo: visual language model for few-shot learning. In Advances in Neural Inf. Process. Syst., pages 2371623736, 2022. 8 [2] Cosmin Ancuti, Codruta O. Ancuti, and Christophe De Vleeschouwer. D-HAZE: dataset to evaluate quantitatively dehazing algorithms. In Proc. IEEE Int. Conf. Image Process., pages 22262230, 2016. 3 [3] Codruta O. Ancuti, Cosmin Ancuti, and Radu Timofte. NH-HAZE: An image dehazing benchmark with nonIn IEEE Conf. homogeneous hazy and haze-free images. Comput. Vis. Pattern Recog. Worksh., pages 444445, 2020. 3 [4] Anthropic. Claude 3.5 sonnet, 2024. https://www. anthropic.com/news/claude-3-5-sonnet. 14 [5] Gwangbin Bae and Andrew Davison. Rethinking inductive biases for surface normal estimation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 95359545, 2024. 17 [6] Gwangbin Bae, Ignas Budvytis, and Roberto Cipolla. Estimating and exploiting the aleatoric uncertainty in surface normal estimation. In Proc. IEEE Int. Conf. Comp. Vis., pages 1313713146, 2021. 27 [7] Tim Brooks, Aleksander Holynski, and Alexei Efros. InstructPix2Pix: Learning to follow image editing instructions. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 18392 18402, 2023. 4, 25 [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Inf. Process. Syst., pages 18771901, 2020. 2, [9] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Frédo Durand. Learning photographic global tonal adjustment with database of input/output image pairs. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 97104, 2011. 3 [10] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. MaskGIT: Masked generative image transformer. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 11315 11325, 2022. 26 [11] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Singleimage depth perception in the wild. In Advances in Neural Inf. Process. Syst., pages 730738, 2016. 3 [12] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Qiao Yu, and Dai Jifeng. InternVL: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 2418524198, 2024. 8, 14 [13] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 12901299, 2022. 27 [14] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PALM: Scaling language modeling with pathways. J. Mach. Learn. Res., 24(240):1113, 2023. 5 [15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning, 2023. 9 [16] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. In Advances in Neural Inf. Process. Syst., pages 1982219835, 2021. 5 [17] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 1287312883, 2021. 4 [18] Hao Fei, Shengqiong Wu, Hanwang Zhang, Tat-Seng Chua, and Shuicheng YAN. VITRON: unified pixel-level vision LLM for understanding, generating, segmenting, editing. In Advances in Neural Inf. Process. Syst., 2024. https:// openreview.net/forum?id=kPmSfhCM5s. 8 [19] Yulu Gan, Sungwoo Park, Alexander Marcel Schubert, Anthony Philippakis, and Ahmed Alaa. InstructCV: Instructiontuned text-to-image diffusion models as vision generalists. In Proc. Int. Conf. Learn. Representations, 2024. 8, 27 [20] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. SEED-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 4 [21] Zigang Geng, Binxin Yang, Tiankai Hang, Chen Li, Shuyang Gu, Ting Zhang, Jianmin Bao, Zheng Zhang, Houqiang Li, Han Hu, Dong Chen, and Baining Guo. InstructDiffusion: generalist modeling interface for vision tasks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 1270912720, 2024. 8, [22] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: dataset for large vocabulary instance segmentation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 53565364, 2019. 3 [23] Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-Key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. 5 [24] Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, and Wei Liu. DialogGen: Multi-modal interactive dialogue system for multi-turn text-to-image generation. arXiv preprint arXiv:2403.08857, 2024. 8 [25] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. HQ-Edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. 4 [26] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 94929502, 2024. 27 [27] Boyi Li, Wenqi Ren, Dengpan Fu, Dacheng Tao, Dan Feng, Wenjun Zeng, and Zhangyang Wang. Benchmarking singleimage dehazing and beyond. IEEE Trans. Image Process., 28 (1):492505, 2018. [28] Boyun Li, Xiao Liu, Peng Hu, Zhongqin Wu, Jiancheng Lv, and Xi Peng. All-in-one image restoration for unknown corruption. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 1745217462, 2022. 27 [29] Chongyi Li, Chunle Guo, Wenqi Ren, Runmin Cong, Junhui Hou, Sam Kwong, and Dacheng Tao. An underwater image enhancement benchmark dataset and beyond. IEEE Trans. Image Process., 29:43764389, 2019. 3 [30] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel Ni, and Heung-Yeung Shum. Mask DINO: Towards unified transformer-based framework for object detection In Proc. IEEE Conf. Comp. Vis. Patt. and segmentation. Recogn., pages 30413050, 2023. 27 [31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proc. Int. Conf. Mach. Learn., pages 1973019742, 2023. 8 [32] Ming Li, Taojiannan Yang, Huafeng Kuang, Jie Wu, Zhaoning Wang, Xuefeng Xiao, and Chen Chen. ControlNet++: Improving conditional controls with efficient consistency feedback. In Proc. Eur. Conf. Comp. Vis., pages 129147, 2024. 25 [33] Ruoteng Li, Loong-Fah Cheong, and Robby Tan. Heavy rain image restoration: Integrating physics model and conditional adversarial learning. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 16331642, 2019. 3 [34] Weifeng Lin, Xinyu Wei, Renrui Zhang, Le Zhuo, Shitian Zhao, Siyuan Huang, Junlin Xie, Yu Qiao, Peng Gao, and Hongsheng Li. PixWizard: Versatile image-to-image visual assistant with open-language instructions. arXiv preprint arXiv:2409.15278, 2024. 2, 8, 25, 26, [35] Dongyang Liu, Shitian Zhao, Le Zhuo, Weifeng Lin, Yu Qiao, Hongsheng Li, and Peng Gao. Lumina-mGPT: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining. arXiv preprint arXiv:2408.02657, 2024. 2, 4, 5, 8, 25, 26, 27, 28, 40 [36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 8 [37] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu, Lei Zhang, Jianfeng Gao, and Chunyuan Li. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437, 2023. 8 [38] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang Yang, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen, Xue Yang, Xizhou Zhu, Yali Wang, Limin Wang, Ping Luo, Jifeng Dai, and Yu Qiao. InternGPT: Solving vision-centric tasks by interacting with chatgpt beyond language. arXiv preprint arXiv:2305.05662, 2023. 8 [39] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [40] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-IO: unified model for vision, language, and multi-modal tasks. In Proc. Int. Conf. Learn. Representations, 2022. 27 [41] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. 8, 25 10 [42] David Marr. Vision: computational investigation into the human representation and processing of visual information. MIT press, 2010. 2 [43] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 60386047, 2023. 25 [44] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2I-Adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proc. Conf. AAAI, pages 42964304, 2024. [45] OpenAI. Hello GPT-4o, 2024. https://openai.com/ index/hello-gpt-4o/. 13, 25, 26 [46] OpenGVLab. InternVL family: Closing the gap to commercial multimodal models with open-source suites pioneering open-source alternative to GPT-4o, 2024. https: //github.com/OpenGVLab/InternVL. 14 [47] Vaishnav Potlapalli, Syed Waqas Zamir, Salman Khan, and Fahad Shahbaz Khan. PromptIR: Prompting for all-in-one image restoration. In Advances in Neural Inf. Process. Syst., pages 7127571293, 2023. 27 [48] Can Qin, Shu Zhang, Ning Yu, Yihao Feng, Xinyi Yang, Yingbo Zhou, Huan Wang, Juan Carlos Niebles, Caiming Xiong, Silvio Savarese, Stefano Ermon, Yun Fu, and Xu Ran. Unicontrol: unified diffusion model for controllable visual generation in the wild. arXiv preprint arXiv:2305.11147, 2023. 25, [49] Qwen Team. Qwen2.5: party of foundation models, 2024. https://qwenlm.github.io/blog/qwen2.5/. 14 [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proc. Int. Conf. Mach. Learn., pages 87488763, 2021. 8 [51] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Proc. Int. Conf. Mach. Learn., pages 88218831, 2021. 4 [52] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In Advances in Neural Inf. Process. Syst., pages 1486614876, 2019. 4 [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 1068410695, 2022. 26 [54] Rico Sennrich. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. [55] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 5 [56] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu Edit: Precise image editing via recognition and generation tasks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 88718879, 2024. 25 [57] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In Proc. Eur. Conf. Comp. Vis., pages 746760, 2012. 26 [58] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 5 [59] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In Proc. Winter Conf. Applications of Comp. Vis., pages 21492159, 2022. [60] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models, 2024. arXiv preprint arXiv:2405.09818, 2024. 4, 8 [61] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 2, 8 [62] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, MarieAnne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. LLAMA 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 5 [63] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-toIn Proc. IEEE Conf. Comp. Vis. Patt. image translation. Recogn., pages 19211930, 2023. 25 [64] Jeya Maria Jose Valanarasu, Rajeev Yasarla, and Vishal Patel. TransWeather: Transformer-based restoration of images degraded by adverse weather conditions. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 23532363, 2022. 3 [65] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In Advances in Neural Inf. Process. Syst., pages 63066315, 2017. 4 [66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Inf. Process. Syst., pages 60006010, 2017. 11 [80] Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, and Jiebo Luo. PromptFix: You prompt and we fix the photo. arXiv preprint arXiv:2405.16785, 2024. 4 [81] Biao Zhang and Rico Sennrich. Root mean square layer In Advances in Neural Inf. Process. Syst., normalization. pages 1238112392, 2019. 5 [82] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. MagicBrush: manually annotated dataset for instructionguided image editing. In Advances in Neural Inf. Process. Syst., pages 3142831449, 2023. 4, 25 [83] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proc. IEEE Int. Conf. Comp. Vis., pages 38363847, 2023. 26 [84] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, and Ran Xu. HIVE: Harnessing human feedback for instructional visual editing. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 90269036, 2024. [85] Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. UltraEdit: Instruction-based fine-grained image editing at scale. arXiv preprint arXiv:2407.05282, 2024. 25 [86] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: 10 million image database for scene recognition. IEEE Trans. Pattern Anal. Mach. Intell., 40(6):14521464, 2017. 26 [87] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ADE20K dataset. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 633641, 2017. 3, 14, 26 [88] Yuqian Zhou, David Ren, Neil Emerton, Sehoon Lim, and Timothy Large. Image restoration for under-display camera. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 9179 9188, 2021. 26 [89] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 8 [67] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in images: generalist painter for in-context visual learning. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 68306839, 2023. [68] Mitchell Wortsman, Peter Liu, Lechao Xiao, Katie Everett, Alex Alemi, Ben Adlam, John Co-Reyes, Izzeddin Gur, Abhishek Kumar, Roman Novak, Jeffrey Pennington, Jascha Sohl-dickstein, and Kelvin Xu. Small-scale proxies for large-scale transformer training instabilities. arXiv preprint arXiv:2309.14322, 2023. 5 [69] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual ChatGPT: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 8 [70] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Wenhai Wang, Zhe Chen, Xizhou Zhu, Lewei Lu, Tong Lu, Ping Luo, Yu Qiao, and Jifeng Dai. VisionLLM v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. arXiv preprint arXiv:2406.08394, 2024. 8 [71] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Shuting Wang, Tiejun Huang, and Zheng Liu. OmniGen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. 2, 8, 25, 26 [72] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-O: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 8 [73] Peng Xing, Haofan Wang, Yanpeng Sun, Qixun Wang, Xu Bai, Hao Ai, Renyuan Huang, and Zechao Li. Csgo: Contentstyle composition in text-to-image generation. arXiv preprint arXiv:2408.16766, 2024. [74] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In Proc. Int. Conf. Mach. Learn., pages 1052410533, 2020. 5 [75] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv preprint arXiv:2406.09414, 2024. 17, 27 [76] Wenhan Yang, Robby Tan, Jiashi Feng, Jiaying Liu, Zongming Guo, and Shuicheng Yan. Deep joint rain detection and removal from single image. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages 13571366, 2017. 26 [77] Wenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang, and Jiaying Liu. Sparse gradient regularized deep retinex network for robust low-light image enhancement. IEEE Trans. Image Process., 30:20722086, 2021. 26 [78] Hanrong Ye and Dan Xu. Inverted pyramid multi-task transformer for dense scene understanding. In Proc. Eur. Conf. Comp. Vis., pages 514530, 2022. 27 [79] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas Huang. Free-form image inpainting with gated In Proc. IEEE Int. Conf. Comp. Vis., pages convolution. 44714480, 2019. 12 A. Details for the Dataset of Explanatory CV Tasks A.1. Describing or Generating Explanatory Instructions For certain terminological-based vision tasks (e.g., Detection and Segmentation), we selected subset of data and manually crafted descriptions based on the category and other label information. For other data, explanatory instructions were generated by constructing prompts and leveraging GPT-4o [45] for automated description generation. For the manually crafted descriptions, we utilized human-written expressions alongside GPT-4o-generated outputs, applying rule-based combinations to produce over 5,000 unique forms of expression. Specifically, the entire Object Detection data, as well as the majority of data for Segmentation, HED Boundary to Image, Surface Normal Estimation, Depth Estimation, and their inverse tasks, were constructed through this approach. For the GPT-4o-generated descriptions, early experiments utilized gpt-4o-2024-05-13 to construct explanatory instructions. Later, for simpler sections of terminological-based vision tasks (e.g., HED Boundary to Image, Deraining, Dehazing, Desnowing), we employed gpt-4o-2024-08-06. For all other tasks, we used chatgpt-4o-latest to generate the instructions. The prompt design is summarized as follows (contents within {**} and {==} are optional while content in {**} is activated only when human evaluation deems GPT-4os descriptions inaccurate): 1 response = client.chat.completions.create( 2 3 4 5 7 8 9 10 11 13 14 15 16 17 19 model = \"chatgpt-4o-latest\",#gpt-4o-2024-05-13, gpt-4o-2024-08-06 response_format = {\"type\": \"json_object\"}, messages = [ { { \"role\": \"system\", \"content\": \"You are an expert in computer vision and describe vison tasks, with exceptional attention to detail. Your task is to provide detailed descriptions of the transformations between the images, explaining how elements appear, disappear, or change. Your analysis is thorough, accurate, and insightful.\"}, \"role\": \"user\", \"content\": [ { \"type\": \"text\", \"text\": \"{*We are currently working on tasks related to ...*} Define the first image as A, the second image as B. Task: {=1) Describe these images.=} 2) Describe 2 scenarios: how to transform image into image B, image into image without referencing the contents of the other image directly. Output format: JSON format with keys only contain {=Image_A_Caption, Image_B_Caption,=} Task_Descriptions_from_A_to_B, Task_Descriptions_from_B_to_A without any nested JSON structures. The descriptions of transformations should be as diverse as possible, either as single paragraph or as step-by-step description. Constraints: 1) The task descriptions should not use any additional tools or references, such as image editing tools. 2) Do not use terms like image A, image B, to transform * into *, or similar phrases.\" }, { \"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/jpeg;base64,{base64_image_A}\", \"detail\": \"high\" 13 } }, { \"type\": \"image_url\", \"image_url\": { \"url\": f\"data:image/jpeg;base64,{base64_image_B}\", \"detail\": \"high\" } } ] } ], max_tokens = 4096 21 22 23 24 25 27 28 29 30 31 33 ) In addition, we experimented with several other models (e.g., InternVL2-8B76B [12, 46], QWen2.5 [49], Claude-3.5Sonnet [4], etc.) to generate explanatory instructions. However, to minimize potential biases in the generated explanatory instructions that could affect validation experiments, we ultimately selected GPT-4o. comparison of selected models using the first image from the ADE20K Validation Set [87] is provided below (Table 1 Table 4). Content that does not meet the description requirements is highlighted in red. Caption of image A: 1. Traditional stone building with steeply pitched red-tiled roof. 2. Multiple chimneys and dormer windows. 3. Large white-framed windows. 4. Well-kept lawn in front. 5. Stone wall in front of the building. 6. Clear blue sky indicating sunny day. Caption of image B: 1. The same building with specific modifications. 2. Building and extension shaded with semi-transparent reddish hue. 3. Sky colored bright blue. 4. Lawn colored vivid green. 5. Stone wall highlighted with grayish color. 6. Bush to the right of the stone wall colored yellow. Explanatory instruction from to B: 1. Color the Sky: 1) Select the sky area. 2) Fill the selected area with bright, distinct color to make the sky stand out. 2. Shade the Building: 1) Select the entire building and its extension, including the roof and chimneys. 2) Apply semi-transparent hue over the selected area to give it shaded appearance while still allowing the underlying details to be visible. 3. Color the Lawn: 1) Select the lawn area in front of the building. 2) Fill this area with bright, vibrant color to emphasize the grassy area. 4. Highlight the Stone Wall: 1) Select the stone wall in front of the building. 2) Apply distinct, muted color to the wall to make it stand out from the lawn and building. 5. Color the Bush: 1) Identify and select the bush or shrub to the right of the stone wall. 2) Fill this area with bright, distinct color to differentiate it from the surrounding elements. Explanatory instruction from to A: 1. Restore the Sky: 1)Select the area of the sky that has been filled with bright, distinct color. 2) Replace the bright color with natural sky color, making it appear more realistic. 2. Remove the Building Shading: 1) Select the area of the building and its extension shaded with semi-transparent hue. 2) Remove the shading to restore the buildings natural appearance, showing its original material and color. 3. Restore the Lawn Color: 1) Select the area of the lawn that has been filled with bright, vibrant color. 2) Replace the bright color with natural grass color to make it look realistic. 4. Remove the Highlight from the Stone Wall: 1) Select the stone wall that has been highlighted with distinct, muted color. 2) Remove the highlighting to return the stone wall to its natural appearance. 5. Restore the Bush Color: 1) Select the bush or shrub to the right of the stone wall that has been colored with bright, distinct color. 2) Replace the bright color with natural color to make it look realistic and blend in with the surrounding environment. Table 1. Examples of outputs from gpt-4o-2024-05-13. 14 Caption of image A: The image shows large, two-story stone house with red-tiled roof. The house has multiple chimneys and windows, some of which are dormer windows protruding from the roof. The building is surrounded by wellmaintained green lawn, and there is stone wall in the foreground. The sky is clear and blue. Caption of image B: The image shows large, two-story stone house with red-tiled roof. The house has multiple chimneys and windows, some of which are dormer windows protruding from the roof. The building is surrounded by wellmaintained green lawn, and there is stone wall in the foreground. The sky is clear and blue, but there is noticeable green overlay on the right side of the house and lawn. Explanatory instruction from to B: 1: Change the sky color to solid bright blue. 2: Add green overlay to the grass area. 3: Add green overlay to the ivy on the right side of the building. 4: Add green overlay to the bushes on the left side of the building. Explanatory instruction from to A: 1: Change the sky color back to natural blue with some cloud details. 2: Remove the green overlay from the grass area. 3: Remove the green overlay from the ivy on the right side of the building. 4: Remove the green overlay from the bushes on the left side of the building. Table 2. Examples of outputs from InternVL2-76B. Caption of image A: This image shows large stone farmhouse or manor house in rural setting. The building has multiple stories with dormer windows on the roof. It features brick chimneys and mix of stone and brick construction. Theres low stone wall in the foreground, and well-maintained lawn area. The sky is clear and blue, indicating sunny day. Some trees or vegetation can be seen in the background. Caption of image B: This image appears to be digitally altered version of Image A. The basic structure and layout of the building remain the same, but the colors have been dramatically changed. The sky is now vibrant, unnatural turquoise blue. The grass in the foreground is bright, artificial-looking green. Theres yellow-green patch on the right side of the building that looks like it could be meant to represent ivy or some other climbing plant, but in highly stylized way. The overall effect is surreal and cartoon-like. Explanatory instruction from to B: To transform Image into something resembling Image without referencing Bs content: 1. Increase the color saturation of the entire image dramatically. 2. Change the sky to bright, unnatural shade of blue. 3. Alter the grass color to vivid, artificial-looking green. 4. Add bright, contrasting color to one side of the building, perhaps representing vegetation in an exaggerated way. 5. Simplify the textures and details to create more flat, cartoon-like appearance. Explanatory instruction from to A: To transform Image into something resembling Image without referencing As content: 1. Reduce the overall color saturation of the image significantly. 2. Adjust the sky color to more natural, pale blue tone. 3. Change the grass to more muted, realistic shade of green. 4. Remove any unnaturally bright or contrasting colors from the building and surroundings. 5. Enhance the texture and details of the building to create more realistic, photographic appearance. Table 3. Examples of outputs from claude-3-5-sonnet-20240620. 15 Caption of image A: The image captures serene scene of traditional English country house, nestled amidst lush green lawn. The house, painted in pristine white, stands out against the clear blue sky. It features prominent red brick chimney, adding touch of rustic charm to its appearance. The house is adorned with multiple windows, each framed by white shutters, allowing ample natural light to filter into the rooms within. large wooden door, painted in contrasting dark brown color, serves as the main entrance to the house. stone wall encircles the property, providing privacy and security. The wall is punctuated by gate, inviting visitors into the tranquil haven beyond. The house is surrounded by an expanse of well-maintained grass, creating sense of openness and tranquility. Despite the detailed description, theres no text visible in the image, keeping the focus solely on the visual elements. The relative positions of these objects suggest well-planned layout, with the house at the center, the stone wall forming boundary around it, and the gate marking the entry point. Overall, the image presents picturesque view of typical English countryside setting, where nature and architecture coexist harmoniously. Caption of image B: This image showcases traditional English country house, set against backdrop of clear blue sky. The house, constructed from red brick, boasts prominent chimney and large wooden door. The roof, covered in tiles, adds touch of rustic charm to the overall structure. The house is situated on lush green lawn, which is enclosed by stone wall. This wall not only provides boundary but also enhances the aesthetic appeal of the property. gate can be seen in the wall, possibly serving as the entrance to the property. In terms of object count, theres one house, one chimney, one door, and one gate. The relative position of these objects suggests well-planned layout - the house is centrally located, the chimney is attached to the house, the door is part of the house, and the gate is integrated into the stone wall surrounding the property. Theres no text present in the image, keeping the focus solely on the visual elements. The image does not provide any information about the actions or movements of the objects, as they are stationary structures. Overall, the image presents picturesque view of typical English countryside setting, where nature and architecture coexist harmoniously. Explanatory instruction from to B: Adjust Color Scheme: Begin by altering the color palette of the main building. You could introduce warmer hues to the exterior, perhaps changing the white walls to soft cream or adding accents of natural stone colors to give it different character. Modify Architectural Features:Modify architectural elements such as the chimney and windows. For instance, you might extend the chimney height, add decorative brickwork, or replace the window shutters with functional window boxes filled with blooming flowers. Landscape Transformation: Redesign the surroundings by introducing garden path leading up to the house or adding small pond near the entrance. Change the lawn to incorporate more diverse flora, like flower beds or shrubberies. Add or Remove Structures: Introduce detached garage or garden gazebo in the background, or conversely, remove existing elements like the stone wall to open up the view and create more expansive landscape. Lighting and Atmospheric Effects: Adjust the lighting to simulate different time of day, such as sunset or twilight, and add atmospheric effects like mist or rain to alter the mood of the scene. Explanatory instruction from to A: Reverse Color Alterations: Restore the original color palette of the building, possibly reverting any warm tones added in the previous transformation back to cooler shades, like the classic white or more subtle hue that contrasts with the greenery. Revert Architectural Modifications: Remove any additional architectural features introduced, such as decorative chimney work or new window features, restoring the building to its simpler, more traditional form. Revive Original Landscape: Return the landscaping to its initial simplicity, eradicating any new paths, ponds, or additional structures like gazebos that were added. Reinstate the uniform green lawn and, if removed, rebuild the enclosing stone wall. Alter Atmospheric Conditions: Adjust the lighting and weather conditions to their prior state, perhaps brightening the scene to reflect midday sunlight or clearing away any atmospheric effects like fog or rain. Restore Original Scene Balance: Ensure all changes collectively bring the scene back to state reminiscent of the original, while not precisely replicating Image A, focusing on balancing the composition and elements to achieve similar ambiance. Table 4. Examples of outputs from Qwen2.5. 16 A.2. Generating Data Samples For subset of terminological-based vision tasks, we employed existing methods to generate image pairs. HED Boundary Detection & HED Boundary to Image. We use HED boundary detection to generate approximately 10k edge-image pairs from randomly selected images in other datasets. Image Depth Estimation & Depth Map to Image. We used the Depth Anything V2 model [75] and obtain about 250k depth-image pairs. Original images are randomly selected from other datasets. Surface Normal Estimation & Normal Map to Image. We generated the surface normal maps by using DSINE [5]. We obtain around 250k normal-image pairs. Original images are randomly selected from other datasets. A.3. More Data Samples for the Dataset of Explanatory CV Tasks In this section, we provide more data samples for the Dataset of Explanatory CV Tasks. A.3.1. More Data Samples for Explanatory-based Vision Tasks More data samples for Explanatory-based Vision Tasks are provided in Fig. 9 16. Explanatory instruction from to B: The dress changes from having long sleeves with ribbons to having off-the-shoulder sleeves. The color of the dress shifts from deep purple to lighter shade with satin finish. The hairstyle transitions from braided and tied back to hair let down with tiara. Gloved hands shift to being ungloved with visible change in the hand accessory. Socks become part of the footwear, altering from lace-up shoes to white socks. The expression shifts from side-facing to front-facing stance with visible smile. Explanatory instruction from to A: Adjust the dress design from off-the-shoulder with satin to one featuring long sleeves with ribbon details. Modify the dress color from light, shiny appearance to deep purple tone with more textured fabric. Change the hairstyle from loose hair with tiara to braided style pulled back. Introduce gloves on the hands, replacing the bare hand appearance. Transition the socks to dark shoes, altering the footwear style. Shift the facial expression from smiling, forward-facing look to neutral, side-facing demeanor. Figure 9. Data sample for explanatory-based vision tasks. Explanatory instruction from to B: Shake up the perspective to bring the animal slightly closer, reposition upright with slight turn of the head. Reduction of debris gives the ground smoother, even appearance. Explanatory instruction from to A: Adjust the animals posture to more upright sitting position, shifting the angle to reveal more surroundings. Enhance the scene with additional greenery and scattered natural elements. Figure 10. Data sample for explanatory-based vision tasks. 17 Explanatory instruction from to B: Reposition the vase so that it is more upright, causing the flowers to shift slightly to the left. Adjust the angle of the sunlight so that shadows fall more prominently to the bottom right, emphasizing more elongated shadow of the floral arrangement. Rotate the floral elements along their central axis, making each petal and leaf face slightly different directions while maintaining their original colors and textures. Explanatory instruction from to A: Tilt the vase towards the viewer, altering the flowers positions to the right. Modify the lighting to change the direction of the shadows, creating more compact and less defined shadow below the arrangement. Slightly adjust the orientation of the petals and leaves, ensuring they align in different configuration yet retain their original forms and hues. Figure 11. Data sample for explanatory-based vision tasks. Explanatory instruction from to B: Brighten the overall scene, adding more sunlit ambiance. Modify the gripping object to reflect natural, green element while altering the environment to more elevated, branch-like setting. Explanatory instruction from to A: Soften the vivid warmth by introducing cooler tones. Exchange the greenery for different object to hold, and replace the leafy background with consistent, textured surface. Figure 12. Data sample for explanatory-based vision tasks. Explanatory instruction from to B: Replace the white square base with large wooden board. Display the spheres in less strict arrangement. Incorporate garlic bulbs and scattering of spices in the background, complemented by soft cloth textures. Explanatory instruction from to A: Shift the spheres onto neat, white square template. Simplify the herb decoration to single leaf per sphere and clean up the overall setting for minimalist and fresh appearance. Figure 13. Data sample for explanatory-based vision tasks. 18 Explanatory instruction from to B: Add crowd at the bottom with open hands raised towards the sky, and increase the number of lanterns, making them appear smaller as they move upwards. Darken the background slightly and include distant bright light at the horizon. Explanatory instruction from to A: Remove the crowd and their raised hands from the bottom, leaving only the lanterns. Brighten individual lanterns and make the scene more clustered with fewer visible lanterns. Figure 14. Data sample for explanatory-based vision tasks. Explanatory instruction from to B: Introduce background featuring variety of green leaves and stems, while adding another flower similar to the first one. Ensure the new flower has more fully opened appearance and slightly different arrangement of petals. Explanatory instruction from to A: Focus on single flower, removing the secondary bloom and simplifying the background to emphasize the central floral subject. Strip away diverse leaves and stems, leaving plain backdrop to highlight the singular floral element. Figure 15. Data sample for explanatory-based vision tasks. Explanatory instruction from to B: Adjust the color scheme from blue-green light spectrum to combination of red and green, creating shift in the color dynamics. Alter the central projection, ensuring that the beam patterns reflect less linearity and more concentric circular formations on the surface. Intensify the red hues across the visible light paths and enhance the circular designs with stronger emphasis on the differences in color, resulting in more elaborate geometric patterns. Explanatory instruction from to A: Change the predominant color from red-green to blue-green spectrum, modifying the overall illumination to convey cooler tones. Modify the light projections on the surface, transitioning from circular formations to design where the beams are more linear and directed from the center point outward. Reduce the complexity of the geometric patterns, focusing on the linear symmetry, and diminish the red hues, emphasizing blues with vibrant green accent. Figure 16. Data sample for explanatory-based vision tasks. 19 A.3.2. More Data Samples for Terminological-based Vision Tasks More data samples for Terminological-based Vision Tasks are provided in Fig. 17 31. Explanatory instruction from to B: Enhance the overall saturation and vibrancy of the image by increasing color intensity, bringing out more vivid blue sky and deepening the reds in the earth tones. Adjust the contrast to sharpen the details of the rocky formations. Explanatory instruction from to A: Reduce the saturation and brightness to subdue the colors, reverting the image to more natural state. Soften the contrast to give the image more balanced and moderate appearance. Figure 17. Data sample for terminological-based vision tasks (Image Restoration). Explanatory instruction from to B: Enhance color saturation while maintaining the current brightness. Emphasize cooler tones for more vibrant and lively appearance. Explanatory instruction from to A: Reduce color saturation to achieve more neutral tone. Neutralize cooler tones to recreate the initial natural look. Figure 18. Data sample for terminological-based vision tasks (Image Restoration). Explanatory instruction from to B: Drain the image of all bright colors, shifting the mood into sepia-like monochrome. The dragon figure remains, but now weathered and eroded, appearing as if composed of twisted, dried-out textures. The background morphs dramatically as the lush flora converts into barren, tangled form entirely overtaken by shadows and creeping vines. The resulting atmosphere becomes dark and decayed, contrasting the lively, colorful origin. Explanatory instruction from to A: Reimagine the desolate, worn textures by replacing them with lively, bright tones. Breathe life and energy back into the figure by restoring its facial features, sharpening its horns, and returning its surroundings to thriving, colorful forest, full of sharp contrasts and distinctly separated flora. Soften the twisted appearance of the dragon, allowing its scales to return to more uniform, sleek form, where bold colors and vivid contrasts dominate the atmosphere. Figure 19. Data sample for terminological-based vision tasks (Style Transfer). 20 Explanatory instruction from to B: Maintain the layout but adjust the overall structure by shifting the visual style to more artistic, painted form. The material should evolve into representation with stronger outlines and exaggerated contrasts between shadows and highlights. All elements transition to bold blocks of color and simplified features, giving the appearance of something more abstract, yet still retaining the core elements of the original composition. Explanatory instruction from to A: Soften all of the rough, exaggerated lines and deep hues present in the scene. Gradually smooth out all blocks of visual complexity forming natural color transitions between shadows and bright areas. Pull the bright elements into more neutral, natural tone, eliminating hard contrasts. Return the visuals to lifelike depiction, but keep the core layout intact. Figure 20. Data sample for terminological-based vision tasks (Style Transfer). Explanatory instruction from to B: For the following object categories, apply the corresponding solid color overlays to fully cover them: Color every wineglass object with aqua solid layer. Paint each fork with linen solid color fill. Color each banana object with darkolivegreen solid overlay. Paint each spectacles with purple solid color fill. Apply solid lawngreen color overlay to fully cover all scarf objects. Explanatory instruction from to A: Delete the solid color covering the objects, restoring their original look. Figure 21. Data sample for terminological-based vision tasks (Segmentation). Explanatory instruction from to B: Paint over every clock, streetlight object with dodgerblue, covering them entirely. Explanatory instruction from to A: Erase the dodgerblue overlay on the clock, streetlight objects, restoring the original appearance. Figure 22. Data sample for terminological-based vision tasks (Segmentation). Explanatory instruction from to B: Draw floralwhite bounding box around all apron objects. Explanatory instruction from to A: Remove all bounding boxes from the objects, restoring the original image. Figure 23. Data sample for terminological-based vision tasks (Object Detection). Explanatory instruction from to B: Remove the raindrop patterns that obscure sections of the scene to reveal clear, unobstructed view. Enhance the visibility of details by eliminating blurred spots. Increase the sharpness and contrast to accentuate the overall clarity, ensuring all objects and surfaces are distinctly visible. Explanatory instruction from to A: Introduce raindrop patterns on the lens to create areas of distortion and blur across the scene, simulating rainy effect. Soften certain details by incorporating these smudged patterns, which partially obscure parts of the view. Decrease the contrast and sharpness to mimic the presence of rain on the camera lens. Figure 24. Data sample for terminological-based vision tasks (Deraining). Explanatory instruction from to B: Introduce layer of fog that softly envelops the scenery, generating slightly diffused appearance with reduced contrast. Increase the density of mist, particularly around the lower parts of the tree and the ground, to create moody and mysterious ambiance. Soften the outlines of distant elements and elements in the foreground, diminishing the clarity to simulate hazy atmosphere. This results in muted color palette, lending an ethereal quality to the overall visual experience. Explanatory instruction from to A: Reduce the visibility of mist around the scene by gradually decreasing the density of fog, improving the clarity and sharpness of the landscape. Carefully remove any diffused light scattering, allowing the intricate details of the surrounding environment to emerge more distinctly. Ensure that the distant elements become more defined and that the colors appear more saturated and less muted, providing an unobstructed view of the complete scene. Figure 25. Data sample for terminological-based vision tasks (Dehazing). 22 Explanatory instruction from to B: Reduce the visibility of falling particles obscuring the background by removing white speckles scattered across the scene. Clarify the view by eliminating opaque overlays, focusing on enhancing the details of the distant objects and sky. Enhance the overall brightness and contrast to highlight the serene atmosphere with unobstructed sightlines. Explanatory instruction from to A: Introduce layer of translucent white specks distributed unevenly across the scene to simulate snowy condition. Gradually decrease the clarity of distant elements by overlaying semitransparent textures while maintaining the overall composition. Weave sense of dynamic motion through the addition of irregular shapes and lighter shades throughout the field, mimicking flurry of snowfall. Figure 26. Data sample for terminological-based vision tasks (Desnowing). Explanatory instruction from to B: Simplify the scene into grayscale silhouette, emphasizing the outlines and contours of prominent subjects while eliminating detailed features and textures. Fade out background patterns into smooth gradient to suggest depth. Focus on transitioning the entire scene into varying shades of gray, concentrating on the key shapes and forms. Explanatory instruction from to A: Introduce complex patterns and vibrant colors to the scene, adding texture and intricate details to key subjects. Enhance foreground subject clarity, incorporating color variations and shading. Overlay the background with rich details and vivid patterns to create more detailed and nuanced appearance, emphasizing depth with contrasting hues. Figure 27. Data sample for terminological-based vision tasks (Depth Estimation). Explanatory instruction from to B: Introduce series of vividly colored dots and lines across the canine figure. Each line should connect to form an intricate pattern that traces the outline and features of the animal, adding whimsical and abstract design element. Explanatory instruction from to A: Remove all the colored dots and lines overlaying the animal. Preserve the misty background along with the tree and the canine, ensuring the environment exudes quietude and mysticism. Figure 28. Data sample for terminological-based vision tasks (Pose Estimation). 23 Explanatory instruction from to B: Focus solely on the anatomical framework by removing all environmental and bodily details, leaving only colorful joint lines on dark canvas to abstractly outline movement. Explanatory instruction from to A: Reintroduce the snowy environment, captured with deer in motion, and omit the line structure to display natural scene of winter and untouched wildlife activity. Figure 29. Data sample for terminological-based vision tasks (Pose Estimation). Explanatory instruction from to B: Apply transformation that generalizes the depiction of depth and orientation using colorful gradient representing angles across the surface. Structures become abstract with smooth transitions between colors, translating the detailed textural elements into flat gradients indicating spatial variations. The skyline should shift into simplified forms with no explicit textures, instead presenting undulating colors to denote geometric relationships. Emphasize prominent architectural features by focusing on their orientation, using color to suggest their form without explicit details. The luminance of the scene should be replaced with vibrant, multi-hued representation that suggests the directionality and depth of each element. Explanatory instruction from to A: Introduce detailed textural elements and realistic color scheme over the abstract tonal surfaces, replacing the gradient with distinct textures that convey material properties and illumination. Reconstruct the background to include various light sources and urban details, accentuating the contrast and natural colors found on physical architectures. Re-establish the shadows and highlights to convey depth using realistic lighting that defines the shapes explicitly and clearly. Transition the simplified forms into detailed and distinct buildings, featuring precise windows, lighting, and signs that illustrate an urban setting at night, capturing the variations in illumination and environment. Figure 30. Data sample for terminological-based vision tasks (Surface Normal Estimation). Explanatory instruction from to B: Highlight only major edges, transforming the image into boundary map. Explanatory instruction from to A: Convert boundary outlines into realistic image, applying textures and colors. Figure 31. Data sample for terminological-based vision tasks (HED Boundaries). B. Results, Details and Samples for Quantitative Experiments B.1. Quantitative Experiments Training settings follow Sec. 4.1 of the paper. Table 5. Comparisons with image-editing baselines evaluated on the Emu Edit and MagicBrush test set. - indicates that the method does not report the corresponding results. CLIPim CLIPout Methods InstructPix2Pix [7] MagicBrush [82] PnP [63] Null-Text Inv. [43] Emu Edit [56] UltraEdit [85] OmniGen [71] PixWizard [34] Lumina-mGPT [35] Ours 0.834 0.838 0.521 0.761 0.859 0.844 0.836 0.845 0.815 0.821 Emu Edit test set ℓ1 0.121 0.100 0.304 0.075 0.094 0.071 - 0.069 0.149 0.132 0.219 0.222 0.089 0.236 0.231 0.283 0.233 0.248 0.283 0.286 DINO CLIPim CLIPout 0.837 0.762 0.883 0.776 0.568 0.153 0.752 0.678 0.897 0.819 0.868 0.793 - 0.804 0.884 0.798 0.855 0.735 0.875 0.768 MagicBrush test set ℓ1 0.093 0.058 0.289 0.077 0.052 0.088 - 0.063 0.114 0.093 0.245 0.261 0.101 0.263 0.261 - - 0.265 0.282 0. DINO 0.767 0.871 0.220 0.664 0.879 0.792 - 0.876 0.786 0.831 B.1.1. Image Editing Results We evaluate the fine-tuned AR-based VLM on two image editing benchmarks, i.e., the MagicBrush test set [82] and the Emu Edit test set [56], which include several human-defined operations, e.g., background alteration, object removal, object addition, localized modifications, etc. In alignment with the evaluation metrics used both by Emu Edit and MagicBrush, we measure four metrics: 1) CLIPim: CLIP image similarity between source image and output image; 2) CLIPout: CLIP text-image similarity between edited image and target caption; 3) ℓ1: ℓ1 distance between source image and output image; 4) DINO: DINO similarity between source image and output image. As shown in Table 5, compared to instruction-guided image editing methods such as InstructPix2Pix [7], PnP [63], and Null-Text Inv.[43], the straightforward fine-tuned AR-based VLM demonstrates certain advantages. However, when compared to more advanced methods such as Emu Edit[56], UltraEdit [85], and recent VLMs employing flow-based Diffusion Transformers (DiT) [41] as backbones (e.g., OmniGen [71] and PixWizard [34]), the performance of our model still shows noticeable gap. Nevertheless, compared to vanilla token-based VLM (i.e., Lumina-mGPT [35]), the fine-tuned model achieves significant performance improvements (Chameleon is excluded due to the lack of image generation capability). B.1.2. Image Generation Results We then evaluate the effectiveness of generation capabilities. We assess its performance across three tasks: controllable image generation (including Canny-to-Image and HED-to-Image), Inpainting, and Outpainting. Controllable Image Generation We adopt the evaluation split of MultiGen-20M for Canny-to-Image and HED-to-Image condition. As MultiGen-20M [48] do not provide image captions in general nature language format and the dataset is not used during training, we use GPT-4o [45] to recaption them. We also provide unseen explanatory instructions for each controllable generation data pair (the construction method for explanatory instructions can refer to Appendix A.1) to evaluate our models instruction-level zero-shot capability. Following ControlNet++ [32], we assess controllability by measuring the similarity between the input conditions and the extracted conditions from the generated images. We use F1-Score for Canny-to-Image and SSIM for HED-to-Image. We adopt FID and CLIP-Score to evaluate the quality of the generated images and their alignment with caption. The resolution of generated images is 448 448 but are resized to 512 512 for evaluation (output resolution for Lumina-mGPT is 512 512). Results are shown in Table 6. As both Canny Edge Detection and Canny-to-Image tasks are not seen during training, results for Canny-to-Image represent the more challenging task-level zero-shot setting. Instructions for Lumina-mGPT used fixed format (Lumina-mGPT does not work if change the instruction format, details can refer to Appendix B.2). However, evaluate instructions for ours are unseen during training. While the results still fall short of those achieved by task-specific or so-called vision generalist models, it demonstrates significant 25 Table 6. Comparisons with task-specific / vision generalist baselines across four representative tasks. Our model adopts unseen explanatory instructions during the inference stage. We use GPT-4o [45] to recaption images in the MultiGen-20M [48] validation set as the original dataset do not provide captions in general nature language format. Scores in the format of / indicate CLIP-Score for previous / recaption images and texts. indicates that the method is incapable of performing the task. - indicates that the method does not report the corresponding results. T. Z.-s. denotes task-level zero-shot, i.e., both the task and instructions are not seen during training. I. Z.-s. denotes instruction-level zero-shot, i.e., the instructions are not seen during training. ControlNet-SD1.5 [83] 34.65 14.73 T2I-Adapter-SD1.5 [44] 23.65 15."
        },
        {
            "title": "Original Images",
            "content": "LDM-4 [53] LaMa [59] DeepFill v2 [79] MaskGIT [10] OmniGen [71] PixWizard [34] Lumina-mGPT [35] Ours [T. Z.-s.] Canny-to-Image [I. Z.-s.] HED-to-Image [T. Z.-s.] Inpainting [T. Z.-s.] Outpainting F1 FID CLIP-S MultiGen-20M SSIM FID CLIP-S FID MultiGen-20M LPIPS FID"
        },
        {
            "title": "Places",
            "content": "IS 0.7621 15.41 100.00 0.00 31.99/33.13 1.0000 0.00 31.99/33.13 0.00 9.39 12.00 - - - 9.27 42.69 15.16 35.54 35.46 15.76 10.09 61.65 20.69 26.93 32.15 31.71 - 32.01 25.33 27.16 32.33 - - - 25.97 29. - 0.8237 - 0.1834 69.49 0.6561 15.91 - - - - 0.00 0.25 0.24 - - - 0.25 0.77 0.48 0.00 - - 11.51 7.80 - 7.54 42.66 15. 27.70 - - 17.70 22.95 - 22.18 11.28 17.20 progress compared to previous models, which were largely incapable of zero-shot generalization at either the instruction level or the task level. Inpainting and Outpainting For image inpainting, we use random rectangle or circle to mask 40%-50% of the image area and measure FID and LPIPS to assess the quality of the generated images. For image extrapolation (outpainting), we follow MaskGIT [10] settings, i.e., extending the image by 50% to the right and use FID and Inception Score (IS) to assess the quality of the generated images. We do not use image captions that directly describe the output images but construct unseen explanatory instructions during inference stage for our model (details can refer to Appendix A.1). Both the inpainting task and the outpainting task are not seen during training (but inpainting may have task-level overlap with Segmentation-to-Image). We evaluated on 10,000 image crops from the Places dataset [86] (training set for the Places dataset are not used during our training stage). The size of generated images is 448 448 but we resize images to 512 512 for evaluation (output resolution for Lumina-mGPT is 512 512). As shown in Table 6, although the results of our generated images still show gap compared to other task-specific or vision generalist models, we represent significant breakthrough in zero-shot capability. B.1.3. Results for Dense Image Prediction and Low-level Tasks Dense Image Prediction We conduct Semantic Segmentation experiments on the ADE20K validation set [87], while Depth estimation and Surface Normal Estimation experiments on the NYU-Depth V2 dataset [57]. For Semantic Segmentation, since we use color to control category generation and managing all categories simultaneously leads to higher color inaccuracies, we test each category individually during the evaluation stage. This approach yields results higher than conventional evaluation methods, and the result is provided as reference only. Accuracy is evaluated using the Mean Intersection over Union (mIoU) metric. For the monocular Depth Estimation task, we adjust the depth values of the generated image to fall within the range of zero to ten meters. Accuracy is measured using the Root Mean Square Error (RMSE). For the Surface Normal Estimation task, we recover the corresponding normal vectors from the output image and assess accuracy using the Mean Angle Error metric. For both Depth Estimation and Surface Normal Estimation tasks, we employ unseen explanatory instructions during evaluation to guide the model toward the target objective. Low-level Tasks We conduct Deraining experiments on the Rain100L [76] dataset and Dehazing experiments on the SOTS [27] dataset, both using unseen explanatory instructions during the evaluation stage. Subsequently, we perform task-level zero-shot evaluations on under-display camera (UDC) Image Restoration task using the UDC (T-OLED) dataset [88] and on Low-light Enhancement task using the LOLv2 dataset [77]. Similarly, the explanatory instructions used during evaluation are unseen during the training phase. We evaluate performance using PSNR and SSIM as distortion metrics. Table 7. Comparison with task-specific and vision generalist baselines across seven dense image prediction or low-level tasks. Our model adopts unseen explanatory instructions during the inference stage. indicates the result is higher than typically expected under standard evaluation method and is provided for reference only. indicates that the method is incapable of performing the task. - indicates that the method does not report the corresponding results. T. Z.-s. denotes task-level zero-shot, i.e., both the task and instructions are not seen during training. I. Z.-s. denotes instruction-level zero-shot, i.e., the instructions are not seen during training. [I. Z.-s.] Depth Est. Semantic Seg. [I. Z.-s.] Surf. Norm. Est. [I. Z.-s.] Derain [I. Z.-s.] Dehazing [T. Z.-s.] (UDC)IR [T. Z.-s.] Low-light Enh. PSNR SSIM PSNR SSIM PSNR PSNR SSIM Methods DepthAnything [75] Marigold [26] Mask DINO [30] Mask2Former [13] Bae et al. [6] InvPT [78] AirNet [28] PromptIR [47] Unified-IO [40] Painter [67] InstructCV [19] InstructDiffusion [21] PixWizard [34] Lumina-mGPT [35] Ours RMSE NYU-Depth V2 0.206 0.224 0.387 0.288 0.297 0.287 0.553 mIoU ADE20K 60.80 56.10 25.71 49.90 47.23 36.76 - 42.12 Mean Angle Error NYU-Depth V2 14.90 19.04 - 19.65 28.49 27. Rain100L 0.951 32.98 0.972 36.37 0.882 29.87 0.741 19.82 0.917 31.43 0.462 16.72 21.04 30.58 - - 28.14 16.90 SOTS 0.884 0.974 - - 0.937 0.542 SSIM UDC(T-OLED) 26.76 - - - 27.22 14.54 0.799 - - - 0.826 0. LOLv2 0.821 0.860 - - 0.807 0.371 19.69 21.23 - - 20.29 14.48 Although the results of the generated images still show gap compared to task-specific or existing vision generalist models, our model successfully understand the task objectives conveyed by the unseen explanatory instructions and effectively complete the corresponding tasks. Unseen explanatory instruction samples for these tasks can be found in Section B.2 (cf. Fig. 33 43). While examples of the generated images for these tasks can be found in Section C. B.1.4. More Merits of Explanatory Instructions For vision tasks have already been included in the training set, explanatory instructions can also contribute to the understanding of zero-shot samples. As shown in Fig. 32, for categories present in the training set (e.g., turtle), both direct labels like turtle and descriptive phrases such as the creature swimming in the water effectively guide the model in completing tasks. However, for categories not included in the training set (e.g., broad-winged damselfly), the model struggles to interpret the category name alone but can be assisted by descriptive expressions like the creature on the leaf. Explanatory Instruction: Apply red color overlay to the turtle. Input Image Explanatory Instruction: Apply red color overlay to the creature swimming in the water in the image. Explanatory Instruction: damselflies. Apply red color overlay to Broad-winged Input Image Explanatory Instruction: Apply red overlay to the creature on the leaf in the image. Output Image Output Image Output Image Output Image Figure 32. Explanatory instructions assist the model in understanding task objectives. (Segmentation). 27 B.2. Details and Unseen Instruction Samples for Quantitative Experiments In this section, we provide more detailed explanation for quantitative experiments. As described in Sec. B.1.2, we do not rely on image captions but construct entirely unseen instructions during the evaluation stage. In contrast, other vision generalist models rely on instructions seen during training and incorporate image captions in their evaluation process. For clarification, we use Lumina-mGPT [35] as an example to illustrate this difference. In the evaluation process for Lumina-mGPT, as shown in Table 6, all instructions follow this format: Generate an image according to the provided image, and according to the following caption: {Image Caption},<image>. For the experiments in Appendix B.1, the instructions are Depth estimation. <image> for the Depth Estimation task, Semantic segmentation. <image> for the Semantic Segmentation task and Surface normal estimation. <image> for the Surface Normal Estimation task. Any alteration to the format of these instructions leads to model failure or significantly degraded its performance. In the following, we directly illustrate the variety of unseen instructions we constructed for each evaluation sample through examples from quantitative experiments (cf. Fig. 33 43). Unseen Explanatory Instruction: The scene shifts from wet and rainy environment to calm and dry one by gradually reducing the intensity of the rain until it stops completely. Input Image Input Image Input Image Input Image Input Image Unseen Explanatory Instruction: Imagine the sky clearing up, the rain stopping entirely, and all moisture being removed from the scene, leaving the surface dry and the sky clear. Unseen Explanatory Instruction: To reach scene where the weather turns fair, imagine the skies clearing and the rain ceasing, bringing out the sun. The ground dries up quickly as the rain evaporates, giving way to brighter ambiance. Figure 33. Instruction-level zero-shot samples for quantitative experiments (Deraining). Unseen Explanatory Instruction: The vibrancy intensifies, revealing spectrum of colors that emphasize the angles and orientations of each surface in the scene. Unseen Explanatory Instruction: Translate the visible structures into range of bright colors reflecting orientation angles, enhancing variations across surfaces. Unseen Explanatory Instruction: Transform the visible elements into vibrant array where distinct colors signify the orientation of surfaces. Ground Truth Ground Truth Ground Truth Ground Truth Ground Truth Input Image Ground Truth Figure 34. Instruction-level zero-shot samples for quantitative experiments (Surface Normal Estimation). 28 Unseen Explanatory Instruction: Reduce the color details and enhance the gradient of light to dark, emphasizing depth transitions. Apply uniform grayscale filter across the entire scene, melding textures and patterns into smooth surfaces. Simplify distinct forms, softening and unifying borders to accentuate spatial differences. Unseen Explanatory Instruction: Introduce gradient effect to the space, ensuring that the light source becomes the dominant feature. Simplify textures and colors to various shades of gray, emphasizing the depth and contours of objects. Focus on the relative positioning and shapes, creating smooth transition from light to dark areas, highlighting the spatial arrangement. Unseen Explanatory Instruction: Convert all color data into grayscale, adjusting the brightness based on perceived depth, with closer objects appearing darker and farther ones lighter. Gradually diminish surface textures and fine details, focusing solely on the contours and relative positions of objects. Preserve the edges and outlines to enhance depth perception, emphasizing structural differences. Amplify the contrast between closer and more distant objects to create clear sense of spatial arrangement. Figure 35. Instruction-level zero-shot samples for quantitative experiments (Depth Estimation). Ground Truth Ground Truth Ground Truth Unseen Explanatory Instruction: Enhance the clarity by reducing the haze, adjusting contrast and brightness levels for sharper and more detailed view. Unseen Explanatory Instruction: Increase the vibrancy and contrast while reducing atmospheric haze to reveal details, making sky and structures more defined. Ground Truth Ground Truth Input Image Input Image Input Image Input Image Input Image Unseen Explanatory Instruction: To enhance clarity and vibrancy, imagine each object gaining sharpness and colors becoming more pronounced. Gradually remove the grayish tone, revealing the street with increased contrast and brightness. Input Image Ground Truth Figure 36. Instruction-level zero-shot samples for quantitative experiments (Dehazing). Input Image Input Image Input Image Input Image Input Image Input Image Unseen Explanatory Instruction: Simplify the intricate details, focusing on large areas of color. Reduce the textures of the walls, roof, and garden to single, solid colors, ensuring clear separation between different elements. Unseen Explanatory Instruction: Simplify the scene by identifying distinct regions such as the sky, structures, and foliage, assigning each distinct color. Unseen Explanatory Instruction: Begin by abstracting detailed features into flat, vivid colors, transforming complex textures into simple hues. Convert the varied landscape elements into solid bands of color, maintaining only their fundamental positions and shapes. The sky turns uniform light blue, and the plane becomes defined mint green silhouette. The mower is simplified into blue shape, with the seat becoming highlight in red. Figure 37. Instruction-level zero-shot samples for quantitative experiments (Semantic Segmentation). Unseen Explanatory Instruction: Adding rich layers of color and texture to the outlines would allow for return of the original settings vibrancy. Explosive growth of texture from flat lines and edges would populate the space with marine life and details, enhancing the divers surroundings and restoring their environment. Subtle gradients and vivid aquatic elements, like fish and the deep blue hues, re-emerge, filling the outlines with life. Unseen Explanatory Instruction: Start by filling in the abstract lines with rich details, deepening the creases of the outline with fur textures and shadowed areas. Build up the dimensionality by adding layers of color, texture, and shading to reconstruct the depth in every feature. Soften the stark contrasts while adding fine details, such as the reflective light in the eyes and the nuanced light scattering of fur. Bring back the ambient background details, incorporating stars and subtle sky gradients to restore fullness to the scene. Unseen Explanatory Instruction: Refill the outlines with authentic textures, colors, and shading. Introduce the interplay of natural lighting, allowing shadows and highlights to emerge once again over every visible surface. Rebuild the snowy forest backdrop, enhancing the dimensionality of the mountain scene with vibrant snow-covered elements under warm, diffuse sunlight. The cabin should regain the illusion of physicality, reflecting the interaction between the structure and the surrounding environment, including intricate snow details on the roof and ground. Figure 38. Instruction-level zero-shot samples for quantitative experiments (HED-to-Image). 30 Ground Truth Ground Truth Ground Truth Ground Truth Ground Truth Ground Truth Unseen Explanatory Instruction: Add colors, lighting, and fur texture gradually to the line-based outline, introducing gradients that replicate the realism of fur. Ensure soft lighting is integrated into the scene with attention to shading and depth on the dogs face and body. Input Image Ground Truth Unseen Explanatory Instruction: Rebuild the scene by filling in all edges with rich color, shading, and texture. Reconstruct the orange clouds in the sky and provide depth to the environment by reintroducing lighting, perspective, and atmospheric effects, along with adding detail to individual objects like the child and bicycle. Input Image Ground Truth Unseen Explanatory Instruction: Starting from abstract outlines, vibrant colors would need to emerge, filling in the spaces between the lines with rich textures, colors, and delicate lighting. Details like the lushness of surroundings and the intricate clothing details would have to grow back into the scene, building sense of volume, dimensionality, and narrative where before there was only flat abstraction. Input Image Ground Truth Figure 39. Task level & instruction-level zero-shot samples for quantitative experiments (Canny-to-Image). Input Image Unseen Explanatory Instruction: To improve visibility and clarity, increase the brightness uniformly across the entire area. Next, adjust the contrast to differentiate objects and their surroundings, ensuring the whites and highlights stand out without losing shadows. Apply color correction to restore the vibrancy of dull tones, balancing any tints to make them appear natural. Enhance sharpness to recover fine details in textures, like the targets, arrows, and floor. Smooth out any noise that might emerge from the exposure increase, ensuring clean and crisp finish while retaining natural lighting properties. Unseen Explanatory Instruction: Begin by enhancing the overall brightness of the scene to bring out hidden details. Gradually increase the contrast to differentiate the elements more clearly. Apply color correction to amplify the muted tones, ensuring that the hues are vivid and lifelike. Finally, utilize sharpening technique to accentuate the edges and textures, providing more defined appearance. Ground Truth Input Image Ground Truth Unseen Explanatory Instruction: Begin by adjusting the overall brightness of the scene to allow more light to penetrate the darker areas. Gradually increase the contrast to ensure that the highlights pop while maintaining some depth in the shadows. Employ techniques to amplify color saturation, bringing out the richness of hues that were previously subdued. Finally, apply sharpening filters to enhance the clarity of fine details that were not discernible in the original. Ground Truth Input Image Figure 40. Task level & instruction-level zero-shot samples for quantitative experiments (Low-light Enhancement). Unseen Explanatory Instruction: Remove the circular obstruction to reveal the previously hidden water and foliage, restoring the full view of the landscape. Input Image Ground Truth Unseen Explanatory Instruction: Reveal the hidden elements by removing the black circle, restoring the visibility of the entire kitchen setup. Input Image Ground Truth Unseen Explanatory Instruction: Remove the central black square to reveal the previously obscured parts of the architectural structure and landscape, restoring the full scene. Input Image Ground Truth Figure 41. Task level & instruction-level zero-shot samples for quantitative experiments (Inpainting). Unseen Explanatory Instruction: Expand the vertical floral display into broader composition, ensuring the blooms are evenly spread across, with minimal visible soil. Input Image Ground Truth Unseen Explanatory Instruction: Expand the view horizontally to capture wider perspective, incorporating additional elements from the periphery to create broader context. Input Image Ground Truth Unseen Explanatory Instruction: Expand the view by extending the horizontal dimensions on both sides to reveal more of the surrounding aisle. This creates wider scene, replicating the perspective of broad corridor lined with books extending into the distance. Input Image Ground Truth Figure 42. Task level & instruction-level zero-shot samples for quantitative experiments (Outpainting). 32 Unseen Explanatory Instruction: Enhancing the details by reducing the blurriness, increasing sharpness, and adjusting the contrast to bring clarity to the scene. Unseen Explanatory Instruction: Increase the sharpness and enhance the contrast to reveal clear details and make the colors more vibrant. Unseen Explanatory Instruction: Gradually refine the clarity by reducing any soft blur and enhancing details, while increasing contrast gently to make textures more pronounced. Ground Truth Ground Truth Ground Truth Input Image Input Image Input Image Figure 43. Task level & instruction-level zero-shot samples for quantitative experiments (Under-display Camera Image Restoration). 33 C. More Examples for Zero-shot Capabilities on Vision Tasks In this section, we provide additional examples to demonstrate instruction-level and task-level zero-shot capabilities. The examples are generated using the model trained on the full DECVT. Note that all task-level zero-shot examples inherently fall under the category of instruction-level zero-shot examples as well. Additionally, we highlight the models shortcomings based on these examples. For more detailed exploration of limitations, please refer to Appendix D."
        },
        {
            "title": "Output Images",
            "content": "Figure 44. Examples of both taskand instruction-level zero-shot capabilities (Low-light Enhancement). Resolution: 448448. Explanatory Instruction: Increase the overall brightness to reveal details in dark areas while preserving highlights. Adjust the contrast to enhance the brightness differences between regions, making the structures and textures more distinct. Optimize color saturation to make previously dull colors more vibrant, such as the blue on the floor becoming more prominent. Apply denoising to reduce noise commonly found in low-light images, improving the overall quality. Ensure the final image appears natural while retaining the authentic style of the scene. Limitations: Controlling the intensity of lighting enhancement through language instructions is challenging, often resulting in significant deviations in the output."
        },
        {
            "title": "Output Images",
            "content": "Figure 45. Examples of both taskand instruction-level zero-shot capabilities (Map-to-Image). Resolution: 448448. Explanatory Instruction: Apply process that introduces intricate details by simulating realistic textures, adding natural elements like trees and vegetation, and enhancing geometric shapes into more organic forms. Convert abstract layouts into visually accurate structures, enhancing color depth and including real-world spatial elements such as roof patterns and environmental surroundings. Limitations: The results still fall short when compared to real-world scenes."
        },
        {
            "title": "Output Images",
            "content": "Figure 46. Examples of instruction-level zero-shot capabilities (Deraining). Resolution: 448448. Explanatory Instruction: Slowly remove the rain falling from the sky in the image, still maintain the state of night, and the girl on the bridge is also still holding the umbrella, but readjust the light in the distance. Limitations: The model struggles to preserve smaller objects and environmental details."
        },
        {
            "title": "Output Images",
            "content": "Figure 47. Examples of instruction-level zero-shot capabilities (Desnowing). Resolution: 448448. Explanatory Instruction: Remove the falling snow from the sky in the image, keep the other objects and snow in the image, still keep it dark, but pay attention to the adjustment of light behind the tree. Limitations: The second generated image struggles to retain nighttime details, while the third and fourth images exhibit poor performance in removing snow from the sky. Additionally, attempting to remove snow from the ground simultaneously can result in significant distortions."
        },
        {
            "title": "Output Images",
            "content": "Figure 48. Examples of both taskand instruction-level zero-shot capabilities (Deblurring). Resolution: 448448. Explanatory Instruction: The image shows noticeable multiple visual overlaps of trees and buildings. would like to remove visual overlaps and restore clear, sharp image without blurring. Do not alter the main content and pay attention to adjusting the light. Limitations: The success rate of guiding the models task-level zero-shot capability through language instructions is relatively low."
        },
        {
            "title": "Output Images",
            "content": "Figure 49. Examples of instruction-level zero-shot capabilities (Dehazing). Resolution: 448448. Explanatory Instruction: Retain the distant clouds in the image while removing as much fog as possible. Attempt to restore the faintly visible sun in the distance, but ensure there is no strong sunlight. Focus on recovering the mountains and the nearby trees as much as possible. Limitations: It will cause distortions in certain objects. 35 In the following, we provide few simple examples used for evaluation. Unseen Explanatory Instruction: Acknowledge the spatial structure and identify variations in light intensity, translating these into gradient scale representing distances. Accentuate enhancing the regions where light diminishes gradually, perception of depth by dimming peripheral areas. Adjust the distribution of luminance to highlight the central vanishing point, converting detailed textures into smooth transitions of grayscale. Unseen Explanatory Instruction: Start by analyzing the spatial layout to identify key structural elements. Gradually obscure less relevant details in the periphery to focus primarily on central depth. Increase contrast between light and dark areas to enhance perception of distance. Transition the textures into smooth gradients to reflect variations in depth, with focus on enhanced luminosity for regions that are further away. Unseen Explanatory Instruction: Convert each regions color intensity to grayscale value corresponding to its relative distance from the viewer, with nearer objects appearing lighter and those farther away darker. Gradually smooth transitions between these regions to reflect continuous depth variation. Remove textural details that do not affect perceived depth to create uniformity based on object proximity. Adjust overall brightness to highlight the spatial configuration without explicit texture representation. Output Image Ground Truth Output Image Ground Truth Output Image Ground Truth Figure 50. Examples of instruction-level zero-shot capabilities (Depth Estimation). Resolution: 448448. Unseen Explanatory Instruction: Translate the visible structures into range of bright colors reflecting orientation angles, enhancing variations across surfaces. Unseen Explanatory Instruction: Convert visual elements into spectrum of colors that represent the directionality of surfaces, capturing the angles and orientations vividly. Unseen Explanatory Instruction: Translate the scene into colorful array to indicate surface orientations and angles. Output Image Ground Truth Output Image Ground Truth Output Image Ground Truth Input Image Input Image Input Image Input Image Input Image Input Image Figure 51. Examples of instruction-level zero-shot capabilities (Surface Normal Estimation). Resolution: 448448. Unseen Explanatory Instruction: bicycles, completely matching their shapes. Apply pink color overlay to Input Image Input Image Unseen Explanatory Instruction: Apply solid grey color tint to fully cover one banana instance.Paint over each stove with powderblue color. Unseen Explanatory Instruction: Spectral_r is the reversed version of Spectral, transitioning through red, yellow, green, and blue. Based on the previously defined colors, help me complete the segmentation task below. Color all instances of bucket, toilet using Spectral_r colors, following their contours precisely. Output Image Ground Truth Output Image Ground Truth Input Image Output Image Ground Truth Figure 52. Examples of instruction-level zero-shot capabilities (Semantic Segmentation). Resolution: 448448. Unseen Explanatory Instruction: Mark the cat-like creature in the picture with yellow box. Input Image Output Image Ground Truth Unseen Explanatory Instruction: Mark trees with snow in the picture with red boxes. Input Image Output Image Ground Truth Instruction: Unseen Explanatory For the following object categories, apply the corresponding bounding box colors: Encircle all toilet objects with gold color border. Draw thistle bounding box around all doorknob objects.Detect fan and apply crimson bounding box around them. Detect air_conditioner, Detect all mirror and draw darksalmon color bounding box.Detect toilet_tissue and apply darkturquoise bounding box around them. palevioletred color: color. box Input Image Output Image Ground Truth Figure 53. Examples of instruction-level zero-shot capabilities (Object Detection). Resolution: 448448. 37 Unseen Explanatory Instruction: Capture the outline and prominent edges of the cylindrical object and its surroundings, simplify everything by removing textures and detailed surfaces, and emphasize only the contours and distinct features while rendering higher contrast between light and dark regions with sharp shifts in tones. Input Image Output Image Ground Truth Unseen Explanatory Instruction: The vibrant scene with multiple colors and details could be simplified into monochrome representation. First, focus on defining the high-contrast areas between light and dark in much starker, black-and-white way. Then, its important to emphasize contours and significant edges, such as the lines around the face, the dress folds, and the furnitures details, while downplaying softer gradients. Removing extraneous colors and textures leaves behind only the essential structural features that provide more abstract, but recognizable silhouette and objects. Unseen Explanatory Instruction: Begin by eliminating most of the intricate details and colors, transforming the vibrant elements into simplified outlines. Keep only the borders and defined structures, ensuring that the environment and figure take on an abstract form. Remove all texture, reducing the entire composition to minimal contrasting edges that define the shapes more than the details. Input Image Input Image Output Image Ground Truth Output Image Ground Truth Figure 54. Examples of instruction-level zero-shot capabilities (HED Boundary Detection). Resolution: 448448. Unseen Explanatory Instruction: Gradually reduce atmospheric interference, allowing clearer visibility of buildings and sharpening the outlines. Enhance clarity and brightness to bring out the details within the cityscape, providing crisper view. Input Image Output Image Ground Truth Unseen Explanatory Instruction: Increasing the clarity by reducing haze, enhancing contrast, and deepening colors to give sharper and more vibrant appearance to the scene. Input Image Output Image Ground Truth Unseen Explanatory Instruction: To achieve clarity and vibrancy, adjust the brightness and reduce the foggy effect. Enhance the sharpness of the trees and structures, allowing their details to stand out against the clear blue sky. Input Image"
        },
        {
            "title": "Ground Truth",
            "content": "Figure 55. Examples of instruction-level zero-shot capabilities (Dehazing). Resolution: 448448. 38 Unseen Explanatory Instruction: Imagine scenario where rainfall suddenly stops and the water settles, clearing up the scene to enhance visibility and eliminate rain streaks. Input Image Output Image Ground Truth Unseen Explanatory Instruction: Remove the raindrops and streaks, focusing on enhancing clarity and brightness to achieve crisp and rain-free appearance in the environment. Input Image Output Image Ground Truth Unseen Explanatory Instruction: Imagine the rainfall gradually lessening until the sky clears completely, leaving only the vibrant greenery and the birds in focus. Input Image Output Image Ground Truth Figure 56. Examples of instruction-level zero-shot capabilities (Deraining). Resolution: 448448. 39 D. Limitations and More Discussions In this section, we provide more limitations and discussions of this work. Due to GPU resource constraints, we were unable to conduct additional validation experiments to address these issues. Therefore, we present only our reflections and insights based on the current results and our experiences. 1) As we have mentioned in Sec. 3 of the paper: While the top-k value is typically set to 5 for text generation in large language models (LLMs), we recommend setting the top-k to 2048 during the image generation stage. This recommendation is also consistent with findings in Lumina-mGPT [35], where larger top-k value lead to improved visual details. However, this phenomenon should not necessarily apply to all vision tasks, i.e., in some vision tasks, larger top-k value should not improve the generate results. For instance, in standard segmentation tasks controlled by colors and classes, the answer should be unique for most of the image, with only few pixels potentially subject to ambiguity. While in such cases, lowering the top-k value still can not improve the models ability to follow these instructions. Our intuition is that the current VQ-based training methods and the straightforward fine-tuning approach may have significant impact on this issue. Descriptions of image flow A: (c) (f) (a) 1. closed structure with no visible contents. 2. The structure begins to reveal an internal object. 3. The internal object becomes partially visible. Descriptions of image flow B: (b) (d) (e) 1. partially revealed internal object within structure. 2. The internal object becomes more visible. 3. The structure closes, hiding the internal object. Explanatory instruction from to B: 1. Start with closed structure. 2. Gradually reveal an internal object. 3. Continue revealing more of the internal object until it is partially visible. 4. Progress to showing the internal object more clearly. 5. Finally, close the structure, hiding the internal object again. Explanatory instruction from to A: 1. Begin with partially revealed internal object within structure. 2. Gradually hide the internal object until it is no longer visible. 3. Ensure the structure is completely closed with no visible contents. 4. Open the structure slightly to start revealing an internal object. 5. Continue to reveal more of the internal object until it is partially visible. Figure 57. Video output sample from GPT-4o. Content that does not meet the description requirements is highlighted in red. 2) Experiments in this work are limited to image pairs, and we do not explore more complex data such as video or 3D data. Actually, in the early stage of our experiments, we attempted to construct explanatory instructions between different frames in videos. Unfortunately, for complex videos, even models like GPT-4o still produced significant errors within the descriptions (cf. Fig. 57). While for simpler videos (e.g., those depicting weather changes), although the descriptions tend to be accurate, we feel that adding data with minimal changes between frames does not significantly contribute to testing the 40 zero-shot capability of the model, which is the main goal of this work (we acknowledge that such data, with minimal changes, could potentially enhance the models ability to control generation and follow instructions, but this is not the focus of this study). Nevertheless, we believe that collecting more complex video data could be beneficial for advancing vision task-level generalization. Such data would not only improve the models scene control capabilities but also increase the diversity of task objectives that the model can understand. While these tasks could be treated as form of editing, they surpass the capabilities of current editing models (e.g., transformation from Fig. 57 (a) (d)). 3) In constructing the Dataset of Explanatory CV Tasks, we adhered as closely as possible to the principle that the provided instructions should avoid obvious inaccuracies. While GPT-4o exhibits one of the most advanced descriptive capabilities among existing models, it still faces challenges such as incomplete descriptions and occasional deviations. Although these data significantly enhance the models ability to interpret wide range of instructions and facilitate task-level generalization, they also lead to certain trade-offs. Specifically, the inclusion of these data can reduce the stability of the models outputs and compromise fidelity to the original image content. Furthermore, these data issues, including the presence of some low-quality datasets for the image editing tasks, may adversely affect the models instruction-following capabilities. 4) Although the dataset construction and training approach described above have several limitations, we believe that the use of explanatory instructions can enhance the models adaptability to complex instructions and objectives. We hypothesize that there exists an optimal level of instruction complexity for models. Up to this threshold, more detailed instructions may improve the models understanding and performance. However, beyond this point, the models ability to grasp the intended objective may begin to decline. Moreover, when instructions encompass overly broad objectives while the model lacks sufficient capacity to process and generalize this information, it may lead to performance degradation. 5) Due to budget constraints during the construction of the Dataset of Explanatory CV Tasks, significant portion of the data was generated by directly instructing the model to output explanatory instructions, bypassing the generation of image captions. However, based on empirical observations, we recommend that the model generate image captions before producing explanatory instructions. As for GPT-4o, if the generated image captions exhibit substantial errors, the explanatory instructions are also likely to be inaccurate. Conversely, when the image captions are accurate, the explanatory instructions tend to be more precise. This approach helps to significantly reduce noise within the dataset. 6) Due to resource limitations, we only conducted our experiments on the vanilla token-based VLM with 7B parameter, and no image-caption-based data were used for image generation during training. While this work demonstrates that explanatory instructions can enable zero-shot generalization at the vision task level, this ability remains unstable. Furthermore, on benchmark datasets, the performance of our model under zero-shot settings still lags behind task-specific models and some vision generalist models. This performance gap remains substantial compared to our expectations: as humans possess the ability to generalize across different tasks during the learning process and, based on these generalizations, can better accomplish existing tasks. We guess the primary reasons for the current limitations lie in the architecture of the model and data noise in the Dataset of Explanatory CV Tasks. 7) Just as with language understanding, visual cognition also vary from person to person. For various vision tasks, even when dealing with vision tasks related to single image pair, different people may interpret and describe the images and tasks differently. Therefore, we think that the simple next-token prediction approach may be well-suited to those vision tasks that have already been generalized and defined by humans (e.g., Depth Estimation and Segmentation). However, for more complex vision tasks, directly training through next-token prediction may not be the most appropriate approach, as task descriptions can vary widely and multiple valid answers may exist."
        }
    ],
    "affiliations": [
        "Baidu",
        "Nanjing University of Science and Technology",
        "Southeast University"
    ]
}