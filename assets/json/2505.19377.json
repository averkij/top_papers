{
    "paper_title": "Absolute Coordinates Make Motion Generation Easy",
    "authors": [
        "Zichong Meng",
        "Zeyu Han",
        "Xiaogang Peng",
        "Yiming Xie",
        "Huaizu Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose a radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with a simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying a strong foundation for future research and motion-related applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 7 7 3 9 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Absolute Coordinates Make Motion Generation Easy",
            "content": "Zichong Meng, Zeyu Han, Xiaogang Peng, Yiming Xie, Huaizu Jiang Northeastern University {meng.zic, han.zeyu, peng.xiaog, xie.yim, h.jiang}@northeastern.edu https://neu-vi.github.io/ACMDM/ Figure 1: Absolute coordinates make motion generation easy. Here we show that our model produces motion of higher fidelity, has better controllability, and reports promising results of generating SMPL-H meshes directly."
        },
        {
            "title": "Abstract",
            "content": "State-of-the-art text-to-motion generation models rely on the kinematic-aware, local-relative motion representation popularized by HumanML3D, which encodes motion relative to the pelvis and to the previous frame with built-in redundancy. While this design simplifies training for earlier generation models, it introduces critical limitations for diffusion models and hinders applicability to downstream tasks. In this work, we revisit the motion representation and propose radically simplified and long-abandoned alternative for text-to-motion generation: absolute joint coordinates in global space. Through systematic analysis of design choices, we show that this formulation achieves significantly higher motion fidelity, improved text alignment, and strong scalability, even with simple Transformer backbone and no auxiliary kinematic-aware losses. Moreover, our formulation naturally supports downstream tasks such as text-driven motion control and temporal/spatial editing without additional task-specific reengineering and costly classifier guidance generation from control signals. Finally, we demonstrate promising generalization to directly generate SMPL-H mesh vertices in motion from text, laying strong foundation for future research and motion-related applications."
        },
        {
            "title": "Introduction",
            "content": "Generating realistic human motion from textual descriptions has rapidly emerged as significant research area. It has great potential for diverse applications, including virtual and augmented reality experiences, immersive metaverse environments, video game development, and robotics. Recently, the introduction of the large-scale HumanML3D [25] dataset has catalyzed significant progress in text-to-motion generation by establishing standardized, kinematic-aware motion representation. Earlier methods based on AutoEncoders [44, 4], GANs [22], or RNNs [83] attempted to model joints and kinematic rotations or joint and trajectory [1, 79, 5, 110, 55, 19], but struggled to Preprint. Under review. produce high-fidelity motion. HumanML3D instead proposes to encode motion relative to the pelvis and to the previous frame, enabling explicit modeling of intra-frame kinematics and inter-frame transitions. This local-relative, kinematic-aware representation, combined with built-in redundancy (non-animatable features such as incorrectly processed [102] relative rotations, local velocities, and foot contacts) as form of data-level regularization [66], substantially simplifies training [12, 62, 66] and boosts the performance of these simple backbones. Recent diffusion-based methods [92, 120, 43] also adopt this representation for text-to-motion generation tasks as default, yielding state-of-the-art performances. While later works have explored architectural improvements [10, 91, 126, 33, 123], generation speedups [12, 15, 14], and retrieval-based enhancements [121], the underlying representation has been largely inherited from HumanML3D [25] without much careful study. However, this de facto representation introduces several fundamental limitations. First, although this representation benefits earlier methods, the redundancy makes it difficult for diffusion models to learn [66], often leading to underperformance in generated motion quality. Second, its inherently relative nature is misaligned with the requirements of downstream tasks such as motion control and temporal/spatial editing [103, 42, 78]. These tasks demand motion generation that is not only semantically meaningful but also aware of absolute joint locations, which are usually provided by users, to enable precise control and intuitive motion editing. Attempts to inject absolute location information into the existing local-relative representation have often resulted in overly complex designs [52] and degraded generation fidelity [42, 103, 15, 14]. In this paper, we revisit the foundational question of motion representation for text-driven motion generative models. We begin by demonstrating that the redundant, local-relative, kinematic-aware formulationcommonly assumed to be essentialis not crucial for the performance of diffusionbased models. Instead, we adopt much simpler and long-abandoned non-kinematic representation in text-to-motion methods: absolute joint coordinates in global space. Through careful analysis of key design choices, we show that even with simple Transformer [94] model (e.g. without UNet [33, 10] or altered attentions [10, 121]) and without additional kinematic losses, this simple formulation can achieve significantly higher motion fidelity, improved text alignment, and strong scalability potential. Furthermore, we show this simple representation naturally supports range of downstream tasks, including motion control and temporal/spatial editing, without requiring task-specific reengineering. With inherent absolute location awareness, our formulation enables direct controllability by eliminating the need for relative-to-absolute post-processing, which often introduces errors, as well as removing reliance on time-consuming classifier guidance from control signals during generation. By discarding the constraints of redundant, local-relative, kinematic-aware representation designs, our approach also opens the door to directly modeling motion from textual inputs beyond standard human joint skeletons. Our formulation shows potential to generalize to other subclasses of absolute coordinates, such as SMPL-H mesh vertices [59] in motion from text, which are largely neglected by existing approaches but crucial toward having vivid, animatable human avatars. This lays foundation for future research in broader text-to-motion generation domains, enabling new applications across diverse motion-related domains. In summary, our contributions are as follows: We propose new formulation for text-to-motion diffusion models using absolute joint coordinates. Through systematic analysis of design choices, our method can achieve state-of-the-art performance with simple Transformer [94] backbones and no auxiliary losses. We demonstrate that this formulation naturally supports downstream motion tasks, including motion control and temporal/spatial editing, achieving better performance and enabling seamless integration without additional reengineering or time-costly guidance generation. We further show promising generalizes beyond joints to directly modeling other subclasses of absolute coordinates, such as mesh vertices. This flexibility marks an important step toward text-driven motion generation across broader domains and serves as foundation for future research and broader real-world applications."
        },
        {
            "title": "2 Related Works",
            "content": "Human Motion Generation. Early approaches in text-driven motion generation [1, 25, 71, 72, 90, 111] attempt to align the latent spaces of text and motion. However, these methods faced significant challenges in generating high-fidelity motion due to the difficulty of seamlessly aligning 2 two fundamentally distinct modalities. Inspired by the success of denoising diffusion models in image generation [29, 87], several pioneering works [92, 43, 120, 12, 116] introduced diffusion-based approaches for human motion generation. Subsequent works have primarily focused on architectural innovations [3, 117, 130, 10, 33, 14, 91, 126, 123, 102] or on improved training methodologies [51, 117, 2, 31, 96, 130, 61, 15, 121]. Other human motion generation works introduce Vector Quantization (VQ), enabling discrete motion token modeling [26, 118, 115, 78, 23, 8, 77, 128, 50, 37, 122, 63, 124] or explore autoregressive generation [125, 11, 86, 126, 91, 66, 102]. Recent works also diversified their focus, exploring human-scene/object interactions [70, 32, 45, 106, 75, 48, 9, 98, 21, 113, 108, 64, 13, 101, 38, 46, 58, 17, 127, 97, 109, 60, 35, 107], human-human interaction [36, 105, 100, 20, 53, 7, 114], stylized human motion generation [129, 24, 49], more datasets [104, 56], longmotion generation [132, 73], shape-aware motion generation [93, 54], fine-grained text controlled generation [133, 34, 112, 85, 39, 82, 89], leveraging 2D data [40, 74, 47], as well as investigating advanced architectures [123, 99]. In contrast, our work revisits the underlying text-to-motion representation itself. We show that adopting simpler yet long-abandoned alternative: absolute joint coordinates, even with simple Transformer backbone and no additional constraints, can significantly improve generation quality. Controllable Text-to-Motion Generation. In addition to synthesizing motion purely from text prompts, recent work has explored controlling motion generation with auxiliary signals such as trajectories or editing constraints [42, 103, 15, 14, 76, 81, 95, 41]. Early approaches such as PriorMDM [84] extended MDM [92] to support end-effector constraints. GMD [42] introduced spatial control by guiding the diffusion process on the root joint trajectory, but required re-engineered motion representation specifically designed for the task. OmniControl and MotionLCM [103, 15] generalized control to arbitrary joints by leveraging ControlNet [119], but both still rely on relative motion representations. Moreover, OmniControl heavily depends on classifier guidance from control signals during generation; without it, motion quality degrades significantly. Input optimization-based approaches [41, 76, 14] proposed directly optimizing the inputs to meet control objectives, but suffer from high computational and time costs due to multi-round optimization and gradient accumulations, making real-time applications impractical. In this work, we show that our proposed absolute joint coordinate formulation enables superior performance without the need for task-specific reengineering and time-consuming classifier guidance or inference-time optimization. Mesh-Level Text-Driven Human Motion Generation. Previous works rarely perform direct mesh vertex generation. Instead, prior methods [92, 12, 103, 42] typically predict HumanML3D representations and convert them to joint positions, followed by SMPL fitting [6]. Other efforts in related fields such as Human-Object Interaction (HOI), Human-Scene Interaction (HSI) and DualPerson motion generation have attempted to directly model SMPL parameters [32, 45, 21, 64, 46, 101, 38, 58, 127, 97, 105] or joint rotations and translations [75, 48, 113], which are then applied to meshes through standard skinning and rigging techniques. However, SMPL fitting is time-consuming and prone to reconstruction errors, while directly modeling SMPL parameters or joint transformations remains challenging and often results in unsatisfactory mesh quality[70, 46]. Moreover, even small joint-level errors can be magnified when propagated to mesh vertices, degrading the visual fidelity of the synthesized motion. Direct mesh vertex generation from textual inputs remains largely unexplored, yet it is critical for achieving high-fidelity, visually realistic motion synthesis. In this work, we show that with our absolute coordinate formulation, we can naturally extend to directly generating mesh vertices from text and achieve strong performance. Text-to-Human Motion Representation Early text-to-motion generation methods, often based on AutoEncoders [44, 4] or GANs [22], attempted to directly predict absolute joint positions [1], but struggled to produce realistic motions. Later approaches incorporated human kinematics by predicting joint rotations [79, 5, 110], combining joint positions with trajectory modeling [55, 19]. However, these designs remained limited in producing high-fidelity and semantically aligned motions. The HumanML3D [25] representation addressed these challenges by encoding motion relative to the pelvis and the previous frame, explicitly modeling intra-frame kinematics and inter-frame transitions. Its local-relative, kinematic-aware design, with built-in redundancy [66, 102] from features such as relative rotations, local velocity, and foot contacts, substantially simplified training [12, 62] and quickly became the dominant choice for subsequent text-to-motion generation methodologies. In this work, we demystify the significance of HumanML3D representation formulation and adopt simpler, long-abandoned non-kinematic formulation: absolute joint coordinates in global space. We show that, with this design, our method achieves better performance using simple Transformer [94] 3 Figure 2: Overview of our proposed ACMDM. (a) Left: The raw/latent absolute coordinates representation is patchified and processed through sequence of ACMDM blocks. Right: Details of ACMDM blocks, where we experiment with two conditioning variants: concatenation and AdaLN. (b) ControlNet-augmented ACMDM for controllable motion generation: Structured control signals are separately encoded and fused into the ACMDM generation process via additive residuals at each ACMDM block, enabling the model to follow both semantical and spatial controlling constraints. backbones without auxiliary losses, and naturally extends to direct modeling other subclasses of absolute coordinates such as mesh vertices."
        },
        {
            "title": "3 ACMDM: Absolute Coordinates Motion Diffusion Model",
            "content": "The majority of recent methods utilize the redundant, local-relative, and kinematic-aware motion representation popularized by HumanML3D [25]. However, this explicit inter-frame kinematic modeling around the pelvis makes the generation prone to accumulating global drift errors through frames, while the intra-frame relative formulation makes it difficult to incorporate absolute location controlling signals for downstream tasks. In contrast, we propose adopting much simpler but long-abandoned alternative, absolute joint coordinates in global 3D space and show it makes human motion generation easy. We first introduce our proposed ACMDM in Section 3.1 that we will systematically investigate and ablate in the experiments section. Next, in Section 3.2, we describe how to extend ACMDM to controllable motion generation through ControlNet integration without much task-specific engineering. Finally, we show how ACMDM generalizes to direct mesh vertex motion generation in Section 3.3."
        },
        {
            "title": "3.1 Absolute Joint Coordinates for Text-to-Motion Diffusion",
            "content": "Absolute Coordinates Representation. We define absolute joint coordinates at each frame as Xi RNj 3, where Nj is the number of joints (e.g., 22 for the HumanML3D dataset), and each joint is represented by its 3D global position (XYZ). This intuitive formulation naturally avoids pelvis drift accumulation and facilitates direct controllability over spatial control signals. Previous works generally avoided this representation due to concerns about generating unnatural, non-humanlike motions [103]. It was widely believed [103, 12, 62] that kinematic features were essential for physically plausible motion synthesis. In the experiment section, we demonstrate that using redundant kinematic features actually degrades motion generation quality, and that absolute joint positions alone are sufficient to achieve high-fidelity and controllable motion generation. Tokenizing Motion Representation. Absolute joint coordinates inherently preserve both spatial and temporal structure of the motion data. Given motion sequence input of shape (L, Nj, din), where is the motion sequence length and din is the input feature dimension (3 for raw absolute coordinates), we apply 2D convolutional layer to transform this structured input into sequence of tokens similar to ViT [18], each with hidden dimension d. The number of tokens is determined by the predefined patch size (PT , PS), where the convolution kernel size and stride are both set equal to (PT , PS), resulting in = . Importantly, we perform tokenization only along the spatial PT Nj PS 4 (joint) dimension while preserving the full temporal resolution (i.e., we define PT = 1), as temporal details are especially critical for motion modeling. In our design, we explore various patch sizes, including 1 22, 1 11, and 1 2, corresponding to different joint-wise granularities. Motion Diffusion with Transformer. After tokenizing the absolute coordinate inputs, the resulting token sequence is fed directly into Transformer for diffusion-based motion generation. Note that the central goal of this work is not to advance model architecture for motion generation. Rather, we focus on investigating the absolute coordinates motion representation. Therefore, we simply adopt simple Transformer similar to DiT [69] and found it works sufficiently well. To incorporate conditioning signals, we follow prior works [92, 118, 23, 78, 66, 77, 12, 121] and use pretrained CLIP-B/32 [80] text encoder to extract the textual embedding c, along with timestep embedder to process diffusion timestep t. We explore two conditioning mechanisms within our ACMDM design: (1) Concatenation, commonly used method in prior text-to-motion works [92, 118, 23, 78, 77, 12], where condition vectors are appended along the sequence dimension; and (2) AdaLN, where the text and timestep embeddings modulate each block via adaptive layer normalization, similar to image diffusion DiT [69]. An illustration of these variants are shown in Figure 2 (a). In line with recent best practices in Transformer models, we also adopt several modern architectural components: Rotary Positional Embedding (RoPE) [88] and QK Normalization [28] are applied in the attention layers, and SwiGLU activations[68] are used in the feed-forward networks (FFNs). We also investigate different denoising targets for training ACMDM, including predicting x0 [29] (the original motion), ϵ [29] (the added noise), and velocity [57] (under flow-matching formulations). In our experimental analysis, we show that prediction consistently yields the best generation performance. All ACMDM variants are trained with standard L2 reconstruction loss on the diffusion objective. More details are provided in the supplemental material. After processing through the motion diffusion Transformer, the output token sequence is linearly projected to match the original shape. Specifically, linear layer is applied to transform each token from dimension back to din PT PS. The output is then reshaped to recover the original 2D structure (i.e., (L, Nj, din)) of the absolute joint coordinates. Latent Motion Encoding with Motion AutoEncoder. Optionally, we convert raw absolute coordinates into latents using motion autoencoder (AE) and perform motion diffusion then, which leads to better generation fidelity as shown in the experiment section. Specifically, given motion sequence X0:N RLNj 3, 2D ResNet-based encoder compresses it into latent representation x0:n RlNj dj , where denotes the downsampled motion sequence length and dj is the dimension of the motion latent. We keep the number of joints Nj unchanged here. Tokenization is then performed over the latent representations (so din = dj), whose output will be fed into the motion diffusion Transformer. decoder later can reconstruct the motion sequence ˆX0:N RLNj 3 via nearest-neighbor upsampling based on the diffusion output. We explore causal AE (i.e., convolution kernels can only access previous frames), non-causal AE, VAE-based variant, and direct modeling on raw absolute joint coordinates in the experimental section. All these motion AE variants are trained with simple smooth L1 reconstruction loss. More details of all the AE variants are provided in the supplemental material. Scaling ACMDM. We scale the model capacity by increasing the motion diffusion Transformer layers depth and width. Specifically, we follow simple scaling strategy where the number of Transformer layers is set equal to the number of attention heads. We define four model sizes: ACMDM-S, ACMDM-B, ACMDM-L, and ACMDM-XL, corresponding to configurations with 8, 12, 16, and 20 layers and attention heads, respectively. This consistent scaling scheme enables systematic exploration of ACMDMs capacity and its effect on generation quality. In addition, we also vary the patch sizes for tokenization. We name different model variants according to their model and patch size (for tokenization); e.g., ACMDM-XL-PS2 refers to the XL variant with patch size of 1 2."
        },
        {
            "title": "3.2 Adding Controls to Absolute Joint Coordinates Generation",
            "content": "Most prior methods face significant challenges in controllable motion generation due to their reliance on local-relative representations, which naturally misalign with user-provided absolute coordinates control signals. In contrast, our absolute coordinates representation removes this misalignment, enabling seamless integration of control without classifier guidance [16] and input optimization [41]. 5 To enable controllable text-driven motion generation, such as trajectory conditioning and temporal/spatial editing with absolute joint coordinates, we follow prior works [103, 15, 14] and integrate ControlNet [119]-style module into the ACMDM architecture. As shown in Figure 2 (b), the noised absolute coordinate latent is first tokenized via 2D convolutional layer and then fed into both the main ACMDM and parallel ControlNet module. At the same time, textual and timestep conditions are encoded and provided to both ACMDM and the ControlNet as conditioning embeddings. Separately, structured control signals (e.g., joint trajectories or partial-body constraints) are processed through dedicated ControlNet condition encoder. The ControlNet receives both the tokenized noised inputs as well as control-specific features in additive combination with the textual and timestep embeddings. These fused features generate residuals, which are injected into the main ACMDM backbone at each layers via additive fusion. This modulation enables the model to follow both semantic instructions and structural constraints. In addition to the standard L2 reconstruction loss on the diffusion target, we also apply an L2 loss between the models prediction and the control signal. We also freeze the parameters of the main ACMDM and only train the ControlNet branch, which is initialized as copies of the main ACMDM blocks, similar to prior works [119, 103, 15, 14]."
        },
        {
            "title": "3.3 Generating Meshes with Absolute Coordinates Representation",
            "content": "Towards achieving vivid, animatable human avatars, joint representations are insufficient; when translated to meshes through fitting models, they often result in shaky body parts, unnatural hand motions, and missing flesh dynamics [92, 12, 15, 14]. Direct motion generation at the mesh level, however, largely falls behind joint counterparts, mainly due to the complexity of modeling mesh representations. Here, we show that our absolute, non-kinematic representation naturally extends to mesh vertices, which is seamlessly supported by ACMDM without major architectural changes. In specific, we explore direct motion generation of SMPL-H [59] mesh vertices, where each frame is represented as set of absolute 3D vertex coordinates with shape (L, Nv, 3), where Nv = 6890 denotes the number of vertices. Unlike absolute joint coordinates, where the number of joints Nj is typically small, directly training diffusion models on full-resolution mesh data with Nv = 6890 is computationally prohibitive and unstable. To address this, we incorporate 2D mesh autoencoder based on the Fully Convolutional Mesh Autoencoder [131]. The encoder spatially compresses the input mesh sequence (L, Nv, 3) into latent representation of shape (L, nv, dv), where we set nv = 28 for diffusion modeling efficiency and reconstruction quality. Once mesh vertices are encoded, we reuse the ACMDM framework to perform motion diffusion in this latent mesh space. The resulting sequence is tokenized using patch sizes of 1 28 and processed with the same formulation as our joint-based ACMDM. In the experiment section, we show the flexibility and scalability of our approach for high-fidelity motion generation over mesh vertices as well in addition to human joints."
        },
        {
            "title": "4.1 Datasets, Training Setups, and Evaluation Protocols",
            "content": "Datasets. To fairly evaluate different ACMDM designs and compare against prior models, we adopt the widely used HumanML3D [25] benchmark for standard text-to-motion generation, downstream tasks such as text-driven trajectory-controlled generation and upper-body editing, and direct text-to-SMPL-H mesh motion generation. We also include text-to-motion evaluations on KITML [79], reported in the Appendix. HumanML3D contains 14,616 motion sequences sourced from AMASS [65] and HumanAct12 [27], each paired with three textual descriptions (44,970 annotations in total). All motions are standardized to 20 FPS and capped at 10 seconds. It is augmented via mirroring and split into training, validation, and test sets using standard 80%/15%/5% split. Training Setups. All ACMDM variants are trained using the AdamW optimizer with β1 = 0.9 and β2 = 0.99. We use batch size of 64 with maximum sequence length of 196 frames. The learning rate is initialized at 2 104 and linearly warmed up over the first 2,000 steps. We apply learning rate decay by factor of 0.1 at 50,000 iterations during the training of 500 epochs. We also use an exponential moving average (EMA) of model weights to improve training stability and performance. During inference, we apply classifier-free guidance (CFG) [30] = 3 for text-to-motion generation and upper-body editing, 2.5 for trajectory control, and 4.5 for text-to-SMPL-H mesh motion generation. 6 Table 1: Ablation study of the design choices of ACMDM on the HumanML3D dataset. The results indicate that kinematic-aware redundancy is not necessary. Instead, absolute coordinates motion representation can achieve high-quality motion generation with AdaLN conditioning, the velocity diffusion objective (v), and latent space modeling."
        },
        {
            "title": "Motion\nAE",
            "content": "Absolute+Redundancy"
        },
        {
            "title": "AdaLN",
            "content": ""
        },
        {
            "title": "AdaLN",
            "content": "Non-Causal VAE Non-Causal AE Causal VAE Diffusion Objective x0 ϵ x0 ϵ x0 ϵ x0 ϵ v FID 0.771.020 0.868.030 0.276.006 0.969.029 0.419.013 0.208.012 0.133.004 0.125.007 0.121.006 0.137.007 0.188.006 0.109.005 0.178.006 0.150.005 0.115.005 Top 1 0.441.002 0.358.003 0.445.002 0.356.003 0.436.002 0.451.003 0.485.002 0.493.002 0.502.002 0.473.002 0.475.003 0.508.002 0.497.002 0.502.003 0.504.002 R-Precision Top 2 0.633.003 0.538.005 0.634.002 0.539.004 0.630.003 0.643.003 0.680.002 0.685.003 0.692.003 0.670.002 0.670.002 0.701.003 0.687.003 0.693.003 0.697. Top 3 0.738.002 0.650.004 0.738.002 0.648.003 0.736.003 0.751.002 0.779.002 0.783.002 0.789.003 0.772.003 0.775.002 0.798.003 0.785.004 0.787.003 0.795.003 Matching 3.632.009 4.168.025 3.613.008 4.362.013 3.717.013 3.544.010 3.386.012 3.343.009 3.304.008 3.451.011 3.393.012 3.253.010 3.323.010 3.296.010 3.278.011 Figure 3: Visual comparisons of generated motion between ACMDM and state-of-the-art methods. ACMDM generates more realistic motion that accurately follows the textual condition. Evaluation Metrics. We adopt the robust evaluation framework proposed by [66], focusing on essential, animatable motion features. Following [25, 66], we report: (1) R-Precision (Top-1/2/3) and Matching (semantic alignment with captions); (2) FID (distribution similarity); (3) MultiModality (motion diversity per prompt); and (4) CLIP-Score (cosine similarity between motion and caption embeddings). For trajectory-control evaluations [42], we additionally report Diversity (variability within generated motions), Foot Skating Ratio, Trajectory Error, Location Error, and Average Joint Error (accuracy of controlled joints at keyframes). Metrics are averaged over five levels of control intensity (1%, 2%, 5%, 25%, 100%). During training, control intensity levels are randomly sampled. For direct SMPL-H mesh generation, we also report Laplacian Surface Distance (LSD) to assess mesh structural preservation relative to the ground-truth T-pose. More metric details are in Appendix."
        },
        {
            "title": "4.2 Ablating ACMDM Designs",
            "content": "Necessity of Kinematic-aware and Redundant Motion Representation. Prior attempts [103] of text-to-absolute-coordinate motion generation adopt InterGen [52]s representation with heavy kinematic-aware redundancy and the x0 objective, but result in unrealistic motion. To systematically analyze this, in the top two sections of Table 1, we train an ACMDM-S-PS22 variant. We match the model size and flattened spatial embedding style used in prior works in two settings: one using absolute coordinates with kinematic-aware and redundant representation (i.e., InterGens representation), and another using plain absolute coordinates (our proposed). The results show that while the previously widely adopted x0-prediction diffusion benefits slightly from the redundancy, velocity prediction (v) with plain absolute coordinates (our proposed) achieves better performance. Notably, by modeling plain absolute coordinates with prediction, ACMDM achieves FID that is 0.563 lower and an R-Precision Top-3 score that is 0.013 higher compared to redundant x0 prediction. These results demonstrate that with more suitable diffusion objective (v prediction), and the previously assumed necessary kinematic-aware redundancy is not required for achieving high-quality motion generation. Therefore, for the rest of the paper, all ACMDM models will adopt the pure absolute coordinates representation without any kinematic-aware or redundant features. 7 Table 2: Quantitative text-to-motion evaluation. We repeat the evaluation 20 times and report the average with 95% confidence interval. We use bold face / underline to indicate the best/2nd results."
        },
        {
            "title": "Methods",
            "content": "Real MDM-50Step [92] MotionDiffuse [120] ReMoDiffuse [121] MLD++ [14] MotionLCM V2 [14] MARDM [66]-ϵ MARDM [66]-v ACMDM-S-PS22 ACMDM-XL-PS2 FID 0.000.000 0.518.032 0.778.005 0.883.021 2.027.021 2.267.023 0.116.004 0.114.007 0.109.005 0.058.004 Top 1 0503.002 0.440.007 0.450.006 0.468.003 0.500.003 0.501.002 0.492.006 0.500.004 0.508.002 0.522.002 R-Precision Top 2 0.696.001 0.636.006 0.641.005 0.653.003 0.691.002 0.693.002 0.690.005 0.695.003 0.701.003 0.713.002 Top 3 0.795.002 0.742.004 0.753.005 0.754.005 0.789.001 0.790.002 0.790.005 0.795.003 0.798.003 0.807. Matching MModality CLIP-score 3.244.005 3.640.028 3.490.023 3.414.020 3.220.008 3.192.009 3.349.010 3.270.009 3.253.010 3.205.008 - 3.604.031 3.179.046 2.703.154 1.924.065 1.780.062 2.470.053 2.231.071 2.156.061 2.077.083 0.639.001 0.578.003 0.606.004 0.621.003 0.639.002 0.640.003 0.637.005 0.642.002 0.642.001 0.652.001 Concatenation vs. AdaLN. In the third section of Table 1, we switch from the widely adopted concatenation-based conditioning to AdaLN conditioning with an ACMDM-S-PS22 variant with pure absolute coordinates. Our results show that across all diffusion objectives, better conditioning mechanism (AdaLN) lead to significant improvements. Notably, with prediction, ACMDM achieves an FID of 0.121 and an R-Precision Top-3 score of 0.789, substantially outperforming concatenationbased conditioning. These findings demonstrate that an effective conditioning mechanism is key factor in achieving high-quality motion generation. Therefore, for all subsequent experiments, we adopt AdaLN-based conditioning mechanism across all ACMDM models. Raw Absolute Coordinates vs. Latent Space. In the fourth section of Table 1, we switch from directly modeling raw absolute coordinates to latent space. Our results show that latent space modeling further improves generation quality while also offering faster inference for prediction, achieving the best FID of 0.109 and R-Precision Top-3 score of 0.798 We additionally compare different AutoEncoder variants: Causal-AE, Non-Causal-AE, and VAE in the last section of Table 1. Among them, Causal-AE achieves the best overall performance. Therefore, for all subsequent experiments, we adopt Causal-AE as our default setup. Since velocity (v) prediction consistently yields the best performance across all settings, we also adopt it as the default diffusion objective. Scaling Model and Decreasing Patch Sizes. In Figure 4, we train 12 ACMDM models over all model configs (S, B, L, XL) and patch sizes (1 22, 1 11, 1 2). In all cases, we find that increasing model size and decreasing patch size lead to improved text-to-motion generation performance both with and without CFG across all metrics. Notably, ACMDM-XL-PS2 achieves an FID of 0.058 and an R-Precision Top-1 score of 0.522, outperforming the most recent stateof-the-art MARDM by 0.056 in FID and 0.022 in R-Precision Top-1. These findings demonstrate the effectiveness of scaling model capacity and decreasing patch sizes with absolute joint coordinates. We include detailed results in Appendix. Figure 4: Scaling of ACMDM with model capacity and decreasing patch size. We use red for S, orange for B, green for L, and blue for XL, with color gradients indicating decreasing patch sizes. ACMDM exhibits strong scalability, with performance consistently improving as model size increases and patch size decreases."
        },
        {
            "title": "4.3 Comparison to State-of-the-Art Text-to-Motion Generation Methods",
            "content": "We present the quantitative comparison between our method and state-of-the-art text-to-motion generation baselines in Table 2, as well as qualitative comparison in Figure 3 and Appendix. As observed, our method achieves superior performance across multiple key metrics, including FID, R-Precision, Matching Score, and CLIP-Score. Compared to existing approaches, ACMDM demonstrates significantly stronger ability to generate high-fidelity, semantically aligned motions that closely follow textual instructions. Notably, even for our smallest ACMDM variant, ACMDM-S-PS22, it outperforms all prior state-of-the-art methods. Larger ACMDM models, such as ACMDM-XL-PS2, further amplify the performance gains across all evaluation metrics. 8 Table 3: Quantitative text-conditioned motion generation with spatial control signals and upperbody editing on HumanML3D. In the first section, methods are trained and evaluated solely on pelvis controls. In the middle section, methods are trained on all joints and evaluated separately on each controlled joint. Only average results are reported for brevity. We include details in Appendix. Last section presents upper-body editing results. bold face / underline indicates the best/2nd results. Avg. err. Diversity Foot Skating Traj. err. Loc. err. Methods FID AITS Classifier Guidance R-Precision Top 3 Ratio. Controlling Joint GT MDM [92] PriorMDM [84] GMD [42] OmniContol [103] MotionLCM V2+CtrlNet [14] ACMDM-S-PS22+CtrlNet OmniContol [103] MotionLCM V2+CtrlNet [14] ACMDM-S-PS22+CtrlNet 16.34 20.19 137.63 81.00 0.066 2.51 81.00 0.066 2. - Train On Pelvis Train On All Joints (Average) Methods UpperBody Edit MDM [92] OmniControl [120] MotionLCM V2+CtrlNet [120] ACMDM-S-PS22+CtrlNet AITS Classifier Guidance 16.34 81.00 0.066 2.51 0.000 1.792 0.393 0.238 0.081 3.978 0.067 0.126 4.504 0. FID 1.918 0.909 3.922 0.076 0.795 0.673 0.707 0.763 0.789 0.738 0.805 0.792 0.715 0.803 10. 9.131 9.847 10.011 10.323 9.249 10.481 10.276 9.230 10.526 - 0.1019 0.0897 0.1009 0.0547 0.0901 0.0591 0.0608 0.1119 0.0596 0. 0.4022 0.3457 0.0931 0.0387 0.1080 0.0075 0.0617 0.2740 0.0117 0.000 0.3076 0.2132 0.0321 0.0096 0.0581 0.0010 0.0107 0.1315 0.0019 0. 0.5959 0.4417 0.1439 0.0338 0.1386 0.0100 0.0404 0.2464 0.0197 R-Precision R-Precision Top 1 0.359 0.428 0.404 0.532 Top 0.556 0.614 0.592 0.719 R-Precision Top 3 0.654 0.722 0.692 0.820 Matching Diversity 4.793 3.694 5.610 3.098 9.210 10.207 9.309 10. - - - - - Table 4: Quantitative results for direct text-to-SMPL-H mesh motion generation on HumanML3D."
        },
        {
            "title": "Size",
            "content": "S XL"
        },
        {
            "title": "Transformer",
            "content": "8 head 512 dim FID 0.211.005 0.181.003 12 head 768 dim 16 head 1024 dim 0.160.004 20 head 1280 dim 0.139.003 R-Precision Top 1 R-Precision Top 2 R-Precision Top 3 Matching 3.405.011 3.345.010 3.341.009 3.309.007 0.784.003 0.783.002 0.790.002 0.794.003 0.682.003 0.691.003 0.696.002 0.704.003 0.478.004 0.490.003 0.497.003 0.498. CLIP-score 0.620.002 0.631.001 0.633.0 0.636.001 LSD 0.0026.0002 0.0024.0002 0.0025.0001 0.0025."
        },
        {
            "title": "4.4 Comparison to State-of-the-Art Controllable Motion Generation Methods",
            "content": "We present quantitative comparisons between our method and state-of-the-art methods on textdriven trajectory control and upper-body editing in Table 3. For the trajectory control task, prior works [42, 103, 14] have shown that inference-time classifier guidance is crucial for achieving strong control performance. However, we show that even with our smallest ACMDM variant that matches to baseline model sizes and embedding formats, our absolute coordinate formulation achieves superior motion fidelity and control accuracy without the need for time-consuming classifier guidance from control signals. This results in significantly faster generation compared to guidance-dependent approaches (2.51 v.s. 81.0 seconds). For the upper-body editing task, we follow the evaluation protocol proposed by [78, 76], where we fix the pelvis, left foot, and right foot joints and edit the upper body motion according to textual prompts. Our method achieves substantially better generation quality across all evaluation metrics, validating the effectiveness of our proposed approach."
        },
        {
            "title": "4.5 Evaluations on Absolute Mesh Vertex Coordinates Motion Generation",
            "content": "We evaluate ACMDM on SMPL-H absolute mesh vertex coordinates motion generation in Table 4. We train and compare four ACMDM model sizesS, B, L, and XL, with the patch size of 1 28. Despite the significantly increased complexity of modeling full mesh sequences compared to joint sequences, our ACMDM models still achieve strong performance. Notably, all variants achieve results competitive with the best text-to-joint generation models, while operating directly on highdimensional vertex spaces. This highlights the effectiveness and flexibility of our absolute coordinates motion representation in handling broader motion generation tasks beyond human joints."
        },
        {
            "title": "5 Conclusion",
            "content": "In conclusion, we presented ACMDM, novel text-driven motion diffusion framework built on an absolute coordinates motion representation. We run extensive analysis to identify an optimal setting, including the velocity prediction diffusion objective, optimized conditioning mechanisms (AdaLN), and latent motion representation. Our model naturally supports downstream control tasks, which removes the misalignment between local motion representation and absolute controlling, and also generalizes to direct SMPL-H mesh vertices motion generation. Extensive experiments demonstrate that ACMDM achieves superior performance and scalability across text-to-motion benchmarks."
        },
        {
            "title": "References",
            "content": "[1] Chaitanya Ahuja and Louis-Philippe Morency. Language2pose: Natural language grounded pose forecasting. In 2019 International conference on 3D vision (3DV), pages 719728. IEEE, 2019. [2] Nefeli Andreou, Xi Wang, Victoria Fernández Abrevaya, Marie-Paule Cani, Yiorgos Chrysanthou, and Vicky Kalogeiton. Lead: Latent realignment for human motion diffusion. arXiv preprint arXiv:2410.14508, 2024. [3] Samaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh, and Sonal Gupta. Make-ananimation: Large-scale text-conditional 3d human motion generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1503915048, 2023. [4] Dor Bank, Noam Koenigstein, and Raja Giryes. Autoencoders. Machine learning for data science handbook: data mining and knowledge discovery handbook, pages 353374, 2023. [5] Uttaran Bhattacharya, Nicholas Rewkowski, Abhishek Banerjee, Pooja Guhan, Aniket Bera, and Dinesh Manocha. Text2gestures: transformer-based network for generating emotive body gestures for virtual agents. In 2021 IEEE virtual reality and 3D user interfaces (VR), pages 110. IEEE, 2021. [6] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael Black. Keep it smpl: Automatic estimation of 3d human pose and shape from single image. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part 14, pages 561578. Springer, 2016. [7] Zhi Cen, Huaijin Pi, Sida Peng, Qing Shuai, Yujun Shen, Hujun Bao, Xiaowei Zhou, and Ruizhen Hu. Ready-to-react: Online reaction policy for two-character interaction generation. In The Thirteenth International Conference on Learning Representations, 2025. [8] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. [9] Jianqi Chen, Panwen Hu, Xiaojun Chang, Zhenwei Shi, Michael Christian Kampffmeyer, and Xiaodan Liang. Sitcom-crafter: plot-driven human motion generation system in 3d scenes. arXiv preprint arXiv:2410.10790, 2024. [10] Ling-Hao Chen, Shunlin Lu, Wenxun Dai, Zhiyang Dou, Xuan Ju, Jingbo Wang, Taku Komura, and Lei Zhang. Pay attention and move better: Harnessing attention for interactive motion generation and training-free editing. arXiv preprint arXiv:2410.18977, 2024. [11] Rui Chen, Mingyi Shi, Shaoli Huang, Ping Tan, Taku Komura, and Xuelin Chen. Taming diffusion probabilistic models for character control. In ACM SIGGRAPH 2024 Conference Papers, pages 110, 2024. [12] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing In Proceedings of the IEEE/CVF your commands via motion diffusion in latent space. conference on computer vision and pattern recognition, pages 1800018010, 2023. [13] Peishan Cong, Ziyi Wang, Zhiyang Dou, Yiming Ren, Wei Yin, Kai Cheng, Yujing Sun, Xiaoxiao Long, Xinge Zhu, and Yuexin Ma. Laserhuman: Language-guided scene-aware human motion generation in free environment. arXiv preprint arXiv:2403.13307, 2024. [14] Wenxun Dai, Ling-Hao Chen, Yufei Huo, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Real-time controllable motion generation via latent consistency model. arXiv preprint, 2024. [15] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Real-time controllable motion generation via latent consistency model. In ECCV, pages 390408, 2025. 10 [16] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [17] Christian Diller and Angela Dai. Cg-hoi: Contact-guided 3d human-object interaction generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1988819901, 2024. [18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [19] Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, and Philipp Slusallek. In Proceedings of the Synthesis of compositional animations from textual descriptions. IEEE/CVF international conference on computer vision, pages 13961406, 2021. [20] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Slusallek. Remos: Reactive 3d motion synthesis for two-person interactions. arXiv preprint arXiv:2311.17057, 2023. [21] Jingyu Gong, Chong Zhang, Fengqi Liu, Ke Fan, Qianyu Zhou, Xin Tan, Zhizhong Zhang, Yuan Xie, and Lizhuang Ma. Diffusion implicit policy for unpaired scene-aware motion synthesis. arXiv preprint arXiv:2412.02261, 2024. [22] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [23] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910, 2024. [24] Chuan Guo, Yuxuan Mu, Xinxin Zuo, Peng Dai, Youliang Yan, Juwei Lu, and Li Cheng. Generative human motion stylization in latent space. arXiv preprint arXiv:2401.13505, 2024. [25] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 51525161, June 2022. [26] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In European Conference on Computer Vision, pages 580597. Springer, 2022. [27] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation of 3d human motions. In Proceedings of the 28th ACM International Conference on Multimedia, pages 20212029, 2020. [28] Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. [29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [30] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [31] Vincent Tao Hu, Wenzhe Yin, Pingchuan Ma, Yunlu Chen, Basura Fernando, Yuki Asano, Efstratios Gavves, Pascal Mettes, Bjorn Ommer, and Cees GM Snoek. Motion flow matching for human motion synthesis and editing. arXiv preprint arXiv:2312.08895, 2023. [32] Siyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusion-based generation, optimization, and planning in 3d scenes. In CVPR, 2023. 11 [33] Yiheng Huang, Hui Yang, Chuanchen Luo, Yuxi Wang, Shibiao Xu, Zhaoxiang Zhang, Man Zhang, and Junran Peng. Stablemofusion: Towards robust and efficient diffusion-based motion generation framework. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 224232, 2024. [34] Yiming Huang, Weilin Wan, Yue Yang, Chris Callison-Burch, Mark Yatskar, and Lingjie Liu. Como: Controllable motion generation through language guided pose code editing. In European Conference on Computer Vision, pages 180196. Springer, 2025. [35] Inwoo Hwang, Bing Zhou, Young Min Kim, Jian Wang, and Chuan Guo. Scenemi: Motion in-betweening for modeling human-scene interactions. arXiv preprint arXiv:2503.16289, 2025. [36] Muhammad Gohar Javed, Chuan Guo, Li Cheng, and Xingyu Li. Intermask: 3d human interaction generation via collaborative masked modelling. arXiv preprint arXiv:2410.10010, 2024. [37] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079, 2023. [38] Nan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, and Siyuan Huang. Scaling up dynamic human-scene interaction modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17371747, 2024. [39] Peng Jin, Yang Wu, Yanbo Fan, Zhongqian Sun, Wei Yang, and Li Yuan. Act as you wish: Fine-grained control of motion diffusion model with hierarchical semantic graphs. Advances in Neural Information Processing Systems, 36, 2024. [40] Roy Kapon, Guy Tevet, Daniel Cohen-Or, and Amit Bermano. Mas: Multi-view ancestral sampling for 3d motion generation using 2d diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19651974, 2024. [41] Korrawe Karunratanakul, Konpat Preechakul, Emre Aksan, Thabo Beeler, Supasorn Suwajanakorn, and Siyu Tang. Optimizing diffusion noise can serve as universal motion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13341345, 2024. [42] Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang. Guided motion diffusion for controllable human motion synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21512162, 2023. [43] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-form language-based motion synthesis & editing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 82558263, 2023. [44] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes, 2013. [45] Nilesh Kulkarni, Davis Rempe, Kyle Genova, Abhijit Kundu, Justin Johnson, David Fouhey, and Leonidas Guibas. Nifty: Neural object interaction fields for guided human motion synthesis. In CVPR, 2024. [46] Jiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig, and Karen Liu. Controllable human-object interaction synthesis. In European Conference on Computer Vision, pages 5472. Springer, 2025. [47] Jiaman Li, Karen Liu, and Jiajun Wu. Lifting motion to the 3d world via 2d diffusion. arXiv preprint arXiv:2411.18808, 2024. [48] Jiaman Li, Jiajun Wu, and Karen Liu. Object motion guided human motion synthesis. ACM Transactions on Graphics (TOG), 42(6):111, 2023. [49] Zhe Li, Yisheng He, Lei Zhong, Weichao Shen, Qi Zuo, Lingteng Qiu, Zilong Dong, Laurence Tianruo Yang, and Weihao Yuan. Mulsmo: Multimodal stylized motion generation by bidirectional control flow. In arXiv 2412.09901, 2024. [50] Zhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong, Zilong Dong, and Laurence T. Yang. Lamp: Language-motion pretraining for motion generation, retrieval, and captioning. In arXiv 2410.07093, 2024. [51] Han Liang, Jiacheng Bao, Ruichi Zhang, Sihan Ren, Yuecheng Xu, Sibei Yang, Xin Chen, Jingyi Yu, and Lan Xu. Omg: Towards open-vocabulary motion generation via mixture of controllers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 482493, 2024. [52] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and Lan Xu. Intergen: Diffusion-based multi-human motion generation under complex interactions. International Journal of Computer Vision, pages 121, 2024. [53] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and Lan Xu. Intergen: Diffusion-based multi-human motion generation under complex interactions. International Journal of Computer Vision, pages 121, 2024. [54] Ting-Hsuan Liao, Yi Zhou, Yu Shen, Chun-Hao Paul Huang, Saayan Mitra, Jia-Bin Huang, and Uttaran Bhattacharya. Shape my moves: Text-driven shape-aware synthesis of human motions. arXiv preprint arXiv:2504.03639, 2025. [55] Angela S. Lin, Lemeng Wu, Rodolfo Corona, Kevin W. H. Tai, Qi-Xing Huang, and Raymond J. Mooney. Generating animated videos of human activities from natural language descriptions. arXiv preprint, 2018. [56] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: large-scale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems, 2023. [57] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [58] Xinpeng Liu, Haowen Hou, Yanchao Yang, Yong-Lu Li, and Cewu Lu. Revisit human-scene interaction via space occupancy. In European Conference on Computer Vision, pages 119. Springer, 2025. [59] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. Smpl: skinned multi-person linear model. ACM Trans. Graph., 34(6), October 2015. [60] Yuke Lou, Yiming Wang, Zhen Wu, Rui Zhao, Wenjia Wang, Mingyi Shi, and Taku Komura. Zero-shot human-object interaction synthesis with multimodal priors. arXiv preprint arXiv:2503.20118, 2025. [61] Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang, and Yi Yang. Diversemotion: Towards diverse human motion generation via discrete diffusion. arXiv preprint arXiv:2309.01372, 2023. [62] Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, and Ruimao Zhang. Scamo: Exploring the scaling law in autoregressive motion generation model. arXiv preprint arXiv:2412.14559, 2024. [63] Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, and Ruimao Zhang. Scamo: Exploring the scaling law in autoregressive motion generation model. arXiv preprint arXiv:2412.14559, 2024. [64] Sihan Ma, Qiong Cao, Jing Zhang, and Dacheng Tao. Contact-aware human motion generation from textual descriptions. arXiv preprint arXiv:2403.15709, 2024. [65] Naureen Mahmood, Nima Ghorbani, Nikolaus Troje, Gerard Pons-Moll, and Michael Black. Amass: Archive of motion capture as surface shapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 54425451, 2019. 13 [66] Zichong Meng, Yiming Xie, Xiaogang Peng, Zeyu Han, and Huaizu Jiang. Rethinking diffusion for text-driven human motion generation. arXiv preprint arXiv:2411.16575, 2024. [67] Carnegie Mellon University CMU Graphics Lab motion capture library. Carnegie mellon university - cmu graphics lab - motion capture library. Carnegie Mellon University - CMU Graphics Lab - motion capture library, 2017. [68] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [69] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [70] Xiaogang Peng, Yiming Xie, Zizhao Wu, Varun Jampani, Deqing Sun, and Huaizu Jiang. Hoi-diff: Text-driven synthesis of 3d human-object interactions using diffusion models. arXiv preprint arXiv:2312.06553, 2023. [71] Mathis Petrovich, Michael Black, and Gül Varol. Temos: Generating diverse human motions from textual descriptions. In ECCV, 2022. [72] Mathis Petrovich, Michael J. Black, and Gül Varol. TMR: Text-to-motion retrieval using contrastive 3D human motion synthesis. In ICCV, 2023. [73] Mathis Petrovich, Or Litany, Umar Iqbal, Michael Black, Gul Varol, Xue Bin Peng, and Davis Rempe. Multi-track timeline control for text-driven 3d human motion generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19111921, 2024. [74] Huaijin Pi, Ruoxi Guo, Zehong Shen, Qing Shuai, Zechen Hu, Zhumei Wang, Yajiao Dong, Ruizhen Hu, Taku Komura, Sida Peng, et al. Motion-2-to-3: Leveraging 2d motion data to boost 3d motion generation. arXiv preprint arXiv:2412.13111, 2024. [75] Huaijin Pi, Sida Peng, Minghui Yang, Xiaowei Zhou, and Hujun Bao. Hierarchical generation of human-object interactions with diffusion probabilistic models. In ICCV, 2023. [76] Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Korrawe Karunratanakul, Pu Wang, Hongfei Xue, Chen Chen, Chuan Guo, Junli Cao, Jian Ren, and Sergey Tulyakov. Controlmm: Controllable masked motion generation. arXiv preprint arXiv:2410.10780, 2024. [77] Ekkasit Pinyoanuntapong, Muhammad Usama Saleem, Pu Wang, Minwoo Lee, Srijan Das, and Chen Chen. Bamm: Bidirectional autoregressive motion model. arXiv preprint arXiv:2403.19435, 2024. [78] Ekkasit Pinyoanuntapong, Pu Wang, Minwoo Lee, and Chen Chen. Mmm: Generative masked motion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15461555, 2024. [79] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big data, 4(4):236252, 2016. [80] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [81] Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, and Or Litany. Trace and pace: Controllable pedestrian animation via guided trajectory diffusion. In CVPR, 2023. [82] Pablo Ruiz-Ponce, German Barquero, Cristina Palmero, Sergio Escalera, and José GarcíaRodríguez. Mixermdm: Learnable composition of human motion diffusion models. arXiv preprint arXiv:2504.01019, 2025. [83] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations by error propagation. In Sematic Scholar, 1986. [84] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit Bermano. Human motion diffusion as generative prior. arXiv preprint arXiv:2303.01418, 2023. [85] Xu Shi, Chuanchen Luo, Junran Peng, Hongwen Zhang, and Yunlian Sun. Generating finegrained human motions using chatgpt-refined descriptions. arXiv preprint arXiv:2312.02772, 2023. [86] Yi Shi, Jingbo Wang, Xuekun Jiang, Bingkun Lin, Bo Dai, and Xue Bin Peng. Interactive character control with auto-regressive motion diffusion models. ACM Transactions on Graphics (TOG), 43(4):114, 2024. [87] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [88] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [89] Shanlin Sun, Gabriel De Araujo, Jiaqi Xu, Shenghan Zhou, Hanwen Zhang, Ziheng Huang, Chenyu You, and Xiaohui Xie. Coma: Compositional human motion generation with multimodal agents. arXiv preprint arXiv:2412.07320, 2024. [90] Guy Tevet, Brian Gordon, Amir Hertz, Amit Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In European Conference on Computer Vision, pages 358374. Springer, 2022. [91] Guy Tevet, Sigal Raab, Setareh Cohan, Daniele Reda, Zhengyi Luo, Xue Bin Peng, Amit Bermano, and Michiel van de Panne. Closd: Closing the loop between simulation and diffusion for multi-task character control. arXiv preprint arXiv:2410.03441, 2024. [92] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. In The Eleventh International Conference on Learning Human motion diffusion model. Representations, 2023. [93] Shashank Tripathi, Omid Taheri, Christoph Lassner, Michael Black, Daniel Holden, and In European Carsten Stoll. Humos: Human motion model conditioned on body shape. Conference on Computer Vision, pages 133152. Springer, 2025. [94] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [95] Weilin Wan, Zhiyang Dou, Taku Komura, Wenping Wang, Dinesh Jayaraman, and Lingjie Liu. Tlcontrol: Trajectory and language control for human motion synthesis. arXiv preprint arXiv:2311.17135, 2023. [96] Weilin Wan, Yiming Huang, Shutong Wu, Taku Komura, Wenping Wang, Dinesh Jayaraman, and Lingjie Liu. Diffusionphase: Motion diffusion in frequency domain. arXiv preprint arXiv:2312.04036, 2023. [97] Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. Towards diverse and natural scene-aware 3d human motion synthesis. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 2042820437. IEEE, June 2022. [98] Wenjia Wang, Liang Pan, Zhiyang Dou, Zhouyingcheng Liao, Yuke Lou, Lei Yang, Jingbo Wang, and Taku Komura. Sims: Simulating human-scene interactions with real world script planning. arXiv preprint arXiv:2411.19921, 2024. [99] Xinghan Wang, Zixi Kang, and Yadong Mu. Text-controlled motion mamba: Text-instructed temporal grounding of human motion. arXiv preprint arXiv:2404.11375, 2024. [100] Zhenzhi Wang, Jingbo Wang, Dahua Lin, and Bo Dai. Intercontrol: Generate human motion interactions by controlling every joint. arXiv preprint arXiv:2311.15864, 2023. 15 [101] Qianyang Wu, Ye Shi, Xiaoshui Huang, Jingyi Yu, Lan Xu, and Jingya Wang. Thor: Text to human-object interaction diffusion via relation intervention. arXiv preprint arXiv:2403.11208, 2024. [102] Lixing Xiao, Shunlin Lu, Huaijin Pi, Ke Fan, Liang Pan, Yueer Zhou, Ziyong Feng, Xiaowei Zhou, Sida Peng, and Jingbo Wang. Motionstreamer: Streaming motion generation via diffusion-based autoregressive model in causal latent space. arXiv preprint arXiv:2503.15451, 2025. [103] Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. Omnicontrol: Control any joint at any time for human motion generation. In The Twelfth International Conference on Learning Representations, 2024. [104] Liang Xu, Shaoyang Hua, Zili Lin, Yifan Liu, Feipeng Ma, Yichao Yan, Xin Jin, Xiaokang Yang, and Wenjun Zeng. Motionbank: large-scale video motion benchmark with disentangled rule-based annotations. arXiv preprint arXiv:2410.13790, 2024. [105] Liang Xu, Xintao Lv, Yichao Yan, Xin Jin, Shuwen Wu, Congsheng Xu, Yifan Liu, Yizhou Zhou, Fengyun Rao, Xingdong Sheng, et al. Inter-x: Towards versatile human-human interaction analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2226022271, 2024. [106] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan Gui. Interdiff: Generating 3d human-object interactions with physics-informed diffusion. In ICCV, 2023. [107] Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, and Liang-Yan Gui. Intermimic: Towards universal whole-body control for physics-based human-object interactions. arXiv preprint arXiv:2502.20390, 2025. [108] Sirui Xu, Ziyin Wang, Yu-Xiong Wang, and Liang-Yan Gui. Interdreamer: Zero-shot text to 3d dynamic human-object interaction. arXiv preprint arXiv:2403.19652, 2024. [109] Mengqing Xue, Yifei Liu, Ling Guo, Shaoli Huang, and Changxing Ding. Guiding humanobject interactions with rich geometry and relations. arXiv preprint arXiv:2503.20172, 2025. [110] Tatsuro Yamada, Hiroyuki Matsunaga, and Tetsuya Ogata. Paired recurrent autoencoders for bidirectional translation between robot actions and linguistic descriptions. IEEE Robotics and Automation Letters, 3(4):34413448, 2018. [111] Sheng Yan, Yang Liu, Haoqiang Wang, Xin Du, Mengyuan Liu, and Hong Liu. Cross-modal retrieval for motion and text via droptriple loss. In Proceedings of the 5th ACM International Conference on Multimedia in Asia, pages 17, 2023. [112] Payam Jome Yazdian, Eric Liu, Rachel Lagasse, Hamid Mohammadi, Li Cheng, and Angelica Lim. Motionscript: Natural language descriptions for expressive 3d human motions. arXiv preprint arXiv:2312.12634, 2023. [113] Hongwei Yi, Justus Thies, Michael Black, Xue Bin Peng, and Davis Rempe. Generating human interaction motions in scenes with text control. In European Conference on Computer Vision, pages 246263. Springer, 2025. [114] Heng Yu, Juze Zhang, Changan Chen, Tiange Xiang, Yusu Fang, Juan Carlos Niebles, and Ehsan Adeli. Socialgen: Modeling multi-human social interaction with language models. arXiv preprint arXiv:2503.22906, 2025. [115] Weihao Yuan, Weichao Shen, Yisheng He, Yuan Dong, Xiaodong Gu, Zilong Dong, Liefeng Bo, and Qixing Huang. Mogents: Motion generation based on spatial-temporal joint modeling. arXiv preprint arXiv:2409.17686, 2024. [116] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physics-guided human motion diffusion model. In ICCV, 2023. [117] Jianrong Zhang, Hehe Fan, and Yi Yang. Energymogen: Compositional human motion generation with energy-based diffusion model in latent space. arXiv preprint arXiv:2412.14706, 2024. [118] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations. arXiv preprint arXiv:2301.06052, 2023. [119] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [120] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022. [121] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 364373, 2023. [122] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024. [123] Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. Motion mamba: Efficient and long sequence motion generation. In European Conference on Computer Vision, pages 265282. Springer, 2024. [124] Zeyu Zhang, Yiran Wang, Wei Mao, Danning Li, Rui Zhao, Biao Wu, Zirui Song, Bohan Zhuang, Ian Reid, and Richard Hartley. Motion anything: Any to motion generation. arXiv preprint arXiv:2503.06955, 2025. [125] Zihan Zhang, Richard Liu, Rana Hanocka, and Kfir Aberman. Tedi: Temporally-entangled diffusion for long-term motion synthesis. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [126] Kaifeng Zhao, Gen Li, and Siyu Tang. Dart: diffusion-based autoregressive motion model for real-time text-driven motion control. arXiv preprint arXiv:2410.05260, 2024. [127] Kaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, and Siyu Tang. Synthesizing diverse human motions in 3d indoor scenes. In International conference on computer vision (ICCV), 2023. [128] Chongyang Zhong, Lei Hu, Zihao Zhang, and Shihong Xia. Attt2m: Text-driven human motion generation with multi-perspective attention mechanism. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 509519, 2023. [129] Lei Zhong, Yiming Xie, Varun Jampani, Deqing Sun, and Huaizu Jiang. Smoodi: Stylized In European Conference on Computer Vision, pages 405421. motion diffusion model. Springer, 2025. [130] Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, and Lingjie Liu. Emdm: Efficient motion diffusion model for fast and high-quality motion generation. In European Conference on Computer Vision, pages 1838. Springer, 2025. [131] Yi Zhou, Chenglei Wu, Zimo Li, Chen Cao, Yuting Ye, Jason Saragih, Hao Li, and Yaser Sheikh. Fully convolutional mesh autoencoder using efficient spatially varying kernels. Advances in neural information processing systems, 33:92519262, 2020. [132] Wenjie Zhuo, Fan Ma, and Hehe Fan. Infinidreamer: Arbitrarily long human motion generation via segment score distillation. arXiv preprint arXiv:2411.18303, 2024. [133] Qiran Zou, Shangyuan Yuan, Shian Du, Yu Wang, Chang Liu, Yi Xu, Jie Chen, and Xiangyang Ji. Parco: Part-coordinating text-to-motion synthesis. In European Conference on Computer Vision, pages 126143. Springer, 2025. 17 We further discuss our proposed approach with the following supplementary materials:"
        },
        {
            "title": "Appendix",
            "content": "Appendix A: Diffusion Prelimenary. Appendix B: Additional Implementation and Metric Details. Appendix C: Detailed ACMDM Model and Patch Size Scaling Results. Appendix D: Detailed Text Driven Controllable Motion Generation Results. Appendix E: Quantitative Text-to-Motion Generation Results on the KIT Dataset. Appendix F: Text Driven Controllable Motion Generation Results with DNO Approach. Appendix G: Quantitative Results on Autoregressive Diffusion Models. Appendix H: Benefit of Direct Text-to-SMPL-H Mesh Vertices Motion Generation. Appendix I: An Explanations on Fully Absolute in Global Space v.s. Joint-Locally Absolute. Appendix J: Additional Qualitative Results of ACMDM. Appendix K: Computation Resources and Training Time. Appendix L: Limitations. Diffusion-based Text-to-Motion Generation Formulation. Diffusion-based text-to-motion models obtains noisy versions of ground-truth motion x0 through an interpolation process. Following DDPM [29], the forward process is: xt = αt, x0 + 1 αt, ϵ, (1) where αt controls the pace of the diffusion process where 0 = αT < < α0 = 1 with assumption that xT (0, I). Alternatively, flow-matching [57] methods define linear interpolation: xt = (1 t)x0 + tϵ, (2) with continuous timestep where [0,1). During training, the model predicts diffusion target, such as x0 or ϵ for DDPM-based methods, and velocity for flow-matching methods given xt and t. The diffusion models are typically optimized using simple MSE loss between the predicted target and its ground truth counterpart. During inference, starting from random Gaussian noise xT , the model iteratively predicts intermediate states by estimating x0, ϵ, or and updates to xt1 via the learned reverse process: typically solving an SDE function for DDPM-based methods, or an ODE function for flow-matching methods."
        },
        {
            "title": "B Additional Implementation and Metric Details",
            "content": "AE Model Details All AutoEncoder variants use 3-block of 3-layer ResNet-based encoder-decoder architecture with hidden dimension of 512, output latent channel of 4, and total temporal downsampling factor of 4. All AutoEncoders are trained with batch size of 256, where each sample contains 64 frames. We train for 50 epochs, and apply learning rate decay by factor of 20 at the 150,000th iteration. KIT Dataset Details KIT-ML includes 3,911 motion clips from the KIT and CMU [67] datasets, annotated with 6,278 textual descriptions (14 per motion), and downsampled to 12.5 FPS. Evaluation Metric Detials We adopt the more robust and recent evaluation framework proposed in [66], which focuses on essential, animatable dimensions of generated motion. Specifically, we use the following metrics following[25, 66]: (1) R-Precision (Top-1, Top-2, and Top-3 accuracies) and Matching, which measures the semantic alignment between generated motion embeddings and their corresponding captions glove embedding; (2) Fréchet Inception Distance (FID), which assesses the statistical similarity between ground truth and generated motion distributions; and (3) MultiModality, which measures the diversity of generated motion embeddings per same text prompt. (4) CLIP-Score, which is the cosine similarity between the generated motion and its caption via CLIP embeddings. 18 Figure A1: Model and patch size scaling results of ACMDM. Top row: FID and R-Precision Top 1 are compared while holding patch size constant. Bottom row: Results are shown while holding model size constant. Our model exhibits strong scalability with increasing model capacity and decreasing patch size. For trajectory-control-specific evaluations, following [42], we additionally report the following metrics: Diversity, which measures variability within the generated motions; Foot Skating Ratio, which indicates the physical plausibility of the motion by quantifying slippage artifacts; Trajectory Error, Location Error, and Average Joint Error, which evaluate the accuracy of controlled joint positions at keyframes. To assess performance under varying supervision levels, we report the average results over five different control sparsity levelsusing randomly sampled 1%, 2%, 5%, 25%, and 100% of the ground-truth keyframes as control inputs. During training, control keyframe intensities are randomly sampled. For mesh vertex generation, we also include Laplacian Surface Distance (LSD) to assess the quality of the generated mesh that preserves the structural shape of the ground-truth T-pose. For all ACMDM evaluations, we convert absolute joint coordinates or extract joints from generated mesh vertices and process them into essential HumanML3D evaluation features for consistent and fair comparisons across all ACMDM and baseline methods."
        },
        {
            "title": "C Detailed ACMDM Model and Patch Size Scaling Results",
            "content": "In Figure 4 of the main paper, we visualize the scalability of ACMDM across different model and patch sizes. Here, we provide the complete table of results in Table A1 and further visualization results in Figure A1. In Table A1, we cover all ACMDM variants with and without classifierfree guidance. These results further underscore that our proposed formulation scales effectively, consistently benefiting from increased model capacity and finer spatial resolution to achieve strong improvements in motion generation quality."
        },
        {
            "title": "D Detailed Text Driven Controllable Motion Generation Results",
            "content": "In the main paper, we presented summarized version of the controllable motion generation comparison results. In Table A2, we provide the complete evaluation table across all joints, following the protocol of OmniControl [103]. Compared to prior methods, our approach not only achieves superior performance on every controlled joint but also enables significantly faster inference (2.51 AITS v.s. 81.0 for OmniControl), demonstrating both efficiency and effectiveness of our proposed formulation. Quantitative Text-to-Motion Generation Results on the KIT Dataset In Table A3, we present quantitative text-to-motion generation results on the KIT dataset, comparing ACMDM against state-of-the-art baselines. Notably, even our smallest variant, ACMDM-S-PS22, already surpasses all prior methods across key evaluation metrics such as FID, R-Precision, Matching Score, and CLIP-Score, further demonstrating the effectiveness our proposed formulation. 19 Table A1: ACMDM model and patch size scaling results on the HumanML3D dataset grouped by model size and patch size. We present all ACMDM variants performances with and without CFG."
        },
        {
            "title": "Patch CFG",
            "content": "FID 8 head 512 dim 12 head 768 dim 16 head 1024 dim XL 20 head 1280 dim 22 11 22 11 2 22 11 22 11 2 0.178.009 0.109.005 0.153.010 0.107.004 0.149.009 0.101.005 0.145.011 0.086.004 0.144.009 0.078.003 0.141.010 0.075.004 0.181.009 0.078.003 0.175.007 0.073.003 0.171.009 0.070.003 0.194.009 0.065.004 0.181.008 0.063.004 0.173.008 0.058.004 Top 1 0.399.002 0.508.002 0.415.002 0.509.003 0.424.003 0.514.003 0.435.002 0.513.003 0.448.003 0.514.002 0.446.003 0.517.002 0.447.003 0.516.003 0.451.002 0.518.002 0.459.003 0.519.003 0.461.002 0.519.003 0.462.002 0.520.003 0.467.002 0.522.002 R-Precision Top 2 0.577.003 0.701.003 0.596.002 0.704.002 0.606.003 0.707.002 0.618.003 0.707.003 0.633.002 0.709.003 0.634.002 0.710.003 0.630.003 0.709.003 0.637.003 0.710.003 0.643.004 0.711.003 0.645.003 0.711.003 0.650.003 0.712.002 0.655.003 0.713. Top 3 0.682.003 0.798.003 0.698.003 0.799.002 0.707.003 0.802.001 0.719.003 0.801.003 0.731.002 0.802.002 0.733.002 0.803.003 0.731.003 0.803.002 0.738.003 0.803.003 0.743.004 0.804.003 0.746.003 0.805.003 0.750.003 0.806.002 0.757.003 0.807.002 Matching CLIP-score 3.938.013 3.253.010 3.826.010 3.251.008 3.764.011 3.227.009 3.697.013 3.214.010 3.627.012 3.211.009 3.613.010 3.209.008 3.628.011 3.210.009 3.591.015 3.208.011 3.556.013 3.207.010 3.542.013 3.209.009 3.532.011 3.206.009 3.521.010 3.205.008 0.558.001 0.639.001 0.571.001 0.642.001 0.578.001 0.644.001 0.589.001 0.646.001 0.597.001 0.647.001 0.598.001 0.648.001 0.601.001 0.648.001 0.604.001 0.649.001 0.608.001 0.650.001 0.611.001 0.650.001 0.611.001 0.651.001 0.613.001 0.652."
        },
        {
            "title": "F Text Driven Controllable Motion Generation Results with DNO Approach",
            "content": "In Table A4, we demonstrate that our absolute coordinate formulation also supports input noise optimization following DNO [41] for text-driven controllable motion generation. However, we strongly discourage using this approach due to its heavy time cost (27.8 AITS) from multi-round optimization and high computational burden from gradient accumulation over 10 iterations with the Euler ODE Solver. Employing higher-order solvers like Euler-50 or DOPRI-5 would further increase both gradient steps and inference time, making the method impractical. Quantitative Results on Autoregressive Diffusion Models. Our absolute coordinate formulation is not limited to standard diffusion models, it also generalizes well to autoregressive (AR) diffusion approaches. In Table A5, we report results using three AR variants: (1) Masked AR, which predicts masked latent segments conditioned on previous unmasked motion; (2) Prefix AR, which generates future motion autoregressively from fixed-length 20frame prefix; and (3) Noisy Conditioned AR, which is trained on noisy versions of arbitrary-length prefixes and performs inference with clean prefixes. Across all AR variants, our absolute coordinate formulation consistently achieves strong performance across evaluation metrics, highlighting the flexibility and effectiveness of our approach. Benefit of Direct Text-to-SMPL-H Mesh Vertices Motion Generation Compared to the common pipeline of generating joints followed by mesh fitting, direct SMPLH-H mesh generation produces more natural mesh motion without jittering body parts. It can implicitly model nuanced hand and dynamic flesh movements and help to prevent self-penetration. We provide qualitative visualizations in Appendix to further illustrate these advantages. Table A2: Quantitative text-conditioned motion generation with spatial control signals and upper-body editing on HumanML3D. In the first section, methods are trained and evaluated solely on pelvis controls. In the middle section, methods are trained on all joints and evaluated separately on each controlled joint. The last section presents upper-body editing results. bold face / underline indicates the best/2nd results. Methods Diversity Avg. err. Traj. err. Loc. err. AITS FID Classifier Guidance R-Precision Top 3 Foot Skating Ratio. Controlling Joint Train On Pelvis Pelvis Left foot Right foot"
        },
        {
            "title": "Average",
            "content": "GT MDM [92] PriorMDM [84] GMD [42] OmniContol [103] MotionLCM V2+CtrlNet [14] ACMDM-S-PS22+CtrlNet OmniContol [103] MotionLCM V2+CtrlNet [14] ACMDM-S-PS22+CtrlNet OmniContol [103] MotionLCM V2+CtrlNet [14] ACMDM-S-PS22+CtrlNet OmniContol [103] MotionLCM V2+CtrlNet [14] ACMDM-S-PS22+CtrlNet OmniContol [103] MotionLCM V2+CtrlNet [14] ACMDM-S-PS22+CtrlNet OmniContol [103] MotionLCM V2+CtrlNet [14] ACMDM-S-PS22+CtrlNet OmniContol [103] MotionLCM V2+CtrlNet [14] ACMDM-S-PS22+CtrlNet OmniContol [103] MotionLCM V2+CtrlNet [14] ACMDM-S-PS22+CtrlNet"
        },
        {
            "title": "UpperBody\nEdit",
            "content": "MDM [92] OmniControl [120] MotionLCM V2+CtrlNet [120] ACMDM-S-PS22+CtrlNet 16.34 20.19 137.63 81.00 0.066 2.51 81.00 0.066 2.51 81.0 0.066 2.51 81.00 0.066 2. 81.00 0.066 2.51 81.00 0.066 2.51 81.00 0.066 2.51 81.00 0.066 2.51 AITS 16.34 81.00 0.066 2. -"
        },
        {
            "title": "Classifier\nGuidance",
            "content": "0.000 1.792 0.393 0.238 0.081 3.978 0.067 0.135 4.726 0.075 0.093 4.810 0.063 0.137 4.756 0. 0.146 4.580 0.081 0.119 4.103 0.065 0.128 4.051 0.066 0.126 4.504 0.070 FID 1.918 0.909 3.922 0. 0.795 0.673 0.707 0.763 0.789 0.738 0.805 0.790 0.713 0.805 0.794 0.706 0.800 0.798 0.705 0.803 0.796 0.715 0. 0.783 0.726 0.804 0.792 0.725 0.802 0.792 0.715 0.803 10.455 9.131 9.847 10.011 10.323 9.249 10.481 10.314 9.209 10. 10.338 9.158 10.542 10.241 9.303 10.591 10.239 9.278 10.520 10.217 9.188 10.480 10.309 9.242 10.484 10.276 9.230 10. - 0.000 0.000 0.000 0.1019 0.0897 0.1009 0.0547 0.0901 0.0591 0.0571 0.1162 0. 0.0692 0.1047 0.0590 0.0668 0.1026 0.0583 0.0556 0.1138 0.0598 0.0562 0.1167 0.0604 0.0601 0.1176 0.0599 0.0608 0.1119 0. 0.4022 0.3457 0.0931 0.0387 0.1080 0.0075 0.0404 0.1617 0.0081 0.0594 0.2607 0.0186 0.0666 0.2459 0.0205 0.0422 0.1971 0.0051 0.0801 0.3965 0. 0.0813 0.3822 0.0091 0.0617 0.2740 0.0117 0.3076 0.2132 0.0321 0.0096 0.0581 0.0010 0.0085 0.0841 0.0011 0.0094 0.1229 0.0034 0.0120 0.1127 0. 0.0079 0.0977 0.0009 0.0134 0.1912 0.0014 0.0127 0.1806 0.0016 0.0107 0.1315 0.0019 0.5959 0.4417 0.1439 0.0338 0.1386 0.0100 0.0367 0.1838 0. 0.0314 0.2304 0.0240 0.0334 0.2278 0.0251 0.0349 0.2136 0.0152 0.0529 0.3150 0.0206 0.0519 0.3079 0.0201 0.0404 0.2464 0. R-Precision R-Precision Top 1 0.359 0.428 0.404 0.532 Top 2 0.556 0.614 0.592 0.719 R-Precision Top 0.654 0.722 0.692 0.820 Matching Diversity 4.793 3.694 5.610 3.098 9.210 10.207 9.309 10.586 - - - - - Table A3: Quantitative text-to-motion evaluation on KIT dataset. We repeat the evaluation 20 times and report the average with 95% confidence interval. We use bold face / underline to indicate the best/2nd results."
        },
        {
            "title": "Methods",
            "content": "MDM [92] MotionDiffuse [120] ReMoDiffuse [121] MARDM [66]-ϵ MARDM [66]-v ACMDM-S-PS22 Top 1 0.333.012 0.344.009 0.356.004 0.375.006 0.387.006 0.391.005 R-Precision Top 2 0.561.009 0.536.007 0.572.007 0.597.008 0.610.006 0.615.005 Top 0.689.009 0.658.007 0.706.009 0.739.006 0.749.006 0.752.006 FID Matching MModality CLIP-score 0.585.043 3.845.087 1.725.053 0.340.020 0.242.014 0.237.010 4.002.033 4.167.054 3.735.036 3.489.018 3.374.019 3.368.019 1.681.107 1.774.217 1.928.127 1.479.078 1.312.053 1.267. 0.605.007 0.626.006 0.665.005 0.681.003 0.692.002 0.696.002 An Explanations on Fully Absolute in Global Space v.s. Joint-Locally"
        },
        {
            "title": "Absolute",
            "content": "In HumanML3D [25], absolute joint coordinates are represented as 223 array per frame. Flattening this to 66-dimensional vector and computing mean/std normalization per channel leads to different outcome than computing statistics directly over the three XYZ channels. This is because in the flattened format, after Z-Normalization, even the same value on the same axis but from different joints can correspond to entirely different spatial positions in global space. In contrast, our formulation performs z-normalization directly across the XYZ channels in the global coordinate space, where the 21 Table A4: Quantitative results of DNO-style input noise optimization with ACMDM for textconditioned motion generation under spatial control on HumanML3D dataset."
        },
        {
            "title": "Methods",
            "content": "AITS FID R-Precision Top 3 Foot Skating Ratio. Traj. err. Loc. err. Avg. err."
        },
        {
            "title": "Pelvis",
            "content": "ACMDM-S-PS22+DNO"
        },
        {
            "title": "Left foot",
            "content": "ACMDM-S-PS22+DNO"
        },
        {
            "title": "Right foot",
            "content": "ACMDM-S-PS22+DNO"
        },
        {
            "title": "Head",
            "content": "ACMDM-S-PS22+DNO"
        },
        {
            "title": "Left wrist",
            "content": "ACMDM-S-PS22+DNO Right wrist ACMDM-S-PS22+DNO"
        },
        {
            "title": "Average",
            "content": "ACMDM-S-PS22+DNO 27.8 27.8 27.8 27.8 27. 27.8 27.8 0.151 0.147 0.153 0. 0.149 0.143 0.147 0.802 0.799 0. 0.801 0.799 0.798 0.800 0.0610 0. 0.0597 0.0591 0.0600 0.0598 0.0600 0. 0.0082 0.0086 0.0025 0.0076 0.0081 0. 0.0002 0.0003 0.0003 0.0002 0.0004 0. 0.0003 0.0089 0.0133 0.0138 0.0084 0. 0.0142 0.0121 Table A5: Quantitative results of autoregressive diffusions using our absolute coordinate formulation on the HumanML3D dataset. Our approach consistently performs well across AR variants. Model & Patch Size R-Precision Top 2 Matching CLIP-score"
        },
        {
            "title": "AR Method",
            "content": "Top 1 Top 3 FID ACMDM S-PS"
        },
        {
            "title": "Masked AR",
            "content": "0.117.006 0.042.002 0.115.006 0.111.005 0.496.002 0.504.003 0.497.003 0.509.003 0.690.002 0.700.003 0.690.004 0.702.003 0.786.003 0.798.003 0.788.003 0.799.003 3.354.008 3.212.007 3.343.010 3.250.009 0.634.002 0.640.001 0.636.003 0643. same numeric value consistently refers to the same physical dimension, enhancing spatial coherence and global awareness during model training."
        },
        {
            "title": "J Additional Qualitative Results of ACMDM",
            "content": "We provide comprehensive video visualizations hosted on locally-run, anonymous HTML page to further demonstrate the effectiveness of our approach. These visualizations include detailed comparisons with state-of-the-art text-to-motion generation baselines, showcasing that our method produces more realistic and semantically aligned motions. We also present side-by-side comparisons with existing text-driven controllable motion generation methods, highlighting that our approach not only achieves higher accuracy but also enables significantly faster inference. Additional visualizations illustrate our methods ability to generate diverse and contextually appropriate motions that accurately follow control signals, including spatial editing scenarios. Furthermore, we demonstrate the benefits of directly generating SMPL-H mesh vertices. Compared to the common pipeline of generating joints followed by mesh fitting, our direct mesh generation results in more natural and expressive human motion, including subtle details like soft tissue and flesh dynamics. We showcase additional examples to highlight the quality and realism of our generated mesh-based motions."
        },
        {
            "title": "K Computation Resources and Training Time",
            "content": "All ACMDM models were trained using either NVIDIA RTX 4090 or H200 GPUs, depending on model size. Smaller variants, such as ACMDM-S-PS22, were trained on single RTX 4090 GPU and required approximately 8 hours of training. In contrast, the largest variant, ACMDM-XL-PS2, was trained on an H200 GPU and took approximately 2 days to complete."
        },
        {
            "title": "L Limitations",
            "content": "While ACMDM demonstrates strong scalability and performance, scaling to larger models demands substantial computational resources and extended training times."
        }
    ],
    "affiliations": [
        "Northeastern University"
    ]
}