{
    "paper_title": "All You Need Is A Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching",
    "authors": [
        "Ze Sheng",
        "Qingxiao Xu",
        "Jianwei Huang",
        "Matthew Woodcock",
        "Heqing Huang",
        "Alastair F. Donaldson",
        "Guofei Gu",
        "Jeff Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Our team, All You Need Is A Fuzzing Brain, was one of seven finalists in DARPA's Artificial Intelligence Cyber Challenge (AIxCC), placing fourth in the final round. During the competition, we developed a Cyber Reasoning System (CRS) that autonomously discovered 28 security vulnerabilities - including six previously unknown zero-days - in real-world open-source C and Java projects, and successfully patched 14 of them. The complete CRS is open source at https://github.com/o2lab/afc-crs-all-you-need-is-a-fuzzing-brain. This paper provides a detailed technical description of our CRS, with an emphasis on its LLM-powered components and strategies. Building on AIxCC, we further introduce a public leaderboard for benchmarking state-of-the-art LLMs on vulnerability detection and patching tasks, derived from the AIxCC dataset. The leaderboard is available at https://o2lab.github.io/FuzzingBrain-Leaderboard/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 5 2 2 7 0 . 9 0 5 2 : r All You Need Is Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching Ze Sheng Texas A&M University College Station, US zesheng@tamu.edu Qingxiao Xu Texas A&M University College Station, US qingxiao@tamu.edu Jianwei Huang Texas A&M University College Station, US jwhuang@tamu.edu Matthew Woodcock Texas A&M University College Station, US matthewwoodc0@tamu.edu Heqing Huang City University of Hong Kong Hong Kong, China heqhuang@cityu.edu.hk Alastair F. Donaldson Imperial College London London, UK alastair.donaldson@imperial.ac.uk Guofei Gu Texas A&M University College Station, US guofei@cse.tamu.edu Jeff Huang Texas A&M University College Station, US jeff@cse.tamu.edu Abstract Our team, All You Need Is Fuzzing Brain, was one of seven finalists in DARPAs Artificial Intelligence Cyber Challenge (AIxCC), placing fourth in the final round. During the competition, we developed Cyber Reasoning System (CRS) that autonomously discovered 28 security vulnerabilitiesincluding six previously unknown zero-daysin real-world open-source and Java projects, and successfully patched 14 of them. The complete CRS is open source at github.com/o2lab/afc-crs-all-you-need-is-a-fuzzing-brain. This paper provides detailed technical description of our CRS, with an emphasis on its LLM-powered components and strategies. Building on AIxCC, we further introduce public leaderboard for benchmarking state-of-the-art LLMs on vulnerability detection and patching tasks, derived from the AIxCC dataset. The leaderboard is available at o2lab.github.io/FuzzingBrain-Leaderboard. Keywords Fuzzing, Large Language Model, Vulnerability Detection, Patching ACM Reference Format: Ze Sheng, Qingxiao Xu, Jianwei Huang, Matthew Woodcock, Heqing Huang, Alastair F. Donaldson, Guofei Gu, and Jeff Huang. 2025. All You Need Is Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching. In Proceedings of Proceedings of the XX Conference (AIxCC). ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn Team lead Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. AIxCC, 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/25/06 https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "1.1 POV and Patch Generation\nIn AIxCC, participants are tasked with building autonomous vul-\nnerability detection and patching systems that operate effectively on\nreal-world open-source projects. These systems must fulfill two crit-\nical requirements: (1) automatically generate Proofs-of-Vulnerability\n(POVs), and (2) produce patches in the form of diff files that remedi-\nate the discovered issues.",
            "content": "Proof-of-Vulnerability (POV). Given target software that contains one or more vulnerabilities (either seeded by AIxCC or zero-days), for each vulnerability, generate an input that triggers sanitizer error when processed by fuzzer harness. AIxCC targets vulnerabilities in and Java projects, and it uses OSS-Fuzz [7]- compatible fuzzers, such as libFuzzer and AFL for projects, and Jazzer for Java projects, to prove vulnerability. For projects, the software can be compiled using various sanitizers: AddressSanitizier, MemorySanitizer, and Undefined-BehaviourSanitizer. The POV generation process can be viewed formally as follows: GeneratePOV(H, S) (ℎ, 𝑠𝑎𝑛, 𝐼 ) s.t. Trigger(ℎ, 𝑠𝑎𝑛, 𝐼, 𝑆) = True where 𝐻 represents collection of fuzzer harnesses, 𝑆 is the target software, and ℎ 𝐻 is fuzzer harness, 𝑠𝑎𝑛 sanitizer type, and 𝐼 sanitizer-error triggering input. The generated POV serves as concrete evidence of the vulnerabilitys exploitability and also provides test case for patch validation. Patch Generation. Given target software 𝑆 which contains one or more vulnerabilities, for each vulnerability, generate patch in the form of diff file that satisfies the following requirements: (1) The patched software 𝑆 compiles successfully (i.e., without syntax or build errors); AIxCC, 2025, Ze et al. Table 1: AIxCC Tasks and Challenge Modes Component Description Two Tasks POV Generation Patch Generation Generate binary exploit (.bin) to trigger vulnerability Generate diff patch to fix vulnerability & pass functionality tests Three Challenge Modes Delta-Scan Mode Full-Scan Mode SARIF Assessment Mode Input: Target commit Detect commit-specific vulnerabilities & Generate remediation Input: Complete codebase Full codebase vulnerability discovery & remediation Input: External reports (SARIF) Validate vulnerability reports (2) The patch eliminates all known proofs of the vulnerability: (ℎ, 𝑠𝑎𝑛, 𝐼 ) POVs . Trigger(ℎ, 𝑠𝑎𝑛, 𝐼, 𝑆 ) where POVs is set of known POVs for the given vulnerability, ℎ 𝐻 is fuzzer harness, and 𝑠𝑎𝑛 is sanitizer type; (3) The patch preserves functional correctness according to given set TestSuite of regression tests for the software: 𝑇 TestSuite . Pass(𝑇 , 𝑆 ) These requirements provide degree of confidence that patches not only remove vulnerabilities but also preserve intended functionality, minimizing the introduction of regressions. Importantly, when the patches submitted by team are evaluated, the set POVs for vulnerability is taken to be the union of valid proofs submitted for the vulnerability across all teams. Therefore, to be valid, it is not enough for teams patch to simply eliminate the POVs discovered by the team; the patch must be general enough to work for POVs discovered by other teams as well. Using the diff format for patches enables seamless integration with existing version control systems and open-source development workflows, facilitating review and maintainability."
        },
        {
            "title": "1.2 Three Challenge Modes\nTo address diverse vulnerability management scenarios in real-\nworld software development, AIxCC defines three distinct challenge\nmodes that participating systems shall support: Delta-Scan, Full-\nScan, and Static Analysis Report-Based (SARIF [10]). Each mode\ncorresponds to a different class of inputs and evaluation tasks.",
            "content": "1.2.1 Delta-Scan Mode. This mode targets commit-based vulnerability analysis, focusing on changes introduced by specific commit. Input: target commit 𝐶, repository base state 𝑆base (state before applying commit 𝐶), source code repository 𝑅, and corresponding OSS-Fuzz fuzzer harnesses 𝐻 . Full-Scan Mode. This mode targets capabilities to discover 1.2.2 vulnerabilities across the entire codebase, rather than restricting analysis to single commit. Input: software state 𝑆 (specific version), source code repository 𝑅, and corresponding OSS-Fuzz fuzzer harnesses 𝐻 . SARIF Assessment Mode. This mode validates externally pro1.2.3 vided vulnerability reports, typically from static analysis tools or issue trackers. More details can be found in Section 5. Objective: Confirm the accuracy of reported vulnerabilities and filter out false positives. Input: Structured vulnerability reports (e.g., SARIF) including affected functions, vulnerability classifications, location information, contextual metadata, etc."
        },
        {
            "title": "1.3 Competition Scoring\nThe AIxCC scoring rule incentivizes submission accuracy and\nspeed:",
            "content": "𝑆𝑐𝑜𝑟𝑒 = 𝐴𝑀 (𝑉 𝐷𝑆 + 𝑃𝑅𝑆 + 𝑆𝐴𝑆 + 𝐵𝐷𝐿) Accuracy Multiplier (AM): 𝐴𝑀 = 1 1𝑟 4 , Time Multiplier: 𝑟 = 𝑎𝑐𝑐 𝑎𝑐𝑐+𝑖𝑛𝑎𝑐𝑐 𝜏 = 0.5 + 𝑡𝑖𝑚𝑒𝑟𝑒𝑚 2𝑡𝑖𝑚𝑒𝑤𝑖𝑛𝑑𝑜𝑤 POV (VDS): 2 𝜏 if crash, else 0 Patch (PRS): 6 𝜏 if valid, else 0 SARIF Assessment (SAS): 1 𝜏 if correct, else 0 Bundle Score (BDL): 1 𝜏 if valid, else 0 AIxCC had total number of 60 challenges in the final round. Each challenge score is the sum of valid POV, patch, SARIF assessment, and bundle points, scaled by an accuracy multiplier. Each valid POV earns 2 points, patch 6 points, and SARIF assessment 1 point, all decaying over time to 50% minimum. In addition, bundle points (BDL) are awarded for grouped submissions. Details can be found in the AIxCC final scoring guide [9]."
        },
        {
            "title": "2 Overview of FuzzingBrain Architecture\nAs depicted in Figure 1, FuzzingBrain consists of four core services:\nCRS WebService, Static Analysis Service, Submission Service, and\nWorker Services. All services execute in parallel on separate VM\nnodes. The first three run as single instances, while multiple in-\nstances of the Worker Services are deployed (around 100 VMs in the\nfinal round) to support parallel task execution.",
            "content": "The CRS Web Service acts as the central coordinator. It decomposes tasks, builds fuzzers, and assigns them to worker services. The Static Analysis Service performs static code analyses to answer queries related to function metadata, reachability, and call paths. The Worker Services generate POVs and patches by running fuzzing and LLM-based strategies. The Submission Service interacts with the competition API, handling submission deduplication, SARIF validation, and bundles."
        },
        {
            "title": "2.1 Task Decomposition\nFigure 2 illustrates the workflow for decomposing and distributing\ntasks across FuzzingBrain’s services. Upon receiving a task (which",
            "content": "All You Need Is Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching AIxCC, 2025, Figure 1: Overview of FuzzingBrain Architecture. contains metadata describing the challenge, e.g., delta-scan or full-scan of target project), the CRS Web Service first builds the target project and its fuzzers using OSS-Fuzz utilities. Each fuzzer is an executable binary generated from fuzzer harness instrumented with sanitizer (AddressSanitizer, MemorySanitizer, or UndefinedBehaviorSanitizer for C/C++1, and Jazzer for Java). The service then prepares isolated workspaces by cloning the target repository, constructing project-specific Docker containers, and generating fuzzer binaries for all supported sanitizer configurations. single project may yield dozens of fuzzers: for example, dropbear contains 17 fuzzer harnesses, each compiled with three sanitizers, producing more than 50 binaries. All fuzzers are distributed across the Worker Services to enable concurrent execution. To minimize communication overhead, only the fuzzer path (fuzzer name and sanitizer type) is sent to worker. The corresponding binary and Docker image are reconstructed locally on the worker node. In parallel, the task is also dispatched to the Static Analysis Service and the Submission Service. The former conducts program analysis, while the latter manages bookkeeping for POV and patch submissions."
        },
        {
            "title": "2.2 Libfuzzer and LLM-based Fuzzing\nUpon receiving a target fuzzer, a Worker Service first builds the\ncorresponding fuzzer binary, and then proceeds to generate POV in-\nputs and patches specific to that fuzzer. Both traditional fuzzing and\nLLM-powered fuzzing are performed on the same file system to dis-\ncover inputs that can trigger sanitizer errors. Each fuzzer executes\ninside a Docker container with all required runtime dependencies.\nThe traditional fuzzing setup is intentionally minimal: we rely\nsolely on libFuzzer, whose fuzzing corpus is configured to reside\nin a shared directory accessible by the LLM-based fuzzing strategies.\nAll LLM-based strategies execute in parallel, and the inputs they\ngenerate that do not immediately trigger crashes are preserved\nin the shared corpus. These inputs are often close to valid crash-\ninducing cases and therefore serve as valuable seeds for libFuzzer.\nSince the parallel strategies can generate a very large number of\ntest inputs per second, the shared corpus directory is periodically\ncleaned to prevent uncontrolled growth. Rather than performing\nlibFuzzer’s built-in corpus minimization, our approach removes\nfiles based on age: any input older than 10 minutes is deleted. This\nlightweight policy keeps the corpus size manageable while still\nallowing recently generated inputs (which are more likely to be\nrelevant) to contribute to subsequent fuzzing iterations.",
            "content": "1FuzzingBrain also supports C++, besides and Java projects. AIxCC, 2025, Ze et al. For each LLM-based fuzzing strategy, the Worker Service spawns dedicated Python subprocess. Once POV is identified, the workflow transitions to the patching phase, where multiple patching processes are launched in parallel. FuzzingBrain currently incorporates 23 distinct LLM-based strategies (10 for POV and 13 for patches), categorized by their operational mode and target language focus, as summarized in Table 2. Each strategy executes in an independent process but adheres to unified interface for receiving task specifications and returning results in consistent format. Further details of our LLM-based POV generation and patching strategies are presented in Sections 3 and 4, respectively."
        },
        {
            "title": "2.4 Static Analyses\nUpon receiving a task, the Static Analysis Service performs whole-\nprogram static analysis of the target project and supports three\ntypes of queries from Worker Services: (1) Function Metadata—given\na function name and an optional file name or path, return all match-\ning functions along with their metadata, including parameters and\nsource code; (2) Reachability—given a fuzzer harness, identify all\nfunctions reachable from its entrypoint, returning each function’s\nname, file path, and start/end line numbers; (3) Call Paths—given a\nfuzzing harness and a target function, enumerate call paths from\nthe fuzzer entrypoint to the target. Each call path comprises an\nordered sequence of functions, including their file paths, names,\nand line ranges. To mitigate path explosion, we cap the number of\ncall paths at 20, returning the first 20 if more exist. We also enforce\na maximum call path depth (default: 50 for C/C++ and 10 for Java)\nto avoid excessively long paths. These defaults were chosen heuris-\ntically based on empirical observations from the exhibition rounds:\nin C/C++ projects, vulnerabilities are often buried deep within com-\nplex call chains, making a higher threshold useful, whereas in Java\nprojects, exploitable issues are typically exposed through shorter,\nhigher-level entrypoints, so a smaller depth bound is sufficient in\npractice.",
            "content": "Developing static analysis tools for real-world projects proved to be both challenging and time-consuming. We encountered numerous performance and soundness issues, ultimately leading us to build two customized static analysis frameworks: one for C/C++ and one for Java. We present further details in Section 6."
        },
        {
            "title": "2.5 Submission Deduplication\nThe Submission Service receives all POV and patch submissions\nfrom Worker Services and applies several mechanisms to eliminate",
            "content": "duplicates. Deduplication is essential because redundant submissions reduce the accuracy multiplier. For POV submissions, each entry is accompanied by signature, defined as the crash location (source file and line number) extracted from the crash call stack. If the crash location is unavailable, heuristics are applied to construct signature from the crash output and sanitizer. Two POVs are considered duplicates if they share the same signature. However, distinct signatures may still correspond to the same underlying vulnerability. To capture such cases, we employ LLM-based comparison: crash reports from two POVs are provided as input to three different LLMs, and the submissions are marked as duplicates if at least two of the models consider them redundant. Patch submissions require different strategy, since multiple distinct patches may be valid attempts for the same vulnerability, and some patches may fail during validation. Deduplication is therefore applied more conservatively, using three rules: (1) If two patch diffs are highly similar, we compute their Levenshtein distance and discard duplicates with distance below 10. (2) If two patches are submitted within short interval (3 seconds) and correspond to the same POV signature, the second of the two patches is discarded. (3) We cap the number of patch submissions per vulnerability at five. For XPatch, where no POV is available, we assume that each task corresponds to single introduced vulnerability. In this case, the canonical signature is derived from the task itself, and the submission cap is stricter: at most three XPatches are allowed per task."
        },
        {
            "title": "2.7 Bundle Creation\nEach bundle groups together two or more items (POV, patch, and/or\nSARIF broadcast) that correspond to the same underlying vulnera-\nbility. In our approach, to enable consistent grouping, we associate\neach vulnerability with a canonical signature, defined as the signa-\nture of the first submitted POV. Patch submissions may also include\nan optional pov_signature when the patch is explicitly derived\nfrom a known POV.",
            "content": "Within the Submission Service, bundles are created and updated according to the following rules: POV submissions. Upon receiving POV, if it is not marked as duplicate and its status is passed (as confirmed by the competition API), then: If the POV matches SARIF that has been assessed as true positive, new bundle containing both the POV and SARIF is created. Otherwise, the POV initializes bundle on its own. Patch submissions. Upon receiving patch, if it is not marked as duplicate and its status is passed, then: All You Need Is Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching AIxCC, 2025, Table 2: FuzzingBrain LLM-based Strategies Language Focus Stage delta-scan delta-scan delta-scan delta-scan delta-scan delta-scan delta-scan delta-scan Strategy Name Mode Delta-Scan Strategies xs0_delta as0_delta patch_delta patch0_delta patch1_delta patch2_delta patch3_delta xpatch_delta Full-Scan Strategies xs0_c_full xs0_java_full xs1_c_full xs1_java_full xs2_java_full as0_full patch_full patch0_full patch1_full patch2_full patch3_full xpatch_full Report-Based Strategies sarif_POV0 xpatch_sarif Unharnessed Strategies generate_fuzzer full-scan full-scan full-scan full-scan full-scan full-scan full-scan full-scan full-scan full-scan full-scan full-scan C/C++, Java C/C++, Java C/C++, Java C/C++, Java C/C++, Java C/C++, Java C/C++, Java C/C++, Java C/C++ Java C/C++ Java Java C/C++, Java C/C++, Java C/C++, Java C/C++, Java C/C++, Java C/C++, Java C/C++, Java report-based C/C++, Java report-based C/C++, Java unharnessed C/C++, Java POV Generation POV Generation Patch Generation Patch Generation Patch Generation Patch Generation Patch Generation Patch Generation POV Generation POV Generation POV Generation POV Generation POV Generation POV Generation Patch Generation Patch Generation Patch Generation Patch Generation Patch Generation Patch Generation POV Generation Patch Generation POV Generation If the patch shares pov_signature with an existing POV, the patch is bundled with that POV. If the POV is already in bundle with SARIF, the patch is added to the existing bundle, extending it to include all three components. SARIF broadcasts. Upon receiving SARIF that has been validated as true positive: If the SARIF matches an existing POV, it is bundled together with that POV. If the POV already belongs to bundle (e.g., with patch), the SARIF is added to that bundle."
        },
        {
            "title": "2.8 Technology Stack\nAs illustrated in Figure 2, FuzzingBrain is implemented primarily in\ntwo programming languages: Go and Python. For the CRS services,\nwe selected Go with the Gin web framework. This choice reflects\nGo’s strengths in efficiently handling large numbers of concurrent\noperations and its mature ecosystem for building high-performance,\nproduction-grade web services.",
            "content": "In contrast, all LLM-based POV and patching strategies are implemented as independent Python modules. Python was chosen due to its rich ecosystem for LLM development and its extensive set of third-party libraries, which allow rapid prototyping and flexible experimentation. Each strategy module can execute independently, enabling modularity and isolation. Finally, for model routing, we developed custom framework for model selection, routing, and fallback. Existing off-the-shelf solutions were found to be unreliable and prone to errors under competition workloads (e.g., failing to handle high request volume, rate limits, server overloaded, etc). To enhance robustness against individual model failures or limitations, our framework employs multi-model fallback mechanism. The system maintains prioritized list of models, including those from Anthropic, Google, and OpenAI. When invoking an LLM, the framework attempts models in the predefined order; if one becomes unavailable or encounters an error, the request is automatically redirected to the next available model in the list. This design ensures continuity of service and minimizes disruptions during critical operations."
        },
        {
            "title": "2.9 Parallelization\nA central design principle of FuzzingBrain is parallelization: every\ncomponent that can be parallelized is parallelized, in order to maxi-\nmize the speed of vulnerability discovery and patch generation.",
            "content": "FuzzingBrains deployment infrastructure in the competition environment consists of: Approximately 100 virtual machines, each provisioned with 32192 cores. Each VM runs between 100 and 10,000 threads concurrently. AIxCC, 2025, Ze et al. This large-scale parallelization enables simultaneous processing of multiple challenges while maintaining high resource utilization and system throughput. The result is an architecture capable of scaling efficiently under heavy workloads, ensuring both rapid vulnerability detection and timely patch generation."
        },
        {
            "title": "3 LLM-based POV Strategies\nFuzzingBrain implements a total of 10 LLM-based POV generation\nstrategies: two designed for delta-scans, six for full-scans, one for\nSARIF-based challenges, and one for unharnessed challenges (i.e.,\nthose without fuzzer harnesses and not scored).",
            "content": "All strategies conform to standardized framework built on iterative, dialogue-based interaction with LLMs. This feedbackdriven refinement loop allows the system to incorporate execution results into successive iterations, enabling the LLM to learn from failed attempts and progressively improve its understanding of the target vulnerability. Each strategy executes as an independent process and adheres to unified interface for task inputs and outputs. Strategies utilize five different frontier LLMsclaude-3.7, chatgpt-latest (gpt-4o at the time of the competition), claude-opus-4, o3, and gemini-2.5-pro. For each model, the framework performs multiple generation attempts, up to maximum of five iterations by default. In addition, each strategy is subject to configurable timeout (default: 30 minutes). If model fails to generate valid POV within its iteration limit, the system automatically falls back to the next model in the priority list. This cascading mechanism increases the likelihood of success by leveraging the complementary strengths of different LLMs, as individual models often excel at distinct categories of vulnerability patterns."
        },
        {
            "title": "3.1 Base Strategies\nFuzzingBrain implements a baseline strategy (xs0_delta) for delta-\nscans, as illustrated in Figure 3. This strategy establishes core\nprompting and feedback patterns that most other strategies inherit\nand extend.",
            "content": "The approach operates entirely via multi-turn, text-based dialogue with LLMs, cleanly decoupling analysis logic from the execution environment. System Prompt. We initiate each dialogue with high-level system instruction, for example: You are world-class software vulnerability detection expert. Do not apologize when incorrect; instead, iteratively refine your analysis and proceed. When possible, identify any additional information that would improve your answer. User Message (Inputs). The user message combines the commit diff and the fuzzer harness source code, together with languageand sanitizer-specific guidance: The provided commit introduces vulnerability. Your task is to craft an input for the fuzzer harness that triggers the vulnerability. Your input will be executed by the following harness: [Fuzzer Source Code] [Sanitizer Guidance] [Language-Specific Guidance] Your output must be Python script that creates file named exactly x.bin, and short description of the vulnerability and the target function. Languageand Sanitizer-Specific Guidance. For C/C++ targets, the prompt includes sanitizer-focused instructions for AddressSanitizer (e.g., buffer overflows, use-after-free), MemorySanitizer (e.g., uninitialized reads), and Undefined-BehaviorSanitizer (e.g., integer overflows, pointer misalignment). For Java targets, we include Jazzer-oriented guidance (e.g., deserialization flaws, injection vectors, runtime exceptions). LLM-Generated Python and Execution. We extract the Python code from the LLM response, execute it to generate x.bin, and then run the fuzzer harness on this input. At this stage, the harness is not performing fuzzing; it is simply used as an execution wrapper with sanitizers enabled to detect crashes (i.e., sanitizer errors). If crash occurs, we record successful POV and submit it to the Submission Service. If the attempt fails at any stage, the process continues with another LLM interaction, where the next user message provides structured feedback derived from the failure. For example: Fuzzer output: { truncate_output(fuzzer_output, 200) } The test case did not trigger the vulnerability. Please analyze the output and try again. Consider: (1) alternative input formats/values; (2) edge cases; (3) focusing on functions modified in the commit; (4) careful attention to boundary conditions; (5) step-by-step reasoning. Coverage-Guided Feedback. If x.bin does not trigger crash, we also supply coverage feedback in the next iteration. The feedback summarizes executed functions and branch decisions with 3 lines of surrounding source context. For C/C++, we build with coverage instrumentation, execute the fuzzer on x.bin to produce coverage.profdata, then use llvm-profdata to derive an LCOVstyle report from which we extract executed branches and nearby lines. For Java, we use JaCoCo to collect coverage data, generate report, and post-process it to recover executed methods/branches with corresponding source excerpts."
        },
        {
            "title": "3.2 Advanced Strategies\nThe as0_delta strategy introduces several enhancements over the\nbase approach to improve vulnerability discovery effectiveness. The\nprimary differences among strategies are summarized in Table 3 and\nTable 4. In this section, we highlight advanced modules that extend\nthe base strategy; different strategies compose these modules in\ndifferent combinations.",
            "content": "Multi-Input Generation. Unlike the base strategy, which produces single test case per iteration, as0_delta generates multiple candidates. Each LLM interaction emits Python script that creates five binary inputs (x1.binx5.bin), yielding five exploitation opportunities per iteration instead of one. Vulnerability CategoryBased Prompting. This module enumerates Common Weakness Enumeration (CWE) classes and applies category-specific prompts to guide input generation toward the intended weakness. C/C++ (10 categories): CWE-119: Buffer Overflow CWE-416: Use After Free All You Need Is Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching AIxCC, 2025, Figure 2: Task Distribution & Strategy Running Table 3: Delta-Scan POV Generation Strategies Comparison Input Analysis Core Characteristics Strategy xs0_delta Commit diff analysis Basic strategy, iterative LLM refinement as0_delta Commit diff analysis Multi-input generation Advanced strategy with 5 inputs per try Generation Method Single input per iteration Table 4: Full-Scan POV Generation Strategies Comparison Function Discovery Strategy xs0_c_full Call graph analysis xs0_java_full Call graph analysis xs1_c_full Dual reachability analysis Multi-model LLM ranking xs1_java_full Dual reachability analysis Multi-model LLM ranking xs2_java_full Dual reachability analysis Multi-model LLM ranking Advanced Java vulnerability detection with early termination as0_full Core Characteristics Basic function filtering with simple LLM ranking Basic function filtering with simple LLM ranking Parallel processing with improved function identification Parallel processing with improved function identification Ranking Method LLM ranking LLM ranking Advanced generation with multi-phase strategies LLM ranking + fuzzing Call graph analysis CWE-476: NULL Pointer Dereference CWE-190: Integer Overflow CWE-122: Heap-based Buffer Overflow CWE-787: Out-of-bounds Write CWE-125: Out-of-bounds Read CWE-134: Format String vulnerabilities CWE-121: Stack-based Buffer Overflow CWE-369: Divide by Zero Java (representative examples; our implementation targets 15 categories): CWE-22: Path Traversal CWE-77/78: Command/OS Command Injection CWE-79: Cross-Site Scripting CWE-89: SQL Injection CWE-502: Unsafe Deserialization CWE-611: XML External Entity (XXE) Processing CWE-918: Server-Side Request Forgery (SSRF) AIxCC, 2025, Ze et al. Figure 3: Basic POV Generation Strategy Modified-Function Context Injection. Beyond the commit diff, we identify all modified files and functions and append the full source of each modified function to the prompt to provide precise context. To stay within model context limits and emphasize salient code, we cap the injected source at 2,000 lines per function. Call-PathBased Analysis. This module queries the Static Analysis Service for call paths from the fuzzer entrypoint to all modified (and thus potentially vulnerable) functions. For each call path, the system crafts targeted prompt asking the LLM to generate an input that exercises that path, steering toward code regions likely to trigger the vulnerability. We limit the number of call paths to 20. If all per-path attempts fail, final aggregated prompt combines all paths for one last POV-generation attempt. based on their likelihood of containing vulnerabilities. The ranking incorporates language-specific vulnerability patterns tailored to C/C++ and Java. Enhanced Full-Scan Strategies. The xs1_c_full, xs1_java_full, and xs2_java_full strategies extend the baseline full-scan approach with more refined call graph construction, advanced ranking heuristics, and specialized vulnerability pattern recognition for their respective languages. Advanced Full-Scan Integration. The as0_full strategy integrates all of the above modules (static call graph analysis, LLMbased ranking, and advanced vulnerability heuristics) into unified workflow for large-scale vulnerability discovery across entire codebases."
        },
        {
            "title": "3.3 Full-Scan Strategies\nFor full-scan scenarios, where no commit diff is available, Fuzzing-\nBrain employs a set of strategies that analyze the entire codebase\nto identify potentially vulnerable functions.",
            "content": "Call GraphBased Analysis. The xs0_c_full and xs0_java_full strategies employ static analysis to narrow the search space prior to applying LLM-based vulnerability detection. Specifically, they query the Static Analysis Service to enumerate functions reachable from fuzzer entrypoints. This pruning step typically reduces the candidate set from thousands of functions to more tractable subset of reachable targets. LLM-Based Vulnerable Function Ranking. Once reachable functions are extracted, LLMs are used to score and rank them"
        },
        {
            "title": "4 LLM-Based Patching Strategies\nFuzzingBrain implements 13 LLM-based patching strategies: six\ndesigned for delta-scans, six for full-scans, and one special XPatch\nstrategy for generating patches without POVs.",
            "content": "Except for XPatch, all patching strategies follow the same workflow, illustrated in Figure 4: (1) Target Function Identification: Identify vulnerable functions using strategy-specific heuristics. (2) Metadata Extraction: Retrieve the complete function source code and surrounding context. (3) Patch Generation: Use LLMs to produce revised version of the function body. All You Need Is Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching AIxCC, 2025, Figure 4: Basic Patch Generation Strategy (4) Function Rewrite: Replace the original function with the LLM-generated content. (5) Diff Creation: Generate .diff file using Git differential tools. (6) Validation: Ensure compilation, execute POV tests, and run functionality tests. (7) Iterative Refinement: Provide structured feedback for failed attempts, iterating until success or timeout."
        },
        {
            "title": "4.1 Patch Validation Criteria\nWe define a patch as a code modification that mitigates a vulnerabil-\nity without altering the program’s intended functionality. Patches\nare generated in standard .diff format and must satisfy four vali-\ndation criteria:",
            "content": "(1) Applicability: The patch applies cleanly to the codebase. (2) Compilability: The patched codebase compiles successfully. (3) Vulnerability Mitigation: Known POVs (if available) no longer reproduce the vulnerability. (4) Functionality Preservation: The patched codebase passes its functionality tests. patch is only considered valid if it passes all four criteria."
        },
        {
            "title": "4.2 Basic Patch Strategy\nTable 5 shows a high-level comparison of the patching strategies.\nThe baseline strategy, patch_delta, operates through multi-turn\nLLM-driven conversations, similar to the POV generation process.",
            "content": "In each iteration, the LLM proposes candidate patch, which is validated against the criteria above. Successful patches are submitted; failures trigger detailed feedback and another iteration, up to the MAX_ITERATION limit. Target Function Identification. Target functions are identified as those suspected to be vulnerable (as determined by LLM analysis). This is the most critical step, as the quality of patching depends heavily on accurate function selection. Identification relies on structured prompts that combine commit diffs and crash logs, for example: \"Your task is to identify all potentially vulnerable functions from code commit and crash log. The commit introduces vulnerability. The vulnerability is found by an expert, with crash log.\" Leveraging POV Generation Context. This strategy reuses conversation history from the POV generation phase. The additional context, including prior failure cases and reasoning traces, improves the LLMs ability to generate correct and targeted patches. Function Metadata Extraction. Once target functions are identified, the system queries the Static Analysis Service to extract detailed metadata, including function boundaries, complete source code, and precise file locations. This ensures the LLM has sufficient context to propose syntactically and semantically valid patches. Multi-Model Resilience and Validation. To improve reliability, the patching process employs multiple LLMs from different providers (e.g., Anthropic, Google, and OpenAI). For each model, the system runs an iterative loop in which the LLM generates AIxCC, 2025, Ze et al. Table 5: Delta-Scan Patch Generation Strategies Comparison Function Identification Failure Feedback LLM analysis only Strategy patch_delta & patch_full patch0_delta & patch0_full Diff extraction only patch1_delta & patch1_full patch2_delta & patch2_full patch3_delta & patch3_full LLM + Diff hybrid LLM + Diff LLM + Diff Special Features - Basic crash info - Basic crash info Basic crash info - + Control flow paths Dynamic execution analysis Basic crash info Expert analysis + Sample patches candidate patch, the patch is applied to the codebase, and its validity is tested through compilation, execution of known POVs using the fuzzer harnesses (with sanitizers enabled), and, when available, functionality tests. If the patch fails at any stage, structured feedbackincluding compiler errors, failed diffs, or fuzzer outputis appended to the conversation, and the LLM attempts revised patch in the next iteration. This process repeats until valid patch is produced, the maximum iteration limit is reached, or the timeout expires. If one model fails to produce valid patch, the system automatically falls back to the next model in the priority list."
        },
        {
            "title": "4.3 Greedy Strategy\nThe patch0_delta strategy implements a greedy approach that\nassumes vulnerable functions are guaranteed to appear within the\ncommit diff.",
            "content": "Function Identification Optimization. Rather than relying on LLM analysis, this strategy directly treats all functions present in the commit diff as vulnerable targets: Vulnerable Functions = all modified functions By extracting modified functions directly from the diff, this strategy reduces the computational overhead of LLM-driven identification in scenarios where vulnerabilities are clearly introduced through recent code changes. Hybrid Enhancement. The patch1_delta strategy combines diff-based extraction with LLM analysis: Potential Vulnerable Functions = LLM-identified functions + all modified functions in the diff This hybrid design preserves the efficiency of direct diff extraction while broadening coverage with LLM-derived insights."
        },
        {
            "title": "4.4 Path-Aware Strategy\nThe patch2_delta strategy enhances patch generation through\ndynamic execution analysis and enriched prompting.",
            "content": "Control-Flow Integration. When patches fail validation, runtime control-flow data is incorporated into subsequent LLM prompts, similar to feedback mechanisms in POV generation. For C/C++ projects, coverage data is collected using LLVM profiling; for Java projects, JVM bytecode coverage analysis is employed. This information is embedded in prompts to enable path-aware patch generation. Enhanced Function Discovery. LLM prompting is extended to encourage the inclusion of functions beyond those explicitly mentioned in crash traces. For example: \"You should include all functions that are potentially vulnerable, including not only those that appear in the crash call stack, but also those not directly mentioned in the crash log.\" This approach increases the likelihood of addressing indirect vulnerabilities and related security issues."
        },
        {
            "title": "4.5 Knowledge-Enhanced Strategy\nThe patch3_delta strategy augments patch generation with expert\nvulnerability analysis and a curated patch catalog.",
            "content": "Expert Analysis Integration. The strategy incorporates the initial analysis response from the POV generation phase as an expert assessment, providing contextual understanding of the vulnerabilitys characteristics and exploitation patterns. Sample Patch Catalog. The strategy retrieves vulnerabilityspecific patch examples from catalog indexed by sanitizer signatures and vulnerability categories. These examples serve as concrete guidance for remediation approaches. Context Retrieval. The strategy also supports dynamic context retrieval, enabling the LLM to request additional source code when needed: \"If you need the source code of any other function, please return the file paths and function names in the following JSON format:\" This feature supports comprehensive reasoning over complex vulnerabilities that span multiple functions or require broader program context."
        },
        {
            "title": "4.7 XPatch Strategy\nThe XPatch strategy addresses cases where no POVs or crash logs\nare available.",
            "content": "For delta-scans, this process is relatively straightforward because the commit diff provides strong contextual clues about where the vulnerability was introduced. We extract all modified functions from the diff and supply their full source code as context to the LLM. For full-scans, where no commit information is available, XPatch relies on LLM-based scoring of all fuzzer-reachable functions. The All You Need Is Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching AIxCC, 2025, LLM assigns likelihood scores to candidate functions based on predefined rubrics, and the top 𝑘 functions (default: 5) are saved. These functions are then used as the input context for patch generation. Function Scoring Prompts. The scoring prompt is tailored to both language and vulnerability class. For C/C++, the rubric targets memory safety issues such as off-by-one errors, integer overflows, and buffer boundary violations. Functions are scored from 110 to reflect the likelihood of flaw: 10 indicates certain violation, 79 strong indicators, 26 weak or indirect hints, and 1 no evidence of problems. For Java, multiple specialized rubrics are supported: Malicious logic detection: identifies intentionally harmful constructs such as backdoors, command injection, data exfiltration, privilege escalation, or kill-switches. Scores range from 110, where 10 indicates definite evidence of malicious intent, 79 strong indicators, 26 weak or indirect hints, and 1 no evidence of malicious behavior. Unsafe deserialization: flags dangerous uses of Java deserialization APIs without proper validation (e.g., unfiltered use of ObjectInputStream, XMLDecoder, or SnakeYAML). Scores range from 1-10 (similar to above). In all cases, the LLM outputs JSON array containing function names, assigned scores, and short justifications, sorted by descending score. Only functions with scores 7 are retained. Patch Generation. Once candidate functions are identified, their metadata and source code are provided to the LLM, with explicit instructions that the vulnerability lies within one or more of these functions. The LLM is then tasked with generating candidate patches to mitigate the issue. Patch Validation with LibFuzzer. Since XPatch operates without known POVs, validation relies entirely on fuzzing. After applying candidate patch, we execute LibFuzzer on the patched binary for 60 seconds. If the fuzzer produces new crash during this run, the patch is deemed unsuccessful. Otherwise, the patch is considered valid under the available test conditions."
        },
        {
            "title": "5 SARIF Analysis and Assessment\nOur SARIF (Static Analysis Results Interchange Format) report-\nbased analysis implements a multi-stage processing pipeline that\ntransforms external vulnerability reports into actionable security\nassessments. Figure 5 illustrates the workflow.",
            "content": "Upon receiving SARIF broadcast, the CRS Web Service first parses the data to extract vulnerability metadata, including stack trace information, vulnerability classifications, precise code locations, affected functions, and contextual data. After processing, typical SARIF report yields structured vulnerability information containing: Affected Functions: Target function names and their associated source file locations Vulnerability Classifications: CWE identifiers and rulebased categorizations Location Information: Precise line numbers, file paths, and code regions Contextual Metadata: Severity levels, confidence scores, and analytical tool information Stack Trace Data: Function call sequences and execution flow information Based on this information, we identify potential vulnerable functions and use the Static Analysis Services to determine whether there is fuzzer that can reach these vulnerable functions from its fuzzer input entry point. If not reachable, then we send validation request to the Submission Service. This request includes relevant source code extracted from the target project, based on the vulnerable file, line numbers, and function name specified in the SARIF report (when available). At the Submission Service, three different LLMs are queried in sequence to perform two checks: 1. Determine whether the SARIF is likely false positive. 2. Determine whether the SARIF is likely true positive. If majority consensus is reached, the resulting assessment is submitted to the competition API. If the outcome is inconclusive (e.g., conflicting results or LLM errors), the SARIF is deferred for later reassessment. Two additional mechanisms are then applied: 1. When the Submission Service receives POV, it checks whether the POV corresponds to any unprocessed SARIF reports. If match is found, the SARIF is confirmed as valid. 2. Independently, the SARIF is broadcast to Worker Services to drive POV generation. If POV is successfully generated based on the SARIF, the SARIF is likewise confirmed as valid and submitted as such. Valid SARIF reports provide valuable contextual information, such as vulnerability description and precise source location, which can help improve the effectiveness of POV and patch generation. Accordingly, when SARIF is classified as valid (but no POV has yet been identified) or remains undecided, it is forwarded to Worker Services to support further analysis and exploration. The Submission Service handles SARIF validation because it tracks all POV submissions. This enables direct correlation between SARIF-reported vulnerabilities and actual POV-triggered crashes. The matching algorithm proceeds in two stages: (1) check whether the SARIF artifactLocation (e.g., vulnerable file and line numbers) appears in the POV crash trace, and (2) if necessary, use LLMs to compare the SARIF vulnerability description with the details of the POV submission."
        },
        {
            "title": "6 Static Analysis Implementation\n6.1 Static Analysis for C/C++\nOur C/C++ analysis pipeline integrates three external tools: LLVM [6],\nSVF [8], and Bear [3]. The workflow is as follows: generate LLVM\nbitcode for each fuzzer binary, use SVF to construct a call graph,\ncompute all functions reachable from each fuzzer, and then build\ncall paths from the fuzzer entrypoint to each reachable function.\nTo improve efficiency, all fuzzers are analyzed in parallel, and for\neach fuzzer–function pair, call paths are constructed concurrently\nusing breadth-first search (BFS). The maximum call path depth is\nlimited to 50.",
            "content": "Generating LLVM bitcode presented significant engineering difficulties. Simply appending -emit-llvm to clang often failed due to missing dependent headers. To address this, we first collect all compile commands of the target project into compilation database by building it inside the OSS-Fuzz base Docker image via the command: bear -o /out/compile_commands.json compile. This uses the Bear tool [3] to intercept and store the compilation AIxCC, 2025, Ze et al. Figure 5: SARIF Report-based Strategy command associated with each source file. We then process each compile command entry to produce bitcode for each source file, maintaining separate bitcode sets for fuzzer harnesses and project source files. For each fuzzer, all bitcode files are subsequently linked into single bitcode module. Despite this, errors persisted, requiring fallback heuristics such as hardcoding common header paths and compiler flags. Ultimately, our tool successfully generated bitcode for over 95% of source files across tested projects, including curl, dropbear, and sqlite3. Performance posed an additional challenge. Linked bitcode files could exceed 50 MB, and SVF frequently exceeded 1 hour or ran out of memory when constructing call graphs from such large bitcode files. To mitigate this, we trim oversized bitcode modules to manageable size (e.g., 25 MB), apply lightweight type-based call graph analysis, and enforce 10-minute timeout. If SVF fails to complete within this time budget, the Static Analysis Service will return empty results for that query."
        },
        {
            "title": "6.2 Static Analysis for Java\nFor Java projects, we leverage CodeQL [4], which provides a more\nmature and reliable analysis infrastructure than the toolchain used\nfor C/C++. The workflow consists of three main steps: (1) building a\nCodeQL database from the project source code, (2) executing queries\nagainst the database, (3) decoding query results and computing per-\nfuzzer data structures (i.e., reachable functions and call paths). We\nimplemented two custom queries: one for extracting reachable\nfunctions, and another for computing call paths. These queries",
            "content": "are designed to handle challenges such as function overloading and dynamic loading, which complicate traditional static analysis approaches. To maximize performance, multiple call path queries are batched to reduce database connection overhead (the primary latency bottleneck). The number of fuzzertarget pairs can be extremely large (up to 100K), so we employ batch execution mode with batch size of 1,000. To avoid race conditions, the database is cloned for each fuzzer path, ensuring that every fuzzer operates on an isolated copy. The call path analysis employs balanced approach to cycle handling that prioritizes comprehensive path discovery over strict cycle prevention, that is, the code prevents direct cycles (intermediate methods cannot be the source or target method) but does not fully prevent indirect cycles. This design choice recognizes that indirect cycles often represent legitimate and valuable execution patterns in real world projects, such as recursive algorithms. The Analysis Service mitigates the theoretical risk of infinite recursion through practical depth limit (10 calls), ensuring both computational efficiency and analytical completeness. In addition, we developed lightweight baseline analysis that conservatively identifies function callees using only class types and function names. This approach sacrifices precision but serves as fallback mechanism when CodeQL queries fail or become too costly. All You Need Is Fuzzing Brain: An LLM-Powered System for Automated Vulnerability Detection and Patching AIxCC, 2025, Our Java static analysis pipeline completes within five minutes for representative OSS-Fuzz projects such as Apache Zookeeper, Tika, and Commons-Compress."
        },
        {
            "title": "7 Performance Optimizations\nAcross the three exhibition rounds, we observed that FuzzingBrain\nis effective and fast at producing POVs and patches: the vast major-\nity were generated within the first 30 minutes. Most vulnerabilities\nwere detected under AddressSanitizer, and our LLM-based strate-\ngies discovered nearly all POVs; traditional fuzzing (in our case,\nlibFuzzer) contributed only one or two POVs. However, we also\nexhausted our allocated OpenAI API credits in one round. Post-\nmortem analysis showed that a substantial portion of credits was\nspent on Worker Services assigned to fuzzers that could not reach\nthe vulnerable code, making POV discovery impossible regardless\nof LLM effort.",
            "content": "Some target projects can yield more than 50 fuzzers (fuzzer harnesses {address, memory, undefined} sanitizers). To curb waste and improve throughput for the final round, we applied the following policies: Sanitizer Selection. We disabled UndefinedBehaviorSanitizer for all projects and disabled MemorySanitizer for projects with more than ten fuzzer harnesses, prioritizing AddressSanitizer where we observed the highest yield. Time Budgeting for LLM-Based Fuzzing. To control API spend, each workers LLM-based fuzzing is capped at 60 minutes, and reduced to 45 minutes if other fuzzers have already produced POVs for the same target. libFuzzer Time Management. To preserve CPU for concurrent fuzzers on the same VM, we limit libFuzzer to at most half of the competition time. Concretely (as implemented in our controller): If one or more POVs have been found (either by LLM-based strategies or libFuzzer) and libFuzzer has already run longer than the half-time budget (or longer than half that budget while multiple fuzzers are active on the same VM), we stop libFuzzer to save resources. If no POVs found, we continue until the half-time budget is reached, then stop. Parallelism and Isolation. Because patching requires full rebuilds (often minutes per attempt), we run 35 parallel processes for each patching strategy. single VM may execute 2030 patching processes concurrently for one vulnerability. Each process operates in its own isolated workspace to prevent cross-contamination between attempts. Patch Submission Caps per Vulnerability. Submitting too many patches, even if valid, reduces the accuracy multiplier and lowers the overall score. Because patches contribute the largest share of points, we balance patching success probability against scoring penalties by capping the number of submissions per vulnerability (keyed by canonical signature): at most five POV-based patches and at most three XPatches. These treatments reflect tradeoffs between efficiency and coverage, aiming to optimize performance under resource constraints. However, they may not guarantee the best possible score in every setting. For example, if many vulnerabilities can only be triggered under MemorySanitizer or UndefinedBehaviorSanitizer, skipping these sanitizers would cause FuzzingBrain to miss them, leading to lower overall performance in the final round."
        },
        {
            "title": "8.1 Concurrency and Parallelization\nFuzzingBrain relies heavily on parallelism, which introduced subtle\nrace conditions and deadlocks:",
            "content": "In one exhibition round, the Submission Service froze after few hours. We later traced the root cause to classical deadlock in Go mutex usage across multiple threads. The issue was resolved by removing mutexes and replacing shared maps with sync.Map. Worker services occasionally submitted false-positive POVs due to file-level race conditions. Multiple POV strategies wrote to the same output path (x.bin), leading to mismatched files being submitted. The fix was to isolate file paths for each subprocess."
        },
        {
            "title": "8.4 Logging and Observability\nAccording to AIxCC competition logs, our system performed ex-\ntremely well for the first three days of the final round, but submitted\nnothing after Day 4. We suspect a critical crash or bug, but the root\ncause remains unknown due to loss of logs:",
            "content": "Our system generated gigabytes of logs across services within hours, but most logs were stored only on ephemeral VM nodes in the Azure VMSS cluster. After the competition, all VM nodes were recycled, and the logs disappeared permanently, preventing postmortem analysis. This revealed the importance of persistent, centralized logging and monitoring in large-scale distributed systems. AIxCC, 2025, Ze et al. [8] [n. d.]. SVF: Static Value-Flow Analysis. https://svf-tools.github.io/SVF/. Accessed: 2025-09-08. [9] 2025. AIxCC Final Competition Procedures and Scoring Guide. https:// aicyberchallenge.com/final-competition-procedures-and-scoring-guide/. Accessed: 2025-09-08. [10] Michael C. Fanning and Laurence J. Golding. 2023. Static Analysis Results Interchange Format (SARIF) Version 2.1.0 Errata 01. Technical Report. OASIS. https://docs.oasis-open.org/sarif/sarif/v2.1.0/errata01/os/sarif-v2.1.0errata01-os.html OASIS Approved Errata."
        },
        {
            "title": "9 FuzzingBrain LeaderBoard\nTo systematically evaluate state-of-the-art LLMs on vulnerability\ndetection and patching, we developed the FuzzingBrain Leaderboard\nbased on the AIxCC benchmarks (36 challenges drawn from the\nthree exhibition rounds, 16 C challenges and 20 Java challenges). In\neach run, FuzzingBrain is restricted to using a single LLM for both\nPOV generation and patching, allowing us to directly measure the\nperformance of that model. Scoring follows the AIxCC rubric: each\nPOV is worth 2 points and each patch is worth 6 points. Models are\nthen ranked according to their total score across all benchmarks,\nproviding a standardized comparison of capability.",
            "content": "We introduce several modifications to make leaderboard evaluation practical and reproducible: Single-VM Execution: FuzzingBrain is executed on single VM, and the vulnerability-triggering fuzzer is provided as input. Precomputed Static Analysis: Static analysis results for each target project are precomputed and stored in JSON format. The Static Analysis Service, therefore, only needs to answer queries and return results, minimizing runtime overhead. Time Limit: Each run is capped at one hour in total, covering both POV generation and patching. The current leaderboard is available at o2lab.github.io/FuzzingBrainLeaderboard. We plan to regularly maintain this evaluation framework to enable transparent, standardized, and reproducible comparisons of different LLMs in real-world vulnerability discovery and remediation tasks. References [1] [n. d.]. The AFL++ fuzzing framework AFLplusplus. https://aflplus.plus/. Accessed: 2025-09-08. [2] [n. d.]. AI Cyber Challenge (AIxCC). https://aicyberchallenge.com/. Accessed: 2025-09-08. [3] [n. d.]. Bear: Build EAR. https://github.com/rizsotto/Bear. Accessed: 2025-09-08. [4] [n. d.]. CodeQL. https://codeql.github.com/. Accessed: 2025-09-08. [5] [n. d.]. Honggfuzz. https://github.com/google/honggfuzz. Accessed: 2025-09-08. [6] [n. d.]. LLVM Compiler Infrastructure. https://llvm.org/. Accessed: 2025-09-08. [7] [n. d.]. OSS-Fuzz. https://google.github.io/oss-fuzz/. Accessed: 2025-09-08."
        }
    ],
    "affiliations": [
        "City University of Hong Kong Hong Kong, China",
        "Imperial College London London, UK",
        "Texas A&M University College Station, US"
    ]
}