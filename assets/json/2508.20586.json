{
    "paper_title": "FastFit: Accelerating Multi-Reference Virtual Try-On via Cacheable Diffusion Models",
    "authors": [
        "Zheng Chong",
        "Yanwei Lei",
        "Shiyue Zhang",
        "Zhuandi He",
        "Zhen Wang",
        "Xujie Zhang",
        "Xiao Dong",
        "Yiling Wu",
        "Dongmei Jiang",
        "Xiaodan Liang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, a high-speed multi-reference virtual try-on framework based on a novel cacheable diffusion architecture. By employing a Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5x speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, a new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through a pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-of-the-art methods on key fidelity metrics while offering its significant advantage in inference efficiency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 6 8 5 0 2 . 8 0 5 2 : r FASTFIT: ACCELERATING MULTI-REFERENCE VIRTUAL TRY-ON VIA CACHEABLE DIFFUSION MODELS Zheng Chong1,2,3, Yanwei Lei1, Shiyue Zhang1, Zhuandi He1, Zhen Wang1, Xujie Zhang1, Xiao Dong1, Yiling Wu3, Dongmei Jiang3 & Xiaodan Liang1,3 1Sun Yat-sen University 2LavieAI {chongzheng98,dx.icandoti,xdliang328}@gmail.com, {leiyw5,zhangshy223,zhuandihe86,wangzh669,zhangxj59}@mail2.sysu.edu.cn, {wuyl02,jiangdm}@pcl.ac.cn 3Pengcheng Laboratory Figure 1: FastFit provides unified and accelerated solution for diverse virtual try-on tasks, including single-reference, person-to-person, and our primary focus, multi-reference composition. By decoupling the reference images from the denoising process, our cacheable diffusion architecture delivers high-fidelity virtual try-on across multiple challenging scenarios at much faster speed."
        },
        {
            "title": "ABSTRACT",
            "content": "Despite its great potential, virtual try-on technology is hindered from real-world application by two major challenges: the inability of current methods to support multi-reference outfit compositions (including garments and accessories), and their significant inefficiency caused by the redundant re-computation of reference features in each denoising step. To address these challenges, we propose FastFit, high-speed multi-reference virtual try-on framework based on novel cacheable diffusion architecture. By employing Semi-Attention mechanism and substituting traditional timestep embeddings with class embeddings for reference items, our model fully decouples reference feature encoding from the denoising process with negligible parameter overhead. This allows reference features to be computed only once and losslessly reused across all steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5 speedup over comparable methods. Furthermore, to facilitate research on complex, multi-reference virtual try-on, we introduce DressCode-MR, new large-scale dataset. It comprises 28,179 sets of high-quality, paired images covering five key categories (tops, bottoms, dresses, shoes, and bags), constructed through pipeline of expert models and human feedback refinement. Extensive experiments on the VITON-HD, DressCode, and our DressCode-MR datasets show that FastFit surpasses state-ofthe-art methods on key fidelity metrics while offering its significant advantage in inference efficiency. Corresponding author. Project page: https://github.com/Zheng-Chong/FastFit."
        },
        {
            "title": "INTRODUCTION",
            "content": "Generative AI-based virtual try-on has recently made remarkable progress. An ideal virtual tryon systemone that could revolutionize online retail and power applications like intelligent outfit visualizationwould allow users to seamlessly mix and match various garments and accessories, rapidly generating photorealistic results to enable an interactive experience. However, two major challenges hinder current methods from achieving this vision. Firstly, most existing methods (Xie et al., 2022; Wang et al., 2018; Xu et al., 2024; Choi et al., 2024; Chong et al., 2024; Jiang et al., 2024) are designed for single reference garment (e.g., top or dress), requiring complete multi-item outfit to be rendered through iterative passes, leading to both inflated computation time and the risk of accumulated synthesis errors. Furthermore, the general lack of support for essential accessories like shoes and bags prevents the generation of truly holistic and realistic outfits. Secondly, the computational inefficiency of current methods stems from two competing yet flawed strategies, as illustrated in Figure 2. On one hand, ReferenceNet-based methods (Huang et al., 2024b; Choi et al., 2024; Xu et al., 2024; Zhang et al., 2024b; Zhou et al., 2024; Jiang et al., 2024) employ separate network to encode references (Figure 2 (a)), which avoids this redundancy but at the cost of substantial parameter overhead, increasing both training and inference costs. On the other hand, in-context learning-based methods (Guo et al., 2025; Chong et al., 2024; Huang et al., 2024a) repeatedly process the concatenated reference and person features at each of the denoising steps (Figure 2 (b)), causing significant computational redundancy. Figure 2: Architectural comparison of multireference try-on methods. Our cacheable U-Net (c) avoids the parameter overhead of ReferenceNet (a) and the computational redundancy of In-Context Learning (b). To overcome these limitations, we introduce FastFit, high-speed framework that enables coherent multi-reference virtual try-on through novel cacheable diffusion architecture. Our proposed Cacheable UNet decouples the reference feature encoding from the iterative denoising process, which is achieved by introducing Reference Class Embedding and Semi-Attention mechanism. This structure enables Reference KV Cache during inference, which allows reference features to be computed only once and losslessly reused in all subsequent steps, fundamentally breaking the efficiency bottleneck and achieving an average 3.5 speedup over comparable methods with negligible parameter overhead. Furthermore, observing the lack of datasets with complete outfit pairings, we construct DressCode-MR, large-scale multi-reference try-on dataset based on Morelli et al. (2022). We developed data-generation pipeline that trains expert models based on Chong et al. (2024) and Labs (2024) to recover canonical images of individual items, and utilizes human feedback to ensure high quality. This results in 28,179 multi-reference image sets spanning five key categories: tops, bottoms, dresses, shoes, and bags. In summary, the contributions of this work include: We propose FastFit, novel framework for high-speed, multi-reference virtual try-on. It is the first to enable coherent multi-reference virtual try-on across five key categories, including tops, bottoms, dresses, shoes, and bags, while achieving an average 3.5 speedup over comparable methods. We design novel Cacheable UNet structure featuring Reference Class Embedding and Semi-Attention mechanism. This design decouples reference feature encoding from the denoising process, enabling lossless Reference KV Cache that breaks the core efficiency bottleneck of subject-driven generation architectures. We construct DressCode-MR, the first large-scale dataset specifically for multi-reference virtual try-on. It comprises 28,179 high-quality image sets, providing solid foundation to foster future research in complex outfit generation. 2 Figure 3: Illustrative examples from our proposed DressCode-MR dataset. Each sample provides pairing of full-body person image with set of corresponding canonical images for each item. We conduct extensive experiments on VITON-HD, DressCode, and our DressCode-MR benchmarks, demonstrating that FastFit surpasses state-of-the-art methods in image fidelity while maintaining its significant efficiency advantage."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 SUBJECT-DRIVEN IMAGE GENERATION To enable finer-grained control in diffusion models for image generation, the research community has rapidly shifted towards subject-driven image generation. Early efforts primarily centered on single reference images, injecting specific subject identities or artistic styles by fine-tuning model weights (Ruiz et al., 2022; Yang et al., 2022; Hu et al., 2021; Huang et al., 2024a) or utilizing lightweight adapters (Ye et al., 2023; Mou et al., 2023; Chen et al., 2023). However, the former approach requires training separate model for each subject, limiting its practical flexibility, while the latter, despite being convenient, often faces challenges in maintaining high fidelity to the reference image. Another line of work based on in-context learning, such as IC-LoRA (Huang et al., 2024a) and OminiControl (Tan et al., 2025b;a), achieves superior detail preservation by concatenating the reference image with noise along the spatial dimension. The trade-off is that the reference must participate in every denoising step, significantly increasing inference time and computational cost. The limitations of these single-reference approaches become apparent when creative needs involve composing elements from multiple, diverse sources. Consequently, some works have begun to explore multi-reference generation; for instance, IC-Custom (Li et al., 2025) inputs multiple images as single concatenated map for multi-concept composition, Face-diffuser investigates the complex multi-person synthesis task, and MultiRef (Chen et al., 2025) provides the first systematic definition and benchmark for this task. Nevertheless, in the domain of virtual try-on, multi-reference generation remains an under-explored area. How to harmoniously compose visual information from multiple references while mitigating the heightened computational burden from increased inputs remains significant and open challenge. 2.2 IMAGE-BASED VIRTUAL TRY-ON Image-based virtual try-on aims to realistically synthesize person wearing target garments. Classic paradigms centered on warp-and-fuse method, which explicitly deforms the garment using either geometric transformations or learned appearance flows before the blending stage (Wang et al., 2018; Han et al., 2018; Choi et al., 2021; Han et al., 2019; Ge et al., 2021; Xie et al., 2021; 2023; Gou et al., 2023; Chong & Mo, 2022); however, these approaches are frequently hampered by visual artifacts from inaccurate warping. Subsequently, the advent of diffusion models revolutionized the field by reframing the task as end-to-end conditional image generation, bypassing the error-prone warping step. The dominant strategy in these modern models involves injecting high-fidelity garment features into the denoising process via sophisticated conditioning mechanisms, such as parallel encoder branches (i.e., ReferenceNets) or ControlNet(Zhang et al., 2023)-like structures, technique employed by vast body of recent work (Zhu et al., 2023; Morelli et al., 2023; Kim et al., 2023; Xu et al., 2024; Wang et al., 2024; Choi et al., 2024; Sun et al., 2024; Zhou et al., 2024; Zhang et al., 2024a; Kim et al., 2024). Recent innovations further push the boundaries by exploring alternative backbones like Diffusion Transformers (Peebles & Xie, 2022) or introducing novel control modalities such as textual prompts and more generalized conditioning schemes (Guo et al., 2025; Jiang et al., 2024). Despite achieving unprecedented realism, their inference speed and general limitation to single garments have become key bottlenecks, hindering the technologys application in real-world scenarios that demand rapid feedback and multi-item outfit composition. Figure 4: Overview of FastFit. Our model accelerates multi-reference virtual try-on through novel cacheable UNet architecture. It enables lossless KV caching for reference features by conditioning them on class embeds instead of the timestep and using Semi-Attention to interact with the denoising features, which eliminates redundant computations."
        },
        {
            "title": "3 METHODS",
            "content": "3.1 OVERVIEW The overall framework of FastFit is built upon the foundation of Latent Diffusion Models (LDMs) (Rombach et al., 2021) and is designed to achieve high-speed, multi-reference virtual try-on through novel conditioning cacheable UNet architecture. The entire workflow is depicted in Figure 4 (a). To ensure the generated image preserves the persons identity and pose while accurately rendering the new garments, we prepare two sets of conditions: Person Conditioning cp: To accurately preserve the persons identity and body pose, we construct the person condition cp. First, we utilize AutoMask (Chong et al., 2024) to generate cloth-agnostic mask Ma from the input image Ip. Subsequently, composite image, Icomp, is created by combining the human pose skeleton extracted via DWPose (Yang et al., 2023) with the person image masked by Ma. cp is formed as: cp = Concat(Interpolate(Ma), E(Icomp)) (1) where is the VAE encoder, Interpolate is downsampling function that resizes the mask Ma, and Concat denotes the channel-wise concatenation. Reference Conditioning {Ri}K extract set of reference latents {Ri}K is defined as: i=1: To capture the detailed appearance of the target garments, we i=1 from the corresponding reference images {IRi}K i=1, which {Ri}K i=1 = {E(IRi)}K i=1 (2) The image generation process is guided by denoising UNet ϵθ, which predicts the noise ϵt at each timestep t. As illustrated in Figure 4 (a), our key innovation is to conceptually partition the function of ϵθ into two streams: time-independent path for reference inputs and time-dependent path for the denoising process. Specifically, each reference latent Ri is processed individually by dedicated, time-independent path within the UNet, conditioned only on its corresponding Class Embedding Ei. This allows us to pre-compute and cache separate feature representation, R(i) cache, for each item before the denoising loop begins. This operation is performed for all {1, . . . , K} and is independent of any timestep t: R(i) cache = ϵθ(Ri, Ei) for = 1, . . . , (3) The resulting set of cached features, {R(i) denoising loop. cache}K i=1, is then collectively used in each step of the main 4 The main denoising loop then proceeds for steps. At each step t, the UNet, ϵθ, processes only the time-dependent inputs: the noisy latent zt, the person condition cp, and the timestep embedding γ(t). It integrates the static reference information by attending to the pre-computed set of cached features, {R(i) i=1, via Semi-Attention mechanism (detailed in Section 3.2): cache}K ϵt = ϵθ(zt, cp, γ(t), {R(i) cache}K i=1) (4) This decomposition of the denoising process is the key to FastFits efficiency, as it shifts the expensive computation for multiple reference features entirely out of the iterative loop. Once the process concludes at = 0, the final clean latent, z0, is mapped back to the pixel space using the VAE decoder D, to produce the high-resolution output image, Iout: Iout = D(z0) (5)"
        },
        {
            "title": "3.2 CACHEABLE UNET FOR EFFICIENT CONDITIONING\nThe primary bottleneck in existing subject-driven diffusion models is the repeated computation of\nreference features at every denoising step. This is because the reference conditioning is typically\ndependent on the timestep t, making the features dynamic. Our key innovation, the Cacheable UNet,\nfundamentally breaks this dependency, enabling reference features to be computed once and reused.\nThis is achieved through two core components: Reference Class Embedding and a Semi-Attention\nmechanism, as illustrated in Figure 4 (b).\nReference Class Embedding. To decouple the reference features from the denoising timestep\nt, we replace the conventional timestep embedding with a static, learnable Reference Class Em-\nbedding for the reference items. Specifically, for a set of K reference items {R1, . . . , RK}, each\nbelonging to a certain category (e.g., ’top’, ’shoes’), we introduce a corresponding set of learnable\nclass embeddings {E1, . . . , EK}. The features for each reference item Ri are conditioned on its\nclass embedding Ei instead of the shared timestep embedding γ(t) used by the denoising features\nX. The reference class embedding is injected in the same manner as the timestep embedding; both\nmodulate the features within the ResNet blocks through an scaling operation. Since the class embed-\ndings are constant throughout the entire denoising process, the resulting reference features become\nstatic and independent of the current timestep t, making them inherently cacheable.\nSemi-Attention Mechanism. Having made reference\nfeatures static, we need a mechanism to inject their infor-\nmation into the denoising process without compromising\ntheir static nature. A standard full self-attention would al-\nlow information to flow from the step-dependent denoising\nfeatures X back to the reference features Ri, thereby ”con-\ntaminating” them and breaking the condition for caching.\nTo solve this, we propose a Semi-Attention mechanism,\nvisualized in Figure 5. In this design, we treat both the\ndenoising features X and all reference features {Ri} as\na single sequence of tokens. The attention calculation is\ngoverned by a specific mask that controls the information\nflow: (1) Denoising-to-All: The tokens of the denoising\nfeature X are allowed to attend to all tokens in the se-\nquence (i.e., to itself and to all reference features Ri). This\nallows the model to effectively ”read” the appearance in-\nformation from each garment and apply it to the person.\n(2) Reference-to-Self: The tokens of each reference fea-\nture Ri are only allowed to attend to themselves. They cannot attend to the denoising features X\nor to any other reference feature Rj (where j ̸= i). This attention mask ensures that the reference\nfeatures act as a static, read-only source of information for the denoising process. Their representa-\ntions are never updated by the dynamic features of X, thus preserving their cacheability across all\ntimesteps.",
            "content": "Figure 5: Visualization of the SemiAttention Mask. Denosing feature attend to all features, while each reference feature Ri is restricted to its own. In summary, the Reference Class Embedding makes the computation of reference features static, while the Semi-Attention mechanism ensures that during interaction, the static reference features only provide information without being affected by the denoising process. This synergistic design forms the Cacheable UNet architecture, laying the foundation for an efficient, cache-based inference pipeline. 5 Figure 6: Qualitative comparison on the DressCode-MR dataset. See Appendix A.2.2 for more results. Please zoom in for more details. INFERENCE ACCELERATION WITH REFERENCE KV CACHE 3.3 The design of our Cacheable UNet enables highly efficient inference pipeline via Reference KV Cache. As depicted in Figure 4(b), the process is split into two stages: Pre-computation and Caching (One-time Cost). Before the iterative denoising loop begins, we perform single forward pass for each reference item Ri through the UNet ϵθ. For each SemiAttention layer, we then compute and store its corresponding Key (K cache ) matrices. This pre-computation step is performed only once per generation request. ) and Value (V cache Accelerated Denoising Loop. For every subsequent denoising step from = 1 down to 0, we completely bypass the computation for the reference branches. Instead, for each Semi-Attention layer, we only compute the Query (QX ), Key (KX ), and Value (VX ) from the current denoising features Xt. We then construct the full key and value matrices, Kfull and Vfull, by concatenating these dynamic tensors with all the cached keys {K cache }K i=1, respectively: Kfull = Concat(KX , cache Vfull = Concat(VX , cache 1 1 }K i=1 and values {V cache , . . . , cache ) , . . . , cache ) The final attention output is then calculated only for the denoising query QX : Attention(QX , Kfull, Vfull) = softmax (cid:19) (cid:18) QX full dk Vfull (6) (7) (8) This strategy effectively reduces the computational cost of attention at each step to be dependent only on the denoising features, regardless of the number or complexity of reference items. This fundamentally resolves the efficiency bottleneck, leading to substantial reduction in inference latency, especially in the multi-reference setting central to our work."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 DATASETS We evaluate our model on three datasets, VITON-HD (Choi et al., 2021), DressCode (Morelli et al., 2022), and our newly proposed DressCode-MR, all at 1024768 resolution. VITON-HD (Choi et al., 2021) provides 13,679 image pairs for upper-body virtual try-on (11,647 train / 2,032 test). DressCode (Morelli et al., 2022) dataset features 53,792 full-body pairs (48,392 train / 5,400 test) 6 Figure 7: Qualitative comparison on the VITON-HD (Choi et al., 2021) and DressCode (Morelli et al., 2022) dataset. See Appendix A.2.1 for more results. Please zoom in for more details. covering tops, bottoms, and dresses. To facilitate multi-reference research, we introduce DressCodeMR, built upon DressCode. As illustrated in Figure 3, it contains 28,179 samples (25,779 train / 2,400 test), each pairing person with complete outfit from up to five categories: tops, bottoms, dresses, shoes, and bags. We constructed this dataset by training five expert restoration models (based on CatVTON (Chong et al., 2024) and FLUX (Labs, 2024)) using VITON-HD, DressCode, and small set of internet-sourced shoe and bag pairs. These models were used to recover the canonical images for items worn in DressCode, and the final high-quality samples were selected through human feedback. IMPLEMENTATION DETAILS 4.2 We train our single-reference try-on model based on the pretrained StableDiffusion v1.5 (Rombach et al., 2021) inpainting on the DressCode (Morelli et al., 2022) and VITON-HD (Choi et al., 2021) datasets for 64,000 steps with batch size of 32 and resolution of 1024768. This version is used for all single-reference quantitative evaluations. Building upon the single-reference model, we fine-tune it on our proposed DressCode-MR dataset for 16,000 steps with the same resolution and batch size. We utilized the AdamW (Loshchilov & Hutter, 2019) optimizer with constant learning rate of 1 105 for both training stages. To enable classifier-free guidance, 20% of the reference images were randomly dropped during the training. All experiments were conducted on 8 NVIDIA H100 GPUs. 4.3 METRICS We evaluate our models performance on two fronts: image fidelity and computational efficiency. Image Fidelity. We use two settings. In the paired setting, where ground-truth images are available, we measure similarity using the Structural Similarity Index (SSIM) (Wang et al., 2004), Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018), Frechet Inception Distance (FID) (Seitzer, 2020), and Kernel Inception Distance (KID) (Binkowski et al., 2021). In the unpaired setting, we assess overall realism and diversity by comparing the distribution of our generated samples to that of real images using FID and KID. Computational Efficiency. We report the total parameters, inference latency, and peak memory usage. These metrics are benchmarked by averaging 100 runs on single NVIDIA H100 GPU, with each run configured for 20 denoising steps and with classifier-free guidance (CFG) (Ho & Salimans, 2022) enabled."
        },
        {
            "title": "4.4 QUANTITATIVE COMPARISON",
            "content": "Single-Reference Virtual Try-On. We conducted quantitative comparison against current state-ofthe-art virtual try-on methods (Guo et al., 2025; Kim et al., 2024; Jiang et al., 2024; Choi et al., 2024; Chong et al., 2024; Xu et al., 2024) on VITONHD (Choi et al., 2021) and DressCode (Morelli et al., 2022) datasets. As shown in Table 3, FastFit achieves competitive results across both datasets under paired and unpaired settings, demonstrating its superior capability in generating high-quality images. Table 1 highlights the efficiency of FastFit, which achieves an average 3.5 speedup over comparable methods while remaining competitive in terms of parameters and memory usage. Table 1: Quantitative comparison of model efficiency. Best and second-best results are in bold and underlined, respectively. Method Params(M) Time(s) Memory(M) Any2AnyTryon PromptDresser FitDiT Leffa IDM-VTON OOTDiffusion CatVTON FastFit 16786.78 6011.03 5870.80 1802.72 7086.91 2229.73 899.06 904.86 12.19 4.29 2.00 3.32 2.76 1.93 2.10 1.16 35218 17364 15992 7996 19072 10154 5500 Method Table 2: Quantitative comparison on DressCode-MR for multi-reference try-on. Best and second-best results are in bold and underlined, respectively. Multi-Reference Virtual Try-On. Table 2 shows our multi-reference try-on results. In the absence of methods designed for simultaneous multi-reference generation, we adapt strong baselines from subjectdriven generation (Ye et al., 2023; Yang et al., 2022; Chen et al., 2023; 2024) and multi-category tryon (Jiang et al., 2024; Chong et al., 2024; Huang et al., 2024b) via sequential single-reference inference. FastFit achieves state-of-theart scores across quality metrics and is also the most efficient method. This demonstrates its superior ability to cohesively synthesize multiple references with high fidelity. 0.235 44.068 23.958 0.215 31.135 17.887 0.173 22.111 9.992 0.187 24.564 10.581 0.106 18.339 7.458 0.089 24.139 10.783 0.122 15.956 5.645 37.138 22.571 0.768 28.296 16.092 0.796 21.074 9.858 0.800 20.313 8.200 0.807 16.131 6.980 0.856 14.459 4.144 0.861 14.722 5.471 0.850 AnyDoor Paint-By-Example MimicBrush Part2Whole CatVTON IP-Adapter FitDIT FID KID SSIM LPIPS FID KID 12.08 5.22 6.62 5.73 8.94 5.62 3.38 0.079 12.059 2.123 9.311 1.512 0.859 Time(s) FastFit unpair Paired 1.90 4.5 QUALITATIVE COMPARISON Single-Reference Virtual Try-On. Figure 7 shows the qualitative comparison for the singlereference try-on task. On the VITON-HD (Choi et al., 2021) dataset, our method excels at preserving fine-grained details, such as the text REBEL on T-shirts, where other methods often produce blurred results. FastFit also realistically renders challenging materials, like the sheer polka-dot top. On the DressCode (Morelli et al., 2022) dataset, our model accurately captures the correct shape and style of complex garments like the high-slit dress. Multi-Reference Virtual Try-On. We further evaluate FastFit on the more challenging multireference virtual try-on task, with results presented in Figure 6. The comparison clearly demonstrates our models superior capability. FastFit successfully synthesizes coherent and realistic final image by seamlessly combining multiple reference items. In contrast, most existing methods, such as AnyDoor (Chen et al., 2023) and PBE (Yang et al., 2022), often fail to properly compose the different garments or produce significant artifacts. Our method, however, maintains the identity and details of each piece of clothing, resulting in natural and believable complete outfit. 4.6 ABLATION STUDIES The results in Table 4 validate our key design choices. Firstly, the Reference KV Cache is crucial for efficiency; disabling it increases inference time from 1.16s to 1.92s, yet this 1.66 speedup comes with no loss in generation quality, as the performance metrics are identical. Secondly, our parameter-sharing strategy is highly effective. Introducing separate ReferenceNet nearly doubles the parameters (904.86M 1729.9M) and increases memory usage, but yields no corresponding performance improvement. Furthermore, replacing Semi-Attention with Full Attention is detrimental, as it not only slows inference to 2.17s but also degrades generation quality (e.g., FID increases to 3.1847). We hypothesize this is because full interaction disrupts the consistency of reference features. Finally, removing the Class Embedding causes slight performance drop, and its effec8 Table 3: Quantitative comparison for single-reference virtual try-on on the VITON-HD (Choi et al., 2021) and DressCode (Morelli et al., 2022) datasets. All metrics are rounded to three decimal places. Best and second-best results in each column are in bold and underlined, respectively. Method VITON-HD DressCode Paired Unpaired Paired Unpaired FID KID SSIM LPIPS FID KID FID KID SSIM LPIPS FID KID Any2AnyTryon 11.195 5.934 PromptDresser 8.176 FitDiT 5.667 Leffa 6.112 IDM-VTON 6.738 CatVTON 5.762 OOTDiffusion 2.806 0.550 1.079 0.692 1.112 1.320 0.267 FastFit 5.629 0. 0.799 0.846 0.838 0.857 0.866 0.881 0.843 0.885 0.194 0.090 0.096 0.076 0.074 0.088 0.072 0.078 9.981 8.885 9.979 10.446 9.249 10.552 9.082 3.496 0.909 1.478 2.640 1.267 2.272 0. 5.111 9.563 5.571 7.193 7.181 3.710 6.975 1.265 4.795 1.901 2.114 3.524 1.010 2.014 8.629 0.665 2.836 0. 0.897 0.858 0.899 0.861 0.891 0.909 0.873 0.907 0.059 0.104 0.058 0.084 0.070 0.062 0.077 0.057 6.709 1.580 10.618 4.978 0.712 4.805 20.099 13.506 4.489 9.167 1.606 5.872 2.886 8.121 4. 0.553 Table 4: Ablation study of the key components in our model on DressCode (Morelli et al., 2022) dataset. The best and second-best results are demonstrated in bold and underlined, respectively. Variants Params(M) Time (s) Memory (M) w/o KV Cache w/ Full Attention w/o Class Embed w/ ReferenceNet FastFit 904.86 904.86 904.85 1729.92 904.86 1.92 2.17 1.16 1.16 1.16 6944 6944 6944 6944 Paired Unpaired FID KID SSIM LPIPS FID KID 2.8585 3.1847 2.9146 2.8474 0.3737 0.5426 0.4000 0.3577 0.9057 0.9056 0.9056 0. 0.0588 0.0606 0.0591 0.0588 4.4206 4.6221 4.4624 4.4365 0.5903 0.6533 0.5929 0.5741 2.8585 0.3737 0. 0.0588 4.4206 0.5903 tiveness in guiding region-specific attention is presented in Appendix A.3. All ablation experiments follow the settings described in Section 4.2, trained for 32K steps, and are evaluated on the DressCode (Morelli et al., 2022) dataset."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we proposed FastFit, high-speed multi-reference virtual try-on framework designed to break the critical trade-offs between versatility, efficiency, and quality in existing technologies. Through an innovative Cacheable UNet, which combines Class Embedding and Semi-Attention mechanism, we decoupled reference feature encoding from the denoising process. This design enables Reference KV Cache that allows reference features to be computed once and reused losslessly across all steps, fundamentally eliminating the computational redundancy that plagues current methods. Experimental results show that FastFit achieves significant efficiency advantagean average 3.5 speedup over comparable methodswithout sacrificing generation quality. For the first time, it enables coherent, synergistic try-on for up to 5 key categories: tops, bottoms, dresses, shoes, and bags. Furthermore, the DressCode-MR dataset we constructed provides valuable foundation for future research in complex outfit generation. In summary, FastFit represents promising advance towards more realistic, efficient, and diverse virtual try-on experience, significantly lowering the barriers for its widespread application in e-commerce and intelligent outfit visualization. Limitations and Future Work. Despite the models strong performance, several areas present opportunities for future exploration. To further enhance realism, the modeling of complex physical interactions and layering among garments could be improved. Expanding the DressCode-MR dataset with such complex interaction pairs would be valuable direction. Another important research path is improving generalization to underrepresented apparel, such as styles with unique topologies or challenging materials. Finally, while our framework significantly accelerates inference, gap remains toward achieving real-time interaction. Exploring techniques such as guidance and step distillation, combined with more advanced caching mechanisms, offers promising path to bridge this gap and enable applications like interactive real-time outfit visualization."
        },
        {
            "title": "REFERENCES",
            "content": "Mikołaj Binkowski, Danica J. Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans, 2021. URL https://arxiv.org/abs/1801.01401. Ruoxi Chen, Siyuan Wu, Dongping Chen, Shiyun Lang, Petr Sushko, Gaoyang Jiang, Sinan Wang, Yao Wan, and Ranjay Krishna. Multiref: Controllable image generation with multiple visual references. In Synthetic Data for Computer Vision Workshop @ CVPR 2025, 2025. URL https: //openreview.net/forum?id=TZwQU36KFd. Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zeroshot object-level image customization. arXiv preprint arXiv:2307.09481, 2023. Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, and Hengshuang Zhao. Zero-shot image editing with reference imitation, 2024. URL https:// arxiv.org/abs/2406.07547. Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. Viton-hd: High-resolution virtual try-on via misalignment-aware normalization. In Proc. of the IEEE conference on computer vision and pattern recognition (CVPR), 2021. Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for virtual try-on. arXiv preprint arXiv:2403.05139, 2024. Zheng Chong and Lingfei Mo. St-vton: Self-supervised vision transformer for image-based virtual try-on. Signal Processing, 2022. Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xujie Zhang, Hanqing Zhao, and Xiaodan Liang. Catvton: Concatenation is all you need for virtual try-on with diffusion models, 2024. URL https://arxiv.org/abs/2407.15886. Yuying Ge, Yibing Song, Ruimao Zhang, Chongjian Ge, Wei Liu, and Ping Luo. Parser-free virtual try-on via distilling appearance flows. arXiv preprint arXiv:2103.04559, 2021. Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen Qian, and Liqing Zhang. Taming the power of diffusion models for high-quality virtual try-on with appearance flow. In Proceedings of the 31st ACM International Conference on Multimedia, 2023. Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, and Jiaming Liu. Any2anytryon: Leveraging adaptive position embeddings for versatile virtual clothing tasks, 2025. URL https://arxiv.org/abs/2501.15891. Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry Davis. Viton: An image-based virtual try-on network. In CVPR, 2018. Xintong Han, Xiaojun Hu, Weilin Huang, and Matthew Scott. Clothflow: flow-based model In Proceedings of the IEEE/CVF International Conference on for clothed person generation. Computer Vision, pp. 1047110480, 2019. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. URL https://arxiv. org/abs/2207.12598. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https: //arxiv.org/abs/2106.09685. Lianghua Huang et al. arXiv:2410.23775. In-context lora for diffusion transformers, 2024a. arXiv preprint Zehuan Huang, Hongxing Fan, Lipeng Wang, and Lu Sheng. From parts to whole: unified reference framework for controllable human image generation, 2024b. Boyuan Jiang, Xiaobin Hu, Donghao Luo, Qingdong He, Chengming Xu, Jinlong Peng, Jiangning Zhang, Chengjie Wang, Yunsheng Wu, and Yanwei Fu. Fitdit: Advancing the authentic garment details for high-fidelity virtual try-on, 2024. URL https://arxiv.org/abs/2411. 10499. Jeongho Kim, Gyojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on, 2023. Jeongho Kim, Hoiyeong Jin, Sunghyun Park, and Jaegul Choo. Promptdresser: Improving the quality and controllability of virtual try-on via generative textual prompt and prompt-aware mask, 2024. URL https://arxiv.org/abs/2412.16978. Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Yaowei Li, Xiaoyu Li, Zhaoyang Zhang, Yuxuan Bian, Gan Liu, Xinyuan Li, Jiale Xu, Wenbo Hu, Yating Liu, Lingen Li, Jing Cai, Yuexian Zou, Yancheng He, and Ying Shan. Ic-custom: Diverse image customization via in-context learning, 2025. URL https://arxiv.org/abs/2507. 01926. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. Dress code: High-resolution multi-category virtual try-on, 2022. Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, and Rita Cucchiara. LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On. In Proceedings of the ACM International Conference on Multimedia, 2023. Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint arXiv:2212.09748, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2021. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. 2022. Maximilian Seitzer. pytorch-fid: FID Score for PyTorch. https://github.com/mseitzer/ pytorch-fid, August 2020. Version 0.3.0. Ke Sun, Jian Cao, Qi Wang, Linrui Tian, Xindi Zhang, Lian Zhuo, Bang Zhang, Liefeng Bo, Wenbo Zhou, Weiming Zhang, and Daiheng Gao. Outfitanyone: Ultra-high quality virtual try-on for any clothing and any person, 2024. URL https://arxiv.org/abs/2407.16224. Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. 2025a. Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. OminiControl2: Efficient Conditioning for Diffusion Transformers, March 2025b. URL http://arxiv.org/ abs/2503.08280. arXiv:2503.08280 [cs] TLDR: OminiControl2 introduces two key innovations: dynamic compression strategy that streamlines conditional inputs by preserving only the most semantically relevant tokens during generation, and conditional feature reuse mechanism that computes condition token features only once and reuses them across denoising steps. Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, and Liang Lin. Toward characteristicpreserving image-based virtual try-on network. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 589604, 2018. Rui Wang, Hailong Guo, Jiaming Liu, Huaxia Li, Haibo Zhao, Xu Tang, Yao Hu, Hao Tang, and Peipei Li. Stablegarment: Garment-centric generation via stable diffusion. arXiv preprint arXiv:2403.10783, 2024. 11 Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600 612, 2004. Zhenyu Xie, Xujie Zhang, Fuwei Zhao, Haoye Dong, Michael C. Kampffmeyer, Haonan Yan, and Xiaodan Liang. Was-vton: Warping architecture search for virtual try-on network, 2021. URL https://arxiv.org/abs/2108.00386. Zhenyu Xie, Zaiyu Huang, Fuwei Zhao, Haoye Dong, Michael Kampffmeyer, Xin Dong, Feida Zhu, and Xiaodan Liang. Pasta-gan++: versatile framework for high-resolution unpaired virtual tryon, 2022. URL https://arxiv.org/abs/2207.13475. Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye Dong, Xijin Zhang, Feida Zhu, and Xiaodan Liang. Gp-vton: Towards general purpose virtual try-on via collaborative local-flow global-parsing learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2355023559, 2023. Yuhao Xu, Tao Gu, Weifeng Chen, and Chengcai Chen. Ootdiffusion: Outfitting fusion based latent diffusion for controllable virtual try-on. arXiv preprint arXiv:2403.01779, 2024. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. arXiv preprint arXiv:2211.13227, 2022. Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, 2023. CV4Metaverse Workshop. Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. 2023. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Xuanpu Zhang, Dan Song, Pengxin Zhan, Qingguo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Anan Liu. Boow-vton: Boosting in-the-wild virtual try-on via mask-free pseudo data training, 2024a. URL https://arxiv.org/abs/2408.06047. Xujie Zhang, Ente Lin, Xiu Li, Yuxuan Luo, Michael Kampffmeyer, Xin Dong, and Xiaodan Liang. Mmtryon: Multi-modal multi-reference control for high-quality fashion generation, 2024b. URL https://arxiv.org/abs/2405.00448. Zijian Zhou, Shikun Liu, Xiao Han, Haozhe Liu, Kam Woh Ng, Tian Xie, Yuren Cong, Hang Li, Mengmeng Xu, Juan-Manuel Perez-Rua, Aditya Patel, Tao Xiang, Miaojing Shi, and Sen He. Learning flow fields in attention for controllable person image generation. arXiv preprint arXiv:2412.08486, 2024. Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. Tryondiffusion: tale of two unets, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 QUANTITATIVE COMPARISON ACROSS GARMENT TYPES For more fine-grained analysis, Table 5 presents quantitative comparison on the DressCode (Morelli et al., 2022) dataset, with results broken down by clothing category. The results highlight the robust and superior performance of our method across all tested categories, including upper, lower, and dresses. FastFit consistently achieves either the best or second-best scores in the vast majority of key metrics, demonstrating its strong and stable performance regardless of the garment type. This showcases the models excellent generalization capability for different clothing styles. Table 5: Quantitative comparison on the DressCode dataset, with results broken down by category (Upper, Lower, and Dresses). The best results are marked in bold and the second-best are underlined. indicates lower is better, while indicates higher is better. Methods Upper Lower Dresses FID KID SSIM LPIPS FID KID SSIM LPIPS FID KID SSIM LPIPS Any2AnyTryon 10.4741 1.7130 0.9206 0.0476 13.1152 PromptDresser Leffa IDM-VTON OOTDiffusion CatVTON FitDiT 1.6539 0.8796 0.0636 0.7174 0.9044 0.0678 32.9093 17.9749 0.8327 0.1352 16.8179 6.8932 0.8363 0.1087 9.2447 0.8594 0.0908 13.3859 2.3701 0.8335 0.1029 11.2549 2.0947 0.8908 0.0578 19.6834 0.8978 0.0655 17.5135 8.3389 0.8585 0.0882 11.2283 3.3860 0.9174 0.0547 11.7878 0.8751 0.0827 14.8496 4.4567 0.8393 0.0963 1.1055 0.9040 0.0528 19.6615 9.5945 1.0851 0.9360 0.0504 0.9236 0.0562 1.0575 0.8669 0.0791 8.6135 7.8465 0.4768 0.8789 0.0562 0.5789 0.9241 0.0417 24.5079 11.7225 0.8944 0.0758 8.0876 5.8985 3.4218 6.1217 1.6574 0.8896 0.0655 8.9453 7. 2.8336 9.1124 FastFit 6.8354 0.2453 0.9318 0.0485 7. 0.7981 0.9207 0.0511 7.5890 0.2446 0.8671 0.0720 A.2 MORE VISUAL COMPARISONS A.2.1 SINGLE-REFERENCE VIRTUAL TRY-ON In the single-reference virtual try-on task, our method demonstrates robust performance across both the VITON-HD (Choi et al., 2021) and DressCode (Morelli et al., 2022) datasets. As illustrated in Figure 8, FastFit excels at preserving high-frequency details on the garments, such as intricate patterns and text logos. Furthermore, our model accurately renders the correct shape and length for various types of clothing. The final results show that the garments are naturally fused with the persons body, effectively handling challenging poses and occlusions. A.2.2 MULTI-REFERENCE VIRTUAL TRY-ON For the more challenging multi-reference task, FastFit exhibits significant advantage over competing methods. Figure 9 showcases our models unique ability to seamlessly combine multiple, distinct reference items into single, coherent outfit. Notably, even during this complex composition process, FastFit faithfully preserves the fine-grained details and logos of each individual item (e.g., SPAS, CHIUS). This capability to generate complete and detailed ensembles in complex scenarios highlights its superiority where other methods often struggle. A.3 VISUAL ANALYSIS OF THE EFFECTIVENESS OF CLASS EMBEDDINGS To visually validate the effectiveness of the Reference Class Embedding as key control mechanism in our model, we conducted an additional ablation study. As shown in Figure 10, the experiment is designed to isolate the influence of the class embedding. For each example row, we provide the model with the exact same source person and reference image. The only variable changed across the columns is the specific class embedding provided (e.g., Upper, Lower, Dresses, Shoes, Bag). The results demonstrate that the class embedding provides fine-grained, semantic control over the try-on process. The model is able to interpret the embedding and selectively transfer the corresponding item from the reference image, even when multiple items are present. This experiment confirms that by applying Class Embedding, the models attention is effectively guided to the corresponding region of the reference image, which is crucial for preventing the features of different reference items from being conflated in multi-reference scenario. 13 Figure 8: More visual comparisons on the VITON-HD (Choi et al., 2021) and DressCode (Morelli et al., 2022) dataset with baseline methods. Please zoom in for more details. 14 Figure 9: More visual comparisons on the DressCode-MR dataset with baseline methods. Please zoom in for more details. Figure 10: Demonstration of the visual effect of class embeddings. By providing different class embeddings (e.g., Upper, Lower, Dresses, Shoes, Bag) for the same reference image, our model can be directed to selectively transfer the corresponding item to the source person."
        }
    ],
    "affiliations": [
        "LavieAI",
        "Pengcheng Laboratory",
        "Sun Yat-sen University"
    ]
}