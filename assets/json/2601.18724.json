{
    "paper_title": "HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences",
    "authors": [
        "Yusuke Sakai",
        "Hidetaka Kamigaito",
        "Taro Watanabe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as \"HalluCitation\" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 2 ] . [ 1 4 2 7 8 1 . 1 0 6 2 : r HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe Nara Institute of Science and Technology (NAIST), Japan {sakai.yusuke.sr9, kamigaito.h, taro}@is.naist.jp"
        },
        {
            "title": "Abstract",
            "content": "ACL 2025 Main: Chang et al. (2025) Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as HalluCitation and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility."
        },
        {
            "title": "Introduction",
            "content": "Thanks to the advances in AI for Science research (Sun et al., 2024; Chen et al., 2025d; Shi et al., 2025) and AI Scientist technologies (Wei et al., 2025b; Huang et al., 2025; Yamada et al., 2025; Tang et al., 2025a; Shao et al., 2025; Xu et al., 2025b), academic research and writing have shifted from traditionally human-centered processes to ones supported by and actively leveraging AI systems such as large language models (LLMs). In particular, LLMs are enabling wide range of tools for writing assistance (Hwang et al., 2025), literature search (Asai et al., 2024), citation recommendation ( Sahinuç et al., 2024a; Çelik and Tekir, 2025), idea discussion (Zheng et al., 2025d), peer-review support and analysis (Jin et al., 2024; Starace et al., 2025; Hossain et al., 2025), and other scientific automation ( Sahinuç et al., 2024b). These benefits have been especially significant for non-native 1 Y. Zhang and Others. 2024. Subsampling for skill improvement in large language models. arXiv . preprint arXiv:2402.12345 Hohloch (2024) EMNLP 2025 Findings: Jalori et al. (2025) Wendi Zhou, Xiao Li, Lin Geng Foo, Yitan Wang, Harold Soh, Caiming Xiong, and Yoonkey Kim. 2024. TEMPO: Temporal representation prompting for large language models in time-series forecasting. Anticipated(cid:58)(cid:58)(cid:58)for arXiv preprint arXiv:2405.18384. (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) NeurIPS (cid:58)(cid:58)(cid:58)(cid:58)2024. Preprint, arXiv:2405.18384 . Shandi et al. (2024) (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) EMNLP 2025 Main: Srivastava (2025) Wei Xu, Yulia Tsvetkov, and Alan Black. 2022. AI for language learning: Conversational agents and personalized feedback. Transactions of the Association for Computational Linguistics (TACL), 10:115. (Non-existent) Title Link: Canine: Pre-training an Efficient TokenizationFree Encoder for Language Representation (Clark et al., 2022) Figure 1: Examples of incorrect reference information. Some references include incorrect arXiv IDs or contain uncertain phrases such as Anticipated for. Even references that appear plausible may contain HalluCitations, e.g., incorrect link information or references to pages where the cited paper does not exist, as in the last case. English speakers, as AI tools can improve translation and grammatical quality and help create fairer opportunities for participation in the international academic community (Lepp and Smith, 2025). However, while these advances have boosted research productivity, the research community has begun to face new challenges arising from the exponential growth in paper submissions, particularly in securing qualified reviewers. As result, the qualification bar has been lowered1, and reviewers are often required to evaluate many papers on unfamiliar topics within limited time. Moreover, recent review processes involve potential penalties 1https://aclrollingreview.org/incentives2025 for delayed reviews, such as desk rejection of reviewers own submissions1. Under such pressure and concern, the focus of peer review has shifted from conducting thorough and rigorous evaluations to completing reviews under strict time constraints. Consequently, we face growing difficulty in carefully inspecting the rigor and factual correctness of submitted manuscripts. In particular, as shown in Figure 1, we have recently observed non-existent citations, i.e., hallucinated references that do not correspond to any real prior work. Such false citations pose serious threat to scientific reliability and undermine respect for existing work. When they appear in published papers, they may also negatively affect the overall quality and credibility of academic conferences and journals. In this study, we refer to such hallucinated references as HalluCitation (Hallucination + Citation, /h@ˌlu:.saIˈteI.S@n/). Papers that include at least one HalluCitation are referred as HalluCited papers (Hallucination + Cited, /ˈhæl.u:ˌsaI.tId/). We systematically investigate their prevalence and impact at top-tier NLP conferences. We analyze over 17,000 papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis and discussion reveal the following findings: Almost 300 papers contain at least one HalluCitation, mostly published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, over 100 such papers are accepted as main and Findings papers in EMNLP 2025, affecting the credibility. We propose HalluCitation detection method based on OCR and database matching, and show that it effectively supports our analysis in identifying actual HalluCitations. In particular, papers with four or more detected candidates exhibit high incidence of HalluCitations, suggesting practical guideline for integration into automated detection toolkits. Even when HalluCitations are present, they cannot be immediately attributed to AI-generated content, as they may also originate from secondary sources, making their causes more complex. Accordingly, the presence of HalluCitations alone should not lead to immediate penalties for authors. Instead, it is important to establish prior consensus on preventive measures, such as author toolkits for pre-submission checks. Figure 2: Our analysis method. First, we extract citations from the collected PDFs. Next, we attempt to identify the corresponding papers in reference paper database using fuzzy matching on citation titles, and flag citations for which no matching paper can be found as HalluCitation candidates. Finally, we manually verify the existence of each candidate by referring to the original PDFs and conducting web searches. If it cannot be objectively identified, we mark it as HalluCitation. Year Venue Main Findings S.I.D. WS Total 562 2024 NAACL 2024 941 ACL 2024 EMNLP 1,268 718 2025 NAACL 2025 1,699 ACL 2025 EMNLP 1,809 296 975 1,003 474 1,387 1,405 609 94 721 84 510 173 177 560 259 1,165 684 1,561 2,721 2,954 1,929 4,510 4,"
        },
        {
            "title": "Overall",
            "content": "6,997 5,540 1,056 4,249 17,842 Table 1: Number of PDFs. S.I.D. denotes the total number of papers from the Student Research Workshop, Industry Track, and Demonstration Track. Note that the Industry Track was not held at ACL 2024, and the Student Research Workshop was not held at EMNLP. Conferences are ordered chronologically by their dates."
        },
        {
            "title": "2 Analysis Methods",
            "content": "This investigation is similar to finding needle in haystack, as it requires checking an enormous number of citations to detect small number of HalluCitation. Therefore, manually verifying all citations is impractical. To address this challenge, as shown in Figure 2, we exploit the assumption that the majority of citations are correct and systematically list up candidate HalluCitation. Then, we manually verify each candidate to identify HalluCitation. Note that, since the actual number of HalluCitation is unknown, our reported results should be interpreted as lower bound, and additional HalluCitation may exist in the wild."
        },
        {
            "title": "2.1 Data Collection",
            "content": "In this study, we target all papers presented at total of six conferences, NAACL, ACL, and EMNLP, held in 2024 and 2025, including main confer2 Year Venue Mean Std. Q1 Q2 Q3 Total Citations Year Venue #Papers #Citations Ave. Max 2024 NAACL 36.33 20.48 21 35 48 2024 ACL 41.26 24.04 27 39 52 2024 EMNLP 42.91 23.90 29 40 53 2025 NAACL 39.33 22.42 25 37 50 42.63 25.94 27 40 54 2025 ACL 2025 EMNLP 42.67 27.68 28 39 52 Overall 41.57 25.03 27 39 52 56,717 112,262 126,744 75,870 192,245 177,818 741, Table 2: Number of extracted citations. We report the mean, standard deviation, quartiles (Q1, Q2=median, Q3), and the total number of citations for each venue. ence papers, Findings papers, and all workshop papers. We collected all PDF files and metadata of archival papers registered as events in the ACL Anthology2, excluding tutorials and proceedings. Table 1 presents the statistics of the collected PDFs. In total, our dataset covers over 17,000 papers."
        },
        {
            "title": "2.2 Citation Extraction",
            "content": "Next, we extract citation information from the PDFs. Through pilot studies, we found that pipeline combining OCR-based extraction of raw bibliographical reference strings with normalization of the corresponding bibliographical references yields the most efficient citation extraction. Accordingly, first, we use MinerU (Wang et al., 2024a) to extract reference entries from the references section at the text-block level. Then, we apply GROBID (GRO, 20082025) to parse and normalize each reference entry, obtaining structured bibliographic information such as titles3. Table 2 shows the statistics of the extracted citation counts. From these results, we observe that the median number of citations per paper is 39 and that the quartiles remain stable across venues. In total, we extracted over 740,000 citations."
        },
        {
            "title": "2.3 Canditate Creation",
            "content": "As shown in Table 2, the total number of extracted citations is extremely large, making it impractical to manually inspect all citations. Therefore, we focus on citations that explicitly indicate references to papers from the ACL Anthology or arXiv, and 2https://aclanthology.org/ 3Although GROBID can directly extract bibliographic information from PDFs, our pilot studies indicate that its citation extraction is not sufficiently robust for ACL-style reference formats. Specifically, when reference lists span multiple pages, GROBID frequently fails to extract references appearing on subsequent pages, leading to under-coverage. For instance, in some cases, only around 10 citations are extracted from papers that actually contain 50 references. To mitigate this, we explicitly provide reference blocks as input to GROBID. 2024 NAACL 2024 ACL 2024 EMNLP 2025 NAACL 2025 ACL 2025 EMNLP 268 (0.47%) 1.27 211 (13.52%) 476 (0.42%) 1.25 381 (14.00%) 556 (0.44%) 1.26 443 (15.00%) 336 (17.42%) 465 (0.61%) 1.38 796 (17.65%) 1,117 (0.58%) 1.40 783 (18.79%) 1,222 (0.69%) 1.56 2024 2025 1,035 (14.30%) 1,300 (0.44%) 1.26 1,915 (18.06%) 2,804 (0.63%) 1. Overall 2,950 (16.53%) 4,104 (0.55%) 1.39 6 4 7 10 17 31 7 31 31 Table 3: Number of candidate papers and citations. We also report the proportion of candidate papers and citations relative to the total. Ave. denotes the average number of candidate citations per paper, while Max denotes the maximum number of candidate citations observed in single paper. extract subset of candidate citations by matching them against reference databases. First, we heuristically narrow down the target citations by checking whether the raw citation strings contain keywords related to ACL conferences or arXiv, such as ACL, EMNLP, or arXiv. Next, we search for corresponding papers primarily using metadata from the ACL Anthology4 and arXiv5. In addition, since this heuristic filtering serves as coarse screening, citations to papers outside ACL venues or arXiv may still remain. To further filter such cases, we additionally leverage DBLP6 and OpenAlex (Priem et al., 2022)7. For database matching, we employ characterlevel fuzzy matching on citation titles using similarity score based on the normalized Levenshtein distance with the RapidFuzz library (Bachmann, 2025). Specifically, we compute title similarity as 1 dlev/ max(s1, s2), where s1 and s2 are the character strings being compared, dlev denotes the Levenshtein edit distance (Levenshtein, 1965), and scale it to the range [0, 1]. If no title in the reference databases achieves similarity score of 0.9 or higher, we regard the citation as potentially non-existent and include it in list of candidates for manual verification. Observation. Table 3 shows the statistics of candidate citations and papers. From these results, we 4https://github.com/acl-org/acl-anthology/ tree/c9d3481/data 5https://www.kaggle.com/datasets/ Cornell-University/arxiv/versions/263 6https://drops.dagstuhl.de/entities/artifact/ 10.4230/dblp.xml.2025-12-01 7We use the REST APIs via PyAlex (De Bruin, 2023). 3 observe that both the average number of candidate citations per paper and the maximum number increase over time. In addition to the raw counts of citations and papers, the corresponding percentages also show consistent upward trend. In particular, when comparing 2024 and 2025, we observe different trends in the proportions of targeted papers and citations. These observations suggest that, although our coarse screening and simple database matching may introduce some noise, the increase in candidate citations exceeds what can be explained by such methodological noise alone. This indicates that factors beyond the limitations of our method are likely contributing to the observed increase, most notably the effect of HalluCitation."
        },
        {
            "title": "2.4 Manual Verification",
            "content": "Finally, we manually verify all candidate citations identified in Section 2.3 to determine whether each citation refers to an actual existing work. Since our goal is to detect papers that contain HalluCitation, we mark paper as HalluCited immediately when at least one HalluCitation is identified and terminate further checks for that paper. We apply this procedure to all 2,950 target papers. If identifiable information, such as clickable link, DOI, arXiv ID, page numbers, or venue information, is provided in the citation, we use this information to search for the referenced paper. If such information is missing or insufficient to locate the paper, we conduct web search using the citation title to find potentially corresponding works. We classify citation as HalluCitation if no corresponding paper can be found, e.g., when search returns only the target paper itself, or if similar paper is found but at least two key attributes, such as the title, authors, venue, or page numbers, do not match any reliable source. This verification procedure adopts conservative criterion that basically trusts the papers, prioritizing precision over recall. Appendix shows the list of HalluCited papers."
        },
        {
            "title": "3 Results",
            "content": "Finding 1: The number of HalluCited papers increased sharply in 2025, with EMNLP accounting for over half of these papers. Table 4 reports the number of papers that contain HalluCitation. The results show that the number of hallucinated papers increased from 20 in 2024 to 275 in 2025. In particular, EMNLP 2025 alone accounts for 154 papers, nearly doubling the number observed at Year Venue Main Findings S.I.D. WS 2024 NAACL 1 0.18 50.0 3 0.31 2024 ACL 42.9 2024 EMNLP 3 0.24 27.3 2025 NAACL 6 0.84 17.1 2025 ACL 14 0.82 16.3 2025 EMNLP 60 3.32 39.0 7 0.25 35.0 80 1.89 29.1 87 1.24 29.5 Total 1 0.16 2 0.12 0.00 0 0.00 50.0 0.00 3 0.42 7 0.26 14.3 0 0.00 42.9 0.00 18.2 11 0.37 2 0.39 36.4 2 1.16 18.2 48.6 35 1.81 5.71 17 3.04 28.6 2 1.13 51.2 86 1.91 11.6 44 3.78 21.0 10 3.86 23.4 154 3.70 8.44 36 5.26 29.2 13 4.83 30.0 20 0.28 6 0.33 25.0 2 0.57 10.0 35.3 275 2.59 9.09 97 4.03 26.5 25 3.55 34.9 295 1.65 9.15 103 2.42 26.4 27 2.56 0 0.00 1 0.10 4 0.40 10 2.11 18 1.30 45 3.20 5 0.22 73 2.24 78 1.41 Overall 2024 Table 4: Number of papers including HalluCitation. We also report the proportion of papers with HalluCitation relative to the total number of papers, as well as track-specific proportions within each venue. We denote these values as NUMVenue% Track% . Note that in some venues and tracks with only small number of papers, the percentages may be sensitive to individual papers. Nevertheless, they remain useful for tracking the trends. ACL 2025. Comparing 2024 and 2025, the number of HalluCited papers increased by more than an order of magnitude. The proportion of HalluCited papers also increased from around 0.28% in 2024 to 2.59% in 2025, reaching 3.7% at EMNLP 2025. Examining venue-specific trends in 2025, we find that while workshop papers account for roughly half of the HalluCited papers at NAACL and ACL, nearly 70% of HalluCited papers at EMNLP 2025 appear in the Main and Findings. similar pattern is observed in the proportions of HalluCited papers across tracks in 2025. At NAACL, only workshop papers exceed 3%, whereas at ACL, the S.I.D. tracks reach 3%. In contrast, at EMNLP, all tracks exceed 3%. These results suggest that the impact of HalluCited papers is no longer confined to workshops or side tracks, but is increasingly affecting main tracks. This finding indicates that the rapid increase in HalluCited papers is less likely to be attributable to deficiencies in specific tracks and instead highlights emerging limitations in the ability of the current review system to detect HalluCitation, revealing its social impact on sustainability and trustworthiness. Finding 2: Papers containing multiple HalluCitation candidates are highly likely to contain actual HalluCitation. Table 5 shows the detailed counts of candidate papers and the actually detected HalluCited papers. From these results, we find that when paper contains around four HalluCi4 Candidates HalluCited Hit Rate (%) Freq. Num. Cum. Num. Cum. 9 8 7 6 5 4 3 2 1 10 16 30 40 49 77 168 694 2,950 10 6 14 10 9 28 91 526 2,256 10 5 13 7 7 17 37 76 10 15 28 35 42 59 96 172 295 Num. Cum. 100.0% 100.0% 83.3% 93.8% 92.9% 93.3% 70.0% 87.5% 77.7% 85.7% 60.7% 76.6% 40.7% 57.1% 14.4% 24.8% 5.45% 10.0% Table 5: Number of candidate papers and detected HalluCited papers. We report the number of papers for each frequency of HalluCitation candidates. Hit Rate denotes the proportion of HalluCited papers within each frequency bin. Num. indicates the raw count for each bin, and Cum. represents the cumulative count aggregated from higher frequencies. For papers containing nine or more candidates, all HalluCited papers were successfully detected, and thus, we group them together. tation candidates, nearly three out of four papers indeed include HalluCitation. This finding indicates that even simple title matching against reference databases can sufficiently detect HalluCitation. In contrast, cases with only few candidates often include noise introduced by OCR or parsing errors, as well as limitations of simple characterlevel fuzzy matching. Although these citations may be erroneously flagged due to parsing failures or high thresholds in fuzzy matching, such mismatches rarely occur consecutively. Therefore, when three or four candidates are detected within single paper, the paper is considered doubtful and recommended for further verification. Finding 3: Manual detection is challenging because most HalluCited papers contain only small number of HalluCitations. As shown in Table 5, when paper contains only one or two HalluCitation candidates, the number of detected papers reaches nearly 200, accounting for around twothirds of all HalluCited papers. This indicates that, rather than papers containing many HalluCitations, the majority of HalluCited papers contain only small number of HalluCitations. This also suggests that such HalluCitations are often disguised among otherwise proper citations. At present, only reviewers or readers with strong expertise closely aligned with the cited work may notice these HalluCitations. However, when citations fall outside an area of expertise, manually detecting such hallucinations becomes nearly infeasible within the lim5 Figure 3: Number and proportion of HalluCited papers by area. We report areas with three or more papers. Area names are abbreviated using the first few words. ited time available, e.g., during the review period. Moreover, we point out that some HalluCitations may not always be introduced intentionally. For instance, authors may obtain citation information from secondary sources such as Google Scholar8, reference management tools, or LLM-based recommendation systems, where the entries themselves may already contain hallucinations. We discuss this possibility in detail in Section 4.3. These HalluCitations highlight that authors often implicitly trust such citations and do not always verify their factuality. Moreover, because such HalluCitations are often embedded among otherwise proper citations, it is preferable to introduce automated detection systems into the validation process."
        },
        {
            "title": "4.1 Trend Analysis",
            "content": "Which areas more frequently include HalluCited papers? To analyze trends in the occurrence of HalluCited papers, we examine the research areas in which such papers are accepted. We focus on EMNLP 20259 and analyze the Main and Findings papers by identifying the assigned research areas for each paper. Figure 3 shows the counts and proportions of HalluCited papers by area. For the official area names and the complete list of areas, please refer to the EMNLP 2025 Call for Papers10. We found that areas such as 8https://scholar.google.com/ 9[Accessed on Dec. 31st, 2025] Area information is available at https://rrplanning2022-my.sharepoint.com/: x:/g/personal/jrachford_randrplanning_com/ IQCJIl5zmNpUSZUv8j8H0z1TAVaIf1rOpMYhjCllLm703RY via https://2025.emnlp.org/program. 10https://2025.emnlp.org/calls/main_conference_ papers/#submission-topics (a) High TF-IDF differences in HalluCited papers. 2024* 2025 Freq. Oct. Dec. Feb. Apr. June Aug. Feb. May July 9 8 7 6 5 4 3 2 1 0 0 0 0 0 0 0 0 3 0 0 0 3 0 39 18 87 186 30 226 6 2 0 1 0 0 3 2 0 3 2 0 5 1 0 6 5 0 14 13 0 60 62 3 17 322 237 0 0 0 0 0 0 2 4 4 0 0 0 1 2 6 17 72 1 0 0 0 2 0 4 37 0 0 0 0 0 0 1 3 57 105 230 34 271 20 409 335 102 cand. total 270 659 967 227 922 127 1,349 1,377 371 % 21.1 15.9 23.8 15.0 29.4 15.7 30.3 24.3 27.5 meta 3.9 review 2.2 AVERAGE REVIEW LOAD 5.0 2. 5.3 2.6 3.1 1.8 6.6 2.3 3.7 1.9 6.5 3.9 5.4 3.2 3.1 2.5 (b) High TF-IDF differences in general papers. Figure 4: Word clouds based on TF-IDF differences between HalluCited papers and general papers. Low-Resource NLP, LLM Efficiency, and AI/LLM Agents exhibit relatively high counts and proportions of HalluCited papers. In contrast, areas including NLP Applications, Interpretability, and Resources and Evaluation show relatively large absolute counts but lower proportions. Notably, several areas with higher proportions of HalluCited papers, including Safety and Alignment in LLMs, AI/LLM Agents, and LLM Efficiency, were newly introduced at EMNLP 202511. This suggests that emerging topics may be difficult to secure qualified reviewers for, which may make rigorous review, such as detecting hallucinations, more challenging. Which keywords are included in HalluCited papers? To investigate characteristic terms and topics of HalluCited papers, we compute TF-IDF scores (Manning et al., 2008) over paper titles for both general papers and HalluCited papers, focusing on the Main and Findings papers at EMNLP 2025. Next, we compare the two groups by examining terms with large differences in their TFIDF values. Figure 4 shows word clouds (Mueller, 2023) visualizing terms with high TF-IDF differences for each group. From these results, we observe that HalluCited papers more frequently contain terms related to topics such as Multimodal, Decoding, and Quantization. In contrast, general papers tend to include terms associated with STATISTICS OF AVAILABLE PREPRINTS submit 1,275 2,604 4,835 881 5,813 488 8,350 7,916 1,451 % 21.2 25.3 20.0 25.8 15.9 26.0 16.2 17.4 25.6 Table 6: Number of candidate papers among opted-in preprints in ARR by frequency of HalluCitation candidates. cand. denotes the number of papers containing at least one HalluCitation candidate, and total denotes the number of opted-in anonymous preprints. Bold values exceed the average percentage of accepted papers (16.53%) reported in Table 3. Note that Oct. and Dec. 2024 are excluded because they are not publicly available. Dec. 2023 corresponds to NAACL 2024, Feb. 2024 to ACL 2024, June 2024 to EMNLP 2024, Feb. 2025 to ACL 2025, and May 2025 to EMNLP 2025. We also report the average number of assignments per meta-reviewer and reviewer. Underlined values indicate cases exceeding the expected value. We report the total number of submissions and the preprint disclosure rate. reinforcement learning-related topics, such as Human, Reasoning, and Preference. In addition, HalluCited papers are more likely to use abbreviations such as LLM in their titles, whereas general papers more often use the full form, such as Large Language Model. This suggests that HalluCited papers tend to adopt concise or abbreviated terminology and focus on efficiency-related topics, which is consistent with the results of Figure 3."
        },
        {
            "title": "4.2 How about the Peer-Review Process?",
            "content": "Table 6 shows the number of papers containing HalluCitation candidates among opted-in preprints in the ACL Rolling Review (ARR)12, together with reviewer assignment statistics and the disclosure rate among all submissions. Main and Findings papers are submitted to ARR, and submissions that select the opt-in option are publicly released as preprints. 11https://2025.emnlp.org/track-changes/ 12https://aclrollingreview.org/ 6 In addition, the ARR Statistics Dashboard13 provides aggregate statistics related to ARR. We leveraged these publicly available resources for our analysis. From Table 6, we observe that for submission iterations linked to conferences since 2024, the proportion of papers containing HalluCitation candidates is high. Nevertheless, when compared with the proportion of accepted papers reported in Table 3, such papers are filtered out to some extent during the peer-review process. We also observe that papers containing HalluCitation candidates are frequently assigned to meta-reviewers, and in some iterations, such as February 2024, they may be assigned at rate of at least one paper per reviewer. This highlights the social impact of HalluCited papers as practical and immediate issue for us. However, since the preprint disclosure rate is around 20%, it remains unclear whether such papers are more likely to opt in or opt out. Therefore, we assumed that these papers are uniformly distributed with respect to disclosure. To facilitate retrospective studies and improve transparency in the peer-review process, greater disclosure of review-related information would be important. Finally, we note that this analysis does not assess the quality of HalluCited papers. It is possible that some low-quality submissions include HalluCitations, and that the other factors already contribute to screening. Nevertheless, given the observed increase in HalluCited papers among accepted papers at EMNLP 2025, filtering such cases is becoming increasingly challenging even with reviewer effort alone. This also suggests the need to implement automated flagging systems to assist reviewer workloads and maintain rigor and quality."
        },
        {
            "title": "Are Often Contaminated",
            "content": "HalluCitations may be introduced unintentionally by authors. One common source of such errors is secondary databases, e.g., Google Scholar and Semantic Scholar14, which may themselves contain incorrect or incomplete entries. Figure 5 shows several cases illustrating this issue. In Case 1, the paper title is truncated, preventing correct matching with the arXiv database. In Case 2, we confirm that the referenced paper does not exist. There was no corresponding publication by the listed authors at ACL 2024, nor could we find any paper with similar title, despite the entry being listed in Google 13https://stats.aclrollingreview.org/ 14https://www.semanticscholar.org/ Case 1: Cited by Li et al. (2025d) Semantic Scholar (Ni and Li, 2024a): Xuanfan Ni and Piji Li. 2024a. systematic evaluation of large language models (cid:58)(cid:58)(cid:58)for(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) natural. ArXiv, abs/2405.10251. (Corpus ID: 261341578) arXiv (Formal) (Ni and Li, 2024b): Xuanfan Ni and Piji Li. 2024b. systematic evaluation of large language models (cid:58)(cid:58)(cid:58)for (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) language natural (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) generation(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) tasks. Preprint, arXiv:2405.10251. (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) Case 2: Cited by Lee et al. (2025) Google Scholar (non-existent reference): 2024. Wei Chen, Arjun Kumar, and Lin Yang. Distraction-based attack prompts: An effective jailbreaking method for llms. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL). (ID: 10238337808766132122) Case 3: Cited by Ji et al. (2025) Google Scholar (ID: 3546576102853818527): Alex Ray, Joshua Achiam, and Dario Amodei. 2019. Benchmarking safe exploration in deep reinforcement learning. arXiv preprint arXiv:1910.01708 , 7(1):2. Fujimoto et al. (2019) Semantic Scholar (Corpus ID: 208283920): Josh Achiam and Dario Amodei Missing First Author, Alex Ray. . 2019. Benchmarking safe exploration in deep reinforcement learning. URL: https://d4mucfpksywv.cloudfront. net/safexp-short.pdf. (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) (Expired Link) OpenAI (Official) (Ray et al., 2019): Alex Ray, Josh Achiam, and Dario Amodei. 2019. Benchmarking safe exploration in deep reinforcement learning. URL: https://cdn.openai.com/ safexp-short.pdf Figure 5: Examples of incorrect database entries from Google Scholar and Semantic Scholar. Case 1 shows an incorrect title, Case 2 refers to non-existent paper, and Case 3 contains inaccurate citation information. Scholar. In Case 3, the citation is associated with an incorrect arXiv ID or is missing an author. These cases demonstrate that retrieving citation information from secondary sources such as Google Scholar does not guarantee correctness. In particular, as shown in Case 2, entirely hallucinated papers may be registered in such databases, and this citation is in fact referenced by other papers (Zhou, 2025). In Case 3, the incorrect arXiv ID has propagated to hundreds of subsequent citations. These findings suggest that the presence of HalluCitation does not necessarily imply that an AI system generated the citation or that AI tools were used, as erroneous citation entries are frequently observed in these databases. It is plausible that authors man7 ually retrieved citation information from search engines or automatically imported it via reference management tools, such as Paperpile15, thereby inadvertently incorporating incorrect entries. Furthermore, it is important to address such hallucinations as early as possible. Recent AI-Scientist agents can retrieve citations via APIs from these databases, potentially amplifying the issue and increasing both its scale and complexity over time. Nevertheless, these cases indicate that authors did not consult the original sources directly and instead relied on secondary sources (Church, 2017). To mitigate this issue, it is necessary to acknowledge that databases are not error-free and to encourage authors to obtain citation information directly from primary sources such as the ACL Anthology or arXiv. Alternatively, the use of citation normalization tools, such as Ribiber16, may help reduce such errors and should be more widely adopted."
        },
        {
            "title": "5 Suggestions and Recommendations",
            "content": "Introducing automatic verification systems for author toolkits and review checks. As discussed in Section 4.3, HalluCitations may originate from errors in secondary sources. Therefore, the presence of HalluCitations cannot be directly attributed to the use of LLMs or AI-generated papers alone. However, regardless of their origin, such HalluCitations indicate that authors often fail to carefully verify reference information and do not always consult the original sources. Therefore, we argue that it is necessary to introduce dedicated HalluCitation detection tools, such as ours or existing LLM-based agents17, and to integrate them into existing toolkits such as ACL pubcheck18. By using these automatic verification systems on both the author and organizer sides, authors can receive early warnings before submission, and organizers can automatically scan submissions at scale. This would enable fairer verification, reduce reviewer workload, and improve overall citation reliability. Definition of HalluCitation and ensuring traceability in the peer-review process. It is important to clearly define what should be considered HalluCitation, as not all citation errors are equally severe. In this study, we define HalluCitations based on the existence of the cited title and the 15https://paperpile.com/ 16https://github.com/yuchenlin/rebiber 17https://gptzero.me/news/iclr-2026/ 18https://github.com/acl-org/aclpubcheck consistency of key identifiers such as IDs and page numbers. However, minor citation errors have long existed in academic writing (Wang and Barabási, 2021), and inaccuracies may also originate from secondary sources (Besançon et al., 2024), making their causes complex. When only few HalluCitations are identified, it is often sufficient to request correction. Unlike journals, where editors may correct citation information, conference papers place this responsibility largely on authors. Therefore, minor citation errors should not be overly penalized, and providing author toolkits in advance can help prevent many formal mistakes. more serious issue arises when it is unclear whether such corrections are reflected in the camera-ready version, highlighting the need for traceability in the revision and peer-review process. Improving it would enable verification of corrections and support rapid analyses of emerging issues such as HalluCitations. Rethinking the Purpose of Conferences and Peer Review. What does peer review truly mean? We consider reviewers to be hidden co-authors of paper, whose role is not merely to complete formal evaluation but to engage in constructive discussion that improves the work. From this perspective, review systems should prioritize sufficient time and manageable workloads that enable meaningful feedback, rather than mechanisms that incentivize rapid review completion under strict time constraints of only few weeks. Currently, early release through preprints, such as arXiv, and blog posts has become common. Despite the lack of formal verification through peer review, such outputs are already widely cited (Church et al., 2025). At the same time, conference reviewing is often less rigorous than journal peer review. If conferences offer neither the immediacy of preprints nor the rigor of journals, their core purpose should be reconsidered. The growing review burden risks undermining conference credibility and raises concerns about long-term sustainability. Without structural change, this situation is likely to worsen. One possible direction is community-wide transition toward mega-journal-style model with asynchronous, rigorous peer review, combined with conference venues that focus on presenting accepted work. Such an evolution could provide more sustainable review process with sufficient time for verification, while still offering meaningful opportunities for authors to present their research. Originally, the ARR system and the Findings track had the potential to fulfill this role. However, ARR cycles became tightly coupled with conferences, and Findings were eventually positioned as companion (Cohn et al., 2020). Now, more than five years after the introduction of ARR and Findings, it may be time to reconsider their roles. One possible redefinition is to treat them as journal-equivalent venues, with conference presentations serving as form of certification or presentation right for accepted work, similar to TMLR. While this would not fundamentally resolve the growth in submission volume, it would at least help preserve credibility. Adapting to such flexible models may become necessary as the community evolves."
        },
        {
            "title": "6 Conclusion",
            "content": "We investigated the presence of HalluCitations in accepted papers at ACL conferences by examining all papers published at ACL, NAACL, and EMNLP in 2024 and 2025. As result, we identified nearly 300 HalluCited papers, more than half of which appeared at EMNLP 2025. We argue that the presence of HalluCitations should not be treated as grounds for immediate penalties. As discussed, such errors can arise unintentionally rather than deliberate misconduct by authors. Instead of punitive measures, we emphasize the importance of developing author toolkits and pre-submission checks to proactively prevent such issues. It is also worth noting that the HalluCited papers were accepted due to their substantive contributions. Accordingly, authors of HalluCited papers should not be penalized post hoc. Since the ACL Anthology allows erratum revisions, it is important to better encourage voluntary corrections19. Acceptance should not be treated as binary reward or punishment, but rather as part of an ongoing process of improving and communicating research. Moreover, it is important to reaffirm the collaborative nature of peer review. Conferences exist to share work, not to reward perfection under batch review and strict time constraints (Church, 2005). No paper is flawless, and healthy research community should remain tolerant of minor errors while actively promoting responsible correction. The ACL community has long been sustained by selfimprovement mechanisms such as Findings papers and ARR. ACL is not an AI/ML conference. The NLP community can now take the initiative in shaping peer review and conferences in the LLM era."
        },
        {
            "title": "Limitations",
            "content": "Scope. This study focuses on six recent top-tier NLP conferences. Although the scope could be expanded, we limit our analysis for several reasons. First, the number of HalluCited papers is negligible in earlier years, e.g., only two cases at NAACL 2024, and the trend is monotonically increasing, making more retrospective analysis less informative. Second, our primary focus is on the sharp surge observed in 2025, which motivates us to concentrate on the most recent conferences. Other venues, such as AACL 2025 or the ARR October 2025 cycle, were not publicly available at the time of writing and therefore could not be analyzed, though our findings suggest that non-trivial number of HalluCitations may also be present there. While extending the analysis to other AI/ML conferences is possible, our goal is to assess the impact within the NLP community specifically. Overall, despite this limited scope, we believe our analysis is sufficiently comprehensive for the intended purpose, and broader investigations, e.g., other conferences and journals, are left for future work. Analysis. We primarily analyzed accepted papers. First, accepted papers are readily accessible, which makes large-scale analysis feasible. Second, we showed that even accepted papers contain HalluCitations, suggesting that the peer-review process may not always sufficiently verify citation correctness under time constraints and heavy reviewer workloads. From this perspective, analyzing accepted papers is both reasonable and informative. Nevertheless, we also examined publicly available ARR data and conducted comprehensive investigation within the limits of available disclosures. However, as discussed in Section 4.2, the transparency of the peer-review process remains limited, and more thorough analysis is currently infeasible due to restricted access to review materials. Moreover, rejected papers are generally not publicly available. Although there have been efforts toward data release20 (Dycke et al., 2023, 2022), these resources are limited to cases where explicit agreement has been obtained. In addition, the traceability of whether reviewer-requested corrections are reflected in the camera-ready versions remains unclear. We believe academic papers are the outcome of collaborative work between authors and reviewers. Thus, papers also reflect reviewer feed19https://aclanthology.org/info/corrections/ 20https://arr-data.aclweb.org/ 9 back. From this perspective, review materials, including rejected papers, should be made publicly available (Church et al., 2025; Yang, 2025; Kim et al., 2025d). Such transparency would facilitate analyses of review quality and outcomes. Methods. We focused on arXiv and the ACL Anthology as solid and well-established data sources. Although extending our analysis to broader domains such as AI, ML, CV, robotics, speech, and journals is an important direction for future work, these domains currently lack unified and consistently maintained bibliographic infrastructure comparable to that of the ACL community. Since this work focuses on ACL conferences. Despite this restricted scope, we identify nearly 300 HalluCited papers, indicating that our approach is effective. For implementation, we employ MinerU for OCR, although alternative OCR tools (Wei et al., 2025a; Cui et al., 2025a) and additional engineering improvements are possible. These implementation choices are not our primary contribution. Instead, the key contribution of this work is to quantitatively and comprehensively demonstrate the prevalence and impact of HalluCitations. Finally, our methodology prioritizes precision, and the reported results should be interpreted as lower bound. Additional HalluCitations may exist beyond those detected here. We hope that this study will serve as baseline and motivate follow-up work that improves detection accuracy and coverage."
        },
        {
            "title": "Ethical Considerations",
            "content": "Information. In this manuscript, all information reflects the state of publicly available data as of January 4, 2026. Unless explicitly stated otherwise, links, metadata, and dataset contents correspond to what was available at that time. Updates, corrections, or removals after this date may change the information and are not reflected in this study. Licenses. Our primary data sources are the ACL Anthology and OpenReview, both of which are distributed under the Creative Commons Attribution 4.0 International License. We confirm that all additional datasets, dumps, and metadata used in this study are publicly available and permissively licensed. The code and tools developed in this work are authored by us. Human verification was conducted solely by the authors of this paper. Therefore, no licensing or consent issues arise from human annotation in this study. Discussion on the protection of personally identifiable information and potential harm. This study analyzes real published papers. Therefore, the author information of HalluCited papers is inherently observable. However, concealing the identities of HalluCited papers would significantly hinder reproducibility. Moreover, the HalluCitations discussed here can already be identified through public search engines and thus constitute existing public information. Moreover, our analysis is comprehensive and systematic and does not target or single out specific individuals. Accordingly, this study is conducted strictly as academic research. Importantly, as explicitly stated in Section 6, we do not attribute HalluCitations to author misconduct, nor do we argue that authors should be penalized. Rather, we highlight limitations of the current review system, particularly the lack of appropriate author toolkits. Accordingly, this analysis and its claims and purpose are not intended to be offensive or accusatory. For transparency and reproducibility, we provide list of HalluCited papers in Section B. We do not introduce new annotations or subjective judgments. We only verify and list HalluCitations that already exist in the public record. As an additional safeguard, Table 7 in Section does not directly display author names or paper titles of HalluCited papers. Instead, each HalluCited paper is referenced only via links to the corresponding reference entries, and we show one example HalluCitation per paper. This design balances transparency and reproducibility with the protection of authors. Therefore, our discussion throughout the paper remains neutral and forward-looking, focusing on constructive solutions rather than blame. We have carefully reviewed the ACL Ethics Policy21 and confirm this research complies with its guidelines. Tool usage. We used DeepL, ChatGPT, and Grammarly for translation and grammatical improvement. All original content was written by the authors, and the authors take full responsibility for the final manuscript. For experiments, we used an NVIDIA A6000 GPU for OCR processing. Other analyses were conducted using MacBook Pro, spreadsheets, and Google Chrome for literature searches. Notably, all records related to HalluCitations were managed locally and were not transmitted to any external APIs or services. Accordingly, tool usage and data management were conducted in compliance with ethical considerations. 21https://ethics.aclweb.org/"
        },
        {
            "title": "References",
            "content": "20082025. Grobid. https://github.com/ kermitt2/grobid. Aastik, Meghana Topu, Chinmay Kulkarni, and Pragya Paramita Sahu. 2025. NormAL LoRA: What is the perfect size? In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1971619731, Suzhou, China. Association for Computational Linguistics. Mohammad Mahdi Abootorabi, Amirhosein Zobeiri, Mahdi Dehghani, Mohammadali Mohammadkhani, Bardia Mohammadi, Omid Ghahroodi, Mahdieh Soleymani Baghshah, and Ehsaneddin Asgari. 2025. Ask in any modality: comprehensive survey on multimodal retrieval-augmented generation. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1677616809, Vienna, Austria. Association for Computational Linguistics. Tewodros Achamaleh, Tolulope Olalekan Abiola, Lemlem Eyob Kawo, Mikiyas Mebraihtu, and Grigori Sidorov. 2025. CIC-NLP@DravidianLangTech 2025: Detecting AI-generated product reviews in Dravidian languages. In Proceedings of the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 502 507, Acoma, The Albuquerque Convention Center, Albuquerque, New Mexico. Association for Computational Linguistics. Priyobroto Acharya, Haranath Mondal, Dipanjan Saha, Dipankar Das, and Sivaji Bandyopadhyay. 2025. JU-NLP: Improving low-resource Indic translation system with efficient LoRA-based adaptation. In Proceedings of the Tenth Conference on Machine Translation, pages 12011209, Suzhou, China. Association for Computational Linguistics. Muhammad Adilazuarda. 2024. Beyond Turing: comparative analysis of approaches for detecting In Proceedings of the machine-generated text. 4th Workshop on Trustworthy Natural Language Processing (TrustNLP 2024), pages 112, Mexico City, Mexico. Association for Computational Linguistics."
        },
        {
            "title": "Tofayel",
            "content": "Aftahee,"
        },
        {
            "title": "Ahmmed",
            "content": "Babu, Jawad HosMD Musa Kalimullah Ratul, sain, and Mohammed Moshiul Hoque. 2025. CUET_NetworkSociety@DravidianLangTech 2025: transformer-based approach for detecting AIgenerated product reviews in low-resource Dravidian languages. In Proceedings of the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 522528, Acoma, The Albuquerque Convention Center, Albuquerque, New Mexico. Association for Computational Linguistics. Mahmoud Ahmad and Habeebah Kakudi. 2025. Stance detection on Nigerian 2023 election tweets using BERT: low-resource transformer-based approach. In Proceedings of the 6th Workshop on Computational Approaches to Discourse, Context 11 (CODI 2025), and Document-Level pages 5463, Suzhou, China. Association for Computational Linguistics."
        },
        {
            "title": "Inferences",
            "content": "Mohammad Sadegh Akhondzadeh, Aleksandar Bojchevski, Evangelos Eleftheriou, and Martino Dazzi. 2025. KurTail : Kurtosis-based LLM quantization. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1740417419, Suzhou, China. Association for Computational Linguistics. Maro Akpobi. 2025. Yankari: Monolingual Yoruba dataset. In Proceedings of the Sixth Workshop on African Natural Language Processing (AfricaNLP 2025), pages 16, Vienna, Austria. Association for Computational Linguistics. Diyam Akra, Mohammed Khalilia, and Mustafa Jarrar. 2025. Active learning for multidialectal Arabic POS tagging. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2496024973, Suzhou, China. Association for Computational Linguistics. Flor Alberts, Ivo Bruinier, Nathalie Palm, Justin Paetzelt, and Erik Varecha. 2025. FENJI at SemEval2025 task 3: Retrieval-augmented generation and hallucination span detection. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 11431151, Vienna, Austria. Association for Computational Linguistics. Falwah Alhamed, Rebecca Bendayan, Julia Ive, and Lucia Specia. 2024. Monitoring depression severity and symptoms in user-generated content: An annotation scheme and guidelines. In Proceedings of the 14th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis, pages 227233, Bangkok, Thailand. Association for Computational Linguistics. Dana Alsagheer, Abdulrahman Kamal, Mohammad Kamal, Cosmo Yang Wu, and Weidong Shi. 2025. The lawyer that never thinks: Consistency and fairness as keys to reliable AI. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 99439954, Vienna, Austria. Association for Computational Linguistics. Jesus Alvarez C, Daua Karajeanes, Ashley Prado, John Ruttan, Ivory Yang, Sean Obrien, Vasu Sharma, and Kevin Zhu. 2025. Advancing uto-aztecan language technologies: case study on the endangered Comanche language. In Proceedings of the Fifth Workshop on NLP for Indigenous Languages of the Americas (AmericasNLP), pages 2737, Albuquerque, New Mexico. Association for Computational Linguistics. Anson Antony and Annika Schoene. 2025. Retrievalenhanced mental health assessment: Capturing selfstate dynamics from social media using in-context learning. In Proceedings of the 10th Workshop on Computational Linguistics and Clinical Psychology (CLPsych 2025), pages 268278, Albuquerque, New Mexico. Association for Computational Linguistics. Saurav Aryal and Kritika Pant. 2025. Howard University-AI4PC at SemEval-2025 task 9: Using open-weight BART-MNLI for zero shot classification of food recall documents. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 19191923, Vienna, Austria. Association for Computational Linguistics. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike Darcy, David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, and 6 others. 2024. Openscholar: Synthesizing scientific literature with retrievalaugmented lms. Preprint, arXiv:2411.14199. Yash Kumar Atri, Ahmed Alaa, and Thomas Hartvigsen. 2025. Lifelong model editing with graph-based external memory. In Findings of the Association for Computational Linguistics: ACL 2025, pages 13336 13352, Vienna, Austria. Association for Computational Linguistics. Mohammad Anas Azeez, Rafiq Ali, Ebad Shabbir, Zohaib Hasan Siddiqui, Gautam Siddharth Kashyap, Jiechao Gao, and Usman Naseem. 2025. Truth, trust, and trouble: Medical AI on the edge. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 10171025, Suzhou (China). Association for Computational Linguistics. Massa Baali, Xiang Li, Hao Chen, Syed Abdul Hannan, Rita Singh, and Bhiksha Raj. 2025. CAARMA: Class augmentation with adversarial mixup regularization. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 97329742, Suzhou, China. Association for Computational Linguistics. Max Bachmann. 2025. rapidfuzz/rapidfuzz: Release 3.13.0. Thomas Bailleux, Tanmoy Mukherjee, Pierre Marquis, and Zied Bouraoui. 2025. Connecting concept layers and rationales to enhance language model interpretability. In Proceedings of the 14th Joint Conference on Lexical and Computational Semantics (*SEM 2025), pages 409429, Suzhou, China. Association for Computational Linguistics. Mohammad Jahid Ibna Basher, Md Kowsher, Md Saiful Islam, Rabindra Nath Nandi, Nusrat Jahan Prottasha, Mehadi Hasan Menon, Tareq Al Muntasir, Shammur Absar Chowdhury, Firoj Alam, Niloofar Yousefi, and Ozlem Garibay. 2025. BnTTS: Few-shot speaker adaptation in low-resource setting. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 49564968, Albuquerque, New Mexico. Association for Computational Linguistics. Samuel Bell, Eduardo Sánchez, David Dale, Pontus Stenetorp, Mikel Artetxe, and Marta R. Costa-Jussà. 2025. Translate, then detect: Leveraging machine translation for cross-lingual toxicity classification. In Proceedings of the Tenth Conference on Machine Translation, pages 253268, Suzhou, China. Association for Computational Linguistics. Anya Belz and Craig Thomson. 2025. HEDS 3.0: In The human evaluation data sheet version 3.0. Proceedings of the Fourth Workshop on Generation, Evaluation and Metrics (GEM2), pages 6081, Vienna, Austria and virtual meeting. Association for Computational Linguistics. Lonni Besançon, Guillaume Cabanac, Cyril Labbé, and Alexander Magazinov. 2024. Sneaked references: Fabricated reference metadata distort citation counts. Journal of the Association for Information Science and Technology, 75(12):13681379. Niloufar Beyranvand, Hamidreza Dastmalchi, Aijun An, Heidar Davoudi, Winston Chan, and Ron DiCarlantonio. 2025. GEAR: scalable and interpretable evaluation framework for RAG-based car assistant systems. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 27922810, Suzhou (China). Association for Computational Linguistics. Debarpan Bhattacharya, Apoorva Kulkarni, and Sriram Ganapathy. 2025. FESTA: Functionally equivalent sampling for trust assessment of multimodal LLMs. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1227712295, Suzhou, China. Association for Computational Linguistics. Nikhil Bhendawade, Irina Belousova, Qichen Fu, Henry Mason, Antonie Lin, Mohammad Rastegari, and Mahyar Najibi. 2025. Speculative streaming: Efficient and scalable speculative decoding with multi-stream attention. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1954719570, Suzhou, China. Association for Computational Linguistics. Guanqun Bi, Zhuang Chen, Zhoufu Liu, Hongkai Wang, Xiyao Xiao, Yuqiang Xie, Wen Zhang, Yongkang Huang, Yuxuan Chen, Libiao Peng, and Minlie Huang. 2025. MAGI: Multi-agent guided interview for psychiatric assessment. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2489824921, Vienna, Austria. Association for Computational Linguistics. Suhas Bn, Dominik O. Mattioli, Andrew M. Sherrill, Rosa I. Arriaga, Christopher Wiese, and Saeed Abdullah. 2025. How real are synthetic therapy conversations? evaluating fidelity in prolonged exposure dialogues. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2098620995, Suzhou, China. Association for Computational Linguistics. 12 Ondrej Bohdal, Mete Ozay, Jijoong Moon, Kyenghun Lee, Hyeonmok Ko, and Umberto Michieli. 2025. Efficient compositional multi-tasking for on-device large language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2812928153, Suzhou, China. Association for Computational Linguistics. Abhilekh Borah, Chhavi Sharma, Danush Khanna, Utkarsh Bhatt, Gurpreet Singh, Hasnat Md Abdullah, Raghav Kaushik Ravi, Vinija Jain, Jyoti Patel, Shubham Singh, Vasu Sharma, Arpita Vats, Rahul Raja, Aman Chadha, and Amitava Das. 2025. Alignment quality index (AQI) : Beyond refusals: AQI as an intrinsic alignment diagnostic via latent geometry, cluster divergence, and layer wise pooled representations. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 28882947, Suzhou, China. Association for Computational Linguistics. Abdessalam Bouchekif, Samer Rashwani, Heba Sbahi, Shahd Gaben, Mutaz Al Khatib, and Mohammed Ghaly. 2025. Assessing large language models on islamic legal reasoning: Evidence from inheritance law evaluation. In Proceedings of The Third Arabic Natural Language Processing Conference, pages 246257, Suzhou, China. Association for Computational Linguistics. Hossam Boudraa, Benoit Favre, and Raquel Urena. 2025. Implicit hate target span detection in zeroand few-shot settings with selective sub-billion parameter models. In Proceedings of the The 9th Workshop on Online Abuse and Harms (WOAH), pages 228 240, Vienna, Austria. Association for Computational Linguistics. Nicolas Bougie and Narimawa Watanabe. 2025. Generative reviewer agents: Scalable simulacra of peer review. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 98116, Suzhou (China). Association for Computational Linguistics. Hongyi Cai, Jie Li, Mohammad Mahdinur Rahman, and Wenzhen Dong. 2025. Low-confidence gold: Refining low-confidence samples for efficient instruction tuning. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 82338240, Suzhou, China. Association for Computational Linguistics. Crivoi Carla and Ana Sabina Uban. 2025. SciBERT meets contrastive learning: solution for scientific hallucination detection. In Proceedings of the Fifth Workshop on Scholarly Document Processing (SDP 2025), pages 336343, Vienna, Austria. Association for Computational Linguistics. Ege Yigit Çelik and Selma Tekir. 2025. CiteBART: Learning to generate citations for local citation recommendation. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 17031719, Suzhou, China. Association for Computational Linguistics. Ernie Chang, Yang Li, Patrick Huber, Vish Vogeti, David Kant, Yangyang Shi, and Vikas Chandra. 2025. AutoMixer: Checkpoint artifacts as automatic data mixers. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1994219953, Vienna, Austria. Association for Computational Linguistics. Yu Chao, Siyu Lin, Xiaorong Wang, Zhu Zhang, Zihan Zhou, Haoyu Wang, Shuo Wang, Jie Zhou, Zhiyuan Liu, and Maosong Sun. 2025. LLMMapReducev3: Enabling interactive in-depth survey generation through MCP-driven hierarchically modular agent system. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 688695, Suzhou, China. Association for Computational Linguistics. Shraddha Chauhan and Abhinav Kumar. 2025. MNLP@DravidianLangTech 2025: Transformerbased multimodal framework for misogyny meme detection. In Proceedings of the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 248253, Acoma, The Albuquerque Convention Center, Albuquerque, New Mexico. Association for Computational Linguistics. Divij Chawla, Ashita Bhutada, Duc Anh Do, Abhinav Raghunathan, Vinod Sp, Cathy Guo, Dar Win Liew, Prannaya Gupta, Rishabh Bhardwaj, Rajat Bhardwaj, and Soujanya Poria. 2025. Evaluating AI for finance: Is AI credible at assessing investment risk appetite? In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 28282839, Suzhou (China). Association for Computational Linguistics. Bowen Chen, Zhao Wang, and Shingo Takamatsu. 2025a. OMS: On-the-fly, multi-objective, selfreflective ad keyword generation via LLM agent. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1859418612, Suzhou, China. Association for Computational Linguistics. Danlu Chen, Freda Shi, Aditi Agarwal, Jacobo Myerston, and Taylor Berg-Kirkpatrick. 2024. LogogramNLP: Comparing visual and textual representations of ancient logographic writing systems for NLP. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1423814254, Bangkok, Thailand. Association for Computational Linguistics. Diyang Chen. 2025. pingan-team at SemEval-2025 task 2: LoRA-augmented qwen2.5 with Wikidatadriven entity translation. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 20652070, Vienna, Austria. Association for Computational Linguistics. Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. 2025b. Benchmarking large language models on answering and explaining challenging medical 13 questions. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 35633599, Albuquerque, New Mexico. Association for Computational Linguistics. Meng Chen, Philip Arthur, Qianyu Feng, Cong Duy Vu Hoang, Yu-Heng Hong, Mahdi Kazemi Moghaddam, Omid Nezami, Duc Thien Nguyen, Gioacchino Tangari, Duy Vu, Thanh Vu, Mark Johnson, Krishnaram Kenthapadi, Don Dharmasiri, Long Duong, and Yuan-Fang Li. 2025c. Mastering the craft of data synthesis for CodeLLMs. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1248412500, Albuquerque, New Mexico. Association for Computational Linguistics. Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, and Wanxiang Che. 2025d. Ai4research: survey of artificial intelligence for scientific research. Preprint, arXiv:2507.01903. Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, and Haifeng Wang. 2025e. Inner thinking transformer: Leveraging dynamic depth scaling to foster adaptive internal thinking. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2824128259, Vienna, Austria. Association for Computational Linguistics. Yuhang Chen, Zhen Tan, Ajay Kumar Jaiswal, Huaizhi Qu, Xinyu Zhao, Qi Lin, Yu Cheng, Andrew Kwong, Zhichao Cao, and Tianlong Chen. 2025f. Bit-flip error resilience in LLMs: comprehensive analysis and defense framework. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1042510435, Suzhou, China. Association for Computational Linguistics. Veer Chheda, Aaditya Uday Ghaisas, Avantika Sankhe, and Dr. Narendra Shekokar. 2025. Extract-explainabstract: rhetorical role-driven domain-specific summarisation framework for Indian legal docIn Proceedings of the Natural Legal uments. Language Processing Workshop 2025, pages 439 455, Suzhou, China. Association for Computational Linguistics. Kenneth Church. 2005. Last words: Reviewing the reviewers. Computational Linguistics, 31(4):575578. Kenneth Ward Church. 2017. Word2vec. Natural Language Engineering, 23(1):155162. Kenneth Ward Church, Raman Chandrasekar, John E. Is peerIn Proceedings of Ortega, and Ibrahim Said Ahmad. 2025. reviewing worth the effort? the 31st International Conference on Computational Linguistics, pages 35893599, Abu Dhabi, UAE. Association for Computational Linguistics. Jonathan H. Clark, Dan Garrette, Iulia Turc, and John Wieting. 2022. Canine: Pre-training an efficient tokenization-free encoder for language representation. Transactions of the Association for Computational Linguistics, 10:7391. Trevor Cohn, Yulan He, and Yang Liu, editors. 2020. Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, Online. Gaye Colakoglu, Gürkan Solmaz, and Jonathan Fürst. 2025. Problem solved? information extraction design space for layout-rich documents using LLMs. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1790817927, Suzhou, China. Association for Computational Linguistics. Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Handong Zheng, Jing Zhang, Jun Zhang, Yi Liu, Dianhai Yu, and Yanjun Ma. 2025a. Paddleocr-vl: Boosting multilingual document parsing via 0.9b ultra-compact vision-language model. Preprint, arXiv:2510.14528. Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das, Bradley A. Malin, and Sricharan Kumar. 2025b. Heuristic-based search algorithm in automatic instruction-focused prompt optimization: survey. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2209322111, Vienna, Austria. Association for Computational Linguistics. Xia Cui. 2025. xiacui at SemEval-2025 task 11: Addressing data imbalance in transformer-based multiIn label emotion detection with weighted loss. Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 247 256, Vienna, Austria. Association for Computational Linguistics. Amitava Das, Yaswanth Narsupalli, Gurpreet Singh, Vinija Jain, Vasu Sharma, Suranjana Trivedy, Aman Chadha, and Amit Sheth. 2025a. YinYang-align: new benchmark for competing objectives and introducing multi-objective preference based text-toimage alignment. In Findings of the Association for Computational Linguistics: ACL 2025, pages 23518 23598, Vienna, Austria. Association for Computational Linguistics. Amitava Das, Suranjana Trivedy, Danush Khanna, Yaswanth Narsupalli, Basab Ghosh, Rajarshi Roy, Gurpreet Singh, Vinija Jain, Vasu Sharma, Aishwarya Naresh Reganti, and Aman Chadha. 2025b. DPO kernels: semantically-aware, kernelenhanced, and divergence-rich paradigm for diIn Findings of the rect preference optimization. 14 Association for Computational Linguistics: ACL 2025, pages 2217422270, Vienna, Austria. Association for Computational Linguistics. Jonathan De Bruin. 2023. PyAlex. Varun Dhanraj and Chris Eliasmith. 2025. Improving rule-based reasoning in LLMs using neurosymbolic representations. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3057730596, Suzhou, China. Association for Computational Linguistics. Yiwen Ding, Jiarui Liu, Zhiheng Lyu, Kun Zhang, Bernhard Schölkopf, Zhijing Jin, and Rada Mihalcea. 2025. Voices of her: Analyzing gender differences in the AI publication world. In Proceedings of the Fourth Workshop on NLP for Positive Impact (NLP4PI), pages 196214, Vienna, Austria. Association for Computational Linguistics. Bonaventure F. P. Dossou and Henri Aïdasso. 2025. Towards open-ended discovery for low-resource NLP. In Proceedings of the 2nd Workshop on Uncertainty-Aware NLP (UncertaiNLP 2025), pages 287297, Suzhou, China. Association for Computational Linguistics. Razvan-Gabriel Dumitru, Minglai Yang, Vikas Yadav, and Mihai Surdeanu. 2025. CopySpec: Accelerating LLMs with speculative copy-and-paste. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2631226343, Suzhou, China. Association for Computational Linguistics. Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2022. Yes-yes-yes: Proactive data collection for ACL In Findings of the rolling review and beyond. Association for Computational Linguistics: EMNLP 2022, pages 300318, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Nils Dycke, Ilia Kuznetsov, and Iryna Gurevych. 2023. NLPeer: unified resource for the computational study of peer review. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50495073, Toronto, Canada. Association for Computational Linguistics. Nellia Dzhubaeva, Katharina Trinley, and Laura Pissani. 2025. Unstructured minds, predictable machines: comparative study of narrative cohesion in human and LLM stream-of-consciousness writing. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pages 10791096, Vienna, Austria. Association for Computational Linguistics. Roxanne El Baff, Dominik Opitz, and Diaoulé Diallo. 2025. CriticalBrew at CQs-gen 2025: Collaborative multi-agent generation and evaluation of critical questions for arguments. In Proceedings of the 12th Argument mining Workshop, pages 314321, Vienna, Austria. Association for Computational Linguistics. Vignesh Ethiraj, Ashwath D, Sidhanth Menon, Divya Vijay, and Vidhyakshaya Kannan. 2025. T-VEC: telecom-specific vectorization model with enhanced semantic understanding via deep triplet loss finetuning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 24492460, Suzhou (China). Association for Computational Linguistics. Egecan Evgin, Ilknur Karadeniz, and Olcay Taner Yıldız. 2025. MetninOzU at BioLaySumm2025: Text summarization with reverse data augmentation and injecting salient sentences. In Proceedings of the 24th Workshop on Biomedical Language Processing (Shared Tasks), pages 179184, Vienna, Austria. Association for Computational Linguistics. Neo Eyal, Nachum Dershowitz, and Kfir Bar. 2025. Layer duplication in LLMs. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1779717807, Suzhou, China. Association for Computational Linguistics. Yijia Fan, Jusheng Zhang, Kaitong Cai, Jing Yang, and Keze Wang. 2025. CCG: Rare-label prediction via neural SEMdriven causal game. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 62436256, Suzhou, China. Association for Computational Linguistics. Maryam Fatima. 2025. FIRMA: Bidirectional formalinformal mathematical language alignment with proof-theoretic grounding. In Proceedings of The 3rd Workshop on Mathematical Natural Language Processing (MathNLP 2025), pages 6276, Suzhou, China. Association for Computational Linguistics. Arvid Frydenlund. 2025. Language models, graph searching, and supervision adulteration: When more supervision is less and how to make more more. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2901129059, Vienna, Austria. Association for Computational Linguistics. Zichuan Fu, Xian Wu, Guojing Li, Yingying Zhang, Yefeng Zheng, Tianshi Ming, Yejing Wang, Wanyu Wang, and Xiangyu Zhao. 2025. Model merging for knowledge editing. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 6: Industry Track), pages 433443, Vienna, Austria. Association for Computational Linguistics. Scott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. 2019. Benchmarking batch deep reinforcement learning algorithms. Preprint, arXiv:1910.01708. Achille Fusco, Maria Letizia Piccini Bianchessi, Tommaso Sgrizzi, Asya Zanollo, and Cristiano Chesi. 2025. Linguistic units as tokens: Intrinsic and extrinsic evaluation with BabyLM. In Proceedings of the 15 First BabyLM Workshop, pages 496507, Suzhou, China. Association for Computational Linguistics. Jack Gallifant, Shan Chen, Kuleen Sasse, Hugo Aerts, Thomas Hartvigsen, and Danielle Bitterman. 2025. Sparse autoencoder features for classifications and transferability. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2992729951, Suzhou, China. Association for Computational Linguistics. Fan Gao, Cheng Huang, Yutong Liu, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, Renzeng Duojie, Gadeng Luosang, Rinchen Dongrub, Dorje Tashi, Xiao Feng Cd, Yongbin Yu, and Hao Wang. 2025. TLUE: Tibetan language understanding evaluation benchmark. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3505935085, Suzhou, China. Association for Computational Linguistics. Rahul Garg, Trilok Padhi, Hemang Jain, Ugur Kursuncu, and Ponnurangam Kumaraguru. 2025. Just KIDDIN : Knowledge infusion and distillation for detection of INdecent memes. In Findings of the Association for Computational Linguistics: ACL 2025, pages 23067 23086, Vienna, Austria. Association for Computational Linguistics. Carol-Luca Gasan and Vasile Pais. 2025. SAG: Enhancing domain-specific information retrieval with semantic-augmented graphs. In Proceedings of the 14th Joint Conference on Lexical and Computational Semantics (*SEM 2025), pages 362 371, Suzhou, China. Association for Computational Linguistics. Ryan George, Akshay Govind Srinivasan, Jayden Koshy Joe, Harshith R, Vijayavallabh J, Hrushikesh Kant, Rahul Vimalkanth, Sachin S, and Sudharshan Suresh. 2025. Enhancing financial RAG with agentic AI and multi-HyDE: novel approach to knowledge retrieval and hallucination reduction. In Proceedings of The 10th Workshop on Financial Technology and Natural Language Processing, pages 1932, Suzhou, China. Association for Computational Linguistics. Akash Ghosh, Debayan Datta, Sriparna Saha, and Chirag Agarwal. 2025. survey of multilingual reasoning in language models. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 89208936, Suzhou, China. Association for Computational Linguistics. Lynn Greschner and Roman Klinger. 2025. Fearful falcons and angry llamas: Emotion category annotations of arguments by humans and LLMs. In Proceedings of the 5th International Conference on Natural Language Processing for Digital Humanities, pages 628646, Albuquerque, USA. Association for Computational Linguistics. Sonam Gupta, Yatin Nandwani, Asaf Yehudai, Dinesh Khandelwal, Dinesh Raghu, and Sachindra Joshi. 2025a. Selective self-to-supervised fine-tuning for generalization in large language models. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 62406249, Albuquerque, New Mexico. Association for Computational Linguistics. Taneesh Gupta, Shivam Shandilya, Xuchao Zhang, Rahul Madhavan, Supriyo Ghosh, Chetan Bansal, Huaxiu Yao, and Saravan Rajmohan. 2025b. CARMO: Dynamic criteria generation for conIn Findings of the text aware reward modelling. Association for Computational Linguistics: ACL 2025, pages 22022261, Vienna, Austria. Association for Computational Linguistics. Akio Hayakawa, Nouran Khallaf, Horacio Saggion, and Serge Sharoff. 2025. UoL-UPF at TSAR 2025 shared task generate-and-select approach for readabilitycontrolled text simplification. In Proceedings of the Fourth Workshop on Text Simplification, Accessibility and Readability (TSAR 2025), pages 193210, Suzhou, China. Association for Computational Linguistics. Chenyuan He, Yuxiang Jia, Fei Gao, Senbin Zhu, Hongde Liu, Hongying Zan, and Min Peng. 2025a. Task-aware contrastive mixture of experts for quadruple extraction in conversations with code-like replies and non-opinion detection. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 41444159, Suzhou, China. Association for Computational Linguistics. Zoe Wanying He, Sean Trott, and Meenakshi Khosla. 2025b. Seeing through words, speaking through pixels: Deep representational alignment between vision and language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3564535660, Suzhou, China. Association for Computational Linguistics. Rushikesh Hiray and Venelin Kovatchev. 2025. Storybranch - generating multimedia content from novels. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (System Demonstrations), pages 485 493, Albuquerque, New Mexico. Association for Computational Linguistics. Sonja Hohloch. 2024. Homoclinic floer homology via direct limits. Preprint, arXiv:2402.12345. Faye Holt, William Held, and Diyi Yang. 2024. Perceptions of language technology failures from South Asian English speakers. In Findings of the Association for Computational Linguistics: ACL 2024, pages 40674081, Bangkok, Thailand. Association for Computational Linguistics. Eftekhar Hossain, Sanjeev Kumar Sinha, Naman Bansal, R. Alexander Knipper, Souvika Sarkar, John Salvador, Yash Mahajan, Sri Ram Pavan Kumar Guttikonda, Mousumi Akter, Md. Mahadi Hassan, Matthew Freestone, Matthew C. Williams Jr., Dongji Feng, and Santu Karmaker. 2025. 16 LLMs as meta-reviewers assistants: case study. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 7763 7803, Albuquerque, New Mexico. Association for Computational Linguistics. Peng Hu, Sizhe Liu, Changjiang Gao, Xin Huang, Xue Han, Junlan Feng, Chao Deng, and ShuLarge language models jian Huang. 2025. are cross-lingual knowledge-free reasoners. In Proceedings of the 2025 Conference of the Nations of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1525 1542, Albuquerque, New Mexico. Association for Computational Linguistics. the Americas Chapter of Yuxuan Huang, Yihang Chen, Haozheng Zhang, Kang Li, Huichi Zhou, Meng Fang, Linyi Yang, Xiaoguang Li, Lifeng Shang, Songcen Xu, Jianye Hao, Kun Shao, and Jun Wang. 2025. Deep research agents: systematic examination and roadmap. Preprint, arXiv:2506.18096. Ziyi Huang and Xia Cui. 2025. PromotionGo at SemEval-2025 task 11: feature-centric framework for cross-lingual multi-emotion detection in short texts. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 21402148, Vienna, Austria. Association for Computational Linguistics. Angel Hsing-Chi Hwang, Q. Vera Liao, Su Lin Blodgett, Alexandra Olteanu, and Adam Trischler. 2025. it was 80% me, 20% ai: Seeking authenticity in co-writing with large language models. Proc. ACM Hum.-Comput. Interact., 9(2). Ahmad Ibrahim Ismail, Bashirudeen Opeyemi Ibrahim, Olubayo Adekanmbi, and Ife Adebara. 2025. Retrieval-augmented generation meets local languages for improved drug information access and comprehension. the Sixth Workshop on African Natural Language Processing (AfricaNLP 2025), pages 108114, Vienna, Austria. Association for Computational Linguistics."
        },
        {
            "title": "In Proceedings of",
            "content": "Raunak Jain and Srinivasan Rengarajan. 2025. Emergent wisdom at BEA 2025 shared task: From lexical understanding to reflective reasoning for pedagogical ability assessment. In Proceedings of the 20th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2025), pages 1108 1120, Vienna, Austria. Association for Computational Linguistics. Charlott Jakob, David Harbecke, Patrick Parschan, Pia Wenzel Neves, and Vera Schmitt. 2025. PolBiX: Detecting LLMs political bias in fact-checking through X-phemisms. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1719217210, Suzhou, China. Association for Computational Linguistics. Gunjan Jalori, Preetika Verma, and Sercan Arik. 2025. FLAIRR-TS - forecasting LLM-agents with iterative refinement and retrieval for time series. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1542715437, Suzhou, China. Association for Computational Linguistics. Dongsuk Jang, Ziyao Shangguan, Kyle Tegtmeyer, Anurag Gupta, Jan Czerminski, Sophie Chheang, and Arman Cohan. 2025. MedTutor: retrievalaugmented LLM system for case-based medical education. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 319353, Suzhou, China. Association for Computational Linguistics. Ala Jararweh, Oladimeji Macaulay, David Arredondo, Yue Hu, Luis Tafoya, Kushal Virupakshappa, and Avinash Sahu. 2025. Protein2Text: Resampling mechanism to translate protein sequences into human-interpretable text. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: Industry Track), pages 918937, Albuquerque, New Mexico. Association for Computational Linguistics. Sungho Jeon and Michael Strube. 2025. Entity tracking in small language models: An attention-based study of parameter-efficient fine-tuning. In Proceedings of the 6th Workshop on Computational Approaches to Discourse, Context and Document-Level Inferences (CODI 2025), pages 4253, Suzhou, China. Association for Computational Linguistics. Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Josef Dai, Boren Zheng, Tianyi Alex Qiu, Jiayi Zhou, Kaile Wang, Boxun Li, Sirui Han, PKUYike Guo, and Yaodong Yang. 2025. SafeRLHF: Towards multi-level safety alignment for LLMs with human preference. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3198332016, Vienna, Austria. Association for Computational Linguistics. Jiazhou Ji and Xinru Lu. 2025. ReFLAIR: Enhancing multimodal reasoning via structured reflection and reward-guided learning. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2540125413, Suzhou, China. Association for Computational Linguistics. Feiran Jia, Tong Wu, Xin Qin, and Anna Squicciarini. 2025a. The task shield: Enforcing task alignment to defend against indirect prompt injection in LLM agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2968029697, Vienna, Austria. Association for Computational Linguistics. Runsong Jia, Mengjia Wu, Ying Ding, Jie Lu, and Yi Zhang. 2025b. HetGCoT: Heterogeneous graphenhanced chain-of-thought LLM reasoning for acaIn Findings of the demic question answering. 17 Association for Computational Linguistics: EMNLP 2025, pages 1595015963, Suzhou, China. Association for Computational Linguistics. Jin Jiang, Jianing Wang, Yuchen Yan, Yang Liu, Jianhua Zhu, Mengdi Zhang, and Liangcai Gao. 2025. Do large language models excel in complex logical reasoning with formal language? In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1688916914, Suzhou, China. Association for Computational Linguistics. Lesheng Jin, Zhenyuan Ruan, Haohui Mai, and Jingbo Shang. 2025. VeriLocc: End-to-end crossIn architecture register allocation via LLM. Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3024030250, Suzhou, China. Association for Computational Linguistics. Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, and Jindong Wang. 2024. AgentReview: Exploring peer review dynamics with LLM agents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 12081226, Miami, Florida, USA. Association for Computational Linguistics. Aniket Joshi, Cyrus Andre DSouza, Sejal Jain, Jitenkumar Babubhai Rana, and Promod Yenigalla. 2025a. I-SEE: An instruction-tuned, SOP-enhanced quality evaluator for product content. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 13791388, Suzhou (China). Association for Computational Linguistics. Nisheeth Joshi, Palak Arora, Anju Krishnia, Riya Lonchenpa, and Mhasilenuo Vizo. 2025b. BVSLP: Machine translation using linguistic embellishments for IndicMT shared task 2025. In Proceedings of the Tenth Conference on Machine Translation, pages 12651270, Suzhou, China. Association for Computational Linguistics. 2025: Durairaj. Srihari K, Vijay Karthick Vaidyanathan, 2025. Thenmozhi and NLP_goats@DravidianLangTech Detecting fake news in Dravidian languages: text classification approach. In Proceedings of the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 345349, Acoma, The Albuquerque Convention Center, Albuquerque, New Mexico. Association for Computational Linguistics. Mukund K. Roy, Karunesh Arora, Praveen Kumar Chandaliya, Rohit Kumar, and Pruthwik Mishra. 2025. CDAC-SVNIT submission for IWSLT 2025 InIn Proceedings of the dic track shared task. 22nd International Conference on Spoken Language Translation (IWSLT 2025), pages 180185, Vienna, Austria (in-person and online). Association for Computational Linguistics. 18 Maithili Sanjay Kadam and Francis Ferraro. 2025. TAGEQA: Textandgraph for event question answering via structured prompting strategies. In Proceedings of the 14th Joint Conference on Lexical and Computational Semantics (*SEM 2025), pages 304315, Suzhou, China. Association for Computational Linguistics. Xiaoqiang Kang, Shengen Wu, Zimu Wang, Yilin Liu, Xiaobo Jin, Kaizhu Huang, Wei Wang, Yutao Yue, Xiaowei Huang, and Qiufeng Wang. 2025. Can GRPO boost complex multimodal table understanding? In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1264212655, Suzhou, China. Association for Computational Linguistics. Aly M. Kassem, Zhuan Shi, Negar Rostamzadeh, and Golnoosh Farnadi. 2025. REVIVING YOUR MNEME: Predicting the side effects of LLM unlearning and fine-tuning via sparse model diffing. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3223832251, Suzhou, China. Association for Computational Linguistics. Olga Kellert, Nemika Tyagi, Muhammad Imran, Nelvin Licona-Guevara, and Carlos Gómez-Rodríguez. 2025. Parsing the switch: LLM-based UD annotation for complex code-switched and low-resource languages. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1593415949, Suzhou, China. Association for Computational Linguistics. Ashinee Kesanam, Gummuluri Venkata Ravi Ram, Chaithanya Swaroop Banoth, and Rama Mohana Reddy. 2025. NITK-VITAL at SemEval-2025 task 11: Focal-RoBERTa: Addressing class imbalance in multi-label emotion classification. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 10771081, Vienna, Austria. Association for Computational Linguistics. Elham Khabiri, Jeffrey O. Kephart, Fenno F. Heath Iii, Srideepika Jayaraman, Yingjie Li, Fateh A. Tipu, Dhruv Shah, Achille Fokoue, and Anu Bhamidipaty. 2025. Declarative techniques for NL queries over heterogeneous data. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 1744 1761, Suzhou (China). Association for Computational Linguistics. Anant Khandelwal, Manish Gupta, and Puneet Agrawal. 2025. CoCoA: Confidenceand context-aware adaptive decoding for resolving knowledge conflicts in large language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 68466866, Suzhou, China. Association for Computational Linguistics. Maira Khatoon, Arooj Kiyani, Tehmina Farid, and Sadaf Abdul Rauf. 2025. FJWU_Squad at SemEval2025 task 1: An idiom visual understanding dataset In Proceedings of the 19th for idiom learning. International Workshop on Semantic Evaluation (SemEval-2025), pages 17591765, Vienna, Austria. Association for Computational Linguistics. Md Tawkat Islam Khondaker, Numaan Naeem, Fatimah Khan, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. 2024. Benchmarking LLaMA-3 on Arabic language generation tasks. In Proceedings of the Second Arabic Natural Language Processing Conference, pages 283297, Bangkok, Thailand. Association for Computational Linguistics. Ruslan Khrulev. 2025. CHECK-MAT: Probing the mathematical reasoning and rubric-alignment of vision-language models on handwritten solutions. In Proceedings of The 3rd Workshop on Mathematical Natural Language Processing (MathNLP 2025), pages 7794, Suzhou, China. Association for Computational Linguistics. Ahrii Kim. 2025. Context is ubiquitous, but rarely changes judgments: Revisiting document-level MT evaluation. In Proceedings of the Tenth Conference on Machine Translation, pages 8197, Suzhou, China. Association for Computational Linguistics. Juhyeong Kim, Sangyeon Yu, Gyunyeop Kim, and Sangwoo Kang. 2025f. FractalLLM: Lossless selfspeculative decoding with layer embedded selfcompression. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2366623673, Suzhou, China. Association for Computational Linguistics. Ðor de Klisura, Astrid Bernaga Torres, Anna Karen Gárate-Escamilla, Rajesh Roshan Biswal, Ke Yang, Hilal Pataci, and Anthony Rios. 2025. multi-agent framework for mitigating dialect biases in privacy policy question-answering systems. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3231832337, Vienna, Austria. Association for Computational Linguistics. Shaghayegh Kolli, Richard Rosenbaum, Timo Cavelius, Lasse Strothe, Andrii Lata, and Jana Diesner. 2025. Hybrid fact-checking that integrates knowledge graphs, large language models, and search-based retrieval agents improves interpretable claim verification. In Proceedings of the 9th Widening NLP Workshop, pages 106115, Suzhou, China. Association for Computational Linguistics. Dongjun Kim, Gyuho Shim, Yongchan Chun, Minhyuk Kim, Chanjun Park, and Heuiseok Lim. 2025a. Benchmark profiling: Mechanistic diagnosis of LLM benchmarks. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1564615661, Suzhou, China. Association for Computational Linguistics. Olga Kononykhina, Anna-Carolina Haensch, and Frauke Kreuter. 2025. Mind the gap: Genderbased differences in occupational embeddings. In Proceedings of the 6th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages 83 91, Vienna, Austria. Association for Computational Linguistics. Heehyeon Kim, Kyeongryul Lee, and Joyce Jiyoung Whang. 2025b. Beneath the facade: Probing safety vulnerabilities in LLMs via auto-generated In Findings of the Association jailbreak prompts. for Computational Linguistics: EMNLP 2025, pages 1766817700, Suzhou, China. Association for Computational Linguistics. Hyeongsik Kim, Yanheng Xu, Chaoqun Dong, and Fei Du. 2025c. Over-generation and compaction: prompting strategy for procedural text adaptation with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1930619337, Suzhou, China. Association for Computational Linguistics. Jaeho Kim, Yunseok Lee, and Seulki Lee. 2025d. Position: The AI conference peer review crisis demands author feedback and reviewer rewards. In Proceedings of the 42nd International Conference on Machine Learning, volume 267 of Proceedings of Machine Learning Research, pages 8163481651. PMLR. Jongwoo Kim, SeongYeub Chu, Bryan Wong, and Mun Yong Yi. 2025e. Not all options are created equal: Textual option weighting for token-efficient LLM-based knowledge tracing. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1611416128, Suzhou, China. Association for Computational Linguistics. Yevheniia Kryklyvets, Mohammed Irfan Kurpath, Sahal Shaji Mullappilly, Jinxing Zhou, Fahad Shahbaz Khan, Rao Muhammad Anwer, Salman Khan, and Hisham Cholakkal. 2025. MAviS: multimodal conversational assistant for avian species. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2860128627, Suzhou, China. Association for Computational Linguistics. Rohit Kumar and Chandan Nolbaria. 2025. Bridging the data gap in financial sentiment: LLM-driven augmentation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pages 12461254, Vienna, Austria. Association for Computational Linguistics. Sandeep Kumar, Samarth Garg, Sagnik Sengupta, Tirthankar Ghosal, and Asif Ekbal. 2025. MixRevDetect: Towards detecting AI-generated content in hybrid peer reviews. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 944953, Albuquerque, New Mexico. Association for Computational Linguistics. Sumit Kumar and Tanmay Basu. 2025. AdaBioBERT: Adaptive token sequence learning for biomediIn Proceedings cal named entity recognition. of the 24th Workshop on Biomedical Language Processing, pages 5662, Viena, Austria. Association for Computational Linguistics. Beong-woo Kwak, Minju Kim, Dongha Lim, Hyungjoo Chae, Dongjin Kang, Sunghwan Kim, Dongil Yang, and Jinyoung Yeo. 2025. ToolHaystack: Stresstesting tool-augmented language models in realistic long-term interactions. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2469624727, Suzhou, China. Association for Computational Linguistics. Roman Kyslyi, Yuliia Maksymiuk, and Ihor Pysmennyi. 2025a. Vuyko mistral: Adapting LLMs for low-resource dialectal translation. In Proceedings of the Fourth Ukrainian Natural Language Processing Workshop (UNLP 2025), pages 8695, Vienna, Austria (online). Association for Computational Linguistics. Roman Kyslyi, Nataliia Romanyshyn, and Volodymyr Sydorskyi. 2025b. The UNLP 2025 shared task on detecting social media manipulation. In Proceedings of the Fourth Ukrainian Natural Language Processing Workshop (UNLP 2025), pages 105111, Vienna, Austria (online). Association for Computational Linguistics. Ilya Lasy, Peter Knees, and Stefan Woltran. 2025. Understanding verbatim memorization in LLMs through circuit discovery. In Proceedings of the First Workshop on Large Language Model Memorization (L2M2), pages 8394, Vienna, Austria. Association for Computational Linguistics. Sunbowen Lee, Junting Zhou, Chang Ao, Kaige Li, Xeron Du, Sirui He, Haihong Wu, Tianci Liu, Jiaheng Liu, Hamid Alinejad-Rokny, Min Yang, Yitao Liang, Zhoufutu Wen, and Shiwen Ni. 2025. Quantification of large language model distillation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 49855004, Vienna, Austria. Association for Computational Linguistics. Haley Lepp and Daniel Scott Smith. 2025. you cannot sound like gpt\": Signs of language discrimination and resistance in computer science publishing. In Proceedings of the 2025 ACM Conference on Fairness, Accountability, and Transparency, FAccT 25, page 31623181. ACM. Vladimir I. Levenshtein. 1965. Binary codes capable of correcting deletions, insertions, and reversals. Soviet physics. Doklady, 10:707710. Amit LeVi, Rom Himelstein, Yaniv Nemcovsky, Avi Mendelson, and Chaim Baskin. 2025. Jailbreak attack initializations as extractors of compliance directions. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 66726705, Suzhou, China. Association for Computational Linguistics. Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir Patil, Matei Zaharia, Joseph E. Gonzalez, and Ion Stoica. 2025a. Language models can easily learn to reason from demonstrations. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1597915997, Suzhou, China. Association for Computational Linguistics. Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, Kai Shu, Lu Cheng, and Huan Liu. 2025b. From generation to judgment: Opportunities and challenges of LLM-as-a-judge. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 27572791, Suzhou, China. Association for Computational Linguistics. Jiazheng Li, Hanqi Yan, and Yulan He. 2025c. Drift: Enhancing LLM faithfulness in rationale generation via dual-reward probabilistic inference. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 68506866, Vienna, Austria. Association for Computational Linguistics. Matthew Li, Santiago Torres-Garcia, Shayan Halder, Phani Kuppa, Sean OBrien, Vasu Sharma, Kevin Zhu, and Sunishchal Dev. 2025d. FrontierScience bench: Evaluating AI research capabilities in LLMs. In Proceedings of the 1st Workshop for Research on Agent Language Models (REALM 2025), pages 428453, Vienna, Austria. Association for Computational Linguistics. Muyao Li, Zihao Wang, Kaichen He, Xiaojian Ma, and Yitao Liang. 2025e. JARVIS-VLA: Post-training large-scale vision language models to play visual games with keyboards and mouse. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1787817899, Vienna, Austria. Association for Computational Linguistics. Pingzhi Li, Prateek Yadav, Jaehong Yoon, Jie Peng, YiLin Sung, Mohit Bansal, and Tianlong Chen. 2025f. Glider: Global and local instruction-driven expert router. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 62516312, Suzhou, China. Association for Computational Linguistics. Rui Li, Heming Xia, Xinfeng Yuan, Qingxiu Dong, Lei Sha, Wenjie Li, and Zhifang Sui. 2025g. How far are LLMs from being our digital twins? benchmark for persona-based behavior chain simulation. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1573815763, Vienna, Austria. Association for Computational Linguistics. Runchao Li, Yao Fu, Mu Sheng, Xianxuan Long, Haotian Yu, and Pan Li. 2025h. FAEDKV: Infinitewindow Fourier transform for unbiased KV cache In Findings of the Association for compression. Computational Linguistics: EMNLP 2025, pages 1685616866, Suzhou, China. Association for Computational Linguistics. Shenglan Li, Jia Xu, and Mengjiao Zhang. 2025i. EmByte: Decomposition and compression learning for small yet private NLP. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 71827201, Suzhou, China. Association for Computational Linguistics. Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, and Cho-Jui Hsieh. 2024. DrAttack: Prompt decomposition and reconstruction makes powerful LLMs In Findings of the Association for jailbreakers. Computational Linguistics: EMNLP 2024, pages 1389113913, Miami, Florida, USA. Association for Computational Linguistics. Zehan Li, Ruhua Pan, and Xinyu Pi. 2025j. Beyond LLMs linguistic approach to causal graph generation from narrative texts. In Proceedings of the The 7th Workshop on Narrative Understanding, pages 36 51, Albuquerque, New Mexico. Association for Computational Linguistics. Zichao Li. 2025a. Formula-text cross-retrieval: benchmarking study of dense embedding methods for mathematical information retrieval. In Proceedings of The 3rd Workshop on Mathematical Natural Language Processing (MathNLP 2025), pages 124 133, Suzhou, China. Association for Computational Linguistics. Zichao Li. 2025b. Knowledge-grounded detection of cryptocurrency scams with retrieval-augmented LMs. the 3rd Workshop on Towards Knowledgeable Foundation Models (KnowFM), pages 4048, Vienna, Austria. Association for Computational Linguistics."
        },
        {
            "title": "In Proceedings of",
            "content": "Zichao Li. 2025c. Retrieval-augmented forecasting with tabular time series data. In Proceedings of the 4th Table Representation Learning Workshop, pages 192199, Vienna, Austria. Association for Computational Linguistics. Zichao Li and Zong Ke. 2025a. Cross-modal augmentation for low-resource language understanding and generation. In Proceedings of the 1st Workshop on Multimodal Augmented Generation via Multimodal Retrieval (MAGMaR 2025), pages 9099, Vienna, Austria. Association for Computational Linguistics. Zichao Li and Zong Ke. 2025b. Domain meets typology: Predicting verb-final order from Universal Dependencies for financial and blockchain In Proceedings of the 7th Workshop on NLP. Research in Computational Linguistic Typology and Multilingual NLP, pages 156164, Vienna, Austria. Association for Computational Linguistics. Zichao Li, Zong Ke, and Puning Zhao. 2025k. Injecting structured knowledge into LLMs via graph neural networks. In Proceedings of the 1st Joint Workshop on Large Language Models and Structure Modeling (XLLM 2025), pages 1625, Vienna, Austria. Association for Computational Linguistics. Emmy Liu, Amanda Bertsch, Lintang Sutawika, Lindia Tjuatja, Patrick Fernandes, Lara Marinov, Michael Chen, Shreya Singhal, Carolin Lawrence, Aditi Raghunathan, Kiril Gashteovski, and Graham Neubig. 2025a. Not-just-scaling laws: Towards better understanding of the downstream impact of language model design decisions. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1640716438, Suzhou, China. Association for Computational Linguistics. Jiarui Liu, Yueqi Song, Yunze Xiao, Mingqian Zheng, Lindia Tjuatja, Jana Schaich Borg, Mona T. Diab, and Maarten Sap. 2025b. Synthetic socratic debates: Examining persona effects on moral decision and persuasion dynamics. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1643916469, Suzhou, China. Association for Computational Linguistics. Quancai Liu, Haihui Fan, Jinchao Zhang, Xiangfang Li, Chuanrong Li, and Bo Li. 2025c. DisComp: two-stage prompt optimization framework combining task-agnostic and task-aware compression. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 10331044, Albuquerque, New Mexico. Association for Computational Linguistics. Siyi Liu and Dan Roth. 2025. Conflicts in texts: Data, In Findings of the implications and challenges. Association for Computational Linguistics: EMNLP 2025, pages 1007310091, Suzhou, China. Association for Computational Linguistics. Wanlong Liu, Junying Chen, Ke Ji, Li Zhou, Wenyu Chen, and Benyou Wang. 2025d. RAGinstruct: Boosting LLMs with diverse retrievalaugmented instructions. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 38653888, Suzhou, China. Association for Computational Linguistics. Yifan Liu, Qianfeng Wen, Mark Zhao, Jiazhou Liang, and Scott Sanner. 2025e. MA-DPR: Manifold-aware distance metrics for dense passage retrieval. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3107331091, Suzhou, China. Association for Computational Linguistics. Zhiwei Liu, Jielin Qiu, Shiyu Wang, Jianguo Zhang, Zuxin Liu, Roshan Ram, Haolin Chen, Weiran Yao, Shelby Heinecke, Silvio Savarese, Huan Wang, and Caiming Xiong. 2025f. MCPEval: Automatic MCPbased deep evaluation for AI agent models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 373402, Suzhou, China. Association for Computational Linguistics. Zhixiang Lu, Mian Zhou, Angelos Stefanidis, and Jionglong Su. 2025. UniMath-CoT: unified 21 framework for multimodal mathematical reasoning with re-inference affirmation. In Proceedings of The 3rd Workshop on Mathematical Natural Language Processing (MathNLP 2025), pages 176 185, Suzhou, China. Association for Computational Linguistics. Hanjun Luo, Yingbin Jin, Yiran Wang, Xinfeng Li, Tong Shang, Xuecheng Liu, Ruizhe Chen, Kun Wang, Hanan Salam, Qingsong Wen, and Zuozhu Liu. 2025a. DynamicNER: dynamic, multilingual, and fine-grained dataset for LLM-based named entity recognition. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1652216546, Suzhou, China. Association for Computational Linguistics. Xiangfeng Luo, Ruoxin Zheng, Jianqiang Huang, and Hang Yu. 2025b. HDiff: Confidence-guided denoising diffusion for robust hyper-relational link prediction. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 74177434, Suzhou, China. Association for Computational Linguistics. Lijia Lv, Yuanshu Zhao, Guan Wang, Xuehai Tang, Wen Jie, Jizhong Han, and Songlin Hu. 2025. Gammaguard: Lightweight residual adapters for robust guardrails in large language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1223012242, Suzhou, China. Association for Computational Linguistics. Jabez Magomere, Elena Kochkina, Samuel Mensah, Simerjot Kaur, and Charese Smiley. 2025. FinNLI: Novel dataset for multi-genre financial natural language inference benchmarking. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 45454568, Albuquerque, New Mexico. Association for Computational Linguistics. Arijit Maji, Raghvendra Kumar, Akash Ghosh, Anushka, and Sriparna Saha. 2025. SANSKRITI: comprehensive benchmark for evaluating language models knowledge of Indian culture. In Findings of the Association for Computational Linguistics: ACL 2025, pages 44344451, Vienna, Austria. Association for Computational Linguistics. Vijit Malik, Akshay Jagatap, Vinayak Puranik, and Anirban Majumder. 2024. PEARL: Preference extraction with exemplar augmentation and retrieval with LLM agents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 1536 1547, Miami, Florida, US. Association for Computational Linguistics. Stasa Mandic, Georg Niess, and Roman Kern. 2025. From in-distribution to out-of-distribution: Joint loss for improving generalization in software mention and relation extraction. In Proceedings of the Fifth Workshop on Scholarly Document Processing (SDP 2025), pages 146153, Vienna, Austria. Association for Computational Linguistics. Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. 2008. Introduction to Information Retrieval. Cambridge University Press, Cambridge, UK. Utsav Maskey, Chencheng Zhu, and Usman Naseem. 2025. Benchmarking large language models for cryptanalysis and side-channel vulnerabilities. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1984919865, Suzhou, China. Association for Computational Linguistics. Blake Matheny, Phuong Nguyen, and Minh Nguyen. 2025. JNLP at SemEval-2025 task 1: Multimodal idiomaticity representation with large language models. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 14791484, Vienna, Austria. Association for Computational Linguistics. Han Meng, Yancan Chen, Yunan Li, Yitian Yang, Jungup Lee, Renwen Zhang, and Yi-Chieh Lee. theory2025. What is stigma attributed to? grounded, expert-annotated interview corpus for demystifying mental-health stigma. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 54535490, Vienna, Austria. Association for Computational Linguistics. Saif M. Mohammad. 2025. Words of warmth: Trust and sociability norms for over 26k English words. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1883018850, Vienna, Austria. Association for Computational Linguistics. Seyedali Mohammadi, Bhaskara Hanuma Vedula, Hemank Lamba, Edward Raff, Ponnurangam Kumaraguru, Francis Ferraro, and Manas Gaur. 2025. Do LLMs adhere to label definitions? examining In their receptivity to external label definitions. Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3236832381, Suzhou, China. Association for Computational Linguistics. Joydeb Mondal and Pramir Sarkar. 2025. Modgenix at SemEval-2025 task 1: Context aware vision language ranking (CAViLR) for multimodal idiomaticity understanding. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 780784, Vienna, Austria. Association for Computational Linguistics. Hoyeon Moon, Byeolhee Kim, and Nikhil Verma. 2025. Quality-aware translation tagging in multilingual RAG system. In Proceedings of the 5th Workshop on Multilingual Representation Learning (MRL 2025), pages 161177, Suzhuo, China. Association for Computational Linguistics. Guy Mor-Lan, Naama Rivlin-Angert, Yael R. Kaplan, Tamir Sheafer, and Shaul R. Shenhav. 2025. HebID: 22 Detecting social identities in Hebrew-language political text. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 98509870, Suzhou, China. Association for Computational Linguistics. Andreas Mueller. 2023. Wordcloud. Gagan Mundada, Yash Vishe, Amit Namburi, Xin Xu, Zachary Novack, Julian McAuley, and Junda Wu. 2025. WildScore: Benchmarking MLLMs in-thewild symbolic music reasoning. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1685816874, Suzhou, China. Association for Computational Linguistics. Nandini Mundra, Aditya Nanda Kishore Khandavally, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, and Mitesh Khapra. 2024. An empirical comparison of vocabulary expansion and initialization approaches for language models. In Proceedings of the 28th Conference on Computational Natural Language Learning, pages 84104, Miami, FL, USA. Association for Computational Linguistics. Tafazzul Nadeem, Riyansha Singh, Suyamoon Pathak, and Ashutosh Modi. 2025. Exploration lab IITK at SemEval-2025 task 11: Bridging the gap in textbased emotion detection. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 18591865, Vienna, Austria. Association for Computational Linguistics. Mahmud Wasif Nafee, Maiqi Jiang, Haipeng Chen, and Yanfu Zhang. 2025. Dynamic retriever for in-context knowledge editing via policy optimizaIn Proceedings of the 2025 Conference on tion. Empirical Methods in Natural Language Processing, pages 1675516768, Suzhou, China. Association for Computational Linguistics. Shahriar Kabir Nahin, Rabindra Nath Nandi, Sagor Sarker, Quazi Sarwar Muhtaseem, Md Kowsher, Apu Chandraw Shill, Md Ibrahim, Mehadi Hasan Menon, Tareq Al Muntasir, and Firoj Alam. 2025. TituLLMs: family of Bangla LLMs with comIn Findings of the prehensive benchmarking. Association for Computational Linguistics: ACL 2025, pages 2492224940, Vienna, Austria. Association for Computational Linguistics. Kamel Nebhi, Amrita Panesar, and Hans Bantilan. 2025. End-to-end automated item generation and scoring for adaptive English writing assessment with large language models. In Proceedings of the 20th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2025), pages 968 977, Vienna, Austria. Association for Computational Linguistics. Bao Nguyen, Binh Nguyen, Duy Nguyen, and Viet Anh Nguyen. 2025a. Distributional surgery for language model activations. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 81928212, Suzhou, China. Association for Computational Linguistics. Viet Cuong Nguyen, Mohammad Taher, Dongwan Hong, Vinicius Konkolics Possobom, Vibha Thirunellayi Gopalakrishnan, Ekta Raj, Zihang Li, Heather J. Soled, Michael L. Birnbaum, Srijan Kumar, and Munmun De Choudhury. 2025b. Do large language models align with core mental health counseling competencies? In Findings of the Association for Computational Linguistics: NAACL 2025, pages 74887511, Albuquerque, New Mexico. Association for Computational Linguistics. Xuanfan Ni and Piji Li. 2024a. systematic evaluation of large language models for natural. ArXiv, abs/2405.10251. Note: Cited from Semantic Scholar. Xuanfan Ni and Piji Li. 2024b. systematic evaluation of large language models for natural language generation tasks. Preprint, arXiv:2405.10251. Note: Cited from ArXiv. Natapong Nitarach, Warit Sirichotedumrong, Panop Pitchayarthorn, Pittawat Taveekitworachai, Potsawee Manakul, and Kunat Pipatanakul. 2025. FinCoT: Grounding chain-of-thought in expert financial reasoning. In Proceedings of The 10th Workshop on Financial Technology and Natural Language Processing, pages 93123, Suzhou, China. Association for Computational Linguistics. Syeda Alisha Noor, Sadia Anjum, Syed Ahmad Reza, and Md Rashadur Rahman. 2025. Celestia@DravidianLangTech 2025: Malayalam-BERT and m-BERT based transformer models for fake news detection in Dravidian languages. In Proceedings of the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 688693, Acoma, The Albuquerque Convention Center, Albuquerque, New Mexico. Association for Computational Linguistics. Jean De Dieu Nyandwi, Yueqi Song, Simran Khanuja, and Graham Neubig. 2025. Grounding multilingual multimodal LLMs with cultural knowledge. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2419824242, Suzhou, China. Association for Computational Linguistics. Samarth and Sanjay Balaji Mahalingam. 2025. The gemma sutras: Fine-tuning gemma 3 for Sanskrit sandhi splitting. In Proceedings of the 9th Widening NLP Workshop, pages 235241, Suzhou, China. Association for Computational Linguistics. Manas Pandya, Avinash Kumar Sharma, and Arpit Shukla. 2025. Swahili news classification: Performance, challenges, and explainability across ML, DL, and transformers. In Proceedings of the Sixth Workshop on African Natural Language Processing (AfricaNLP 2025), pages 203209, Vienna, Austria. Association for Computational Linguistics. 23 Lei Pang, Hanyi Mao, Quanjia Xiao, Chen Ruihan, Jingjun Zhang, Haixiao Liu, and Xiangyi Li. 2025. In2X at WMT25 translation task. In Proceedings of the Tenth Conference on Machine Translation, pages 671679, Suzhou, China. Association for Computational Linguistics. Angelina Parfenova and Jürgen Pfeffer. 2025. Measuring what matters: Evaluating ensemble LLMs with label refinement in inductive coding. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1080310816, Vienna, Austria. Association for Computational Linguistics. Seongwan Park, Taeklim Kim, and Youngjoong Ko. 2025. Decoding dense embeddings: Sparse autoencoders for interpreting and discretizing dense retrieval. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2647926496, Suzhou, China. Association for Computational Linguistics. Priyaranjan Pattnayak, Amit Agarwal, Hansa Meghwani, Hitesh Laxmichand Patel, and Srikant Panda. 2025. Hybrid AI for responsive multi-turn online conversations with novel dynamic routing and feedback adaptation. In Proceedings of the 4th International Workshop on Knowledge-Augmented Methods for Natural Language Processing, pages 215229, Albuquerque, New Mexico, USA. Association for Computational Linguistics. Nguyen Pham Hoang Le, An Dinh Thien, Son T. Luu, and Kiet Van Nguyen. 2025. DocIE@XLLM25: ZeroSemble - robust and efficient zero-shot document information extraction with heterogeneous large language model ensembles. In Proceedings of the 1st Joint Workshop on Large Language Models and Structure Modeling (XLLM 2025), pages 288297, Vienna, Austria. Association for Computational Linguistics. Pakawat Phasook, Rapepong Pitijaroonpong, Jiramet Kinchagawat, Amrest Chinkamol, Tossaporn Saengja, Kiartnarin Udomlapsakul, Jitkapat Sawatphol, and Piyalitt Ittichaiwong. 2025. VeReaFine: Iterative verification reasoning refinement RAG for hallucination-resistant on open-ended clinical QA. In Proceedings of the 24th Workshop on Biomedical Language Processing (Shared Tasks), pages 281 288, Vienna, Austria. Association for Computational Linguistics. Emiliana Pulido, Robert Pugh, and Zoey Liu. 2025. speak for the árboles: Developing dependency treebank for Spanish L2 and heritage speakers. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pages 814822, Vienna, Austria. Association for Computational Linguistics. Saif Punjwani and Larry Heck. 2025. Weight-ofthought reasoning: Exploring neural network weights for enhanced LLM reasoning. In Proceedings of the 1st Workshop for Research on Agent Language Models (REALM 2025), pages 471485, Vienna, Austria. Association for Computational Linguistics. Junlang Qian, Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Zepeng Zhai, and Kezhi Mao. 2025. Beyond the next token: Towards prompt-robust zeroshot classification via efficient multi-token prediction. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 7093 7115, Albuquerque, New Mexico. Association for Computational Linguistics. Neel Prabhanjan Rachamalla, Aravind Konakalla, Gautam Rajeev, Ashish Kulkarni, Chandra Khatri, and Shubham Agarwal. 2025. Pragyaan: Designing and curating high-quality cultural post-training datasets for Indian languages. In Proceedings of the 5th Workshop on Multilingual Representation Learning (MRL 2025), pages 285321, Suzhuo, China. Association for Computational Linguistics. Javad Rafiei Asl, Sidhant Narula, Mohammad Ghasemigol, Eduardo Blanco, and Daniel Takabi. 2025. NEXUS: Network exploration for eXploiting unsafe sequences in multi-turn LLM jailbreaks. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2427824306, Suzhou, China. Association for Computational Linguistics. Rafiuddin and Muntaha Nujat Khan. 2025. Learning what to remember: Adaptive probabilislantic memory retention for memory-efficient guage models. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 39693981, Suzhou, China. Association for Computational Linguistics. Renjie Pi, Haoping Bai, Qibin Chen, Xiaoming Simon Wang, Jiulong Shan, Xiaojiang Liu, and Meng Cao. 2025. MR. judge: Multimodal reasoner as In Proceedings of the 2025 Conference on judge. Empirical Methods in Natural Language Processing, pages 2019220216, Suzhou, China. Association for Computational Linguistics. Shahid Iqbal Rai, Danilo Croce, and Roberto Basili. Injecting frame semantics into large lan2025. guage models via prompt-based fine-tuning. In Proceedings of the 14th Joint Conference on Lexical and Computational Semantics (*SEM 2025), pages 3147, Suzhou, China. Association for Computational Linguistics. Jason Priem, Heather Piwowar, and Richard Orr. 2022. Openalex: fully-open index of scholarly works, authors, venues, institutions, and concepts. Preprint, arXiv:2205.01833. Nishat Raihan, Joanna C. S. Santos, and Marcos Zampieri. 2025. MojoBench: Language modeling and benchmarks for mojo. In Findings of the Association for Computational Linguistics: NAACL 24 2025, pages 41094128, Albuquerque, New Mexico. Association for Computational Linguistics. Rahul Raja and Arpita Vats. 2025. Parallel corpora for machine translation in low-resource Indic languages: comprehensive review. In Proceedings of the Eighth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2025), pages 129143, Albuquerque, New Mexico, U.S.A. Association for Computational Linguistics. Alex Ray, Josh Achiam, and Dario Amodei. 2019. Benchmarking safe exploration in deep reinforcement learning. Saed Rezayi, Le An Ha, Yiyun Zhou, Andrew Houriet, Angelo DAddario, Peter Baldwin, Polina Harik, Ann King, and Victoria Yaneva. 2025. Automated scoring of communication skills in physician-patient interaction: Balancing performance and scalability. In Proceedings of the 20th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2025), pages 891897, Vienna, Austria. Association for Computational Linguistics. Naama Rivlin-Angert and Guy Mor-Lan. 2025. The enemy from within: study of political delegitimization discourse in israeli political speech. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1664516658, Suzhou, China. Association for Computational Linguistics. Keonwoo Roh, Yeong-Joon Ju, and Seong-Whan Lee. 2025. XLQA: benchmark for locale-aware multilingual open-domain question answering. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2879728809, Suzhou, China. Association for Computational Linguistics. Pablo Romero, Zihao Li, and Matthew Shardlow. 2025. Efficient on-device text simplification for firefox In Proceedings with synthetic data fine-tuning. of the Fourth Workshop on Text Simplification, Accessibility and Readability (TSAR 2025), pages 105115, Suzhou, China. Association for Computational Linguistics. Kalaivani S, Sanjay R, Thissyakkanna M, and Nirenjhanram K. 2025a. KSK@DravidianLangTech 2025: Political multiclass sentiment analysis of Tamil (Twitter) comments using incremental learning. In Proceedings of the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 221225, Acoma, The Albuquerque Convention Center, Albuquerque, New Mexico. Association for Computational Linguistics. Vishal S, Rajalakshmi Sivanaiah, and Angel Deborah S. 2025b. TechSSN3 at SemEval-2025 task 11: Multi-label emotion detection using ensemble transformer models and lexical rules. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 186192, Vienna, Austria. Association for Computational Linguistics. Jon Saad-Falcon, Rajan Pathe Vivek, William Berrios, Nandita Shankar Naik, Matija Franklin, Bertie Vidgen, Amanpreet Singh, Douwe Kiela, and Shikib Mehri. 2025. LMUNIT: Fine-grained evaluation with natural language unit tests. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 33033324, Suzhou, China. Association for Computational Linguistics. Syeda Jannatus Saba and Steven Skiena. 2025. Evaluating language translation models by playing telephone. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1033210347, Suzhou, China. Association for Computational Linguistics. Mushtari Sadia, Zhenning Yang, Yunming Xiao, Ang Chen, and Amrita Roy Chowdhury. 2025. SQUiD: Synthesizing relational databases from unstructured In Proceedings of the 2025 Conference on text. Empirical Methods in Natural Language Processing, pages 3197532000, Suzhou, China. Association for Computational Linguistics. Muhammed Saeed, Shaina Raza, Ashmal Vayani, Muhammad Abdul-Mageed, Ali Emami, and Shady Shehata. 2025. Beyond content: How grammatical gender shapes visual representation in text-toIn Findings of the Association for image models. Computational Linguistics: EMNLP 2025, pages 2467324695, Suzhou, China. Association for Computational Linguistics. Furkan Sahinuç, Ilia Kuznetsov, Yufang Hou, and Iryna Gurevych. 2024a. Systematic task exploration with LLMs: study in citation text generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 48324855, Bangkok, Thailand. Association for Computational Linguistics. Furkan Sahinuç, Thy Thy Tran, Yulia Grishina, Yufang Hou, Bei Chen, and Iryna Gurevych. 2024b. Efficient performance tracking: Leveraging large language models for automated construction of scientific leaderboards. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 79637977, Miami, Florida, USA. Association for Computational Linguistics. Jonathan Sakunkoo and Annabella Sakunkoo. 2025. Lost and found: Computational quality assurance of crowdsourced knowledge on morphological defectivity in Wiktionary. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pages 9981003, Vienna, Austria. Association for Computational Linguistics. Msvpj Sathvik, Raj Sonani, and Ravi Teja Potla. 2025a. Detection of religious hate speech during elections in Karnataka. In Proceedings of the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 562566, Acoma, The 25 Albuquerque Convention Center, Albuquerque, New Mexico. Association for Computational Linguistics. Sai Sathvik, Muralidhar Palli, Keerthana NNL, Balasubramanian Palani, Jobin Jose, and Siranjeevi Rajamanickam. 2025b. Team-risers@DravidianLangTech 2025: AI-generated product review detection in Dravidian languages using transformer-based embeddings. In Proceedings of the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 3337, Acoma, The Albuquerque Convention Center, Albuquerque, New Mexico. Association for Computational Linguistics. Kevin Scaria, Himanshu Gupta, Siddharth Goyal, Saurabh Sawant, Swaroop Mishra, and Chitta Baral. 2024. InstructABSA: Instruction learning for aspect based sentiment analysis. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 720736, Mexico City, Mexico. Association for Computational Linguistics. Miriam Schirmer, Angelina Voggenreiter, Juergen Pfeffer, and Agnes Horvat. 2025. Detecting child objectification on social media: Challenges in language modeling. In Proceedings of the The 9th Workshop on Online Abuse and Harms (WOAH), pages 396 412, Vienna, Austria. Association for Computational Linguistics."
        },
        {
            "title": "Aishwarya",
            "content": "2025. 2025: Selvamurugan. DravLinMultimodal gua@DravidianLangTech hate speech detection in Dravidian languages using late fusion of muril and Wav2Vec models. In Proceedings of the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 694699, Acoma, The Albuquerque Convention Center, Albuquerque, New Mexico. Association for Computational Linguistics. Uzair Shah, Md. Rafiul Biswas, Marco Agus, Mowafa Househ, and Wajdi Zaghouani. 2024. MemeMind at ArAIEval shared task: Generative augmentation and feature fusion for multimodal propaganda detection in Arabic memes through advanced language In Proceedings of the Second and vision models. Arabic Natural Language Processing Conference, pages 467472, Bangkok, Thailand. Association for Computational Linguistics. Jianshu She, Wenhao Zheng, Zhengzhong Liu, Hongyi Wang, Eric P. Xing, Huaxiu Yao, and Qirong Ho. 2025. Token level routing inference system for edge devices. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 159 166, Vienna, Austria. Association for Computational Linguistics. Rajvee Sheth, Shubh Nisar, Heenaben Prajapati, Himanshu Beniwal, and Mayank Singh. 2024. Commentator: code-mixed multilingual text annotation framework. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 101109, Miami, Florida, USA. Association for Computational Linguistics. Zhengliang Shi, Yiqun Chen, Haitao Li, Weiwei Sun, Shiyu Ni, Yougang Lyu, Run-Ze Fan, Bowen Jin, Yixuan Weng, Minjun Zhu, Qiujie Xie, Xinyu Guo, Qu Yang, Jiayi Wu, Jujia Zhao, Xiaqiang Tang, Xinbei Ma, Cunxiang Wang, Jiaxin Mao, and 7 others. 2025. Deep research: systematic survey. Preprint, arXiv:2512.02038. Ibne Farabi Shihab, Sanjeda Akter, and Anuj Sharma. 2025. Efficient unstructured pruning of mamba state-space models for resource-constrained environments. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1110911137, Suzhou, China. Association for Computational Linguistics. Siddharth Shukla, Tanuj Tyagi, Abhay Singh Bisht, Ashish Sharma, and Basant Agarwal. 2025. NyayGraph: knowledge graph enhanced approach for legal statute identification in Indian law using large language models. In Proceedings of the Natural Legal Language Processing Workshop 2025, pages 147 156, Suzhou, China. Association for Computational Linguistics. Shikhhar Siingh, Abhinav Rawat, Chitta Baral, and Vivek Gupta. 2025. GETReason: Enhancing image context extraction through hierarchical multiagent reasoning. In Proceedings of the 63rd Annual the Association for Computational Meeting of Linguistics (Volume 1: Long Papers), pages 29779 29800, Vienna, Austria. Association for Computational Linguistics. Naim Shandi, Jason M. Merlo, and Jeffrey A. Nanzer. Decentralized picosecond synchronizaPreprint, 2024. tion for distributed wireless systems. arXiv:2405.18384. Chenyang Shao, Dehao Huang, Yu Li, Keyu Zhao, Weiquan Lin, Yining Zhang, Qingbin Zeng, Zhiyu Chen, Tianxing Li, Yifei Huang, Taozhong Wu, Xinyang Liu, Ruotong Zhao, Mengsheng Zhao, Jiaoyang Li, Xuhua Zhang, Yue Wang, Yuanyi Zhen, Fengli Xu, and 2 others. 2025. Omniscientist: Toward co-evolving ecosystem of human and ai scientists. Preprint, arXiv:2511.16931. Divyansh Singh, Brodie Mather, Demi Zhang, Patrick Lehman, Justin Ho, and Bonnie Dorr. 2025. FIDELITY: Fine-grained interpretable distillation for effective language insights and topic yielding. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 24602472, Albuquerque, New Mexico. Association for Computational Linguistics. Charlotte Siska, Katerina Marazopoulou, Melissa Ailem, and James Bono. 2024. Examining the robustness of LLM evaluation to the distributional assumptions of benchmarks. In Proceedings 26 of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1040610421, Bangkok, Thailand. Association for Computational Linguistics."
        },
        {
            "title": "Leveraging LLMs",
            "content": "Konstantinos Skianis, A. Seza Dogruöz, and John Pavlopoulos. 2024. for translating and classifying mental health data. In Proceedings of the Fourth Workshop on Multilingual Representation Learning (MRL 2024), pages 236241, Miami, Florida, USA. Association for Computational Linguistics. Yejin Son, Saejin Kim, Dongjun Min, and Youngjae Yu. 2025. Multimodal UNcommonsense: From odd to ordinary and ordinary to odd. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1758617609, Suzhou, China. Association for Computational Linguistics. Woomin Song, Saket Dingliwal, Sai Muralidhar Jayanthi, Bhavana Ganesh, Jinwoo Shin, Aram Galstyan, and Sravan Babu Bodapati. 2025a. Accelerated test-time scaling with model-free speculative sampling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3061130624, Suzhou, China. Association for Computational Linguistics. Xiaoying Song, Sharon Lisseth Perez, Xinchen Yu, Eduardo Blanco, and Lingzi Hong. 2025b. Echoes of discord: Forecasting hater reactions to counterspeech. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 48924905, Albuquerque, New Mexico. Association for Computational Linguistics. Shashank Sonkar, Naiming Liu, and Richard Baraniuk. 2024. Student data paradox and curious case of single student-tutor model: Regressive side effects of training LLMs for personalized learning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1554315553, Miami, Florida, USA. Association for Computational Linguistics. Mohsen Sorkhpour, Abbas Yazdinejad, and Ali Dehghantanha. 2025. RedHit: Adaptive red-teaming of large language models via search, reasoning, and preference optimization. In Proceedings of the The First Workshop on LLM Security (LLMSEC), pages 716, Vienna, Austria. Association for Computational Linguistics. Alexander Spangher, Tenghao Huang, Yiqin Huang, Lucas Spangher, Sewon Min, and Mark Dredze. 2025a. novel multi-document retrieval benchmark: Journalist source-selection in newswriting. In Proceedings of the 4th International Workshop on Knowledge-Augmented Methods for Natural Language Processing, pages 180204, Albuquerque, New Mexico, USA. Association for Computational Linguistics. Alexander Spangher, Michael Lu, Sriya Kalyan, Hyundong Justin Cho, Tenghao Huang, Weiyan Shi, and Jonathan May. 2025b. NewsInterview: dataset and playground to evaluate LLMs grounding gap via informational interviews. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3289532925, Vienna, Austria. Association for Computational Linguistics. Shashank Srivastava. 2025. Large language models threaten languages epistemic and communicative foundations. the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2865028664, Suzhou, China. Association for Computational Linguistics."
        },
        {
            "title": "In Proceedings of",
            "content": "Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, Johannes Heidecke, Amelia Glaese, and Tejal Patwardhan. 2025. PaperBench: Evaluating AI ability to replicate AI research. In Proceedings of the 42nd International Conference on Machine Learning, volume 267 of Proceedings of Machine Learning Research, pages 5684356873. PMLR. Jie Sun, Junkang Wu, Jiancan Wu, Zhibo Zhu, Xingyu Lu, Jun Zhou, Lintao Ma, and Xiang Wang. 2025. Robust preference optimization via dynamic tarIn Findings of the Association for get margins. Computational Linguistics: ACL 2025, pages 5399 5416, Vienna, Austria. Association for Computational Linguistics. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2024. Scieval: multi-level large language model evaluation benchmark for scientific research. Proceedings of the AAAI Conference on Artificial Intelligence, 38(17):1905319061. Gokul Swamy, Aman Gulati, Srinivas Virinchi, and Anoop Saladi. 2025. An address intelligence frameIn Proceedings work for E-commerce deliveries. of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 10261034, Suzhou (China). Association for Computational Linguistics. Jiabin Tang, Lianghao Xia, Zhonghang Li, and Chao Huang. 2025a. Ai-researcher: Autonomous scientific innovation. Preprint, arXiv:2505.18705. Zuojin Tang, Bin Hu, Chenyang Zhao, De Ma, Gang Pan, and Bin Liu. 2025b. VLASCD: visual language action model for simultaneous chatting and decision making. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 92239243, Suzhou, China. Association for Computational Linguistics. Mehrab Tanjim, Yeonjun In, Xiang Chen, Victor Bursztyn, Ryan A. Rossi, Sungchul Kim, Guang-Jie Ren, Vaishnavi Muppala, Shun Jiang, Yongsung Kim, and Chanyoung Park. 2025. Disambiguation in conversational question answering in the era of 27 LLMs and agents: survey. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 95489561, Suzhou, China. Association for Computational Linguistics. Chong Tian, Qirong Ho, and Xiuying Chen. 2025. learning framework for symbolic adversarial evolving fake news generation and detection. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1230712321, Suzhou, China. Association for Computational Linguistics. Manyana Tiwari. 2025. Extended abstract: Probingguided parameter-efficient fine-tuning for balancing linguistic adaptation and safety in LLM-based social influence systems. In Proceedings of the Third Workshop on Social Influence in Conversations (SICon 2025), pages 145147, Vienna, Austria. Association for Computational Linguistics. MeiHan Tong and Shuai Wang. 2025. NovelCR: large-scale bilingual dataset tailored for longspan coreference resolution. In Findings of the Association for Computational Linguistics: ACL 2025, pages 51615173, Vienna, Austria. Association for Computational Linguistics. Octavian Alexandru Trifan, Jason Lee Weber, Marc Titus Trifan, Alexandru Nicolau, and Alexander Veidenbaum. 2025. Grammar pruning: Enabling lowlatency zero-shot task-oriented language models for edge AI. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1695516968, Suzhou, China. Association for Computational Linguistics. 2025:"
        },
        {
            "title": "Srihari",
            "content": "Durairaj."
        },
        {
            "title": "Thenmozhi",
            "content": "Vaidyanathan,"
        },
        {
            "title": "Karthick\nand",
            "content": "V Vijay 2025a. K, NLP_goats@DravidianLangTech Towards safer social media: Detecting abusive language directed at women in Dravidian languages. In Proceedings of the Fifth Workshop on Speech, Vision, and Language Technologies for Dravidian Languages, pages 350354, Acoma, The Albuquerque Convention Center, Albuquerque, New Mexico. Association for Computational Linguistics. Vijay Karthick Vaidyanathan, Srihari K, Mugilkrishna U, and Saritha Madhavan. 2025b. NLP_goats at SemEval-2025 task 11: Multi-label emotion classification using fine-tuned roberta-large tranformer. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 1023 1027, Vienna, Austria. Association for Computational Linguistics. Abhiram Vinjamuri and Weiwei Sun. 2025. Transfer learning for dependency parsing of Vedic Sanskrit. In Proceedings of the 9th Widening NLP Workshop, pages 5055, Suzhou, China. Association for Computational Linguistics. Jan Philip Wahle, Terry Ruas, Yang Xu, and Bela Gipp. 2024. Paraphrase types elicit prompt engineering capabilities. In Proceedings of the 2024 Conference on 28 Empirical Methods in Natural Language Processing, pages 1100411033, Miami, Florida, USA. Association for Computational Linguistics. Neng Wan, Sicong Huang, Esha Ubale, and Ian Lane. 2025. UCSC at SemEval-2025 task 8: Question answering over tabular data. In Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025), pages 20502058, Vienna, Austria. Association for Computational Linguistics. Benlu Wang, Iris Xia, Yifan Zhang, Junda Wang, Feiyun Ouyang, Shuo Han, Arman Cohan, Hong Yu, and Zonghai Yao. 2025a. From scores to steps: Diagnosing and improving LLM performance in evidencebased medical calculations. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1082010844, Suzhou, China. Association for Computational Linguistics. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, Bo Zhang, Liqun Wei, Zhihao Sui, Wei Li, Botian Shi, Yu Qiao, Dahua Lin, and Conghui He. 2024a. Mineru: An open-source solution for precise document content extraction. Preprint, arXiv:2409.18839. Dashun Wang and Albert-László Barabási. 2021. The Science of Science. Cambridge University Press. Jiayou Wang, Rundong Liu, Yue Hu, Huijia Wu, and Zhaofeng He. 2025b. SecDecoding: Steerable decoding for safer LLM generation. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2050420521, Suzhou, China. Association for Computational Linguistics. Qidong Wang, Junjie Hu, and Ming Jiang. 2025c. VSEAM: Visual semantic editing and attention modulating for causal interpretability of vision-language models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1740717431, Suzhou, China. Association for Computational Linguistics. Song Wang, Zhen Tan, Zihan Chen, Shuang Zhou, Tianlong Chen, and Jundong Li. 2025d. AnyMAC: Cascading flexible multi-agent collaboration In Proceedings of the via next-agent prediction. 2025 Conference on Empirical Methods in Natural Language Processing, pages 1156611578, Suzhou, China. Association for Computational Linguistics. WenHao Wang, Mengying Yuan, Zijie Yu, Guangyi Liu, Rui Ye, Tian Jin, Siheng Chen, and Yanfeng Wang. 2025e. MobileA3gent: Training mobile GUI agents using decentralized self-sourced data from diverse In Proceedings of the Fourth Workshop on users. Bridging Human-Computer Interaction and Natural Language Processing (HCI+NLP), pages 79112, Suzhou, China. Association for Computational Linguistics. Xiaochen Wang, Jiaqi Wang, Houping Xiao, Jinghui Chen, and Fenglong Ma. 2024b. FEDKIM: Adaptive federated knowledge injection into medical foundation models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 81418154, Miami, Florida, USA. Association for Computational Linguistics. Chengrui Xiang, Tengfei Ma, Xiangzheng Fu, Yiping Liu, Bosheng Song, and Xiangxiang Zeng. 2025. From knowledge to treatment: Large language model assisted biomedical concept representation for drug repurposing. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1396713982, Suzhou, China. Association for Computational Linguistics. Xiaonan Wang, Bo Shao, and Hansaem Kim. 2025f. AI knows where you are: Exposure, bias, and inference in multimodal geolocation with KoreaGEO. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 98979914, Suzhou, China. Association for Computational Linguistics. Haoran Wei, Yaofeng Sun, and Yukun Li. 2025a. Contexts optical compression. Deepseek-ocr: Preprint, arXiv:2510.18234. Jiaqi Wei, Yuejin Yang, Xiang Zhang, Yuhan Chen, Xiang Zhuang, Zhangyang Gao, Dongzhan Zhou, Guangshuai Wang, Zhiqiang Gao, Juntai Cao, Zijie Qiu, Ming Hu, Chenglong Ma, Shixiang Tang, Junjun He, Chunfeng Song, Xuming He, Qiang Zhang, Chenyu You, and 8 others. 2025b. From ai for science to agentic science: survey on autonomous scientific discovery. Preprint, arXiv:2508.14111. Qiming Wu, Zichen Chen, Will Corcoran, Misha Sra, and Ambuj Singh. 2025a. GraphEval36K: Benchmarking coding and reasoning capabilities of large In Findings language models on graph datasets. of the Association for Computational Linguistics: NAACL 2025, pages 80958117, Albuquerque, New Mexico. Association for Computational Linguistics. Zekun Wu, Seonglae Cho, Umar Mohammed, Cristian Enrique Munoz Villalobos, Kleyton Da Costa, Xin Guan, Theo King, Ze Wang, Emre Kazim, and Adriano Koshiyama. 2025b. LibVulnWatch: deep assessment agent system and leaderboard for uncovering hidden vulnerabilities in open-source AI libraries. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pages 608656, Vienna, Austria. Association for Computational Linguistics. Magdalena Wysocka, Danilo Carvalho, Oskar Wysocki, Marco Valentino, and Andre Freitas. 2025. SylloBioNLI: Evaluating large language models on biomedical syllogistic reasoning. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 72357258, Albuquerque, New Mexico. Association for Computational Linguistics. Eric Xia, Karthik Srikumar, Keshav Karthik, Advaith Renjith, and Ashwinee Panda. 2025. Beyond the haystack: Sensitivity to context in legal reference recall. In Proceedings of the Natural Legal Language Processing Workshop 2025, pages 4853, Suzhou, China. Association for Computational Linguistics. Yunze Xiao, Yujia Hu, Kenny Tsu Wei Choo, and Roy Ka-Wei Lee. 2024. ToxiCloakCN: Evaluating robustness of offensive language detection in Chinese with cloaking perturbations. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 60126025, Miami, Florida, USA. Association for Computational Linguistics. Yunze Xiao, Lynnette Hui Xian Ng, Jiarui Liu, and Mona T. Diab. 2025. Humanizing machines: Rethinking LLM anthropomorphism through multilevel framework of design. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 33313350, Suzhou, China. Association for Computational Linguistics. Pengcheng Xu, Sicheng Shen, Jieli Zhou, and Hongyi Xin. 2025a. Team XSZ at BioLaySumm2025: Section-wise summarization, retrieval-augmented LLM, and reinforcement learning fine-tuning for lay summaries. In Proceedings of the 24th Workshop on Biomedical Language Processing (Shared Tasks), pages 275280, Vienna, Austria. Association for Computational Linguistics. Wanghan Xu, Yuhao Zhou, Yifan Zhou, Qinglong Cao, Shuo Li, Jia Bu, Bo Liu, Yixin Chen, Xuming He, Xiangyu Zhao, Xiang Zhuang, Fengxiang Wang, Zhiwang Zhou, Qiantai Feng, Wenxuan Huang, Jiaqi Wei, Hao Wu, Yuejin Yang, Guangshuai Wang, and 88 others. 2025b. Probing scientific general intelligence of llms with scientist-aligned workflows. Preprint, arXiv:2512.16969. Yuebin Xu, Zhiyi Chen, and Zeyi Wen. 2025c. EcoTune: Token-efficient multi-fidelity hyperparameter optimization for large language model inferIn Proceedings of the 2025 Conference on ence. Empirical Methods in Natural Language Processing, pages 77467756, Suzhou, China. Association for Computational Linguistics. Zicheng Xu, Guanchu Wang, Guangyao Zheng, YuNeng Chuang, Alex Szalay, Xia Hu, and Vladimir Braverman. 2025d. Self-ensemble: Mitigating confidence distortion for large language models. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1660316615, Suzhou, China. Association for Computational Linguistics. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and David Ha. 2025. The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Preprint, arXiv:2504.08066. 29 Bohao Yang, Dong Liu, Chenghao Xiao, Kun Zhao, Chen Tang, Chao Li, Lin Yuan, Yang Guang, and Chenghua Lin. 2025a. Crafting customisable characters with LLMs: persona-driven role-playing agent framework. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2021620240, Suzhou, China. Association for Computational Linguistics. Dayu Yang, Antoine Simoulin, Xin Qian, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, and Grey Yang. 2025b. DocAgent: multi-agent system for automated code documentation generation. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 460471, Vienna, Austria. Association for Computational Linguistics. Jing Yang. 2025. Position: The artificial intelligence and machine learning community should adopt more transparent and regulated peer review process. In Proceedings of the 42nd International Conference on Machine Learning, volume 267 of Proceedings of Machine Learning Research, pages 8239982408. PMLR. Seungyoun Yi, Minsoo Khang, and Sungrae Park. 2025. ZERA: Zero-init instruction evolving refinement agent from zero instructions to structured prompts via principle-based optimization. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2333423348, Suzhou, China. Association for Computational Linguistics. Kai Yin, Xiangjue Dong, Chengkai Liu, Lipai Huang, Yiming Xiao, Zhewei Liu, Ali Mostafavi, and James Caverlee. 2025. DisastIR: comprehensive information retrieval benchmark for disaster management. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 18361867, Suzhou, China. Association for Computational Linguistics. Ko Yoshida, Daiki Shiono, Kai Sato, Toko Miura, Momoka Furuhashi, and Jun Suzuki. 2025. Batchwise convergent pre-training: Step-by-step learnIn ing inspired by child language development. Proceedings of the First BabyLM Workshop, pages 508524, Suzhou, China. Association for Computational Linguistics. Language Processing: Industry Track, pages 2688 2712, Suzhou (China). Association for Computational Linguistics. Chenchen Yuan, Zheyu Zhang, Shuo Yang, Bardh Prenkaj, and Gjergji Kasneci. 2025a. Probabilistic aggregation and targeted embedding optimization for collective moral reasoning in large language models. In Findings of the Association for Computational Linguistics: ACL 2025, pages 1115111168, Vienna, Austria. Association for Computational Linguistics. Mengying Yuan, WenHao Wang, Zixuan Wang, Yujie Huang, Kangli Wei, Fei Li, Chong Teng, and Donghong Ji. 2025b. Cross-document crosslingual NLI via RST-enhanced graph fusion and interpretability prediction. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 3159531617, Suzhou, China. Association for Computational Linguistics. Mengying Yuan, WenHao Wang, Zixuan Wang, Yujie Huang, Kangli Wei, Fei Li, Chong Teng, and Donghong Ji. 2025c. Cross-document cross-lingual NLI via RST-enhanced graph fusion and interpretability prediction. In Proceedings of the 5th Workshop on Multilingual Representation Learning (MRL 2025), pages 1133, Suzhuo, China. Association for Computational Linguistics. Aizan Zafar, Kshitij Mishra, and Asif Ekbal. 2024. MedLogic-AQA: Enhancing medicare question answering with abstractive models focusing on logiIn Findings of the Association for cal structures. Computational Linguistics: EMNLP 2024, pages 1684416867, Miami, Florida, USA. Association for Computational Linguistics. Ofir Zafrir, Igor Margulis, Dorin Shteyman, Shira Guskin, and Guy Boudoukh. 2025. FastDraft: How to train your draft. In Findings of the Association for Computational Linguistics: ACL 2025, pages 22488 22505, Vienna, Austria. Association for Computational Linguistics. Yiming Zeng, Wanhao Yu, Zexin Li, Tao Ren, Yu Ma, Jinghan Cao, Xiyan Chen, and Tingting Yu. 2025. Bridging the editing gap in LLMs: FineEdit for precise and targeted text modifications. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 21932206, Suzhou, China. Association for Computational Linguistics. Yao-Ching Yu, Tsun-Han Chiang, Cheng-Wei Tsai, Chien-Ming Huang, and Wen-Kwang Tsao. 2025a. Primus: pioneering collection of open-source datasets for cybersecurity LLM training. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1040210424, Suzhou, China. Association for Computational Linguistics. Hao Zhang, Bo Huang, Zhenjia Li, Xi Xiao, Hui Yi Leong, Zumeng Zhang, Xinwei Long, Tianyang Wang, and Hao Xu. 2025a. Sensitivity-LoRA : Low-load sensitivity-based fine-tuning for large lanIn Findings of the Association for guage models. Computational Linguistics: EMNLP 2025, pages 1318513199, Suzhou, China. Association for Computational Linguistics. Zhaojian Yu, Yinghao Wu, Yilun Zhao, Arman Cohan, and Xiao-Ping Zhang. 2025b. Z1: Efficient test-time scaling with code. In Proceedings of the 2025 Conference on Empirical Methods in Natural Hengrui Zhang, Pin-Siang Huang, Zhen Zhang, Peican Lin, Yao-Ching Yu, Bo Hu, and Yulu Du. 2025b. PathwiseRAG: Multi-dimensional exploIn Proceedings ration and integration framework. 30 of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2291522936, Suzhou, China. Association for Computational Linguistics. Jiaxin Zhang. 2025. Confidence-aware reasoning: Optimizing self-guided thinking trajectories in large reasoning models. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 2081 2095, Suzhou (China). Association for Computational Linguistics. Shiwen Zhang, Lingxiang Wang, Hainan Zhang, Ziwei Wang, Sijia Wen, and Zhiming Zheng. 2025c. Beyond the surface: solution-aware retrieval model for competition-level code generation. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 52375246, Suzhou, China. Association for Computational Linguistics. Tianyu Zhang, Xinyu Wang, Lu Li, Zhenghan Tai, Jijun Chi, Jingrui Tian, Hailin He, and Suyuchen STRICT: Stress-test of renderWang. 2025d. ing image containing text. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2114821161, Suzhou, China. Association for Computational Linguistics. Wei Zhang, Hongcheng Guo, Jian Yang, Zhoujin Tian, Yi Zhang, Yan Chaoran, Zhoujun Li, Tongliang Li, Xu Shi, Liangfan Zheng, and Bo Zhang. 2024. mABC: Multi-agent blockchain-inspired collaboration for root cause analysis in micro-services architecture. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 40174033, Miami, Florida, USA. Association for Computational Linguistics. Yiming Zhang, Siyue Zhang, Junbo Zhao, and Chen Zhao. 2025e. RPDR: round-trip predictionbased data augmentation framework for longIn Proceedings of the tail question answering. 2025 Conference on Empirical Methods in Natural Language Processing, pages 2200922023, Suzhou, China. Association for Computational Linguistics. Yizhe Zhang and Navdeep Jaitly. 2025. SAGE: Steering dialog generation with future-aware state-action augmentation. In Proceedings of the The 4th Workshop on Perspectivist Approaches to NLP, pages 123132, Suzhou, China. Association for Computational Linguistics. Yukun Zhang and Xueqing Zhou. 2025. Continuoustime attention: PDE-guided mechanisms for longIn Proceedings of the sequence transformers. 2025 Conference on Empirical Methods in Natural Language Processing, pages 2165421674, Suzhou, China. Association for Computational Linguistics. Zhuoxuan Zhang, Jinhao Duan, Edward Kim, and Kaidi Xu. 2025f. Sparse neurons carry strong signals of question ambiguity in LLMs. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1609216110, Suzhou, China. Association for Computational Linguistics. Chuanyang Zheng, Yihang Gao, Guoxuan Chen, Han Shi, Jing Xiong, Xiaozhe Ren, Chao Huang, Zhenguo Li, and Yu Li. 2025a. Self-adjust softmax. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 78277847, Suzhou, China. Association for Computational Linguistics. Shunfeng Zheng, Meng Fang, and Ling Chen. 2025b. SpatialWebAgent: Leveraging large language models for automated spatial information extraction and map grounding. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 252266, Vienna, Austria. Association for Computational Linguistics. Shunfeng Zheng, Yudi Zhang, Meng Fang, Zihan Zhang, Zhitan Wu, Mykola Pechenizkiy, and Ling Chen. 2025c. Benchmarking foundation models with retrieval-augmented generation in olympiclevel physics problem solving. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 2192721956, Suzhou, China. Association for Computational Linguistics. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025d. DeepResearcher: Scaling deep research via reinforcement learning in real-world environments. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 414431, Suzhou, China. Association for Computational Linguistics. Zhonghua Zheng, Lizi Liao, Yang Deng, Libo Qin, and Liqiang Nie. 2024. Self-chats from large language models make small emotional support chatbot better. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1132511345, Bangkok, Thailand. Association for Computational Linguistics. Jingzhuo Zhou. 2025. Can mllms detect phishing? comprehensive security benchmark suite focusing on dynamic threats and multimodal evaluation in academic environments. Preprint, arXiv:2511.15165v1. Note: The issue was fixed in version 2. Shu Zhou, Yunyang Xuan, Yuxuan Ao, Xin Wang, Tao Fan, and Hao Wang. 2025a. MERIT: Multi-agent collaboration for unsupervised time series representation learning. In Findings of the Association for Computational Linguistics: ACL 2025, pages 24011 24028, Vienna, Austria. Association for Computational Linguistics. WenJie Zhou, Bohan Wang, Wei Chen, and Xueqi Cheng. 2025b. BSFA: Leveraging the subspace dichotomy to accelerate neural network training. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 31 1884518860, Suzhou, China. Association for Computational Linguistics. Yue Zhou and Barbara Di Eugenio. 2025. Veracity bias and beyond: Uncovering LLMs hidden beliefs in problem-solving reasoning. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2129821310, Vienna, Austria. Association for Computational Linguistics. Zebiao Zhou, Hui Li, Xiangxun Zhu, and Kangzhen Liu. 2025c. TranssionMTs submission to the Indic MT shared task in WMT 2025. In Proceedings of the Tenth Conference on Machine Translation, pages 12711275, Suzhou, China. Association for Computational Linguistics. Zihan Zhou, Simon Kurz, and Zhixue Zhao. 2025d. Revisiting pruning vs quantization for small language models. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 1205512070, Suzhou, China. Association for Computational Linguistics. He Zhu, Guanhua Chen, and Wenjia Zhang. 2025a. PlanGPT: Enhancing urban planning with tailored agent framework. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 6: Industry Track), pages 764 783, Vienna, Austria. Association for Computational Linguistics. Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, Robert Tang, Heng Ji, and Jiaxuan You. 2025b. MultiAgentBench : Evaluating the collaboration and competition of LLM agents. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 85808622, Vienna, Austria. Association for Computational Linguistics. Lichao Zhu, Maria Zimina-Poirot, Cristian Valdez, and Stephane Patin. 2025c. Fine-tuning NMT models and LLMs for specialised EN-ES translation using aligned corpora, glossaries, and synthetic data: MULIn TITAN at WMT25 terminology shared task. Proceedings of the Tenth Conference on Machine Translation, pages 12841291, Suzhou, China. Association for Computational Linguistics. Runchuan Zhu, Bowen Jiang, Lingrui Mei, Fangkai Yang, Lu Wang, Haoxiang Gao, Fengshuo Bai, Pu Zhao, Qingwei Lin, Saravan Rajmohan, and Dongmei Zhang. 2025d. AdaptFlow: Adaptive workflow optimization via meta-learning. In Findings of the Association for Computational Linguistics: EMNLP 2025, pages 32873302, Suzhou, China. Association for Computational Linguistics. Runchuan Zhu, Zinco Jiang, Jiang Wu, Zhipeng Ma, Jiahe Song, Fengshuo Bai, Dahua Lin, Lijun Wu, and Conghui He. 2025e. GRAIT: Gradient-driven refusal-aware instruction tuning for effective hallucination mitigation. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 40064021, Albuquerque, New Mexico. Association for Computational Linguistics."
        },
        {
            "title": "A More Suggestions",
            "content": "Toward easy verification formats and proper citation practices. HalluCitations are often disguised within the reference section, making perceptual detection by human reviewers costly. Although this study relies on OCR-based parsing and manual verification to identify such cases, parsing is not always perfect. Therefore, we believe that reference sections in ACL style formats could be improved to be more machine-friendly to facilitate automatic verification, e.g., by introducing clearer structural delimiters between citations, more explicit formatting of key fields such as titles, and printing URLs rather than embedding them only as hyperlinks. Moreover, we argue that verifiable identifiers, e.g., clickable links and DOIs, should be mandatory. This would facilitate verification, reduce manual effort, and enable automatic verification. Eliminating area selection or improving area transparency. As discussed in Section 4.1, the proportion of HalluCited papers varies across research areas, with higher rates in emerging areas. We argue that this discrepancy is partly an artifact of area partitioning. Accordingly, we suggest either moving to single area or improving transparency by publicly releasing area information for accepted papers. As cross-area research becomes more popular, fixed area selection can lead to reviewer mismatches and reduced review rigor. Given that modern review systems such as OpenReview support automated reviewer matching, explicit area selection may no longer be necessary. Alternatively, since area information is currently disclosed only partially9, archiving it in resources such as the ACL Anthology would help reduce mismatches and support more effective reviews, including the detection of HalluCitations."
        },
        {
            "title": "B List of HalluCited Papers",
            "content": "Table 7 shows all identified HalluCited papers and the corresponding evidence, i.e., HalluCitations. For privacy reasons, we present the information in reference format rather than directly displaying the HalluCited paper details. To identify the corresponding paper, please follow the provided link or refer to the reference section. 32 Table 7: List of hallucinated papers, i.e., HalluCited Papers, and their corresponding HalluCitations. EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 1 naacl-2024 2024.naacl-short.63 Scaria et al. (2024) 2 naacl2024.trustnlp-1.1 Adilazuarda (2024) 3 acl-2024 2024.acl-long.560 Siska et al. (2024) 4 acl-2024 2024.acl-long.611 Zheng et al. (2024) 5 acl2024.acl-long.768 Chen et al. (2024) 6 7 8 acl-2024 2024.arabicnlp-1.24 Khondaker et al. (2024) acl-2024 2024.arabicnlp-1.45 Shah et al. (2024) acl-2024 2024.findings-acl.241 Holt et al. (2024) acl-2024 2024.wassa-1.18 Alhamed et al. (2024) 10 emnlp-2024 2024.conll-1.8 Mundra et al. (2024) 11 emnlp-2024 2024.emnlp-demo.11 Sheth et al. (2024) Himanshu Gupta, Kevin Scaria, Swaroop Mishra, and Chitta Baral. 2024b. Beyond the data bottleneck: Optimizing instruction tuning with difficulty-based exemplar selection. ArXiv preprint. Daphne Ippolito, Daniel Duckworth, Chris CallisonBurch, and Douglas Eck. 2020. Discriminating between human-produced and machine-generated text: survey. arXiv preprint arXiv:2012.03358. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2023. An adversarial winograd schema challenge at scale. arXiv preprint arXiv:2305.06300. Chun-Hung Yeh, Anuradha Welivita, and Pearl Pu Faltings. 2015. dialogue dataset containing emotional support for people in distress. arXiv preprint arXiv:1503.08895. Mengjie Fang, Linlin Xu, and Pascale Fung. 2020. Unsupervised cross-lingual transfer learning for contextualized word embeddings. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26162629. Isabel Moreno and Wei Zhang. 2024. Evaluating multilingual models on nlp tasks in arabic. Computational Linguistics, 50(3):425445. Naoya Inoue, Pontus Stenetorp, and Kentaro Inui. 2021. Interplay between preferences and machine learning in language model fine-tuning. arXiv preprint arXiv:2110.08413. Junjie Li, Jieyu Wu, and Richard Socher. 2021. Understanding code-switching in language models. arXiv preprint arXiv:2109.04278. David Benetka, Alicia Moreno-Moral, Lorena RomeroFombuena, and Juan Lopez-Gazpio. 2020. An annotation scheme for mental health discussions in social media. In International Conference on Computational Linguistics (Proceedings of the Conference: Long Papers, 2020), pages 26172627. Association for Computational Linguistics. Adam Alabi, Saleha Nawaz, and Vincent Ng. 2022. Alabi: light-weight approach for multilingual biomedical language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 97179727. Stephan Druskat, Ulrike Gut, Nils Reiter, Stefan Schweter, and Manfred Stede. 2014. Atomic: An open-source tool for working with anaphora in multiple languages. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 7176."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "33 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 12 emnlp-2024 2024.emnlp-industry.112 Malik et al. (2024) 13 emnlp-2024 2024.emnlp-main.345 Xiao et al. (2024) 14 emnlp-2024 2024.emnlp-main.464 Wang et al. (2024b) 15 emnlp-2024 2024.emnlp-main.617 Wahle et al. (2024) 16 emnlp-2024 2024.findings-emnlp.232 Zhang et al. (2024) emnlp-2024 2024.findings-emnlp.813 Li et al. (2024) 18 emnlp-2024 2024.findings-emnlp.912 Sonkar et al. (2024) 19 emnlp-2024 2024.findings-emnlp.981 Zafar et al. (2024) emnlp-2024 2024.mrl-1.20 Skianis et al. (2024) 21 naacl-2025 2025.americasnlp-1.4 Alvarez et al. (2025) 22 naacl-2025 2025.clpsych-1.23 Antony and Schoene (2025) Jiajia Liu, Mengyuan Yang, Yankai Yu, Haixia Xu, Kang Li, and Xiaobo Zhou. 2023a. Enhancing customer experience with ai-driven conversational agents. arXiv preprint arXiv:2306.04325. Nelson Liu, Tony Wu, Duane Boning, and Tanmoy Choudhury. 2020b. AI bug detector: Adversarial input detection for natural language processing models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, page 187196. Yu Gu, Robert Tinn, Hao Cheng, Youzheng Ben, Zhuozhao Liu, Jingqi Zhou, Michael Wang, Shizhuo Wang, Hongfang Zhou, and Yanshan Shen. 2021. Biomedgpt: large-scale biomedical generative pretrained transformer for biomedical text mining. arXiv preprint arXiv:2104.07497. Dominik Meier, Jan Philip Wahle, Terry Ruas, and Bela Gipp. 2024. human study on atomic paraphrase type generation. arXiv. Yi Chen et al. 2023. Empowering cloud rca with augmented large language models. arXiv preprint arXiv:2311.00000. Ruochen Wang, Ting Liu, Cho-jui Hsieh, and Boqing Gong. 2023. DPO-DIFF:on Discrete Prompt Optimization for text-to-image DIFFusion modelsgenerating Natural Language Adversarial Examples. Preprint, arXiv:2311.07998. Yoav Goldberg. 2022. Assessing claims about large language models. arXiv preprint arXiv:2212.09273. Xinyu Wang, Tao Sun, Deqing Zou, Wei Wu, and Jiawei Han. 2021. Logic-guided data augmentation and regularization for consistency learning. arXiv preprint arXiv:2104.04379. Shaoxiong Ji, Yanzhao Zhang, Leilei Sun, and Jia Wang. 2022. Mentalbert: pretrained language model for mental healthcare. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). Manuel Mager, Arturo Oncevay, Annette Rios, Jamshidbek Mirzakhalov, and Katharina Kann. 2023. The role of computational linguistics in indigenous language revitalization: Challenges and opportunities. In Proceedings of the 1st Workshop on Computation for Indigenous Languages (C3NLP), pages 2331. Ilias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2022. An empirical study on neural methods for legal judgment prediction. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 47834798. Association for Computational Linguistics."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "34 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 23 naacl2025.dravidianlangtech1.117 Noor et al. (2025) 24 naacl-2025 2025.dravidianlangtech1.118 Selvamurugan (2025) 25 naacl-2025 2025.dravidianlangtech1.38 et al. (2025a) 26 naacl2025.dravidianlangtech1.43 Chauhan and Kumar (2025) 27 naacl-2025 2025.dravidianlangtech1.61 et al. (2025) 28 naacl-2025 2025.dravidianlangtech1.62 Vaidyanathan et al. (2025a) 29 naacl2025.dravidianlangtech1.7 Sathvik et al. (2025b) G. Shimi et al. 2024. Language identification for dravidian languages: crucial step for fake news detection in multilingual settings. TBD. Anil Kumar, Ajay Kumar Ojha, Shervin Malmasi, and Marcos Zampieri. 2021. Benchmarking aggression identification in social media for low-resource languages. In Proceedings of the Workshop on Online Abuse and Harms (ACL). Simran Khanuja, Kaustav Dey, El Moatez Billah Karim Nagoudi, et al. 2020. new dataset and strong baselines for the detection of code-mixed offensive language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 17191726. Douwe Kiela, Mohamed Elhoseiny, Lin Zhang, Marco Baroni, and Geoffrey Hinton. 2019. Supervised multimodal hashing for scalable cross-modal retrieval. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 17401751. F. Wu and M. Dredze. 2019. Social media as sensor of public opinion. Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP 2019), 2019:10011010. Jiho Park, Jihyung Shin, Sangyoun Lee, and Changhyun Seo. 2018. survey of hate speech detection: Data, methods, and challenges. In Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018), pages 385395. International Committee on Computational Linguistics. M. H. Al-Adhaileh and F. W. Alsaade. 2022. Bidirectional long-short term memory (bilstm) networks for fake review detection: comparative study. Springer. 30 naacl-2025 2025.dravidianlangtech1. Achamaleh et al. (2025) Anna Kolesnikova and Sergey Ivanov. 2023. Exploring multilingual text representations with transformer models. Transactions of the ACL, 11:212230. R. Raja, A. Kumar, and S. Joseph. 2023. Fake news detection in low-resource languages: Challenges and advancements. Computational Linguistics Review, 15(2):123137. Simran Khanuja, Sandipan Dandapat, Ritesh Kumar, Sunayana Sitaram, K. P. Soman, and Anup Kumar. 2021. Mahanlp: Towards indic language understanding using bert models for hindi, marathi, and kannada. arXiv preprint arXiv:2106.07469. W. Liu, X. Li, W. Yang, Y. Lin, X. Liu, S. Wang, C. Xie, L. Xu, W. Zhuang, X. Zhao, and L. Li. 2020. Minilm: Deep self-attention distillation for tiny transformers. arXiv preprint arXiv:2002.10957."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "31 naacl-2025 2025.dravidianlangtech1.91 Aftahee et al. (2025) 32 naacl2025.dravidianlangtech1.97 Sathvik et al. (2025a) 33 naacl-2025 2025.findings-naacl.132 Singh et al. (2025) 35 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 34 naacl2025.findings-naacl.223 Zhu et al. (2025e) 35 naacl-2025 2025.findings-naacl.230 Raihan et al. (2025) 36 naacl-2025 2025.findings-naacl.257 Magomere et al. (2025) 37 naacl-2025 2025.findings-naacl. Song et al. (2025b) 38 naacl-2025 2025.findings-naacl.279 Basher et al. (2025) naacl-2025 2025.findings-naacl.349 Gupta et al. (2025a) 40 naacl-2025 2025.findings-naacl.418 Nguyen et al. (2025b) naacl-2025 2025.findings-naacl.452 Wu et al. (2025a) 42 naacl-2025 2025.findings-naacl.58 Liu et al. (2025c) 43 naacl-2025 2025.knowledgenlp-1.18 Spangher et al. (2025a) 44 naacl-2025 2025.knowledgenlp-1.20 Pattnayak et al. (2025) 45 naacl-2025 2025.loresmt-1.12 Raja and Vats (2025) 46 naacl2025.naacl-demo.39 Hiray and Kovatchev (2025) Krishnateja Killamsetty and Rishabh Iyer. 2021. Subset selection with gradient matching. arXiv preprint arXiv:2103.00123. OpenAI. 2024. Gpt-4 omni: comprehensive multimodal model for language, vision, and beyond. arXiv preprint arXiv:2408.01234. Chaitanya Shivade et al. 2019. Mednli-a natural language inference dataset for the clinical domain. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium. Association for Computational Linguistics, pages 15861596. Honnibal and Montani. 2017. spacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing. neural machine translation. In Proceedings of the Association for Computational Linguistics (ACL), pages 688697. Y. Xu et al. 2023. Cross-lingual transfer for lowresource text-to-speech. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 510520. Association for Computational Linguistics. Zijun Zhang, Yue Wu, Hao Guan, Xinlei Chen, and Yue Zhang. 2023. Continual learning with transformers: Challenges and solutions. arXiv preprint arXiv:2302.13713. Danny Hernandez, Jared Kaplan, Tom Henighan, Tom Brown, Jack Clark, Preetum Nakkiran, Catherine Olsson, and Afshin Rahimi. 2023. Measuring data contamination in large language models. arXiv preprint arXiv:2303.13375. Abhijit Srivastava, Nathan Major, Jasmine D. Hernandez, and et al. 2022. Beyond the imitation game: Quantifying and extrapolating llm capabilities with big-bench. arXiv preprint arXiv:2206.04615. Xiang Jiang et al. 2023c. Mistral-7b-instruct-v0.2: An efficient large language model for instruction following. arXiv preprint arXiv:2309.00001. Tushar Khot, Ashish Sabharwal, and et al. 2023. Decomposition-driven reasoning in language models. arXiv preprint arXiv:2304.xxxxx. et al. Zhao, W. 2020. retrieval-augmented encoder-decoder for knowledge-intensive nlp tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). Kalika Bali, Jatin Sharma, Monojit Choudhury, and Yogarshi Vyas. 2014. Code-mixing: challenge for language identification in the indian context. In Proceedings of the First Workshop on Computational Approaches to Code Switching (EMNLP), pages 1323. T. Lee et al. 2023. Bivdiff: training-free framework for general-purpose video synthesis. arXiv preprint arXiv:2302.02918."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "36 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 47 naacl2025.naacl-industry.68 Jararweh et al. (2025) 48 naacl-2025 2025.naacl-long.182 Chen et al. (2025b) 49 naacl-2025 2025.naacl-long.363 Qian et al. (2025) 50 naacl2025.naacl-long.371 Wysocka et al. (2025) 51 naacl-2025 2025.naacl-long.620 Chen et al. (2025c) 52 naacl-2025 2025.naacl-long.72 Hu et al. (2025) 53 naacl2025.naacl-short.79 Kumar et al. (2025) 54 naacl-2025 2025.nlp4dh-1.52 Greschner and Klinger (2025) 55 naacl-2025 2025.wnu-1.10 Li et al. (2025j) 56 acl2025.acl-demo.16 She et al. (2025) 57 acl-2025 2025.acl-demo.25 Zheng et al. (2025b) 58 acl-2025 2025.acl-demo.44 Yang et al. (2025b) Haotian Liu, Chunyuan Lin, Fangyun Zeng, and et al. 2023a. Llava: Large language and vision assistant. arXiv preprint arXiv:2304.08485. Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, et al. 2023. Falcon-40b: an open large language model with stateof-the-art performance. Findings of the Association for Computational Linguistics: ACL, 2023:1075510773. Zhengbao Jiang, Frank Xu, Jun Araki, and Graham Neubig. 2020. Can prompt-based learning be practical for low-resource tasks? arXiv preprint arXiv:2001.07676. Jakob Prange, Khalil Mrini, and Noah A. Smith. 2023. Challenges and opportunities in nlp for systematic generalization: Beyond compositionality. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. Jiawei Liu, Yifeng Ding, Naman Jain, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang, and Yuxiang Wei. 2024a. Starcoder2-instruct: Fully transparent and permissive self-alignment for code generation. Preprint, arXiv:2307.08701. Mor Geva, Yoav Goldberg, and Jonathan Berant. 2021. Strategyqa: question answering benchmark for reasoning about strategies. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Sankha Bhattacharjee et al. 2023. Adversarial learning for robust ai-generated text detection. arXiv preprint arXiv:2304.07812. Chiara Leoni, Mauro Coccoli, Ilaria Torre, and Gianni Vercelli. 2018. Your paper title here. In Proceedings of the Fifth Italian Conference on Computational Linguistics (CLiC-it 2018), Torino, Italy. Accademia University Press. Xingxing Lu, Yuning Mao, and Jason Wei. 2023. Autoeval: Llm-based automatic evaluation framework for text summarization. In Findings of ACL. Wenhao Zheng and 1 others. 2025a. Citer: Confidencebased token routing for collaborative inference with large language models. arXiv preprint arXiv:2503.01013. Wei Zhang, Pengfei Liu, and Qiang Shen. 2020. Handling ambiguities in geographic texts using nlp techniques. Transactions in GIS, 24(3):519538. Ziniu Wu, Cheng Liu, Jindong Zhang, Xinyun Li, Yewen Wang, Jimmy Xin, Lianmin Zhang, Eric Xing, Yuxin Lu, and Percy Liang. 2023. Autogen: Enabling next-generation multi-agent communication with language models. arXiv preprint arXiv:2309.07864."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "37 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 59 acl2025.acl-industry.30 Fu et al. (2025) 60 acl-2025 2025.acl-industry.54 Zhu et al. (2025a) 61 acl-2025 2025.acl-long.1034 Zhou and Di Eugenio (2025) 62 acl2025.acl-long.1369 Chen et al. (2025e) 63 acl-2025 2025.acl-long.1409 Frydenlund (2025) 64 acl-2025 2025.acl-long.1435 Jia et al. (2025a) 65 acl2025.acl-long.1439 Siingh et al. (2025) 66 acl-2025 2025.acl-long.1554 Klisura et al. (2025) Yihan Chen, Dongkuan Zhang, Xiang Wang, Yifan Yang, and Heng Wang. 2023. Dare: ldirect parameter editing for adaptive mode reconfiguration. arXiv preprint arXiv:2310.09570. Yijie Deng, He Zhu, Wen Wang, Minxin Chen, Junyou Su, and Wenjia Zhang. 2025. Urban planning bench: comprehensive benchmark for evaluating urban planning capabilities in large language models. Equal contribution. *Corresponding author: wenjiazhang@tongji.edu.cn. Jane Doe and Wei Chen. 2023. Debiasing reasoning in language models: quantitative study. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, and Wolfgang Macherey. 2024a. Scaling laws across model architectures: comparative analysis of dense and mixture of experts models. arXiv preprint arXiv:2410.05661. Jason Lee, Elman Mansimov, and Kyunghyun Cho. 2018. Deterministic non-autoregresinterpreting gpt: the logit lenssive neural sequence modeling by iterative refinement. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 11731182, Brussels, Belgium. Association for Computational Linguistics. Mei Li et al. 2024. Securing tool use in llm agents: Challenges and strategies. arXiv preprint arXiv:2402.03014. Wei Xu, Shujie Liu, Minlie Huang, Si Wei, and Hang Li. 2018. survey on hallucination in neural machine translation. arXiv preprint arXiv:1803.02577. Accessed: 2025-02-14. Dorottya Demeszky, Weijie Gong, and Diyi Yang. 2019. Analyzing bias and framing in news articles on artificial intelligence: The case of sentiment analysis. In Proceedings of the 2019 Annual Conference on Empirical Methods in Natural Language Processing, pages 16551661. 67 acl-2025 2025.acl-long.1580 Spangher et al. (2025b) Minjun Chang et al. 2023. Self-reflection with 68 acl-2025 2025.acl-long.248 Lee et al. (2025) 69 acl2025.acl-long.272 Meng et al. (2025) 70 acl-2025 2025.acl-long.340 Li et al. (2025c) generative agents. arXiv preprint arXiv:2311.09214. Wei Chen, Arjun Kumar, and Lin Yang. 2024. Distraction-based attack prompts: An effective jailbreaking method for llms. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL). Chen et al. 2023a. Nl2fol: Natural language to firstorder logic. arXiv preprint arXiv:2304.09102. openai. 2024. Learning to reason with llm. Preprint, arXiv:2406.19949."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "38 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 71 acl2025.acl-long.421 Zhu et al. (2025b) 72 acl-2025 2025.acl-long.491 Alsagheer et al. (2025) 73 acl-2025 2025.acl-long.922 Mohammad (2025) 74 acl2025.acl-long.979 Chang et al. (2025) 75 acl-2025 2025.acl-srw.40 Wu et al. (2025b) 76 acl-2025 2025.acl-srw.56 Pulido et al. (2025) 77 acl2025.acl-srw.73 Sakunkoo and Sakunkoo (2025) 78 acl-2025 2025.acl-srw.85 Dzhubaeva et al. (2025) 79 acl-2025 2025.acl-srw.98 Kumar and Nolbaria (2025) 80 acl2025.africanlp-1.1 Akpobi (2025) 81 acl-2025 2025.africanlp-1.15 Ismail et al. (2025) Xu Sun, Xiaoya Zhang, Yicheng Feng, Shiyang Wang, Shuming Ma, Jiuding He, Zhixu Zhang, Yuxian Gu, Yi Xu, Hao Zhou, and Zhiyuan Liu. 2023. systematic capability evaluation of large vision-language models. arXiv preprint arXiv:2305.16372. Hao Zhong, Jieyu Tang, Tianyang Xu, et al. 2020. Does nlp benefit legal system? case study of document representation. arXiv preprint arXiv:2005.01647. Jan Philip Wahle, Krishnapriya Vishnubhotla, Bela Gipp, and Saif M. Mohammad. 2025. Affect, body, cognition, demographics, and emotion: The abcde of text features for computational affective science. arXiv. Xinyu Li, Rahul Gupta, and Emily Brown. 2024. Datacomplm: Benchmarking data quality in large language models. arXiv preprint arXiv:2402.23456. [first names omitted for brevity] Zhang and colleagues. 2024. Metagpt: Meta programming sota autonomous multi-agent cooperative llm workflows. arXiv preprint arXiv:2404.14496. Sabrina J. Mielke, Tal Linzen, and Jason Eisner. 2021. What kind of knowledge is captured by contextualized word representations? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics, pages 12501265. Christo Kirov, John Sylak-Glassman, Ryan Que, David Yarowsky, Jason Eisner, and Ryan Cotterell. 2018. Universal morphological inflection generation using multilingual dataset. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 5262. Y. Wang, H. Li, and X. Zhang. 2024. Consistency of personality traits in quantized role-playing dialogue agents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 123130. Raghuraj Sailesh Shah, Ankur Anand, Srini Chodisetti, Ankit Gupta, Raj Sanjay Patel, Sameer Patel, and Manish Gupta. 2022. FinservNLP: library of financial shared tasks and benchmarks. In Proceedings of the Third Workshop on Economics and Natural Language Processing, pages 144150. Association for Computational Linguistics. Adewale Oladipo et al. 2023. Wura: multilingual dataset of african languages. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. J. Sohn and 1 others. 2024. Rationale-guided rag for medical question answering. arXiv preprint arXiv:2411.00300."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "39 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 82 acl2025.africanlp-1.30 Pandya et al. (2025) 83 acl-2025 2025.argmining-1.30 El Baff et al. (2025) 84 acl-2025 2025.bea-1.66 Rezayi et al. (2025) 85 acl2025.bea-1.73 Nebhi et al. (2025) 86 acl-2025 2025.bea-1.86 Jain and Rengarajan (2025) 87 acl-2025 2025.bionlp-1.6 Kumar and Basu (2025) 88 acl2025.bionlp-share.22 Evgin et al. (2025) 89 acl-2025 2025.bionlp-share.33 Xu et al. (2025a) 90 acl-2025 2025.bionlp-share.34 Phasook et al. (2025) 91 acl2025.findings-acl.114 Gupta et al. (2025b) 92 acl-2025 2025.findings-acl.1140 Cui et al. (2025b) 93 acl-2025 2025.findings-acl.1143 Das et al. (2025b) M. Martin et al. 2022. Swahbert: Enhancing swahili nlp with pretrained transformers. In Proc. EACL. Yiqiu Xu, Alessio Frosini, Mattia Vanni, Anisa Rula, and Roberto Navigli. 2024. Frase: Frame-based semantic enrichment for sparql query generation. arXiv preprint arXiv:2503.22144. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. 2024. Qwen2.5: The next generation of qwen large language models. arXiv preprint arXiv:2407.10671. Elizabeth Mayfield and Alan W. Black. 2020. Automated scoring of written essays with transformer models. In Proc. BEA at ACL 2020. Tianyu Zhang, Sayak Basu, Douwe Kiela, and Oriol Vinyals. 2024. Cascade-aware training and inference for more resource-efficient language models. arXiv preprint arXiv:2405.12345. Ilias Chalkidis, Michail Fergadiotis, Roger Stradling, Nikolaos Pappas, and Prodromos Malakasiotis. 2020. Transformer-based models for legal and biomedical document classification: comparative study. In Proceedings of ACL. Qingxiu Jia, Qipeng Xu, Weiting Yu, Yitong Duan, JianYun Nie, and Zhiyuan Liu. 2022. Alignscore: Evaluating factual consistency with contextual alignment. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). Yang Liu, Ani Nenkova, and Kathleen McKeown. 2022. Incorporating controllability in neural abstractive summarization. In Proceedings of the 60th Annual Meeting of the ACL, pages 876890. ACL. Sahil Dhuliawala, Raghav Gupta, and Shashi Narayan. 2023. Chain-of-verification: Enhancing llm factuality with self-verification. In NAACL. Yao Du, Kaitao Qian, Sanmi Koyejo, Zheng Zheng, and Chao Zhang. 2023. Alpaca: strong, replicable instruction-following model. arXiv preprint arXiv:2303.16199. Xiaoyu Li, Wei Zhang, Jiahui Chen, and Hongwei Liu. 2024c. Iris: Benchmarking large language models for information retrieval tasks. arXiv preprint arXiv:2401.12345. Raphael Rafailov, Orion Redwood, et al. 2023. Direct preference optimization: You dont need rewards to finish rlhf. arXiv preprint arXiv:2305.11517. Preprint, arXiv:2305.11517."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "40 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 94 acl2025.findings-acl.1156 Zafrir et al. (2025) 95 acl-2025 2025.findings-acl.1184 Garg et al. (2025) 96 acl-2025 2025.findings-acl.1208 Das et al. (2025a) 97 acl2025.findings-acl.1231 Zhou et al. (2025a) 98 acl-2025 2025.findings-acl.1278 Bi et al. (2025) 99 acl-2025 2025.findings-acl.1279 Nahin et al. (2025) 100 acl-2025 2025.findings-acl. Maji et al. (2025) 101 acl-2025 2025.findings-acl.268 Tong and Wang (2025) 102 acl-2025 2025.findings-acl. Sun et al. (2025) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Xsum: new dataset for abstractive summarization of news articles. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 701711. William Berrios, Yonglong Tian, Chiraag Lala, Alec Parrish, and Trevor Darrell. 2023. Lens: learnable embedding name space for multi-modal, multi-task learning. arXiv preprint arXiv:2306.11527. Rakesh Gupta, Susan Johnson, and Wei Li. 2023. Prompt abstraction: Leveraging language-image embeddings to scale creativity. arXiv preprint arXiv:2302.12345. Oron Anschel, Nir Baram, and Nahum Shimkin. 2018. Learning representations in multi-agent environment. In International Conference on Learning Representations. DeepSeek-AI. 2024. Deepseek llm: Scaling opensource language models with longevity value. arXiv preprint arXiv:2401.02954. Lian, Goodson, Pentland, et al. 2023. Openorca: An open dataset of gpt augmented flan reasoning traces. arXiv preprint arXiv:2305.11206. Ramesh Patel et al. 2023. Evaluating cultural understanding in multilingual models. arXiv preprint arXiv:2302.12345. David Bamman. 2020. Booknlp: Natural language processing for long-form text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 17. Association for Computational Linguistics. Tianle Li, Wei-Lin Chiang, Lisa Dunlap Evan Frick, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. 2024. From live data to high-quality benchmarks: The arena-hard pipeline. arXiv preprint, abs/2406.11939. 103 acl-2025 2025.findings-acl. Parfenova and Pfeffer (2025) Diana Parfenova et al. 2024. Automating qualitative analysis with llms. Proceedings of ACL 2024. 104 acl-2025 2025.findings-acl.581 Yuan et al. (2025a) 105 acl2025.findings-acl.690 Atri et al. (2025) 106 acl-2025 2025.findings-acl.813 Li et al. (2025g) Kevin Meng, Alex Andonian, David Bau, Yonatan Belinkov, and Fei-Fei Li. 2023. MEMIT: Massediting memory in transformer. arXiv preprint arXiv:2302.04761. Peter Hase and Mohit Bansal. 2023. Compositional edits in language models. EMNLP. OpenAI. 2024. Gpt-4o: Scaling and performance improvements. arXiv preprint arXiv:2401.XXXXX. 107 acl-2025 2025.findings-acl.861 Abootorabi et al. (2025) Gunnar Sigurdsson, Gul Varol, Giovanni Maria Farinella, et al. 2018. Charadesego: dataset for egocentric video understanding. arXiv preprint arXiv:1804.09626."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "41 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 108 acl-2025 2025.findings-acl. Li et al. (2025e) 109 acl-2025 2025.gebnlp-1.7 110 acl-2025 2025.gem-1.6 Kononykhina et al. (2025) Belz and Thomson (2025) 111 acl-2025 2025.iwslt-1.16 K. Roy et al. (2025) 112 acl-2025 2025.knowllm-1. Li (2025b) 113 acl-2025 2025.l2m2-1.7 Lasy et al. (2025) 114 acl-2025 2025.llmsec-1. Sorkhpour et al. (2025) 115 acl-2025 2025.magmar-1.9 Li and Ke (2025a) 116 acl-2025 2025.nlp4pi-1. Ding et al. (2025) 117 acl-2025 2025.realm-1.33 Punjwani and Heck (2025) 118 acl-2025 2025.sdp-1. Mandic et al. (2025) 119 acl-2025 2025.sdp-1.32 Carla and Uban (2025) Zihao Wang, Haowei Lin, Ruilin Yan, Xiangyu Wang, Jiaqi Li, Weiye Shi, Xiaojian Ma, Anji Liu, Yitao Liang, et al. 2024e. Optimizing inference-time reasoning in llms via retrieval-augmented reflection. arXiv preprint arXiv:2403.05313. Mingyuan Zhang, Jiayi Wang, and et al. 2024a. Retrieval-augmented icd coding: two-stage system beats vanilla llms by 94 arXiv preprint arXiv:2401.12345. Anya Belz, Simon Mille, and Craig Thomson. 2025b. taxonomy of quality criterion names and definitions for evaluating nlp systems in terms of standard comparable aspects of quality. Elizabeth Salesky, Ramon Sanabria, Alan W. Black, and Florian Metze. 2021. Exploring low-resource speechto-text translation across modalities. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics, pages 11341145. Wei Zhang, Li Chen, and Nicolas Christin. 2022. Dynamic graph learning for cryptocurrency fraud detection. In IEEE International Conference on Blockchain and Cryptocurrency, pages 19. Nikhil Stoehr, Kha Pham Doan, and Louis-Philippe Morency. 2023. Localizing memorization in transformer language models. arXiv preprint arXiv:2310.01331. Robert Shelby, Carl Vondrick, and 1 others. 2023. Can llms be safely released? evaluating the impact of red teaming on language model behavior. arXiv preprint arXiv:2304.10685. Rohit Gupta, Ankit Kumar, and Rahul Singh. 2022. Crisismm: framework for multimodal crisis response. arXiv preprint arXiv:2203.01234. Zhijing Jin, Zhiheng Lyu, Yiwen Ding, Mrinmaya Sachan, Kun Zhang, Rada Mihalcea, and Bernhard Schoelkopf. 2022. AI Scholars: dataset for NLPinvolved causal inference. Harsha Nori, Andrew He, Chenguang Wang, Po-Sen Huang, Yejin Yang, Hao-Tong Tung, Shashank Singh, Hossein Hosseini, Mark Hughes, Matei Zaharia, and 1 others. 2023. Can language models teach weaker agents? training reasoning teachers that can explain their actions. arXiv preprint arXiv:2311.10731. Kai Sun, Rui Zhang, and Xiang Ren. 2022. FS-NER: few-shot joint entity and relation extraction framework. In Findings of the Association for Computational Linguistics: ACL 2022, pages 42924303, Dublin, Ireland. Association for Computational Linguistics. Xiang Deng, Han Zhang, Wenhao Yu, Clare Lee, and Mohit Bansal. 2024. Factscore 2.0: Dualencoder contrastive pre-training for factual consistency. Transactions of the Association for Computational Linguistics, 12:119."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "42 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 120 acl-2025 2025.semeval-1. 121 acl-2025 2025.semeval-1.135 Mondal and Sarkar (2025) Vaidyanathan et al. (2025b) 122 acl-2025 2025.semeval-1. Kesanam et al. (2025) 123 acl-2025 2025.semeval-1.151 Alberts et al. (2025) 124 acl-2025 2025.semeval-1. Matheny et al. (2025) 125 acl-2025 2025.semeval-1.231 Khatoon et al. (2025) 126 acl-2025 2025.semeval-1. Nadeem et al. (2025) 127 acl-2025 2025.semeval-1.250 Aryal and Pant (2025) 128 acl-2025 2025.semeval-1. Wan et al. (2025) 129 acl-2025 2025.semeval-1.268 Chen (2025) 130 acl-2025 2025.semeval-1. et al. (2025b) 131 acl-2025 2025.semeval-1.278 Huang and Cui (2025) 43 Xiao Tan and Yuan Jiang. 2021. Exploring idioms in question answering systems. In Proceedings of the 2021 Conference of the North American Chapter of the ACL. Pranav Goel, Shrey Agarwal, and Amir Hussain. 2021. Emotion intensity regression using transformers and loss function optimization. In ACL 2021 Findings, pages 14321440. Shailza Choudhary and Tanmoy Chakraborty. 2020. hybrid feature extraction approach for multi-label emotion classification. IEEE Transactions on Affective Computing. Jian Wu, Junwei Xie, Bing Tang, Hongyan Yan, and Lijun Wang. 2022. Dense passage retrieval: study of the representations of multiple sentences in passage. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL 2022). Silvio Ricardo Cordeiro, Aline Villavicencio, Marco Idiart, and Carlos Ramisch. 2019. computational model of nominal compound compositionality. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages 27622773. Association for Computational Linguistics. Vered Shwartz and Ido Dagan. 2018. sequential neural model for multiword expression identification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT). Frank Acheampong, Henry Nunoo-Mensah, and Wei Chen. 2020. Transformer models for text-based emotion detection: comparative analysis. arXiv preprint arXiv:2004.13704. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. arXiv preprint arXiv:1812.01813. Danqi Chen, Pengcheng Xie, Shiyu Wang, et al. 2020. Tabfact: large-scale dataset for table-based fact verification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL). Min Li, Liangyou Li, and Philipp Koehn. 2022. Lowfrequency entity translation in neural machine translation. In Findings of the Association for Computational Linguistics: ACL 2022, pages 30213033. S. Kumar et al. 2023. Sentilex: Enhancing emotion classification with lexicon-based methods. In ACL Workshop on Sentiment Analysis. Linhao Zhang, Shuai Wang, and Bing Liu. 2018. Learning to detect multi-label emotions in tweets with label co-occurrence and dependency. In Proceedings of the 27th International Conference on Computational Linguistics, pages 32993309."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 132 acl-2025 2025.semeval-1.37 Cui (2025) 133 acl-2025 2025.sicon-1.12 Tiwari (2025) 134 acl-2025 2025.sigtyp-1.15 Li and Ke (2025b) 135 acl-2025 2025.trl-1.16 Li (2025c) 136 acl-2025 2025.unlp-1.10 Kyslyi et al. (2025a) 137 acl-2025 2025.unlp-1.12 Kyslyi et al. (2025b) 138 acl-2025 2025.woah-1.21 Boudraa et al. (2025) 139 acl-2025 2025.woah-1.36 Schirmer et al. (2025) 140 acl-2025 2025.xllm-1.25 Pham Hoang Le et al. (2025) 141 acl-2025 2025.xllm-1.3 Li et al. (2025k) 142 emnlp-2025 2025.arabicnlp-main.20 Bouchekif et al. (2025) 143 emnlp-2025 2025.babylm-main. Fusco et al. (2025) 144 emnlp-2025 2025.babylm-main.36 Yoshida et al. (2025) 44 Yuqing Zhang, Yansong Feng, Yinwen Zhang, and Shujie Liu. 2021. Attention-based multi-label emotion classification with label-wise attention mechanism. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics (ACL 2021), pages 59435952. Yixuan Zhang, Leo Cheng, Lichang Zhang, Lu Liu, Jingcheng Li, Haoning Liu, and Xu. 2024. Alert: comprehensive safety benchmark suite for large language models. Preprint, arXiv:2402.12441. Kevin Zhang and 1 others. 2022. Code meets prose: Syntactic patterns in cryptocurrency whitepapers. In LAW@ACL. Manual analysis of 200 whitepapers showing SVO/SOV mixing. Haoyang Ding, Zijian Liu, Xiao Chen, Lidong Bing, and Wai Lam. 2021. Financial table processing with pretrained language models. EMNLP. Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Ahmed Ali, and Stephan Vogel. 2017. Arabic dialect identification for the dsl 2017 shared task. In Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial). Firoj Alam, Shaden Shaar, Alex Nikolov, and 1 others. 2021. survey on nlp for fake news detection. Computational Linguistics, 47(4):905960. Hande Kartal, Dilek Hakkani-Tür, and Gokhan Tur. 2022. Span-based detection of biased statements in news articles. In Proceedings of the 2022 Conference on Computational Linguistics. COLING. Xing Jiang, Hugo Touvron, Ludovic Denoyer, Hervé Jégou, and Guillaume Lample. 2023. Mistral 7b: small model that thinks big. arXiv preprint arXiv:2310.06825. Micha Livne, Roberto Dessì, and Douwe Kiela. 2023. How to train large language model to be reliable information extraction system. Computing Research Repository, arXiv:2309.16396. Version 3. Yujia Zhang, Xiaobo Liu, and Zhenhua Li. 2020. Graphbased hierarchical relationships for text representation. arXiv preprint arXiv:2007.12345. Faiza Qamar, Seemab Latif, and Rabia Latif. 2024. benchmark dataset with larger context for non-factoid question answering over islamic text. arXiv preprint arXiv:2409.09844. Leonie Weissweiler, Valentin Hofmann, David Mortensen, and Janet Pierrehumbert. 2023. Evaluating morphological generalization with wug tests. arXiv preprint. ArXiv:230x.xxxxx. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Preprint, arXiv:1901.04513."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 145 emnlp-2025 2025.codi-1.4 Jeon and Strube (2025) 146 emnlp-2025 2025.codi-1. Ahmad and Kakudi (2025) 147 emnlp-2025 2025.emnlp-demos.24 Jang et al. (2025) 148 emnlp-2025 2025.emnlp-demos.27 Liu et al. (2025f) 149 emnlp-2025 2025.emnlp-demos. Chao et al. (2025) 150 emnlp-2025 2025.emnlp-industry.123 Khabiri et al. (2025) 151 emnlp-2025 2025.emnlp-industry.146 Zhang (2025) 152 emnlp-2025 2025.emnlp-industry.168 Ethiraj et al. (2025) 153 emnlp-2025 2025.emnlp-industry.182 Yu et al. (2025b) 154 emnlp-2025 2025.emnlp-industry.187 Beyranvand et al. (2025) 155 emnlp-2025 2025.emnlp-industry.189 Chawla et al. (2025) Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. BERT: study of attention in BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 57175726, Hong Kong, China. Association for Computational Linguistics. Yuan Lan, Chao Huang, Wayne Xin Zhao, and Jun Li. 2024. Cola: Collaborative role infused multiagent debate framework for stance detection. arXiv preprint arXiv:2403.01234. Rong Chen, Siyun Zhang, Yiyi Zheng, Qiuhua Yu, and Chu-huai Wang. 2025. Enhancing treatment decision-making for low back pain: novel framework integrating large language models with retrieval-augmented generation technology. arXiv preprint arXiv:2501.34567. Jihan Zhu, Zhizheng Lin, Jun Gao, Zhaoxuan Zhou, Yaodong Zhang, Zhaofan Liu, Ceyao Zheng, Cheng Li, Zhaokai Wang, Zili Wang, and 1 others. 2024. survey of ai agent evaluation: hundred unsolved problems and one-stop open-source library. arXiv preprint arXiv:2406.09844. Jinhao Liang, Yihang Chen, Haozheng Zhang, and 1 others. 2025. Surveyx: An end-to-end solution for automated survey generation. arXiv preprint arXiv:2501.12345. Takeshi Kojima, Shinn Yao, Jinyi Zhao, Xiang Ren, et al. 2023. ToolLLM: Facilitating LLMs to master 16000+ real-world APIs. arXiv preprint arXiv:2312.11568. Emmanouil Panousis and Tom Rainforth. 2022. principled and unified framework for bayesian optimal experimental design. arXiv preprint arXiv:2202.04647. Zhen Li and 1 others. 2023. Towards general text embeddings with mixture of experts. Preprint, arXiv:2311.05723. https://arxiv.org/abs/2311.05723. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. 2025. Adaptthink: Llm can learn when to think. https://arxiv.org/abs/2501.12570. Jun Xu, Fan Wang, and Shuo Chen. 2022. Automated question-answering for vehicle manuals using nlp and knowledge graphs. IEEE Transactions on Intelligent Transportation Systems. Yifan Liu, Jiaqi Wang, and Zhi Zhang. 2024. Bias in ai investment advice: An analysis of home bias in large language models. arXiv preprint arXiv:2209.04538."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "45 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 156 emnlp-2025 2025.emnlp-industry.69 Azeez et al. (2025) 157 emnlp-2025 2025.emnlp-industry.70 Swamy et al. (2025) 158 emnlp-2025 2025.emnlp-industry.8 Bougie and Watanabe (2025) 159 emnlp-2025 2025.emnlp-industry.96 Joshi et al. (2025a) 160 emnlp-2025 2025.emnlp-main.1021 Pi et al. (2025) 161 emnlp-2025 2025.emnlp-main.1070 Zhang et al. (2025d) 162 emnlp-2025 2025.emnlp-main.1097 Zhang and Zhou (2025) 163 emnlp-2025 2025.emnlp-main.1119 Zhang et al. (2025e) 164 emnlp-2025 2025.emnlp-main.1167 Zhang et al. (2025b) 165 emnlp-2025 2025.emnlp-main.1190 Yi et al. (2025) 166 emnlp-2025 2025.emnlp-main.1232 Nyandwi et al. (2025) 167 emnlp-2025 2025.emnlp-main.1235 Rafiei Asl et al. (2025) 168 emnlp-2025 2025.emnlp-main.1337 Dumitru et al. (2025) Juho Kim et al. 2023. Does gpt-4 pass the bar exam? case study on the performance of large language models on legal multiple choice questions. In Proceedings of the 2023 Conference of the North American Chapter of the Association for Computational Linguistics. Florian Schuster et al. 2023. Getml: Pareto-based feature learning for machine learning applications. arXiv preprint arXiv:XXXX.XXXXX. Anonymous and 1 others. 2023b. Is llm reliable reviewer? comprehensive evaluation of llm on reviewrevision multiple-choice questions. ACL Anthology 2024.lrec-main.816. Siva Reddy, Daniel Khashabi, Benjamin Lo, Chandra Bhagavatula, Maarten Sap, and Yejin Choi. 2023. Do large language models understand instructions? arXiv preprint arXiv:2301.01193. Ziyang Xiong et al. 2024. Llava-critic: Visual instruction tuning with feedback. arXiv preprint arXiv:2402.12345. Yiming Zhang and Zhouhui Lian. 2024a. Brush your text: Enhancing text rendering in diffusion models with attention map interventions. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL). X. Li, P. Wang, and Y. Chen. 2020. Iterative transformer: Recurrent attention updates for sequence modeling. arXiv preprint arXiv:2010.02536. Chenyan Yu, Lee Xiong, and Jamie Callan. 2022. survey on conversational dense retrieval: Methods and challenges. arXiv preprint arXiv:2201.08452. Zhengbao Jiang, Frank Xu, Jun Araki, and Graham Neubig. 2024. Flare: Active retrieval augmentation for hallucination mitigation. arXiv preprint arXiv:2401.07019. Bogdan Gliwa, Iwona Mochol, Michał Biesek, and Aleksander Wawer. 2019. Samsum corpus: human-annotated dialogue summary dataset. arXiv preprint arXiv:1911.12237. Wenxuan Zhang, Da Yin, Zhong Ding, and Radu Soricut. 2023. M3exam: multilingual, multimodal exam-style qa benchmark. arXiv preprint arXiv:2306.05179. Mark Russinovich, Jingwen Cai, Yifan Hou, Yuxiao Jiang, Yiping Jiang, Jieyu Kang, Prithviraj Kang, Urvashi Khandelwal, Nitish Khurana, Pang Wei Koh, et al. 2024. Crescendo: Towards realistic redteaming of llms with language agents. arXiv preprint arXiv:2402.13249. Jia Chen and Hao Xu. 2023. Parallel decoding with speculative sampling for large language models. arXiv preprint arXiv:2306.15478."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "46 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 169 emnlp-2025 2025.emnlp-main.1345 Park et al. (2025) 170 emnlp-2025 2025.emnlp-main.138 Li et al. (2025b) 171 emnlp-2025 2025.emnlp-main.1429 Bohdal et al. (2025) 172 emnlp-2025 2025.emnlp-main.145 Borah et al. (2025) 173 emnlp-2025 2025.emnlp-main.1455 Kryklyvets et al. (2025) 174 emnlp-2025 2025.emnlp-main.1457 Srivastava (2025) 175 emnlp-2025 2025.emnlp-main.1466 Roh et al. (2025) 176 emnlp-2025 2025.emnlp-main.1521 Gallifant et al. (2025) 177 emnlp-2025 2025.emnlp-main.1538 Jin et al. (2025) 178 emnlp-2025 2025.emnlp-main.1556 Dhanraj and Eliasmith (2025) 179 emnlp-2025 2025.emnlp-main.1558 Song et al. (2025a) 180 emnlp-2025 2025.emnlp-main.1582 Liu et al. (2025e) 181 emnlp-2025 2025.emnlp-main.1611 Yuan et al. (2025b) Rodrigo Nogueira and Jimmy Lin. 2019. From doc2query to doctttttquery. arXiv preprint arXiv:1904.08375. Weixi Tong and Tianyi Zhang. 2024. Codejudge: Evaluating code generation with large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 2003220051. Aliaksei Severyn, Eric Emil Malmi, Jonathan Stephen Mallinson, Sascha Rothe, and Sebastian Krause. 2021. Grammatical error correction using large multilingual language models. In ACL. Junnan Li, Ziqing Wang, Yilun Xu, et al. 2024. Alignment Degradation in LLMs: Measuring, Visualizing, and Mitigating Latent Drift. arXiv preprint arXiv:2401.04200. S. Thoppilan and et al. 2023. Gemini: unified multimodal large language model. arXiv preprint arXiv:2303.06601. Wei Xu, Yulia Tsvetkov, and Alan Black. 2022. AI for language learning: Conversational agents and personalized feedback. Transactions of the Association for Computational Linguistics (TACL), 10:115. Amanpreet Singh, Yujia Wang, Yulia Tsvetkov, and Percy Liang. 2024. Global-mmlu: Evaluating cultural and linguistic biases in multilingual language understanding. arXiv preprint arXiv:2412.03304. Trenton Bricken, Jonathan Marcus, Siddharth MishraSharma, Meg Tong, Ethan Perez, Mrinank Sharma, Kelley Rivoire, and Thomas Henighan. 2023. Towards monosemanticity: Decomposing language models with dictionary learning. ArXiv, abs/2309.08600. Liang Liu, Marc Brockschmidt, Vivek Murali, and et al. 2024. The meta llm compiler: suite of opensource models for code optimization. arXiv preprint arXiv:2407.02524. Jianfeng Wang, Fei Huang, and 1 others. 2023. Efficient fine-tuning of large language models with lora. arXiv preprint arXiv:2303.01234. Mor Geva, Tal Schuster, Jonathan Berant, and Omer Levy. 2023. Token recycling: Making llms faster and more data-efficient. arXiv preprint arXiv:2310.02548. Sabine Ploux and Hong Ji. 2003. Semantic spaces: Modeling the distribution of word senses with unsupervised learning techniques. Computational Linguistics, 29(2):259275. Laura Banarescu, Claire Bonial, Sheila Condon, Emily Faries, Jon Niekrasz, and Tim Connor. 2018. Amr for multi-document summarization. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 577583."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "47 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 182 emnlp-2025 2025.emnlp-main.1629 Sadia et al. (2025) 183 emnlp-2025 2025.emnlp-main.164 Xiao et al. (2025) 184 emnlp-2025 2025.emnlp-main.1641 Kassem et al. (2025) Srivatsan Sundar, Dhruv Jain, Yuwei Zhang, and H. V. Jagadish. 2024. gtbls: Generating tables from text by learning table structures. In Proceedings of the 2024 Conference of the Association for Computational Linguistics. Kai Jiang, Hao Liu, and Rodney Brooks. 2022. Trustperformance paradox in industrial hri: Empirical evidence from warehouse robotics. arXiv preprint arXiv:2208.14637. Baolin Peng and 1 others. 2024. Efficient model editing at scale. arXiv preprint arXiv:2401.05911. 185 emnlp-2025 2025.emnlp-main.1648 Mohammadi et al. (2025) Swaroop Mishra, Daniel Khashabi, Chitta Baral, 186 emnlp-2025 2025.emnlp-main.1777 Gao et al. (2025) 187 emnlp-2025 2025.emnlp-main.1806 He et al. (2025b) 188 emnlp-2025 2025.emnlp-main.192 Liu et al. (2025d) 189 emnlp-2025 2025.emnlp-main.207 He et al. (2025a) 190 emnlp-2025 2025.emnlp-main.319 Li et al. (2025f) and Hannaneh Hajishirzi. 2022. Cross-task generalization via natural language instructions. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 34703487, Dublin, Ireland. Association for Computational Linguistics. Jack FitzGerald, Ryan Cotterell, Bahadir Avci, Sebastian Ruder, Graham Neubig, Melvin Johnson, and Orhan Firat. 2022. Massive: 51-language multilingual dataset for task oriented semantic parsing. arXiv preprint arXiv:2204.08582. A. Kirstain and 1 others. 2023. Pick-a-pic: dataset for evaluating the robustness of vision-language models. Preprint, arXiv:2305.01569. Rishi Taori et al. 2023. Alpaca: 175b-parameter model for instruction following. ArXiv preprint arXiv:2303.11366. Shuang Chen, Yang Liu, and Erik Cambria. 2023. Knowledge-enhanced aspect-based sentiment analysis with external commonsense graphs. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023), pages 1356713579. Association for Computational Linguistics. Kevin Lin, Ben Tan, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. 2019b. Ropes: Reading comprehension over paragraphs. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 191 emnlp-2025 2025.emnlp-main.348 Khandelwal et al. (2025) Alex Mallen, Zexuan Zhang, Xinyu Liu, Yichong Wu, Yiming Zhang, and William Yang Wang. 2023b. When not to trust language models: Investigating effectiveness of parametric and retrieval-based knowledge. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12341245. Todor Mihaylov and 1 others. 2018. Openbookqa: Factual knowledge assessment in question answering. EMNLP."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "192 emnlp-2025 2025.emnlp-main.394 Xu et al. (2025c) 48 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 193 emnlp-2025 2025.emnlp-main.397 Zheng et al. (2025a) 194 emnlp-2025 2025.emnlp-main.468 Tang et al. (2025b) 195 emnlp-2025 2025.emnlp-main.482 Tanjim et al. (2025) 196 emnlp-2025 2025.emnlp-main.501 Wang et al. (2025f) Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021b. Efficient routing transformers: Dynamic token interaction models for natural language processing. arXiv preprint arXiv:2003.05997. Justin Fu, Kelvin Zhang, Utkarsh Sanyal, Lantao Yu, Collin Moses, Fan Yang, Stefano Ermon, and Zhibin Zhao. 2023. Driving with reasoning: Reinforcement learning with generalist language models for interpretable policies. arXiv preprint arXiv:2303.00745. Meta AI. 2024. Llama 3.2: Advancing open-weight language models. ArXiv preprint arXiv:2408.00001. Xinyu Fan, Li Zheng, Bing Yu, Yifan Liu, Yichi Zhang, Yu Zhang, Hao Tang, Yikai Wang, Weinan Chen, Yizhou Wang, and 1 others. 2023. Urban identityaware geo-reasoning in street views. arXiv preprint arXiv:2311.16456. 197 emnlp-2025 2025.emnlp-main.524 Saba and Skiena (2025) Markus Freitag, Yaser Al-Onaizan, Shuo Sun Ma, and 1 others. 2022a. High-quality low-resource machine translation: new benchmark. In Findings of EMNLP. Wei Zhang, Ming Li, Hao Wang, and Yang Liu. 2024b. Deepseekmath: Scalable math pre-training and group relative policy optimization for mathematical reasoning. arXiv preprint arXiv:2402.03300. Salah Lhoussaine, Georgios Tziantzioulis, Michael B. Sullivan, Vilas Sridharan, Nathan DeBardeleben, Christian Engelmann, and Simranjit Singh. 2024. first look at bfloat16 soft-error resilience in large language models. Preprint, arXiv:2412.07192. Harsha Nori, Allison Webster, Matthew McInnis, et al. 2023b. Medprompt: Large language models are reasoning engines with structured prompts. Preprint, arXiv:2307.13880. Szymon Tworkowski, Konrad Przybysz, Tomasz Korbak, Pedro Rodriguez, Wojciech Rozemłyn, Kamil Rakowski, Maciej Grzelak, Albert Webson, Maciej Szafraniec, Robin Sorsch, and 1 others. 2024. Mambaformer: Efficient language modeling with selective state spaces and attention. arXiv preprint arXiv:2312.00752. DeepSeek. 2025. Deepseek r1: Towards deep reinforcement learning for language models. arXiv preprint arXiv:2501.12948. Yifan Xu, Xiaoyu Liu, Baolin Peng, and et al. 2024. Evaluating the instruction-following robustness of large language models. In Proceedings of EMNLP 2024. Jing Qian and et al. 2018. Neural network-based fake news detection: Learning to identify deceptive content. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "198 emnlp-2025 2025.emnlp-main.527 Yu et al. (2025a) 199 emnlp-2025 2025.emnlp-main.528 Chen et al. (2025f) 200 emnlp-2025 2025.emnlp-main.548 Wang et al. (2025a) 201 emnlp-2025 2025.emnlp-main.562 Shihab et al. (2025) 202 emnlp-2025 2025.emnlp-main.584 Wang et al. (2025d) 203 emnlp-2025 2025.emnlp-main.614 Lv et al. (2025) 204 emnlp-2025 2025.emnlp-main.619 Tian et al. (2025) 49 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 205 emnlp-2025 2025.emnlp-main.637 Kang et al. (2025) 206 emnlp-2025 2025.emnlp-main.789 Kim et al. (2025a) 207 emnlp-2025 2025.emnlp-main.813 Zhang et al. (2025f) 208 emnlp-2025 2025.emnlp-main.830 Liu et al. (2025a) 209 emnlp-2025 2025.emnlp-main.831 Liu et al. (2025b) 210 emnlp-2025 2025.emnlp-main.835 Luo et al. (2025a) 211 emnlp-2025 2025.emnlp-main.841 Rivlin-Angert and Mor-Lan (2025) 212 emnlp-2025 2025.emnlp-main.848 Nafee et al. (2025) 213 emnlp-2025 2025.emnlp-main.853 Mundada et al. (2025) 214 emnlp-2025 2025.emnlp-main.855 Jiang et al. (2025) 215 emnlp-2025 2025.emnlp-main.858 Trifan et al. (2025) 216 emnlp-2025 2025.emnlp-main.880 Wang et al. (2025c) 217 emnlp-2025 2025.emnlp-main.938 Chen et al. (2025a) Hang Yu, Tianle Li, and et al. 2025. Dapo: Data-aware policy optimization for open-source llm alignment. arXiv preprint arXiv:2503.04111. Mor Geva, Daniel Khashabi, Elad Segal, Jonathan Berant, and Ido Dagan. 2021. Aristotle: dataset for focused logical reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 114, Punta Cana, Dominican Republic. Association for Computational Linguistics. Mark Finlayson and 1 others. 2023. Causal analysis of language model behavior with induced interventions. arXiv preprint arXiv:2301.12928. Yixin Nie, Jing Wang, Mohit Bansal, and Kai-Wei Chang. 2020. Adversarial natural language inference. In Proceedings of ACL. Paola Espinosa, Sarah Kadi, Stefanie Kamps, and et al. 2017. Gender differences in empathy-related processes: Exploring behavioral and neurophysiological correlates. Psychoneuroendocrinology, 85:3443. Xuezhe Li and Xiaodan Sun. 2020. Dice loss for dataimbalanced nlp tasks: Application to named entity recognition. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 46534661. David Hebron, Avi Shmidman, and Moshe Koppel. 2023. Hero: hebrew roberta model for hebrew nlp tasks. Preprint, arXiv:2309.12345. Linyong Lu, Qingxiu Zhang, Xiang Li, and Jingjing Liu. 2022. Promptpg: Prompting for policy gradient training. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP). Lukasz Czajka, Malihe Alikhani, Patrick Verga, and Yonatan Belinkov. 2024b. Musictheory-bench. ArXiv preprint arXiv:2410.02084. Victor Zhong, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi, and Noah Smith. 2022. Analytical reasoning of text: Unifying machine reading and logical reasoning. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 23072323. Liang Pan, Lei Zhang, and Zhenzhong Yang. 2023. Can chatgpt do zero-shot dialogue state tracking? In arXiv preprint arXiv:2302.01180. Weijie Chen, Yizhe Zhang, Qian Wu, and 1 others. 2024. Internvl: Scaling up vision-language pretraining with multimodal reinforcement learning. arXiv preprint arXiv:2402.00028. Xiaoxi Zhong et al. 2023. Agentverse: Facilitating multi-agent collaboration and exploration with llms. arXiv preprint arXiv:2309.07864."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "50 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 218 emnlp-2025 2025.emnlp-main.952 Zhou et al. (2025b) Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, and 1 others. 2024. Minicpm: Unveiling the potential of small language models with warmup-stable-decay learning rate scheduler. arXiv preprint arXiv:2404.06395. 219 emnlp-2025 2025.emnlp-main.986 Bhendawade et al. (2025) Yi Tay, Dara Bahri, Donald Metzler, et al. 2022. Scale efficiently: Insights from training and scaling large language models. arXiv preprint arXiv:2210.03863. H. Yang and K. Li. 2023. Boostaug: Hybrid instance filtering framework for boosting text augmentation. In Findings of the Association for Computational Linguistics: ACL 2023. Xianzhi Li, Zihang Dai, and 1 others. 2022. Branchtuning: Efficient fine-tuning of pre-trained vision models via branching. arXiv preprint arXiv:2202.06924. Rodrigo Gomez et al. 2018. Ciphergan: Unsupervised cipher cracking using gans. arXiv preprint arXiv:1801.04883. Mo Yu, Yisi Sang, Kangsheng Pu, Zekai Wei, Han Wang, Jing Li, and Jie Zhou. 2022. Character understanding in movies: benchmark for movie character analysis. arXiv preprint arXiv:2211.04684. Zhen Chiang, Noah Shinn, Siyuan Zhuang, and 1 others. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. Preprint, arXiv:2304.05335. Suyeon Lee, Sunghwan Kim, et al. 2024b. Counselingeval: Towards evaluating the quality of llmbased psychological counseling. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL). X. Du et al. 2022a. Grit1: grammar error correction dataset for llm evaluation. In Proceedings of EMNLP 2022. X. Du et al. 2022b. Grit2: Extending grammar error correction for multilingual llms. In Findings of EMNLP 2022. OpenAI. 2023a. Gpt-4: Openais generative pre-trained transformer 4 model. Preprint, arXiv:arXiv:2301.00000. Baptiste Roziere, Gautier Izacard, Jan Leike Botha, and Others. 2023. Code LLaMA: Large language models for code. Preprint, arXiv:2308.08545. Damai Bi, Kunhao Lv, Xiang Ji, Yining Wang, Zecheng Lyu, Zihan Du, et al. 2024. Deepseek: Advancing ai through enhanced language models. arXiv preprint arXiv:2401.14196. Aditya Maharana and 1 others. 2024. Locomo: Long context modeling benchmark for conversational agents. arXiv preprint arXiv:2402.13968."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "220 emnlp-2025 2025.findingsKim et al. (2025c) emnlp.1052 221 emnlp-2025 2025.findingsAastik et al. (2025) emnlp. 222 emnlp-2025 2025.findingsMaskey et al. (2025) emnlp.1082 223 emnlp-2025 2025.findingsYang et al. (2025a) emnlp. 224 emnlp-2025 2025.findingsWang et al. (2025b) emnlp.1118 225 emnlp-2025 2025.findingsBn et al. (2025) emnlp. 226 emnlp-2025 2025.findings-emnlp.118 Zeng et al. (2025) 227 emnlp-2025 2025.findingsZheng et al. (2025c) emnlp.1196 228 emnlp-2025 2025.findingsKim et al. (2025f) emnlp.1286 229 emnlp-2025 2025.findingsSaeed et al. (2025) emnlp.1343 230 emnlp-2025 2025.findingsKwak et al. (2025) emnlp.1344 51 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 231 emnlp-2025 2025.findingsAkra et al. (2025) emnlp.1359 232 emnlp-2025 2025.findingsJi and Lu (2025) emnlp.1384 233 emnlp-2025 2025.findings-emnlp.175 Zhu et al. (2025d) Ling Chen, Ming Zhao, and 1 others. 2024. Efficient active learning for biomedical image segmentation with minimal annotations. arXiv preprint arXiv:2405.01701. Yiming Li, Xia Chen, Liang Zhou, and Zheng Wang. 2024b. Hindsight: Selective reflection for robust reasoning. arXiv preprint arXiv:2406.12050. Yujia Hong, Junyang Wu, Fan Zhang, and Shuo Zhang. 2024b. Adaptive agents with code and memory for solving math word problems. arXiv preprint arXiv:2403.01290. 234 emnlp-2025 2025.findings-emnlp.176 Saad-Falcon et al. (2025) Dahyun Koo, Yejin Choi, and Eunsol Choi. 2023. 235 emnlp-2025 2025.findings-emnlp.212 Rafiuddin and Khan (2025) 236 emnlp-2025 2025.findings-emnlp.281 Zhang et al. (2025c) 237 emnlp-2025 2025.findings-emnlp.331 Fan et al. (2025) 238 emnlp-2025 2025.findings-emnlp.354 LeVi et al. (2025) 239 emnlp-2025 2025.findings-emnlp.379 Li et al. (2025i) 240 emnlp-2025 2025.findings-emnlp.391 Luo et al. (2025b) 241 emnlp-2025 2025.findings-emnlp.435 Nguyen et al. (2025a) Cognitive Biases in Large Language Models as Evaluators. ArXiv Preprint arXiv:2312.05441. Tim Dettmers, Yannic Kilcher, Henry Minsky, Anna McDowell, Neha Nangia, Andreas Vlachos, and the Microsoft Phi Team. 2025. Phi-4-mini: Compact yet powerful multimodal models. arXiv preprint arXiv:2503.01743. Yinlin Liu, Pengcheng Yin, and Graham Neubig. 2019. Conala: The code/natural language challenge. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP). Yuyang Huang and Clark Glymour. 2016. survey of causal discovery and causal inference. arXiv preprint. Q. Zhang and 1 others. 2025. Gradient-free adversarial attacks on llms: Transferability and optimization. . This paper primarily focuses on gradient-free attacks on LLMs but includes gradient-based attacks, with transferability in black-box settings using surrogate models. It does not emphasize jailbreaking or Greedy Coordinate Gradient (GCG), making it less aligned with the texts focus. Wang et al. (2025, ) on gradient-based jailbreaking with GCG is more precise alternative. Yinhan Tang, Xavier Garcia, Alessandro Raganato, Vishrav Chaudhary, and Jiatao Gu. 2022. An efficient multilingual byte-to-byte model for sequenceto-sequence tasks. In Findings of the Association for Computational Linguistics: EMNLP 2022. Wenbin Xiong, Zhan Liu, Yihan Yang, Ge Yu, Ge Zhang, and Tian Song. 2023. survey on hyperrelational knowledge graphs: From data models to applications. arXiv preprint arXiv:2301.03869. Yi Cheng, Yilun Wu, Adrian Weller, Matt J. Kusner, and Pushmeet Kohli. 2024. Integrative decoding: Improving factuality in large language model generation via implicit self-consistency. In International Conference on Learning Representations."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "52 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 242 emnlp-2025 2025.findings-emnlp.437 Cai et al. (2025) 243 emnlp-2025 2025.findings-emnlp.474 Ghosh et al. (2025) 244 emnlp-2025 2025.findings-emnlp.517 Baali et al. (2025) 245 emnlp-2025 2025.findings-emnlp.523 Mor-Lan et al. (2025) 246 emnlp-2025 2025.findings-emnlp.534 Liu and Roth (2025) 247 emnlp-2025 2025.findings-emnlp.645 Zhou et al. (2025d) 248 emnlp-2025 2025.findings-emnlp.657 Bhattacharya et al. (2025) 249 emnlp-2025 2025.findings-emnlp.709 Zhang et al. (2025a) 250 emnlp-2025 2025.findings-emnlp.751 Xiang et al. (2025) 251 emnlp-2025 2025.findings-emnlp.834 Jalori et al. (2025) 252 emnlp-2025 2025.findings-emnlp.863 Kellert et al. (2025) Wenhui Wang, Li Dong, Hao Cheng, Furu Wei, and Ming Zhou. 2022. Minilmv2: Multi-task pre-training for multi-task all-purpose text representations. In Findings of the Association for Computational Linguistics: ACL 2022, pages 29072918. Yankai Lin, Jiapeng Zhou, Yiming Shen, Wenxuan Zhou, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2021. Xcsqa: benchmark for cross-lingual conversational question answering. In EMNLP. Jue Gao, Tri Dang, Michael Seltzer, and Rif Saurous. 2022. Conversasynth: Exploring the landscape of synthetic conversations for audio understanding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 65436558. David Hebron, Avi Shmidman, and Moshe Koppel. 2023. Hero: hebrew roberta model for hebrew nlp tasks. Preprint, arXiv:2309.12345. Ellie Pavlick and Joel Tetreault. 2016. Semantically motivated future directions in linguistic ambiguity detection. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). Sho Kurita, Jonas Pfeiffer, Ivan Vulic, Edoardo Ponti, and Anna Korhonen. 2020. weighted approach to unsupervised multilingual transformer fine-tuning. In Proc. of AACL-IJCNLP. Yifei Li, Zhou Zhao, Xiaohan Yu, and Deng Cai. 2023. Multimodal uncertainty estimation for deep learning models. arXiv preprint arXiv:2304.02637. E. Zaken, Y. Goldberg, and S. Ravfogel. 2022. Bitfit: Simple parameter-efficient fine-tuning for transformers. Transactions of the Association for Computational Linguistics (TACL), 10:116. VN Ioannidis, Song, Manchanda, Li, Pan, Zheng, Ning, Zeng, and Karypis. 2022. Drkg-drug repurposing knowledge graph for covid19. 2020. arXiv preprint arXiv:2010.09600, pages 113. Wendi Zhou, Xiao Li, Lin Geng Foo, Yitan Wang, Harold Soh, Caiming Xiong, and Yoonkey Kim. 2024. TEMPO: Temporal representation prompting for large language models in time-series forecasting. arXiv preprint arXiv:2405.18384. Anticipated for NeurIPS 2024. Preprint, arXiv:2405.18384. Shruti Rijhwani, Lawrence Wolf-Sonkin, Victor Kuperman, Timothy Baldwin, and Thamar Solorio. 2017. Analyzing code-switched social media text. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "53 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 253 emnlp-2025 2025.findings-emnlp.864 Jia et al. (2025b) 254 emnlp-2025 2025.findings-emnlp.866 Li et al. (2025a) 255 emnlp-2025 2025.findings-emnlp.874 Kim et al. (2025e) 256 emnlp-2025 2025.findings-emnlp.902 Xu et al. (2025d) 257 emnlp-2025 2025.findings-emnlp.914 Li et al. (2025h) 258 emnlp-2025 2025.findings-emnlp.932 Jakob et al. (2025) 259 emnlp-2025 2025.findings-emnlp.943 Akhondzadeh et al. (2025) 260 emnlp-2025 2025.findings-emnlp.954 Son et al. (2025) 261 emnlp-2025 2025.findings-emnlp.960 Kim et al. (2025b) Jiawei Chen, Yuanhang He, Yada Zhang, Jiachen Ji, and Jie Tang. 2024. Generate-on-graph: Treat llm as both agent and knowledge graph for incomplete kgqa. arXiv preprint arXiv:2404.14741. Eric Mitchell, Roberta Raileanu, Colin Raffel, John Levine, Yulia Tsvetkov, and Christopher Manning. 2023. Ieval: An instruction following benchmark. arXiv preprint arXiv:2310.07724. Zhiwei Chen, Yichao Lu, and Mingjun Zheng. 2020. Dkt+: Enhanced deep knowledge tracing with regularization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 12341244. Zixuan Wang, Xiaocheng Li, and Yang Liu. 2023. Posterior prompt tuning: Toward faithful and calibrated llms. In Empirical Methods in Natural Language Processing (EMNLP). RangRang Ge, ShiYe Song, Zhaorui Liu, Wei Liu, Yuesheng Wang, Dongling Wang, Bofang Zhou, Zhicheng Dou, and Ji-Rong Wen. 2023. survey on KV cache compression for large language models. arXiv preprint arXiv:2312.10546. Shaina Ashraf, Isabel Bezzaoui, Ionut Andone, Alexander Markowetz, Jonas Fegert, and Lucie Flek. 2024. Defakts: fine-grained dataset for analyzing disinformation in german media. In Proceedings of The 2024 Joint International Conference, on Computational Linguistics, Language Resources and Evaluation, Torino, Italia. European Language Resources Association. Michael Boratko, Harsh Padigela, Deepak Mikkilineni, Pavan Yuvraj, Rajarshi Das, Andrew McCallum, Mihai Chang, Achille Fokoue, Pavan Kapanipathi, Nicholas Mattei, et al. 2018. Arc: machine reading comprehension dataset for reasoning over science text. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 14141423. Jack Hessel, Jingkang Zhao, Ranjay Krishna, Angel Chang, and Yonatan Bisk. 2022. Abductionrules: Leveraging commonsense knowledge and probabilistic reasoning for visual abduction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 14511463. Xudong Liu, Paul Röttger, Johannes Welbl, Yonatan Belinkov, Hila Gonen, Eric Wallace, Samuel R. Bowman, Ryan Cotterell, and Noah A. Smith. 2023a. Harmbench: Measuring the propensity of language models to produce harmful content. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1103311051."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "54 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 262 emnlp-2025 2025.findings-emnlp.967 Eyal et al. (2025) 263 emnlp-2025 2025.findings-emnlp. Yin et al. (2025) 264 emnlp-2025 2025.findings-emnlp.973 Colakoglu et al. (2025) 265 emnlp-2025 2025.finnlp-2.3 George et al. (2025) 266 emnlp-2025 2025.finnlp-2.8 Nitarach et al. (2025) 267 emnlp-2025 2025.hcinlp-1.8 Wang et al. (2025e) 268 emnlp-2025 2025.mathnlp-main.13 Lu et al. (2025) 269 emnlp-2025 2025.mathnlp-main.5 Fatima (2025) 270 emnlp-2025 2025.mathnlp-main.6 Khrulev (2025) 271 emnlp-2025 2025.mathnlp-main.9 Li (2025a) 272 emnlp-2025 2025.mrl-main.12 Moon et al. (2025) 273 emnlp-2025 2025.mrl-main.2 Yuan et al. (2025c) Jackson Petty, Sjoerd van Steenkiste, Ishita Dasgupta, Fei Sha, Dan Garrette, and Tal Linzen. 2024. The impact of depth on compositional generalization in transformer-based neural networks. arXiv preprint arXiv:2310.19956. Alice Oh, Kalpesh Krishna, Eric Wallace, Yichong Zhao, Patrick Lewis, and Antoine Bosselut. 2023. Instructir: Making dense retrievers follow instructions. Preprint, arXiv:2305.14252. Jian Zhong et al. 2020. Docextractor: An end-to-end system for information extraction from forms and receipts. arXiv preprint arXiv:2012.04573. Peter Henderson, Koustuv Sinha, Nicolas AngelardGontier, Nan Rosemary Ke, Geneviève Fried, Ryan Lowe, and Joelle Pineau. 2023. Foundation models for legal reasoning. arXiv preprint arXiv:2307.03557. Xuezhi Ma, Yan Zhou, Yining Wang, and 1 others. 2023. Financialqa: reasoning benchmark for financial question answering. arXiv preprint arXiv:2302.07304. OpenAI. 2023. Gpt-4: large-scale multimodal model. arXiv preprint arXiv:2303.08774. Zhe Chen, Weiyun Zhang, Wen Wang, Yiliang Liu, Zhaoyang Zhang, Jian Wang, Jie Luo, Yu Qiao, and Wenhai Wang. 2024. Internvl 1.5: general visionlanguage model. arXiv preprint arXiv:2404.16821. Jingwen Xin, Zhengying Liu, Yifan Luo, and 1 others. 2025. Deepseek-prover-v2: Scaling natural-language graph-based test time compute for automated theorem proving. arXiv preprint arXiv:2503.11657. Zhen Yuan, Yifan Zhang, Jing Liu, Yuxiang Wang, Jie Zhang, Hanwang Liu, and Tat-Seng Chua. 2024. Fermat: benchmark for evaluating vlms ability in factual error correction of handwritten math solutions. arXiv preprint arXiv:2405.10100. Qwen Team. 2024. Qwen2.5-math: Scaling reasoning in mathematical domains. arXiv preprint arXiv:2409.XXXXX. Model card and technical report. Leonardo Ranaldi, Barry Haddow, and Alexandra Birch. 2025. CrossRAG: Cross-lingual retrieval-augmented generation for knowledge-intensive tasks. arXiv preprint arXiv:2504.03616. Laura Banarescu, Claire Bonial, Sheila Condon, Emily Faries, Jon Niekrasz, and Tim Connor. 2018. Amr for multi-document summarization. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 577583."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "55 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 274 emnlp-2025 2025.mrl-main.20 Rachamalla et al. (2025) NVIDIA. 2025. Llama-nemotron-post-training dataset: comprehensive collection of instruction tuning and alignment data. arXiv preprint arXiv:2505.00949. Shounak Paul, Arpan Mandal, Pawan Goyal, and Saptarshi Ghosh. 2022b. Inlegalbert: Pre trained language models for indian legal texts. arXiv preprint arXiv:2209.06049. Shivansh Nigam, Sarvesh Dubey, Ayush Agarwal, Dhananjay Kumar, and Saket Maheshwary. 2025. Legalseg: Unlocking the structure of indian legal documents. arXiv preprint arXiv:2502.05836. Ghita Houir Alami and 1 others. 2024. Legalbench-rag: benchmark for retrieval-augmented systems in the legal domain. Preprint, arXiv:2408.10343. Barbara Plank. 2022. Human label variation: Challenges and opportunities. Computational Linguistics, 48(4):9991015. esse Dunietz, Sam Thomson, Chris Dyer, and Noah A. Smith. 2020. An interpretable, lexicalized model for implicit event causality. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 17031713. Jinhyuk Kim, Wonjin Kim, Jinhyuk Lee, Joongbo Lee, Kyunghyun Lee, Sunghwan Yoon, and Jaewoo Kang. 2024. Meerkat: medical reasoning benchmark for large language models. arXiv preprint arXiv:2402.00000. Harish Madabushi and 1 others. 2024. Frame-based embeddings for coherent question answering. In Proceedings of EACL. Sumedha Bhan, Aaditya Prabhu, Huaxiu Ma, and Zachary C. Lipton. 2025. Complete textual concept bottleneck models: Addressing concept completeness and classification leakage. arXiv preprint arXiv:2502.12345. Ben Dodd, Betty van Aken, Paul Röttger, and Isabelle Augenstein. 2021. AUTORANK: systematic approach to benchmark and compare machine learning models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 170185, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics Mozilla NLP Team. 2023. Mozilla translations: Opensource neural translation in the browser. In Proceedings of Machine Translation Summit. Yi Zhang, Jialin Li, and Danqi Chen. 2023. Coannotating: Human-ai collaborative annotation via uncertainty-guided task allocation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Junxian Li, Xuezhe Ma, and Eduard Hovy. 2019. Dependency parsing with partial bi-affine attention. In Proceedings of NAACL-HLT."
        },
        {
            "title": "CONTINUED ON NEXT PAGE",
            "content": "275 emnlp-2025 2025.nllp-1.11 Shukla et al. (2025) 276 emnlp-2025 2025.nllp-1.32 Chheda et al. (2025) 277 emnlp-2025 2025.nllp-1.5 Xia et al. (2025) 278 emnlp-2025 2025.nlperspectives-1.11 Zhang and Jaitly (2025) 279 emnlp-2025 2025.starsem-1.24 Kadam and Ferraro (2025) 280 emnlp-2025 2025.starsem-1.29 Gasan and Pais (2025) 281 emnlp-2025 2025.starsem-1. Rai et al. (2025) 282 emnlp-2025 2025.starsem-1.33 Bailleux et al. (2025) 283 emnlp-2025 2025.tsar-1.16 Hayakawa et al. (2025) 284 emnlp-2025 2025.tsar-1. Romero et al. (2025) 285 emnlp-2025 2025.uncertainlpmain.24 Dossou and Aïdasso (2025) 286 emnlp-2025 2025.winlp-main.12 Vinjamuri and Sun (2025) 56 EVENT ID PAPER ID HALLUCITED PAPER EXAMPLE OF HALLUCITATION 287 emnlp-2025 2025.winlp-main.19 Kolli et al. (2025) 288 emnlp-2025 2025.winlp-main.35 and Mahalingam (2025) 289 emnlp-2025 2025.wmt-1.105 Joshi et al. (2025b) 290 emnlp-2025 2025.wmt-1.106 Zhou et al. (2025c) 291 emnlp-2025 2025.wmt-1.108 Zhu et al. (2025c) 292 emnlp-2025 2025.wmt-1.15 Bell et al. (2025) 293 emnlp-2025 2025.wmt-1.43 Pang et al. (2025) 294 emnlp-2025 2025.wmt-1.5 Kim (2025) 295 emnlp-2025 2025.wmt-1.95 Acharya et al. (2025) Yujia Tan, Wenpeng Zhang, Xiang Ren, and Qiji Chen. 2023b. Oe-fact: Open-domain explanation-enhanced fact-checking with large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023. Rahul Aralikatte, Neelamadhav Gantayat, Naveen Panwar, Anush Sankaran, and Senthil Mani. 2018. Sanskrit sandhi splitting using double decoder rnn. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 49094914. Association for Computational Linguistics. Ahmed, T., Hasan, M. K., Hoque, M. T., & Sultana, N. (2023). comparative study on subword segmentation strategies for low-resource neural machine translation. In *Proceedings of the Eighth Conference on Machine Translation (WMT 2023)* (pp. 912920). Association for Computational Linguistics. https://aclanthology.org/2023.wmt1.87 Anoop Kunchukuttan and 1 others. 2023. Indictrans2: Towards high-quality and low-resource machine translation for indic languages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Martin Vechev Luca Beurer-Kellner, Marc Fischer. 2024. Guiding llms the right way: Fast, noninvasive constrained decoding. arXiv. Alfio Gliozzo and Carlo Strapparava. 2006. Exploiting lexical alignment for cross language textual entailment. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 3340, Trento, Italy. Association for Computational Linguistics. Aaron Chiang and et al. 2023. Autoreviewer: Enabling model self-review for dataset quality control. arXiv preprint arXiv:2303.14112. Zheng Jiang, Yang Yu, Yang Feng, Bing Qin, and Ting Liu. 2022. Blonde: An automatic evaluation metric for document-level natural language generation. In Proceedings of NAACL, pages 16791698. Gowtham Ramesh, Vishrav Chaudhary, Divyanshu Kakwani, Sai Praneeth Golla, Abhishek Philip, et al. 2023. Indictrans2: Towards high-quality and efficient multilingual translation for indic languages. arXiv preprint arXiv:2304.09105."
        },
        {
            "title": "CONCLUDED",
            "content": ""
        }
    ],
    "affiliations": [
        "Nara Institute of Science and Technology (NAIST), Japan"
    ]
}