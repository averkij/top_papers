{
    "paper_title": "DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance",
    "authors": [
        "Yuxuan Luo",
        "Zhengkun Rong",
        "Lizhen Wang",
        "Longhao Zhang",
        "Tianshu Hu",
        "Yongming Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose a diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to overcome these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to full-body views, we employ a progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensuring long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 1 4 2 7 1 0 . 4 0 5 2 : r DreamActor-M1: Holistic, Expressive and Robust Human Image Animation with Hybrid Guidance Yuxuan Luo* Zhengkun Rong* Lizhen Wang* Longhao Zhang* Tianshu Hu* Yongming Zhu Bytedance Intelligent Creation {luoyuxuan, rongzhengkun, wanglizhen.2024, zhanglonghao.zlh, tianshu.hu, zhuyongming}@bytedance.com Figure 1. We introduce DreamActor-M1, DiT-based human animation framework, with hybrid guidance to achieve fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence."
        },
        {
            "title": "Abstract",
            "content": "While recent image-based human animation methods achieve realistic body and facial motion synthesis, critical gaps remain in fine-grained holistic controllability, multi-scale adaptability, and long-term temporal coherence, which leads to their lower expressiveness and robustness. We propose diffusion transformer (DiT) based framework, DreamActor-M1, with hybrid guidance to over- *Equal contribution. Corresponding author. come these limitations. For motion guidance, our hybrid control signals that integrate implicit facial representations, 3D head spheres, and 3D body skeletons achieve robust control of facial expressions and body movements, while producing expressive and identity-preserving animations. For scale adaptation, to handle various body poses and image scales ranging from portraits to fullbody views, we employ progressive training strategy using data with varying resolutions and scales. For appearance guidance, we integrate motion patterns from sequential frames with complementary visual references, ensur1 ing long-term temporal coherence for unseen regions during complex movements. Experiments demonstrate that our method outperforms the state-of-the-art works, delivering expressive results for portraits, upper-body, and full-body generation with robust long-term consistency. Project Page: https://grisoon.github.io/DreamActor-M1/. 1. Introduction Human image animation has become hot research direction in video generation and provides potential applications for film production, the advertising industry, and video games. While recent advances in image and video diffusion have enabled basic human motion synthesis from single image, previous works like [14, 20, 29, 39, 52, 57, 61] have made great progress in this area. However, existing methods remain constrained to coarse-grained animation. There are still some critical challenges in achieving fine-grained holistic control (e.g., subtle eye blinks and lip tremors), generalization ability to multi-scale inputs (portrait/upperbody/full-body), and long-term temporal coherence (e.g. long-term consistency for unseen garment areas). In order to handle these complex scenarios, we propose DiT-based framework, DreamActor-M1, to achieve holistic, expressive and robust human image animation with hybrid guidance. Recent developments in image-based animation have explored various directions, yet critical shortcomings persist in attaining photorealistic, expressive, and adaptable generation for practical applications. While single-image facial animation approaches employ landmark-driven or 3DMMdriven methodologies through GAN [3, 36, 38, 45, 47, 58] or NeRF [56] for expression manipulation, they frequently face limitations in image resolution and expression accuracy. Although certain studies attain robust expression precision via latent face representations [5, 6, 43, 50, 62] or achieve enhanced visual quality with diffusion-based architectures [40, 59], their applicability is typically restricted to portrait regions, failing to meet the broader demands of real-world scenarios. Diffusion-based human image animation [1, 14, 42, 52, 57] are able to generate basic limb articulation, plausible garment deformations and hair dynamics, but neglect the fine-grained facial expressions reenactment. Current solutions remain under-explored in addressing holistic control of facial expressions and body movements while failing to accommodate real-world multi-scale deployment requirements. Furthermore, existing methods fail to maintain temporal coherence in long-form video synthesis, especially for unseen areas in the reference image. In this work, we focus on addressing multi-scale driven synthesis, fine-grained face and body control, and long-term temporal consistency for unseen areas. Solving these challenges is non-trivial. First, it is very challenging to accurately manage both detailed facial expressions and body movements using single control signal, especially for the precise control over subtle facial expressions. Second, due to incomplete input data and the inability to generate extended video sequences in single pass, the model inevitably loses information about unseen regions (such as clothing textures on the back) during the continuation process that relies solely on the reference image and the final frame of prior clips. This gradual information decay leads to inconsistencies in unseen areas across sequentially generated segments. Third, under multi-scale inputs, the varying information density and focus priorities make it hard to achieve holistic and expressive animation within single framework. To tackle these issues, we introduce stronger hybrid control signals, and complementary appearance guidance that can fill missing information gaps, and progressive training strategy with training dataset including diverse samples at different scales (e.g. portrait talking and full-body dancing). Specifically, for motion guidance, we design hybrid control signals including implicit face latent for fine-grained control of facial expressions, explicit head spheres for head scale and rotation and 3D body skeletons for torso movements and bone length adjustment which achieves robust adaptation across substantial shape variations. For scenarios with limited information (e.g., multi-turn rotations or partial-body references), we introduce complementary appearance guidance by first sampling distinct poses from target movements, then generating multi-frame references to provide unseen area textures, and finally propagating these references across video segments to maintain consistent details throughout long-term synthesis. To enable multi-scale adaptation, we train the model with progressive strategy on diverse dataset that includes different types of scenes like portrait acting, upper-body talking, and full-body dancing. In summary, our key contributions are as follows. We propose holistic DiT-based framework and progressive training strategy for human image animation that supports flexible multi-scale synthesis. We design hybrid control signals combining implicit facial representations, explicit 3D head spheres, and body skeletons to enable the expressive body and facial motion synthesis while supporting diverse character styles. We develop complementary appearance guidance to mitigate information gaps of unseen areas between video segments, enabling consistent video generation over long durations. 2. Related Works Recent advancements in human image animation can be broadly categorized into single-image facial animation and body animation, each addressing distinct technical challenges in photorealistic and expressive human motion synthesis. 2 Figure 2. Overview of DreamActor-M1. During the training stage, we first extract body skeletons and head spheres from driving frames and then encode them to the pose latent using the pose encoder. The resultant pose latent is combined with the noised video latent along the channel dimension. The video latent is obtained by encoding clip from the input full video using 3D VAE. Facial expression is additionally encoded by the face motion encoder, to generate implicit facial representations. Note that the reference image can be one or multiple frames sampled from the input video to provide additional appearance details during training and the reference token branch shares weights of our DiT model with the noise token branch. Finally, the denoised video latent is supervised by the encoded video latent. Within each DiT block, the face motion token is integrated into the noise token branch via cross-attention (Face Attn), while appearance information of ref token is injected to noise token through concatenated self-attention (Self Attn) and subsequent cross-attention (Ref Attn). 2.1. Single-Image Facial Animation Early work primarily relied on GAN to create portrait animation through warping and rendering. These studies typically focused on improving driving expressiveness by exploring various motion representations including neural keypoints [12, 36, 47, 58] or 3D face model parameters [4, 31]. These methods can effectively decouple identity and expression features, but the reproduction of expressions, especially subtle and exaggerated ones, is quite limited. Another class of methods learned latent representation [5, 6, 43] directly from the driving face, enabling higher-quality expression reproduction. However, limited by GAN, these methods faced challenges in generating high-quality results and adapting to different portrait styles. In recent years, diffusion-based methods have demonstrated strong generative capabilities, leading to significant advancements in subsequent research. EMO [40] first introduced the ReferenceNet into the diffusion-based portrait video generation task. Follow-your-emoji [24] utilized expression-aware landmarks with facial fine-grained loss for precise motion alignment and micro-expression details. Megactor-Σ [53] designed diffusion transformer integrating audio-visual signals for multi-modal control. XPortrait [49] combined ControlNet for head pose and expression with patch-based local control and cross-identity training. X-Nemo [59] developed 1-D latent descriptors to disentangle identity-motion entanglement. SkyReelsA1 [30] proposed portrait animation framework leveraging the diffusion transformer [28]. 2.2. Single-Image Body Animation As pioneering work in body animation, MRAA [37] proposed distinct motion representations for animating articulated objects in an unsupervised manner. Recent advancements in latent diffusion models [32] have significantly boosted the development of body animation [1, 18, 24, 41, 46, 48]. Animate Anyone [14] introduced an additional ReferenceNet to extract appearance features from reference images. MimicMotion [57] designed confidence-aware pose guidance mechanism and used facial landmarks to reenact facial expressions. Animate-X [39] employed implicit and explicit pose indicators for motion and pose features, respectively, achieving better generalization across anthroInstead of using skeleton maps, pomorphic characters. Champ [61] and Make-Your-Anchor [16] leveraged 3D human parametric model, while MagicAnimate [52] uti3 lized DensePose [11] to establish dense correspondences. TALK-Act [10] enhanced the textural awareness with explicit motion guidance to improve the generation quality. StableAnimator [42] and HIA [51] addressed the identity preservation and motion blur modeling, respectively. Additionally, some previous studies [20, 29, 54] followed plugin paradigm, enhancing effectiveness without requiring additional training of existing model parameters. To address missing appearances under significant viewpoint changes, MSTed [13] introduced multiple reference images as input. MIMO [25] and Animate Anyone 2 [15] separately modeled humans, objects, and backgrounds, aiming to animate characters with environmental affordance. Apart from UNet-based diffusion, recent works [8, 34] have also begun adapting diffusion transformers [28] for human animation. 3. Method Given one or multiple reference images IR along with driven video VD, our objective is to generate realistic video that depicts the reference character mimicking the In this section, we motions present in the driving video. begin with concise introduction to our Diffusion Transformer (DiT) backbone in Sec. 3.1. Following that, we offer an in-depth explanation of our carefully-designed hybrid control signals in Sec. 3.2. Subsequently, we introduce the complementary appearance guidance in Sec. 3.3. Finally, we present the progressive training processes in Sec. 3.4. 3.1. Preliminaries As shown in Fig. 2, our overall framework adheres to the Latent Diffusion Model (LDM) [32] for training the model within the latent space of pre-trained 3D Variational Autoencoder (VAE) [55]. We utilize the MMDiT [7] as the backbone network, which has been pre-trained on text-tovideo and image-to-video tasks, Seaweed [22]. Note that we employ Flow Matching [23] as the training objective. Unlike the prevailing ReferenceNet-based human animation approaches [14, 15], we refrain from employing copy of the DiT as the ReferenceNet to inject the reference feature into the DenoisingNet following [21]. Instead, we flatten the latent feature (cid:101)IR and (cid:101)VD extracted through the VAE, patchify, concatenate them together, and then feed them into the DiT. It facilitates the information interaction between the reference and video frames through 3D self-attention layers and spatial cross-attention layers integrated throughout the entire model. Specifically, in each DiT block, given the concatenated token R(thw)c, we perform selfattention along the first dimension. Then, we split it into TR RtR(hw)c and TD RtD(hw)c and reshape them to TR R1(hwtR)c and TD RtD(hw)c to perform cross-attention along the second dimension. 3.2. Hybrid Motion Guidance To achieve expressive and robust human animation, in this paper, we intricately craft the motion guidance and propose hybrid control signals comprising implicit facial representations, 3D head spheres, and 3D body skeletons. Implicit Facial Representations. In contrast to conventional approaches that rely on facial landmarks for expression generation, our method introduces implicit facial representations. This innovative approach not only enhances the preservation of intricate facial expression details but also facilitates the effective decoupling of facial expressions, identity and head pose, enabling more flexible and realistic animation. Specifically, our pipeline begins by detecting and cropping the faces in the driving video, which are then resized to standardized format Rt3224224. pre-trained face motion encoder Ef and an MLP layer are employed to encode faces to face motion tokens Rtc. The is fed into the DiT block through crossattention layer. The face motion encoder is initialized using an off-the-shelf facial representation learning method [43], which has been pre-trained on large-scale datasets to extract identity-independent expression features. This initialization not only accelerates convergence but also ensures that the encoded motion tokens are robust to variations in identity, focusing solely on the nuances of facial expressions. By leveraging implicit facial representations, our method achieves superior performance in capturing subtle expression dynamics while maintaining high degree of flexibility for downstream tasks such as expression transfer and reenIt is worth noting that we additionally train an actment. audio-driven encoder capable of mapping speech signals to face motion token. This encoder enables facial expression editing, particularly lip-syncing, without the need for driving video. 3D Head Spheres. Since the implicit facial representations are designed to exclusively control facial expressions, we introduce an additional 3D head sphere to independently manage head pose. This dual-control strategy ensures that facial expressions and head movements are decoupled, enabling more precise and flexible animation. Specifically, we utilize an off-the-shelf face tracking method [44] to extract 3D facial parameters from the driving video, including camera parameters and rotation angles. These parameters are then used to render the head as color sphere projected onto the 2D image plane. The spheres position is carefully aligned with the position of the driving head in the video frame, ensuring spatial consistency. Additionally, the size of the sphere is scaled to match the reference heads size, while its color is dynamically determined by the driving heads orientation, providing visual cue for head rotation. This 3D sphere representation offers highly flexible and intuitive way to control head pose, significantly reducing the models learning complexity by abstracting complex 4 Figure 3. Overview of our inference pipeline. First, we (optionally) generate multiple pseudo-references to provide complementary appearance information. Next, we extract hybrid control signals comprising implicit facial motion and explicit poses (head sphere and body skeleton) from the driving video. Finally, these signals are injected into DiT model to synthesize animated human videos. Our framework decouples facial motion from body poses, with facial motion signals being alternatively derivable from speech inputs. 3D head movements into simple yet effective 2D representation. This approach is particularly advantageous for preserving the unique head structures of reference characters, especially those from anime and cartoon domains. 3D Body Skeletons. For body control, we introduce 3D In particubody skeletons with bone length adjustment. lar, we first use 4DHumans [9] and HaMeR [27] to estimate body and hand parameters of the SMPL-X [26] model. Then we select the body joints, project them onto the 2D image plane, and connect them with lines to construct the skeleton maps. We opt to use skeletons instead of rendering the full body, as done in Champ [61], to avoid providing the model with strong guidance on body shape. By leveraging skeletons, we encourage the model to learn the shape and appearance of the character directly from the reference images. This approach not only reduces bias introduced by predefined body shapes but also enhances the models ability to generalize across diverse body types and poses, leading to more flexible and realistic results. The body skeletons and the head spheres are concatenated in the channel dimension, and fed into pose encoder Ep to obtain the pose feature. The pose feature and noised video feature are then concatenated and processed by an MLP layer to obtain the noise token. During inference, to address variations in skeletal proportions across subjects, we adopt normalization process to adjust the bone length. First, we use pre-trained image editing model [35] to transform reference and driving images into standardized A-pose configuration. Next, we leverage RTMPose [17] to calculate the skeletal proportions of both the driving subject and the reference subject. Finally, we perform anatomical alignment by proportionally adjusting the bone lengths of the driving subject to match the skeletal measurements of the reference subject. 3.3. Complementary Appearance Guidance We propose novel multi-reference injection protocol to enhance the models capability for robust multi-scale, multiview, and long-term video generation. This approach addresses the challenges of maintaining temporal consistency and visual fidelity across diverse viewing angles and extended timeframes. During training, we compute the rotation angles for all frames in the input video and sort them based on their z-axis rotation values (yaw). From this sorted set, we strategically select three key frames corresponding to the maximum, minimum, and median z-axis rotation angles. These frames serve as representative viewpoints, ensuring comprehensive coverage of the objects orientation. Furthermore, for videos featuring full-body compositions, we introduce an additional step: single frame is randomly selected and cropped to half-body portrait format, which is then included as an auxiliary reference frame. This step enriches the models understanding of both global and local structural details. During inference, our protocol offers an optional twostage generation mode to handle challenging scenarios, such as cases where the reference image is single frontal half-body portrait while the driving video features full-body frames with complex motions like turning or side views. First, the model is utilized to synthesize multi-view video sequence from the single reference image. This initial output captures range of plausible viewpoints and serves as foundation for further refinement.We apply the same frame selection strategy used during training to select the most informative frames. These selected frames are then reintegrated into the model as the complementary appearance guidance, enabling the generation of final output that exhibits enhanced spatial and temporal coherence. This iterative approach not only improves the robustness of the model but also ensures high-quality results even under constrained input conditions. 3.4. Progressive Training Process Our training process is divided into three distinct stages to ensure gradual and effective adaptation of the model. In the first stage, we utilize only two control signals: 3D body skeletons and 3D head spheres, deliberately excluding the implicit facial representations. This initial stage is designed to facilitate the transition of the base video generation model to the task of human animation. By avoiding overly complex control signals that may hinder the models learning process, we allow the model to establish strong Figure 4. The comparisons with human image animation works, Animate Anyone [14], Champ [61], MimicMotion [57] and DisPose [20]. Our method demonstrates results with better fine-grained motions, identity preservation, temporal consistency and high fidelity. foundational understanding of the task. In the second stage, we introduce the implicit facial representations while keeping all other model parameters frozen. During this stage, only the face motion encoder and face attention layers are trained, enabling the model to focus on learning the intricate details of facial expressions without the interference of other variables. Finally, in the third stage, we unfreeze all model parameters and conduct comprehensive training session, allowing the model to fine-tune its performance by jointly optimizing all components. This staged approach ensures robust and stable training process, ultimately leading to more effective and adaptable model. 4. Experiments 4.1. Experimental Setups Implementation Details. Our training weights are initialized from pretrained image-to-video DiT model [22]. Then, we train the first stage with 20,000 steps, the second stage with 20,000 steps, and the third stage with 30,000 steps. To enhance the models generalization capability for arbitrary durations and resolutions, during training, the length of the sampled video clips is randomly selected from 25 to 121 frames, while the spatial resolution is resized to an area of 960 640, maintaining the original aspect ratio. All stages are trained with 8 H20 GPUs using the AdamW optimizer with learning rate of 5e6. During inference, each video segment contains 73 frames. To ensure fullvideo consistency, we use the last latent from the current segment as the initial latent for the next segment, modeling the next segment generation as an image-to-video generation task. The classifier-free guidance (cfg) parameters for both the references and motion control signals are set to 2.5. Datasets. For training, we construct comprehensive dataset by collecting video data from various sources, totaling 500 hours of footage. This dataset encompasses diverse range of scenarios, including dancing, sports, film scenes, and speeches, ensuring broad coverage of human motion and expressions. The dataset is balanced in terms of framing, with full-body shots and half-body shots each 6 Figure 5. Our comparisons with portrait image animation works, LivePortrait [12], X-Portrait [49], Skyreels-A1 [30] and Runway ActOne [33]. Our method demonstrates more accurate and expressive portrait animation capabilities. accounting for approximately 50% of the data. In addition, we leverage Nersemble [19] to further improve the synthesis quality of faces. For evaluation, we utilize our collected dataset, which provides varied and challenging benchmark, enabling robust assessment of the models generalization capabilities across different scenarios. Evaluation Metrics. We adhere to established evaluation metrics employed in prior research, including FID, SSIM, LPIPS, PSNR, and FVD. The first four are used to evaluate the generation quality of each frame, while the last one is used to assess the video fidelity. 4.2. Comparisons with Existing Methods To comprehensively demonstrate the effectiveness of our work, we conducted experiments on both body animation and portrait animation tasks. Note that our method demonstrates strong performance with just single reference image in most cases. To ensure fairness in comparison with other methods, we only used multiple reference images in the ablation study, while single reference image was employed in the comparative analysis. We strongly recommend readers refer to the supplementary video. Comparisons with Body Animation Methods. We perform the qualitative and quantitative evaluation of DreamActor-M1 with our collected dataset and compare with state-of-the-art body animation methods, including Animate Anyone [14], Champ [61], MimicMotion [57], and DisPose [20], as shown in Tab. 1 and Fig. 4. We can see that our proposed DreamActor-M1 outperforms the current state-of-the-art results. Comparisons with Portrait Animation Methods. We FID SSIM PSNR LPIPS FVD AA [14] Champ [61] MimicMotion [57] DisPose [20] 36.72 40.21 35.90 33.01 0.791 0.732 0.799 0.804 Ours 27.27 0.821 21.74 20.18 22.25 21. 23.93 0.266 0.281 0.253 0.248 0.206 158.3 171.2 149.9 144.7 122.0 Table 1. Quantitative comparisons with body animation methods on our collected dataset. also compare the DreamActor-M1 with state-of-the-art portrait animation methods, including LivePortrait [12], XPortrait [49], SkyReels-A1 [30], and Act-One [33], as shown in Tab. 2 and Fig. 5. As shown in Tab. 2, the videodriven results consistently outperform all competing methods across all metrics on our collected dataset. While facial expressions and head pose are decoupled in our framework, our method can also be extended to audiodriven facial animation. Specifically, we train face motion encoder to map speech signals to face motion tokens, leading to realistic and lip-sync animations. As an extended application, we omit quantitative comparisons. Please refer to our supplementary video for more results. 4.3. Ablation Study We conducted comprehensive ablation studies to evaluate the impact of several core components of our method. Multi-Reference Protocol. We compare two settings: (a) inference with single reference image, (b) two-stage inference approach as described in Sec. 3.3, where pseudo reference images are generated first, followed by multi7 FID SSIM PSNR LPIPS FVD LivePortrait [12] X-Portrait [49] SkyReels-A1 [30] Act-One [33] 31.72 30.09 30.66 29. 0.809 0.774 0.811 0.817 Ours 25.70 0.823 24.25 22.98 24.11 25.07 28. 0.270 0.281 0.262 0.259 0.238 147.1 150.9 133.8 135.2 110.3 Table 2. Quantitative comparisons with portrait animation methods on our collected dataset. FID SSIM PSNR LPIPS FVD Single-R Multi-R (pseudo) 28.22 26. 0.798 0.812 25.86 26.22 0.223 0.219 120.5 116.6 Table 3. Ablation study on multi-reference. Figure 6. Ablation study of 3D skeletons with bone length adjustment (BLA) and implicit face features. reference inference. Results are shown in Tab. 3. It demonstrates that pseudo multi-reference inference outperforms single-reference inference in terms of long-term video generation quality and temporal consistency. This is because during the extended video generation process, the supplementary reference images provide additional visual infor8 mation about unseen areas, enabling the video generation process to leverage reference details. This helps avoid information loss and thereby maintains consistency throughout the video. Nevertheless, the performance achieved by single reference image remains competitive, demonstrating its sufficiency for most scenarios. Hybrid Control Signals. We further investigate the contribution of our hybrid control signals by ablating key components: (a) substituting 3D head sphere and skeleton with 3D mesh (b) substituting implicit facial representations with 3d facial landmarks. Results are shown in Fig. 6. It reveal significant performance degradation under these settings, emphasizing the importance of each component in our hybrid control framework. Specifically, the 3D skeletons with bone length adjustment provide more accurate spatial guidance, and the implicit facial representations capture subtle expression details more effectively than traditional landmarks. These findings demonstrate the effectiveness and superiority of our proposed hybrid control signals in achieving high-quality and realistic human image animation. 5. Conclusion In this paper, we present holistic human image animation framework DreamActor-M1 addressing multi-scale adaptation, fine-grained facial expression and body movement control, and long-term consistency in unseen regions. We employ progressive training strategy using data with varying resolutions and scales to handle various image scales ranging from portraits to full-body views. By decoupling identity, body pose, and facial expression through hybrid control signals, our method achieves precise facial dynamics and vivid body movements while preserving the character identity. The proposed complementary appearance guidance resolves information gaps in cross-scale animation and unseen region synthesis. We believe these innovations provide potential insights for future research in complex motion modeling and real-world deployment of expressive human animation. Limitation. Our framework faces inherent difficulties in controlling dynamic camera movements, and fails to generate physical interactions with environmental objects. In addition, the bone length adjustment of our method using [35] exhibits instability in edge cases, requiring multiple iterations with manual selections for optimal cases. These challenges still need to be addressed in future research. Ethics considerations. Human image animation has possible social risks, like being misused to make fake videos. The proposed technology could be used to create fake videos of people, but existing detection tools [2, 60] can spot these fakes. To reduce these risks, clear ethical rules and responsible usage guidelines are necessary. We will strictly restrict access to our core models and codes to prevent misuse. Images and videos are all from publicly available sources. If there are any concerns, please contact us and we will delete it in time."
        },
        {
            "title": "Acknowledgment",
            "content": "We extend our sincere gratitude to Shanchuan Lin, Lu Jiang, Zhurong Xia, Jianwen Jiang, Zerong Zheng, Chao Liang, Youjiang Xu and Ming Zhou for their invaluable contributions and supports to this research work."
        },
        {
            "title": "References",
            "content": "[1] Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. arXiv preprint arXiv:2311.12052, 2023. 2, 3 [2] Haoxing Chen, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Yaohui Li, Jun Lan, Huijia Zhu, Jianfu Zhang, Weiqiang Wang, et al. Demamba: Ai-generated video detection on million-scale genvideo benchmark. arXiv preprint arXiv:2405.19707, 2024. 8 [3] Yu Deng, Duomin Wang, Xiaohang Ren, Xingyu Chen, and Baoyuan Wang. Portrait4d: Learning one-shot 4d head In Proceedings of avatar synthesis using synthetic data. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71197130, 2024. 2 [4] Michail Christos Doukas, Stefanos Zafeiriou, and Viktoriia Sharmanska. Headgan: One-shot neural head synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1439814407, 2021. 3 [5] Nikita Drobyshev, Jenya Chelishev, Taras Khakhulin, Aleksei Ivakhnenko, Victor Lempitsky, and Egor Zakharov. Megaportraits: One-shot megapixel neural head avatars. In Proceedings of the 30th ACM International Conference on Multimedia, pages 26632671, 2022. 2, 3 [6] Nikita Drobyshev, Antoni Bigata Casademunt, Konstantinos Vougioukas, Zoe Landgraf, Stavros Petridis, and Maja Pantic. Emoportraits: Emotion-enhanced multimodal one-shot head avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8498 8507, 2024. 2, [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 4 [8] Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, and Jianke Zhu. Humandit: Pose-guided diffusion transformer for longarXiv preprint form human motion video generation. arXiv:2502.04847, 2025. 4 [9] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: ReIn constructing and tracking humans with transformers. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1478314794, 2023. 5 [10] Jiazhi Guan, Quanwei Yang, Kaisiyuan Wang, Hang Zhou, Shengyi He, Zhiliang Xu, Haocheng Feng, Errui Ding, Jingdong Wang, Hongtao Xie, et al. Talk-act: Enhance texturalawareness for 2d speaking avatar reenactment with diffusion model. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. 4 [11] Rıza Alp Guler, Natalia Neverova, and Iasonas Kokkinos. In Densepose: Dense human pose estimation in the wild. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 72977306, 2018. [12] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 3, 7, 8 [13] Fa-Ting Hong, Zhan Xu, Haiyang Liu, Qinjie Lin, Luchuan Song, Zhixin Shu, Yang Zhou, Duygu Ceylan, and Dan Xu. Free-viewpoint human animation with pose-correlated reference selection. arXiv preprint arXiv:2412.17290, 2024. 4 [14] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 2, 3, 4, 6, 7 [15] Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. Animate anyone 2: High-fidelity character image arXiv preprint animation with environment affordance. arXiv:2502.06145, 2025. 4 [16] Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, and Tong-Yee Lee. Make-your-anchor: diffusion-based 2d avatar generation framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69977006, 2024. 3 [17] Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, and Kai Chen. Rtmpose: Realtime multi-person pose estimation based on mmpose. arXiv preprint arXiv:2303.07399, 2023. [18] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion In 2023 image-to-video synthesis via stable diffusion. IEEE/CVF International Conference on Computer Vision (ICCV), pages 2262322633. IEEE, 2023. 3 [19] Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, and Matthias Nießner. Nersemble: Multi-view radiance field reconstruction of human heads. ACM Transactions on Graphics (TOG), 42(4):114, 2023. 7 [20] Hongxiang Li, Yaowei Li, Yuhang Yang, Junjie Cao, Zhihong Zhu, Xuxin Cheng, and Long Chen. Dispose: Disentangling pose guidance for controllable human image animation. arXiv preprint arXiv:2412.09349, 2024. 2, 4, 6, 7 [21] Gaojie Lin, Jianwen Jiang, Jiaqi Yang, Zerong Zheng, and Chao Liang. Omnihuman-1: Rethinking the scaling-up of one-stage conditioned human animation models. arXiv preprint arXiv:2502.01061, 2025. 4 [22] Shanchuan Lin, Xin Xia, Yuxi Ren, Ceyuan Yang, Xuefeng Xiao, and Lu Jiang. Diffusion adversarial post-training for 9 one-step video generation. arXiv preprint arXiv:2501.08316, 2025. 4, [23] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 4 [24] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. 3 [25] Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Mimo: Controllable character video synthesis with spatial decomposed modeling. arXiv preprint arXiv:2409.16160, 2024. 4 [26] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019. 5 [27] Georgios Pavlakos, Dandan Shan, Ilija Radosavovic, Angjoo Kanazawa, David Fouhey, and Jitendra Malik. Reconstructing hands in 3D with transformers. In CVPR, 2024. 5 [28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 3, 4 [29] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, MingChang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. 2, [30] Di Qiu, Zhengcong Fei, Rui Wang, Jialin Bai, Changqian Yu, Mingyuan Fan, Guibin Chen, and Xiang Wen. Skyreels-a1: Expressive portrait animation in video diffusion transformers. arXiv preprint arXiv:2502.10841, 2025. 3, 7, 8 [31] Yurui Ren, Ge Li, Yuanqi Chen, Thomas H. Li, and Shan Liu. Pirenderer: Controllable portrait image generation via semantic neural rendering, 2021. 3 [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, 4 [33] Runway. https : / / runwayml . com / research / introducing-act-one, 2024. 7, [34] Ruizhi Shao, Youxin Pang, Zerong Zheng, Jingxiang Sun, and Yebin Liu. Human4dit: 360-degree human video generation with 4d diffusion transformer. arXiv preprint arXiv:2405.17405, 2024. 4 [35] Yichun Shi, Peng Wang, and Weilin Huang. Seededit: Align image re-generation to image editing. arXiv preprint arXiv:2411.06686, 2024. 5, 8 [36] Aliaksandr Siarohin, Stephane Lathuili`ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019. 2, 3 [37] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1365313662, 2021. 3 [38] Jingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, and Yebin Liu. Next3d: Generative neural texture rasterization for 3d-aware head avatars. In CVPR, 2023. [39] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. ICLR 2025, 2025. 2, 3 [40] Linrui Tian, Qi Wang, Bang Zhang, and Liefeng Bo. Emo: Emote portrait alive - generating expressive portrait videos with audio2video diffusion model under weak conditions, 2024. 2, 3 [41] Zhengyan Tong, Chao Li, Zhaokang Chen, Bin Wu, and Wenjiang Zhou. Musepose: pose-driven image-to-video framework for virtual human generation. arxiv, 2024. 3 [42] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: Highquality identity-preserving human image animation. arXiv preprint arXiv:2411.17697, 2024. 2, 4 [43] Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum, and Baoyuan Wang. Progressive disentangled representation learning for fine-grained controllable talking head synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2, 3, 4 [44] Lizhen Wang, Zhiyuan Chen, Tao Yu, Chenguang Ma, Liang Li, and Yebin Liu. Faceverse: fine-grained and detailcontrollable 3d face morphable model from hybrid dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2033320342, 2022. 4 [45] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: generative model for sculpting 3d digital avatars using diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45634573, 2023. 2 [46] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, ChungChing Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93269336, 2024. [47] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencIn Proceedings of the IEEE/CVF conference on coming. puter vision and pattern recognition, pages 1003910049, 2021. 2, 3 [48] Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, and Nong Sang. Unianimate: Taming unified video diffusion models for consistent human image animation. arXiv preprint arXiv:2406.01188, 2024. 3 10 Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, pages 145162. Springer, 2024. 2, 3, 5, 6, 7 [62] Yongming Zhu, Longhao Zhang, Zhengkun Rong, Tianshu Infp: Audio-driven Hu, Shuang Liang, and Zhipeng Ge. interactive head generation in dyadic conversations. arXiv preprint arXiv:2412.04037, 2024. 2 [49] You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, and Linjie Luo. X-portrait: Expressive portrait animation with hierarchical motion attention. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. 3, 7, [50] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. Vasa-1: Lifelike audio-driven talking faces generated in real time. Advances in Neural Information Processing Systems, 37:660684, 2025. 2 [51] Zhongcong Xu, Chaoyue Song, Guoxian Song, Jianfeng Zhang, Jun Hao Liew, Hongyi Xu, You Xie, Linjie Luo, Guosheng Lin, Jiashi Feng, et al. High quality human image animation using regional supervision and motion blur condition. arXiv preprint arXiv:2409.19580, 2024. 4 [52] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human imIn Proceedings of age animation using diffusion model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. 2, 3 [53] Shurong Yang, Huadong Li, Juhao Wu, Minhao Jing, Linze Li, Renhe Ji, Jiajun Liang, Haoqiang Fan, and Jin Wang. Megactor-Σ: Unlocking flexible mixed-modal control in portrait animation with diffusion transformer. arXiv preprint arXiv:2408.14975, 2024. 3 [54] Sunjae Yoon, Gwanhyeong Koo, Younghwan Lee, and Chang Yoo. Tpc: Test-time procrustes calibration for diffusion-based human image animation. Advances in Neural Information Processing Systems, 37:118654118677, 2024. 4 [55] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [56] Wangbo Yu, Yanbo Fan, Yong Zhang, Xuan Wang, Fei Yin, Yunpeng Bai, Yan-Pei Cao, Ying Shan, Yang Wu, Zhongqian Sun, et al. Nofa: Nerf-based one-shot facial avatar reconstruction. In ACM SIGGRAPH 2023 conference proceedings, pages 112, 2023. 2 [57] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation arXiv preprint with confidence-aware pose guidance. arXiv:2406.19680, 2024. 2, 3, 6, 7 [58] Jian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36573666, 2022. 2, 3 [59] Xiaochen Zhao, Hongyi Xu, Guoxian Song, You Xie, Chenxu Zhang, Xiu Li, Linjie Luo, Jinli Suo, and Yebin Liu. X-nemo: Expressive neural motion reenactment via disentangled latent attention. ICLR 2025, 2025. 2, 3 [60] Jiachen Zhou, Mingsi Wang, Tianlin Li, Guozhu Meng, and Kai Chen. Dormant: Defending against pose-driven human image animation. arXiv preprint arXiv:2409.14424, 2024. 8 [61] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu"
        }
    ],
    "affiliations": [
        "Bytedance Intelligent Creation"
    ]
}