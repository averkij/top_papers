{
    "paper_title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization",
    "authors": [
        "Wei-Tse Cheng",
        "Yen-Jen Chiou",
        "Yuan-Fu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS."
        },
        {
            "title": "Start",
            "content": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization Wei-Tse Cheng Yen-Jen Chiou Yuan-Fu Yang National Yang Ming Chiao Tung University andy5552555.ii13@nycu.edu.tw, remi.ii13@nycu.edu.tw, yfyangd@nycu.edu.tw 5 2 0 2 8 2 ] . [ 1 5 0 7 0 0 . 1 0 6 2 : r Figure 1. Overview of the proposed RGS-SLAM pipeline. The system integrates dense feature matching and multi-view triangulation for one-shot Gaussian initialization, followed by differentiable 3DGS optimization and real-time tracking."
        },
        {
            "title": "Abstract",
            "content": "We introduce RGS-SLAM, robust Gaussian-splatting residual-driven SLAM framework densification stage of GS-SLAM with training-free Instead of correspondence-to-Gaussian initialization. replaces that the progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through confidence-aware inlier classifier, generating well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20%, yielding higher rendering fidelity in texturerich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS. 1. Introduction Recent advances in 3D Gaussian Splatting (3DGS) have enabled high-quality view synthesis and real-time mapping. However, most pipelines still rely on residual-driven densification, where Gaussians are iteratively spawned and merged as errors are detected. This causes non-stationary objectives, unstable convergence, and sensitivity to texturerich or cluttered regions due to delayed coverage and uneven geometry. We take different approach by initializing from complete and well-distributed Gaussian set rather than growing it incrementally. Using Dense Feature Matching (DFM), we obtain confidence-weighted correspondences within short keyframe window, triangulate them into structureaware matches, and instantiate the Gaussian set before optimization begins. Subsequent updates refine means, covariances, opacities, and colors while keeping topology fixed, resulting in stable and stationary optimization with strong spatial support even in high-frequency regions. Integrated into monocular SLAM, this single-step seeding shortens time to usable maps, stabilizes early pose and shape estimation, and removes the need for additional netIt directly replaces densification with works or losses. brief feature-matching pass at keyframes while leaving the rest of the pipeline unchanged. An overview of the RGSSLAM framework, including the initialization pipeline, is shown in Figure 1. Our contributions are summarized below. Single-Step Dense Initialization. one-shot triangulation replaces residual-driven densification within the standard GS-SLAM pipeline MonoGS [11], enabling stationary optimization and 20% faster convergence. Improved Localization Accuracy. Confidence-weighted correspondences stabilize early pose estimation, reducing drift by over 30%. Lightweight and Efficient Mapping. Spatially balanced Gaussians lower computation and memory, achieving 20% higher rendering throughput in real time. High-Fidelity Reconstruction. Dense seeding enhances early coverage and geometric consistency, yielding about 20% better reconstruction accuracy and completeness. 2. Related Work 3D Gaussian Splatting and Densification. 3D Gaussian splatting (3DGS) enables real-time view synthesis with anisotropic splats and visibility-aware renderer, yet most systems rely on residual-driven densification [8]. We instead seed fixed topology from dense multi-view correspondences and then refine only splat parameters. Gaussian Splats for SLAM. SLAM systems mapping with Gaussians include GS-SLAM, MonoGS, SplaTAM, and Gauss-SLAM [7, 11, 20]. They rely on densification, causing early non-stationarity. Our training-free dense seed removes this stage and drops into MonoGS with minimal modifications. Differentiable Rendering and Real-Time SLAM. PhotoSLAM, GLORIE-SLAM, and Point-SLAM couple differentiable rendering with pose optimization for fast updates via analytic/lightweight gradients [4, 12, 23]. We retain this in Gaussian renderer and use stationary initialization so early steps do not change topology. Feature Matching and Dense Correspondence. SuperPoint/SuperGlue remain strong baselines [1, 13]. Detectorfree transformers (LoFTR, LightGlue) extend coverage in low-texture regions, and dense matchers (DKM) provide broad two-view coverage with confidence for geometry [2, 9, 17]. We aggregate confidence-weighted dense correspondences over short keyframe window into an explicit Gaussian seed. Initialization and Training-Free Priors. SfM and multiview stereo stabilize early optimization via geometric priors and learned depth [14, 22]. In Gaussian splatting, schedules and regularizers typically retain densification. Our correspondence-to-Gaussian initialization follows trainingfree priors and yields stationary objective from the start. 3. Method 3.1. Gaussian Splatting Representation We map the scene with set of anisotropic Gaussians = {Gi}. Each Gi carries optical properties, color vector ci and an opacity αi [0, 1], and geometric properties, R3 and symmetric positive definite covariance mean µW R33 expressed in world coordinates. For brevity we ΣW describe color as single vector and later allow spherical harmonics for view dependence. Let TW = [R t] be the worldtocamera pose of the current view and let π() be the calibrated perspective projection. 3D Gaussian (µW ) induces 2D Gausi sian on the image plane through firstorder linearization of the projection around µW . The projected mean and covarii ance are , ΣW = π(cid:0)TW µW µI (cid:1), 2 ΣI = Ji ΣW RJ , (1) Figure 2. Detailed RGS-SLAM pipeline. Each keyframe triggers dense multi-view triangulation that yields one-shot Gaussian initialization, subsequently refined through joint tracking and mapping within differentiable 3DGS renderer using analytic SE(3) Jacobians. where Ji = π(RX+t) tion at µW the sensor. (cid:12) (cid:12)X=µW is the Jacobian of the projec3.2. Tracking and Camera Pose Optimization . This relates the ellipsoid in 3D to an ellipse on Rendering is performed by rasterizing Gaussians instead of ray marching. For pixel we collect the fronttoback ordered set (p) of screenspace Gaussians whose 2D footprints overlap p. The pixel color is obtained by αcompositing Cp = (cid:88) ci αi i1 (cid:89) (cid:0)1 αj (cid:1), (2) iN (p) j=1 where αi [0, 1] denotes the screenspace opacity at pixel p, obtained by modulating the primitive opacity with the value of the 2D Gaussian density at using the parameters (µI ) (the dependence on is omitted for brevity). Empty space does not contribute because the renderer iterates over primitives that actually cover the pixel. , ΣI Equations (1) and (2) are differentiable in color, opacity, mean, covariance, and pose. Gradients flow through the splat weights and the projection Jacobian, which allows firstorder optimizers to refine both optical and geometric parameters until the rendered image matches the observation with high fidelity. In subsequent sections we will initialize {µW , αi, ci} densely in single-step and then refine them under this differentiable renderer. , ΣW 3 3.2.1. Objective and Per-Frame Update For each incoming frame, we estimate the camera pose by minimizing an image-domain objective under the 3DGS renderer in Figure 2. Let I(G, TW C) = S(G, TW C) be the rendered image and the observation. The photometric residual is Epho = (cid:13) (cid:13) I(G, TW C) (cid:13) (cid:13)1, (3) augmented with an affine brightness model to absorb exposure changes, i.e., we jointly estimate gain and bias and substitute I() + into Eq. (3). We optimize = Epho. Pixels with low screen-space opacity or low image gradient are downweighted to reduce the influence of textureless regions. In practice we perform tens of gradient steps per frame to reach stable update. 3.2.2. Alpha Compositing for Color Rendering is carried out by rasterizing Gaussians in screen space and composing them front to back. For pixel p, let (p) be the set of overlapping Gaussians sorted from near to far. The color follows the standard α-compositing in Eq. (2), which naturally handles occlusion via transmittance (cid:81) j<i(1 αj). No depth map is produced or used in our tracking objective. 3.2.3. Minimal Pose Jacobians on SE(3) We update the world-to-camera pose by left-multiplicative twist SE(3), new keyframe is created when IoU(Ik, Ik ) is less than τ and the inter-view parallax is above bound. Accepted keyframes are stored in bounded buffer that provides neighbours for multi-view initialization. TW exp( ˆξ) TW C, (4) 3.4. Dense Feature Matching where we differentiate the objective with respect to ξ in minimal coordinates. Let µW be 3D Gaussian mean in world coordinates and µC = µW + its camera-space position for TW = [R t]. The 3D point Jacobian with respect to the pose twist is the standard 3 6 form µC ξ = (cid:2) [µC] (cid:3), (5) where [] is the skew-symmetric matrix. With calibrated projection π, the image-plane mean µI = π(µC) has Jacobian = Jπ(µC) (cid:2) [µC] (cid:3), (6) µI ξ where Jπ is the 2 3 projection Jacobian evaluated at µC. The screen-space covariance ΣI from Eq. (1) depends on both the projection Jacobian and the rotation, using the chain rule, ΣI ξ = ΣI J µC µC ξ + ΣI R ξ , (7) with R/ξ obtained from the Lie algebra relation δR [δω]R for an infinitesimal rotation δω. These analytic Jacobians remove the overhead of generic autodiff and match the degrees of freedom of the pose, which is essential for fast and stable tracking under tight per-frame budget. 3.2.4. Optimization Solver and Weighting Scheme We minimize the photometric objective in Eq. (3) using first-order optimizer with cosine learning rate schedule, and apply robust penalty to per-pixel residuals. The perpixel weight combines exposure correction, edge awareness, and visibility (via screen-space opacity) so that informative regions dominate the update. Because the 3DGS renderer and the pose Jacobians are fully analytic, gradients propagate through Eq. (2) and Eq. (6) without resorting to expensive automatic differentiation. 3.3. Keyframe Scheduling by Co-Visibility Given the last accepted keyframe Ik , we measure covisibility between the current frame Ik and Ik by the intersection-over-union of visible Gaussians IoU(Ik, Ik ) = (Ik) (Ik ) (Ik) (Ik ) , (8) where (I) collects Gaussians whose screen-space opacity exceeds small threshold on sufficient fraction of pixels. We extract dense visual descriptors using DINOv3 [15], which provide semantically consistent features across views. These descriptors are used to establish multi-view dense correspondences, replacing the residual-driven densification process in GS-SLAM. confidence-aware inlier classifier is then applied to filter unreliable matches, ensuring stable multi-view geometry. Finally, one-shot triangulation is performed to initialize uniformly distributed set of 3D Gaussian seeds. Dense Correspondence. Let Ir be the current keyframe and let Nr be neighbours selected by pose proximity and parallax. dense matcher outputs, for each pair (r, n) with Nr, displacement field urn(p) on Ir and confidence map κrn(p) [0, 1]. correspondence is represented as the pixel pair (cid:0)p, + urn(p)(cid:1), (9) and low-confidence matches are filtered by symmetric epipolar test and spatial blue-noise thinning. We aggregate per-view confidence for each retained reference pixel by κ(p) = 1 Nr (cid:88) nNr κrn(p). (10) Multi-view Triangulation. For each retained pixel and neighbour Nr, we solve two-view linear triangulation. Let Pr, Pn R34 be the camera projection matrices and xr, xn P2 the homogeneous pixel coordinates. We form = P 3 xx 3 xy nP 3 xx nP 3 xy 1 2 1 2 , = arg min X=1 X2, then ˆX = X1:3/ X4 (obtained as the right singular vector of associated with the smallest singular value). Among all neighbours we keep the hypothesis with the lowest reprojection error, breaking ties by larger baseline angle. Candidates with small parallax or large reprojection error are rejected. Gaussian Parameter Initialization. Each valid triangulation spawns one Gaussian Gi with world mean = ˆX(p). µW (11) Construct local orthonormal frame Ui = [t1, t2, v], where is the surface normal estimated by plane fit over neighbouring triangulated points, and t1, t2 span the tangent 4 plane. Initialize the covariance as an anisotropic ellipsoid aligned with this frame = Ui diag(cid:0)s2 ΣW , , s2 (cid:1) , (12) where is obtained by back-projecting one-pixel image uncertainty via the projection Jacobian at the reference view, and is set larger to encode depth uncertainty that increases when the baseline angle is small or the triangulation residual is high. The color ci is the median of bilinearly sampled RGB values across supporting views after applying the exposure parameters estimated by tracking. The initial opacity αi is monotone mapping of the aggregated correspondence confidence κ(p) so that unreliable candidates remain visually weak at insertion. Finally, Gaussians are subsampled to maintain uniform spatial coverage before being inserted into the map. 3.5. Joint Mapping and Photometric Refinement At each accepted keyframe, we first perform the single-step dense initialization (Sec. 3.4) to generate Gaussian seeds from multi-view correspondences. The surviving seeds are immediately inserted into without any densification stage. Each newly inserted Gaussian participates in mapping right away. Insertion, Lightweight Merging, and Pruning. Each Gaussian Gi tracks its observation count mi, cumulative visibility vi, and an exponential moving average of its screen-space footprint. To keep memory bounded and remove unstable outliers, we apply lightweight periodic cleanup and prune splats that violate mi < mmin, (cid:1) > σ2 max, tr(cid:0)ΣW vi < vmin, αi < αmin. Neighbouring Gaussians with highly overlapping footprints and similar colors are merged, retaining visibilityweighted mean of color and covariance to avoid overpopulation. Sliding-Window Photometric Refinement. Let denote window around the latest keyframe. We jointly refine {TW C}IW and {ci, αi, µW i } by minimizing , ΣW = (cid:88) IW λpho (cid:13) (cid:13) gI S(G, TW C) + bI (cid:13) (cid:13)1 + R, (13) where gI , bI compensate exposure changes. The regularizer = λiso (cid:88) (cid:13) (cid:13)ΣW (cid:13) tr(ΣW ) 3 I3 (cid:13) (cid:13) (cid:13)F (cid:88) + λα ψ(αi) + λµ (cid:88) (cid:13) (cid:13)µW µW (cid:13) 2 (cid:13) (14) discourages needle-shaped ellipsoids, avoids degenerate transmittance, and stabilizes early iterations via an EMA 5 anchor µW . We alternate pose-only updates and full map updates with robust per-pixel weights. Gradients propagate through the analytic α-compositing in Eq. (2) and the pose Jacobians in Eq. (6). 3.6. System Schedule and Computational Profile Each incoming frame is tracked for Kt gradient steps using the photometric objective in Eq. (3). When the co-visibility test Eq. (23) accepts keyframe, we select neighbours from and execute the single-step dense initialization of Sec. 3.4 (dense correspondence, weighted multi-view triangulation, and parameter initialization) in one pass. The resulting Gaussians are immediately inserted into G, followed by Km mapping iterations over the current window optimizing Eq. (13), and lightweight cleanup as described in Sec. 3.5. Replacing iterative densification with this singlestep initialization reduces the drift of newly added parameters and lowers the number of mapping iterations required to reach the same photometric fidelity, improving wall-clock throughput without changing the objective or renderer. 4. Experiments 4.1. Experimental Setup Datasets. We evaluate on TUM RGB-D and Replica. TUM RGB-D is evaluated in both monocular and RGB-D settings. Replica is employed for photometric map evaluation on room02 and office04, matching the splits used in our tables to ensure comparability of rendering metrics and throughput. Implementation. Gaussian rasterization and gradients are implemented in CUDA, and the remaining pipeline is in PyTorch. Mixed precision is enabled where beneficial. Tracking runs in real time, while mapping executes asynchronously within bounded local window. Non-standard hyperparameters (learning rate schedule, window sizes, keyframe and culling thresholds) are provided in the supplementary. Evaluation Metrics. Tracking accuracy uses RMSE of Absolute Trajectory Error (ATE) on keyframes. Photometric quality adopts PSNR [3], SSIM [19], and LPIPS [24]. Reconstruction quality reports Acc. [cm], Comp. [cm], and Comp.Ratio (%). Unless specified, we uniformly sample 50K surface points, set τ = 5 cm, and average per scene. Photometric metrics are computed on every fifth frame excluding keyframes. Reconstruction metrics use the same sampling protocol. Each experiment is repeated three times, and the mean results are reported. Baseline Methods. We compare with iMAP [16], NICE-SLAM [26], Vox-Fusion [21], ESLAM [6], SplaTAM [7], Point-SLAM [12], Co-SLAM [18], Gauss-SLAM [20], and MonoGS [11]. We also include SNI-SLAM [25] for reconstruction and Photo-SLAM [4], rendering. GLORIE-SLAM [23], RK-SLAM [10] RGB-Donly methods run in RGB-D, with monocular results reported only when supported. Hyperparameters follow official documented defaults on the same splits. for 4.2. Training and Convergence Analysis The system optimizes scene specific Gaussian parameters and camera poses using the same optimizer, schedule, and window size as MonoGS, with densification removed. Each frame is tracked for Kt steps. Each accepted keyframe triggers the single-step dense initialization followed by Km mapping steps with exposure compensation. Wall clock time is measured on identical hardware and stopping criteria. Results are summarized in Table 1. On TUM RGB-D the average training time decreases from 14.8 to 12 minutes while maintaining localization and rendering quality. Table 1. Optimization time (min) on TUM RGB-D sequences fr1/desk, fr2/xyz, and fr3/office."
        },
        {
            "title": "Method",
            "content": "fr1/desk fr2/xyz fr3/office Avg. MonoGS [11] Ours (RGB) 6.4 4.9 20.6 16. 17.5 15.0 14.8 12.0 4.3. Localization Accuracy We evaluate trajectory accuracy on Replica and TUM (Tables 2, 3). On Replica, the average ATE is 0.61 cm across r0r2, o0o4, outperforming iMAP (2.58), NICE-SLAM (1.07), Vox-Fusion (3.09), and ESLAM (0.90 cm), while remaining competitive with Point-SLAM and MonoGS. the average ATE is 1.02 cm on On TUM RGB-D, fr1/desk, fr2/xyz, and fr3/office, achieving better results than MonoGS and surpassing the same baselines. Figure 3 shows trajectory comparison on living-room scene from the Replica dataset, where the red line indicates the ground-truth (GT) path and the green line represents the estimated trajectory. Among existing methods, SplaTAM, iMAP, Vox-Fusion, and Point-SLAM exhibit large localization drift, with evident deviations from the GT path. MonoGS and our method perform significantly better. In this visualization, the red line is drawn above the green line, so greater overlap, where the red line covers the green one, intuitively reflects smaller localization error. Our method achieves higher overlap ratio, indicating closer adherence to the GT trajectory and better pose consistency. The zoom-in comparison further shows smoother alignment and reduced drift compared with MonoGS, demonstrating stronger robustness in long-term tracking and loop-closure maintenance. 4.4. Rendering Quality and Throughput We report fidelity and throughput in Table 4 and 5. On Replica, our initializer averages 925 FPS, exceedTable 2. Camera tracking results on the Replica dataset under the RGB-D setting. Reported values denote RMSE of ATE across room02 and office04. Method room0 room1 room2 office0 office1 office2 office3 office4 Avg. iMAP [16] NICE-SLAM [26] Vox-Fusion [21] ESLAM [6] Point-SLAM [12] MonoGS [11] Ours (RGB) 3.12 0.97 1.37 0.71 0.61 0.62 0.45 2.54 1.31 4.70 0.70 0.41 0.62 0. 2.31 1.07 1.47 0.52 0.37 0.77 0.53 1.69 0.88 8.48 0.57 0.38 0.44 0.52 1.03 1.00 2.04 0.55 0.48 0.52 0.78 3.99 1.06 2.58 0.58 0.54 0.23 1.03 4.05 1.10 1.11 0.72 0.69 0.62 0.45 1.93 1.13 2.94 0.63 0.72 2.25 0. 2.58 1.07 3.09 0.63 0.53 0.76 0.61 Table 3. Camera tracking results on the TUM RGB-D dataset. Values denote RMSE of ATE over fr1/desk, fr2/xyz, and fr3/office. Method fr1/desk fr2/xyz fr3/office Avg. iMAP [16] NICE-SLAM [26] DI-Fusion [5] Vox-Fusion [21] ESLAM [6] Co-SLAM [18] Point-SLAM [12] MonoGS [11] Ours (RGB) 4.90 4.26 4.40 3.52 2.47 2.40 4.34 1.50 1.02 2.00 6.19 2.00 1.49 1.11 1.70 1.31 1.44 0.98 5.80 3.87 5.80 26.01 2.42 2.40 3.48 1.49 1.05 4.23 4.77 4.07 10.34 2.00 2.17 3.04 1.47 1.02 ing MonoGS (769 FPS) while maintaining competitive PSNR [3], SSIM [19], and LPIPS [24] across room02 and office04. On TUM, the system runs in real time (2.53.2 FPS) with PSNR/SSIM comparable to SplaTAM, PhotoSLAM, and GLORIE-SLAM, and low LPIPS. The throughput gain arises from the keyframe-triggered single-step denTable 4. Rendering quality results on the Replica dataset across room02 and office04. Method (FPS) Metric room0 room1 room2 office0 office1 office2 office3 office4 Avg. NICE-SLAM [26] (6.54) Vox-Fusion [21] (2.17) Point-SLAM [12] (1.33) Co-SLAM [18] SplaTAM [7] PSNR[dB] 22.12 SSIM 0.689 LPIPS 0.330 PSNR[dB] 22.39 SSIM 0.683 LPIPS 0.303 PSNR[dB] 32.40 SSIM 0.974 LPIPS 0. PSNR[dB] 28.88 SSIM 0.892 LPIPS 0.213 PSNR[dB] 32.49 SSIM 0.975 LPIPS 0.072 Gauss-SLAM [20] PSNR[dB] 29.57 0.944 0.197 SSIM LPIPS MonoGS [11] (769) Ours (RGB) (925) PSNR[dB] 34.83 SSIM 0.954 LPIPS 0.068 PSNR[dB] 35.95 SSIM 0.852 LPIPS 0.085 22.47 0.757 0.271 22.36 0.751 0.269 34.08 0.977 0.116 28.51 0.843 0. 33.72 0.970 0.096 31.61 0.952 0.184 36.43 0.959 0.076 33.55 0.945 0.092 24.52 0.814 0.208 23.92 0.798 0. 35.50 0.979 0.111 29.37 0.851 0.215 34.65 0.980 0.078 33.46 0.973 0.148 37.49 0.965 0.075 32.45 0.985 0. 29.07 0.874 0.229 27.79 0.857 0.241 38.26 0.982 0.100 35.44 0.854 0.177 38.29 0.982 0.086 38.39 0.985 0. 39.95 0.971 0.072 34.45 0.952 0.088 30.34 0.868 0.181 29.83 0.876 0.184 39.16 0.986 0.118 34.63 0.826 0. 39.04 0.982 0.093 39.62 0.991 0.097 42.09 0.974 0.055 35.45 0.925 0.096 19.66 0.797 0.235 20.33 0.794 0. 33.98 0.962 0.156 26.56 0.814 0.172 31.91 0.965 0.100 32.91 0.974 0.158 36.24 0.964 0.078 34.87 0.952 0. 22.23 0.801 0.209 23.47 0.803 0.213 33.48 0.960 0.132 28.79 0.866 0.163 30.05 0.952 0.110 33.62 0.982 0. 36.70 0.963 0.065 34.02 0.855 0.101 24.94 24.42 0.856 0.809 0.198 0.233 25.21 24.41 0.847 0.801 0.199 0.236 33.49 35.17 0.979 0.975 0.142 0.124 32.16 28.42 0.856 0.837 0.176 0. 31.83 30.98 0.949 0.953 0.150 0.179 34.26 30.90 0.979 0.972 0.138 0.229 36.07 37.50 0.957 0.960 0.099 0.070 35.85 34.57 0.961 0.928 0.096 0.093 Figure 3. Trajectory comparison on living-room scene. The red line indicates the ground-truth path and the green line shows the estimated trajectory. Our method aligns more closely with the ground-truth and exhibits fewer large drifts than previous systems. Figure 4. Rendering results on the TUM dataset. The proposed keyframe-triggered single-step initialization produces sharper edges, fewer transparency artifacts, and more consistent colors than residual-driven densification. se initialization, which fixes Gaussian topology upfront and removes residual-driven densification, reducing per-frame cost. Qualitative results in Figure 4 show sharper edges, fewer transparency artifacts, and more consistent colors than residual-driven baselines. 4.5. Reconstruction Fidelity Geometric fidelity is evaluated using accuracy, completeness, and completeness ratio in Table 6, computed on aligned point clouds under standard thresholds. Our method attains 1.537 cm accuracy and 1.477 cm completeness with 97.843% completeness ratio. Relative to SNI-SLAM, ac7 Table 5. Rendering quality results on the TUM RGB-D dataset. Table 6. Reconstruction results on the Replica dataset. Lower is better for Acc./Comp., higher for Comp.Ratio. Method (FPS) Metric fr1/desk fr2/xyz fr3/office Avg. Point-SLAM [12] Photo-SLAM [4] MonoGS [11] PSNR[dB] SSIM LPIPS PSNR[dB] SSIM LPIPS PSNR[dB] SSIM LPIPS GLORIE-SLAM [23] PSNR[dB] SplaTAM [7] RK-SLAM [10] Ours (RGB) SSIM LPIPS PSNR[dB] SSIM LPIPS PSNR[dB] SSIM LPIPS PSNR[dB] SSIM LPIPS 13.79 0.625 0. 20.97 0.740 0.230 19.67 0.730 0.330 20.26 0.790 0.310 21.49 0.839 0.255 22.31 0.741 0.254 23.11 0.853 0. 17.62 0.710 0.584 21.07 0.730 0.170 16.17 0.720 0.310 25.62 0.720 0.090 25.06 0.950 0.099 22.47 0.729 0. 24.85 0.896 0.198 18.29 0.749 0.452 19.59 0.690 0.240 20.63 0.770 0.340 21.21 0.720 0.320 21.17 0.861 0. 20.67 0.710 0.251 23.59 0.801 0.219 16.57 0.695 0.527 20.54 0.720 0.213 18.82 0.740 0.327 22.36 0.743 0. 22.57 0.883 0.192 21.82 0.727 0.242 23.85 0.850 0.216 curacy improves by 20.9% and completeness by 13.2%, together with 1.22-point gain in completeness ratio. The margins over ESLAM and Vox-Fusion are larger, including 42% reduction in completeness error against Vox-Fusion. The improvements are consistent across scenes with thin structures and clutter, where coverage gaps and overregularization commonly inflate geometric error. Qualitative inspection shows reduced truncation at object boundaries, cleaner reconstruction of high-frequency edges, and better recovery of small appendages. We attribute these outcomes to anisotropic Gaussian primitives with visibilityaware α-compositing, which sharpen depth gradients and limit color bleeding, and to bounded, keyframe-related optimization that preserves spatial coverage without topology changes. By keeping the Gaussian set fixed after dense seeding, the optimization remains stationary and avoids late-map artifacts, which stabilizes surface inference and suppresses oversmoothing during refinement. 4.6. Ablation Study Effect of Dense Initialization. Consistent rendering gains on TUM RGB, with higher PSNR/SSIM and lower LPIPS/RMSE across all sequences. On fr1.desk, fr2.xyz, and fr3.office, PSNR improves to 23.11, 24.85, and 23.59 with SSIM gains and LPIPS/RMSE drops  (Table 7)  . Distributed Gaussian seeds, whose multi-view triangulation stabilizes mapping under larger motion and maintains coverage in low-parallax segments. This yields faster converMethods iMAP [16] NICE-SLAM [26] Vox-Fusion [21] Co-SLAM [18] ESLAM [6] SNI-SLAM [25] Ours Reconstruction Acc. [cm] Comp. [cm] Comp.Ratio (%) 3.624 2.373 1.882 2.104 2.082 1.942 1.537 4.934 2.645 2.563 2.082 1.754 1.702 1. 80.515 91.137 90.936 93.435 96.427 96.624 97.843 gence and fewer artifacts on thin structures and cluttered regions. Without dense initialization, residual driven densification converges slowly, exhibits early spatial inconsistency, and tends to over-smooth before adequate coverage is established. Table 7. Impact of DFM on the TUM RGB-D dataset. Method PSNR SSIM LPIPS RMSE fr1 desk fr2 xyz fr3 office w/o DFM 19.67 23.11 Ours w/o DFM 16.17 24.85 Ours w/o DFM 20.63 23.59 Ours 0.73 0.853 0.72 0.896 0.77 0.801 0.33 0.232 0.31 0.198 0.34 0. 1.5 1.02 1.44 0.98 1.49 1.05 Effect of Gaussian Count per Keyframe on Tracking. We vary the number of newly triangulated 3D points per keyframe, with each verified point instantiated as Gaussian primitive, so the abscissa in Figure 5 corresponds to the count of Gaussians. Increasing the budget from 200 to 1000 points reduces the tracking error sharply, reaching about 0.7cm at 1000. Beyond this regime the curve plateaus and improvements are marginal, approaching roughly 0.6 cm at 2000. We therefore adopt 1000 points per keyframe as the default trade-off between accuracy, memory, and runtime. 5. Conclusion We presented RGS-SLAM, Gaussian-splatting SLAM framework that replaces residual-driven densification with keyframe-triggered one-shot initialization. By integrating dense feature matching and multi-view triangulation, the system provides stable Gaussian seeds for differentiable optimization under analytic SE(3) Jacobians. Experiments on Replica and TUM RGB-D demonstrate improved efficiency without sacrificing localization accuracy or rendering fidelity. The proposed design remains fully compatible with existing SLAM pipelines, offering practical path to8 Radiance Field Rendering. ACM Transactions on Graphics, 2023. 2 [9] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. Lightglue: Local feature matching at light speed. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 2 [10] Xiasheng Ma, Ci Song, Yimin Ji, and Shanlin Zhong. Related keyframe optimization Gaussiansimultaneous localization and mapping: 3D Gaussian Splatting-based simultaneous localization and mapping with related keyframe optimization. Applied Sciences, 2025. 6, 8 [11] Hidenobu Matsuki, Riku Murai, Paul Kelly, and Andrew J. In Proceedings of Davison. Gaussian Splatting SLAM. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 5, 6, 8 [12] Erik Sandstrom, Yue Li, Luc Van Gool, and Martin R. Oswald. Point-SLAM: Dense neural point cloud-based SLAM. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023. 2, 5, 6, [13] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, Superglue: Learning feature and Andrew Rabinovich. In Proceedings of matching with graph neural networks. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2 [14] Johannes L. Schonberger and Jan-Michael Frahm. StructureIn Proceedings of the IEEE/CVF from-motion revisited. Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2 [15] Oriane Simeoni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, Timothee Darcet, Theo Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, Herve Jegou, Patrick LaarXiv preprint batut, and Piotr Bojanowski. DINOv3. arXiv:2508.10104, 2025. 4 [16] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J. Davison. imap: Implicit mapping and positioning in real-time. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. 5, 6, 8 [17] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. LoFTR: Detector-free local feature matching with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2 [18] Hengyi Wang, Jingwen Wang, and Lourdes Agapito. CoSLAM: Joint coordinate and sparse parametric encodings for In Proceedings of the IEEE/CVF neural real-time SLAM. Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 5, 6, [19] Zhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: From error visibility IEEE Transactions on Image Proto structural similarity. cessing, 13(4):600612, 2004. 5, 6 Figure 5. Tracking error versus Gaussian count. Error decreases rapidly with denser seeding and plateaus near 1000 Gaussians, indicating diminishing returns beyond this density. ward scalable, differentiable mapping."
        },
        {
            "title": "References",
            "content": "[1] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2018. 2 [2] Johan Edstedt, Ioannis Athanasiadis, Marten Wadenback, and Michael Felsberg. DKM: Dense kernelized feature In Proceedings of the matching for geometry estimation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 2 [3] Alain Hore and Djemel Ziou. Image quality metrics: PSNR vs. SSIM. In Proceedings of the IEEE International Conference on Image Processing (ICIP), 2010. 5, 6 [4] Huajian Huang, Longwei Li, Hui Cheng, and Sai-Kit Yeung. Photo-SLAM: Real-time simultaneous localization and photorealistic mapping for monocular, stereo, and RGB-D In Proceedings of the IEEE/CVF Conference on cameras. Computer Vision and Pattern Recognition (CVPR), 2024. 2, 5, 8 [5] Jiahui Huang, Shi-Sheng Huang, Haoxuan Song, and ShiMin Hu. DI-Fusion: Online implicit 3D reconstruction with deep priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 6 [6] Mohammad Mahdi Johari, Camilla Carta, and Francois Fleuret. ESLAM: Efficient dense SLAM system based on hybrid representation of signed distance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. 5, 6, 8 [7] Nikhil Keetha, Jay Karhade, Krishna Murthy Jatavallabhula, Gengshan Yang, Sebastian Scherer, Deva Ramanan, and Jonathon Luiten. Splatam: Splat, track & map 3D Gaussians for dense RGB-D SLAM. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 5, 6, [8] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3D Gaussian Splatting for Real-Time [20] Chao Yan, Zirui Wang, Zhiqiang Li, Wei Gao, Hao Wang, Guofeng Zhang, Hujun Bao, and Xiaowei Zhou. GS-SLAM: 9 In ProDense visual SLAM with 3D Gaussian Splatting. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 5, 6 [21] Xingrui Yang, Hai Li, Hongjia Zhai, Yuhang Ming, Yuqian Liu, and Guofeng Zhang. Vox-fusion: Dense tracking and mapping with voxel-based neural implicit representation. In Proceedings of the IEEE International Symposium on Mixed and Augmented Reality (ISMAR), pages 499507, 2022. 5, 6, 8 [22] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. MVSNet: Depth inference for unstructured multiview stereo. In Proceedings of the European Conference on Computer Vision (ECCV), 2018. [23] Ganlin Zhang, Erik Sandstrom, Youmin Zhang, Manthan Patel, Luc Van Gool, and Martin R. Oswald. GLORIE-SLAM: Globally optimized RGB-only implicit encoding point cloud SLAM. arXiv preprint arXiv:2403.19549, 2024. 2, 6, 8 [24] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of In Proceedings of deep features as perceptual metric. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 5, 6 [25] Siting Zhu, Guangming Wang, Hermann Blum, Jiuming Liu, Liang Song, Marc Pollefeys, and Hesheng Wang. SNISLAM: Semantic neural implicit SLAM. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2116721177, 2024. 5, 8 [26] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun Bao, Zhaopeng Cui, Martin R. Oswald, and Marc Pollefeys. NICE-SLAM: Neural implicit scalable encoding for In Proceedings of the IEEE/CVF Conference on SLAM. Computer Vision and Pattern Recognition (CVPR), 2022. 5, 6, 8 10 RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Reproducibility and Code Release All components of the system are implemented in PyTorch with custom CUDA kernels for rasterization and analytic Jacobians. The repository provides the full SLAM pipeline, configuration files, scripts for evaluation on TUM RGB-D and Replica, and instructions for reproducing all quantitative tables and qualitative renderings in the main paper. The training and optimization settings match those described in Sections 3 and 4 of the main paper. An anonymized version of the codebase is available at: https://anonymous.4open.science/r/RGSSLAM 7. Discussion 7.1. Limitations RGS-SLAM still exhibits several practical limitations despite the gains over residual-driven densification. The current evaluation focuses on indoor static scenes in Replica and TUM RGB-D, so the robustness of the one-shot Gaussian initialization in highly dynamic scenes, or under severe rolling shutter remains unclear. The dense correspondences are derived from single pretrained DINOv3 backbone together with confidence aware inlier classifier, which can degrade under drastic appearance changes, strong motion blur, or camera viewpoints far outside the training distribution, and in these regimes the triangulated seeds may become biased or incomplete. The fixed topology after each keyframe initialization improves stability but can leave persistent coverage gaps when large textureless surfaces, fine specular structures, or objects with weak visual support never accumulate enough consistent matches, which constrains the reconstruction quality. The current implementation also assumes calibrated pinhole camera and synchronized RGB-D streams, without exploiting lidar cues or inertial measurements that are available on many robotic platforms. Finally, the system still requires GPU with moderate memory to sustain high frame rates, and the practical impact of memory budgets, long-term map growth, and large-scale loop closure has not yet been characterized on resource constrained devices. 7.2. Societal Impact RGS-SLAM advances camera based dense SLAM with training-free one-shot Gaussian initialization that stabilizes optimization and improves throughput, which can benefit robotics, extended reality, and digital twin systems through more reliable mapping and safer physical interaction. At the same time, dense reconstruction of indoor environments raises privacy risks whenever RGB or RGB-D streams are captured and stored without informed consent, since metrically consistent maps can reveal layouts, personal belongings, and usage patterns. The method is training-free and easily integrated into existing SLAM stacks, which lowers the barrier for large-scale deployment and makes responsible use dependent on appropriate safeguards such as transparent data handling, limited retention of raw sensor data, access control to stored maps, and preference for local processing. Our experiments rely on public benchmarks without personally identifiable information and the code release is intended for reproducible research, although future work should pair technical advances in robust mapping with privacy aware data design and interdisciplinary guidelines for ethical deployment. 7.3. Training Details Optimization of Gaussian maps. The optimization settings used by our Gaussian SLAM system are summarized in Table 8. For both initialization and online mapping we use the Adam optimizer with shared parameterization for geometry and appearance. The base learning rate is set to 2.0 103 for color and opacity parameters and 1.0 103 for positions and rotations, with (β1, β2) = (0.9, 0.999). cosine decay schedule is applied within each optimization window so that early iterations focus on rapid geometry refinement while later iterations stabilize the map. Initialization uses window of five frames and runs for 1050 iterations before the system starts live tracking. During online operation every incoming frame is refined for 30 tracking iterations and each accepted keyframe-triggers 60 mapping iterations over the same local window. The loss combines an L1 term and an SSIM term weighted by λdssim = 0.2. Dynamic density control uses the same thresholds across all experiments and adopts the opacity culling and densification settings listed in Table 8. Mixed precision training and gradient norm clipping are enabled to keep the per frame compute budget and memory stable. Dense initialization baseline. As an additional baseline we adopt dense correspondence based initialization strategy that shares the same differentiable Gaussian representation as our system. The training configuration is summarized in Table 9. The model is optimized for 30000 iterations with Adam and separate learning rates for spatial and appearance parameters. Position updates follow decayed 1 Table 8. Optimization configuration for the Gaussian SLAM. Table 9. Training configuration of the dense initialization baseline."
        },
        {
            "title": "Value",
            "content": "Optimizer Adam betas Base LR (color, opacity) Base LR (position, rotation) Learning rate schedule Loss SSIM weight λdssim Initialization iterations Tracking iterations per frame Mapping iterations per keyframe Local window size Densification interval Opacity reset interval Opacity culling threshold Gradient clipping max norm Precision Adam β1 = 0.9, β2 = 0.999 2.0 103 1.0 103 Cosine decay L1 + λdssimLSSIM 0.2 1050 30 60 5 frames 100 iterations 3000 iterations 0.05 1.0 Mixed FP16 / FP32 schedule from 1.6 104 to 1.6 106, while feature, opacity, scaling, and rotation parameters use constant rates that match the public configuration. The reconstruction loss combines an ℓ1 term with structural similarity component with λdssim = 0.2. Densification is enabled from iteration 500 to 15000 with an interval of 100 iterations and gradient based threshold of 2.0 104. All experiments use degree three spherical harmonics, batch size 64, and fixed background color without random perturbations on CUDA device. 8. Implementation Details Datasets. We evaluate on the TUM RGB-D and Replica benchmarks, consistent with Section 4 of the main paper. TUM RGB-D is used in both monocular and RGB-D configurations, following standard practice in visual SLAM. Replica is employed for photometric map evaluation and trajectory accuracy on the eight standard indoor scenes room0 to room2 and office0 to office4. These splits match those used in the main tables so that rendering metrics, throughput, and localization error remain directly comparable across methods. Evaluation Metrics. Camera tracking accuracy is measured using the root mean square error of the Absolute Trajectory Error (ATE) computed on keyframes. Photometric map quality is evaluated with three rendering metrics: PSNR, SSIM, and LPIPS. Geometric reconstruction quality is assessed with accuracy (Acc. [cm]), completeness (Comp. [cm]), and completeness ratio (Comp.Ratio [%]). Accuracy is defined as the mean nearest-neighbour distance from reconstructed points to the ground-truth surface. Total iterations Optimizer Spherical harmonics degree Batch size Position LR (init final) Feature learning rate Opacity learning rate Scaling learning rate Rotation learning rate SSIM weight λdssim Dense correspondence ratio Densification interval Densification range Densification gradient threshold Opacity reset iteration Exposure LR (init final) Random background Logging train camera index Logging test camera index Dataset resolution White background Data device"
        },
        {
            "title": "30000\nAdam\n3\n64\n1.6 × 10−4 → 1.6 × 10−6\n2.5 × 10−3\n2.5 × 10−2\n5.0 × 10−3\n1.0 × 10−3\n0.2\n0.01\n100 iterations\niter. 500 to 15000\n2.0 × 10−4\n30000\n1.0 × 10−2 → 1.0 × 10−4\ndisabled\n50\n10\noriginal\nfalse\nCUDA",
            "content": "Completeness is the mean nearest-neighbour distance from ground-truth surface samples to the reconstruction. The completeness ratio is the proportion of ground-truth samples within distance threshold τ from the reconstruction. Unless stated otherwise, we uniformly sample 50K points on each surface, set τ = 5 cm, and average per-scene scores over the benchmark sequences. For photometric rendering metrics, we evaluate every fifth frame and exclude keyframes to avoid bias toward training views. Reconstruction metrics are computed with the same surface-sampling protocol. Each experiment is repeated three times on identical hardware and stopping criteria, and all tables report the mean scores. In every table, the best result is typeset in bold and the second best is underlined. 8.1. Compare Model Settings We compare RGS-SLAM with set of representative dense SLAM pipelines that cover implicit volumes, voxel grids, point clouds, and Gaussian splats under the same scene types and benchmarks. NICE-SLAM, Co-SLAM, VoxFusion, DI-Fusion, and SNI-SLAM operate on RGB-D input and maintain volumetric implicit or voxel based representations that are optimized per scene. iMAP and PointSLAM map monocular or RGB-D streams to neural fields or point clouds with scene specific training and joint pose refinement. SplaTAM, Gauss-SLAM, MonoGS, and RK2 SLAM adopt 3D Gaussian splatting and couple Gaussian renderer with pose and appearance optimization, while Photo-SLAM and GLORIE-SLAM combine differentiable rendering with explicit point or mesh structures. based refinement of the camera pose TW C. We render synthesized image ˆI(x; G, TW C) at the native resolution and apply an affine brightness model with gain gI and bias bI for each frame, 8.2. Computing Resource Configuration II (x; G, TW C) = gI ˆI(x; G, TW C) + bI . (17) All experiments are run on workstation equipped with two NVIDIA L40 GPUs and an Intel Xeon Platinum 8362 CPU at 2.80 GHz. Time-critical components, including 3D Gaussian rasterization and gradient computation, are implemented in CUDA, while the remaining SLAM pipeline is implemented in PyTorch. Mixed precision is enabled for rendering and backpropagation whenever this improves throughput without degrading stability. The tracking loop operates in real-time, and mapping is executed asynchronously within bounded local window so that latency remains stable as the map grows. 9. Methodology Details 9.1. Gaussian Splatting Representation Each Gaussian Gi is represented by compact tuple coni R3, an opacity paramtaining the world space mean µW eter αi [0, 1], set of view dependent color coefficients, and covariance parameterization. In the underlying model the covariance appears as full matrix ΣW , while in the imi plementation it is encoded as rotation matrix Ri SO(3) and three axis aligned scales si R3 ΣW = Ri diag(s2 + such that ) . The parameters (gI , bI ) are estimated by small least squares problem (gI , bI ) = arg min g,b (cid:88) (cid:16) xΩI I(x) ˆI(x; G, TW C) (cid:17)2 , (18) and the resulting solution is substituted into the photometric objective so that pose updates remain invariant to slow exposure drift."
        },
        {
            "title": "The tracking loss can be written in the form",
            "content": "Ltrack(TW C) = (cid:88) xΩI wI (x) (cid:13) (cid:13)I(x) II (x; G, TW C)(cid:13) (cid:13)1, (19) where wI (x) denotes per pixel weight. In practice this weight is factored into opacity and gradient terms, wI (x) = wα(x) w(x), with wα(x) = clip (15) w(x) = clip (cid:19) , 0, 1 (cid:18) ˆα(x) τα (cid:18) I(x)2 τ , (cid:19) , 0, 1 , (20) (21) (22) The rotation is stored as unit quaternion and the scales are optimized in logarithmic space, which guarantees positive definiteness under gradient updates. Colors are represented by second order spherical harmonics in camera space. For viewing direction S2, the color of Gi is Ci(v) = 2 (cid:88) ℓ (cid:88) ℓ=0 m=ℓ ci,ℓm ℓ (v), (16) where ci,ℓm R3 are learned RGB coefficients and real spherical harmonics. ℓ are Projection to the image plane uses the calibrated camera intrinsics together with the Gaussian projection model, and screen space compositing applies standard alpha compositing. All attributes are packed into contiguous GPU buffers and updated in place. This layout lets the renderer handle several million Gaussians without fragmentation and keeps memory access patterns coherent during both forward and backward passes. where ˆα(x) is the accumulated opacity at pixel x, τα and τ are fixed thresholds, and clip(z, a, b) = min(max(z, a), b). Pixels with low opacity or weak gradients therefore have reduced influence in the optimization. The pose is updated in the minimal twist coordinates ξ R6 using standard left multiplicative update rule on SE(3). The Jacobians of the camera projection and the image formation model are implemented analytically and are reused across all Gaussians that share the same pose, which reduces both computation and memory traffic. The derivative of the projected covariance with respect to pose is evaluated by an explicit chain rule involving the projection Jacobian and the local rotation. This avoids generic automatic differentiation through the entire renderer. In practice, between thirty and sixty gradient steps per frame are performed using the Adam optimizer with cosine learning rate schedule centered around 5 103. This configuration yields stable tracking even when large parts of the image are textureless or underexposed. 9.2. Tracking and Camera Pose Optimization 9.3. Keyframe Scheduling by Co-Visibility Given the current map and new RGB or RGB-D frame, pose tracking alternates between rendering and gradient Keyframe scheduling uses co-visibility based policy. For each incoming frame we maintain binary visibility mask that records, for every pixel, whether its accumulated screen space opacity exceeds small threshold. Let Va and Vb denote the sets of visible pixels in images Ia and Ib. The covisibility score is defined as IoU(Ia, Ib) = Va Vb Va Vb . (23) The intersection and union are counted entirely on the GPU. frame Ik is promoted to keyframe when the covisibility IoU(Ik, Ik ) with the last keyframe Ik falls below user defined threshold τ and when the relative translation and rotation exceed small geometric bounds. These additional bounds avoid accepting nearly redundant viewpoints that would increase memory and computation without improving triangulation baselines. Accepted keyframes are stored in bounded buffer together with their poses. Between eight and twelve recent keyframes are kept, which is sufficient to form wellconditioned local triangulation baselines while keeping all multi view operations inexpensive. The same buffer also provides the neighbour set used in the mapping stage. 9.4. Dense Feature Matching and Triangulation Dense matching and multi view initialization rely on DINOv3 features computed at fixed feature resolution obtained by downsampling the RGB images. The descriptors are ℓ2 normalized per pixel. For each reference keyframe Ir neighbour set Nr is selected based on pose proximity and parallax, using the current camera estimates. Let fr(p) and fn(q) denote DINOv3 descriptors at pixels and in images Ir and In. For each neighbour Nr and displacement urn(p), the raw descriptor similarity is srn(p) = (cid:10)fr(p), fn (cid:0)p + urn(p)(cid:1)(cid:11). (24) This similarity is combined with forward backward consistency and epipolar agreement into scalar score κrn(p) = wsim srn(p) + wfb ρfb(p) + wepi ρepi(p), (25) where ρfb and ρepi quantify consistency of the forward backward displacement and the epipolar distance, and wsim, wfb, and wepi are fixed weights. The confidence κrn(p) is obtained by piecewise linear mapping to [0, 1], κrn(p) = clip (cid:18) κrn(p) γ0 γ1 γ (cid:19) , 0, 1 , (26) with fixed thresholds γ0 and γ1. This design keeps the inlier classifier training-free and dataset agnostic. Before triangulation, correspondences are thinned in image space with blue noise pattern in order to avoid redundant seeds in locally homogeneous regions. The aggregated confidence κ(p) is cached for each surviving reference pixel and encodes agreement across multiple neighbours. Linear triangulation uses homogeneous linear system solved with double precision singular value decomposition. Candidates with very small parallax or reprojection error above conservative threshold are discarded, and among hypotheses obtained from different neighbours the one with the smallest reprojection error and sufficiently large baseline angle is kept. 9.5. Gaussian Initialization and Joint Mapping Each valid triangulated point spawns Gaussian Gi whose = ˆX(p). world mean is set to the reconstructed point µW local orthonormal frame Ui = [t1, t2, v] is constructed by fitting plane to triangulated neighbours inside fixed radius around ˆX(p), where is the normal and t1, t2 span the tangent plane in the neighbourhood. The covariance is initialized as an anisotropic ellipsoid aligned with this frame, = Ui diag(cid:0)s2 ΣW , s2 , s2 (cid:1) , (27) where the tangential scale is obtained by back projecting one pixel footprint through the projection Jacobian at the reference view and the axial scale is calibrated function of the baseline angle and triangulation residual, which increases depth uncertainty in poorly conditioned configurations. The initial color ci is the median of bilinear RGB samples across all supporting views after applying the exposure parameters estimated during tracking. The initial opacity αi is obtained as monotone mapping of κ(p), αi = αmin + (cid:0)αmax αmin (cid:1) κ(p), (28) with fixed bounds αmin and αmax in (0, 1). Unreliable seeds with low aggregated confidence therefore enter the map as visually weak Gaussians and are easy to prune. Before insertion, Poisson disk subsampling in world space is applied to promote uniform coverage and to avoid excessive density in locally flat regions. After insertion, the mapping module maintains an observation count mi, cumulative visibility score vi, and an exponential moving average of the world position µW for each Gaussian. The moving average is updated after each mapping step according to µW (t+1) = (1 η) µW (t) + η µW (t+1) , (29) with fixed smoothing factor η (0, 1). The mapping loss is evaluated over sliding window around the latest keyframe with per frame brightness parameters gI and bI and per pixel weights wI (x) as in Eq. (20). regularizer discourages highly elongated covariances, avoids degenerate transmittance, and anchors early updates to µW . The coefficients of this regularizer are kept fixed across all sequences and are not adapted per scene, which keeps the behaviour of the optimizer comparable on TUM and Replica. Pose only updates and full map updates are alternated, and robust per pixel weights are used so that outliers in the photometric residual have limited influence. lightweight merging operation averages color and covariance for pairs of Gaussians that have almost identical screen space footprints and similar appearance. Pruning removes Gaussians that violate bounds on mi, vi, tr(ΣW ), or αi. These maintenance steps keep the representation compact and prevent numerical instabilities caused by extremely large or extremely transparent splats. 9.6. System Schedule and Computational Profile The runtime schedule separates tracking and mapping. Each incoming frame is tracked for Kt gradient steps using the photometric loss described above. This stage updates only the current pose and does not modify the map. If the co-visibility test does not accept the frame as keyframe, the system immediately proceeds to the next image. Let ρ denote the empirical fraction of frames that are selected as keyframes. The average number of gradient based optimization steps per frame is then approximately frame steps Kt + ρ Km, (30) where Km is the number of joint refinement iterations executed after each keyframe. When keyframe is accepted, set of neighbours from is selected and dense matching, confidence aggregation, triangulation, and Gaussian initialization are executed in single GPU pass. The resulting Gaussians are inserted into and immediately participate in mapping. The system then runs Km iterations of joint refinement over the window using the mapping loss and regularizer defined above, followed by maintenance pass that performs merging and pruning. This schedule replaces iterative residual-driven densification with one-shot dense seed and thereby reduces the early drift of newly added parameters. The number of mapping iterations required to reach given photometric fidelity decreases, which improves wall clock throughput while keeping the renderer and optimization objectives identical to those used in the main method description. 10. Additional Experiments 10.1. Additional Rendering on Replica On the Replica dataset, extended qualitative comparisons in Figure 6 show that our keyframe-triggered single-step initialization yields sharper object boundaries and more stable shading than the residual-driven densification baseline across living room and office scenes. The reconstructed views exhibit fewer transparency artifacts around thin structures such as chair legs and table edges, and color transitions remain consistent across viewpoints, which confirms that the proposed initialization creates well-conditioned Gaussian map for subsequent optimization. Challenging regions for Gaussian splatting, including large textureless walls and slanted ceilings, also show reduced blotchy artifacts because the one-shot dense seed avoids early gaps in coverage. These qualitative trends agree with the quantitative gains in reconstruction metrics reported in the main paper and indicate that the initializer improves both convergence speed and the final visual fidelity of the radiance field. 10.2. Camera Tracking on Replica Offices To assess tracking robustness, we visualize top view camera trajectories for two Replica office scenes in Figure 7 and 8. The proposed system maintains tight alignment with ground truth over long paths that include turns, loops, and revisits, and the strong overlap between the predicted and reference trajectories indicates that the jointly optimized poses and Gaussians provide accurate geometric constraints for downstream mapping and loop closure. In the office0 sequence the path combines slow pans and rapid rotations around the desk area, yet our trajectory returns to previously visited regions without noticeable misalignment, while competitor methods accumulate drift near corners and walls. In the office2 sequence the camera passes through narrow corridors before entering wider workspace, and the estimated path from our method preserves the global layout without evident shearing or scale distortion. These qualitative patterns are consistent with the Absolute Trajectory Error reported in the main paper and support the claim that dense Gaussian initialization yields stable optimization landscape for pose refinement. 10.3. Cluttered Desk Reconstruction In cluttered desk sequence with strong self occlusion and fine scale objects such as cables, pencils, and plush toys, shown in Figure 9, we evaluate an additional reconstruction. The left image shows the input RGB view and the right image illustrates the corresponding Gaussian map rendered from novel viewpoint. The reconstruction preserves thin structures and surface boundaries while avoiding the truncation and over smoothing artifacts observed in residual driven pipelines, which demonstrates that the proposed initialization and refinement strategy scales to scenes with complex object layouts and high frequency details. Fine elements such as tripod legs, monitor edges, and scattered stationery remain clearly separated from the background even when objects move partially in and out of view, so the map integrates evidence from multiple viewpoints without duplicated Gaussians. 5 Figure 6. Rendering results on the Replica dataset. The proposed keyframe-triggered single-step initialization produces sharper edges, fewer transparency artifacts, and more consistent colors than residual-driven densification. Figure 7. Tracking on Replica office0. Top-view trajectories, with ground truth in red and model predictions in green. 6 Figure 8. Tracking on Replica office2. Top-view trajectories, with ground truth in red and model predictions in green. Figure 9. Qualitative reconstruction on cluttered desk scene, where the left input RGB frame and the right Gaussian map view demonstrate dense coverage with preserved fine structures and reduced truncation artifacts."
        }
    ],
    "affiliations": [
        "National Yang Ming Chiao Tung University"
    ]
}