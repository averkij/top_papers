{
    "paper_title": "Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners",
    "authors": [
        "Xin Xu",
        "Cliveb AI",
        "Kai Yang",
        "Tianhao Chen",
        "Yang Wang",
        "Saiyong Yang",
        "Can Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Reward (RLVR) effectively solves complex tasks but demands extremely long context lengths during training, leading to substantial computational costs. While multi-stage training can partially mitigate this, starting with overly short contexts often causes irreversible performance degradation, ultimately failing to reduce overall training compute significantly. In this paper, we introduce **T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet effective adaptation to RLVR that bridges long Chain-of-Thought (CoT) distillation and standard RLVR. TFPI employs a simple *ThinkFree* operation, explicitly discarding the thinking content via a direct *</think>* append, to reduce token usage during inference. Training with *ThinkFree*-adapted inputs improves performance and lowers token consumption, even in the original slow-thinking mode. Extensive experiments across various benchmarks have shown that TFPI accelerates RL convergence, achieves a higher performance ceiling, and yields more token-efficient reasoning models without specialized rewards or complex training designs. With TFPI only, we train a 4B model to reach 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 6 2 2 6 2 . 9 0 5 2 : r TFPI: Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners 2025-10-01 Xin Xu1,2, Cliveb AI1, Kai Yang1, Tianhao Chen2, Yang Wang3, Saiyong Yang1,, Can Yang2, 1LLM Department, Tencent 2The Hong Kong University of Science and Technology 3The University of Hong Kong (cid:66) macyang@ust.hk (cid:66) stevesyang@tencent.com"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Reward (RLVR) effectively solves complex tasks but demands extremely long context lengths during training, leading to substantial computational costs. While multi-stage training can partially mitigate this, starting with overly short contexts often causes irreversible performance degradation, ultimately failing to reduce overall training compute significantly. In this paper, we introduce Thinking-Free Policy Initialization (TFPI), simple yet effective adaptation to RLVR that bridges long Chain-of-Thought (CoT) distillation and standard RLVR. TFPI employs simple ThinkingFree operation, explicitly discarding the thinking content via direct </think> append, to reduce token usage during inference. Training with ThinkingFree-adapted inputs improves performance and lowers token consumption, even in the original slow-thinking mode. Extensive experiments across various benchmarks have shown that TFPI accelerates RL convergence, achieves higher performance ceiling, and yields more tokenefficient reasoning models without specialized rewards or complex training designs. With TFPI only, we train 4B model to reach 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours. Github Repo Models [GitHub Page] [Huggingface Models]"
        },
        {
            "title": "Introduction",
            "content": "Reasoning is fundamental aspect of human cognition, and equipping artificial intelligence (AI) with strong reasoning capabilities is critical for its deployment and applications (Morris et al., 2023; Huang et al., 2024; Xu et al., 2025c). Progress in pretraining (Shao et al., 2024; Yang et al., 2024; Chen et al., 2025b), supervised fine-tuning (SFT) (Yu et al., 2023; Xu et al., 2024; Tong et al., 2024; Ye et al., 2025; Muennighoff et al., 2025), rigorous evaluation (Rein et al., 2024; Phan et al., 2025; Xu et al., 2025b), reinforcement learning (RL) (Jaech et al., 2024; Guo et al., 2025) has significantly enhanced the reasoning abilities of large language models (LLMs). Among these, RL with verifiable rewards (RLVR) stands out as an effective approach that enables large language models (LLMs) to generate long Chains-of-Thought (CoT) (Wei et al., 2022) spontaneously, and empowers them with unprecedented performance on challenging reasoning tasks. Thus, these RLVR-trained LLMs are termed as long-CoT LLMs, slow-thinking LLMs, or large reasoning models (LRMs). Compared with initializing from base LLM, starting from an SFT-distilled LRM typically yields better results and accelerates convergence in RLVR (Guo et al., 2025; An et al., 2025). However, SFT-distilled LRMs often produce excessively long responses during the rollout stage of RLVR, which necessitates large training context length for RLVR. Using such large training contexts also incurs substantial computational costs. common mitigation strategy is multistage RLVR, which begins with relatively short context and gradually increases the training length (Luo et al., 2025b; An et al., 2025). Nonetheless, Zeng et al. (2025a) argue that multistage training might cause irreversible performance degradation. Moreover, even multistage training demands significant computational resources. For instance, training 4B model while progressively increasing the maximum context length from 40K to 48K to 52K tokens requires approximately 8K H800 GPU hours (An et al., 2025). These limitations underscore the need for more efficient RLVR training methods. In this work, we introduce Thinking-Free Policy Initialization (TFPI), simple yet effective adaptation to RLVR that bridges long Chain-of-Thought (CoT) distillation and standard RLVR. We first observe that ThinkingFree operationexplicitly discarding the thinking content via direct </think> appendcan substantially reduce token usage during inference (Section 3.1). Training with ThinkingFree-adapted inputs improves performance and lowers token consumption, even when evaluated in the original slow-thinking mode (Section 3.2). We then formally define TFPI in Section 3.3. As illustrated in Figure 1, TFPI accelerates RL convergence, achieves higher Corresponding Authors. 1 Figure 1: Our proposed TFPI accelerates the convergence of RLVR to higher performance ceiling (left) and yields more token-efficient reasoning models (right). Left: avg@32 versus training compute, measured in H20 hours. Direct RL refers to directly training Qwen3-4B with 32K context window using DAPO, while TFPI + RL denotes running 32K-context DAPO after initialization with our 3-stage TFPI. The x-axis for TFPI uses linear scale during the TFPI phase, followed by logarithmic scale, with the transition indicated by black vertical line. Right: Average accuracy on 4 reasoning datasets (AIME24/25, Beyond AIME, and GPQA) versus average output tokens. Points in the upper-left region indicate better performance. Baseline names and their corresponding numbers are listed in Table 3. Red dots denote different stages of our TFPI. performance ceiling, and produces more token-efficient reasoning models without requiring specialized rewards or complex training designs (Section 4). Our contributions are as follows: ❶ We find that ThinkingFree can substantially reduce inference costs for distilled LRMs, and that training with ThinkingFree-adapted inputs enhances slow-thinking capability. ❷ We propose TFPI, fast, low-cost initialization phase for long-CoT RL that accelerates RL convergence and generalizes across domains, even when trained solely on mathematics. ❸ We show that long-CoT RL following TFPI achieves higher performance ceiling while producing more token-efficient LRMs without the need for specialized rewards or complex training designs, offering an effective and efficient route to training high-performing LRMs. ❹ We provide both behavioral and parameter-level analyses and reveal that TFPI not only preserves the slow-thinking reasoning pattern but also enables substantially faster rollouts in subsequent long-CoT RL stages."
        },
        {
            "title": "2 Preliminary",
            "content": "Notation. In this paper, we define an LLM parameterized by θ as policy πθ. Let denote query and the set of queries. Given response to query x, its likelihood under the policy πθ is expressed as πθ(y x) = t=1 πθ (yt x, y<t) , where denotes the number of tokens in y. queryresponse pair (x, y) is scored by rule-based outcome reward r(x, y) {0, 1}, indicating whether the response aligns with the ground truth of x. Proximal Policy Optimization (PPO). PPO (Schulman et al., 2017) constrains the policy update within proximal region of the old policy πθold through the clipping mechanism. Specifically, PPO employs the following objective (we omit the KL regularization term hereinafter for brevity): JPPO(θ) = xD, yπθold (x) (cid:34) 1 y t=1 (cid:16) min rt(θ) (cid:98)At, clip (rt(θ), 1 ε, 1 + ε) (cid:98)At (cid:35) (cid:17) , (1) with the importance ratio of the token yt is defined as rt(θ) = πθ (ytx,y<t) value model, and ε is the clipping range of importance ratios. πθold (ytx,y<t) , the advantage (cid:98)At of yt is estimated by Group Relative Policy Optimization (GRPO). GRPO (Shao et al., 2024) bypasses the need for the value model by computing the relative advantage of each response within group of responses to the same query. Specifically, GRPO optimizes JGRPO(θ) = xD [JGRPO(θ, x)] , where: (cid:34) yi t= ri,t(θ) (cid:98)Ai,t, clip (cid:0)ri,t(θ), 1 ε, 1 + ε(cid:1) JGRPO(θ, x) = 1 yi i=1 min 1 (cid:98)Ai,t (2) (cid:16) (cid:17) (cid:35) . Here {yi}G importance ratio ri,t(θ) and advantage (cid:98)Ai,t of token yi,t are: i=1 πθold (x), is the number of generated responses to each query (i.e., the group size), and the ri,t(θ) = πθ(yi,tx, yi,<t) πθold (yi,tx, yi,<t) , (cid:98)Ai,t = (cid:98)Ai = r(x, yi) mean (cid:0){r(x, yi)}G i=1 std (cid:0){r(x, yi)}G i=1 (cid:1) (cid:1) , (3) Figure 2: Results of the meta-experiment on the ThinkingFree operation. Left: Average output tokens in thinking mode and ThinkingFree mode on AIME25. Right: Evolution of avg@32 and average output tokens on AIME24 with thinking-mode evaluation over training steps. respectively, where all the tokens in yi share the same advantage as (cid:98)Ai. Numerous variants have been proposed to improve GRPO, and any RLVR algorithm can be applied to our proposed TFPI stage. In all our experiments, we adopt one widely used variant, DAPO (Yu et al., 2025), as our RLVR algorithm for fair comparison. Details are provided in Appendix A.1."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Thinking-Free Mode Enables More Efficient Reasoning To generate response πθ( x) for query using an SFT-distilled LRM πθ, the query is typically formatted with chat template. Template 1 illustrates the template adopted by the Qwen model family (Yang et al., 2025a). We define ThinkingFree as an operator that transforms an input query into modified query = ThinkingFree(x), in which the thinking content is explicitly omitted. Under this transformation, response generation follows πθ( x). This mechanism provides explicit control over whether reasoning content is present or absent in the generated output (see Template 2). Additional examples using other chat templates are provided in Appendix A.2. Hereinafter, we use to denote query formatted with the thinking template (e.g., Template 1), and or ThinkingFree(x) to denote the corresponding thinking-free version (e.g., Template 2). Template 1 (Thinking Mode) <im start>systemnPlease reason step by step, and put your final answer within boxed{}.<im end>n<im start >usern{question (x)} <im end>n<im start>assistantn Template 2 (Thinking-Free Mode) <im start>systemnPlease reason step by step, and put your final answer within boxed{}.<im end>n<im start >usern{question (x)} <im end>n<im start>assistantn<think>nn</think> During inference, converting into its thinking-free version can substantially reduce token consumption. To assess how this transformation affects the reasoning capability of SFT-distilled LRMs, we evaluate DeepSeek-Distilled-Qwen-1.5B (abbreviated as DS-1.5B) and Qwen3-4B on the AIME25. Detailed experimental setup is delayed to Appendix B.1. As shown in Figure 2 (Left), applying the ThinkingFree reduces the number of output tokens by more than 70% for both models. It is worth noting that Qwen3-4B is fastslow fusion model, whereas DS-1.5B is an SFT-distilled long-CoT model; nevertheless, both exhibit the same trend. 3.2 Thinking-Free Training Is Beneficial to Slow-Thinking To train an SFT-distilled LRM, the training response length should not be too short (Setlur et al., 2025), as this is detrimental to testing performance (An et al., 2025). As evidenced by the dotted line in Figure 2 (Right), training Qwen-3-4B with 4K response length using DAPO indeed leads to substantial drop in performance on AIME25. Given that using the ThinkingFree variant for inference can significantly reduce token consumption, we pose an audacious question: can we apply the ThinkingFree operation to all input queries during the rollout stage of RLVR? Moreover, will this approach be beneficial to preserving the original slow-thinking capability of the trained LLM? Surprisingly, RL trained with ThinkingFree rollout can slightly improve accuracy and reduce token consumption when evaluated in thinking mode, even with very short training context lengths. Figure 2 (Right) illustrates the training dynamics of Qwen-3-4B trained with 4K response length under the ThinkingFree transformation of queries (detailed settings are in Appendix B.2). Even with 4K output length, ThinkingFree RL increases the accuracy on AIME25 in thinking mode by approximately 2% , while reducing output tokens by around 20%. In 3 contrast, standard RLVR with 4K length reduces avg@32 by more than 40%. These results suggest that applying ThinkingFree during rollout can yield steady improvements with minimal training compute. 3.3 Thinking Free Policy Initialization Hu et al. (2025) introduce pre-pretraining stage using formal language to accelerate convergence of pretraining, while Wang et al. (2025b) propose mid-training stage between pretraining and RL-zero to facilitate RLVR. Inspired by these works, we ask: can dedicated stage for SFT-distilled LRMs improve the efficiency and effectiveness of standard RLVR scaling? ThinkingFree RL, which enhances slow-thinking within short training context windows, requires substantially less compute than standard RLVR. Initializing RLVR with ThinkingFree RL policy could therefore reduce rollout tokens while achieving stronger downstream performance. We term this step as Thinking Free Policy Initialization (TFPI), stage preceding standard RLVR for SFT-distilled LRMs that aims to lower rollout costs, raise the ceiling of reasoning ability, and accelerate convergence. Consider the RLVR objective JRLVR(θ) = xD [JRLVR(θ, x)] , where JRLVR(θ, x) denotes the per-example objective of any RLVR algorithm (e.g., GRPO in eq. (2) or DAPO in eq. (5)). For the TFPI stage, we use xD [JRLVR(θ, x)] , where = ThinkingFree(x). In the rollout stage of TFPI, modified objective: JTFPI(θ) = responses are generated conditioned on x: {yi}G i=1 πθold ( x). The importance ratios and advantages (eq. (3)) are then adapted as: ri,t(θ) = πθ(yi,t x, yi,<t) πθold (yi,t x, yi,<t) , (cid:98)Ai,t = (cid:98)Ai = r(x, yi) mean (cid:16) {r(x, yj)}G j=1 (cid:16) std {r(x, yj)}G j= (cid:17) (cid:17) , (4) where r(x, y) = r(x, y) since the ThinkingFree operator does not alter the ground-truth answer of the original problem. In our experiments, we instantiate RLVR with DAPO, i.e., JRLVR(θ) = JDAPO(θ) (see eq. (5)). The details of DAPO (Yu et al., 2025) algorithm is given in Appendix A."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings In this section, we provide brief overview of the key experimental setup, including training procedures, baselines, and evaluation details. Additional information can be found in Appendix C. Training Details. We build on the VeRL codebase (Sheng et al., 2024) with RLVR, following the DAPO recipe (Yu et al., 2025), which enables dynamic sampling and clipping higher. All methods use the same hyperparameters (batch size 256, learning rate 1106, no warm-up) and rollout settings (temperature 1, topp 1, topk 1, 8 rollouts/problem). Training is conducted on DS-1.5B, Qwen3-4B, and DS-7B using Polaris-53K (An et al., 2025). Direct RLVR uses maximum output length of 16K for DS-1.5B/DS-7B and 32K for Qwen3-4B, while TFPI adopts multi-stage training: 2K 4K 8K for DS-1.5B/DS-7B and 4K 8K 16K for Qwen3-4B. Baselines. We compare TFPI with direct RLVR training from an SFT-distilled LRM (Direct RL) under matched total training compute  (Table 1)  . To assess TFPI as pre-RLVR stage, we run TFPI + RL with similar compute and compare against Direct RL  (Table 2)  , also including competitive LRMs of the same size, such as Polaris (An et al., 2025), DeepScaleR (Luo et al., 2025b), Skywork-OR1 (He et al., 2025), and so on. For efficiency analysis, we compare with several RL-based efficient reasoning baselines  (Table 3)  under the same evaluation for fair comparison, including TLMRE (Arora & Zanette, 2025), AdaptThink (Zhang et al., 2025b), AutoThink (Tu et al., 2025), Laser (Liu et al., 2025a), L1Max (Aggarwal & Welleck, 2025), and ThinkLess (Fang et al., 2025). Evaluation Details. Our evaluation benchmarks cover: ❶ Math Reasoning: AIME24/25 and BeyondAIME (ByteDanceSeed, 2025). ❷ Multi-Task Reasoning: GPQA-Diamond (Rein et al., 2024). ❸ Code Generation: LiveCodeBench (Jain et al., 2024). ❹ Instruction Following: IFEval (Zhou et al., 2023). ❷ ❸ ❹ are out-of-domain evaluation. Following Guo et al. (2025), we generate multiple outputs (ranging from 4 to 32 depending on the size of the test set) per problem and report pass@1 accuracy. Note that ❶ is in-domain evaluation, and For IFEval, we report the strict prompt accuracy. All evaluation scripts are adapted from the DeepscaleR codebase (Luo et al., 2025b), with detailed decoding parameters provided in Appendix C.3. 4.2 TFPI Enhances the Slow-Thinking of Distilled Reasoning Models To evaluate the impact of TFPI on the slow-thinking capabilities, we present the results of TFPI versus Direct RL under the same training compute in Table 1. From the table, we have: ❶ TFPI substantially enhances the slow-thinking capabilities of distilled LRMs even when trained with small response length. For example, on DS-1.5B, TFPI Stage 1 raises the overall average accuracy from 22.0% to 26.7% (+4.7%) despite being restricted to 2K training response length. Results of DS-1.5B continue to improve 4 Table 1: Results of TFPI vs. direct RL across different benchmarks. Avg@k denotes the average accuracy (%) over random generations (i.e., pass@1). All models are evaluated in thinking mode. The total training compute for the 3 stages of TFPI equals that of direct RL for fair comparison. Darker colors in the cell background denote better results within each model group. Mathematics Multi-Task Code Instruction Overall Models Initial Model - Direct RL - TFPI stage 1 - TFPI stage 2 - TFPI stage 3 Initial Model Direct RL TFPI stage 1 TFPI stage 2 TFPI stage 3 Initial Model - Direct RL - TFPI stage 1 - TFPI stage 2 - TFPI stage 3 AIME 24 AIME 25 Beyond AIME Avg@32 Avg@ Avg@8 GPQA Avg@8 LiveCode Avg@8 IFEval Avg@4 Overall Avg. DeepSeek-Distill-Qwen-1.5B 29.6 33.9 32.1 36.8 40.1 73.6 75.7 75.2 76.0 79.9 56.3 57.9 59.4 61.8 62.0 23.0 27.1 26.9 28.4 30.8 68.3 67.0 67.8 68.2 70.6 40.0 40.4 40.4 43.9 44. 8.7 11.9 13.9 14.5 13.8 Qwen3-4B 43.4 43.6 42.4 44.7 46.7 16.3 19.2 29.3 27.8 29.6 56.8 56.3 57.9 57.8 58.5 DeepSeek-Distill-Qwen-7B 25.0 26.6 29.2 31.5 31.1 36.9 38.0 49.0 48.0 46.8 17.7 18.2 18.4 20.5 19.9 54.9 52.5 55.3 54.8 57.0 39.5 38.3 39.6 42.0 42.1 36.6 41.8 39.5 42.3 40. 64.9 66.0 66.0 64.8 70.2 55.3 56.7 56.2 57.1 60.2 22.0 25.3 26.7 28.4 29.2 60.3 60.2 60.8 61.0 63.8 42.2 43.0 45.6 47.4 47.8 through stages 1 to 3 on AIME25, accuracy increases from 23.0% (initial model) to 26.9% after Stage 1, 28.4% after Stage 2, and 30.8% after Stage 3, representing total gain of +7.8%. Similar improvements are observed for Qwen3-4B and DS-7B. These findings indicate that TFPI enables effective training under low-cost settings (short context windows), and that combining this with multi-stage RL yields substantial accuracy gains. ❷ Compared with Direct RL, TFPI delivers faster and larger accuracy improvements under the same training cost. TFPI outperforms Direct RL in nearly all configurations. For example, Qwen3-4B with TFPI attains 63.8% overall accuracy versus 60.2% for Direct RL (+3.6%), while DS-7B improves by +4.8% (47.8% vs. 43.0%). Given that the equal training compute, these results imply that TFPI achieves convergence more efficiently than conventional long-CoT RL training. ❸ Improvements of TFPI exhibit some degree of generalizability across domains, even when trained exclusively on mathematics. Although TFPI is trained solely on Polaris-53K (math-specific data), it demonstrates great out-of-domain improvements as well. For example, on DeepSeek-Distill-Qwen-1.5B, GPQA accuracy increases from 16.3% to 29.6%, LiveCodeBench from 17.7% to 19.9%, and IFEval from 36.6% to 40.8% after Stage 3. Notably, improvements on mathematical benchmarks are often consistent across successive training stages, whereas other domains sometimes exhibit fluctuations (e.g., GPQA for DS-7B and LiveCodeBench for DS-1.5B). This suggests that incorporating more diverse training data spanning multiple domains could be helpful for TFPI. 4.3 TFPI as Foundation for RLVR to Achieve Higher Performance Table 2 shows the representative results between TFPI + RL with Direct RL under the same training cost (full results in Table 7 in Appendix E). The results lead to the following observations: ❶ TFPI can raise the upper bound of RL-trained performance. RL training on TFPI-trained model can still yield notable improvements, particularly in mathematics. For example, adding an RL stage after TFPI for Qwen3-4B boosts AIME25 accuracy from 70.6% to 76.0% (+5.4%). Similarly, Beyond AIME scores increase by more than 2%. Across different model scales, applying RL after TFPI consistently achieves higher accuracies than Direct RL under the same compute budget. For instance, on Qwen3-4B, the overall accuracy rises from 62.0% (Direct RL) to 65.7% (TFPI+RL).These results suggest that incorporating TFPI as an intermediate stage between long CoT distillation and standard RLVR can be beneficial for elevating final performance. 5 Table 2: Results (%) of RL after TFPI (TFPI+RL) vs. Direct RL across different benchmarks. Avg@k denotes the average accuracy (%) over random generations (i.e., pass@1). For LRMs marked with *, results are taken from the corresponding reports (see Appendix C.4); results of 4B models are from our own runs with 48K response length. All models are evaluated in thinking mode. The total training compute for TFPI+RL is matched to that of Direct RL for fair comparison. Darker colors in the cell background denote better results. Mathematics Multi-Task Code Instruction Overall Models AIME 24 AIME 25 Beyond AIME Avg@32 Avg@32 Avg@ DeepSeek R1 Seed-1.5-Thinking Claude4 Opus Thinking Qwen3-235B-Thinking Qwen3-4B Direct RL Qwen3-4B TFPI stage 3 Qwen3-4B TFPI + RL Qwen3-4B-2507-Thinking - TFPI only 79.8 86.7 - 85.7 78.8 79.9 80.8 84.7 89.0 65.0 74.0 75.5 81. 71.5 70.6 76.0 79.2 81.2 42.4 48.0 - - 46.4 46.7 49.5 51.5 51.3 GPQA Avg@ LiveCode Avg@8 IFEval Avg@4 Overall Avg. 71.5 77.3 79.6 71.1 56.2 58.5 61.1 66.4 70. 64.3 64.9 48.9 55.7 54.3 57.0 55.7 62.4 65.5 86.1 87.4 89.7 83.4 65.1 70.2 71.3 68.0 66. 68.2 73.0 - - 62.0 63.8 65.7 68.7 70.6 Table 3: Comparison of the Thinking-Free inference mode of TFPI with efficient reasoning baselines across various reasoning tasks. Avg@k denotes the average accuracy (in %) over generations (i.e., pass@1), and Toks indicates the average output length in thousands of tokens (K). Models with * are trained from DeepScaleR-1.5B, while the remaining are from DS-1.5B. Darker colors in the cell background denote better results. Models 1) TLMRE-DS-1.5B (α = 0.1) 2) AdaptThink-1.5B (δ = 0.05) 3) AutoThink-DS-1.5B-Stage3 4) Laser-DE-L4096-1.5B 5) AutoThink-Stage3 6) L1-1.5B-Max 7) Thinkless-1.5B-RL DS-1.5B (Thinking) DS-1.5B (Thinking-Free) - TFPI stage 1 - TFPI stage 2 - TFPI stage AIME 24 AIME 25 Beyond AIME GPQA Overall avg@ Toks avg@32 Toks avg@8 Toks avg@ Toks Avg. Toks 27.6 28.1 30.3 30.3 38.9 27.2 28.4 29.6 12.4 21.9 31.5 37.5 12.9 8.0 10.2 8.3 8.7 3.2 11.3 16.7 5.7 1.6 3.4 5.3 24.8 22.6 25.2 24.9 28.9 26.3 24.1 23.0 10.9 15.3 24.2 28. 12.5 7.9 9.1 7.4 7.7 2.9 11.1 16.5 4.4 1.4 3.1 5.0 9.2 10.0 9.4 9.7 11.6 9.1 8.1 8.7 4.4 8.7 10.1 12.4 11.9 4.8 8.7 7.4 7.8 3.1 11.7 14.4 3.4 1.3 2.9 4.9 14.8 14.8 17.4 21.1 27.3 32.4 20.3 16.3 4.2 32.9 35.3 35.6 7.5 5.1 7.0 4.7 5.4 2.3 12.7 9.8 0.9 0.8 1.6 2.6 19.1 18.9 20.6 21.5 26.7 23.8 20.2 19.4 8.0 19.7 25.3 28. 11.2 6.5 8.7 6.9 7.4 2.9 11.7 14.3 3.6 1.3 2.7 4.4 ❷ TFPI + RL is an effective and efficient strategy for training high-performing LRMs. From Figure 1 (Left), the total compute cost of all three TFPI stages amounts to less than 20% of standard RL training with 32K token sequences. From Table 2, for DS-7B, TFPI+RL achieves approximately the same overall performance as Polaris-7B-Preview, despite using maximum response length of 16K, whereas Polaris-7B-Preview follows 16K 24K 32K progression during RL. Similarly, Polaris-4B-Preview employs 40K 48K 52K length schedule and consumes approximately 8K H800 GPU hours, while our TFPI+RL requires only about 1.5K H800 GPU hours (see Appendix C.1) and achieves superior performance under 48K testing length. Even without subsequent RL stage, TFPI alone allows DS-1.5B to outperform DeepScaleR, which uses maximum training length of 24K. Using only TFPI (4K8K16K), Qwen3-4B-2507 achieves 89% accuracy on AIME24. Remarkably, this 4B model outperforms Qwen3-235B-Thinking in math reasoning and code generation. These findings suggest that TFPI can serve both as strong standalone training approach and as an efficient foundation for subsequent RL, producing competitive LRMs with significantly reduced compute requirements. 4.4 TFPI Improves the Token Efficiency of Distilled Reasoning Models We compare the thinking-free inference with other RL-based efficient reasoning baselines in 1.5B size in Table 3 (see also Table 8 in Appendix E). We draw the following conclusions: ❶ Both accuracy and token efficiency steadily improve after stage 3 of TFPI. For DS-1.5B, thinking-free accuracy on AIME24 increases from 29.6% (initial model) to 37.5% (TFPI stage 3), while the average output length remains substantially shorter than that of the original thinking mode (5.3K vs. 16.7K tokens). similar trend is observed for Qwen3-4B, where accuracy improves from 26.9% to 75.1% across stages, with output lengths still far below those of the original thinking model. These results demonstrate that TFPI naturally produces more 6 Figure 3: Behaviour-Level Analysis of DS-1.5B over the TFPI Training Course. The ratio of verification steps and the average output tokens over training steps on the training set in thinking-free mode (Left) and on AIME25 in thinking mode (Right) in 3 stages of TFPI. token-efficient LRMs, offering an alternative pathway to train models that deliver both high accuracy and reduced output length. ❷ Compared with other RL-based token control methods, TFPI achieves the best performanceefficiency trade-off without specialized reward or training designs. In DS-1.5B, both TFPI stage 2 and stage 3 outperform almost all baselines in terms of overall accuracy while maintaining competitive or lower token usage. We visualize the overall accuracytoken usage trade-off in Figure 1 (Right), where TFPI consistently lies on the Pareto frontier across different stages. This observation motivates rethinking of existing token-efficient reasoning RL designs that rely heavily on specialized length reward shaping: training with TFPI offers an alternative paradigm in which strong slow-thinking LRM can be obtained, and more efficient variant can be realized simply by switching to the thinking-free mode without any additional length-control mechanisms."
        },
        {
            "title": "5 Analysis",
            "content": "5.1 Why TFPI Leads to Better Thinking-Mode Inference? In this section, we delve deeper into why TFPI, which leverages void thinking content in Template 2, can generalize to enhance reasoning in Template 1. We analyze DS-1.5B from two perspectives: ❶ Behavioural level: The learned verification behaviours after </think> during TFPI can generalize to the slow-thinking verification occurring within the <think> and </think>. The blue lines in Figure 3 show the ratio of verification steps (i.e., the number of verification steps divided by the total number of steps; see Appendix for details) for the training set (thinking-free mode) and AIME 25 dataset (thinking mode). We observe that the verification ratio exhibits similar trend: rapid drop in Stage 1, followed by steady growth in Stage 2, and sharp increase in Stage 3. Notably, the sharp decline in Stage 1 resembles an information compression process. In Stages 2 and 3, the model begins to explore more extensively (Figure 3 Right), which may explain why TFPI achieves superior performance. As verification is believed to be vital for slow-thinking reasoning (Setlur et al., 2025), the observed generalization of verification behavior suggests transfer from thinking-free training to inference in thinking mode. ❷ Parameter level: TFPI explores the parameter space more extensively at faster pace, with its parameter update directions progressively aligning with those of Direct RL. As shown in Figure 4 (Left), the PCA visualization of the initial model, TFPI-trained checkpoints, and Direct RL checkpoints exibits distinct trajectories in parameter space. The TFPI begins from the initial model (A), moves through intermediate points (B1), (B2), and (B3), and ultimately converges to region near the Direct RL final checkpoint (C). This indicates that TFPI traverses larger and more diverse region of parameter space before reaching point close to the RL-trained model. Such broad exploration may help explain why TFPI can lead to better LRMs. Furthermore, Figure 4 (Right) shows that the cosine similarity between the TFPI-trained checkpoints and the Direct RL final checkpoint steadily increases across nearly all layers throughout training. This suggests that during TFPI training, the parameter updates share similarities with those in standard long-CoT training. 5.2 About Reasoning Pattern and Rollout Speed TFPI Preserves the Reasoning Pattern of Thinking Mode. In thinking mode (Template 1), response comprises long thinking section and concise answer yans. In ThinkingFree mode (Template 2), the thinking section is omitted, and yans contains shorter reasoning path. While standard RL tends to lengthen the thinking part, TFPI increases the length of yans due to the absence of explicit thinking. As shown in Figure 5 (Left), for DS-1.5B trained with TFPI and evaluated in thinking mode, yans remains stable at 500580 tokens, whereas yans/y rises 7 Figure 4: Parameter-Level Analysis. Left: PCA projection of model parameters from DS-1.5B to final checkpoints. TFPI (blue) starts at A, passes through intermediate points (B1, B2, B3), and ultimately converges near the Direct RL final checkpoint (C). Right: Cosine similarity between parameter updates of TFPI-trained checkpoints and (C-A) across layers during training. Figure 5: Left: For TFPI with DS-1.5B on AIME25 in thinking mode, showing the average number of answer tokens (excluding the thinking part) and the ratio of answer length to total response length over training steps. Right: For long CoT RL with DS-1.5B, showing the average number of tokens during rollout on the training set over training steps, with and without the TFPI stage. as the total length decreases. This indicates that TFPI preserves the core reasoning pattern of slow-thinking rather than drifting toward an excessively extended slow-slow thinking behavior. TFPI Speeds Up Rollout for Long-CoT RL Training. Another advantage of TFPI is its ability to speed up the rollout stage in standard long-CoT RL training. As shown in Figure 5 (Right), when directly performing RL from DS-1.5B, the average output tokens during rollout on the training set start at over 9K, decrease to around 7.5K within the first 300 steps, and then fluctuate around 7.5K in the later stages. In contrast, when RL is performed after the TFPI phase, the average output tokens start at only 6K and the maximum length is below 7K tokens."
        },
        {
            "title": "6 Related Work",
            "content": "LRMs & Efficiency RLVR. RLVR has enabled the development of numerous high-performing large reasoning models (LRMs) (Yang et al., 2025a; Zeng et al., 2025a; An et al., 2025), inspiring research on model behaviors (Liu et al., 2025b; Wang et al., 2025a), novel algorithms (Liu et al., 2025b; Zheng et al., 2025), multimodal extensions (Meng et al., 2025; Xiao et al., 2025; Wang et al., 2025c), tool integration (Jin et al., 2025; Feng et al., 2025; Li et al., 2025a; Xue et al., 2025; Team et al., 2025a), and other related directions (Xu et al., 2025a; Zhang et al., 2025a; Chen et al., 2025c; Zhang et al., 2025c). RLVR can be applied directly to base LLMs (RL zero) (Zeng et al., 2025b; Guo et al., 2025) or initialized from SFT-distilled long-CoT models, the latter typically yielding stronger results (Luo et al., 2025b; An et al., 2025). major challenge for RLVR is the cost of training with long contexts, as longer outputs are often necessary for harder tasks (Shrivastava et al., 2025; Zeng et al., 2025a), consuming high computational resources. Multi-stage RLVR mitigates this by starting with shorter contexts and gradually extending them (Luo et al., 2025b; An et al., 2025; He et al., 2025), while algorithmic approaches modify GRPO to reduce length bias (Yu et al., 2025; Liu et al., 2025b; Wang et al., 2025a). Orthogonal to these strategies, we introduce TFPI as lightweight stage before RLVR, improving efficiency and strengthening the slow-thinking mode at inference with minimal training cost, thus facilitating more effective subsequent RLVR. 8 Efficient Reasoning. To address the issue of overthinking (Chen et al., 2024; Sui et al., 2025), considerable efforts have been made, including prompt-based methods (Muennighoff et al., 2025; Yang et al., 2025c; Fu et al., 2025a; Chen et al., 2025a; Fu et al., 2025b), SFT-based approaches (Kang et al., 2025; Ma et al., 2025; Munkhbat et al., 2025; Luo et al., 2025a), and RL-related designs. RL-related approaches can be further categorized into length-based reward shaping (Team et al., 2025b; Aggarwal & Welleck, 2025; Arora & Zanette, 2025; Liu et al., 2025a), integration of fast and slow thinking (Fang et al., 2025; Zhang et al., 2025b; Lou et al., 2025; Tu et al., 2025; Jiang et al., 2025; Zhang et al., 2025d), and thinking budget control (Li et al., 2025b; Hammoud et al., 2025; Wen et al., 2025). These methods primarily trade accuracy for efficiency and rely on specialized reward functions or training strategies to encourage more efficient reasoning. In contrast, our proposed TFPI naturally yields even more efficient LRMs without specialized rewards or training designs."
        },
        {
            "title": "7 Conclusion",
            "content": "After recognizing the benefits of ThinkingFree for both inference and the training of distilled reasoning models, we introduce TFPI, cost-efficient intermediate stage between long-CoT distillation and standard RL training. As strong initialization point, TFPI accelerates RL convergence, enhances attainable performance, and promotes more token-efficient reasoning without complex reward shaping or elaborate training pipelines. We further explain the factors behind TFPIs success from both the behavioral and parameter levels. Overall, TFPI provides complementary path for building token-efficient LRMs, offering an effective and efficient alternative to current RL paradigms."
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. Chenxin An, Zhihui Xie, Xiaonan Li, Lei Li, Jun Zhang, Shansan Gong, Ming Zhong, Jingjing Xu, Xipeng Qiu, Mingxuan Wang, and Lingpeng Kong. Polaris: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https://hkunlp.github.io/blog/2025/Polaris. Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. ByteDance-Seed. Beyondaime: Advancing math reasoning evaluation beyond high school olympiads. https: //huggingface.co/datasets/ByteDance-Seed/BeyondAIME, 2025. Hugging Face repository. Runjin Chen, Zhenyu Zhang, Junyuan Hong, Souvik Kundu, and Zhangyang Wang. Seal: Steerable reasoning calibration of large language models for free. arXiv preprint arXiv:2504.07986, 2025a. Tianhao Chen, Xin Xu, Zijing Liu, Pengxiang Li, Xinyuan Song, Ajay Kumar Jaiswal, Fan Zhang, Jishan Hu, Yang Wang, Hao Chen, et al. Gpas: Accelerating convergence of llm pretraining via gradient-preserving activation scaling. arXiv preprint arXiv:2506.22049, 2025b. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Zigeng Chen, Xinyin Ma, Gongfan Fang, Ruonan Yu, and Xinchao Wang. Verithinker: Learning to verify makes reasoning model efficient. arXiv preprint arXiv:2505.17941, 2025c. Gongfan Fang, Xinyin Ma, and Xinchao Wang. Thinkless: Llm learns when to think. arXiv preprint arXiv:2505.13379, 2025. Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025. Yichao Fu, Junda Chen, Yonghao Zhuang, Zheyu Fu, Ion Stoica, and Hao Zhang. Reasoning without self-doubt: More efficient chain-of-thought through certainty probing. In ICLR 2025 Workshop on Foundation Models in the Wild, 2025a. Yichao Fu, Xuewei Wang, Yuandong Tian, and Jiawei Zhao. Deep think with confidence. arXiv preprint arXiv:2508.15260, 2025b. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 9 Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie Bou-Zeid, Marzyeh Ghassemi, and Bernard Ghanem. Train long, think short: Curriculum learning for efficient reasoning. arXiv preprint arXiv:2508.08940, 2025. Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. Michael Hu, Jackson Petty, Chuan Shi, William Merrill, and Tal Linzen. Between circuits and chomsky: Pre-pretraining on formal languages imparts linguistic biases. arXiv preprint arXiv:2502.19249, 2025. Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, et al. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai. Advances in Neural Information Processing Systems, 37:1920919253, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, and Furu Wei. Think only when you need with large hybrid-reasoning models. arXiv preprint arXiv:2505.14631, 2025. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. C3ot: Generating shorter chain-of-thought without compromising effectiveness. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2431224320, 2025. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025a. Zheng Li, Qingxiu Dong, Jingyuan Ma, Di Zhang, and Zhifang Sui. Selfbudgeter: Adaptive token allocation for efficient llm reasoning. arXiv preprint arXiv:2505.11274, 2025b. Wei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, and Junxian He. Learn to reason efficiently with adaptive length-based reward shaping. arXiv preprint arXiv:2505.15612, 2025a. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025b. Chenwei Lou, Zewei Sun, Xinnian Liang, Meng Qu, Wei Shen, Wenqi Wang, Yuntao Li, Qingping Yang, and Shuangzhi Wu. Adacot: Pareto-optimal adaptive chain-of-thought triggering via reinforcement learning. arXiv preprint arXiv:2505.11896, 2025. Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025a. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025b. Notion Blog. Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Length-compressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601, 2025. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Tiancheng Han, Botian Shi, Wenhai Wang, Junjun He, et al. Mm-eureka: Exploring the frontiers of multimodal reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Meredith Ringel Morris, Jascha Sohl-Dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, and Shane Legg. Levels of agi for operationalizing progress on the path to agi. arXiv preprint arXiv:2311.02462, 2023. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. Self-training elicits concise reasoning in large language models. arXiv preprint arXiv:2502.20122, 2025. Long Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Sean Shi, Michael Choi, Anish Agrawal, Arnav Chopra, et al. Humanitys last exam. ArXiv preprint, abs/2501.14249, 2025. URL https: //arxiv.org/abs/2501.14249. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. Amrith Setlur, Matthew YR Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, and Aviral Kumar. e3: Learning to explore enables extrapolation of test-time compute for llms. arXiv preprint arXiv:2506.09026, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Vaishnavi Shrivastava, Ahmed Awadallah, Vidhisha Balachandran, Shivam Garg, Harkirat Behl, and Dimitris Papailiopoulos. Sample more to think less: Group filtered policy optimization for concise reasoning. arXiv preprint arXiv:2508.09726, 2025. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025a. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025b. Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, and Junxian He. Dart-math: Difficulty-aware rejection tuning for mathematical problem-solving. ArXiv preprint, abs/2407.13690, 2024. URL https://arxiv. org/abs/2407.13690. Songjun Tu, Jiahao Lin, Qichao Zhang, Xiangyu Tian, Linjing Li, Xiangyuan Lan, and Dongbin Zhao. Learning when to think: Shaping adaptive reasoning in r1-style models via multi-stage rl. arXiv preprint arXiv:2505.10832, 2025. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025a. Zengzhi Wang, Fan Zhou, Xuefeng Li, and Pengfei Liu. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025b. Zhenhailong Wang, Xuehang Guo, Sofia Stoica, Haiyang Xu, Hongru Wang, Hyeonjeong Ha, Xiusi Chen, Yangyi Chen, Ming Yan, Fei Huang, et al. Perception-aware policy optimization for multimodal reasoning. arXiv preprint arXiv:2507.06448, 2025c. 11 Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/ paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html. Hao Wen, Xinrui Wu, Yi Sun, Feifei Zhang, Liye Chen, Jie Wang, Yunxin Liu, Ya-Qin Zhang, and Yuanchun Li. Budgetthinker: Empowering budget-aware llm reasoning with control tokens. arXiv preprint arXiv:2508.17196, 2025. Tong Xiao, Xin Xu, Zhenya Huang, Hongyu Gao, Quan Liu, Qi Liu, and Enhong Chen. Advancing multimodal reasoning capabilities of multimodal large language models via visual perception reward. arXiv preprint arXiv:2506.07218, 2025. Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can Yang, and Yang Wang. Can llms solve longer math word problems better? ArXiv preprint, abs/2405.14804, 2024. URL https://arxiv.org/abs/2405.14804. Xin Xu, Tianhao Chen, Fan Zhang, Wanlong Liu, Pengxiang Li, Ajay Kumar Jaiswal, Yuchen Yan, Jishan Hu, Yang Wang, Hao Chen, et al. Double-checker: Enhancing reasoning of slow-thinking llms via self-critical fine-tuning. arXiv preprint arXiv:2506.21285, 2025a. Xin Xu, Qiyun Xu, Tong Xiao, Tianhao Chen, Yuchen Yan, Jiaxin Zhang, Shizhe Diao, Can Yang, and Yang Wang. Ugphysics: comprehensive benchmark for undergraduate physics reasoning with large language models. arXiv preprint arXiv:2502.00334, 2025b. Xin Xu, Jiaxin Zhang, Tianhao Chen, Zitong Chao, Jishan Hu, and Can Yang. Ugmathbench: diverse and dynamic benchmark for undergraduate-level mathematical reasoning with large language models. arXiv preprint arXiv:2501.13766, 2025c. Zhenghai Xue, Longtao Zheng, Qian Liu, Yingru Li, Zejun Ma, and Bo An. Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. https://simpletir.notion.site/report, 2025. Notion Blog. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Chenxu Yang, Qingyi Si, Mz Dai, Dingyu Yao, Mingyu Zheng, Minghui Chen, Zheng Lin, and Weiping Wang. Test-time prompt intervention. arXiv preprint arXiv:2508.02511, 2025b. Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Qiaowei Li, Zheng Lin, Li Cao, and Weiping Wang. Dynamic early exit in reasoning models. arXiv preprint arXiv:2504.15895, 2025c. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. ArXiv preprint, abs/2309.12284, 2023. URL https://arxiv.org/abs/2309.12284. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Aohan Zeng, Xin Lv, Qinkai Zheng, Zhenyu Hou, Bin Chen, Chengxing Xie, Cunxiang Wang, Da Yin, Hao Zeng, Jiajie Zhang, et al. Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471, 2025a. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025b. Fuxiang Zhang, Jiacheng Xu, Chaojie Wang, Ce Cui, Yang Liu, and Bo An. Incentivizing llms to self-verify their answers. arXiv preprint arXiv:2506.01369, 2025a. 12 Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, and Juanzi Li. Adaptthink: Reasoning models can learn when to think. arXiv preprint arXiv:2505.13417, 2025b. Xiaoying Zhang, Hao Sun, Yipeng Zhang, Kaituo Feng, Chaochao Lu, Chao Yang, and Helen Meng. Critique-grpo: Advancing llm reasoning with natural language and numerical feedback. arXiv preprint arXiv:2506.03106, 2025c. Xiaoyun Zhang, Jingqing Ruan, Xing Ma, Yawen Zhu, Haodong Zhao, Hao Li, Jiansong Chen, Ke Zeng, and Xunliang Cai. When to continue thinking: Adaptive thinking mode switching for efficient reasoning. arXiv preprint arXiv:2505.15400, 2025d. Chujie Zheng, Shixuan Liu, Mingze Li, Xiong-Hui Chen, Bowen Yu, Chang Gao, Kai Dang, Yuqiong Liu, Rui Men, An Yang, et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. doi: 10.48550/arXiv.2311.07911. URL https://arxiv.org/abs/2311.07911."
        },
        {
            "title": "A Background and Preliminary",
            "content": "A.1 RLVR Algorithms Numerous variants have been proposed to improve GRPO. For example, DAPO (Yu et al., 2025) introduces token-level normalization and dynamic sampling; Dr GRPO (Liu et al., 2025b) removes length bias to prevent incorrect responses from growing longer over time; and Wang et al. (2025a) train selectively on forking tokens (see also Section 6). Our TFPI is orthogonal to these RLVR algorithms. That is to say, any RLVR algorithm can be applied to our proposed TFPI stage. To mitigate the effect of RLVR algorithms, we employ DAPO as our RLVR algorithm in all experiments for fair comparison. Dynamic sAmpling Policy Optimization (DAPO) Building on GRPO, DAPO (Yu et al., 2025) introduces clip-higher mechanism, incorporates dynamic sampling and applies token-level policy gradient loss. The objective is given by JDAPO(θ) = xD [JDAPO(θ, x)] , where: JDAPO(θ, x) = (cid:34) 1 i=1 yi i=1 yi t=1 (cid:16) min ri,t(θ) (cid:98)Ai,t, clip(cid:0)ri,t(θ), 1 εlow, 1 + εhigh (cid:1) (cid:98)Ai,t (cid:35) (cid:17) , s.t. 0 < {yi is equivalent(yi, x)} < G, (5) where {yi}G ri,t(θ), (cid:98)Ai,t is computed as in eq. (3). i=1 πθold (x) and is the number of generated responses to each query (i.e., the group size) and A.2 ThinkingFree Operation In Section 3.1, we have shown chat templates for both the original query and its thinking-free version = ThinkingFree(x) for Qwen models. Here, we showcase one additional example under the DeepSeek (Guo et al., 2025) template as below. Template 3 (Thinking Mode (DeepSeek)) Please reason step by step, and put your final answer within boxed{}.<User>{question (x)}<Assistant >n Template 4 (Thinking-Free Mode (DeepSeek)) Please reason step by step, and put your final answer within boxed{}.<User>{question (x)}<Assistant>n<think>nn</think>"
        },
        {
            "title": "B Meta Experiments",
            "content": "B.1 Token Consumption of ThinkingFree We provide the detailed experimental setup for the meta-experiment in Section 3.1 in this appendix. For DS-1.5B, we use the decoding parameters suggested by Guo et al. (2025) in thinking mode: temperature = 0.6, top-p = 0.95, and top-k = 1. For Qwen3-4B, we adopt the recommended settings from Yang et al. (2025a) in thinking mode: temperature = 0.6, top-p = 0.95, and top-k = 20. For ThinkingFree, we set temperature = 0.7, top-p = 0.8, and use both top-k = 1 and top-k = 20. The maximum output length is fixed at 32K tokens for both modes. For evaluation, we sample 32 generations per query on AIME 2025 and report the average number of output tokens. The parameters are given in Table 4 and the results are shown in Figure 2 (Left). Table 4: Decoding Parameters of Meta-Experiment in Section 3.1 DS-1.5B Qwen3-4B Thinking Mode = 0.6, top-p = 0.95, and top-k = 1 = 0.6, top-p = 0.95, and top-k = Thinking-Free Mode = 0.7, top-p = 0.8, and top-k = 20 = 0.7, top-p = 0.8, and top-k = 20 B.2 Detailed Setup of ThinkingFree Training We provide the detailed experimental setup for the meta-experiment discussed in Section 3.2 in this appendix. For training, we use the DAPO (Yu et al., 2025) algorithm, with configurations identical to those in Appendix C.1, except that the maximum output length is set to 4K. For evaluation, we set the maximum output length to 48K and perform testing in thinking mode as described in Template 1. All other evaluation parameters follow Appendix C.3. Note that the initial avg@32 value in Figure 2 (Right) is higher than the value reported in (Yang et al., 2025a) (68.2 vs. 65.6), because we adopt the RoPE scaling method described in Polaris (An et al., 2025). 14 Table 5: Training Steps and Time of Main Experiments. kh denotes one thousand H20 Hours. DS-1.5B Qwen3-4B DS-7B DS-1.5B Qwen3-4B DS-7B TFPI Stage 1 TFPI Stage 2 TFPI Stage 3 TFPI Total Direct RLVR TFPI: RLVR TFPI + RLVR Total Direct RLVR Total 2K, 1K steps 4K, 440 steps 8K, 440 steps - 16K, 456 steps 16K, 472 steps - 16K, 896 steps 4K, 100 steps 8K, 56 steps 16K, 64 steps - 32K, 20 steps 32K, 192 steps - 32K, 216 steps 2K, 1K steps 4K, 232 steps 8K, 144 steps - 16K, 280 steps 536 steps - 16K, 820 steps 0.84 kh 0.62 kh 1.21 kh 2.66 kh 2.67 kh 2.49 kh 5.16 kh 5.17 kh 0.35 kh 0.52 kh 0.91 kh 1.79 kh 1.9 kh 13.5 kh 15.4 kh 15.7 kh 1.73 kh 0.82 kh 0.94 kh 3.50 kh 3.52 kh 6.55 kh 10 kh 10.1 kh"
        },
        {
            "title": "C Experimental Details",
            "content": "In this appendix, we provide the details of our main experiments in Section 4. C.1 Training Details We build on the VeRL codebase (Sheng et al., 2024), with the RLVR loss following the DAPO recipe (Yu et al., 2025). Specifically, the RLVR loss is defined in eq. (5). For our TFPI, it becomes = ThinkingFree(x). (cid:2)JDAPO(θ, x)(cid:3) , (6) xD For fair comparison, we use identical hyperparameters across methods. For clip-higher, we set εlow = 0.2 and εhigh = 0.28. Training is performed with batch size and mini-batch size of 256, learning rate of 106, and no warm-up scheduling. Both KL divergence loss and entropy loss are excluded. For rollout, we use temperature = 1, topp = 1, and topk = 1. We generate 8 rollouts per problem. Experiments are conducted on DS-1.5B, Qwen3-4B, and DS-7B, with Polaris-53K (An et al., 2025) as the training dataset. In principle, we could apply dataset filtering at each training stage to accelerate training An et al. (2025). However, for fairness, we deliberately use the full training set for all experiments. For Direct RLVR, the maximum output length is set to 16K for DS-1.5B and DS-7B, and 32K for Qwen3-4B. For TFPI, we adopt multi-stage training strategy (An et al., 2025; Luo et al., 2025b): DS-1.5B and DS-7B: 2K 4K 8K. Qwen3-4B: 4K 8K 16K. Our experiments are conducted with 32 H20 GPUs. summary of the number of training steps and training time are provided in Table 5. C.2 Baselines To evaluate the efficacy of TFPI, we compare TFPI with direct RLVR training from an SFT-distilled LRM (Direct RL for short) under the same total training compute, i.e., the combined compute of the three TFPI stages equals that of direct RLVR  (Table 1)  . To examine the effect of inserting TFPI stage before RLVR, we apply TFPI to an SFT-distilled model (TFPI + RL), continue with standard RLVR, and compare the results with Direct RL using approximately the same training compute  (Table 2)  . We also include several high-performing LRMs of the same model size from previous works for reference, including Polaris (An et al., 2025), DeepScaleR (Luo et al., 2025b), Skywork-OR1 (He et al., 2025), and AReal-RL. For both Table 1 and Table 2, all models are evaluated in thinking mode (see also Appendix C.3). To assess the impact of TFPI on reasoning efficiency, we compare our TFPI-trained model with various RL-based efficient reasoning baselines, including TLMRE (Arora & Zanette, 2025), AdaptThink (Zhang et al., 2025b), AutoThink (Tu et al., 2025), Laser (Liu et al., 2025a), L1Max (Aggarwal & Welleck, 2025), and ThinkLess (Fang et al., 2025)  (Table 3)  . The training and testing settings of these baselines, as reported in their original papers, are summarized in Table 6. For fair comparison, we standardize the testing parameters to topp = 0.95, topk = 1, and = 0.6 with 32K maximum length, following the recommendations of DeepSeek-R1 (Guo et al., 2025). C.3 Evaluation Details To comprehensively evaluate model capabilities, we employ diverse set of benchmarks covering mathematical reasoning, multi-task reasoning, code generation, and instruction-following: 15 Table 6: Training and evaluation details of efficient reasoning baselines from original papers. We unify the evaluation setting for fair comparison in Table 3. Details are provided in Appendix C.2 and C.3. Model Training Data Training Length Test Length Evaluation Details TLMRE-DS-1.5B (α = 0.1) AdaptThink-1.5B (δ = 0.05) AutoThink-DS-1.5B-Stage3 Laser-DE-L4096-1.5B 3.2K examples from NuminaMath DeepScaleR-40K DeepScaleR-40K DeepScaleR-40K 16K 8K 16K 24K DeepSeek-Distill-Qwen-1.5B AutoThink-Stage3 L1-1.5B-Max Thinkless-1.5B-RL DeepscaleR-1.5B-Preview DeepScaleR-40K DeepScaleR-40K DeepScaleR-40K 8K 16K 24K 4K 24K 32K 16K 32K 8K 10 generations for AIME24 = 0.6, 16 generations for AIME24 = 0.6, 16 generations 16 generations for AIME24 = 0.6, 16 generations 1. Mathematical reasoning: We evaluate on AIME24, AIME25, and BeyondAIME (ByteDance-Seed, 2025). For AIME24 and AIME25, we report pass@1 accuracy using 32 samples per problem (avg@32); for BeyondAIME, we report avg@8. 2. Multi-task reasoning: We evaluate on GPQA-Diamond (Rein et al., 2024) and report pass@1 with 8 samples per problem. 3. Code generation: We assess coding ability on LiveCodeBench (Jain et al., 2024) (2024-082025-01 subset, aligned with DeepSeek-R1 (Guo et al., 2025)), reporting pass@1 with 8 samples per problem. 4. Instruction-following: We evaluate on IFEval (Zhou et al., 2023) and report pass@1 of the strict prompt accuracy with 4 samples per problem. All evaluation codes are adapted from the DeepscaleR (Luo et al., 2025b) codebase, where vLLM (Kwon et al., 2023) is leveraged to accelerate inference. For IFEval, we use the same codes provided by the official paper (Zhou et al., 2023). We provide our decoding parameters as follows: Table 1: We set the temperature to 0.6 and topp = 0.95. For LRMs trained from DS-1.5B and DS-7B, we use topk = 1 with maximum sequence length of 32K tokens. For LRMs trained from Qwen3-4B, we use topk = 20 with maximum sequence length of 48K tokens, applying RoPE scaling as proposed by An et al. (2025). Table 2: We set the temperature to 0.6 and topp = 0.95. For LRMs initialized from DS-1.5B and DS-7B, we use topk = 1 with maximum sequence length of 32K tokens. For LRMs initialized from Qwen3-4B, we use topk = 20 with maximum sequence length of 48K tokens, again applying RoPE scaling as proposed by An et al. (2025). For models marked with *, we report the results from their original publications (see Appendix C.4). Table 3: For efficient reasoning baselines listed in Table 6 and the thinking mode of initial models, we set topp = 0.95, topk = 1, and = 0.6, with maximum sequence length of 32K tokens (48K for Qwen3-4B). For the thinking-free mode of initial models and our TFPI, we set topp = 0.8, topk = 20, and = 0.7 with maximum sequence length of 32K tokens, following Yang et al. (2025a). C.4 Source of Some Results in Table 2 Results for LRMs marked with * are taken directly from the Seed-1.5-Thinking report (Seed et al., 2025) and the corresponding Hugging Face page of Qwen3-2507. Note that the LiveCodeBench test set subsets of these results, and the metric of IFEval may differ from those in our experiments; their results are included for reference only."
        },
        {
            "title": "D Analysis Details",
            "content": "As verification is an important indicator of slow-thinking capabilities (Setlur et al., 2025), we conduct experiments in Section 5.1 to examine how verification can generalize to slow-thinking, even when trained with TFPI (thinking-free mode). Following Yang et al. (2025b), for the experiments in Figure 3, we segmented the reasoning trajectories using nn as delimiters and classified each step according to whether it contained verification-related phrases such as wait, let me verify, let me check, checking, verifying, or double-check."
        },
        {
            "title": "E More Results",
            "content": "Due to page limit, we present only representative results in Sections 4.3 and 4.4, with the complete results provided in Tables 7 and 8, respectively. 16 Table 7: Results (%) of RL after TFPI (TFPI+RL) vs. Direct RL across different benchmarks. Avg@k denotes the average accuracy (%) over random generations (i.e., pass@1). For LRMs marked with *, results are taken from the corresponding reports (see Appendix C.4); all other results are from our own runs. All models are evaluated in thinking mode. The total training compute for TFPI+RL is matched to that of Direct RL for fair comparison. Mathematics Multi-Task Code Instruction Overall Models DeepSeek R1 Seed-1.5-Thinking Claude4 Opus Thinking Qwen3-235B-Thinking Qwen3-8B DS-R1-0528-Qwen3-8B DeepScaleR-1.5B DS-1.5B Direct RL DS-1.5B TFPI stage 3 DS-1.5B TFPI + RL Polaris-4B-Preview Qwen3-4B Direct RL Qwen3-4B TFPI stage 3 Qwen3-4B TFPI + RL Qwen3-4B-2507-Thinking - TFPI only AReal-boba-RL-7B Skywork-OR1-7B-Preview Polaris-7B-Preview DS-7B Direct RL DS-7B TFPI stage 3 DS-7B TFPI + RL AIME 24 AIME 25 Beyond AIME Avg@32 Avg@32 Avg@8 GPQA Avg@8 LiveCode Avg@8 IFEval Avg@ Overall Avg. 79.8 86.7 - 85.7 75.3 82.8 37.8 37.2 40.1 42.3 73.2 78.8 79.9 80.8 84.7 89.0 61.5 61.2 70.6 62.3 62.0 65.3 65.0 74.0 75.5 81.5 67.0 74. 31.6 28.5 30.8 32.9 70.7 71.5 70.6 76.0 79.2 81.2 46.1 46.4 48.8 47.9 44.6 47.1 42.4 48.0 - - 43.2 50.6 1.5B Size 13.1 12.9 13.8 15. 4B Size 39.9 46.4 46.7 49.5 51.5 52.5 7B Size 30.7 31.2 37.0 30.1 31.1 33.2 71.5 77.3 79.6 71.1 61.7 61.4 19.1 24.6 29.6 27. 54.9 56.2 58.5 61.1 66.4 70.1 35.4 35.2 34.1 36.9 46.8 45.9 64.3 64.9 48.9 55.7 51.7 59.0 21.9 19.5 19.9 20.9 39.7 54.3 57.0 55.7 62.4 65.5 34.2 43.3 43.9 42.8 42.1 43. 86.1 87.4 89.7 83.4 66.0 73.9 40.5 39.5 40.8 41.0 63.9 65.1 70.2 71.3 68.0 66.7 57.5 55.4 55.6 57.1 60.2 57.3 68.2 73.0 - - 61.7 67.0 27.3 27.0 29.2 30. 57.0 62.0 63.8 65.7 68.7 70.8 44.2 45.5 48.3 46.2 47.8 48.7 Table 8: Comparison of the Thinking-Free inference mode of TFPI with efficient reasoning baselines across various reasoning tasks. Avg@k denotes the average accuracy (in %) over generations (i.e., pass@1), and Toks indicates the average output length in thousands of tokens (K). Models AIME 24 AIME Beyond AIME GPQA Overall avg@32 Toks avg@ Toks avg@8 Toks avg@8 Toks Avg. Toks TLMRE-DS-1.5B (α = 0.1) AdaptThink-1.5B (δ = 0.05) AutoThink-DS-1.5B-Stage3 Laser-DE-L4096-1.5B Initial Model (Thinking) Initial Model (Thinking-Free) - TFPI stage 1 - TFPI stage 2 - TFPI stage 3 AutoThink-Stage3 L1-1.5B-Max Thinkless-1.5B-RL Initial Model (Thinking) Initial Model (Thinking-Free) - TFPI stage 1 - TFPI stage 2 - TFPI stage 3 Qwen3-4B-Instruct-2507 27.6 28.1 30.3 30.3 29.6 12.4 21.9 31.5 37.5 38.9 27.2 28. 73.6 26.9 43.9 63.7 75.1 60.4 DeepSeek-Distill-Qwen-1.5B 12.9 8.0 10.2 8.3 16.7 5.7 1.6 3.4 5.3 24.8 22.6 25.2 24.9 23.0 10.9 15.3 24.2 28.4 12.5 7.9 9.1 7.4 16.5 4.4 1.4 3.1 5.0 9.2 10.0 9.4 9.7 8.7 4.4 8.7 10.1 12. DeepscaleR-1.5B-Preview 8.7 3.2 11.3 18.0 7.2 3.7 7.2 10.5 7.9 11.6 9.1 8.1 43.4 11.0 23.2 33.0 42.1 33.0 28.9 26.3 24. 7.7 2.9 11.1 Qwen3-4B 68.3 20.2 31.1 52.8 66.9 46.1 22.3 5.5 3.4 8.0 12.7 7.4 17 11.9 4.8 8.7 7.4 14.4 3.4 1.3 2.9 4. 7.8 3.1 11.7 23.5 4.2 3.4 7.0 12.7 7.3 14.8 14.8 17.4 21.1 16.3 4.2 32.9 35.3 35.6 27.3 32.4 20.3 56.8 45.0 47.8 50.9 55.9 64.4 7.5 5.1 7.0 4.7 9.8 0.9 0.8 1.6 2. 5.4 2.3 12.7 10.7 2.3 1.5 2.3 3.7 4.8 19.1 18.9 20.6 21.5 19.4 8.0 19.7 25.3 28.5 26.7 23.8 20.2 60.5 25.8 36.5 50.1 60.0 51.0 11.2 6.5 8.7 6.9 14.3 3.6 1.3 2.7 4. 7.4 2.9 11.7 18.6 4.8 3.0 6.1 9.9 6."
        }
    ],
    "affiliations": [
        "LLM Department, Tencent",
        "The Hong Kong University of Science and Technology",
        "The University of Hong Kong"
    ]
}