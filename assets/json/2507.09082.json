{
    "paper_title": "Taming generative video models for zero-shot optical flow extraction",
    "authors": [
        "Seungwoo Kim",
        "Khai Loong Aw",
        "Klemen Kotar",
        "Cristobal Eyzaguirre",
        "Wanhee Lee",
        "Yunong Liu",
        "Jared Watrous",
        "Stefan Stojanov",
        "Juan Carlos Niebles",
        "Jiajun Wu",
        "Daniel L. K. Yamins"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Extracting optical flow from videos remains a core computer vision problem. Motivated by the success of large general-purpose models, we ask whether frozen self-supervised video models trained only for future frame prediction can be prompted, without fine-tuning, to output flow. Prior work reading out depth or illumination from video generators required fine-tuning, which is impractical for flow where labels are scarce and synthetic datasets suffer from a sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting a small tracer perturbation into a next-frame predictor and tracking its propagation, we extend this idea to generative video models. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recent Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: a novel test-time procedure that injects a localized perturbation into the first frame, rolls out the model one step, and computes the Kullback-Leibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid Kubric (4.7% relative improvement). Our results indicate that counterfactual prompting of controllable generative video models is a scalable and effective alternative to supervised or photometric-loss approaches for high-quality flow."
        },
        {
            "title": "Start",
            "content": "Taming generative video models for zero-shot optical flow extraction Seungwoo Kim* Khai Loong Aw* Klemen Kotar* Cristobal Eyzaguirre Wanhee Lee Yunong Liu Juan Carlos Niebles Jiajun Wu Daniel L.K. Yamins Jared Watrous Stefan Stojanov 5 2 0 2 1 1 ] . [ 1 2 8 0 9 0 . 7 0 5 2 : r a"
        },
        {
            "title": "Stanford University",
            "content": "Figure 1: We introduce zero-shot test-time inference procedure called KL-tracing, which extracts robust optical flow and point tracking from generative world model on challenging in-the-wild videos. In every column, the green line links the query location in the first frame (top) to the position predicted by our method in the second frame (bottom). All clips are real-world internet videos and contain phenomena that classical, appearance-based optical flow methods find challenging: (A) Newtons cradle, where both frames have four balls in the middle, but the balls are different; the example involves physical reasoning. (B) Globe has challenging in-place object rotation and the query point is in the textureless ocean. (C) Dog weaving through occluding poles with large, rapid motion, including depth changes and motion blur. (D) Soccer tackle with fast, diagonal motion with motion blur and partial occlusion. (E) Windmill rotation where the repetitive blades and uniform sky make local matching challenging. These examples highlight the benefits of leveraging powerful world model to extract optical flow for challenging real-world scene dynamics."
        },
        {
            "title": "Abstract",
            "content": "Extracting optical flow from videos remains core computer vision problem. Motivated by the recent success of large general-purpose models, we ask whether frozen self-supervised video models trained only to predict future frames can be prompted, without fine-tuning, to output flow. Prior attempts to read out depth or illumination from video generators required fine-tuning; that strategy is ill-suited for flow, where labeled data is scarce and synthetic datasets suffer from sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting small tracer perturbation into next-frame predictor and tracking its propagation, we extend this idea to generative video models for zero-shot flow extraction. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; *Equal contribution 1 and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recently introduced Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: novel test-time inference procedure that injects localized perturbation into the first frame, rolls out the model one step, and computes the KullbackLeibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method outperforms state-of-the-art models on real-world TAP-Vid DAVIS dataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid Kubric (4.7% relative improvement), despite being trained on real-world videos. Our results indicate that counterfactual prompting of controllable generative video models is scalable and effective alternative to supervised or photometric-loss approaches for high-quality optical flow."
        },
        {
            "title": "Introduction",
            "content": "Extracting motion information (optical flow) from videos is fundamental yet open challenge in computer vision with many applications. Due to the intractable cost of obtaining ground-truth labels from real-world videos, most supervised baselines [37, 39] are trained on synthetic datasets, e.g., FlyingChairs [12], FlyingThings [27] and Sintel [6]. While invaluable for progress, these datasets cover narrow slice of motion statisticspredominantly rigid objects, limited lighting variation, and short temporal horizons. As result, they under-represent the long-tail of real-world phenomena such as non-rigid deformation, atmospheric effects, rapid camera shake and textureless regions. Self-supervised models [21, 36] that can be trained on real videos attempt to bridge this gap, but they often rely on task-specific heuristics, such as photometric consistency or smoothness which fail under complex lighting, occlusion, or long-range motion. Consequently, both supervised and self-supervised optical flow baselines struggle to generalize to challenging in-the-wild videos. Inspired by successes across vision and language where large general-purpose models outperform smaller task-specific ones, we explore large-scale video models as possible solution. Trained on massive repositories of real-world data, modern video models already demonstrate strong scene understanding [5, 29, 1, 26], suggesting an implicit grasp of optical flow [3, 38]. However, prior work extracting visual intermediates such as depth and illumination from these models still required supervised fine-tuning [33, 11]. Extending that recipe to optical flow would again depend on synthetic labels, facing the same sim-to-real domain gap. This motivates our search for zero-shot procedure that can extract accurate optical flow from off-the-shelf video models without any additional training. In fact, such procedure has been proposed in the Counterfactual World Model (CWM) framework [3]. Zero-shot optical flow is obtained by adding small tracer perturbation to the source frame and tracking how pretrained next-frame predictor propagates the tracer to the target frame (Section 3). In practice, however, because deterministic video models like CWM can only predict single future state, it encourages predictions that average over future possibilities, yielding perceptually blurry frames that wash out the injected tracer, leading to less precise motion estimates (Section 4.1). To overcome this, we implement the perturb-and-track method with generative video models, which generate crisp predictions as they sample from distribution instead of regressing to mean. This is insufficient, however, as each state-of-the-art generative video model faces its own flow extraction challenges. Stable Video Diffusion (SVD) [5] produces photorealistic frames, but its conditioning is weak and non-localized: generation is guided by single global latent inverted from the target frame, so pixel-level edits introduce noisy differences, corrupting the extracted flow (Section 4.2). The recent Cosmos model [29] can condition generation on ground-truth patches from the target frame, but its autoregressive rollout is strictly in raster order, so these provided patches must all lie in the raster top-left region of the image, far less informative than the same number of patches distributed across the image, thus failing to accurately reconstruct the target frame (Section 4.3). Through systematic analysis of state-of-the-art video models, we find three key properties for accurate zero-shot optical flow extraction: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) local tokenization, which encodes each spatio-temporal patch independently; and (3) random access decoding that can condition on any subset of future pixels. These are present in the recent Local Random Access Sequence (LRAS) architecture [24], whose patch-level conditioning 1Project website at: https://neuroailab.github.io/projects/kl_tracing/ 2 and random-order decoding provide much more fine-grained controllability than previous generative video models (Section 4.4). In addition, we propose new method, KL-tracing, that further improves upon the probe-and-track method by using the autoregressive nature of LRAS which exposes the probability distributions for predictions. KL-tracing computes the perturbation difference in the prediction logit space, thereby bypassing noisy RGB differences resulting from sampling randomness. We achieve state-of-the-art results on the challenging real-world TAP-Vid DAVIS dataset and the synthetic TAP-Vid Kubric dataset relative to supervised and unsupervised flow baselines (Section 6). Overall, we show how to tame generative modelsunruly, hard to control, and normally only steered with previous frames or coarse text promptsinto tool that obeys fine-grained patch conditioning and KL-tracing for extraction of precise optical flow. Our contributions are threefold: (1) We provide the first systematic study of optical flow extraction from large generative video models, highlighting failure modes of both deterministic regressors and underconstrained generative models, and discover key model properties for flow extraction; (2) we introduce KL-tracing, simple yet effective test-time inference flow procedure for tracing perturbations through the predicted distributions of future states; and (3) we tame LRAS with KL-tracing to obtain both quantitative and qualitative results of accurate flow traces on both real-world and synthetic benchmarks, as well as in-the-wild videos."
        },
        {
            "title": "2 Related Work",
            "content": "Optical Flow refers to estimating the per-pixel 2D motion between pair of video frames. Most models [37, 39] are supervised on synthetic video datasets [12, 27, 6], posing sim-to-real gap, i.e., predominantly rigid objects, limited lighting variation, and short temporal horizons, failing to capture the long tail of real-world phenomena such as non-rigid deformation, atmospheric effects, fast camera shakes, and fine-scale textureless regions. Alternatively, self-supervised models [22, 25, 36] train on real-world videos, but often rely on task-specific architectures and heuristics such as photometric consistency and smoothness loss [22, 25, 36]. Regardless, these optical flow methods are designed for short frame gaps and relatively non-complex scenes with little motion dynamics. Point Tracking follows set of points across longer time horizons. Most methods are supervised [9, 17, 23], often on synthetic datasets, facing the sim-to-real gap. Alternatively, self-supervised methods rely on task-specific heuristics such as smoothness losses and cycle consistency [20, 4, 34], i.e., tracking point forward in time, then backward, should return to its original position. In contrast, we use zero-shot method to extract flow from general-purpose self-supervised video models trained on diverse, large-scale datasets, as they better learn challenging real-world dynamics (Section 6). Deterministic video models predict single future frame/latent [15, 14, 16, 3, 2]. Trained with either latent or pixel ℓ1/ℓ2 reconstruction losses, they are implicitly optimised to output the expectation over plausible futures. Whenever the conditioning signala single frame F1, multiple frames (FN . . .F1), or partially masked target masked 2 can lead to multiple future possibilities, the model averages them, producing spatially blurred predictions which hurt flow estimation. Generative Video Models. Generative models can sample different image or video generations from probabilistic distribution. Diffusion models iteratively denoise single noise sample into generation [18, 35, 32, 19, 5]. Autoregressive models [30, 7, 31, 8] generate outputs sequentially, modeling the joint distribution over pixels or tokens, each element conditioned on previously generated elements. However, many of these models, e.g., Cosmos [29], predict tokens in raster order and use global tokenizer where each image patch is encoded with information from other patches. Hence, they lack fine-grained controllability which makes them challenging to use for flow extraction (Section 4.3) In contrast, recent class of models, Local Random Access Sequence (LRAS), has several key properties which make them suitable for optical flow extraction (Section 4.4). Applying our novel test-time inference procedure to LRAS achieves state-of-the-art optical flow (Section 6)."
        },
        {
            "title": "3 Test-time inference procedure and evaluation setup for optical flow",
            "content": "We use zero-shot procedure to extract optical flow from various pre-trained video models. For all models, we utilize the generic method template illustrated in Figure 2, based on the Counterfactual World Model (CWM) paper [3]. Step 1. Inject small perturbation. We duplicate the initial frame F1 and perturb it with small white bump\" to form F1, i.e., Gaussian centered at the query location xq with amplitudes 255 on each RGB channel and standard deviation σ equal 2.0. Step 2. Run model twice. Both clean and perturbed initial frames are separately forwarded through the frozen model, each time with sparse mask of the second frame (F masked ). Hence, two forward 2 3 Figure 2: Test-time inference procedure for extracting flow from pre-trained, frozen, generative video model, based on the Counterfactual World Model (CWM) paper [3]. This involves three steps: (1) Perturbation: add small, white-colored 2D Gaussian dot perturbation to frame 1 at the location of the point we wish to track. (2) Generate model predictions conditioned on the two frames. For CWM, Cosmos, and LRAS, we provide frame 1 and masked patches of frame 2 (Sections 4.1, 4.3, 4.4). For Stable Video Diffusion, we provide the noised latents of both frames (Section 4.2). (3) Estimate optical flow by computing the RGB difference between the clean and perturbed predictions. pred pred passes: (F1, masked 2 2 the RGB difference between the two predictions, pred where the perturbation was carried to. ) and ( F1, masked 2 2 2 ). Step 3. Estimate optical flow. Compute and pred . Then, take the arg-max to identify 2 We find that this procedure can be extended to any video model that exposes this interface of nextframe prediction. For example, this works with models that allow providing subset or masking of patches (Sections 4.1, 4.3, 4.4), and also video diffusion models, by converting frames 1 and 2 into noised latents and using them as conditioning (Section 4.2). Evaluation setup. We use TAP-Vid DAVIS [9] and Kubric [13] for evaluation. TAP-Vid DAVIS contains real-world videos with human-annotated flow and occlusion labels, while Kubric is synthetic dataset with ground-truth labels. Metrics. We use the following metrics. (1) Average Distance (AD) (or endpoint error (EPE)), the Euclidean distance between the predicted and groundtruth flow, as well as metrics from TAP-Vid [9]: (2) Average Jaccard (AJ), the true positives\" divided by true positives\" plus false positives\" plus false negatives\", averaged over various thresholds; (3) < δx avg, the fraction of points that are within threshold of the ground truth location, averaged over various thresholds, and (4) Occlusion Accuracy (OA) for predicting occluded points."
        },
        {
            "title": "4 Model evaluations",
            "content": "We conduct several studies extracting optical flow from various video model classes, highlighting issues we found with each. We investigate deterministic models (Section 4.1), diffusion models (Section 4.2), and autoregressive models (Section 4.3). Finally, we identify the key properties of generative models that enable precise optical flow, and model class that satisfies them (Section 4.4). 4.1 Study 1: Deterministic models produce blurry-averaged predictions. Method: Deterministic video models such as the Counterfactual World Model (CWM) [3, 38] predict single future state. CWM is visual foundation model that can be zero-shot prompted to perform many tasks, such as predicting flow, keypoints, object segments, counterfactuals, and depth. CWM was trained with an ℓ2 loss to perform masked next-frame prediction: (F1, masked ). It deterministically predicts single future state pred , as it lacks mechanism for sampling. To extract 2 flow from CWM, we use the procedure in Section 3 with small, red perturbation, following [3]. pred 2 2 Findings: Deterministic video models such as CWM regress to the mean future state, blurring predictions. Deterministic models, such as the Counterfactual World Model (CWM), often produce blurry predictions as they model single, average future state (Figure 4A). This hurts optical flow extraction in two ways. (1) At the location where the perturbation is carried to, the perturbation is less visible due to blurriness from prediction uncertainty. (2) At locations where the perturbation is not expected to propagate, blurriness manifests as minor RGB differences. The clean and perturbed 4 predictions are different in these locations due to the added perturbation in the first frame (F1). Combined, these introduce errors when using arg-max to select the destination for the query point. To address the issue of blurry predictions from CWM, we next explore extracting flow from Stable Video Diffusion (SVD), diffusion model that produces sharp predictions. 4.2 Study 2: Diffusion models global latent code lacks sufficient fine-grained controllability Method: Applying the method above (Section 3, Figure 2) to Stable Video Diffusion [5] is challenging as generation operates on full-frame latents rather than pixels, so these models do not natively support partial frames as input. However, we need to provide information from the second frame to anchor the generation, otherwise the model will hallucinate arbitrary future frames unsuitable for flow estimation. Below, we describe how we adapt the flow extraction procedure to the latent diffusion setting, while keeping the generations anchored to the observed video. Step 1. Frame selection and perturbation. As SVD is trained with fixed framerate and more than two frames, we first retrieve an interpolated sequence of intermediate frames bridging the query (F1) and target (FN ) frames = {F1, F2, . . . , FN }. We add dot perturbation to F1, obtaining F1. Step 2. Latent inversion and paired generation. To anchor the sampling trajectory to the original video, we apply latent inversion technique introduced in VideoShop [10]. Inversion addresses the drift in generation by finding an initial noise vector that reconstructs the input video when fed into the diffusion process. Specifically, we first obtain the latents for every frame ℓ = {ℓ1, ℓ2, . . . , ℓN } and then partially apply the inversion process, adding noise to the latents up to fraction of the total noising steps. This is done using SVDs UNet and an inverted scheduler, resulting in noisy latents ℓ = {ℓ1, . . . , ℓN }. From these latents we perform two denoising passes with identical hyperparameters, but with different conditioning frames, F1 and the perturbed F1, obtaining two denoised latents ℓpred and ℓ , which can be decoded to produce the clean predand perturbed reconstruction pred. Step 3. Estimate optical flow (same as in Section 3). We compute the RGB difference between the clean and perturbed generations to localize the perturbation. We compare two generations, rather than the perturbed generation to the original input, in order to suppress noise from imperfect inversion and VAE artifacts, improving the accuracy and robustness of flow predictions even under lossy latent compression. pred Findings: Stable Video Diffusion (SVD) lacks fine-grained controllability as it generates each frame by globally denoising coarse latent code. Thus, local perturbations in pixel space are washed out or remapped unpredictably during sampling. This stochastic, scene-level regeneration prevents deterministic, point-wise correspondences. As result, its clean and perturbed predictions differ in locations where the perturbation is not supposed to be carried to, resulting in noisy differences. Attempting to address the precision issues arising from coarse global latent, the recent DiffTrack [28] uses one-to-one frame-to-latent mapping to avoid temporal compression. It uses query-key matching in the 3D attention blocks across frames to perform point tracking. Nevertheless, DiffTrack performs worse than specialized flow models  (Table 3)  . Next, we explore autoregressive models which allow us to provide actual patches of the second frame as more fine-grained conditioning. 4.3 Study 3: Raster-order autoregressive models struggle with partial-frame conditioning Method: Cosmos [29] includes both diffusion-based and an autoregressive foundation model. We evaluate flow extractions from the autoregressive model which allows us to provide the actual frame patches directly as conditioning. We use the Cosmos autoregressive model with 4B parameters, which comes with 7B diffusion decoder for generating images. As the Cosmos autoregressive model does not have pointer tokens, autoregressive rollout predictions for image tokens cannot be performed in random-access order. Instead, they can only be made in raster order, i.e., from top left to bottom right of the image. Nevertheless, we make best-effort attempt to extract flow by trying multiple settings. 2 pred 2 ) and perturbed ( F1, top-10%-raster Setting A. We provide the first frame and the top 10% raster tokens of the second frame: clean (F1, top-10%-raster 2 Setting B. We provide the first frame and overwrite random 10% of the predicted tokens during the models autoregressive rollout with ground truth tokens of the second frame: clean (F1, overwrite-random-10% Setting C. We provide both frames fully: clean (F1, F2 pred Findings: All Cosmos evaluation settings perform poorly (Figure 7 and Table 2). ). ) and perturbed ( F1, F2 pred ) and perturbed ( F1, overwrite-random-10% pred 2 pred 2 pred ). ). 2 2 2 5 Figure 3: KL-tracing, our novel yet simple test-time inference procedure for extracting optical flow from controllable generative models such as LRAS. We follow the same steps for perturbation and conditioned prediction as in Figure 2, but estimate optical flow by computing the KL divergence between the clean and perturbed prediction logits. Setting A. The model prediction effectively repeats the first frame, unable to propagate the perturbation (Figure 7A). This is because the top 10% raster patches reveal less about the second frame than random 10% patches. At the same time, revealing more patches will hurt performance; if the perturbation should be carried to revealed patch, the model will not generate the perturbation. Setting B. This also results in the model prediction repeating the first frame, because the model generates many new patches with less than 10% ground truth patches. This is much more challenging than getting all the revealed ground truth patches at the start. Setting C. The model prediction correctly matches the target frame but does not contain the perturbation. We evaluate this setting as the model has diffusion decoder that has the potential to (but unfortunately does not) reproduce the perturbation in pred . Cosmos flow extraction is challenging due to the lack of non-local tokenizer and pointer tokens for random-access decoding order. Therefore, we next explore LRAS, which has these properties. 4.4 Study 4: Key model properties for flow extraction are present in LRAS Method: From Sections 4.1, 4.2, and 4.3, we discover that flow extraction requires generative models to have three key properties  (Table 1)  : (1) distributional prediction of future frames (avoiding blurry or noisy outputs; which CWM lacks); (2) local tokenizer that treats each spatio-temporal patch independently (which SVD and Cosmos lack); and (3) random-access decoding order that allows the model to condition on any subset of the second frame patches (which SVD and Cosmos lack). We identify Local Random Access Sequence (LRAS), recent family of autoregressive visual foundation world models that has these properties and can be zero-shot prompted to perform many tasks, such as 3D object manipulation, novel view synthesis, and depth estimation. We use its autoregressive generative video world model with 7 billion (7B) parameters. Findings: LRAS model produces clean and perturbed predictions with minimal variations due to sampling randomness. However, these small variations result in slightly noisy difference maps, harming flow extraction (Figure 5). We address this with the KL-tracing procedure below."
        },
        {
            "title": "5 KL-tracing bypasses sampling randomness",
            "content": "Method: We design novel test-time inference procedure, KL-tracing, for optical flow extraction from models that predict probability distribution at every patch of the target frame (Figure 3). KL-tracing is the same as the RGB flow extraction method in Section 3, except for the third step. Step 3. Estimate optical flow with patchwise KL-divergence. We use the patch-wise logits for the clean (F pred ) predictions: {zij} and {z pert ij } (Figure 3). For every patch ij )(cid:3). The resulting KL divergence (i, j) we compute the KL-divergence: DKL(i, j) = KL(cid:2)(zij) (z pert map should peak at the patch where the perturbation is carried to, giving crisp optical flow estimate (Figure 3). We take an arg-max to identify the patch with the highest KL divergence to estimate optical flow. To detect if point is occluded, we simply use threshold on the KL divergence value. ) and perturbed ( pred 2 2 6 Figure 4: Our method, KL-tracing using LRAS extracts better flow than other generative video models. (A) Deterministic models, such as CWM [3], often produce blurry predictions as they model single, average future state. (B) Stable Video Diffusion lacks fine-grained controllability due to its coarse global latent code. Its clean and perturbed predictions differ in locations where the perturbation is not supposed to be carried to. (C) The Cosmos autoregressive world model lacks fine-grained controllability as it does not utilize pointers to denote the position of each token, making it challenging to prompt for flow extraction. (D) The LRAS model is highly controllable and has minimal differences between the clean and perturbed predictions. We use KL-tracing to compute the difference in logit instead of RGB space, obtaining sharp flow extractions. Findings: KL divergence of prediction logits bypasses noisy differences resulting from sampling randomness. Each RGB generation is sample drawn from the prediction distribution. Instead of sampling multiple RGB generations and averaging them to suppress the noisy differences resulting from sampling randomness, it is more efficient to directly use the predicted distribution. This is an important benefit of autoregressive models as they directly expose the probability distribution. Empirically, we observe two benefits. First, there are minor differences between the clean and perturbed predictions at locations where the perturbation is not expected to propagate, because adding the perturbation affects the rest of the image due to the transformers global attention. KLtracing empirically results in smaller noisy differences than computing the RGB difference. Second, occasionally the added perturbation does not visually get carried over in the perturbed RGB prediction, but it still appears as elevated uncertainty in logit space, detected by KL-tracing."
        },
        {
            "title": "6 Large video world models capture aspects of real-world dynamics that are",
            "content": "challenging for specialized flow models Method: We compare KL-tracing with supervised flow methods such as RAFT [37] and SEA-RAFT [39], as well as unsupervised methods such as Doduo [21] and SMURF [36] (Table 3, Figure 6). Findings: Powerful video models capture challenging aspects of real-world dynamics. For example, globe undergoes in-place rotation with the query point on textureless surface (Figure 1), or physical motion examples such as Newtons cradle (Figure 6). These real-world dynamics are challenging for specialized flow models which are small and not trained on diverse, large-scale video datasets. Video world models use generic training objectives, so they do not suffer from failure modes introduced by training heuristics used by specialized flow models. Supervised methods are trained on synthetic datasets as labeling real-world videos is expensive, and hence face sim-to-real gap where they do not observe challenging real-world dynamics. Self-supervised methods 7 Figure 5: KL-divergence of prediction distributions bypasses noisy RGB differences resulting from sampling randomness. Computing the KL divergence of the clean and perturbed prediction logits (last column) is more efficient yet functionally similar to computing the average RGB difference over many samples (second last column). Figure 6: Large world models such as LRAS capture aspects of real-world dynamics that are challenging for specialized flow models relying on visual similarity or photometric loss. Specialized optical flow baselines, both supervised and self-supervised, struggle on complex, realworld scenes that are not fully captured by their training heuristics (Section 6). deeper understanding of dynamics beyond feature/photometric similarity allows large world models such as LRAS to resolve motion in the presence of homogeneous objects, motion blur, and partial occlusion, among others. such as Doduo [21] and SMURF [36] use photometric loss for training, which results in challenges in predicting flow for frame pairs with significant differences in light intensity, and also enforce strong global consistency and smoothness constraints, which introduce failure modes for complex non-uniform dynamics that contain various magnitudes of flow across the same frame pair."
        },
        {
            "title": "7 Discussion",
            "content": "Taming generative world models with fine-grained controllability. key finding of our work is that zero-shot prompting many mainstream generative video models for visual property extraction can be challenging due to their lack of fine-grained controllability. However, series of design features can help tame these models: (1) predicting distribution of future states rather than deterministic 8 Distributional prediction of future frames No Yes Non-global tokenizer Yes Random-access decoding order Yes No No Yes No No Yes Yes Yes CWM [3] SVD [5] Cosmos [29] LRAS [24] Table 1: Key properties of video models for precise flow extraction: (1) distributional prediction of future frames, thus avoiding blurry or noisy outputs, (2) non-global tokenizer that treats each spatio-temporal patch independently, (3) random-access decoding order that allows conditioning on any subset of second-frame patches. Model TAP-Vid DAVIS Subset (3%) Endpoint Error (EPE) LRAS RGB (5MM, 8MS, 2STD) (ours) LRAS KL (5MM, 8MS, 2STD) (ours) Stable Video Diffusion [5] Cosmos (top 10% raster) (5MM, 2STD, 512512) [29] Cosmos (overwrite 10% during rollout) (5MM, 2STD, 512512) [29] Cosmos (provide full second frame) (5MM, 2STD, 512512) [29] 8.4797 5.0762 74.7990 35.4338 37.7552 66.5521 Table 2: KL-tracing with LRAS beats other video models. MM = multi-mask (average over multiple random masks), MS = multi-scale (zoom into image at multiple scales). Method TAP-Vid First RAFT [37] SEA-RAFT [39] DAVIS Kubric AJ AD < δx avg OA AJ AD < δx avg OA 41.77 43. 25.33 20.18 54.37 58.69 66.40 66. 71.93 75.06 5.60 6.54 82.15 84. 88.54 89.50 Doduo [21] 23.34 13.41 48. 47.91 54.98 5.31 72.20 73.56 SMURF [36] DiffTrack [28] CWM [3, 38] LRAS with KL-tracing (ours) 30.64 - 15.00 44.16 27.28 - 23.53 11.18 44.18 46.90 26.30 65.20 59.15 - 76.63 74.58 65.81 - 28.77 65. 6.81 - 11.64 5.06 80.57 - 41.63 81.66 87.91 - 84.93 87.63 Table 3: TAP-Vid First: quantitative results on DAVIS and Kubric. Tracking starts when point first appears and continues to the video end, thus involving large frame gaps. LRAS with KL-tracing outperforms two-frame baselines. S\" = supervised, W\" = weakly supervised, U\" = unsupervised. average, (2) encoding each spatio-temporal patch independently, and (3) having mechanism to decode individual parts of frame in any given order. Currently, the LRAS [24] framework was the only one we identified that contained all of these properties, but they could be adopted by other generative models as well, increasing the ease with which they can be visually prompted. Future work. (1) natural next step is extending this test-time inference procedure to extract other visual intermediates such as object segments, material properties or motion affordances. By moving away from fine-tuning our world models on labeled datasets to paradigm of zero-shot prompting, the vision community can experience shift akin to the one seen with large language models, moving from rigid representations constructed by fine-tuning toward dynamic, task-specific and adaptable ones extracted on the fly. (2) Extracting flow from large video models is more computationally demanding than dense flow estimator such as RAFT [37]. However, our model can act as an oracle for high-quality labels. Our next step is distilling LRAS with KL-tracing into faster architecture. Overall, our results indicate that prompting controllable, self-supervised world models is scalable and effective alternative to supervised or photometric-loss approaches for high-quality optical flow."
        },
        {
            "title": "8 Acknowledgments",
            "content": "This work was supported by the following awards: Simons Foundation grant SFI-AN-NC-GBCulmination-00002986-05, National Science Foundation CAREER grant 1844724, National Science Foundation Grant NCS-FR 2123963, Office of Naval Research grant N00014-20-1-2589, ONR MURI N00014-21-1-2801, ONR MURI N00014-24-1-2748, and ONR MURI N00014-22-1-2740. We also thank the Stanford HAI, Stanford Data Sciences, the Marlowe team, and the Google TPU Research Cloud team for their computing support."
        },
        {
            "title": "References",
            "content": "[1] Mido Assran, Adrien Bardes, David Fan, Quentin Garrido, Russell Howes, Mojtaba, Komeili, Matthew Muckley, Ammar Rizvi, Claire Roberts, Koustuv Sinha, Artem Zholus, Sergio Arnaud, Abha Gejji, Ada Martin, Francois Robert Hogan, Daniel Dugas, Piotr Bojanowski, Vasil Khalidov, Patrick Labatut, Francisco Massa, Marc Szafraniec, Kapil Krishnakumar, Yong Li, Xiaodong Ma, Sarath Chandar, Franziska Meier, Yann LeCun, Michael Rabbat, and Nicolas Ballas. V-jepa 2: Self-supervised video models enable understanding, prediction and planning, 2025. [2] Adrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael Rabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas. Revisiting Feature Prediction for Learning Visual Representations from Video, February 2024. arXiv:2404.08471 [cs]. [3] Daniel M. Bear, Kevin Feigelis, Honglin Chen, Wanhee Lee, Rahul Venkatesh, Klemen Kotar, Alex Durango, and Daniel L. K. Yamins. Unifying (Machine) Vision via Counterfactual World Modeling, June 2023. arXiv:2306.01828 [cs]. [4] Zhangxing Bian, Allan Jabri, Alexei A. Efros, and Andrew Owens. Learning Pixel Trajectories with Multiscale Contrastive Random Walks, April 2022. arXiv:2201.08379 [cs]. [5] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, Varun Jampani, and Robin Rombach. Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets, November 2023. arXiv:2311.15127 [cs]. [6] Daniel J. Butler, Jonas Wulff, Garrett B. Stanley, and Michael J. Black. Naturalistic Open Source Movie for Optical Flow Evaluation. In David Hutchison, Takeo Kanade, Josef Kittler, Jon M. Kleinberg, Friedemann Mattern, John C. Mitchell, Moni Naor, Oscar Nierstrasz, C. Pandu Rangan, Bernhard Steffen, Madhu Sudan, Demetri Terzopoulos, Doug Tygar, Moshe Y. Vardi, Gerhard Weikum, Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid, editors, Computer Vision ECCV 2012, volume 7577, pages 611625. Springer Berlin Heidelberg, Berlin, Heidelberg, 2012. Series Title: Lecture Notes in Computer Science. [7] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An Improved Autoregressive Generative Model, December 2017. arXiv:1712.09763 [cs]. [8] Aidan Clark, Jeff Donahue, and Karen Simonyan. Adversarial Video Generation on Complex Datasets, September 2019. arXiv:1907.06571 [cs]. [9] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adrià Recasens, Lucas Smaira, Yusuf Aytar, João Carreira, Andrew Zisserman, and Yi Yang. TAP-Vid: Benchmark for Tracking Any Point in Video, March 2023. arXiv:2211.03726 [cs]. [10] Xiang Fan, Anand Bhattad, and Ranjay Krishna. Videoshop: Localized semantic video editing with noise-extrapolated diffusion inversion, 2024. [11] Ye Fang, Zeyi Sun, Shangzhan Zhang, Tong Wu, Yinghao Xu, Pan Zhang, Jiaqi Wang, Gordon Wetzstein, and Dahua Lin. Relightvid: Temporal-consistent diffusion model for video relighting. arXiv preprint arXiv:2501.16330, 2025. [12] Philipp Fischer, Alexey Dosovitskiy, Eddy Ilg, Philip Häusser, Caner Hazırbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. FlowNet: Learning Optical Flow with Convolutional Networks, May 2015. arXiv:1504.06852 [cs]. 10 [13] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J. Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji, Hsueh-Ti, Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: scalable dataset generator, March 2022. arXiv:2203.03570 [cs]. [14] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to Control: Learning Behaviors by Latent Imagination, March 2020. arXiv:1912.01603 [cs]. [15] Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning Latent Dynamics for Planning from Pixels, June 2019. arXiv:1811.04551 [cs]. [16] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering Atari with Discrete World Models, February 2022. arXiv:2010.02193 [cs]. [17] Adam W. Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories, July 2022. arXiv:2204.04153 [cs]. [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models, December 2020. arXiv:2006.11239 [cs]. [19] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video Diffusion Models, June 2022. arXiv:2204.03458 [cs]. [20] Allan Jabri, Andrew Owens, and Alexei A. Efros. Space-Time Correspondence as Contrastive Random Walk, December 2020. arXiv:2006.14613 [cs]. [21] Zhenyu Jiang, Hanwen Jiang, and Yuke Zhu. Doduo: Learning Dense Visual Correspondence from Unsupervised Semantic-Aware Flow, September 2023. arXiv:2309.15110 [cs]. [22] Rico Jonschkowski, Austin Stone, Jonathan T. Barron, Ariel Gordon, Kurt Konolige, and Anelia Angelova. What Matters in Unsupervised Optical Flow, August 2020. arXiv:2006.04902 [cs]. [23] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. CoTracker: It is Better to Track Together, October 2024. arXiv:2307.07635 [cs]. [24] Wanhee Lee, Klemen Kotar, Rahul Mysore Venkatesh, Jared Watrous, Honglin Chen, Khai Loong Aw, and Daniel L. K. Yamins. 3D Scene Understanding Through Local Random Access Sequence Modeling, April 2025. arXiv:2504.03875 [cs]. [25] Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao Wang, Ying Tai, Donghao Luo, Chengjie Wang, Jilin Li, and Feiyue Huang. Learning by Analogy: Reliable Supervision from Transformations for Unsupervised Optical Flow Estimation, November 2020. arXiv:2003.13045 [cs]. [26] Neelu Madan, Andreas Moegelmose, Rajat Modi, Yogesh S. Rawat, and Thomas B. Moeslund. Foundation models for video understanding: survey, 2024. [27] Nikolaus Mayer, Eddy Ilg, Philip Häusser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 40404048, June 2016. arXiv:1512.02134 [cs]. [28] Jisu Nam, Soowon Son, Dahyun Chung, Jiyoung Kim, Siyoon Jin, Junhwa Hur, and Seungryong Kim. Emergent temporal correspondences from video diffusion transformers, 2025. [29] NVIDIA, Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, Daniel Dworakowski, Jiaojiao Fan, Michele Fenzi, Francesco Ferroni, Sanja Fidler, Dieter Fox, Songwei Ge, Yunhao Ge, Jinwei Gu, Siddharth Gururani, Ethan He, Jiahui Huang, Jacob Huffman, Pooya Jannaty, 11 Jingyi Jin, Seung Wook Kim, Gergely Klár, Grace Lam, Shiyi Lan, Laura Leal-Taixe, Anqi Li, Zhaoshuo Li, Chen-Hsuan Lin, Tsung-Yi Lin, Huan Ling, Ming-Yu Liu, Xian Liu, Alice Luo, Qianli Ma, Hanzi Mao, Kaichun Mo, Arsalan Mousavian, Seungjun Nah, Sriharsha Niverty, David Page, Despoina Paschalidou, Zeeshan Patel, Lindsey Pavao, Morteza Ramezanali, Fitsum Reda, Xiaowei Ren, Vasanth Rao Naik Sabavat, Ed Schmerling, Stella Shi, Bartosz Stefaniak, Shitao Tang, Lyne Tchapmi, Przemek Tredak, Wei-Cheng Tseng, Jibin Varghese, Hao Wang, Haoxiang Wang, Heng Wang, Ting-Chun Wang, Fangyin Wei, Xinyue Wei, Jay Zhangjie Wu, Jiashu Xu, Wei Yang, Lin Yen-Chen, Xiaohui Zeng, Yu Zeng, Jing Zhang, Qinsheng Zhang, Yuxuan Zhang, Qingqing Zhao, and Artur Zolkowski. Cosmos World Foundation Model Platform for Physical AI, March 2025. arXiv:2501.03575 [cs]. [30] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray Kavukcuoglu. Conditional Image Generation with PixelCNN Decoders, June 2016. arXiv:1606.05328 [cs]. [31] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating Diverse High-Fidelity Images with VQ-VAE-2, June 2019. arXiv:1906.00446 [cs]. [32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding, May 2022. arXiv:2205.11487 [cs]. [33] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. arXiv preprint arXiv:2406.01493, 2024. [34] Ayush Shrivastava and Andrew Owens. Self-Supervised Any-Point Tracking by Contrastive Random Walks, September 2024. arXiv:2409.16288 [cs]. [35] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations, February 2021. arXiv:2011.13456 [cs]. [36] Austin Stone, Daniel Maurer, Alper Ayvaci, Anelia Angelova, and Rico Jonschkowski. SMURF: Self-Teaching Multi-Frame Unsupervised RAFT with Full-Image Warping, May 2021. arXiv:2105.07014 [cs]. [37] Zachary Teed and Jia Deng. RAFT: Recurrent All-Pairs Field Transforms for Optical Flow, August 2020. arXiv:2003.12039 [cs]. [38] Rahul Venkatesh, Honglin Chen, Kevin Feigelis, Daniel M. Bear, Khaled Jedoui, Klemen Kotar, Felix Binder, Wanhee Lee, Sherry Liu, Kevin A. Smith, Judith E. Fan, and Daniel L. K. Yamins. Counterfactual World Modeling for Physical Dynamics Understanding, December 2023. arXiv:2312.06721 [cs]. [39] Yihan Wang, Lahav Lipson, and Jia Deng. SEA-RAFT: Simple, Efficient, Accurate RAFT for Optical Flow, May 2024. arXiv:2405.14793 [cs]."
        },
        {
            "title": "Predictions based on conditioning Compute difference",
            "content": "White Gaussian CWM [3] White Gaussian Noised latents of Frame 1 and 2 SVD [5] Cosmos [29] White Gaussian LRAS, RGB [24] White Gaussian White Gaussian LRAS, KL Frame 1, Partial Frame 2 Frame 1, Partial Frame 2 Frame 1, Partial Frame 2 Frame 1, Partial Frame"
        },
        {
            "title": "RGB difference\nRGB difference\nRGB difference\nRGB difference\nKL divergence of logits",
            "content": "Table 4: Flow extraction for all models follows the same three-step template: (1) Perturbation of Frame 1 with small, white-colored Gaussian bump, (2) Conditioning the model on frames 1 and 2 to generate clean and perturbed predictions, and (3) Compute the difference between the clean and perturbed predictions to extract optical flow."
        },
        {
            "title": "B Additional results",
            "content": "Figure 7: All evaluation settings for Cosmos [29]) result in poor flow extractions. See Section 4.3 for more detailed explanation of each result."
        }
    ],
    "affiliations": []
}