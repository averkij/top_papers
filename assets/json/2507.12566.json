{
    "paper_title": "Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models",
    "authors": [
        "Gen Luo",
        "Wenhan Dou",
        "Wenhao Li",
        "Zhaokai Wang",
        "Xue Yang",
        "Changyao Tian",
        "Hao Li",
        "Weiyun Wang",
        "Wenhai Wang",
        "Xizhou Zhu",
        "Yu Qiao",
        "Jifeng Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into a single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed a new visual parameter space into a pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates a set of visual experts through a multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and re-organizes the pre-training process in an efficient manner. During inference, it includes a fused CUDA kernel to speed up its MoE operations. With these designs, Mono-InternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with Mono-InternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL."
        },
        {
            "title": "Start",
            "content": "MONO-INTERNVL-1.5 1 Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu Qiao, Jifeng Dai 5 2 0 2 J 6 1 ] . [ 1 6 6 5 2 1 . 7 0 5 2 : r AbstractThis paper focuses on monolithic Multimodal Large Language Models (MLLMs), which integrate visual encoding and language decoding into single model. Existing structures and pre-training strategies for monolithic MLLMs often suffer from unstable optimization and catastrophic forgetting. To address these challenges, our key idea is to embed new visual parameter space into pre-trained LLM, enabling stable learning of visual knowledge from noisy data via delta tuning. Based on this principle, we first introduce Mono-InternVL, an advanced monolithic MLLM that incorporates set of visual experts through multimodal mixture-of-experts architecture. In addition, we design an innovative Endogenous Visual Pretraining (EViP) for Mono-InternVL to maximize its visual capabilities via progressive learning. Mono-InternVL achieves competitive performance against existing MLLMs but also leads to relatively expensive data cost. Therefore, we further present Mono-InternVL-1.5, cheaper and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++ introduces additional visual attention experts to Mono-InternVL-1.5 and reorganizes the pre-training process in an efficient manner. During inference, Mono-InternVL-1.5 includes fused CUDA kernel to speed up its MoE operations. With these designs, MonoInternVL-1.5 significantly reduces training and inference costs, while still maintaining competitive performance with MonoInternVL. To evaluate our approach, we conduct extensive experiments across 15 benchmarks. Results demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared to its modular counterpart, i.e., InternVL1.5, Mono-InternVL-1.5 achieves similar multimodal performance while reducing first-token latency by up to 69%. Code and models are released at https://github.com/OpenGVLab/Mono-InternVL. Index TermsMultimodal Large Language Model, Visual Pretraining, Monolithic Model I. INTRODUCTION Recent years have witnessed the significant achievement of Multimodal Large Language Models (MLLMs) [1][3] in various vision-language tasks. As illustrated in Fig. 1(a), most existing Multimodal Large Language Models (MLLMs) adopt modular architecture, where visual encoding and language decoding are handled separately. This approach is typically realized by combining pre-trained visual encoder [4] with an LLM [5][7]. In contrast, monolithic MLLMs [8][10] have become another popular research trend in the community, as shown in Fig. 1(b), which integrate visual perception and Gen Luo, Wenhao Li, Weiyun Wang and Yu Qiao are with Shanghai Artificial Intelligence Laboratory. Wenhan Dou, Xizhou Zhu and Jifeng Dai are with Tsinghua University. Changyao Tian, Hao Li and Wenhai Wang are with The Chinese University of Hong Kong. Zhaokai Wang and Xue Yang are with Shanghai Jiao Tong University. Corresponding author: Jifeng Dai (daijifeng@tsinghua.edu.cn). TABLE I: Overall comparison of Mono-InternVL and Mono-InternVL-1.5. Mono-InternVL-1.5 greatly improves the training and inference efficiency while maintaining competitive downstream performance. Method Training Data Inference Speed VQA Bench MLLM Bench Mono-InternVL [11] Mono-InternVL-1.5 1.1B 0.5B -58% 61 tokens/s 77 tokens/s +26% 70.1 70.4 53.7 55.6 +0.3% +1.9% multimodal understanding within unified LLM framework. Compared to modular MLLMs, monolithic MLLMs often exhibit better potential in terms of design simplicity and deployment efficiency [9], [10]. Despite these advancements, training monolithic MLLM that achieves competitive performance still remains significant challenge. Among them, native pre-training [12] pre-trains monolithic MLLM from scratch using combination of textonly and multimodal data. However, this method demands extremely high computational resources and is prone to optimization instability [12]. Another promising solution is to extend the pre-trained LLM to multimodality via additional visual pre-training, namely continuous pre-training [9]. Such approaches typically require much cheaper training costs but easily incurs the catastrophic forgetting issue [13], thereby undermining the pre-trained language knowledge. In this paper, we aim to address the forgetting issue of continuous pre-training from the perspective of delta tuning [14]. Specifically, delta tuning fine-tunes set of newly added parameters in the model while keeping the rest frozen, thereby preserving the original knowledge. However, existing methods adopt shared architecture for joint vision and language modeling, where optimizations for vision can negatively impact language capabilities. Therefore, it is natural thought to introduce an independent visual parameter set into the pre-trained LLM, thus retaining the language knowledge by freezing the entire LLM while facilitating visual learning. This principle is also aligned with previous endeavors in modular MLLMs, e.g., QwenVL [15] and InternVL [6], where the visual parameters are placed outside the LLM. Based on the above principle, we propose novel monolithic MLLM, namely Mono-InternVL. As shown in Fig. 2, the visual parameters in Mono-InternVL are instantiated as set of expert networks via the mixture-of-experts (MoEs) mechanism. Based on this architecture, we present an innovative Endogenous Visual Pre-training (EViP) method to optimize the visual MONO-INTERNVL-1.5 2 Fig. 1: Comparison of Mono-InternVL, Mono-InternVL-1.5 and existing MLLMs. Compared with modular MLLMs, Mono-InternVL and Mono-InternVL-1.5 embed visual experts into the pre-trained LLM and integrates visual encoding and language decoding into single LLM. Through endogenous visual pre-training (EViP), Mono-InternVL significantly pushes the performance boundaries of monolithic MLLMs. With EViP++, Mono-InternVL-1.5 not only significantly reduces data costs, but also maintains the competitive performance of downstream tasks. parameters. Specifically, EViP is formulated as progressive learning process of three stages: 1) concept learning to grasp basic visual concepts, 2) semantic learning to capture high-level semantics, e.g., world knowledge, and 3) alignment learning to align knowledge with downstream tasks. Benefiting from the architecture and the pre-training strategy, the visual scalability of Mono-InternVL is fully unleashed, where the downstream performance consistently improves as the scale of the pretraining data increases. Nevertheless, Mono-InternVL still requires expensive expenditures for its pre-training, e.g., billions of image-text pairs, and its deployment is still unfriendly due to the modality-specific MoEs. To overcome above limitations, we further present Mono-InternVL-1.5, cheaper and faster monolithic MLLM equipped with an improved Endogenous Visual Pre-training (EViP++). Compared to EViP, the core idea of EViP++ is to maximize the learning ability of the model while minimizing redundancy of the data. In particular, EViP++ firstly enlarges the visual parameter space and learning capability by embedding visual attention experts into Mono-InternVL-1.5. Then, EViP++ reorganizes the training data according to the principle of less is more [16], i.e., small in quantity but high in quality. To further facilitate the efficiency, we introduce fused CUDA kernel to speed up the computation of the multimodal mixtureof-experts mechanism. As shown in Tab. I, the training data and inference cost of Mono-InternVL-1.5 can be significantly reduced, while the performance is still improved. To validate our method, we develop Mono-InternVL and Mono-InternVL-1.5 using the pre-trained LLM InternLM21.8B [3], and conduct extensive experiments on 15 multimodal benchmarks. Experimental results demonstrate the significant performance improvements of Mono-InternVL and MonoInternVL-1.5 against previous monolithic MLLMs. For instance, Mono-InternVL-1.5 with 1.8 billion activated parameters can obviously outperform existing monolithic MLLMs with 8 billion parameters, e.g., +2.8% over Emu3 [17] on average. Compared to the modular baseline, i.e., InternVL-1.5 [6], MonoInternVL-1.5 shows comparable performance on 15 multimodal benchmarks while reducing first token latency by 69.3%. In conclusion, our contributions can be summarized in five aspects: We present Mono-InternVL, novel monolithic MLLM that seamlessly integrates set of visual experts via multimodal mixture-of-experts architecture. This architecture effectively extends the pre-trained LLM to monolithic MLLM while retaining the pre-trained knowledge. We propose novel visual pre-training approach for MonoInternVL called endogenous visual pre-training (EViP). EViP adopts progressive learning strategy to encourage visual experts to continuously grasp visual knowledge from noisy data to high-quality data. We introduce visual attention experts and an improved EViP (EViP++) to boost the data efficiency during pre-training. Based on these strategies, we present Mono-InternVL-1.5, cheaper and faster monolithic MLLM that achieves stronger performance than Mono-InternVL using only 42% data. We propose an innovative fused cuda kernel for the multimodal MoE in Mono-InternVL and Mono-InternVL-1.5, which greatly speeds up the model inference by up to 26%. Extensive experiments on 15 multimodal benchmarks demonstrate that our monolithic MLLMs can reach the comparable performance and superior efficiency to leading modular MLLMs, opening new avenues for designing future MLLMs. This paper is built upon our work published in CVPR 2025 [11]. Compared to the original version, we have made MONO-INTERNVL-1.5 3 substantial extensions in five aspects in terms of model designs and experiments. 1) We present Mono-InternVL-1.5, cheaper and faster monolithic MLLM than the original Mono-InternVL. Mono-InternVL-1.5 demonstrates stronger downstream performance than Mono-InternVL on multiple MLLM benchmarks. 2) In Mono-InternVL-1.5, we introduce visual attention experts and an improved endogenous visual pretraining (EViP++) to significantly improve the data efficiency while retaining powerful performance. Fig. II illustrates the scaling property and advantages of EViP++. 3) We propose novel CUDA kernel for multimodal mixture-of-experts, which can obviously speed up inference. Our comparison in Tab. XII confirms its advantages against the default Pytorch implementation. 4) In Tab. VII - X, we conduct more ablations and qualitative analysis to further compare the impact of different designs in Mono-InternVL. 5) In Tab. IV - VI, XI, XIII and Fig. 5 and 6, we conduct extensive experiments and visualizations to validate Mono-InternVL-1.5 in terms of effectiveness and efficiency. II. RELATED WORK Modular multimodal large language models. Recent advancements in large language models (LLMs) have driven the fusion of vision and language modalities, resulting in the development of multimodal large language models (MLLMs) [6], [17][23]. Both commercial models like GPT-4o [18] and Gemini series [19] and open-source ones like BLIP series [7], [24], [25], LLaVA series [5], [20], [26], Qwen-VL [15], [27], [28] and InternVL [6], [29][32] have been actively working on this fusion. They often link LLMs [2], [3], [33], [34] with large vision models (LVMs) [4], [29], [35] through intermediate layers. Leveraging the advantages of extensively pre-trained visual encoders and state-of-the-art language models, these modular structures exhibit impressive performance across broad range of multimodal tasks. Recent open-source frameworks, e.g. InternVL 2.5 [30] and Qwen2.5-VL [28], demonstrate the efficacy of modular designs. Through largescale multimodal pre-training and advanced visual-language alignment techniques, they achieve outcomes on par with leading commercial models. However, as noted in [9], such encoder-based vision-language models confront several issues. These include restrictions in visual processing due to pretrained encoders, inefficiencies in deployment, and difficulties in balancing the capabilities of LLMs and LVMs."
        },
        {
            "title": "Monolithic multimodal",
            "content": "large language models. The problems linked to modular MLLMs have directed research towards encoder-free architectures, also referred to as monolithic MLLMs, which can be divided into two types. The first type centers on generating continuous visual tokens via lightweight structures prior to inputting them into MLLMs. For instance, Fuyu-8B [8] processes images directly using simple linear projection, adeptly handling high-resolution input images without requiring specialized visual encoder. EVE-7B [9] emphasizes vision-language pre-alignment from an LLM-focused perspective and improves image recognition via visual distillation. SOLO [10] puts forward an open-source training approach to facilitate the advancement of monolithic MLLMs. In comparison, the second type introduces models based on VQ tokenizers to generate discrete visual tokens for image creation. Representative works include Chameleon [12], Show-o [36], Transfusion [37], and Emu3 [17]. These models convert images into discrete tokens, which simplifying the processing of visual information and enhancing generative capabilities. Monolithic MLLMs offer benefits such as not depending on pre-trained visual encoders, simplicity in design, and efficiency in deployment. Nonetheless, training highperformance monolithic MLLM is still significant challenge. Multimodal mixture-of-experts. VLMo [38] and BEiT3 [39] use set of modality experts to replace the feed-forward network in the Transformer. They effectively capture modalityspecific information by switching to different modality experts and employ shared self-attention across modalities to align visual and linguistic information. Based on the above works, VL-MoE [40] introduces mixture-of-experts (MoE) [41] to enhance efficiency of training and deployment. MoMa [42] also utilizes multimodal mixture-of-experts for pre-training MLLMs [12] and collaborates with sparse components, such as MoE and mixture-of-depths (MoD) [43], to boost the efficiency of pre-training from scratch with trillions of mixed-modal tokens. ARIA [44] further makes use of fine-grained multimodal MoEs to aid in understanding inputs from various data distributions, showcasing the potential of MoE architectures in constructing powerful MLLMs. Drawing inspiration from the above literature, we propose integrating multimodal mixtureof-experts (specifically, visual expert and language expert) into both multi-head attentions and feed-forward networks for pre-training monolithic MLLMs. We also introduce novel progressive learning strategies, namely endogenous visual pretraining (EViP and EViP++), to address the unique challenges of training monolithic MLLMs. III. MONO-INTERNVL A. Monolithic Architecture As illustrated in Fig. 2, we first present the architecture of Mono-InternVL, which comprises tokenizers and multimodal mixture-of-experts structure. Visual and textual embeddings. In contrast to modular MLLMs, Mono-InternVL directly convert images to input visual sequences with lightweight patch embedding module. Specifically, given the input image RHW 3, the input visual embedding xv R(hw)d is obtained by xv = MLP(PatchEmbed(I) + PE). (1) Here, PatchEmbed() denotes patch embedding layer with stride of 28, i.e. each visual token correspond to 2828 image patch. PE R(hw)d is the learnable positional embedding, similar to that in InternVL-1.5 [6]. We also add an additional thumbnail to provide global visual information into the model. Subsequently, an MLP layer MLP() is employed to project visual patches into the d-dimensional embedding space of the LLM. This simple visual tokenizer enables Mono-InternVL to process images of arbitrary resolution with up to 8 millions of pixels, equivalent to 10, 240 image patches, covering most high-resolution scenarios. MONO-INTERNVL-1.5 4 Fig. 2: Monolithic architecture of Mono-InternVL and Mono-InternVL-1.5. Mono-InternVL is designed as multimodal MoE structure, where visual and textual tokens are processed by the corresponding experts. Mono-InternVL-1.5 further integrates the attention experts and the MoE CUDA kernel to facilitate the visual pre-training while retaining the model efficiency. In Mono-InternVL, the textual tokenizer remains unchanged from the original one in the LLM. Given the input text Zn, we obtain textual embedding xt Rnd by xt = Tokenizer(T ). (2) Afterward, the multimodal embedding is constructed by concatenating visual and textual embeddings, denoted as xm Rnd. Multimodal mixture-of-experts structure. The core idea of Mono-InternVL is to embed visual experts into pre-trained LLM. This allows Mono-InternVL not only to facilitate visual pre-training by leveraging the pre-trained LLM knowledge but also to significantly alleviate the catastrophic forgetting problem during pre-training. Specifically, given the multimodal input xm Rnd, decoder-only LLM with set of visual experts is utilized to generate the textual tokens step by step, which can be formulated by (3) ps = Fllm(ysxm, y0:s1; θ, θv). Here, RS and denote the word length and its length, respectively. ps Rm is the next-token probability and is the size of the word vocabulary. Fllm and θ denote the LLM and its pre-trained parameters, respectively. θv refers to the parameters of the patch embedding layer and visual experts. As shown in Fig. 2, Fllm is designed as multimodal mixture-of-experts structure. Specifically, we adopt static routing strategy that assigns visual and textual experts to their corresponding tokens. Therefore, the l-th LLM layer can be defined by xl = xl1 = xl xl + MHA(RMSNorm(xl1 )), + MMoE(RMSNorm(xl m)). Here, MHA() and RMSNorm() denote the multi-head attention [45] and the layer normalization [46], respectively. (4) MMoE() is the proposed multimodal mixture-of-experts, formulated as MMoE(x) = (cid:40) FFNv(x) FFNt(x) if xv, if xt. (5) Here, Rd is the element of xm. FFNv and FFNt denote the visual and textual experts, respectively. In practice, FFNv is initialized from the FFNt to utilize the pre-trained knowledge. As defined in Eq. 4 and 5, the MMoE structure has two distinct advantages over the existing monolithic MLLMs. Firstly, the visual learning of Mono-InternVL can largely benefit from the pre-trained language knowledge, while the language ability can still be preserved by freezing FFNt. Secondly, the MMoE structure significantly improves the models capacity for vision-and-language modeling, and the additional inference cost is almost negligible due to the MoE mechanism. B. Endogenous Visual Pre-training The aim of Endogenous Visual Pre-training (EViP) is to maximize the benefits of Mono-InternVL from visual experts through pre-training on large amount of noisy and synthetic data. Unlike existing methods [9], [12], we formulate EViP from the perspective of delta tuning [14], where most LLM parameters are frozen to preserve the pre-trained knowledge. Therefore, the objective of EViP can be defined as arg min θ L(Fllm(xm; θ, θv), ˆy), (6) where L() and ˆy denote the auto-regressive loss and the ground truth, respectively. As shown in Fig. 3, θ represents the parameters of patch embedding and visual experts in the concept and semantic learning stages, i.e., θv, while in the alignment learning stage, θ also includes the parameters of multi-head attentions. Based on Eq. 6, EViP is designed as MONO-INTERNVL-1. 5 Fig. 3: The training recipe of Mono-InternVL (top) and Mono-InternVL-1.5 (bottom). In the first stage, Mono-InternVL is progressively pre-trained on massive data via three sub-stages (S1.1, S1.2, S1.3), where most parameters of LLM are frozen to preserve the pre-trained knowledge. In the second stage (S2), the entire model is optimized to accommodate various instructions. Compared to Mono-InternVL, Mono-InternVL-1.5 integrates visual attention experts and reduces up to 58% training data. progressive learning process. As illustrated in Fig. 3 and Tab. II, EViP consists of three sub-stages, namely concept learning (S1.1), semantic learning (S1.2) and alignment learning (S1.3). We use carefully partitioned data for each stage to achieve coarse-to-fine visual learning. Concept learning. Concept learning aims to enable the model to learn basic visual concepts, such as object categories or basic shapes. Therefore, we first pre-train Mono-InternVL with about 922 million noisy data sampled from Laion-2B [47] and Coyo-700M [48]. In this sub-stage, Mono-InternVL uses simple prompt for generative learning, i.e., provide onesentence caption for the image. We limit the maximum number of image patches of the visual tokenizer to 1,280 for training efficiency. To preserve the language capabilities while enabling visual specialization, the entire LLM is frozen during concept learning, and only the patch embedding and visual experts are optimized. Semantic learning. After concept learning, Mono-InternVL can understand basic concepts in the image, but it is still challenging to organize this information to generate reasonable descriptions. To achieve higher-level visual understanding, we utilize the pre-trained InternVL2-8B [6] to generate short captions for 258 million images. Compared to the noisy captions in concept learning, synthetic captions provide complex visual knowledge like relationship and world knowledge, etc., while containing less noisy information unrelated to the image, such as the shooting time or the photographer. In this sub-stage, Fig. 4: Illustration of Mono-InternVL-1.5 fused kernel workflow. The left thread blocks handle textual tokens while those on the right handle visual tokens. Although two thread blocks are assigned per data block, nearly half exit immediately upon entry, making the kernel effectively behave as singlebranch implementation. we adopt the same optimization strategy as in concept learning, except that the maximum number of image patches is increased to 1,792. MONO-INTERNVL-1.5 6 TABLE II: Summary of datasets used in the endogenous visual pre-training and instruction finetuning. In S1.2, caption for each image is synthetically produced by the pre-trained InternVL2-8B [6]. Stage Datasets S1.1 Laion-EN (en) [47], COYO (en) [48] S1.2 Laion-EN (en) [47], COYO (en) [48], SAM (en) [49] S1.3 Captioning: Laion-EN (en) [47], Laion-ZH (zh) [47], COYO (zh) [48], GRIT (zh) [50], COCO (en) [51], TextCaps (en) [52] Detection: Objects365 (en&zh) [53], GRIT (en&zh) [50], All-Seeing (en&zh) [54] OCR (large): Wukong-OCR (zh) [55], LaionCOCO-OCR (en) [56], Common Crawl PDF (en&zh) OCR (small): MMC-Inst (en) [57], LSVT (zh) [58], ST-VQA (en) [59], RCTW-17 (zh) [60], ReCTs (zh) [61], ArT (en&zh) [62], SynthDoG (en&zh) [63], COCO-Text (en) [64], ChartQA (en) [65], CTW (zh) [66], DocVQA (en) [67], TextOCR (en) [68], PlotQA (en) [69], InfoVQA (en) [70] S2 Captioning: TextCaps (en) [52], ShareGPT-4o (en&zh) [6] General QA: VQAv2 (en) [71], GQA (en) [72], OKVQA (en) [73], VSR (en) [74], VisualDialog (en) [75] Science: AI2D (en) [76], ScienceQA (en) [77], TQA (en) [78] Chart: ChartQA (en) [65], MMC-Inst (en) [57], DVQA (en) [79], PlotQA (en) [69], LRV-Instruction (en) [80] Mathematics: GeoQA+ (en) [81], TabMWP (en) [82], MathQA (en) [83], CLEVR-Math/Super (en) [84], [85], Geometry3K (en) [86] Knowledge: KVQA (en) [87], A-OKVQA (en) [88], ViQuAE (en) [89], Wikipedia (en&zh) [90] OCR: OCRVQA (en) [91], InfoVQA (en) [70], TextVQA (en) [92], ArT (en&zh) [62], COCO-Text (en) [64], CTW (zh) [66], LSVT (zh) [58], RCTW-17 (zh) [60], ReCTs (zh) [61], SynthDoG (en&zh) [63], ST-VQA (en) [59] Document: DocVQA (en) [67], Common Crawl PDF (en&zh) Grounding: RefCOCO/+/g (en) [93], [94], Visual Genome (en) [95] Conversation: LLaVA-150K (en&zh) [5], LVIS-Instruct4V (en) [96], ALLaVA (en&zh) [97], Laion-GPT4V (en) [98], ShareGPT (en&zh) [99], SVIT (en&zh) [100] Text-only: OpenHermes2.5 (en) [101], Alpaca-GPT4 (en) [102], COIG-CQIA (zh) [103], ShareGPT (en&zh) [99] Video: EgoTaskQA (en) [104], Mementos (en) [105], STAR (en) [106], NTU RGB+D (en) [107], VideoChat2IT (en&zh) [108], LSMDC-QA (en) [109], ShareGPT-4o (en&zh) [6] Handwritten: SROIE (en) [110], FUNSD (en) [111], POIE (en) [112] Alignment learning. To improve the visual capability for downstream tasks, we adopt perform alignment learning on Mono-InternVL. Our alignment data are sampled from the pretraining data of InternVL-1.5 [6], including 143 million samples of image captioning, detection and optical character recognition (OCR), as shown in Tab. II. Specifically, captioning, detection and OCR data account for about 53.9%, 5.2% and 40.9% of the total total, respectively. In this sub-stage, we use the taskspecific prompts from InternVL-1.5 for the generative learning, and increase the maximum number of image patches to 3,328. Compared to previous sub-stages, we additionally unfreeze the multi-head attention layers for better vision-language alignment. C. Instruction Tuning In this stage, we follow InternVL-1.5 [6] to perform supervised learning using around 7 million bilingual instructions, covering various tasks like visual question answering, multimodal dialogue, knowledge, mathematics, etc. In this stage, the entire models are unfreezed, and the maximum number of image patches is increased to 6,400 to handle highresolution images. We list details of instruction data in Tab. II. IV. MONO-INTERNVL-1. A. Improved Endogenous Visual Pre-training Visual attention experts. In S1.1 and 1.2, the learning capability of Mono-InternVL is limited since its attention layers are frozen and initialized with textual knowledge. However, directly fine-tuning the attention parameters will lead to the catastrophic forgetting of textual knowledge. Therefore, to further improve the learning capability of the model, MonoInternVL-1.5 inserts additional visual experts into the multihead attentions (MHA), yielding fully multimodal mixtureof-experts (MMoEs) architecture. In particular, the l-th LLM layer can be rewritten as: )), xl = xl1 = xl xl + MMHA(RMSNorm(xl1 + MMoE(RMSNorm(xl m)). As shown in Fig. 2, the calculation of MMHA() is similar to MHA(), i.e., softmax(a qk)v, but when computing the query, key and value in the attention, visual and textual tokens are assigned with different linear expert layers. For example, given the input features Rld, the computation of the query Rld can be defined as: (7) (cid:40) = Linearv(x) Lineart(x) if xv, if xt. (8) Similarly, we can obtain the key Rld and the value Rld. With this architecture, we find that the training efficiency of Mono-InternVL-1.5 is significantly improved during the pre-training stage. Furthermore, the inference efficiency can also be retained through the MoE mechanism. through our empirical studies, Improved training strategies and data organization. In EViP, concept learning consumes almost billions of data to learn basic visual concepts, leading to relatively expensive expenditure. However, the performance gain of concept learning grows slowly as the data scale increases. On the one hand, only the visual experts are optimized during concept learning, which yields suboptimal learning efficiency. On the other hand, most samples for concept learning are noisy and simple, so it is difficult for MLLM to quickly learn useful information from them. Existing methods [16] also show that small amount of high-quality data can achieve performance comparable to that of large-scale low-quality data. Motivated by the above analysis, we improve the training strategies and data organization of EViP from two aspects, MONO-INTERNVL-1. 7 TABLE III: Comparison with existing MLLMs on general MLLM benchmarks. #A-Param denotes the number of activated parameters. Average scores are computed by normalizing each metric to range between 0 and 100. InternVL-1.5-2B adopts the same LLM and high-quality training data with Mono-InternVL-2B, so we consider it as the modular counterpart. Bold indicates the highest performance among monolithic MLLMs."
        },
        {
            "title": "Model",
            "content": "#A-Param MMB MMVet MMMU MathVista SEED-I OCRBench HallB CCB Avg Modular MLLMs: MobileVLM-V2-3B [113] Mini-Gemini-2B [114] MM1-3B-MoE-Chat [115] DeepSeek-VL-1.3B [116] PaliGemma-3B [117] MiniCPM-V-2 [118] InternVL-1.5-2B [6] Qwen2VL-2B [27] InternVL-2.5-2B [119] Monolithic MLLMs: Fuyu-8B (HD) [8] SOLO [10] Chameleon-7B1 [12] EVE-7B [9] EVE-7B (HD) [9] Emu3 [17] VoRA [120] VoRA-AnyRes [120] Mono-InternVL-2B Mono-InternVL-1.5-2B 3.0B 3.5B 3.5B 2.0B 2.9B 2.8B 2.2B 2.1B 2.2B 8B 7B 7B 7B 7B 8B 7B 7B 1.8B 1.8B 63.2 59.8 70.8 64.6 71.0 69.1 70.9 74.9 74. 10.7 31.1 49.5 52.3 58.5 64.2 61.3 65.5 64.0 31.1 42.2 34.8 33.1 41.0 39.3 49.5 60.8 21.4 8.3 25.6 25.7 37.2 33.7 33.7 40.1 54.0 31.7 38.6 32.2 34.9 38.2 34.6 41.1 43.6 25.4 32.3 32.6 31.6 32.2 32.0 33.7 39.1 29.4 32.6 31.1 28.7 38.7 41.1 43.0 51. 34.4 22.3 25.2 34.2 45.7 42.3 69.4 66.7 69.6 67.1 69.8 64.4 30.6 61.3 64.6 68.2 67.5 68.9 67.4 66.9 409 614 605 654 809 804 7 327 398 687 767 801 37.6 41.9 27.6 29.6 45.0 32.2 45.3 49.5 36.1 63.5 52.7 37.5 41.7 42.6 17.1 21.1 26.4 34.8 32.5 3.5 17.3 12.4 32.5 16.3 36.5 66.3 53.7 65.7 55.6 as shown in Fig. 3. Firstly, we integrate visual attention experts into Mono-InternVL-1.5 and optimize their parameters during visual pre-training. By doing so, the visual and textual modalities can be quickly aligned in multi-head attentions, thereby leading to better training efficiency. Notably, similar to visual experts of Mono-InternVL, the optimization of visual attention experts will not affect the language capabilities. Secondly, we re-organize the training data of EViP based on the principle in the existing literature [16], i.e., less noisy data and more valuable data. Specifically, we reduce the pre-training data from 922 million and 258 million to 250 million and 150 million for S1.1 and S1.2, respectively. Then, the data of S1.3 and instruction tuning is slightly increased to compensate for model performance. With these strategies, the training data of Mono-InternVL-1.5 is reduced by about 58%, while the downstream performance can still be improved. B. Speeding Up Mono-InternVL-1.5 with Fused CUDA Kernel As shown in Fig. 2, Mono-InternVL-1.5 adopts full multimodal MoE architecture, where visual and textual tokens are processed by two different experts, respectively. Unfortunately, 1Chameleon-7B frequently rejects to perform the task with response of cant help you with this\", thus resulting in poor performance. none of the mainstream frameworks or libraries support the deployment of this modality-specific MoE. In paractice, visual and textual tokens must be separated and processed sequentially, which limits GPU parallelism, especially during inference when the amount of data is relatively small. To address this issue, we propose fused CUDA kernel handles both branches jointly, thereby reducing latency and improving GPU utilization. The core idea is based on the observation that, if the input sequence is partitioned into findgrained blocks, the likelihood that both token types co-occur within single block becomes low. As illustrated in Fig. 4, we divide the sequence into smaller blocks and assign two thread blocks to each: one responsible for visual tokens and the other for textual tokens. Upon initialization, each thread block checks for the presence of relevant tokens, and if none are found, it exits immediately. This design ensures that only small portion of blocks require both thread blocks to be active, while the majority can be handled by single thread block, thus closely approaching the efficiency of single-branch computation. MONO-INTERNVL-1. 8 TABLE IV: Comparison with existing MLLMs on visual question answering benchmarks."
        },
        {
            "title": "Model",
            "content": "#A-Param TextVQA SQA-I GQA DocVQA AI2D ChartQA InfoVQA"
        },
        {
            "title": "Avg",
            "content": "Modular MLLMs: MobileVLM-V2-3B [113] Mini-Gemini-2B [114] MM1-3B-MoE-Chat [115] DeepSeek-VL-1.3B [116] PaliGemma-3B [117] MiniCPM-V-2 [118] InternVL-1.5-2B [6] Qwen2VL-2B [27] InternVL-2.5-2B [119] Monolithic MLLMs: Fuyu-8B (HD) [8] SOLO [10] Chameleon-7B1 [12] EVE-7B [9] EVE-7B (HD) [9] Emu3 [17] VoRA [120] VoRA-AnyRes [120] Mono-InternVL-2B Mono-InternVL-1.5-2B 3.0B 3.5B 3.5B 2.0B 2.9B 2.8B 2.2B 2.1B 2.2B 8B 7B 7B 7B 7B 8B 7B 7B 1.8B 1.8B 57.5 56.2 72.9 57.8 68.1 74.1 70.5 79.7 74.3 4.8 51.9 56.8 64.7 56.3 58.7 72.6 73. 70.0 76.1 84.9 73.3 47.2 63.0 64.9 89.2 75.9 72.0 93.6 90.5 66.1 61.6 60.8 62.6 60.3 59.5 59.6 34.2 71.9 85.0 90.1 88.7 1.5 22.0 53.0 76.3 80.0 81. 51.5 68.3 62.9 69.8 74.7 74.9 64.5 61.4 46.0 48.5 61.0 70.0 65.6 61.1 68.6 67.4 74.8 73.5 79.2 2.9 19.5 59.1 68.6 73.7 72.2 55.4 65.5 60.9 5.0 20.0 25.0 43.8 43.0 47. 71.7 17.9 40.8 54.6 67.6 70.1 70.4 V. EXPERIMENTS A. Evaluation Benchmarks include MMBench-EN test We evaluate Mono-InternVL and existing MLLMs on 15 comprehensive multimodal benchmarks and 4 natulanguage processing (NLP) benchmarks. Specifically, ral MLLM benchmarks [121], MMVet [122], MMMU val [123], MathVista testmini [124], SEED-Image [125], OCRBench [126], HallusionBench [127], and CCBench dev [121]. Visual question answering benchmarks include TextVQA val [92], SQA test [77], GQA test-dev [72], DocVQA test [67], AI2D test [76], ChartQA test [65], and InfographicVQA test [70]. NLP benchmarks include MMLU [128], CMMLU [129], AGIEval [130] and MATH [131]. The evaluation metrics follow existing methods [6], [9]. Some results of Chameleon and EVE are evaluated with VLMEvalKit [132] or from the OpenCompass leaderboard [133]. B. Implementation Details We build Mono-InternVL upon InternLM2-1.8B [3] with newly added visual tokenizer and visual experts. For MonoInternVL-1.5, visual attention experts are also added. The visual experts are initialized from pre-trained MLPs in the original InternLM2-1.8B to utulize existing learned representations for improved visual feature extraction. The visual experts account for 1.2 billion parameters. Similarly, the visual attention experts are also initialized from the pre-trained attention weights in InternLM2-1.8B. For both Mono-InternVL and Mono-InternVL1.5, we adopt similar dynamic high-resolution strategy from InternVL-1.5 [6] to align an optimal resolution for input image, which is then patchified to visual tokens. The other configurations are identical to InternLM2-1.8B. For MonoInternVL, the endogenous visual pre-training and instruction tuning take approximately 17 days on 256 NVIDIA A100 GPUs. For Mono-InternVL-1.5, the total training time is reduced to 9.5 days. C. Main Results Comparisons with modular MLLMs. In Tab. IV and III, we compare Mono-InternVL, Mono-InternVL-1.5 and existing MLLMs on 15 multimodal benchmarks. The first observation is that most modular MLLMs outperform existing monolithic MLLMs by significant margins. For example, the average performance of InternVL-1.5-2B [6] on 9 MLLM benchmarks greatly exceeds the SoTA monolithic MLLM, i.e., + 15.5% over EVE-7B (HD) [9]. These results strongly suggest the challenges in existing monolithic MLLMs. In contrast, MonoInternVL-2B with slightly smaller model size can even outperform the modular baseline, i.e., + 0.8% against InternVL1.5-2B on average. Notably, Mono-InternVL-2B demonstrates distinct advantages on MathVista and OCRBench, suggesting its seamless text recognition and reasoning capabilities. Compared to SoTA modular MLLM, i.e., Qwen2VL [27], Mono-InternVL2B is still comparable in most benchmarks, e.g., MathVista. We also observe that Mono-InternVL is still inferior to InternVL1.5 on high-resolution benchmarks, e.g., -12.4% on InfoVQA. This may be because the relatively shallow model depth limits the visual encoding ability of Mono-InternVL, as shown in Fig. 6. MONO-INTERNVL-1. 9 TABLE V: Zero-shot pre-training performance of Mono-InternVL and existing MLLMs. S1.2 and S1.3 denote pre-training stages of semantic learning and alignment learning, respectively. Images of COCO have been seen in MonoInternVL-S1.3, so we mark its performance in gray."
        },
        {
            "title": "Model",
            "content": "#A-Param"
        },
        {
            "title": "COCO Caps",
            "content": "Flickr30k"
        },
        {
            "title": "NoCaps",
            "content": "VQAv2 Flamingo [134] MM1 [135] Chameleon [12] Mono-InternVL-S1.2 Mono-InternVL-S1.3 Mono-InternVL-1.5-S1.2 Mono-InternVL-1.5-S1.3 3B 3.5B 34B 1.8B 1.8B 1.8B 1.8B >2.1B >2.3B >1.4B 0.9B 1.1B 0.4B 0.5B 0 0 2 0 0 0 0 73.0 73.5 120.2 87.3 135.6 91.5 133. 74.7 72.7 77.3 71.6 76.7 55.6 54.1 116.5 53.5 115.3 49.2 46.2 66.0 71.1 - 70.7 TABLE VI: Ablation of different strategies for visual pre-training. All models are pre-trained on 61 million image-text pairs from Laion-2B [47] and fine-tuned on instruction data from LLaVA-665k [20]. Full and Delta denote full tuning and delta tuning, respectively. T-Param refers to trainable parameters."
        },
        {
            "title": "Setting Model",
            "content": "#T-Param Strategy MME-P DocVQA InfoVQA SQA-I GQA ChartQA AI2D (1) (2) (3) (4) InternLM2 (1) + V-Expert (1) + V-Expert (3) + A-Expert 1.8B 3.0B 1.2B 3.0B"
        },
        {
            "title": "Full\nFull\nDelta\nDelta",
            "content": "753 948 995 1000 16.1 18.6 18.9 21.1 11.6 11.9 14.6 15.2 36.7 37.7 56.5 57.8 51.4 53.0 53.4 53.7 10.8 11.1 13.5 12. 27.7 26.6 42.7 43.7 TABLE VII: Ablation of freezing and unfreezing attention in alignment learning. T-Param refers to trainable parameters. All models are pre-trained on 20 millions of data in alignment learning and fine-tuned on LLaVA-665k [20]. Base model is the Mono-InternVL. Methods #T-Param MME-P DocVQA InfoVQA SQA-I GQA ChartQA AI2D Freeze Unfreeze 1.2B 1.8B 1136 39.5 49.3 19.7 22.7 56.5 59.1 61.8 59.9 27.2 49.5 44.1 46.4 TABLE VIII: Ablation of S1.2 vs. Longer training iterations of S1.1. Both models are Mono-InternVL and fine-tuned on LLaVA-665k. Data TextVQA SQA-I GQA DocVQA ChartQA InfoVQA 1024M (S1.1) 49.42 55.97 55.77 29. 922M (S1.1) + 102M (S1.2) 52.90 57.71 57.43 35.32 14.76 18. 17.47 19.17 Comparisons with monolithic MLLMs. Compared to existing monolithic MLLMs, performance gains of MonoInternVL become distinct. For example, compared to EVE-7B (HD) [9], Mono-InternVL achieves up to 15.4% average gains on VQA tasks. Note that EVE-7B requires high-quality data for pre-training, so scaling it with more data remains challenging. Furthermore, compared to Emu3 [17], Mono-InternVL still demonstrates better results on 9 of 12 benchmarks, while using much fewer parameters. Compared to Mono-InternVL, MonoInternVL-1.5 shows comparable or even better performance on multiple benchmarks. As shown in Tab. IV, on OCRrelated benchmarks, Mono-InternVL-1.5 outperforms MonoInternVL by large margins, e.g., +1.7 on DocVQA and +4.9 on InfoVQA. On common MLLM benchmarks, advantages of Mono-InternVL-1.5 are also obvious, e.g., +13.9% on MMVet against Mono-InternVL. Compared to the newly proposed monolithic MLLM called VoRA [120], Mono-InternVL-1.5 TABLE IX: Ablation of combining and separating S1.1 and S1.2. Both models are Mono-InternVL and fine-tuned on LLaVA-665k. Strategy TextVQA SQA-I GQA DocVQA ChartQA InfoVQA Combined (62M) Separated (31M+31M) 38.61 39.75 56.92 54.72 54.98 55.51 22.26 22.29 14.36 15.60 15.10 15. TABLE X: NLP results of shared and unshared (i.e. separated vision and text experts) architectures. We use Mono-InternVL as the base model, which is trained with 60M S1.1 data and then fine-tuned on LLaVA-665k."
        },
        {
            "title": "Architecture\nShared\nUnshared",
            "content": "MMLU 26.17 42.30 CMMLU 25.40 41.11 AGIEval 14.89 31.30 MATH 0.24 1.90 achieves better performance on most benchmarks, e.g., +15.0% on textVQA. Compared to Mono-InternVL, Mono-InternVL1.5 shows comparable or even better performance on multiple benchmarks. As shown in Tab. IV, on OCR-related benchmarks, Mono-InternVL-1.5 outperforms Mono-InternVL by large margins, e.g., +1.7% on DocVQA and +4.9% on InfoVQA. On common MLLM benchmarks, advantages of Mono-InternVL1.5 are also obvious, e.g., +13.9% on MMVet against MonoInternVL. In Tab. XI, we further compare the NLP ability of MonoInternVL with existing monolithic MLLMs. From this table, we notice that Mono-InternVL can well preserve its pre-trained NLP ability, which retains similar performance with InternLM2Chat. However, monolithic MLLMs like EVE, even with larger parameter size, are still inferior to Mono-InternVL in multiple NLP tasks. In addition, we also find that Mono-InternVL-1.5 has slight performance drop on some NLP tasks, but still outperforms EvE by margins. Considering the much cheaper training cost than Mono-InternVL, such performance drop is MONO-INTERNVL-1.5 10 Fig. 5: Ablation studies of EViP and EViP++ with the increase of pre-training data size across three sub-stages: (S1.1) Concept learning; (S1.2) Semantic learning; (S1.3) Alignment learning. For each data point, we fine-tune the corresponding pre-trained model on the instruction data of LLaVA-665k and obtain the downstream performance. still acceptable. These results further confirm the advantages of Mono-InternVL and Mono-InternVL-1.5 against existing monolithic MLLMs. Comparisons of pre-training results. In Tab. V, we further compare the pre-training performance of Mono-InternVL and existing MLLMs. We observe that after concept and semantic learning, Mono-InternVL-S1.2 already exceeds existing modular MLLMs, e.g., +13.8 CIDEr over MM1 [115] on COCO Captions, demonstrating that Mono-InternVL-S1.2 is effective in capturing basic multimodal relationships. When compared with monolithic MLLMs, Mono-InternVL also shows superior performance. For instance, even though Chameleon has much larger model size, it is still inferior to Mono-InternVL-S1.3 by -2.6 CIDEr on Flickr30k [136]. Compared to Mono-InternVL, Mono-InternVL-1.5 demonstrates superior training efficiency. With total of 0.5 billion pre-training data, Mono-InternVL-1.5 reaches very competitive zero-shot performance against MonoInternVL, e.g., 76.7 vs. 77.3 on Flickr30k. It is also worth noting that pre-training in Mono-InternVL-1.5 only consumes about 0.5B image-text pairs, but the cost in MM1 and Flamingo is much more expensive, e.g., more than 2B data. These results further confirm the effectiveness of EViP and EViP++. D. Ablation studies. Cumulative ablations of Mono-InternVL and MonoInternVL-1.5. To validate the design of Mono-InternVL, MONO-INTERNVL-1.5 11 TABLE XI: Comparison of Mono-InternVL, MonoInternVL-1.5 and existing monolithic MLLMs on four common NLP tasks. Except for Chameleon, models are evaluated using opencompass toolkit [133]. TABLE XII: Latency comparison of fused cuda kernel and PyTorch implementation for multimodal MoEs. The results are reported in µs. Linear MoE and MLP MoE is used in self-attentions and feed-forward networks, respectively. Speed is tested on single A100 GPU. Models InternLM2-Chat [3] EVE [9] Chameleon [12] Mono-InternVL Mono-InternVL-1.5 #A-Param MMLU CMMLU AGIEval MATH 46.1 33.4 - 44.0 41.7 1.8B 7B 7B 2B 2B 13.9 0.7 11.5 12.3 15.1 38.8 22.6 - 40.9 38.9 47.1 43.9 52.1 45.1 44. we conduct extensive ablation studies. Specifically, Tab. VI compares different strategies for visual pre-training. The first row is the common strategy used in existing monolithic MLLMs, i.e., full tuning of the LLM, which yields the worst downstream performance in the table. After employing visual experts (the second row), such full-tuning strategy becomes more effective, e.g., +1.6% on GQA. These comparisons well confirm the sub-optimal design of the shared architecture for joint vision and language modeling. We also observe that the delta tuning strategy greatly benefits the visual pretraining, providing +18.8% and 16.1% gains on SQA-I and AI2D, respectively. Compared to full tuning, delta tuning can effectively preserve the knowledge of the pre-trained LLM, which is also crucial for multimodal modeling. Impact of scaling data size in EViP and EViP++. Fig. 5 further demonstrates the relationship between downstream performance and pre-training data size. We can observe that performance of Mono-InternVL will gradually reach an upper bound in the concept learning. Through additional semantic learning and alignment learning, capabilities of Mono-InternVL consistently boost as the data size increases. It is important to note that that the alignment learning plays significant role for VQA tasks, which can provide sufficient task-related knowledge, e.g., OCR knowledge. These results suggest that the low-quality data of S1.1 contribute less to the performance than high-quality ones. Therefore, Mono-InternVL-1.5 adopts new design principle for data organization, i.e., small in quantity but high in quality. As shown in Figure 5, Even with much less noisy data, Mono-InternVL-1.5 can easily achieve similar performance to Mono-InternVL. Furthermore, thanks to the new architecture, the training efficiency of Mono-InternVL-1.5 greatly exceeds that of Mono-InternVL, further reducing the requirement of data size. These results not only demonstrate the data scalability of Mono-InternVL and Mono-InternVL-1.5, but also confirm the coarse-to-fine learning of EViP and the data efficiency of EViP++. Ablations of micro-designs in Mono-InternVL. In Tab. VII, we examine the effects of freezing and unfreezing attention layers in alignment learning. We observe that unfreezing attention results in consistent improvements across all metrics, suggesting that it is crucial to optimize the multi-head attentions in this sub-stage for better vision-language alignment. To validate the effectiveness of synthetic data in S1.2, we compare two models: training S1.1 + S1.2 and training S1.1 only. Both models use the same amount of training data. From Tab. VIII, Method Sequence Length 2K 4K 16K 32K 64K 128K Linear MoE (20484096) Pytorch Fused Kernel Speedup 508 276 3,169 846 1,769 436 1.84 1.94 1.79 6,317 2,745 2.30 12,520 5,408 2.31 49,497 21,317 2.32 MLP MoE (2048 8192 2048) Pytorch Fused Kernel Speedup 15,008 3,968 2,063 1,204 8,614 2,305 1.71 1.72 1.74 28,948 15,821 1.82 59,237 34,301 1.72 117,824 68,064 1.73 we observe that synthetic data helps to improve the performance. In Tab. IX, we examine whether we can merge S1.1 and S1.2 into one stage with small amount of data, and find that separated stages have slight advantages. Finally, we further conduct experiments by removing vision experts and using shared FFN for vision and text, and evaluating its performance on NLP benchmarks. In Tab. X, using shared architecture significantly affects the NLP performance, suggesting that it is necessary to use separate experts to preserve the pre-trained language capability. Ablations of the fused CUDA kernel. In Tab. XII, we compare the latency of fused CUDA kernel and PyTorch implementation. From this table, we observe that our fused CUDA kernel significantly outperforms the PyTorch implementation. In the setting of \"Linear MoE\", the fused CUDA kernels achieve up to 2.32 times speedup against the PyTorch implementation. Similar efficiency can also be observed in the setting of MLP MoE, e.g., up to 1.82 times speedup. As the sequence length increases, the advantages of our fused CUDA kernel are consistent, which greatly confirm its technical contribution. Comparison of inference efficiency. In Tab. XIII, we compare the inference speed of Mono-InternVL, Mono-InternVL1.5 and InternVL-1.5 using the popular deployment library LMDeploy [137]. From this table, we can find that due to the elimination of visual encoder, Mono-InternVL demonstrates superior efficiency under different number of input tokens. In particular, the first-token time is greatly reduced in MonoInternVL, e.g., up to -67% against InternVL-1.5. Benefiting from this, the overall throughput is correspondingly increased by around 31%. Compared to Mono-InternVL, the latency of Mono-InternVL-1.5 is slightly increased due to the additional visual experts in attentions. After equipping with our fused CUDA kernel, we observe significant improvements in inference efficiency, -19% of TTFT against Pytorch implementation. These results not only validate the efficiency of Mono-InternVL and Mono-InternVL-1.5, but also confirm the benefit of our fused CUDA kernel. MONO-INTERNVL-1.5 12 TABLE XIII: Inference speed comparison of InternVL-1.5, Mono-InternVL, and Mono-InternVL-1.5. Models are deployed on an NVIDIA A100 GPU using LMDeploy with Pytorch backend [137]. We use concurrency of 16 and the number of output tokens fixed as 120. TTFT and TPS denotes the time to first token in seconds and throughput in tokens per second, respectively."
        },
        {
            "title": "Model",
            "content": "InternVL-1.5-2B Mono-InternVL-2B Mono-InternVL-1.5-2B Mono-InternVL-1.5-2B + Fused Kernel InternVL-1.5-2B Mono-InternVL-2B Mono-InternVL-1.5-2B Mono-InternVL-1.5-2B + Fused Kernel InternVL-1.5-2B Mono-InternVL-2B Mono-InternVL-1.5-2B Mono-InternVL-1.5-2B + Fused Kernel #Image Tokens 768 768 768 768 1792 1792 1792 1792 3840 3840 3840 3840 #Text Tokens 256 256 256 256 256 256 256 256 256 256 256 256 #Total Input Tokens 1024 1024 1024 1024 2048 2048 2048 2048 4096 4096"
        },
        {
            "title": "TPS",
            "content": "0.242 0.090 0.092 0.083 (-65.7%) 0.453 0.151 0.151 0.139 (-69.3%) 1.938 0.795 0.810 0.659 (-66.0%) 382 436 433 467 (+22.3%) 183 232 221 255 (+39.3%) 52 68 61 77 (+48.1%) Fig. 6: Visualization of attention maps in Mono-InternVL and Mono-InternVL-1.5. The first blue segment, green segment and the second green segment in the axes represent the system prompt tokens (text), image tokens (visual) and user prompt tokens (text), respectively. The numbers on the left side of attention maps indicate the number of tokens. E. Visualizations Attention patterns of Mono-InternVL and MonoInternVL-1.5. To gain in-depth insights into Mono-InternVL and Mono-InternVL-1.5, we visualize its attention maps of different layers in Fig. 6. From Fig. 6, we can draw two noteworthy conclusions. Firstly, despite the global connectivity in the Transformer architecture, we find locality still exists in the visual encoding of shallow layers. As shown in Fig. 6, within the first layer, visual tokens only interact with their nearby content, resulting in patterns that are highly similar to those generated by convolutional neural networks [138]. Second, modalities exhibit little interaction in shallow layers MONO-INTERNVL-1.5 13 but gradually merge as the layers become deeper. The attention weights between visual and textual tokens are extremely low in the first layer and become higher in deeper layers. In Mono-InternVL-1.5, the attention maps demonstrate slightly different pattern. In particular, after using visual experts in attentions, attention weights between modalities become larger. As shown in Fig. 6, language tokens focus more densely on visual tokens, which confirms the advantage of visual experts in visual-language alignment. We hope these examples will provide useful hints for the design of monolithic MLLMs. VI. CONCLUSION In this paper, we propose Mono-InternVL, monolithic MLLM that integrates visual encoding and textual decoding into single LLM. In Mono-InternVL, group of visual experts is embedded into the pre-trained LLM using mixture-of-experts mechanism. By freezing the LLM, Mono-InternVL ensures that visual capabilities are optimized without undermining the pretrained language knowledge. Then, an innovative Endogenous Visual Pre-training (EViP) is introduced to achieve coarse-tofine visual learning of Mono-InternVL. To further improve the efficiency, we present EViP++ and propose cheaper and faster model called Mono-InternVL-1.5. Compared to Mono-InternVL, Mono-InternVL-1.5 benefits from additional visual attention experts, efficient data organization, and the multimodal MoE fused CUDA kernel. With these designs, Mono-InternVL-1.5 reduces the data requirement and inference latency by 58% and 19%, respectively, while reaching better performance. Extensive experiments not only showcase the advantages of each design in Mono-InternVLs, but also verify their effectiveness and efficiency compared to existing MLLMs. Our work significantly pushes the boundaries of monolithic MLLMs, offering new possibilities for the advancement of MLLMs."
        },
        {
            "title": "REFERENCES",
            "content": "[1] OpenAI, GPT-4 technical report, arXiv: 2303.08774, 2023. 1 [2] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and T. Zhu, Qwen technical report, arXiv: 2309.16609, 2023. 1, 3 [3] Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen, Z. Chen, Z. Chen, P. Chu et al., Internlm2 technical report, arXiv preprint arXiv:2403.17297, 2024. 1, 2, 3, 8, 11 [4] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning transferable visual models from natural language supervision, in ICML, vol. 139, 2021, pp. 87488763. 1, 3 [5] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in NeurIPS, 2023. 1, 3, 6 [6] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma et al., How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites, arXiv:2404.16821, 2024. 1, 2, 3, 5, 6, 7, [7] J. Li, D. Li, S. Savarese, and S. C. H. Hoi, BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models, in ICML, vol. 202, 2023, pp. 19 73019 742. 1, 3 [8] R. Bavishi, E. Elsen, C. Hawthorne, M. Nye, A. Odena, A. Somani, and S. Tasırlar, Introducing our multimodal models, 2023. [Online]. Available: https://www.adept.ai/blog/fuyu-8b 1, 3, 7, 8 [9] H. Diao, Y. Cui, X. Li, Y. Wang, H. Lu, and X. Wang, Unveiling encoder-free vision-language models, arXiv preprint arXiv:2406.11832, 2024. 1, 3, 4, 7, 8, 9, 11 [10] Y. Chen, X. Wang, H. Peng, and H. Ji, single transformer for scalable vision-language modeling, arXiv preprint arXiv:2407.06438, 2024. 1, 3, 7, 8 [11] G. Luo, X. Yang, W. Dou, Z. Wang, J. Liu, J. Dai, Y. Qiao, and X. Zhu, Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training, in CVPR, 2025. 1, 2 [12] ChameleonTeam, Chameleon: Mixed-modal early-fusion foundation models, arXiv preprint arXiv:2405.09818, 2024. 1, 3, 4, 7, 8, 9, [13] Y. Zhai, S. Tong, X. Li, M. Cai, Q. Qu, Y. J. Lee, and Y. Ma, Investigating the catastrophic forgetting in multimodal large language models, arXiv preprint arXiv:2309.10313, 2023. 1 [14] N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen, C.-M. Chan, W. Chen et al., Delta tuning: comprehensive study of parameter efficient methods for pre-trained language models, arXiv preprint arXiv:2203.06904, 2022. 1, 4 [15] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, Qwen-vl: frontier large vision-language model with versatile abilities, arXiv preprint arXiv:2308.12966, 2023. 1, 3 [16] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu et al., Lima: Less is more for alignment, Advances in Neural Information Processing Systems, vol. 36, pp. 55 00655 021, 2023. 2, 6, 7 [17] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu, Y. Zhao, Y. Ao, X. Min, T. Li, B. Wu, B. Zhao, B. Zhang, L. Wang, G. Liu, Z. He, X. Yang, J. Liu, Y. Lin, T. Huang, and Z. Wang, Emu3: Next-token prediction is all you need, arXiv: 2409.18869, 2024. 2, 3, 7, 8, 9 [18] Z. Yang, L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, and L. Wang, The dawn of lmms: Preliminary explorations with gpt-4v (ision), arXiv: 2309.17421, vol. 9, 2023. 3 [19] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth et al., Gemini: family of highly capable multimodal models, arXiv: 2312.11805, 2023. [20] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, arXiv: 2310.03744, 2023. 3, 9 [21] G. Luo, Y. Zhou, Y. Zhang, X. Zheng, X. Sun, and R. Ji, Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models, arXiv preprint arXiv:2403.03003, 2024. 3 [22] Z. Wang, X. Zhu, X. Yang, G. Luo, H. Li, C. Tian, W. Dou, J. Ge, L. Lu, Y. Qiao, and J. Dai, Parameter-inverted image pyramid networks for visual perception and multimodal understanding, arXiv preprint arXiv:2501.07783, 2025. 3 [23] H. Li, C. Tian, J. Shao, X. Zhu, Z. Wang, J. Zhu, W. Dou, X. Wang, H. Li, L. Lu et al., Synergen-vl: Towards synergistic image understanding and generation with vision experts and token folding, arXiv preprint arXiv:2412.09604, 2024. 3 [24] J. Li, D. Li, C. Xiong, and S. C. H. Hoi, BLIP: bootstrapping language-image pre-training for unified vision-language understanding and generation, in ICLR, vol. 162, 2022, pp. 12 88812 900. 3 [25] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. C. H. Hoi, Instructblip: Towards general-purpose vision-language models with instruction tuning, in NeurIPS, 2023. 3 [26] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee, Llava-next: Improved reasoning, ocr, and world knowledge, 2024. [Online]. Available: https://llava-vl.github.io/blog/2024-01-30-llava-next/ 3 [27] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge et al., Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, arXiv preprint arXiv:2409.12191, 2024. 3, 7, [28] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang et al., Qwen2. 5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. 3 [29] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, B. Li, P. Luo, T. Lu, Y. Qiao, and J. Dai, Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, arXiv: 2312.14238, 2023. 3 [30] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu et al., Expanding performance boundaries of opensource multimodal models with model, data, and test-time scaling, arXiv preprint arXiv:2412.05271, 2024. 3 [31] W. Wang, Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, J. Zhu, X. Zhu, L. Lu, Y. Qiao et al., Enhancing the reasoning ability of multimodal MONO-INTERNVL-1.5 large language models via mixed preference optimization, arXiv preprint arXiv:2411.10442, 2024. 3 [32] Z. Gao, Z. Chen, E. Cui, Y. Ren, W. Wang, J. Zhu, H. Tian, S. Ye, J. He, X. Zhu et al., Mini-internvl: flexible-transfer pocket multi-modal model with 5% parameters and 90% performance, Visual Intelligence, vol. 2, no. 1, pp. 117, 2024. 3 [33] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, Llama: Open and efficient foundation language models, arXiv: 2302.13971, 2023. 3 [34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., Llama 2: Open foundation and fine-tuned chat models, arXiv: 2307.09288, 2023. 3 [35] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, An image is worth 16x16 words: Transformers for image recognition at scale, in ICLR, 2021. 3 [36] J. Xie, W. Mao, Z. Bai, D. J. Zhang, W. Wang, K. Q. Lin, Y. Gu, Z. Chen, Z. Yang, and M. Z. Shou, Show-o: One single transformer to unify multimodal understanding and generation, arXiv preprint arXiv:2408.12528, 2024. 3 [37] C. Zhou, L. Yu, A. Babu, K. Tirumala, M. Yasunaga, L. Shamis, J. Kahn, X. Ma, L. Zettlemoyer, and O. Levy, Transfusion: Predict the next token and diffuse images with one multi-modal model, arXiv preprint arXiv:2408.11039, 2024. [38] H. Bao, W. Wang, L. Dong, Q. Liu, O. K. Mohammed, K. Aggarwal, S. Som, S. Piao, and F. Wei, Vlmo: Unified vision-language pre-training with mixture-of-modality-experts, Advances in Neural Information Processing Systems, vol. 35, pp. 32 89732 912, 2022. 3 [39] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som, and F. Wei, Image as foreign language: Beit pretraining for all vision and vision-language tasks, arXiv: 2208.10442, 2022. 3 [40] S. Shen, Z. Yao, C. Li, T. Darrell, K. Keutzer, and Y. He, Scaling vision-language models with sparse mixture of experts, arXiv preprint arXiv:2303.07226, 2023. 3 [41] S. E. Yuksel, J. N. Wilson, and P. D. Gader, Twenty years of mixture of experts, IEEE transactions on neural networks and learning systems, vol. 23, no. 8, pp. 11771193, 2012. 3 [42] X. V. Lin, A. Shrivastava, L. Luo, S. Iyer, M. Lewis, G. Gosh, L. Zettlemoyer, and A. Aghajanyan, Moma: Efficient early-fusion pre-training with mixture of modality-aware experts, arXiv preprint arXiv:2407.21770, 2024. 3 [43] D. Raposo, S. Ritter, B. Richards, T. Lillicrap, P. C. Humphreys, and A. Santoro, Mixture-of-depths: Dynamically allocating compute in transformer-based language models, arXiv preprint arXiv:2404.02258, 2024. [44] D. Li, Y. Liu, H. Wu, Y. Wang, Z. Shen, B. Qu, X. Niu, F. Zhou, C. Huang, Y. Li et al., Aria: An open multimodal native mixture-ofexperts model, arXiv preprint arXiv:2410.05993, 2024. 3 [45] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, in NIPS, 2017, pp. 59986008. 4 [46] B. Zhang and R. Sennrich, Root mean square layer normalization, Advances in Neural Information Processing Systems, vol. 32, 2019. 4 [47] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman et al., Laion-5b: An open large-scale dataset for training next generation image-text models, NeurIPS, vol. 35, pp. 25 27825 294, 2022. 5, 6, 9 [48] M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim, Coyo-700m: Image-text pair dataset, https://github.com/kakaobrain/coyo-dataset, 2022. 5, 6 [49] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W. Lo, P. Dollár, and R. B. Girshick, Segment anything, arXiv: 2304.02643, 2023. 6 [50] Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, and F. Wei, Kosmos-2: Grounding multimodal large language models to the world, arXiv preprint arXiv:2306.14824, 2023. 6 [51] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollár, and C. L. Zitnick, Microsoft coco captions: Data collection and evaluation server, arXiv preprint arXiv:1504.00325, 2015. [52] O. Sidorov, R. Hu, M. Rohrbach, and A. Singh, Textcaps: dataset for image captioning with reading comprehension, in ECCV, vol. 12347, 2020, pp. 742758. 6 [53] S. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li, and J. Sun, Objects365: large-scale, high-quality dataset for object detection, in ICCV, 2019, pp. 84308439. 6 [54] W. Wang, M. Shi, Q. Li, W. Wang, Z. Huang, L. Xing, Z. Chen, H. Li, X. Zhu, Z. Cao et al., The all-seeing project: Towards panoptic visual recognition and understanding of the open world, in ICLR, 2024. 6 [55] J. Gu, X. Meng, G. Lu, L. Hou, N. Minzhe, X. Liang, L. Yao, R. Huang, W. Zhang, X. Jiang et al., Wukong: 100 million large-scale chinese cross-modal pre-training benchmark, NeurIPS, vol. 35, pp. 26 418 26 431, 2022. 6 [56] C. Schuhmann, A. Köpf, R. Vencu, T. Coombes, and R. Beaumont, Laion coco: 600m synthetic captions from laion2b-en. https://laion.ai/blog/laion-coco/, 2022. 6 [57] F. Liu, X. Wang, W. Yao, J. Chen, K. Song, S. Cho, Y. Yacoob, and D. Yu, Mmc: Advancing multimodal chart understanding with largescale instruction tuning, arXiv preprint arXiv:2311.10774, 2023. [58] Y. Sun, Z. Ni, C.-K. Chng, Y. Liu, C. Luo, C. C. Ng, J. Han, E. Ding, J. Liu, D. Karatzas et al., Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt, in ICDAR, 2019, pp. 15571562. 6 [59] A. F. Biten, R. Tito, A. Mafla, L. Gomez, M. Rusinol, E. Valveny, C. Jawahar, and D. Karatzas, Scene text visual question answering, in ICCV, 2019, pp. 42914301. 6 [60] B. Shi, C. Yao, M. Liao, M. Yang, P. Xu, L. Cui, S. Belongie, S. Lu, and X. Bai, Icdar2017 competition on reading chinese text in the wild (rctw-17), in ICDAR, vol. 1, 2017, pp. 14291434. 6 [61] R. Zhang, Y. Zhou, Q. Jiang, Q. Song, N. Li, K. Zhou, L. Wang, D. Wang, M. Liao, M. Yang et al., Icdar 2019 robust reading challenge on reading chinese text on signboard, in ICDAR, 2019, pp. 15771581. 6 [62] C. K. Chng, Y. Liu, Y. Sun, C. C. Ng, C. Luo, Z. Ni, C. Fang, S. Zhang, J. Han, E. Ding et al., Icdar2019 robust reading challenge on arbitraryshaped text-rrc-art, in ICDAR, 2019, pp. 15711576. 6 [63] G. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, D. Han, and S. Park, Ocr-free document understanding transformer, in ECCV, 2022. [64] A. Veit, T. Matera, L. Neumann, J. Matas, and S. Belongie, Coco-text: Dataset and benchmark for text detection and recognition in natural images, arXiv preprint arXiv:1601.07140, 2016. 6 [65] A. Masry, X. L. Do, J. Q. Tan, S. Joty, and E. Hoque, Chartqa: benchmark for question answering about charts with visual and logical reasoning, in ACL, 2022, pp. 22632279. 6, 8 [66] T.-L. Yuan, Z. Zhu, K. Xu, C.-J. Li, T.-J. Mu, and S.-M. Hu, large chinese text dataset in the wild, Journal of Computer Science and Technology, vol. 34, pp. 509521, 2019. 6 [67] C. Clark and M. Gardner, Simple and effective multi-paragraph reading comprehension, in ACL, 2018, pp. 845855. 6, 8 [68] A. Singh, G. Pang, M. Toh, J. Huang, W. Galuba, and T. Hassner, Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text, in CVPR, 2021, pp. 88028812. [69] N. Methani, P. Ganguly, M. M. Khapra, and P. Kumar, Plotqa: Reasoning over scientific plots, in WACV, 2020, pp. 15271536. 6 [70] M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. Jawahar, Infographicvqa, in WACV, 2022, pp. 16971706. 6, 8 [71] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, Making the in VQA matter: Elevating the role of image understanding in visual question answering, in CVPR, 2017, pp. 63256334. 6 [72] D. A. Hudson and C. D. Manning, GQA: new dataset for real-world visual reasoning and compositional question answering, in CVPR, 2019, pp. 67006709. 6, 8 [73] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi, Ok-vqa: visual question answering benchmark requiring external knowledge, in CVPR, 2019, pp. 31953204. [74] F. Liu, G. Emerson, and N. Collier, Visual spatial reasoning, TACL, vol. 11, pp. 635651, 2023. 6 [75] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh, and D. Batra, Visual dialog, in CVPR, 2017, pp. 326335. 6 [76] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi, diagram is worth dozen images, in ECCV, 2016, pp. 235251. 6, 8 [77] P. Lu, S. Mishra, T. Xia, L. Qiu, K. Chang, S. Zhu, O. Tafjord, P. Clark, and A. Kalyan, Learn to explain: Multimodal reasoning via thought chains for science question answering, in NeurIPS, 2022. 6, 8 [78] A. Kembhavi, M. Seo, D. Schwenk, J. Choi, A. Farhadi, and H. Hajishirzi, Are you smarter than sixth grader? textbook question answering for multimodal machine comprehension, in CVPR, 2017, pp. 49995007. 6 [79] K. Kafle, B. Price, S. Cohen, and C. Kanan, Dvqa: Understanding data visualizations via question answering, in CVPR, 2018, pp. 56485656. 6 MONO-INTERNVL-1. 15 [80] F. Liu, K. Lin, L. Li, J. Wang, Y. Yacoob, and L. Wang, Aligning large multi-modal model with robust instruction tuning, arXiv preprint arXiv:2306.14565, 2023. 6 [81] J. Cao and J. Xiao, An augmented benchmark dataset for geometric question answering through dual parallel text encoding, in COLING, 2022, pp. 15111520. 6 [82] P. Lu, L. Qiu, K.-W. Chang, Y. N. Wu, S.-C. Zhu, T. Rajpurohit, P. Clark, and A. Kalyan, Dynamic prompt learning via policy gradient for semistructured mathematical reasoning, arXiv preprint arXiv:2209.14610, 2022. 6 [83] L. Yu, W. Jiang, H. Shi, J. Yu, Z. Liu, Y. Zhang, J. T. Kwok, Z. Li, A. Weller, and W. Liu, Metamath: Bootstrap your own mathematical questions for large language models, arXiv preprint arXiv:2309.12284, 2023. 6 [84] A. D. Lindström and S. S. Abraham, Clevr-math: dataset for compositional language, visual and mathematical reasoning, arXiv preprint arXiv:2208.05358, 2022. [85] Z. Li, X. Wang, E. Stengel-Eskin, A. Kortylewski, W. Ma, B. Van Durme, and A. L. Yuille, Super-clevr: virtual benchmark to diagnose domain robustness in visual reasoning, in CVPR, 2023, pp. 14 96314 973. 6 [86] P. Lu, R. Gong, S. Jiang, L. Qiu, S. Huang, X. Liang, and S.-C. Zhu, Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning, arXiv preprint arXiv:2105.04165, 2021. 6 [87] S. Shah, A. Mishra, N. Yadati, and P. P. Talukdar, Kvqa: Knowledgeaware visual question answering, in AAAI, vol. 33, no. 01, 2019, pp. 88768884. 6 [88] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi, A-okvqa: benchmark for visual question answering using world knowledge, in ECCV, 2022, pp. 146162. 6 [89] P. Lerner, O. Ferret, C. Guinaudeau, H. Le Borgne, R. Besançon, J. G. Moreno, and J. Lovón Melgarejo, Viquae, dataset for knowledgebased visual question answering about named entities, in SIGIR, 2022, pp. 31083120. 6 [90] C. He, Z. Jin, C. Xu, J. Qiu, B. Wang, W. Li, H. Yan, J. Wang, and D. Lin, Wanjuan: comprehensive multimodal dataset for advancing english and chinese large models, arXiv preprint arXiv:2308.10755, 2023. 6 [91] A. Mishra, S. Shekhar, A. K. Singh, and A. Chakraborty, Ocr-vqa: Visual question answering by reading text in images, in ICDAR, 2019, pp. 947952. [92] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach, Towards VQA models that can read, in CVPR, 2019. 6, 8 [93] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, Modeling context in referring expressions, in ECCV, vol. 9906, 2016, pp. 6985. 6 [94] J. Mao, J. Huang, A. Toshev, O. Camburu, A. L. Yuille, and K. Murphy, Generation and comprehension of unambiguous object descriptions, in CVPR, 2016, pp. 1120. 6 [95] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei, Visual genome: Connecting language and vision using crowdsourced dense image annotations, IJCV, vol. 123, no. 1, pp. 3273, 2017. 6 [96] J. Wang, L. Meng, Z. Weng, B. He, Z. Wu, and Y.-G. Jiang, To see is to believe: Prompting gpt-4v for better visual instruction tuning, arXiv preprint arXiv:2311.07574, 2023. 6 [97] G. H. Chen, S. Chen, R. Zhang, J. Chen, X. Wu, Z. Zhang, Z. Chen, J. Li, X. Wan, and B. Wang, Allava: Harnessing gpt4v-synthesized data for lite vision-language model, arXiv preprint arXiv:2402.11684, 2024. 6 [98] LAION, https://huggingface.co/datasets/laion/ dataset, Gpt-4v gpt4v-dataset, LAION, 2023. 6 [99] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing et al., Judging llm-as-a-judge with mt-bench and chatbot arena, NeurIPS, vol. 36, 2024. 6 [100] B. Zhao, B. Wu, and T. Huang, SVIT: scaling up visual instruction tuning, arXiv: 2307.04087, 2023. 6 [101] Teknium, Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, https://huggingface.co/datasets/teknium/ OpenHermes-2.5, HuggingFace, 2023. [102] R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, Alpaca: strong, replicable instructionfollowing model, Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, vol. 3, no. 6, p. 7, 2023. 6 [103] Y. Bai, X. Du, Y. Liang, Y. Jin, Z. Liu, J. Zhou, T. Zheng, X. Zhang, N. Ma, Z. Wang et al., Coig-cqia: Quality is all you need for chinese instruction fine-tuning, arXiv preprint arXiv:2403.18058, 2024. 6 [104] B. Jia, T. Lei, S.-C. Zhu, and S. Huang, Egotaskqa: Understanding human tasks in egocentric videos, Advances in Neural Information Processing Systems, vol. 35, pp. 33433360, 2022. 6 [105] X. Wang, Y. Zhou, X. Liu, H. Lu, Y. Xu, F. He, J. Yoon, T. Lu, G. Bertasius, M. Bansal et al., Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences, arXiv preprint arXiv:2401.10529, 2024. 6 [106] B. Wu, S. Yu, Z. Chen, J. B. Tenenbaum, and C. Gan, Star: benchmark for situated reasoning in real-world videos, in Thirty-fifth Conference on Neural Information Processing Systems (NeurIPS), 2021. 6 [107] A. Shahroudy, J. Liu, T.-T. Ng, and G. Wang, Ntu rgb+ d: large scale dataset for 3d human activity analysis, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 10101019. [108] K. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, and Y. Qiao, Videochat: Chat-centric video understanding, arXiv preprint arXiv:2305.06355, 2023. 6 [109] A. Rohrbach, A. Torabi, M. Rohrbach, N. Tandon, C. Pal, H. Larochelle, A. Courville, and B. Schiele, Movie description, International [Online]. Available: http://link.springer.com/article/10.1007/s11263-016-0987-1? wt_mc=Internal.Event.1.SEM.ArticleAuthorOnlineFirst 6 of Computer Vision, Journal 2017. [110] Z. Huang, K. Chen, J. He, X. Bai, D. Karatzas, S. Lu, and C. Jawahar, Icdar2019 competition on scanned receipt ocr and information extraction, in 2019 International Conference on Document Analysis and Recognition (ICDAR). IEEE, 2019, pp. 15161520. 6 [111] J.-P. T. Guillaume Jaume, Hazim Kemal Ekenel, Funsd: dataset for form understanding in noisy scanned documents, in Accepted to ICDAR-OST, 2019. 6 [112] J. Kuang, W. Hua, D. Liang, M. Yang, D. Jiang, B. Ren, and X. Bai, Visual information extraction in the wild: practical dataset and end-toend solution, in International Conference on Document Analysis and Recognition. Springer, 2023, pp. 3653. 6 [113] X. Chu, L. Qiao, X. Zhang, S. Xu, F. Wei, Y. Yang, X. Sun, Y. Hu, X. Lin, B. Zhang et al., Mobilevlm v2: Faster and stronger baseline for vision language model, arXiv preprint arXiv:2402.03766, 2024. 7, 8 [114] Y. Li, Y. Zhang, C. Wang, Z. Zhong, Y. Chen, R. Chu, S. Liu, and J. Jia, Mini-gemini: Mining the potential of multi-modality vision language models, arXiv: 2403.18814, 2024. 7, 8 [115] B. McKinzie, Z. Gan, J. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng, F. Weers, A. Belyi, H. Zhang, K. Singh, D. Kang, A. Jain, H. Hè, M. Schwarzer, T. Gunter, X. Kong, A. Zhang, J. Wang, C. Wang, N. Du, T. Lei, S. Wiseman, G. Yin, M. Lee, Z. Wang, R. Pang, P. Grasch, A. Toshev, and Y. Yang, MM1: methods, analysis & insights from multimodal LLM pre-training, arXiv: 2403.09611, 2024. 7, 8, [116] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun et al., Deepseek-vl: Towards real-world vision-language understanding, arXiv preprint arXiv:2403.05525, 2024. 7, 8 [117] L. Beyer, A. Steiner, A. S. Pinto, A. Kolesnikov, X. Wang, D. Salz, M. Neumann, I. Alabdulmohsin, M. Tschannen, E. Bugliarello et al., Paligemma: versatile 3b vlm for transfer, arXiv preprint arXiv:2407.07726, 2024. 7, 8 [118] Y. Yao, T. Yu, A. Zhang, C. Wang, J. Cui, H. Zhu, T. Cai, H. Li, W. Zhao, Z. He et al., Minicpm-v: gpt-4v level mllm on your phone, arXiv preprint arXiv:2408.01800, 2024. 7, 8 [119] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu et al., Expanding performance boundaries of opensource multimodal models with model, data, and test-time scaling, arXiv preprint arXiv:2412.05271, 2024. 7, 8 [120] H. Wang, Y. Ye, B. Li, Y. Nie, J. Lu, J. Tang, Y. Wang, and C. Huang, Vision as lora, arXiv preprint arXiv:2503.20680, 2025. 7, 8, 9 [121] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, K. Chen, and D. Lin, Mmbench: Is your multi-modal model an all-around player? arXiv: 2307.06281, 2023. 8 [122] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang, Mm-vet: Evaluating large multimodal models for integrated capabilities, arXiv: 2308.02490, 2023. [123] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun et al., Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, arXiv: 2311.16502, 2023. 8 MONO-INTERNVL-1.5 16 [124] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao, Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, arXiv: 2310.02255, 2023. 8 [125] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan, Seed-bench: Benchmarking multimodal llms with generative comprehension, arXiv: 2307.16125, 2023. 8 [126] Y. Liu, Z. Li, H. Li, W. Yu, M. Huang, D. Peng, M. Liu, M. Chen, C. Li, L. Jin et al., On the hidden mystery of ocr in large multimodal models, arXiv preprint arXiv:2305.07895, 2023. [127] T. Guan, F. Liu, X. Wu, R. Xian, Z. Li, X. Liu, X. Wang, L. Chen, F. Huang, Y. Yacoob et al., Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models, arXiv: 2310.14566, 2023. 8 [128] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, Measuring massive multitask language understanding, arXiv preprint arXiv:2009.03300, 2020. 8 [129] H. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin, Cmmlu: Measuring massive multitask language understanding in chinese, arXiv preprint arXiv:2306.09212, 2023. 8 [130] W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan, Agieval: human-centric benchmark for evaluating foundation models, arXiv preprint arXiv:2304.06364, 2023. 8 [131] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt, Measuring mathematical problem solving with the math dataset, arXiv preprint arXiv:2103.03874, 2021. 8 [132] H. Duan, J. Yang, Y. Qiao, X. Fang, L. Chen, Y. Liu, X. Dong, Y. Zang, P. Zhang, J. Wang, D. Lin, and K. Chen, Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, 2024. [Online]. Available: https://arxiv.org/abs/2407.11691 8 [133] Contributors, Opencompass: universal evaluation platform for foundation models, https://github.com/open-compass/opencompass, 2023. 8, 11 [134] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds et al., Flamingo: visual language model for few-shot learning, NeurIPS, vol. 35, pp. 23 716 23 736, 2022. 9 [135] B. McKinzie, Z. Gan, J.-P. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng, F. Weers et al., Mm1: Methods, analysis & insights from multimodal llm pre-training, arXiv preprint arXiv:2403.09611, 2024. [136] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions, TACL, vol. 2, pp. 6778, 2014. 10 [137] LMDeployContributors, Lmdeploy: toolkit for compressing, deploying, and serving llm, https://github.com/InternLM/lmdeploy, 2023. 11, 12 [138] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in CVPR, 2016, pp. 770778."
        }
    ],
    "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}