{
    "paper_title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images",
    "authors": [
        "Boammani Aser Lompo",
        "Marc Haraoui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual reasoning over structured data such as tables is a critical capability for modern vision-language models (VLMs), yet current benchmarks remain limited in scale, diversity, or reasoning depth, especially when it comes to rendered table images. Addressing this gap, we introduce Visual-TableQA, a large-scale, open-domain multimodal dataset specifically designed to evaluate and enhance visual reasoning over complex tabular data. Our generation pipeline is modular, scalable, and fully autonomous, involving multiple reasoning LLMs collaborating across distinct roles: generation, validation, and inspiration. Visual-TableQA comprises 2.5k richly structured LaTeX-rendered tables and 6k reasoning-intensive QA pairs, all produced at a cost of under USD 100. To promote diversity and creativity, our pipeline performs multi-model collaborative data generation via cross-model prompting ('inspiration') and LLM-jury filtering. Stronger models seed layouts and topics that weaker models elaborate, collectively distilling diverse reasoning patterns and visual structures into the dataset. Empirical results show that models fine-tuned on Visual-TableQA generalize robustly to external benchmarks, outperforming several proprietary models despite the dataset's synthetic nature. The full pipeline and resources are publicly available at https://github.com/AI-4-Everyone/Visual-TableQA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 6 6 9 7 0 . 9 0 5 2 : r Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images Boammani Aser Lompo École de Technologie Supérieure Montreal, Canada boammani.lompo.1@ens.etsmtl.ca Marc Haraoui mharaoui.pro@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Visual reasoning over structured data such as tables is critical capability for modern vision-language models (VLMs), yet current benchmarks remain limited in scale, diversity, or reasoning depth, especially when it comes to rendered table images. Addressing this gap, we introduce Visual-TableQA, large-scale, opendomain multimodal dataset specifically designed to evaluate and enhance visual reasoning over complex tabular data. Our generation pipeline is modular, scalable, and fully autonomous, involving multiple reasoning LLMs collaborating across distinct roles: generation, validation, and inspiration. Visual-TableQA comprises 2.5k richly structured LaTeX-rendered tables and 6k reasoning-intensive QA pairs, all produced at cost of under $100. To promote diversity and creativity, our pipeline performs multi-model collaborative data generation via cross-model prompting (inspiration) and LLM-jury filtering. Stronger models seed layouts and topics that weaker models elaborate, collectively distilling diverse reasoning patterns and visual structures into the dataset. Empirical results show that models fine-tuned on Visual-TableQA generalize robustly to external benchmarks, outperforming several proprietary models despite the datasets synthetic nature. The full pipeline and resources are publicly available in our GitHub repository."
        },
        {
            "title": "Introduction",
            "content": "Vision-language models (VLMs) have significantly advanced in recent years, achieving remarkable performance in various tasks involving visual and textual inputs. Despite these advancements, complex reasoning tasks, especially those requiring deep comprehension of tabular data structures, continue to pose significant challenges. Table complexity can manifest in various ways, including structural layout, information density, and the diversity of visual components such as the integration of diagrams. The more complex table is, the more it lends itself to challenging reasoning tasks, requiring advanced cognitive abilities to extract relevant information and perform multi-step logical analysis. For example, the table in Figure 1 exemplifies this complexity through its use of multirow cells, integrated diagrams, and color encoding. Answering the question requires the VLM to interpret information across all cells and perform sequence of reasoning steps. Existing table-based QA datasets predominantly fall into two categories: (i) those represented purely in textual formatsuch as WikiTableQuestions Pasupat & Liang (2015), HybridQA Chen et al. (2020b), and AIT-QA Katsis et al. (2022)which bypass the challenges of visual layout interpretation; and (ii) those that lack diversity in visual layouts, visual complexity, and reasoning depth due to being domain-specific (e.g., TAT-DQA Zhu et al. (2022)), or having standardized queries (e.g., TableVQABench Kim et al. (2024)), or highly technical in nature (e.g., Table-VQA Tom Agonnoude (2024)). Equal contribution, random ordering. Equal contribution, random ordering. Preprint. Figure 1: Sample question in our benchmark This second datasets category typically rely on limited set of layout templates and involve relatively simple visual tasks or basic QA scenarios, falling short of the complexity required for thorough evaluation and advancement of reasoning capabilities. More recent effortssuch as ChartQA Masry et al., ReachQA He et al., and MATH-Vision Wang et al. (2024b)have aimed to address the need for open-domain coverage, incorporating more diverse visual features, varied question types, and deeper reasoning challenges. However, these datasets primarily focus on charts and function plots, overlooking tablesand with them, an entire dimension of informational structure and layout diversity. An extensive comparison of diverse chart and table datasets is provided in Table 1. Inspired by ReachQAs Code-as-Intermediary Translation (CIT)a technique that translates chart images into textual representations while faithfully preserving visual featureswe introduce VisualTableQA, novel synthetic, multimodal, and open-domain dataset tailored to enhance reasoning capabilities through complex table-based question-answering tasks. Visual-TableQA capitalizes on the ability of reasoning-oriented LLMs to generate intricate LaTeX tables, thus significantly reducing costs and eliminating the need for extensive manual annotations. Visual-TableQA emphasizes structural reasoning over domain knowledge. Each entry couples rendered table image with complex, visually grounded reasoning task. Tasks require interpreting visual layout cues such as cell alignment, hierarchical headers, merged cells, or embedded symbolic contentemulating realworld documents where visual context is essential for correct interpretation. The dataset contains 2.5k reasoning-intensive tables and 6k QA pairs crafted to assess both information extraction and multi-step reasoning capabilities, all generated at cost of under $100. The entire dataset has been validated using committee of high-performing reasoning LLMs, the ROSCOE step by step reasoning score Golovneva et al., and sample of 800 QA pairs has undergone manual verification by human annotators. In contrast to previous synthetic datasets, Visual-TableQA is less guided in its generation process, allowing for more diversity and creativity in both table complexity (e.g., structural layout, information density, visual component variety) and the design of QA pairs explicitly crafted to challenge visual reasoning skills. We evaluated broad range of VLMs, from lightweight models to state-of-the-art architectures, and benchmarked their performance against existing datasets. The results show that most VLMs continue to struggle with table understanding. In sum, our main contributions are:(i) high-quality, visually diverse, and open-domain dataset for table-based reasoning; (ii) an LLM-driven, low-cost generation pipeline using cross-model inspiration; (iii) an empirical analysis comparing Visual-TableQA to existing table and chart datasets; (iv) an extensive evaluation of open and proprietary VLMs, showing performance gains after finetuning. Our dataset and code are publicly available at https://github.com/AI-4-Everyone/ Visual-TableQA. 2 Table 1: Comparison of existing chart and table datasets across data, Q&A, and dataset properties. Abbreviations: Repr=Representation, Vis= Visual, Comp= Complexity, Temp = Template, Refer = Reference, Rat = Rational, Synth= Synthetic, Scal = Scalable. Cells marked with indicate mixed attributes (e.g., partially template-based; scalable Q&A but non-scalable chart data) Data Properties Dataset Properties Q&A Properties Type Data Repr. Vis. Comp. Temp. Free Vis. Refer. Rat. Annot. Synth. #Samples / #QA Scal. Datasets WikiTableQuestions (Pasupat & Liang, 2015) HybridQA (Chen et al., 2020b) AIT-QA (Katsis et al., 2022) TAT-DQA (Zhu et al., 2022) Table-VQA (Tom Agonnoude, 2024) TableVQA-Bench (Kim et al., 2024) ChartQA (Masry et al.) DocVQA (Mathew et al., 2020) MultiModalQA (Talmor et al., 2021) MATH-Vision (Wang et al., 2024b) REACHQA (He et al.) # Layouts/ # Topics /1 /1 / Table Text Table Text Table Text Image Image Table 11/ Table Image 3/15 20/5 16/ /16 Chart Image Image Image Image 32/ Chart Image Visual-TableQA (ours) / Table Image"
        },
        {
            "title": "2 Visual-TableQA Dataset",
            "content": "2.1k/22k 13k/70k 116/515 2.5k/16.5k 16.4k/82.3k 894/1.5k 21.9k/32.7k 12.7k/50k 29,918 3k/3k 3.7k/22k 2.3k/ 6k Unlike previous datasets that rely heavily on textual input or handcrafted annotations, Visual-TableQA leverages scalable generation pipeline rooted in LaTeX-rendered table images, automated reasoning task creation, and LLM-based evaluation. This strategy enables high diversity and reasoning depth while keeping annotation costs minimal, totaling under $100 using combination of open-access APIs and limited usage tiers. In this section, we describe our LaTeX-based table encoding 2.1, the data generation pipeline 2.2, and the quality assurance process 2.3."
        },
        {
            "title": "2.1 Table Representation in LaTeX",
            "content": "Our approach is inspired by He et al., which demonstrated that state-of-the-art VLMs can reason about visual content even in the absence of explicit visual input. Building on this insight, and leveraging the strong coding capabilities of reasoning-oriented language models across multiple programming languages, we chose to use an intermediate representation of tables in LaTeX rather than directly generating rendered table images. This strategy enables the generation of complex visual tables as compact LaTeX codetypically around 100 lines per tabledrastically reducing the cost of generation by minimizing the number of output tokens required in API calls. Our observations  (Table 2)  align with those of Kale & Nadadur (2025), who reported that LLMs struggle with LaTeX generationparticularly as task complexity increases, leading to notable drop in accuracy. Table 2 presents the performance of various models in generating LaTeX tables that compile without errors."
        },
        {
            "title": "2.2 Data Generation Pipeline",
            "content": "This section provides detailed description of the generation pipeline. Figure 2 gives an overview of the whole process. Seed Tables and Topics Collection: The first step involves collecting diverse set of table layouts to serve as inspiration for LLMs during the generation process. We explored various sources, including scientific journals, financial report databases, online newspapers, and table design galleries. Our search included both table and diagram images to introduce greater visual and structural complexity into the dataset. We selected 20 representative images (Figure 6a) and passed them to visual language model, VLM-0 (GPT-o3 OpenAI (2025)), to generate accurate LaTeX representations. In parallel, we used LLM-0 (GPT-4o) OpenAI (2024) to generate list of 5,000 distinct topic prompts. These initial table samples and topics serve as the first layer of inspiration for subsequent LLM 3 Table 2: Percentage of successful LaTeX compilations for various models. Each accuracy is computed from at least 500 generated samples. The Adjust column indicates the level of manual correction needed to make the table look good: Low means minimal or no adjustments, Medium corresponds to 35 required fixes, and High indicates more than 5 adjustments were necessary. The tables generated by DeepSeek-R1-Distill-Qwen-32B never compiled. Model Acc. (%) Adjust Model Llama 4 Maverick 17B-128E Instruct Meta AI (2025) Gemini 2.0 Flash Google (2025a) 69 High 65. Low Gemini 2.5 Flash Google (2025b) Gemini 2.5 Pro Google (2025c) GPT-4.1 OpenAI (2025a) Qwen3-30B-A3B Qwen Team (2025a) Qwen-QwQ-32B Qwen Team (2025c) 43 19.6 41.5 69.4 38.2 Medium Low Low Low Low DeepSeek-R1-Distill-Qwen-32B DeepSeekAI (2025) DeepSeek-R1T-Chimera TNG Technology Consulting GmbH (2025) Claude Sonnet 4 Anthropic (2025) Claude 3.5 Haiku Anthropic (2024) Grok 3 Beta xAI (2025) Reka Flash 3 Reka AI (2025) Acc. (%) 0.0 Adjust 43.4 Medium 56 64.4 47.3 19. Low Low Low Medium Figure 2: Overview of the full pipeline architecture of Visual-TableQA. subset of initial table images is first converted to LaTeX using visual language model (VLM-0). The resulting LaTeX code, along with topic prompts, is then passed to language model (LLM-1) to generate new, diverse tables. These newly generated tables are used as inputs for further iterations of table generation. All generated tables are then submitted to second language model (LLM-2), which produces corresponding question-answer pairs. Finally, the QA pairs are evaluated by jury of high-performing LLMs, and their quality is assessed using the ROSCOE score. generationsthough the pool of inspirations expands automatically, as detailed in Section 2.2. For reproducibility, all resources are publicly available in our GitHub repository. Table Generation: For each iteration, we randomly select an LLM-1 from the models short-list presented in Table 2. The model receives one table sample from our pool and three topics randomly selected from the topic list, all delivered through single instruction prompt. The output from LLM-1 is returned as JSON file containing three newly generated LaTeX-formatted tables in plain text, each corresponding to one of the provided topics. We require that the generated tables be inspired by the input table but include substantial layout variations and, when appropriate, additional data to enhance complexity. The resulting LaTeX code is then compiled using standard LaTeX compilation stack (pdflatex + pdf2image), and cropped to produce high-resolution table images. human reviewer then inspects the table and makes adjustments to the LaTeX code if necessary. The prompt used for generation are provided in Figure 7. Evolving Layouts through Iterations: subset of the generated tables is manually selected to enrich the pool of table inspirations. This feedback loop encourages the emergence of increasingly complex and diverse layouts by amplifying visual variations and enabling cross-model inspiration across different LLM-1s over successive iterations. This process is facilitated by the fact that LLMs differ in architecture and tend to focus on distinct structural and stylistic aspects of tables. As result, combining inspirations across models leads to highly diversified and creative layout types. We refer to this phenomenon as cross-model prompting (inspiration). QA Generation: Next, for each generated table, we randomly select model, denoted LLM-2, from the same list of models in Table 2 to generate three QA pairs. The model receives the table in LaTeX 4 format and is instructed to produce questions that require multi-step reasoning, pattern recognition, and symbolic interpretation. For instance, the sample in Figure 1 illustrates how the questions extend beyond basic information extraction, requiring interpretative reasoning to identify patterns within the presented data. We do not fact-check the generated tables; as result, some table content may be non-factual. While this is important to consider when using the dataset for training, it can be beneficial, as it encourages models to rely on reasoning rather than prior knowledge."
        },
        {
            "title": "2.3 Quality Assurance",
            "content": "To ensure the validity of the tables and QA pairs, panel of independent LLMsserving as reasoning juryevaluates each table and its associated QA pairs by providing binary correctness judgments. The evaluation is based on four criteria: (i) the generated document is valid table and is relevant to the given topic; (ii) the table and any associated figures are coherent and meaningful; (iii) the question is fully grounded in the table, requiring no external knowledge; and (iv) the answer is completely supported by the table content. If any of these four criteria are not met, the corresponding table and its QA pairs are discarded. The LLM jury includes Qwen3-32B Qwen Team (2025b) , DeepSeek-R1-Distill-LLaMA-70B, Gemini-2.5-pro, GPT-4.1, and DeepSeek-Prover-v2 Ren et al. (2025)models chosen for their strong reasoning abilities. Final acceptance is determined via majority vote across the jury. The prompt used is provided in Figure 9. The next step involved computing the ROSCOE reasoning scores as introduced in Golovneva et al.. These metrics assess the coherence, logical soundness, and contextual grounding of step-by-step generated rationales. The ROSCOE framework encompasses thirteen evaluation criteria, which we report in Table 7 along with their corresponding values computed over our dataset. The results indicate near-perfect alignment with the expected directionality of each metric, supporting the overall quality of the generated reasoning chains. Test Set Construction and Human Evaluation: The dataset was divided into three subsets: training, validation, and testing. To prevent data leakage, all entries {table, question, answer} derived from single table were assigned to the same subset. The testing set was also used for human evaluation. Two human annotatorseach holding at least Masters degree and with prior experience in data annotationwere hired to evaluate the quality of 800 QA pairs. Each QA pair was assessed for validity and rated on scale from 1 to 5. Overall, 92% of the evaluated QA pairs received rating of at least 4 stars from both annotators."
        },
        {
            "title": "3.1 Benchmark Comparison",
            "content": "Evaluated Benchmarks and Model Selection: We evaluate range of state-of-the-art reasoning VLMs on Visual-TableQA and compare their performance across three other benchmarks focused on table and chart-based visual question answering: ChartQA Masry et al., ReachQA He et al., and MATH-Vision Wang et al. (2024b). Our model selection includes powerful proprietary models such as GPT-4o, GPT-4o Mini OpenAI (2025b), Gemini 2.5 Flash, Gemini 2.5 Pro, and Claude 3.5 Sonnet, as well as open-source models like LLaMA 4 Maverick 17B-128E Instruct, Mistral Small 3.1 24B Instruct Mistral AI (2025), Qwen2.5-VL-32B-Instruct Chen et al. (2024a), Qwen2.5-VL-7B-Instruct Team (2025), LLaVA-Next-Llama3-8B Li et al. (2024), MiniCPM-V2.5-Llama3 Yao et al. (2024), and InternVL2-8B Chen et al. (2024b). Where performance metrics were available, we did not re-evaluate models on these datasets; instead, we report the results published in the original papers, official leaderboards, or model cards. For all other cases, we carefully fine-tuned and evaluated the models following the instructions provided in their respective official GitHub repositories. Evaluation Protocol: All models are evaluated on the test sets of the four selected datasets. Each model receives image-question pairs, formatted within unified prompt that includes system message tailored to elicit the models reasoning capabilities (Section F). For the Visual-TableQA dataset, we additionally construct variant in which data is provided not as rendered images but in LaTeX code format. This textual-code version is referred to as Visual-TableQA-CIT. 5 MiniCPM-V2.5-Llama3, LLaVA-Next-Llama3-8B, and For Qwen2.5-VL-7B-Instruct, we conducted two supervised fine-tuning (SFT) experiments: (i) using the ReachQA training split (denoted as Model_Name + ReachQA) and (ii) using the Visual-TableQA training split (denoted as Model_Name + Visual-TableQA). We applied LowRank Adapters (LoRA) Hu et al. to all linear layers, following the SFT setup and hyperparameters described in the He et al. GitHub repository when possible (Section H). The fine-tuning phase for all models was limited to one epoch to ensure consistency and reduce overfitting. InternVL2-8B, All models are allocated maximum of 5,000 tokens during inference to accommodate extended chain-of-thought reasoning. Model responses are evaluated using the same jury of high-performing VLMs and majority-vote protocol as described in Section 2.3. The jury confidence score, computed as the ratio of the highest vote count to the total jury size, averages above 0.87 for all models and all datasets. In addition, evaluations are run twice, to ensure reproducibility."
        },
        {
            "title": "3.2 Experimentation Results",
            "content": "The average models accuracies are displayed in Table 3. These results reveal that: Visual-TableQA Effectively Evaluates Visual Reasoning Capabilities: Model performances on Figure 3: Correlation of model rankings on Visual-TableQA with those on three established datasetsChartQA (recognition-focused), ReachQA (balanced), and MATH-Vision (reasoningfocused)using Spearmans ρ and Kendalls τ metrics. Higher values indicate stronger alignment in model performance trends. Visual-TableQA shows strong correlation with ReachQA, suggesting it effectively balances both visual recognition and reasoning, while its weaker correlation with ChartQA and MATH-Vision highlights its unique position as comprehensive visual reasoning benchmark. Visual-TableQA follow similar trends to those observed on real-world, human-annotated datasets such as ChartQA and MATH-Vision, suggesting that synthetic datasets can effectively evaluate reasoning capabilities. direct comparison between Visual-TableQA and its textual variant, VisualTableQA-CIT, shows notable performance gap: on average, models perform +6.26% better on Visual-TableQA-CIT. This highlights the added challenge posed by the image-based format in Visual-TableQA, demonstrating its effectiveness at testing visual reasoning over purely textual input. To further validate Visual-TableQA as reasoning benchmark, we compared model rankings across datasets. For each dataset, we extracted the models (except the fine-tuned ones) performance rankings and compared them to the rankings on Visual-TableQA using two correlation measures: (i) Spearmans ρ Lee Rodgers & Nicewander (1988): Captures monotonic consistency in rankings (regardless of exact scores); (ii) Kendalls τ Kendall (1948): Measures the fraction of concordant vs. discordant ranking pairs and is more robust to ties. Both metrics range from 1 to 1, with values closer to 1 indicating strong alignment in model rankings. To ensure fairness, we adjusted all scores computed with Relaxed Accuracy by subtracting 5%, before comparison. The results are shown in Figure 3. Each dataset varies in how much it emphasizes visual recognition versus reasoning: (i) ChartQA Recognition-heavy, (ii) ReachQA Balanced, (iii) MATH-Vision Reasoning-heavy Interestingly, Visual-TableQA rankings align most closely with ReachQA, but not with ChartQA or MATH-Vision individually. This suggests that Visual-TableQA does not favor models that excel 6 solely at recognition or solely at reasoning. Instead, it rewards models capable of bothmaking it comprehensive benchmark for evaluating all aspects of visual reasoning. Visual-TableQA Effectively Transfers to Other Benchmarks: To assess the transferability of Visual-TableQA, we investigated how fine-tuning on Visual-TableQA impacts performance across other benchmarks. As shown in Table 3, supervision from Visual-TableQA led to significant generalization beyond its native domain. Notably, it improved the accuracy of Qwen2.5-VL-7B-Instruct on ReachQA from 49.23% to 60.95%, and on MATH-Vision from 25.10% to 49.77%, despite these datasets not being explicitly table-focused. This finding is further supported by Table 4, which reports similar gains in generalization across three additional models: LLaVA-Next-Llama3-8B, MiniCPM-V2.5-Llama3, and InternVL2-8B. However, this transferability is not reciprocal. Fine-tuning Qwen2.5-VL-7B-Instruct on ReachQA alone yields only modest in-domain gains (49.23% 55.75%) and leads to reduced performance on both ChartQA and Visual-TableQA. This suggests that Visual-TableQA provides more generalizable reasoning signalrooted in layout understanding, symbolic interpretation, and multi-step reasoningcompared to standard benchmarks. Proprietary Models Outperform Open-Source Models on Average: Claude 3.5 Sonnet achieves the highest performance across nearly all benchmarks. However, fine-tuning on VisualTableQA substantially narrows the gap between proprietary and open-source models. Notably, the performance of Qwen2.5-VL-7B-Instruct increases significantly across all evaluated benchmarkssurpassing several state-of-the-art proprietary models, including GPT-4o, GPT-4o-mini, and Gemini 2.5 Pro. Table 3: Model performance on the test sets of four benchmarks: ChartQA, ReachQA, MATH-Vision, Visual-TableQA, and Visual-TableQA-CIT. Visual-TableQA-CIT is the variant of our dataset where tables are represented in LaTeX code form rather than as rendered images. The ReachQA score is reported as the average across its two evaluation splits: Reasoning and Recognition. The values in blue are from our own evaluation using the LLM jury, while the remaining values are taken from model authors or official leaderboards/model cards. When fine-tuned model achieves better performance, the result is annotated with ; if the performance worsens, it is marked with . The best performance for each model variants and task is in bold."
        },
        {
            "title": "Models",
            "content": "Human GPT-4o GPT-4o mini Gemini 2.5 Flash Gemini 2.5 Pro Claude 3.5 Sonnet Llama 4 Maverick 17B-128E Instruct Mistral Small 3.1 24B Instruct Qwen2.5-VL-32B-Instruct Qwen2.5-VL-7B-Instruct"
        },
        {
            "title": "ReachQA",
            "content": "MATHVisionF LL VisualTableQA VisualTableQA-CIT 85.7 77.52 77.68 85.73 90.8 85.3 86.24 83.29 87."
        },
        {
            "title": "Baseline",
            "content": "74."
        },
        {
            "title": "Proprietary VLMs",
            "content": "53.25 40.35 56.97 61.87 63 Open-Source VLMs 47.98 42.45 51.95 49.23 Finetuned VLMs 68.82 30.39 28.85 41.3 73.3 32. 45.89 32.45 38.1 25.1 77.04 65.41 79.34 78.62 84.26 73.47 73.2 75.89 71.35 84.89 79.81 82.62 81.03 86. 80.5 80.25 81.93 Qwen2.5-VL-7B-Instruct + Visual-TableQA 84.52 60.95 77.59 55.75 Qwen2.5-VL-7B-Instruct + ReachQA * Performance metrics are measured using Relaxed Accuracy, which allows for small numerical deviations in the predicted answers. We assume that this accuracy inflates the actual accuracy by at least 5%. This margin is subtracted when selecting the best-performing results, which are shown in bold. 49.77 48.57 82.98 60.68 N/A N/A Table 4: Performance of fine-tuned models on the two splits of the ReachQA test set: Recognition (Reco) and Reasoning (Reas), each consisting of exactly 1,000 samples. Best performances per model category are in bold. The values in blue are from our own evaluation using the LLM jury, while the remaining values are taken from He et al.. Model LLaVA-Next-Llama3-8B + ReachQA + Visual-TableQA MiniCPM-V2.5-Llama3 + ReachQA + Visual-TableQA Reco 17.9 29.6 28. 25.3 35.10 36.20 Reas 6.5 11.1 20.2 10.3 11 31.50 Model InternVL2-8B + ReachQA + Visual-TableQA Qwen2.5-VL-7B-Instruct + ReachQA + Visual-TableQA Reco 33.7 49.8 45.6 66.20 69.6 70.3 Reas 16.2 21.3 34. 33.10 40.30 50.6 + ReachQA + Visual-TableQA +10.25 +9.35 +4.4 +17.68 Average gains"
        },
        {
            "title": "4.1 Visual-TableQA vs ReachQA",
            "content": "The ReachQA dataset is divided into two equally sized subsets: Recognition, which tests models ability to extract relevant information from charts, and Reasoning, which evaluates models capacity to understand complex and abstract data structures. Table 4 reports the performance gains of multiple fine-tuned models on these two tasks. On average, models fine-tuned on ReachQA exhibit an accuracy improvement of +10.25 points on the Recognition task and +4.4 points on the Reasoning task. In comparison, models fine-tuned on Visual-TableQA show an average gain of +9.35 on Recognitiona comparable resultbut significantly larger gain of +17.68 on Reasoning. This stark contrast in reasoning performance can be attributed to the presence of high-quality rationales in Visual-TableQA annotations, along with the inclusion of more complex and diverse visual structures. In other words, despite being roughly three times smaller than ReachQA in terms of sample count, Visual-TableQA places stronger emphasis on qualitative richness over quantity. As result, it appears to enable more effective knowledge distillation, particularly for tasks requiring symbolic interpretation and multi-step reasoning."
        },
        {
            "title": "4.2 Visual-TableQA’s Advantages Compared to Other Datasets",
            "content": "Table 1 shows that only few table-focused QA datasetsnamely TAT-DQA, Table-VQA, and TableVQA-Benchrepresent tables as rendered images. Visual-TableQA surpasses these by offering richer layout diversity, broader topic coverage, systematic visual complexity, and highquality rationales. These attributes make it particularly effective for training models with transferable reasoning skills. Supporting this, models fine-tuned solely on Visual-TableQAsuch as LLaVA-Next-Llama3-8Bdemonstrated significant gains on external benchmarks (Table-VQA and TableVQA-Bench), as seen in Table 5. Interestingly, Qwen2.5-VL-7B-Instruct did not follow the same performance trend: it showed degradation on tasks such as VTabFact (Yes/No fact verification), VWTQ (Wikipedia table retrieval), and VWTQ-Syn (synthetic variants). To understand this, we manually analyzed its errors before and after fine-tuning on VTabFact, categorizing them into eight types: partial data extraction, hallucination, incoherence, misunderstanding, reasoning errors, evaluation mistakes, dataset ambiguity, and annotation flaws. Results (Figure 11) show that while the total number of errors slightly increased post-finetuning, most now fall into the incoherence class, with all other error types significantly reduced. This suggests sharpening of reasoning patterns but also highlights need for future work targeting specific error types through synthetic supervision. Further details are provided in Section I. Beyond transferability and diversity, key advantage of Visual-TableQA lies in its modularity and scalability as explained in Section 4.4 . 8 Table 5: Performance of fine-tuned models on Table-VQA test set and the four splits of the TableVQABench dataset: FinTabNetQA (finance-related tables), VTabFact (table-based fact verification with Yes/No questions), VWTQ (information retrieval from Wikipedia tables), and VWTQ-Syn (synthetic visual variants of VWTQ). Best performances per model variants are shown in bold. Values in blue are from our own evaluation using the DeepSeek-Prover-v2, while remaining values are reported from Fu et al. (2025). Model GPT-4o LLaVA-Next-Llama3-34B LLaVA-Next-Llama3-8B + Visual-TableQA Qwen2.5-VL-7B-Instruct + Visual-TableQA TableVQA-Bench Table-VQA FinTabNetQA VTabFact VWTQ VWTQ-Syn 54.8 52.55 94.8 93.6 89.6 71. 42.04 52.0 84.4 77.25 66.5 36.4 23.6 32.93 68.99 59.6 73.2 38. 25.2 33.6 74.0 69.6 25.84 28.89 77.92 73."
        },
        {
            "title": "4.3 Evaluator Consistency Compared to the Literature",
            "content": "In line with recent studies (He et al.; Fu et al., 2025; Agarwal & Ciuca, 2025), we employed high-performing LLM jury with majority-vote strategy to evaluate model predictions. The jury consisted of Qwen3-32B, DeepSeek-R1-Distill-LLaMA-70B, Gemini-2.5-Pro, GPT-4.1, and DeepSeek-Prover-v2. In Table 3, baseline values (in black) are reported from He et al., who used GPT-4o as the sole evaluator. While our evaluation pipeline involves broader and more powerful set of models, making it arguably more reliable and robust, we still consider the two evaluation protocols broadly comparable. In fact, due to the stricter majority-vote requirement across diverse models, our approach may even yield more demanding or rigorous evaluations. This comparability also holds for Table 5, where baseline performances (in black) are taken from Fu et al. (2025), who employed GPT-4 for their evaluation. We argue that despite methodological differences, all evaluations are consistent enough to be analyzed jointly for comparative purposes."
        },
        {
            "title": "4.4 Scalability of the Pipeline and Its Benefits for Knowledge Distillation",
            "content": "This modular pipeline supports scalable generation with clean separation of concernstable structure synthesis, QA creation, and validationmaking each component independently reusable and upgradable. By automating the entire process from table generation to jury-based quality control, Visual-TableQA provides cost-efficient and high-quality benchmark for advancing multimodal reasoning over complex visual inputs. central component of our pipeline is the mechanism of cross-model inspiration 2.2, collaborative prompting strategy. In this process, stronger models generate layout seeds that guide weaker models in synthesizing structurally diverse tables, fostering novel visual configurations through iterative transfer. The same principle extends to questionanswer generation: models are prompted with both layout and topical cuesoften proposed by stronger modelsto create new QA pairs. This enables weaker models to contribute meaningfully to the dataset by expanding the range of questions and reasoning patterns. Through this dual-inspiration process, the pipeline cultivates collaborative multi-model co-creation space, where models of varying capabilities distill collective knowledge not through imitation, but through generative inspiration, while maintaining data quality. In this regard, Visual-TableQA distinguishes itself from other synthetic datasets Aboutalebi et al. (2024); Wang et al. (2024a); Li et al. (2025); He et al.."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduced Visual-TableQA, large-scale, open-domain, multimodal dataset designed to rigorously evaluate visual reasoning capabilities over complex table images. Building on the principles of Code-as-Intermediary Translation (CIT), we developed fully automated, modular pipeline for generating LaTeX-rendered tables, reasoning-intensive questionanswer pairs, and highquality rationalesall verified by jury of strong LLMs. Despite being cost-efficient (generated for under $100), Visual-TableQA offers unprecedented diversity in table structures, visual features, and reasoning depth. We showed that Visual-TableQA not only challenges existing visual language 9 models (VLMs) but also serves as an effective training signal for improving reasoning performance. Fine-tuning on Visual-TableQA led to substantial gains across multiple benchmarksboth tablecentric and general-purposeincluding ReachQA and MATH-Vision, demonstrating the datasets capacity to bridge the performance gap between open-source and proprietary models."
        },
        {
            "title": "References",
            "content": "Hossein Aboutalebi, Hwanjun Song, Yusheng Xie, Arshit Gupta, Justin Sun, Hang Su, Igor Shalyminov, Nikolaos Pappas, Siffi Singh, and Saab Mansour. Magid: An automated pipeline for generating synthetic multi-modal datasets. arXiv preprint arXiv:2403.03194, 2024. Pranav Agarwal and Ioana Ciuca. Supernova event dataset: Interpreting large language models personality through critical event analysis. arXiv preprint arXiv:2506.12189, 2025. Anthropic. sonnet. Claude-3-Model-Card-October-Addendum.pdf, 2024. Accessed: 2025-08-01. Claude 3.5 haiku and upgraded claude 3.5 https://assets.anthropic.com/m/1cd9d098ac3e6467/original/ Model card addendum: Anthropic. Claude opus 4 & claude sonnet 4 system card. https://www.anthropic.com/ claude-4-system-card, May 2025. Accessed: 2025-08-01. Shaohan Chen, Yujia Zhang, Xiangpeng Cao, Shaolei He, Chen Zhao, Zhihua Liu, Chongming Li, Jing Liu, Qiang Liu, Fan Liu, et al. Qwen-vl: versatile vision-language model with image, text, and box comprehension. arXiv preprint arXiv:2403.18751, 2024a. URL https: //arxiv.org/abs/2403.18751. Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. Tabfact: large-scale dataset for table-based fact verification. In International Conference on Learning Representations, 2020a. URL https://openreview. net/forum?id=rkeJRhNYDH. Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. HybridQA: dataset of multi-hop question answering over tabular and textual data. In Trevor Cohn, Yulan He, and Yang Liu (eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 10261036, Online, November 2020b. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.91. URL https://aclanthology.org/2020. findings-emnlp.91/. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024b. DeepSeek-AI. DeepSeek-R1-Distill-Qwen-32B. https://huggingface.co/deepseek-ai/ DeepSeek-R1-Distill-Qwen-32B, 2025. Model card. Accessed: 2025-08-01. Xingyu Fu, Minqian Liu, Zhengyuan Yang, John Corring, Yijuan Lu, Jianwei Yang, Dan Roth, Dinei Florencio, and Cha Zhang. Refocus: Visual editing as chain of thought for structured image understanding. arXiv preprint arXiv:2501.05452, 2025. Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Roscoe: suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations. Google. Gemini 2.0 flash: Model card. https://storage.googleapis.com/model-cards/ documents/gemini-2-flash.pdf, 2025a. Published: 2025-04-15. Accessed: 2025-08-01. Google. Gemini 2.5 flash: Model card. https://storage.googleapis.com/model-cards/ documents/gemini-2.5-flash.pdf, 2025b. Updated: 2025-06-26. Accessed: 2025-08-01. Google. Gemini 2.5 pro: Model card. https://storage.googleapis.com/model-cards/ documents/gemini-2.5-pro.pdf, 2025c. Model card. Last updated: 2025-06-27. Accessed: 2025-08-01. 10 Wei He, Zhiheng Xi, Wanxu Zhao, Xiaoran Fan, Yiwen Ding, Zifei Shan, Tao Gui, Qi Zhang, and Xuanjing Huang. Distill visual chart reasoning ability from llms to mllms. Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations. Sahil Kale and Vijaykant Nadadur. Texpert: multi-level benchmark for evaluating latex code generation by llms. arXiv preprint arXiv:2506.16990, 2025. Yannis Katsis, Saneem Chemmengath, Vishwajeet Kumar, Samarth Bharadwaj, Mustafa Canim, Michael Glass, Alfio Gliozzo, Feifei Pan, Jaydeep Sen, Karthik Sankaranarayanan, and Soumen Chakrabarti. AIT-QA: Question answering dataset over complex tables in the airline industry. In Anastassia Loukina, Rashmi Gangadharaiah, and Bonan Min (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track, pp. 305314, Hybrid: Seattle, Washington + Online, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-industry.34. URL https://aclanthology.org/2022.naacl-industry.34/. Maurice George Kendall. Rank correlation methods. 1948. Yoonsik Kim, Moonbin Yim, and Ka Yeon Song. Tablevqa-bench: visual question answering benchmark on multiple table domains. arXiv preprint arXiv:2404.19205, 2024. Joseph Lee Rodgers and Alan Nicewander. Thirteen ways to look at the correlation coefficient. The American Statistician, 42(1):5966, 1988. Andrew Li, Rahul Thapa, Rahul Chalamala, Qingyang Wu, Kezhen Chen, and James Zou. Smir: Efficient synthetic data pipeline to improve multi-image reasoning. arXiv preprint arXiv:2501.03675, 2025. Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. Minesh Mathew, Dimosthenis Karatzas, Manmatha, and CV Jawahar. Docvqa: dataset for vqa on document images. corr abs/2007.00398 (2020). arXiv preprint arXiv:2007.00398, 2020. Meta AI. Llama 4 Maverick 17B-128E Instruct. https://huggingface.co/meta-llama/ Llama-4-Maverick-17B-128E-Instruct, 2025. Model card. Accessed: 2025-08-01. Mistral AI. Mistral Small 3.1 24B Instruct. https://huggingface.co/mistralai/ MistralâĂŚSmallâĂŚ3.1âĂŚ24BâĂŚInstructâĂŚ2503, 2025. Model card. Accessed: 2025-08-01. OpenAI. GPT-4o. https://openai.com/index/gpt-4o, 2024. Accessed: 2025-07-30. OpenAI. GPT-4.1. https://openai.com/index/gpt-4-1/, 2025a. Accessed: 2025-08-01. OpenAI. GPT-4o-mini. https://platform.openai.com/docs/models/gpt-4o, 2025b. Accessed: 2025-08-01. OpenAI."
        },
        {
            "title": "OpenAI",
            "content": "o3 Reasoning Model. https://openai.com/index/ introducing-o3-and-o4-mini/, 2025. Accessed: 2025-07-31. Panupong Pasupat and Percy Liang. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 14701480, Beijing, China, July 2015. Association for Computational Linguistics. doi: 10.3115/v1/P15-1142. URL https://aclanthology.org/P15-1142. 11 Qwen Team. Qwen3-30B-A3B. https://huggingface.co/Qwen/Qwen3-30B-A3B, 2025a. Model card. Accessed: 2025-08-01. Qwen Team. Qwen3-Qwen3-32B. https://huggingface.co/Qwen/Qwen3-32B, 2025b. Model card. Accessed: 2025-08-01. Qwen Team. Qwen3-QwQ-32B. https://huggingface.co/Qwen/QwQ-32B, 2025c. Model card. Accessed: 2025-08-01. Reka AI. Reka Flash 3. https://huggingface.co/RekaAI/reka-flash-3, 2025. Model card. Accessed: 2025-08-01. Z. Z. Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, Z. F. Wu, Zhibin Gou, Shirong Ma, Hongxuan Tang, Yuxuan Liu, Wenjun Gao, Daya Guo, and Chong Ruan. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801, 2025. doi: 10.48550/arXiv.2504.21801. URL https://arxiv.org/abs/2504.21801. Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. Multimodal{qa}: complex question answering over text, tables and images. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=ee6W5UgQLa. Qwen Team. Qwen2.5-vl, January 2025. URL https://qwenlm.github.io/blog/qwen2. 5-vl/. TNG Technology Consulting GmbH. Deepseek-r1t-chimera, April 2025. URL https:// huggingface.co/tngtech/DeepSeek-R1T-Chimera. Cyrile Delestre Tom Agonnoude, 2024. URL https://huggingface.co/datasets/cmarkea/ table-vqa. Jiankang Wang, Jianjun Xu, Xiaorui Wang, Yuxin Wang, Mengting Xing, Shancheng Fang, Zhineng Chen, Hongtao Xie, and Yongdong Zhang. graph-based synthetic data pipeline for scaling high-quality reasoning instructions. CoRR, 2024a. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024b. URL https://openreview.net/forum?id=QWTCcxMpPA. xAI. Grok 3 beta the age of reasoning agents. https://x.ai/news/grok-3, 2025. Accessed: 2025-08-01. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Xinyi Zheng, Douglas Burdick, Lucian Popa, Xu Zhong, and Nancy Xin Ru Wang. Global table extractor (gte): framework for joint table identification and cell structure recognition using visual context. In 2021 IEEE Winter Conference on Applications of Computer Vision (WACV), pp. 697706. IEEE, 2021. Victor Zhong, Caiming Xiong, and Richard Socher. Seq2sql: Generating structured queries from natural language using reinforcement learning. CoRR, abs/1709.00103, 2017. Fengbin Zhu, Wenqiang Lei, Fuli Feng, Chao Wang, Haozhou Zhang, and Tat-Seng Chua. Towards complex document understanding by discrete reasoning. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 48574866, 2022."
        },
        {
            "title": "A Extended Related Works",
            "content": "The vast majority of table-based QA datasetssuch as HybridQA Chen et al. (2020b), WikiTableQuestions Pasupat & Liang (2015), WikiSQL Zhong et al. (2017), and AIT-QA Katsis et al. (2022)represent tables in textual format rather than as rendered images, thereby bypassing the challenges associated with visual layout interpretation. In contrast, our work focuses exclusively on multimodal datasetsthose that contain both textual and visual (image-based) information. These can generally be grouped into two main categories: real-world datasets, collected from authentic documents, and synthetic datasets, generated using automated tools. Real-world multimodal QA datasets that emphasize tablessuch as TAT-DQA Zhu et al. (2022), and TableVQA-Bench Kim et al. (2024)tend to be highly domain-specific, limiting diversity in both table layouts and question types. For example, TAT-DQA Zhu et al. (2022) combines tabular and textual data from financial reports and, while it introduces hybrid contexts for realistic reasoning, its questions rely heavily on reading textual input rather than interpreting visual structure. Similarly, TableVQA-Bench Kim et al. (2024) consists of 83% real-world tables (1,250 out of 1,500), primarily sourced from task-specific datasets such as WikiTableQuestions (information retrieval), TabFact Chen et al. (2020a) (fact verification), and FinTabNet Zheng et al. (2021) (financial data extraction). Due to its relatively small size and the specialized nature of its subsets, the dataset exhibits limited visual diversity. This limitation also extends to the remaining 16% synthetic tables, whose visual variation is restricted to basic formatting attributes such as background color, border size, font size, and style. More recently, ChartQA Masry et al. and DocVQA Mathew et al. (2020) have introduced large open-domain datasets for visual question answering. ChartQA focuses on reasoning over charts and plots; however, its tasks primarily involve shallow reasoning and do not reflect the structural complexity and layout diversity found in real-world tables. In contrast, DocVQA offers greater diversity in document layouts and structures, but lacks significant visual challengerecent VLMs, including relatively lightweight models like Qwen2.5-VL-7B-Instruct Team (2025), achieve over 94% accuracy on this benchmark. Among synthetic datasets, MultiModalQA Talmor et al. (2021) stands out as the only open-domain resource focused on tables. It combines real-world figures, diagrams, and text passages sourced from Wikipedia, with QA pairs crafted to assess both reasoning and visual comprehension. Although it incorporates real content, the dataset is considered synthetic due to the way it links independent modalities and generates QA pairs through formalized templates. However, this approach results in limited diversity, as the questions are derived from finite set of templates. In contrast, TableVQA Tom Agonnoude (2024) is fully synthetic dataset generated using state-of-the-art LLMs. Nonetheless, it lacks visual diversity and complexityits tables follow similar formats and are predominantly centered around technical domains such as statistics, physics, and algorithms, all of which are heavily numerical in nature. Most of real-world datasets rely heavily on manual labeling, data collection, and preprocessingfactors that significantly constrain their scalability. Recently, ReachQA He et al. introduced more scalable and innovative approach through its Code-as-Intermediary Translation (CIT) pipeline. This method generates synthetic charts and reasoning questions by leveraging textual intermediaries such as Python code, demonstrating that advanced reasoning capabilities of large language models (LLMs) can be effectively transferred to visual models. While ReachQA successfully addresses both scalability and reasoning complexity, its approach is tailored to chart-based visualizations and does not extend to structured tabular data. To summarize, existing table-based benchmarks consistently fall short in one or more key areas: visual diversity, reasoning depth, or scalability. Notably, aside from MultiModalQA, there is no open-domain dataset designed to evaluate model performance on rendered table images, despite their prevalence in real-world settings such as reports, academic papers, and spreadsheets. In this work, we introduce Visual-TableQA, multimodal open-domain synthetic dataset specifically created to assess reasoning capabilities over table images using LLMs"
        },
        {
            "title": "B Considerations and Limitations",
            "content": "The main limitations of this work relate to the use of Code-as-Intermediary Translation (CIT) He et al. and the assessment of data quality. While we adopted LaTeX as an intermediate representation for 13 tables, its expressiveness is limited when handling more complex or visually rich images. Developing robust, bidirectional image-to-text encoding system remains an open and promising area for future research. In terms of data quality evaluation, although automatic metrics such as ROSCOE provide useful insights, they are not yet as reliable as human judgment. As result, human annotators continue to play critical role in ensuring high-quality data, especially when scaling synthetic datasets for reasoning tasks. In addition, we also observed that certain models, such as Qwen2.5-VL-7B-Instruct, did not consistently benefit from Visual-TableQA supervision across all downstream tasks, highlighting potential limitation in generalization that warrants further investigation."
        },
        {
            "title": "C TableQA Layout and Topic Diversity",
            "content": "As described in Section 2.2, we sampled 5,000 distinct topics using GPT-4o to serve as inspirations for table generation. To better illustrate topic diversity, we grouped these topics into 20 semantic clusters using the K-Means algorithm. Figure 5 displays 2D projection of these clusters, where each color represents distinct semantic group. For the 12 largest clusters, we highlight representative topics to give sense of their thematic content. In addition, Figure 4 shows the cumulative percentage of topics covered as clusters are added in descending order of size. The smooth progression of the curve indicates that the clusters are relatively uniform in size, confirming balanced distribution of topic diversity throughout the dataset. Figure 4: Cumulative topic coverage as clusters are added by descending size. The uniform slope indicates an even distribution of topics across clusters. To illustrate the diversity of table layouts produced by our pipeline, Figure 6 displays side-by-side comparison between the initial seed tables used in the first generation iteration and sample of layouts generated in subsequent steps. The wide range of structures highlights the pipelines capacity to create rich and varied visual designs from limited starting templates."
        },
        {
            "title": "D Table Generation Settings",
            "content": "The first stage of the generation pipeline involves LLM-1, which is responsible for producing new tables based on given inspirations. Specifically, it receives one LaTeX-formatted table as layout inspiration and three distinct topic inspirations. Based on these, it generates three new tables, each aligned with one of the provided topics while drawing structural influence from the layout example. The full prompt used to guide LLM-1 during this step is shown in Figure 7. The second stage of the generation pipeline involves LLM-2, which is responsible for producing questionanswer (QA) pairs based on single LaTeX table. The full prompt used to guide LLM-2 is shown in Figure 8. To encourage creativity while maintaining factual accuracy, the temperature parameter for each model during this phase is set to 0.7. In Table 6, we present the average validity rates of the generated QA pairs for each model involved in the QA generation phase. 14 Figure 5: 2D projection of the 5,000 topics using UMAP and K-Means clustering. Each color denotes semantic cluster. Representative topics are listed for the 12 largest clusters to illustrate diversity. (a) Layout seeds used for the initial table generation. (b) Sample of diverse layouts generated by our pipeline. Figure 6: Visual diversity of table layouts. Left: seed layouts used during the first iteration of table generation. Right: layouts generated through cross-model inspiration and iterative refinement."
        },
        {
            "title": "E Common Anomalies in Generated Tables",
            "content": "As summarized in Table 2, several tables generated by different LLMs required post-processing adjustments to ensure visual clarity, LaTeX correctness, and topic alignment. Below, we categorize the most common types of anomalies observed during generation:"
        },
        {
            "title": "Layout and Formatting Issues",
            "content": "Table overflow: Tables exceeding page margins due to improper column widths or missing column dimensioning. 15 You are an expert in generating synthetic datasets composed of LaTeX-formatted tables, optionally accompanied by illustrative diagrams. Your task is to produce structured content suitable for datacentric documents, ensuring each table (and diagram, if included) is clear, well-organized, and visually informative. Your final output should start with ```json and end with ``` as plain text, not just formatting. Like this: ```json { \"table_1\": \"BEGIN_LATEX <LaTeX code for table 1 (with/without diagram) here> END_LATEX\", \"table_2\": \"BEGIN_LATEX <LaTeX code for table 2 (with/without diagram) here> END_LATEX\", \"table_3\": \"BEGIN_LATEX <LaTeX code for table 3 (with/without diagram) here> END_LATEX\" } ``` Requirements: The tables and diagrams will be used to generate reasoning questions. Therefore: - If topic inspirations are supplied, ensure every generated table aligns with those topics. - Each LaTeX output must primarily consist of table. Include diagram only if it meaningfully complements the table; avoid adding one unnecessarily. Do not generate diagrams alone. If diagram is empty or non necessary DON'T INCLUDE it. - Keep any diagram minimal-smaller than the table, chart-free, and purely illustrative-serving only to reinforce the tables content without adding new information. - Each table and their diagram must contain realistic, domain-relevant content. They must be selfcontained, include clear descriptive title and not rely on external data to compile. - The type of information presented should be diverse-such as numerical data or qualitative. The variety and richness of visual elements is essential to the overall quality of the table and their diagram. Table quality should also come with large number of rows and columns. - Table and diagram layouts should be creatively designed-taking inspiration from reference example (when provided) but incorporating meaningful variations such as colors, multi-row or multi-column cells, custom formatting adjustments, or any other visual enhancement that promotes structural diversity. - Table layouts should be at least as complex as the example provided, don't try to simplify ( diagrams are not mandatory). Table complexity should also come with large number of rows and columns. - Do NOT escape any characters in the LaTeX code. The LaTeX must be written as plain text, exactly as it would appear in .tex file, with real line breaks and single backslashes (), not JSONescaped. - All LaTeX tables and diagrams must be constrained to fit entirely within the printable area of standard A4 page when compiled to PDF, without overflowing horizontally or vertically. Use appropriate formatting techniques such as adjusting column widths, reducing font size, or enabling landscape mode if necessary but NEVER rotation. - Make sure each LaTeX table and diagram includes all required usepackage declarations and is enclosed within complete, compilable LaTeX document structure, including the appropriate preamble and begin{document}...end{document} block. - Make sure each LaTeX codes start and end with BEGIN_LATEX and END_LATEX, respectively. - Make sure to wrapp your final answer with ```json at the beginning and ``` at the end. Figure 7: LLM prompt used for table generation. Text overflow: Cell content spilling outside the cell boundary, especially in narrow columns or with long strings. Invisible content: Multirow cells with background colors that obscure cell text (e.g., white text on white background). Improper horizontal lines: midrule or hline splitting across multirow cells, breaking visual coherence."
        },
        {
            "title": "Content Relevance and Correctness",
            "content": "Empty or irrelevant tables: Tables with placeholder content or unrelated to the assigned topic. Incorrect topic alignment: Generated tables that do not match the intended topic inspiration. 16 You are an expert in generating questionanswer pairs from LaTeX-formatted. Your task is to create structured dataset consisting of challenging, reasoning-based questions and their corresponding answers derived from given LaTeX formatted table with optional diagram. Input: You will be provided with sample LaTeX table as context. Based on this table or diagram, your goal is to generate JSON object with the following structure: questions: python list of 3 challenging questions that require reasoning and analysis based on the data presented in the table and the optional diagram. The questions must be answerable using only the information in the table or diagram, combined with basic general knowledge if necessary. The questions should be stored as strings. answers: python list of 3 detailed answers to the 3 questions, including clear chain of thought explaining the reasoning process. The answers should be stored as strings. Requirements: All questions must be relevant to the table's context and designed to test deeper understanding or inference. When possible, all questions should make full use of the visual or structural elements of the table or diagram (such as rows, columns, headers, colors and patterns) while maintaining clear relevance to the tables content. All entries (both questions and answers) should be returned as lists of string values. The global result should be single JSON object wrapped in markdown code block using ```json at the beginning and ``` at the end, and containing all two key-value pairs. This means your output should start with ```json and end with ``` as plain text, not just formatting. Figure 8: LLM prompt used for QA generation. Table 6: Average QA pair validity across different QA generation models. Accuracies are computed from sample of at least 500 QA pairs per model. Model Acc. (%) Llama 4 Maverick 17B-128E Instruct Gemini 2.0 Flash Gemini 2.5 Flash Gemini 2.5 Pro GPT-4.1 Qwen3-30B-A3B Qwen3-QwQ-32B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1T-Chimera Claude Sonnet 4 Claude 3.5 Haiku Grok 3 Beta Reka Flash 3 88 89.1 93.1 89.3 90.4 76.6 92.6 73.4 89.4 91 90.2 89.4 79 Duplicate outputs: All three tables generated for prompt are identical or nearly identical in structure/content. Missing external resources: References to images or figures not included or available in the output. Incorrect LaTeX syntax: Math symbols placed outside of math environments, leading to compilation errors. Diagram-Specific Issues Missing tables: Some generations return only diagram without an accompanying table. Node placement errors: Overlapping or misaligned nodes in TikZ diagrams. Arrow misplacement: Arrows that do not connect to correct nodes or that overlap diagram elements improperly. Legend/title confusion: Titles or legends positioned incorrectly or detached from the relevant diagram elements. Visual inconsistencies: General drawing flaws, such as missing anchors, inconsistent line styles, or unintended overlaps. These issues highlight the need for validation loop in the TableQA pipeline and justify the inclusion of human verification stages to ensure dataset quality."
        },
        {
            "title": "F LLM Jury Settings",
            "content": "LLM juries were involved at two key stages of the pipeline: (i) quality assurance, where generated tables and QA pairs were validated before inclusion in the dataset, and (ii) evaluation benchmarking, where model responses were assessed for accuracy and reasoning quality. The prompt used for quality assurance is shown in Figure 9, while the prompt used for evaluation during benchmarking is shown in Figure 10. Each jury consisted of multiple high-performing vision-language or reasoning-capable LLMs, and final decisions were made via majority voting. To ensure consistency and reproducibility across evaluations, all LLM jury calls were executed with temperature setting between 0.2 and 0.3. You are reasoning question answer expert. You will be given LaTeX formated table with/without diagram, list of 3 topics, and pair of question and its answer. Your task is to evaluate the pair of question answer based solely on the data in the LaTeX code and these criteria: - Does the LaTeX code contain Table (not some charts alone or diagrams alone) ? - Does the table, any optional diagrams, and the rest of the document are on one single topic from the provided list of topics, and internally consistent (be careful to off-topic diagrams)? - Is the question clear and related to the table or the diagram? - Is the answer (including its reasoning) correct and fully supported by the table or diagram data? If the four criteria are true, mark the pair as True. If one of the criteria is not met, mark it as False. Return single JSON object in the following format: ```json { \"decision\": true_or_false_for_qa_pair } ``` This means your output should start with ```json and end with ``` as plain text, not just formatting. Do not provide explanations. Only output the JSON object. Figure 9: LLM prompt used for QA evaluation. Evaluate if the answer matches the ground truth. To do so, read the question and determine whether the provided answer conveys the same meaning as the ground truth. Output JSON response as follows: {{\"verdict\": 1}} for correct or {{\"verdict\": 0}} for incorrect. Figure 10: LLM prompt used for Benchmark evaluation."
        },
        {
            "title": "G ROSCOE Metrics Scores",
            "content": "The ROSCOE evaluation scores are reported in Table 7."
        },
        {
            "title": "H Model Finetuning Hyperparameters",
            "content": "The hyperparameters used for LoRA are reported in Table 8."
        },
        {
            "title": "I Errors Taxonomy",
            "content": "Evaluating complex visual reasoning tasks remains challengingno evaluation system is flawless. single question can often admit multiple valid formulations of an answer. For example, Yes/No 18 reasoning metrics averaged over the whole dataset. The Table 7: ROSCOE Golovneva et al. Direction column indicates whether higher or lower values correspond to better performance for each metric. Metric Direction Mean Std Faithfulness-Step Informativeness-Step Informativeness-Chain Faithfulness-Token Repetition-Token Repetition-Step Discourse-Representation Coherence-Step Perplexity-Step Perplexity-Chain Perplexity-Step-Max Grammar-Step Grammar-Step-Max Avg Avg Avg"
        },
        {
            "title": "Avg",
            "content": "Semantic Adequacy () Redundancy & Risk () Logical Inference () Fluency & Perplexity () Grammaticality () 0.99 0.99 0.98 0.99 0.06 0. 0.68 0.7 0.01 0.05 8e-3 0.96 0.9 5e-4 5e-3 1.4e-2 2e-3 0.12 0.12 0.41 0. 0.01 0.03 8e-3 0.05 0.13 0.99 0.06 0.69 0. 0.93 Table 8: Hyperparameters Used for Fine-Tuning with LoRA. More details in our GitHub repository. Abbreviations: lr=learning rate, r= LoRA rank, α= LoRA α, Targets=targets modules for LoRA."
        },
        {
            "title": "Model",
            "content": "lr α"
        },
        {
            "title": "Targets",
            "content": "2e-5 all-linear 16 64 16 16 8 32 8 32 all-linear (llm frozen) LLaVA-Next-Llama3-8B Qwen2.5-VL-7B-Instruct MiniCPM-V2.5-Llama3 InternVL2-8B * The InternVL training source code sets the LoRA alpha as twice the LoRA rank, as shown in their official implementation here. We followed this convention for full reproducibility and assumed that other baselines applied the same rule. ** The InternVL and MiniCPM (here) training source codes automatically freeze the LLM when LoRA is used. We followed this convention for full reproducibility and assumed that other baselines applied the same rule. question might be correctly addressed through an implication or inference without explicitly stating \"Yes\" or \"No\", making it difficult for LLM juries to assess correctness with full confidence. Such answers are not strictly wrong, but they introduce ambiguity in evaluation. This limitation underscores the need for hybrid evaluation strategies, combining automated scoring with human or jury-based verification. To better understand the nature of model errors, we conducted manual analysis of the predictions made by Qwen2.5-VL-7B-Instruct, both before and after fine-tuning on the VTabFact dataset. We classified the observed errors into eight categories: Partial Data Extraction: The model overlooks some relevant entries (e.g., stops counting too early). Hallucination: The model references information not present in the table. Incoherence: The model extracts the correct data but then misinterprets it or contradicts itself later (deductive error). Misunderstanding: The model produces factual statements that do not actually answer the question. Faulty Methodology/Reasoning: The reasoning is too shallow, or the model fails to satisfy all the constraints of the query. Incorrect Evaluation: The LLM jury misjudges correct prediction as incorrect. Ambiguity (Gray Area): Both the ground truth and the models prediction can be reasonably justified. Dataset Mistake: The original dataset contains annotation or label errors. The results of this taxonomy are shown in Figure 11. Our findings indicate that while the total number of errors slightly increases after fine-tuning, the nature of the mistakes shifts significantly: most errors now fall into the Incoherence category, while all other error types are reduceddemonstrating more consistent and focused reasoning patterns post-finetuning. recurring example of Partial Data Extraction appears when the model validates statement such as the count of this item is n. It often stops counting after reaching the expected n, even if more instances exist in the table. Hallucination errors also appear frequently, particularly when the query suggests cell should satisfy two conditions. In these cases, the model tends to assume both conditions are meteven when they are not. Interestingly, the model performs better when the query does not suggest the presence of specific content in the table. (a) Histogram of Erros for Qwen2.5-vl-7b. (b) Histogram of Erros for Qwen2.5-vl-7b after Finetuning. Figure 11: Comparison of error distributions between the pretrained Qwen2.5-VL-7B model (left) and its finetuned version(right). Visual-TableQA Sample Table 9 gives some more detailed examples of our dataset samples. Image-to-LaTeX Dataset We have also released an additional dataset, Img2TeX, available on Hugging Face. It contains all the table images generated during the construction of Visual-TableQA, along with their corresponding LaTeX source code. This dataset is intended to complement the work of Kale & Nadadur (2025), which focuses on evaluating models ability to generate LaTeX from textual prompts. In contrast, Img2TeX targets the task of generating LaTeX documents from visual (image-based) inputs. This distinction opens up new evaluation pathway for vision-language models (VLMs), particularly those aiming to learn structured document generation from visual cues. As such, Img2TeX serves as valuable benchmark for assessing the visual-to-structured-text generation capabilities of multimodal models. 20 Table 9: Sample of reasoning-intensive QA pairs. The first rows question and answer are truncated for readability. These questions address multiple visual aspects and extend beyond simple information extraction to test interpretive reasoning, as illustrated in the second row with how question. Question"
        },
        {
            "title": "Table",
            "content": "Based on the visual patterns and row coloring in the table, what relationship exists between the alternating yellowhighlighted rows and the brewing methods characteristics [...]? Looking at the tables visual structure, the yellow-highlighted rows (rows 2, 4, 6, and 8) correspond to French Press, Chemex, Espresso, and Cold Brew methods respectively. These methods represent distinct extraction categories: French Press uses immersion, Chemex uses slow filtration, Espresso uses highpressure extraction, and Cold Brew uses cold water extraction. The alternating color pattern appears to highlight methods that represent fundamental brewing principles or landmark techniques within each difficulty tier.[...] How does the genealogy of origami bases demonstrate progressive complexity of folding techniques, and what key transformational steps are evident in the progression from square sheet to more complex bases? The genealogy reveals systematic progression from simple to complex bases, starting with the fundamental Square Sheet. The Preliminary Fold introduces diagonal and perpendicular creases, serving as critical intermediate step. This base enables more complex transformations like the Kite and Fish Bases, which develop pointed structures. The progression continues to the Bird Base, which combines multiple folding techniques to create more intricate models with distinct appendages. Each base builds upon previous folding techniques, demonstrating an incremental approach to increasing geometric complexity and model potential. Which UNESCO site has the highest cultural indicator, and impact what symbolic meaning does it represent? How does this relate to its visitor numbers and year of inscription? The Great Wall of China has the highest cultural impact indicator with value of 120. It symbolizes Strength and perseverance (as indicated by the blue color coding and symbolism type 2). Despite having the second-highest visitor numbers (10 million/year), it achieved maximum cultural impact. This suggests that while visitor numbers contribute to cultural impact, other factors like historical significance and symbolic representation play stronger role. The site was inscribed in 1987, earlier than many others, giving it more time to establish its cultural influence."
        }
    ],
    "affiliations": [
        "École de Technologie Supérieure Montreal, Canada"
    ]
}