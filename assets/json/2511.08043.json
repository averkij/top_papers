{
    "paper_title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces",
    "authors": [
        "Xueliang Zhao",
        "Wei Wu",
        "Jian Guan",
        "Qintong Li",
        "Lingpeng Kong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named \\textsc{DynaAct} for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 3 4 0 8 0 . 1 1 5 2 : r DYNAACT: Large Language Model Reasoning with Dynamic Action Spaces Xueliang Zhao Wei Wu Jian Guan Qintong Li Lingpeng Kong The University of Hong Kong Ant Group {xlzhao,qtli,lpk}@cs.hku.hk {wuwei19850318,jianguanthu}@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose novel framework named DYNAACT for automatically constructing compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates proxy for the complete action space by extracting general sketches observed in corpus covering diverse complex reasoning problems using large language models. We then formulate submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in complex reasoning with Large Language Models (LLMs) [Achiam et al., 2023, Jaech et al., 2024] have established prevalent self-improvement paradigm: given mass of problems paired with final answers, model developers first search for correct reasoning paths from base model using test time scaling strategies [Snell et al., 2024], then improve it to internalize these patterns [Guo et al., 2025] through supervised fine-tuning [Guan et al., 2025] or reinforcement learning [Guo et al., 2025]. While LLMs have exhibited remarkable reasoning capabilities, current approaches to long-term reasoning in these models often suffer from fundamental limitations. On the one hand, some approaches explicitly define an action space and state space and structure reasoning hierarchically, where action selection and state prediction are carried out iteratively [Hao et al., 2023, Qi et al., 2024] along the reasoning path. However, because action spaces are often heuristically designed, the resulting actions tend to either be too specific to generalize across domains or too broad to effectively guide reasoning. On the other hand, in the absence of an explicit definition for action spaces, other approaches impose specific format on generation and perform reasoning in an autoregressive manner [Lightman et al., 2023, Guo et al., 2025]. These approaches inherently search the entire natural language space for reasoning, thereby necessitating powerful base models. We investigate LLM reasoning within the framework of Markov Decision Process (MDP) [Hao et al., 2023], where reasoning trace consists of series of actions and states. Instead of focusing on This work was done during an internship at Ant Group. Corresponding authors. 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: Overview of the proposed method. Given the proxy action space A, the method searches for the subset At that maximizes the submodular function, which consists of utility term and diversity term. The subset At is then used for the subsequent reasoning steps. policy learning or reward modeling, we pay special attention to action space construction, as welldefined action space is fundamental to MDP-based reasoning. Specifically, we identify two essential properties that qualified action space should possess: (1) Scalability: it should be automatically learned from demonstration data, rather than being manually engineered, to strike balance between generalization and utility; and (2) Compactness: it should maintain dynamically constructed, information-dense structure so that for each step desired action can be picked up from small yet complete candidate set. The core technical challenge lies in developing principled approach that simultaneously optimizes both objectives: distilling generalizable action patterns from demonstrations while eliminating redundant candidates that would otherwise impede efficient exploration. To achieve both scalability and compactness, we frame the problem of action space construction as subset selection task and propose DYNAACT, in which the action space for each reasoning step is dynamically determined by submodular function that is learned through data-driven approach. The key idea is to approximate small subset from the entire action space that achieves the optimal balance between utility and diversity, leveraging the diminishing returns property of submodular functions to ensure linear computational complexity. Our method begins by extracting general reasoning patterns from diverse corpus of complex problems to construct the complete action space. We then define submodular function to evaluate candidate actions by jointly considering their utility to the current state and their diversity contribution. By maximizing this function via greedy algorithm, we obtain an optimal subset of actions. Consequently, reasoning follows standard Markov process: at each step, candidate actions are selected via the submodular optimization, an action is then chosen according to Q-function estimated via Monte Carlo tree search, and reasoning step is finally generated conditioned on the current reasoning context and the chosen action. Notably, throughout this process, only lightweight embedding model used in the submodular function requires training, while the base LLM remains frozen. We conduct extensive experiments on six benchmarks spanning general, reasoning, and math tasks. Evaluation results indicate that DYNAACT achieves significant improvements over baselines across all tasks, including MMLU, MMLU-Pro, GPQA, ARC-C, GSM8K, and MATH-500. Notably, DYNAACT excels at solving complex problems, achieving 6.8% absolute gain over the recently proposed strong model rStar on MATH-500. Furthermore, an extended study demonstrates that while dynamic action space construction enhances efficacy, it does not introduce significant additional latency during inference compared to the baselines. Our contributions are three-fold: (1) We propose dynamic action space construction as novel research question, orthogonal to the extensive studies on LLM reasoning in the community; (2) We introduce submodular function for action space construction, which significantly improves problem-solving accuracy while maintaining reasonable inference efficiency; and (3) We empirically verify the efficacy of our method across wide range of tasks."
        },
        {
            "title": "2 Preliminaries",
            "content": "Before delving into DYNAACT, we provide some background information. We begin by formulating MDP-based reasoning, then describe the search strategy applied. The section finally gives brief introduction to submodular functions, which form the theoretical basis for action space construction. 2 2.1 Reasoning Framework We formulate LLM reasoning as an MDP, defined by tuple (S, A, , R, γ). Here, represents the state space where each state st encodes the cumulative reasoning context up to step t, with the initial state s0 derived from the input prompt. denotes the action space with at an action indicating the progression of the reasoning context. The transition function : determines how actions transform the reasoning state, while : assigns rewards based on the quality of reasoning steps. The discount factor γ [0, 1] balances immediate and future rewards. At each time step t, the LLM selects an action at from candidate set At A, which may be generated automatically [Hao et al., 2023] or specified manually [Qi et al., 2024]. In our implementation, the action selection is governed by learnable value function Q(st, a) that estimates the expected cumulative reward: at = arg max aAt Q(st, a), Q(st, a) = (cid:34) (cid:88) (cid:35) γkR(st+k, at+k) st, at = . k= Our primary focus is on developing principled approach to construct the candidate action set At at each step, as this significantly impacts the efficiency and efficacy of the reasoning process in complex problem-solving scenarios. 2.2 Monte Carlo Tree Search We employ Monte Carlo Tree Search (MCTS) for estimating Q(st, a) [Silver et al., 2016]. In nutshell, the estimation is achieved by simulating multiple continuations of the reasoning process from the current state after applying the candidate action. During these simulations, the outcomes of the extended reasoning traces are evaluated, and the value of an action is approximated as the average result observed across the simulations. In this way, MCTS effectively balances the exploration of less frequently visited candidate actions with the exploitation of those that have demonstrated promising progress, which is critical to ensure that the estimated value function reliably reflects the potential benefit of each action in guiding the overall reasoning process. We defer additional technical details of MCTS, including how selection, expansion, simulation, and backpropagation are performed, to Appendix C. 2.3 Submodular Functions The problem of action space construction can naturally be formalized as subset selection task, where the goal is to identify small, high-value subset from much larger set of potential candidates. typical approach to subset selection involves assessing the utility of each element, ensuring that every new element added contributes additional value to the overall set. To achieve this, submodular functions are often employed due to their diminishing returns property, which prioritizes the selection of elements that offer unique and informative value [Fujishige, 2005, Kothawade et al., 2022, Chen et al., 2024]. As result, subset selection with submodular functions ensures that the marginal benefit of adding an element to smaller subset is greater than adding it to larger one, thus enhancing the compactness of the subset. Formally, given two candidate sets and an action , submodular function (; ) satisfies the following condition: (X {a}; st) (X; st) (X {a}; st) (X ; st). (1) Heading toward scalable approach for constructing compact action spaces, we take advantage of submodular functions. The problem then boils down to (1) how to define proper submodular function; (2) how to learn the submodular function from data; and (3) how to perform action space construction with the submodular function, as will be presented in the following Section. 3 Algorithm 1 Complete Pipeline of Sequential Reasoning. Require: Input question q, dataset D, number of groups k, candidate selection budget m, maximum reasoning steps 1: /* Proxy Action Space Estimation (performed once) */ 2: Partition the dataset into groups: {D1, D2, . . . , Dk} 3: for = 1 to do Extract an observation sketch oi = (cid:10)a1, a2, . . . , aoi 4: 5: Form the action space = (cid:83)k 6: Train the embedding function using Q-learning objective (Eq. (5)) with observation sketches as demon- (cid:11) LLMQuery(Di) (cid:10)a1, a2, . . . , aoi i=1 oi = (cid:83)k i=1 (cid:11) stration data /* Candidate Action Selection via Greedy Algorithm */ for = 1 to do Xd arg maxaXd (X {a}; st) {a} 7: s0 InitializeState(q) 8: for = 0 to 1 do 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: Output: {s0, a0, s1, . . . , sT } At(st) /* Action Evaluation using MCTS */ for all At(st) do Q(st, a) MCTS(st, a) /* Action Selection and State Update */ at arg maxaAt(st) Q(st1, a) st+1 UpdateState(st, at)"
        },
        {
            "title": "3 Method",
            "content": "We detail the approach to constructing At given the current state st. Specifically, our method consists of three stages: estimating an approximation of the complete action space as (3.1), defining submodular function (At, st) based on st (3.2), and utilizing the function to derive At (3.3). Figure 1 provides an overview of the method. 3.1 Proxy Action Space Estimation We first estimate an approximation as proxy of the complete action space, denoted as A. Specifically, we follow Wang et al. [2024a] by employing observations as candidate actions, where observations are typically cues that guide the reasoning process (cf. Figure 1). Given problem corpus (e.g., mathematical questions, logical puzzles, etc.), we randomly divide it into groups and feed each group to an LLM for observation collection. The division strategy ensures that each group is appropriately sized, avoiding prohibitive computational costs from the LLM. We then query the LLM to extract general observation sketchs per group that can be applied broadly and focus solely on the core operations. After that, the resulting observations are collected and form the proxy action space with duplicate items removed. The prompt for observation sketch extraction is provided in Appendix D. Notably, can be easily scaled up by incorporating examples from broader domains or fields; one can also develop various agents by applying the method to domain-specific corpora. 3.2 Submodular Function Definition To construct an optimal candidate action set that balances both utility (in terms of expected rewards) and diversity, we propose submodular function (At; st) for candidate subset At as follows: (At; st) = α futil(At; st) + β fdiv(At), (2) where futil(At; st) measures the expected utility of the candidate actions in advancing the reasoning process, fdiv(At) promotes diversity within the selected set, and α, β are balancing parameters. To ensure that (At; st) defined by Eq. (2) meets the condition given by Eq. (1), we define the utility 4 term as: futil(At; st) = log (cid:32) (cid:88) (cid:16) (cid:17) e(st)T e(a) exp (cid:33) , (3) aAt where e() is an embedding function that maps states and actions to shared representation space. Then the diversity term is defined as (cid:88) (cid:16) (cid:17) , 1 e(ai)T e(aj) fdiv(At) = min aj At aj =ai This formulation encourages the selection of actions that are maximally distinct from each other in the embedding space, preventing redundancy in the candidate set. Lemma 1. Given the definitions of the relevance term futil(At; st) in Eq. (3) and the diversity term fdiv(At) in Eq. (4), the function (At; st) defined in Eq. (2) is submodular with respect to the candidate action set At A. aiAt (4) The proof of Lemma 1 is provided in Appendix E. fundamental requirement of our framework is ensuring that At contains actions that maximize expected rewards in the reasoning process. To this end, we design the embedding function e() to capture the effectiveness of actions in advancing the reasoning process. We formalize this requirement through Q-learning, where e(st)T e(a) approximates the Q-valuethe expected future reward of executing action in state st. By incorporating this formulation into the standard Q-learning update equation [Watkins and Dayan, 1992, Reddy et al., 2019], we derive the following optimization objective: L(st, a, st+1) = e(st)T e(a) + log (cid:32) exp (cid:0)e(st+1)T e(a)(cid:1) (cid:33)(cid:33)2 , (5) (cid:32) (cid:88) aA where the training data consists of state-action pairs (st, at), with at being the ground-truth action obtained from the observation sketches. The reward is defined as = 1 for = at and = 0 for all other actions {at}. We train the embedding model by minimizing over all states st in the training set and all actions A. This objective ensures that the embedding function learns to prioritize actions that contribute substantively to the problem-solving progression. 3.3 Action Space Construction With and (; ) at hand, we aim to derive At by selecting elements from that maximize (; st). Formally, we construct At by solving At(st) = arg max XA, X=m (X; st). (6) Owing to the submodularity of (; ), the combinatorial optimization problem (Eq. (6)) can be effectively approximated by greedy algorithm [Nemhauser et al., 1978] with complexity of O(m2A), where denotes the size of A. In practice, we begin with an empty set and iteratively add the element from that produces the highest marginal increase in (; ) when combined with the current set X. Formally, for each iteration, we define the set of remaining candidates Xd = and select the element such that = arg max aXd (X {a}; st). The process is repeated until elements have been selected, and the final set is then taken as At. Algorithm 1 summarizes the complete pipeline of our reasoning method, encompassing both action space construction and MCTS-based reasoning path search."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Benchmarks We employ six standard benchmarks covering three domains: general, reasoning, and math. Specifically, we use the following datasets: 5 (1) MMLU [Hendrycks et al., 2020] is benchmark designed to evaluate models ability to answer wide variety of tasks, including reading comprehension, reasoning, and problem-solving, across general domains. It is widely used for assessing language model performance in broad tasks; (2) MMLU-Pro [Wang et al., 2024b] is an extension of MMLU, containing more challenging and professional-level problems. This dataset tests the models capabilities on more complex problems in general domains; (3) GPQA [Rein et al., 2023] focuses on evaluating reasoning and problem-solving skills, providing real-world open-domain problems that require advanced reasoning to solve; (4) ARC-challenge (ARC-C) [Clark et al., 2018] is part of the AI2 Reasoning Challenge and contains science-based multiple-choice questions. These questions require deep reasoning and are specifically designed to challenge models in complex reasoning tasks; (5) GSM8K [Cobbe et al., 2021] is dataset consisting of grade-school level math word problems that require logical reasoning. It tests models ability to solve elementary-level math problems; and (6) MATH-500 [Lightman et al., 2023] is dataset containing high school-level math problems. It serves to assess models ability to handle more advanced mathematical reasoning. 4.2 Evaluation Metrics We use exact match accuracy as the primary metric for evaluating the performance of our method. Specifically, for multiple-choice question-answering tasks, such as MMLU, MMLU-Pro, GPQA, and ARC-C, accuracy is calculated based on the exact match between the predicted choice and the ground-truth choice (typically denoted by letter representing the correct answer). For math problem-solving tasks, such as GSM8K and MATH-500, accuracy is calculated by comparing the predicted final answer, enclosed by boxed{}, with the ground-truth answer. 4.3 Baseline Methods We compare the proposed method with the following baselines: (1) Zero-shot CoT: This baseline uses Llama 3.1 [Dubey et al., 2024] with zero-shot Chain-of-Thought (CoT) prompting [Wei et al., 2022], generating reasoning paths in single pass; (2) SC@maj16: This method applies the selfconsistency (SC) technique [Wang et al., 2022], where multiple reasoning paths are generated and the most frequent result is selected. We use the SC@maj16 variant, which runs 16 rollouts to increase the accuracy of reasoning; (3) RAP: The method [Hao et al., 2023] integrates world model and reasoning agent, balancing exploration and exploitation to efficiently find high-reward reasoning paths via MCTS. The action space is formed by automatically generated sub-questions; and (4) rStar: The method [Qi et al., 2024] utilizes 5 manually defined actions as the action space. Reasoning traces are searched with MCTS rollouts, and the final trace is determined by small LLM as discriminator. All of these baselines use Llama-3.1-8B-Instruct [Dubey et al., 2024] as the backbone, consistent with our method. Additionally, both RAP and rStar use 16 rollouts, as in our method. 4.4 Implementation Details For the proxy action space estimation (3.1), we use the Open-Platypus [Lee et al., 2023] corpus, which covers wide range of topics, including math, scientific reasoning, and more. The corpus contains total of 24, 652 problems, which are used to form the problem set. We divide the corpus into = 2, 500 groups. To extract observations, we query Llama-3.1-70B-Instruct [Dubey et al., 2024], resulting in 40, 822 observations in total. In our submodular function definition (3.2), we set the balancing parameters α = 0.9 and β = 0.1 to ensure proper balance between utility and diversity. For embedding efficiency, we use Llama-3.2-1B-Instruct [Dubey et al., 2024] as the backbone and select the last tokens embedding as the output of the embedding function e(). The embedding function is fine-tuned using the Q-learning objective, with total of 83, 083 state-action pairs, and the learning rate is set to 1e 5. For the action space construction (3.3), we set the size of the candidate action set At at each time step = 5. We use Llama-3.1-8B-Instruct [Dubey et al., 2024] as the world model [Hao et al., 2023], which generates reasoning steps during the MCTS process. 4.5 Main Results The results of our experiments, presented in Table 1, reveal several key observations: (1) Our method outperforms all baseline methods across the six evaluated benchmarks, achieving significant 6 Table 1: Evaluation results on different benchmarks. Numbers in bold denote the best performance."
        },
        {
            "title": "Model",
            "content": "MMLU MMLU-Pro GPQA ARC-C GSM8K MATH-500 Zero-shot CoT 68.87 69.66 SC@maj16 69.46 RAP 68.61 rStar"
        },
        {
            "title": "DYNAACT",
            "content": "70.22 43.45 49.36 48.70 48.81 51.40 31.82 34.34 38.89 36.87 39.39 81.06 80.63 85.41 86. 88.31 76.12 86.66 87.79 87.11 89.16 45.40 52.00 51.60 54.20 61.00 improvements in general, reasoning, and math tasks. This confirms the effectiveness of our method in wide range of problem-solving tasks; (2) In the math domain, we observe the most notable improvements. Our method achieves 1.37% and 6.80% improvements over the baselines on GSM8K and MATH-500, respectively. The MATH-500 dataset, which requires higher level of reasoning capability, particularly benefits from the use of the submodular function. The ability to construct more compact and utility-optimized action space enables more efficient exploration, leading to improved performance in complex mathematical reasoning tasks; and (3) While rStar performs better than RAP on MATH-500, its performance in other benchmarks suffers due to the limited scalability of its manually defined action space. This limits rStars ability to effectively handle the full range of tasks, making it less scalable compared to our method."
        },
        {
            "title": "5 Discussions",
            "content": "In addition to the comprehensive evaluation across multiple benchmarks, we aim to dive deeper into DYNAACT to gain further insights into its underlying mechanisms. Specifically, we investigate the following research questions: (1) RQ1: How do different components affect performance? (2) RQ2: Can DYNAACT learn to have compact action space, thereby facilitating efficiency in reasoning? (3) RQ3: What is the utility of the actions selected by DYNAACT? (4) RQ4: Does DYNAACT introduce additional latency during inference? Besides, we are also curious about (5) RQ5: How does reasoning performance vary across different levels of difficulty? and (6) RQ6: Can the submodular function enhance diversity of actions? We leave the discussions to RQ5 and RQ6 to Appendix F. 5.1 Ablation Study for RQ1 We exploit ARC-C and MATH-500 to strike balance between difficulty and domain diversity, and examine four variants of DYNAACT, including: exclusion of the utility term, denoted as - util; exclusion of the diversity term, referred to as - div; submodular function without Q-learning defined by Eq. (5), denoted as - qlearning, which uses Llama-3.2-1B-Instruct for embedding directly; and removal of the submodular function, represented as - submodular, which generates action spaces using Llama-3.1-8B-Instruct directly. - util - div - q-learning - submodular Model Table 2: Ablation study. ARC-C MATH-500 DYNAACT (full) 88.31 87.63 86.52 87.80 85.15 61.00 53.40 53.80 55.80 52.00 The results shown in Table 2 indicate that the full version of DYNAACT achieves the best performance across both benchmarks, underscoring the critical importance of each component in our method. Removing the utility term leads to slight decrease in performance, highlighting its contribution to overall reasoning effectiveness. Excluding the diversity term results in further performance degradation, emphasizing the need for diversity in the candidate action set. Omission of the Qlearning objective causes noticeable drop in performance, demonstrating the necessity of this learning procedure for constructing an effective submodular function. Finally, the model without the submodular function performs the worst, reinforcing the essential role of the submodular strategy in achieving effective action selection. 7 5.2 Compactness Study for RQ2 We examine whether DYNAACT can produce compact action spaces. Ideally, compact action space would enhance search efficiency, leading to better reasoning performance with smaller size. Figure 2 compares DYNAACT with RAP where the actions are constrained to be sub-questions. From the results, we observe that: (1) The action spaces of RAP are highly redundant. Increasing the number of rollouts yields limited performance improvements when = 5 or = 10. Only when is increased to 15 do we observe noticeable performance gains with respect to the number of rollouts, though the marginal gains are still much slower compared to DYNAACT. (2) On the other hand, even with set to 5, DYNAACT still demonstrates significant performance improvements as more rollouts are carried out. The advantages over RAP remain consistent across all values of m, highlighting the efficacy of DYNAACT in generating compact action spaces. Figure 2: comparison of RAP and DYNAACT with respect to different action space sizes (i.e., m). The x-axis indicates the number of rollouts, while the y-axis shows the accuracy on MATH-500. 5.3 Utility Study for RQ3 Table 3: Evaluation results on the utility of action spaces. Accuracy is measured on subset of MATH-500 with Level 5 difficulty (i.e., the most difficult subset). This subset is chosen because the utility of actions (i.e., if critical steps are triggered) plays more critical role in solving complex problems. Model We select rStar as the baseline since its manually designed actions are presumed to be overly broad, limiting their utility for reasoning. Typically, there is no standard method for measuring utility. Therefore, following Zhao et al. [2024], we identify critical steps in problem-solving (e.g., key insights, decisions, or calculations essential for solving problem) and examine whether the reasoning steps triggered by actions contain these critical steps. The ratio of solutions containing critical steps (referred to as the Critical Step Coverage) is then used as proxy metric for utility. Table 3 presents the evaluation results. We can see that DYNAACT enables more effective identification and triggering of essential reasoning steps, which also explains its superiority over rStar in terms of accuracy. Critical Step Coverage () Accuracy ()"
        },
        {
            "title": "DYNAACT\nrStar",
            "content": "31.34 26.87 0.63 0.47 5.4 Latency Study for RQ4 To assess whether DYNAACT introduces additional latency during inference, we compare it with rStar and RAP based on the relative time required to complete tasks on the MATH-500 dataset. Specifically, relative time is calculated as the time taken by rStar or RAP, relative to the time taken by Table 4: Comparison of relative time and accuracy for different methods. Method Rel. Time () Accuracy () DYNAACT rStar RAP 1.00 0.95 1.12 61.00 54.20 51.60 8 DYNAACT. We set the action space size (denoted as m) to 5 and the number of rollouts to 16 for all methods. Table 4 presents the results, including accuracy on MATH-500 to examine the latency-accuracy trade-off. The results show that while DYNAACT incurs slight increase in latency compared to rStar, it achieves substantial improvements in accuracy. When compared to RAP, our method demonstrates lower latency, which can be attributed to the fact that the most computationally intensive operation in constructing the action space At is the encoding of e(st), with e(a) precomputed and cached for subsequent use. This contrasts with RAP, where sub-questions are generated in real-time, leading to higher computational overhead. Additionally, the submodular function can be computed efficiently using an approximate algorithm with linear complexity, further contributing to the reduced latency of our approach."
        },
        {
            "title": "6 Related Work",
            "content": "6.1 LLM Reasoning The exploration of LLMs reasoning capabilities has emerged alongside studies of prompting strategies, among which Wei et al. [2022] demonstrated that simple prompt can elicit chain-of-thought reasoning in LLMs; Zhou et al. leveraged prompts to guide LLMs in breaking down complex problems into simpler ones; and Shinn et al. [2024] enabled LLMs to recognize their mistakes and perform self-correction. Soon after, the research on reasoning shifted focus from prompting to data curation [Zhao et al., 2025b,a] and learning methods [Zelikman et al., 2022, 2024], whereby the community witnessed rapid progress in tackling complex problems such as mathematics [Yu et al., 2023, Gou et al., 2023, Mitra et al., 2024, Toshniwal et al., 2024], coding [Luo et al., 2024], visual comprehension [Hu et al., 2024], and decision-making [Chen et al., 2023]. Recently, the success of OpenAI o1 [Jaech et al., 2024] and DeepSeek r1 [Guo et al., 2025] has catalyzed the rise of test-time scaling [Snell et al., 2024], where increased computation at inference enables LLMs to engage in long-term reasoning and achieve significant improvements on Olympiad-level math [AIME-2024], challenging coding benchmarks [Jain et al., 2024], and graduate-level QA tasks [Rein et al., 2023]. Our study contributes to test-time scaling research but takes an orthogonal approach to most existing efforts. Rather than focusing on data curation [Muennighoff et al., 2025, Guan et al., 2025] or reinforcement learning [Guo et al., 2025, Qi et al., 2024], we aim to construct compact yet effective action space to facilitate MDP-based reasoning. 6.2 Submodular Optimization Submodular optimization [Fujishige, 2005] has been successfully used in many applications, including model interpretability [Elenberg et al., 2017, Chen et al., 2018, 2024] and computer vision [Pervez et al., 2023]. For instance, Elenberg et al. [2017] applied submodular functions to model interpretability, framing it as combinatorial maximization problem for efficient model explanations. Similarly, Chen et al. [2018] used submodular functions for instance-wise feature selection to explain deep learning decisions. Pervez et al. [2023] introduced conditional Poisson sampling to select key features for image and text recognition, focusing on improving recognition accuracy. More recently, Chen et al. [2024] applied submodular subset selection to deep model attribution, enhancing interpretability by identifying critical regions and reducing misattribution. Our work applies submodular optimization to action space selection in sequential reasoning tasks. Unlike prior methods focused on feature selection and model attribution, our approach optimizes action selection for enhanced compactness and scalability, offering novel method that improves performance in complex problem-solving."
        },
        {
            "title": "7 Conclusion",
            "content": "We propose DYNAACT, novel approach for automatically constructing compact action spaces to enhance sequential reasoning in complex problem-solving tasks. DYNAACT incorporates submodular function that optimizes action selection based on both utility and diversity, leading to improved inference efficiency and performance. Extensive experiments across six standard benchmarks demonstrate that DYNAACT not only outperforms existing methods but also maintains efficient inference without introducing substantial latency. These results underscore the effectiveness and versatility of our approach in tackling wide range of problem solving challenges."
        },
        {
            "title": "Acknowledgements",
            "content": "We extend our gratitude to the HKU NLP group and the anonymous reviewers for their invaluable suggestions, which significantly enhanced this work. This work was supported in part by the joint research scheme of the National Natural Science Foundation of China (NSFC) and the Research Grants Council (RGC) under grant number N_HKU714/21, and by the Ant Group Research Intern Program."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. AIME-2024. https://huggingface.co/datasets/ai-mo/aimo-validation-aime. Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. Fireact: Toward language agent fine-tuning. arXiv preprint arXiv:2310.05915, 2023. Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An informationtheoretic perspective on model interpretation. In International conference on machine learning, pages 883892. PMLR, 2018. Ruoyu Chen, Hua Zhang, Siyuan Liang, Jingzhi Li, and Xiaochun Cao. Less is more: Fewer interpretable region via submodular subset selection. arXiv preprint arXiv:2402.09164, 2024. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Ethan Elenberg, Alexandros Dimakis, Moran Feldman, and Amin Karbasi. Streaming weak submodularity: Interpreting neural networks on the fly. Advances in Neural Information Processing Systems, 30, 2017. Satoru Fujishige. Submodular functions and optimization. Elsevier, 2005. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, and Mao Yang. rstar-math: Small llms can master math reasoning with self-evolved deep thinking. arXiv preprint arXiv:2501.04519, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, and Ariel Fuxman. Visual program distillation: Distilling tools and programmatic reasoning into vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95909601, 2024. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Suraj Kothawade, Saikat Ghosh, Sumit Shekhar, Yu Xiang, and Rishabh Iyer. Talisman: targeted active learning for object detection with rare classes and slices using submodular mutual information. In European Conference on Computer Vision, pages 116. Springer, 2022. Ariel Lee, Cole Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement of llms. arXiv preprint arXiv:2308.07317, 2023. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with evol-instruct. In The Twelfth International Conference on Learning Representations, 2024. Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. George Nemhauser, Laurence Wolsey, and Marshall Fisher. An analysis of approximations for maximizing submodular set functionsi. Mathematical programming, 14:265294, 1978. Adeel Pervez, Phillip Lippe, and Efstratios Gavves. Scalable subset sampling with neural conditional poisson networks. In The Eleventh International Conference on Learning Representations, 2023. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195, 2024. Siddharth Reddy, Anca Dragan, and Sergey Levine. Sqil: Imitation learning via reinforcement learning with sparse rewards. arXiv preprint arXiv:1905.11108, 2019. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. David Silver, Aja Huang, Chris Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484489, 2016. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. Planning in natural language improves llm search for code generation. arXiv preprint arXiv:2409.03733, 2024a. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024b. Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279292, 1992. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024. Xueliang Zhao, Xinting Huang, Tingchen Fu, Qintong Li, Shansan Gong, Lemao Liu, Wei Bi, and Lingpeng Kong. Bba: Bi-modal behavioral alignment for reasoning with large vision-language models. arXiv preprint arXiv:2402.13577, 2024. Xueliang Zhao, Wei Wu, Jian Guan, Zhuocheng Gong, and Lingpeng Kong. Promptcot 2.0: Scaling prompt synthesis for large language model reasoning. arXiv preprint arXiv:2509.19894, 2025a. Xueliang Zhao, Wei Wu, Jian Guan, and Lingpeng Kong. Promptcot: Synthesizing olympiad-level problems for mathematical reasoning in large language models. arXiv preprint arXiv:2503.02324, 2025b. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. In The Eleventh International Conference on Learning Representations."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The abstract and introduction accurately reflect the papers contributions and scope. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Details are discussed in the Limitations section. Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] Justification: Details are included in Appendix E. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Details are discussed in Section 4.4. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? 14 Answer: [Yes] Justification: We have submitted the source code to facilitate the reproduction of our results. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Details are discussed in Section 4.4. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We do not report error bars due to the substantial computational cost associated with repeated runs. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Details are discussed in Section 5.4. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, the research adheres fully to the NeurIPS Code of Ethics. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Details are discussed in the Broader Impacts section. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. 16 The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not involve the release of data or models with high risk for misuse. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Details are discussed in Section 4.4. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. 17 If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: The answer NA means that the paper does not release new assets. Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: The core method does not rely on LLMs for any essential components. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described."
        },
        {
            "title": "A Limitations",
            "content": "While DYNAACT represents significant advancement in action space construction for reasoning tasks, there are several limitations that should be addressed in future work to fully realize its potential: (1) DYNAACT demonstrates strong performance across general, reasoning, and math tasks. However, its reliance on MCTS for selecting specific actions from the constructed action space can be computationally intensive, especially for large or complex tasks. Alternative test-time scaling methods, such as beam search or other exploration strategies, could be explored to further optimize the balance between exploration and exploitation, improving efficiency without sacrificing reasoning accuracy. Expanding the models ability to scale across different search algorithms remains promising direction for future work. (2) While DYNAACT improves reasoning, it still depends on pre-trained Llama models, which may face challenges when handling extremely large or highly specialized action spaces. Exploring methods to optimize the models scalability, such as combining DYNAACT with stronger backbone model, could help tackle this limitation in future research."
        },
        {
            "title": "B Broader Impacts",
            "content": "This work proposes framework for dynamic action space construction to improve sequential reasoning with language models. By enabling more structured, efficient, and interpretable decisionmaking, our approach has the potential to benefit applications such as educational tutoring systems, automated theorem proving, and scientific problem-solvingdomains where reasoning efficiency and transparency are critical. The ability to distill compact, high-utility action spaces may also reduce reliance on brute-force search and large-scale inference, contributing to more sustainable and accessible AI systems. At the same time, these benefits raise important concerns. Automatically constructed action spaces are learned from data and may reflect biases present in the underlying corpora, potentially reinforcing harmful patterns during reasoning. Furthermore, as our method enhances the ability of LLMs to perform multi-step reasoning, it may be misused to generate misleading arguments or automate complex forms of manipulation. While the explicit structure introduced by our framework offers interpretability benefits, future work should explore mechanisms for auditing, constraint enforcement, and safe deployment in sensitive domains."
        },
        {
            "title": "C Details of the MCTS Process",
            "content": "In this appendix, we provide detailed formal description of the Monte Carlo Tree Search (MCTS) procedure used to estimate the value function Q(s, a) for candidate actions. The MCTS process comprises four stages: selection, expansion, simulation, and backpropagation. For simplicity, let the current state be denoted by s, and let the set of candidate actions from state be represented as A(s). Selection Starting from the root node corresponding to the current state s, the selection phase traverses the tree by recursively choosing actions based on balance between exploitation and exploration. Each node in the search tree is associated with two statistics: and, for each action A(s), (s) (the number of visits to state s) (s, a) Q(s, a) is the number of times that action has been selected from state s, is the estimated average reward for taking action from state s. The action is selected at state according to the Upper Confidence Bound for Trees (UCT) formula: (cid:34) (cid:115) = arg max aA(s) Q(s, a) + (cid:35) , ln (s) (s, a) 20 where is an exploration constant. This selection continues recursively until node is reached that is either terminal state or not fully expanded (i.e., there exists an A(s) such that the corresponding child node has not yet been created). Expansion Upon reaching node that is not terminal and has unexplored actions, the expansion phase selects one such action A(s) for which no child node exists. new child node is then created to represent the state resulting from taking action from s. The statistics for the new edge (s, a) are initialized as: (s, a) = 0, Q(s, a) = 0."
        },
        {
            "title": "Simulation",
            "content": "From the newly expanded node s, simulation (or rollout) is conducted to estimate the value of the state-action pair (s, a). The simulation proceeds by following default policyoften random or heuristic strategyto generate complete reasoning trajectory until terminal state is reached. Let denote the reward obtained at the terminal state; this reward serves as an estimate for the value of taking action from state s. Backpropagation Once the simulation concludes with reward r, the backpropagation phase updates the statistics along the path from the expanded node back to the root. For every state and action along this path, the updates are performed as follows: (s, a) (s, a) + 1, Q(s, a) Q(s, a) + Q(s, a) (s, a) . Additionally, the visit count for each state along the path is updated: (s) (s) + 1. These updates refine the estimates Q(s, a) based on the observed reward r, thereby improving the accuracy of the value function with successive simulations. The MCTS process iterates through these stepsselection, expansion, simulation, and backpropagationuntil specified computational budget is reached. Prompt for Universal Problem-Solving Sketch Extraction Below is the prompt used for extracting universal problem-solving sketch from set of problems. This prompt is designed to guide the extraction process so that the generated subgoals are broadly applicable, capture core actions, and can be flexibly applied across various problem domains. 21 Problem-Solving Sketch Prompt Imagine you are problem-solving expert tasked with creating universal problem-solving sketch. You will be shown the following {number of problems} problems: {problem_text} From these problems, extract up to {n} essential subgoals that form universal sketch. The subgoals should: Apply broadly across different types of problems and disciplines Use casual, everyday language from the first person perspective Avoid sequential markers like \"first\", \"next\", \"then\", \"finally\" Focus on the core action or insight needed at each stage Be as creative as possible, going beyond what you think is intuitively correct Format your response as numbered list, where each item expresses one subgoal. Make each subgoal self-contained so it can be applied flexibly rather than in fixed sequence. This prompt ensures that the extracted subgoals capture the essential observations needed for constructing the proxy action space, and that they are effective for wide range of problem-solving scenarios. Proof of Submodularity of (X; st) Proof. Let = A. set function : 2V is submodular if for every and for every , (X {x}) (X) (Y {x}) (Y ). We write where and (X; st) = α F1(X) + β F2(X), F1(X) = log exp (cid:0)e(st)T e(a)(cid:1) (cid:33) , (cid:32) (cid:88) aX F2(X) = (cid:88) aX mX (a), with mX (a) = min bX{a} (cid:0)1 e(a)T e(b)(cid:1) . (i) Submodularity of F1: Define g(X) = (cid:88) aX exp (cid:0)e(st)T e(a)(cid:1). Note that for any X, g(X {x}) = g(X) + exp (cid:0)e(st)T e(x)(cid:1). Thus, for and , we have: F1(X {x}) F1(X) = log 1 + (cid:32) and F1(Y {x}) F1(Y ) = log 1 + (cid:32) Since implies g(X) g(Y ), it follows that exp (cid:0)e(st)T e(x)(cid:1) g(X) (cid:33) exp (cid:0)e(st)T e(x)(cid:1) g(Y ) (cid:33) , . exp (cid:0)e(st)T e(x)(cid:1) g(X) exp (cid:0)e(st)T e(x)(cid:1) g(Y ) , 22 and because log(1 + z) is an increasing function for 0, we obtain F1(X {x}) F1(X) F1(Y {x}) F1(Y ). Thus, F1 is submodular. (ii) Submodularity of F2: For any and X, the marginal gain for F2 is given by F2(x X) F2(X {x}) F2(X). For each X, the value mX (a) = minbX{a} (cid:0)1 e(a)T e(b)(cid:1) updates upon addition of to mX{x}(a) = min mX (a), (cid:0)1 e(a)T e(x)(cid:1) (cid:111) (cid:110) . Similarly, for the newly added element x, mX{x}(x) = min aX (cid:0)1 e(x)T e(a)(cid:1) . Hence, F2(x X) = (cid:88) (cid:104) (cid:105) min{mX (a), (cid:0)1 e(a)T e(x)(cid:1)} mX (a) aX Let us denote, for each X, (cid:0)1 e(x)T e(a)(cid:1) . + min aX Then, δX (a, x) = min{mX (a), (cid:0)1 e(a)T e(x)(cid:1)} mX (a). F2(x X) = (cid:88) aX δX (a, x) + min aX (cid:0)1 e(x)T e(a)(cid:1) . Now, consider and . For any X, since we have mY (a) = min (cid:110) mX (a), min bY (cid:0)1 e(a)T e(b)(cid:1) (cid:111) mX (a). Thus, δY (a, x) = min{mY (a), (cid:0)1 e(a)T e(x)(cid:1)}mY (a) min{mX (a), (cid:0)1 e(a)T e(x)(cid:1)}mX (a) = δX (a, x). Also, Therefore, min aY (cid:0)1 e(x)T e(a)(cid:1) min (cid:0)1 e(x)T e(a)(cid:1) . aX F2(x X) = F2 (x ) = (cid:88) aX (cid:88) aY δX (a, x) + min aX (cid:0)1 e(x)T e(a)(cid:1) , δY (a, x) + min aY (cid:0)1 e(x)T e(a)(cid:1) . Since contains all elements of and possibly additional elements with non-positive marginal increments (i.e., δY (a, x) 0 for X), it follows that (cid:88) aX δX (a, x) (cid:88) aX δY (a, x) and min aX (cid:0)1 e(x)T e(a)(cid:1) min aY (cid:0)1 e(x)T e(a)(cid:1) . Moreover, the additional terms from in F2(x ) further decrease the total marginal gain. Thus, we obtain F2(x X) F2(x ). Hence, F2 is submodular. (iii) Combination: Since (X; st) = α F1(X) + β F2(X) with α, β 0, it follows that for all and for all : (X {x}; st) (X; st) = α F1(x X) + β F2(x X), α F1(x ) + β F2(x ) = (Y {x}; st) (Y ; st). Thus, (X; st) is submodular. 23 Table 5: Evaluation results on the Level 3, Level 4, and Level 5 subsets of MATH-500. Numbers in bold denote the best performance. Level 3 Level 4 Level 5 rStar DYNAACT - util - q-learning 72.38 76.19 68.57 71.43 50.78 58.59 52.34 53.13 15.67 31.34 17.16 20."
        },
        {
            "title": "F Additional Experimental Results and Analysis",
            "content": "F.1 Reasoning Performance Across Difficulty for RQ5 We evaluate the effectiveness of both the utility term and the Q-learning objective using the MATH500 dataset, which includes problems categorized by difficulty levels. This allows us to assess how each component impacts problem-solving performance across tasks of varying complexity. As shown in Table 5, the removal of the utility term leads to more significant performance drop on harder problems (Level 5) compared to easier ones (Level 3). This indicates that the utility term is crucial for selecting actions that meaningfully contribute to the solution process, particularly for complex problems. In comparison, removing the Q-learning objective results in slight performance drop. Without Q-learning, the embedding function e() still selects actions relevant to the current state, but it fails to capture the long-term utility of those actions. As result, while the model continues to choose relevant actions, the lack of Q-learning limits its ability to optimize the effectiveness of its actions over time. We further compare our method with rStar, which demonstrates better performance than other baselines on MATH-500. However, due to rStars manually defined action space, it faces challenges in scaling to more complex problems. As shown in the results for Level 5 problems in Table 5, rStars performance drops significantly to 15.67%, whereas our method achieves higher accuracy of 31.34%. This highlights the scalability advantage of our method. F.2 Action Diversity Analysis for RQ6 Table 6: Evaluation results showing the impact of the diversity term. Model Diversity Accuracy Ours - div 0.73 0.49 31.34 24. To evaluate the impact of the diversity term, we follow Wang et al. [2024a] and calculate the diversity score of candidate action set by measuring how dissimilar the actions are within the set. Specifically, the diversity score is determined by computing the ratio of dissimilar pairs of actions to the total number of possible action pairs in the set. This ratio is then averaged over all candidate action sets. We test on the Level 5 subset of the MATH-500 dataset, which consists of more complex problems that are sensitive to redundancy in the selected actions. The results are shown in Table 6. We observe that removing the diversity term results in significant drop in the diversity score, which subsequently leads to decrease in accuracy. F.3 Additional Analysis on Diversity Term in Submodular Function To assess the sensitivity of DYNAACT to the choice of the diversity term fdiv in Eq. 4, we conducted additional experiments using two alternative diversity metrics: Mean Pairwise Distance and Mean Cosine Distance. These alternatives were chosen to evaluate whether simpler diversity formulations could yield comparable or improved performance. As shown in Table 7, both alternative diversity metrics lead to performance drop compared to our original formulation. We attribute this to the fact that these metrics do not preserve the submodular property, which is central to the efficiency and theoretical guarantees of our greedy subset selection algorithm. 24 Table 7: Performance comparison using different diversity terms."
        },
        {
            "title": "Method",
            "content": "Accuracy (MATH-500)"
        },
        {
            "title": "Mean Pairwise Distance\nMean Cosine Distance\nDYNAACT",
            "content": "57.80 58.20 61.00 F.4 Resource Consumption Analysis While 5.4 presents relative latency comparison, we include here the raw inference time per example to offer more complete view of resource consumption. All measurements were taken on an 8A100 GPU machine using the MATH-500 dataset, with each method evaluated under consistent rollout settings. Table 8: Per-example raw inference time and accuracy on MATH-500 across methods."
        },
        {
            "title": "Method",
            "content": "Raw Time () Accuracy () Zero-shot CoT SC@maj16 RAP rStar DYNAACT 1.68s 26.88s 64.51s 54.72s 57.60s 45.40 52.00 51.60 54.20 61.00 As shown in Table 8, DYNAACT requires comparable runtime to other MCTS-based baselines such as RAP and rStar, while yielding significantly higher accuracy. Although MCTS-based approaches naturally incur more latency than single-pass generation methods like Zero-shot CoT, they also enable more effective reasoning. Since our method is orthogonal to the choice of search algorithm, future work could explore integration with more efficient test-time strategies (e.g., beam search or sample-efficient MCTS variants) to further reduce resource consumption while preserving accuracy. F.5 Comparison with Non-Submodular Selection Methods To further understand the benefits of our submodular formulation, we compare DYNAACT against RL-based pruning baseline. In this baseline, action space truncation is performed by selecting the top 5 candidate actions based solely on Q-value estimates, without considering submodular diversity or joint utility. Table 9: Comparison of DYNAACT with an RL-based pruning baseline on the MATH-500 dataset. Method Raw Time () Accuracy () RL-based Pruning DYNAACT 56.89s 57.60s 53.20 61.00 As shown in Table 9, while the RL-based pruning method achieves slightly lower inference latency, it significantly underperforms in accuracy compared to DYNAACT. This gap highlights the limitations of greedy RL-based pruning, which may select redundant or suboptimal actions. In contrast, our submodular approach explicitly optimizes for both utility and diversity, yielding more compact yet expressive action set. F.6 Scalability with Large Proxy Action Spaces To assess the scalability of DYNAACT to large-scale corpora, we varied the size of the proxy action space from 40, 000 to 1, 000, 000 entries, while keeping the candidate set size = 5 fixed. All experiments were conducted on the MATH-500 dataset. As shown in Table 10, the inference latency increases moderately with larger proxy spacesrising by approximately 18 seconds when scaling from 40k to 1M actions. Importantly, performance remains Table 10: Scalability analysis of DYNAACT with varying proxy action space sizes on MATH-500. Proxy Action Space Size Raw Time () Accuracy () 40k 200k 400k 600k 800k 1M 57.60s 60.52s 63.99s 67.93s 71.47s 75.84s 61.0 61.8 62.0 62.0 61.6 61.8 stable or slightly improves, indicating that DYNAACT effectively handles large candidate pools without significant degradation in efficiency. This scalability is largely attributed to the caching of action embeddings and the linear-time greedy algorithm used in submodular optimization. F.7 Empirical Study on the Utility-Diversity Trade-off To better understand how the balancing parameters α and β in Eq. 2 affect the trade-off between utility and diversity in the submodular function, we conducted series of experiments on the MATH-500 dataset. We varied the relative weighting of the utility term futil and the diversity term fdiv, while keeping their sum fixed (α + β = 1). α, β Accuracy (MATH-500) (0.9, 0.1) (0.7, 0.3) (0.5, 0.5) (0.3, 0.7) (0.1, 0.9) 61.00 60.80 55.40 54.60 54. Table 11: Effect of varying α and β on model performance. As shown in Table 11, performance is highly sensitive to the relative weighting of the utility term. Accuracy drops substantially when utility and diversity are given equal weight or when diversity dominates. However, once the utility coefficient α exceeds 0.7, the model achieves strong and stable performance. These findings suggest that while diversity contributes to more robust action space, prioritizing utility is essential for effective reasoning in complex problem-solving tasks. F.8 Comparison with Few-Shot and Fine-Tuned Baselines To assess whether the performance of DYNAACT could be attributed to exemplar-based prompting or supervised adaptation, we compare it with several baselines involving few-shot prompting and fine-tuning. The few-shot baseline retrieves the top 3 most similar examples from Open-Platypus using cosine similarity over embeddings from Llama-3.2-1B-Instruct, and provides them as context to Llama-3.1-8B-Instruct. The fine-tuned baseline directly trains Llama-3.1-8B-Instruct on the same corpus for 3 epochs with learning rate of 1 105. For both approaches, we also evaluate their self-consistency variants (SC@maj16), where 16 reasoning paths are sampled and majority voting is used for answer selection. Table 12: Comparison of DYNAACT with few-shot and fine-tuned baselines across six benchmarks. Method MMLU MMLU-Pro GPQA ARC-C GSM8K MATH-500 Zero-shot CoT Few-shot baseline Fine-tuned baseline Few-shot SC@maj16 Fine-tuned SC@maj16 DYNAACT 68.87 68.94 68.72 69.80 69.89 70.22 43.45 43.23 43.56 44.22 45.89 51.40 31.82 29.80 34.34 35.86 34.85 39.39 81.06 81.83 82.08 84.73 86.18 88. 76.12 76.65 78.17 84.00 84.38 89.16 45.40 46.60 48.20 50.20 52.60 61.00 26 As shown in Table 12, both few-shot prompting and fine-tuning lead to modest improvements over the zero-shot baseline. However, DYNAACT consistently outperforms all baselines across tasks, demonstrating that its gains stem from principled, dynamic action space construction rather than static exemplar use or model adaptation alone. F.9 Case Study To further demonstrate the effectiveness of DYNAACT, we present series of case studies in Figures 3 - 5, each consisting of problem and its corresponding predicted solution. For each step in the solution, the process begins with the selection of an action (chosen using Q(st, a), as described in 2.1), followed by the corresponding reasoning step. These examples demonstrate how DYNAACT enhances the effectiveness of the reasoning process. Problem: Compute: 1 2 + 3 4 + 5 + 99 100 Solution: Step 1: clarify the problem by identifying the key elements, constraints, and what Im trying to find or achieve, and rephrase it in my own words to ensure understand it correctly. We need to compute the sum of an alternating series where each term increases by 1, starting from 1 and ending at 100. Step 2: should look for connections, patterns, or relationships between the components, and consider how they interact or influence each other, whether its in mathematical, logical, or contextual sense. The series alternates between addition and subtraction, which suggests pattern where each pair of consecutive terms (one positive and one negative) could be combined to simplify the calculation. Step 3: Im searching for patterns, connections, and relationships between the different parts, and thinking about how they might help me solve the problem. We can pair the terms as follows: (1 2) + (3 4) + (5 6) + + (99 100). Each pair sums to 1. Step 4: evaluate my progress, checking if my approach is working and adjusting it as needed to ensure Im on the right track. There are 50 pairs in the series, each summing to 1. Step 5: break down complex problems into smaller, manageable parts, focusing on one aspect at time, and identifying the essential variables, assumptions, and constraints that will help me make progress, which might involve creating diagram, setting up equations, or developing step-by-step plan. The sum of the series is the sum of the 50 pairs, which is 50 (1). Step 6: Complete the solution and present the final answer within . 50 Figure 3: Case study: solution to alternating series sum. 27 Problem: The Greek army contained two types of soldiers: the upper class and the lower class soldiers. If there were total of 5 upper class soldiers, and 10 lower class soldiers in certain part of Athens, and the battle of Thermopylae demands force of 4 upper class soldiers and 8 lower class soldiers, how many different battalions can be sent? Solution: Step 1: clarify the problem by identifying the key elements, unknowns, and goals, and try to rephrase it in my own words to ensure understand whats being asked. We need to find out how many different groups (battalions) can be formed from mix of 5 upper class soldiers and 10 lower class soldiers, where each group must consist of 4 upper class soldiers and 8 lower class soldiers. Step 2: look for opportunities to use analogies, metaphors, or abstractions to reframe the problem or make it more tractable, and think about how can leverage my existing knowledge or experience to inform my solution. This problem can be approached by considering it as combinatorics problem, specifically involving combinations since the order of selection does not matter. Step 3: consider the implications and potential consequences of my solution, and think about how can test, validate, or refine it further to ensure its correct, complete, and relevant to the original problem. To form battalion, we need 4 upper class soldiers out of 5 and 8 lower class soldiers out of 10. The number n! of ways to choose 4 upper class soldiers from 5 is given by the combination formula C(n, k) = k!(nk)! , where is the total number of items, and is the number of items to choose. For the upper class, this becomes C(5, 4). Similarly, for the lower class, its C(10, 8). Step 4: consider how the problem might be transformed or re-expressed in way that makes it more tractable can reframe it in simpler terms, or find an analogous problem thats easier to solve, or identify key insight that unlocks more straightforward path to the solution? The formula for combinations is C(n, k) = For the lower class, this is C(10, 8) = 10! k!(nk)! . For the upper class, this is C(5, 4) = 5! 4!(54)! = 5. n! 8!(108)! = 45. Step 5: Are there any constraints, assumptions, or rules that need to take into account when exploring solutions or possibilities? Since the selection of upper class soldiers and lower class soldiers is independent of each other, the total number of different battalions that can be sent is the product of the number of ways to choose the upper class soldiers and the number of ways to choose the lower class soldiers. Step 6: Once think have solution, Ill consider its implications and potential limitations. Ill ask myself questions like What are the potential consequences of this solution?, Are there any potential biases or errors in my thinking?, and How might this solution impact different stakeholders or scenarios? The total number of battalions is the product of the number of ways to choose the upper class and the lower class soldiers, which is 5 45 = 225. Step 7: Look for patterns, symmetries, and analogies that can help me understand the problem more deeply and make connections to other areas of knowledge or experience are there any clever ways to reframe, reorganize, or reinterpret the problem that could reveal new insights or opportunities? The solution is straightforward and based on the combination formula. Step 8: Complete the solution and present the final answer within . Figure 4: Case study: solution to Greek army battalion formation problem. 225 28 Problem: hexagon is inscribed in circle: What is the measure of α, in degrees? Solution: Step 1: need to identify the underlying structure or pattern in the problem, whether its mathematical relationship, logical argument, or conceptual framework. The problem involves an inscribed hexagon in circle, with specific angles labeled. The goal is to find the measure of angle α. Step 2: will decide on suitable representation or model for the problem, and determine the best approach to explore the solution space, whether its through calculation, simulation, or other means. To find the measure of α, can use the fact that the sum of the interior angles of hexagon is 720. Additionally, since the hexagon is inscribed in circle, the measure of each central angle formed by two adjacent vertices of the hexagon is equal to the measure of the corresponding inscribed angle. Step 3: evaluate my progress, checking for errors, inconsistencies, and areas for improvement, and adapt my approach as needed to ensure that Im making progress toward solution that is accurate, complete, and relevant. Since the sum of the interior angles of hexagon is 720, and the sum of the measures of the given angles is 105 + 110 = 215, the measure of angle α can be found by subtracting the sum of the given angles from the total sum of the interior angles of hexagon. Step 4: break down complex problems into smaller, manageable parts, focusing on one aspect at time, and identifying the essential variables, assumptions, and constraints that will help me make progress, which might involve creating diagram, setting up equations, or developing step-by-step plan. Lets denote the measure of angle α as x. Since the sum of the interior angles of hexagon is 720, we have the equation + 215 = 720. Step 5: Are there any constraints, assumptions, or rules that need to take into account when exploring solutions or possibilities? The problem does not provide any additional constraints or assumptions beyond the given information. Step 6: should look for potential paths forward by asking myself questions like What are the implications of this?, How does this relate to that?, What would happen if I..., and What are the boundaries or constraints of this problem? - this helps me generate possibilities and explore different avenues for solving the problem. Now, lets solve the equation + 215 = 720 for x. Step 7: break down complex problems into smaller, manageable parts, focusing on one aspect at time, and identifying the essential variables, assumptions, and constraints that will help me make progress, which might involve creating diagram, setting up equations, or developing step-by-step plan. To solve for x, we can subtract 215 from both sides of the equation: = 720 215. Step 8: Complete the solution and present the final answer within = 720 215 = 505. However, since the measure of an angle cannot exceed 180, we need to subtract 360 from 505 to get the correct measure of angle α. Therefore, α = 505 360 = 145. . 145 Figure 5: Case study: solution to inscribed hexagon angle problem."
        }
    ],
    "affiliations": [
        "Ant Group",
        "The University of Hong Kong"
    ]
}