{
    "paper_title": "Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas",
    "authors": [
        "Christoffer Koo Øhrstrøm",
        "Rafael I. Cabral Muchacho",
        "Yifei Dong",
        "Filippos Moumtzidellis",
        "Ronja Güldenring",
        "Florian T. Pokorny",
        "Lazaros Nalpantidis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding."
        },
        {
            "title": "Start",
            "content": "Where to Attend: Principled Vision-Centric Position Encoding with Parabolas Christoffer Koo Øhrstrøm 1 Rafael I. Cabral Muchacho 2 Yifei Dong 2 Filippos Moumtzidellis 1 Ronja uldenring 1 Florian T. Pokorny 2 Lazaros Nalpantidis 1 6 2 0 2 1 ] . [ 1 8 1 4 1 0 . 2 0 6 2 : r Figure 1. Parabolic Position Encoding (PaPE). (a) PaPE encodes positions using an attention bias based on sum of learnable parabolas with the relative position between tokens as the dependent variable. (b) Our experiments show that PaPE is generaleither PaPE or PaPE-RI outperforms all baselines on 7 out of 8 datasets that span 4 vision modalities. (c) PaPE has remarkable classification extrapolation, showing high robustness beyond the training resolution of 2242."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction & Related Works We propose Parabolic Position Encoding (PaPE), parabola-based position encoding for vision modalities in attention-based architectures. Given set of vision tokenssuch as images, point clouds, videos, or event camera streamsour objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nDstructures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from translation invariance, rotation inprior work: variance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at this URL. 1Technical University of Denmark 2KTH Royal Institute of Technology, Sweden. Correspondence to: Christoffer Koo Øhrstrøm <chkoo@dtu.dk>. Preprint. February 3, 2026. 1 In this work, we propose position encoding that is designed specifically for vision modalities. Transformers (Vaswani et al., 2017) are widely used for computer vision and robotics tasks. They have proven to be highly flexible, finding applications in several visionbased modalities such as images (Dosovitskiy et al., 2020), point clouds (Wu et al., 2024), videos (Bertasius et al., 2021; Arnab et al., 2021), and event cameras (Sabater et al., 2022; Gehrig & Scaramuzza, 2023). Despite their success in the vision domain, they tend to use position encodings that are first designed for language (Vaswani et al., 2017; Press et al., 2021; Su et al., 2024) and later adapted for vision (Wang & Liu, 2021; Fuller et al., 2023; Schenck et al., 2025). Although adaptations can account for vision-specific characteristicse.g., directionality (Heo et al., 2024; Fuller et al., 2024))we hypothesize that they are incomplete in their coverage of those, because some characteristics matter little in language. This leads us to identify relevant ideas in prior works and use these as guiding design principles for our proposed position encoding. Absolute position encodings. Vaswani et al. (2017) propose sinusoidal position embeddings that are added directly to the input embeddings. These are extended from 1D to 2D by Wang & Liu (2021) by using half of the position vector for one position dimension and the other half for the second position dimension. The extension to arbitrary dimensions Where to Attend: Principled Vision-Centric Position Encoding with Parabolas follows naturally from this. While sinusoidal position encoding handles multiple positional dimensions, its absolute nature lacks properties that RoPE-based and attention bias methods possess. However, it provides the exact location of each token, which can be relevant for some tasks, as we observe in Section 5.2. RoPE-based methods. RoPE (Su et al., 2024) stands out as state-of-the-art position encoding. It rotates queries and keys by product of their position and set of fixed frequencies, so the query-key dot product encodes the relative position, yielding translation invariance. One of the properties of RoPE is that attention strength diminishes as tokens move farther apart, property we denote as distance decay. While vanilla RoPE operates purely in 1D, Axial RoPE (Chu et al., 2024) generalizes RoPE to arbitrary dimensionality by assigning disjoint subsets of the query and key vectors to each positional dimension and applying RoPE separately to each. However, because these dimensions are treated independently, Axial RoPE cannot naturally capture diagonal interactions. RoPE-Mixed (Heo et al., 2024), along with related approaches (Ostmeier et al., 2025; Yu et al., 2025; Schenck et al., 2025), introduces distinct learnable frequencies per positional dimension to imbue directionality into RoPE. More recently, context-aware variants of RoPE (Veisi et al., 2025; Wang et al., 2025) have shown that letting frequencies depend on each token improves the ability of language models to generalize to sequence lengths beyond what is seen during training. Attention biases modify attention scores by adding bias term directly into the attention matrix before the softmax. This bias depends on the relative positions of the query and key, reinforcing the importance of translation invariance. However, these bias terms often require materializing the full attention matrix, making them incompatible with efficient attention kernels (Dao et al., 2022; Dao, 2024). ALiBI (Press et al., 2021), designed for autoregressive language models, subtracts multiple of the relative distance between the tokens. This simple mechanism enforces both translation invariance and distance decay of attention scores. 2D-ALiBi (Fuller et al., 2023) generalizes this idea to images by subtracting multiple of the 2D Euclidean distance between positions, further endowing the encoding with rotation invariance. LookHere (Fuller et al., 2024) extends 2D-ALiBi with directionality by restricting the field of view of each head to different directions. This adds directionality, but at the cost of losing rotation invariance. From the prior works, we have identified the following set of guiding design principles that we consider to be important for position encoding of vision modalities, and elaborate on in Section 3: translation invariance, rotation invariance, distance decay, directionality, and context awareness. To the best of our knowledge, no existing position encoding is derived from all five principles. We use the principles to propose novel position encoding: Parabolic Position Encoding (PaPE). PaPE treats the relative position between two tokens as the dependent variable in sum of parabolas and uses this to encode positions. Concretely, we incorporate quadratic term (capturing distance) and linear term (capturing direction) into the attention logits. Therefore, PaPE falls under attention bias methods; however, its formulation supports separate query-key transformations, which means PaPE does not have to materialize the full attention matrix. In addition, we propose rotation invariant version PaPE-RI. We summarize our contributions as follows: 1 Parabolic Position Encoding. We propose PaPE and PaPE-RI: parabola-based position encodings for vision modalities that are designed from the compiled principles. 2 Compatibility with efficient attention kernels. PaPE uses query-key transformationssimilar to RoPE (Su et al., 2024)to be compatible with efficient attention kernels (Dao et al., 2022; Dao, 2024). 3 High Generality. We evaluate PaPE on 8 datasets across 4 vision modalities (images, point clouds, videos, and event cameras) and find that PaPE is superior to the baselines in 6 of 8 datasets and PaPE-RI in 1 of 8. 4 Strong Classification Extrapolation. PaPE is found to extrapolate significantly better beyond its training resolution in image classification than any other baseline considered. 2. Preliminaries Attention (Vaswani et al., 2017) for collection of tokens = {x1, , xn} with xi Rd is given by = softmax (cid:18) (cid:19) V, (1) where queries, keys, and values are derived through learnable matrices Wq, Wk, Wv Rhd, so that qi = Wq xi, ki = Wk xi, and vi = Wv xi. For clarity, we omit attention heads here and in the next section, noting that the extension to multi-head attention is straightforward. We define = and obtain from Equation (1) that Sij = qi, kj, (2) i.e., token similarity is measured as the dot products between all query-key pairs. Since attention is permutation invariant, we require positional information to encode information about the arrangement of tokens. To this end, we associate position vector ri Rp with each token, where is the position dimensionality. For example, = 2 for images (x, y) and = 3 for point clouds (x, y, z). Our goal is to leverage these position vectors to inject positional information into the dot products. 2 Where to Attend: Principled Vision-Centric Position Encoding with Parabolas 3. Design Principles We now describe and motivate the design principles that we identified in prior works. These principles are not rigid checklist for the perfect position encoding. They are set of properties that we reasonand empirically evaluate in the experimentsare valuable in practice. We give mathematical definitions of the principles when applicable.1 Translation invariance matters because vision tasks usually depend on patterns defined by how parts relate to each other, not by where they are. cat is cat whether it appears in the top-left or bottom-right of an image. This calls for position encodings that are invariant to global translations. Definition 3.1. function is translation invariant if, (ri + t, rj + t) = (ri, rj), t, ri, rj Rp. (3) Rotation invariance is valuable in settings where the orientation of an object should not affect the prediction. For instance, classifying 3D object from point cloud should not depend on how it is rotated in space. However, we do not treat rotation invariance as universal requirement: orientation often carries key information about motion and action. We therefore consider rotation invariance as taskdependent, special-case principle. Definition 3.2. function is rotation invariant if, (Rri, Rrj) = (ri, rj), ri, rj Rp, SO(p). (4) Distance decay captures the intuition that nearby tokens should interact more strongly than distant ones. Specifically, attention between two tokens should decrease as their distance increases. This biases the position encoding towards local interactions. Directionality is the ability to modulate attention not just by how far tokens are from each other, but also by which direction. Unlike language, which is 1D and largely indifferent to geometric direction, vision is inherently directional: above, below, left, right, and diagonal often carry distinct semantic roles. As we move to higher-dimensional settings, e.g. 3D and spatio-temporal data, the amount of possible directions amplify this effect. good position encoding for vision modalities should account for these directional cues. Context awareness. Position encodings must be capable of emphasizing local neighborhoods while also enabling long-range interactions. At first glance, these appear to be conflicting goals. Context awareness reconciles the two by letting the model adapt the decay and direction strength conditioned on the content of the token. Rather than enforcing fixed decay and direction pattern, the model can decide when to emphasize local neighborhoods and when to focus on long-range interactions. 4. Parabolic Position Encoding We now introduce Parabolic Position Encoding (PaPE) and its rotation invariant version, PaPE-RI. We begin by deriving the method and then illustrate how the design principles are incorporated into PaPE. Afterwards, we derive querykey transformations that ensure compatibility with efficient attention kernels. general form 1D parabola can be written as = ax2 + bx + c. (5) Our goal is to reshape the attention score in Equation (2) into sum of such parabolas. To this end, we first define rij = Wp (rj ri), (6) where Wp Rmp is learnable projection.2 rij serves as the dependent variable of the parabolas (x = rij). We obtain the and coefficients directly from the token representation, xi, using the learnable projections Wa, Wb Rmd: ai = softplus(Wa xi) bi = Wb xi, (7) (8) where softplus followed by negation guarantees ai Rm <0, ensuring that each parabola is concave. We now present the main equation that describes PaPE, where ()2 denotes the Hadamard (element-wise) square: Sij = ai, ij + bi, rij + qi, kj. (9) Figure 2 visualizes this equation as sum of three attention maps. Expanding the dot products makes the parabolic structure explicit: the attention score decomposes into sum of general form parabolas, Sij = (cid:88) ℓ= aiℓr2 ijℓ (cid:124) (cid:123)(cid:122) (cid:125) ax2 + + biℓrijℓ (cid:124) (cid:123)(cid:122) (cid:125) bx . qi, kj (cid:124) (cid:123)(cid:122) (cid:125) (10) PaPE instantiates our design principles, as we will see next. PaPE is translation invariant. This follows directly from modeling the relative position, rij, between pairs of tokens: Wp((rj + t) (ri + t)) = Wp(rj ri), Rp. PaPE-RI: rotation invariant instantiation of PaPE. In the form given in Equation (9), PaPE is not rotation invariant. By imposing simple constraints on its parameters, we obtain 1We assume that token positions reside in standard pdimensional Euclidean space (Rp). 2While Wp is not strictly required to achieve our goal, our empirical results (see Section 5.4) show that it is beneficial. 3 Where to Attend: Principled Vision-Centric Position Encoding with Parabolas Figure 2. Overview of Parabolic Position Encoding (PaPE). PaPE decomposes attention (a) into distance (b), direction (c), and semantics (d). Using the dogs eye as the query, PaPE learns to look in bottom-right direction, while decaying attention with distance. The attention (a) is compatible with efficient attention kernels through separate query-key transformations. Colormap: rotation invariant instantiation, which we call PaPE-RI. PaPE-RI is defined by setting all bi = 0, choosing aik = αi with αi R<0, and enforcing Wp = wpIp Rpp with wp R. Under these constraints, PaPE-RI becomes provably rotation invariant; we provide the formal proof in Appendix A.1. An implication of setting bi = 0 is that PaPE-RI loses directionality. PaPE has distance decay. Since ai is constrained to be negative, it follows that the square terms penalize the distances as rij moves away from the origin. PaPE is directional. Recall that the dot product between two vectors x, is proportional to the cosine of the angle θ between them: 4.1. Compatibility with Efficient Attention Kernels The formulation in Equation (9) is not directly compatible with efficient attention kernels such as FlashAttention (Dao et al., 2022; Dao, 2024), because it explicitly depends on all pairwise relative positions. To recover compatibility, we draw inspiration from RoPE (Su et al., 2024) and inject positional information directly into the dot product through separate position-aware transformations of the query and key. These transformations must ensure that the resulting dot product exactly recover Equation (9). That is, to guarantee that fq(qi, ri), fk(kj, rj) = ai, r2 ij + bi, rij + qi, kj. (12) x, = xy cos(θ). (11) The dot product is maximized when θ = 0 since cos(0) = 1. Therefore, bi, rij acts as direction term and it is largest when rij is aligned with the direction defined by bi. PaPE has context awareness. The distance decay and direction are governed by ai and bi, respectively. Because both vectors are computed from the token content itself, Equations (7) and (8), PaPE naturally adapts to context instead of relying on fixed pattern. This enables PaPE to support long-range interactions. By effectively turning off positional informationletting ai 0 and setting bi = 0Equation (9) reduces to the standard attention score in Equation (2). In this regime, PaPE no longer depends on position, enabling long-range interactions. PaPE decomposes attention. The decomposition in Equation (9) cleanly separates distinct contributions to the attention score: ai, r2 ij encodes distance, bi, rij captures direction, and qi, kj is the familiar semantic term between tokens from Equation (2). Although this separation is not one of our design principles, it is valuable property for model analysis, which we briefly investigate in Section 5.6. Crucially, this preserves the standard attention computation. Queries and keys are transformed independently using only absolute positions, and the attention kernel itself remains unchanged. As result, PaPE becomes plug-and-play compatible with efficient attention kernels. Let denote the Hadamard (element-wise) product, vector concatenation, and si = Wp ri. We can then transform the query as fq(qi, ri) = qi ai, s2 ai (2 ai si) bi, si bi, (13) and the key as fk(kj, rj) = kj 1 j sj 1 sj. (14) We prove in Appendix A.2 that these transformations exactly recover Equation (9). This construction preserves compatibility with efficient attention kernels, but introduces overhead: the dimensionality of queries and keys increases from to + 3m + 2. The additional cost is negligible for small m, but becomes significant as grows. We measure this effect in Section 5.5. 4 Where to Attend: Principled Vision-Centric Position Encoding with Parabolas 5. Experiments We demonstrate the usefulness of PaPE by 3 testing its generality on 8 datasets across 4 vision modalities (Section 5.2), and 4 probing its ability to extrapolate in classification (Section 5.3). Beyond this, we validate components of PaPE in an ablation study (Section 5.4), measure PaPEs efficiency (Section 5.5), and briefly explore how the decomposed attention can be used for model analysis (Section 5.6). 5.1. Experimental Setup Baselines. We compare to five representative baselines: nD-sincos (Wang & Liu, 2021) serves as multidimensional generalization of the classic sinusoidal positional encoding of Vaswani et al. (2017). We use it to compare against an absolute position encoding. RoPE (Su et al., 2024) is state-of-the-art and frequently used position encoding, combining translation invariance with distance decay. We specifically adopt the axial RoPE variant (Chu et al., 2024) to handle multiple position dimensions. RoPE-Mixed (Heo et al., 2024) is included both because it is common choice for adapting RoPE to vision and because it extends RoPE with directionality. nD-ALiBi3 (Fuller et al., 2023) extends ALiBi (Press et al., 2021) to multiple dimensions using Euclidean distances, thereby achieving translation invariance, rotation invariance, and distance decay. LookHere (Fuller et al., 2024) imbues directionality to nD-ALiBi and loses rotation invariance in doing so. It delivers high classification accuracy and strong extrapolation. However, LookHere is only defined for images and does not admit straightforward extension to other modalities, so we have to restrict it to imagebased experiments. Datasets. We evaluate PaPE on 8 datasets that span diverse vision modalities and tasks, showcasing its broad applicability. ImageNet-1K (Deng et al., 2009; Russakovsky et al., 2015) offers 1M images for classification of 1K categories. COCO (Lin et al., 2014) provides more than 200K images for 2D object detection of 80 categories. Together, ImageNet-1K and COCO evaluate PaPE on large-scale image-based datasets. ScanNet (Dai et al., 2017) and ModelNet40 (Wu et al., 2015) are 3D point cloud datasets. ScanNet focuses on indoor 3Wang & Liu (2021) and Fuller et al. (2023) introduce 2D variants of sinusoidal and ALiBi, respectively, but extending them to arbitrary dimensionality is straightforward. 5 scenes with widely distributed points and point-level semantic segmentation. ModelNet40 contains densely sampled object shapes and targets object-level classification. We consider three spatio-temporal datasets. UCF101 (Soomro et al., 2012) contains videos for action recognition of 101 actions. DvsGesture (Amir et al., 2017) and GEN1 (de Tournemire et al., 2020) are event camera datasets. Event streams are asynchronous, spatially sparse and temporally continuous, in contrast to synchronous, spatially dense, and temporally discrete videos. DvsGesture targets action recognition of 11 actions and has high spatial sparsity. GEN1 provides 39 hours of automotive driving for 2D object detection of cars and pedestrians and has medium spatial sparsity. Thus, we get rich variety of spatio-temporal data types. Finally, nuScenes (Caesar et al., 2020) combines camera and LiDAR inputs for 3D object detection. We use nuScenes to test PaPE in challenging multi-modal real-world setting. Models. The different modalities call for modality-specific Transformer variants. We choose to use standard models that stay close to the original Transformer architecture. For images, we adopt ViT (Dosovitskiy et al., 2020) and equip it with YOLOv10 (Wang et al., 2024) head for object detection. For point clouds, we rely on Point Transformer V3 (Wu et al., 2024), optimized for 3D reasoning. For videos, we use ViViT (Arnab et al., 2021), dedicated video vision transformer. For event cameras, we employ ViT with spatio-temporal tokens obtained from Spiking Patches (Øhrstrøm et al., 2025) and re-use the YOLOv10 head for object detection on GEN1. For nuScenes, we use UniTR (Wang et al., 2023), unified multi-modal transformer. Training. We implement all models in PyTorch (Ansel et al., 2024) using 16-bit mixed precision and optimize them with AdamW (Loshchilov & Hutter, 2019). All PaPE models are trained with = 50. Models are validated at every epoch, and we select the checkpoint with the highest validation score for final evaluation on the test set. All models are randomly initialized with uniform Kaiming initialization (He et al., 2015). Crucially, for each dataset we fix the model size and all hyperparameters across position encodings, so that the only systematic difference between runs is the choice of position encoding (up to training stochasticity). This setup yields fair comparison of the position encodings. See Section for additional training details. 5.2. Generality Table 1 reports the results on the 8 datasets. PaPE emerges as the top position encoding on 6 of 8 datasets, with PaPERI being best on 1 of 8, and achieves the highest average score of 66.3surpassing the next-best method, RoPE, by clear margin of 1 point (65.3 vs. 66.3). These results Where to Attend: Principled Vision-Centric Position Encoding with Parabolas Table 1. PaPE: general vision position encoding. Comparison of PaPE and baselines across 8 diverse datasets, with best in bold and next-best underlined. mAP is measured at 0.5:0.95:0.05 IoU intervals. PaPE has the highest score on 6 of 8 datasets, with PaPE-RI leading on 1, demonstrating the usefulness of PaPE across multiple vision modalities and tasks. Image Point cloud Spatio-temporal Multi-modal Position Encoding nD-sincos (Wang & Liu, 2021) RoPE (Su et al., 2024) RoPE-Mixed (Heo et al., 2024) nD-ALiBi (Fuller et al., 2023) LookHere (Fuller et al., 2024) PaPE (ours) PaPE-RI (ours) ImageNet-1K COCO ScanNet ModelNet40 UCF101 DvsGesture GEN1 mAP mIoU mAP Acc. Acc. Acc. Acc. nuScenes mAP Average 79.9 80.3 80.0 80.0 80.5 80.6 80.0 34.7 38.8 38.0 33.9 37.3 38.9 34. 72.6 71.7 71.1 70.6 - 71.0 69.3 92.2 93.0 93.0 93.2 - 93.3 92.7 38.7 43.9 41.2 41.6 - 49.5 42. 93.4 93.1 83.3 89.2 - 93.4 90.6 27.3 33.6 31.7 28.8 - 34.8 28.5 67.4 68.3 66.3 68.5 - 68.7 68. 63.3 65.3 63.1 63.2 - 66.3 63.3 highlight PaPEs strong generality across modalities and tasks. In contrast, PaPE-RI lags behind with markedly lower average score of 63.3, highlighting the importance of directionality and the added flexibility in ai and Wp. Breaking down the results reveals further insight: consistently lags behind on object detection tasks (COCO, GEN1, and nuScenes). While there are few exceptions (ScanNet and DvsGesture), the pattern is clear: the results suggest that translation invariance is often crucial, as nDsincos is the only encoding in our study that lacks it. PaPE is well-suited for images. Across image datasets, PaPE edges out strong baselines, even if by slim margins. On ImageNet-1K, PaPE increases accuracy by 0.1 over LookHere, and on COCO it lifts mAP by 0.1 over RoPE. These gains are small but consistent, showing PaPEs applicability to large-scale image datasets. PaPE excels on spatio-temporal data. On UCF101, PaPE delivers the largest absolute accuracy gain among all datasets: 49.5 vs. 43.9 for RoPE. We follow the official 3-fold cross-validation and report the mean accuracy, highlighting the robustness of the result. potential reason for PaPEs strong UCF101 performance is that many actions unfold very quickly, causing objects to move rapidly across framesmaking directionality and context awareness matter greatly for understanding fast moving object. PaPE also leads on event camera datasets, outperforming RoPE by 1.2 mAP on GEN1 and tying for top accuracy with sinusoidal encodings on DvsGesture. By contrast, RoPE-Mixed performs notably poorly in the spatio-temporal setting, especially on DvsGesture. These results underscore how challenging spatio-temporal reasoning isand how non-trivial it is for models to capture these relations effectively. PaPE is capable of handling multiple modalities. PaPERI delivers the strongest performance on nuScenes with 68.9 mAP, trailed by PaPE at 68.7, with nD-ALiBi close behind at 68.5. This ranking is striking and points to an unexpectedly important role of rotation invariance in multi-modal processing. Mirroring our findings on spatio-temporal data, RoPE-Mixed again lags behind in this setting, showing how challenging it is to learn multi-modal positional structure. When translation invariance matters. nD-sincos ranks as the lowest performing encoding on 4 of 8 datasets and is trailing as second-lowest on 2 of 8 datasets. In particular, it When absolute position encoding matters. The ScanNet results reveal additional findings. It is the only dataset on which PaPE trails the baselines. What is more, the sinusoidal baseline surpasses all others, including RoPE by 0.9 mIoU. This suggests that absolute positional information is important for ScanNet. potential explanation is that everyday household objects have characteristic absolute sizes. For example, table typically sits about 1 above the floor. Purely relative encodings may fail to capture such relationships when the context in Point Transformer V3 does not span sufficiently distant points. This explanation is supported by ModelNet40, where the translation invariant encodings surpass sinusoidal encodings by at least 0.8 accuracy. Here, each sample is single object with no surrounding scene, so absolute position carries less semantic weight. In this setting, all methods achieve similar accuracy, yet PaPE remains the top performer, reinforcing it as robust position encoding. Rotation invariance matters less than hypothesized. Our point cloud experiments show that rotation invariant position encodings do not improve over rotation variant alternatives. This is evident from how neither rotation invariant encoding (nD-ALiBi and PaPE-RI) manages to outperform the rotation-variant encodings. One possible explanation is that the rotation invariant position encodings trade strict rotation invariance for less representation flexibility, and that decreased flexibility matters more than what they gain from invariance (Wilson, 2025). 5.3. Classification Extrapolation We want models that train cheaply at low resolutions, yet seamlessly run inference at higher ones. We call this capability extrapolation. Extrapolation reduces training costs and 6 Where to Attend: Principled Vision-Centric Position Encoding with Parabolas Table 2. Ablation of PaPE on ImageNet-1K. All components of PaPE contribute to its accuracy as removing any component results in lower accuracy. Table 4. Efficiency. Average inference time for single image on ImageNet-1K and the number of learnable positional parameters. PaPE increases inference time by small amount, up to 0.4 ms, and adds up to 13.6M position-specific parameters."
        },
        {
            "title": "Accuracy",
            "content": "PaPE - ai, r2 ij - bi, rij - Wa, Wb - ai activation - Wp 79.2 77.2 77.0 77.2 77.5 78.9 nD-sincos nD-ALiBi RoPE RoPE-Mixed Time (ms) Pos. Params. 1.5 1.5 0 1.6 0 1.6 18.4K 2 8 16 32 64 PaPE Time (ms) Pos. Params. 1.7 0.5M 0.9M 1.8M 3.6M 7.1M 13.6M 1.9 1.7 1.8 1.8 1. Table 3. Ablation of on ImageNet-1K. PaPE requires at least = 8 on ImageNet-1K to perform well as the effect saturates beyond that point. 2 4 8 32 64 Accuracy 77.1 77.3 79. 79.0 78.9 79.2 is an indicator of robustness. Moreover, strong extrapolation suggests that model pretrained at low resolutions can be fine-tuned more easily to high-resolution target domains. Prior works (Sun et al., 2023; Heo et al., 2024; Fuller et al., 2024) have shown that position encoding greatly determines the extrapolation capabilities of model. Motivated by this, we assess PaPEs extrapolation ability (for classification) by evaluating how well it scales from its training resolution of 2242 up to 10242 on ImageNet-1K. Importantly, we do not change the models in any way for evaluation at higher resolutions. The results are shown in Figure 1c. See details in Section B, including position interpolation experiments. We find that PaPE extrapolates far better than any baseline, even outperforming LookHere, which is itself highly capable at extrapolating. PaPE increases accuracy by 1% for resolutions up to and including 5122, and only falls below its training-resolution accuracy after 6402. In contrast, PaPE-RI does not share the strong extrapolation. This may be consequence of the trade-off we made, sacrificing directionality to achieve rotation invariance. Similarly, LookHere can be viewed as an nD-ALiBi variant, enhanced with directionality. In both cases, introducing directionality leads to better extrapolation, indicating that directionality is key ingredient for effective extrapolation. Unlike the baselines, PaPE is context aware. This aligns with the findings of Wang et al. (2025), who find that adding context aware term significantly boosts RoPEs extrapolation in language models. Taken together, these results suggest that context awareness has role to play in extrapolation. 7 5.4. Ablation Study We systematically ablate each component of PaPE to understand its contribution and to simultaneously validate the instantiations of distance decay, directionality, and context awareness. To ablate context awareness, we remove Wa and Wb and instead treat ai and bi as learned parameters, where the softplus activation is still applied to the coefficients. They are learned distinctly for all layers and heads, but are shared between tokens such that ai = aj and bi = bj for any pair of and j. This way, we maintain distance decay and directionality without context awareness. To keep the study computationally feasible, we conduct all ablations on ImageNet-1K using ViT-S instead of ViT-B model size, and train for 150 epochs rather than 300. The primary ablation results are shown in Table 2. Overall, we see that the complete PaPE method has an accuracy of 79.2 and that all ablations result in lower accuracy. Noticeably, removing any of the three instantiations of the design principlesdistance decay (ai, r2 ij ), directionality (bi, rij), and context awareness (Wa, Wb)causes an accuracy drop around 2%, validating the relevance of instantiating those principles in PaPE. Distance decay is further validated by how the model struggles to use the squared distance term when removing the softplus activation, resulting in lower accuracy of 77.5. Lastly, although its impact is modest at 0.3 difference in accuracy, adding Wp contributes positively to PaPE. Beyond our main ablations, we also vary mthe output dimensionality of Wpand report the results in Table 3. We observe that PaPE needs at least = 8 to approach the base performance of 79.2 (achieved with = 50), and that increasing beyond 8 yields diminishing returns. 5.5. Efficiency As shown in Section 4.1, PaPE increases the effective head size from to + 3m + 2. It also introduces positional parameters: Wa, Wb, and Wp. We measure the number of additional positional parameters and the impact on inference Where to Attend: Principled Vision-Centric Position Encoding with Parabolas are often sparse, it is helpful to refine the analysis by considering only the top-attended keys for each query, which we define as the minimal set of keys whose cumulative attention in Ai reaches threshold τ . Figure 3 breaks down how each head in the ImageNet-1K model focuses on positions and semantics on average by visualizing the scores z, using the threshold τ = 80%. Most heads balance positions and semantics quite evenly. Yet, few stand out: some heads (e.g., L2H2) are clearly positionfocused, whereas L1H10 is highly driven by semantics. Interestingly, these specialized heads cluster in the early layers (L1L5), suggesting that emphasizing positional or semantic signals is especially beneficial when transforming low-level inputs into higher-level representations. 6. Limitations & Future Work PaPEs main limitation is its extra inference cost and additional parameters, both affected by m. Future work will focus on how to lower in practice, or removing Wp altogether and thereby keeping = p, which is small for all modalities and tasks considered here. complementary direction is to apply PaPE to dynamic token selection. This has the potential to increase efficiency by reducing the number of attention dot products. More broadly, PaPE can be viewed as the first instance of larger family of multivariate polynomials on rij with context-dependent coefficients. We elaborate on this perspective in Section C. Although we leave this direction unexplored in the current work, we are eager to investigate other members of this function family in future work. Finally, we want to investigate whether rotation invariance and directionality can be jointly achieved, since the derivation of PaPE-RI indicates that one can only get one. 7. Conclusions We introduce Parabolic Position Encoding (PaPE), principled, vision-centric position encoding. PaPE, along with its rotation invariant version PaPE-RI, is derived from principles identified in prior research and is compatible with efficient attention kernels. We evaluate PaPE on 8 datasets spanning images, point clouds, videos, event cameras, and multi-modal settings. The results demonstrate the broad applicability of PaPE: it achieves the best performance on 6 of 8 datasets, while PaPE-RI attains the highest score on 1 of the 8. We further observe that PaPE exhibits exceptional classification extrapolation capabilities, substantially outperforming all baselines. Thus, PaPE paves new path for position encoding of vision modalities, and we anticipate that its underlying principles will motivate future research. Figure 3. Model analysis on ImageNet-1K. Red (z > 0) highlights heads that lean heavily on positional information, while blue (z < 0) marks heads that prioritize semantic content in deciding what to attend to. Positions are used most strongly in early layers. time for different in Table 4. The analysis considers ViT-B/16 on ImageNet-1K at resolution 2242. standard ViT-B (including its classification head) has 86.4M parameters. With PaPE, this footprint grows only by 0.6%15.7%. Although this exceeds the baseline sizes, PaPE still delivers its advantages with only moderate increase in parameters. Inference time is measured on an RTX 4090 GPU with batch size of 1. PaPE increases inference time between 0.2 ms and 0.4 ms over the fastest baselines. In absolute terms, this overhead is tiny, so PaPE remains fast. However, the relative increase is between 13%27% and can become significant for very large inputs and models. Consequently, offers trade-off between accuracy and efficiency, as demonstrated in Tables 3 and 4. 5.6. Model Analysis The decomposed attention in PaPE eases the investigation of how the model uses position and semantic information. Although model analysis is not the focus of this work, here we demonstrate an analysis method that is enabled by PaPE. The attention matrix candisregarding the dot product scaling factorbe represented as Aij = 1 PijYij, with posiγi tional components Pij = exp (cid:0)(cid:10)ai, r2 ij (cid:11) + bi, rij(cid:1) , (15) semantic components Yij = exp (qi, kj), and normalization factors γi = (cid:80) PijYij. We introduce the score z, = Eij (cid:104) 1 γi (cid:105) (Pij Yij) , (16) to quantify the relative importance of positional vs. semantic terms of given attention head. Since attention patterns 8 Where to Attend: Principled Vision-Centric Position Encoding with Parabolas"
        },
        {
            "title": "Acknowledgment",
            "content": "This work has been supported by Innovation Fund Denmark through the project Safety and Autonomy for Vehicles in Agriculture (SAVA), 2105-00013A."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Amir, A., Taba, B., Berg, D., Melano, T., McKinstry, J., Di Nolfo, C., Nayak, T., Andreopoulos, A., Garreau, G., Mendoza, M., Kusnitz, J., Debole, M., Esser, S., Delbruck, T., Flickner, M., and Modha, D. Low Power, Fully Event-Based Gesture Recognition System. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 72437252, 2017. Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang, Y., Liang, J., Lu, Y., Luk, CK., Maher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang, X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G., Wu, P., and Chintala, S. PyTorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation. In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS 24). ACM, April 2024. doi: 10.1145/3620665.3640366. Arnab, A., Dehghani, M., Heigold, G., Sun, C., Luˇcic, M., and Schmid, C. ViViT: Video Vision Transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 68366846, 2021. Bertasius, G., Wang, H., and Torresani, L. Is Space-Time Attention All You Need for Video Understanding? In Proceedings of the 38th International Conference on Machine Learning, volume 139, pp. 813824. PMLR, July 2021. Caesar, H., Bankiti, V., Lang, A. H., Vora, S., Liong, V. E., Xu, Q., Krishnan, A., Pan, Y., Baldan, G., and Beijbom, O. nuScenes: Multimodal Dataset for Autonomous Driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11621 11631, 2020. Chen, S., Wong, S., Chen, L., and Tian, Y. Extending Context Window of Large Language Models via Positional Interpolation, June 2023. Chu, X., Su, J., Zhang, B., and Shen, C. VisionLLaMA: Unified LLaMA Backbone for Vision Tasks. In European Conference on Computer Vision, pp. 118. Springer, 2024. ISBN 978-3-031-72848-8. Cubuk, E. D., Zoph, B., Shlens, J., and Le, Q. V. Randaugment: Practical Automated Data Augmentation With Reduced Search Space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 702703, 2020. Dai, A., Chang, A. X., Savva, M., Halber, M., Funkhouser, T., and Niessner, M. ScanNet: Richly-Annotated 3D In Proceedings of Reconstructions of Indoor Scenes. the IEEE Conference on Computer Vision and Pattern Recognition, pp. 58285839, 2017. Dao, T. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In The Twelfth International Conference on Learning Representations, 2024. Dao, T., Fu, D., Ermon, S., Rudra, A., and Re, C. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 1634416359. Curran Associates, Inc., 2022. de Tournemire, P., Nitti, D., Perot, E., Migliore, D., and Sironi, A. Large Scale Event-based Detection Dataset for Automotive, January 2020. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255. IEEE, 2009. Ding, Y., Zhang, L. L., Zhang, C., Xu, Y., Shang, N., Xu, J., Yang, F., and Yang, M. LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens. In Forty-First International Conference on Machine Learning, June 2024. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In International Conference on Learning Representations, October 2020. 9 Where to Attend: Principled Vision-Centric Position Encoding with Parabolas Fuller, A., Millard, K., and Green, J. CROMA: Remote Sensing Representations with Contrastive Radar-Optical Masked Autoencoders. Advances in Neural Information Processing Systems, 36:55065538, December 2023. Fuller, A., Kyrollos, D. G., Yassin, Y., and Green, J. R. LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate. Advances in Neural Information Processing Systems, 37:1968319739, December 2024. Gehrig, M. and Scaramuzza, D. Recurrent Vision Transformers for Object Detection with Event Cameras. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1388413893, June 2023. Ghiasi, G., Cui, Y., Srinivas, A., Qian, R., Lin, T.-Y., Cubuk, E. D., Le, Q. V., and Zoph, B. Simple copy-paste is strong data augmentation method for instance segmentation. 2021 ieee. In CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 29172927, 2020. Gu, F., Sng, W., Hu, X., and Yu, F. EventDrop: Data augmentation for event-based learning. In 30th International Joint Conference on Artificial Intelligence, IJCAI 2021, June 2021. He, K., Zhang, X., Ren, S., and Sun, J. Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1026 1034, 2015. Heo, B., Park, S., Han, D., and Yun, S. Rotary Position Embedding for Vision Transformer. In European Conference on Computer Vision, pp. 289305. Springer Nature Switzerland, 2024. ISBN 978-3-031-72684-2. doi: 10.1007/978-3-031-72684-2 17. Kingma, D. P. and Ba, J. L. Adam: Method for Stochastic Gradient Descent. In 3rd International Conference for Learning Representations, San Diego, 2015. Li, Y., Kim, Y., Park, H., Geller, T., and Panda, P. Neuromorphic Data Augmentation for Training Spiking Neural Networks. In European Conference on Computer Vision, 2022a. Li, Y., Mao, H., Girshick, R., and He, K. Exploring Plain Vision Transformer Backbones for Object Detection. In European Conference on Computer Vision, June 2022b. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P., and Zitnick, C. L. Microsoft COCO: Common Objects in Context. In European Conference on Computer Vision, pp. 740755. Springer International Publishing, 2014. ISBN 978-3-319-10602-1. doi: 10.1007/978-3-319-10602-1 48. Loshchilov, I. and Hutter, F. Decoupled Weight Decay Regularization. In International Conference on Learning Representations, 2019. Øhrstrøm, C. K., Guldenring, R., and Nalpantidis, L. Spiking patches: Asynchronous, sparse, and efficient tokens for event cameras. arXiv preprint arXiv:2510.26614, 2025. Ostmeier, S., Axelrod, B., Varma, M., Moseley, M. E., Chaudhari, A., and Langlotz, C. LieRE: Lie Rotational Positional Encodings. In Forty-Second International Conference on Machine Learning, 2025. Press, O., Smith, N., and Lewis, M. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In International Conference on Learning Representations, October 2021. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., BernImageNet stein, M., Berg, A. C., and Fei-Fei, L. InternaLarge Scale Visual Recognition Challenge. tional Journal of Computer Vision, 115(3):211252, December 2015. ISSN 0920-5691, 1573-1405. doi: 10.1007/s11263-015-0816-y. Sabater, A., Montesano, L., and Murillo, A. C. Event Transformer. Sparse-Aware Solution for Efficient Event Data Processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2677 2686, 2022. Schenck, C., Reid, I., Jacob, M. G., Bewley, A., Ainslie, J., Rendleman, D., Jain, D., Sharma, M., Dubey, K. A., Wahid, A., Singh, S., Wagner, R., Ding, T., Fu, C., Byravan, A., Varley, J., Gritsenko, A. A., Minderer, M., Kalashnikov, D., Tompson, J., Sindhwani, V., and Choromanski, K. M. Learning the RoPEs: Better 2D and 3D Position Encodings with STRING. In Forty-Second International Conference on Machine Learning, June 2025. Smith, L. N. and Topin, N. Super-convergence: Very fast training of neural networks using large learning rates. In Artificial Intelligence and Machine Learning for MultiDomain Operations Applications, volume 11006, pp. 369 386. SPIE, May 2019. doi: 10.1117/12.2520589. Soomro, K., Zamir, A. R., and Shah, M. UCF101: Dataset of 101 Human Actions Classes From Videos in The Wild, December 2012. Su, J., Ahmed, M., Lu, Y., Pan, S., Bo, W., and Liu, Y. RoFormer: Enhanced transformer with Rotary Position Embedding. Neurocomputing, 568:127063, February 2024. ISSN 0925-2312. doi: 10.1016/j.neucom.2023. 127063. 10 Where to Attend: Principled Vision-Centric Position Encoding with Parabolas Yu, H., Jiang, T., Jia, S., Yan, S., Liu, S., Qian, H., Li, G., Dong, S., and Yuan, C. ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 45084517, 2025. Yun, S., Han, D., Oh, S. J., Chun, S., Choe, J., and Yoo, Y. CutMix: Regularization Strategy to Train Strong Classifiers With Localizable Features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 60236032, 2019. Zhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. Mixup: Beyond Empirical Risk Minimization. In International Conference on Learning Representations, 2018. Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., and Wei, F. LengthExtrapolatable Transformer. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1459014604, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.816. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention In Advances in Neural Information is All you Need. Processing Systems, volume 30. Curran Associates, Inc., 2017. Veisi, A., Fartoot, D., and Amirzadeh, H. Context-aware Rotary Position Embedding, July 2025. Wang, A., Chen, H., Liu, L., Chen, K., Lin, Z., Han, J., and Ding, G. YOLOv10: Real-Time End-to-End Object Detection. Advances in Neural Information Processing Systems, 37:107984108011, December 2024. Wang, H., Tang, H., Shi, S., Li, A., Li, Z., Schiele, B., and Wang, L. UniTR: Unified and Efficient Multi-Modal Transformer for Birds-Eye-View Representation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 67926802, 2023. Wang, Y., Shen, S., Munos, R., Zhan, H., and Tian, Y. Positional Encoding via Token-Aware Phase Attention, September 2025. Wang, Z. and Liu, J.-C. Translating math formula images to LaTeX sequences using deep neural networks with sequence-level training. International Journal on Document Analysis and Recognition (IJDAR), 24(1): 6375, June 2021. ISSN 1433-2825. doi: 10.1007/ s10032-020-00360-2. Wilson, A. G. Position: Deep learning is not so mysterious or different. In Forty-second International Conference on Machine Learning Position Paper Track, 2025. Wu, X., Jiang, L., Wang, P.-S., Liu, Z., Liu, X., Qiao, Y., Ouyang, W., He, T., and Zhao, H. Point Transformer In Proceedings of the V3: Simpler, Faster, Stronger. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 48404851, June 2024. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., and Xiao, J. 3D ShapeNets: Deep Representation for Volumetric Shapes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 19121920, 2015. 11 Where to Attend: Principled Vision-Centric Position Encoding with Parabolas A. Proofs A.1. PaPE-RI is Rotation Invariant By the definition of PaPE-RI, assume bi = 0, aiℓ = αi R<0, and Wp = wpIp with wp R, where Ip represents the identity matrix of shape p. Let rij = Wp (rj ri) and ij = Wp (Rrj Rri) for SO(p). In this proof, we show that restricting the model to the conditions assumed above, guarantees that Aij=A Aij depend on rij, and entries ij depend respectively on ij. Thus, satisfying Theorem 3.2. ij, where entries Inserting the conditions in the position-dependent terms leads to the simplified expression, (cid:32) (cid:88) aiℓr2 ijℓ + biℓrijℓ (cid:33) ℓ=1 (cid:88) = ℓ= aiℓr2 ijℓ using biℓ = 0 (cid:88) = αi r2 ijℓ using aiℓ = αi ℓ=1 = αirT ijrij. The equivalence of the position dependent terms (rT ij ij = rT ijrij) proves the rotation invariance of PaPE-RI: ij ij rT = (Wp R(rj ri))T (Wp R(rj ri)) = w2 p(rj ri)T RT R(rj ri) = w2 p(rj ri)T (rj ri) = rT ijrij = Aij = ij . using Wp = wpIp using RT = Ip, SO(p) A.2. PaPE is Compatible with Efficient Attention Kernels We claim in Section 4.1 that fq(qi, ri), fk(kj, rj) = ai, r2 ij + bi, rij + qi, kj. Recall that we define in Equations (13) and (14) (using si = Wp ri) the feature vectors fq and fk as fq(qi, ri) = qi ai, s2 fk(kj, rj) = kj ai s2 (2 ai si) bi, si bi, sj. sj We verify Equation (27) below: 2 ai si, sj bi, si + bi, sj + ai, s2 fq(qi, ri), fk(kj, rj) = qi, kj + ai, s2 = qi, kj + ai, s2 + s2 = qi, kj + ai, (sj si)2 + bi, sj si = qi, kj + ai, (Wp rj Wp ri)2 + bi, Wp rj Wp ri = qi, kj + ai, ((Wp (rj ri))2 + bi, Wp (rj ri) = qi, kj + ai, r2 ij + bi, rij = ai, r2 2 si sj + bi, si + sj ij + bi, rij + qi, kj . using m2 + n2 2mn = (m n)2 (17) (18) (19) (20) (21) (22) (23) (24) (25) (26) (27) (28) (29) (30) (31) (32) (33) (34) 12 Where to Attend: Principled Vision-Centric Position Encoding with Parabolas Table 5. Extrapolation results without position interpolation. Position encodings are trained on ImageNet-1K at resolution of 2242 and evaluated without modifications at resolutions up to 10242. Best in bold and next-best underlined. PaPE improves classification extrapolation significantly by up to 10.5% over the next-best encoding. Position Encoding 2242 3202 3842 4482 5122 7682 8962 10242 nD-sincos (Wang & Liu, 2021) RoPE (Su et al., 2024) RoPE-Mixed (Heo et al., 2024) nD-ALiBi (Fuller et al., 2023) LookHere (Fuller et al., 2024) PaPE (ours) PaPE-RI (ours) 79.9 80.3 80.0 80.0 80. 80.6 80.0 76.8 80.9 80.4 78.4 81.4 81.6 80.2 72.6 80.6 79.2 77.8 81.1 81.7 79.2 65.9 79.7 77.1 76.6 80. 81.6 78.1 57.5 78.2 73.6 75.3 79.7 81.3 76.9 41.3 74.0 61.5 72.2 77.5 80.5 73.6 27.2 67.0 43.4 68.3 74. 79.2 69.7 16.8 56.2 27.1 64.1 69.6 77.3 64.6 9.3 42.3 17.1 59.1 64.4 74.9 58.6 Table 6. Extrapolation results with position interpolation. The brackets indicate the increase (green) or decrease (red) compared to the corresponding configuration without interpolation in Table 5. Best in bold and next-best underlined. nD-sincos benefits greatly from interpolation while RoPE and RoPE-Mixed benefit from resolution 6402 and above. Position interpolation is detrimental to all of the attention bias methods (nD-ALiBi, LookHere, PaPE, and PaPE-RI). Position Encoding 2242 3202 3842 4482 6402 7682 8962 10242 nD-sincos (Wang & Liu, 2021) RoPE (Su et al., 2024) RoPE-Mixed (Heo et al., 2024) nD-ALiBi (Fuller et al., 2023) LookHere (Fuller et al., 2024) PaPE (ours) PaPE-RI (ours) 79.9 80.3 80.0 80.0 80.5 80.6 80.0 79.9 (+3.1) 80.6 (-0.3) 72.9 (-7.5) 77.9 (-0.5) 80.9 (-0.5) 78.8 (+6.2) 80.0 (-0.6) 72.1 (-7.1) 77.2 (-0.6) 80.2 (-0.9) 77.4 (+11.5) 78.9 (-0.8) 73.0 (-4.1) 76.0 (-0.6) 78.9 (-1.5) 75.8 (+18.3) 77.6 (-0.6) 67.8 (-5.8) 74.4 (-0.9) 77.4 (-2.3) 71.8 (+30.5) 74.5 (+0.5) 63.5 (+2.0) 70.6 (-1.6) 73.6 (-3.9) 66.9 (+39.7) 69.9 (+2.9) 56.1 (+12.7) 65.4 (-2.9) 68.8 (-5.4) 61.3 (+44.5) 64.8 (+8.6) 49.5 (+22.4) 59.8 (-4.3) 63.7 (-5.9) 56.0 (+46.7) 59.8 (+17.5) 42.9 (+25.8) 53.9 (-5.2) 59.0 (-5.4) 80.7 (-0.9) 80.3 (+0.1) 79.6 (-2.1) 79.4 (+0.2) 78.1 (-3.5) 78.1 (0.0) 76.5 (-4.8) 76.4 (-0.5) 72.0 (-8.5) 72.2 (-1.4) 66.4 (-12.8) 66.5 (-3.2) 60.4 (-16.9) 59.7 (-4.9) 54.8 (-20.1) 52.8 (-5.8) B. Extrapolation Details Here, we further elaborate on the extrapolation performance of PaPE and the baseline methods. First, Table 5 reports the raw results corresponding to the extrapolation experiment shown in Figure 1. From this we observe the previously mentioned 10.5% improvement in absolute terms from LookHere to PaPE at resolution of 10242. Without position interpolation. The main extrapolation experiment evaluates how the methods perform without position interpolation. Concretely, during training we only see positions in the range [1, 14] because we train at resolution of 2242 with patch size of 16, since 224/16 = 14. At higher resolutions, however, this expands up to [1, 64], since 1024/16 = 64. As mentioned above, we see in Table 5 that PaPE excels in this setting. With position interpolation. Prior works find that sinusoidal and RoPE-based position encodings extrapolate better with position interpolation (Chen et al., 2023; Ding et al., 2024). It works by mapping the test-time position range back into the original training range. We do this by scaling the positions by factor of 224/R, where is the target resolution. For example, at = 448, each position is multiplied by 0.5, compressing the effective positions back into the [1, 14] training range. Table 6 shows how PaPE and the baselines perform with position interpolation. We clearly see how nD-sincos in particular is greatly improved with position interpolation. RoPE and RoPE-Mixed drop for resolutions below 6402, but greatly increase their accuracy from there and up. These results confirm that position interpolation is useful for sinusoidal encodings and partially useful for RoPE-based encodings. We also find that all four attention bias position encodings, including PaPE, are negatively affected by position interpolation. Best-performing configuration. Although position interpolation partially mitigates the shortcomings of sinusoidal and RoPE-based encodings, there is still no encoding that approaches the extrapolation performance of PaPE without position interpolation. This is evident in Table 7, which reports extrapolation scores for the best-performing configuration of each position encoding. In this comparison, nD-sincos, RoPE, and RoPE-Mixed rely on interpolation, whereas nDALiBi, LookHere, PaPE, and PaPE-RI do not. We observe in particular that PaPE remains the strongest encoding across all resolutions, with LookHere consistently ranking as the next-best method for extrapolation. As result, the 10.5% improvement at resolution 10242 persists even after taking position interpolation into account. Where to Attend: Principled Vision-Centric Position Encoding with Parabolas Table 7. Extrapolation results for best-performing configuration per position encoding. With position interpolation: nD-sincos, RoPE, and RoPE-Mixed. Without: nD-ALiBi, LookHere, PaPE, and PaPE-RI. Best in bold and next-best underlined. Although position interpolation is detrimental to PaPE, it remains the best encoding for classification extrapolationeven when applying position interpolation to the encodings that benefit from it. Position Encoding 2242 3202 4482 5122 6402 7682 8962 nD-sincos (Wang & Liu, 2021) RoPE (Su et al., 2024) RoPE-Mixed (Heo et al., 2024) nD-ALiBi (Fuller et al., 2023) LookHere (Fuller et al., 2024) PaPE (ours) PaPE-RI (ours) 79.9 80.3 80.0 80.0 80.5 80.6 80.0 79.9 80.6 72.9 78.4 81.4 81.6 80. 78.8 80.0 72.1 77.8 81.1 81.7 79.2 77.4 78.9 73.0 76.6 80.4 81.6 78.1 75.8 77.6 67.8 75.3 79.7 81.3 76. 71.8 74.5 63.5 72.2 77.5 80.5 73.6 66.9 69.9 56.1 68.3 74.2 79.2 69.7 61.3 64.8 49.5 64.1 69.6 77.3 64. 56.0 59.8 42.9 59.1 64.4 74.9 58.6 C. Polynomial Generalizations of PaPE We here describe polynomial generalizations of PaPE. We believe these generalizations to be interesting to explore further in future work. Let xi Rd and xj Rd be two tokens at positions ri Rp and rj Rp. An expressive family of continuous, context-aware, and translation invariant functions are polynomials of the difference (rj ri) Rp, g(xi, ri, xj, rj) = (cid:88) n=0 an(xi, xj)(rj ri)n, (35) with coefficients an R1p and where ()n represents the element-wise exponentiation. For compatibility with efficient attention kernels, the function needs to be representable as dot product of finite feature vectors that each only depend on the position and content of either of the two tokens. Since (rj ri)n can be expanded to sums of monomials rl , the expression is separable into feature vectors, as long as the coefficients an are of the type an = hq(xi)hk(xj) for functions hq, hk. This results in terms of type hq(xi)rl hx(kj). Note that in PaPE we set hk(xj) = 1, leading to coefficients of type an(xi, xj) = hq(xi). i, rnl irnl further generalization of PaPE is into the multivariate polynomials. To simplify notation, let = (rj ri) = (x1, x2, . . . , xp). Multivariate polynomials are finite sums of terms of the type cαxα1 , where cα R, and α = (α1, α2, . . . , αp) Np is multi-index. The degree of the multivariate polynomial is the largest sum of exponents corresponding to non-zero coefficients, i.e., = max{(cid:80)p This broader family can represent terms that cannot be represented in the former, e.g., bilinear forms (rj ri)T B(rj ri) for any Rpp. Similar to the former case, multivariate polynomials can also be separated into feature vectors for efficient attention kernel compatibility, as long as coefficients cα are of the type hq(qi)hk(kj). i=1 αi cα = 0}. 2 . . . xαp 1 xα2 D. Comparison of nD-ALiBi and PaPE-RI The methods nD-ALiBi and PaPE-RI have methodological similarities, which we compare here. nD-ALiBi works by subtracting the distance matrix D, between token positions from the matrix (pre-softmax attention matrix), with constant scaling factor for each attention head. For simplicity, we consider the case of one head without loss of generality, Dij = rj ri2, (36) SALiBi = QK + αD α R<0. 14 (37) (38) Where to Attend: Principled Vision-Centric Position Encoding with Parabolas Table 8. Hyperparameters. The hyperparameters are shared for all position encodings for given dataset. Parameter ImageNet-1K COCO ScanNet ModelNet40 UCF101 DvsGesture GEN1 nuScenes Batch size Learning rate Weight decay Momemtum (B1) Momemtum (B2) Warmup epochs Total epochs Training resolution (W H) Patch size Drop path Label smoothing 1024 0.0006 0.05 0.9 0.999 15 300 2242 16 0.1 0.1 64 0.0006 0.05 0.9 0.999 15 150 6402 16 0.1 - 12 0.006 0.05 0.9 0.999 40 800 - 1024 0.3 - 32 0.001 0.01 0.9 0.999 5 300 - 1024 0.3 - 32 0.00003 0.05 0.9 0.999 20 200 320 240 16 0.1 - 32 0.00005 0.05 0.9 0.999 15 300 1282 16 0.1 0. 64 0.0001 0.05 0.9 0.999 7.5 150 304 240 16 0.1 - 3 0.003 0.03 0.9 0.99 1 10 704 256 8 - - Similar behavior is obtained by PaPE-RI, inducing the same propertiestranslation and rotation invariance. The main difference being that PaPE-RI is designed to subtract the squared distance matrix, where the coefficient can also depend on the token content, leading to SPaPE-RI = QK + α(X) D2 αij(X) = αij(xi) R<0. (39) (40) further difference is that PaPE-RI is compatible with efficient attention kernels, while nD-ALiBi is not. E. Training Details We now detail the training configurations used for all datasets. All models are optimized with AdamW (Loshchilov & Hutter, 2019), using OneCycle (Smith & Topin, 2019) learning rate schedule with cosine decay. The only exception is UniTR on nuScenes, which uses Adam (Kingma & Ba, 2015). Table 8 summarizes the common hyperparameters for all datasets. Unless otherwise specified, all models use the same Transformer settings (i.e. #layers, #heads, dimensionality, and head size) as ViT-B (Dosovitskiy et al., 2020). Dataset-specific configurations are detailed in the following paragraphs. ImageNet-1K. We treat the official validation split as our test set and divide the original training split into new training set and validation set. The ratio is 99% for training and 1% for validation, using stratified split that preserves class balance in each subset. For the classifier head, we initialize the weights to 0 and the bias to log(1000), so that the initial class probabilities are uniform. We use the following set of augmentations: RandAugment (Cubuk et al., 2020), HorizontalFlip, MixUp (Zhang et al., 2018), and CutMix (Yun et al., 2019). COCO. The detection model pairs ViT-B backbone with ViTDet (Li et al., 2022b) neck (strides 8, 16, and 32) and YOLOv10 (Wang et al., 2024) head and loss. For data augmentation, we keep RandAugment but explicitly drop shear and rotation, as they corrupt ground-truth bounding boxes. In addition, we apply Large Scale Jitter (Ghiasi et al., 2020), following ViTDet. ScanNet and ModelNet40. We use Point Transformer V3 (Wu et al., 2024) with the dataset-specific configurations from the official code repository. This includes the same augmentations as well as model configuration and size. However, we do not initialize the weights from pretrained models. UCF101. We follow ViViT (Arnab et al., 2021), using 2-tubelets for each video sample and drawing up to 5 samples per video, with 10 frames separating consecutive 2-tubelets. The augmentations are RandAugment and HorizontalFlip. For both training and evaluation, we use the official 3-fold cross-validation splits and report the mean test performance. Mirroring the ImageNet-1K protocol, we further partition each training split into new training and validation sets. We do this by assigning the first video group of every action class to the validation set. DvsGesture. We utilize Spiking Patches (Øhrstrøm et al., 2025) to extract asynchronous and spatially sparse spatio-temporal tokens, configured with spike threshold, σ, of 256 and refractory period, , of 100 ms. The model is ViT-B where we follow the token embedding method of Øhrstrøm et al. (2025). Since the temporal positions are at microsecond resolution, we rescale them by 1/50000. We apply two sets of augmentations: NDA (Li et al., 2022a) and EventDrop (Gu et al., 2021). GEN1. Similar to DvsGesture, we extract tokens using Spiking Patches with σ = 256 and = 100 ms, and process 15 Where to Attend: Principled Vision-Centric Position Encoding with Parabolas them with ViT-B backbone. Here, however, we scale temporal positions by 1/100000 and sample the most recent 500K events relative to the prediction time. Because the tokens are asynchronous, they are not directly compatible with the image-like tensors expected by the ViTDet neck and YOLOv10 head. Therefore, we first convert the backbone outputs into an image-like tensor. The way it works is that for each spatial location, we fill its slot with the most recent token at that spatial position. We augment with EventDrop and rotations. nuScenes. We use UniTR (Wang et al., 2023), following the same architecture, model size, hyperparameters, and augmentations as in the official code repository."
        }
    ],
    "affiliations": [
        "Lund University",
        "Royal Institute of Technology (KTH)"
    ]
}