{
    "paper_title": "A Survey on LLM-as-a-Judge",
    "authors": [
        "Jiawei Gu",
        "Xuhui Jiang",
        "Zhichao Shi",
        "Hexiang Tan",
        "Xuehao Zhai",
        "Chengjin Xu",
        "Wei Li",
        "Yinghan Shen",
        "Shengjie Ma",
        "Honghao Liu",
        "Yuanzhuo Wang",
        "Jian Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of \"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 4 9 5 5 1 . 1 1 4 2 : r Survey on LLM-as-a-Judge JIAWEI GU1,*, XUHUI JIANG1,*, ZHICHAO SHI1,2,*, HEXIANG TAN2, XUEHAO ZHAI3, CHENGJIN XU1, WEI LI2, YINGHAN SHEN2, SHENGJIE MA1,4, HONGHAO LIU1, YUANZHUO WANG2, JIAN GUO1,, 1IDEA Research, International Digital Economy Academy 2Institute of Computing Technology, Chinese Academy of Sciences 3Department of Civil and Environmental Engineering, Imperial College London 4Gaoling School of Artificial Intelligence, Renmin University of China , China"
        },
        {
            "title": "ABSTRACT",
            "content": "Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of \"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains significant challenge that requires careful design and standardization. This paper provides comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by novel benchmark designed for this purpose. To advance the development and realworld deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as foundational reference for researchers and practitioners in this rapidly evolving field. https://github.com/IDEA-FinAI/LLM-as-Evaluator."
        },
        {
            "title": "1 INTRODUCTION",
            "content": "Judgment is the faculty of thinking the particular as contained under the universal. It involves the capacity to subsume under rules, that is, to distinguish whether something falls under given rule. Kant, Critique of Judgment [38], Introduction IV, 5:179; Critique of Pure Reason [37], A132/B171. Recently, Large Language Models (LLMs) have achieved remarkable success in numerous domains, ranging from artificial intelligence and software engineering to education and social science. The adoption of LLMs as evaluatorscommonly referred to as \"LLM-as-a-Judge\" [135]has surged, driven by their ability to emulate human-like reasoning and decision-making processes. This capability enables LLMs to undertake roles traditionally reserved for human experts, offering cost-effective and scalable alternative. For instance, we usually rely on experts to evaluate the accuracy of mathematics and physics competition questions at the Olympiad level [28], which can be assessed through LLM-as-a-Judge now. Additionally, in recent peer reviews of research submissions, LLM-as-a-Judge is introduced to address the rising number of paper submissions and reviewer workload1, which is designed to identify potential issues in reviews and offer constructive feedback to reviewers. These trends underscore key motivation for adopting LLM-as-a-Judge: 1* These authors contributed equally to this research. 2 Corresponding author. 1https://blog.iclr.cc/2024/10/09/iclr2025-assisting-reviewers/ . , Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. the potential to enhance evaluation efficiency while addressing limitations inherent in human assessments, such as scalability and consistency. LLM-as-a-Judge presents compelling alternative to both human evaluations and traditional automated methods, offering distinct advantages in scalability, efficiency, and adaptability. Human evaluations, though often considered the gold standard, face challenges in scalability, cost, and consistency. They are time-consuming, require substantial expert effort, and are expensive to scale due to limited availability of qualified evaluators. Coordinating and training evaluators adds complexity, and fatigue during lengthy tasks can compromise reliability and accuracy. In contrast, LLMs provide scalable, cost-effective, and efficient evaluations with reduced subjective variability, enhancing objectivity. Traditional automated methods, such as BLEU, ROUGE, and METEOR for software artifact summarization, often fail to align with human judgment or provide clear insights in specialized domains like software engineering. LLMs offer flexibility to process diverse input types, including text, semi-structured data, and multi-modal content, allowing evaluations to integrate qualitative insights with quantitative rigor. This human-aligned adaptability makes LLMs effective for complex, context-aware assessments beyond the limits of conventional metrics [10, 51, 110]. Despite its wide advantages, LLM-as-a-Judge poses significant challenges for reliability. This necessitates the capability of LLM-as-a-Judge framework to subsume under rules, that is, to distinguish whether something falls under given rule [37]. As LLM-as-a-Judge becomes more commonly used as an effective evaluator in different areas, collecting evaluations with LLM-as-a-Judge is relatively simple. Therefore, central to this survey is the fundamental question: How to build reliable LLM-as-a-Judge systems? To address this question, we explore two core aspects: (1) strategies for enhancing the reliability of LLM-as-a-Judge and (2) methodologies for evaluating reliability of LLM-as-a-Judge systems themselves. For the first aspect of enhancing LLM-as-a-Judge reliability, we review the main strategies aimed at optimizing their performance for diverse evaluation tasks. These strategies include improving consistency, mitigating biases, and refining adaptability to different assessment scenarios. For the second aspect, we examine the metrics, datasets, and methodologies used to evaluate the performance of LLM-as-a-Judge systems, discussing potential sources of bias and corresponding mitigation techniques. Building on this foundation, we introduce novel benchmark specifically designed for LLM-as-a-Judge evaluations. Using established metrics and datasets, this benchmark provides framework for analyzing the effectiveness of various reliability enhancement strategies. Additionally, we explore practical application scenarios, identify specific challenges unique to each context, and propose solutions to address these issues. Finally, we discuss future research directions, emphasizing key areas for advancing the reliability, scalability, and applicability of LLM-as-a-Judge systems. This paper aims to provide comprehensive overview of the LLM-as-a-Judge research landscape while offering insights into how reliable LLM-as-a-Judge can be constructed. We hope that this work will serve as valuable reference for researchers and practitioners, fostering further research and facilitating the real-world deployment of LLM-as-a-Judge. The rest of this survey is organized as Figure 1. In Section 2, we provide comprehensive overview of the LLM-as-a-Judge field. We define LLM-as-a-Judge through formal and informal definitions, and categorize existing methods and approaches for its use. For quick guide on implementing an LLM-as-a-Judge for specific scenarios, you can find answers in Quick Practice (2.5). Then, we discuss the problems of \"How to improve\" and \"how to evaluate\" LLM-as-a-Judge in Section 3 to 4 . Next, we show the applications of LLM-as-a-Judge in Section 6. The discussions of the challenges and future directions come at last in Section 7 and Section 8. , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge Fig. 1. Paper Structure. , Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al."
        },
        {
            "title": "2 BACKGROUND AND METHOD\nIn evaluative tasks, especially those that are subjective, human assessment is often considered\nthe gold standard due to its reliable and open-ended nature [22, 85]. However, this approach\nis typically slow and costly [124, 135, 141]. To address these challenges, LLMs are increasingly\nemployed as substitutes for human evaluators. Since these models are frequently trained using\nReinforcement Learning from Human Feedback (RLHF), they demonstrate strong alignment with\nhuman perspectives, leading to the approach known as \"LLM-as-a-Judge\".",
            "content": "In general, LLM-as-a-Judge is process that uses LLM to evaluate different objects in various scenarios for diverse tasks. For instance, roles such as \"Assessors\", \"Critics,\" and \"Verifiers\" utilize LLMs to facilitate evaluation at different stages of the process, whether during intermediate steps or throughout the entire workflow. To date, definition of how to effectively use LLM-as-a-Judge for evaluation tasks has been largely informal or vague, lacking clear and formal expression. Therefore, we will provide formal definition of LLM-as-Evaluator as follows: PL (𝑥 C) E: The final evaluation obtained from the whole LLM-as-a-Judge process in the expected manner. It could be score, choice, or sentence, etc. PL M: The probability function defined by the corresponding LLM, and the generation is an auto-regressive process. 𝑥: The input data in any available types (text, image, video), which waiting to be evaluated. C: The context for the input 𝑥, which is often prompt template or combined with history information in dialogue. : The combination operator combines the input 𝑥 with the context C, and this operation can vary depending on the context, such as being placed at the beginning, middle, or end. The formulation of LLM-as-a-Judge reflects that LLM is type of auto-regressive generative model, which generates subsequent content based on the context then obtain target evaluation from it. The form of LLM-as-a-Judge illustrates how we utilize LLM for evaluation tasks, encompassing input design, model selection and training, as well as output post-processing. The different basic approaches of implementing LLM-as-a-Judge can be classified according to the formulation: In-Context Learning, Model Selection, Post-processing Method and Evaluation Pipeline, which concluded in Figure 2. By following this pipeline, we can build basic LLM-as-a-Judge for evaluation. faster practice guide is available in section 2.5. 2.1 In-Context Learning To apply LLM-as-a-Judge, it is helpful to start by defining the evaluation task using In-Context Learning methods. This process involves two key aspects: the design of prompt and input. For input design, it is important to consider the type of variables to be evaluated (such as text, image, or video), the manner of input (e.g., individually, in pairs, or in batches), and the position of the input (e.g., at the beginning, middle, or end). As for the prompt design, four different methods can be adopted, as illustrated in Figure 2. The four methods include generating scores, solving true/false questions, conducting pairwise comparisons, and making multiple-choice selections. Further details will be provided in the following sections."
        },
        {
            "title": "2.1.1 Generating scores. It is quite intuitive to represent an evaluation using a corresponding\nscore. What requires more careful consideration, however, is the nature and range of the score used\nfor evaluation. The score can be discrete, with common ranges like 1-3, 1-5 [36], or 1-10 [51, 141].\nAlternatively, it can be continuous, ranging from 0 to 1 or 0 to 100 [117]. The simplest way to",
            "content": ", Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge Fig. 2. LLM-as-a-Judge Evaluation Pipelines. score is through the context, setting the range of scores and the main criteria for scoring. For example, \"Please rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on scale of 1 to 10, where higher score indicates better overall performance\" [141]. slightly more complex way is to provide more detailed scoring criteria. More complex scoring situations can be as Language-Model-as-an-Examiner [3], which use Likert scale scoring functions as an absolute evaluative measure showed in Figure 3. The evaluator assigns scores to given response along predefined dimensions including accuracy, coherence, factuality and comprehensiveness. Each of these dimensions is scored on scale of 1 to 3, ranging from worst to best. The evaluator is also asked to provide an overall score ranging from 1 to 5, based on the scores assigned to the previous 4 dimensions. This score serves as an indicator of the overall quality of the answer. Evaluate the quality of summaries written for news article. Rate each summary on four dimensions: {Dimension_1}, {Dimension_2}, {Dimension_3}, and {Dimension_4}. You should rate on scale from 1 (worst) to 5 (best). Article: {Article} Summary: {Summary} Fig. 3. The template for Likert scale scoring from Gao et al. [22]."
        },
        {
            "title": "2.1.2 Solving Yes/No questions. A Yes/No question requires a judgment on a given statement,\nfocusing solely on its accuracy. This type of question is simple and direct, providing only two fixed\nresponses—yes or no, true or false—without any additional comparisons or choices.",
            "content": "This type of evaluation is often utilized in intermediate processes, creating the conditions for feedback loop. For example, it promotes self-optimization cycle, as seen in Reflexion [86], which generates verbal self-reflections to provide valuable feedback for future attempts. In scenarios with sparse reward signals, such as binary success status (success/fail), the self-reflection model uses the current trajectory and persistent memory to generate nuanced and specific feedback. Similarly, in self-improvement contexts [98], Yes/No questions can be employed to evaluate custom phrases, , Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. such as \"Modification needed.\" and \"No modification needed.\", facilitating entry into the next cycle. Moreover, these evaluations are common for testing knowledge accuracy and assessing whether statements align with established facts [92], like \"Given question and the associated retrieved knowledge graph triples (entity, relation, entity), you are asked to answer whether its sufficient for you to answer the question with these triples and your knowledge (Yes or No).\" detailed and specific example can be seen in the Figure 4. Is the sentence supported by the article? Answer \"Yes\" or \"No\". Article: {Article} Sentence: {Sentence} Fig. 4. The template for Yes/No evaluation for example."
        },
        {
            "title": "2.1.3 Conducting pairwise comparisons. Pairwise comparison refers to comparing two options\nand selecting which one is superior or more aligned with a specific standard, showed in Figure 5. It\ninvolves making a decision between two options rather than judgement between ’yes’ or ’no’. The\ncomparison can be subjective or based on objective criteria. This evaluation is a relative evaluation.\nPairwise comparison is often used for ranking multiple options or prioritizing them, where several\ncomparisons are made between pairs to identify the better choice or establish a hierarchy.",
            "content": "Pairwise comparison is well-established method that has significantly impacted variety of fields [76]. As noted by [62], LLM and human evaluations are more aligned in the context of pairwise comparisons compared to score-based assessments. Numerous studies have demonstrated that pairwise comparative assessments outperform other judging methods in terms of positional consistency [63, 136]. Furthermore, pairwise comparisons can be extended to more complex relationbased assessment frameworks, such as list-wise comparisons, using advanced ranking algorithms [62, 76], data filtering [124]. In pairwise comparative assessments, LLM-as-a-Judge is prompted to select the response that better answers the question at hand. To accommodate the possibility of tie, several option modes are introduced. The Two-Option mode requires judges to choose the better response from two given options. The Three-Option mode introduces an additional choice, allowing judges to indicate tie if neither response is preferable. Evaluations typically involve determining the outcomes of win, tie, or loss for responses [110] through pairwise comparisons, with win rounds counted for each response. The Four-Option mode further expands the choices, allowing judges to classify responses as either \"both good tie\" or \"both bad tie.\" Given new article, which summary is better? Answer \"Summary 0\" or \"Summary 1\". You do not need to explain the reason. Article: {Article} Summary 0: {Summary_0} Summary 1: {Summary_1} Fig. 5. The template for pairwise comparison from Gao et al. [22] , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge"
        },
        {
            "title": "2.1.4 Making multiple-choice selections. Multiple-choice selections involve providing several\noptions, not giving relative choices in pairwise comparison, nor making a yes/no judgment. The\nevaluator must choose the most appropriate or correct one. This method allows for a broader range\nof responses compared to true/false questions and can assess deeper understanding or preferences\nand an example is showed in Figure 6. However, this kind of prompt design is more rare than the\nfirst three.",
            "content": "You are given summary and some semantic content units. For each semantic unit, choose those can be inferred from the summary, return their number. Summary: {Summary} Semantic content units: 1. {SCU_1} 2. {SCU_2} ...... n. {SCU_n} Fig. 6. The template for multiple-choice for example."
        },
        {
            "title": "2.2.2 Fine-tuned LLM. However, relying on external API for evaluation may introduce con-\nsideration about privacy leakage, and the opacity of API models also challenges the evaluation\nreproducibility. Therefore, follow-up works suggest fine-tuning language models specialized in\nevaluations. For instance, PandaLM [110] constructs data based on Alpaca instructions and GPT-3.5\nannotation, and then fine-tunes LLaMA-7B [100] as an evaluator model. JudgeLM [141] constructs\ndata from diversified instruction sets and GPT-4 annotations, and fine-tunes Vicuna [101] as a\nscalable evaluator model. Auto-J [51] constructs evaluation data upon multiple scenarios to train a\ngenerative evaluator model, which can provide both evaluation and critical opinion. Prometheus\n[40] defines thousands of evaluation criteria and construct a feedback dataset based on GPT-4,\nand fine-tunes a fine-grained evaluator model. The typical process for fine-tuning a judge model\ninvolves three main steps. Step 1: Data Collection. The training data generally consists of three\ncomponents: instructions, the objects to be evaluated, and evaluations. Instructions are typically\nsourced from instruction datasets, while evaluations can come from either GPT-4 or human an-\nnotations. Step 2-Prompt Design. The structure of the prompt template can vary based on the\nevaluation scheme, which already detailed in § 2.1. Step 3: Model Fine-Tuning. Using the designed\nprompts and collected data, the fine-tuning process for the evaluator model typically adheres to",
            "content": ", Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. the instruction fine-tuning paradigm [72]. The model receives an instruction along with one or more responses to generate output that includes evaluation results and possibly explanations. After fine-tuning, the evaluator model can be employed to evaluate the target object. While these fine-tuned models often demonstrate superior performance on self-designed test sets, they are identified several limitations in their evaluation capabilities, which detailed in Section 4.2."
        },
        {
            "title": "2.3.2 Normalizing the output logits. LLM-as-a-Judge in the intermediate steps with Yes/No\nsetting often normalize the output logits to obtain the evaluation in the form of a continuous decimal\nbetween 0 and 1. This is also very common in agent methods and prompt-based optimization\nmethods [27, 112, 144]. For example, the self-consistency and self-reflection scores [112] within\none forward pass of MEvaluator, are effectively obtained by constructing a prompt [(𝑥 ⊕ C) , \"Yes\"]\nand acquire the probability of each token conditioned on the previous tokens 𝑃 (𝑡𝑖 |𝑡<𝑖 ). The auto-\nregressive feature is leveraged, thus aggregate the probability of the relevant tokens to compute the\nself-consistent score 𝜌Self-consistency and self-reflection score 𝜌Self-reflection. The final score is produced\nby 𝜌 𝑗 = 𝜌SC,𝑗 · 𝜌SR,𝑗 .",
            "content": "𝜌SC 𝜌SR (cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32) (cid:122) (cid:123) (cid:125)(cid:124) (cid:122)(cid:125)(cid:124)(cid:123) (𝑥 C) \"Yes\" (cid:40)𝜌SC = (cid:206)𝑡𝑖 𝛼 𝑃 (𝑡𝑖 𝑡<𝑖 ) (cid:206)𝑡𝑖 𝛽 𝑃 (𝑡𝑖 𝑡<𝑖 ) 𝜌SR = (cid:206)𝑡𝑖 \"Yes\" 𝑃 (𝑡𝑖 𝑡<𝑖 ) , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge In addition, Self-evaluation [27] is also common using this method for LLM-as-a-Judge. It can be helpful to let the LLM evaluate itself by asking, \"Is this reasoning step correct?\" and then reward it based on the probability of the next word being \"Yes.\""
        },
        {
            "title": "2.3.3 Selecting sentences. In addition to selecting specific tokens and normalizing the output\nlogits, the content extracted by LLM-as-a-Judge may also be a sentence or paragraph. As showed\nin Figure 2, agent for reasoning task [27], builds a reasoning tree by iteratively considering the\nmost promising reasoning steps (actions, sub-questions) by LLM-as-a-Judge.",
            "content": "2."
        },
        {
            "title": "Evaluation Pipeline",
            "content": "There are three common scenarios for using LLM-as-a-Judge evaluation pipelines showed in Figure 2, which are LLM-as-a-Judge for LLMs, LLM-as-a-Judge for data, and LLM-as-a-Judge for agent respectively."
        },
        {
            "title": "2.4.1 LLM-as-a-Judge for model. It is universally known that the best way to evaluate LLMs is\nhuman judgment, but collecting human annotations can be costly, time-consuming, and laborious\n[72, 137]. Using strong LLMs (usually closed-source ones, e.g., GPT-4, Claude, ChatGPT) as an\nautomated proxy for assessing LLMs has become a natural choice [139]. With appropriate prompt\ndesign, the quality of evaluation and agreement to human judgment can be promising [19, 106,\n131, 137]. However, the cost concern still exists when calling the APIs of these proprietary models,\nespecially when there is a frequent need for model validation on large-scale data. Moreover, closed-\nsource LLM-as-a-Judge leads to low reproducibility due to potential changes in models behind\nthe API. Some recent works have started to make attempts for open-source alternatives. SelFee\n[121] collects generations, feedback, and revised generations from ChatGPT and fine-tunes LLaMA\nmodels to build a critique model. Shepherd [107] trains a model that can output critiques for single-\nresponse with the data of feedback from online communities and human annotation. PandaLM\n[110] trains a model to conduct pairwise comparison for LLM Instruction Tuning Optimization,\nand Zheng et al. [137] also fine-tune Vicuna [101] on a 20K pairwise comparison dataset to explore\nthe potential of open-source models as a more cost-friendly proxy.",
            "content": "Recent advancements in using Large Multimodal Models (LMMs) as evaluators have showcased their potential to perform complex judgment tasks in vision-language scenarios. Proprietary models like GPT-4V and GPT-4o have been pivotal in benchmarks such as detailed captioning and visual chats, utilizing both pointwise and pairwise evaluation methods [58, 65, 130]. Open-source alternatives have emerged, with Prometheus-Vision [47] being the first vision-language model specifically trained to act as an evaluator for user-designed scoring criteria. While PrometheusVision introduced the concept of open-source evaluators with focus on specialized tasks, it remains limited to predefined criteria. In contrast, LLaVA-Critic [117], another open-source innovation, expands the scope by serving as generalist evaluator. Trained on diverse and detailed datasets, LLaVA-Critic provides robust scoring and preference learning, closely aligning with human and proprietary evaluations. These models mark significant progress in democratizing and enhancing multimodal evaluation tools."
        },
        {
            "title": "2.4.2 LLM-as-a-Judge for data. Data annotation generally refers to the labeling or generating\nof raw data with relevant information, which could be used for improving the efficacy of machine\nlearning models. The process, however, is labor-intensive and costly. The emergence of LLMs\npresents an unprecedented opportunity to automate the complicated process of data annotation by\nLLM-as-a-Judge. Most of the data need to be evaluated by LLM-as-a-Judge is generated by models,\nor large-scale crawled data. Language models first conduct supervised fine-tuning to imitate how\nto align with human instructions [95, 109]. After that, reinforcement learning techniques have been",
            "content": ", Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. Fig. 7. LLM-as-a-Judge appears in two common forms in the agent. The left diagram is Agent-as-a-Juge, designing complete agent to serve as an evaluator. The right diagram shows using LLM-as-a-Judge in the process of an Agent. explored to align language models with human preferences [73, 79]. The most successful way is applying RLHF framework [73] via training reward model on human feedback and using PPO [83] to obtain the policy model for language generation. However, in practices, the PPO training paradigm is complex in coding and hyper-parameter tuning while it needs four models that are hard for training. This motivates us to explore simpler and more straightforward methods to align language models with human preferences. This involves how to use LLM-as-a-Judge to evaluate whether different responses are aligned with human preferences. For example, [17, 124] use general LLM (ChatGPT) to get better alignment with human preferences. The Aplaca prompts [95] is used as sampling queries to different models generate responses. And these data was evaluated by LLM-as-a-Judge to obtain human preference scores (reward score) to train new language model. Other works would like to use Supervised Fine-Tuning (SFT) model itself as evaluator, like generating better-aligned datasets for SFT including hindsight-modified prompts [59, 129] and principle-driven self-alignment [94]. In addition, the lack of domain-specific model training data is common phenomenon. In order to obtain annotated high-quality data, it is also very common to use LLM-as-a-Judge for the generation and evaluation of domain data. WizardMath [66] would use its Instruction Reward Model (IRM) as Evaluator, aiming to judge the quality of the evolved instructions on three aspects: i) Definition, ii) Precision, and iti) Integrity. To produce the ranking list training data of IRM, for each instruction, ChatGPT and Wizard-E are used to generate 2-4 evolved instructions respectively. Then we leverage Wizard-E to rank the quality of those 4-8 instructions. Recent research on evaluating multimodal data focuses on addressing vision-language misalignments in Multimodal Large Language Models (MLLMs), which often cause hallucinationsoutputs inconsistent with visual or contextual evidence [15, 54, 104]. Techniques like Reinforcement Learning from Human Feedback (RLHF) and Factually Augmented RLHF have been employed to improve model alignment by incorporating structured ground-truth data and image captions, enhancing hallucination detection [93]. Benchmarks such as MLLM-as-a-Judge [9] assess these models using tasks like scoring, pair comparison, and batch ranking, revealing limitations in alignment with human preferences. Persistent issues include biases (e.g., position, verbosity) and hallucinations, , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge with even advanced models like GPT-4V displaying challenges. While pair comparison tasks align better with human judgment, scoring and batch ranking require significant improvements for reliable deployment. These findings emphasize the need for innovative frameworks and datasets to refine MLLM evaluation and alignment."
        },
        {
            "title": "2.4.3 LLM-as-a-Judge for agent. There are two ways to apply LLM-as-a-Judge for an agent.\nOne is to evaluate the entire process of the intelligent agent [145], and the other is to evaluate it at\na specific stage in the agent framework process [27, 86]. Both approaches are briefly illustrated in\nFigure 7. Using LLM as the brain of agent, an agentic system [145] could evaluate like a human, it\nwould reduce the need for human involvement and eliminate the trade-off between thoroughness\nand effort. In addition, the agent [86] can interact with the environment through language and\nreceive feedback on actions through LLM to make decisions for the next action.",
            "content": "2."
        },
        {
            "title": "Quick Practice",
            "content": "Fig. 8. Flowchart of Quick Practice To effectively apply LLM-as-a-Judge design, it is more recommended to find more effective settings in the testing cycle for different scenarios. The process of quick practice for LLM-as-a-Judge involves four main stages. First, thinking, where users define the evaluation objectives by determining what needs to be evaluated, understanding how humans typically perform such evaluations, and identifying some reliable evaluation examples. Next is prompt design, detailed in Section 2.1. The most efficient and generally effective approach involves specifying scoring dimensions, emphasizing relative comparisons for improved assessments, and creating effective examples to guide the LLM. The third stage, model selection (Section 2.2), focuses on choosing large-scale model with strong reasoning and instruction-following abilities to ensure reliable evaluations. Finally, standardizing the evaluation process ensures that the outputs are structured (Section 2.3). This can be achieved by using specific formats like boxed{XX}, numerical scores, or binary responses (e.g., \"Yes\" or \"No\"). The entire process includes iterative testing with cases and refinement through retesting to enhance reliability."
        },
        {
            "title": "3 IMPROVEMENT STRATEGY\nWhen directly utilizing LLMs to conduct evaluation tasks such as scoring, selection, pairwise\ncomparison or ranking, the inherent biases of LLMs like length bias, positional bias and concreteness\nbias[75] will lead to poor evaluation results. Addressing these inherent biases and improving the",
            "content": ", Vol. 1, No. 1, Article . Publication date: November 2024. Design Strategy of Evaluation Prompts (Sec. 3.1) Improvement Strategy Improvement Strategy of LLMs Abilities (Sec. 3.2) Optimization Strategy of Final Results (Sec. 3.3) J. Gu, X. Jiang, Z. Shi, J. Guo, et al. Optimizing LLMs Understanding of Evaluation Tasks Few-shot prompting: FActScore [69] / SALAD-Bench [52] / GPTScore [20] Evaluation steps decomposition: G-Eval [60] / DHP [111] / SocREval [29] / BSM [81] Evaluation criteria decomposition: HD-Eval [61] / Hu and Gao et al. [30] Shuffling contents: Wang et al. [105] / Auto-J [51] / JudgeLM [141] / PandaLM [110] Optimizing LLMs Output Forms Fine-tuning via Meta Evaluation Datasets Iterative Optimization Based on Feedbacks Integration of multiple Evaluation Results Direct Optimization of LLMs Outputs Conversion of evaluation tasks: Liu et al. [62] Constraining outputs in structured formats: G-Eval [60] / DHP [111] / LLM-EVAL [12] Providing evaluations with explanations: CLAIR [8] / FLEUR [48] PandaLM [110] / SALAD-Bench [52] / OffsetBias [75] / JudgeLM [141] / CritiqueLLM [39] INSTRUCTSCORE [118] / JADE [127] Summarize by multiple rounds: Sottana et al. [90] / PsychoBench [32] / Auto-J [51] Vote by multiple LLMs: CPAD [56] / Bai et al. [4] Score smoothing: FLEUR [48] / G-Eval [60] / DHP [111] Self validation: TrueTeacher [23] Fig. 9. Structure of Improvement Strategy. overall evaluation performance of LLMs is critical challenge for applying LLMs as evaluators. In this section, we introduce three improvement strategy aimed at enhancing the evaluation performance of LLM-as-a-judge: design strategy of evaluation prompts (in-context learning based), improvement strategy of LLMs evaluation capabilities (model based), and optimization strategy of final evaluation results (post-processing based). Our categorization is based on the formal definition of LLM-as-Evaluator in Section 2, focusing on enhancing the evaluation effectiveness by targeting three key phases of the process: the context , the abilities of LLMs themselves PL and the post processing to obtain the final results E"
        },
        {
            "title": "3.1.1 Optimizing LLMs’ Understanding of Evaluation Tasks. In optimization methods of\nprompting LLMs to better understand evaluation tasks, one of the most commonly used and\neffective approaches is few-shot prompting[7]. By incorporating several high-quality evaluation\nexamples into the evaluation prompts, LLM evaluators can effectively grasp the objectives, general\nprocesses and rough evaluation criteria of evaluation tasks. Many research works employ this\nprompt paradigm for evaluation, such as FActScore[69], SALAD-Bench[52] and GPTScore[20].",
            "content": "In addition to providing hight-quality examples for LLMs to inference, refining the evaluation task instructions is also an effective approach to optimize LLMs understanding of evaluation tasks. Current methods for refining evaluation tasks mainly including the decomposition of evaluation steps and criteria: (a) Decomposition of Evaluation Steps entails breaking down the entire evaluation tasks into smaller steps, providing detailed definitions and constraints for each small , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge step in prompts, thereby guiding LLMs comprehensively through the whole evaluation pipeline. For instance, G-Eval[60] and DHP[111] use Chain-of-Thought(CoT)[113] to provide guidance for LLMs. SocREval[29] employs the Socratic method to meticulously design each step to enhance evaluation performance. Saha et al. proposes Branch-Solve-Merge(BSM)[81], which divides evaluation tasks into multiple parallel sub-tasks for separate evaluation and final merge. (b) Decomposition of Evaluation Criteria involves breaking down coarse evaluation criteria like Fluency into finergrained sub-criteria like Grammar, Engagingness and Readability, and then generating overall scores based on these difference dimensions. HD-Eval[61] iteratively aligns LLM evaluators with human preference via hierarchical criteria decomposition and thereby addressing the potential bias in LLMs. Hu and Gao et al.[30] summarize and clearly define an explicit hierarchical classification system encompassing 11 criteria, addressing the issue of LLMs potentially confusing different evaluation standards. These refinements specific to enable LLMs to understand the details of evaluation tasks more deeply, thereby aligning evaluation results more closely with human evaluation requirements and preferences. Furthermore, the evaluation capabilities can be optimized based on specific shortcomings of LLMs in prompts. For instance, to address specific biases like position bias which is common in pairwise evaluations, several research efforts have optimized prompts design by randomly swapping contents to be evaluated. Wang et al.[105] analyzed and validated the impact of position bias on LLM-as-a-judge, and proposed calibration framework to mitigate this bias by swapping the contents and averaging the scores. Auto-J[51] and JudgeLM[141] also enhance the evaluation consistency by shuffling the texts to be evaluated. In contrast to averaging scores, PandaLM[110] annotates the conflicting evaluation results after swapping as \"Tie\" to address the position bias. To address the challenge of LLMs absolute scoring being less robust than relative comparing[77], some research works convert scoring tasks into pairwise comparison, thereby enhancing the reliability of evaluation results. Liu et al.[62] transform the scoring evaluation to ranking evaluation and introduce Pairwise-Preference Search (PARIS), which employs LLMs to conduct pairwise comparisons locally and efficiently ranks candidate texts globally, making evaluation results more aligned with human preferences. In summary, the design of prompts for better understanding evaluation tasks is core method for optimizing LLMs in-contextual learning abilities. By refining evaluation task instructions and criteria in prompts or few-shot prompting with high-quality examples, the details of evaluation prompts can be enriched and the understanding of LLMs on evaluation tasks can be directly or indirectly enhanced. Additionally, targeted adjustments to prompts can address potential biases of LLMs such as position bias."
        },
        {
            "title": "3.1.2 Optimizing LLMs’ Output Forms. Directly requiring LLM evaluators to output evaluation\nresults poses robustness problems. The response text may unexpectedly vary due to the inherent\ngenerative randomness of LLMs, such as outputting text like \"low relevance\" while asked to measure\nit with discrete scores, which hinders the automated and accurate extraction of evaluation results\nfrom LLMs’ output. An effective method to enhance the robustness of output forms is to constrain\nLLMs’ output in structured formats within prompts. G-Eval[60] and DHP framework[111] perform\nevaluation tasks with a form-filling paradigm, constraining outputs with formats like \"X: Y\", where\nX represents the dimension or metric to be evaluated and Y denotes an identifiable output form\nlike scores or specific tokens. LLM-EVAL[12] further codifies this form-filling paradigm, efficiently\noutput evaluation results in JSON dictionary format and obtain multidimensional scores, leveraging\nLLMs’ high understanding and generation capabilities of code-like textural formats.",
            "content": "Apart from challenges in robustness, directly outputting evaluation results by LLMs also suffer from the lack of interpretability. The meaning of evaluation results from LLM evaluators is difficult , Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. to align consistently with instructions and metrics provided in prompts. To address the challenges, CLAIR[8] requires LLMs to output evaluation scores between 0-100 simultaneously with relevant reasons as explanations in JSON format, which enhancing the rationality and interpretability of the scores. FLEUR[48] utilizes LLaVA to first provide quality scores for image captions and subsequently asks with \"Why? Tell me the reason.\" for explanations with the images, captions and scores as inputs, offering stepwise approach to provide interpretable scores. In general, by constraining or guiding the output process and format of LLM evaluators within prompts, the robustness and rationality of evaluation results can be effectively improved through structured outputs. This also facilitates the automated post-processing of evaluation results in subsequent steps, thereby enhancing the overall stability of the evaluation pipeline."
        },
        {
            "title": "3.2.1 Fine-tuning via Meta Evaluation Datasets. A straightforward approach to enhancing\nthe evaluation capabilities of LLMs is to fine-tune them via meta evaluation datasets specifically\nconstructed for evaluation tasks, which helps improve the LLMs’ understanding of specific evalua-\ntion prompts, boosts the evaluation performance, or addresses potential biases. The most critical\nstep in this optimization strategy is the collection and construction of training data. A common\nmethod involves sampling evaluation questions from publicly available datasets, modifying them\nwith certain templates, and supplementing the dataset with evaluation responses generated either\nmanually or by powerful LLMs like GPT4. For instance, PandaLM[110] samples inputs and instruc-\ntions from Alpaca 52K[95] and generate responses using GPT-3.5 to construct training data, while\nSALAD-Bench[52] builds its training data from a subset of LMSYS-Chat[134] and Toxicchat[55].\nTo better align with the requirements of evaluation tasks, many research works further transform\ninputs and instructions sampled from public datasets to construct more targeted training data.\nOffsetBias[75] aims to reduce biases of LLMs by using GPT4 to generate off-topic versions of the\noriginal inputs and then having GPT-3.5 respond to the new inputs to produce bad responses. By\npairing good and bad responses as training data to fine-tune the LLMs as evaluators, the biases in\nLLMs are significantly reduced, including length bias, concreteness bias, knowledge bias and so\non. JudgeLM[141] enhances LLMs’ evaluation capabilities by creating different types of training\ndata through paradigms like reference support and reference drop. CritiqueLLM[39] proposes a\nmulti-path prompting approach, combining pointwise-to-pairwise and referenced-to-reference-free\nprompting strategies to restructure referenced pointwise grading data into four types, which helps\ncreate Eval-Instruct to fine-tune LLMs, addressing shortcomings in pointwise grading and pairwise\ncomparison.",
            "content": "In summary, constructing meta evaluation training data targeted at specific evaluation tasks and fine-tuning LLMs can directly adjust the models internal parameterized knowledge and language , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge abilities. This is the most straightforward method to improve the evaluation performance of LLM evaluators and address potential biases. Iterative Optimization Based on Feedback of Evaluation Results. Fine-tuning LLMs 3.2.2 on meta evaluation datasets give them the ability to produce evaluations which are more aligned with human preferences. However, LLM-as-a-judge may still introduce biases during evaluation process in practice, which can impact the overall evaluation quality. natural improvement strategy is to iteratively optimize the model based on feedback of evaluation results, which mainly comes from stronger models or directly from human evaluators correction of the evaluation results. typical example is INSTRUCTSCORE[118]. To improve model performance and further benefit the final quality score calculation, this score framework collects failure modes of metric outputs, query GPT-4 on each failure mode to gather automatic feedback, and finally selects explanations most aligned with human preferences to iteratively fine-tune the LLaMA model. Unlike INSTRUCTSCORE which directly optimizes the model, the LLM evaluator in JADE[127] relies on human judges to correct LLMs evaluation results and updates the most frequently corrected samples into the example sets for few-shot prompting. JADE utilizes this relatively low-cost method to achieve iterative updates of the evaluation capabilities. Since the feedback is more closely aligned with human preferences, LLM evaluators can dynamically align with human when optimizing evaluation capabilities based on this feedback, leading to better evaluation results. This feedback-based iterative optimization strategy address the problem of models imperfect generalization and improve the evaluation capabilities through dynamic updates."
        },
        {
            "title": "3.3 Optimization Strategy of Final Results\nThrough the optimization based on in-context learning and the model’ own capabilities, LLMs have\nbecome fairly reliable evaluators which are capable of understanding evaluation task requirements\nand providing rational evaluation results. However, the inherent generation randomness within\nthe black box of LLMs still introduces significant instability to the entire evaluation pipeline, affect-\ning the overall evaluation quality. Therefore, optimization strategies during the post-processing\nstage from LLM evaluators’ outputs to final evaluation results are necessary. In this survey, these\noptimization strategies are categorized into three types: integration of multiple evaluation results,\ndirect optimization of LLMs’ outputs, and conversion of evaluation tasks from pointwise evaluation\nto pairwise comparison.",
            "content": "Integration of Multiple Evaluation Results. Integrating multiple evaluation results for 3.3.1 the same content to obtain the final result is common strategy in various experiments and engineering pipelines, which can reduce the impacts of accidental factors and random errors. The most basic optimization strategy is to perform multiple runs of evaluation on the same content with different hyper-parameters and settings, and then summarize these results. For example the work of Sottana et al.[90] reduces randomness in evaluations by averaging multiple scores of the same sample. Similarly, PsychoBench[32] takes the mean and standard deviation from ten independent runs. Auto-J[51] further amplifies the differences between evaluation rounds, which combine critiques with and without scenario criteria to obtain the final results. In addition to integrating results from multiple rounds of evaluation, using multiple LLM evaluators to assess the contents simultaneously and the integrating the results is another effective method, which can reduce biases introduced by LLMs. For instance, CPAD[56] utilizes ChatGLM-6B[18], Ziya-13B[125] and ChatYuan-Large-v2[126] as evaluators to evaluate the contents and obtain the final results by voting. Bai et al.[4] propose novel evaluation method called decentralized peer , Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. review of LLMs, which utilizes LLMs that generate contents to evaluate each others generated contents and eventually integrate the results. In summary, forming the final evaluation results by combining multiple rounds of evaluations or multiple LLM evaluators can reduce the random effects caused by accidental factors in single round and reduce the potential biases of single LLM evaluator. This strategy significantly enhances the stability and reliability of the evaluation results."
        },
        {
            "title": "3.3.2 Direct Optimization of LLMs’ Outputs. Different from obtaining evaluations results\nbased on the outputs of multiple rounds or LLMs, directly optimizing the output of single LLM\nevaluator involves further processing the evaluation output to make it more reliable, especially\nwhen dealing with scoring outputs from LLM evaluators. Due to the inherent randomness in LLMs’\ngeneration, the scores may not fully reflect the LLMs’ complete view of the evaluation criteria.\nTherefore, to obtain more reliable evaluation results, it is necessary to optimize the LLM’s score\noutputs. An effective optimization strategy is to combine the implicit logits which capture the LLMs’\nrandomness with the explicit output scores. For example, FLEUR[48] proposes a score smoothing\nstrategy. For scores generated by LLaVA, the probability of the token corresponding to each digit\n𝑙 (0≤ 𝑙 ≤9) would be used as the weight to smooth the explicit scores and calculate the final\nevaluation scores.",
            "content": "However, methods like score smoothing, which combine implicit logits and explicit outputs require the LLMs to be open-source, or to provide interfaces that allow access to token probabilities, which brings some limitations. Inspired by the work of Weng et al.[114] and Madaan et al.[68], self-verification can be used to filter out the evaluation results without sufficient robustness. For example, TrueTeacher[23] applies self-verification in its evaluation of distilled data by asking the LLM evaluator for its certainty about the evaluation results after providing them, and retaining only those results that pass self-verification. Self-verification is suitable for all LLMs and require no complex computing and processing. In summary, compared to integrating multiple evaluation results, directly optimizing the LLMs outputs to obtain the final results is faster and more low-cost, although the effectiveness still needs further validation. However, these two approaches are not mutually exclusive. Performing integration after direct optimization of LLMs output may lead to more stable evaluation results."
        },
        {
            "title": "4.1 Basic Metric\nThe main objective of LLM-as-a-judge is to achieve alignment with human judges. Numerous\nstudies approach this by considering the LLM evaluator as a virtual annotator and evaluating the\nextent of its agreement with human annotators. The percentage agreement metric represents the\nproportion of samples on which LLM and human annotators agree [96].",
            "content": "Agreement = (cid:205)𝑖 I(Sllm = Shuman) where is the dataset, 𝑆llm and 𝑆human is the evaluation result of LLM evaluator and human judge respectively, which can be in the form of both score or rank. Additionally, widely used correlation , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge metrics such as Cohens Kappa [96] and Spearmans correlation [4, 62] are also employed to access agreement. Other works treat the LLM-as-a-judge task as classification problem, where human annotations serve as the labels, and compute precision, recall, and F1 scores to evaluate the performance [110, 142]. Both of above metrics rely on the datasets with LLM-generated response and responding human judgements. Therefore, there is also practical need to construct comprehensive benchmark for the meta-evaluation. In [137], MTBench and Chatbot Arena Conversations are proposed. The former has only 80 human-crafted queries, each with several LLMs responses and expert-level human annotation on pairwise comparison; the latter is large collection of crowdsourced data, with more than 30K queries from real-world users and their vote on pairs of responses from different LLMs. FairEval [106] is based on the 80 queries from VicunaBench [101] with human annotated labels between ChatGPT and Vicuna responses. PandaLM [110] constructs test set comprising 999 pairwise samples, with queries from 252 user-oriented instructions in [108]. LLMEval2 [131] is much larger than the previous two, with 2,553 samples compiled from multiple data sources with human-annotated preferences. Shepherd [107] collects 352 samples from multiple sources for its critique model as test set to evaluate the quality of the critiques. Table 1 shows the benchmarks and their statistics. Current meta-evaluation primarily focuses on LLM-as-a-judge for models, while there is lack of sufficient meta-evaluation when these LLM evaluators are used for automatically annotating largescale datasets (Section 2.4.2). We advocate for more rigorous accessment of the alignment between LLM-as-a-judge and human judgment when they are employed for large-scale data annotation. Additionally, it is also crucial to assess the potential bias and robustness, which will be discussed in the following sections. Benchmark Release Year Size Annotation Format Evaluation Dimension Agreement Position Bias Length Bias Bias Types MTBench [137] Chatbot Arena [137] FairEval [106] PandaLM [110] LLMEval2 [131] Shepherd [107] EvalBiasBench [75] CALM [120] 2023 2023 2023 2023 2023 2023 2023 2024 80 30k 80 - 2553 1317 80 4356 Pairwise Pairwise Pairwise Pairwise Pairwise Score Pairwise Pairwise & Score Table 1. Benchmark for meta-evaluation of LLM-judge. 3 3 1 0 0 0"
        },
        {
            "title": "4.2 Bias\nIn this section, we systematically review various types of biases in the LLM-as-a-judge context,\nincluding their definitions, relevant metrics, and datasets that can be used for evaluation.",
            "content": "Position Bias is the tendency of LLM evaluators to favor responses in certain positions within the prompt [85, 96, 105, 120]. This bias may have detrimental effects, as Vicuna-13B could outperform ChatGPT when evaluated by ChatGPT, simply by positioning the response of Vicuna-13B in the second place [105]. To measure this bias, [85] proposed two metrics: Position Consistency, which quantifies how frequently judge model selects the same response after changing their positions, and Preference Fairness, which measures the extent to which judge models favor response in certain positions. [105] introduced metric Conflict Rate to measure the percent of disagreement after change the position of two candidate responses. Their analytical experiments reveal that the , Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. degree of positional bias fluctuates depending on the disparity in response quality and the preferred position varies with different LLMs. For instance, GPT-4 tends to favor the first position, while ChatGPT shows preference for the second position. Length Bias refers to the tendency to favor responses of particular length, such as preference for more verbose responses which is also known as verbosity bias [31, 75, 120, 137]. Length bias can be revealed by rephrasing one of the original response into more verbose one [120, 137]. Even though these expansions do not introduce new information, there is still concern regarding changes to the original response in terms of perplexity, fluency, or style. Alternatively, [82] investigated this bias by comparing multiple sampled responses and revealed statistical tendency towards longer answers. However, ensuring the comparable quality of multiple samples remains challenging problem. Self-Enhancement Bias describe the phenomenon that LLM evaluators may prefer response generated by themselves [120, 137]. Considering the significant self-enhancement bias, [120] suggested that we should avoid using the same model as the evaluator. This is only stopgap, as we may not use the optimal evaluator when evaluating the most advanced LLMs. Other Bias. Diversity Bias refers to bias against certain demographic groups [120], including certain genders [11], race, and sexual orientation [46]. [11] revealed evaluators tendency toward visually appealing content, regardless of its actual validity, such as the text with emoji. Concreteness bias reflects that LLM evaluators favor responses with specific details, including citation of authoritative sources, numerical values and complex terminologies, which is called authority bias [75] or citation bias [11, 120]. Furthermore, LLM evaluators may favor response with certain emotional tones, such as cheerful, sad, angry, and fearful, which is defined as seyiment bias [49, 120]. To advance the development of LLM-as-a-Judge systems, future efforts should address two key challenges: (i) Need for Systematic Benchmark. Due to the diversity of biases, it is crucial to propose systematic benchmark to evaluate the extent of various biases. As shown in Table 1, [75] proposed EVALBIASBENCH as test set to measure six types of bias. [120] is dedicated to proposing unified bias testing process, including automated perturbation and unified metric. They constructed bias quantification framework CALM, which covers 12 types of bias. Despite these efforts, there is still no systematic benchmark and dataset that includes all types of biases. (ii) Challenges of Controlled Study. When conducting an investigation into certain type of bias, it is challenging to isolate the specific direction of interest from other biases and quality-related characteristics. For instance, in the case of position bias, lengthening the response could potentially alter the style, fluency, and coherence, or even introduce new biases such as self-enhancement bias. Additionally, the tendency for GPT-4 to favor its own responses over those of GPT-3.5 can be interpreted as either self-enhancement bias or proper tendency towards higher quality text. Therefore, it is essential for analytical work to carefully control for these variances."
        },
        {
            "title": "4.3 Adversarial Robustness\nAdversarial robustness refers to the ability of a model to withstand deliberate attempts to manipulate\nthe scores through carefully crafted inputs. Unlike bias evaluations (Section 4.2) which mainly\nfocus on naturally occurring samples, adversarial robustness involves samples intentionally crafted\nto manipulate scoring, such as inserting phrases that artificially enhance scores. Robustness is\ncrucial because insufficient robustness allows trivial manipulations to deceive the evaluators and\nto undermine the evaluation of text quality. Ensuring robust evaluators is essential for maintaining\naccurate and reliable assessments, particularly in high-stakes applications.",
            "content": "[77] constructed surrogate model from the black-box LLM-evaluator and the learn adversarial attack phrases based on it. The evaluation score can be drastically inflated by universally inserting the learned attack phrases without improving the text quality. Furthermore, [138] demonstrated , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge that even \"null model\" that outputs constant response irrelevant to input instructions can achieve high win rates for various LLM-as-a-judge methods. [42, 120] proposed to increase the evaluation score by adding the majority opinions, such as 90% believe this is better. [42, 120] evaluated robustness against meaningless statement in the System Prompt, e.g., \"Assistant loves eating pasta. These works revealed that LLM-as-a-judge are still insufficiently robust against interference irrelevant to text quality. Defensive measures like the perplexity score [33, 77] can only detect limited types of adversarial examples. Therefore, constructing more robust LLM-as-a-judge is crucial research direction for the future."
        },
        {
            "title": "5.1 Experiment Settings\n5.1.1 Evaluation Dimensions and Benchmarks. The most direct meta-evaluation metric for reflecting\nthe quality of automatic evaluation is the alignment with human evaluation. We use LLMEval2 [131]\nto assess the alignment of LLM-as-a-judge with human evaluations. LLMEval2 is the largest and\nmost diverse evaluation benchmark for LLM-as-a-judge to date, with 2,553 samples compiled from\nmultiple data sources with human-annotated preferences. Each sample consists of a question, a\npair of candidate responses, and a human label indicating the preferred response.",
            "content": "Bias is also crucial dimension for assessing the quality of LLM-as-a-judges evaluation results. We use EVALBIASBENCH [75] to measure six types of biases in LLMs, including length bias, concreteness bias, empty reference bias, content continuation bias, nested instruction bias, and familiar knowledge bias. EVALBIASBENCH consists of 80 samples, each containing question, pair of candidate responses, and label indicating the correct response without bias influence. In addition to the six types of biases, we also evaluated position bias. The meta-evaluation samples for position bias are the paired samples constructed by swapping the position of candidate responses within prompts in samples of LLMEval2 and EVALBIASBENCH."
        },
        {
            "title": "5.1.2 Evaluation Metrics. For the alignment with human evaluation, we use the Percentage\nAgreement Metric for evaluation[96], as shown in Section 4.1. For biases, we use the Accuracy\nfor evaluation, which represents the proportion of samples on which LLMs select the correct\ncandidate response annotated in EVALBIASBENCH.",
            "content": "For position bias, we use Position Consistency as metric, which quantifies how frequently judge model selects the same response after changing their positions. Formally, given 𝑁 samples {(𝑞𝑖, 𝑟 1𝑖, 𝑟 2𝑖 )}𝑁 𝑖=1, for each sample (𝑞𝑖, 𝑟 1𝑖, 𝑟 2𝑖 ), we query the LLM evaluator with two prompts and 𝑆𝑟 21 𝑃 (𝑞𝑖, 𝑟 1𝑖, 𝑟 2𝑖 ) and 𝑃 (𝑞𝑖, 𝑟 2𝑖, 𝑟 1𝑖 ), and obtain corresponding two evaluation results 𝑆𝑟 12 . 𝑖 𝑖 , Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. Model Alignment with Human (n=5106) Position Length (n=2633) (n=34) Concreteness (n=28) Empty Content Nested Familiar Reference Continuation Instruction Knowledge (n=26) (n=24) (n=24) (n=24) GPT-4-turbo LLaMA3-8B-Instruct GPT-3.5-turbo 61.57 50.72 54. 80.49 38.85 68.78 91.18 20.59 20.59 89.29 57.14 64.29 65.38 65.38 23.08 95.83 75.00 91.67 70.83 45.83 58. 100.0 54.17 54.17 Biases GPT-3.5-turbo - w/ explanation - w/ self-validation - w/ multi rounds Multi LLMs Table 2. The meta-evaluation results for LLMs and improvement strategies based on GPT-3.5-turbo. All the values are percentages. 48.97 69.31 70 11 32.28 38.46 23.08 23.08 46.15 52.47 54.86 54.68 57. 41.67 41.67 54.17 66.67 50.00 50.00 50.00 62.50 91.67 91.67 95.83 87.50 60.71 60.71 67.86 64.28 35.29 23.53 26.47 26.47 Each 𝑆𝑖 is 𝑟 1𝑖 , 𝑟 2𝑖 or \"TIE\". Then we calculate the position consistency as follows: Position Consistency = (cid:205)𝑁 𝑖=1 I(𝑆𝑟 12 𝑖 𝑁 = 𝑆𝑟 21 𝑖 ) where I() is the indicator function."
        },
        {
            "title": "5.2 Experiment Results and Analysis\nThe experiment results are shown in Table 2. Comparing the evaluation performance of different\nLLMs, we found significant differences between them. GPT-4 outperforms GPT-3.5 and LLaMA3-\n8B-Instruct with a large margin across all meta-evaluation dimensions and shows fewer biases.\nTherefore, when economically feasible, using GPT-4 for automatic evaluation may obtain objective\nresults with minimal biases. LLaMA3-8B-Instruct and GPT-3.5 have similar metrics across most\nmeta-evaluation dimensions. However, LLaMA3-8B-Instruct performs poorly in position bias, while\nGPT-3.5 struggles in empty reference bias. Since LLaMA3-8B-Instruct is open-source, it may also\nachieve good evaluation performance as a relatively lightweight automatic evaluator after specific\ndebiasing fine-tuning.",
            "content": "Comparing the performance of different improvement strategies, it reveals that no all modifications effectively improve LLM-as-a-judges evaluation outcomes. Providing with Explanation 2https://platform.openai.com/docs/models 3https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge (- w/ explanation) provides interpretability by offering reasons alongside evaluation scores or selections, which aids in logical backtracking during human review. However, in terms of evaluation performance and bias mitigation, it generally has negative impact. This performance decline is speculated to be caused by deeper biases introduced by self-explanation. Self Validation (- w/ self-validation) shows minimal effectiveness, likely due to the LLMs overconfidence, which may limit its re-evaluation efforts during self-validation. We will further discuss this limitation in Section 7.1. Summarize by Multiple Rounds (- w/ multi rounds) is strategy with clear benefits, showing improvements across multiple dimensions. It suggests that repeated evaluations help reduce the impact of randomness in LLMs, thereby addressing bias issues. Vote by Multiple LLMs (Multi LLMs) shows noticeable decline in performance of position bias, but with either slight improvements or minimal differences in other dimensions, when compared to GPT-3.5-turbo. This trend is influenced by the comprehensive performance of GPT-4-turbo, GPT-3.5-turbo, and LLaMA3-8B-Instruct. GPT-4turbo generally aids in correction, but it is significantly impacted in position bias, where LLaMA3-8BInstruct underperforms. Similarly, GPT-3.5-turbos weak performance in handling empty reference bias notably reduces the corresponding metric. This indicates that when multiple LLMs are used for joint evaluation, the differences in their capabilities need to be carefully considered. In summary, due to the inherent capabilities and potential risks of LLMs, common improvement strategies for LLM-as-a-judge are not fully effective in improving the evaluation performance or mitigating biases. The limitations and challenges will be further discussed in Section 7."
        },
        {
            "title": "6.1.2 Multi-Modal AI Applications. In the field of multi-modal AI, benchmarks have been created\nto assess LLM-based systems that function across text and vision modalities. These benchmarks\nhave enabled the evaluation of tasks such as image captioning and mathematical reasoning, where\nLLMs aligned with human preferences in pairwise comparisons but performed poorly in scoring\nand batch ranking [9]. For Chinese multi-modal alignment, benchmarks have identified challenges\nin coherence and reasoning, leading to the proposal of a calibrated evaluation model that achieves\ngreater consistency than existing systems [116]. Furthermore, advancements in multi-modal and\nmulti-agent systems have been reviewed, emphasizing collaboration mechanisms to improve\nrationality and minimize biases [34].",
            "content": ", Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al."
        },
        {
            "title": "6.2 Other specific domains",
            "content": "Finance. LLMs have demonstrated significant potential in the finance domain, particularly 6.2.1 in tasks such as forecasting, anomaly detection, and personalized text generation [133], thereby driving an increasing demand for LLM evaluators. In the context of LLM-as-a-judge applications within finance, expert knowledge is crucial for domain-specific evaluations. Current research can be divided into two areas: one focuses on designing LLM-based evaluators that leverage expert knowledge for specific tasks. For instance, Brief et al. (2024) conducted case study on multi-task fine-tuning in finance to enhance LLM performance [6], while Yu et al. (2024) introduced FinCon, multi-agent system that uses conceptual verbal reinforcement to improve financial decision-making [123]. The second area of research aims to provide benchmarks to evaluate and enhance LLMs understanding of domain-specific knowledge. These benchmarks include user-feedback-based UCFE [119], IndoCareera dataset of professional exam questions [43], and AI-generated domain-specific evaluation sets [78]. Additionally, the concept of LLM-as-a-judge shows promising applications in credit scoring [2, 122] and Environmental, Social, and Governance (ESG) scoring [133]. This work remains in its early stages, necessitating further exploration to refine evaluation methods and expand applications in the finance domain."
        },
        {
            "title": "6.2.2 Law. LLMs have shown growing capabilities in providing professional advice in specialized\nfields such as legal consultation, particularly excelling in tasks like text summarization and legal\nreasoning. However, compared to other fields, the legal sector is more concerned about potential\nbiases and factual inaccuracies within LLMs. Similar to the finance domain, existing research in\nlaw can be divided into two main categories.",
            "content": "The first category focuses on developing LLM evaluators specifically for legal applications by addressing professional limitations or designing evaluators themselves. For example, Cheong et al. (2024) propose four-dimensional framework for constructing responsible LLMs for legal advice, emphasizing (a) user attributes and behaviors, (b) the nature of queries, (c) AI capabilities, and (d) social impacts [13]. Ryu et al. (2023) developed Eval-RAG, retrieval-augmented generator (RAG)- based evaluator that assesses the validity of LLM-generated legal texts. Testing on Korean legal question-answering task, they found that combining Eval-RAG with traditional LLM evaluation methods aligns more closely with human expert evaluations [80]. The second category of research involves creating benchmarks for evaluating LLM applicability in legal scenarios. Examples include multi-domain evaluation sets, such as the IndoCareer dataset for professional exams in Indonesia [43] and LegalBench, collaboratively built benchmark for assessing legal reasoning capabilities in LLMs across multiple domains and languages [25]. These benchmarks are often language-specific like LexEval for Chinese legal texts [50] and Eval-RAG for Korean [80]. Other benchmarks target specific attributes, such as ethics [128] and harmfulness [1]."
        },
        {
            "title": "6.2.3 AI for Science. LLMs have demonstrated notable potential in scientific fields, especially in\nareas like medical question-answering and mathematical reasoning, where they serve as evaluators\nto improve accuracy and consistency. In medical applications, studies by Brake et al. (2024) and\nKrolik et al. (2024) showed that models like LLaMA2 can assess clinical notes and Q&A responses\nwith a level of accuracy approaching that of human experts [5, 44]. This approach leverages prompt\nengineering to embed expert knowledge, enabling LLMs to handle complex, nuanced information,\nwhich provides a reliable first-line assessment that lessens the load on human experts.",
            "content": "In mathematical reasoning, reinforcement learning (RL) and cooperative reasoning methods further enhance LLMs capability as an evaluator, especially for theory-proofing works [64]. For example, WizardMath was introduced by employing RL through step-by-step feedback to refine , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge reasoning in mathematical tasks [67]. Zhu et al. (2023) proposed Cooperative Reasoning (CoRe) framework that combines generation and verification to mimic human-like dual-process reasoning, enhancing the models problem-solving accuracy [143]. Additionally, Lu et al. (2023) developed MathVista, benchmark for evaluating mathematical reasoning in visual contexts, which assesses LLMs like GPT-4V on tasks involving mathematical reasoning with visual components [64]. These methods highlight the value of combining RL, cooperative reasoning, and prompt engineering in improving LLMs evaluative and reasoning skills across mathematical reasoning."
        },
        {
            "title": "6.2.4 Others. LLMs have been employed as evaluators across various fields to enhance efficiency\nand consistency. In software engineering, a method was proposed for using LLMs to evaluate bug\nreport summarizations, demonstrating high accuracy in assessing correctness and completeness,\neven surpassing human evaluators who experienced fatigue. This approach offers a scalable solution\nfor evaluation [45]. In education, automated essay scoring and revising were explored using open-\nsource LLMs, achieving performance comparable to traditional deep-learning models. Techniques\nsuch as few-shot learning and prompt tuning improved scoring accuracy, while revisions effectively\nenhanced essay quality without compromising original meaning [89]. In content moderation, an\nLLM-based approach was developed to identify rule violations on platforms like Reddit, achieving\nhigh true-negative rates but encountering challenges with complex rule interpretation, emphasizing\nthe necessity of human oversight for nuanced cases [41]. These applications of LLMs as evaluators\nhighlight their growing potential in diverse sectors, emphasizing the need for integrating domain-\nspecific knowledge and refining methodologies.",
            "content": "Moreover, LLMs as evaluators demonstrate significant advantages in qualitative assessments that are difficult to quantify, such as evaluating service quality, analyzing user experience feedback, and assessing creative content like art or literature reviews. LLMs capability to understand and generate nuanced language makes them well-suited for subjective evaluation tasks traditionally requiring human judgment. Future research will increasingly focus on these areas, exploring how LLMs as judges can enhance assessment accuracy and consistency where traditional quantitative methods fall short."
        },
        {
            "title": "7.1 Reliability\nEvaluating the reliability of LLMs when used as judges reveals several pressing challenges. Both\nhuman and LLM judges exhibit biases, which raises concerns regarding the consistency and fairness\nof their evaluations. Specifically, human judges are also found to have inherent bias [115, 135] and\nmay not even provide reliable answers [14, 26]. As an alternative to humans, LLM evaluations are\nalso found to have certain biases, and the annotation results require more evaluation [74], as we\ndiscussed in § 4. The bias of LLM-as-a-judge is more due to the fact that LLM is a probabilistic\nmodel, as we have defined in § 4. Moreover, Reinforcement Learning with Human Feedback (RLHF)\nimproves LLM performance by aligning them with human preferences. However, ensuring models\ntrained with RLHF produce robust and consistent outputs remains an ongoing challenge.",
            "content": ", Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. In this section, to better understand reliability, we discuss the reliability issues that arise from biases, overconfidence, and challenges in generalization. Overconfidence. Instruction-tuned LLMs have been demonstrated to possess the issue of overconfidence, which means they tend to offer overly favorable scores when evaluating their own responses [97]. The overconfidence is also highly likely to exist in the scenario of LLM-as-a-judge, which is also engaged in evaluating the responses generated by LLMs. Consequently, when LLMas-a-judge is utilized with the latest LLMs, which are typically instruction-tuned, the existence and impact of overconfidence need to be meticulously examined. Fairness and Generalization. Another significant aspect of reliability is fairness and generalization. Evaluations by LLM-as-a-judge can exhibit considerable inconsistency depending on the context. This is why prompt-based methods are often used to improve LLM-as-a-judge performance. However, challenges related to fairness and generalization may arise due to the sensitivity of prompt engineering. For example, the order of the examples in the context can significantly affect the models output, leading to unfair evaluations if the examples are poorly arranged. Moreover, LLMs struggle to handle long context windows effectively, often showing degraded performance or prioritizing later examples in the sequence. These issues raise concerns about fairness and generalization in LLM-based evaluations."
        },
        {
            "title": "7.2 Robustness\nDespite LLM’s superior power, it is found prone to adversarial attacks [35, 84, 146], under which\nLLMs can be induced to generate harmful content. While existing works on LLM attacks mainly\nfocus on NLG tasks, more attacks on LLM-as-a-judge are relatively under-explored [11]. This means\nthat we will face some robustness challenges when using LLM-as-a-Judge, and these risks are\nunknown.",
            "content": "Addressing these robustness challenges requires deeper understanding of the specific vulnerabilities associated with LLM-as-a-Judge tasks. Unlike traditional adversarial attacks on natural language generation (NLG), where the goal is often to mislead the model into generating harmful or incorrect outputs, attacks on LLM-as-a-Judge aim to exploit biases, inconsistencies, or loopholes in the models decision-making processes. For instance, subtle manipulations in input phrasing or context framing could potentially lead to significant deviations in judgments, raising concerns about reliability in high-stakes applications. Currently, we have some methods to defend against such attacks to maintain robustness. These approaches mainly involve post-processing techniques, such as response filtering and consistency checks, which are essential for improving evaluation quality. However, these techniques still face significant challenges. One major issue is self-consistency, as LLMs often produce inconsistent outputs when evaluating the same input multiple times. Another challenge is random scoring, where the model assigns arbitrary or overly positive scores that fail to accurately reflect the true quality of the generated outputs. Such limitations undermine the reliability and robustness of these defense mechanisms."
        },
        {
            "title": "7.3 Powerful Backone Model\nAlthough LLMs show superior performance in text-based evaluation, the field lacks robust multi-\nmodal models to effectively serve as reliable judges for multi-modal content. Current multi-modal\nLLMs, such as GPT-4 Vision, still struggle with complex reasoning across different modalities. This\nlimitation poses a challenge to achieving reliable evaluations on multi-modal assessment tasks.",
            "content": ", Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge Even in many cases, our LLM cannot complete high-quality evaluation content due to insufficient powerful instruction-following ability and reasoning ability for evaluating text content."
        },
        {
            "title": "8.1 More Reliable LLM-as-a-Judge\nAs highlighted in our Formulation (§ 2) and Strategy (§ 3), LLMs are probabilistic models that\nrequire extensive research and optimization to enhance their reliability as judges. Although current\nmethods have improved the reliability of LLM-as-a-Judge, many challenges, including adaptability\nand robustness, remain unresolved. To enable probabilistic models to deliver evaluations closely\naligned with real-world scenarios, future research should prioritize refining and implementing\nLLM-as-a-Judge across the evaluation pipeline.",
            "content": "There is considerable potential for improving reliability in various aspects, including In-Context Learning, model selection, post-processing techniques, and the overall evaluation framework for LLM-as-a-Judge. These efforts should prioritize not only enhancing the reliability of assessments but also developing methodologies to systematically evaluate and validate the robustness of these assessments. Furthermore, the establishment of comprehensive evaluation benchmarks and interpretable analytical tools will be crucial for assessing and improving the reliability of LLM evaluators. Finally, the uncertain and evolving nature of robustness risks underscores the necessity of proactive mitigation strategies. These strategies should include the development of adversarial training techniques tailored to judgment tasks, the integration of robust uncertainty quantification methods, and the implementation of human-in-the-loop systems to oversee critical decisions. By addressing these challenges, we can build more resilient and dependable systems capable of maintaining high levels of reliability even under adversarial conditions."
        },
        {
            "title": "8.2 LLM-as-a-Judge for Data Annotation\nDespite its wide applications, data annotation poses significant challenges for current machine\nlearning models due to the complexity, subjectivity, and diversity of data. This process requires\ndomain expertise and is resource-intensive, particularly when manually labeling large datasets.\nAdvanced LLMs such as GPT-4 [70], Gemini [24], and LLaMA-2 [102] offer a promising opportunity\nto revolutionize data annotation. LLMs serve as more than just tools but play a crucial role in\nimproving the effectiveness and precision of data annotation. Their ability to automate annotation\ntasks [132], ensure consistency across large volumes of data, and adapt through fine-tuning or\nprompting for specific domains [88], significantly mitigates the challenges encountered with\ntraditional annotation methods, setting a new standard for what is achievable in the realm of NLP.\nWhether in the field of scientific research or industry, we are all still suffering from insufficient\ntarget data and domain-specific data, or situations where the data quality is not high enough.\nAssuming that LLM-as-a-judge can achieve stable performance and be fair and reliable, we can\nuse LLM to annotate data in scenarios where data is insufficient to expand the data. In scenarios\nwith low data quality, we can assess the data quality through LLM, and label the quality tags to\nachieve the goal of selecting high-quality data. Currently, we have not been able to experimentally\nrely solely on LLM for a reliable evaluation of various different scenarios of data; most of the time,",
            "content": ", Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. we still rely on human annotation to ensure professionalism and reliability. LLM-as-a-judge often needs to learn from human annotations in order to perform certain labeling tasks."
        },
        {
            "title": "8.3 MLLM-as-a-Judge\nAI systems are rapidly evolving into highly multifunctional entities. For example, whereas in the\npast we had special-purpose solutions for different language processing tasks (e.g., sentiment\nanalysis, parsing, dialogue), LLMs are competent at all these tasks using a single set of weights [91].\nUnified systems are also being built across data modalities: instead of using a different architecture\nfor processing images versus text, recent models, such as GPT4-V [71], Gemini [24], and LLaVA [57],\nhandle both modalities In brief, AI systems are becoming more uniform in their structures and\nfunctions. This also applies to LLM-as-a-Judge.",
            "content": "The future of evaluation lies in developing robust multi-modal evaluators that can handle complex, multi-modal content, such as text, images, and video. Current multi-modal LLMs, while promising, lack the reasoning depth and reliability of their text-based counterparts. Future research should focus on: practical multi-modal evaluator will not only advance research but also open new possibilities for tasks such as multi-modal content moderation and knowledge extraction."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Maksym Andriushchenko, Alexandra Souly, Mateusz Dziemian, Derek Duenas, Maxwell Lin, Justin Wang, Dan Hendrycks, Andy Zou, Zico Kolter, Matt Fredrikson, et al. 2024. AgentHarm: Benchmark for Measuring Harmfulness of LLM Agents. arXiv preprint arXiv:2410.09024 (2024). [2] Golnoosh Babaei and Paolo Giudici. 2024. GPT classifications, with application to credit lending. Machine Learning with Applications 16 (2024), 100534. [3] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. 2024. Benchmarking foundation models with language-model-as-an-examiner. Advances in Neural Information Processing Systems 36 (2024). [4] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. 2024. Benchmarking foundation models with language-model-as-an-examiner. Advances in Neural Information Processing Systems 36 (2024). [5] Nathan Brake and Thomas Schaaf. 2024. Comparing Two Model Designs for Clinical Note Generation; Is an LLM Useful Evaluator of Consistency? arXiv preprint arXiv:2404.06503 (2024). [6] Meni Brief, Oded Ovadia, Gil Shenderovitz, Noga Ben Yoash, Rachel Lemberg, and Eitam Sheetrit. 2024. Mixing It Up: The Cocktail Effect of Multi-Task Fine-Tuning on LLM PerformanceA Case Study in Finance. arXiv preprint arXiv:2410.01109 (2024). [7] Tom Brown. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020). [8] David Chan, Suzanne Petryk, Joseph Gonzalez, Trevor Darrell, and John Canny. 2023. CLAIR: Evaluating Image Captions with Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 1363813646. , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge [9] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. arXiv preprint arXiv:2402.04788 (2024). [10] Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, et al. 2024. Data-juicer: one-stop data processing system for large language models. In Companion of the 2024 International Conference on Management of Data. 120134. [11] Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. 2024. Humans or llms as the judge? study on judgement biases. arXiv preprint arXiv:2402.10669 (2024). [12] Yen-Ting Lin Yun-Nung Chen. 2023. LLM-EVAL: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models. In The 5th Workshop on NLP for Conversational AI. 47. [13] Inyoung Cheong, King Xia, KJ Kevin Feng, Quan Ze Chen, and Amy Zhang. 2024. (A) Am Not Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice. In The 2024 ACM Conference on Fairness, Accountability, and Transparency. 24542469. [14] Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah Smith. 2021. All thats humanis not gold: Evaluating human evaluation of generated text. arXiv preprint arXiv:2107.00061 (2021). [15] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. 2023. Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287 (2023). [16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248255. [17] Hanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023. RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. [18] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 320335. [19] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Alpacafarm: simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387 (2023). [20] Jinlan Fu, See Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2024. GPTScore: Evaluate as You Desire. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 65566576. [21] Isabel Gallegos, Ryan Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen Ahmed. 2024. Bias and fairness in large language models: survey. Computational Linguistics (2024), 179. [22] Mingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023. Human-like summarization evaluation with chatgpt. arXiv preprint arXiv:2304.02554 (2023). [23] Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor. 2023. TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 20532070. [24] Google. 2023. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805 (2023). [25] Neel Guha, Julian Nyarko, Daniel Ho, Christopher Ré, Adam Chilton, Alex Chohlas-Wood, Austin Peters, Brandon Waldon, Daniel Rockmore, Diego Zambrano, et al. 2024. Legalbench: collaboratively built benchmark for measuring legal reasoning in large language models. Advances in Neural Information Processing Systems 36 (2024). [26] Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating large language models in generating synthetic hci research data: case study. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 119. [27] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992 (2023). [28] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. 2024. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 (2024). [29] Hangfeng He, Hongming Zhang, and Dan Roth. 2024. Socreval: Large language models with the socratic method for reference-free reasoning evaluation. In Findings of the Association for Computational Linguistics: NAACL 2024. 27362764. [30] Xinyu Hu, Mingqi Gao, Sen Hu, Yang Zhang, Yicheng Chen, Teng Xu, and Xiaojun Wan. 2024. Are LLM-based Evaluators Confusing NLG Quality Criteria?. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 95309570. , Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. [31] Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024. An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers. arXiv preprint arXiv:2403.02839 (2024). [32] Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, and Michael Lyu. 2023. On the humanity of conversational ai: Evaluating the psychological portrayal of llms. In The Twelfth International Conference on Learning Representations. [33] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. 2023. Baseline defenses for adversarial attacks against aligned language models. arXiv preprint arXiv:2309.00614 (2023). [34] Bowen Jiang, Yangxinyu Xie, Xiaomeng Wang, Weijie Su, Camillo Jose Taylor, and Tanwi Mallick. 2024. Multi-modal and multi-agent systems meet rationality: survey. In ICML 2024 Workshop on LLMs and Cognition. [35] Shuyu Jiang, Xingshu Chen, and Rui Tang. 2023. Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks. arXiv:2310.10077 [cs.CL] [36] Jaylen Jones, Lingbo Mo, Eric Fosler-Lussier, and Huan Sun. 2024. Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models. arXiv preprint arXiv:2402.11676 (2024). [37] Immanuel Kant. 1781. Critique of Pure Reason (a/b ed.). Macmillan, London. Akademie-Ausgabe, Vol. 3, A132/B171. [38] Immanuel Kant. 1790. Critique of Judgment. Hackett Publishing Company, Indianapolis. Akademie-Ausgabe, Vol. 5, 5:179. [39] Pei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, et al. 2024. CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1303413054. [40] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. 2023. Prometheus: Inducing Fine-grained Evaluation Capability in Language Models. arXiv preprint arXiv:2310.08491 (2023). [41] Mahi Kolla, Siddharth Salunkhe, Eshwar Chandrasekharan, and Koustuv Saha. 2024. LLM-Mod: Can Large Language Models Assist Content Moderation?. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 18. [42] Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. 2023. Benchmarking cognitive biases in large language models as evaluators. arXiv preprint arXiv:2309.17012 (2023). [43] Fajri Koto. 2024. Cracking the Code: Multi-domain LLM Evaluation on Real-World Professional Exams in Indonesia. arXiv preprint arXiv:2409.08564 (2024). [44] Jack Krolik, Herprit Mahal, Feroz Ahmad, Gaurav Trivedi, and Bahador Saket. 2024. Towards Leveraging Large Language Models for Automated Medical Q&A Evaluation. arXiv preprint arXiv:2409.01941 (2024). [45] Abhishek Kumar, Sonia Haiduc, Partha Pratim Das, and Partha Pratim Chakrabarti. 2024. LLMs as Evaluators: Novel Approach to Evaluate Bug Report Summarization. arXiv preprint arXiv:2409.00630 (2024). [46] Abhishek Kumar, Sarfaroz Yunusov, and Ali Emami. 2024. Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models. arXiv preprint arXiv:2405.14555 (2024). [47] Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, and Minjoon Seo. 2024. Prometheusvision: Visionlanguage model as judge for fine-grained evaluation. arXiv preprint arXiv:2401.06591 (2024). [48] Yebin Lee, Imseong Park, and Myungjoo Kang. 2024. FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using Large Multimodal Model. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. 37323746. [49] Alice Li and Luanne Sinnamon. 2023. Examining query sentiment bias effects on search results in large language models. In The Symposium on Future Directions in Information Access (FDIA) co-located with the 2023 European Summer School on Information Retrieval (ESSIR). [50] Haitao Li, You Chen, Qingyao Ai, Yueyue Wu, Ruizhe Zhang, and Yiqun Liu. 2024. LexEval: Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models. arXiv preprint arXiv:2409.20288 (2024). [51] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023. Generative judge for evaluating alignment. arXiv preprint arXiv:2310.05470 (2023). [52] Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao. 2024. SALADBench: Hierarchical and Comprehensive Safety Benchmark for Large Language Models. In Findings of the Association for Computational Linguistics ACL 2024. 39233954. [53] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. AlpacaEval: An Automatic Evaluator of Instruction-following Models. https://github.com/tatsulab/alpaca_eval. [54] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355 (2023). , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge [55] Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. 2023. Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation. arXiv preprint arXiv:2310.17389 (2023). [56] Chengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Changlong Sun, Kun Kuang, and Fei Wu. 2023. GoalOriented Prompt Attack and Safety Evaluation for LLMs. arXiv e-prints (2023), arXiv2309. [57] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Instruction Tuning. In NeurIPS. [58] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Advances in neural information processing systems 36 (2024). [59] Hao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023. Languages are rewards: Hindsight finetuning using human feedback. arXiv preprint arXiv:2302.02676 (2023). [60] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-Eval: NLG Evaluation using Gpt-4 with Better Human Alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 25112522. [61] Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2024. HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 76417660. [62] Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulic, Anna Korhonen, and Nigel Collier. 2024. Aligning with human judgement: The role of pairwise preference in large language model evaluators. In The 1st Conference on Language Modeling. [63] Adian Liusie, Potsawee Manakul, and Mark Gales. 2024. LLM comparative assessment: Zero-shot NLG evaluation through pairwise comparisons using large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers). 139151. [64] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. 2023. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255 (2023). [65] Yujie Lu, Dongfu Jiang, Wenhu Chen, William Yang Wang, Yejin Choi, and Bill Yuchen Lin. 2024. WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences. arXiv preprint arXiv:2406.11069 (2024). [66] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. arXiv preprint arXiv:2308.09583 (2023). [67] Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct. arXiv preprint arXiv:2308.09583 (2023). [68] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems 36 (2024). [69] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 1207612100. [70] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] [71] OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [72] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 2773027744. https://proceedings. neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf [73] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155 (2022). [74] Nicholas Pangakis, Samuel Wolken, and Neil Fasching. 2023. Automated Annotation with Generative AI Requires Validation. arXiv preprint arXiv:2306.00176 (2023). [75] Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. 2024. Offsetbias: Leveraging debiased data for tuning evaluators. arXiv preprint arXiv:2407.06551 (2024). [76] Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, et al. 2023. Large language models are effective text rankers with pairwise ranking prompting. arXiv preprint , Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. arXiv:2306.17563 (2023). [77] Vyas Raina, Adian Liusie, and Mark Gales. 2024. Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment. arXiv preprint arXiv:2402.14016 (2024). [78] Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, and Urmish Thakkar. 2024. Constructing domain-specific evaluation sets for llm-as-a-judge. arXiv preprint arXiv:2408.08808 (2024). [79] Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. 2022. Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization. arXiv preprint arXiv:2210.01241. https://arxiv.org/abs/2210.01241 [80] Cheol Ryu, Seolhwa Lee, Subeen Pang, Chanyeol Choi, Hojun Choi, Myeonggee Min, and Jy-Yong Sohn. 2023. Retrieval-based Evaluation for LLMs: Case Study in Korean Legal QA. In Proceedings of the Natural Legal Language Processing Workshop 2023. 132137. [81] Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. 2024. Branch-Solve-Merge Improves Large Language Model Evaluation and Generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 83458363. [82] Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto. 2023. Verbosity bias in preference labeling by large language models. arXiv preprint arXiv:2310.10076 (2023). [83] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017). [84] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2023. \"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models. arXiv:2308.03825 [cs.CR] [85] Lin Shi, Weicheng Ma, and Soroush Vosoughi. 2024. Judging the Judges: Systematic Investigation of Position Bias in Pairwise Comparative Assessments by LLMs. arXiv preprint arXiv:2406.07791 (2024). [86] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems 36 (2024). [87] Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, and Seunghyeok Hong. 2024. Llm-as-a-judge & reward model: What they can and cannot do. arXiv preprint arXiv:2409.11239 (2024). [88] Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. 2023. Preference Ranking Optimization for Human Alignment. arXiv preprint arXiv:2306.17492 (2023). [89] Yishen Song, Qianta Zhu, Huaibo Wang, and Qinhua Zheng. 2024. Automated Essay Scoring and Revising Based on Open-Source Large Language Models. IEEE Transactions on Learning Technologies (2024). [90] Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. 2023. Evaluation metrics in the era of GPT-4: reliably evaluating large language models on sequence to sequence tasks. arXiv preprint arXiv:2310.13800 (2023). [91] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 (2022). [92] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo. 2023. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. arXiv preprint arXiv:2307.07697 (2023). [93] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. 2023. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525 (2023). [94] Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan. 2023. Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision. arXiv preprint arXiv:2305.03047 (2023). [95] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_ alpaca. [96] Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. 2024. Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges. arXiv preprint arXiv:2406.12624 (2024). [97] Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, and Christopher Manning. 2023. Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975 (2023). [98] Yufei Tian, Abhilasha Ravichander, Lianhui Qin, Ronan Le Bras, Raja Marjieh, Nanyun Peng, Yejin Choi, Thomas Griffiths, and Faeze Brahman. 2023. MacGyver: Are Large Language Models Creative Problem Solvers? arXiv preprint , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge arXiv:2311.09682 (2023). [99] SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024. comprehensive survey of hallucination mitigation techniques in large language models. arXiv preprint arXiv:2401.01313 (2024). [100] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [101] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Vicuna: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [102] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [103] Tempest van Schaik and Brittany Pugh. 2024. field guide to automatic evaluation of llm-generated summaries. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 28322836. [104] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. 2023. Evaluation and analysis of hallucination in large vision-language models. arXiv preprint arXiv:2308.15126 (2023). [105] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, and Zhifang Sui. 2024. Large Language Models are not Fair Evaluators. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 94409450. [106] Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926 (2023). [107] Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean OBrien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023. Shepherd: Critic for Language Model Generation. arXiv preprint arXiv:2308.04592 (2023). [108] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560 (2022). [109] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, 1348413508. https://doi.org/10.18653/v1/2023.acl-long.754 [110] Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al. 2023. PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization. arXiv preprint arXiv:2306.05087 (2023). [111] Yicheng Wang, Jiayi Yuan, Yu-Neng Chuang, Zhuoer Wang, Yingchi Liu, Mark Cusick, Param Kulkarni, Zhengping Ji, Yasser Ibrahim, and Xia Hu. 2024. DHP Benchmark: Are LLMs Good NLG Evaluators? arXiv preprint arXiv:2408.13704 (2024). [112] Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra, Vincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, et al. 2024. Speculative rag: Enhancing retrieval augmented generation through drafting. arXiv preprint arXiv:2407.08223 (2024). [113] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 2482424837. [114] Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. 2023. Large Language Models are Better Reasoners with Self-Verification. In Findings of the Association for Computational Linguistics: EMNLP 2023. 25502575. [115] Minghao Wu and Alham Fikri Aji. 2023. Style over substance: Evaluation biases for large language models. arXiv preprint arXiv:2307.03025 (2023). [116] Yuhang Wu, Wenmeng Yu, Yean Cheng, Yan Wang, Xiaohan Zhang, Jiazheng Xu, Ming Ding, and Yuxiao Dong. 2024. AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models. arXiv preprint arXiv:2406.09295 (2024). [117] Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. 2024. LLaVA-Critic: Learning to Evaluate Multimodal Models. arXiv preprint arXiv:2410.02712 (2024). [118] Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Wang, and Lei Li. 2023. INSTRUCTSCORE: Towards Explainable Text Generation Evaluation with Automatic Feedback. In Proceedings of the , Vol. 1, No. 1, Article . Publication date: November 2024. J. Gu, X. Jiang, Z. Shi, J. Guo, et al. 2023 Conference on Empirical Methods in Natural Language Processing. 59675994. [119] Yuzhe Yang, Yifei Zhang, Yan Hu, Yilin Guo, Ruoli Gan, Yueru He, Mingcong Lei, Xiao Zhang, Haining Wang, Qianqian Xie, et al. 2024. UCFE: User-Centric Financial Expertise Benchmark for Large Language Models. arXiv preprint arXiv:2410.14059 (2024). [120] Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al. 2024. Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge. arXiv preprint arXiv:2410.02736 (2024). [121] Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. 2023. SelFee: Iterative Self-Revising LLM Empowered by Self-Feedback Generation. Blog post. https://kaistai.github.io/SelFee/ [122] Sungwook Yoon. 2023. Design and Implementation of an LLM system to Improve Response Time for SMEs Technology Credit Evaluation. International Journal of Advanced Smart Convergence 12, 3 (2023), 5160. [123] Yangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan Suchow, Rong Liu, Zhenyu Cui, Denghui Zhang, et al. 2024. FinCon: Synthesized LLM Multi-Agent System with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making. arXiv preprint arXiv:2407.06567 (2024). [124] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rrhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302 (2023). [125] Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, et al. 2022. Fengshenbang 1.0: Being the foundation of chinese cognitive intelligence. arXiv preprint arXiv:2209.02970 (2022). [126] Liang Xu Xuanwei Zhang and Kangkang Zhao. 2022. Chatyuan: large language model for dialogue in chinese and english. [127] Mi Zhang, Xudong Pan, and Min Yang. 2023. Jade: linguistics-based safety evaluation platform for llm. arXiv preprint arXiv:2311.00286 (2023). [128] Ruizhe Zhang, Haitao Li, Yueyue Wu, Qingyao Ai, Yiqun Liu, Min Zhang, and Shaoping Ma. 2024. Evaluation Ethics of LLMs in Legal Domain. arXiv preprint arXiv:2403.11152 (2024). [129] Tianjun Zhang, Fangchen Liu, Justin Wong, Pieter Abbeel, and Joseph Gonzalez. 2023. The Wisdom of Hindsight Makes Language Models Better Instruction Followers. arXiv preprint arXiv:2302.05206 (2023). [130] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda Ruth Petzold. 2023. Gpt-4v (ision) as generalist evaluator for vision-language tasks. arXiv preprint arXiv:2311.01361 (2023). [131] Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862 (2023). [132] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493 (2022). [133] Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, et al. 2024. Revolutionizing finance with llms: An overview of applications and insights. arXiv preprint arXiv:2401.11641 (2024). [134] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, et al. 2023. Lmsys-chat-1m: large-scale real-world llm conversation dataset. arXiv preprint arXiv:2309.11998 (2023). [135] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems 36 (2023), 4659546623. [136] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems 36 (2023), 4659546623. [137] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. arXiv preprint arXiv:2306.05685 (2023). [138] Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, and Min Lin. 2024. Cheating automatic llm benchmarks: Null models achieve high win rates. arXiv preprint arXiv:2410.07137 (2024). [139] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 (2023). [140] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528 (2023). , Vol. 1, No. 1, Article . Publication date: November 2024. Survey on LLM-as-a-Judge [141] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv:2310.17631 (2023). [142] Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023. Judgelm: Fine-tuned large language models are scalable judges. arXiv preprint arXiv:2310.17631 (2023). [143] Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Ruyi Gan, Jiaxing Zhang, and Yujiu Yang. 2023. Solving Math Word Problems via Cooperative Reasoning Induced Language Models. arXiv preprint arXiv:2210.16257 (2023). [144] Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor Bursztyn, Ryan Rossi, Somdeb Sarkhel, and Chao Zhang. 2023. Toolchain*: Efficient action space navigation in large language models with a* search. arXiv preprint arXiv:2310.13227 (2023). [145] Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, et al. 2024. Agent-as-a-Judge: Evaluate Agents with Agents. arXiv preprint arXiv:2410.10934 (2024). [146] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. 2023. Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043 [cs.CL] , Vol. 1, No. 1, Article . Publication date: November 2024."
        }
    ],
    "affiliations": [
        "Department of Civil and Environmental Engineering, Imperial College London",
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "IDEA Research, International Digital Economy Academy",
        "Institute of Computing Technology, Chinese Academy of Sciences"
    ]
}