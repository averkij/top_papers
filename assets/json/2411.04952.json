{
    "paper_title": "M3DocRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding",
    "authors": [
        "Jaemin Cho",
        "Debanjan Mahata",
        "Ozan Irsoy",
        "Yujie He",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction tools such as optical character recognition (OCR). However, there are difficulties in applying these methods in real-world scenarios: (a) questions often require information across different pages or documents, where MLMs cannot handle many long documents; (b) documents often have important information in visual elements such as figures, but text extraction tools ignore them. We introduce M3DocRAG, a novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DocRAG finds relevant documents and answers questions using a multi-modal retriever and an MLM, so that it can efficiently handle single or many documents while preserving visual information. Since previous DocVQA datasets ask questions in the context of a specific document, we also present M3DocVQA, a new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. In three benchmarks (M3DocVQA/MMLongBench-Doc/MP-DocVQA), empirical results show that M3DocRAG with ColPali and Qwen2-VL 7B achieves superior performance than many strong baselines, including state-of-the-art performance in MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and retrieval models. Lastly, we qualitatively show that M3DocRAG can successfully handle various scenarios, such as when relevant information exists across multiple pages and when answer evidence only exists in images."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 ] . [ 1 2 5 9 4 0 . 1 1 4 2 : r M3DOCRAG: Multi-modal Retrieval is What You Need for Multi-page Multi-document Understanding Jaemin Cho1* Debanjan Mahata2 Ozan Irsoy2 Yujie He2 Mohit Bansal1 1UNC Chapel Hill 2Bloomberg {jmincho,mbansal}@cs.unc.edu {dmahata,oirsoy,yhe247}@bloomberg.net"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction and Background Document visual question answering (DocVQA) pipelines that answer questions from documents have broad applications. Existing methods focus on handling single-page documents with multi-modal language models (MLMs), or rely on text-based retrieval-augmented generation (RAG) that uses text extraction tools such as optical character recognition (OCR). However, there are difficulties in applying these methods in real-world scenarios: (a) questions often require information across different pages or documents, where MLMs cannot handle many long documents; (b) documents often have important information in visual elements such as figures, but text extraction tools ignore them. We introduce M3DOCRAG, novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (singlehop and multi-hop), and evidence modalities (text, chart, figure, etc.). M3DOCRAG finds relevant documents and answers questions using multi-modal retriever and an MLM, so that it can efficiently handle single or many documents while preserving visual information. Since previous DocVQA datasets ask questions in the context of specific document, we also present M3DOCVQA, new benchmark for evaluating open-domain DocVQA over 3,000+ PDF documents with 40,000+ pages. In three benchmarks (M3DOCVQA/MMLongBench-Doc/MP-DocVQA), empirical results show that M3DOCRAG with ColPali and Qwen2-VL 7B achieves superior performance than many strong baselines, including state-of-the-art performance in MP-DocVQA. We provide comprehensive analyses of different indexing, MLMs, and retrieval models. Lastly, we qualitatively show that M3DOCRAG can successfully handle various scenarios, such as when relevant information exists across multiple pages and when answer evidence only exists in images. *Work done during an internship at Bloomberg as recipient of the Bloomberg Data Science Ph.D. Fellowship. Document visual question answering (DocVQA) [14, 31, 40, 42, 57] is multi-modal task that answers textual questions by interpreting information contained within document images. Existing methods on DocVQA either focus on visual question answering (VQA) on single-page document (Fig. 1 (a)) or extract text from documents (e.g., via optical character recognition (OCR) [43, 53] or PDF text extraction [18, 49]) and use retrieval-augmented generation (RAG) [35], where retrieval model finds relevant paragraphs and language model answers questions given the paragraphs (Fig. 1 (b)). However, there are difficulties in applying these methods in real-world document understanding scenarios: (a) questions often require information across different pages or documents, where existing VQA methods cannot handle many long documents; (b) some documents feature complex visual formats such as tables, charts, and mixed layouts, but text extraction methods such as OCR ignore these nuances, leading to incomplete or inaccurate document interpretations. Accurately and efficiently answering questions across numerous, lengthy documents with intricate layouts would greatly benefit many domains such as finance, healthcare, and law, where document AI assistants can streamline the daily processing of large volumes of documents, improving productivity and enabling faster, more informed decision-making. To overcome these limitations of existing DocVQA approaches, we introduce M3DOCRAG (Multi-modal Multipage Multi-Document Retrieval-Augmented Generation; Sec. 2), novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multihop), and evidence modalities (text, chart, figure, etc.). As illustrated in Fig. 1 (c), the M3DOCRAG framework retrieves relevant document pages using multi-modal retrieval model, such as ColPali [17], and generates answers to questions from the retrieved pages using multimodal language model (MLM), such as Qwen2-VL [59]. In (1) document M3DOCRAG operates in three stages: 1 Figure 1. Comparison of multi-modal document understanding pipelines. Previous works focus on (a) Single-page DocVQA that cannot handle many long documents or (b) Text-based RAG that ignores visual information. Our (c) M3DOCRAG framework retrieves relevant documents and answers questions using multi-modal retrieval and MLM components, so that it can efficiently handle many long documents while preserving visual information. Figure 2. Comparison of existing DocVQA datasets (left; e.g., DocVQA [42]) and our M3DOCVQA dataset (right). In contrast to previous DocVQA datasets that have questions that are specific to single provided PDF (e.g., What was the gross profit in the year 2009?), M3DOCVQA has information-seeking questions that benchmark open-domain question answering capabilities across more than 3,000 PDF documents (i.e., 40,000+ pages). embedding (Sec. 2.1), we convert all document pages into RGB images and extract visual embeddings (e.g., via ColPali) from the page images. In (2) page retrieval (Sec. 2.2), we retrieve the top-K pages of high similarity with text queries (e.g., MaxSim operator for ColPali). For the opendomain setting, we create approximate page indices, such as inverted file index (IVF) [52, 66], for faster search. In (3) question answering (Sec. 2.3), we conduct visual question answering with MLM to obtain the final answer. Please also see Fig. 3 for the detailed illustration of the framework. M3DOCRAG can flexibly handle DocVQA in both closed domain (i.e., single document) and open-domain (i.e., large corpus of documents) settings. While M3DOCRAG framework supports DocVQA in an open-domain setting, the existing DocVQA datasets are not adequate for this setting, since their questions are in the context of specific document, such as What was the gross profit in the year 2009? [14, 40, 42, 57], as illustrated in Fig. 2 (left). Hence, we also introduce M3DOCVQA (Multi-modal Multi-page Multi-Document Visual Question Answering), an open-domain dataset that significantly raises the challenge of DocVQA to answering questions from large document corpus (Sec. 3). By extending the MultimodalQA datasets [54] closed-domain context to an open-domain setting, M3DOCVQA introduces 2,441 multi-hop questions spanning 3,368 PDF docFigure 3. Our M3DOCRAG framework (Sec. 2) consists of three stages: (1) document embedding (Sec. 2.1), (2) page retrieval (Sec. 2.2), and (3) question answering (Sec. 2.3). In (1) document embedding, we extract visual embedding (with ColPali) to represent each page from all PDF documents. In (2) page retrieval, we retrieve the top-K pages of high relevance (MaxSim scores) with text queries. In an open-domain setting, we create approximate page indices for faster search. In (3) question answering, we conduct visual question answering with multi-modal LM (e.g. Qwen2-VL) to obtain the final answer. uments, which collectively contain over 41,005 pages of diverse multi-modal content, including text, images, and tables. This dataset presents real-world challenges by requiring models to navigate complex reasoning paths across pages and within various types of document elements, better reflecting the intricacies of document understanding. To demonstrate the effectiveness of M3DOCRAG, we compare M3DOCRAG with state-of-the-art baselines in three benchmarks: M3DOCVQA, MMLongBenchDoc [40], and MP-DocVQA [57], which cover both opendomain (Sec. 5.1) and closed-domain (Sec. 5.2) DocVQA settings. Experiment results show that M3DOCRAG with ColPali and Qwen2-VL 8B achieves superior performance than many strong baselines, including the state-of-the-art performance in MP-DocVQA. We also provide comprehensive analysis (Sec. 5.3) about different indexing, MLMs, and retrieval components. Finally, we show qualitative examples (Sec. 5.4) where M3DOCRAG can successfully handle various scenarios, such as when the relevant information exists across multiple pages and when answer evidence only exists in images. Overall, M3DOCRAG is an effective, efficient, and flexible framework for answering questions from multi-modal documents in various settings. 2. M3DOCRAG: Unified Framework for Multi-modal, Multi-page, Multi-document Understanding We propose M3DOCRAG, novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). As illustrated in Fig. 3, M3DOCRAG operates in three stages: (1) encoding document images into visual embeddings (Sec. 2.1), (2) retrieving relevant document pages (Sec. 2.2), and (3) generating answers to questions based on the retrieved pages (Sec. 2.3). Below, we explain the problem definition and the details of each stage. Problem definition. We define corpus of documents as = {D1, D2, . . . , DM }, where is the total number of documents, and each document Di consists of set of pages, Pi, represented as RGB images. From the documents in C, we construct global set of page images = (cid:83)M i=1 Pi = {p1, p2, . . . , pN }, where each pj represents an individual page image, and is the total number of page images across all documents in (i.e., = (cid:80)M i=1 Pi). The objective of M3DOCRAG is to accurately answer given question using the multi-modal information available in the corpus of documents C. First, we identify K, the top ( ) pages that are most relevant to answering the query from the global page set . Then, we obtain the final answer with question answering model that takes retrieved page images and query as inputs. The problem of question answering can be categorized into two settings with different document context sizes: Closed-domain question answering The query should be answerable from given single document Di. The retrieval model outputs the top relevant page images"
        },
        {
            "title": "P q",
            "content": "K, from the page images Pi of the document Di. Open-domain question answering The query may require information from single or multiple documents within the entire document corpus C. The retrieval model outputs the top relevant page images from the entire set of page images . 2.1. Document Embedding In M3DOCRAG, both textual query and page images are projected into shared multi-modal embedding space using ColPali [17]. ColPali is multi-modal retrieval model based on late interaction mechanism, which encodes the text and image inputs into unified vector representations and retrieves the top most relevant images. ColPali adopts both training objective and similarity scoring from ColBERT [29, 50], which utilizes shared architecture to encode either textual or visual inputs. In our framework, each page Pi of document Di is treated as single image with fixed dimensions (width height). From an image of page, we extract dense visual embedding Ep Rnvd, where nv represents the number of visual tokens per page (which remains constant across all pages), and denotes the embedding dimension (e.g., 128). For textual query q, we similarly obtain an embedding Eq Rnqd, where nq is the number of text tokens. For efficiency, we treat each page of document independently. This allows us to flatten all pages in the document corpus into single page-level embedding tensor: EC RN nvd, where represents the total number of pages in the entire document corpus, nv is the number of visual tokens per page, and is the embedding dimension. M3DOCRAG can flexibly adapt to different retrieval settings, such as single-page document (N = 1), single document with multiple pages (e.g. = 100), and large corpus of multi-page documents (e.g. > 1, 000). 2.2. Page Retrieval The relevance between the query and the page is computed using the MaxSim score s(q, p): s(q, p) = nq (cid:88) i=1 max j[nv] Eq i, Ep j, where denotes the dot product, and Ei, Rd denotes the i-th row (vector) of the embedding matrix Rnd. We then identify K, the top ( ) pages that are most relevant to answering the query q; i.e. we search pages scoring highest s(q, p). That is, = {pq 1, pq 2, . . . , pq K} = argtop-kpP s(q, p) Approximate indexing for open-domain page retrieval. Searching pages over in large document corpus can be 4 time-consuming and computationally expensive. When faster search is desired, we create page indices offline by applying approximate nearest neighborhood search, based on Faiss [16, 26]. We use exact search for closed-domain page retrieval and employ inverted file index (IVF) [52, 66] (IVFFlat in Faiss) for an open-domain setting, which could reduce page retrieval latency from 20s/query to less than 2s/query when searching across 40K pages. See Sec. 5.3 for detailed comparison of speed-accuracy tradeoffs across different indexing methods. 2.3. Question Answering We run visual question answering by giving the text query and retrieved page images to multi-modal language model to obtain the final answer. For this, we employ multimodal language models (e.g. Qwen2-VL [59]) that consist of visual encoder EncVis and language model LM. The visual encoder takes K-retrieved page images as inputs and outputs visual embeddings (different from ColPali encoders outputs). The language model takes the visual embeddings and text embeddings of query as inputs and outputs the final answer in the autoregressive manner: = LM(EncVis(P K), q). 3. M3DOCVQA: New Benchmark for Opendomain Document Understanding We present M3DOCVQA (Multi-modal Multi-page MultiDocument Visual Question Answering), new opendomain DocVQA benchmark designed to evaluate the ability to answer questions using multi-modal information from large corpus of documents. As illustrated in Fig. 2, existing DocVQA datasets [31, 40, 42, 57] primarily focus on evaluating question answering within the context of single document (i.e., closeddomain). These datasets are not well-suited for benchmarking open-domain visual question answering, where relevant information, often in multiple modalities such as text, images, and tables, must be retrieved from multiple documents. This limitation stems from their questions being designed around specific content on certain pages within single document. In real-world scenarios, users often seek answers that span across multiple documents and modalities, making open-domain settings critical. However, the questions in the existing DocVQA datasets are not applicable in such an open-domain setting. For example, question from MP-DocVQA, such as What was the gross profit in the year 2009? assumes that the model already has access to specific information within the document. M3DOCVQA challenges models in an open-domain DocVQA setting, where they must navigate large haystack of multi-modal documents and retrieve relevant Figure 4. Illustration of PDF collections in M3DOCVQA. We first collect the URLs of all supporting contexts (Wikipedia documents) of individual questions of MultimodalQA [54]. Then, we create PDF versions from their URLs by rendering them in web browser. information to generate the final answer. The dataset consists of 2,441 multi-hop questions spread across 3,368 PDF documents, totaling 41,005 pages. Each question is supported by evidence found in one or more documents, spanning multiple modalities such as text, images, and tables, capturing the complexity and diversity typical of real-world documents. Additionally, we provide the training split, consisting of 24,162 Wikipedia PDFs. Although the documents in the training split were not utilized in our experiments, they offer future researchers the opportunity to explore even larger-scale retrieval tasks or use the documents for training models, further expanding the potential applications of M3DOCVQA. To create M3DOCVQA, we extend the question-answer pairs from short-context VQA dataset to more complex setting that includes 1) PDF documents and 2) open-domain contexts. Specifically, we use the question-answer pairs from the development split1 of MultimodalQA [54], where models answer multi-hop questions based on short multimodal contexts (e.g., short text passages, 1-2 images, table) sourced from Wikipedia. We retrieved the URLs of all Wikipedia documents used as context in any of the MultimodalQA development split questions. Then we generated PDF versions of the Wikipedia pages by rendering them in Chromium web browser [56], using the Playwright Python package [45]. These PDFs retain all vector graphics and metadata, ensuring zoom-in functionality and maintaining operational hyperlinks. In addition, no objects are split between different pages in the resulting PDFs. While both M3DOCVQA and MultimodalQA [54] share the goal of evaluating question answering given multimodal context, M3DOCVQA introduces more demanding scenario by requiring models to retrieve relevant information from large set of documents, as opposed to being provided with short context. In MultimodalQA, models are given short, curated context (e.g., paragraph 1The test split of MultimodalQA [54] is unavailable, and previous works have used the development split for comparison. from Wikipedia document) that directly contains the information needed to answer the questions, simplifying In the task to reasoning within the provided material. contrast, M3DOCVQA presents an open-domain setting, where models must retrieve information from diverse collection of 3,368 PDF documents before attempting to answer any question. This not only requires handling largescale document retrieval but also dealing with multi-modal contenttext, images, and tablesdistributed across multiple documents. This key distinction highlights M3DOCVQAs ability to simulate real-world challenges, where the relevant data is often spread across multiple sources. Consequently, M3DOCVQA serves as robust benchmark for retrieval-augmented generation tasks in document understanding, pushing the boundaries of models to deal with large-scale, multi-modal, and multi-document settings. 4. Experiment Setup Datasets. We benchmark M3DOCRAG on three PDF document understanding datasets that represent different scenarios: (1) M3DOCVQA (Open-domain DocVQA); (2) MMLongBench-Doc [40] (Closed-domain DocVQA); In (3) MP-DocVQA [57] M3DOCVQA, M3DOCRAG processes over 3,000 PDFs, totaling more than 40,000 pages. For MP-DocVQA, models handle single PDF with up to 20 pages for each question. For MMLongBench-Doc, models handle single PDF with up to 120 pages for each question. (Closed-domain DocVQA). Evaluation Metrics. For M3DOCVQA, we follow For the evaluation setup of MultimodalQA [54]. MMLongBench-Doc [40] and MP-DocVQA [57], we follow their official evaluation setups. For M3DOCVQA, we evaluate answer accuracy with exact match (EM) and F1. For MMLongBench-Doc, we extract short answers with GPT4o [46] from the model outputs and report answer accuracy with generalized accuracy (based on rule-based 5 Table 1. Open-domain DocVQA evaluation results on M3DOCVQA. The scores are based on F1, unless otherwise noted. Index: FlatIP + IVFFlat."
        },
        {
            "title": "Method",
            "content": "# Pages"
        },
        {
            "title": "Image Table Text",
            "content": "Single-hop Multi-hop EM F1 Text RAG (w/ ColBERT v2) Llama 3.1 8B Llama 3.1 8B Llama 3.1 8B M3DOCRAG (w/ ColPali) Qwen2-VL 7B (Ours) Qwen2-VL 7B (Ours) Qwen2-VL 7B (Ours) 1 2 4 1 2 8.3 7.7 7.8 25.1 26.8 24.7 15.7 16.8 21.0 27.8 30.4 30.4 29.6 31.7 34.1 39.6 42.1 41. 25.3 27.4 29.4 37.2 41.0 43.2 12.3 12.1 15.2 25.0 25.2 26.6 15.4 15.8 17.8 27.9 29.9 31. 20.0 21.2 23.7 32.3 34.6 36.5 evaluation script covering different answer types) and F1 score. For MP-DocVQA, we report answer accuracy with ANLS [8] and page retrieval with accuracy (same as recall@1, as there is single page annotation for each question) by submitting the generation results to the test server.2 Models. We mainly experiment with the ColPali v1 [17]3 retrieval model and various recent open source multi-modal LMs with <10B parameters, including Idefics 2 [33], Idefics 3 [32], InternVL 2 [12], and Qwen2-VL [59]. We also experiment with text-based RAG pipeline by combining recent widely used text retrieval and language models: ColBERT v2 [50] and Llama 3.1 [37]. We also compare ColPali v1 with ColQwen v0.1 [17],4 another recent multi-modal retrieval model that was trained with same objective/dataset as ColPali but initialized with Qwen2-VL 2B [59] backbone. For reproducible evaluation, we use deterministic greedy decoding for answer generation. We compare these multi-modal and text-based RAG pipelines with recent top entries with comparable parameters (<10B) reported on the leaderboards. Other implementation details. We use PyTorch [47, 48], Transformers [60], and FlashAttention-2 [13] libraries for running models. We use Tesseract [53] for OCR in text RAG baselines, following Ma et al. [40]. We use Faiss [16, 26] for document indexing. We use the pdf2image [6] library to convert each PDF page into an RGB image with resolution of DPI=144. While all PDF pages in M3DOCVQA have the same size 8.5 (width) 11 (height) in inches (i.e. US letter size) and 1224 (width) 1584 (height) in pixels, in MP-DocVQA and MMLongBench-Doc datasets, pages have slightly different sizes. To handle this, we resize page images to the most common image size within the dataset 1700 (width) 2200 (height) for MP-DocVQA, and to the most common image size within each PDF document for MMLongBenchDoc. All experiments are conducted with single H100 80GB GPU. We provide up to 4 pages as visual inputs to our multi-modal LMs, the maximum number of images we could fit in the single GPU. 5. Results and Key Findings In the following, we describe experiment results of M3DOCRAG and baselines in both open-domain (Sec. 5.1) and closed-domain settings (Sec. 5.2). Next, we provide ablation studies (Sec. 5.3) about different page indexing strategies and different multi-modal LMs and retrieval models. Lastly, we show qualitative examples (Sec. 5.4) where M3DOCRAG can tackle M3DOCVQA questions whose answer source exists in various modalities. 5.1. Open-domain DocVQA Multi-modal RAG outperforms text RAG, especially on non-text evidence sources. Table 1 shows the evaluation results on M3DOCVQA. As model needs to find relevant documents from 3,000+ PDFs for each question, we focus solely on RAG pipelines. We observe that our M3DOCRAG (ColPali + Qwen2-VL 7B) significantly outperforms text RAG (ColBERT v2 + Llama 3.1 8B), across all different evidence modalities / question hops / # pages. The performance gap is especially big when the evidence involves images, underscoring that M3DOCRAG addresses the information loss over non-textual content by text-only pipelines. We also notice that providing more retrieved pages as context generally increases the performance of both text RAG and M3DOCRAG (using the top 4 pages gives higher performance than the top 1 and 2 pages). 5.2. Closed-domain DocVQA 2https://rrc.cvc.uab.es/?ch=17&com=tasks 3https://huggingface.co/vidore/colpali 4https://huggingface.co/vidore/colqwen2-v0. Multi-modal RAG boosts long document understanding of MLMs. In MMLongBench-Doc, the models must handle long PDF document (up to 120 pages) for each ques6 Table 2. Closed-domain DocVQA evaluation results on MMLongBench-Doc. We report the generalized accuracy (ACC) across five evidence source modalities: text (TXT), layout (LAY), chart (CHA), table (TAB), and image (IMG), and three evidence locations: singlepage (SIN), cross-page (MUL), and unanswerable (UNA). The scores from non-RAG methods are from Ma et al. [40]. Method # Pages Evidence Modalities Evidence Locations Overall TXT LAY CHA TAB IMG SIN MUL UNA ACC F1 LMs ChatGLM-128k [5] Mistral-Instruct-v0.2 [25] Text RAG ColBERT v2 + Llama 3.1 ColBERT v2 + Llama 3. Multi-modal LMs DeepSeek-VL-Chat [38] Idefics2 [33] MiniCPM-Llama3-V2.5 [61, 64] InternLM-XC2-4KHD [15] mPLUG-DocOwl 1.5 [22] Qwen-VL-Chat [4] Monkey-Chat [36] M3DOCRAG ColPali + Idefics2 (Ours) ColPali + Qwen2-VL 7B (Ours) ColPali + Qwen2-VL 7B (Ours) Text Pipeline 12.7 13.4 14.8 17.7 9.7 10.2 12.7 14. 10.2 10.1 17.4 24.0 Multi-modal Pipeline 6.5 10.6 10.8 14.3 8.4 9.0 7.2 11.1 21.0 23.5 1.6 4.8 5.1 7.7 2.0 5.4 3. 5.2 4.1 5.9 6.3 3.4 2.2 6.7 6.0 18.5 18.9 7.7 16.4 20.1 23.4 19.9 20.1 23.7 7.2 9.0 11.9 9.9 8.2 5.5 6. 10.9 25.7 30.0 up to 120 up to 120 1 4 up to 120 up to 120 up to 120 up to 120 up to 120 up to 120 up to 120 1 1 4 12.2 11. 7.4 11.9 7.6 8.7 12.2 13.0 9.9 6.9 9.4 15.7 19.7 20.8 18.8 16.9 21.8 25.7 5.2 7.7 9.5 12.6 7.4 5.2 6. 15.4 30.4 32.4 11.5 11.3 7.8 12.2 7.0 7.2 9.5 7.6 6.4 7.1 6.2 7.2 10.6 14.8 18.1 24. 41.3 38.1 12.8 5.0 4.5 9.6 6.2 6.2 6.2 8.1 5.8 5.8 16.3 16.4 21.0 23.5 7.4 7.0 8.5 10.3 6.9 6.1 6. 11.2 18.8 21.0 14.9 13.8 16.1 19.7 5.4 6.8 8.6 9.8 6.3 5.4 5.6 11.0 20.1 22.6 tion. Since many multi-modal LMs have limited context length, Ma et al. [40] employed concatenation strategy that combines all screenshot pages into either 1 or 5 images and inputs these concatenated images to multi-modal LMs. Table 2 shows that ColPali + Idefics2 surpass Idefics2 without RAG, as well as all previous multi-modal entries. In addition, ColPali + Qwen2-VL 7B achieves the best scores in overall F1 and most evidence modality/page settings. This demonstrates the effectiveness of multi-modal retrieval over handling many pages by concatenating low-resolution images. As observed in M3DOCVQA experiments, we also notice that providing more retrieved pages as context generally increases the performance of both text RAG and M3DOCRAG (using the top 4 pages gives higher performance than the top 1 page). M3DOCRAG achieves the state-of-the-art performance in MP-DocVQA. In MP-DocVQA, the models must handle PDF document of up to 20 pages for each question. Table 3 presents the top-performing entries in the MP-DocVQA test split leaderboard, comparing text-based and multi-modal RAG pipelines. While the text RAG (ColBERT v2 + Llama 3.1) falls short compared to existing approaches, all multi-modal RAG pipelines outperform their text-based counterpart. Notably, the M3DOCRAG pipeline (ColPali + Qwen2-VL 7B) delivers the state-of-the-art results on MP-DocVQA. It is interesting that while the existing entries were fine-tuned specifically for MP-DocVQA, the components used in M3DOCRAG (ColPali or Qwen2Table 3. Closed-domain DocVQA evaluation results on MPDocVQA. The RAG methods retrieve single page to the downstream QA models."
        },
        {
            "title": "Method",
            "content": "Multimodal LMs Arctic-TILT 0.8B [10] GRAM [9] GRAM C-Former [9] ScreenAI 5B [3] Text RAG ColBERT v2 + Llama 3.1 8B M3DOCRAG ColPali + Qwen2-VL 7B (Ours)"
        },
        {
            "title": "ANLS",
            "content": "0.8122 0.8032 0.7812 0.7711 0.5603 0.8444 R@1 50.79 19.98 19.98 77.88 75. 81.05 VL 7B) were not tailored to this dataset although Qwen2VL 7B might have been trained on DocVQA [42], which shares some images with MP-DocVQA. 5.3. Additional analysis speed and accuracy. Different page indexing: In Table 4, we analyze the speed and accuracy of ColPali+Qwen2-VL 7B pipeline with different document embedding indexing methods. While the naive indexing with exact search (FlatIP) is slow (21s per query), we find that using approximate indexing such as inverted file [52, 66] (IVFFlat) and product quantization [27] (IVFPQ) can retain most of the accuracy, while making the search significantly faster (< 2s per query). We use Table 4. Speed-accuracy tradeoff with different indexing strategies on M3DOCVQA. Method: ColPali + Qwen2-VL 7B. # Pages"
        },
        {
            "title": "Indexing",
            "content": "Latency (s) () Accuracy ()"
        },
        {
            "title": "Retrieval VQA EM",
            "content": "1 1 1 2 2 4 4 FlatIP FlatIP + IVFFlat FlatIP + IVFPQ FlatIP + IVFFlat FlatIP + IVFPQ FlatIP + IVFFlat FlatIP + IVFPQ 21.0 1.8 0.2 1.8 0.2 1.8 0.2 1.1 1.1 1.1 2.4 2.4 4.8 4. 28.9 27.9 25.9 29.9 29.0 31.4 29.9 F1 33.7 32.3 30.3 34.6 33. 36.5 34.7 FlatIP+IVFFlat indexing by default, and users can choose appropriate indexing methods depending on their deployment requirements. Table 5. Comparison of different multimodal LMs within M3DOCRAG, evaluated across different document understanding benchmarks. For retrieval, we use the top-1 page from ColPali for all datasets. We use FlatIP+IVFFlat indexing for M3DOCVQA. Multimodal LMs Idefics2 8B Idefics3 8B InternVL2 8B Qwen2-VL 7B M3DOCVQA MMLongBench-Doc MP-DocVQA F1 () 27.8 31.8 30.9 32.3 Acc () ANLS () 10.8 16.4 17.3 18.8 0.56 0.77 0.81 0. Idefics2 8B [33], Different multi-modal LMs. In Table 5, we compare four different multi-modal LMs in the M3DOCRAG framework: InternVL2 8B [12], and Qwen2-VL 7B [59]. The Qwen2-VL 7B model outperforms other MLMs in all three benchmarks. Thus, we use the model as our default MLM component. Idefics3 8B [32], Table 6. Comparison of different multi-modal retrieval models within M3DOCRAG framework, evaluated across different document understanding benchmarks. We provide Qwen2-VL 7B with top-4 pages for MMLongBench-Doc/M3DOCVQA and top1 page for MP-DocVQA from the retrieval models. We use FlatIP+IVFFlat indexing for M3DOCVQA. Ret. Models ColPali v1 ColQwen v0. M3DOCVQA MMLongBench-Doc MP-DocVQA F1 () 36.5 32.1 Acc () 21.0 21.5 ANLS () 0.84 0.86 Different multi-modal retrieval models. In Table 6, we compare two different multi-modal retrival models in M3DOCRAG framework: ColPali v1 and ColQwen v0.1 8 (see Sec. 4 for details). Both models are trained with the same training objectives but are initialized with different MLM architectures: PaliGemma 2B [7] and Qwen2VL 2B [59], respectively. We find that ColPali achieves significantly better performance in M3DOCVQA, while ColQwen achieves slightly better performance in MPDocVQA and MMLongBench-Doc. Thus, we use ColPali as our default retrieval model. 5.4. Qualitative Examples In Fig. 5, Fig. 6, and Fig. 7, we provide qualitative examples of M3DOCRAG (ColPali + Qwen2-VL 7B)s question answering results on several M3DOCVQA examples. In Fig. 5, the answer information is only visually stored within the game logo (man is leaning on motorcycle), and M3DOCRAG could find the information. In Fig. 6, the question requires multi-hop reasoning across different pages/documents, and M3DOCRAG could combine information from multiple retrieved pages. In Fig. 7, although ColPali did not retrieve the page that contains information about team whose logo features bat, Qwen-2 VL leverages its own knowledge Valencia CF has logo featuring bat, and could provide the final answer. Overall, the qualitative examples showcase that M3DOCRAG can successfully tackle different questions whose answer sources exist in various modalities. 6. Related Work Document visual question answering. Mathew et al. [42] proposed document visual question answering task, where model extracts information (DocVQA) from documents by treating them as images, like in generic visual question answering [1]. Most research on DocVQA focuses on handling single-page document [22, 23, 30, 34, 41, 42, 55, 58, 63], and it has been now common practice to include the single-page DocVQA [42] as part of the image understanding evaluation suite among recent MLMs [7, 12, 20, 32, 46, 59]. Several recent works study applying MLMs for DocVQA on multi-page documents [31, 40, 57]. However, all previous works on DocVQA have focused on handling questions in the context of specific document, such as What was the gross profit in the year 2009? [14, 40, 42, 57]. While this is probably due to the limited context length of the backbone multi-modal LMs, this does not reflect real-world scenarios, where users often ask questions that require information across different pages/documents. We address the limitation and propose M3DOCRAG framework and M3DOCVQA dataset for effective, efficient, and flexible document understanding under various document contexts (closed-domain and open-domain), question hops (singlehop and multi-hop), and evidence modalities (text, chart, figure, etc.). Figure 5. Qualitative example of ColPali + Qwen2-VL 7B on M3DOCVQA. Image regions relevant to the question/answer are highlighted with orange boxes. The answer information is only stored visually within the game logo, where man is leaning on motorcycle. Figure 6. Qualitative example of ColPali + Qwen2-VL 7B on M3DOCVQA. Image regions relevant to the question/answer are highlighted with orange boxes. The question requires multi-page/document reasoning. 9 multi-modal contexts, including textual, tabular, and visual information across different pages and documents. 7. Conclusion We introduce M3DOCRAG, novel multi-modal RAG framework that flexibly accommodates various document contexts (closed-domain and open-domain), question hops (single-hop and multi-hop), and evidence modalities (text, chart, figure, etc.). In M3DOCRAG, multi-modal retrieval model identifies relevant pages from single or multiple documents, which are then processed by multimodal language model, where all documents are represented as pixels. Next, we introduce M3DOCVQA, the first benchmark that evaluates open-domain multi-modal document understanding capabilities. M3DOCVQA consists of 2,000+ questions and 3,000+ PDF documents, and the questions need to be answered with various modalities such as images, text, and tables. Our experiments in three datasets (M3DOCVQA, MP-DocVQA, and MMLongBench-Doc) demonstrate significant advantages of M3DOCRAG over existing methods, including the state-of-the-art performance in MP-DocVQA. We also provide analysis comparing different indexing strategies, multi-modal LMs, and multi-modal retrieval models. Finally, we show qualitative examples where M3DOCRAG can successfully tackle different questions whose answer sources exist in various modalities. We hope that our work encourages future advancements in multi-modal frameworks for document understanding, paving the way for more robust, scalable, and practical solutions in real-world applications."
        },
        {
            "title": "Ethical Considerations",
            "content": "Limitations. Since our multimodal retrieval models and multimodal LMs were trained with English-heavy datasets, they might not understand prompts or documents written in non-English. While our M3DOCRAG framework can benefit many document understanding applications, the model components could present false or biased information. Thus, the framework should be used with human supervision in real-world applications. Note that M3DOCRAG is designed with flexibility so that users can update or replace components as more accurate solutions for each element of the framework become available in the future. Data collection. We do not involve human subjects during data collection. We do not claim ownership/rights of the Wikipedia documents, and we attribute the source Wikipedia document URLs to all pages."
        },
        {
            "title": "References",
            "content": "[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Figure 7. Qualitative example of ColPali + Qwen2-VL 7B on M3DOCVQA. Image regions relevant to the question/answer are highlighted with orange boxes. The VQA component could combine both the retrieved knowledge (Tropi was transferred on 11 July 2017) and its own knowledge (Valencia CF has logo with bat) to provide the final answer. Retrieval-augmented generation. Retrieval-augmented generation (RAG) [35] has emerged as hybrid approach combining retrieval systems with generative models to improve the quality and relevance of generated content [19]. RAG has been widely studied for open-domain question answering [2, 21, 24, 28, 39, 65], where the community has well-established practices for text-based pipelines. line of work in VQA studies RAG on visual questions that require world knowledge [11, 44, 51, 62], but their retrieval context is usually generic images and/or short text snippets and does not cover DocVQA settings. To the best of our knowledge, no prior work has explored RAG setting for multi-modal document understanding only with multi-modal models (instead of using OCR methods). Our framework tackles opendomain question answering over documents with complex 10 Parikh. VQA: Visual question answering. In ICCV, 2015. 8 [2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection, 2023. [3] Gilles Baechler, Srinivas Sunkara, Maria Wang, Fedir Zubach, Hassan Mansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen, and Abhanshu Sharma. ScreenAI: Vision-Language Model for UI and Infographics Understanding, 2024. 7 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: frontier large vision-language model arXiv preprint, abs/2308.12966, with versatile abilities. 2023. 7 [5] Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li. LongAlign: recipe for long context alignment of large language models. arXiv preprint, abs/2401.18058, 2024. 7 [6] Edouard Belval. pdf2image, 2017. 6 [7] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Boˇsnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. PaliGemma: versatile 3B VLM for transfer, 2024. 8 [8] Ali Furkan Biten, Andres Mafla, Lluis Gomez, Valveny Jawahar, and Dimosthenis Karatzas. Scene Text Visual Question Answering. In ICCV, 2019. 6 [9] Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts, Roy Ganz, Elad Ben Avraham, Aviad Aberdam, Shahar Tsiper, and Ron Litman. Gram: Global reasoning for multi-page vqa. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1559815607, 2024. 7 [10] Łukasz Borchmann, Michał Pietruszka, Wojciech Jaskowski, Dawid Jurkiewicz, Piotr Halama, Paweł Joziak, Łukasz Garncarek, Paweł Liskowski, Karolina Szyndler, Andrzej Gretkowski, Julita Ołtusek, Gabriela Nowakowska, Artur Zawłocki, Łukasz Duhr, Paweł Dyda, and Michał Turski. Arctic-TILT. Business Document Understanding at SubBillion Scale, 2024. [11] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. arXiv preprint arXiv:2210.02928, 2022. 10 [12] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 6, 8 [13] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. 6 [14] Yihao Ding, Siwen Luo, Hyunsuk Chung, and Soyeon Caren Han. Pdfvqa: new dataset for real-world vqa on pdf documents. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 585601. Springer, 2023. 1, 2, 8 [15] Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, et al. Internlm-Xcomposer24KHD: pioneering large vision-language model handling arXiv preprint, resolutions from 336 pixels to 4k hd. abs/2404.06512, 2024. 7 [16] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazare, Maria Lomeli, Lucas Hosseini, and Herve Jegou. The faiss library, 2024. 4, [17] Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Celine Hudelot, and Pierre Colombo. ColPali: Efficient Document Retrieval with Vision Language Models, 2024. 1, 4, 6 [18] Mathieu Fenniak and PyPDF2 Contributors. The PyPDF2 library, version 2, 2022. 1 [19] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. Retrieval-augmented generation for large language models: survey. arXiv preprint arXiv:2312.10997, 2023. 10 [20] Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. 8 [21] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-Augmented Language Model Pre-Training. In ICML, 2020. 10 [22] Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang Zhang, Bo Zhang, Chen Li, Ji Zhang, Qin Jin, Fei Huang, and Jingren Zhou. mplug-docowl 1.5: Unified structure learning for ocr-free document understanding, 2024. 7, 8 [23] Yupan Huang, Tengchao Lv, Lei Cui, Yutong Lu, and Furu Wei. Layoutlmv3: Pre-training for document ai with unified In Proceedings of the 30th ACM text and image masking. International Conference on Multimedia, page 40834091, New York, NY, USA, 2022. Association for Computing Machinery. [24] Gautier Izacard and Edouard Grave. Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering. In EACL, 2021. 10 [25] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothee Lacroix, and William El Sayed. Mistral 7b, 2023. 7 [26] Jeff Johnson, Matthijs Douze, and Herve Jegou. Billionscale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535547, 2021. 4, 6 [27] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE Transactions 11 on Pattern Analysis and Machine Intelligence, 33(1):117 128, 2011. [28] Vladimir Karpukhin, Barlas, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense Passage Retrieval for Open-Domain Question Answering. In EMNLP, pages 67696781, 2020. 10 [29] Omar Khattab and Matei Zaharia. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT. SIGIR 2020 - Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3948, 2020. 4 [30] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision (ECCV), 2022. 8 [31] Jordy Van Landeghem, Rafał Powalski, Rub`en Tito, Dawid Jurkiewicz, Matthew Blaschko, Łukasz Borchmann, Mickael Coustaty, Sien Moens, Michał Pietruszka, Bertrand Ackaert, Tomasz Stanisławek, Paweł Joziak, and Ernest Valveny. Document Understanding Dataset and Evaluation (DUDE). In ICCV, 2023. 1, 4, 8 [32] Hugo Laurencon, Andres Marafioti, Victor Sanh, and Leo Building and better understanding visioninsights and future directions, 2024. 6, Tronchon. language models: 8 [33] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024. 6, 7, [34] Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2struct: screenshot parsing as pretraining for visual language understanding. In Proceedings of the 40th International Conference on Machine Learning. JMLR.org, 2023. 8 [35] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen Tau Yih, Tim Rocktaschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In NeurIPS, 2020. 1, 10 [36] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang, Jingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai. Monkey: Image resolution and text label are important things for large multi-modal models. arXiv preprint, abs/2311.06607, 2023. 7 [37] Llama Team. The llama 3 herd of models, 2024. 6 [38] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et al. DeepSeek-VL: towards real-world vision-language understanding. arXiv preprint, abs/2403.05525, 2024. 7 [39] Hongyin Luo, Yung-Sung Chuang, Yuan Gong, Tianhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, Helen Meng, and James Glass. Sail: Search-augmented instruction learning, 2023. 10 [40] Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. Mmlongbench-doc: Benchmarking long-context docarXiv preprint ument understanding with visualizations. arXiv:2407.01523, 2024. 1, 2, 3, 4, 5, 6, 7, 8 [41] Minesh Mathew, Viraj Bagal, Rub`en Perez Tito, Dimosthenis Karatzas, Ernest Valveny, and C.V. Jawahar. Infographicvqa. 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 25822591, 2021. 8 [42] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. 1, 2, 4, 7, 8 [43] Jamshed Memon, Maira Sami, Rizwan Ahmed Khan, and Mueen Uddin. Handwritten optical character recognition (ocr): comprehensive systematic literature review (slr). IEEE Access, 8:142642142668, 2020. 1 [44] Thomas Mensink, Jasper Uijlings, Lluis Castrejon, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, Andre Araujo, and Vittorio Ferrari. Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories. In Proceedings of the IEEE International Conference on Computer Vision, pages 30903101, 2023. 10 [45] Microsoft. Playwright for python, 2021. 5 [46] OpenAI. Hello gpt-4o, 2024. 5, 8 [47] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chana, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in PyTorch. In NIPS Workshop, 2017. 6 [48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32 (NeurIPS), 2019. 6 [49] pdfminer. pdfminer.six, 2019. 1 [50] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction. NAACL 2022 - 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, pages 37153734, 2022. 4, [51] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge, 2022. 10 [52] Sivic and Zisserman. Video google: text retrieval approach In Proceedings Ninth IEEE to object matching in videos. International Conference on Computer Vision, pages 1470 1477 vol.2, 2003. 2, 4, 7 [53] Ray Smith. An overview of the tesseract ocr engine. In ICDAR, 2007. 1, 6 [54] Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. Multimodalqa: Complex question 12 answering over text, arXiv:2104.06039, 2021. 2, 5 tables and images. arXiv preprint [55] Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael Zeng, Cha Zhang, and Mohit Bansal. Unifying vision, text, and layout for universal document processing, 2023. 8 [56] The Chromium Project Authors. The chromium projects, 2024. 5 [57] Rub`en Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multipage docvqa. Pattern Recognition, 144:109834, 2023. 1, 2, 3, 4, 5, 8 [58] Dongsheng Wang, Natraj Raman, Mathieu Sibue, Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, and Xiaomo Liu. DocLLM: layout-aware generative language model for multimodal document understanding, 2023. 8 [59] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 4, 6, 8 [60] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie Brew. HuggingFaces Transformers: State-of-the-art Natural Language Processing. In EMNLP, 2020. 6 [61] Ruyi Xu, Yuan Yao, Zonghao Guo, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. LLaVA-UHD: An LMM perceiving any aspect ratio and arXiv preprint, abs/2403.11703, high-resolution images. 2024. 7 [62] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-Augmented Multimodal Language Modeling. In ICML, 2023. [63] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Mingshi Yan, Guohai Xu, Chenliang Li, Junfeng Tian, Qi Qian, Ji Zhang, Qin Jin, Liang He, Xin Lin, and Feiyan Huang. Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model. In Conference on Empirical Methods in Natural Language Processing, 2023. 8 [64] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. RLAIF-V: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint, abs/2405.17220, 2024. 7 [65] Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua. Retrieving and reading: comprehensive survey on open-domain question answering. arXiv preprint arXiv:2101.00774, 2021. 10 [66] Justin Zobel and Alistair Moffat. Inverted files for text search engines. ACM Comput. Surv., 38(2):6es, 2006. 2, 4,"
        }
    ],
    "affiliations": [
        "UNC Chapel Hill",
        "Bloomberg"
    ]
}