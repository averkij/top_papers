{
    "paper_title": "TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models",
    "authors": [
        "Harold Haodong Chen",
        "Disen Lan",
        "Wen-Jie Shu",
        "Qingyang Liu",
        "Zihan Wang",
        "Sirui Chen",
        "Wenkai Cheng",
        "Kanghao Chen",
        "Hongfei Zhang",
        "Zixin Zhang",
        "Rongjin Guo",
        "Yu Cheng",
        "Ying-Cong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3's chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 4 0 7 3 1 . 1 1 5 2 : r TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models Harold Haodong Chen1,2, Disen Lan3, Wen-Jie Shu2, Qingyang Liu4, Zihan Wang1, Sirui Chen1,7, Wenkai Cheng1, Kanghao Chen1,2,7, Hongfei Zhang1, Zixin Zhang1,2,7, Rongjin Guo5, Yu Cheng6#, Ying-Cong Chen1,2# 1HKUST(GZ), 2HKUST, 3FDU, 4SJTU, 5CityUHK, 6CUHK, 7Knowin Equal Contribution, #Corresponding Author The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3s chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning & Search, ii) Spatial & Visual Pattern Reasoning, iii) Symbolic & Logical Reasoning, and iv) Action Planning & Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting foundation for future research in this emerging field. (cid:128) Project: https://haroldchen19.github.io/TiViBench-Page/ Github: https://github.com/EnVision-Research/TiViBench Figure 1 Pass@1 performance overview on TiViBench across 24 tasks within 4 dimensions."
        },
        {
            "title": "Introduction",
            "content": "The rapid development of large language models (LLMs) (Achiam et al., 2023; Brown et al., 2020; Bai et al., 2023; Guo et al., 2025a) has fundamentally transformed the field of artificial intelligence, pushing the boundaries of what machines can achieve in both understanding and reasoning. Initially excelling at tasks requiring basic comprehension, LLMs have evolved to tackle complex reasoning problems step-by-step (Hao et al., 2025; Gu et al., 2025), as shown in Figure 2 (Left). Similarly, visual generative models (Rombach et al., 2022; He et al., 2022; Batifol et al., 2025; Wu et al., 2025a; Wan et al., 2025; Chen et al., 2025) have transitioned from producing 1 visually plausible outputs to tackling more sophisticated tasks that require physical plausibility and logical consistency. Among these, video generation has emerged as particularly promising paradigm, with wide range of applications, e.g., vision-language action (Chi et al., 2025; Li et al., 2025b; Zhang et al., 2025b) and novel view synthesis (YOU et al., 2025; Zhou et al., 2025; Voleti et al., 2024). natural question thus arises: can video generative models exhibit reasoning capabilities comparable to those of LLMs? The recent breakthrough of Veo 3 (Wiedemer et al., 2025) has hinted at this possibility by introducing the concept of \"chain-of-frames\" reasoning in image-to-video (I2V) generation, highlighting the possibility of leveraging video frame generation as medium for step-by-step visual reasoning. This raises the intriguing prospect of \"GPT moment\" for video generation models: one in which they transcend their current focus on visual fidelity and become generalpurpose vision foundation models capable of solving complex reasoning tasks. However, despite the promising advancements, existing benchmarks (Duan et al., 2025; Feng et al., 2024; Meng et al.; Huang et al., 2024b) for video generation fail to evaluate such reasoning abilities adequately (see Figure 2 (Right)). Current evaluations predominantly focus on visual fidelity, temporal smoothness, physical plausibility, and adherence to input prompts, which, while essential, fail to capture higher-order reasoning abilities. This gap motivates the need for new, complementary benchmark that can rigorously evaluate the reasoning potential of video models, paving the way for future research. Figure 2 (Left) Language models have evolved from basic understanding tasks to advanced reasoning capabilities. (Middle) Can video generative models exhibit reasoning capabilities comparable to those of LLMs? (Right) Existing I2V benchmarks focus on general generation capabilities (e.g., spatial fidelity, temporal smoothness), while our TiViBench complements these by introducing reasoning-oriented benchmark, enabling comprehensive evaluation across both general and reasoning abilities. In this work, we propose TiViBench, hierarchical benchmark designed specifically to evaluate the reasoning capabilities of I2V generation. Building on Veo 3s (Wiedemer et al., 2025) testing tasks like graph traversal and maze solving, we expand and diversify the evaluation scope to include more complex scenarios, e.g., strategic card game reasoning and mathematical problem solving, as demonstrated in Figure 3. Specifically, TiViBench offers systematic suite structured around four key dimensions: ❶ Structural Reasoning & Search, testing structure traversal, pathfinding, and constrained exploration; ❷ Spatial & Visual Pattern Reasoning, assessing capacities to detect, complete, or extrapolate patterns across time and space; ❸ Symbolic & Logical Reasoning, focusing on higher-order abstract reasoning tasks; and ❹ Action Planning & Task Execution, evaluating multi-step actions in temporally coherent manner. Each dimension comprises about 150 evaluation samples across three hierarchical levels (i.e., easy, medium, hard), totally covering 24 task scenarios. Through extensive evaluation on TiViBench, we observe that commercial models demonstrate stronger reasoning potential compared to open-source models, as shown in Figure 1. However, open-source ones also exhibit potential, albeit with inconsistent performance. To further unlock this potential, we propose simple yet effective test-time strategy, dubbed VideoTPO. Unlike strategies like SFT with domain-specific data, which are intuitively likely to enhance reasoning capabilities (Wu et al., 2025b) but require constructing large and diverse datasets with significant costs, VideoTPO avoids such overhead by operating entirely at test time. Specifically, different from conventional single-pass prompt rewriting (Xue et al., 2025), VideoTPO draws inspiration from test-time preference optimization (Li et al., 2025c) in LLMs. By leveraging multi-pass generation and aligning candidates through preference alignment, enabling more fine-grained and accurate prompt optimization, VideoTPO serves as both complementary method to TiViBench and practical solution for improving reasoning performance without weight updates. To summarize, this work contributes threefold: 2 Figure 3 Overview of TiViBench. TiViBench represents an image-to-video (I2V) benchmark tailored to comprehensively evaluate the emerging visual reasoning capabilities across four key categories: (1st) Structural Reasoning & Search, (2nd) Spatial & Visual Pattern Reasoning, (3rd) Symbolic & Logical Reasoning, and (4th) Action Planning & Task Execution. Each category encompasses six diverse tasks to challenge video generative models to perform complex reasoning beyond general generation. We propose TiViBench, hierarchical benchmark tailored to systematically evaluate the reasoning capabilities of video generative models, covering 4 reasoning dimensions across 24 diverse scenarios and 3 difficulty levels. Through extensive experiments, we analyze the reasoning potential of 3 commercial and 4 open-source advanced video models. Our findings highlight the advantages of commercial models while exposing their limitations, and reveal the latent capabilities of open-source models that remain constrained by current scales. We further introduce VideoTPO, simple yet effective test-time strategy that unlocks reasoning potential on-the-fly via preference optimization. It requires no additional training, data, or reward models, offering scalable solution for improving video generation models."
        },
        {
            "title": "2 Related Work",
            "content": "Image-to-Video Generation. Recent advancements in generative models have extended visual generation from images (Batifol et al., 2025; Wu et al., 2025a; Rombach et al., 2022; Wu et al., 2025c) to videos (Wan et al., 2025; OpenAI, 2025; Yang et al., 2024b; Kong et al., 2024). As subfield, image-to-video (I2V) generation enables more personalized outputs compared to text-to-video (T2V) models (Ni et al., 2023; Hu, 2024; Karras et al., 2023; Shao et al., 2025) and has served as testing ground for key concepts like physical plausibility (Liu et al., 2024; Yang et al., 2025; Li et al., 2025a). With general generation capabilities reaching new heights, researchers (Yang et al., 2024a; Wiedemer et al., 2025) have recently begun investigating whether video generative models can exhibit reasoning abilities akin to LLMs. However, there is currently lack of systematic benchmarks to evaluate reasoning capabilities in video generation models. 3 Evaluation of I2V Models. Early evaluations of I2V models relied on metrics like FVD (Heusel et al., 2017) on datasets like UCF101 (Soomro et al., 2012), later expanded by more recent benchmarks with fine-grained dimensions (e.g., 10 I2V dimensions in VBench++ (Huang et al., 2024b)). These benchmarks (Zhang et al., 2025a; Feng et al., 2024; Huang et al., 2024b; Duan et al., 2025; Zhang et al., 2024c; Fan et al., 2023) provide robust standards for assessing general generation capabilities, including spatial fidelity, temporal smoothness, and physical plausibility. However, none of these benchmarks systematically evaluate visual reasoning. Recent concurrent works, such as MME-CoF (Guo et al., 2025b) and VideoThinkBench (Tong et al., 2025), have begun to explore reasoning capabilities in video generation models. MME-CoF focuses on fine-grained reasoning dimensions derived from specific task types, while VideoThinkBench evaluates video generation models on both vision-centric and text-centric tasks. These works highlight the potential of video generation models for reasoning but focus on specific task designs or general multimodal reasoning without dedicated framework for systematically scaling task difficulty. In contrast, we propose TiViBench, hierarchical benchmark dedicated to visual reasoning in I2V models. TiViBench systematically evaluates models across 4 high-level reasoning dimensions and 24 task scenarios, each categorized into 3 difficulty levels, offering comprehensive and nuanced assessment of zero-shot reasoning capabilities. Prompt Optimization for Video Generative Model. While supervised fine-tuning (SFT) (Brown et al., 2020) and reinforcement fine-tuning (RFT) (Shao et al., 2024; Rafailov et al., 2023) enhance specific capabilities, they incur high costs due to additional data and training. Test-time prompt optimization offers lightweight alternative. Existing methods can be categorized into pre-inference (Wan et al., 2025; Wiedemer et al., 2025) and post-inference (Xue et al., 2025) rewriting. The former enriches prompts using LLMs for reasoning or imagination but risks deviating from user intent, while the later iteratively refines prompts based on generated results, improving outputs. However, single-pass strategies (i.e., generating one sample per round) limit optimization granularity. To address this, we propose VideoTPO, inspired by test-time preference optimization (Li et al., 2025c). By generating multiple candidate videos, VideoTPO identifies both general shortcomings and model-specific preferences, enabling more fine-grained optimization for video generative reasoning without parameter updates. 3 TiViBench: Benchmarking Visual Reasoning Potential To evaluate the visual reasoning capabilities of I2V generation, we introduce TiViBench, as shown in Figure 4 (Left), comprehensive benchmark covering 4 dimensions, 24 task scenarios, and 595 image-prompt samples. Each dimension is structured around 3 difficulty levels: easy, medium, and hard. TiViBench provides foundation for evaluating video generative reasoning. We next detail the evaluation dimension (3.1), prompt suite (3.2), and metric suite (3.3), with data statistics shown in Figure 5."
        },
        {
            "title": "3.1 Evaluation Dimension",
            "content": "To comprehensively evaluate the visual reasoning capabilities of I2V generation models, we extend and diversify the testing tasks introduced in (Wiedemer et al., 2025). Our benchmark spans four key reasoning dimensions: (i) Structural Reasoning & Search, (ii) Spatial & Visual Pattern Reasoning, (iii) Symbolic & Logical Reasoning, and (iv) Action Planning & Task Execution. Each dimension includes tasks designed to probe distinct reasoning abilities, as shown in Figure 3, with samples categorized into three difficulty levels. Structural Reasoning & Search. This dimension focuses on models ability to understand and navigate structured environments, solve constrained problems, and extrapolate patterns. Tasks in this category emphasize logical exploration, temporal coherence, and systematic problem-solving, including: ➀ graph traversal, ➁ maze solving, ➂ sorting numbers, ➃ temporal ordering, ➄ rule extrapolation, and ➅ game move reasoning. Spatial & Visual Pattern Reasoning. This dimension evaluates the models ability to recognize, manipulate, and reason about spatial relationships and visual patterns. Tasks in this category emphasize perceptual understanding and spatial transformations, including: ➀ shape fitting, ➁ connecting colors, ➂ pattern recognition, ➃ odd-one-out, ➄ counting objects, and ➅ visual analogy. 4 Figure 4 Overview of our proposed (Left) TiViBench benchmark and (Right) VideoTPO framework. Symbolic & Logical Reasoning. This dimension focuses on higher-order reasoning tasks that require abstract thinking, logical inference, and symbolic manipulation. Tasks include: ➀ simple Sudoku completion, ➁ arithmetic operations, ➂ symbolic reasoning, ➃ visual deduction, ➄ transitive reasoning, and ➅ game rule reasoning. Action Planning & Task Execution. This dimension evaluates the models ability to plan and execute multi-step actions in temporally coherent and goal-directed manner. Tasks include: ➀ tool use, ➁ robot navigation, ➂ goal-directed planning, ➃ multi-step manipulation, ➄ visual instruction following, and ➅ game strategy planning. Data Collection & Standards. To ensure the quality and diversity of our benchmark, we collect data from three primary sources: internet data, existing datasets (e.g., lecture videos in Video-MMLU (Song et al., 2025), tool use images in PhysToolBench (Zhang et al., 2025c)), and synthetic data created using Python scripts. Unlike previous I2V benchmarks (Huang et al., 2024b; Zhang et al., 2024c) that primarily contain initial inference images, our focus on video data allows us to capture the initial state, process state, and the target state, enabling more reliable evaluations. Additionally, our data collection process prioritizes quality and diversity. First, all data samples are curated to meet high-quality standards and are adapted to model input requirements, e.g., 720p resolution for horizontal videos. Second, to ensure diversity, we require that samples of the same type and difficulty level differ in background, style, or format as much as possible. Finally, each sample is reviewed by at least three human annotators to ensure both quality and diversity. Details are provided in Appendix A."
        },
        {
            "title": "3.2 Prompt Suite",
            "content": "Unlike the prompt style in LLM reasoning, which heavily instructs models (e.g., \"Find the optimal path from to B...\"), visual reasoning prompts for generative models emphasize task subjectivity and narrative descriptiveness (e.g., \"The blue ball slides smoothly along the white path, stopping at the red point...\"). These prompts should leave room for the model to infer intermediate steps while also providing sufficient details to guide reasoning (e.g., \"The blue ball never crosses into the black areas...\"). To meet these requirements, we adopt Gemini-2.5-Pro (DeepMind, 2024) as powerful assistant for generating prompts, leveraging initial state and target state images to construct prompts that are visually grounded and reasoning-driven. Specifically, prompts are tailored to each dimension: Structural Reasoning & Search. ➀ Goal Clarity: Define start and end states without specifying the solution path; ➁ Implicit Rules: Incorporate hidden constraints or rules that the model must infer; and ➂ Temporal Coherence: Ensure prompts describe tasks that unfold logically over time. 5 Figure 5 Overview of TiViBenchs statistical distributions. (Left) Word distribution of prompt suites; (Middle) Data distribution across 24 tasks; and (Right) Data distribution across 3 difficulty levels. Spatial & Visual Pattern Reasoning. ➀ Visual Specificity: Provide rich descriptions of visual elements, e.g., shapes, colors, and positions; ➁ Pattern Identification: Encourage recognition and extension of visual patterns; and ➂ Open-ended Tasks: Allow for multiple valid solutions. Symbolic & Logical Reasoning. ➀ Implicit Rule Discovery: Avoid explicitly stating rules, letting models infer them from the prompt; ➁ Symbol-Visual Integration: Combine symbolic reasoning with visual elements; and ➂ Logical Progression: Ensure tasks involve clear logical sequences. Action Planning & Task Execution. ➀ Goal-Oriented Descriptions: Define the goal while leaving intermediate steps implicit; ➁ Multi-step Reasoning: Encourage models to plan and execute sequential actions; and ➂ Causal Logic: Ensure prompts with clear cause-and-effect relationships. Prompt Quality Assurance. After generating the initial prompts, we conducted rigorous manual reviews to ensure quality, clarity, and alignment with our TiViBenchs goals. Specifically: (i) each prompt is reviewed by three human annotators. Any prompt flagged by even one annotator as unclear or unsuitable is revised; and (ii) only prompts meeting the expectations of all three annotators are adopted. Detailed information can be found in Appendix B."
        },
        {
            "title": "3.3 Metric Suite",
            "content": "Unlike general I2V benchmarks, visual reasoning tasks are inherently more verifiable due to explicit groundtruth information, including initial, intermediate, and target states. To evaluate reasoning capabilities effectively, we categorize metrics into two types, both focusing on correctness. Process-and-Goal Consistency. These tasks evaluate both the reasoning process and the final result, ensuring the generated video aligns with the expected trajectory and reaches the correct target state. For instance, in maze navigation, tools with tracking (Ren et al., 2024) can be used to track the subject across frames and validate the trajectory. Final-State Validation. These tasks assess whether the generated video achieves the correct target state, with no emphasis on intermediate reasoning steps. For example, Sudoku completion can be validated by comparing the generated grid (e.g., via OpenCV (OpenCV Development Team, 2025)) with the ground truth; and sequence completion can be validated by comparing extracted features (e.g., via DINO (Caron et al., 2021)). While metrics are grouped into these two categories, the validation method may vary across tasks and even within the same task type depending on the specific format, e.g., mathematical reasoning can be evaluated by checking the output after the equals sign for fill-in-the-blank tasks or the selected option for multiple-choice questions. Details in Appendix C. 4 VideoTPO: Prompt Preference Optimization On-the-Fly for Video Generative Reasoning Despite the rigorous prompt quality control in TiViBench to ensure compatibility with most I2V models, differences in pretraining data and architectures often lead to varying prompt preferences across models. To 6 address this, we propose VideoTPO, novel test-time prompt optimization strategy tailored for TiViBench, which aims to further unlock the potential of I2V models without additional tuning, as demonstrated in Figure 4 (Right). Existing prompt rewriting methods are typically classified as pre-inference (Wan et al., 2025; Wiedemer et al., 2025) (i.e., enriching prompts by hallucinating details) and post-inference (Xue et al., 2025) (i.e., modifying prompts based on the generation result). However, visual reasoning tasks are inherently more complex than general I2V tasks, requiring more nuanced and adaptive approach. To this end, we introduce the concept of test-time preference optimization (TPO) (Li et al., 2025c) for language models, which enables finer-grained optimization by comparing preferences across multiple generated samples. Different from TPO, which generates multiple samples (e.g., 4) and relies on external reward models to rank preferences, our VideoTPO generates only two samples per round and tasks VLM with self-analyzing their strengths and weaknesses. This eliminates external rewards, making VideoTPO as simple as possible to be practical. Textual Loss. Given an inference image with corresponding text prompt Pt at iteration t, the I2V model generates two candidate videos 1 . We then assign VLM (i.e., GPT-4o (Achiam et al., 2023)) denoted as to conduct self-analysis, which compares their strengths and weaknesses to produce textual critiques. The critiques highlight the advantages of the preferred video and the shortcomings of the non-preferred video, forming the textual loss: and 2 Lt = M(V 1 (1) where Lt encapsulates qualitative feedback rather than numerical scores, enabling more interpretable optimization. Textual Gradient. Based on Lt, the VLM generates actionable suggestions as textual gradient Gt (Yuksekgonul et al., 2025) to improve the prompt Pt. These suggestions guide the refinement of the prompt by specifying changes that better align the generated videos with the desired reasoning or visual outcomes: , Pt), , 2 Gt = M(Pt, Lt). (2) The textual gradient Gt serves as direct interpretation of the textual loss, ensuring the optimization remains lightweight and avoids reliance on external reward models. Prompt Update. The prompt Pt is then updated iteratively using Gt to produce refined prompt Pt+1: Pt+1 = M(Pt, Gt). (3) Detailed task prompts regarding textual loss, gradient calculations, and updating can be found in Appendix D."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we conduct extensive experiments to answer the following research questions: (RQ1) Do video generative models possess inherent reasoning potential? (RQ2) What are the primary factors contributing to reasoning failures? (RQ3) Can test-time optimization serve as an efficient and effective method to guide and enhance reasoning?"
        },
        {
            "title": "5.1 Experimental Settings",
            "content": "Models. We conduct evaluation on TiViBench with advanced I2V models: ❶ Open-Source: Wan2.2-I2VA14B, Wan2.1-I2V-14B (Wan et al., 2025), HunyuanVideo-I2V (Kong et al., 2024), CogVideoX1.5-I2V (Yang et al., 2024b). ❷ Commercial: Veo 3.1-fast (Google Gemini, 2025), Sora 2 (OpenAI, 2025), and Kling 2.1 (kli, 2025). We further apply our VideoTPO to Wan2.1-I2V-14B (Wan et al., 2025) and HunyuanVideo-I2V (Kong et al., 2024), as neither includes built-in prompt rewriter. Evaluations. Since TiViBench focuses on the correctness of visual reasoning, we report Pass@1 and Pass@5 for comparisons. Here, Pass@k indicates the accuracy of the model in producing at least one correct output within the predictions, where we infer open-source models under multiple random seeds to ensure more comprehensive evaluation. For commercial models, due to their strong performance and black-box nature, we 7 Table 1 Pass@1 performance of 7 advanced models on TiViBench. We highlight the best and second best results. Model Structural & Search Spatial & Visual Pattern Symbolic & Logical Planning & Execution Overall Easy Med. Hard Over. Easy Med. Hard Over. Easy Med. Hard Over. Easy Med. Hard Over. CVX1.5 HYV Wan2.1 2.22 2.22 8.89 2.04 0. 1.42 2.04 0.00 1.42 6.12 2. 5.76 Wan2.2 11.11 6.12 4.44 7. 2.04 2.04 4.08 4.08 2.00 2. 2.00 2.00 0.00 0.00 2.00 2. 1.34 1.34 2.68 2.68 2.00 4. 6.00 8.00 0.00 0.00 0.67 14. 0.00 0.00 4.46 2.00 0.00 2. 16.33 11.54 5.36 10.83 4.00 2.00 4. 30.61 19.23 12.50 20.38 6.00 4.00 6.00 30.61 19.23 14.39 21.02 2. 4.03 8.40 9.41 Open-Source Models Kling 2.1 8.89 4. 2.22 5.04 10.20 4.00 2.00 5. 12.00 8.00 4.00 8.00 32.65 28.85 19.64 26.75 11. Veo 3.1 17.78 8.16 4.44 10.07 30.61 20.00 16.00 22.15 36.00 16.00 2.00 18.00 77.55 40.38 39.29 51.59 26.05 Sora 26.67 22.45 6.67 18.71 38.78 32.00 24.00 31.76 32.00 26.00 8.00 22.00 46.94 42.31 26.79 38.22 27.90 Commercial Models Table 2 Pass@5 performance of open-source models on TiViBench. The best and second best results are highlighted. Model Structural & Search Spatial & Visual Pattern Symbolic & Logical Planning & Execution Overall Easy Med. Hard Over. Easy Med. Hard Over. Easy Med. Hard Over. Easy Med. Hard Over. CVX1. 2.22 HYV 4.44 2.04 2.04 Wan2.1 24. 16.33 0.00 0.00 8.89 1.42 2. 2.04 8.16 2.00 2.00 2.01 4. 2.00 4.70 8.00 8.00 2.00 2. 4.00 30.61 0.00 4.00 2.00 4. 34.69 17.31 0.00 8.93 9.55 19. 4.37 8.07 16.55 14.29 6.00 4. 8.54 10.00 4.00 4.00 6.00 44. 26.92 19.64 29.94 15.29 Wan2.2 24.44 18. 11.11 17.99 8.16 4.00 4.00 5. 14.00 6.00 4.00 8.00 46.94 32. 23.21 33.76 16.47 report only Pass@1. Following VBench++ (Huang et al., 2024b), we further adjust the input image resolution before inference to align with the preferences of each model, ensuring fair and optimal testing conditions."
        },
        {
            "title": "5.2 Main Results (RQ1)",
            "content": "To answer RQ1, we present evaluation results across three difficulty levels, providing global analysis of model performance on our TiViBench, as shown in Table 1 for Pass@1 and Table 2 for Pass@5 accuracy. Key observations are summarized as follows: Takeaway ❶: Sufficient data and scale likely contribute to clear reasoning potential. From Table 1, commercial models (e.g., Sora 2 and Veo 3.1) consistently outperform open-source models across all difficulty levels and reasoning dimensions. Notably, Sora 2 achieves the highest overall performance of 27.9%, demonstrating reasoning capabilities that remain robust even as task difficulty increases. This suggests that reasoning ability is not an inherent limitation of generative models but rather emerges with access to sufficiently large and diverse datasets, coupled with high parameter scales and optimized architectures. Takeaway ❷: Pass@5 improvements reveal the emerging reasoning potential of open-source models. Table 2 shows clear improvement in Pass@5 over Pass@1 for advanced open-source models (e.g., Wan2.2 and Wan2.1), indicating that they are capable of generating correct solutions, albeit inconsistently. This suggests that open-source models possess latent reasoning potential, but their unstable performance highlights limitations in the scale of their current training. Further scaling of training data, model parameters, or reasoning-specific optimization shows the necessity to realize the reasoning capability better."
        },
        {
            "title": "5.3 Failure Case Analysis (RQ2)",
            "content": "To answer RQ2, we first conduct an evaluation across 24 tasks for more granular analysis, with performance shown in Figure 1. Subsequently, we further demonstrate failure cases from the tasks with the lowest accuracy, as shown in Figure 6. We give the following observations: Takeaway ❸: Reasoning failures stem from insufficient rule modeling and fine-grained visual feature extraction. Figure 6 reveals that while Sora 2 and Veo 3.1 excel in general video generation, they exhibit varying performance across reasoning-specific tasks. For instance, both models achieve relatively high accuracy in tasks, e.g., visual deduction (VD) and instruction following (IF), where reasoning is less dependent on 8 Table 3 Evaluation on TiViBench with VideoTPO. We bold the best results. Qualitative results are in Appendix E. Model Structural & Search Spatial & Visual Pattern Symbolic & Logical Planning & Execution Easy Med. Hard Over. Easy Med. Hard Over. Easy Med. Hard Over. Easy Med. Hard Over. HunyuanVideo 2. 2.04 0.00 1.42 2.04 2.00 0.00 1. 4.00 2.00 0.00 2.00 16.33 11.54 5.36 10.83 + Pre-Rewriter 4. 2.04 0.00 2.16 6.12 0.00 0.00 2. 6.00 4.00 0.00 3.33 20.41 11.54 1.79 10.83 + Post-Rewriter 8. 4.08 0.00 4.32 6.12 4.00 2.00 4. 8.00 6.00 0.00 4.67 20.41 13.46 5.36 12.74 Overall 4. 4.71 6.55 + VideoTPO (Ours) 13.33 6.12 4.44 7.91 8.16 6.00 2.00 5.37 12.00 6.00 2.00 6.67 36.73 21.15 12.50 22.93 10.25 Wan2.1 8.89 6. 2.22 5.76 4.08 2.00 2.00 2.68 6. 4.00 2.00 4.00 30.61 19.23 12.50 20.38 8.40 + Pre-Rewriter 11.11 8.16 2. 7.19 8.16 4.00 4.00 5.37 10.00 2.00 2.00 4.00 38.78 25.00 14.29 25.48 10. + Post-Rewriter 15.56 8.16 4.44 9.35 12.24 6.00 4. 7.38 8.00 4.00 2.00 4.67 36.73 26.92 16.07 26.11 12.10 + VideoTPO (Ours) 28.89 20.41 8.89 19.42 16.33 8.00 6.00 10.07 14.00 8.00 4.00 8.67 48.98 30.77 23.21 33.76 18.15 Figure 6 (Top) Performance of the best-performing models (i.e., Sora 2 and Veo 3.1) on TiViBench across 24 tasks. (Bottom) Case study of the lowest-performing tasks, i.e., maze solving (MS), temporal ordering (TO), odd-one-out (Odd), and sudoku completion (SC). strict rule modeling or symbolic manipulation. However, their performance significantly drops in tasks like maze solving, temporal ordering, odd-one-out, and sudoku completion, which require explicit logical reasoning, including adherence to scene rules, symbolic manipulation, and subtle categorical reasoning. This contrast highlights that current models are better suited for tasks emphasizing general understanding and visual realism, but struggle when reasoning demands structured, rule-based logic. These failures are likely attributable to two key factors: (i) models struggle to interpret high-level rules, as seen in maze solving tasks where prompts explicitly forbid crossing maze boundaries, yet violations persist; (ii) symbolic reasoning requires precise visual feature extraction, but encoders like VAE compress features excessively, losing critical details needed for reasoning. Addressing these gaps will require explicit task rule encoding, reinforcement learning for process-level optimization, and more fine-grained visual feature representations and structured processing."
        },
        {
            "title": "5.4 Results with VideoTPO (RQ3)\nBuilding on the above observations, we further sought to investigate whether test-time scaling could deliver\nmore efficient inference optimization than large-scale training. To answer RQ3, we conducted a comprehensive\nevaluation of our proposed VideoTPO in Table 3, alongside two baseline strategies: pre-rewriter based on",
            "content": "9 Figure 7 (Left) Agreement between our metrics and human judgments in Wan 2.1 evaluation; (Right) Comparison of different prompt strategies, w/ HYV Prompt indicates using VideoTPO optimized prompts based on HunyuanVideo. (Google Cloud), and post-rewriter based on (Madaan et al., 2023). The following observation is drawn: Takeaway ❹: VideoTPO is an effective test-time video generation reasoning enhancer. Table 3 demonstrates that our VideoTPO consistently improves reasoning accuracy across all dimensions and difficulty levels, outperforming both the base models and baseline strategies. For instance, applying VideoTPO to HunyuanVideo improves overall performance from 4.03% to 10.25%, while for Wan2.1, the improvement is even more pronounced, increasing from 8.40% to 18.15%. These gains highlight the ability of VideoTPO to refine inference-time reasoning without requiring additional training. Furthermore, pre-rewriter and post-rewriter strategies also yield performance improvements. This indicates that test-time scaling can effectively unlock reasoning capabilities for video generation."
        },
        {
            "title": "5.5 Further Analysis",
            "content": "Analysis of Metric-Human Alignment. To evaluate the reliability of our proposed metrics, we compare the alignment with human judgments, as shown in Figure 7 (Left). Our metrics demonstrate high alignment with human assessments, validating the robustness of our metrics in capturing reasoning-specific task performance, offering reliable alternative to manual evaluation. Analysis of VideoTPO Refined Prompt. We further evaluate the impact of prompt optimization by comparing Wan2.1 with two refined prompts: w/ HYV Prompt (optimized by VideoTPO on HunyuanVideo) and + VideoTPO. As shown in Figure 7 (Right), Wan2.1 w/ HYV Prompt shows limited improvement or even degradation, while Wan2.1 + VideoTPO achieves significant gains across all dimensions. Beyond validating the effectiveness of our VideoTPO, this further demonstrates that different models exhibit varying preferences for prompts."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present TiViBench, hierarchical benchmark designed to evaluate reasoning capabilities of I2V generation models across four dimensions: Structural Reasoning & Search, Spatial & Visual Pattern Reasoning, Symbolic & Logical Reasoning, and Action Planning & Task Execution. With 595 samples across 24 task scenarios and 3 difficulty levels, TiViBench provides comprehensive suite for benchmarking the reasoning capabilities in video generation models. Our evaluation reveals that while commercial models demonstrate stronger and more consistent reasoning capabilities, open-source models show promising yet unstable performance. To this end, we propose VideoTPO, lightweight test-time strategy leveraging multipass generation and preference alignment to unlock model potential without additional training, achieving fine-grained optimization at test time."
        },
        {
            "title": "References",
            "content": "Kling AI: Next-Generation AI Creative Studio. https://klingai.com/global/, 2025. Accessed: 2025-10-11. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pages arXiv2506, 2025. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. Harold Haodong Chen, Haojian Huang, Qifeng Chen, Harry Yang, and Ser-Nam Lim. Hierarchical fine-grained preference optimization for physically plausible video generation. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. https://openreview.net/forum?id=y0SRR9XGlZ. Xiaowei Chi, Peidong Jia, Chun-Kai Fan, Xiaozhu Ju, Weishi Mi, Kevin Zhang, Zhiyuan Qin, Wanxin Tian, Kuangzhi Ge, Hao Li, et al. Wow: Towards world omniscient world model through embodied interaction. arXiv preprint arXiv:2509.22642, 2025. DeepMind. Gemini pro: Advanced multimodal ai model. https://deepmind.google/models/gemini/pro/, 2024. https://deepmind.google/models/gemini/pro/. Accessed: 2025-11-04. Haoyi Duan, Hong-Xing Yu, Sirui Chen, Li Fei-Fei, and Jiajun Wu. Worldscore: unified evaluation benchmark for world generation. arXiv preprint arXiv:2504.00983, 2025. Fanda Fan, Chunjie Luo, Wanling Gao, and Jianfeng Zhan. Aigcbench: Comprehensive evaluation of image-to-video content generated by ai. BenchCouncil Transactions on Benchmarks, Standards and Evaluations, 3(4):100152, 2023. Weixi Feng, Jiachen Li, Michael Saxon, Tsu-jui Fu, Wenhu Chen, and William Yang Wang. Tc-bench: Benchmarking temporal compositionality in text-to-video and image-to-video generation. arXiv preprint arXiv:2406.08656, 2024. Google Cloud. Veo on vertex ai video generation prompt guide. https://cloud.google.com/vertex-ai/ generative-ai/docs/video/video-gen-prompt-guide. Accessed: 2025-10-23. Google Gemini. Video generation overview. https://gemini.google/overview/video-generation, 2025. Accessed: 2025-10-19. Jiawei Gu, Yunzhuo Hao, Huichen Will Wang, Linjie Li, Michael Qizhe Shieh, Yejin Choi, Ranjay Krishna, and Yu Cheng. Thinkmorph: Emergent properties in multimodal interleaved chain-of-thought reasoning. arXiv preprint arXiv:2510.27492, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633638, 2025a. Ziyu Guo, Xinyan Chen, Renrui Zhang, Ruichuan An, Yu Qi, Dongzhi Jiang, Xiangtai Li, Manyuan Zhang, Hongsheng Li, and Pheng-Ann Heng. Are video models ready as zero-shot reasoners? an empirical study with the mme-cof benchmark. arXiv preprint arXiv:2510.26802, 2025b. Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can MLLMs reason in multimodality? EMMA: An enhanced multimodal reasoning benchmark. In Forty-second International Conference on Machine Learning, 2025. https://openreview.net/forum?id=v26vwjxOEz. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 11 Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2180721818, 2024a. Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024b. Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2262322633. IEEE, 2023. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Chenyu Li, Oscar Michel, Xichen Pan, Sainan Liu, Mike Roberts, and Saining Xie. PISA experiments: Exploring physics post-training for video diffusion models by watching stuff drop. In Forty-second International Conference on Machine Learning, 2025a. https://openreview.net/forum?id=RFCp1QzzHQ. Hengtao Li, Pengxiang Ding, Runze Suo, Yihao Wang, Zirui Ge, Dongyuan Zang, Kexian Yu, Mingyang Sun, Hongyin Zhang, Donglin Wang, et al. Vla-rft: Vision-language-action reinforcement fine-tuning with verified rewards in world simulators. arXiv preprint arXiv:2510.00406, 2025b. Yafu Li, Xuyang Hu, Xiaoye Qu, Linjie Li, and Yu Cheng. Test-time preference optimization: On-the-fly alignment via iterative textual feedback. In Forty-second International Conference on Machine Learning, 2025c. https: //openreview.net/forum?id=ArifAHrEVD. Shaowei Liu, Zhongzheng Ren, Saurabh Gupta, and Shenlong Wang. Physgen: Rigid-body physics-grounded image-to-video generation. In European Conference on Computer Vision, pages 360378. Springer, 2024. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/ forum?id=KUNzEQMWU7. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Fanqing Meng, Jiaqi Liao, Xinyu Tan, Quanfeng Lu, Wenqi Shao, Kaipeng Zhang, Yu Cheng, Dianqi Li, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. In Forty-second International Conference on Machine Learning. Haomiao Ni, Changhao Shi, Kai Li, Sharon Huang, and Martin Renqiang Min. Conditional image-to-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1844418455, 2023. OpenAI. Sora 2: Powerful media generation model with synced audio. https://openai.com/index/sora-2/, 2025. Accessed: 2025-10-11. OpenCV Development Team. OpenCV Library. https://github.com/opencv/opencv, 2025. Accessed: 2025-10-10. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. https://openreview.net/forum?id=HPuSIXJaa9. Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: unified vision model for open-world object detection and understanding. arXiv preprint arXiv:2411.14347, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. Dian Shao, Mingfei Shi, Shengda Xu, Haodong Chen, Yongle Huang, and Binglu Wang. Finephys: Fine-grained human action generation by explicitly incorporating physical laws for effective skeletal guidance. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 19051916, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Enxin Song, Wenhao Chai, Weili Xu, Jianwen Xie, Yuxuan Liu, and Gaoang Wang. Video-mmlu: massive multi-discipline lecture understanding benchmark. arXiv preprint arXiv:2504.14693, 2025. Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. Jingqi Tong, Yurong Mou, Hangcheng Li, Mingzhe Li, Yongzhuo Yang, Ming Zhang, Qiguang Chen, Tianyi Liang, Xiaomeng Hu, Yining Zheng, et al. Thinking with video: Video generation as promising multimodal reasoning paradigm. arXiv preprint arXiv:2511.04570, 2025. Vikram Voleti, Chun-Han Yao, Mark Boss, Adam Letts, David Pankratz, Dmitry Tochilkin, Christian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d generation from single image using latent video diffusion. In European Conference on Computer Vision, pages 439457. Springer, 2024. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024. Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025a. Jialong Wu, Tianhao Huang, Changjing He, and Mingsheng Long. Miniveo3-reasoner: Thinking with videos from open-source priors. https://github.com/thuml/MiniVeo3-Reasoner, 2025b. Xianfeng Wu, Yajing Bai, Haoze Zheng, Harold Haodong Chen, Yexin Liu, Zihao Wang, Xuran Ma, Wen-Jie Shu, Xianzu Wu, Harry Yang, et al. Lightgen: Efficient image generation through knowledge distillation and direct preference optimization. arXiv preprint arXiv:2503.08619, 2025c. Qiyao Xue, Xiangyu Yin, Boyuan Yang, and Wei Gao. Phyt2v: Llm-guided iterative self-refinement for physicsgrounded text-to-video generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1882618836, 2025. Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce, Andre Barreto, Pieter Abbeel, and Dale Schuurmans. Video as the new language for real-world decision making. arXiv preprint arXiv:2402.17139, 2024a. 13 Xindi Yang, Baolu Li, Yiming Zhang, Zhenfei Yin, Lei Bai, Liqian Ma, Zhiyong Wang, Jianfei Cai, Tien-Tsin Wong, Huchuan Lu, et al. Vlipp: Towards physically plausible video generation with vision and language informed physical prior. arXiv preprint arXiv:2503.23368, 2025. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024b. Meng YOU, Zhiyu Zhu, Hui LIU, and Junhui Hou. NVS-solver: Video diffusion model as zero-shot novel view synthesizer. In The Thirteenth International Conference on Learning Representations, 2025. https://openreview. net/forum?id=zDJf7fvdid. Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. Optimizing generative ai by backpropagating language model feedback. Nature, 639(8055):609616, 2025. Ailing Zhang, Lina Lei, Dehong Kong, Zhixin Wang, Jiaqi Xu, Fenglong Song, Chun-Le Guo, Chang Liu, Fan Li, and Jie Chen. Ui2v-bench: An understanding-based image-to-video generation benchmark. arXiv preprint arXiv:2509.24427, 2025a. Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, and Donglin Wang. GEVRM: Goal-expressive video generation model for robust visual manipulation. In The Thirteenth International Conference on Learning Representations, 2025b. https://openreview.net/forum?id=hPWWXpCaJ7. Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, et al. Lmms-eval: Reality check on the evaluation of large multimodal models. arXiv preprint arXiv:2407.12772, 2024a. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2024b. Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, and Kai Chen. Pia: Your personalized image animator via plug-and-play modules in text-to-image models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 77477756, 2024c. Zixin Zhang, Kanghao Chen, Xingwang Lin, Lutao Jiang, Xu Zheng, Yuanhuiyi Lyu, Litao Guo, Yinchuan Li, and Ying-Cong Chen. Phystoolbench: Benchmarking physical tool understanding for mllms. arXiv preprint arXiv:2510.09507, 2025c. Jensen Jinghao Zhou, Hang Gao, Vikram Voleti, Aaryaman Vasishta, Chun-Han Yao, Mark Boss, Philip Torr, Christian Rupprecht, and Varun Jampani. Stable virtual camera: Generative view synthesis with diffusion models. arXiv preprint arXiv:2503.14489, 2025."
        },
        {
            "title": "A More Details of Evaluation Dimension",
            "content": "A.1 Motivation for Each Scenario To comprehensively evaluate visual reasoning in video generative models, we design 24 diverse task scenarios across four key dimensions. Each scenario is carefully crafted to challenge specific aspects of visual reasoning, ensuring systematic assessment of models ability to perform beyond general video generation. Below, we outline the motivation for each scenario: Structural Reasoning & Search. Structural reasoning tasks assess models ability to understand and navigate complex spatial structures, sequences, and rules, which are critical for reasoning in dynamic environments. Graph Traversal: Tests the models capability to explore structured graphs and identify valid traversal paths, simulating real-world spatial reasoning. Maze Solving: Challenges models to navigate through constrained environments, requiring spatial planning and decision-making. Sorting Numbers: Evaluates logical ordering of visual elements, emphasizing reasoning over numerical structures in dynamic contexts. Temporal Ordering: Assesses the models ability to infer sequential relationships between events or frames. Rule Extrapolation: Tests the models understanding of abstract rules and its ability to generalize them to new scenarios. Game Move: Simulates decision-making in strategic games, requiring models to predict valid moves based on spatial and logical reasoning. Spatial & Visual Pattern Reasoning. These scenarios focus on recognizing patterns, relationships, and visual consistencies, which are foundational to reasoning in visual contexts. Shape Fitting: Challenges models to match shapes into predefined spaces, testing spatial alignment and pattern recognition. Connecting Colors: Evaluates the ability to identify and connect visually related elements based on color patterns. Pattern Recognition: Assesses models capacity to detect recurring patterns and infer underlying rules. Odd-one-out: Tests models ability to identify anomalies in visual sets, requiring attention to detail and comparative reasoning. Counting Objects: Focuses on numerical reasoning by evaluating the models ability to count and quantify visual elements. Visual Analogy: Assesses abstract reasoning by requiring models to identify analogical relationships between visual objects. Symbolic & Logical Reasoning. Symbolic reasoning tasks test the ability to understand abstract symbols, logical rules, and numerical relationships. Sudoku Completion: Challenges models to complete structured puzzles based on logical constraints, testing symbolic reasoning. Symbolic Reasoning: Evaluates the models ability to infer relationships between abstract symbols and make logical deductions. Arithmetic: Tests numerical reasoning by requiring models to solve basic arithmetic problems presented visually. Visual Deduction: Assesses the ability to infer logical conclusions from visual cues, such as completing partially visible objects. Transitive Reasoning: Challenges models to infer indirect relationships between elements based on transitive logic. Figure 8 Data demonstration across easy, medium, and hard. (Top Left) Structural Reasoning & Search. (Top Right) Symbolic & Logical Reasoning. (Bottom Left) Spatial & Visual Pattern Reasoning. (Bottom Right) Action Planning & Task Execution. Game Rule: Evaluates understanding of abstract rules and their application in visual environments. Action Planning & Task Execution. These tasks simulate real-world scenarios requiring multi-step planning, execution, and adaptability in dynamic environments. Tool Use: Assesses models ability to infer the correct use of tools based on visual cues and task requirements. Robot Navigation: Challenges models to plan and execute navigation in complex spatial environments, simulating robotic reasoning. Goal-directed Planning: Tests multi-step planning towards achieving specific goals in dynamic settings. Multi-step Manipulation: Evaluates the ability to coordinate and execute sequential actions to manipulate objects. Instruction Following: Assesses models capacity to interpret visual instructions and execute tasks accordingly. Game Strategy: Challenges strategic reasoning by requiring models to plan and execute moves in visually dynamic games. A.2 Data Demonstration Here, we present examples of our TiViBench in Figure 8 to provide more vivid illustration of the three difficulty levels: easy, medium, and hard."
        },
        {
            "title": "B More Details of Prompt Suite",
            "content": "B.1 Prompt Generation We adopt Gemini-2.5-Pro (DeepMind, 2024) as powerful assistant to generate an initial version of the inference prompt for our TiViBench. Here we further detail the task prompt for each dimension: Structural Reasoning & Search \"\"\" You are senior researcher in computer vision . You are tasked with generating detailed prompts for Image - to - Video ( I2V ) data samples that evaluate Structural Reasoning & Search abilities . You are given two images : { initial_image } shows the initial state , and { target_image } shows the target state . The corresponding task is { task }. 16 Generate detailed , narratively rich prompt describing how the main subject logically evolves from the initial to the target state . Key points to emphasize : - Center on video content , avoiding overly directive instructions . - Clearly define the start and end states without revealing the exact solution path , maintaining goal clarity . - Imply hidden constraints or rules that the model must infer to understand the transform ation . - Ensure the prompt describes task that unfolds logically and coherently over time , highlighting temporal progression . - Keep the prompt length under 150 tokens . Describe the transformation as logical exploration or structured problem - solving journey , inviting the model to infer intermediate steps and rules that connect the two states . \"\"\" Spatial & Visual Pattern Reasoning \"\"\" You are senior researcher in computer vision . You are tasked with generating detailed prompts for Image - to - Video ( I2V ) data samples that evaluate Spatial & Visual Pattern Reasoning abilities . You are given two images : { initial_image } shows the initial spatial arrangement , and { target_image } shows the target arrangement . The corresponding task is { task }. Generate vivid , descriptive prompt explaining how the main subject spatially transforms from the initial to the target state . Key points to emphasize : - Center on video content , avoiding overly directive instructions . - Provide rich visual descriptions of shapes , colors , positions , and spatial relationships to enhance visual specificity . - Encourage recognition and extension of visual patterns , such as shape fitting , rotations , or color connections . - Allow for open - ended interpretations or multiple valid transformations , without restricting to single solution . - Keep the prompt length under 150 tokens . Narrate the spatial evolution as dynamic visual story , focusing on how the subject spatial configuration changes over time . \"\"\" Symbolic & Logical Reasoning \"\"\" You are senior researcher in computer vision . You are tasked with generating detailed prompts for Image - to - Video ( I2V ) data samples that evaluate Symbolic & Logical Reasoning abilities . You are given two images : { initial_image } shows the initial symbolic or logical state , and { target_image } shows the target state . The corresponding task is { task }. Generate detailed , narratively engaging prompt describing how the symbolic elements or logical conditions in the initial image evolve into those in the target image . Key points to emphasize : - Center on video content , avoiding overly directive instructions . - Avoid explicitly stating the rules ; instead , imply constraints so that the model discovers them implicitly . - Integrate symbolic reasoning tightly with the visual elements present in the images . - Ensure the task involves clear logical progression or sequence of reasoning steps connecting the two states . - Keep the prompt length under 150 tokens . Describe the transformation as story of abstract reasoning and symbolic manipulation unfolding through logical inference . \"\"\" Action Planning & Task Execution \"\"\" You are senior researcher in computer vision . You are tasked with generating detailed prompts for Image - to - Video ( I2V ) data samples that evaluate Action Planning & Task Execution abilities . You are given two images : { initial_image } shows the initial scenario , and { target_image } shows the final scenario . The corresponding task is { task }. Generate richly descriptive , narrative prompt explaining how the main subject plans and executes sequence of actions to reach the target state . 17 Key points to emphasize : - Center on video content , avoiding overly directive instructions . - Define the overall goal clearly while leaving intermediate steps implicit , encouraging goal - oriented interpretation . - Highlight the necessity of multi - step reasoning and sequential action planning . - Emphasize causal relationships and logical cause - and - effect connections between actions and outcomes . - Keep the prompt length under 150 tokens . Frame the transformation as purposeful , temporally coherent journey of task execution and goal fulfillment . \"\"\" B.2 Case Study Here we provide case studies on our prompt creation process in Figure 9."
        },
        {
            "title": "C More Details of Metric Suite",
            "content": "Unlike the metrics commonly used for evaluating video generation models (e.g., temporal coherence, semantic alignment (Huang et al., 2024b,a)), most visual reasoning tasks often have verifiable targets. However, unlike LLM reasoning, which can rely on expert models (e.g., GPT-4o (Wang et al., 2024; Lu et al., 2024)) at the text level, evaluating visual reasoning requires models to demonstrate wide range of visual capabilities, e.g., OCR, counting, and tracking. This makes it challenging to achieve comprehensive evaluation using single expert model. To this end, we design task-specific metrics to accurately and systematically assess different types of tasks. C.1 Final-State Validation OpenCV-based Metrics. To evaluate visual reasoning tasks with clear and verifiable targets, we leverage OpenCV-based (OpenCV Development Team, 2025) metrics tailored to specific task types. These metrics are designed to assess the models ability to perform nuanced visual operations such as edge detection, contour extraction, object segmentation, and OCR. ❶ Sudoku Recognition: This metric evaluates the ability to extract and interpret the digits within Sudoku Figure 9 Example demonstrations of our prompt creation process. grid from an image or video frame. The process involves detecting the grid structure via edge detection and contour approximation, applying perspective transformation, and segmenting the grid into cells. The extracted digit matrix is compared against the ground truth for correctness. ❷ Mathematical Evaluation: For tasks involving mathematical equations, this metric assesses the accuracy of OCR-based text recognition and the semantic equivalence of mathematical expressions. After preprocessing the image (e.g., binarization), the recognized text is parsed and evaluated. The comparison accounts for both exact textual matches and equivalence in computed results, ensuring comprehensive assessment of the models reasoning capabilities. ❸ Visual Multiple Choice: This metric is designed for tasks requiring the identification of correct answers from visual cues, such as detecting red boxes containing letters. It utilizes color segmentation in HSV space to identify candidate regions and applies OCR to extract the letter within each detected box. The correctness is determined by matching the extracted letter with the ground truth answer. ❹ Numeric Sequence Completion: For tasks requiring the completion of numerical sequences, this metric evaluates the accuracy of OCR-based recognition of digits. Through preprocessing and binarization, the sequence is extracted from the video frame and compared with the ground truth. This metric focuses on precise textual recognition and sequence matching. ❺ Graphic Sorting Tasks: This metric assesses the models ability to detect and compare graphical elements, such as blue bars in sorting tasks. Using color segmentation and contour analysis, the heights of bars are measured and compared against the ground truth. The evaluation accounts for both the number of detected bars and their relative heights, ensuring alignment with the expected order. ❻ Match-3-like Games: For visual tasks resembling games (e.g., \"match-3\" or elimination games), this metric compares the structural and pixel-level similarity between the final frame and the ground truth. Edge detection and SSIM are used to evaluate the overlap in patterns and overall image alignment, ensuring the models output adheres to the expected configuration. DINO-based Metrics. To evaluate tasks requiring complex visual reasoning and spatial understanding, we design set of metrics based on DINO (Caron et al., 2021). These metrics are particularly suited for tasks that involve structured visual patterns, such as completing shape sequences, refining sketches, organizing temporal events, solving puzzles, spatial reasoning (e.g., mirroring, rotation), and board game recognition. By leveraging DINOs ability to extract robust and high-level semantic features, we ensure that the evaluation is both adaptable and precise. The core idea behind these metrics is to focus on task-relevant regions within the visual input, rather than evaluating the entire frame. For each sample, we manually annotate the target state with bounding box that specifies the area of interest. The cropped regions from the models output and ground truth are passed through DINO to extract high-dimensional semantic features. Cosine similarity between these features quantifies alignment, with task-specific thresholds determining correctness. This approach ensures robustness to low-level variations while capturing high-level semantic alignment. DINO-based metrics provide flexible framework for assessing diverse visual reasoning tasks, combining localized evaluation with powerful feature extraction to bridge the gap between pixel-level comparisons and semantic understanding. DINO-X-based Grounding Metrics. For tasks requiring complex visual grounding or dynamic target detection, we propose DINO-X-based (Ren et al., 2024) metrics, leveraging DINO-Xs powerful grounding capability. These metrics are particularly suited for scenarios where target areas cannot be predefined or require advanced recognition, e.g., free-space mathematical reasoning, object counting, graph traversal, and odd-one-out detection tasks. The core idea is to dynamically ground task-relevant objects or regions based on high-level semantic prompts. For instance, in graph traversal tasks, we evaluate the number and types of nodes by grounding their visual 20 attributes; and for odd-one-out tasks, we assess the positional and semantic differences of grounded objects (e.g., \"colored circles\") between the generated and ground truth frames. DINO-X enables flexible and robust evaluation by dynamically adapting to task-specific prompts and extracting high-level semantic features. This approach ensures that tasks with diverse visual reasoning requirements are evaluated consistently and accurately, even under challenging conditions where predefined regions or static rules are insufficient. C.2 Process-and-Goal Consistency DINO-X-based Tracking Metrics. While final-state validation is sufficient for some tasks, many require evaluating the entire process to ensure both the correctness of the goal and the validity of the intermediate steps. To address this, we propose DINO-X-based tracking metrics that leverage video tracking and trajectory analysis to assess process-and-goal consistency. These metrics are particularly suitable for tasks such as maze solving, where the solution must avoid invalid actions (e.g., crossing walls or boundaries), and sequential elimination tasks, where objects must disappear in specific order. The core methodology involves using DINOXs visual grounding capabilities to track taskrelevant objects or regions across frames. For example, in trajectory-based tasks, we extract object trajectories by uniformly sampling frames and grounding specific prompts (e.g., \"blue block\") to detect and record object positions over time. Trajectories are then compared against ground truth, ensuring alignment in both spatial and temporal dimensions; and for sequential tasks, we analyze the presence and disappearance of objects (e.g., \"blue ball\", \"red ball\") across sampled frames. The metric validates both the final state (e.g., all objects are eliminated) and the intermediate process (e.g., objects disappear in the correct sequence). Gemini-based QA Metrics. For tasks requiring extensive factual reasoning, such as action planning or tool use, traditional metrics based on visual grounding or trajectory analysis may fall short in capturing the nuanced logical dependencies and causal relationships inherent to these tasks. To address this limitation, we introduce VLM-based QA Metrics (Zhang et al., 2024b; Lu et al., 2024; Zhang et al., 2024a), that leverages the reasoning capabilities of Gemini-2.5-Pro (DeepMind, 2024) to assess task performance through question answering. Specifically, for each sample in this category, we design two or three binary questions tailored to the tasks core requirements (e.g., \"Is the wrench picked up in the video?\"). These questions are constructed to capture key aspects of the tasks correctness, including intermediate actions, causal dependencies, and goal achievement. The generated video is then provided to Gemini-2.5-Pro along with the questions, and its responses are compared against the ground truth. sample is deemed correct only if all three answers align with the ground truth, ensuring high standard of evaluation fidelity."
        },
        {
            "title": "D More Details of VideoTPO",
            "content": "D.1 Prompt Design Following TextGrad [Nature25] (Yuksekgonul et al., 2025), we adopt GPT-4o (Achiam et al., 2023) as the optimizer and adopt the vanilla prompts for textual gradient calculation and prompt update from its implementation. To meet the requirements of video generation optimization, we further designed the textual loss calculation prompt:"
        },
        {
            "title": "Textual Loss Calculation",
            "content": "\"\"\" You are video generation system optimization expert tasked with evaluating target text prompt and the generated video . Analyze the strengths and weaknesses of each generated video step by step , and explain why the video is not good or why it is good . ** Current Prompt **: { cu rrent_prompt } ** Reasoning Task **: { task _ de finition } ** Note **: - The videos were stitched together vertically to form single video for comparison purposes . - Your output should only include the analysis . - There may be instances where both videos are subpar , necessitating strict adherence to the task definition . ** Input Videos **: { input_videos } \"\"\" D.2 More Analysis In TPOs (Li et al., 2025c) setting, reward model is Analysis of Self-Analysis vs. Reward Model. employed to select preferred sample and non-preferred sample from the generated candidates, which are then used to compute textual loss and gradients. However, VideoTPO eliminates the need for an additional reward model by leveraging task-specific VLMs (i.e., GPT-4o) to conduct self-analysis among candidate samples. The self-analysis process identifies strengths and weaknesses of each sample, directly informing optimization without relying on external scoring models. To validate the effectiveness of our strategy, we compare VideoTPO with two widely-used reward strategies: CLIP scoring and GPT scoring, as shown in Figure 10 (Left). Results show that VideoTPO achieves significantly better accuracy, outperforming these reward-based methods across all reasoning dimensions. This advantage is likely due to the complexity of reasoning tasks, where candidate samples often exhibit subtle differences. In such cases, relying on reward model to identify the \"best\" and \"worst\" samples provides limited utility, while self-analysis enables more nuanced understanding of sample quality. Figure 10 (Left) Analysis of VideoTPOs rewarding strategies; (Middle) Scaling width across sample numbers; and (Right) Scaling depth across test-time training steps. Analysis of Scaling in Width and Depth. To further evaluate the scalability of VideoTPO, we explore its performance under varying candidate sample numbers (width) and scaling steps (depth), with our default settings of 2 samples and 2 steps, respectively. Figure 10 (Middle and Right) illustrates the impact of scaling in both dimensions. In terms of width, increasing the number of candidate samples consistently improves accuracy, as the selfanalysis process benefits from broader pool of options to identify optimal reasoning pathways. Similarly, scaling in depthby increasing the number of test-time training stepsalso yields substantial performance gains, demonstrating the robustness of VideoTPO under extended optimization. These results highlight the flexibility and effectiveness of VideoTPO, making it scalable solution for reasoning-intensive video generation tasks."
        },
        {
            "title": "E Exhibition Board",
            "content": "Demonstration of Results with VideoTPO. Here we provide qualitative results of VideoTPO from Figure 11 to Figure 18. Demonstration of Results on TiViBench. For more comparison results of evaluation on TiViBench, please refer to our project page: https://haroldchen19.github.io/TiViBench-Page/. 23 Figure 11 Case demonstration of VideoTPO. 24 Figure 12 Case demonstration of VideoTPO. Figure 13 Case demonstration of VideoTPO. 26 Figure 14 Case demonstration of VideoTPO. 27 Figure 15 Case demonstration of VideoTPO. Figure 16 Case demonstration of VideoTPO. 29 Figure 17 Case demonstration of VideoTPO. 30 Figure 18 Case demonstration of VideoTPO."
        }
    ],
    "affiliations": [
        "CUHK",
        "CityUHK",
        "FDU",
        "HKUST",
        "HKUST(GZ)",
        "Knowin",
        "SJTU"
    ]
}