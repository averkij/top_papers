{
    "paper_title": "Can Language Models Replace Programmers? REPOCOD Says 'Not Yet'",
    "authors": [
        "Shanchao Liang",
        "Yiran Hu",
        "Nan Jiang",
        "Lin Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have achieved high accuracy, i.e., more than 90% pass@1, in solving Python coding problems in HumanEval and MBPP. Thus, a natural question is, whether LLMs achieve comparable code completion performance compared to human developers? Unfortunately, one cannot answer this question using existing manual crafted or simple (e.g., single-line) code generation benchmarks, since such tasks fail to represent real-world software development tasks. In addition, existing benchmarks often use poor code correctness metrics, providing misleading conclusions. To address these challenges, we create REPOCOD, a code generation benchmark with 980 problems collected from 11 popular real-world projects, with more than 58% of them requiring file-level or repository-level context information. In addition, REPOCOD has the longest average canonical solution length (331.6 tokens) and the highest average cyclomatic complexity (9.00) compared to existing benchmarks. Each task in REPOCOD includes 313.5 developer-written test cases on average for better correctness evaluation. In our evaluations of ten LLMs, none of the models achieve more than 30% pass@1 on REPOCOD, indicating the necessity of building stronger LLMs that can help developers in real-world software development. REPOCOD is available at https://github.com/lt-asset/REPOCOD"
        },
        {
            "title": "Start",
            "content": "Can Language Models Replace Programmers? REPOCOD Says Not Yet Shanchao Liang Purdue University liang422@purdue.edu Yiran Hu Purdue University hu954@purdue.edu Nan Jiang Purdue University jiang719@purdue.edu Lin Tan Purdue University lintan@purdue.edu 4 2 0 N 3 ] . [ 3 7 4 6 1 2 . 0 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have achieved high accuracy, i.e., more than 90% pass@1, in solving Python coding problems in HumanEval and MBPP. Thus, natural question is, whether LLMs achieve comparable code completion performance compared to human developers? Unfortunately, one cannot answer this question using existing manual crafted or simple (e.g., single-line) code generation benchmarks, since such tasks fail to represent real-world software development tasks. In addition, existing benchmarks often use poor code correctness metrics, providing misleading conclusions. To address these challenges, we create REPOCOD, code generation benchmark with 980 problems collected from 11 popular real-world projects, with more than 58% of them requiring file-level or repository-level context information. In addition, REPOCOD has the longest average canonical solution length (331.6 tokens) and the highest average cyclomatic complexity (9.00) compared to existing benchmarks. Each task in REPOCOD includes 313.5 developerwritten test cases on average for better correctness evaluation. In our evaluations of ten LLMs, none of the models achieve more than 30% pass@1 on REPOCOD, indicating the necessity of building stronger LLMs that can help developers in real-world software development. REPOCOD is available at https://github.com/ltasset/REPOCOD"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (Bian et al., 2023; Zheng et al., 2023) have seen rapid advancements in recent years, increasingly becoming integral to various aspects of our daily lives and significantly enhancing productivity. One critical application of these models is code generation (Li et al., 2023a; Ouyang et al., 2023), where the models generate executable code snippets based on provided natural language descriptions (Li et al., 2023b; Lozhkov et al., 2024; Touvron et al., 2023; Dubey et al., 2024; Achiam et al., 2023). Such code completion tasks are an integral part for software development. Research of LLMs for code completion requires high-quality dataset, whose code completion tasks satisfy three properties: (1) real-world code completion tasks, (2) realistic task complexity, and (3) reliable correctness evaluation metrics. Specifically, first, it should contain real-world code completion tasks in real-world projects. While manually crafted code completion tasks are useful, they may not represent the actual codecompletion tasks developers have to complete. HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) only contain such crafted code completion tasks, thus, they are not sufficient to evaluate LLMs real-world code completion abilities. Second, the code completion tasks should have realistic complexity. Realistic code completion tasks often require the generation of code that has dependencies on other functions, files, and classes in the project. HumanEval and MBPP require no external contexts, failing to evaluate LLMs ability to help developers build real-world projects. Benchmarks such as ClassEval (Du et al., 2024), CoderEval (Zhang et al., 2024), CrossCodeEval (Ding et al., 2023), RepoBench (Liu et al., 2024), and Long-Code-Arena (Bogomolov et al., 2024) include code generation problems with project-level dependencies. Yet, their generation targets are restricted to single-line code or short functions, while realistic projects requires up to hundreds of lines of code for one function. Third, the code completion data set should include reliable evaluation metrics and data that can accurately evaluate the correctness of LLMgenerated code snippets. Some project-level code generation benchmarks (Ding et al., 2023; Liu et al., 2024; Bogomolov et al., 2024) use similarity/matching-based evaluation metrics, such as exact match or CodeBLEU (Ren et al., 2020), thorough study of ten LLMs performance on REPOCOD, and the key findings include: LLMs are ineffective at generating functions with high content complexity (up to 25% pass@1 for tasks requiring repository-level context). Using the current file as context results in better model performance than Dense or BM25 retrieval methods for commercial models. There are many uniquely solvable problems for each LLM, showing REPOCODs diversity."
        },
        {
            "title": "2.1 LLMs for Code",
            "content": "LLMs have been widely used for code-related tasks, among which code generation is the most important and common task (Achiam et al., 2023; Li et al., 2023b; Lozhkov et al., 2024; Nijkamp et al., 2023; Rozière et al., 2024; Guo et al., 2024; Luo et al., 2024). Existing work on code generation typically forms this task as generating the self-contained programs given the function signature and natural language description. They have shown an impressive ability to generate self-contained programs. However, the performance of LLMs for code generation within the context of projects or repositories remains underexplored. Despite the use of retrieval-augmented generation (RAG) to retrieve relevant context from the project (Ding et al., 2023; Zhang et al., 2024; Wu et al., 2024), generating code within the context of projects remains more challenging and yields lower accuracy compared to generating self-contained code. In addition to code generation, LLMs have also been used for program repair (Jiang et al., 2023; Xia et al., 2023; Hossain et al., 2024; Jin et al., 2023), automated testing (Yang et al., 2017; Deng et al., 2023), code translation (Yang et al., 2024; Roziere et al., 2020; Pan et al., 2023) and code summarization (Zhang et al., 2020; LeClair et al., 2020; Ahmed et al., 2024), greatly enhancing the productivity within these domains."
        },
        {
            "title": "2.2 Code Generation Benchmarks",
            "content": "Code generation benchmarks are important to evaluate LLMs ability to generate code. Selfcontained benchmarks (Hendrycks et al., 2021; Li et al., 2022; Xia et al., 2024; Liu et al., 2023) are unable to evaluate LLMs capability to generate code for real-world projects. Figure 1: Two examples from RepoBench showing misleading metrics results. The yellow and blue highlights indicate the difference between ground truth (GT) and the LLM-generated code. which fail to accurately evaluate the correctness of LLM-generated code. Those metrics often lead to semantically-equivalent code being considered incorrect, and incorrect (but syntactically similar) code being considered correct. For example, Figure 1 (a) illustrates two examples from RepoBench. Although the LLM-generated code is semantically equivalent to the ground truth, CodeBLEU, edit similarity, and exact match would incorrectly consider the code incorrect. In Figure 1 (b), CodeBLUE and edit similarity assign high scores to the LLM-generated code even though the code is incorrect. These examples highlight the limitations of similarity/matching-based metrics in distinguishing correct and incorrect generated solutions. This paper proposes REPOCOD, complex code generation benchmark that not only evaluates LLMs capability in generating functions for real-world projects but also utilizes developerwritten test cases to validate the correctness of LLM-generated functions. This paper makes the following contributions: challenging code generation dataset, consisting of real-world complex coding tasks, with developer-created tests for better correctness evaluation. REPOCOD includes 980 code generation problems collected from 11 popular projects. The average length of our canonical solutions contains 331.6 tokens, more complex than existing code generation benchmarks. On average, each instance includes 331.6 developer-written test cases. Over one quarter (257, 26.22%) of instances in REPOCOD require repository-level contexts. novel real-world data collection pipeline to construct code generation dataset. Figure 2: Data collection pipeline and instance structure of REPOCOD. few project-level code generation benchmarks have been developed. CrossCodeEval (Ding et al., 2023), RepoBench (Liu et al., 2024), and LongCode-Arena (Bogomolov et al., 2024) are multilingual benchmarks collected from GitHub projects. They consist of problems that require LLMs to generate single line of code given the incomplete code and project context. These benchmarks share two limitations: (1) their target code is limited to single-line outputs, making it inadequate for evaluating LLMs ability to generate complex, multi-line functions, and (2) their similarity-based evaluation metrics fail to accurately evaluate the correctness of LLM-generated code. Besides these single-line project-level code generation benchmarks, ClassEval (Du et al., 2024) is manually crafted benchmark with 100 problems that require LLMs to generate code for given class. CoderEval (Zhang et al., 2024) is made up of 230 problems collected from GitHub repositories with human-labeled documentation, which requires LLMs to generate solutions given project-level contexts. Although these two benchmarks provide test cases for evaluation and extend beyond single-line code generation, they are still inadequate in terms of problem complexity and scale. In contrast, REPOCOD combines more complex real-world problems (analyzed in Section 3.3.2) with rigorous test-based assessments and consists of 980 project-level code generation tasks that require LLMs to write large functions instead of single-line snippets, aligning model evaluation with the expectations of modern software development."
        },
        {
            "title": "3 REPOCOD Benchmark",
            "content": "This section introduces REPOCODs automated data collection process (Section 3.1) and data structure (Section 3.2) first, and then demonstrates the statistics of REPOCOD (Section 3.3)."
        },
        {
            "title": "3.1 Data Collection Pipeline",
            "content": "Figure 2 (a) shows the data collection pipeline. REPOCOD utilizes GitHub as the source of our data. To filter noisy data such as functions with missing descriptions and test cases, we employ three-stage data collection pipeline to efficiently collect target functions with good documentation and the corresponding test cases for validation. Step I: Repository Selection. The selection criteria include open-sourced repositories with Python as the primary language (70%) and those with no less than 2k stars, as popular repositories tend to be well-maintained (Jimenez et al., 2024). We clone the latest version of these repositories for step II and step IIIs analysis. Step II: Target Function Selection. For each collected repository, we perform both static and dynamic analysis to accurately identify the functions defined in the repository that are invoked by the developer-provided test cases, with detailed docstrings as function specifications. We first collect all the test functions in repository using PyTests test discovery rules1. Then, we identify the functions invoked within these test functions using static and dynamic analysis techniques. For static analysis, we use treesitter (Brunsfeld et al., 2024) to parse the test functions and collect the functions that are invoked. For dynamic analysis, Pythons trace module is used to examine the execution of each test case. This approach also identifies the indirect function calls invoked by test functions, which are challenging to detect through static analysis alone. Finally, we filter the target functions, retaining only those with more than ten lines of docstrings and at least two lines of function body. Step III: Relevant Test Cases Collection. To validate the correctness of LLM-generated code efficiently, we can skip running the entire test suite and instead focus on executing only the relevant test casesthose that specifically invoke the target functions. This targeted approach significantly reduces verification time in large repositories with extensive test suites. While the previous step provides mapping between each target function and relevant test cases, it cannot guarantee complete accuracy. For instance, 1https://docs.pytest.org/en/stable/announce/ release-8.3.0.html Dataset #NL #GT Cyclo. #Funcs. #NL #GT Cyclo. #Funcs. #NL #GT Cyclo. #Funcs. #NL #GT Cyclo. #Funcs. Repository-level File-level Self-contained Total astropy datasets flask more-itertools plotly.py pylint scikit-learn seaborn sphinx sympy xarray 446.4 274.9 308.4 497.1 215.0 448.0 242.8 87.3 1806.0 3393.0 - 398.9 426.0 - 347.6 - - 222.1 349.5 - 918.6 - 9.4 9.1 6.5 2.7 132.0 - 7.7 10.3 - 8.8 - Total 269.4 396.7 8.3 10 19 2 6 1 0 208 6 0 5 0 399.4 407.1 297.3 196.7 326.2 152.3 300.2 136.8 1366.9 932.2 163.9 432.1 262.9 258.3 201.2 236.0 191.7 606.6 903.9 586.7 902.1 418.5 537.6 394.9 8.8 4.8 5.2 4.3 23.5 15.2 5.3 6.4 17.7 17.6 11.4 10.6 37 20 13 23 19 9 43 42 12 58 40 291.5 284.8 311.9 135.3 99.4 231.2 233.4 96.3 1661.7 605.1 176.4 193.1 204.3 220.9 240.8 211.6 263.8 127.8 837.8 277.6 593.0 160. 316 522.8 241.4 7.9 3.6 3.6 5.0 23.9 7.0 5.0 6.7 4.0 8.6 5.3 8.2 38 20 28 57 56 17 63 30 21 34 43 336.5 357.0 366.6 211.9 270.0 120.8 252.0 106.5 1589.9 723.5 172.0 275.8 224.1 344.0 227.9 241.3 237.5 301.9 881.5 466.0 742.0 284. 407 461.1 331.6 8.5 5.8 4.2 4.6 25.3 9.8 6.8 6.8 8.9 14.0 8.2 9.0 85 59 43 86 76 26 314 78 33 97 83 Table 1: Basic statistics of REPOCOD, with details broken down by each collected repository. test functions might use subprocess.run(command) to invoke repository functions indirectly, which may be missed by static or dynamic analysis. Therefore, we employ two-step collection procedure to ensure the capture of all relevant test cases. First, we execute all test cases in the repository to establish reference result. Then, for each target function, we replace it with an assertion failure, rerun all test cases, and compare the results to the reference. If test result changes from pass to fail, it indicates that the test case is relevant to the target function."
        },
        {
            "title": "3.2 Benchmark Structure",
            "content": "Figure 2 (b) illustrates the components of REPOCODs instances: the target function description, repository snapshot, relevant test cases, and canonical solution. REPOCOD uses the developerprovided docstring as the target function description. The repository snapshot, with the target function body removed, is provided as context. The relevant test cases are prepared for each task instance and the LLM generated solution is considered correct if it passes all relevant tests. Finally, the canonical solutions are the original developerwritten function bodies of all instances."
        },
        {
            "title": "3.3.1 Basic Statistics",
            "content": "Overall, REPOCOD includes 980 data instances collected from total of 11 popular repositories. These repositories cover wide range of functionalities, including data science, scientific computing, web development, and software development tools. Table 1 presents detailed statistics for instances from each repository, categorized by context complexity types: repository-level, file-level, and selfcontained. For each category, it shows #NL (number of tokens in target function descriptions), #GT (number of tokens in canonical solutions), Cyclo. Datasets #Instances #Tokens Cyclo. #Tests CrossCodeEval RepoBench Long-Code-Arena CoderEval ClassEval REPOCOD 2,665 23,561 32,803 230 100 980 13.2 13.7 12.0 108.2 123.7 331. N/A N/A N/A 4.71 2.89 9.00 N/A N/A N/A - 33.1 313.49 Table 2: Benchmarks Comparison. Tokens represent the average amount of tokens of the canonical solutions (average cyclomatic complexity of the canonical solution) (McCabe, 1976)), and #Funcs. (number of target functions). We define three types of context complexities: Repository-level involves canonical solutions that call functions from other files in the repository; filelevel involves calls to functions within the same file (excluding the target function) but not from other files; and self-contained includes functions that only use commonly used libraries (e.g., numpy) or Python built-in modules (e.g., os). Among the three settings, the repository-level functions have the longest token length for the canonical solutions (396.7), compared to the filelevel functions (394.9) and self-contained functions (241.4). Additionally, the repository-level functions have the shortest natural language prompt length (537.6), compared to both file-level and selfcontained functions (522.8). The combination of the longest average token length for the canonical solution and the shortest natural language description makes repository-level functions the most challenging programming scenario. The average cyclomatic complexity across the three settings is quite similar, ranging from 8.3 to 10.6, indicating comparable control flow complexity for the canonical solutions at all complexity levels."
        },
        {
            "title": "3.3.2 Comparison with Existing Benchmarks",
            "content": "Table 2 compares REPOCOD with other popular code generation benchmarks that use repositoryDataset Repository-level File-level Self-contained Total CoderEval ClassEval REPOCOD 23 0 257 123 314 84 153 407 230 467 980 Table 3: Data context complexity distribution of code generation benchmarks. level or class-level context, including CrossCodeEval (Ding et al., 2023), RepoBench (Liu et al., 2024), Long-Code-Arena (Bogomolov et al., 2024), CoderEval (Zhang et al., 2024), and ClassEval (Du et al., 2024). The metrics compared include #Instances (the number of samples in each benchmark), #Tokens (the average number of tokens of the canonical solution), Cyclo. (cyclomatic complexity), and #Tests (the average number of test cases per instance). Data entry with N/A represents not applicable, and - represents this data is not publicly available. CrossCodeEval, RepoBench, and Long-CodeArena contain only single-line code generation tasks, resulting in shorter canonical solutions compared to REPOCOD. Additionally, they lack test cases and rely on similarity-based metrics, which are insufficient to evaluate code quality. Therefore, we mainly compare REPOCOD against CoderEval and ClassEval, as they require full-function generation and include test cases for evaluation. Dataset Scale. REPOCOD has the largest number of instances compared with CoderEval and ClassEval, indicating its ability to perform more comprehensive evaluations of code generation models. Furthermore, REPOCOD is collected through our automated pipeline, and is easier to further expand than the others that involve manual annotations. Problem Complexity. REPOCOD presents more challenging problems compared to CoderEval and ClassEval. It has the largest average token count for canonical solutions at 331.6  (Table 2)  , meaning LLMs are required to generate more than twice as much code to complete the task. It also includes cyclomatic complexity score of 9.00, indicating that the canonical solutions in REPOCOD have higher structural complexity regarding control flow. These statistics highlight that the problems in REPOCOD are the most challenging and require the longest generation length in existing benchmarks. Context Complexity. In addition to the complexity of canonical solutions, we compare the context complexity of REPOCOD with CoderEval and ClassEval, which measures the difficulty of managing, understanding, and utilizing the project context. Table 3 shows REPOCOD has significantly more functions with higher context complexity compared to other benchmarks. Specifically, REPOCOD contains the most repository-level dependencies (257), notably higher than CoderEval (23) and ClassEval (0). With the highest context complexity among existing code generation benchmarks, REPOCOD is the most suitable for evaluating LLMs performance in repository-level code generation tasks. Test Scale. REPOCOD utilizes an average of 312.9 tests per instance, significantly surpassing other benchmarks, highlighting REPOCOD robustness in evaluating models."
        },
        {
            "title": "4 Experiment Setup",
            "content": "Due to the large repository sizes, most SOTA LLMs face context window limitations that prevent them from processing all the context provided by REPOCOD. We evaluate with three retrieval settings to extract relevant context subsets for generating solution code: sparse retrieval, dense retrieval, and current file, detailed below."
        },
        {
            "title": "4.1 Retrieval Settings",
            "content": "Sparse Retrieval. For sparse retrieval, we use BM25 (Robertson and Zaragoza, 2009) to extract relevant functions from repositories as context for the evaluated model. The signatures, docstrings, file paths (relative to the projects root), and bodies of the retrieved functions are provided as context. Dense Retrieval. We encode the repository functions using text-embedding-3-small model. The target functions signature and specification are encoded into query to be compared against the function embeddings. The top-ranked functions based on cosine similarity are retrieved and provided as context, in the same format as sparse retrieval. Current File. We introduce new setting, where the context is limited to the file containing the target function, with the entire file provided as context, excluding the target functions body."
        },
        {
            "title": "4.2 Prompting Format",
            "content": "As described in Section 3.2, the data instance of REPOCOD consists of the repository snapshot, target function signature, and docstring. Figure 3 demonstrates an example of the prompt construction process, the format of the prompt, and the evaluation pipeline used for all our experiments. The section highlighted in green represents the data instance of REPOCOD (step 1). The format Figure 3: Illustration example of prompting LLMs and evaluating the LLM-generated code. Models BM25 Dense Current-File CodeLlama-7B CodeLlama-34B DeepSeekCoder-6.7B DeepSeekCoder-33B OpenCodeInterpreter-6.7B OpenCodeInterpreter-33B Claude 3.5 Sonnet DeepSeek-V2.5 GPT-4o-Mini GPT-4o 10.71 12.35 13.98 16.73 12.14 15.31 14.39 18.47 15.10 27.35 10.41 12.76 14.08 17.14 12.45 16. 17.45 20.71 15.00 27.04 5.71 9.59 10.92 14.90 13.16 18.27 19.80 27.04 18.67 26.84 Table 4: Pass@1(%) of SOTA LLMs on REPOCOD. of the prompt is detailed in the section highlighted in blue (step 2). Our prompt consists of the system prompt, the file path to the target function (relative to the repository root), the retrieval context, and the target function description (the function signature and docstring of the target function). The retrieval context contains the relative file path, as well as the signature, the docstring, and the body of the retrieved functions. If the context exceeds LLMs context window, we truncate it from the beginning. Once the LLM generates the solution (step 3), the code snippet is inserted into the repository for testing (step 4, modified file highlighted in blue). Then REPOCOD executes the relevant test cases (step 5) and determines the correctness of the solution (step 5). As one of the test cases fails, this generated code snippet is considered incorrect."
        },
        {
            "title": "4.3 Model Setup",
            "content": "Given the complexity of REPOCOD, we only select the LLMs that satisfy the following requirements to evaluate: (1) the LLMs show SOTA performance on existing benchmarks such as HumanEval and MBPP, (2) the LLMs support more than 16K context length, as shorter context length are unable to fit in the prompt we use in Figure 3. Thus, we evaluate GPT-4o, GPT-4o-mini, Figure 4: Passed Evaluations under different retrievals. Figure 5: Commercial LLMs passed evaluations. DeepSeek-V2.5, and Claude 3.5 Sonnet, representing top-performing commercial LLMs. We also evaluate LLMs in CodeLlama, DeepSeek-Coder, and OpenCodeInterpreter series, ranging from 6.7B to 34B parameters on REPOCOD. For commercial models, we use their official API, and for the open-sourced LLMs, we use the implementations provided by HuggingFace and vLLM (Kwon et al., 2023). Under each experimental setting (retrieval approach), we let each LLM generate one output per instance in REPOCOD using greedy decoding."
        },
        {
            "title": "5.1 SOTA LLMs’ Pass@1 on REPOCOD",
            "content": "Table 4 shows ten LLMs performance on REPOCOD, under three retrieval settings. On all retrieval methods, commercial LLMs have better performance. Specifically, GPT-4o has the best result, reaching up to 27.35% pass@1. On the other hand, none of the open-sourced LLMs has over 20% pass@1 in any retrieval settings."
        },
        {
            "title": "3.5 Sonnet may lack some knowledge commonly\nshared among other commercial models, resulting\nin fewer overlapping solutions.",
            "content": "This result shows that SOTA LLMs still struggle with repository-level code generation. Compared to their pass@1 on HumanEval (about 90% (Guo et al., 2024)) and MBPP, SOTA LLMs are still far away from writing real-world programs requiring repository-level information. We provide detailed discussion of the results below. Impact of Retrieval Approach. The pass@1 results are higher for commercial models when using contexts from the current-file setting, while BM25 and dense retrieval yield similar but lower outcomes. This trend is also observed in the OpenCodeInterpreter series of LLMs. In contrast, other open-source LLMs perform better with BM25 and dense retrieval compared to the current-file setting. These findings suggest that commercial models are more likely to benefit from contexts drawn from the current-file. Figure 4 illustrates the overlap and uniqueness of model results using GPT-4os result as an example. Though most solutions (more than 150) are solvable by all retrieval settings, each retrieval setting has uniquely solvable problems, which highlights the value of each method in capturing distinct aspects of the context. This suggests that different retrieval techniques provide complementary information. Impact of Model Size. Results from Table 4 also demonstrate the consistent advantage of larger models in solving complex repository-level code completion tasks compared to smaller models within the same architecture. Overlap and Uniqueness of Results in Different Settings. Figure 5 provides breakdown of all correctly generated results from the commercial LLMs on REPOCOD under the Dense retrieval setting. Notably, each model has its own unique set of problems that it alone can solve, highlighting the diversity of REPOCOD. Additionally, two modelsClaude 3.5 Sonnet and GPT-4ohave higher number of uniquely solvable problems, indicating that they excel at certain tasks compared to other models. Another interesting finding is that, despite Claude 3.5 Sonnet being only the third-best performing commercial model in the dense retrieval setting, it has the fewest overlaps in correct evaluations with other models. This suggests that Claude"
        },
        {
            "title": "5.2 Performance under Different Complexity",
            "content": "We show the performance of SOTA LLMs in solving problems with different complexities. We compare two types of complexities: Context Complexity, and Canonical Solution Length. The results are collected under the dense retrieval setting. Context Complexity. Figure 6a details the comparison of the evaluation results of SOTA LLMs for functions with different context complexities. All models have the lowest pass@1 when solving functions with repository-level context compared to functions with less complex contents. The overall solve rate decreases as the complexity of the context level increases. In addition, the low solve rate highlights the difficulty of solving problems with repository-level contexts and demonstrates the strength of REPOCOD compared to all existing code generation benchmarks. Canonical Solution Length. Figure 6b presents the performance of LLMs across different length complexity. Similar to the findings from the previous section, the pass@1 of the LLMs gradually decreases as the complexity of the functions increases. In the challenging scenarios (token length > 232), even the top-performing model, GPT-4o, achieves less than 10% pass@1. This low pass@1 performance indicates that while SOTA LLMs are effective at solving simpler problems, they struggle significantly with problems that have medium to long canonical solution lengths in REPOCOD. Combining the fact that REPOCOD has the longest canonical solution length  (Table 2)  and the most problems with repository-level context  (Table 3)  , REPOCOD sets new standard for LLMs in code generation."
        },
        {
            "title": "5.3 Ablation Studies",
            "content": "5.3.1 Inference Length V.S. GT length We analyze the relation between the inference length and the canonical solutions length under all retrieval settings. Table 5 presents the average inference-to-canonical solution length ratio using micro-averaging. lower ratio indicates the models ability to closely match the canonical solutions functionality. Most open-source models produce significantly longer correct solutions compared to the canonical solution when using BM25 (a) Pass@1(%) for LLMs across different context complexities. (b) Pass@1(%) for LLMs across different length complexities. Figure 6: Pass@1 under different complexity settings: context complexity and length complexity. BM Dense Current-File Models 0 [0,0.5) [0.5,1) 1 Models Pass Fail Pass Fail Pass CodeLlama-7B CodeLlama-34B DeepSeekCoder-6.7B DeepSeekCoder-33B OpenCodeInterpreter-6.7B OpenCodeInterpreter-33B 43.6 16.1 51.0 24.7 33.8 8.3 72.1 11.1 1.4 0.9 1.4 1.4 DeepSeek-V2.5 Claude 3.5 Sonnet GPT-4o-Mini GPT-4o 2.7 18.6 3.6 1.3 1.6 4.0 2.7 1.2 38.3 16.8 63.3 22.8 8.6 31.6 9.6 77.6 1.4 1.4 0.9 1. 2.5 21.9 3.9 1.3 1.6 2.7 2.6 1.2 25.0 19.0 16.4 14.5 1.4 1.4 2.9 7.8 9.9 2.1 Fail 18.7 28.9 11.3 22.4 1.4 0. 1.4 5.7 1.2 1.0 Table 5: Ratio of generation length against the length of canonical solutions. or Dense retrieval methods, suggesting high portion of irrelevant information or computations are included in the generated solution. The CurrentFile method yields slightly lower ratio but still exhibits these tendencies. In contrast, commercial models show relatively low ratios, indicating greater precision. On average, the LLM generated result has longer length than canonical solutions."
        },
        {
            "title": "5.3.2 Retrieval Results Impact",
            "content": "Table 6 presents the pass@1 performance of LLMs for retrieved content across different recall rate ranges: 0, (0, 0.5), [0.5, 1), and 1. These four settings reflect the proportion of correct content within the top 10 retrieved items. As the retrieved content includes more ground truth contexts, the pass@1 scores increase. This analysis focuses on samples with repository-level and file-level contexts. Across all models, pass@1 scores improve when the recall rate increases from 00.5 to 0.51 or 1. Interestingly, when recall is low, model performance is comparable to that with 0 recall rate. Higher recall in retrieved content consistently leads to better model performance. CodeLlama-7B CodeLlama-34B DeepSeekCoder-6.7B DeepSeekCoder-33B OpenCodeInterpreter-6.7B OpenCodeInterpreter-33B DeepSeek-V2.5 Claude 3.5 Sonnet GPT-4o-Mini GPT-4o 6.05 5.04 5.54 7.81 6.05 9.07 10.33 6.55 6.55 14.61 0.00 2.50 5.00 5.00 0.00 5. 5.00 5.00 5.00 7.50 13.16 21.05 13.16 15.79 15.79 15.79 18.42 7.89 18.42 26.32 13.27 16.33 21.43 25.51 16.33 20.41 26.53 29.59 26.53 35.71 Table 6: Pass@1(%) for instances under different recall rates of retrieved contents with dense retrieval."
        },
        {
            "title": "6 Limitation",
            "content": "Our work has several limitations. First, we currently collect data from only 11 repositories, covering small subset of those that could be included in this benchmark; future versions of REPOCOD will expand to include more popular Python repositories. Second, we evaluate only ten models, representing subset of popular LLMs. With more time and resources, we could test broader range, providing more comprehensive view of LLM capabilities in repository-level code generation. We will publish REPOCOD so the community can evaluate additional models, broadening the scope of tested LLMs."
        },
        {
            "title": "7 Conclusion",
            "content": "We present REPOCOD, real-world, complex dataset designed for code generation tasks with repository-level context. REPOCOD comprises 980 instances from 11 popular Python repositories, including 257 requiring repository-level context and 316 requiring file-level context, highlighting the complexity and comprehensiveness of the benchmark. Additionally, we introduce an innovative automatic extraction method that can scale REPOCOD to any Python project, offering scalable solution for repository-level code generation benchmarks. Our evaluation of SOTA LLMs on REPOCOD reveals maximum pass@1 of 27.35%, with even lower scores for functions requiring repositorylevel context, underscoring that current models fall short of generating realistic repository-level code. This work underscores the need for further research in repository-level code completion."
        },
        {
            "title": "References",
            "content": "Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. Preprint, arXiv:2107.03374. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Yinlin Deng, Chunqiu Steven Xia, Chenyuan Yang, Shizhuo Dylan Zhang, Shujing Yang, and Lingming Zhang. 2023. Large language models are edge-case fuzzers: Testing deep learning libraries via fuzzgpt. Preprint, arXiv:2304.02014. Toufique Ahmed, Kunal Suresh Pai, Premkumar Devanbu, and Earl Barr. 2024. Automatic semantic augmentation of language model prompts (for code summarization). In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, pages 113. Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, and Bing Xiang. 2023. Crosscodeeval: diverse and multilingual benchmark for cross-file code completion. Preprint, arXiv:2310.11248. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021. Program synthesis with large language models. ArXiv, abs/2108.07732. Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, Ben He, Shanshan Jiang, and Bin Dong. 2023. Chatgpt is knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models. arXiv preprint arXiv:2303.16421. Egor Bogomolov, Aleksandra Eliseeva, Timur Galimzyanov, Evgeniy Glukhov, Anton Shapkin, Maria Tigina, Yaroslav Golubev, Alexander Kovrigin, Arie van Deursen, Maliheh Izadi, and Timofey Bryksin. 2024. Long code arena: set of benchmarks for longcontext code models. Preprint, arXiv:2406.11612. Max Brunsfeld, Andrew Hlynskyi, Amaan Qureshi, Patrick Thomson, Josh Vera, Phil Turnbull, dundargoc, Timothy Clem, ObserverOfTime, Douglas Creager, Andrew Helwer, Rob Rix, Daumantas Kavolis, Hendrik van Antwerpen, Michael Davis, Ika, TuãnAnh Nguyên, Amin Yahyaabadi, Stafford Brunk, Matt Massicotte, Niranjan Hasabnis, bfredl, Mingkai Dong, Samuel Moelius, Steven Kalt, Will Lillis, Kolja, Vladimir Panteleev, and Jonathan Arnett. 2024. tree-sitter/tree-sitter: v0.22.6. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz X. Du, M. Liu, K. Wang, H. Wang, J. Liu, Y. Chen, J. Feng, C. Sha, X. Peng, and Y. Lou. 2024. Evaluating large language models in class-level code generation. In 2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE), pages 982 994, Los Alamitos, CA, USA. IEEE Computer Society. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, et al. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. 2024. Deepseek-coder: When the large language model meets programming the rise of code intelligence. Preprint, arXiv:2401.14196. Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. 2021. Measuring coding challenge competence with apps. NeurIPS. Soneya Binta Hossain, Nan Jiang, Qiang Zhou, Xiaopeng Li, Wen-Hao Chiang, Yingjun Lyu, Hoan Nguyen, and Omer Tripp. 2024. deep dive into large language models for automated bug localization and repair. ArXiv, abs/2404.11595. Nan Jiang, Kevin Liu, Thibaud Lutellier, and Lin Tan. 2023. Impact of code language models on automated program repair. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pages 14301442. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, and Alexey Svyatkovskiy. 2023. Inferfix: End-to-end program repair with llms. In Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, pages 16461656. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Alexander LeClair, Sakib Haque, Lingfei Wu, and Collin McMillan. 2020. Improved code summarization via graph neural network. In Proceedings of the 28th International Conference on Program Comprehension, ICPC 20, page 184195, New York, NY, USA. Association for Computing Machinery. Jia Li, Ge Li, Yongmin Li, and Zhi Jin. 2023a. Enabling programming thinking in large language arXiv preprint models toward code generation. arXiv:2305.06599. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023b. StarPreprint, coder: may the source be with you! arXiv:2305.06161. Yujia Li, David Choi, Junyoung Chung, et al. 2022. Competition-level code generation with alphacode. Science, 378(6624):10921097. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and LINGMING ZHANG. 2023. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. In Advances in Neural Information Processing Systems, volume 36, pages 2155821572. Curran Associates, Inc. Tianyang Liu, Canwen Xu, and Julian McAuley. 2024. Repobench: Benchmarking repository-level code In The Twelfth Internaauto-completion systems. tional Conference on Learning Representations. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada, Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, WenDing Li, Megan Risdal, Jia Li, Jian Zhu, Terry Yue Zhuo, Evgenii Zheltonozhskii, Nii Osae Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang, Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet, Jennifer Robinson, Carolyn Jane Anderson, Nicolas Chapados, Mostofa Patwary, Nima Tajbakhsh, Yacine Jernite, Carlos Muñoz Ferrandis, Lingming Zhang, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2024. Starcoder 2 and the stack v2: The next generation. Preprint, arXiv:2402.19173. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2024. Wizardcoder: Empowering code large language models with evolinstruct. In The Twelfth International Conference on Learning Representations. T.J. McCabe. 1976. complexity measure. IEEE Transactions on Software Engineering, SE-2(4):308 320. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. Codegen: An open large language model for code with multi-turn program synthesis. Preprint, arXiv:2203.13474. Shuyin Ouyang, Jie Zhang, Mark Harman, and Meng Wang. 2023. Llm is like box of chocolates: the nondeterminism of chatgpt in code generation. arXiv preprint arXiv:2308.02828. Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. 2023. Understanding the effectiveness of large language models in code translation. arXiv preprint arXiv:2308.03109. ACM/IEEE 42nd International Conference on Software Engineering, ICSE 20, page 13851397, New York, NY, USA. Association for Computing Machinery. Yakun Zhang, Wenjie Zhang, Dezhi Ran, Qihao Zhu, Chengfeng Dou, Dan Hao, Tao Xie, and Lu Zhang. 2024. Learning-based widget matching for migrating gui test cases. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering, volume 66 of ICSE 24, page 113. ACM. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio Blanco, and Shuai Ma. 2020. Codebleu: method for automatic evaluation of code synthesis. Preprint, arXiv:2009.10297. Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333389. Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised In Adtranslation of programming languages. vances in Neural Information Processing Systems, volume 33, pages 2060120611. Curran Associates, Inc. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2024. Code llama: Open foundation models for code. Preprint, arXiv:2308.12950. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan, and Xiaofei Ma. 2024. Repoformer: Selective retrieval for repository-level code completion. Preprint, arXiv:2403.10059. Chunqiu Steven Xia, Yinlin Deng, and Lingming Zhang. 2024. Top leaderboard ranking= top coding proficiency, always? evoeval: Evolving coding benchmarks via llm. arXiv preprint arXiv:2403.19114. Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated program repair in the era of large pre-trained language models. In 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), pages 14821494. Jinqiu Yang, Alexey Zhikhartsev, Yuefei Liu, and Lin Tan. 2017. Better test cases for better automated In Proceedings of the 2017 11th program repair. Joint Meeting on Foundations of Software Engineering, ESEC/FSE 2017, page 831841, New York, NY, USA. Association for Computing Machinery. Zhen Yang, Fang Liu, Zhongxing Yu, et al. 2024. Exploring and unleashing the power of large language models in automated code translation. Proc. ACM Softw. Eng., 1(FSE). Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun, and Xudong Liu. 2020. Retrieval-based neural source code summarization. In Proceedings of the"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Recall of Retrieval approach Table 7 presents the recall rates of two different retrieval settingssparse and denseapplied across all selected repositories. These repositories include pylint, sphinx, seaborn, flask, sympy, datasets, more-itertools, plotly.py, and astropy. The recall rates reflect performance at both the repository-level and file-level contexts, providing insights into how each setting performs for individual repositories as well as overall. scikit-learn, xarray,"
        },
        {
            "title": "Sparse Dense",
            "content": "astropy datasets flask more-itertools plotly.py pylint scikit-learn seaborn sphinx sympy xarray"
        },
        {
            "title": "Total",
            "content": "0.28 0.25 0.30 0.11 0.07 0.46 0.07 0.24 0.06 0.22 0.12 0.13 0.25 0.33 0.45 0.33 0.42 0.46 0.05 0.26 0.12 0.23 0.29 0.15 Table 7: Recall rate of the top three and top ten retrieved contents across retrieval settings for repository-level and file-level contexts. The recall rates for Dense retrieval are generally higher than those for BM25 retrieval across most repositories, highlighting the effectiveness of the Dense method in retrieving the invoked functions relevant to the target function. There are notable variations between repositories. For instance, more-itertools and plotly.py show significantly higher recall rates with the Dense setting, while scikit-learn demonstrates relatively low recall rates in both settings. The total recall rate is computed using microaveraging, and the result shows that neither retrieval method achieves high recall for the oracle contexts. This indicates the possibility of improving the LLM evaluation result by enhancing the retrieval approach. Figure 7: Prompt Example for KMeans.fit in the file sklearn/cluster/_kmeans.py from REPOCOD. LLM capabilities. Second, GPT4-o is the strongest LLM on REPOCOD, consistently outperforming or matching other models across all repositories. Finally, LLMs exhibit specialization across repositories. For instance, while Claude 3.5 Sonnet and DeepSeekCoder-33B perform similarly overall, Claude excels on pylint, whereas DeepSeekCoder performs better on plotly.py. A.2 Evaluation Result By Repository A.3 Case Studies Table 8 presents detailed LLM performance on REPOCOD, revealing several key insights. First, repositories differ significantly in difficulty. Models perform well on flask, with pass@1 often above 40% , indicating an alignment with Prompt Example. Figure 7 shows an example of REPOCOD prompt, consisting of the system prompt, context information retrieved by BM25, and the target function details, with specific content replaced by .... Setting pylint sympy sphinx seaborn flask more-itertools scikit-learn xarray datasets plotly.py astropy Total CodeLlama-7B CodeLlama-34B DeepSeekCoder-6.7B DeepSeekCoder-33B OpenCodeInterpreter-6.7B OpenCodeInterpreter-33B Claude 3.5 Sonnet DeepSeek-V2.5 GPT-4o-Mini GPT-4o 7.69 0.00 7.69 3.85 3.85 11.54 19.23 15.38 11.54 19.23 7.22 4.12 6.19 7.22 4.12 7.22 11.34 13.40 11.34 15.46 9.09 18.18 15.15 27.27 12.12 21. 15.15 27.27 15.15 27.27 23.08 21.79 26.92 28.21 16.67 20.51 25.64 30.77 25.64 37.18 37.21 32.56 41.86 37.21 41.86 41.86 39.53 41.86 39.53 58.14 13.95 24.42 12.79 11.63 18.60 29. 38.37 36.05 18.60 43.02 5.10 5.73 6.37 7.32 6.37 8.28 6.05 9.87 6.05 13.69 6.02 16.87 16.87 15.66 10.84 13.25 9.64 16.87 10.84 24.10 22.03 18.64 16.95 33.90 16.95 25. 27.12 30.51 33.90 47.46 6.58 14.47 25.00 43.42 21.05 23.68 36.84 35.53 25.00 44.74 5.88 10.59 14.12 16.47 12.94 16.47 10.59 16.47 9.41 23.53 10.41 12.76 14.08 17.14 12.45 16. 17.45 20.71 15.00 27.04 Table 8: Pass@1(%) of LLMs on REPOCOD under dense retrieval, with details shown for each repository. from self-contained to file-level and eventually to repository-level contexts. This trend aligns with the insights discussed in Section 5.2. An interesting finding is that GPT-4-o achieves higher pass@1 rate for instances under repositorylevel context complexity compared to file-level instances. This observation suggests two potential insights: (1) specific retrieval methods, like BM25, may enhance performance on repository-level task instances; (2) higher context complexity does not necessarily result in reduced LLM performance. Passed Example. Figure 8 shows an example of correctly generated solution from REPOCOD generated by GPT4-o under dense retrieval setting. This example passes all test cases and is considered correct. Upon inspection, the generated code snippet is identical to the canonical solution except for the text content in the exception message. Failed Example. Figure 9 shows an example of an incorrectly generated solution by GPT4-o under BM25 retrieval setting. The canonical solution incorporates repository-level context; however, the solution generated by GPT-4o fails to match the functionality of the canonical solution. A.4 Uniqueness and Overlap of Correct"
        },
        {
            "title": "Generations",
            "content": "Commercial LLMs with BM25. Figure 10a presents the UpSet plot comparing the performance of four commercial models using contexts from BM25 retrieval, the largest overlap (more than 40) is shared by all models, indicating significant degree of commonality. GPT-4o has the highest number of unique cases (over 60), suggesting its ability to capture distinct results in this scenario. Commercial LLMs with Current-File. Figure 10b shows the Current File retrieval setting result. The overlap between all models increases greatly compared to the BM25 retrieval (to over 75 cases), demonstrating stronger alignment across models with this retrieval setting. In addition, the unique solvable problem by GPT4-o reduces greatly, to similar level compared to Claude 3.5 Sonnet and DeepSeek-V2.5. A.5 Impact of Retrieval Methods on Pass@1 across Varying Context Complexities Figure 11a and Figure 11b illustrate the pass@1 for instances across various context complexities under different retrieval settings: BM25 and current file. In both settings, LLM performance tends to decrease as context complexity rises, progressing Figure 8: An example of the correct output generated by LLMs, for function false_alarm_probability in the file astropy/timeseries/periodograms/lombscargle/core.py from REPOCOD. Figure 9: An example of the wrong output generated by LLMs, for function transform in the file sklearn/decomposition/_factor_analysis.py from REPOCOD. (a) Result for BM25. (b) Result for Current-File. Figure 10: Correct generation results relationship under different retrieval settings. (a) Pass@1(%) for LLMs based on BM25 retrieval. (b) Pass@1(%) for LLMs based on current file retrieval. Figure 11: Pass@1 under different context complexity."
        }
    ],
    "affiliations": [
        "Purdue University"
    ]
}