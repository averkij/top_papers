{
    "paper_title": "MigrationBench: Repository-Level Code Migration Benchmark from Java 8",
    "authors": [
        "Linbo Liu",
        "Xinle Liu",
        "Qiang Zhou",
        "Lin Chen",
        "Yihan Liu",
        "Hoan Nguyen",
        "Behrooz Omidvar-Tehrani",
        "Xi Shen",
        "Jun Huan",
        "Omer Tripp",
        "Anoop Deoras"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid advancement of powerful large language models (LLMs) in recent years, a wide range of software engineering tasks can now be addressed using LLMs, significantly enhancing productivity and scalability. Numerous benchmark datasets have been developed to evaluate the coding capabilities of these models, while they primarily focus on code generation and issue-resolution tasks. In contrast, we introduce a new coding benchmark MigrationBench with a distinct focus: code migration. MigrationBench aims to serve as a comprehensive benchmark for migration from Java $8$ to the latest long-term support (LTS) versions (Java $17$, $21$), including a full dataset and its subset selected with $5,102$ and $300$ repositories respectively. Selected is a representative subset curated for complexity and difficulty, offering a versatile resource to support research in the field of code migration. Additionally, we provide a comprehensive evaluation framework to facilitate rigorous and standardized assessment of LLMs on this challenging task. We further propose SD-Feedback and demonstrate that LLMs can effectively tackle repository-level code migration to Java $17$. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback achieves $62.33\\%$ and $27.33\\%$ success rate (pass@1) for minimal and maximal migration respectively. The benchmark dataset and source code are available at: https://huggingface.co/collections/AmazonScience/migrationbench-68125452fc21a4564b92b6c3 and https://github.com/amazon-science/MigrationBench respectively."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 2 9 6 5 9 0 . 5 0 5 2 : r MigrationBench: Repository-Level Code Migration Benchmark from Java 8 Linbo Liu1 Xinle Liu1 Qiang Zhou1 Lin Chen1 Yihan Liu1 Hoan Nguyen1 Jun Huan1 Omer Tripp1 Anoop Deoras1 Behrooz Omidvar-Tehrani1 Xi Shen 1AWS AI Labs linbol@amazon.com"
        },
        {
            "title": "Abstract",
            "content": "With the rapid advancement of powerful large language models (LLMs) in recent years, wide range of software engineering tasks can now be addressed using LLMs, significantly enhancing productivity and scalability. Numerous benchmark datasets have been developed to evaluate the coding capabilities of these models, while they primarily focus on code generation and issue-resolution tasks. In contrast, we introduce new coding benchmark MigrationBench with distinct focus: code migration. MigrationBench aims to serve as comprehensive benchmark for migration from Java 8 to the latest long-term support (LTS) versions (Java 17, 21), including full dataset and its subset selected with 5, 102 and 300 repositories respectively. selected is representative subset curated for complexity and difficulty, offering versatile resource to support research in the field of code migration. Additionally, we provide comprehensive evaluation framework to facilitate rigorous and standardized assessment of LLMs on this challenging task. We further propose SD-Feedback and demonstrate that LLMs can effectively tackle repository-level code migration to Java 17. For the selected subset with Claude-3.5-Sonnet-v2, SD-Feedback achieves 62.33% and 27.33% success rate (pass@1) for minimal and maximal migration respectively. The benchmark dataset and source code are available at: https://huggingface.co/collections/AmazonScience and https://github.com/amazon-science/MigrationBench respectively."
        },
        {
            "title": "Introduction",
            "content": "The rapid development of large language models (LLMs) with billions of parameters has led to an unprecedented growth in their size and reasoning capabilities [19, 35, 11]. The software engineering (SWE) industry involves various coding tasks with relatively objective goals [24], requiring strong domain knowledge and reasoning abilities. Consequently, the research community has been actively and effectively leveraging LLMs in this domain [28, 7]. Initially, LLMs were employed for standalone and interview-style code generation tasks [3, 12, 6]. For instance, GitHub Copilot2 assists in completing functions within Integrated Development Environments (IDEs). Subsequently, their applications expanded to more complex use cases, like SWE-Bench [18], which generates pull requests to address open issues for GitHub repositories. As LLMs demonstrated remarkable success in these coding tasks [36, 15, 30, 22], attention has shifted to even more complex challenges in software engineering, such as code migration [26, 4]. Unlike code generation or issue resolution, which often focuses on isolated functions or files, code migration tasks require holistic approach involving the entire repository. Migration entails navigating Equal contribution 2 https://github.com/features/copilot Preprint. multi-step process to address numerous interconnected issues across files, making it far more intricate task [20]. This complexity highlights the need to evaluate LLMs on such advanced use cases, especially considering the near-saturated performance on simpler benchmarks like code generation. However, creating benchmark dataset for migration tasks poses significant challenges, given the breadth and depth of repository-wide changes involved. To fill in this gap, we introduce MigrationBench, the first large-scale benchmark dataset in Java, designed specifically to assess LLM capabilities in code migration. Curating high-quality migration dataset is complex and multifaceted task that demands meticulous attention to various factors. Our approach employs rigorous and scalable multi-step process that takes into account diverse aspects of the repository. We begin by applying license and repository quality filters to ensure compliance and maintain certain level of quality. Subsequently, we employ robust techniques to identify the most appropriate snapshot as the starting point, ensuring the datasets integrity and reliability. Furthermore, we design comprehensive evaluation framework for migration success due to the absence of ground truth. Our approach ensures that both the benchmark datasets and final evaluation are of exceptional quality, providing researchers and practitioners with reliable and comprehensive resource for studying and analyzing code migration patterns. Our major contributions are summarized below: 1. We introduce MigrationBench for repository-level code migration task from Java 8, the most popular programming language (PL) for around 20 years.3 To the best of our knowledge, MigrationBench is the first benchmark dataset in the code migration scope in large scale among all PLs. 2. The MigrationBench dataset has three subsets, hosted at https://huggingface.co/ collections/AmazonScience: full, selected and UTG. full and UTG datasets are complementary to each other, based on whether there are any test cases or test files present. We also curate more representative and challenging subset selected of 300 repositories from full, to accelerate future research on code migration. 3. We propose and discuss an automated and comprehensive evaluation framework, which checks minimal migration requirements, approximately validates functional equivalence (FE) after migration and verifies dependencies upgrading to their latest major versions. 4. We present SD-Feedback to resolve code migration tasks effectively with LLMs, achieving reasonable efficacy (pass@1) by providing various syntactic and semantic feedback. The paper is structured as follows: Section 2 provides an overview of related work in the field, situating our research within the existing body of knowledge. Section 3 details the meticulous process to curate the dataset. In Section 4, we delve into the evaluation metrics. Section 5 introduces our baseline solution SD-Feedback, which leverages the capabilities of LLMs to tackle the challenge of code migration. Section 6 presents the experimental results obtained from applying our LLM-based solution to the curated dataset which serves as the starting point for code migration. We summarize the limitations in Section 7 and finally major findings in Section 8."
        },
        {
            "title": "2 Related Work",
            "content": "LLM Applications in the Software Engineering Industry. In the software engineering (SWE) industry, code maintenance [5] such as platform migration, library version upgrades, code refactoring and bug fixing, is an essential component in daily SWE work, requiring even significantly more SWE hours than developing new features [21]. Therefore, the community has an increasing interest in leveraging LLMs in nearly all fields of SWE tasks [9]. Reflexion [30], Agentless [36] and Agentcoder [15] mainly study the LLMs ability in code generation to solve the interview-style coding problems. ChatUniTest [8] and TESTPILOT [28] explore the feasibility to use LLMs for automated unit test generation (UTG), while Toggle investigates fault localization and program repair [14]. Building on these foundational applications, the introduction of workflows and agents further enhances the utility of LLMs by addressing key limitations like hallucination and reasoning constraints through external feedback mechanisms [13, 15, 37]. Tools such as OpenDevin [33] and Cursor4 excel at resolving complex coding tasks, automating repetitive processes and offering intelligent debugging 3 https://www.tiobe.com/tiobe-index 4 https://www.cursor.com 2 solutions. Similarly, GitHub Copilot5 and Amazon Developer6 stand out for providing realtime code suggestions and enhancing developer productivity, while Replit Ghostwriter7 focuses on collaborative coding and debugging. These innovations showcase how workflows and agents enable LLMs to handle longer, more complex trajectories effectively. Benchmark Datasets: Code Generation, Maintenance and Translation. Benchmark datasets for coding tasks are abundant, but most focus on use cases other than code migration, as shown in 200 coding benchmark datasets covering all phases of the software development life survey of cycle comprehensively [32]. HumanEval [6] is coding benchmark dataset designed to evaluate the correctness of LLMs in generating Python functions based on the provided docstrings. Mostly Basic Programming Problems (MBPP) [3] offers more diverse dataset, assessing LLMs overall problem-solving abilities in real-world coding scenarios with natural language problem descriptions. These datasets have been further enhanced into HumanEval+ and MBPP+ [23] to provide more comprehensive evaluation framework for LLMs. APPS [12] extends the prior work by measuring the models ability to take an arbitrary natural language specification and generate Python code. While the aforementioned benchmarks primarily focus on Python code, MBXP and Multilingual HumanEval [2] extend the evaluation to over 10 PLs, enabling more comprehensive assessment of multilingual code generation capabilities. DevAI [40], benchmark of 55 realistic AI development tasks organized hierarchically as directed acyclic graph (DAG), evaluates agentic systems on their ability to handle diverse aspects of AI development pipelines. In code maintenance, SWE-Bench [17] compiles collection of popular Python repositories to facilitate research on leveraging LLMs for effective Github issue resolution. While SWE-Bench-Java [38] expands to the Java PL with 91 issues, SWE-PolyBench [27] is more comprehensive with 2, 110 issues in multiple PLs, including Java, JavaScript, TypeScript and Python. Notably, while DevAI, SWE-Bench and its extensions include components that address broader development contexts with more advanced and difficult SWE tasks, none of them comprehensively tackles coding problems at the repository level as MigrationBench does. Instead, repository-level research has been primarily focusing on code translation tasks, typically involving few medium-sized repositories. Therefore, it lacks both well-designed comprehensive benchmark dataset and sufficient complexity. While AlphaTrans [16] focuses on translating Java to Python for 10 repositories and RepoTransBench [34] attempts to translate 100 repositories in the opposite direction, neither of them has ground truth target repository available. There are more research on translation to Rust as the target PL, from source PL of Go [39] or [29]. These translations only involve less than 10 repositories and up to 7, 000 lines of code (LOC) for each repository. CodePlan [4] is closer to our work, demonstrating LLMs can resolve C# package migration and Python code edits, with 3 repositories for each use case. However, MigrationBench is large-scale benchmark with more than 5k repositories and robust evaluation framework."
        },
        {
            "title": "3 MigrationBench",
            "content": "MigrationBench is benchmark comprising collection of Github open-source repositories, all written in specific source language version (LS). Given target language version (LT ), the task is to migrate the repository to ensure compatibility with LT with necessary code changes. In this paper, we present its Java version, that is LS = Java 8 and build tool is maven. These Maven-based applications are intended to be migrated to long-term support (LTS) LT such as Java 17 or 21. To facilitate large-scale repository processing, we leverage AWS Elastic Map Reduce (EMR) Serverless, running under Amazon Linux 2 and maven 3.9.6, with 4 vCPUs and 16G memory for both the driver and workers. 3.1 Problem Formulation Notations. Let denote the content of code repository including dependency declaration files (e.g. pom.xml), source code files, test files, auxiliary files (e.g. README, graphs, tables), etc. is usually associated with PL version (e.g. Java 8, 17, 21) and can be built, tested or verified 5 https://github.com/features/copilot 6 https://aws.amazon.com/q/developer 7 https://replit.com/learn/intro-to-ghostwriter 3 by collection of verifiers v. For example, Maven Java repository can be built and tested by = mvn clean verify. We define status function (R, L, v) that maps code repository R, its associated language and verifiers to boolean value, representing whether the verifiers pass or not, i.e. (R, L, v) = True or False. Let RS and RT denote the code repository before and after the migration respectively. As MigrationBench does not aim at bug fixing, we require that the original code repository RS is in passing state before the migration under source language LS, i.e. (RS, LS, v) = True. Given the target language LT , we formulate the migration problem as to find transformation from RS to RT such that (RT , LT , v) = True. Note that the ending states RT subject to (RT , LT , v) = True are usually not unique. In this paper we specifically discuss the scenario where LS and LT are Java 8 and 17 respectively, and mvn clean verify is one component of the verifiers v. Note that some repositories may have hard-coded compiler versions in the dependency files (pom.xml), then they require seed changes (seed) [4] when migrating repository. For example, valid seed change can replace any hard-coded Java 8 with 17 in pom.xml files."
        },
        {
            "title": "3.2 Data Collection",
            "content": "We introduce the data collection process which produces the MigrationBench dataset. We scrape all Java repositories hosted in Github and apply multiple filters sequentially detailed in Appendix to ensure high dataset quality. Fi, 1 { } Stage 1: Initial Repository Candidates. This stage includes applying filters F1 to F3. We start by collecting Java repositories on Github with MIT or Apache 2.0 licenses (F1), with reasonable quality by requiring at least 3 stars (F2) and with build tool being Maven (F3). This results in 16, 154 repositories as of March 2024. Stage 2: Search for LS-compatible Base Commit ID (Hb). This is the most time-consuming step for the data collection pipeline (F4). Given repository, we need to find out the right base commit compatible with LS, as the latest commit doesnt necessarily qualify. Figure 1 demonstrates the flowchart on how to collect and filter repositories up to F4, to find out the right base commit id (Hb) for repository. This serves as the start point for the code migration problem. Stage 3: Dedup and Partition into full and UTG datasets. This includes applying F5 and F6. We remove duplicate repositories based on their snapshots at Hb (F5), and further partition them into two subsets based on presence of test cases (F6). In the end, we collect 5, 102 and 4, 814 repositories in migration-bench-full (full) and migration-bench-utg (UTG) subsets respectively, summing up to 9916 repositories in total. 3.2.1 migration-bench-selected (selected): Representative and Challenging Subset While we collect thousands of open-source repositories from Github to cover the entire problem space, the whole dataset is not always the best benchmark for evaluating LLMs migration capabilities. significant portion of the repositories may share similar issues, whereas benchmark dataset should be diverse enough to reflect range of challenges encountered in real-world migrations. Additionally, running LLM-assisted migration on all 5k repositories is prohibitively timeand resource-intensive. To address these limitations, we introduce migration-bench-selected (selected), subset of 300 repositories carefully curated from full based on objective repository statistics - placing greater emphasis on larger and multi-module repositories - and through human expert selection. By prioritizing repositories that present more complex migration challenges, selected is designed to better capture the diversity and intricacies of real-world code migration scenarios. 3.3 Dataset Statistics We report statistics for both MigrationBenchs full and selected subsets in Table 1. On average, repositories in the full subset have 2.4 modules, 7, 698.3 cumulative LOC, 65.1 java files, 8.7 test files in the src/test directory and 34.8 test cases. For the selected subset, metrics average, median and standard deviation values are typically larger than the full superset. Figures 3 and 4 further compare metrics histogram and cumulative distribution function for full and selected. Rs Latest Snapshot S0 Pass F{0,1,2}? Java version = vi { } Previous Commit {pom.xml} vi 1.8 and vi > 1.8 vs 1.8? vi < 1.8 = or vi = 1.8 Previous Commit Rs Snapshot S1 vi 1.8 and vi > 1.8 Timeout? Java version = vi { } vs 1.8? vi < 1.8 = or vi = 1.8 Pass mvn cmd? Compiled version = ui { } = or ui > 52 ui < 52 vs 52? ui = 52 Keep (R, Hb) Reject Figure 1: The flowchart to find out the base commit id Hb for code migration, starting with Github repository Rs latest snapshot as of March 2024. It assumes both the explicitly hard coded java versions and complied class major versions are monotonic. Given the commit history of based on all pom.xml files, one can infer the first candidate snapshot S1. Hb is the commit id of the last S1 where java version passes version check, mvn command succeeds and compiled class major version passes version check."
        },
        {
            "title": "4 Evaluation",
            "content": "Measuring the success of typical code generation or SWE benchmarking tasks is usually well-defined, as either the ground truth solution is available or its straightforward to conduct an evaluation with comprehensive tests focusing on limited scope e.g. with only function level or file level changes. Thus, they have very limited room for variations though the solutions are not necessarily unique. However, none of them applies for coding problems at the repository level. Code migration evaluation presents unique challenges in that necessary code changes to LT typically scale with repository size, which may vary significantly depending on the application types. Approximate FE: Invariants. The most rigorous way to evaluate success is to verify FE before and after migration, which is challenging by nature due to the huge problem and solution space. Instead, we adopt few key metrics as approximations below, primarily replying on mvn clean verify or compile command and few invairants as the measure of migration success, to automate the final evaluation and enable comparison across various migration efforts. 4.1 Minimal Migration Maven Command Success (r1). For successful migration to Java 17, the minimum requirement is that the application builds successfully and passes all existing unit and integration tests when 5 Table 1: MigrationBench statistics at base commit Hb for both the full and selected subset. The avg rows top two maximal values are in bold and underlined respectively. Other than the #Repositories metric, all other metrics are computed on per-repository basis. We partition the whole selected dataset (all) into success and failure subsets, based on Claude-3.5-Sonnet-v2s maximal migration result. It counts all *.java files in the src/test/ directory only. It counts test cases by executing mvn test -f . in the root directory. Metric"
        },
        {
            "title": "Statistics",
            "content": "full.0 #Repositories - #Modules LOC #Java Files #Test Files #Test Cases avg std median max avg std median max avg std median max avg std median max avg std median max selected success.0 82.0 failure.0 218.0 3.6 - - - 7.8 - - - all. 300.0 6.6 15.1 1.0 192.0 5, 102.0 2.4 5.8 1.0 192.0 7, 698.3 35, 937.3 2, 044.5 1, 773, 940.0 22, 397.2 108, 203.9 7, 351.5 1, 773, 940. 9, 748.5 - - - 27, 155.0 - - - 65.1 218.6 25.0 10, 970.0 8.7 26.6 2.0 815.0 34.8 289.9 2.0 14, 450.0 168.4 650.7 82.0 10, 970. 8.8 21.8 1.0 194.0 110.6 854.1 8.5 14, 450.0 87.3 - - - 11.2 - - - 48.9 - - - 198.9 - - - 7.9 - - - 133.9 - - - running on Java 17, verified by the mvn clean verify command. It performs series of steps: it cleans cached files, compiles the source code, runs unit tests, packages the application, executes integration tests and validates the entire build process. When it completes without any build or test errors, the build is considered successful. Compiled Class Major Version Validation (r2). To ensure that the compiler is genuinely utilizing Java 17, we implement guardrail to verify the compiled classes major versions (under */target/classes/**/*.class) are indeed 61.8 Invariance: List of Test Methods (r3). To make sure test methods (annotated with @Test) are not renamed or even removed after code migration, we compare test classes and annotated test methods based on abstract syntax tree (AST) parsing for all files in the dedicated src/test directory before and after migration, and the migration is considered unsuccessful if the test methods dont match. Invariance: Number of Test Cases (r4). Static analysis of test directory in r3 doesnt guarantee runtime behaviors, therefore we further require number of test cases to be non-decreasing, primarily to avoid disabling those tests in pom.xml files hence effectively treating them as plain text files. When migrated repository satisfies requirements and the corresponding efficacy (pass@1) is: { r1, r2, r3, r4 , we define it as minimal migration } ηminimal = # migrated repositories passing {r1, r2, r3, r4} # total repositories 8 https://mkyong.com/java/list-of-java-class-file-major-version-numbers/"
        },
        {
            "title": "4.2 Maximal Migration",
            "content": "Dependencies Latest Major Versions (r5). One key advantage of upgrading Java is to leverage the dependency libraries latest major versions for efficiency and security reasons. For instance, the latest Spring Boot version 3.3.x is incompatible with Java 8, requiring at least Java 17 since 3.0.9 Therefore, to ensure modern and robust migration, we require using all dependencies stable and latest major versions available on Maven Central as of November 2024. We specify list of version requirements for the most frequent 240 dependencies occurring in the selected subset, and see Appendix C.1 for the top 50. Similar to minimal migration, we define maximal migration and the corresponding efficacy is: ηmaximal = # migrated repositories passing {r1, r2, r3, r4, r5} # total repositories ηminimal and (ii) For maximal migration, we incorporate one Note that (i) By definition ηmaximal extra step in seed to upgrade libraries to their latest major version before migration, and add r5 to the final evaluation to avoid any downgrade after migration. Compilation Efficacy. We also define ηcompile minimal and ηcompile with 1, weaker command mvn clean compile, to address compilation errors only. maximal, by dropping and replacing r1 r3, r4 { } Complementary to Sections 4.1 and 4.2, there could be even more specifications on migration tasks, and an incomplete list includes more comprehensive FE check through enhancing test coverage in Appendix C.2, adopting modernized syntax and new security features in LT , etc."
        },
        {
            "title": "5 SD-Feedback",
            "content": "Motivated by the self-debugging work [7], we introduce SD-Feedback for code migration and illustrate its flowchart in Figure 5 in Appendix D.1. Given list of build errors or test errors, SD-Feedback leverages LLMs strong coding and reasoning abilities, to resolve errors sequentially and provide feedback as specific as possible when not going into the right direction. To be more specific, we provide syntactic feedback when LLMs are unable to generate patch , or semantic feedback when is valid but doesnt change the errors. Appendix has more details on abstracting the implementation, inputs to LLMs, interacting with LLMs and providing feedback respectively. To improve task efficacy, Self-Refine [25] takes different approach relying on LLMs to generate iterative feedback and refinement across broader scope, including code optimization, mathematical reasoning, sentiment reversal and more. In contrast, SD-Feedback takes rule-based approach which leverages various offline verifications and domain knowledge in an empirical way, gets tailored specifically for the code migration task and offers more precise and context-specific feedback."
        },
        {
            "title": "6 Experiments",
            "content": "We run multiple experiments to collect efficacy numbers, as reference for future research for full and selected. (i) Baseline efficacy: We apply the OpenRewrite recipe directly, org.openrewrite.java.migrate.UpgradeToJava17,11 without seed and (ii) SD-Feedback efficacy: We also conduct experiments with two LLMs publicly available in AWS Bedrock: Llama-3.1-70B-Instruct (Llama) [10] and Claude-3.5-Sonnet-v2 (Claude) [1]. For both the selected and sampled full subsets, we collect efficacy numbers with mvn clean verify. Moreover, for the selected subset we also compare directly with Claudes efficacy based on mvn clean compile, to highlight the importance of unit tests in code migration. 6.1 Results Efficacy. In Table 2, we report minimal and maximal migration efficacy, with cutoff of 80 LLM API calls (equivalent to 80 iterations in SD-Feedback) for generative model. We also plot efficacy 9 https://docs.spring.io/spring-boot/docs/3.0.0/reference/html/getting-started. html 10 https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle 11 https://docs.openrewrite.org/recipes/java/migrate/upgradetojava 7 Table 2: Minimal and maximal migration efficacy for migration-bench-selected."
        },
        {
            "title": "Method",
            "content": "OpenRewrite (Baseline) Llama-3.1-70B-Instruct Claude-3.5-Sonnet-v2 Migration Efficacy (%) ηminimal 24.33 30.67 62.33 ηmaximal 7.33 15.67 27.33 dependence on the number of LLM API calls in Figure 2, emphasizing that efficacy is not static number, and it depends on inference-time compute. See [31] for more discussion on the scaling law at inference time. Comparative Analysis and Case Study. To explore the difficulty and challenge of the problem, we conduct comparative analysis for migrated repositories, separate repositories based on Claudes maximal migration status and compare their statistics in Table 1. In Appendix we also show few successful migration examples by Claude, to highlight the code migration quality by LLMs. Figure 2: Minimal and maximal migration efficacy comparison (with either mvn clean verify or compile), the migration-bench-selected dataset using Claude-3.5-Sonnet-v2. as function of number of SD-Feedback iterations, for 6.2 Discussion 1. Minimal and Maximal Migration Efficacy. Table 2 and Figure 2 show clearly persistent and non-trivial gap between ηminimal and ηmaximal. For Claude model with 80 SD-Feedback iterations, ηminimal is 2.3 for Llama model under the same settings. It implies that though minimal migration is relatively straightforward, maximal migration is much more challenging, since dependency packages may have to go through major version upgrades and are likely to introduce deprecated or breaking APIs with the same dependency, or even deprecated or completely renamed of ηmaximal, while its 2. 8 dependencies. At the same time, it provides an alternative path to maximal migration: How to leverage minimal migration as an intermediate step for maximal migration? The idea is to break down difficult problem into two stages, minimal JDK upgrade followed by series of incremental package upgrades, so that maximal migration is more manageable and intuitive. sequential migration strategy provides greater observability and flexible step-wise control, to incorporate human intervention and ultimately enhance the efficiency for full maximal migration. Comparative analysis based on the Claude model is shown in Table 1, demonstrating that maximal migration is more challenging for repositories with more modules, LOC, java files and tests. 2. Compilation Efficacy. Recall that ηcompile maximal are the minimal and maximal migration efficacy using mvn clean compile command in requirement 1. When resolving compilation errors only with 80 iterations and skipping all tests, the task become significantly easier: Claude model achieves 91.00% and 52.33% for minimal and maximal migration respectively. Therefore it highlights best practice of increasing test coverage through UTG before code migration, ensuring additional validation and strengthening functional equivalence for the migration. minimal and ηcompile 3. Comparing full and selected Efficacy. Table 4 in Appendix shows efficacy for 188 repositories sampled from the full subset. ηmaximal for the whole full subset is estimated to be 66.49%, which is 2.4 of the efficacy for the selected subset. This observation substantiates that the curated selected subset exhibits greater complexity as evidenced by the static statistics in Table 1, and presents increased difficulty as reflected in both the minimal and maximal migration efficacy. As such, we believe the selected subset will encourage more future research in code migration."
        },
        {
            "title": "7 Limitations",
            "content": "We summarize few limitations of the current paper below. Test Augmentation and Coverage. We dont enforce test coverage and fully reply on existing test cases. Although we do require the existence of test cases or dedicated test directory at Hb with source LS, for lines without any coverage mavens verify command is equivalent to compile only, and there is less guarantee on functional equivalence. Fortunately, the UTG subset can fill in the gap, serving as large scale benchmark for unit test generation. Code Migration Quality. Though Section 4 and Appendix propose few requirements for the final evaluation, they might be still insufficient to ensure high code migration quality. Furthermore, differentiating good versus great migrations is non-trivial, in that both evaluating and empowering LLMs to write modernized and idiomatic code in LT is strongly desired yet challenging."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduce MigrationBench, comprehensive code migration benchmark dataset from Java 8 to subsequent LTS versions like Java 17 and 21, with both full dataset and its selected subset containing 5, 102 and 300 repositories respectively, reflecting real-world code migration scenarios. UTG is complementary to full, secondary benchmark dataset with 4, 814 repositories for UTG and code migration purposes. We design comprehensive evaluation framework to approximate functional equivalence and rigorously assess migration success. This framework encompasses key criteria, including maven validations, verification of compiled class major versions, consistency in the list of test methods, non-decrease in the number of test cases and adherence to optional dependency version requirements (particularly relevant in maximal migration scenarios). We demonstrate the feasibility of code migration from Java 8 to 17 through deterministic workflow with SD-Feedback, and show preliminary results with promising efficacy for both minimal (62.33%) and maximal (27.33%) migration for the selected subset with Claude-3.5-Sonnet-v2. We envision MigrationBench as valuable resource for LLM practitioners and researchers, fostering further exploration and innovation in coding tasks."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude 3 model card, 2024. URL https://www-cdn.anthropic. com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf. Accessed: 2025-01-23. [2] Ben Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan, Wasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, et al. Multi-lingual evaluation of code generation models. arXiv preprint arXiv:2210.14868, 2022. [3] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. [4] Ramakrishna Bairi, Atharv Sonwane, Aditya Kanade, Arun Iyer, Suresh Parthasarathy, Sriram Rajamani, Ashok, and Shashank Shet. Codeplan: Repository-level coding using llms and planning. Proceedings of the ACM on Software Engineering, 1(FSE):675698, 2024. [5] Gerardo Canfora and Aniello Cimitile. Software maintenance. In Handbook of Software Engineering and Knowledge Engineering: Volume I: Fundamentals, pages 91120. World Scientific, 2001. [6] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https: //arxiv.org/abs/2107.03374. [7] Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023. [8] Yinghao Chen, Zehao Hu, Chen Zhi, Junxiao Han, Shuiguang Deng, and Jianwei Yin. Chatunitest: framework for llm-based test generation, 2024. URL https://arxiv.org/abs/ 2305.04764. [9] Hao Ding, Ziwei Fan, Ingo Guehring, Gaurav Gupta, Wooseok Ha, Jun Huan, Linbo Liu, Behrooz Omidvar-Tehrani, Shiqi Wang, and Hao Zhou. Reasoning and planning with large language models in code development. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 64806490, 2024. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [11] Deep Ganguli, Danny Hernandez, Liane Lovitt, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, et al. Predictability and surprise in large generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 17471764, 2022. [12] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, et al. Measuring coding challenge competence with apps. arXiv preprint arXiv:2105.09938, 2021. [13] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 2023. [14] Soneya Binta Hossain, Nan Jiang, Qiang Zhou, Xiaopeng Li, Wen-Hao Chiang, Yingjun Lyu, Hoan Nguyen, and Omer Tripp. deep dive into large language models for automated bug localization and repair. Proc. ACM Softw. Eng., 1(FSE), July 2024. doi: 10.1145/3660773. URL https://doi.org/10.1145/3660773. [15] Dong Huang, Qingwen Bu, Jie Zhang, Michael Luck, and Heming Cui. Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. arXiv preprint arXiv:2312.13010, 2023. [16] Ali Reza Ibrahimzada, Kaiyao Ke, Mrigank Pawagi, Muhammad Salman Abid, Rangeet Pan, Saurabh Sinha, and Reyhaneh Jabbarvand. Repository-level compositional code translation and validation, 2024. URL https://arxiv.org/abs/2410.24117. [17] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024. URL https://arxiv.org/abs/2310.06770. [18] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=VTF8yNQM66. [19] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. [20] K. Kontogiannis, J. Martin, K. Wong, R. Gregory, H. Müller, and J. Mylopoulos. Code migration through transformations: an experience report. In CASCON First Decade High Impact Papers, CASCON 10, page 201213, USA, 2010. IBM Corp. doi: 10.1145/1925805.1925817. URL https://doi.org/10.1145/1925805.1925817. [21] Bennet Lientz, E. Burton Swanson, and Gail Tompkins. Characteristics of application software maintenance. Communications of the ACM, 21(6):466471, 1978. [22] Vijay Lingam, Sujay Sanghavi, Sayan Ghosh, Linbo Liu, Jun Huan, Anoop Deoras, Gaurav Gupta, and Behrooz Omidvar-Tehrani. Enhancing language model agents using diversity of thoughts. The Thirteenth International Conference on Learning Representations, 2025. [23] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36, 2024. [24] Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, and Yiling Lou. Large language model-based agents for software engineering: survey. arXiv preprint arXiv:2409.02977, 2024. [25] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=S37hOerQLB. [26] Stoyan Nikolov, Daniele Codecasa, Anna Sjovall, Maxim Tabachnyk, Satish Chandra, Siddharth Taneja, and Celal Ziftci. How is google using ai for internal code migrations? arXiv preprint arXiv:2501.06972, 2025. [27] Muhammad Shihab Rashid, Christian Bock, Yuan Zhuang, Alexander Buchholz, Tim Esler, Simon Valentin, Luca Franceschi, Martin Wistuba, Prabhu Teja Sivaprasad, Woo Jung Kim, Anoop Deoras, Giovanni Zappella, and Laurent Callot. Swe-polybench: multi-language benchmark for repository level evaluation of coding agents, 2025. URL https://arxiv.org/ abs/2504.08703. 11 [28] Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip. An empirical evaluation of using large language models for automated unit test generation. IEEE Transactions on Software Engineering, 2023. [29] Manish Shetty, Naman Jain, Adwait Godbole, Sanjit A. Seshia, and Koushik Sen. Syzygy: Dual code-test to (safe) rust translation using llms and dynamic analysis, 2024. URL https: //arxiv.org/abs/2412.14234. [30] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36, 2024. [31] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv. org/abs/2408.03314. [32] Kaixin Wang, Tianlin Li, Xiaoyu Zhang, Chong Wang, Weisong Sun, Yang Liu, and Bin Shi. Software development life cycle perspective: survey of benchmarks for code large language models and agents, 2025. URL https://arxiv.org/abs/2505.05283. [33] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. OpenHands: An Open Platform for AI Software Developers as Generalist Agents, 2024. URL https://arxiv.org/abs/2407. 16741. [34] Yanli Wang, Yanlin Wang, Suiquan Wang, Daya Guo, Jiachi Chen, John Grundy, Xilin Liu, Yuchi Ma, Mingzhi Mao, Hongyu Zhang, and Zibin Zheng. Repotransbench: real-world benchmark for repository-level code translation, 2024. URL https://arxiv.org/abs/2412. 17744. [35] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. [36] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489, 2024. [37] Luke Yoffe, Alfonso Amayuelas, and William Yang Wang. Debunc: mitigating hallucinations in large language model agent communication with uncertainty estimations. arXiv preprint arXiv:2407.06426, 2024. [38] Daoguang Zan, Zhirong Huang, Ailun Yu, Shaoxin Lin, Yifan Shi, Wei Liu, Dong Chen, Zongshuai Qi, Hao Yu, Lei Yu, Dezhi Ran, Muhan Zeng, Bo Shen, Pan Bian, Guangtai Liang, Bei Guan, Pengjie Huang, Tao Xie, Yongji Wang, and Qianxiang Wang. Swe-bench-java: github issue resolving benchmark for java, 2024. URL https://arxiv.org/abs/2408. 14354. [39] Hanliang Zhang, Cristina David, Meng Wang, Brandon Paulsen, and Daniel Kroening. Scalable, validated code translation of entire projects using large language models, 2024. URL https: //arxiv.org/abs/2412.08035. [40] Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, et al. Agentas-a-judge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934, 2024."
        },
        {
            "title": "A Data Collection",
            "content": "We show filter details for MigrationBench dataset collection, to augment Section 3.2. F1. Licenses. We collect Github repositories under the MIT or Apache 2.0 licenses only. F2. Enforce Repository Quality through Github Stars. To minimize the inclusion of potentially low-quality repositories, we require repositories to have at least 3 stars. F3. Build Tool is maven. For Java projects, Apache Maven, Gradle and legacy Apache Ant are all widely used build automation tools to manage dependencies, compile packages and run tests. In this paper, we limit the scope to maven, which is not only the most popular option for Java 8 but also represents mature and well-documented build tool. As part of our criteria, repositorys latest snapshot must pass with the mvn clean verify command using at least one of Java 8, 11 or 17 without any modifications. This ensures that each selected repository is in stable and passing state, so that its reasonable to assume the existence of valid migration path for those repositories. By applying filtering up to F3, we narrow down to 16, 154 repositories as of March 2024. All subsequent filters are conducted at the time of writing in 2025. Disclaimer. We encounter significant maven throttling when executing high number of concurrent jobs, leading to potential incompleteness in the 16k list. 1. Despite this, we consider false positives to be more detrimental than false negatives, thus the 16k list serves as sufficiently robust and large-scale starting point for benchmark datasets like MigrationBench. 2. Moreover, while false negatives may persist up to stage F3, we have adopted best-effort runtime strategies in subsequent stages, F4 through F6, to mitigate the throttling issue. F4. Search for LS-compatible Base Commit ID (Hb). We check that the Github url remains valid, and then search for the last commit id, i.e. base commit Hb, which is compatible under source language LS for each repository. LS-compatibility for repositorys given snapshot is defined as below: 1. It has at least one pom.xml file. 2. It has valid source versions, i.e. each pom.xml file in repository has no hard-coded Java versions or the hard-coded version is Java 8, defined either through maven.compiler.source, maven.compiler.target and maven.compiler.release fields in properties, or through artifact maven-compiler-plugins source, target and release configurations. 3. Its able to pass mvn clean verify with Java 8 and 4. We further require the major versions of the final compiled classes, exported to the target/classes/ or customized output directory, are 52. This class validation makes sure the effective pom.xml files are indeed Java 8. Appendix A.1 covers more details on the assumptions and approximations for faithful collection. In the end, it reduces to 9, 934 repositories with valid Hb and at least one *.java file present. F5. Dedup Repositories by Exact Match. There are numerous duplicate repositories on Github and F5 aims at removing them. Given snapshot for repository, we compute its snapshot id for the whole repository at base commit Hb, by hashing the tree structure of the repositorys files, each *.java files hash and both filenames (relative to root) and raw content for each pom.xml file, concatenated by new lines. De-duplicating based on results in 9, 916 repositories and theyll be used as the starting point for MigrationBench. F6. Partition into full and UTG Subsets. As discussed in Section 6.2, the migration efficacy can be significantly different when including unit tests. Therefore, we further require repositories to have at least one test case to ensure at least partial functional equivalence after migration. We identify unit tests based on: 1. Whether there are any *.java files present in the dedicated test directory src/test/ or 13 2. Whether there are any test cases triggered by mvn test -f . command in the root directory. We further partition these repositories into full dataset containing 5, 102 repositories with unit tests, and UTG subset containing the remaining 4, 814 repositories without any unit tests. Although this work primarily discusses full due to its direct relevance to migration evaluation, the UTG subset can be of independent interest and serve as benchmark for unit test generation. Similar to F4 in Appendix A.1, due to reproducibility issues we run F6 twice and keep repositories in the full subset when and only when both runs report presence of unit tests. A.1 F4: Search for Base Commit ID Hb When we go through the repositorys commit history in the reverse order up to March 2024, we assume both Java versions and compiled class major versions are chronologically monotonic, which holds for most repositories. For initial hard coded java versions, while larger than Java 8, we fully leverage commit history for pom.xml files only to accelerate identifying the last commit with Java 8, instead of going through commit history for the whole repository. Whenever we encounter either hard coded Java versions or complied classes major versions are less than the specification before finding valid base commit, we terminate the search and exclude the repository from the benchmark dataset. When it takes too long to run mvn clean verify or repository is in broken state with too many commits, and were unable to find valid base commit within 15 minutes, we also exclude it from MigrationBench. This is the best effort to keep the process to be more reproducible, take less compute time and preserve as many repositories as possible. Due to the importance of base commit id Hb, for the whole MigrationBench dataset including both the full and UTG subsets, we re-run the process twice and only keep the ones reporting the same Hb across the two runs. Though we may miss repositories due to inconsistency introduced by e.g. maven throttling issues and runtime cutoff, it provides stronger guarantee on both correctness and reproducibility. Benchmark Dataset Statistics: full and selected Comparison In addition to dataset statistics shown in Table 1, in Figures 3 and 4 we compare the discrete probability distribution function (PDF) and cumulative distribution function (CDF) respectively, for repositories in MigrationBenchs full and selected subsets: 1. Number of modules: It shows number of pom.xml files for each repository. 2. LOC: It computes the cumulative LOC for all *.java files for each repository. 3. Number of all or test-only Java files: It counts all or test-only (in the src/test directory) *.java files in repository. 4. Number of test cases: It parses number of test cases based on mvn test -f . commands output at the root directory, valid when non-negative, while negative number means there are some issues running the tests or parsing the output, likely due to maven throttling issues though we try to mitigate it with multiple efforts."
        },
        {
            "title": "C Evaluation",
            "content": "This section provides more details and extends Section 4. C.1 Maximal Migration Table 3 shows an incomplete list (top 50) of the most popular dependencies and their latest versions, sorted by frequency showing up in the selected subset. 14 Figure 3: Discrete probability density function comparison for number of modules, LOC, all or test Java files and test cases for repositories in MigrationBenchs full and selected subsets. C.2 Unit Test Coverage Unit testing plays pivotal role in ensuring functional equivalence during code migration, as it provides robust mechanism to verify that the migrated code performs identically to its original counterpart, independent of whether they contain bugs or not. By isolating individual components of the code and testing them against predefined inputs and expected outputs, unit tests captures discrepancies introduced during migration, such as logic errors, incompatibilities or unintended side effects. This granular validation process builds confidence that the repositorys new state in LT retains the intended functionality as LS and facilitates early detection and resolution of inconsistency issues. Passing existing unit tests is already ensured in requirement r1 by executing mvn clean verify. However, it is strongly recommended to generate additional unit tests to enhance test coverage, especially when the existing coverage is very low. SD-Feedback As discussed briefly in Section 5, SD-Feedback is motivated by the original self-debugging work [7], and our primary enhancement is the feedback module, which evaluates whether proposed patch makes positive progress towards the final migration goal. When and only when positive progress is detected with the suggested patch , SD-Feedback accepts and proceeds to the next iteration. Otherwise, it rejects . 15 Figure 4: Cumulative distribution function comparison for number of modules, LOC, all or test-only Java files and test cases (dropping invalid negative values) for repositories in MigrationBenchs full and selected subsets. D.1 Flowchart for SD-Feedback Figure 5 is the flowchart to leverage LLMs for code migration. D.2 Workflow: Language Agnostic We emphasize that the workflow is mostly language agnostic, and the main difference across languages is three-fold: 1. The source and target languages LS and LT . 2. The command in the verifiers to emit errors and 3. The utility functions to extract list of errors based on the stdout and/ or stderr outputs. We further point out that there are typically two types of errors revealed by the build commands: build errors and test errors. Though build errors are typically more straightforward to deal with, with the exact filename, line number and even column number, there might be hidden build errors due to dependency graph across different files and/ or modules. In contrast, test errors are more tricky and difficult to tackle in that 1. It fully depends on the testing framework(s) associated with the language, and their failure content may vary. 2. simple logging level change may suppress or surface test failure details and 3. Itll be further complicated by user logging messages, decreasing signal to noise ratio for both human developers and LLMs. Both types of errors are often intermingled in the build log, which is typically lengthy and includes unrelated information. To address this challenge, we developed dedicated log parsers for build and test errors. These parsers streamline the logs by extracting only the critical lines that reveal the error Table 3: The top 50 most popular dependencies and their latest versions as of November 2024."
        },
        {
            "title": "Dependency",
            "content": "org.springframework.boot:spring-boot-starter-parent junit:junit org.springframework.boot:spring-boot-starter-test org.springframework.boot:spring-boot-starter-web org.junit.jupiter:junit-jupiter-engine org.junit.jupiter:junit-jupiter-api com.fasterxml.jackson.core:jackson-databind org.slf4j:slf4j-api org.assertj:assertj-core org.mockito:mockito-core org.apache.commons:commons-lang3 org.junit.jupiter:junit-jupiter commons-io:commons-io com.google.guava:guava org.springframework.boot:spring-boot-starter javax.servlet:javax.servlet-api ch.qos.logback:logback-classic org.springframework.boot:spring-boot-starter-actuator org.junit.jupiter:junit-jupiter-params org.springframework.boot:spring-boot-configuration-processor com.google.code.gson:gson org.springframework.boot:spring-boot-starter-data-jpa mysql:mysql-connector-java com.h2database:h2 org.springframework.boot:spring-boot-devtools org.springframework.boot:spring-boot-starter-security org.springframework.boot:spring-boot-starter-thymeleaf org.apache.httpcomponents:httpclient com.fasterxml.jackson.core:jackson-core com.fasterxml.jackson.core:jackson-annotations org.springframework.security:spring-security-test org.slf4j:slf4j-simple org.apache.logging.log4j:log4j-api commons-codec:commons-codec org.apache.logging.log4j:log4j-core org.apache.shenyu:shenyu-plugin-base com.puppycrawl.tools:checkstyle org.springframework:spring-test org.junit:junit-bom org.springframework:spring-context org.apache.shenyu:shenyu-common org.hsqldb:hsqldb org.hamcrest:hamcrest com.alibaba:fastjson org.springframework:spring-web com.squareup.okhttp3:okhttp org.junit.platform:junit-platform-surefire-provider org.junit.vintage:junit-vintage-engine org.apache.curator:curator-recipes javax.xml.bind:jaxb-api"
        },
        {
            "title": "Version",
            "content": "3.3.4 4.13.2 3.3.4 3.3.4 5.11.0 5.11.0 2.17.2 2.0.16 3.26.3 5.13.0 3.17.0 5.11.0 2.17.0 23.0 3.3.4 4.0.1 1.5.8 3.3.4 5.11.0 3.3.4 2.11.0 3.3.4 8.0.33 2.3.232 3.3.4 3.3.4 3.3.4 4.5.14 2.17.2 2.17.2 6.3.3 2.0.16 2.24.0 1.17.1 2.24.0 2.6.1 10.18.1 6.1.13 5.11.0 6.1.13 2.6.1 2.7.3 3.0 2.0.53 6.1.13 4.12.0 1.3.2 5.11.0 5.7.0 2.3.1 messages. By reducing the logs context length, the parsers not only enable LLMs to focus on the most relevant information but also lower LLMs token usage and improve efficiency. 17 Repository Code Migration Errors Y SUCCESS FAILURE LLM Count = 0? Max iteration? Prompt LLM Response Pass sanity check? Valid patch Positive progress? N: Feedbackinst. follow N: Feedbacknon-positive progress Reject Accept Figure 5: SD-Feedback flowchart to resolve code migration errors, potentially with chain of feedback to LLMs. The reported success might be inconsistent with that in the final evaluation, which may include extra checks as shown in Section 4. D.3 Inputs to LLMs We design template prompt for LLM to perform code migration task with multiple model families, including Llama-3.1-70B-Instruct and Claude-3.5-Sonnet-v2. Simply based on whether the prompts are error-specific or not, we separate them into system prompts (error-agnostic) and user prompts (error-specific). 1. (system) An explicit role setting of Java software engineer expert working on code migration task, from Java 8 to the specified target Java version. 2. (system) constraints section summarizing our requirements and instructions for LLMs to follow, including inputs at the high level, relevant definitions and formats for the outputs, etc. 3. (user) Relevant project file (pom.xml) and optional relevant source code file (*.java). 4. (user) Details of the errors, including the exact build error or test error if available and 5. (user) Optional code snippet section to highlight where the build error is, when there is valid line number specified in the extracted error messages. D.3.1 System Prompt In Figure 6 we show SD-Feedbacks system prompt which is error agnostic. 18 Figure 6: System prompt used for repository-level code migration, where $REQUIREMENTS is shown in Figure 7 separately due to page length limit. D.3.2 User Prompt In contrast with system prompt, user prompt is error specific and its template is shown in Figure 8. 19 Figure 7: The requirements section of the system prompt in Figure 6. D.4 Reasoning: Interaction with LLMs The key of SD-Feedback is how to interact with LLMs in multiple dimensions, and we roughly categorize them into syntactic feedback and semantic feedback. Syntactic feedback comes in when LLM is unable to follow specified format, and were unable to generate valid patch based on the LLM response. For example, due to hallucination it may attempt to edit file that doesnt exist, modify code snippet that is absent in the given file or even modify code snippet without any changes. Without syntactic issues, one is able to parse LLM response as valid code patches, and then semantic feedback plays crucial role to improve code migration efficacy. The bottleneck is to figure out whether its making positive progress and why when it is not, by comparing errors right before and right after the suggested patch, i.e. Epre vs Epost. We adopt simple heuristic on this evaluation, when and only when Epre and Epost are different, we accept the suggested patch. 20 Figure 8: User prompt template used for repository-level code migration, where project_path and file_path are the full path to root directorys pom.xml and corresponding *.java with the error compile_error respectively. FILE__*s are to load the content of corresponding files, while line_number and column_number are the line number and column number where the error is from. code_snippet is to show the snippet covering line_numberth line and its neighbors, especially useful when the file contains many lines and its difficult for LLMs to figure out which line is problematic. Evaluation on migration-bench-full As reported in Table 1, migration-bench-full dataset contains 5, 102 repositories and running SD-Feedback on the whole dataset is not computationally feasible. To produce an estimation of the migration efficacy on the full dataset, we sample another subset from full with 188 repositories instead. Different from selected, we first sort the repositories by their Github URL in lower case alphabetically and sample repositories roughly evenly. In Table 4, we report migration-bench-full dataset with the same LLMs as Table 2. the minimal and maximal migration efficacy on the sampled Table 4: Minimal and maximal migration efficacy for 188 sampled repositories from the migration-bench-full dataset. Method Llama-3.1-70B-Instruct Claude-3.5-Sonnet-v2 Migration Efficacy (%) ηminimal 44.68 84.04 ηmaximal 38.83 66.49 21 Case Study: Successful Migration Examples We show two successful migration examples below to demonstrate LLMs capability on resolving the code migration problem at the repository level, as supplemental material to Section 6: 1. xmpp-light: From migration-bench-utg 2. dust: From migration-bench-selected F.1 xmpp-light We study https://github.com/tumakha/xmpp-light upgrading spring-boot to latest v3.3 to demonstrate functional equivalence. Error 1. cannot find symbol symbol: method formLogin((login)->l[...]All()). This is because spring-boots v3.3 uses spring Security 6.x framework, changes were introduced: the following 1. authorizeRequests( 2. The form login config should not be nested inside the authorization config 3. Each security config aspect (authorization, form login, logout, remember-me) should be ) is deprecated in favor of authorizeHttpRequests( ) configured separately. SD-Feedback proposes the following change to resolve it: 1 2 3 4 - 5 - 6 - 7 - 8 - 9 - 10 + 11 + 12 + 13 + 14 + 15 + 16 + 17 + 18 + 19 + 20 + 21 + 22 + 23 ec urit yFil te rC ha in filterChain ( HttpSecurity http ) throws Exception { http . csrf ( withDefaults () ) ; http . authorizeRequests ( requests -> requests . formLogin ( login -> login // Compilation error is in this line . loginPage ( \" / login \" ) . permitAll () ) . logout ( logout -> logout . deleteCookies ( \" remember - me \" ) . permitAll () ) . rememberMe ( withDefaults () ) ) ; . th oriz eHttp Reques ts ( requests -> requests . requestMatchers ( \" / login \" ) . permitAll () . anyRequest () . authenticated () ) . formLogin ( login -> login . loginPage ( \" / login \" ) . permitAll () ) . logout ( logout -> logout . deleteCookies ( \" remember - me \" ) . permitAll () ) . rememberMe ( withDefaults () ) ; return http . build () ; Error 2. method does not override or implement method from supertype. This is because the ErrorController interface in newer versions of Spring Boot has been simplified and no longer contains the getErrorPath() method. SD-Feedback suggests removing the @Override annotation correctly: 22 1 @@ -1 ,4 +1 ,3 @@ 2 - @Override public String getErrorPath () { return ERROR_PATH ; 4 5 } F.2 dust We perform case study for https://github.com/LevelFourAB/dust below. Error 1. <anonymous se.l4.dust.validation.ValidationModule$1> is not abstract and does not override abstract method releaseInstance(javax.validation.ConstraintValidator<?,?>) in javax.validation.ConstraintValidatorFactory. This is because in Java 8 the interface doesnt require releaseInstance(...), however, in newer versions (required for Java 17 support) this method is mandatory. SD-Feedback proposes the following change and the error disappears: 1 @@ -62 ,6 +62 ,12 @@ public class ValidationModule 2 { 3 4 5 + 6 + 7 + 8 + 9 + 10 + 11 12 13 return injector . getInstance ( key ) ; } @Override public void releaseInstance ( ConstraintValidator <? , ? > instance ) { // No explicit cleanup needed as Guice handles instance lifecycle } }) ; return config ;"
        }
    ],
    "affiliations": [
        "AWS AI Labs"
    ]
}