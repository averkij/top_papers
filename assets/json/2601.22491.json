{
    "paper_title": "SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization",
    "authors": [
        "Jinyang Wu",
        "Changpeng Yang",
        "Yuhao Shen",
        "Fangzhi Xu",
        "Bolin Ni",
        "Chonghua Liao",
        "Yuchen Liu",
        "Hongzhen Wang",
        "Shuai Nie",
        "Shuai Zhang",
        "Haoran Luo",
        "Jiaming Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards has emerged as a powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the ``sweet spot'' concept in tennis-the racket's core region that produces optimal hitting effects, we introduce \\textbf{S}weet \\textbf{S}pot \\textbf{L}earning (\\textbf{SSL}), a novel framework that provides differentiated guidance for agent optimization. SSL follows a simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweet-spot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/long-term planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5X sample efficiency gains and effective cross-task transferability. Our work establishes SSL as a general principle for training capable and robust agents."
        },
        {
            "title": "Start",
            "content": "SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Jinyang Wu 1 * Changpeng Yang 2 * Yuhao Shen 3 Fangzhi Xu 4 Bolin Ni 5 Chonghua Liao 1 Yuchen Liu 2 Hongzhen Wang 2 Shuai Nie 2 Shuai Zhang 1 Haoran Luo 4 Jiaming Xu"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning with verifiable rewards has emerged as powerful paradigm for training intelligent agents. However, existing methods typically employ binary rewards that fail to capture quality differences among trajectories achieving identical outcomes, thereby overlooking potential diversity within the solution space. Inspired by the sweet spot concept in tennis-the rackets core region that produces optimal hitting effects, we introduce Sweet Spot Learning (SSL), novel framework that provides differentiated guidance for agent optimization. SSL follows simple yet effective principle: progressively amplified, tiered rewards guide policies toward the sweetspot region of the solution space. This principle naturally adapts across diverse tasks: visual perception tasks leverage distance-tiered modeling to reward proximity, while complex reasoning tasks reward incremental progress toward promising solutions. We theoretically demonstrate that SSL preserves optimal solution ordering and enhances the gradient signal-to-noise ratio, thereby fostering more directed optimization. Extensive experiments across GUI perception, short/longterm planning, and complex reasoning tasks show consistent improvements over strong baselines on 12 benchmarks, achieving up to 2.5 sample efficiency gains and effective cross-task transferability. Our work establishes SSL as general principle for training capable and robust agents. 6 2 0 2 0 3 ] . [ 1 1 9 4 2 2 . 1 0 6 2 : r 1. Introduction Reinforcement learning with verifiable rewards (RLVR) has emerged as transformative paradigm for training intelli1Tsinghua University 2Xiaomi Corporation 3Zhejiang University 4Nanyang Technological University 5Institute of Automation, Chinese Academy of Sciences. Correspondence to: Jinyang Wu <wu-jy23@mails.tsinghua.edu.cn>, Changpeng Yang <yangchangpeng@xiaomi.com>. Preprint. February 2, 2026. 1 Figure 1. Performance comparison with Binary (RL-B.) and Continuous (RL-C.) Reward RL. SSL (our method) exhibits remarkable improvements across diverse tasks, including long- /short-term planning and complex reasoning. gent multimodal agents with sophisticated reasoning and planning capabilities (Jaech et al., 2024; Guo et al., 2025; Team et al., 2025; Nguyen et al., 2025; Jiang et al., 2025). Unlike supervised fine-tuning (SFT) that typically depends on costly human-annotated reasoning chains, RLVR directly leverages automatically computable reward signals to optimize agent behaviors. This approach enables agents to autonomously develop complex cognitive abilities, including chain-of-thought reasoning (Wei et al., 2022), problem decomposition (Zhou et al., 2022), self-correction (Gandhi et al., 2025), and multi-step planning (Luo et al., 2025a). However, mainstream RLVR typically adopts binary rewards, grouping all trajectories into success or failure and obscuring finer distinctions among them. Consider two GUI navigation trajectories that both successfully open application settings: one reaches the target in three actions, while another succeeds after eight redundant stepsyet both receive the same reward. Similarly, in maze navigation, multiple paths may reach the goal with vastly different characteristics: some take circuitous detours, while others follow direct corridors. These examples illustrate how SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Optimization Guidance: tiered, proximity-aligned reward signals provide stronger directional feedback during policy updates, enabling more stable and effective optimization; (ii) Enhanced Efficiency: SSL matches or surpasses GRPO using only 40% of its samples, achieving up to 2.5 dataefficiency; (iii) Transferability & Superior Performance: SSL consistently improves performance across diverse tasks, demonstrating strong transferability. Our contributions are: We propose sweet spot learning, unified reward principle that integrates sweet-spot modeling into reinforcement learning. SSL offers tiered, quality-sensitive rewards with theoretical guarantees on preserving optimal solution ordering and improving gradient optimization. We instantiate SSL across diverse agent tasks: distancetiered rewards for visual perception and navigation, and progress-tiered rewards for complex reasoning. This demonstrates SSLs broad applicability. We conduct extensive experiments on 12 benchmarks spanning GUI perception, short-/long-term planning, and complex reasoning, showing consistent gains over strong baselines, enhanced efficiency, and transferability. 2. Related Work RLVR for Agent Optimization. RLVR has become key paradigm for training capable agents by leveraging automatically computable success criteria (Guo et al., 2025; Team et al., 2025; Wu et al., 2025a). Recent work typically applies on-policy algorithms like GRPO with binary terminal rewards across tasks such as math reasoning (Wang et al., 2025), GUI navigation (Nguyen et al., 2025; Zhang et al., 2024), and puzzle solving (Inc., 2025). However, binary rewards treat all successful trajectories equally, ignoring solution quality-e.g., three-step GUI path and an eightstep detour receive the same reward. This leads to sample inefficiency and policy fragility (overfitting to incidental patterns) (Sutton et al., 1998; Xu et al., 2025; Amodei et al., 2016). SSL addresses these issues by providing differentiated guidance through quality-ordered zone partitioning. Reward Shaping in RLVR. Reward shaping techniques augment sparse terminal signals with detailed feedback (Ng et al., 1999; Grzes, 2017; Hu et al., 2020). Classical potential-based methods rely on handcrafted Markovian potentials (Sutton et al., 1998), while modern approaches employ distance rewards for navigation (Ivanitskiy et al., 2023), milestone rewards for reasoning (Luo et al., 2025b), and hindsight relabeling for sparse rewards (Velu et al., 2023). Yet, these methods often require considerable taskspecific designs or lack theoretical guarantees (Ibrahim et al., 2024; Zou et al., 2019). In contrast, SSL introduces unified, task-agnostic principle: partitioning solution space into Figure 2. Sweet Spot Learning Overview. We visualize sweetspot zones and their instantiation across diverse tasks . coarse, undifferentiated rewards induces three challenges: (i) Optimization Ambiguity: without differentiated guidance, gradient updates may lack directional information to indicate which task-advancing behaviors merit reinforcement or discouragement (Ng et al., 1999; Baheri, 2023); (ii) Learning Inefficiency: given reward signals fail to reveal solution-quality differences, agents cannot effectively extract fruitful knowledge from diverse trajectories, leading to under-exploration of solution space and poor sample utilization (Eschmann, 2021); (iii) Policy Fragility: policies may overfit to incidental patterns (e.g., fortunate click) rather than robust task understanding (Velu et al., 2023). Can we design unified reward principle that provides differentiated guidance across the full solution space? Inspired by the sweet spot in tennis (Cross, 1998)the rackets core region that produces optimal hitting impact, we propose Sweet Spot Learning (SSL), versatile reward modeling scheme for training intelligent agents. The key insight is simple yet powerful: progressively amplified, tiered rewards guide policies toward the sweet-spot region of solution space. Rather than relying on coarse success-failure split, SSL assigns hierarchical rewards to trajectories based on trajectory proximity to task completion. As shown in Figure 2, this principle adapts naturally across tasks: for GUI perception and navigation, progress manifests as spatial proximity to targets, quantified via distance-tiered rewards; for complex reasoning (e.g., maze navigation), rewards reflect incremental alignment with ground-truth solutions, inducing progress-tiered structure. Through quality-conditioned reward shaping, SSL provides differentiated guidance that steer exploration toward high-quality solutions. Theoretically, we demonstrate that SSL preserves the relative quality ordering over the solution space and improves gradient signal-to-noise ratios by providing more informative feedback for policy updates. Unlike sparse binary rewards that offer only success-failure supervision, SSL delivers granular feedback that accelerates convergence and improves sample efficiency. Empirically, as shown in Figure 1, we validate SSLs generality and effectiveness across GUI perception, short/long-term planning, and complex reasoning, demonstrating consistent improvements over representative baselines. SSL offers three key advantages: (i) Clear 2 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Figure 3. Flowchart of SSL. SSL computes step proximities, aggregates them into trajectory-level scores, discretizes them into predefined sweet-spot zones, and finally applies the resulting structured rewards to downstream optimization tasks. quality-ordered zones and assigning tiered rewards based on proximity to optimality. This enables generalizable, efficient learning with preservation of optimal solution ordering. discretization. This value measures how closely trajectory approaches high-quality completion. Combining this with binary verification yields the SSL reward: 3. Methodology 3.1. Problem Formulation We formulate agent training as Markov Decision Process = (S, A, , R), where denotes the state space, represents the action space, : (S) defines the state transition function, is the reward function. trajectory τ = (s0, a0, . . . , sT , aT ) is generated by policy πθ(as), and task-specific verifier C(τ ) {0, 1} evaluates terminal success (e.g., whether goals are achieved for planning tasks). Standard RLVR maximizes expected return under binary rewards: Rbin(τ ) = C(τ ), Jbin(θ) = Eτ πθ [Rbin(τ )]. (1) 3.2. Sweet Spot Learning Core Principle: Actions closer to the tasks sweet-spot deserve higher rewards, but tiered. Inspired by the tennis sweet spot-where strikes nearer to the rackets optimal zone yield stronger effects, we propose Sweet Spot Learning, which assigns progressively higher rewards to trajectories that approach task-optimal regions. The key insight is to partition the solution space into hierarchical zones based on proximity to optimality, delivering directional guidance toward high-quality solutions. We present SSL framework in Figure 3 and pseudo-code in Algorithm 1. RSSL(τ ) = C(τ ) + α ˆS(τ ), α > 0, (2) where C(τ ) {0, 1} encodes trajectory correctness, ˆS(τ ) guides learning toward superior solution regions, and α controls the strength of this guidance. Predefined Components. SSL relies on two components that form the basis for computing trajectory sweet-spot values, which are instantiated concretely in Section 3.3. Sweet-spot zones. SSL begins by defining finite set of ordered sweet-spot zones {Zk, sk}K k=1, where each interval Zk specifies proximity range and sk is the sweet-spot value assigned to the region. Concretely, we parameterize zones by boundaries 0 = bK < < b1 < b0 = 1 and set Zk = [bk1, bk). Step proximity. SSL decomposes trajectory into step-level units for fine-grained assessment of action contributions. For each step (st, at), SSL computes proximity score h(st, at) [0, 1], which measures how well the action taken at state st aligns with desirable task behavior. Trajectory-Level Aggregation. Having established steplevel proximity, SSL aggregates these local evaluations into unified trajectory-level proximity measure: S(τ ) = 1 + (cid:88) t=0 h(st, at). (3) Sweet Spot Learning Reward. We first introduce discretized sweet-spot value ˆS(τ ) [0, 1], where the hat means This aggregation captures how well the entire trajectory progresses toward desirable solution regions. By bridging 3 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization local and global granularities, SSL reveals gradual improvement trends that single-step signals cannot capture. This trajectory-level proximity then serves as input to subsequent discretization step that produces tiered sweet-spot values. Trajectory-Proximity Score Discretization. The trajectory proximity S(τ ) is then mapped into tiered sweet-spot value via predefined zone-based discretization. Using the boundary 0 = bK < < b1 < b0 = 1 that define Zk = [bk1, bk), we obtain the discretized sweet-spot value (cid:98)S(τ ) = (cid:88) k=1 sk 1[bk1 S(τ ) < bk] . (4) While the mapping itself is simple, its effectiveness stems from the fact that the predefined zones (Zk) are constructed to reflect task-level structure. Mapping S(τ ) into these zones suppresses small and noisy score fluctuations, yielding more discriminative reward tiers that provide clearer learning signals for agent optimization. Policy Optimization. Given SSL rewards {RSSL(τi)}N i=1 for sampled trajectories, we update the policy model following the standard RLVR paradigm. SSL integrates seamlessly with mainstream methods such as GRPO (Guo et al., 2025) by simply replacing binary rewards with tiered SSL rewards. This differentiated feedback yields more stable gradient estimates and higher sample efficiency than binary signals. 3.3. Agent Instantiations under SSL We show SSL principles flexibility by applying it to two broad agent classes: GUI agents that reason over visual screens to select and execute grounded actions, and complex reasoning agents that operate on structured grids (mazes, Sudoku, ARC patterns). Despite substantial differences in semantics and action spaces, SSL adapts naturally through task-aligned zone design and proximity scoring. 1 GUI Agents: Distance-Tiered Zones. In visual grounding, the agent outputs 2D point = (x, y) for target UI element. Given ground-truth bounding box = (x1, y1, x2, y2) and gaussian spatial fields are commonly used to model human pointing behavior and motor uncertainty (MacKenzie, 1992), we compute continuous proximity ϕ(p; B) (0, 1] via Gaussian field centered at using normalized Mahalanobis distance. Points outside receive ϕ(p; B) = 0, treating them as incorrect. This aligns with the common-correctness setting, where correctness is prioritized before sweet-spot refinement. We parameterize the Gaussian width σ proportional to the bounding box to ensure that proximity judgments adapt to target scale. We then instantiate sweet-spot tiers {Zk, sk}K k=1 by selecting thresholds 0 = bK < < b1 < b0 = 1. Each bk corresponds to the boundary value of σ-level band of the Algorithm 1 Sweet Spot Learning (SSL) Require: Policy πθ; proximity function h(s, a) [0, 1]; t=0 do k=1; coefficient α > 0 sweet-spot zones {Zk, sk}K 1: for each training iteration do Sample trajectory set {τi}N i=1 πθ 2: for each trajectory τi = {(st, at)}Ti 3: C(τi) Verifier(τi) {0, 1} 4: // Trajectory-level aggregation 5: ht h(st, at) for = 0, . . . , Ti 6: (cid:80)Ti S(τi) 1 t=0 ht 7: // Trajectory-Proximity Score Discretization 8: Find zone index such that S(τi) Zk 9: (cid:98)S(τi) sk 10: // SSL reward computation 11: RSSL(τi) C(τi) + α (cid:98)S(τi) 12: 13: 14: 15: end for end for Update θ using policy gradient with {RSSL(τi)}N Ti+1 i=1 Gaussian field inscribed within B, discretizing the continuous decay into nested sweet-spot regions. trajectory falls into tier Zk if ϕ(p; B) (bk, bk1], (cid:98)S(τ ) = sk = bk, (5) where Z1 denotes predictions closest to the box center and ZK denotes the outermost valid region. Predictions outside are assigned (cid:98)S(τ ) = 0. Extension to GUI Planning. While sweet-spot discretization applies directly to grounding, it yields substantial downstream gains in GUI planning performance. The planner conditions on interaction history to decide the next operation; high-level action types (e.g., click, type) are evaluated via binary correctness C(τ ), while spatial components receive graded sweet-spot feedback. We observe that refining grounding alone produces cleaner supervision signals that transfer effectively to the overall planning objective and leads to notable gains in end-to-end task success. This suggests that grounding quality is principal bottleneck in GUI planning, and localized sweet-spot refinement is sufficient to lift higher-level decision-making. 2 Complex Reasoning Agents: Blockwise Sweet-Spot Construction. For complex reasoning tasks like maze navigation, Sudoku completion, and ARC-AGI-style pattern induction, we leverage their shared grid structure to develop unified sweet-spot construction. Despite differing semantics and domains (pathfinding for maze, constraint satisfaction for Sudoku, symbolic induction for ARC-AGI), all permit localized partial-correctness evaluation through blockwise comparison. Unified Blockwise Formulation. For maze navigation, both predicted and reference paths are represented as binary oc4 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization cupancy grids Mpred, Mref {0, 1}HW . We partition the maze into 3 3 grid of blocks = {(i, j) i, {1, 2, 3}} and count per-block matched cells ni,j between prediction and reference. Each block is assigned local sweet-spot value based on match count: high (79 matches, si,j = 1), medium (46 matches, si,j = 2/3), or low (03 matches, si,j = 1/3). The trajectory-level sweet-spot score aggregates block values: S(τ ) = 1 (cid:88) si,j. (i,j)B (6) This blockwise construction applies directly to other tasks like Sudoku and ARC-AGI without modification. In Sudoku, ni,j measures digit agreement within each 3 3 subgrid, so partial correctness contributes to S(τ ) even when the global configuration violates row/column constraints. In ARCAGI, ni,j quantifies symbolic agreement between predicted and target patterns within each block, rewarding recovery of local structure before the full transformation rule is inferred. While this construction aggregates block-level scores rather than computing S(τ ) directly from trajectory-level features (Eq. (3)), it embodies the same sweet-spot principle: local consistency with optimal solutions is quantified and aggregated into trajectory-level feedback. This provides meaningful guidance under partial solutions and offers higherresolution supervision, demonstrating SSLs flexibility as general framework rather than rigid one. 3.4. Theoretical Analysis We establish the theoretical analysis of SSL, demonstrating how its tiered reward structure enables finer policy discrimination and improves optimization efficiency. We define the success rate of policy πθ (denoted as π for simplicity) as SR(π) = Eτ π[C(τ )] and the expected sweet-spot score as µS(π) = Eτ π[S(τ )]. From Eq. (2), the expected return under SSL decomposes as JSSL(π) = SR(π) + α µS(π). (7) This decomposition reveals that SSL simultaneously optimizes for task success and solution-space proximity, with α controlling the relative emphasis between these objectives. Proposition 3.1 (Quality Ordering under Equal Success Rate. Proof in Appendix A). For two policies with identical success rates SR(π1) = SR(π2), we have JSSL(π1) > JSSL(π2) µS(π1) > µS(π2). Gradient Signal-to-Noise Ratio Enhancement. Beyond improving policy discrimination, SSL also enhances optimization efficiency by reshaping the advantage distribution in the policy gradient estimator. Following GRPO (Guo et al., 2025), we write (cid:98)J ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Ai gi. (8) (cid:80)N Here Ai := R(τi) and gi := θ log πθ(τi), where = 1 i=1 R(τi). Thus, optimization efficiency is governed by how the advantages {Ai} weight and select trajectory gradients {gi}. Binary rewards yield low-resolution advantage signal: after subtracting the batch mean, Ai takes only two discrete values, so many trajectories receive identical weights, limiting fine-grained credit assignment. In contrast, raw continuous proximity scores offer high-resolution, but often encode nuisance variations weakly aligned with useful gradient directions, increasing gradient variance. SSL introduces an advantage obtained by centering the sweet-spot score. This advantage is continuous yet selective: it preserves fine-grained ordering among informative trajectories while suppressing uninformative regions, achieving better balance between resolution and variance. Consequently, SSL increases the effective information density of the policy gradient and improves its signal-to-noise ratio. Proposition 3.2 (Projected SNR Improvement. Proof in Appendix A). Let be the unit direction of the binary gradient and ℓ(τ ) = θ log πθ(τ ). If the sweet-spot score satisfies the positive alignment condition then where Cov(cid:0)S(τ ), ℓ(τ )u(cid:1) 0, SNRSSL(u) SNRbin(u), SNR(u) = (cid:12) E[ (cid:98)J u] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:113) Var[ (cid:98)J u] . Intuition. The alignment condition states that higher sweetspot scores correlate with gradient directions that improve policy performance. When this natural condition holds, SSLs differentiated feedback amplifies meaningful gradient components while suppressing noise, yielding higher SNR and more stable, sample-efficient optimization. Intuition. Binary rewards assign identical value to policies with the same success rate and thus fail to distinguish trajectory quality. Sweet-spot scores recover this ordering by rewarding proximity to high-quality solutions, enabling finer policy discrimination. Sweet-Spot Shaping vs. Dense Rewards. Dense rewards impose monotonic objective that uniformly encourages proximity minimization. In contrast, SSL is not designed to optimize distance itself, but to selectively concentrate learning signals within an informative region of the solution 5 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Table 1. Performance on short-term planning tasks. * indicates SFT on Mix-3K. All RL experiments are based on standard RLVR paradigm and trained on Mix-3K by default. denotes relative improvements over RL-Binary baseline ( SSL RL-Binary 1). Models GUI-Act-Web OmniAct-Web OmniAct-Desktop AndroidControl-Low Avg. Type GR SR Type GR SR Type GR SR Type GR SR Zero Shot GPT-4o QwenVL2.5-3B QwenVL2.5-7B 77.09 56.10 86.59 45.02 64.28 84. 41.84 55.61 78.63 79.33 50.63 79.15 42.79 46.89 71.32 34.06 47.02 71.21 79.97 56.95 84.74 63.25 47.97 79. 50.67 46.89 79.66 74.33 62.03 83.44 38.67 74.07 87.08 28.39 59.32 62.50 54.62 55.65 79.05 Post-Training (Supervised Fine-Tuning or Reinforcement Learning) Os-Atlas-4B Os-Atlas-7B QwenVL2.5-3B* QwenVL2.5-7B* UI-R1-3B GUI-R1-3B GUI-R1-7B 79.22 86.95 76.95 87.66 75.89 89.86 90.85 RL-Continous-3B 86.38 82.84 RL-Binary-3B SSL-3B (Ours) 85.77 (, %) +3.5 RL-Continous-7B 94.15 90.78 RL-Binary-7B 94.54 SSL-7B (Ours) (, %) +4.1 58.57 75.61 66.34 84.77 79.43 87.42 88.06 85.49 83.19 87.52 +5. 89.25 88.06 89.25 +1.4 42.62 57.02 61.69 79.89 67.31 76.31 80.31 72.69 75.15 85.22 +13.4 86.46 80.31 87.23 +8.6 46.74 85.63 66.24 81.62 75.42 88.58 91.16 78.92 75.98 81.95 +7. 93.73 91.16 95.39 +4.6 49.24 69.35 56.91 73.45 61.35 75.10 77.29 72.56 68.85 73.45 +6.7 76.78 75.69 77.73 +2.7 22.99 59.15 53.02 73.39 61.33 75.08 77.35 72.21 68.60 73.39 +7. 76.78 75.32 77.43 +2.8 63.30 90.24 77.62 86.23 73.41 91.86 92.20 89.83 79.97 91.53 +14.5 91.69 91.52 92.38 +0.9 42.55 62.87 62.54 80.17 64.12 78.37 83.36 78.30 72.20 80.17 +11. 81.35 81.35 83.39 +2.5 26.94 56.73 63.76 79.80 63.98 78.31 83.33 78.30 72.20 80.17 +11.0 82.04 81.35 83.39 +2.5 64.58 73.00 71.08 84.00 79.15 83.68 85.17 82.93 81.78 83.57 +2. 83.12 81.87 85.17 +4.0 71.19 73.37 74.53 85.74 82.41 81.59 84.02 84.08 80.93 84.63 +4.6 83.34 81.59 87.08 +6.7 40.62 50.94 58.79 64.32 66.44 64.41 66.52 68.28 65.77 81.50 +23. 65.81 64.75 70.72 +9.2 50.71 70.07 65.79 80.09 70.85 80.88 83.30 79.16 75.62 82.41 +9.0 83.70 81.97 85.31 +4.1 space, referred to as the sweet-spot region. By suppressing trivial and uninformative areas and emphasizing this region, SSL acts as selective shaping mechanism rather than dense reward formulation. 4. Experiments 4.1. Experimental Setup Baselines. For GUI tasks, we compare to: (i) RLbinary, GRPO-based RLVR with binary rewards; (ii) RLcontinuous, continuous reward-based RLVR from GUIG2 (Tang et al., 2025); and (iii) other baselines like GUIR1 (Luo et al., 2025a). For complex reasoning, we compare with RL-binary and RL-continuous (block-wise relatively continuous reward). Training Datasets. We use the GUI training data (Mix3K) from GUI-R1-3K (Luo et al., 2025a) and obtain 3K examples spanning both web and mobile interfaces. For generalization analysis, we extract single-step perception data from this collection, yielding approximately 2K samples (Perception-2K). For other tasks (e.g., maze), we use the standard training splits from their respective datasets. Evaluation Benchmarks. We evaluate four tasks across (1) short-term planning: GUItwelve benchmarks: Act-Web (Chen et al., 2024), OmniAct-Web (Kapoor et al., 2024), OmniAct-Desktop (Kapoor et al., 2024), (2) long-term AndroidControl-Low (Li et al., 2024); planning: AndroidControl-High (Li et al., 2024), GUIOdyssey (Lu et al., 2025); (3) fine-grained perception: Screenspot (Cheng et al., 2024), Screenspot-Pro (Li et al., 2025); and (4) complex reasoning: Sudoku Solving (Inc., 2025), Maze Navigation (Ivanitskiy et al., 2023), ARC-AGI1 (Chollet, 2019), and ARC-AGI-2 (Chollet et al., 2025). Evaluation Metrics. For GUI planning tasks, we follow OS-Atlas (Wu et al., 2025b) and report action-type accuracy (Type), grounding accuracy (GR), and step success rate (SR). Type measures exact matches of predicted action types (e.g., click, scroll); GR evaluates GUI element grounding; SR computes step-wise success where both the action and its arguments (e.g., coordinates, input text) must be correct. For other tasks, we report accuracy as the primary metric. Implementation Details. For SFT, we use QwenVL2.53B/7B (Bai et al., 2025) with LLaMAFactory (Zheng et al., 2024), training for one epoch. RL experiments adopt EasyR1 (Zheng et al., 2025) for over three epochs with α = 0.2 (SSL). We apply the same zero-shot inference prompts across all methods for fair comparison. 4.2. Performance Enhancement We use GUI-based tasks as primary benchmarks due to their multimodal nature, coupling language understanding, finegrained visual grounding, and sequential decision making. 6 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Table 2. Performance on complex reasoning tasks. We evaluate SSL on Sudoku, Maze navigation, and ARC-AGI using Qwen2.53/7B. denotes relative improvement over RL-Binary (B/A-1). Method Sudoku Maze ARC-1 ARC-2 Avg. RL-Continuous-3B RL-Binary-3B SSL-3B (Ours) (, %) RL-Continuous-7B RL-Binary-7B SSL-7B (Ours) (, %) 17.3 15.5 31.0 +100.0 45.0 44.7 45.4 +7. 62.3 55.8 72.5 +30.1 85.9 85.7 86.6 +1.0 30.9 28.3 33.3 +17.7 53.7 52.8 58.2 +10.0 17.9 14.9 20.3 +36.1 38.4 39.0 40.3 +3. 32.1 28.6 40.0 +39.8 55.7 55.3 57.2 +3.4 Figure 4. Performance on long-term planning tasks. We report Type, GR, SR (%) across two challenging benchmarks. Figure 5. Performance on GUI perception tasks. We report accuracy across different interface platforms like Development and Programming, Creative, Office and Operating System. This provides rigorous testbed for SSLs robustness and adaptability. Our goal is to expose key phenomena and establish general design principle for RLVR-based agent training, rather than to chase absolute state-of-the-art. GUI Agent Planning. Tables 1 and Figure 4 present our main results on agent planning tasks, where SSL shows substantial and consistent improvements across both shortterm and long-term scenarios. We have three key findings: SSL steers policies toward the sweet-spot region of solution space. On short-term planning, SSL-3B attains an 82.41% average accuracy, exceeding RL-Binary-3B by 9.0%. On long-term planning, SSL-3B reaches 57.11%, 14.6% gain. Unlike binary rewards that provide undifferentiated guidance or continuous shaping that introduces gradient noise, SSLs zone-tiered guidance offers stable feedback progressively refining policies toward optimal solutions. It particularly benefits long-horizon scenarios (e.g., +37.4% SR relative gains on AndroidControl-High). Detailed longterm planning results are provided in the Appendix. SSL particularly strengthens spatial grounding. The most pronounced improvements mainly appear in GR and SR: e.g., +11.0% GR/SR on OmniAct-Desktop and +23.9% SR on AndroidControl-Low (3B). Since SR requires both correct action types and precise spatial parameters, these gains reveal that distance-tiered zones alleviate the grounding bottleneck, enabling better task success. 7 Figure 6. Sample efficiency analysis. We report average success rate (%) across four short-term planning benchmarks. SSL scales across model sizes and planning horizons. SSL-7B achieves 85.31% (short-term) and 56.92% (longterm) average accuracy, improving over RL-Binary-7B by +4.1% and +3.6%, respectively. Benefits hold across model scales, platforms, and varying horizons, supporting SSL as general principle for agent optimization. Fine-Grained Perception. Figure 5 further evaluates the models ability to precisely localize UI elements across different interface platforms (e.g., Dev, OS). SSL consistently outperforms the baselines on ScreenSpot-Pro. For example, SSL-3B achieves +5.7% gains on Office type, with similar improvements on the 7B model. These results indicate that zone-tiered guidance largely benefits precise spatial alignment and multimodal representations, which also explains why SSL yields strong performance in complex planning tasks. Detailed grounding results are provided in Appendix. Complex Reasoning. Table 2 reports results on challenging reasoning tasks, including Sudoku, Maze Navigation, and ARC-AGI. For these tasks, RL-continuous uses celllevel binary correctness as continuous rewards (e.g., counting matched cells across the grid), while RL-binary uses only trajectory-level success/failure signals. SSL delivers substantial gains across all benchmarks, with especially large improvements on the 3B model, where limited capacity amplifies the benefit of differentiated rewards. For example, the +100.0% gain on Sudoku illustrates SSLs strength: blockwise sweet-spot scoring rewards partial constraint satisfaction (e.g., reducing violations from 20 to 5), while binary rewards remain silent until full correctness. Notably, consistent improvements across Sudoku (constraint-based), SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Table 3. Cross-task transfer from perception to planning. We train models on GUI perception tasks and evaluate on both short-term and long-term planning benchmarks. denotes relative improvements over RL-Binary baseline ( SSL RL-Binary 1). Short-Term Planning Long-Term Planning Method GUI-Act-Web AndroidControl-Low AndroidControl-High GUI-Odyssey Type GR SR Type GR SR Type GR SR Type GR SR RL-Binary-3B SSL-3B (Ours) (, %) RL-Binary-7B SSL-7B (Ours) (, %) 74.4 78.2 +5.2 92.4 94.3 +2.1 80.2 83.1 +3.7 88.2 89.0 +0. 67.9 73.3 +8.0 76.3 86.8 +13.7 73.9 77.9 +5.4 81.9 82.9 +1.3 78.8 85.4 +8.4 82.0 86.2 +5. 60.1 65.8 +9.5 63.2 72.5 +14.8 58.0 59.6 +2.8 70.5 71.6 +1.6 52.2 57.6 +10.4 60.5 64.5 +6. 41.0 55.6 +35.5 55.3 58.5 +5.9 60.4 62.8 +4.1 72.0 72.0 +0.0 40.2 42.7 +6.2 46.5 46.2 -0. 37.7 39.8 +5.7 41.2 42.5 +3.0 Avg. 55.4 61.8 +11.7 69.2 72.2 +4.5 Table 4. Ablation on zone granularity (K). We report average success rate (%) on planning benchmarks for both models. 2 4 8 Short-Term Long-Term 3B 7B Avg. 3B 7B Avg. Overall 77.3 82.4 80. 82.6 85.3 84.7 80.0 83.9 82.6 51.6 57.1 56.2 55.8 57.0 56.7 53.7 57.1 56.5 66.9 70.5 69. Maze (path-finding), and ARC (pattern induction) show that SSLs tiered reward principle generalizes across domains. 4.3. Sample Efficiency Analysis Figure 6 examines SSLs sample efficiency across training set proportions from 20% to 100%. SSL demonstrates superior efficiency at all data scales: with only 40% of training data, SSL-3B matches or exceeds RL-Binary-3B trained on the full dataset, achieving up to 2.5 data efficiency. This gain stems from SSLs ability to extract richer learning signals from each trajectory: while binary rewards provide feedback only for successful outcomes, SSLs tiered scoring enables learning from the quality distribution of both successful and near-successful attempts. As data increases from 20% to 100%, SSL maintains its advantage. Consistent gains reveal that sample efficiency is fundamental property of SSL rather than an artifact of model capacity. 4.4. Cross-Task Transferability Figure 1 presents comprehensive performance overview across short-/long-term planning, and complex reasoning. SSL consistently outperforms binary-reward baselines across all tasks, demonstrating that the sweet-spot principle generalizes across diverse agent tasks. This validates our core thesis: tiered, proximity-aligned reward modeling provides more differentiated guidance for agent optimizatoin. To further assess within-domain transfer, we train SSL on Perception-2K and evaluate on GUI planning tasks. As Figure 7. Offset Distributions in GUI Grounding. We visualize the xy offset distributions for the base model, binary GRPO, and SSL models after filtering out invalid predictions. shown in Table 3, SSL-3B achieves significantly outperforms RL-Binary, with similar gains for 7B. These results suggest that distance-tiered rewards learned from static grounding could transfer to spatial reasoning in equential decision-making tasks. SSLs zone-based feedback helps agents prioritize proximity to task-relevant regions, transferable skill across planning horizons. 4.5. Ablation Study on Zone Granularity We investigate the impact of zone numbers on SSLs performance. Table 4 shows results with {2, 4, 8} on GUI planning tasks. Using = 4 zones achieves optimal balance: too few zones (K = 2) provide insufficient differentiation similar to binary rewards, while excessive zones (K = 8) may introduce noise from over-segmentation. 4.6. Qualitative Analysis Figure 7 visualizes the x-y offsets before and after applying the binary and sweet-spot reward. Base model (Baseline) yields diffuse patterns, indicating imprecise localization. Compared with binary shaping, SSL produces far more concentrated cluster, with peripheral sparsity driven by dense accumulation near the center. This shows that tiered grounding rewards offer properly finer-grained guidance than binary reward shaping, resulting in more centered GUI localization and supporting improved GUI planning. 8 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization 5. Conclusion We introduce Sweet Spot Learning, unified framework that provides differentiated guidance via tiered, proximityaligned rewards. By discretizing solution spaces into hierarchical zones, SSL addresses fundamental limitations of binary rewards: optimization ambiguity, learning inefficiency, and policy fragility. Experiments demonstrate consistent gains across diverse tasks, suggesting SSL as general principle for intelligent agentic optimization."
        },
        {
            "title": "Impact Statement",
            "content": "Ethical Considerations. We believe that SSL raises no ethical concerns regarding its motivation, design, or implementation. The framework operates within the existing RLVR paradigm using verifiable reward signals that provide objective feedback without subjective biases. Our method does not introduce novel data collection or architectural requirements that would raise privacy or safety concerns beyond standard agent post-training. All experiments adhere to responsible AI practices, including transparency and reproducibility. While improved agent capabilities could potentially be misused, SSL is general reward shaping principle without inherent malicious intent. Societal Implications. SSL has the potential to improve accessibility to capable agent systems by enabling more sample-efficient training. By providing differentiated guidance through tiered rewards, our framework could accelerate progress in assistive technologies, accessibility tools, and productivity applications. The improved sample efficiency reduces computational costs, lowering barriers for resourceconstrained researchers and practitioners. However, more capable autonomous agents may raise concerns around accountability, over-reliance on automation, and equitable access. We encourage responsible development of SSLtrained agents with attention to safety evaluation, human oversight mechanisms, and fair deployment practices."
        },
        {
            "title": "References",
            "content": "Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and Mane, D. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. Baheri, A. Understanding reward ambiguity through optimal transport theory in inverse reinforcement learning. arXiv preprint arXiv:2310.12055, 2023. Bai, S., Chen, K., Liu, X., Wang, J., Ge, W., Song, S., Dang, K., Wang, P., Wang, S., Tang, J., et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Chen, W., Cui, J., Hu, J., Qin, Y., Fang, J., Zhao, Y., Wang, C., Liu, J., Chen, G., Huo, Y., et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024. Cheng, K., Sun, Q., Chu, Y., Xu, F., YanTao, L., Zhang, J., and Wu, Z. SeeClick: Harnessing GUI grounding for advanced visual GUI agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 93139332, Bangkok, Thailand, August 2024. Association for Computational Linguistics. Chollet, F. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. Chollet, F., Knoop, M., Kamradt, G., Landers, B., and Pinkard, H. Arc-agi-2: new challenge for frontier ai reasoning systems. arXiv preprint arXiv:2505.11831, 2025. Cross, R. The sweet spots of tennis racquet. Sports Engineering, 1(2):6378, 1998. Eschmann, J. Reward function design in reinforcement learning. In Studies in Computational Intelligence, pp. 4560. Springer, 2021. Gandhi, K., Chakravarthy, A., Singh, A., Lile, N., and Goodman, N. D. Cognitive behaviors that enable selfimproving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Grzes, M. Reward shaping in episodic reinforcement learning. 2017. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hu, Y., Wang, W., Jia, H., Wang, Y., Chen, Y., Hao, J., Wu, F., and Fan, C. Learning to utilize shaping rewards: new approach of reward shaping. Advances in Neural Information Processing Systems, 33:1593115941, 2020. Ibrahim, S., Mostafa, M., Jnadi, A., Salloum, H., and Osinenko, P. Comprehensive overview of reward engineering and shaping in advancing reinforcement learning applications. IEEE Access, 2024. Inc., S. Hardest sudoku puzzle dataset v2, 2025. URL https://huggingface.co/datasets/sapientinc/ sudoku-extreme. Dataset available at Hugging Face: sapientinc/sudoku-extreme. Ivanitskiy, M. I., Shah, R., Spies, A. F., Rauker, T., Valentine, D., Rager, C., Quirke, L., Mathwin, C., Corlouer, G., Behn, C. D., et al. configurable library for generating and manipulating maze datasets. arXiv preprint arXiv:2309.10498, 2023. 9 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Ng, A. Y., Harada, D., and Russell, S. Policy invariance under reward transformations: Theory and application In Icml, volume 99, pp. 278287. to reward shaping. Citeseer, 1999. Jiang, B., Xie, Y., Wang, X., Yuan, Y., Hao, Z., Bai, X., Su, W. J., Taylor, C. J., and Mallick, T. Towards rationality in language and multimodal agents: survey. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 36563675, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. Kapoor, R., Butala, Y. P., Russak, M., Koh, J. Y., Kamble, K., AlShikh, W., and Salakhutdinov, R. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. In European Conference on Computer Vision, pp. 161178. Springer, 2024. Li, K., Meng, Z., Lin, H., Luo, Z., Tian, Y., Ma, J., Huang, Z., and Chua, T.-S. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. Li, W., Bishop, W., Li, A., Rawles, C., Campbell-Ajala, F., Tyamagundlu, D., and Riva, O. On the effects of data scale on computer control agents. arXiv preprint arXiv:2406.03679, 2024. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2023. Lu, Q., Shao, W., Liu, Z., Du, L., Meng, F., Li, B., Chen, B., Huang, S., Zhang, K., and Luo, P. Guiodyssey: comprehensive dataset for cross-app gui navigation on mobile devices. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2240422414, 2025. Luo, R., Wang, L., He, W., Chen, L., Li, J., and Xia, X. Guir1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025a. Luo, R., Zheng, Z., Wang, Y., Ni, X., Lin, Z., Jiang, S., Yu, Y., Shi, C., Chu, R., Zeng, J., et al. Ursa: Understanding and verifying chain-of-thought reasoning in multimodal mathematics. arXiv preprint arXiv:2501.04686, 2025b. MacKenzie, I. S. Fitts law as research and design tool in human-computer interaction. Human-computer interaction, 7(1):91139, 1992. Nguyen, D., Chen, J., Wang, Y., Wu, G., Park, N., Hu, Z., Lyu, H., Wu, J., Aponte, R., and Xia, Yu, e. a. GUI agents: survey. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 2252222538, Vienna, Austria, July 2025. Association for Computational Linguistics. Sutton, R. S., Barto, A. G., et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. Tang, F., Gu, Z., Lu, Z., Liu, X., Shen, S., Meng, C., Wang, W., Zhang, W., Shen, Y., Lu, W., et al. Gui-g2: Gaussian reward modeling for gui grounding. arXiv preprint arXiv:2507.15846, 2025. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Velu, A., Vaidyanath, S., and Arumugam, D. Hindsight-dice: Stable credit assignment for deep reinforcement learning. arXiv preprint arXiv:2307.11897, 2023. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, Bangkok, Thailand, August 2024. Association for Computational Linguistics. Wang, P.-Y., Liu, T.-S., Wang, C., Wang, Y.-D., Yan, S., Jia, C.-X., Liu, X.-H., Chen, X.-W., Xu, J.-C., Li, Z., et al. survey on large language models for mathematical reasoning. arXiv preprint arXiv:2506.08446, 2025. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wu, J., Liao, C., Feng, M., Zhang, S., Wen, Z., Shao, P., Xu, H., and Tao, J. Thought-augmented policy optimization: Bridging external guidance and internal capabilities. arXiv preprint arXiv:2505.15692, 2025a. 10 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Wu, Z., Wu, Z., Xu, F., Wang, Y., Sun, Q., Jia, C., Cheng, K., Ding, Z., Chen, L., Liang, P. P., and Qiao, Y. OS-ATLAS: Foundation action model for generalIn The Thirteenth International Conist GUI agents. ference on Learning Representations, 2025b. URL https://openreview.net/forum?id=n9PDaFNi8t. Xu, F., Hao, Q., Zong, Z., Wang, J., Zhang, Y., Wang, J., Lan, X., Gong, J., Ouyang, T., Meng, F., et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. Yu, X., Peng, B., Galley, M., Gao, J., and Yu, Z. Teaching language models to self-improve through interactive demonstrations. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 51275149, Mexico City, Mexico, June 2024. Association for Computational Linguistics. Zhang, C., He, S., Qian, J., Li, B., Li, L., Qin, S., Kang, Y., Ma, M., Liu, G., Lin, Q., et al. Large language model-brained gui agents: survey. arXiv preprint arXiv:2411.18279, 2024. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., and Luo, Z. LlamaFactory: Unified efficient fine-tuning of 100+ language In Proceedings of the 62nd Annual Meeting models. of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 400410, Bangkok, Thailand, August 2024. Association for Computational Linguistics. Zheng, Y., Lu, J., Wang, S., Feng, Z., Kuang, D., and Xiong, Y. Easyr1: An efficient, scalable, multi-modality rl training framework. https://github.com/hiyouga/ EasyR1, 2025. Zhou, D., Scharli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., et al. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. Zou, H., Ren, T., Yan, D., Su, H., and Zhu, J. Reward shaping via meta-learning. arXiv preprint arXiv:1901.09330, 2019. SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization A. Theoretical Proofs In this section, we provide detailed proofs for the theoretical propositions presented in Section 2.4. We first establish notation and then prove each proposition in order. A.1. Notation and Preliminaries We work within the MDP framework = (S, A, , R) with policy πθ (abbreviated as π when context is clear). Key quantities include: Binary correctness: C(τ ) {0, 1} indicates task completion Trajectory proximity: S(τ ) [0, 1] measures solution quality via Eq. (3) Discretized sweet-spot value: (cid:98)S(τ ) [0, 1] obtained via zone-based discretization Success rate: SR(π) = Eτ π[C(τ )] Expected sweet-spot score: µS(π) = Eτ π[S(τ )] The SSL reward is defined as: yielding expected return: RSSL(τ ) = C(τ ) + α (cid:98)S(τ ), α > 0, JSSL(π) = Eτ π[RSSL(τ )] = SR(π) + α Eτ π[ (cid:98)S(τ )]. Since (cid:98)S(τ ) is discretized version of S(τ ) and both take values in [0, 1], we use S(τ ) in our analysis for simplicity, noting that discretization preserves the ordering properties. Thus: JSSL(π) = SR(π) + α µS(π). A.2. Proof of Proposition 2.1: Quality Ordering Proposition 2.1 (Quality Ordering under Equal Success Rate). For two policies π1 and π2 with identical success rates SR(π1) = SR(π2), we have JSSL(π1) > JSSL(π2) µS(π1) > µS(π2). Proof. Given SR(π1) = SR(π2), we compare the SSL objectives: JSSL(π1) JSSL(π2) = (cid:2)SR(π1) + αµS(π1)(cid:3) (cid:2)SR(π2) + αµS(π2)(cid:3) = SR(π1) SR(π2) + α(cid:2)µS(π1) µS(π2)(cid:3) = α(cid:2)µS(π1) µS(π2)(cid:3). where the last equality follows from SR(π1) = SR(π2). Since α > 0 by definition: JSSL(π1) > JSSL(π2) α(cid:2)µS(π1) µS(π2)(cid:3) > 0 µS(π1) > µS(π2). This completes the proof. The proposition shows that SSL strictly preserves the quality ordering induced by sweet-spot scores among policies with equal success rates, enabling finer policy discrimination than binary rewards alone. 12 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization A.3. Proof of Proposition 2.2: SNR Enhancement We adopt the GRPO-style policy gradient estimator with sampled trajectories: (cid:98)J ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:0)R(τi) R(cid:1)θ log πθ(τi), where = 1 signal-to-noise ratio: (cid:80)N i=1 R(τi) is the sample baseline. For any unit direction Rd with = 1, we define the projected SNR(u) = (cid:12) E[ (cid:98)J u] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:113) Var[ (cid:98)J u] . Proposition 2.2 (Projected SNR Improvement)."
        },
        {
            "title": "We use the shorthand",
            "content": "the score function of the trajectory, which satisfies the score-function identity E[ℓ(τ )] = 0. ℓ(τ ) = θ log πθ(τ ), Let and let If then gbin = (cid:2)(C(τ ) C) ℓ(τ )(cid:3) , = gbin gbin . Cov (cid:0)S(τ ), ℓ(τ )u(cid:1) 0, SNRSSL(u) SNRbin(u). Proof. We use the scorefunction identity: E[ℓ(τ )] = 0. Step 1: Projected Mean Gradient. The SSL gradient can be written as E[ (cid:98)JSSL] = E[(C(τ ) C)ℓ(τ )] + α E[(S(τ ) S)ℓ(τ )]. Projecting onto gives E[ (cid:98)J SSLu] = E[(C(τ ) C)ℓ(τ )u] + α E[(S(τ ) S)ℓ(τ )u]. Using E[ℓu] = 0, we obtain Since = gbin/gbin, we have E[(C(τ ) C) ℓ(τ )u] = Cov(C(τ ), ℓ(τ )u). Cov(C(τ ), ℓ(τ )u) = gbin > 0. Step 2: Signal Improvement. The SSL signal is By the alignment assumption, and with α > 0, this implies Since we obtain E[ (cid:98)J SSLu] = Cov(C, ℓu) + α Cov(S, ℓu). Cov(S(τ ), ℓ(τ )u) 0, (cid:12) (cid:12) (cid:12) E[ (cid:98)J (cid:12) (cid:12) (cid:12) (cid:12) SSLu] (cid:12)Cov(C(τ ), ℓ(τ )u)(cid:12) (cid:12) . E[ (cid:98)J binu] = Cov(C(τ ), ℓ(τ )u), (cid:12) (cid:12) (cid:12) E[ (cid:98)J (cid:12) (cid:12) (cid:12) SSLu] (cid:12) (cid:12) (cid:12) E[ (cid:98)J (cid:12) (cid:12) (cid:12) . binu] 13 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Step 3: Controlled Variance. Let = (C(τ ) C) ℓ(τ )u, = (S(τ ) S) ℓ(τ )u."
        },
        {
            "title": "The SSL variance is",
            "content": "Var(X + αY ) = Var(X) + α2 Var(Y ) + 2α Cov(X, ). Since 0 S(τ ) 1 and ℓ(τ )u matches the binary case, each term is bounded, implying that the variance grows at most quadratically in α. Step 4: SNR Comparison. The projected SNR is SNR(u) = E[ (cid:98)J (cid:12) (cid:12) u] (cid:12) (cid:12) (cid:12) (cid:12) (cid:113) Var[ (cid:98)J u] . Combining Step 2 and the bounded variance in Step 3 yields SNRSSL(u) SNRbin(u). A.4. Discussion: Practical Implications The theoretical results provide important insights for SSLs design and application: Proposition 2.1 guarantees that SSL can distinguish policy quality beyond binary success metrics, enabling more nuanced optimization even when policies achieve similar task completion rates. Proposition 2.2 ensures that when sweet-spot scores align with gradient directionsa natural condition in welldesigned tasksSSL improves gradient quality, leading to faster convergence and better sample efficiency. Hyperparameter selection: The coefficient α controls the balance between binary correctness and sweet-spot guidance. Our experiments show that α [0.1, 0.5] works well across diverse tasks, providing sufficient differentiation without overwhelming the correctness signal. Zone design: The discretization into sweet-spot zones {Zk, sk}K k=1 suppresses noise in continuous proximity scores while preserving meaningful quality differences. Empirically, = 3 to = 5 zones provide an effective trade-off. B. Additional Implemenation Details B.1. Gaussian Field Construction for GUI For GUI grounding tasks, we compute step proximity h(st, at) using Gaussian field centered at the target bounding box. Given predicted point = (x, y) and ground-truth bounding box = (x1, y1, x2, y2), we first compute the box center and dimensions: Step Proximity (Gaussian Field). Given = (x, y) and = (x1, y1, x2, y2), let = (cid:16) x1+x2 2 , y1+y2 2 (cid:17) , = x2x1 2 , = y2y1 2 . Define the normalized distance and Gaussian field (choose σ = 1 3 so the 3σ contour matches the inscribed ellipse): d2(p, c) = + (x cx)2 a2 exp(cid:0) d2(p,c) 2σ2 0, (cid:40) , (y cy)2 b2 (cid:1), B, otherwise. ϕ(p; B) = 14 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Zone Boundary Determination. Let the kσ level be Define four zones inside B:"
        },
        {
            "title": "Assign discrete scores",
            "content": "(cid:18) τk = exp (cid:19) , k2 {0, 1, 2, 3}. Z1 = {p ϕ(p; B) τ1}. Z2 = {p τ2 ϕ(p; B) < τ1}. Z3 = {p τ3 ϕ(p; B) < τ2}. Z4 = {p 0 < ϕ(p; B) < τ3}. s(p) = 1.00, Z1, 0.75, Z2, 0.50, Z3, 0.25, Z4, / B. 0, B.2. Blockwise Sweet-Spot Construction Details Grid Partitioning. For maze with dimensions , we partition it into 3 3 grid of blocks. Each block (i, j) with i, {1, 2, 3} covers cells: Bi,j = (cid:110) (r, c) (cid:25) (cid:24) (i 1)H 3 < (cid:24) iH (cid:25) (cid:24) (j 1)W 3 , (cid:25) < (cid:24) jW (cid:25) (cid:111) . Block-Level Matching. For each block (i, j), we compute the number of matched cells between predicted and reference grids: ni,j = (cid:88) 1[Mpred r,c = Mref r,c], where Mpred and Mref are binary occupancy grids indicating path presence. (r,c)Bi,j Block Sweet-Spot Assignment. Each block is assigned sweet-spot value based on its match count ni,j relative to block size Bi,j: si,j = 1.0 0.67 0.33 0. (high match), if ni,j 0.75Bi,j if 0.5Bi,j ni,j < 0.75Bi,j if 0.25Bi,j ni,j < 0.5Bi,j (very low match). otherwise (medium match), (low match), The trajectory-level sweet-spot score is then: S(τ ) = 1 (cid:88) si,j. i,j{1,2,3} Extension to Sudoku. For Sudoku, we use the natural 33 subgrid structure. Each block (i, j) corresponds to one of the nine Sudoku subgrids, and ni,j counts digit matches within that subgrid. The sweet-spot assignment follows the same thresholds as maze navigation, adapted to subgrid size (9 cells per block). Extension to ARC-AGI. For ARC-AGI tasks, predicted and reference outputs may have varying dimensions. We first normalize both grids to the same size using nearest-neighbor interpolation, then apply the 33 partitioning scheme. For pattern matching, we consider both pixel value equality and spatial alignment within each block. SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Table 5. Step proximity function implementation for different tasks."
        },
        {
            "title": "Task Type",
            "content": "h(st, at) Implementation"
        },
        {
            "title": "GUI Grounding",
            "content": "Gaussian field value ϕ(p; B) where is predicted point"
        },
        {
            "title": "GUI Planning",
            "content": "h(st, at) = ϕ(p; B) if action involves spatial grounding (click, drag); otherwise h(st, at) = C(at) (binary correctness) Maze Navigation Computed via blockwise aggregation (Section B.2)"
        },
        {
            "title": "Sudoku",
            "content": "Computed via blockwise aggregation over 33 subgrids ARC-AGI"
        },
        {
            "title": "Computed via blockwise aggregation with normalized grids",
            "content": "B.3. Step Proximity Function Implementation Table 5 summarizes the concrete implementation of step proximity function h(st, at) for different task types. B.4. Verifier Implementation The binary verifier C(τ ) {0, 1} is implemented differently for each task category: GUI Grounding. prediction is correct if the predicted point falls within the ground-truth bounding box B: C(τ ) = 1[x1 x2 and y1 y2]. GUI Planning. trajectory is successful if all of the following conditions are met: 1. Action type matches ground truth: apred type = aref type 2. For grounding actions (click, drag): predicted point falls in target bounding box 3. For text input actions: predicted text exactly matches reference text 4. Terminal state satisfies task goal (e.g., correct page reached, form submitted) Maze Navigation. path is valid if: 1. Start position matches the designated start 2. End position matches the designated goal 3. All intermediate cells are walkable (not walls) 4. Path is continuous (adjacent cells differ by one step in Manhattan distance) Sudoku. solution is correct if: 1. All cells are filled with digits 1-9 2. Each row contains digits 1-9 exactly once 3. Each column contains digits 1-9 exactly once 4. Each 33 subgrid contains digits 1-9 exactly once ARC-AGI. prediction is correct if the predicted output grid exactly matches the reference output grid in both dimensions and all cell values. SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization B.5. Integration with RLVR Algorithms We integrate SSL with Group Relative Policy Optimization (GRPO). The standard GRPO objective is: LGRPO(θ) = Eτ πθ min (cid:34) (cid:32) πθ(τ ) πold(τ ) A(τ ), clip (cid:18) πθ(τ ) πold(τ ) (cid:19) (cid:33)(cid:35) , 1 ϵ, 1 + ϵ A(τ ) + βDKL(πθπref). where A(τ ) = R(τ ) 1 and β controls KL divergence regularization. (cid:80)N i=1 R(τi) is the advantage computed using group-relative baseline, ϵ is the clipping parameter, SSL Integration. We simply replace the binary reward Rbin(τ ) = C(τ ) with the SSL reward: while keeping all other GRPO components unchanged. The advantage becomes: RSSL(τ ) = C(τ ) + α (cid:98)S(τ ), ASSL(τ ) = RSSL(τ )"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 RSSL(τi). This plug-and-play integration show SSLs compatibility with existing RLVR frameworks. The same integration strategy applies to other policy gradient algorithms (PPO, REINFORCE, etc.) by replacing their reward signals. B.6. Training Configuration. Table 6 summarizes training hyperparameters. All experiments are based on 8 NVIDIA A100-80G GPUs. Table 6. Training hyperparameters in SFT and RL training. Parameter SFT Stage RL Stage Learning rate Batch size Max gradient norm Warmup ratio Weight decay Optimizer β1, β2 Training epochs Max sequence length KL penalty coefficient Number of rollouts Attn implementation 1e-5 32 1.0 0.1 0.01 AdamW (0.9, 0.999) 1 2048 N/A N/A flash attention 1e-6 128 1.0 0.05 0.01 AdamW (0.9, 0.999) 10 2048 0.1 8 flash attention 2 C. Training Data and Evaluation Details The statistics of the datasets are recorded in Table 7. Note that the evaluation metrics for most datasets are consistent with their official standards, such as accuracy, success rate, which are introduced in Section 3.1 in the main text. GUI Planning Benchmarks. GUI-Act (Chen et al., 2024) is web-based GUI action prediction benchmark containing 1,410 test examples. Each sample provides screenshot of web interface along with natural language task description, requiring models to predict the appropriate action type (e.g., click, type, scroll) and corresponding parameters (e.g., coordinates, input text). We use this dataset to evaluate short-term planning capabilities in web environments. SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Table 7. Dataset statistics."
        },
        {
            "title": "Split",
            "content": "#Samples"
        },
        {
            "title": "Task",
            "content": "Perception-2K Mix-3K Maze-Train Sudoku-Train ARC-AGI-1-Train ARC-AGI-2-Train GUI-Act-Web OmniAct-Web OmniAct-Desktop AndroidControl-Low AndroidControl-High GUI-Odyssey ScreenSpot ScreenSpot-Pro Maze-Test Sudoku-Test ARC-AGI-1-Test ARC-AGI-2-Test"
        },
        {
            "title": "Training Data",
            "content": "2,098 3,570 1,000 1,000 400 1,000 Web + Mobile Web + Mobile Spatial Navigation Logic Puzzle Abstract Reasoning Abstract Reasoning Perception Perception + Planning Complex Reasoning + Planning Complex Reasoning Complex Reasoning Complex Reasoning"
        },
        {
            "title": "Evaluation Benchmarks",
            "content": "1,410 1,427 594 7,708 7,708 17,920 1,272 1,581 1,000 100 400 120 Web Web Desktop Mobile Mobile Web + Mobile Multi-platform Multi-platform Spatial Navigation Logic Puzzle Abstract Reasoning Abstract Reasoning Short-term Planning Short-term Planning Short-term Planning Short-term Planning Long-term Planning Long-term Planning Perception Perception Complex Reasoning + Planning Complex Reasoning Complex Reasoning Complex Reasoning OmniAct (Kapoor et al., 2024) provides comprehensive GUI automation benchmarks across both web (1,427 samples) and desktop (594 samples) platforms. The dataset emphasizes diverse interaction types and complex multi-step scenarios, requiring models to ground actions in realistic application contexts. We leverage both splits to assess cross-platform generalization in short-term planning tasks. AndroidControl (Li et al., 2024) offers mobile GUI automation benchmarks at two complexity levels. AndroidControlLow (7,708 samples) focuses on single-step or short-sequence tasks requiring 1-3 actions, while AndroidControl-High (7,708 samples) presents long-horizon scenarios with 5-15 sequential actions. Both benchmarks evaluate end-toend mobile app interaction capabilities, and we use them to assess short-term and long-term planning performance respectively. GUI-Odyssey (Lu et al., 2025) is challenging long-term planning benchmark comprising 17,920 test cases spanning both web and mobile platforms. Tasks require multi-step sequential reasoning, testing models ability to maintain task context and execute coherent action sequences. We employ this benchmark to evaluate sustained planning capabilities across extended interaction horizons. Fine-Grained Perception Benchmarks. ScreenSpot (Cheng et al., 2024) is comprehensive vision-language benchmark for GUI grounding that evaluates models ability to locate UI elements based on natural language instructions. The dataset contains 1,272 test samples spanning mobile, desktop, and web interfaces across diverse application domains including development tools, creative software, office applications, and operating systems. Each sample consists of screenshot paired with natural language description of target UI element, requiring models to predict precise 2D coordinates. We utilize this benchmark to assess fine-grained spatial grounding capabilities. ScreenSpot-Pro (Li et al., 2025) extends the original ScreenSpot benchmark with 1,581 more challenging test cases that emphasize complex interface layouts, ambiguous element descriptions, and cross-platform diversity. The enhanced dataset includes additional categories and more nuanced evaluation metrics to better differentiate model performance on subtle grounding tasks. We employ this benchmark to validate our methods robustness on difficult perception scenarios. 18 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Complex Reasoning Benchmarks. Maze Navigation (Ivanitskiy et al., 2023) is configurable pathfinding benchmark that evaluates spatial reasoning and sequential planning capabilities. The dataset consists of 1,000 training mazes and 1,000 test mazes with varying complexity levels, including different maze sizes (99 to 2525), obstacle densities, and path tortuosity. Each instance requires finding valid path from designated start position to goal location while avoiding walls. The benchmark tests models ability to perform long-horizon spatial reasoning. We utilize this dataset to assess our methods performance on structured spatial planning and reasoning tasks. Sudoku Solving (Inc., 2025) provides comprehensive constraint satisfaction benchmark based on the classic Sudoku puzzle. The dataset includes 1,000 training puzzles and 100 test puzzles spanning multiple difficulty levels (easy, medium, hard, extreme). Each puzzle presents partially filled 99 grid that must be completed such that every row, column, and 33 subgrid contains digits 1-9 without repetition. The benchmark emphasizes logical deduction and constraint propagation capabilities. We employ this dataset to evaluate our methods ability to handle complex combinatorial reasoning with strict constraints. ARC-AGI (Chollet, 2019; Chollet et al., 2025) (Abstraction and Reasoning Corpus) is challenging benchmark designed to measure abstract reasoning and few-shot learning capabilities. We evaluate on two test sets: ARC-AGI-1 (400 samples) from the original 2019 release and ARC-AGI-2 (120 samples) from the 2025 updated version. Each task presents 2-4 input-output demonstration pairs followed by test input, requiring models to induce the underlying transformation rule and apply it to generate the correct output grid. Tasks involve diverse reasoning patterns including object manipulation, symmetry detection, pattern completion, and compositional transformations. The dataset is particularly challenging as it requires discovering novel abstractions rather than applying memorized patterns. We use both versions to comprehensively assess our methods abstract reasoning and generalization capabilities. D. Statistical Significance Analysis Table 8 reports statistical significance tests for SSL improvements over RL-Continuous baseline. We conduct paired t-tests across 3 independent runs for each benchmark. Results demonstrate that SSL achieves statistically significant improvements (p<0.05) across all task categories, with particularly strong gains in complex reasoning tasks where differentiated rewards provide clearer learning signals. The consistent improvements across diverse benchmarks validate SSLs effectiveness as general principle for agent optimization. Table 8. Statistical significance of SSL vs RL-Continuous (3B models). We report average success rate (%) across benchmarks within each category. Task Category SSL-3B RL-Cont-3B p-value Short-term Planning Long-term Planning Complex Reasoning 80.071.2 52.051.8 39.281.5 72.871.5 43.332.1 32.101. Overall Average 57.131.5 49.431.8 <0.05 <0.05 <0.05 <0.05 E. Additional Experimental Results E.1. Complete Long-Term Planning Results Table 9 presents the complete results for long-term planning tasks mentioned in Section 3.2. E.2. Complete GUI Perception Results Table 10 presents the complete results for GUI grouding tasks mentioned in Section 3.2. 19 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Table 9. Performance on more challenging long-term planning tasks. * indicates SFT on Mix-3K. All RL experiments are based on RL-Binary 1). standard RLVR paradigm and trained on Mix-3K by default. denotes relative improvements over RL-Binary baseline ( SSL"
        },
        {
            "title": "Models",
            "content": "AndroidControl-High SR GR Type GUI-Odyssey GR"
        },
        {
            "title": "Type",
            "content": "SR Avg."
        },
        {
            "title": "Zero Shot",
            "content": "GPT-4o QwenVL2.5-3B QwenVL2.5-7B 63.06 47.81 68.67 30.90 46.51 59.71 21.17 38.90 47.06 37.50 37.40 55.60 14.17 26.49 37. 5.36 26.69 34.37 28.69 37.30 50.53 Post-Training (Supervised Fine-Tuning or Reinforcement Learning) OS-Atlas-4B OS-Atlas-7B QwenVL2.5-3B* QwenVL2.5-7B* UI-R1-3B 49.01 57.44 52.05 69.15 57.85 RL-Continuous-3B 64.72 59.56 RL-Binary-3B 65.96 SSL-3B (Ours) (, %) +10. RL-Continuous-7B 70.45 70.82 RL-Binary-7B 71.79 SSL-7B (Ours) (, %) +1.4 49.51 54.90 49.53 58.69 55.70 60.91 56.24 62.41 +11.0 62.77 63.43 65.56 +3.4 22.77 29.83 41.22 48.11 45.44 49.96 44.85 61.62 +37. 49.09 49.50 52.31 +5.7 49.63 60.42 43.69 56.78 52.16 58.57 56.78 63.75 +12.3 66.26 63.47 65.90 +3.8 34.63 39.74 32.21 38.65 34.46 43.97 42.67 46.44 +8. 43.33 44.79 46.02 +2.7 20.25 26.96 27.31 34.44 32.49 36.69 38.78 42.47 +9.5 38.61 37.67 39.93 +6.0 37.63 44.88 41.00 50.97 46.35 52.47 49.81 57.11 +14. 55.09 54.95 56.92 +3.6 Table 10. Performance on GUI grounding tasks. * indicates SFT on Mix-3K. All RL experiments are based on standard RLVR paradigm and trained on Mix-3K by default. denotes relative improvements over RL-Binary baseline ( SSL RL-Binary 1). Models Dev Creative CAD ScreenSpot-Pro Scientific Office OS Avg. Web Desktop Avg. ScreenSpot Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon Text Icon GPT-4o QwenVL2.5-3B QwenVL2.5-7B SeeClick-9.6B FOCUS-2B ShowUI-2B Os-Atlas-4B Os-Atlas-7B UGround-7B CogAgent-18B Aria-GUI Claude** QwenVL2.5-3B* QwenVL2.5-7B* UI-R1-3B UGround-7B 1.3 16.2 33.1 0.6 22.8 16.9 7.1 33.1 26.6 14.9 16.2 22.0 20.3 31.4 22.7 26. 0.0 1.4 2.1 0.0 1.7 1.4 0.0 1.4 2.1 0.7 0.0 3.9 1.8 1.8 4.1 2.1 RL-Continuous-3B RL-Binary-3B SSL-3B (Ours) (, %) RL-Continuous-7B RL-Binary-7B SSL-7B (Ours) (, %) 27.3 28.0 31.4 +12.1 48.0 40.0 50.7 +26. 4.2 4.2 4.9 +16.7 4.1 3.4 5.5 +61.8 1.0 23.3 23.7 1.0 23.7 9.1 3.0 28.8 27.3 9.6 23.7 25.9 24.6 27.3 27.3 27.3 40.4 35.4 37.9 +7.1 40.9 40.4 42.0 +4. Zero Shot 0.0 1.4 3.5 2.0 10.2 12.2 0.0 4.7 6.3 2.1 38.2 36.8 0.0 6.4 7. 1.1 24.3 37.8 0.0 3.8 7.5 0.0 15.0 30.8 0.0 1.1 6.9 Post-Training (Supervised Fine-Tuning or Reinforcement Learning) 0.0 1.7 0.0 1.4 2.8 2.8 0.0 2.1 3.4 2.8 3.5 3.5 2. 5.6 4.2 7.0 +66.7 9.0 6.9 9.1 +31.9 2.5 7.6 2.5 2.0 12.2 14.2 7.1 7.6 14.5 11.2 15.7 11.2 14.2 27.4 25.9 26.4 +1.9 27.4 22.8 24.4 +7.0 0.0 3.1 0.0 0.0 4.7 1.6 3.1 1.6 3.7 4.7 5.1 6.3 1. 9.4 7.8 10.9 +39.7 4.7 4.7 5.1 +8.5 3.5 25.0 13.2 9.0 37.5 31.9 22.2 27.1 33.9 39.5 40.7 43.4 31.9 54.9 61.1 61.8 +1.1 61.1 54.8 59.0 +7.7 1.1 23.2 15.3 5.1 33.9 31.6 13.0 20.3 30.1 28.6 39.7 32.2 31. 49.2 44.7 45.8 +2.5 56.4 55.9 57.1 +2.1 0.0 7.7 7.5 3.8 5.7 11.3 0.0 1.9 16.3 5.7 8.9 11.3 11.3 11.4 7.5 13.2 +76.0 24.5 20.7 26.5 +28.0 2.8 17.8 10.3 5.6 27.1 17.8 5.6 4.7 11.0 17.8 32.4 13.1 17. 26.2 25.3 27.1 +7.1 41.1 41.1 42.1 +2.4 0.0 7.1 7.3 5.5 7.3 2.7 1.8 6.4 15.8 6.4 7.9 11.8 2.7 19.1 20.0 21.9 +9.5 12.7 11.8 13.7 +16.1 0.6 12.2 17.3 1.0 12.0 7.1 3.5 16.6 14.2 6.5 9.3 15.4 13.8 18.4 16.0 14.2 0.0 2.5 2.2 0.0 4.5 0.0 0.0 0.0 4.5 2.2 6.9 4.5 0.0 3.4 4.5 7.9 +75.6 12.3 12.3 13.5 +9.8 23.2 22.4 24.7 +10. 28.5 26.2 29.1 +11.1 11.1 60.8 86.9 7.8 43.5 65.1 20.6 70.1 89.7 19.4 35.0 60.0 14.7 52.4 75. 55.7 81.7 81.7 82.6 90.8 80.4 70.4 - - 73.0 87.8 85.2 80.4 84.3 87.4 88.3 +1.0 91.3 91.3 93.0 +1.9 32.5 68.5 63.6 63.1 74.2 70.4 28.6 - - 48.5 68.2 73.3 70.4 66.0 67.9 68.4 +0.7 76.6 75.7 75.7 +1. 72.2 80.9 76.3 72.1 91.7 82.5 74.2 - - 85.7 90.3 90.2 82.5 93.3 90.3 91.3 +1.1 92.2 92.7 93.3 +0.6 30.0 65.0 61.1 45.7 62.8 63.6 20.0 - - 46.2 62.8 59.3 63.6 62.8 61.4 65.0 +5.9 70.0 68.6 70.0 +2. 47.6 74.0 70.7 65.9 79.9 74.2 48.3 - - 63.4 77.3 77.0 74.2 76.6 76.8 78.3 +2.0 82.5 82.1 83.0 +1.1 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization E.3. Why Tiered Rewards Over Continuous Rewards? natural question about SSL is: why should discretized, tiered rewards outperform continuous reward shaping when ground-truth solutions are available? We provide detailed analysis answering this concern. Sources of Gradient Noise in Continuous Rewards. Even with ground-truth solutions, continuous rewards introduce gradient noise through sample-level variance rather than reward measurement error. Consider the GRPO gradient estimator: θJ ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) (R(τi) R)θ log πθ(τi) i=1 When rewards are continuous (e.g., R(τ ) = distance ratio [0, 1]), three issues arise: 1. Small reward differences dominate the advantage term: Consider two GUI trajectories with click offsets of 45px and 50px from targettheir distance ratios differ by only 0.02-0.05. In batch of N=128 samples, such small differences get amplified through the baseline subtraction (R(τi) R), creating noisy gradient signals where trajectories with negligible quality differences receive opposing update directions. 2. High variance in policy gradient estimates: Table 11 shows that continuous rewards produce 2.1-3.5 higher gradient variance than tiered rewards across tasks. This is because continuous scores scatter samples uniformly across [0,1], whereas tiered scores cluster samples into discrete groups, producing more stable advantage estimates within each group. 3. Spurious optimization signals: Continuous rewards can create misleading gradients when slight quality differences (e.g., 42px vs 47px offset) correlate randomly with unrelated trajectory features. SSLs discretization filters out these spurious signals by treating both as equivalent mid-tier solutions. Table 11. Gradient variance comparison: continuous vs tiered rewards (3B model, measured by Var[θJ u] where is the dominant gradient direction). Task Type Continuous Tiered (K=4) Variance Ratio GUI Grounding Short-term Planning Complex Reasoning 0.0470.008 0.0530.009 0.0910.015 0.0220.004 0.0250.005 0.0260.006 2.14 2.12 3.50 Why Finer Differentiation Is Not Always Better. natural question is that theoretically, finer reward granularity should provide more information. However, this intuition breaks down in practice due to the sample efficiency-discrimination tradeoff: Statistical power: With finite samples per batch (N128), having K=10+ zones means each zone contains only 10-15 samples on average. This sparse coverage leads to unreliable gradient estimates per zone. Over-fitting to noise: Very fine zones (K8) risk over-fitting to spurious correlations in the training data. For example, in GUI tasks, continuous distance might spuriously correlate with screen region (top-left clicks happening to be closer on average), leading the policy to overfit to screen positions rather than semantic targets. SSLs Design Is More Principled Than Arbitrary Continuous Shaping. The reviewer raises concerns about task-specific zone design in SSL. We argue that SSLs zone selection is actually more systematic and less arbitrary than continuous reward shaping: Evidence from our experiments: RL-Continuous baseline uses carefully designed continuous rewards (Gaussian distance for GUI, cell-wise matching for reasoning), yet SSL consistently outperforms it (Table 1, 2). SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Table 12. Comparison of design complexity: continuous shaping vs SSL."
        },
        {
            "title": "Aspect",
            "content": "Design Choices Tuning Effort Interpretability"
        },
        {
            "title": "Continuous Reward Shaping",
            "content": "SSL (Tiered Rewards) - Distance function (L1, L2, normalized?) - Scaling factor α - Potential function shape (linear, exponential, logarithmic?) - Normalization strategy - Zone count (robust to K[3,5]) - Boundary placement (natural thresholds: σ-levels, quartiles) - Score assignment (uniform sk=k/K) Requires careful tuning of functional form and scaling to avoid reward hacking or vanishing gradients Continuous values lack clear semantic meaning (what does reward=0.637 mean?) Default configuration works across tasks; minimal tuning needed Discrete tiers have clear interpretation (e.g., within 1σ band = high quality) SSLs zone design follows task-natural structure (Gaussian σ-levels for spatial tasks, quartiles for matching), requiring minimal manual tuning. Cross-task transfer experiments  (Table 3)  show SSL trained on perception transfers to planning without zone redesign, demonstrating generalizability. Theoretical Justification: Signal-to-Noise Ratio Enhancement. Our Proposition 3.2 formalizes why tiered rewards improve optimization. The key insight is that discretization acts as noise filter: SNRSSL(u) = E[J (cid:113) Var[J SSLu] SSLu] SNRbin(u) This holds when sweet-spot scores S(τ ) satisfy the alignment condition Cov(S(τ ), ℓ(τ )u 0. Importantly, this condition is easier to satisfy with tiered rewards than continuous ones because: Tiered scores ˆS(τ ) aggregate similar-quality trajectories, reducing the impact of outlier samples that violate alignment Continuous scores S(τ ) are more susceptible to spurious anti-correlation with gradient directions for individual noisy samples When Might Continuous Rewards Be Preferable? We acknowledge scenarios where continuous rewards could be beneficial: Infinite sample regime: With unlimited training data, continuous rewards can theoretically provide finer optimization signals Noise-free environments: In deterministic simulators with no sampling variance, continuous rewards may converge faster Learned reward models: When rewards come from well-calibrated neural reward model (e.g., PRM), continuous confidence scores might be more informative than discretized tiers However, none of these conditions hold in our setting: we have finite data (3K-10K samples), stochastic policy sampling, and verifier-based (not learned) rewards. Conclusion. Tiered rewards outperform continuous rewards not because they provide more information, but because they provide more reliable information under finite-sample, stochastic gradient estimation. The discretization-induced noise reduction outweighs the loss of fine-grained differentiation. SSLs zone design is task-aware but systematic, requiring less manual engineering than arbitrary continuous reward functions while providing better optimization properties. 22 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization F. Case Studies Across GUI planning, maze navigation, and ARC-style pattern induction, Sweet-Spot Learning (SSL) consistently improves the agents ability to make structured progress in reasoning tasks. In GUI planning, we partition the 2D click-offset distribution into Gaussian-based sweet-spot zones, providing fine-grained supervision on how well each click and stabilizing high-level action planning; Figure 8 shows the GUI agent successfully completing complex multi-step task involving clicking, typing, and scrolling; consistent with this qualitative behavior, our method achieves substantial gains on complex multi-step GUI benchmarks. Figure 9 illustrates maze navigation under SSL: graded reward reflects partial progress (moving closer to the goal), which in turn yields more reliable global path formation; in this example, the agent follows the optimal path from start to goal. Figure 10 illustrates ARC-style pattern induction: although this task is inherently difficult and the model achieves only partial correctness (e.g., correct colors and local alignments), the outcome is reasonable. SSL rewards intermediate structural consistency across inputoutput grids, helping the model validate and refine hypothesized transformation rules rather than relying solely on final binary correctness. Together, these case studies demonstrate that SSL offers unified mechanism for guiding the model through ambiguous or sparsely-rewarded reasoning spaces, enabling more stable and interpretable improvements across diverse domains. G. Broader Impact SSL aims to improve the training efficiency and performance of intelligent agents across diverse applications. Potential positive impacts include: Accessibility: By improving sample efficiency, SSL reduces computational costs, making advanced agent training more accessible to researchers with limited resources. Robustness: Tiered rewards encourage exploration of diverse high-quality solutions, potentially yielding more robust agents. Generalization: SSLs cross-task transferability may accelerate progress toward general-purpose agents. H. Additional Discussion and Analysis H.1. Discussion on Reward Hacking and Misalignment Potential Misalignment in Block-wise Rewards. For tasks like Sudoku where global constraint satisfaction is required, block-wise sweet-spot rewards may create misalignment between local and global objectives. For instance, an agent could maximize block-level matches while violating row/column constraints, leading to reward hacking. Observed Failure Modes. We observe such failure cases in approximately 8% of Sudoku trajectories during training, where models achieve high block scores (S(τ ) > 0.7) but fail global verification C(τ ) = 0. These cases typically involve: Repetition of digits within rows/columns across block boundaries Correct block configurations that create contradictions globally Mitigation Strategies. SSL mitigates reward hacking through two mechanisms: 1. Binary correctness gating: The reward RSSL = C(τ ) + α ˆS(τ ) ensures sweet-spot rewards only amplify correct trajectories. Failed trajectories receive RSSL = α ˆS(τ ) < 1, providing weaker signal than successful ones. 2. Zone discretization: By grouping similar proximities into coarse zones, SSL reduces sensitivity to spurious local optima that continuous rewards might overfit to. Recommended Practices. For tasks with complex global constraints, we recommend: (i) Increasing α moderately (0.1-0.3) to maintain correctness priority; (ii) Incorporating cross-block consistency checks in zone design where feasible; (iii) Monitoring the ratio of high-S(τ ) but failed trajectories during training as an early indicator of misalignment. Despite these limitations, SSLs improvements on Sudoku (+100% for 3B) demonstrate that even imperfect sweet-spot signals substantially accelerate learning compared to sparse binary rewards. 23 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization H.2. Comparison with Learned Reward Models Why Not Compare with PRM/ORM? Process Reward Models (PRMs) and Outcome Reward Models (ORMs) (Lightman et al., 2023; Wang et al., 2024; Uesato et al., 2022; Yu et al., 2024) have shown promise in mathematical reasoning domains. However, direct comparison is infeasible for our GUI and spatial reasoning tasks due to: 1. Data scarcity: Training effective PRMs requires large-scale step-level human annotations of solution quality. Current GUI datasets (e.g., Mix-3K) lack such annotations, and existing PRMs trained on math/code domains do not transfer well to visual grounding or spatial planning. 2. Domain mismatch: PRMs for math reasoning (Lightman et al., 2023; Wang et al., 2024) operate on text-based chainof-thought, while GUI tasks require multimodal visual-spatial understanding. Adapting PRMs to score (screenshot, action) pairs would require substantial architectural changes and new annotation efforts. 3. No open-source baselines: To our knowledge, no publicly available PRM/ORM models exist for GUI agents or visual reasoning tasks, preventing fair comparison. Conceptual Relationship. SSL can be viewed as lightweight alternative to PRMs that leverages task structure (e.g., spatial distance, block-wise matching) instead of learned models. While PRMs learn quality assessment from data, SSL derives it from geometric or structural proximity to ground truth. This makes SSL: More sample-efficient (no reward model training needed) More interpretable (zone thresholds have clear semantic meaning) But potentially less flexible for tasks where proximity is hard to define Future Work. We plan to train task-specific PRMs/ORMs for GUI domains using our collected trajectories and sweetspot annotations as weak supervision. Comparing learned reward models with SSLs structured rewards could reveal complementary strengths: PRMs may capture subtle quality nuances beyond spatial/structural proximity, while SSL provides stronger inductive biases for geometric tasks. Hybrid approaches that combine both paradigms represent promising direction. Figure 8. step-wise illustration of our GUI agent executing multi-turn instruction 24 SSL: Sweet Spot Learning for Differentiated Guidance in Agentic Optimization Figure 9. Maze-solving task that is constructed as 3030 symbolic grid. The left panel shows the raw input grid, and the right panel shows the models predicted solution path. Figure 10. ARC-style pattern induction task. The model receives pairs of inputoutput grids, then must infer the underlying transformation and apply it to new test input grid."
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Sciences",
        "Nanyang Technological University",
        "Tsinghua University",
        "Xiaomi Corporation",
        "Zhejiang University"
    ]
}