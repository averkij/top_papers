{
    "paper_title": "DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization",
    "authors": [
        "Zhenglin Zhou",
        "Xiaobo Xia",
        "Fan Ma",
        "Hehe Fan",
        "Yi Yang",
        "Tat-Seng Chua"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-3D generation automates 3D content creation from textual descriptions, which offers transformative potential across various fields. However, existing methods often struggle to align generated content with human preferences, limiting their applicability and flexibility. To address these limitations, in this paper, we propose DreamDPO, an optimization-based framework that integrates human preferences into the 3D generation process, through direct preference optimization. Practically, DreamDPO first constructs pairwise examples, then compare their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D representation with a preference-driven loss function. By leveraging pairwise comparison to reflect preferences, DreamDPO reduces reliance on precise pointwise quality evaluations while enabling fine-grained controllability through preference-guided optimization. Experiments demonstrate that DreamDPO achieves competitive results, and provides higher-quality and more controllable 3D content compared to existing methods. The code and models will be open-sourced."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 0 7 3 4 0 . 2 0 5 2 : r DreamDPO: Aligning Text-to-3D Generation with Human Preferences via Direct Preference Optimization"
        },
        {
            "title": "A PREPRINT",
            "content": "Zhenglin Zhou1 Xiaobo Xia2 1Zhejiang University {zhenglinzhou, hehefan, yangyics}@zju.edu.cn Fan Ma3 Hehe Fan1 2National University of Singapore Yi Yang1 Tat-Seng Chua 3Yale University {xbx, dcscts}@nus.edu.sg flowerfan524@gmail.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Text-to-3D generation automates 3D content creation from textual descriptions, which offers transformative potential across various fields. However, existing methods often struggle to align generated content with human preferences, limiting their applicability and flexibility. To address these limitations, in this paper, we propose DreamDPO, an optimization-based framework that integrates human preferences into the 3D generation process, through direct preference optimization. Practically, DreamDPO first constructs pairwise examples, then compare their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D representation with preference-driven loss function. By leveraging pairwise comparison to reflect preferences, DreamDPO reduces reliance on precise pointwise quality evaluations while enabling fine-grained controllability through preference-guided optimization. Experiments demonstrate that DreamDPO achieves competitive results, and provides higher-quality and more controllable 3D content compared to existing methods. The code and models will be open-sourced. DreamDPO Webpage."
        },
        {
            "title": "Introduction",
            "content": "3D content generation is pivotal in driving innovation across diverse fields, including product design, medical imaging, scientific visualization, and the rapidly growing domains of virtual and augmented reality [1]. Despite its extensive applications, it remains challenging to create high-quality 3D content, which requires substantial time and effort, even for professionals. In response, text-to-3D generation has emerged as solution by automating 3D generation from textual descriptions, which archives remarkable advancements in the field [2, 3, 4, 5, 6, 7, 8, 9, 10]. Nevertheless, some researchers [11, 12] emphasize that 3D content generated by existing methods often fails to align with human preferences fully, highlighting the need for continued refinement and innovation in these methods. Previous work [12] has leveraged reward models to integrate human preferences into the generation process, leading to enhanced 3D generation outcomes. The core idea is to regularize the generated 3D content to achieve high pointwise score from the reward model. Despite these improved results, several issues remain to be addressed. First, it heavily depends on the reward models ability to accurately evaluate the pointwise quality of generated content, which places significant demands on the reward model. Second, since the reward model can only provide quality-relevant scores, it lacks the flexibility to enable controllability from other perspectives. The issues reduce the applicability and adaptability of the current method. This falls short of meeting diverse requirements or providing broader control in 3D generation, which is never our desideratum. To relieve the issues of prior work and better align 3D generation with human preferences, we propose DreamDPO. Essentially, DreamDPO is an optimization-based method for text-to-3D generation. It achieves alignment through direct preference optimization, leveraging preferences derived from either reward models or large multimodal models. Specifically, DreamDPO operates by initializing 3D representation [13, 14] and optimizing it through three-step iterative process. First, pairwise examples are constructed on-the-fly by applying different Gaussian noise. Second, reward model or large multimodal model ranks these examples based on their matching with the input text prompt Corresponding author."
        },
        {
            "title": "A PREPRINT",
            "content": "Figure 1: Overview of our method. DreamDPO first constructs pairwise examples, then compares their alignment with human preferences using reward or large multimodal models, and lastly optimizes the 3D presentation with preference-driven loss function. The loss function pulls the win example xwin closer and pushes the lose example xlose away. As piecewise objective, it selectively pushes xlose only when the preference score gap sgap exceeds threshold τ , preventing chaotic gradients from overly similar xlose . or specific instructions, which matches human preferences about the pairwise examples. Finally, reward loss is computed from the pairwise preferences, which guides the update of the 3D representation. By incorporating human preferences into the optimization loop, DreamDPO generates 3D assets that achieve superior alignment with textual inputs, along with enhanced texture and geometry quality. DreamDPO can be justified as follows. While absolute quality evaluation inherently provides ranking on pairwise examples, ranking requires only the scores to distinguish relative preferences, but need not be perfectly accurate (c.f., [16]). DreamDPO takes advantage of this distinction, and changes the previous score-guided optimization [12] to preference-guided optimization. Therefore, it lowers the demand for precise scoring and requires only distinguishable scores. Additionally, DreamDPO can make use of the preferences provided by large multimodal models. By constructing preferred and less preferred examples based on specific instructions about the attributes of generation content (e.g., the object number and motion), it directs the optimization process to align more closely with the desired outcomes. This strategy enhances adherence to instructions and introduces fine-grained controllability, meeting diverse requirements effectively. Moreover, we conduct series of experiments to justify our claims. Empirical results demonstrate that our method flexibly accommodates either reward models or large multimodal models, which enables the generation of higher-quality and more controllable 3D content. Before delving into details, we clearly emphasize our contribution as follows. Conceptually, we propose DreamDPO that shifts the paradigm of text-to-3D generation by reducing dependence on precise absolute quality evaluation. Instead, it leverages distinguishable relative preferences and integrates large multimodal models, which achieves remarkable alignment with human preferences while enabling fine-grained controllability. Technically, DreamDPO pioneers three-step optimization process, combining online pair construction, preference ranking, and ranking-driven updates. This innovative design ensures superior alignment with input prompts, significantly enhancing the quality and adaptability of generated 3D content. Empirically, extensive results establish DreamDPO as new benchmark for text-to-3D generation. Both quantitative and qualitative results are thoroughly analyzed, supported by detailed discussions and ablation studies. DreamDPO outperforms 13 state-of-the-art methods, achieving the best quantitative performance across two key metrics while delivering highly impressive and controllable qualitative results. The rest of this paper is organized as follows. Section 2 introduces some background knowledge relevant to this work. Section 3 presents the technical details of our proposed DreamDPO. Experimental results are analyzed and discussed in Section 4. Conclusions are given in Section 5. Reward models are trained using pairwise data reflecting human preferences, while large multimodal models are inherently aligned with these preferences [15]."
        },
        {
            "title": "2 Preliminaries",
            "content": "Text-to-3D generation aims to create high-quality 3D assets aligned with given text prompt y. The pipeline typically distills knowledge from parametrized diffusion model ϵϕ [17, 7] into learnable 3D representation with parameters θ Θ (e.g., NeRF [13], DMTet [18], and 3DGS [14]), where Θ is the space of θ with the Euclidean metric. Score distillation sampling (SDS) [2] is used to guide the distillation process. Diffusion models. The diffusion model has been widely used in generative tasks [19, 20, 21, 22]. Generally, it involves forward process to gradually add Gaussian noise to data points and reverse process to transform Gaussian noise into data points from target distribution pdata. The reverse process starts from an initial noise zT (0, I). At each diffusion step t, the model refines noisy data zt into cleaner one zt1 until finally producing z0 = pdata. Therefore, the transitions p(zt1zt) can be learned effectively by the diffusion model. Score distillation sampling (SDS). SDS was proposed in DreamFusion [2], and has been widely studied [3, 5, 8, 9, 10, 23, 12]. Technically, for rendered image from 3D representation, random noise ϵ is added at timestep t. pre-trained diffusion model predicts this noise. The SDS loss is computed as the difference between predicted and added noise, which optimizes set of parameters θ. The gradient of the SDS loss with respect to θ is: θLSDS = Et,ϵ[w(t)(ϵs ϕ(xt; y, t) ϵ) θ ], (1) where xt = αtx0 +σtϵ, w(t) is weighting function, and is pre-defined scalar of classifier-free guidance (CFG) [24]. The minimization of the SDS loss follows the score function of the diffusion model to move to the text description region, ensuring the generated 3D representation aligns with the given text prompt. Note that to make continuous and smooth presentation, we provide detailed review of related work in Appendix A."
        },
        {
            "title": "3 Method",
            "content": "Overview. DreamDPO is an optimization-based text-to-3D generation method. It begins by initializing 3D representation, e.g., NeRF [13]. In each training iteration, the optimization procedure involves three key steps: (1) pairwise example construction: pairwise examples are generated online by applying different Gaussian noises during the diffusion process; (2) pairwise example comparison: reward model or large multimodal model (LMM) compares the generated examples based on their alignment with the desired text prompt; and (3) preference-guided optimization: piecewise reward loss is calculated using the pairwise comparison, and the 3D representation is updated accordingly. Overall, our DreamDPO guides the optimization process with human preferences, leading to 3D assets with improved alignment to input text and enhanced texture/geometry quality. The framework overview of our method is provided in fig. 1. complete algorithm flow of our method can be checked in Appendix B. 3.1 Algorithm Details Pairwise example construction. Given sampled camera pose, an RGB image can be rendered from the 3D representation using renderers. Then two different Gaussian noise ϵ1 and ϵ2 are added to at timestep t, resulting in pairwise noisy images x1 and x2 : = αtx0 + σtϵ1, x2 x1 = αtx0 + σtϵ2, (2) where x0 = x, αt and σt are hyperparameters satisfying α0 1, σ0 0, α0 0, σ0 1 (c.f ., [19, 38]). Afterward, we feed the pairwise noisy images into pre-trained text-to-image diffusion model ϵϕ [7, 17] and generate corresponding predictions: x1 x2 ˆx1 0 = ˆx2 0 = ; y, t) ; y, t) 1 αtϵϕ(x1 αt 1 αtϵϕ(x2 αt and x2 , , (3) where ˆx1 0 and ˆx2 0 are predicted x0 of single step for x1 , respectively [39]. Pairwise comparison. After pair construction, at step t, we utilize rank model denoted by r() to compare x1 and less preferred one xlose This yields preferred prediction xwin , where xwin and xlose = x2 and x2 . , or vice versa. = t"
        },
        {
            "title": "A PREPRINT",
            "content": "Table 1: Qualitative comparisons on 110 prompts generated by GPTEval3D [25]. We calculate the ImageReward score (IR) [26] for human preference evaluation, the CLIP score [27] for text-image alignment evaluation, and GPTEval3D [25] for comprehensive 3D quality evaluation. The best performance in each case is shown in bold."
        },
        {
            "title": "Method",
            "content": "DreamFusion [2] DreamGaussian [28] Fantasia3D [29] Instant3D [30] Latent-NeRF [31] Magic3D [32] Point-E [33] ProlificDreamer [3] Shap-E [34] SJC [35] SyncDreamer [36] Wonder3D [37] MVDream [7] DreamDPO (ours) IR -1.51 -1.56 -1.40 -0.91 -0.42 -1.11 -2.24 -0.50 -2.10 -0.82 -1.77 -1.70 -0.58 -0.35 Alignment Plausibility T-G Coherency. Geo Details Tex Details Overall GPTEval3D 1000.0 1100.6 1067.9 1200.0 1222.3 1152.3 725.2 1261.8 842.8 1130.2 1041.2 985.9 1270.5 1298. 1000.0 953.6 891.9 1087.6 1144.8 1000.8 689.8 1058.7 842.4 995.1 968.8 941.4 1147.5 1171.9 1000.0 1158.6 1006.0 1152.7 1156.7 1084.4 688.6 1152.0 846.0 1033.5 1083.1 931.8 1250.6 1276.4 1000.0 1126.2 1109.3 1152.0 1180.5 1178.1 715.7 1246.4 784.4 1079.9 1064.2 973.1 1324.9 1373. 1000.0 1130.8 1027.5 1181.3 1160.8 1084.6 745.5 1180.6 862.9 1042.5 1045.7 967.8 1255.5 1296.9 1000.0 951.4 933.5 1097.8 1178.7 961.7 618.9 1012.5 843.8 993.8 963.5 970.9 1097.7 1203.1 It is worth noting that our DreamDPO supports both reward models [26, 40] and LMM-based AI annotators [41, 42], where reward models are used as default. Preference-guided optimization. The proposed method leverages the pairwise comparison (xwin ) to enable efficient sampling via optimization to yield human preferred 3D assets. To achieve this, we need differentiable loss function, where preferred images have low losses and less preferred images have high losses. To this end, inspired by [43, 44, 45], we reformulate SimPO [44] to eliminate the need for reference model and derive differentiable objective: , xlose LReward = Et (cid:2)w(t) (cid:0)ϵwin ϵϕ(xwin and xlose ; y, t) 2 ϵlose ϵϕ(xlose ; y, t)2 (4) 2 respectively. Intuitively, LReward encourages ϵϕ to pull (cid:1)(cid:3) , where ϵwin and ϵlose denote Gaussian noise for xwin xwin closer and push xlose further away. Following [2], we consider the gradient of LReward and omit the U-Net Jacobian term for effective optimization. It leads to the following gradient for optimizing 3D representations with preference pairwise comparisons: θLReward = Et (cid:20) w(t) (cid:0)ϵϕ(xwin ; y, t) ϵwin(cid:1) (cid:0)ϵϕ(xlose ; y, t) ϵlose(cid:1) θ (cid:21) . (5) However, in practice, the gradient in eq. (5) fails to produce realistic results (refer to fig. 6). Delving into the optimization process, we observe that the pairwise comparison results can be overly similar, leading to nearly equal scores. In this case, directly pushing xlose away leads to chaotic gradients. To address this, we introduce piecewise optimization loss that selectively pulls xwin when the preference score gap sgap is small. The gradient of the final loss is defined as: θLReward* := (cid:40)Et Et (cid:105) (cid:16) (cid:104) w(t) (cid:2)w(t) (cid:0)win := ϵs ϵs ϕ(xwin ; y, t) lose , (cid:3) , ; y, t) ϵwin, and lose sgap < τ, otherwise. (cid:17) θ (cid:1) θ (6) ; y, t) ϵlose. Note where τ = 0.001 is pre-defined threshold, win that sgap := r(xwin , y) r(xlose . Instruction questions can also be incorporated into LMM-based ranking models to provide explicit guidance (see empirical evaluations in 4.3). ϕ(xlose , y) indicates the discrepancy of preference scores between xwin and xlose ϕ(xwin := ϵ1 t t"
        },
        {
            "title": "4 Experiments",
            "content": "In this section, series of experiments are conducted to justify our claims. We first detail experiment setups (4.1). The comprehensive results and comparison with previous advanced methods are then presented and discussed (4.2). Finally, we carry out an analysis study to further elaborate on and discuss the superiority of our method (4.3)."
        },
        {
            "title": "A PREPRINT",
            "content": "Figure 2: Qualitative comparisons on the benchmark of GPTEval3D [25]. Existing methods struggle with text matching, as marked in red. DreamDPO improves text matching, which provides better human preference results. (Zoom in to see the details.) 4.1 Experimental Setups Datasets and measurements. We here evaluate the proposed method with 110 prompts from GPTEval3D [25], which covers range of creativity and complexity use cases. Based on this, two evaluation strategies are exploited. (1) We utilize text-to-image reward model named ImageReward [26] to evaluate human preference for 3D assets. We calculate the average preference score across 120 rendered images of 3D asset and its corresponding text prompt. (2) We use GPT-4V to perform pairwise comparisons with baselines, generating Elo ratings that align with human judgments on text alignment, 3D plausibility, and texture-geometry coherence, etc. More details of the two measurements can be found in Appendix C.1. Baselines. Following GPTEval3D [25], we benchmark our method against 13 baselines categorized into text-guided and image-guided approaches respectively. Specifically, the text-guided group includes DreamFusion [2], DreamGaussian [28], Instant3D [30], Fantasia3D [29], Latent-NeRF [31], Magic3D [32], MVDream [7], Point-E [33], ProlificDreamer [6], Shap-E [34], and SJC [35]. Besides, the image-guided group includes SyncDreamer [36] and Wonder3D [37]. Implementation. We conduct experiments using PyTorch [46] and threestudio [47], with MVDream [7] as the backbone of our method. Note that we use PyTorch auto-differentiation to compute analytic normals for geometry evaluation in GPTEval3D and do not use the Lambertian shading trick [32] due to memory limitation. We follow the training strategy of MVDream and use HPSv2 [40] as the default reward model. The optimization process takes around two hours on single NVIDIA RTX A6000 GPU. 4.2 Comparison with Prior Methods 4.2.1 Qualitative Comparisons We conduct two qualitative evaluations, which include comparing 13 benchmarks of GPTEval3D, MVDream [7], and DreamReward [12]. The evaluations show improvements in text alignment, generation stability, and texture-geometry details, respectively. As seen in fig. 2, while the comparing baselines produce high-fidelity results, they often fail in text alignment, as marked in red. For instance, in the prompt small, rustic cabin sits alone in peaceful, snow-covered forest, most existing methods miss key elements like the forest (the first row in fig. 2). In contrast, our method accurately captures both objects, showcasing its effectiveness for improving text alignment. Further comparisons"
        },
        {
            "title": "A PREPRINT",
            "content": "Figure 3: Qualitative comparisons with MVDream [7]. DreamDPO performs well across short to long prompts, offering better human preference results, marked in red. (Zoom in to see the details.) with MVDream, are shown in fig. 3. Although MVDream is capable of generating multiview consistent 3D assets, it struggles with long prompts (e.g., the second and fourth rows in fig. 3). Instead, our method performs well across both short to long prompts. Lastly, the comparisons with DreamReward demonstrate that our method not only improves text alignment (e.g., ensuring leaves trail of flowers appears under the bicycle shown in the first row in fig. 7) but also enhances geometric and texture details (e.g., generating more luxuriant oak shown in the second row in fig. 7). More visualization results can be found in Appendix D.1. 4.2.2 Quantitative Comparisons We provide extensive quantitative comparison results to justify our claims. Human preference evaluations are first conducted using ImageReward, as illustrated in the first column of table 1. Our method achieves competitive performance compared to existing methods, highlighting the benefits of preference-guided optimization via our reward loss. Then we perform comprehensive evaluation using GPTEval3D. The results indicate that our method outperforms previous state-of-the-art (SOTA) methods, and ranks first across all metrics. Specifically, our method achieves improvements in text-asset alignment (+28.4), 3D plausibility (+24.4), text-geometry alignment (+25.8), texture details (+48.4), geometry details (+41.4), and overall performance (+24.4). It showcases the superiority of the proposed method in enhancing text and geometry details while maintaining 3D consistency. 4.3 More Analyses and Justifications Evaluation on different backbones. We here further investigate the impact of backbone selection on our method. The performance of DreamDPO using Stable Diffusion v2.1 (SD2.1) [17] is provided. Note that as the previous method ProlificDreamer [3] also utilizes SD2.1, we compare DreamDPO with SD2.1 against ProlificDreamer. As shown"
        },
        {
            "title": "A PREPRINT",
            "content": "Figure 4: The analysis of backbone. We present the results of DreamDPO using Stable Diffusion v2.1 (SD2.1) [17]. DreamDPO demonstrates effective performance with SD2.1, highlighting its potential to leverage more advanced backbone diffusion models for further improvements. Figure 5: The analysis of reward models. We present the results of DreamDPO using ImageReward [26]. DreamDPO demonstrates effective performance with ImageReward, highlighting its potential to leverage stronger reward models to further enhance generation quality. Figure 6: The analysis of the score gap threshold τ . We conduct 2D toy experiments with τ ranging from 0.01 to 0. The results indicate that small but non-zero τ effectively filters out overly similar lose examples, leading to more detailed outputs. in fig. 4, our method can perform effectively with SD2.1 and achieve competitive results compared to ProlificDreamer. Importantly, DreamDPO does not need LoRA training and is more efficient than ProlificDreamer. Evaluation on different reward models. We study the impact of reward model selection on our method. Specifically, ImageReward [26] is used, which is an image-based reward model with similarity function comparable to HPSv2 [40]. As shown in fig. 5, the results demonstrate that our method performs effectively across different reward models, demonstrating its flexibility and scalability. For instance, in the prompt mug filled with steaming coffee, both HPSv2 and ImageReward successfully capture the coffee, and ImageReward places greater emphasis on the steam. While ImageReward demonstrates improvement over the baseline, HPSv2 yields superior results due to its better generalization across diverse image distributions [40]. Therefore, we adopt HPSv2 as our default reward model. Also, it highlights the potential of leveraging stronger reward models, e.g., Reward3D [12] and VisionReward [49], to further enhance generation quality. Evaluation on different score gaps. We investigate the impact of the score gap τ . Specifically, 2D toy experiments with τ range from 0.01 to 0 are conducted. We provide results in fig. 6, which show that smaller τ produces more detailed outputs. Note that small τ means choosing to lose examples with scores close to win samples, and focusing the training process on hard cases. However, we observe that τ = 0 (the last column in fig. 4) results in chaotic gradient. To balance high-fidelity generation and stable training, we suggest using small but non-zero τ , which excludes overly similar lose examples. Evaluation on different pair examples. The influence of different pair example generation methods is studied. Specifically, we compare: (1) different noises, by adding different Gaussian noises with the same timesteps; (2) difference steps, by adding the same Gaussian noise with different timesteps. As shown in fig. 9, using different Gaussian noise yields better results than different timesteps. We attribute that noisy latents with different timesteps are easier to distinguish, making them less effective as challenging examples. It highlights the importance of generating meaningful and challenging lose examples. Accordingly, we adopt different noises as the default setting. Ranking model design. We explore the potential of our method to leverage large multi-modal models (LMMs) for explicit guidance. Instead of relying on reward model, we use large visual-language models, such as QwenVL [41], to rank paired results. Specifically, we extract yes or no questions from the text prompt, query the LMM with rendered paired examples, and count the yes responses to calculate the reward score. Then, we use eq. (6) with threshold τ ="
        },
        {
            "title": "A PREPRINT",
            "content": "Figure 7: Qualitative comparisons with DreamReward [12]. DreamDPO improves both text matching (marked in red) and geometric/texture details. for 3D assets generation. As shown in fig. 8, our method effectively improves text alignment by using LMM to guide the optimization with user instruction (e.g., correcting the number and attribute of 3D assets). Additionally, our method flexibly supports using lose examples to guide optimization. For example, given the prompt dancing elephant, the baseline generates an elephant standing rather than dancing. By setting elephant stays on the ground as the lose example and pushing it away, our method encourages the elephant to lift its leg, leading to dancing pose. It highlights the potential of our method to integrate LMMs into 3D generation. More implementation details of the LMMs-based comparison can be found in Appendix B.2. Further application. To showcase the potential of our method, we provide empirical results on text-to-avatar generation. In specific, we replace the score sampling loss in HeadStudio [48] which is 3DGS [14] based avatar generation framework, with our reward loss. As illustrated in fig. 10, our method achieves great generation results. This underscores the broader applicability of our method to various generation tasks, e.g., 4D generation [50] and scene generation [51], etc."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we propose DreamDPO, an optimization-based 3D generation method that offers human preferences and fine-grained control for generation. The method is built on three key steps: pairwise example construction, pairwise example comparison, and preference-guided optimization. Unlike existing methods that rely on precise pointwise quality evaluations, DreamDPO uses pairwise comparison informed by reward models or large multimodal models. It enables more flexible optimization process. By incorporating human preferences directly into the optimization,"
        },
        {
            "title": "A PREPRINT",
            "content": "Figure 8: The generation results of DreamDPO with large multi-modal models (LMMs). We explore the potential of our method to leverage LMMs, such as QwenVL [41] for explicit guidance in correcting the number and attribute of 3D assets. The left corner shows the details of pairwise comparisons using the LMM, including the question and win/lose criteria. By carefully designing the question, DreamDPO can leverage both win and lose examples to guide optimization. (Zoom in to see the details.) Figure 9: The analysis of pairwise example construction. We compare (1) different noises: adding different Gaussian noises with the same timesteps, and (2) difference timesteps: adding the same Gaussian noise with different timesteps. Figure 10: The further application of DreamDPO. We conduct toy experiments on text-to-avatar generation by combining DreamDPO with Gaussian-based avatar generation framework [48]. More details can be checked in Appendix B.3. DreamDPO generates 3D assets that are better aligned with input text and exhibit enhanced texture and geometry quality. Comprehensive experimental results demonstrate that DreamDPO surpasses previous state-of-the-art methods in both output quality and controllability. Lastly, we hope DreamDPO paves the way for more refined, adaptable, and human-aligned 3D content generation solutions. Limitations and future work. While DreamDPO has shown improvements in aligning 3D generation with human preferences, several avenues for future research could further enhance its performance and applicability. The primary limitations of DreamDPO are as follows: (1) AI feedback can be used for guidelines but is largely limited to the inherent power of generative models. (2) Open API can provide more freedom but actually bring more instability, where instruction prompts should be designed carefully. To address these limitations, we suggest the following directions for future work: (1) Enhancing generative models. Incorporating image prompts [52] to introduce explicit guidance could improve alignment with user expectations by providing more detailed context for generation. (2) Improving the robustness of models in pairwise comparison. Exploring prompt-free methods, such as leveraging object detection models [53] or grounding models [54], for number and attribute correction, might reduce dependencies on prompt design. Additionally, using the diffusion model itself as model for pairwise comparison [55] could enhance stability and performance by ensuring consistency across the generation and comparison."
        },
        {
            "title": "References",
            "content": "[1] Chenghao Li, Chaoning Zhang, Joseph Cho, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho Bae, and Choong Seon Hong. Generative ai meets 3d: survey on text-to-3d in aigc era. arXiv preprint arXiv:2305.06131, 2023. [2] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [3] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023. [4] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In CVPR, 2022. [5] Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, and Xiaojuan Qi. Text-to-3d with classifier score distillation. arXiv preprint arXiv:2310.19415, 2023. [6] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. In NeurIPS, 2024. [7] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512, 2023. [8] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani Lischinski. Noise-free score distillation. arXiv preprint arXiv:2310.17590, 2023. [9] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes. In CVPR, 2024. [10] Zike Wu, Pan Zhou, Xuanyu Yi, Xiaoding Yuan, and Hanwang Zhang. Consistent3d: Towards consistent high-fidelity text-to-3d generation with deterministic sampling prior. In CVPR, 2024. [11] Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Yi Zhou, Sai Bi, Soren Pirk, and Arie Kaufman. Carve3d: Improving multi-view reconstruction consistency for diffusion models with rl finetuning. In CVPR, 2024. [12] Junliang Ye, Fangfu Liu, Qixiu Li, Zhengyi Wang, Yikai Wang, Xinzhou Wang, Yueqi Duan, and Jun Zhu. Dreamreward: Text-to-3d generation with human preference. In ECCV, pages 259276, 2025. [13] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. [14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. [15] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. [16] Zhen-Yu Zhang, Siwei Han, Huaxiu Yao, Gang Niu, and Masashi Sugiyama. Generating chain-of-thoughts with direct pairwise-comparison approach to searching for the most promising intermediate thought. In ICML, 2024. [17] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. [18] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis. In NeurIPS, pages 60876101, 2021. [19] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, pages 22562265, 2015. [20] Kunpeng Song, Ligong Han, Bingchen Liu, Dimitris Metaxas, and Ahmed Elgammal. Diffusion guided domain adaptation of image generators. arXiv preprint arXiv:2212.04473, 2022. [21] Fan Bao, Chongxuan Li, Jun Zhu, and Bo Zhang. Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models. In ICLR, 2022. [22] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. [23] Wenjie Zhuo, Fan Ma, Hehe Fan, and Yi Yang. Vividdreamer: invariant score distillation for hyper-realistic text-to-3d generation. In ECCV, 2024. [24] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022."
        },
        {
            "title": "A PREPRINT",
            "content": "[25] Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Wetzstein. Gpt-4v (ision) is human-aligned evaluator for text-to-3d generation. In CVPR, pages 2222722238, 2024. [26] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. In NeurIPS, 2024. [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. [28] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. [29] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In ICCV, pages 2224622256, 2023. [30] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. [31] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In CVPR, pages 1266312673, 2023. [32] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In CVPR, pages 300309, 2023. [33] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: system for generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022. [34] Heewoo Jun and Alex Nichol. arXiv:2305.02463, 2023. Shap-e: Generating conditional 3d implicit functions. arXiv preprint [35] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In CVPR, pages 1261912629, 2023. [36] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. [37] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In CVPR, pages 99709980, 2024. [38] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, volume 33, pages 68406851, 2020. [39] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. [40] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. [41] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [42] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023. [43] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2024. [44] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. arXiv preprint arXiv:2405.14734, 2024. [45] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In CVPR, 2024. [46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019."
        },
        {
            "title": "A PREPRINT",
            "content": "[47] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-Xin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: unified framework for 3d content generation. https://github.com/threestudio-project/threestudio, 2023. [48] Zhenglin Zhou, Fan Ma, Hehe Fan, Zongxin Yang, and Yi Yang. Headstudio: Text to animatable head avatars with 3d gaussian splatting. In ECCV, 2024. [49] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, Jiayan Teng, Zhuoyi Yang, Wendi Zheng, Xiao Liu, Ming Ding, Xiaohan Zhang, Xiaotao Gu, Shiyu Huang, Minlie Huang, Jie Tang, and Yuxiao Dong. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation, 2024. [50] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, and David Lindell. 4d-fy: Text-to-4d generation using hybrid score distillation sampling. In CVPR, pages 79968006, 2024. [51] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. Text2nerf: Text-driven 3d scene generation with neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 2024. [52] Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, and Tao Mei. Vp3d: Unleashing 2d visual prompt for text-to-3d generation. In CVPR, pages 48964905, 2024. [53] Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio Torralba, Hengshuang Zhao, and Shengjin Wang. Detecting everything in the open world: Towards universal object detection. In CVPR, pages 1143311443, 2023. [54] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [55] Junjiao Tian, Lavisha Aggarwal, Andrea Colaco, Zsolt Kira, and Mar Gonzalez-Franco. Diffuse attend and segment: Unsupervised zero-shot segmentation using stable diffusion. In CVPR, pages 35543563, 2024. [56] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [57] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [58] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion model in generative ai: survey. arXiv preprint arXiv:2303.07909, 2023. [59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 1068410695, 2022. [60] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-guided text-to-image diffusion models. In SIGGRAPH, pages 111, 2023. [61] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, 2023. [62] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [63] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arxiv:2208.12242, 2022. [64] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, pages 1839218402, 2023. [65] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In CVPR, pages 54705479, 2022. [66] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In CVPR, 2022. [67] Chenghao Li, Chaoning Zhang, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho Bae, and Choong Seon Hong. Generative ai meets 3d: survey on text-to-3d in aigc era. arXiv preprint arXiv:2305.06131, 2023."
        },
        {
            "title": "A PREPRINT",
            "content": "[68] Junzhe Zhu, Peiye Zhuang, and Sanmi Koyejo. Hifa: High-fidelity text-to-3d generation with advanced diffusion guidance. arXiv preprint arXiv:2305.18766, 2023. [69] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, pages 1314213153, 2023. [70] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. In NeurIPS, volume 36, 2024. [71] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, pages 92989309, 2023. [72] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. In NeurIPS, volume 36, 2024. [73] Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes, and Daniel Cohen-Or. Set-the-scene: Global-local training for generating controllable nerf scenes. arXiv preprint arXiv:2303.13450, 2023. [74] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. arXiv preprint arXiv:2303.11989, 2023. [75] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing 3d scenes with instructions. In ICCV, 2023. [76] Hiromichi Kamata, Yuiko Sakuma, Akio Hayakawa, Masato Ishii, and Takuya Narihira. Instruct 3d-to-3d: Text instruction guided 3d-to-3d conversion. arXiv preprint arXiv:2303.15780, 2023. [77] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-Yee Wong. Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models. arXiv preprint arXiv:2304.00916, 2023. [78] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai, Mingming He, Dongdong Chen, and Jing Liao. Avatarcraft: Transforming text into neural human avatars with parameterized shape and pose control. arXiv preprint arXiv:2303.17606, 2023. [79] Xiao Han, Yukang Cao, Kai Han, Xiatian Zhu, Jiankang Deng, Yi-Zhe Song, Tao Xiang, and Kwan-Yee Wong. Headsculpt: Crafting 3d head avatars with text. arXiv preprint arXiv:2306.03038, 2023. [80] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [81] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [82] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Ren Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf: Scaling reinforcement learning from human feedback with ai feedback. In ICML, 2024. [83] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf. arXiv:2309.14525, 2023. [84] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In CVPR, pages 1380713816, 2024. [85] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. [86] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023. [87] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023. [88] Kevin Clark, Paul Vicol, Kevin Swersky, and David Fleet. Directly fine-tuning diffusion models on differentiable rewards. arXiv preprint arXiv:2309.17400, 2023."
        },
        {
            "title": "A PREPRINT",
            "content": "[89] Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, Kangwook Lee, and Kimin Lee. Reinforcement learning for fine-tuning text-to-image diffusion models. In NeurIPS, 2024. [90] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In CVPR, pages 90269036, 2024. [91] Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In NeurIPS, volume 30, 2017. [92] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. In NeurIPS, 2022."
        },
        {
            "title": "Appendix",
            "content": "1 Introduction 2 Preliminaries 3 Method"
        },
        {
            "title": "3.1 Algorithm Details .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experiments"
        },
        {
            "title": "4.1 Experimental Setups",
            "content": ". . . . . ."
        },
        {
            "title": "4.2 Comparison with Prior Methods",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 Qualitative Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.2 Quantitative Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 More Analyses and Justifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Conclusion Related Work A.1 Text-to-Image Generation . A.2 Text-to-3D Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Learning from Human Preferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Implementation Details B.1 Pseudo-Code for DreamDPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Details of LMM-based Pairwise Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Details of Text-to-Avatar Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Supplementary Experimental Settings C.1 Details of Measurement Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Supplementary Experimental Results D.1 More Qualitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 3 3 4 5 5 5 6 8 16 16 16 16 16 17 18 18 18"
        },
        {
            "title": "A Related Work",
            "content": "A.1 Text-to-Image Generation With the development of vision-language models [27] and diffusion models [19, 38], great advancements recently have been made in text-to-image generation [56, 57, 58]. In particular, Stable Diffusion [59] is notable framework that trains the diffusion models on latent space, leading to reduced complexity and detailed preservation. In addition, with the emergence of text-to-2D models, more applications have been developed, e.g., spatial control [60, 61], concept control [62, 63], and image editing [64]. A.2 Text-to-3D Generation The success of the 2D generation is incredible. However, it is challenging to transfer image diffusion models to 3D because of the difficulty of 3D data collection. Fortunately, Neural Radiance Fields (NeRF) [13, 65] provided new insights for 3D-aware generation, where only 2D multi-view images are needed in 3D scene reconstruction. Combining prior knowledge from text-to-2D models, several methods, such as DreamField [66], DreamFusion [2], and SJC [4], have been proposed to generate 3D objects guided by text prompts [67]. However, the vanilla score distillation sampling loss [2] suffers from issues such as over-saturation, over-smoothing, and Janus problems, etc. Recently, several works have proposed improvements to enhance generation quality [3, 5, 68, 8, 9, 10, 23]. Additionally, with the availability of large 3D datasets [69, 70], some works [71, 7, 72, 36, 37] leverage multi-view information to address the Janus problem more effectively. Moreover, the recent advancement of text-to-3D generation also inspired multiple applications that include but are not limited to text-guided scene generation [73, 74], text-guided 3D editing [75, 76], and text-guided avatar generation [77, 78, 79, 48]. A.3 Learning from Human Preferences Learning from human preferences is essential for improving the alignment and performance of generative models across various domains, including large language models (LLMs) [80, 81, 82], large multimodal models (LMMs) [83, 84, 85], text-to-image diffusion models [86, 87, 88, 26, 45, 89, 90], and text-to-3D generation [11, 12]. Reinforcement Learning from Human Feedback (RLHF) [91] has been proven effective in refining LLMs to better align with human preferences. It often involves collecting human feedback datasets, training reward model, and fine-tuning the language model using reinforcement learning. InstructGPT [92] is notable work that employs two-stage fine-tuning strategy to align GPT-3 with human instructions, which leads to more coherent and contextually appropriate outputs. The success of RLHF in LLMs has inspired the applications in text-to-image diffusion models to enhance image generation quality and align with human preferences. In more detail, RWR [87] first introduces human feedbackbased reward fine-tuning for diffusion models, which fine-tunes Stable Diffusion [17] using log probabilities of the denoising process. ImageReward [26] proposes reward model specifically for text-to-image tasks and further develops reward feedback Learning for refining diffusion models. DiffusionDPO [45] uses DPO to optimize diffusion models using human comparative data, while DPOK [89] integrates policy optimization with KL regularization for improved alignment. Recently, advancements in multi-view diffusion models have facilitated significant progress in text-to-3D generation. For instance, Carve3D [11] enhances text-to-3D generation with Multi-view Reconstruction Consistency (MRC) metric for improved consistency and quality. DreamReward [12] improves text-to-3D models using human feedback. It collects 3D dataset with human annotations, trains Reward3D as multi-view reward model, and introduces DreamFL to create 3D assets aligned with human preferences. However, these works rely heavily on large-scale datasets to train reward model or utilize it as preference feedback, which is very expensive for 3D generation."
        },
        {
            "title": "B Additional Implementation Details",
            "content": "B.1 Pseudo-Code for DreamDPO more detailed pseudo-code for DreamDPO is presented in algorithm 1. 16 Algorithm 1 Pseudo-code for DreamDPO Input: Text-to-image diffusion model ϵϕ. Ranking model r. Learning rate η for 3D representation parameters. prompt y. Evaluating threshold of score gap τ ."
        },
        {
            "title": "A PREPRINT",
            "content": "1: Initialization 3D representation presenting with NeRF θ 2: while not converged do 3: 4: 5: 6: Randomly sample camera pose c, pairwise 2D noise ϵ1 and ϵ2, and timestep Uniform({1, , }). Render at pose to get image x0. Add noise ϵ1 and ϵ2 to x0 and get x1 Denoise with the predicting noise: , respectively. and x2 x1 x2 ˆx1 0 = ˆx2 0 = 1 αtϵθ(x1 αt 1 αtϵθ(x2 αt ; y, t) ; y, t) , . 7: 8: 9: 10: 11: 12: Score the prediction (ˆx1 Compute the score gap: 0, ˆx2 0) online via rank model r, yielding the pairwise comparison (xwin , xlose ). if sgap < τ then else sgap = r(xwin , y) r(xlose , y). θLReward = Et (cid:20) w(t) (cid:0)ϵs ϕ(xwin ; y, t)(cid:1) θ (cid:21) , θLReward = Et (cid:20) w(t) (cid:0)(cid:0)ϵs ϕ(xwin ; y, t) ϵwin(cid:1) (cid:0)ϵ1 ϕ(xlose ; y, t) ϵlose(cid:1)(cid:1) θ (cid:21) . end if θ θ ηθLReward. 13: 13: 14: end while B.2 Details of LMM-based Pairwise Comparison We detail the implementation of the LMM-based pairwise comparison. We use the large visual-language model qwen-vl-plus-latest from QwenVL [41] as the default LMM. Given pairwise examples, we conduct the comparison query sequentially. For each query, we first insert predefined yes or no question into the comparison prompt, such as Is the leaf shouting? for the prompt shouting leaf. Then, the LMM performs visual question answering based on the provided image and query. Finally, we extract the number of yes responses as the score. The following prompts are used for the queries: Comparison Query [Task Description]: You are an expert in evaluating the alignment between given text description and an image. Your task is to answer each of the alignment questions with either Yes or No based on the image. Provide your responses in the format specified below. [Evaluation Instruction]: 1. Carefully analyze the provided image and answer questions based on the image. 2. For each question, answer with either Yes or No. Do not provide explanations or additional information. [Evaluation Question(s)]: Q1: {Question} ... [Output Format]: A1: [Yes/No] ..."
        },
        {
            "title": "A PREPRINT",
            "content": "B.3 Details of Text-to-Avatar Generation We detail the toy exploration of text-to-avatar generation using DreamDPO. Specifically, we integrate the reward loss into HeadStudio [48], Gaussian-based avatar generation framework. HeadStudio is an optimization-based method that utilizes score sampling loss [8] with ControlNet [61] to optimize an animatable head prior model. By replacing the score sampling loss with the reward loss, we leverage ControlNet to generate avatars."
        },
        {
            "title": "C Supplementary Experimental Settings",
            "content": "C.1 Details of Measurement Metrics In the main paper, we employ two evaluation strategies to demonstrate the superiority of the proposed method. Here we supplementary the details of the measurements. Evaluation with ImageReward. ImageReward [26] is text-to-image human preference reward model. Due to its effectiveness, it has been broadly used for human preference evaluation in text-to-image generation [89] and text-to-3D generation [12]. Given (text, image) pair, it extracts image and text features, combines them with cross-attention, and uses an MLP to generate scalar for preference comparison. For each 3D asset, we uniformly render 120 RGB images from different viewpoints. Afterward, the ImageReward score is computed from the multi-view renderings and averaged for each prompt. Evaluation with GPTEval3D. We utilize GPTEval3D [25], which is comprehensive benchmark for text-to-3D generation evaluation. GPTEval3D includes 13 baseline methods M, 110 text prompts, and 5 criteria that are text-asset alignment, 3D plausibility, texture details, geometry details, and texture-geometry coherency respectively. For new method, GPTEval3D employs GPT-4V to compare 3D assets generated by this new method and one of the baseline methods with the same input text prompt. These pairwise comparison results are then used to calculate the Elo rating for each model. Specifically, let be matrix where Aij represents the number of times that the i-th model outperforms the j-th model in comparisons. The Elo ratings for the models are computed by optimizing the following objective: σ = arg max σ (cid:88) i=j Aij log (cid:16) 1 + 10(σj σi)/400(cid:17) , (7) where σi is the Elo rating of the i-th model. In this work, we calculate Elo ratings within the existing tournament, initializing, and freezing baseline scores as specified in the official code. For interested readers, please refer to [25]."
        },
        {
            "title": "D Supplementary Experimental Results",
            "content": "D.1 More Qualitative Results We present additional qualitative results in fig. 11 and fig. 12. The comparisons demonstrate that our method generates human preferred 3D assets, with improved text alignment and enhanced geometric and texture details. https://github.com/3DTopia/GPTEval3D/blob/main/data/tournament-v0/config.json"
        },
        {
            "title": "A PREPRINT",
            "content": "Figure 11: More qualitative results using DreamDPO."
        },
        {
            "title": "A PREPRINT",
            "content": "Figure 12: More qualitative results using DreamDPO."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Yale University",
        "Zhejiang University"
    ]
}