{
    "paper_title": "Intelligent Sensing-to-Action for Robust Autonomy at the Edge: Opportunities and Challenges",
    "authors": [
        "Amit Ranjan Trivedi",
        "Sina Tayebati",
        "Hemant Kumawat",
        "Nastaran Darabi",
        "Divake Kumar",
        "Adarsh Kumar Kosta",
        "Yeshwanth Venkatesha",
        "Dinithi Jayasuriya",
        "Nethmi Jayasinghe",
        "Priyadarshini Panda",
        "Saibal Mukhopadhyay",
        "Kaushik Roy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autonomous edge computing in robotics, smart cities, and autonomous vehicles relies on the seamless integration of sensing, processing, and actuation for real-time decision-making in dynamic environments. At its core is the sensing-to-action loop, which iteratively aligns sensor inputs with computational models to drive adaptive control strategies. These loops can adapt to hyper-local conditions, enhancing resource efficiency and responsiveness, but also face challenges such as resource constraints, synchronization delays in multi-modal data fusion, and the risk of cascading errors in feedback loops. This article explores how proactive, context-aware sensing-to-action and action-to-sensing adaptations can enhance efficiency by dynamically adjusting sensing and computation based on task demands, such as sensing a very limited part of the environment and predicting the rest. By guiding sensing through control actions, action-to-sensing pathways can improve task relevance and resource use, but they also require robust monitoring to prevent cascading errors and maintain reliability. Multi-agent sensing-action loops further extend these capabilities through coordinated sensing and actions across distributed agents, optimizing resource use via collaboration. Additionally, neuromorphic computing, inspired by biological systems, provides an efficient framework for spike-based, event-driven processing that conserves energy, reduces latency, and supports hierarchical control--making it ideal for multi-agent optimization. This article highlights the importance of end-to-end co-design strategies that align algorithmic models with hardware and environmental dynamics and improve cross-layer interdependencies to improve throughput, precision, and adaptability for energy-efficient edge autonomy in complex environments."
        },
        {
            "title": "Start",
            "content": "Intelligent Sensing-to-Action for Robust Autonomy at the Edge: Opportunities and Challenges Amit Ranjan Trivedi1, Sina Tayebati1, Hemant Kumawat2, Nastaran Darabi1, Divake Kumar1, Adarsh Kumar Kosta4, Yeshwanth Venkatesha3, Dinithi Jayasuriya1, Nethmi Jayasinghe1, Priyadarshini Panda3, Saibal Mukhopadhyay2, and Kaushik Roy4 1University of Illinois at Chicago, 2Georgia Institute of Technology, 3Yale University, 4Purdue University 5 2 0 2 4 ] . [ 1 2 9 6 2 0 . 2 0 5 2 : r AbstractAutonomous edge computing in robotics, smart cities, and autonomous vehicles relies on the seamless integration of sensing, processing, and actuation for real-time decision-making in dynamic environments. At its core is the sensing-to-action loop, which iteratively aligns sensor inputs with computational models to drive adaptive control strategies. These loops can adapt to hyper-local conditions, enhancing resource efficiency and responsiveness, but also face challenges such as resource constraints, synchronization delays in multi-modal data fusion, and the risk of cascading errors in feedback loops. This article explores how proactive, context-aware sensing-to-action and actionto-sensing adaptations can enhance efficiency by dynamically adjusting sensing and computation based on task demands, such as sensing very limited part of the environment and predicting the rest. By guiding sensing through control actions, action-tosensing pathways can improve task relevance and resource use, but they also require robust monitoring to prevent cascading errors and maintain reliability. Multi-agent sensing-action loops further extend these capabilities through coordinated sensing and actions across distributed agents, optimizing resource use via collaboration. Additionally, neuromorphic computing, inspired by biological systems, provides an efficient framework for spikebased, event-driven processing that conserves energy, reduces latency, and supports hierarchical controlmaking it ideal for multi-agent optimization. This article highlights the importance of end-to-end co-design strategies that align algorithmic models with hardware and environmental dynamics, improve cross-layer interdependencies to improve throughput, precision, and adaptability for energy-efficient edge autonomy in complex environments. Index TermsEdge computing, sensing-to-action loops, autonomous systems, neuromorphic computing, multi-agent systems, spike-based processing, energy-efficient architectures, hardwaresoftware co-design, adaptive control, machine learning. I. INTRODUCTION Autonomous edge computing in domains such as robotics, smart cities, and autonomous vehicles relies on the seamless integration of sensing, processing, and actuation to enable real-time decision-making in dynamic environments. Central to this integration is the sensing-to-action loop, which iteratively aligns sensor inputs with computational models to drive adaptive control strategies. Unlike centralized systems that rely on static, generalized models, sensing-to-action loops can seamlessly adapt to hyper-local conditions, such as environmental variations, sensor and processor health, and task-specific priorities, thereby enabling optimized resource allocation, reduce communication latency, and faster responsiveness, and minimizing dependencies on external networks. However, the sophisticated manipulability of sensing-toaction loops at the edge also presents significant challenges. Unlike predominant deep learning pipelines that primarily focus on feed-forward sensing-to-insight, where latency primarily impacts inference speed, delays in cyclical sensing-toaction loops risk propagating outdated environmental states, thereby significantly degrading decision accuracy. Therefore, while sensing-to-insight pathways are more amenable to cloudbased centralized computations, such as under batch processing, edge systems must perform continuous, localized sensingaction loop computations for real-time hypothesis testing and action refinement, making them highly sensitive to resource constraints. Additionally, improving system observability often requires high-fidelity, multi-modal sensors, which can be resource-intensive and impractical due to constraints on power, bandwidth, and form factor. The fusion of heterogeneous, highbandwidth sensor streams further introduces synchronization delays, communication overhead, and latency issues. In this perspective article, we closely examine such challenges in closed-loop sensing-to-action mechanisms at the edge, while also highlighting untapped and emerging opportunities to enable precise, low-latency control with minimal resource requirements. By shifting from reactive sensing-to-insight pipelines to proactive, context-aware sensing-action loop adaptations, we explore strategies essential for autonomous systems operating in complex environments. Unlike traditional machine learning pipelines, the bi-directional information flow in sensing-action loops offers unprecedented co-optimization opportunities, allowing systems to dynamically adjust sensing and computation based on task demands and environmental context. For example, tasks less sensitive to sensor noise or feature reduction can be executed under lower signal-to-noise ratios or reduced precision, conserving resources. Likewise, sensing-to-action loops can be fine-tuned based on scene-specific dynamics, enabling systems to allocate resources more efficiently by adjusting sensor refresh rates, resolution, or modality usage in response to environmental changes. For example, environmental monitoring sensors can reduce their sampling rates during stable periods and increase them during sudden events, such as pollutant surges. Similarly, autonomous systems can deprioritize redundant sensor streams during lowrisk tasks while enhancing accuracy for high-stakes operations. These loops also support hierarchical control, where low-level actions such as adjusting sensor thresholds complement higher-level planning decisions, enabling efficient distribution of computational effort. By leveraging such interplay between sensing and action, sensing-to-action loops therefore open new pathways for adaptive, energy-efficient edge autonomy. Moreover, we discuss how beyond conventional digital and analog processing, spike-based representations in neuromorphic computing domain provide natural framework for sensingto-action loop optimization. Unlike clock-driven architectures, neuromorphic systems use event-driven, asynchronous computations, where spikes trigger processing only for relevant sensory events, reducing latency and energy use. This sparse encoding dynamically adjusts computational loads inherently based on activity levels. Neuromorphic architectures also excel in hierarchical and distributed processing, enabling local spiking circuits to handle low-level actions (e.g., sensor threshold adjustments) while coordinating high-level planning through more complex pathways. Their inherent parallelism and decentralized design make them ideal for multi-agent optimization, where collaborative sensing and resource allocation are crucial. Finally, multi-agent sensing-action loops further extend the potential of sensing-to-action loops through distributed optimization. By understanding the interactions between sensing and actuation groups, agents can dynamically adjust resource allocation based on collaboration and task requirements. Through information sharing and coordinated actions, agents can optimize sensing and processing resources across the network. For example, one agent can reduce its sensing load if another has superior coverage or access to relevant data, improving overall system efficiency. This distributed coordination makes sensing-to-action loops suited for edge autonomy. Towards exploring such opportunities for sensing-action loop optimization, central focus of the paper is to underscore the importance of end-to-end co-design strategies that align algorithmic models with hardware constraints and environmental dynamics to improve latency, energy efficiency, and robustness. Unlike modular optimizations that only address individual components in isolation, end-to-end approaches can leverage cross-layer interdependencies, unlocking unprecedented gains in throughput, precision, and resource allocation. II. INTELLIGENT SENSING-TO-ACTION In Fig.1, at high level, sensing-to-action loops at the edge can be deconstructed into three primary components: the sensing module, the learning module, and the actuation module. This process begins with the environment generating stimuli that are captured by sensors and converted into data streams for downstream processing. The sensing block often handles multiple modalitiessuch as vision, sound, and environmental readingsthat must be fused and pre-processed to extract meaningful features. These features are then passed to the processing pipeline, where machine learning or decisionmaking models predict control actions. These actions, executed by actuators, influence the environment, completing the loop as the updated environment feeds into the next sensing stage. While these loops enable low-latency decision-making, they also introduce unique challenges due to the constrained nature Fig. 1: Opportunities for Intelligent Sensing-to-Action: In sensing-toaction loops, significant gains can be achieved by selectively sensing critical environmental regions while predicting less critical areas based on training data. This frugal sensing strategy is especially beneficial for resource-intensive modalities, such as LiDAR, enhancing task accuracy without unnecessary overhead. Similarly, action-to-sensing optimizations can adjust control variables to opportunistically reduce sensing demands based on task relevance. While these frameworks improve loop efficiency, ensuring reliability requires robust and computationally efficient monitors to continuously assess fidelity and support aggressive optimizations. In multi-agent sensing-action loops, agents can collaborate by sharing sensing tasks or complementing each others sensing capabilities. Moreover, emerging paradigms, such as neuromorphic sensingaction loops, offer unified frameworks by adapting sensing and processing rates based on event dynamics, enabling seamless sensing and control. of edge devices. Limited computational resources, memory, and energy impose strict trade-offs between model complexity and real-time performance. High-fidelity sensing, though necessary for reliable feature extraction, can quickly overwhelm edge hardware due to power and bandwidth constraints. Additionally, fusing heterogeneous data streams can lead to synchronization delays and communication overhead, further complicating realtime feedback. The cyclical nature of the loop also amplifies sensitivity to outdated or noisy data, as errors can propagate and compound, degrading downstream decisions. Despite these challenges, sensing-to-action loops can also offer significant and untapped opportunities for innovation and optimization. In the pursuit of these opportunities, in this paper we explore several fundamental questions. For example, in dynamic environments rich with features and high-dimensional stimuli, key question is: Which sensory inputs are truly critical for decision-making? Some inputs may overlap with existing model knowledge or provide redundant information, while others may have minimal impact on the task at hand. For example, in autonomous vehicles, repeated detection of static objects like buildings may add little new information, whereas detecting unexpected moving objects is crucial for safety. By selectively sensing novel or task-relevant features, systems can simplify processing and improve efficiency. To this end, in Sec. 3, we discuss generative sensing, where generative models reconstruct or dream out most of the environment based on prior knowledge, thus reducing the need for exhaustive realtime data collection, without impacts on action abilities. Action-to-sensing pathways present an equally compelling and unique avenue for enhancing system efficiency and robustness. key question in this context is: How can control Fig. 2: An end-to-end computing pipeline comparison sensing-processing-action loop between biological and neuromorphic system. In biological system, inputs are perceived as changes in intensity (events and frames) and color (frames) by the eye. In contrast, neuromorphic system uses frame cameras to capture analog intensity at low rates and event cameras to detect motion-induced variations, generating events. The brains parallel and recurrent connections enable computation within memory. Neuromorphic system emulates this by combining ANNs, SNNs, and hybrid ANN-SNN models to balance accuracy and efficiency. These algorithms also benefit from hardware acceleration via in-memory (IMC) and near-memory (NMC) computing by efficiently implementing synaptic functionality and, work alongside CPU/GPU architectures to enhance efficiency and reduce latency. actions be used to proactively guide sensing, ensuring that data acquisition remains task-relevant and resource-efficient? For instance, in robotic navigation task, adjusting the sensors field of view based on control objectivessuch as steering toward targetcan reduce redundant data collection. Instead of passively gathering information, action-to-sensing frameworks can dynamically adjust sensing parameters, such as sampling rates and resolutions, to align with control demands. Towards this, as an exemplary technique, we discuss that Koopman operator-based representations provide powerful approach by transforming complex non-linear dynamics into linearly decomposable embedding space. By identifying key system eigenvalues, this framework can enable more efficient, taskinformed control with fewer interactions, making autonomous systems more adaptable and resource-aware. Another critical question is: How to ensure the reliability of sensing-to-action loops amid evolving environmental dynamics, shifting application objectives, hardware degradations, and adversities, as sensing outputs directly influence actions, which, in turn, shape subsequent sensing stages. Without proper monitoring, these loops can drift from expected behaviors or become destabilized over time. For example, misclassification in surveillance drones early detection phase could trigger inappropriate flight adjustments, skewing subsequent sensor coverage and compounding errors further. We examine methodologies that leverage robust statistical representations of intermediate features to detect deviations from the expected. In multi-agent systems, sensing-to-action loops offer further opportunities for cooperative optimization and robust decision-making. fundamental question in this context is: How can distributed agents coordinate sensing and actions to enhance global performance while balancing individual resource constraints? In dynamic, feature-rich environments, redundant observations by multiple agents can lead to inefficient data processing, while critical, unique sensory inputs may remain underutilized. For example, in autonomous drone swarms, overlapping views of static objects may add little new information, whereas coordinated sensing of unexpected, fastmoving obstacles can improve safety and responsiveness. Finally, we ask are there alternative representations that naturally fuse sensing, processing, and action variables to enable systematic approaches that harness this unification for disruptive efficiency advancements? To this end, neuromorphic computing offers compelling solution by integrating sensing and computation through event-driven, parallel processing, as illustrated in Fig. 2. Unlike traditional clock-based systems, neuromorphic architectures trigger computations only in response to sensory events, enabling ultra-low-power, real-time performance. Neuromorphic computing also aligns seamlessly with the cyclical nature of sensing-to-action loops, providing unified and efficient pathway through the tighter coupling of sensing, learning, and acting. Systematically exploring each of these opportunities, Sec. 3 explores strategies for optimizing sensing-to-action loops and Sec. 4 discusses action-to-sensing loop optimization. Sec. 5 focuses on dynamically monitoring the fidelity of sensing-toaction loops. Sec. 6 examines neuromorphic representations of sensing, feature, and action variables. Sec. 7 extends the discussion to multi-agent interactions. Finally, Sec. 8 concludes. TABLE I: Average Precision (AP) of R-MAE against current methods on KITTI ( results are reproduced by us). Model Car Pedestrian Cyclist SECOND [4] + Occ.-MAE [5] + ALSO [6] + R-MAE (Ours) PV-RCNN [7] + Occ.-MAE [5] + ALSO [6] + R-MAE (Ours) 79.08 79.12+0.04 78.980.10 79.10+0.02 82.28 82.43+0.15 82.52+0.24 82.82+0.54 44.52 45.35+0.83 45.33+0.81 46.93+2.41 51.51 48.133.38 52.63+1.12 51.61+0. 64.49 63.271.22 66.53+2.04 67.75+3.26 69.45 71.51+2.06 70.20+0.75 73.82+4.37 TABLE II: Comparison of Conventional LiDAR and R-MAE Framework Metric Scene Coverage Energy per Laser Pulse Model Parameters FLOPs per 360 Scan Sensing Energy per Scan Reconstruction Overhead Conventional 100% (full scan) 50 µJ Not applicable None 72 mJ Not applicable R-MAE < 10% (active) 5.5 µJ 830K 335M 792 µJ 7.1 mJ generative decoder for reconstructing unobserved regions and downstream object detection heads. R-MAE uses range-aware radial masking strategy to optimize LiDAR beam activation while accounting for light propagation physics. The masking operates in two stages: (1) grouping voxels into angular segments and sampling subset for sensing, and (2) applying distance-dependent probabilistic masking to address the R4 energy scaling with range. This approach addresses LiDARs energy-accuracy-range trade-offs without hardware modifications. While improving angular precision (θ) typically requires increasing the aperture diameter (D) or using shorter wavelengths (λ), practical constraints like size and eye safety limit these options. Instead, R-MAEs twostage masking reduces redundant data collection and conserves energy, maintaining robust scene coverage within constraints. R-MAE architecture, in Fig. 3, integrates key components: 3D sparse convolutional encoder processes the partial point cloud into latent representation capturing geometric and semantic features, followed by an occupancy decoder that reconstructs the full 3D scene. To balance efficiency and reconstruction quality, the encoder processes only non-empty voxels, preserving geometric structure while reducing memory usage compared to Transformer-based methods [8][10]. The decoder uses deconvolution layers with batch normalization and ReLU activation to progressively refine the scene at higher resolutions, with binary cross-entropy loss ensuring accurate occupancy prediction and spatial consistency. Our prior work [3] demonstrated the effectiveness of RMAE through extensive evaluations across multiple datasets. As shown in Table I, R-MAE exhibited strong generalization, significantly improving accuracy on the KITTI validation set (40 recall positions at moderate difficulty). On the Waymo dataset, it outperformed baselines by up to 5.59% in mAP/mAPH, even with 90% of the scene masked. Similar gains were observed on the nuScenes dataset, improving LiDAR-only models NDS scores by 2.31% to 3.17%. Additionally, R-MAE achieved substantial energy savings, reducing average laser pulse enFig. 3: Generative Sensing: Sense only what you really need: Generative sensing optimizes resource use by focusing on essential environmental features, reducing unnecessary data collection and enhancing real-time responsiveness. For LiDAR proessing, in this approach, the input point cloud is voxelized and radially masked based on voxel distance from the sensor to minimize redundant information. 3D spatially sparse convolutional encoder extracts latent features, while decoder reconstructs the 3D scene, enabling efficient perception that supports adaptive sensing-to-action strategies. III. OPTIMIZING SENSING-TO-ACTION PATHWAYS We begin by discussing the first optimization opportunity: prioritizing sensory inputs to balance observability and energy efficiency. key question is which sensory inputs and domain regions are truly critical for decision-making? Not all inputs contribute equallysome overlap with existing model knowledge or provide redundant information, while others have minimal impact on task outcomes. By selectively acquiring only the most relevant sensory data, active sensing systems can significantly reduce energy consumption without compromising performance, enabling efficient, real-time perception that adapts dynamically to environmental and taskspecific demands. This approach is particularly impactful for active sensing modalities like LiDAR, which are essential for depth perception and robust object detection but have high energy consumptionaround 25W compared to 12W for conventional cameras [1], [2]posing significant challenges for resource-constrained edge systems. Our recent research proposed the concept of generative sensing [3], which reimagines LiDAR-environment interaction by sampling only 810% of the scene and using generative models to reconstruct unobserved regions. This builds on the insight that many scene regions are either predictable from pre-training or have minimal accuracy impact. In Fig. 3, such generative sensing is enabled by Radially Masked Autoencoding (R-MAE), which combines masked autoencoder with Fig. 4: Our approach conditions visual representations on the task policy by incorporating contrastive spectral Koopman encoding and reinforcement learning (RL)-guided control. This high-level framework unifies perception and control, enabling task-aware sensing adjustments. The RoboKoop model leverages these representations to dynamically adjust sensing parameters based on control objectives. (Adapted from RoboKoop [18]) ergy to 5.5 µJ compared to 50 µJ in conventional systems. While adding computational overhead (830K parameters and 335M FLOPs per 360 scan), the combined energy consumption of sensing and reconstruction was 9.11 lower than traditional LiDAR, particularly for long-range measurements where energy costs scale with the fourth power of distance. Generative sensing thus represents significant advancement for ultra-frugal perception systems, rethinking the sensingprocessing pipeline to leverage trends in sensing and computing energy costs. This approach can extend beyond LiDAR to modalities such as radar, cameras, and acoustic sensors, where selective sampling and reconstruction similarly enhance performance and energy efficiency. Future work could explore adaptive masking, multi-modal fusion, and broader applications to active sensing, advancing sensor data compression and reconstruction for ultra-low-power autonomous systems. To minimize generative sensing workload techniques such as in-memory computing [11][13], analog computing [14], and beyond CMOS devices [15][17] can be explored. IV. OPTIMIZING ACTION-TO-SENSING PATHWAYS While sensing-to-action loops focus on extracting sensory data to inform decisions, the reverse pathwayaction-tosensingpresents an equally important optimization opportunity: using system actions to guide and refine subsequent sensing. Autonomous systems often operate in dynamic environments [19][21] where sensory context [22] and task priorities [23], [24] evolve, raising key research question: How can actions be leveraged to proactively adjust sensing strategies to improve efficiency without compromising situational awareness? By incorporating feedback from past actions and outcomes, action-to-sensing pathways can dynamically modulate sensing parameters such as focus, sampling rates [25], and resolution based on the current context. This transforms sensing from passive data collection into an active, goalFig. 5: (a) Computational Performance under external disturbances. (Adapted from RoboKoop [18]) load of state-of-the-art dynamical models. (b) directed process that adapts to environmental changes and resource constraints. Towards this goal, our prior work [18] hypothesized that robust agent representations can be learned with fewer interactions if the task embedding space can be modeled linearly and finite set of stable (negative) eigenvalues of the Koopman operator are identified. Fig. 4 illustrates the approach for learning linear Koopman embedding manifold using contrastive spectral Koopman encoder. This encoder generates key and query samples for each observation at time t, where positive samples apply random cropping augmentations to the state xt, and negative samples use augmentations on other states. The query encoder maps visual observations to complex-valued Koopman embedding space with learnable eigenvalues µi +jω. Using this embedding and the spectral Koopman operator, optimal control strategies are derived by solving Linear Quadratic Regulator (LQR) problem over finite time horizon. The goal state, provided in visual space, is similarly mapped via the key encoder. Dual Q-value functions within the Soft Actor-Critic (SAC) framework guide updates based on the LQR controllers cost. Training involves optimizing three key losses: the SAC loss for training the critic and Koopman parameters, the contrastive loss to refine the encoder, and the next latent prediction loss to regularize Koopman embedding dynamics. We assessed computational efficiency by implementing MLP-based dynamics [26], dense Koopman model [27], Transformer model [28], [29], and recurrent model [30]. Fig. 5(a) shows that our spectral Koopman-based approach required the fewest Multiply-Accumulate (MAC) operations for control and prediction, highlighting its efficiency in dynamic system modeling. To test robustness, we applied an external Fig. 6: Ensuring Sensing-Action Loop Reliability: STARNet enhances the reliability of sensing-to-action loops by ingesting feature representations from primary task networks. VAE models the typical distribution of these features, and during inference, STARNet uses gradient-free optimization to compute likelihood regret, identifying discrepancies between sensed and learned distributions to alert the system to potential inaccuracies. force Uniform(amin, amax) to the cart-pole system during evaluation, with disturbance probability p. Fig. 5(b) shows that our model maintained high performance even with disturbance probability of 0.25, demonstrating superior resilience compared to other methods. This analysis shows that action-to-sensing pathways, combined with efficient Koopman-based representations, enhance the adaptability and resilience of autonomous systems by linking control actions directly to optimized sensing strategies. Future work could extend this framework to handle non-stationary dynamics by learning time-varying Koopman operators that adapt to environmental shifts, such as sensor degradation or task transitions. Additionally, incorporating uncertainty quantification within Koopman representations to adjust sensing actions based on confidence estimates can help reduce cascading errors in uncertain environments. V. RELIABILITY OF SENSING-TO-ACTION LOOPS key challenge is maintaining accurate and consistent feedback as systems interact with evolving surroundings. This raises critical questions: How can sensing-to-action loops remain stable despite environmental changes, sensor degradation, or adversarial disruptions? What mechanisms can detect and correct deviations before they cascade into failures? While task-conditioned mechanisms in previous sections adaptive, enhance flexibility and responsiveness, they also introduce risksdynamic adjustments may amplify noise, propagate erroneous data, or destabilize feedback loops [31][33]. Without robust monitoring, these systems risk propagating inaccuracies. To address these challenges, we proposed STARNet, twostage mechanism that monitors sensor data trustworthiness in real time to maintain sensing-to-action loop integrity [34]. Instead of relying on static sensing configurations, STARNet evaluates intermediate sensor features from primary tasks to detect untrustworthy data streams deviating from expected distributions. As shown in Fig. 6, STARNets core components include Variational Autoencoder (VAE) that learns the distribution of Fig. 7: Object Detection Accuracy for KITTI Dataset: The VAE-based approach analyzes LiDAR point clouds and object labels, producing bounding boxes for cars, pedestrians, and cyclists. The network was tested under challenging conditions, such as varying snow intensities and other corruptions. normal sensor embeddings to model complex, high-dimensional data. Likelihood Regret (LR) metric [35] quantifies how much the VAEs distribution must adjust for new input, with large LR scores indicating anomalies. To reduce computational overhead, STARNet uses gradient-free optimization techniques such as Simultaneous Perturbation Stochastic Approximation (SPSA) [36], [37], making it suitable for low-power edge devices. Additionally, Low-Rank Adaptation (LoRA) [38], [39] enables efficient on-device fine-tuning by constraining updates to low-dimensional subspace while preserving core model weights for fast adaptation. We evaluated STARNet using KITTI-C data [40] across natural corruptions (e.g., rain, fog) [31], external disruptions (e.g., beam missing, motion blur) [41], and internal sensor failures (e.g., crosstalk [42], cross-sensor interference [43]). In LiDAR-only tests, STARNet achieved AUC values above 0.90 for crosstalk (0.9658) and cross-sensor interference (0.9938), demonstrating strong detection capabilities without explicit training on these faults. When fusing LiDAR with camera inputs, STARNet further improved anomaly detection under heavy snow while maintaining high task accuracy for detecting cars and pedestrians by filtering unreliable sensor data. As shown in Fig. 7, STARNet increased object detection accuracy by 15%, restoring performance to clean data. By reinforcing sensing-to-action loops with proactive anomaly detection, STARNet ensures reliable adaptive sensing and robust decision-making in complex environments. Future enhancements include context-aware anomaly detection to reduce false positives, adaptive fusion to adjust sensor weights based on reliability, and temporal consistency checks for detecting gradual sensor degradation. Additionally, uncertainty-aware [44], [45] control mechanisms can modulate actions based on confidence levels [46], [47]. VI. SENSING-ACTION LOOPS IN NEUROMORPHIC SYSTEMS Biological systems, renowned for their efficiency in sensing, processing, and interacting with their environment, inspire the design of sensing-to-action frameworks for resourceconstrained edge devices. For instance, the fruit fly (Drosophila melanogaster) navigates complex environments using just 100,000 neurons, consuming only 26.6 W/kg during flight [52]. This efficiency arises from an integrated architecture where Fig. 8: Neuromorphic sensing-action loop architectures. a) Full-ANN [48], Full-SNN [49], and Hybrid SNN-ANN [50] models for optical flow estimation using event data. b) Fusion-FlowNet [51] integrates event-based and frame-based modalities for enhanced feature extraction. Outputs are aggregated at the final SNN layer. (Adapted from Fusion-FlowNet [51]). neurons simultaneously perform sensing, computation, and memoryforming tightly coupled sensing-action loops that selectively respond to relevant stimuli while remaining inactive otherwise. What makes biological sensing-action loops resilient, and how can these principles inform artificial systems? In our prior work [53], we explored neuromorphic sensingaction loops inspired by event-driven, bio-plausible processing to create asynchronous architectures that compute only for meaningful events. Neuromorphic systems [54] also co-locate computation and memory, enabling massively parallel processing while reducing energy consumption and data transfer delaysmaking them ideal for edge systems. Comprehensive neuromorphic frameworks leverage event-based sensors, spiking neural networks (SNNs), and bio-inspired learning to emulate the brains sensing-processing-action loop. This approach reduces latency, conserves energy, and supports adaptive control, bringing artificial an biological systems closer."
        },
        {
            "title": "Recent advances",
            "content": "in event-driven sensors have further strengthened the potential of neuromorphic sensing-action loops. Frame-based cameras, while standard for vision tasks, are unsuitable during rapid motion scenario due to their low temporal resolution, increasing storage and latency. By contrast, event-based cameras like DVS128 [55] and DAVIS240 [56] asynchronously capture pixel-wise intensity changes offering superior temporal resolution (10µs vs 3ms), lower power consumption (10mW vs 3W ), and wider dynamic range (120dB vs 74dB) compared to frame-based cameras [32]. However, frame-based data still enhances accuracy in some tasks [57], highlighting the need for sensor fusion approaches [51]. SNNs [58] are well-suited for processing event-based sensor inputs due to their sparse, event-driven computations and intrinsic memory for sequential tasks [59]. This makes them efficient alternatives to RNNs and LSTMs [60]. However, training deep SNNs is challenging due to vanishing spikes and non-differentiable activations [61], [62]. Recent advancements, such as ANN-to-SNN conversion [63], learnable neuronal dynamics [49], [64], and surrogate gradient methods [62], [65], address these limitations. For example, recent work, AdaptiveSpikeNet [49] employs learnable spiking neuronal dynamics to achieve 20% lower average endpoint error (AEE) than Fig. 9: Average Endpoint Error (AEE) comparison for Optical flow estimation on the MVSEC [66] dataset. The left shows the Average Endpoint Error (AEE) for baseline models, EvFlow-Net (EvF) [48], Spike-FlowNet (SpF) [50], and Fusion-FlowNet (FF) [51]). The right showcases how AEE varies with model size for Adaptive-SpikeNet and corresponding full-ANN models. (Adapted from Adaptive-SpikeNet [49]). traditional ANNs for optical flow estimation, with 48 fewer parameters and consuming 10 less energy. Efforts to develop hybrid SNN-ANN architectures leverage the strengths of both networks, improving performance while reducing training complexity and energy consumption. SpikeFlowNet [50], which combines an SNN encoder with an ANN decoder for optical flow estimation, outperforms full-ANN models [48] on the MVSEC dataset [66] with 1.21 energy reduction. Sensor-fusion models like Fusion-FlowNet [51] integrate events and frames, achieving 40% lower error with nearly half the parameters and 1.87 lower energy. For simpler tasks like object detection, full-SNN models excelDOTIE [67], lightweight, single-layer SNN, filters events based on speed and clusters them into bounding boxes. These algorithmic advancements when coupled with suitable neuromorphic hardware powered by in-memory computing (IMC) architectures such as [68][72], could enable the sensing-processing-action loop characteristic of the brain, resulting in truly end-to-end neuromorphic system capable of brain-like intelligence and efficiency. VII. FEDERATED, MULTI-AGENT SENSING-ACTION LOOPS Multi-agent sensing-action loops have the potential to enhance system-wide adaptability and efficiency by enabling agents to collaboratively sense, learn, and act. However, key research questions remain: How can agents dynamically share sensing and computation tasks to avoid redundancy while maintaining both individual and collective performance? How can robust decision-making be ensured in the face of network laenabled by precision-reconfigurable simulator, allowing realtime adjustments to meet energy, latency, and area constraints. Speculative decoding [78] exemplifies how edge-cloud collaboration can further enhance multi-agent systems. By parallelizing token predictions and verifying outputs probabilistically, speculative decoding accelerates autoregressive tasks such as conversational AI and multimodal processing. This approach enables resource-constrained edge devices to perform lightweight inference locally while offloading complex computations to the cloud. For example, quadruped robots in disaster zones can process multimodal inputssuch as text instructions, visual data, and sensor readingsto generate context-aware responses in real time. The edge handles low-latency predictions, while the cloud refines and updates models as needed, reducing communication overhead even in dynamic scenarios. Fig. 11 illustrates how adaptive frameworks like DC-NAS and HaLo-FL significantly reduce energy, latency, and area utilization while maintaining performance on datasets like CIFAR10. These approaches highlight the importance of integrating adaptive model architectures, real-time profiling, and predictive resource allocation in distributed multi-agent systems. Emerging hardware paradigms, such as in-memory computing and low-precision representations, further support energy-efficient execution by minimizing data movement and overheads. Ultimately, advancing multi-agent sensing-action loops requires bridging algorithmic innovations with hardware-aware design. This includes balancing edge-cloud workloads, synchronizing model updates, and leveraging speculative decoding for efficient decision-making. By co-optimizing sensing, computation, and communication, these systems can achieve robust, scalable performance, paving the way for responsive and resource-efficient AI across diverse applications. VIII. CONCLUSIONS This article highlighted the crucial role of sensing-to-action loops in enabling real-time decision-making for autonomous edge computing. These loops can enhance system adaptability and resource efficiency by dynamically aligning sensor inputs with computational models for task-specific control. However, they also introduce challenges such as synchronization delays, resource constraints, and cascading errors, necessitating robust cross-layer co-design strategies. To address these challenges, we discussed generative sensing frameworks that selectively sense critical parts of the environment and use generative models to reconstruct predictable regions. This approach showed that only 8% of the environment needs to be actively sensed, significantly reducing sensing overhead. Similarly, action-tosensing pathways demonstrated how control objectives can proactively adjust sensing strategies to maintain situational awareness while minimizing redundant data acquisition. The use of Koopman operator-based representations improved computational efficiency across models, including transformers. Despite these gains, sensing-to-action loops face destabilization risks due to runtime adaptations. To mitigate this, we presented the STAR-Net framework, which employs metrics like likelihood regret to monitor and enhance the reliability of these loops, improving prediction accuracy by over 10% Fig. 10: Key aspects of dynamic multi-agent systems: resource heterogeneity, adaptable architectures, hardware-aware optimization, and workload management across server-client interactions. Fig. 11: Performance comparison of DC-NAS and HaLo-FL on the CIFAR-10 dataset, showing relative reductions in energy, latency, and area with adaptive model optimization. tency, hardware heterogeneity, and agent failures? Addressing these challenges requires frameworks that balance collaboration and independence, adapt to real-time conditions, and optimize resource allocation across distributed network of agents. Federated learning (FL) has emerged as promising approach to decentralized, collaborative learning without the need to share raw data. By aggregating insights from distributed agents, FL preserves data privacy and enhances security, making it effective for applications in healthcare [73], Internet of Things (IoT) [74], and autonomous systems [75]. However, real-world FL deployments face challenges such as hardware heterogeneity, intermittent connectivity, and varying application requirements. Participating devices often have diverse constraints, including differences in compute power, memory, and energy availability  (Fig. 10)  . Traditional FL approaches, which assume uniform client capabilities and static models, are illsuited for such diverse, dynamic environments. Dynamic federated learning frameworks address this heterogeneity by adapting models and training processes in real time. DC-NAS [76], for instance, tailors neural network architectures to client-specific constraints through topology and channel pruning, enabling efficient collaboration without overburdening resource-limited agents. By dynamically adjusting model complexity, DC-NAS improves both training efficiency and robustness. Similarly, HaLo-FL [77] incorporates hardware-aware precision selector that optimizes weights, activations, and gradients based on client capabilities, reducing energy consumption and latency while preserving accuracy. This adaptability is on complex datasets. We also demonstrated that multi-agent sensing-to-action loops, leveraging federated learning and distributed collaboration, can achieve threefold reduction in energy consumption. Lastly, we explored efficient representations that integrate sensing, perception, and action, highlighting the potential of neuromorphic systems. These eventdriven architectures synchronize sensing and computation, enabling energy-efficient, low-latency processing well-suited for resource-constrained environments. Acknowledgment: This work was supported by COGNISENSE and CoCoSys, two of seven centers in JUMP 2.0, Semiconductor Research Corporation (SRC) program sponsored by DARPA, and NSF Awards #2329096, #2106964, and #2317974."
        },
        {
            "title": "REFERENCES",
            "content": "[1] C. Rablau, Lidara new (self-driving) vehicle for introducing optics to broader engineering and non-engineering audiences, in Education and training in optics and photonics. Optica Publishing Group, 2019, p. 11143 138. [2] F. E. Sahin, Long-range, high-resolution camera optical design for assisted and autonomous driving, in photonics. MDPI. [3] S. Tayebati, T. Tulabandhula, and A. R. Trivedi, Sense less, generate more: Pre-training lidar perception with masked autoencoders for ultraefficient 3d sensing, arXiv preprint arXiv:2406.07833, 2024. [4] Y. Yan, Y. Mao, and B. Li, Second: Sparsely embedded convolutional detection, Sensors, vol. 18, no. 10, p. 3337, 2018. [5] C. Min, L. Xiao, D. Zhao, Y. Nie, and B. Dai, Occupancy-mae: Self-supervised pre-training large-scale lidar point clouds with masked occupancy autoencoders, IEEE Transactions on Intelligent Vehicles, 2023. [6] A. Boulch, C. Sautier, B. Michele, G. Puy, and R. Marlet, Also: Automotive lidar self-supervision by occupancy estimation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13 45513 465. [7] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, Pv-rcnn: Point-voxel feature set abstraction for 3d object detection, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 10 52910 538. [8] S. Contributors, Spconv: Spatially sparse convolution library, https:// github.com/traveller59/spconv, 2022. [9] R. Xu, T. Wang, W. Zhang, R. Chen, J. Cao, J. Pang, and D. Lin, Mv-jar: Masked voxel jigsaw and reconstruction for lidar-based self-supervised pre-training, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 13 44513 454. [10] G. Hess, J. Jaxing, E. Svensson, D. Hagerman, C. Petersson, and L. Svensson, Masked autoencoder for self-supervised pre-training on lidar point clouds, in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2023, pp. 350359. [11] P. Shukla, A. Shylendra, T. Tulabandhula, and A. R. Trivedi, Mc2ram: Markov chain monte carlo sampling in sram for fast bayesian inference, in 2020 IEEE International Symposium on Circuits and Systems (ISCAS). [12] P. Shukla, A. Muralidhar, N. Iliev, T. Tulabandhula, S. B. Fuller, and A. R. Trivedi, Ultralow-power localization of insect-scale drones: Interplay of probabilistic filtering and compute-in-memory, IEEE transactions on very large scale integration (VLSI) systems, vol. 30, no. 1, pp. 6880, 2021. [13] S. Nasrin, D. Badawi, A. E. Cetin, W. Gomes, and A. R. Trivedi, Mfnet: Compute-in-memory sram for multibit precision inference using memory-immersed data conversion and multiplication-free operators, IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 68, no. 5, pp. 19661978, 2021. [14] A. Shylendra, P. Shukla, S. Mukhopadhyay, S. Bhunia, and A. R. Trivedi, Low power unsupervised anomaly detection by nonparametric modeling of sensor statistics, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 28, no. 8, pp. 18331843, 2020. [15] A. R. Trivedi, M. F. Amir, and S. Mukhopadhyay, Ultra-low power electronics with si/ge tunnel fet, in 2014 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2014, pp. 16. [16] V. K. Sangwan, S. E. Liu, A. R. Trivedi, and M. C. Hersam, Twodimensional materials for bio-realistic neuronal computing networks, Matter, vol. 5, no. 12, pp. 41334152, 2022. [17] G. Finocchio, J. A. C. Incorvia, J. S. Friedman, Q. Yang, A. Giordano, J. Grollier, H. Yang, F. Ciubotaru, A. V. Chumak, A. J. Naeemi et al., Roadmap for unconventional computing with nanotechnology, Nano Futures, vol. 8, no. 1, p. 012001, 2024. [18] H. Kumawat, B. Chakraborty, and S. Mukhopadhyay, Robokoop: Efficient control conditioned representations in robotics using koopman operator, 2024. [Online]. Available: https: //arxiv.org/abs/2409.03107 from visual input [19] K. Samal, H. Kumawat, M. Wolf, and S. Mukhopadhyay, methodology for understanding the origins of false negatives in dnn based object detectors, in 2022 International Joint Conference on Neural Networks (IJCNN), 2022, pp. 18. [20] M. Lee, S. Sharma, W. C. Wang, H. Kumawat, N. M. Rahman, and S. Mukhopadhyay, Cognitive sensing for energy-efficient edge intelligence, in 2024 Design, Automation & Test in Europe Conference & Exhibition (DATE), 2024, pp. 16. [21] K. Samal, H. Kumawat, P. Saha, M. Wolf, and S. Mukhopadhyay, Task-driven rgb-lidar fusion for object tracking in resource-efficient autonomous system, IEEE Transactions on Intelligent Vehicles, vol. 7, no. 1, pp. 102112, 2022. [22] H. Kumawat and S. Mukhopadhyay, Radar guided dynamic visual attention for resource-efficient rgb object detection, in 2022 International Joint Conference on Neural Networks (IJCNN), 2022, pp. 18. [23] H. Kumawat, B. Chakraborty, and S. Mukhopadhyay, STAGE net: Spatio-temporal attention-based graph encoding for learning multi-agent interactions in the presence of hidden agents, 2024. [Online]. Available: https://openreview.net/forum?id=tsj6rDzI0V [24] , STEMFold: Stochastic temporal manifold for multi-agent interactions in the presence of hidden agents, in Proceedings of the 6th Annual Learning for Dynamics & Control Conference, ser. Proceedings of Machine Learning Research, A. Abate, M. Cannon, K. Margellos, PMLR, 1517 Jul 2024, and A. Papachristodoulou, Eds., vol. 242. pp. 14271439. [Online]. Available: https://proceedings.mlr.press/v242/ kumawat24a.html [25] H. Kumawat and S. Mukhopadhyay, Adacred: Adaptive causal decision transformers with feature crediting, 2024. [Online]. Available: https://arxiv.org/abs/2412. [26] A. Srinivas, M. Laskin, and P. Abbeel, Curl: Contrastive unsupervised representations for reinforcement learning, 2020. [27] X. Lyu, H. Hu, S. Siriya, Y. Pu, and M. Chen, Task-oriented koopman-based control with contrastive encoder, in 7th Annual Conference on Robot Learning, 2023. [Online]. Available: https: //openreview.net/forum?id=q0VAoefCI2 [28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, Attention is all you need, 2023. [29] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and I. Mordatch, Decision transformer: Reinforcement learning via sequence modeling, 2021. [30] D. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, Dream to control: Learning behaviors by latent imagination, 2020. [31] R. Heinzler, P. Schindler, J. Seekircher, W. Ritter, and W. Stork, Weather influence and classification with automotive lidar sensors, in 2019 IEEE intelligent vehicles symposium (IV). IEEE, 2019, pp. 15271534. [32] G. Gallego, T. Delbruck, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis et al., Eventbased vision: survey, IEEE transactions on pattern analysis and machine intelligence, vol. 44, no. 1, pp. 154180, 2020. [33] A. Sole, O. Mano, G. P. Stein, H. Kumon, Y. Tamatsu, and A. Shashua, Solid or not solid: Vision for radar target validation, in IEEE Intelligent Vehicles Symposium, 2004. IEEE, 2004, pp. 819824. [34] N. Darabi, S. Tayebati, S. Ravi, T. Tulabandhula, A. R. Trivedi et al., Starnet: Sensor trustworthiness and anomaly recognition via approximated likelihood regret for robust edge autonomy, arXiv preprint arXiv:2309.11006, 2023. [35] Z. Xiao, Q. Yan, and Y. Amit, Likelihood regret: An out-of-distribution detection score for variational auto-encoder, Advances in neural information processing systems, vol. 33, pp. 20 68520 696, 2020. [36] S. Ghadimi and G. Lan, Stochastic first-and zeroth-order methods for nonconvex stochastic programming, SIAM Journal on Optimization, vol. 23, no. 4, pp. 23412368, 2013. [37] S. Bhatnagar and L. Prashanth, Generalized simultaneous perturbation stochastic approximation with reduced estimator bias, in 2023 57th Annual Conference on Information Sciences and Systems (CISS). 2023, pp. 16. IEEE, [38] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, arXiv preprint arXiv:2106.09685, 2021. [39] D. Jayasuriya, N. Darabi, M. B. Hashem, and A. R. Trivedi, Neural precision polarization: Simplifying neural network inference with duallevel precision, arXiv preprint arXiv:2411.05845, 2024. [40] L. Kong, Y. Liu, X. Li, R. Chen, W. Zhang, J. Ren, L. Pan, K. Chen, and Z. Liu, Robo3d: Towards robust and reliable 3d perception against corruptions, arXiv preprint arXiv:2303.17597, 2023. [41] T. Gandhi and M. M. Trivedi, Pedestrian protection systems: Issues, survey, and challenges, IEEE Transactions on intelligent Transportation systems, vol. 8, no. 3, pp. 413430, 2007. [42] A. L. Diehm, M. Hammer, M. Hebel, and M. Arens, Mitigation of crosstalk effects in multi-lidar configurations, in Electro-Optical Remote Sensing XII, vol. 10796. SPIE, 2018, pp. 1324. [43] B. M. Kalasapati and S. L. Tripathi, Robustness evaluation of electrical characteristics of sub-22 nm finfets affected by physical variability, Materials Today: Proceedings, vol. 49, pp. 22452252, 2022. [44] A. C. Stutts, D. Erricolo, S. Ravi, T. Tulabandhula, and A. R. Trivedi, Mutual information-calibrated conformal feature fusion for uncertaintythe edge, in 2024 IEEE aware multimodal 3d object detection at International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 20292035. [45] A. C. Stutts, D. Erricolo, T. Tulabandhula, and A. R. Trivedi, Lightweight, uncertainty-aware conformalized visual odometry, in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 77427749. [46] N. Darabi, P. Shukla, D. Jayasuriya, D. Kumar, A. C. Stutts, and A. R. Trivedi, Navigating the unknown: Uncertainty-aware computein-memory autonomy of edge robotics, in 2024 Design, Automation & Test in Europe Conference & Exhibition (DATE). IEEE, 2024, pp. 16. [47] D. Parente, N. Darabi, A. C. Stutts, T. Tulabandhula, and A. R. Trivedi, Conformalized multimodal uncertainty regression and reasoning, in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 69856989. [48] A. Z. Zhu, L. Yuan, K. Chaney, and K. Daniilidis, Ev-flownet: Selfsupervised optical flow estimation for event-based cameras, arXiv preprint arXiv:1802.06898, 2018. [49] A. K. Kosta and K. Roy, Adaptive-spikenet: event-based optical flow estimation using spiking neural networks with learnable neuronal dynamics, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 60216027. [50] C. Lee, A. K. Kosta, A. Z. Zhu, K. Chaney, K. Daniilidis, and K. Roy, Spike-flownet: event-based optical flow estimation with energy-efficient hybrid neural networks, in European Conference on Computer Vision. Springer, 2020, pp. 366382. [51] C. Lee, A. K. Kosta, and K. Roy, Fusion-flownet: Energy-efficient optical flow estimation using sensor fusion and deep fused spiking-analog network architectures, in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 65046510. [52] H. J. Zhu and M. Sun, Kinematics measurement and power requirements of fruitflies at various flight speeds, Energies, vol. 13, no. 16, p. 4271, 2020. [53] N. Rathi, I. Chakraborty, A. Kosta, A. Sengupta, A. Ankit, P. Panda, and K. Roy, Exploring neuromorphic computing based on spiking neural networks: Algorithms to hardware, ACM Computing Surveys, vol. 55, no. 12, pp. 149, 2023. [54] J. Seo, S. Kang, D. Kumar, W. Shin, J. Cho, T. Kim, Y. Kim, B. C. Jang, A. R. Trivedi, and H. Yoo, Random number generators and spiking neurons from metal oxide/small molecules heterojunction n-shape switching transistors, Advanced Functional Materials, p. 2411348, 2024. [55] P. Lichtsteiner, C. Posch, and T. Delbruck, 128 128 120 db 15 µs latency asynchronous temporal contrast vision sensor, IEEE Journal of Solid-State Circuits, vol. 43, no. 2, pp. 566576, Feb 2008. [56] C. Brandli, R. Berner, M. Yang, S. Liu, and T. Delbruck, 240180 130 db 3 µs latency global shutter spatiotemporal vision sensor, IEEE Journal of Solid-State Circuits, vol. 49, no. 10, pp. 23332341, 2014. [57] N. Messikommer, C. Fang, M. Gehrig, and D. Scaramuzza, Data-driven feature tracking for event cameras, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 5642 5651. [58] W. Maass, Networks of spiking neurons: the third generation of neural network models, Neural networks, vol. 10, no. 9, pp. 16591671, 1997. [59] K. Roy, A. Jaiswal, and P. Panda, Towards spike-based machine intelligence with neuromorphic computing, Nature, vol. 575, no. 7784, pp. 607617, 2019. [60] W. Ponghiran and K. Roy, Spiking neural networks with improved inherent recurrence dynamics for sequential learning, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 7, 2022, pp. 80018008. [61] N. Rathi, G. Srinivasan, P. Panda, and K. Roy, Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation, in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum? id=B1xSperKvH [62] E. O. Neftci, H. Mostafa, and F. Zenke, Surrogate gradient learning in spiking neural networks, IEEE Signal Processing Magazine, vol. 36, pp. 6163, 2019. [63] Y. Cao, Y. Chen, and D. Khosla, Spiking deep convolutional neural networks for energy-efficient object recognition, International Journal of Computer Vision, vol. 113, pp. 5466, 2015. [64] N. Rathi and K. Roy, Diet-snn: low-latency spiking neural network with direct input encoding and leakage and threshold optimization, IEEE Transactions on Neural Networks and Learning Systems, 2021. [65] C. Lee, S. S. Sarwar, P. Panda, G. Srinivasan, and K. Roy, Enabling spike-based backpropagation for training deep neural network architectures, Frontiers in Neuroscience, vol. 14, 2020. [66] A. Z. Zhu, D. Thakur, T. Ozaslan, B. Pfrommer, V. Kumar, and K. Daniilidis, The multivehicle stereo event camera dataset: An event camera dataset for 3d perception, IEEE Robotics and Automation Letters, vol. 3, no. 3, pp. 20322039, 2018. [67] M. Nagaraj, C. M. Liyanagedera, and K. Roy, Dotie-detecting objects through temporal isolation of events using spiking architecture, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 48584864. [68] S. Kim, S. Kim, S. Um, S. Kim, K. Kim, and H.-J. Yoo, Neuro-cim: 310.4 tops/w neuromorphic computing-in-memory processor with low wl/bl activity and digital-analog mixed-mode neuron firing, in 2022 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits). IEEE, 2022, pp. 3839. [69] A. Agrawal, M. Ali, M. Koo, N. Rathi, A. Jaiswal, and K. Roy, Impulse: 65-nm digital compute-in-memory macro with fused weights and learning tasks, IEEE membrane potential for spike-based sequential Solid-State Circuits Letters, vol. 4, pp. 137140, 2021. [70] A. Ankit, A. Sengupta, P. Panda, and K. Roy, Resparc: reconfigurable and energy-efficient architecture with memristive crossbars for deep spiking neural networks, in Proceedings of the 54th Annual Design Automation Conference 2017, 2017, pp. 16. [71] C. Lee, L. Rahimifard, J. Choi, J.-i. Park, C. Lee, D. Kumar, P. Shukla, S. M. Lee, A. R. Trivedi, H. Yoo et al., Highly parallel and ultra-lowpower probabilistic reasoning with programmable gaussian-like memory transistors, Nature Communications, vol. 15, no. 1, p. 2439, 2024. [72] N. Darabi, M. B. Hashem, H. Pan, A. Cetin, W. Gomes, and A. R. Trivedi, Adc/dac-free analog acceleration of deep neural networks with frequency transformation, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 2024. [73] R. S. Antunes, C. Andre da Costa, A. Kuderle, I. A. Yari, and B. Eskofier, Federated learning for healthcare: Systematic review and architecture proposal, ACM Transactions on Intelligent Systems and Technology (TIST), vol. 13, no. 4, pp. 123, 2022. [74] L. U. Khan, W. Saad, Z. Han, E. Hossain, and C. S. Hong, Federated learning for internet of things: Recent advances, taxonomy, and open challenges, IEEE Communications Surveys & Tutorials, vol. 23, no. 3, pp. 17591799, 2021. [75] Y. Xianjia, J. P. Queralta, J. Heikkonen, and T. Westerlund, Federated learning in robotic and autonomous systems, Procedia Computer Science, vol. 191, pp. 135142, 2021. [76] Y. Venkatesha, Y. Kim, H. Park, and P. Panda, Divide-and-conquer the nas puzzle in resource-constrained federated learning systems, Neural Networks, vol. 168, pp. 569579, 2023. [77] Y. Venkatesha, A. Bhattacharjee, A. Moitra, and P. Panda, Halofl: Hardware-aware low-precision federated learning, in 2024 Design, Automation & Test in Europe Conference & Exhibition (DATE), 2024, pp. 16. [78] Y. Leviathan, M. Kalman, and Y. Matias, Fast inference from transformers via speculative decoding, in International Conference on Machine Learning. PMLR, 2023, pp. 19 27419 286."
        }
    ],
    "affiliations": [
        "Georgia Institute of Technology",
        "Purdue University",
        "University of Illinois at Chicago",
        "Yale University"
    ]
}