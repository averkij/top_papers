{
    "paper_title": "Inverse-and-Edit: Effective and Fast Image Editing by Cycle Consistency Models",
    "authors": [
        "Ilia Beletskii",
        "Andrey Kuznetsov",
        "Aibek Alanov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in image editing with diffusion models have achieved impressive results, offering fine-grained control over the generation process. However, these methods are computationally intensive because of their iterative nature. While distilled diffusion models enable faster inference, their editing capabilities remain limited, primarily because of poor inversion quality. High-fidelity inversion and reconstruction are essential for precise image editing, as they preserve the structural and semantic integrity of the source image. In this work, we propose a novel framework that enhances image inversion using consistency models, enabling high-quality editing in just four steps. Our method introduces a cycle-consistency optimization strategy that significantly improves reconstruction accuracy and enables a controllable trade-off between editability and content preservation. We achieve state-of-the-art performance across various image editing tasks and datasets, demonstrating that our method matches or surpasses full-step diffusion models while being substantially more efficient. The code of our method is available on GitHub at https://github.com/ControlGenAI/Inverse-and-Edit."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 3 0 1 9 1 . 6 0 5 2 : r Inverse-and-Edit: Effective and Fast Image Editing by Cycle Consistency Models Ilia Beletskii HSE University, AIRI ibeletskiy@hse.ru Andrey Kuznetsov AIRI, Sber, Innopolis kuznetsov@airi.net Aibek Alanov HSE University, AIRI alanov.aibek@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in image editing with diffusion models have achieved impressive results, offering fine-grained control over the generation process. However, these methods are computationally intensive because of their iterative nature. While distilled diffusion models enable faster inference, their editing capabilities remain limited, primarily because of poor inversion quality. High-fidelity inversion and reconstruction are essential for precise image editing, as they preserve the structural and semantic integrity of the source image. In this work, we propose novel framework that enhances image inversion using consistency models, enabling high-quality editing in just four steps. Our method introduces cycle-consistency optimization strategy that significantly improves reconstruction accuracy and enables controllable trade-off between editability and content preservation. We achieve state-of-the-art performance across various image editing tasks and datasets, demonstrating that our method matches or surpasses full-step diffusion models while being substantially more efficient. The code of our method is available on GitHub: github.com/ControlGenAI/Inverse-and-Edit."
        },
        {
            "title": "Introduction",
            "content": "Diffusion-based generative models [Ho et al., 2020, Song et al., 2021] have become the standard approach for text-to-image generation, owing to their stable training dynamics, comprehensive data distribution coverage, and ability to produce high-quality, diverse images. One of their key applications is text-guided image editing, which leverages iterative sampling control to enable fine-grained image modifications. Most text-guided editing methods begin with an inversion step that follows the estimated probability flow ODE (PF-ODE) trajectory of pretrained score-based diffusion model. This process yields latent representation xT in the models prior space, aligned with the source prompt. The resulting representation then serves as the starting point for generation, conditioned on target prompt that specifies the desired edits. To improve editing quality, various approaches have been proposed, including optimization-based techniques [Mokady et al., 2022], attention manipulation methods [Cao et al., 2023, Hertz et al., 2022], and guidance-driven strategies [Bansal et al., 2023, Titov et al., 2024]. For example, Guide-and-rescale [Titov et al., 2024] caches latent representations during the forward process to better align the sampling trajectory. In parallel, variety of distillation-based methods have been proposed to reduce the number of inference steps required for image generation. These methods can be broadly categorized into two groups: ODE-based approaches [Song et al., 2023, Salimans and Ho, 2022] and fast generator-based models [Sauer et al., 2024, Yin et al., 2024]. ODE-based methods preserve the theoretical underpinnings of diffusion by optimizing solvers for the backward differential equation, while generator-based models train neural network Gθ to map standard Gaussian noise directly to high-quality images in just few steps. Diffusion distillation has shown promising results in generation tasks, often achieving quality comparable to that of full-step diffusion models. Distilled methods often combine optimization and inversion techniques [Samuel et al., 2025, Tian et al., 2025, Garibi et al., 2024], as their inversion Preprint. Under review. properties differ from those of full-step models. Some fast methods also rely primarily on inversion [Starodubcev et al., 2024, Deutch et al., 2024]. In particular, Starodubcev et al. [2024] base their inversion approach on two consistency models, one dedicated to the inversion process and the other to generation. However, existing methods have notable limitations. Full-step approaches produce impressive editing quality, but they are computationally expensive. Optimization-based methods require even more time due to additional iterations. Despite their success in image generation, distilled methods still struggle with editing tasks, largely due to limitations in inversion quality. The theoretical structure of distilled models constrains their ability to act as forward ODE solvers [Samuel et al., 2025], as the approximation error becomes too large. In our research, we find that image reconstruction remains weak in distilled methods, limiting their practical use for editing. Since inversion quality defines lower bound on content and detail preservation, improving it is essential for high-fidelity image editing. In our work, we adopt consistency models [Song et al., 2023] as baseline for improving inversion, due to their structure, which preserves the probability-flow nature of diffusion. Following Starodubcev et al. [2024], we specifically focus on the forward consistency model, as it is primarily responsible for inversion quality. Since distilled methods operate with small number of steps for both inversion and generation, we propose targeted method to enhance image reconstruction quality through full-process optimization. We introduce cycle-consistency loss that reduces structural and semantic differences between the original image and its reconstruction through fine-tuning the forward consistency model. Unlike full-step diffusion methods, where direct backpropagation through the entire inversion and generation pipeline is computationally infeasible, our approach is applicable to fast models, and we demonstrate its effectiveness in this work. We use pretrained models and keep the backward model frozen to preserve the generation quality. Our optimization significantly improves image inversion according to image preserving metrics such as LPIPS and MSE. Furthermore, our fine-tuning enhances editing quality, even without relying on additional techniques such as Prompt-toPrompt or MasaCTRL. In contrast to several competing methods, our approach requires no additional blend words to outperform them. We achieve strong editing results simply by switching the source prompt to target after inversion. We also adapt the Guide-and-Rescale [Titov et al., 2024] method to consistency models, enabling smooth control and an accurate trade-off between content preservation and editability. We validate our approach through extensive experiments on multiple datasets for image editing and reconstruction. Our main contributions are as follows: We propose cycle-consistency optimization method applied to the full-process optimization of image inversion and generation. This approach outperforms existing distilled methods in image reconstruction tasks, enhances baseline image editing techniques, and increases overall editing capacity. The improved inversion quality enables us to adapt self-guidance mechanism for guidancedistilled consistency models. Our method outperforms existing image editing approaches using the same number of steps [Starodubcev et al., 2024, Deutch et al., 2024, Xu et al., 2023b, Samuel et al., 2025], and achieves results comparable to full step diffusion models [Mokady et al., 2022, 2023, Titov et al., 2024] while being several times faster."
        },
        {
            "title": "2 Related work",
            "content": "Diffusion-based approaches are widely used for image generation due to their rich priors, which are capable of representing diverse and high-quality semantic content. These properties make them particularly suitable for image editing. Most approaches rely on an inversion procedure, where the sampling process is reversed using pretrained model to obtain latent representation of the input. This representation then serves as the starting point for new generation process, conditioned on the editing prompt. Editing methods aim to strike balance between incorporating new information from the target prompt and preserving alignment with the original content. They are commonly categorized into three groups: optimization-based, attention manipulation, and guidance-driven methods. Editing with full-step diffusion models The distinctions between these approaches are especially pronounced in full-step methods. Optimization-based approaches [Miyake et al., 2024, Mokady et al., 2022] perform per-sample inversion optimization, which involves additional, computation2 ally expensive iterations during the editing process. These methods improve inversion quality by optimizing prompt embeddings. Attention-based approaches [Cao et al., 2023, Hertz et al., 2022] demonstrate strong performance but often lack fine-grained controllability. Prompt-to-Prompt exploits cross-attention by preserving differences between source and target prompts and adjusting attention maps accordingly. For shared tokens, the original maps from the source inference are retained; for new tokens, the maps are updated to reflect the target prompt. This method often requires either an auxiliary model for text alignment or carefully selected blend words to enable effective editing. MasaCTRL, in contrast, introduces mutual self-attention and proposes replacing keys and values in the self-attention layers of the target prompt with those from the source prompt inference. Guidance-driven approaches [Titov et al., 2024, Bansal et al., 2023] use energy-based functions to align the generation trajectory with predefined conditions. For example, Guide-and-Rescale modifies the trajectory based on feature differences observed in the U-Net upsampling blocks. Editing with accelerated diffusion models Distilled methods trade precise control for faster inference, and their inversion quality is typically lower than that of full-step models due to the reduced number of diffusion steps. To compensate, accelerated approaches often combine multiple techniques. InfEdit [Xu et al., 2023b] integrates MasaCTRL [Cao et al., 2023] and Prompt-toPrompt [Hertz et al., 2022] within virtual inversion framework. GNRi [Samuel et al., 2025] and PostEdit [Tian et al., 2025] perform optimization guided by energy-based functions, following the principles of guidance-driven editing. Invertible Consistency Distillation [Starodubcev et al., 2024] trains separate forward and backward models for inversion and generation, which are then combined with Prompt-to-Prompt to improve content preservation."
        },
        {
            "title": "3 Preliminaries",
            "content": "Diffusion model Our method is based on Classifier-free guidance (CFG) distilled Stable Diffusion v1-5 [Rombach et al., 2022]), latent diffusion text-to-image model (LDM) that encodes images into low-dimensional space using variational autoencoder (VAE). Classifier-free guidance [Ho and Salimans, 2022] strengthens the models focus on the textual prompt by adjusting the predicted noise using the formula: ˆϵθ(zt, t, y) = ϵθ(zt, t, ) + ω (ϵθ(zt, t, y) ϵθ(zt, t, )) key limitation of this approach is that it requires two forward passes per diffusion timestep t. To address this, classifier-free guidance distillation [Meng et al., 2023] is used to approximate CFG with single forward pass via an additional MLP layer, and is particularly common in diffusion distillation approaches. (1) Guidance Following Ho and Salimans [2022], diffusion model can be enhanced with additional conditioning signals by incorporating energy functions g, which guide samples toward target distribution. To enable this, it is crucial that be differentiable with respect to zt. Guidance is applied by adjusting the predicted noise in the same way as in classifier-free guidance (CFG), as shown in Equation 1: ˆϵθ = ϵθ(zt, y, t) + γ ztg(zt), (2) where γ is the guidance coefficient. For example, Titov et al. [2024] use such functions to align self-attention maps from the generation process with those obtained during inversion. Guidance has also been used to improve inversion itself. In particular, Samuel et al. [2025] introduce strong prior as guidance term to help maintain zt within the correct latent distribution. Consistency distillation We use consistency-distilled models (Song et al. [2023]) for their theoretical foundation in approximating function fθ that maps noisy point zt at any timestep of the diffusion ODE trajectory to its origin z0. Given pretrained teacher diffusion model ϵψ, this is achieved through the consistency distillation objective: LCD(θ) = E[d(fθ(ztn1, tn1), fθ(ztn , tn))] min θ , (3) where ztn1 is obtained by applying one solver step of the teacher model. This loss enforces the self-consistency property: fθ(zt, t) = fθ(zt, t) [t0, tN ] Figure 1: (a): Visual comparison of results produced by our fine-tuned model and the baseline. (b): Quantitative evaluation of the reconstruction quality of our method and the baseline on the MS-COCO validation set. Since it is computationally difficult to find parameters θ that fully capture the data distribution, higher sample quality can be achieved through multi-step sampling. To this end, Song et al. [2023] propose an iterative stochastic procedure that gradually reduces the noise amplitude. Inversion in consistency models Invertible Consistency distillation [Starodubcev et al., 2024] demonstrates that image inversion can be achieved using forward consistency model (fCM), which is trained jointly with backward consistency model (CM). The ODE trajectory is divided into multiple segments: the fCM is trained to map any point within segment to its final boundary, while the CM maps it to the starting boundary. The consistency distillation loss from Equation (4) is adapted for both the fCM and CM training objectives and is combined with additional preservation losses for forward (Lf ) and backward (Lr) models. Boundary points are computed using DDIM solver applied to the teacher model, and zs0 denotes the VAE latent corresponding to the original image x0. The main purpose of the additional preservation losses is to ensure consistency between the forward and backward models (fCM and CM)."
        },
        {
            "title": "4 Method",
            "content": "4.1 Global Consistency Inversion Alignment Diffusion-based image editing typically involves noising an input image using source prompt, followed by denoising with target prompt. core requirement for high-quality edits is the accurate reconstruction of the original image. If sufficient content from the original image is not preserved under the source prompt, the resulting edits will likely lack semantic fidelity and visual coherence. Moreover, image editing approaches often enforce that the sample zt follows the trajectory defined by the source prompt during the generation process, treating this trajectory as reference path for generation. The existing iCD [Starodubcev et al., 2024] approach attempts to enforce local consistency across timesteps by aligning trajectories of the forward and backward models using preservation losses. However, these local constraints do not guarantee global alignment between the original image and its latent representation. Direct optimization for reconstruction in full-step diffusion models is computationally infeasible due to the need for backpropagation through approximately 100 U-Net evaluations. Fortunately, this becomes tractable in accelerated models, where the number of model calls is reduced by roughly factor of 10. We exploit this property and propose novel fine-tuning strategy for the forward consistency model (fCM), which introduces cycle-consistency loss to improve global alignment. Let θ and θ+ denote the pretrained weights of the forward and backward consistency models, respectively. We define: Forward noising function Fθ : Takes an image x0, encodes it via the VAE, and performs four forward passes through the fCM to produce latent representation z4. Backward generation function Gθ+: Takes z4 as input and generates an approximation ˆx0 of the original image via the backward CM and VAE decoder. (See Figure 9 for an overview.) To improve reconstruction, we optimize forward model using perceptual reconstruction loss: Lrec(x0) = LPIPS(Gθ+(Fθ(x0)), x0) min θ (4) 4 In addition, we retain LCD(θ) to preserve internal consistency, and Lf (θ, θ+) to ensure local alignment between the forward and backward models. Since the inversion and generation each require only four steps, backpropagation is computationally feasible relative to full-step diffusion approaches. To maintain the generation quality of the base iCD model, we freeze the backward CM and fine-tune only the forward CM, which directly affects the inversion quality. Both models are initialized from public iCD checkpoints. Our cycle-consistency loss in Equation 4 significantly improves inversion quality and enhances content preservation in editing tasks (see Figure 1). Unlike the baseline iCD model, which relies on Prompt-to-Prompt [Hertz et al., 2022] to maintain content preservation, our method eliminates the need for this mechanism. It also surpasses it in editing quality. The improved inversion fidelity of our approach enables more accurate and visually coherent edits through simple noising and denoising procedure (see Figure 2). Figure 2: (a): Visual comparison of editing results produced by our fine-tuned model and the baseline. (b): Quantitative evaluation of the editing results from our method and the baseline. 4.2 Image Editing with Guidance Although our model produces high-quality edits using only the source and target prompts, certain challenging cases where the target prompt dominates require more precise control. To address this, we adopt guidance mechanism inspired by Guide-and-Rescale [Titov et al., 2024]. We extend this mechanism to consistency-based models by incorporating gradient-based guidance during the denoising stage. During the forward noising phase, we cache 4 . Editing begins by initializing z4 = 4 and denoising it using the backward CM, following multi-step procedure [Heek et al., 2024]. At each denoising step, we adjust the predicted noise using gradient derived from an energy function to improve coherence with the source image: 1 , 2 , 3 , ˆϵθ = ϵθ(zt, ytrg, t) + γ ztg(zt, , t, ysrc), ztn1 = αtn1 (cid:18) ztn σtn ˆϵθ αtn (cid:19) + σtn1 ˆϵθ, (5) (6) We use self-attention guider to align the self-attention maps between zt and during generation, which helps preserve the overall layout of the initial image. In addition, feature guider is employed to align visual features and enhance local detail consistency. , t, ysrc) = 1 Guiders The self-attention energy function is defined as: g(zt, 2 2, where is the number of U-Net layers. Aself Aself denotes self-attention maps comi puted from the forward trajectory using ϵθ(z refers to those from the sampling trajectory, computed using ϵθ(zt, t, ysrc). To better preserve local details, Titov et al. [2024] propose computing the difference between the ResNet up-block features of the U-Net, extracted from ϵθ(z The corresponding en2 Here, Φ = ergy function is defined as: features(ϵθ(z , t, ysrc)) and Φ = features(ϵθ(zt, t, ytrg)) denote the extracted visual features. , t, ysrc, ytrg, Φ, Φ) = mean Φ Φ2 , t, ysrc) and from ϵθ(zt, t, ytrg). , t, ysrc), and Aself i=1 Aself g(zt, (cid:80)L 5 Figure 4: Schematic illustration of the Cycle-Consistency method with guidance. Image editing is performed by noising over four steps using the fine-tuned forward consistency model, followed by denoising with corrections from the guider energy function. Noise rescale Strong guidance from energy functions can significantly reduce editability. Guide-and-Rescale Titov et al. [2024] proposes rescaling the coefficients of the energy functions based on term from the CFG formula (see Equation 1). Specifically, the norm of the difference ω (ϵθ(zt, t, ytrg) ϵθ(zt, t, )) is used to define the scaling factor γ. However, since we use guidance-distilled model, performing an additional forward pass solely to compute this coefficient is redundant. Instead, we propose using the difference (ϵθ(zt, t, ytrg)ϵθ(z , t, ysrc)) as an estimate of the relative influence of the target prompt, since ϵθ(z , t, ysrc) is already computed as part of the guider functions and can be reused. We define the current rescaling ratio rcur(t) as: rcur(t) = (ϵθ(zt, t, ytrg) ϵθ(z (cid:80) ztgi(zt, , t, ysrc))2 , t, ysrc, ytrg)2 2 2 , (7) Figure 3: Visualization of editing results produced by prompt switching (second column) and by our method with guidance (third column). γ = r(t) rcur(t), where r(t) is dynamic multiplier that depends on the timestep and two hyperparameters, rlower and rupper, providing flexible control over editing strength. We follow the same strategy for r(t) as in Guide-and-Rescale. The full pipeline of our method is described in Figure 4."
        },
        {
            "title": "5 Experiments",
            "content": "Fine-tune setup Fine-tuning is performed by optimizing the LPIPS objective with VGG-16 backbone [Simonyan and Zisserman, 2015, Zhang et al., 2018], as it captures structural and perceptual differences relevant to image reconstruction. Images are divided into nine 224x224 patches to match the VGG-16 training setup. We freeze the backward consistency model and optimize only the forward consistency model parameters θ over 6000 iterations using total batch size of 16. To keep local consistency properties within each segment, we also retain the forward preservation loss Lf , along with the consistency distillation loss, to enforce the consistency properties of forward model. Fine-tuning is conducted on the training split of MS-COCO [Lin et al., 2015] and evaluated on the validation split. 6 Figure 5: Examples of image reconstruction obtained from our method and from other approaches. Inversion and editing setup We evaluate inversion and editing performance on multiple datasets. For inversion experiments, we use Pie-Bench [Ju et al., 2023] for qualitative evaluation, and more than 2700 high-resolution images from the MS-COCO [Lin et al., 2015] for quantitative evaluation. For image reconstruction, we use classifier-free guidance equal to zero for all methods (see Appendix A.2). For editing experiments, we use 420 images from Pie-Bench, following Starodubcev et al. [2024], which includes broad range of edit types for qualitative and quantitative evaluation. Additionally, we use custom set of 60 images that feature object replacement (e.g., animals), local emotion changes, and appearance modifications. Unlike iCD [Starodubcev et al., 2024], we adopt dynamic classifierfree guidance (CFG) schedule, rather than simply disabling CFG at the first step. Specifically, we start with zero CFG at the first step, increase it to 7 at the second step, to 11 at the third step, and to 19 at the final step. We found that guidance should be enabled during the early steps to support structural edits. However, high level can result in supersaturated images. During our experiments, we vary the feature and self-attention guider coefficients, as well as the lower and upper bounds for noise rescaling. Our method does not rely on blend words, either when editing with guidance or without it. All baseline methods were run with their default settings as provided by the authors or official implementations. 5.1 Image Inversion We compare our method with iCD [Starodubcev et al., 2024], GNRi [Samuel et al., 2025], DDIM inversion [Song et al., 2022] and ReNoise SDXL-Turbo [Garibi et al., 2024] on the image reconstruction task. Qualitative evaluation We find that our method performs significantly better on well-defined Pie-Bench prompts. subset of our results is shown in Figure 5, where our approach outperforms other methods, including full-step DDIM, in terms of structural consistency and detail preservation (see Appendix A.3 for more examples). Quantitative evaluation We evaluate image reconstruction quality using mean-squared error (MSE), ImageReward and LPIPS. In Table 1 and in Figure 6, we demonstrate that our method outperforms existing fast inversion methods and is only slightly behind full-step DDIM inversion, with most of the error attributed to approximation mismatches between adjacent timesteps t. However, the LPIPS gap is significantly smaller compared to other methods, and the ImageReward score is approximately the same demonstrating that the result is sufficiently accurate for an accelerated approach. Table 1: Metrics for image reconstructions obtained using our method and other approaches on the MSCOCO validation set. Model DDIM (50 steps) GNRi (4 steps) iCD (4 steps) ReNoise SDXL-Turbo (4 steps) Ours (4 steps) MSE ImageReward LPIPS 0.027 0.085 0.081 0.140 0.077 0.369 0.420 -0.028 0.370 0.344 0.268 0.424 0.372 0.444 0. Figure 6: Quantitative evaluation of our method and other approaches on the image reconstruction task on the MSCOCO validation set. 5.2 Text-guided image editing To validate our approach, we compare it with leading fast methods (iCD [Starodubcev et al., 2024], TurboEdit [Deutch et al., 2024], InfEdit [Xu et al., 2023b], ReNoise SDXL-Turbo [Garibi et al., 2024]) and full-step diffusion-based methods (NTI [Mokady et al., 2022], NPI [Miyake et al., 2024], Guide-and-Rescale [Titov et al., 2024]) Qualitative evaluation We present subset of our results in Figure 7. As can be seen, our method enables precise edits while preserving the context and details of the original image. Despite the strong influence of the target prompt, ReNoise and TurboEdit exhibit low level of content preservation, iCD outputs often contain artefacts and fail to maintain subject identity. InfEdit significantly reduces editability while strongly preserving the original image. Some images show no visible edits at all, while others exhibit incomplete or minimal changes (see Appendix A.4 for more examples). For NPI and NTI, our approach provides more precise edits. Furthermore, it achieves results comparable to those of full-step diffusion-based models. Quantitative evaluation We evaluate results using ImageReward [Xu et al., 2023a], DINOv2 [Oquab et al., 2024], LPIPS, and CLIPScore [Hessel et al., 2022]. Most accelerated models tend to achieve stronger edit impact at the cost of content preservation. Elevated DINOv2 cosine distances and LPIPS metrics reflect the difficulty accelerated diffusion models encounter in preserving structural and visual detail. Our method outperforms nearly all accelerated approaches in preserving image content, achieving results comparable to full-step methods, while maintaining sufficient level of editing strength. Although InfEdit shows better scores in preserving image content, it performs worse in editing metrics such as ImageReward and CLIPScore. In addition, InfEdit requires more sampling steps and relies on additional blend words for Prompt-to-prompt [Hertz et al., 2022]. In comparison with full-step methods, our method outperforms NPI and NTI, and achieves results comparable to Guide-And-Rescale. While the CLIPScore is lower, ImageReward is higher, showing that the edits are more aligned with human preferences despite being less favored by CLIP-based evaluation. Our LPIPS scores are higher, while DINOv2 cosine similarity is better. This suggests that our method preserves semantic and structural content more effectively, even though perceptual similarity appears lower. Overall results we present in Table 2 and in Figure 8. Our approach demonstrates strong performance across both settings, enabling smooth trade-off between fidelity and content preservation. 5.3 Ablations Qualitative evaluation In Figure 3 we show that editing with guidance significantly improves subject identity preservation, detail retention and structural consistency, especially when the target prompt has significantly stronger influence than the source prompt. Quantitative evaluation We show in Table 3 that guidance-based editing improves content preservation, which we found to be key to better visual quality. At the same time, we apply the guidance approach to the baseline model and show that this is not sufficient to achieve the same level of quality. 8 Figure 7: Examples of image editing results obtained using our method with guidance and other approaches. Table 2: Metrics for results produced by our method and other baselines Model DINOv2 LPIPS CLIP IR Many-step methods NPI (50 steps) NTI (50 steps) ReNoise (50 steps) GaR (50 steps) InfEdit (12 steps) Few-step methods TurboEdit (4 steps) GNRi (4 steps) iCD (4 steps) ReNoise Turbo (4 steps) Ours (4 steps) 0.632 0.795 0.504 0.721 0.781 0.663 0.685 0.701 0.561 0.747 0.302 0.250 0.446 0.277 0.236 0.358 0.394 0.323 0.426 0.296 0.302 0.294 0.315 0.307 0. 0.307 0.298 0.302 0.307 0.302 0.224 -0.034 0.362 0.249 0.158 0.536 0.199 0.100 0.374 0."
        },
        {
            "title": "6 Conclusion",
            "content": "Figure 8: Quantitative evaluation of editing results obtained by our method and other baselines on the Pie-Bench dataset We propose novel approach for forward consistency model optimization over the entire process of image reconstruction, including inversion and generation. Our method outperforms other distilled approaches on the image reconstruction task. We adapt the Guide-and-Rescale framework for guidance-distilled consistency models, enabling smooth trade-off between editing strength and content preservation. Our method outperforms other accelerated models and achieves results comparable to full-step diffusion-based models on image editing tasks. Table 3: Guidance and image reconstruction optimization ablation Model DINOv2 IR LPIPS CLIP iCD w/o guidance iCD with guidance Ours w/o guidance Ours with guidance 0.599 0.642 0.719 0.747 0.402 0.371 0.313 0.279 0.38 0.357 0.312 0.296 0.31 0.308 0.304 0."
        },
        {
            "title": "References",
            "content": "Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models, 2023. URL https: //arxiv.org/abs/2302.07121. Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing, 2023. URL https://arxiv.org/abs/2304.08465. Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, and Daniel Cohen-Or. Turboedit: Text-based image editing using few-step diffusion models, 2024. URL https://arxiv.org/abs/2408. 00735. Daniel Garibi, Or Patashnik, Andrey Voynov, Hadar Averbuch-Elor, and Daniel Cohen-Or. Renoise: Real image inversion through iterative noising, 2024. URL https://arxiv.org/abs/2403. 14602. Jonathan Heek, Emiel Hoogeboom, and Tim Salimans. Multistep consistency models, 2024. URL https://arxiv.org/abs/2403.06807. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Promptto-prompt image editing with cross attention control, 2022. URL https://arxiv.org/abs/ 2208.01626. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning, 2022. URL https://arxiv.org/abs/ 2104.08718. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance, 2022. URL https://arxiv. org/abs/2207.12598. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL https://arxiv.org/abs/2006.11239. Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code, 2023. URL https://arxiv.org/abs/2310. 01506. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405.0312. Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models, 2023. URL https://arxiv.org/ abs/2210.03142. Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki Tanaka. Negative-prompt inversion: Fast image inversion for editing with text-guided diffusion models, 2024. URL https://arxiv.org/ abs/2305.16807. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models, 2022. URL https://arxiv.org/abs/2211. 09794. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 60386047, June 2023. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. URL https://arxiv.org/abs/2304.07193. 10 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, June 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models, 2022. URL https://arxiv.org/abs/2202.00512. Dvir Samuel, Barak Meiri, Haggai Maron, Yoad Tewel, Nir Darshan, Shai Avidan, Gal Chechik, and Rami Ben-Ari. Lightning-fast image inversion and editing for text-to-image diffusion models, 2025. URL https://arxiv.org/abs/2312.12540. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation, 2024. URL https://arxiv.org/abs/2403.12015. Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition, 2015. URL https://arxiv.org/abs/1409.1556. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. URL https://arxiv.org/abs/2010.02502. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021. URL https://arxiv.org/abs/2011.13456. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models, 2023. URL https://arxiv.org/abs/2303.01469. Nikita Starodubcev, Mikhail Khoroshikh, Artem Babenko, and Dmitry Baranchuk. Invertible consistency distillation for text-guided image editing in around 7 steps, 2024. URL https: //arxiv.org/abs/2406.14539. Feng Tian, Yixuan Li, Yichao Yan, Shanyan Guan, Yanhao Ge, and Xiaokang Yang. Postedit: Posterior sampling for efficient zero-shot image editing, 2025. URL https://arxiv.org/abs/ 2410.04844. Vadim Titov, Madina Khalmatova, Alexandra Ivanova, Dmitry Vetrov, and Aibek Alanov. Guideand-rescale: Self-guidance mechanism for effective tuning-free real image editing, 2024. URL https://arxiv.org/abs/2409.01322. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation, 2023a. URL https://arxiv.org/abs/2304.05977. Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. Inversion-free image editing with natural language, 2023b. URL https://arxiv.org/abs/2312.04965. Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T. Freeman, and Taesung Park. One-step diffusion with distribution matching distillation, 2024. URL https: //arxiv.org/abs/2311.18828. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric, 2018. URL https://arxiv.org/abs/ 1801.03924."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Fine-tune setup Figure 9: Diagram of our fine-tune method. We optimize the forward consistency model by backpropagating through the image reconstruction process. patch-wise LPIPS loss is used to enforce perceptual similarity between the original and reconstructed images We present diagram (see Figure 9) of our fine-tuning method. The models are based on guidancedistilled Stable Diffusion 1.5 backbone, with different LoRA adapters (rank 64) used for the forward and backward consistency models. Only the LoRA adapter of the forward consistency model is optimized during training, while all other components remain frozen. Analogously to Starodubcev et al. [2024], we set the learning rate to 1e 6 and the forward preservation coefficient to 1.5. The total number of iterations is 6000, with convergence typically reached around iteration 3000. The coefficient for our reconstruction loss is set to 1.0, and the total batch size is 16. We utilize LPIPS as the reconstruction loss, since other latent-based variants (Huber, L2) do not perform well and often result in structural and visual mismatches. Fine-tuning is performed using four H100 GPUs. We find that using zero timestep for noising in our loss yields the best results. However, for fCD and the forward preservation loss, we retain timestep of 19, as in Starodubcev et al. [2024], to ensure better coherence with the initial model. Classifier-free guidance (CFG) is disabled during fine-tuning in order to preserve the models ability to respond sensitively to editing operations. A.2 Editing and inversion setup In our experiments we adopt the following notation: ˆϵθ(zt, y, t) = ϵθ(zt, , t) + (1 + ω) (ϵθ(zt, y, t) ϵθ(zt, , t)), which is widely used in works on guidance-distilled models. Since CFG in these models has detrimental effect on overall picture quality, we mitigate this issue by gradually increasing CFG throughout the generation process. Following the notation of guidance-distilled models, 0 for the first step, 7 for the second, 11 for the third, and 19 for the fourth step, instead of deactivating CFG at the first step and using 19 for all subsequent steps. Deactivating or reducing CFG for the second, third 12 and fourth steps decreases editability, while using high CFG at the second and third steps introduces not only structural and semantic edits but also leads to over-saturation of the resulting image. For editing with guidance, we set the self-attention guider weight to 20000 and the feature guider weight to 0.5. Noise rescale is required to enhance the overall robustness of the method, if the norm of the difference ϵθ(zt, ytrg, t) ϵθ(z , ysrc, t) exceeds the norm of the sum of gradients of the guiders, we limit the effect of the guiders to prevent visual artefacts by setting the upper bound rupper to 1.0. The lower bound rlower is set to zero, since small norm of the difference allows guidance to be disabled. For inversion we disable CFG for all steps and use the source prompt for inversion and generation processes. A. Inversion results Figure 10: Examples of image reconstruction obtained using our method and from other approaches. 13 A.4 Editing results Figure 11: Examples of image editing results obtained using our method with guidance and other approaches."
        },
        {
            "title": "B Limitations",
            "content": "Since the LPIPS backbone is trained to operate in pixel space, our method requires additional backpropagation through VAE decoder, which increases the overall computational cost of optimization. Our method involves loading two consistency models, both based on the same guidance-distilled Stable Diffusion v1.5 backbone, but each equipped with different LoRA adapter of rank 64. Due to the nature of guidance distillation, our approach may produce over-saturated outputs in image editing tasks, resulting in overly vibrant colors."
        }
    ],
    "affiliations": [
        "AIRI",
        "HSE University",
        "Innopolis",
        "Sber"
    ]
}