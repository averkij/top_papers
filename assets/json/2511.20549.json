{
    "paper_title": "Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning",
    "authors": [
        "Guanjie Chen",
        "Shirui Huang",
        "Kai Liu",
        "Jianchen Zhu",
        "Xiaoye Qu",
        "Peng Chen",
        "Yu Cheng",
        "Yifu Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion Models have emerged as a leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is a promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriously unstable and easily falls into reward hacking. In this work, we introduce Flash-DMD, a novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only $2.1\\%$ its training cost. Second, we introduce a joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as a powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and text-image alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 9 4 5 0 2 . 1 1 5 2 : r Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning Guanjie Chen1,2* Xiaoye Qu3 Shirui Huang2* Kai Liu2 Jianchen Zhu2 Peng Chen2 Yu Cheng4 Yifu Sun2 1Shanghai Jiao Tong University 2Tencent 3Huazhong University of Science and Technology 4The Chinese University of Hong Kong chenguanjie@sjtu.edu.cn yifusun@tencent.com Figure 1. Samples from 4-step Flash-DMD on SDXL and SD3-Medium. Flash-DMD takes less than 3% training cost of DMD2 [52]."
        },
        {
            "title": "Abstract",
            "content": "Diffusion Models have emerged as leading class of generative models, yet their iterative sampling process remains computationally expensive. Timestep distillation is promising technique to accelerate generation, but it often requires extensive training and leads to image quality degradation. Furthermore, fine-tuning these distilled models for specific objectives, such as aesthetic appeal or user preference, using Reinforcement Learning (RL) is notoriIn ously unstable and easily falls into reward hacking. *Equal Contribution. Corresponding Authors. this work, we introduce Flash-DMD, novel framework that enables fast convergence with distillation and joint RL-based refinement. Specifically, we first propose an efficient timestep-aware distillation strategy that significantly reduces training cost with enhanced realism, outperforming DMD2 with only 2.1% its training cost. Second, we introduce joint training scheme where the model is fine-tuned with an RL objective while the timestep distillation training continues simultaneously. We demonstrate that the stable, well-defined loss from the ongoing distillation acts as powerful regularizer, effectively stabilizing the RL training process and preventing policy collapse. Extensive experiments on score-based and flow matching models show 1 that our proposed Flash-DMD not only converges significantly faster but also achieves state-of-the-art generation quality in the few-step sampling regime, outperforming existing methods in visual quality, human preference, and textimage alignment metrics. Our work presents an effective paradigm for training efficient, high-fidelity, and stable generative models. Codes are coming soon. 1. Introduction Diffusion models [8, 12, 16, 33, 37] have demonstrated remarkable success in text-to-image generation in recent years. Numerous iterative denoising steps poses significant obstacle to real-time or resource-constrained deployment. To address this issue, various diffusion distillation techniques have been developed to distill multi-step teacher diffusion models into efficient student models that can produce comparable image quality in just one or few inference steps [3, 10, 23, 26, 27, 45, 52, 53]. However, most existing distillation methods require thousands of GPU hours for training. This significantly limits its accessibility to research groups and institutions with limited resources, and hinders rapid deployment in practical applications. Among existing distillation methods, Distribution Matching Distillation (DMD) methods [10, 26, 52, 53] stand out for their superior generative quality, leveraging variational score distillation objectives [54] to align the output distributions of student and teacher models. However, this objective function suffers from unstable training and tendency to mode seeking. Some approaches have employed adversarial methods to mitigate these problems. DMD2 [52] proposes latent adversarial regulations with real images and designs Two-Time scale Update Rule (TTUR) to stabilize training, but it combines the GAN [40, 41] framework with DMD in naive manner, neglecting the timestep aware feature of timestep distilled diffusion models, and the fake score µf ake is trained both to discriminate real and generated images, but also to track the distribution changes of student models. These design compromises its efficiency in matching the distribution of teacher models. Furthermore, DM loss is inefficient in the latter part of distillation as it is hard to guide detailed learning, preventing it from effectively guiding the student diffusion model. These observations motivate our core research questions: Q1 In the early phase, how can we more effectively coordinate distribution matching with perceptual realism enhancement to accelerate convergence? Q2 In the later phase, how can we more effectively refine the student model for better visual details and perceptual fidelity in direct way? To address the inefficiencies of the distribution matching methods, we proposed Flash-DMD, twofold method in the few-step distillation task that outperforms DMD2 with much smaller training cost, while achieving superior perceptual realism. Specifically, our Flash-DMD follows different principles in the early and later generation phases. In the early phase, as the denoising performance at different timesteps varies, the distillation target should also differ. To this end, we decouple the adversarial training and distribution matching frameworks with timestep aware strategy. Specifically, at high-noise timesteps, the denoising models primary objective is to learn global composition and structure from the teacher. Considering DM loss is effective to process noisy latents, we use pure DM loss to align the student model with the teacher models output distribution. At low-noise timesteps, the model focuses on refining finegrained details and enhancing perceptual realism. Thus, we use Pixel-GAN to match the distribution of real images and improve the photorealism of the generated images. In the later phase, we further optimize generation quality to align with human preferences. Previous reinforcement learning works on few-step distilled models [30, 36] suffer from the reward hacking phenomenon, which produces oil painting artifacts. We combine the Distillation framework with latent reinforcement learning designed especially for fewstep models to refine the their handling of fine-grained details efficiently. The framework can effectively alleviating reward hacking problem. By combining faster convergence in the early phase with joint finer optimization in the latter phase, we demonstrate the efficiency and superior performance of our method of distilling from SDXL to produce high-quality, realistic images. In particular, our method achieves the highest human preference scores while requiring the lowest training cost in DMD series methods to date. To summarize, our main contributions are threefold: At the first stage, we decouple the training objectives of Distribution Matching series via timestep-aware strategy to efficiently distill the fundamental distribution of the teacher model in low-SNR timesteps and refine perceptual quality and texture in high-SNR timesteps, and we counteract the mode-seeking of the DM loss with Pixel-GAN that robustly enhances realism. We also improve the score estimator for fastest convergence and stabilized distillation. In this stage, we achieve the best performance with only 2.1% training cost of DMD2. At the next stage, we design reinforcement learning specifically for the distilled model and integrate it into the distillation process. These innovations eliminate the need for separate reinforcement and distillation phases, significantly reducing computational training costs, avoiding reward-hacking and achieving the best fine-grained details and perceptual fidelity in few steps of generation. By combining stages 1 and 2, we propose Flash-DMD. Extensive experiments demonstrate that our method 2 Figure 2. Overview of our proposed Flash-DMD. We decouple the distillation objective by timestep into Diffusion Matching loss and an adversarial loss. During high-noise timesteps, the DMD loss enables rapid alignment with the teacher model, while at low-noise timesteps and on real images, Pixel-GAN loss is employed to enhance realism and texture details. This design achieves more efficient distillation. Building upon this, we further introduce reinforcement strategy specifically tailored for few-step distilled models, which seamlessly integrates with the distillation objective to achieve superior and more stable performance. achieves superior performance compared to both the teacher model and baseline in terms of image quality, human preference, and text-image alignment metrics. Our method exhibits strong generalization ability on both score-based diffusion and flow matching models. The prediction is performed by blocks of transformer[13] or UNet[38] networks; Generating complete image requires iterating this reverse step numerous times, making it timeconsuming process. In this work, we focus on distilling the solution of the reverse process more efficiently. 2. Related Work Diffusion Models. Diffusion models[5, 12, 32, 37, 43] are powerful family of generative models that have demonstrated state-of-the-art performance across diverse generative tasks. In text-to-image task, diffusion models operate through two primary stages: forward process and reverse process. The forward process disturbs the real image x0 Preal with noise ϵ (0, I) by stochastic equation, at each timestep {1 . . . }: xt = αtxt1 + 1 αtϵt1 (1) where α determines the noise level and finally xT will reach (0, I). The reverse process then solve the probability flow(SDE) ordinary differential equation to reconstruct x0. The SDE forward of the denoising diffusion probabilistic model (DDPM) is solved as follows, where µθ and Σθ refer to the predicted mean and covariance: pθ(xt1xt) = (xt1; µθ(xt, t), Σθ(xt, t)), (2) 3 Diffusion Distillation. Progressive Distillation [23, 29, 39] reduce inference steps in diffusion models by iteratively halving them, ultimately producing one-step generator. Although effective, this iterative process is computationally expensive and is constrained by the preceding teacher models quality, leading to compounding errors. Consistency Distillation [27, 28] enforces consistency constraint for the diffusion models, stipulating that any point on given trajectory will revert to its starting point. However, it leads to performance degradation in the few-step inference. To migrate the issue, recent works [36, 45, 59] have segmented the trajectory and progressively perform distillation on timestep segments. Adversarial Distillation introduces discriminator to align the few-step students output with the multi-step teachers, either at the pixel level [41] or latent level [40], DMD2 [52]. DMD2 [52] also uses latent adversarial training to match real-world data distribution, but its straightforward combination of adversarial loss and distribution matching may introduce conflicting objectives that can hinder overall distillation efficiency. Score Distillation was adapted for the distillation of diffusion models themselves [9, 31, 53]. An early approach, Distribution Matching Distillation (DMD) [53], aims to minimize the KL-divergence between the teacher and student distributions. DMD2 [52] replaced regression loss with adversarial loss for better realism. Building on this, Adversarial Distribution Matching (ADM) [26] introduced GAN framework with Hinge loss, while SenseFlow [10] optimized scorers and discriminators for efficient distillation of larger models. Reinforcement Learning in T2I Generation. Reinforcement learning is rapidly migrating to image generation tasks to align large-scale diffusion models with human feedback. Direct Preference Optimization (DPO) [17, 20, 21, 30, 44, 57] and Group Relative Policy Optimization (GRPO) [11, 19, 25, 46, 50, 55] are two popular paradigms. The former methods construct offline or online win-lose pairs and back-propagate the preference order by the Bradley-Terry formed objectives. The latter methods sample group of images on the SDE/mixed ODESDE trajectory, calculate the normalized advantage within the group, and constrain the policy generation direction. However, current research on performing RL on few-step models remains quite limited. Pairwise Sample Optimization (PSO)[30] strengthens the relative likelihood margin between the training and reference sets. 3. Methodology 3.1. Preliminary Given pretrained diffusion model Tϕ(xt, t) as teacher model, where xt is noisy sample at timestep U(1, T), DMD [53] and DMD2 [52] distill it into few-step efficient generator Gθ(xt, t) by minimizing the reverse KL divergence between the teacher models distribution pτ and the few-step generators distribution pgen. DMD series methods estimate pτ through score estimator µτ (xt, t), and pgen is tracked with estimator µgen(xt, t). Score function of the diffused distribution is: s(xt, t) = xt αtµ(xt, t) σ2 ; (3) where αt, σt > 0 are scalars determined by the noise schedule. sgen and sτ are vector fields that point towards higher density of distribution. Gradient of Distribution Matching objective w.r.t. θ is, θLDMD = Ez,t (cid:20) sτ (Gθ()) sgen(Gθ()) (cid:21) dGθ() dθ , (4) where (0, I), U(0, T). In addition to the Distribution Matching objective, DMD2 introduces the combination with adversarial training with real images. Gradient of generators adversarial objective w.r.t. θ is, θLAdvGen = Ez,t (cid:20) log (Gθ()) (cid:21) , dGθ() dθ (5) where is the discrimator forward process. The score estimator of teacher Tϕ() is itself, the generators score estimator µψ gen() is initialized with Tϕ(), and is dynamiclly updated to track pgen with diffusion loss: LDiffusion = Ext1,t,ϵN (0,I)[µψ gen(xt, t) ϵ2 2], (6) DMD2 reuses the parameter ψ of µψ gen(xt, t) and extra trainable heads to distinguish pgen and real image distribution preal, gradient of their adversarial objective w.r.t. ψ is, () = log () dD () dψ , (7) ψLAdvDisc = Ez,t,xpreal [P (x) (Gθ())] , 3.2. Training Inefficiency of DMD Series Despite their impressive performance, methods in the DMD series are characterized by significant computational overhead during distillation. This is evident in the extensive training schedules required by prominent models. For instance, the original DMD[53] required 20, 000 iterations with batch size of 2,304 to distill Stable Diffusion v1.5 [37] for single-step generation. Similarly, DMD2 [52] used 24, 000 iterations to distill SDXL [33] for four-step generation, and ADM [26] used 16, 000 iterations for singlestep SDXL distillation. Given the strong empirical results and open-source implementation of DMD2, we select it as the foundation for our investigation into these inefficiencies. One primary source of inefficiency in DMD2 stems from its optimization strategy. As noted by [6], DMD2 simultaneously optimizes the model using two distinct gradients: distribution-matching gradient (Eq. (4)) and an adversarial gradient (Eq. (5)). direct summation of these gradients can introduce conflicting objectives, potentially steering the model toward suboptimal state. This conflict can degrade both the accuracy of the distribution matching and the perceptual quality of the generated images, thereby hindering efficient convergence. second challenge lies in the dual role assigned to the generators score estimator. It is tasked with two demanding objectives: tracking the output distribution of Gθ()(Eq. (6)) and discriminating between real and generated samples (Eq. (7)). To stabilize this complex dynamic, two-time scale update rule (TTUR) is employed in DMD2, where score estimator is updated five times for every single update of the generator Gθ(). This significantly contributes to the models overall training inefficiency. 3.3. Faster Convergence at First Stage Adversarial Training is Necessary. DMD2 framework optimizes the generator by naively summing the 4 Distribution-Matching (DM) loss from the teacher and an adversarial loss against real images at every timestep. This superposition of gradients can result in suboptimal and inefficient optimization. When we remove the adversarial teacher entirely, we observe that under pure DM loss supervision, the generator rapidly converges to suboptimal domain, producing outputs with unnaturally high contrast and lacking fine-grained textures. We attribute this behavior to the mode-seeking nature of the reverse KL divergence, an observation also discussed in ADM [26]. This finding underscores the necessity of the adversarial loss with real images for perceptual fidelity. Decoupling Losses with Timestep-Aware Strategy. We observe that the generators objective changes throughout the denoising process. For Few-step distilled model, the initial, high-noise timesteps (low Signal-to-Noise Ratio, or SNR) primarily establish global composition and structure, and the low-noise timesteps (high SNR) focus on refining details, textures, and color tones to enhance realism. This observation is corroborated by findings of [6] in video generation tasks, which noted that the adversarial training in DMD2 is most active at high SNRs, whereas DM loss excels at guiding the model through high-noise regimes. Based on these insights, we assign DM loss and adversarial loss to distinct timesteps: 1.During the high-noise regime, we optimize the generator exclusively with the DM loss (Eq. (4)). This allows the model to efficiently learn the teachers fundamental distribution and ODE trajectory in the early phases of generation. 2.For the low-noise step, we apply the adversarial loss against real images, enabling the model to refine perceptual quality and texture in the final denoising step. During each generator update, we sample one timestep from high-noise timesteps and xt from DMD2s back-simulation forward process to compute the DM loss θLAT DMD , then employ to propagate the denoised output xt1 to final clean image x0: xt1 = Gθ(xt, t); x0 = Detach(B(xt1, 0)), (8) where Detach denotes stop gradient, x0 is then used for the adversarial loss computation. We perform diffusion forward on x0 at ˆt from low-noise timesteps to obtain the noisy sample ˆx. Gradient for adversarial loss is: θLTA AdvGen = (cid:20) Eˆt,ˆx log (cid:0)V (cid:0)Gθ(ˆx, ˆt)(cid:1)(cid:1) dGθ() dθ (cid:21) , (9) where the D() is the pixel-level discriminator, and is the decode process of VAE. This timestep-aware strategy reduces interference between these two optimization objectives. Our experiments prove that this approach significantly improves training efficiency while generating highquality images with enhanced realism and textural detail. 5 Pixel-GAN Alleviates Mode-Seeking. To enforce realism and structural coherence, the present study performs adversarial learning directly in the pixel space utilizing discriminator. In contrast to conventional latent-space GAN, the discriminator in our model is constructed upon the frozen vision encoder of the Segment Anything Model (SAM)[14] to extract hierarchical features with multiple trainable discriminator heads attached. The discriminators trainable parameters ω are updated via: AdvDisc = Exreal [ log Dω ()] + Ez [log Dω (V())] , (10) LPG The discriminator is characterized by its exceptional sensitivity to local geometric structures and fine-grained textures, capability that is facilitated by SAMs powerful, generalpurpose representations, as noted by [26]. This pixel-level supervision exerts stringent realism constraint from the trainings earliest stages, compelling the generator to expeditiously discern and anchor to diverse, high-fidelity modes within the data distribution. Visualizations and experiments prove it effectively prevents premature convergence to simplistic or blurry solutions (mode-seeking). Stabilize Score Estimator. In contrast to DMD2 [52], where the score estimator is required to serve as discriminator (Eq. (7)) and thus faces conflicting optimization objectives, our approach trains µψ gen solely via the diffusion loss (Eq. (6)), eliminating the tasking burden and training complexity for Gθ(). Our experiments show that updating the score estimator only once or twice per generator update (TTUR=1,2) is sufficient for stable and accurate distribution tracking. This lightweight coupling leads to more stable training dynamics and superior sample fidelity compared to DMD2 with TTUR=5, while reducing computational overhead. Similar to implicit distribution alignment proposed by [10], we also adopt an Exponential Moving Average (EMA) update strategy to ensure that the score estimator µψ gen accurately tracks the evolving distribution of the generator Gθ(). Specifically, after each generator update, we inject the latest generator parameters into the score estimator using an EMA coefficient λema, i.e., ψ λemaψ + (1 λema)θ, (11) gen to closely follow the generators trajecwhich enables µψ tory with minimal additional updates. Putting Everything Together. We introduce FlashDMD, highly efficient framework for Distribution MatchIn summary, Flash-DMDs training obing Distillation. jectives via timestep-aware strategy efficiently distill the fundamental distribution of the teacher model in low-SNR timesteps and refine perceptual quality and texture in the final high-SNR timestep. To counteract the mode-seeking tendency of the DM loss, we introduce SAM-based PixelGAN that robustly enhances realism. The combination of these strategies and the stabilized score estimator enables more effective and balanced optimization of Gθ(). 3.4. Reinforcement Learning for Distilled Model Using the training paradigm above, we have developed student generator that can compete with the teacher model. Subsequently, our focus shifts to enhancing its performance beyond that of the teacher and deploying it in practical scenarios. Preference optimization provides direct way to improve image fidelity and detail richness effectively on diffusion models. However, previous attempts on few-step reinforcement learning like PSO[30] and HyperSD[36] encounter serious reward hacking, phenomenon overfitting on oil painting or smoothed images with less details. Reasons for Serious Reward Hacking. PSO[30] and HyperSD[36] rely on the sampling trajectory to denoise noisy latent iteratively. Preference optimization on clean images confines gradient backpropagation to low-noise timesteps, making the model overfit the reward biases, where the model prioritizes superficial features (e.g., specific color palettes). Specifically, HyperSD utilizes ImageReward [48] and produces overexposed and oil-painted results, as shown in Fig.4. PSO selects PickScore[15] as reward model and generates smoothed images. Improved Preference Optimization for Distilled Model. The direct solution is to cover the sampling trajectory, including high-noise timesteps. First, reward models are required to score noisy latent representations at any timestep. LRM[58] inherently meets our demands. Multiple candidates are sampled at each timestep from shared noise initialization and are rated by LRM to construct win-lose pairs. We further find that not all timesteps are necessary. As shown in Fig.3, with the same initial noise, it is evident that images sampled by few-step distilled model at highnoise steps exhibit better diversity in layout and fine-grained details compared to those from low-noise steps. As result, we only perform stochastic sampling in the high-noise phase, altering latent representations that are deemed optimal/suboptimal by the reward model. Second, we combine the logarithmic likelihood loss with the vanilla loss of Flash-DMD during the training process rather than applying preference optimization separately for stable training. We show that the combination can further boost the generation performance of the accelerated diffusion models towards human preference and text-image alignment. Formal Descriptions. Given generator Gθ() distilled from pretrained diffusion model Tϕ(), it can sample clean images from pure noise zT (0, I), conditioned on text prompt c, within = 4 steps. At high-noise timesteps, we sample set of noisy latent images {z1 t1} from the same initial latent image zt. LRM predicts preference t1, ..., zk Figure 3. Sampling variance analysis at different time steps. The first row displays samples obtained at the 999th denoising step, while the second row corresponds to the 499th step. scores. The samples corresponding to the highest and lowest normalized scores are selected as win-lose pairs, thereby constructing paired training data (zt, zw t1) to the sampling pool. These pairs are subsequently used to minimize the loss function: t1, zl Lrl = [log σ (βH(w, l))] , pθ(zw pref (zw t1zt, c) t1zt, c) H(w, l) = log log (12) pθ(zl pref (zl t1zt, c) t1zt, c) , (13) where σ and β are inherent regularization constants, p(z t1zt, c) denotes the backward process to denoise zt in the LCM scheduler. 4. Experiments 4.1. Implementation Details Experiment Setup For the first phase, we conduct experiment on Score-based diffusion model SDXL[33] and Flow Matching based SD3-Medium[7]. We utilize filtered set from the LAION 5B [42] dataset to provide high-quality image-text pairs for training, following the setting of [51]. For discriminator conditioning, we adopt the vision encoder from [14] as the backbone to extract image representations. The structure of trainable discriminator heads follows the 2D architecture of [26]. For the second phase, we adopt the training dataset from the first phase and utilize the Latent Reward Model from [57], and experiment on distilled 4-step SDXL. We sample set of noisy latent images at the high-noise timestep = 749, 999 and set = 4. We conduct experiments on NVIDIA H20 GPUs. Evaluation Tasks and Baseline The evaluation of image generators is conducted on 10K prompts from COCO 2014 [24], adhering to the DMD2 [51] framework, containing 10,000 images. We present the result of CLIP score [35] (ViT-B/32) to evaluate text-image similarity, and we adopt set of advanced preference-based metrics to thoroughly evaluate the quality of generated images from 6 Table 1. Comparison of Flash-DMD on SDXL under stage 1 with other distillation methods on the COCO-10k dataset. ImgRwd denotes ImageReward score. Cost refers to the product of batch size and training iterations. Best performance is highlight with Bold, and the second is with underline. Method SDXL #NFE ImgRwd CLIP Pick HPSv2 MPS Cost 100 0.7143 0.3295 0.2265 0.2865 11.87 - 4 LCM-SDXL 4 SDXL-Lightning SDXL-Turbo 4 NitroSD-Realism 4 4 NitroSD-Vibrant 4 DMD2-SDXL 0.5562 0.6952 0.8338 0.9112 0.8419 0.8748 0.3250 0.2236 0.2818 0.3268 0.2285 0.2888 0.3302 0.2286 0.2899 0.3274 0.2291 0.2975 0.3201 0.2205 0.2865 0.3302 0.2309 0.2937 11.11 12.15 12.25 12.43 11.13 12.41 - - - - - 128*24k Flash-DMD under Phase TTUR1-1k TTUR2-4k TTUR2-8k TTUR5-18k 4 4 4 4 0.9509 0.9450 0.9740 0.9426 0.3292 0.2322 0.2968 0.3291 0.2322 0.2969 0.3298 0.2327 0.2981 0.3302 0.2319 0.2982 12.67 64k (2.1%) 12.65 12.71 12.63 64*4k 64*8k 64*18k Table 2. Comparison of Flash-DMD on SD3 under stage 1 with other distillation method and baseline on COCO-10k dataset. Method #NFE ImgRwd CLIP Pick HPSv2 MPS Cost SD3-Medium 28 SD3-Flash TTUR2-4k TTUR2-7k 4 4 4 1.0173 0.8459 0.3301 0. 0.2933 0.3258 0.2259 0.2849 12.05 11. - - Flash-DMD under Phase 1 1.0193 1.0214 0.3269 0.3266 0.2285 0. 0.2976 0.2975 12.43 12.46 32*4k 32*7k multiple human-aligned perspectives. We use HPSv2 [47] to measure fine-grained image-text semantic alignment, focusing on how well the generated content adheres to the input prompt. ImageReward [48] and PickScore [15] are employed to assess overall aesthetic quality and perceptual appeal, reflecting general human preferences in visual coherence and composition. Furthermore, we evaluate multidimensional human preferences using MPS [56], recently proposed metric that captures diverse aspects of human judgment, such as object accuracy, spatial relation, and attribute binding-beyond global similarity. Together, these metrics provide comprehensive and humancentric evaluation of both fidelity and preference in textto-image generation. To demonstrate the effectiveness of phase 1 distillation, we compare our 4-step generative models against SDXL[33], as well as other open-sourced timestep distillation methods, including LCM-SDXL [27], SD3-Lighting[23], SDXL-Turbo [41], Realism and Vibrant version of NitroSD [4], Flash-SD3[2], and DMD2 [52]. For phase 2, we further evaluate our approach by comparing it with three reinforcement learning-finetuned models, HyperSDXL [36], PSO-DMD2 [30], and LPO-SDXL [57]. 4.2. Experiment Analysis Phase 1: Efficient Distillation Our method achieves highly efficient distillation from the teacher model, leadTable 3. Comparison of Flash-DMD under phase2 with other models with reinforcement learning on COCO-10k dataset. Method #NFE ImgRwd CLIP Pick HPSv2 MPS GPU Hours Hyper-SDXL PSO-DMD2 LPO-SDXL Flash-DMD 4 4 40 4 1.085 0.9157 1.0417 1. 0.3300 0.2324 0.3285 0.2338 0.3324 0.2342 0.3285 0.2346 0.3030 0.2897 0.2965 0.2930 12.45 12.53 12.58 12.84 400 A100 160 A100 92 A100 12 H20 ing to state-of-the-art performance across all benchmarks, by decoupling the distribution matching (DM) and adversarial losses with timestep-aware strategy, employing PixGAN to alleviate mode-seeking behavior, and stabilizing the generators score estimator. As shown in Tab. 1, we distill SDXL using various two-time update rules (TTUR), which corresponds to the update frequency ratio between the score estimator and generator. In contrast to DMD2, which uses TTUR of 5 and thus hinders training efficiency, we experiment with TTUR values of 1, 2, and 5. Our results demonstrate significant improvements in both efficiency and performance. With TTUR of 5, our model surpasses DMD2 on all benchmarks while requiring only 37.5% of the training cost (batch size training steps). Reducing the TTUR to 2 allows us to maintain superior human preference scores and comparable text-image consistency with only 8.3% of DMD2s training cost. In the most extreme case, training for only 1,000 steps with TTUR of 1, with merely 2.1% of DMD2s training cost, we still yield higher human preference score. Notably, under all tested settings, our model consistently outperforms the original teacher model. We also extend Flash-DMD to SD3-Medium with solid performance, outperforming other methods. Results are shown in Tab. 2. This further demonstrates the generalizability of our method. We further extend Flash-DMD to the SD3-Medium[7] model under the Flow Matching framework, employing LoRA, with TTUR ratio of 2. As shown in Tab. 2, our approach achieves significantly better results using only 4K training steps, substantially outperforming both the teacher model (NFE=28, CFG=7) and SD3-Flash[2] (NFE=4, CFG=0). This demonstrates that our distillation strategy is also highly effective and efficient within the Flow Matching paradigm. Phase 2: Boost Performance with RL. Incorporating reinforcement learning, Flash-DMD achieves performance comparable to other reinforcement approaches with fewer computational resources, as shown in the Tab. 3 and Fig. 4. Flash-DMD scores the highest on PickScore and MPS, and image fidelity surpasses SDXL and other competitors. Although Hyper-SDXL has the highest ImageReward score and HPSv2 score, it generates overexposed colors and unnatural images. LPO-SDXL gets the highest CLIP score, but it produces oversmoothed images. We speculate that the reason may be that these models use the trained mod7 Figure 6. Evaluation results of Flash-DMD (ours) with or without EMA on ImageReward, PickScore, and HPSv2. The training steps range from 1,000 to 8,000. Both models are trained with two-time scale update rule (TTUR). The generator and the score estimator are updated at rate of 1:2, i.e., TTUR=2. Figure 7. Evaluation results of Reinforcement Learning with and without pixel-GAN. Both models use 5:1 setting. Table 4. Comparison of Flash-DMD at stage2 with different variants in reinforcement learning experiments . Method ImgRwd CLIP PickScore HPSv2 MPS GPU Hours TTUR1-1k Phase 0.9508 0.3292 0.2322 0.296 12.672 1:1 2:1 5:1 10:1 Post-Train LPO all noise + pixelgan Flash-DMD Phase 0.9135 0.9315 0.9808 0.9640 0.9795 0.9421 0.9678 1.0004 0.3284 0.3271 0.3275 0.3272 0.3284 0.3294 0.3280 0.3285 0.2330 0.2329 0.2345 0.2344 0.2345 0.2331 0.2345 0. 0.2945 12.755 0.2942 12.770 0.2904 12.764 0.2874 12.685 0.2882 12.689 0.2925 12.800 0.2914 12.812 0.2931 12.813 5.7 5.2 4.8 4.7 5.0 7.3 5.8 12.0 Phase 1: Significance of EMA in Score Estimator. As described in Sec.3.3, we employ an Exponential Moving Average (EMA) strategy to help the score estimator more accurately track the generators distribution, especially under high-frequency updates. To validate the effectiveness of this approach, we conduct an ablation study comparing performance with and without the EMA strategy. As shown in Fig. 6, the model with EMA achieves higher ImageReward and Pickapic scores in later training stages. The HPSv2 scores remain nearly identical. This confirms that the EMA strategy enhances visual quality and human preference without compromising text alignment. Phase 2: Trade-Off in Time Scale Update Rule for RL. Rather than superimposing multiple loss functions via weighted sum, we use an alternating update strategy to update the generator, applying different loss functions at different frequencies. We initialize the generator and the fake score estimator with weights from TTUR1-1k experiment, and the real score estimator is initialized with SDXL. We train the model for 2,000 iterations on single H20 GPU under different frequency ratios between reinforcement loss Figure 4. Qualitative comparisons with other reinforcement approaches on SDXL. com Figure 5. Evaluation results of DMD2(red) and Flash-DMD (blue) with TTUR at the ratio of 2 on SDXL. els directly for reinforcement training. This is prone to reward hacking and only rewards the results preferred by the reward model. Meanwhile, Flash-DMD introduces preference optimization during the training process of Phase 1, with the constraints from distribution matching and PixelGAN, which alleviates the problem of reward hacking. 4.3. Ablation Studies Comparision with DMD2 under Samller TTUR. We set TTUR=2 for ablate the performance of DMD2 and Flash-DMD under phase 1. The results, presented in Fig. 5, show that our method exhibits stable and continuous improvement throughout the training process. In contrast, DMD2 shows slight initial gains but quickly degrades as training progresses. This comparison validates that our approach offers much better training stability and efficiency. 8 and distribution matching loss (1:1, 2:1, 5:1, 10:1). The metric comparisons are shown in Tab. 4. The 5:1 ratio achieves the highest score among these settings. Phase 2: Other Ablation Experiments on Reinforcement Learning. 1. Online training VS Post-training: We compare online training with post-training by LPO [58] alone. Our method demonstrates superior performance over PostTrain LPO, which validates the advantage of our proposed training paradigm. 2.High-noise VS all noise: Training only on high-noise steps achieves better results than training on all-noise steps. 3. Including PixelGAN loss: Furthermore, the incorporation of an additional Pixel-Gan objective yields positive gain, resulting in marginal improvement in the metrics. Fig.7 supplements the results with and without GAN loss across 1,000 to 5,000 iterations. We selected the 5k-step model with GAN loss as our final enhanced version, as it delivered the best performance. The training cost for this model was 12 H20 GPU hours. 5. Conclusion In this paper, we present Flash-DMD, twofold approach that addresses the inefficiencies of existing diffusion distillation methods by via timestep-aware objectives and optimizing the distillation process. In the early phase, FlashDMD accelerates convergence by coordinating distribution matching and perceptual realism enhancement. In the later phase, it refines visual details using latent reinforcement learning while preventing mode collapse and artifacts. Experiments show Flash-DMD achieves superior generation quality and the highest human preference scores with significantly reduced training costs. Our method makes diffusion distillation more efficient and accessible, paving the way for advancements in low-step generative modeling."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1 [2] Clement Chadebec, Onur Tasar, Eyal Benaroche, and Benjamin Aubin. Flash diffusion: Accelerating any conditional diffusion model for few steps image generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1568615695, 2025. 7 [3] Clement Chadebec, Onur Tasar, Eyal Benaroche, and Benjamin Aubin. Flash diffusion: Accelerating any conditional diffusion model for few steps image generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1568615695, 2025. 2 [4] Dar-Yen Chen, Hmrishav Bandyopadhyay, Kai Zou, and YiZhe Song. Nitrofusion: High-fidelity single-step diffusion In IEEE/CVF Conthrough dynamic adversarial training. ference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 7654 7663. Computer Vision Foundation / IEEE, 2025. 7, 1 [5] Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Xiaoye Qu, Tianlong Chen, and Yu Cheng. Towards stabilized and efficient diffusion transformers through long-skip-connections with spectral constraints. arXiv preprint arXiv:2411.17616, 2024. 3 [6] Jiaxiang Cheng, Bing Ma, Xuhua Ren, Hongyi Jin, Kai Yu, Peng Zhang, Wenyue Li, Yuan Zhou, Tianxiang Zheng, Phased one-step adversarial and Qinglin Lu. arXiv preprint equilibrium for video diffusion models. arXiv:2508.21019, 2025. 4, Pose: [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning. 6, 7, 2 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2 [9] Jean-Yves Franceschi, Mike Gartrell, Ludovic Dos Santos, Thibaut Issenhuth, Emmanuel de Bezenac, Mickael Chen, and Alain Rakotomamonjy. Unifying gans and score-based diffusion as generative particle models. Advances in Neural Information Processing Systems, 36:5972959760, 2023. 4 [10] Xingtong Ge, Xin Zhang, Tongda Xu, Yi Zhang, Xinjie Zhang, Yan Wang, and Jun Zhang. Senseflow: Scaling distribution matching for flow-based text-to-image distillation. CoRR, abs/2506.00523, 2025. 2, 4, 5 [11] Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, and Bo Zhang. Tempflow-grpo: When timing matters for grpo in flow models. arXiv preprint arXiv:2508.04324, 2025. 4 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2, [13] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah. Transformers in vision: survey. ACM computing surveys (CSUR), 54(10s):141, 2022. 3 [14] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 5, 6 [15] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. Advances in neural information processing systems, 36:36652 36663, 2023. 6, 7, 1 [16] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 2 [17] Kyungmin Lee, Xiahong Li, Qifei Wang, Junfeng He, Junjie Ke, Ming-Hsuan Yang, Irfan Essa, Jinwoo Shin, Feng Yang, and Yinxiao Li. Calibrated multi-preference optimization for aligning diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 1846518475. Computer Vision Foundation / IEEE, 2025. 4 [18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pages 1288812900. PMLR, 2022. 1 [19] Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flowbased grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025. 4 [20] Zejian Li, Yize Li, Chenye Meng, Zhongni Liu, Yang Ling, Shengyuan Zhang, Guang Yang, Changyuan Yang, Zhiyuan Yang, and Lingyun Sun. Inversion-dpo: Precise and efficient post-training for diffusion models. CoRR, abs/2507.11554, 2025. 4 [21] Zhanhao Liang, Yuhui Yuan, Shuyang Gu, Bohan Chen, Tiankai Hang, Mingxi Cheng, Ji Li, and Liang Zheng. Aesthetic post-training diffusion models from generic preferences with step-by-step preference optimization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2025, Nashville, TN, USA, June 11-15, 2025, pages 1319913208. Computer Vision Foundation / IEEE, 2025. 4, 1 [22] Xinyao Liao, Wei Wei, Xiaoye Qu, and Yu Cheng. Step-level reward for free in rl-based t2i diffusion model fine-tuning. arXiv preprint arXiv:2505.19196, 2025. [23] Shanchuan Lin, Anran Wang, and Xiao Yang. SdxlProgressive adversarial diffusion distillation. lightning: arXiv preprint arXiv:2402.13929, 2024. 2, 3, 7, 1 [24] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. 6 [25] Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online RL. CoRR, abs/2505.05470, 2025. 4 [26] Yanzuo Lu, Yuxi Ren, Xin Xia, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Andy J. Ma, Xiaohua Xie, and Jian-Huang Lai. Adversarial distribution matching for diffusion distillation towards efficient image and video synthesis. CoRR, abs/2507.18569, 2025. 2, 4, 5, [27] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Latent consistency models: Synthesizing highZhao. resolution images with few-step inference, 2023. 2, 3, 7 [28] Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. 3 [29] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. In IEEE/CVF On distillation of guided diffusion models. 10 Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pages 1429714306. IEEE, 2023. 3 [30] Zichen Miao, Zhengyuan Yang, Kevin Lin, Ze Wang, Zicheng Liu, Lijuan Wang, and Qiang Qiu. Tuning timestepdistilled diffusion model using pairwise sample optimization. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. 2, 4, 6, 7, [31] Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 78077816, 2024. 4 [32] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. 3 [33] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 4, 6, 7, 1 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 1 [35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 6 [36] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. Advances in Neural Information Processing Systems, 37:117340117362, 2024. 2, 3, 6, 7, [37] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, 4 [38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. 3 [39] Tim Salimans and Jonathan Ho. Progressive distillation arXiv preprint for fast sampling of diffusion models. arXiv:2202.00512, 2022. 3 [40] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast highresolution image synthesis with latent adversarial diffusion In SIGGRAPH Asia 2024 Conference Papers, distillation. pages 111, 2024. 2, 3 [52] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. In NeurIPS, 2024. 1, 2, 3, 4, 5, [53] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In CVPR, 2024. 2, 4 [54] Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, SongHai Zhang, and Xiaojuan Qi. Text-to-3d with classifier score distillation. arXiv preprint arXiv:2310.19415, 2023. 2 [55] Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, et al. survey of reinforcement learning for large reasoning models. arXiv preprint arXiv:2509.08827, 2025. 4 [56] Sixian Zhang, Bohan Wang, Junqiang Wu, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan Wang. Learning multidimensional human preference for text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80188027, 2024. 7, 1 [57] Tao Zhang, Cheng Da, Kun Ding, Kun Jin, Yan Li, Tingting Gao, Di Zhang, Shiming Xiang, and Chunhong Pan. Diffusion model as noise-aware latent reward model for steplevel preference optimization. CoRR, abs/2502.01051, 2025. 4, 6, 7, 1 [58] Tao Zhang, Cheng Da, Kun Ding, Huan Yang, Kun Jin, Yan Li, Tingting Gao, Di Zhang, Shiming Xiang, and Chunhong Pan. Diffusion model as noise-aware latent reward model for step-level preference optimization. arXiv preprint arXiv:2502.01051, 2025. 6, 9, 1 [59] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. CoRR, 2024. [41] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin In European Rombach. Adversarial diffusion distillation. Conference on Computer Vision, pages 87103. Springer, 2024. 2, 3, 7, 1 [42] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 6, 1, 2 [43] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. 3 [44] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignIn IEEE/CVF ment using direct preference optimization. Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pages 82288238. IEEE, 2024. 4 [45] Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased consistency models. Advances in neural information processing systems, 37:8395184009, 2024. 2, 3 [46] Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-grpo: Pairwise preference reward-based grpo for staarXiv preprint ble text-to-image reinforcement learning. arXiv:2508.20751, 2025. [47] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 7, 1 [48] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. 6, 7, 1 [49] Jiazheng Xu, Yu Huang, Jiale Cheng, Yuanming Yang, Jiajun Xu, Yuan Wang, Wenbo Duan, Shen Yang, Qunlin Jin, Shurun Li, Jiayan Teng, Zhuoyi Yang, Wendi Zheng, Xiao Liu, Ming Ding, Xiaohan Zhang, Xiaotao Gu, Shiyu Huang, Minlie Huang, Jie Tang, and Yuxiao Dong. Visionreward: Fine-grained multi-dimensional human preference learning for image and video generation, 2024. 1 [50] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, and Ping Luo. Dancegrpo: Unleashing GRPO on visual generation. CoRR, abs/2505.07818, 2025. 4 [51] Tianwei Yin, Michael Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. 6 11 Flash-DMD: Towards High-Fidelity Few-Step Image Generation with Efficient Distillation and Joint Reinforcement Learning"
        },
        {
            "title": "Supplementary Material",
            "content": "Table 5. Comparison of mainstream reward models in four aspects: Scope, Evaluation Dimensions, utilized Feature Extractor (FE) and Timestep Awareness. EvalDim denotes evaluation dimension, Time-aware denotes timestep awareness. Aes denotes aesthetics, Align denotes text-image alignment, Fid denotes fidelity. Method EvalDim Space Time-aware FE PickScore [15] ImageReward [48] MPS [56] HPSv2 [47] SPM [21] LRM [57] VisionReward [49] Pixel Pixel Pixel Pixel Pixel Latent Pixel Fid Aes+Fid Aes+Align+Fid Align+Fid Aes+Align Aes+Align Align+Fid+Safety CLIP [34] BLIP [18] CLIP CLIP CLIP SDXL/SD1.5 [33] Qwen2.5-VL [1] 6. Latent Reward Model: Selection Rationale As noted earlier, the previous practice in online preference optimization [30, 36] involves applying reward models[15, 21, 22, 4749, 56] in the final denoising stage, leading to reward hacking. natural question arises: why not deploy optimization across more diverse set of timesteps? Two primary factors explain this limitation. Computationally, reward evaluation would necessitate VAE decoding at multiple steps, which would increase GPU memory and computation requirements. More fundamentally, their reward models lack timestep sensitivity, and the training paradigms are time-agnostic, having been developed on static image datasets without diffusion process context. We empirically investigate the prevalent reward models and systematically summarize their characteristics in the Tab. 5, including scope space, evaluation dimensions, and consideration of timestep. In light of the analysis above, LRM [57] is the most suitable choice. It can evaluate noisy latent at any timestep without switching to pixel space. Specifically, it takes noisy latent and feeds it into pretrained diffusion model, leveraging the models native understanding of latent representation across all noise levels. 7. Algorithm of Flash-DMD Algorithm 1 provides an overview of Flash-DMD . At stage 1, we propose an advanced step distillation method based on distribution matching that converges quickly. Timestepaware strategy, together with Pixel-GAN constraint, effectively alleviates mode-seeking and improves the realism of generation. At stage 2, we carefully select and incorporate latent reward model into the training paradigm, further enhancing the image fidelity and image aesthetics. Table 6. Comparison of Flash-DMD on SDXL under stage 1 with other distillation methods on the COCO-10k dataset. ImgRwd denotes ImageReward score. Best performance is highlighted in bold, and the second best is underlined. Method SDXL LCM-SDXL SDXL-Lightning Hyper-SD TTUR2-3k TTUR2-6k 8 8 8 8 8 #NFE ImgRwd CLIP Pick HPSv2 MPS Cost 100 0.7143 0.3295 0.2265 0.2865 11.87 0.6122 0.7187 0.9119 0.3247 0.2261 0.2874 0.3268 0.2291 0.2900 0.3287 0.2310 0. 11.59 12.12 12.35 8-steps Flash-DMD at Stage 1 0.9159 0.9416 0.3281 0.2319 0.2981 0.3284 0.2318 0.2989 12.60 48*3k 12.63 48*6k - - - - 8. 4-steps and 8-steps Flash-DMD on SDXL To demonstrate the effectiveness of Flash-DMD on SDXL [33], we extend our method to distill the full SDXL model into an 8-step generative model. In the first stage, we select the timesteps [999, 874, 749, 629, 499, 374, 249, 124] and apply Pixel-GAN at the final timestep. We adopt the Two-Time-Scale Update Rule (TTUR) with score estimator update frequency of 2, training on the LAION dataset [42] with batch size of 48. Two variants are trained for 3k and 6k iterations, respectively, denoted as TTUR2-3k and TTUR2-6k. In the second stage, we construct winlose preference pairs using samples generated from the timesteps [999, 874]. We initialize this stage from the TTUR2-3k model (i.e., the 3k-step checkpoint from Stage 1) and continue training for an additional 3k and 6k steps, respectively. The results for Stage 1 and Stage 2 are reported in Tab. 6 and Tab. 7, respectively. Results show that Flash-DMD keeps outperforming other distillation and reinforcement learning methods at 8-steps generative task, highlighting the advantage of Flash-DMD. In terms of subjective results, Fig. 9 and Fig. 11 show the results of the 4step inference at stages 1 and stage 2 of Flash-DMD based on SDXL, respectively. Fig. 12 and Fig. 13 show the results of the 8-step inference at stages 1 and 2 of FlashDMD , respectively. Our Flash-DMD can generate highquality results with both realism and aesthetic appeal under small number of steps. Furthermore, we compare our results with SDXL, SDXL-Lighting[23], SDXL-Turbo[41], Hyper-SDXL[36], DMD2[52], LPO [58], Realism version of NitroSD[4], PSO[30]. Fig. 8 demonstrates that our model not only surpasses other distillation models but also outperforms the teacher model in refining image quality. 1 Figure 8. Qualitative comparisons with other models. Table 7. Comparison of Flash-DMD under stage2 with other models with reinforcement learning on COCO-10k dataset. Method #NFE ImgRwd CLIP Pick HPSv2 MPS GPU Hours Hyper-SDXL LPO-SDXL 8 40 0.9119 1.0417 0.3287 0.2310 0.3324 0. 0.2977 0.2965 12.35 12.58 200 A100 92 A100 8-steps Flash-DMD (TTUR2-3k) at Stage 2 Flash-DMD-3k Flash-DMD-6k 8 1.0012 1.0106 0.3299 0.2338 0.3290 0.2343 0.2986 0.2998 12.75 12.84 12 H20 24 H20 encoders. These captions were then used to synthesize highquality images using SD3.5 Large, configured with 28 denoising steps and CFG scale of 4.5. The final training set was curated through rigorous manual selection process from the generated candidates. We successfully extend Flash-DMD in SD3-Medium after stage 1, Fig. 14 displays some visual results that highlight the strengths of our algorithm. 9. 4-step Flash-DMD on SD3-Medium 10. Additional Visualizations and Captions Instead of training on the LAION dataset [42], we curated proprietary, high-quality training set of 100,000 instances. It encompasses diverse range of subjects and scenes, including portraits, architecture, flora and fauna, and foodrendered in realistic photographic style. Our dataset construction followed structured methodology. First, we generated set of clear, detailed captions to fully leverage the representational capacity of the SD3 [7] text Fig. 9 captions from left to right, top to bottom are 1. cat next to window behind cans and bottles. 2. bunch of red roses bunched together. 3. brown teddy bear standing next to bottles of honey. 4. brown lamb looking up as other sheep eat hay in : field. 5. brown and white horse walking down road. 6. brass-colored vase with flower bouquet in it. 2 Algorithm 1 Flash-DMD Training Algorithm Require: pretrained teacher model µreal, real dataset Dreal, generator is updated with the ratio TTUR, inference steps K, timestep set = {τ1, . . . , τk} and its noisy set Snoisy, high noisy timesteps Tnoisy, Pixel-level discriminator Dω, VAE decoder layers V, latent reward model R, training stage flag FLAG. Ensure: trained few-step generator Gθ 1: Gθ copyWeights(µreal) 2: µϕ copyWeights(µreal) 3: Dω initializeTrainableHeads() 4: for iteration = 1 to max iters do 5: 6: (0, I) Sample τi from Sample xreal Dreal xτi backwardSimulation(z, τk τi) xτ1 backwardSimulation(xτi, τi τ1) Gθ(xτi, τi) preal, pfake Dω(V(xτ1)) if iteration mod TTUR == 0 then tj Tnoisy LDMD distributionMatchingLoss(µreal, µω, x, tj) Ladv generatorAdversarialLoss(preal) LGθ LDMD + λ Ladv Gθ update(Gθ, LGθ ) µϕ EMA(θ, ϕ, λema1) end if x.detach() U(0, 1) xt forwardDiffusion(x, t) Ldenoise diffusionLoss(µϕ(xt, t), x) µϕ update(µϕ, Ldenoise) LDω discriminatorAdversarialLoss(preal, pfake) Dω update(Dω, LDω ) if FLAG == Stage2 then spool iterativeSample(Snoisy, K) swin, sloss filter(R(spool)) Lrl prefenceOptimization(swin, sloss) Gθ update(Gθ, Lrl) 29: 30: 31: 32: 33: end for end if 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: Initialize generator Initialize score estimnator of generator Initialize trainable heads of discriminator Pick timestep for current iteration Use backward simulation to get noisy image Use backward simulation to get clean image Get real or fake probability Use Eq. (9) for faster convergence in noisy timesteps Use Eq. (10) for enhanced realism and details Final loss function for generator Stop gradient Add noise Update fake score network µfake Discriminators Hinge loss Update discriminator Dω Use reinforcement learning at the second stage Sample image latent for reward model evaluation Construct win-loss pairs with Use Eq. (13) to boost performance 7. boy closely examining frog in his yard. 8. boy in green shorts and tie posing in front of tower. 9. furry kitten lying on laptop. 10. confection of cake, whipped cream, strawberries, and two candles. 11. Two birds standing around box of birdseed. 12. soldier riding red motorcycle down busy street. Fig. 10 captions from left to right, top to bottom are: 1. dog looking up and running to catch frisbee. 2. cake designed to resemble cup. 3. beautiful red-haired woman holding cup while wear4. row of wooden park benches sitting next to lake. 5. close-up of baseball player bending down with glove. 6. bag of strawberries on table with tomatoes. 7. crow standing on plant in body of water. 8. black-and-white cat sitting on bed. 9. brown-and-white cow standing on grassy hill. 10. brown leather couch in living room. 11. body of water with an elephant in the background. 12. case containing small doll with blue hair, shoes, and clothes. ing sweater. Fig. 11 captions from left to right, top to bottom are: 3 Figure 9. Qualitative results from Stage 1 of the 4-step Flash-DMD framework on SDXL. The model is trained with TTUR = 1 for 1,000 steps. Figure 10. Qualitative results from Stage 1 of the 4-step Flash-DMD framework on SDXL. The model is trained with TTUR = 2 for 4,000 steps. 5 Figure 11. Qualitative results from Stage 2 of the 4-step Flash-DMD framework on SDXL. The model is initialized from the TTUR1-1k checkpoint and fine-tuned for 5,000 steps. 6 Figure 12. Qualitative results from Stage 1 of the 8-step Flash-DMD framework on SDXL. The model is trained with TTUR = 2 for 3,000 steps. Figure 13. Qualitative results from Stage 2 of the 8-step Flash-DMD framework on SDXL. The model is initialized from the 8-step TTUR2-3k checkpoint and fine-tuned for 3,000 steps. 8 Figure 14. Qualitative results from stage 1 of the 4-step Flash-DMD framework on SD3-Medium. The model is trained with TTUR=2 for 7.000 steps. 9 1. bowl of apples and bananas sitting on woven cloth. 2. cake decorated with surfer and palm trees. 3. close-up of person eating doughnut. 4. brown teddy bear sitting at table next to cup of coffee. 5. close-up of metallic elephant statue. 6. baby in gray jacket eating piece of pizza crust. 7. couch and chair sitting in room. 8. bench in the park on rainy day. 9. brown-and-white cat sitting on windowsill. 10. bedsit with kitchenette featuring white cabinets. 11. bottle of wine placed next to glass of wine. 12. blurred motorcycle against red brick wall. Fig. 12 captions from left to right, top to bottom are: 1. bird on plank looking at green water. 2. boy holding pitchers mitt at park. 3. clock near the front of house. 4. bench sitting atop lush green hillside. 5. black bear walking through field of grass and straw. 6. black motorcycle parked on gray cobblestones. 7. child in bed wearing striped sweater and colorful blanket. 8. brass-colored vase with flower bouquet in it. 9. clean bedroom with dog on the bed. 10. chef preparing sushi on countertop. 11. close-up of sandwich with French fries. 12. close-up image of cat and keyboard. Fig. 13 captions from left to right, top to bottom are: 1. bench along sidewalk in winter, covered in snow. 2. man holding little blond girl. 3. confection of cake, whipped cream, strawberries, and candles. 4. bedroom with silky bedspread and pillows. 5. cat lying on sofa next to some pillows. 6. brown teddy bear seated on chair beside wooden drawer. 7. baseball player standing next to home plate. 8. sleek motorcycle in cityscape. 9. beautiful vase full of flowers, with pictures placed beside it. 10. beautiful bird standing on the bank of river. 11. bagel topped with egg and other ingredients on plate. 12. furry kitten lying on laptop. Fig. 14 captions from left to right, top to bottom are: 1. mustard-yellow armchair with button tufting sits on speckled white floor. The chair has dark wooden legs. large potted plant is partially visible to the left. window with dark frames shows green trees and foliage. white rock sits on the floor near the chair. The floor is partially shaded by the window. 2. brown deer with large, curved antlers stands in forest setting. The deers coat is uniform brown color. Its antlers are dark brown and have multiple points. The deers face is partially visible, showing its nose and eyes. The background is blurred with green foliage. The deer appears to be looking directly at the camera. 3. Several cupcakes are arranged on wire rack. One cupcake has dark blue frosting, gold sprinkles, and jack-olantern topper. Other cupcakes have orange frosting and are decorated with orange and purple sprinkles. The cupcakes are in purple and white patterned wrappers. The background is blurred. 4. doll with brown curly hair, blue eyes, and rosy cheeks wears large black velvet hat with gold embroidery. The dolls dress is black with gold trim and white scarf tied around its neck. The dolls face is white with painted features. Other dolls are visible in the background. The doll appears to be wearing black velvet dress with gold trim. The dolls hat has wide brim and decorative band. 5. vibrant orange rose is the focal point, its petals slightly unfurled. The rose is attached to green stem with small thorns. The background is blurred green foliage. The lighting is soft and natural. The rose appears to be in natural setting. 6. young woman with fair skin, dark brown hair styled in an updo with decorative hairpiece, wears lightcolored, sheer, embroidered traditional East Asian garment. She looks directly at the camera with slight smile. The garment has floral embroidery in white and light blue. She wears small, dangling earrings. 7. green and pink parrot with red beak eats passion fruit. The parrots head and neck are predominantly green, with pink band around the neck. The beak is red and orange. The passion fruit is dark purple with yellow interior and black seeds. 8. reddish-orange mushroom with white speckled stem grows in forest floor covered with fallen leaves and grass. small, light brown leaf rests on the mushrooms cap. The background features blurred green foliage and trees. The lighting is soft and diffused. 9. white, fluffy dog lies in field of green grass. The dog has its tongue out and is wearing collar with blue bone-shaped tag. The sun is low in the sky, creating backlight. The dogs fur appears slightly ruffled. The grass is tall and slightly blurred. Trees are visible in the background. The sky is gradient of blue and light yellow. 10. bee is perched on vibrant orange flower. The flower has multiple petals and yellow center. Several other orange flowers and green leaves are visible in the background, some out of focus. The bee appears to be collecting nectar. The flowers are in garden setting. The 10 image is brightly lit, highlighting the orange hues of the flowers. 11. clear glass teapot containing yellow liquid sits on round wooden tray. Several ripe red strawberries with green stems are placed on the tray alongside the teapot. The tray rests on light brown, textured surface. blurred background of green foliage suggests an outdoor setting. The teapot has copper-colored handle and lid. The light is bright and natural. 12. black mug with gold design sits on gray textured surface. The design depicts stylized mountains, trees, crescent moon, and campfire. The words Mountain Kind are written below the design. Steam rises from the mug. lit candle, pine cones, cinnamon sticks, and string of lights are also present. The background is blurred and dark."
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "Shanghai Jiao Tong University",
        "Tencent",
        "The Chinese University of Hong Kong"
    ]
}