{
    "paper_title": "UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving",
    "authors": [
        "Hao Lu",
        "Ziyang Liu",
        "Guangfeng Jiang",
        "Yuanfei Luo",
        "Sheng Chen",
        "Yangang Zhang",
        "Ying-Cong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 4 6 8 9 0 . 2 1 5 2 : r UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving"
        },
        {
            "title": "ByteDance Seed",
            "content": "See Contributions section for full author list."
        },
        {
            "title": "Abstract",
            "content": "Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations. Project Page: https://seed-uniugp.github.io/ Date: December 11,"
        },
        {
            "title": "Introduction",
            "content": "Autonomous driving (AD) has recently made remarkable progress, especially in areas such as birds-eye view perception [27, 38, 42, 43, 46], end-to-end [8, 25, 30, 40], scene reconstruction [45, 82, 93, 100], and video generation [15, 44, 70, 75]. Recently, given the superior capabilities of multimodal large language models (MLLMs) in world knowledge, reasoning ability, and interpretability, they have been widely applied in AD [26, 34, 48, 90]. One promising direction is the end-to-end vision-language-action (VLA) model [37, 72, 102], which leverages pre-trained vision-language model (VLM) to directly extract scene features from visual observations and language instructions, subsequently generating vehicle control commands (e.g., speed and trajectory). This paradigm not only simplifies system architecture and minimizes information loss, but also enables the utilization of the models world knowledge to analyze driving environments and reason about safe decisions in complex scenarios [6, 31, 32, 68]. However, they were unable to fully utilize the large number of un-labeled driving videos, which limited their ability to learn visual causal reasoning from large-scale datasets. In addition to the advanced VLA technology, the world model can learn visual causal reasoning by predicting the next frame of the video [9, 23, 84, 91, 95]. The world model can learn visual causal reasoning by 1 Table 1 Comparison of Different Methods. \"VLA\" refers to the use of additional models to predict more accurate trajectories, which is different from VLMs text-based prediction. \"Reason.\" refers to whether the model can generate chain of thoughts. \"Inter.\" refers to whether the model can change its trajectory based on human instructions. \"Cont.\" refers to whether the token is continuous or discrete value. \"World Model\" only lists the methods that can simultaneously generate future images and trajectories. \"FM.\" means flow matching. Category Method Model Reason. Inter. Modality Cont. Method Cont. Understanding Generation Action World Model World Model OccWorld [95] Epona [91] - - VLM VLM VLM VLA VLA VLA VLA VLA DriveLM [58] Impr. VLA [10] OmniDrive [67] ReCogDrive [37] ORION [14] AutoVLA [102] DriveMoE [86] Alpamayo-R1 [72] Llama2 Qwen2.5-VL-3B LLaMA2-7B Qwen2.5-VL-3B Vicuna v1.5 InternVL3-8B π0 Cosmos-Reason Unified Model Doe-1 [97] Unified Model Occ-LLM [81] Unified Model OccLlama [74] Unified Model HERMES [101] Unified Model FSDrive [88] Lumina-mGPT Llava-7B Llama-7B InternVL2-2B Qwen2-VL-2B Unified Model UniUGP Qwen2.5-VL-3B Occ Video - - - - - - - - Video Occ Occ LiDAR Video Video - - - - - - - - Codebook FM. Text Text Text Diffusion VAE Codebook FM. FM. Text Text Text Text Text FM. predicting the next frame of the video, which has been proven to be helpful in achieving the final end-to-end AD [84, 88, 91]. But the world model is unable to match the world knowledge, reasoning ability, and interaction capability from large language models. Summaries of different methods are presented in Tab. 1. Unified models, which aim to bridge perception, reasoning, and action, can simultaneously combine the advantages of the world model and the VLA model. However, there are several additional issues here: 1) How to efficiently establish unified model to fully utilize the pre-trained VLM and world model. 2) How to effectively and repeatedly utilize various driving data (such as VQA pairs, video trajectory pairs, etc.) to fully exploit the potential of the unified model. 3) How to evaluate the capabilities of the unified model, especially in terms of understanding, reasoning, and planning in complex scenarios. To address these limitations, we propose UniUGP, unified UnderstandingGeneratePlanning framework for end-to-end AD that jointly models complex scene reasoning, future video generation, and trajectory planning. Built on hybrid expert architecture, UniUGP fully exploits the causal reasoning capabilities of pre-trained MLLMs and video generation models, while further enhancing cross-modal causal alignment through large-scale multimodal data training. Specifically, UniUGP takes natural language instructions and continuous image sequences as inputs, and outputs three complementary results: chain-of-thought (CoT) reasoning process for interpretability, physically consistent trajectory for safe driving, and coherent future video for visual causal validation. To ensure output consistency and accuracy, we design multi-term loss function that enforces CoT logical consistency, trajectory temporal smoothness, and video visual coherence. Moreover, we propose four-stage training framework that sequentially builds foundational scene understanding, visual dynamic modeling, text-based reasoning, and multi-capability fusion, leveraging over 10 diverse AD datasets to cover common scenarios and long-tailed cases. Our main contributions are summarized as follows: 1) We construct multiple specialized datasets for AD-oriented VLAs, which provides explanations, reasoning, and planning for complex scenarios. 2 2) We propose unified Understanding-Generation-Planning (UniUGP) framework with hybrid expert architecture, which synergizes scene reasoning, future video generation, and trajectory planning. 3) We develop four-stage training strategy that leverages diverse AD datasets to enable the mutual enhancement of understanding, generation, and planning task capabilities."
        },
        {
            "title": "2.1 VLA for Autonomous Driving",
            "content": "End-to-end autonomous driving models [24, 30, 39, 83] have shown strong performance in structured environments but struggle in long-tail and unstructured scenarios due to limited generalization and lack of world knowledge. To mitigate this, recent work introduces VLMs for reasoning and scene understanding [66? ], integrating them into driving frameworks to enhance adaptability in complex situations [10, 14, 29, 37, 67, 102]. Early attempts used VLMs to generate high-level meta-actions or abstract driving decisions [6, 31, 32, 68], which guided modular or end-to-end planners but disrupted joint optimization across perception, decision, and control. Later, the VLA frameworks directly mapped visual language inputs to trajectories. Impromptu VLA [10] trained VLMs on text-based trajectory representations, while AutoVLA [102] discretized trajectories into action tokens decoded into continuous paths. Diffusion-based models such as ReCogDrive, DiffVLA, and ORION [14, 29, 37] further bridged the gap between semantic reasoning and continuous trajectory generation. Despite progress, the existing methods are often unable to utilize unlabeled video data to learn visual causal reasoning."
        },
        {
            "title": "2.2 World Models for Autonomous Driving",
            "content": "World models [2, 51, 80, 92, 103] aim to infer ego-centric states and predict dynamic surroundings from historical observations, enabling accurate future prediction and planning. In autonomous driving, world models are primarily applied in three areas: driving scenario generation [17, 33, 36, 52], planning [20, 73, 95], and representation learning [50, 85, 87]. For driving scenario generation, most prior works rely on diffusion models. GAIA-1 [21] is notable exception that combines progressive next-token predictor with an auxiliary diffusion image decoder. More recently, Epona [91] advances this direction by employing an autoregressive diffusion model to unify world modeling and planning. However, existing approaches face two major limitations. First, they do not fully exploit the complementary strengths of pre-trained VLMs and video generation models, missing opportunities to leverage both linguistic reasoning and visual dynamics modeling. Second, most methods are trained and evaluated on limited single-dataset scenarios, restricting their generalization ability. Our method addresses these gaps by integrating knowledge from both pre-trained VLMs and video generative models, while scaling training across diverse datasets to unlock emergent capabilities."
        },
        {
            "title": "2.3 Unified Models",
            "content": "Unified models aim to dissolve modular boundaries across understanding and generation. [12, 49, 53, 57, 61, 62, 64, 71, 7678, 99]. Extending to embodied intelligence, works like [47] and WALL-OSS [89] integrate action modules into unified frameworks. F1 uses three-expert MoT to synthesize goal-conditioned visual foresight and model action as foresight-guided inverse dynamics, while WALL-OSS employs tightly coupled MoE design to align discrete action priors and continuous control, enhancing long-horizon manipulation. In autonomous driving, recent works [88, 97, 101] have explored unified architectures. However, as shown in Tab. 1, they lack critical capabilities including CoT reasoning, natural language interaction, and diffusion-based planning, while failing to leverage large-scale unlabeled data, and pre-trained VLM and video generation model knowledgelimiting both their interpretability and generalization potential."
        },
        {
            "title": "3 Method",
            "content": "The proposed UniUGP is unified model for understanding, generation, and planning to further enhance the causal reasoning ability across different modalities: 1) Large-scale challenging data pairs are collected and processed to train and evaluate the understanding, reasoning, generation and planning capabilities of the 3 Figure 1 Illustration of UniUGP, unified model with three hybrid experts. The understanding expert performs the next-token prediction for causal reasoning. The planning expert forms MoT architecture with the understanding expert, and performs the velocity prediction in flow matching for production future actions. The generation expert is cascaded as world model to produce future videos. unified model, as elaborated in Sec. 3.1. 2) An elegant unified framework combines the advantages of VLA and world models, as elaborated in Sec. 3.2. 3) well-designed training strategy enables the unified model to fully acquire knowledge from different modalities on various datasets, as elaborated in Sec. 3.3."
        },
        {
            "title": "3.1 Challenging Long-tail Driving Dataset",
            "content": "Prior benchmarks emphasize structured scenes and simulator-based evaluations [10, 63, 67], which overlook the challenges of long-tail driving events. To address this, we have collected large number of challenging long-tail driving videos, including Waymo-E2E [79], DADA2000 [13], Lost and Found (LaF) [55], StreetHazards (StHa) [18], SOM [59], and AADV [7]. These datasets were further processed to be used separately for training and evaluating the perception ability, causal reasoning ability, planning ability, and the ability to follow instructions. For the comprehension task, we have designed three sub-tasks: small objects, accident subject relationships, and accident prediction. The questions and answers of these tasks are labeled based on the provided labels from the dataset and the advanced VLM model. For more details, please refer to the supplementary materials. CoT reasoning: We employed the results of future planning and reasonable prompts to force the advanced VLM to generate the accurate CoT. We carried out manual calibration for CoT. Finally, it is worth mentioning that we will assign corresponding instructions to each predicted trajectory in the future, which enables our model to have the ability to follow instructions. The more detailed data processing procedures are listed in the Appendix."
        },
        {
            "title": "3.2 Unified Model With Hybrid Expert",
            "content": "We adopted the Hybrid Expert structure to achieve unified understanding, generation and planning. This architecture can leverage the advanced features of the diffusion policy and the knowledge of pre-trained VLM and world models. Unified Model Understanding and Planning Experts. Based on the validity proof of the existing work [3, 14, 47, 57], the understanding and planning experts constitute Mixture-of-Transformers (MoT) architecture, as shown in Figure 2. For the understanding expert, we choose the Qwen2.5-VL [1] as our backbone model. The text instructions and the observation images are firstly mapped to aligned cross-modal understanding tokens xund, by the text tokenizer and the ViT encoder, respectively. 4 Figure 2 Dataset Construction Pipeline. This figure depicts the pipeline of data collection (integrating multiple challenging driving datasets) and data processing (featuring four task categories: understanding, chain-of-thought, planning, and instruction following) to train and assess the cognitive abilities of end-to-end autonomous driving models within unified QA framework. For the planning expert, the action chunk is modeled by flow matching process [41], where the expert learns to reverse gradual noise-addition process added in the forward process. During training, random noise ϵ (0, I) and timestep τ [0, 1] are sampled to model the noised actions: aτ = τ + (1 τ )ϵ The aτ along with the history states are prjoected into planning expert tokens xplan = Proj.([s, aτ ]). The xund and xplan are passed through several MoT layers, where each layer can be formalized as: hund , hund = MSHA([QKVund(xund), QKVplan(xplan)]) (1) (2) The QKV denote the linear projections that map understanding and planning tokens (or hidden states) into queries, keys, and values. The MSHA denotes the multi-head self-attention. Then, the modality-specific feed-forward networks (FFNs) are utilized to process the hund , separately. and hund hund = FFNund(hund ), hplan = FFNplan(hplan ) (3) (4) The final resulting hidden states hund and hplan are mapped into the logits in text and the predicted denoising vector filed. The training objectives for the understanding and planning experts are formalized as: Plogits =LMHead(hund), =unProj.(hplan) uplan τ Lund =E Lplan =E xund uplan τ [log(P (xund xund <i ))], [uplan τ (ϵ a)2] (5) (6) (7) (8) Generation Expert. The generation expert interacts with the understanding and planning experts in serial manner. On mobile devices, the generation expert can be disabled to save computational effort, without compromising the performances of the former experts. 5 Table 2 Hyperparameter Configuration for the Four-Stage Training Framework Hyperparameter Stage 1 Stage 2 Stage 3 Stage 4 Trained Components Dataset Und. Expert: Und. Expert: Und. Expert: Und. Expert: Gen. Expert: Gen. Expert: Gen. Expert: Gen. Expert: Plan. Expert: Plan. Expert: Plan. Expert: Plan. Expert: Custom long-tail Imdataset, promptuVLA [10] nuScenes [4], NuPlan [5], Waymo [60], Lyft [35], Cosmos [56]"
        },
        {
            "title": "CoT",
            "content": "Mixture of Stages 13 (ratio 0.1 : 0.4 : 0.5) Learning Rate Understanding Resolution 104 (224, 224) 10 104 (224, 224) Generation Resolution (512, 512) 104 (224, 224) (512, 512) GPU Resources Batch Size Training Steps 8 nodes 8 gpus (80GB) 8 nodes 8 gpus (80GB) 8 nodes 8 gpus (80GB) 8 nodes 8 gpus (80GB) 1M 64 4M 64 1M 4M The generation expert produces future videos via flow matching process, same as the planning expert. It is composed of several DiT [54] blocks. In practice, we adopt Wan2.1 [65] as the base model and inherit its pre-trained parameters. Note that any other DiT-based video generation models are feasible here. Both future and history images are encoded into tokens by VAE. As shown in Figure 2, the history image tokens vhist and noised future image tokens vf ut are concatenated as the input. The understanding hidden states hund and the action embeddings are concatenated as the condition. The final denoising vector feild is computed as: τ ugen τ = W([vhist, vf ut τ ], [hund, A], τ ) (9) During inference, the action embeddings are obtained by projecing the predicted future actions ˆa from the planning expert, as = Proj.(ˆa). During training, The embeddings are computed either from ground truth actions, or from actions obtained via single-step denoising: (cid:40) Proj.(a) Proj.(aτ (1 τ )uplan τ if rand(0 1) > 0.5, else. ) (10) As such, the generation expert receives both semantic and physical signals from the understanding and planning experts, facilitating more realistic video generation. The training objective can be formalized as: Lgen =Eugen τ [ugen τ (ϵ vf ut)2] (11)"
        },
        {
            "title": "3.3 Training Recipe",
            "content": "We design four-stage training framework, which sequentially builds foundational scenario understanding, visual dynamic modeling, text-based reasoning, and multi-capability fusion, detailed are presented below. The training parameters and details can be found in Tab. 2. Stage 1: Continuous Training for the Understanding of the basic scenarios is to enable the Understanding Expert to establish comprehensive understanding of diverse driving scenarioscovering common traffic and long-tailed cases. Only the Understanding Expert is trained in this stage. The training dataset at this stage 6 includes the long-tail data set that we have labeled and the ImpromptuVLA (80,000 meticulously curated video clips from 8 open-source large-scale datasets) [10]. Stage 2: Visual Dynamics Modeling and Planning Training focuses on learning visual dynamics and motion planning capabilities. During this training phase, driving videos with trajectories were used for training the Generation Expert and the Planning Expert. We utilized multiple public datasets including: nuScenes [4], NuPlan [5], Waymo [60], Lyft [35], and Cosmos [56]. Stage 3: Text Reasoning Learning for Causal Validation integrates CoT reasoning into the Understanding Expert, enabling the model to validate the logic of its perceptions and planning using natural language. This stage enhances model interpretability and ensures decisions are grounded in explicit causal reasoning. This part is trained using our own annotated CoT dataset. Stage 4: Mixed Training for Multi-Capability Fusion resolves potential misalignments between individual stages and enhances generalization across scenarios. In Stage 4, the three experts are jointly trained to achieve consistent end-to-end performance. We mix datasets from Stages 13 at fixed proportion to balance foundational, reasoning, planning, and generation capabilities. The overall objective Ltotal is weighted sum of three sub-objectives: Ltotal = α Lund + β Lplan + γ Lgen where α = 0.3, β = 0.5, γ = 0.2. This alignment ensures the model operates as unified system rather than collection of isolated components. (12)"
        },
        {
            "title": "4.1 Dataset",
            "content": "We utilize two complementary categories of datasets to evaluate multimodal world understanding and decisionmaking under both common and safety-critical driving conditions. For Perception and Understanding, we adopt anomaly and accident anticipation datasets including DADA2000 [13], Lost and Found (LaF) [55], StreetHazards (StHa) [18], SOM [59], and AADV [7]. These datasets contain rare obstacles, out-of-distribution objects, and pre-accident sequences collected in diverse real-world environments, enabling assessment of visual scene comprehension, hazard awareness, and recognition of long-tail semantic patterns. For reasoning, planning, and instruction following, we use the Waymo Open Dataset Long-tail End-to-End Driving [79], which consists of 4,021 real-world driving segments specifically curated to capture rare and high-risk events that occur in less than 0.003% of daily driving. Each segment provides surround-view camera streams, ego-motion history, and route signals, and the benchmark task is to predict future 5-second trajectories under uncertain and interaction-heavy conditions. This dataset enables rigorous evaluation of robustness, generalization, and physically grounded decision-making in long-tail scenarios, complementing the perception-focused anomaly datasets above. Following the previous methods [11, 67], we evaluate scene understanding on DriveLM [58]. This dataset features keyframe descriptions paired with QA annotations covering full-stack autonomous driving (perception, prediction, planning), offering comprehensive language support for development. Consistent with methods [16, 30], we evaluate trajectory planning and future frames generation on the nuScenes [4]. The nuScenes contains 1,000 scenes of approximately 20 seconds each captured by 32-beam LiDAR and six cameras providing 360-degree field of view. Specifically, the dataset provides 28,130 (train), 6,019 (val), and 193,082 (unannotated) samples. In addition, we utilize large number of publicly available datasets for training, including: ImpromptuVLA [10], NuPlan [5], Waymo [60], Lyft [35], and Cosmos [56]."
        },
        {
            "title": "4.2 Metrics",
            "content": "We have established new long-tail benchmark consisting of understanding, CoT reasoning, Planning and Instruction Following. For comprehension questions (mainly multiple-choice and true/false questions), we evaluate them based on the accuracy rate. For CoT, we use the API of GPT-4o to rate it in terms of consistency, rationality and fluency, and also provide score from Blue. For Planning, we use L2 3s to rate 7 Table 3 Performance Comparison on Driving Evaluation Benchmark. Note: GPT-4o and Qwen-2.5-VL-72B are provided with historical trajectory information and trajectory explanations, enabling trajectory prediction evaluation. Model Understanding CoT Planning Following Small Relationship Abnor. Pred. GPT Blue L2 (3s) L2 (3s) GPT-4o 64.2% Qwen-2.5-VL-72B 75.8% 86.5% Our w/o CoT 83.7% Our w/o Gen. Our 89.3% 63.5% 74.9% 85.7% 82.9% 88.6% 72.8% 81.5% 93.2% 90.6% 95.8% 0.55 0.72 0.83 0.80 0.88 0.125 0.188 0.218 0.203 0.240 2.63 1.94 1.58 1.72 1.45 2.58 1.89 1.53 1.67 1. it. For instruction following, we score the trajectories corresponding to different instructions using L2 (3s). More details are provided in the appendix. We evaluate trajectory planning using L2 displacement error and collision rate following previous methods [91]. Following existing methods [69, 83], we report Fréchet Inception Distance (FID) [19] to measure the future frames generation quality. DriveLM GVQA [58] metrics include language metrics like BLEU, ROUGE_L, and CIDEr for text generation, the ChatGPT Score for open-ended Q&A and accuracy for multiple-choice questions."
        },
        {
            "title": "4.3 Evaluation of Understanding Ability",
            "content": "To validate the effectiveness of our benchmark and the performance of our model, we conduct comparative experiments with state-of-the-art vision-language models, and the results are presented in Tab. 3. The evaluation metrics are categorized into four key dimensions to comprehensively measure model performance: Understanding (assessing scene and object comprehension, including Small (small object recognition), accident subject relationship, and Acci.Pred. (accident event prediction)), CoT (Chain-of-Thought reasoning ability, evaluated by GPT (subjective GPT score) and Blue (BLEU score)), Planning (short-term driving planning, measured by L2 distance (3s) with smaller values indicating better performance), and Following (trajectory following accuracy, also measured by L2 distance (3s)). The comparative models include GPT 4o, Qwen 2.5 VL, and two ablated versions of our model (Our w/o CoT: without Chain-of-Thought module; Our w/o Gen.: without generation module), alongside our full model (Our). As shown in Table 3, our full model outperforms state-of-the-art methods (GPT-4o, Qwen-2.5-VL-72B) and our ablated versions (Our w/o CoT, Our w/o Gen.) across all key metrics: in Understanding (89.3%, 88.6%, 95.8% for Small, Relationship, Abnor. Pred.), CoT (GPT score 0.88, Blue score 0.240), Planning (L2=1.45), and Following (L2=1.40), all surpassing baselines and ablated models with lower performance; these results confirm that integrating the generation and CoT modules significantly enhances the models comprehensive driving capabilities. The generation model has significantly enhanced the performance of the model. We conducted further qualitative analysis as shown in Fig. 3. The world model forces VLA to learn visual causal inference, particularly focusing on distant objects to generate better future frames. This enables the VLA model to predict potential dangers in advance, thereby ensuring driving safety."
        },
        {
            "title": "4.4 Evaluation of Planning Ability",
            "content": "As shown in Table 4, our model (Ours) achieves competitive performance under the setting of front camera input (Camera) and QA auxiliary supervision: it attains an average L2 distance of 1.23m and an average collision rate of 0.33%, outperforming multiple comparative methods with similar input constraints. Specifically, compared to Doe-1 [98] with the same input and auxiliary supervision (Camera+QA), our model reduces the average L2 distance from 1.26m to 1.23m and the average collision rate from 0.53% to 0.33%. It also performs favorably against advanced methods like GenAD [96] (average L2: 0.91m, collision rate: 0.43%) and UniAD [25] (average L2: 1.03m, collision rate: 0.31%) considering our more constrained input (only front camera vs. full camera suite). Additionally, our model surpasses Epona [91] (Camera+None, average L2: 1.25m, collision rate: 0.36%) under similar input conditions, with lower average L2 distance and collision rate. These results demonstrate the effectiveness of our unified models capabilities in trajectory planning accuracy and driving safety even with unified model. 8 Figure 3 The ablation experiment on the absence or presence of world model knowledge. The world model enables the VLA to pay more attention to future causal relationships, thereby focusing on the semantics of distant objects. Table 4 End-to-end motion planning performance on the nuScenes [4] dataset. Note that our model achieves low collision rate, demonstrating its understanding of basic traffic rules via simple next-frame prediction. represents only using the front camera as input. Method Input Auxiliary Supervision L2 (m) 2s 3s Avg. Collision Rate (%) 3s Avg. 1s 2s 1s ST-P3 [22] UniAD [25] OccWorld [95] VAD-Tiny [30] VAD-Base [30] GenAD [96] Camera Camera Camera Camera Camera Camera Camera Doe-1 [98] Epona [91] Camera UniUGP (Ours) Camera Map & Box & Depth 1.33 2.11 2.90 Map & Box & Motion & Tracklets & Occ 0.48 0.96 1.65 0.52 1.27 2.41 0.60 1.23 2.06 0.54 1.15 1.98 0.36 0.83 1.55 3D-Occ Map & Box & Motion Map & Box & Motion Map & Box & Motion QA None QA 0.50 1.18 2.11 0.61 1.17 1.98 0.58 1.14 1.95 1. 2.11 1.03 1.40 1.30 1.22 0.91 1.26 1.25 0.23 0.62 1.27 0.05 0.17 0.71 0.12 0.40 2.08 0.31 0.53 1.33 0.04 0.39 1.17 0.06 0.23 1.00 0.04 0.37 1.19 0.01 0.22 0.85 0.81 0.01 0.19 0.71 0.31 0.87 0.72 0.53 0.43 0.53 0. 0.33 Table 5 Future Frames Generation Quality Comparison on the NuScenes Dataset. Method Type Res. FID FVD DriveDreamer [69] Drive-WM [73] GenAD [83] GEM [17] Doe-1 [98] Epona [91] FSDrive [88] UniUGP [CVPR24] [CVPR25] [arXiv24] [NeurIPS25] Ours [CVPR24] [ECCV24] [ICCV25] Diff 128192 52.6 452. Diff 192384 15.8 122."
        },
        {
            "title": "Diff",
            "content": "AR"
        },
        {
            "title": "Diff",
            "content": "256448 5761024 384672 5761024 AR 128192 AR+Diff 512512 15.4 184.0 10.5 - 15.9 - 7.5 82.8 10.1 - 7.4 75."
        },
        {
            "title": "4.5 Evaluation of Generation Ability",
            "content": "As shown in Table 5, our method is evaluated under the same protocol as Epona [91] and FSDrive [88], achieving significant improvements in generation quality. This gain stems from the effective utilization of pre-trained generative model, which enhances the models ability to capture realistic scene dynamics and appearance. We have provided trajectory-controllable visualization as shown in Fig. 4, which demonstrates the controllability of our generated experts."
        },
        {
            "title": "4.6 Results on DriveLM Dataset",
            "content": "As shown in Table 6, under the same evaluation protocol as FSDrive [67], our model (UniUGP) achieves superior performance on the DriveLM GVQA Benchmark compared to existing state-of-the-art methods. Our final score reaches 0.59, which is higher than FSDrive (0.57), OmniDrive [67] (0.56), SimpleLLM4AD [94] (0.53), TrackingMeetsLMM [28] (0.52), Cube-LLM [11] (0.50), and the DriveLM baseline [58] (0.32). Across 9 Figure 4 Trajectory controllable generation visualization. We control the generation of future frames of the video by modifying the trajectories fed into the generation model, which demonstrates the controllability of our generation experts. Table 6 Results on DriveLM GVQA Benchmark. Method Acc. GPT BLEU_1 ROUGE_L CIDEr Match Final Score DriveLM baseline [ECCV24] [58] Cube-LLM [ICLR25] [11] TrackingMeetsLMM [arxiv25] [28] SimpleLLM4AD [arxiv24] [94] OminiDrive [CVPR25] [67] FSDrive [NeurIPS25] [67] UniUGP (Ours) 0.00 0.39 0.60 0.66 0.70 0.72 0.74 0.65 0.89 0.58 0.57 0.65 0.63 0. 0.05 0.16 0.72 0.76 0.52 0.76 0.78 0.08 0.20 0.72 0.73 0.73 0.74 0.76 0.10 0.31 0.04 0.15 0.13 0.17 0. 0.28 0.36 0.36 0.35 0.37 0.39 0.41 0.32 0.50 0.52 0.53 0.56 0.57 0.59 key metrics, our accuracy is 0.74, outperforming all comparative methods; BLEU is 0.78 and ROUGE is 0.76, both leading FSDrive (0.76, 0.74) and other SOTA models; the match metric is 0.41, which is also higher than existing methods. While our ChatGPT score (0.64) and CIDEr score (0.19) are competitive among the methods, the overall leading performance across core evaluation dimensions demonstrates the advantage of our method over existing state-of-the-art approaches in scene understanding and language interaction capabilities."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose UniUGP, unified Understanding-Generation-Planning framework addressing critical challenges in temporal dynamics and long-tail generalization. Leveraging hybrid expert architecture with pre-trained VLMs and video generation models, UniUGP produces interpretable reasoning, physically consistent trajectories, and coherent future videos from multimodal inputs. Our four-stage training strategy progressively aligns these capabilities across diverse datasets. Extensive experiments demonstrate state-of-the-art performance and robust generalization, establishing strong foundation for future autonomous driving research."
        },
        {
            "title": "Contributions",
            "content": "Authors: Hao Lu1,2,,, Ziyang Liu2,, Guangfeng Jiang3, Yuanfei Luo2,, Sheng Chen2, Yangang Zhang2, Ying-Cong Chen1, Affiliations: 1HKUST-GZ, 2ByteDance Seed, 3Independent Researcher Co-first authors Project leads Corresponding author. Note: Hao LU work done at ByteDance Seed during internship."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Hengwei Bian, Lingdong Kong, Haozhe Xie, Liang Pan, Yu Qiao, and Ziwei Liu. Dynamiccity: Large-scale 4d occupancy generation from dynamic scenes. In ICLR, 2025. [3] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. corr, abs/2410.24164, 2024. doi: 10.48550. arXiv preprint ARXIV.2410.24164. [4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yuxin Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. CVPR, 2020. [5] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. nuplan: closed-loop ml-based planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.11810, 2021. [6] Tianhui Cai, Yifan Liu, Zewei Zhou, Haoxuan Ma, Seth Z. Zhao, Zhiwen Wu, and Jiaqi Ma. Driving with regulation: Interpretable decision-making for autonomous vehicles with retrieval-augmented reasoning via llm. ArXiv, abs/2410.04759, 2024. URL https://api.semanticscholar.org/CorpusID:273186209. [7] Fu-Hsiang Chan, Yu-Ting Chen, Yu Xiang, and Min Sun. Anticipating accidents in dashcam videos. In Asian Conference on Computer Vision, pages 136153. Springer, 2016. [8] Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning. arXiv preprint arXiv:2402.13243, 2024. [9] Yuntao Chen, Yu-Quan Wang, and Zhaoxiang Zhang. Drivinggpt: Unifying driving world modeling and planning with multi-modal autoregressive transformers. arXiv preprint arXiv:2412.18607, 2024. [10] Haohan Chi, Huan ang Gao, Ziming Liu, Jianing Liu, Chenyu Liu, Jinwei Li, Kaisen Yang, Yangcheng Yu, Zeda Wang, Wenyi Li, Leichen Wang, Xingtao Hu, Hao Sun, Hang Zhao, and Hao Zhao. Impromptu vla: Open weights and open data for driving vision-language-action models. ArXiv, abs/2505.23757, 2025. URL https://api.semanticscholar.org/CorpusID:278996666. [11] Jang Hyun Cho, Boris Ivanovic, Yulong Cao, Edward Schmerling, Yue Wang, Xinshuo Weng, Boyi Li, Yurong You, Philipp Krähenbühl, Yan Wang, et al. Language-image models with 3d understanding. ICLR, 2025. [12] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Shi Guang, and Haoqi Fan. Emerging properties in unified multimodal pretraining. ArXiv, abs/2505.14683, 2025. URL https://api.semanticscholar.org/CorpusID:278768720. [13] Jianwu Fang, Dingxin Yan, Jiahuan Qiao, Jianru Xue, and Hongkai Yu. Dada: Driver attention prediction in driving accident scenarios. IEEE Transactions on Intelligent Transportation Systems, 2022. [14] Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, Dingkang Liang, Chong Zhang, Dingyuan Zhang, Hongwei Xie, Bing Wang, and Xiang Bai. Orion: holistic end-to-end autonomous driving framework by vision-language instructed action generation. ArXiv, abs/2503.19755, 2025. URL https://api.semanticscholar. org/CorpusID:277314093. [15] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control. ICLR, 2024. [16] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. In NeurIPS, 2024. [17] Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Pedro Rezende, Yasaman Haghighi, David Brüggemann, Isinsu Katircioglu, Lin Zhang, Xiaoran Chen, Suman Saha, Marco Cannici, Elie Aljalbout, Botao Ye, Xi Wang, Aram Davtyan, Mathieu Salzmann, Davide Scaramuzza, Marc Pollefeys, Paolo Favaro, and Alexandre Alahi. 11 Gem: generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control. CVPR, 2025. [18] Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. ICML, 2022. [19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. [20] Anthony Hu, Gianluca Corrado, Nicolas Griffiths, Zak Murez, Corina Gurau, Hudson Yeo, Alex Kendall, Roberto Cipolla, and Jamie Shotton. Model-based imitation learning for urban driving. NeurIPS, 2022. [21] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: generative world model for autonomous driving. arXiv preprint arXiv:2309.17080, 2023. [22] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. St-p3: End-to-end vision-based autonomous driving via spatial-temporal feature learning. In ECCV, 2022. [23] Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, and Ping Tan. Drivingworld: Constructing world model for autonomous driving via video gpt. arXiv preprint arXiv:2412.19505, 2024. [24] Yi Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wen Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. CVPR, pages 1785317862, 2022. URL https://api.semanticscholar.org/CorpusID:257687420. [25] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. In CVPR, 2023. [26] Yuxiao Hu, Qian Li, Dongxiao Zhang, Jinyue Yan, and Yuntian Chen. Context-alignment: Activating and enhancing LLMs capabilities in time series. In ICLR, 2025. [27] Junjie Huang, Guan Huang, Zheng Zhu, Yun Ye, and Dalong Du. Bevdet: High-performance multi-camera 3d object detection in bird-eye-view. arXiv preprint arXiv:2112.11790, 2021. [28] Ayesha Ishaq, Jean Lahoud, Fahad Shahbaz Khan, Salman Khan, Hisham Cholakkal, and Rao Muhammad Anwer. Tracking meets large multimodal models for driving scenario understanding. ArXiv preprint arXiv:2503.14498, 2025. [29] Anqing Jiang, Yu Gao, Zhigang Sun, Yiru Wang, Jijun Wang, Jinghao Chai, Qian Cao, Yuwen Heng, Hao Jiang, Zongzheng Zhang, Xianda Guo, Hao Sun, and Hao Zhao. Diffvla: Vision-language guided diffusion planning for autonomous driving. ArXiv, abs/2505.19381, 2025. URL https://api.semanticscholar.org/CorpusID: 278904380. [30] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. ICCV, 2023. [31] Bo Jiang, Shaoyu Chen, Bencheng Liao, Xingyu Zhang, Wei Yin, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Senna: Bridging large vision-language models and end-to-end autonomous driving. ArXiv, abs/2410.22313, 2024. URL https://api.semanticscholar.org/CorpusID:273661831. [32] Bo Jiang, Shaoyu Chen, Qian Zhang, Wenyu Liu, and Xinggang Wang. Alphadrive: Unleashing the power of vlms in autonomous driving via reinforcement learning and reasoning. ArXiv, abs/2503.07608, 2025. URL https://api.semanticscholar.org/CorpusID:276928398. [33] Bohan Li, Jiazhe Guo, Hongsi Liu, Yingshuang Zou, Yikang Ding, Xiwu Chen, Hu Zhu, Feiyang Tan, Chi Zhang, Tiancai Wang, et al. Uniscene: Unified occupancy-centric driving scene generation. CVPR, 2025. [34] Boyi Li, Yue Wang, Jiageng Mao, Boris Ivanovic, Sushant Veer, Karen Leung, and Marco Pavone. Driving everywhere with large language model policy adaptation. In CVPR, 2024. [35] Guopeng Li, Yiru Jiao, Victor Knoop, Simeon Calvert, and JWC Van Lint. Large car-following data based on lyft level-5 open dataset: Following autonomous vehicles vs. human-driven vehicles. In ITSC, pages 58185823. IEEE, 2023. 12 [36] Xiang Li, Pengfei Li, Yupeng Zheng, Wei Sun, Yan Wang, and Yilun Chen. Semi-supervised vision-centric 3d occupancy world model for autonomous driving. ICLR, 2025. [37] Yongkang Li, Kaixin Xiong, Xiangyu Guo, Fang Li, Sixu Yan, Gangwei Xu, Lijun Zhou, Long Chen, Haiyang Sun, Bing Wang, Guang Chen, Hangjun Ye, Wenyu Liu, and Xinggang Wang. Recogdrive: reinforced cognitive framework for end-to-end autonomous driving. ArXiv, abs/2506.08052, 2025. URL https://api. semanticscholar.org/CorpusID:279261178. [38] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer: learning birds-eye-view representation from lidar-camera via spatiotemporal transformers. IEEE TPAMI, 2024. [39] Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, and Xinggang Wang. Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving. CVPR, pages 1203712047, 2024. URL https://api.semanticscholar.org/CorpusID:274192736. [40] Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, and Xinggang Wang. Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving. CVPR, 2025. [41] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [42] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun. Petr: Position embedding transformation for multi-view 3d object detection. In ECCV, pages 531548. Springer, 2022. [43] Hao Lu, Jiaqi Tang, Xinli Xu, Xu Cao, Yunpeng Zhang, Guoqing Wang, Dalong Du, Hao Chen, and Yingcong Chen. Scaling multi-camera 3d object detection through weak-to-strong eliciting. arXiv preprint arXiv:2404.06700, 2024. [44] Hao Lu, Zhuang Ma, Guangfeng Jiang, Wenhang Ge, Bohan Li, Yuzhan Cai, Wenzhao Zheng, Yunpeng Zhang, and Yingcong Chen. 4d driving scene generation with stereo forcing. arXiv preprint arXiv:2509.20251, 2025. [45] Hao Lu, Tianshuo Xu, Wenzhao Zheng, Yunpeng Zhang, Wei Zhan, Dalong Du, Masayoshi Tomizuka, Kurt Keutzer, and Yingcong Chen. Drivingrecon: Large 4d gaussian reconstruction model for autonomous driving. NeurIPS, 2025. [46] Hao Lu, Yunpeng Zhang, Qing Lian, Dalong Du, and Yingcong Chen. Towards generalizable multi-camera 3d object detection via perspective debiasing. AAAI, 2025. [47] Qi Lv, Weijie Kong, Hao Li, Jia Zeng, Zherui Qiu, Delin Qu, Haoming Song, Qizhi Chen, Xiang Deng, and Jiangmiao Pang. F1: vision-language-action model bridging understanding and generation to actions. ArXiv, abs/2509.06951, 2025. URL https://api.semanticscholar.org/CorpusID:281204333. [48] Yingzi Ma, Yulong Cao, Jiachen Sun, Marco Pavone, and Chaowei Xiao. Dolphins: Multimodal language model for driving, 2024. [49] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. In CVPR, pages 77397751, 2025. [50] Chen Min, Dawei Zhao, Liang Xiao, Jian Zhao, Xinli Xu, Zheng Zhu, Lei Jin, Jianshu Li, Yulan Guo, Junliang Xing, Liping Jing, Yiming Nie, and Bin Dai. Driveworld: 4d pre-trained scene understanding via world models for autonomous driving. CVPR, 2024. [51] Chaojun Ni, Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Wenkang Qin, Guan Huang, Chen Liu, Yuyin Chen, Yida Wang, Xueyang Zhang, Yifei Zhan, Kun Zhan, Peng Jia, Xianpeng Lang, Xingang Wang, and Wenjun Mei. Recondreamer: Crafting world models for driving scene reconstruction via online restoration. In CVPR, 2025. [52] Jingcheng Ni, Yuxin Guo, Yichen Liu, Rui Chen, Lewei Lu, and Zehuan Wu. Maskgwm: generalizable driving world model with video mask reconstruction. CVPR, 2025. [53] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. 13 [54] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [55] Peter Pinggera, Sebastian Ramos, Stefan Gehrig, Uwe Franke, Carsten Rother, and Rudolf Mester. Lost and found: detecting small road hazards for self-driving vehicles. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2016. [56] Xuanchi Ren, Yifan Lu, Tianshi Cao, Ruiyuan Gao, Shengyu Huang, Amirmojtaba Sabour, Tianchang Shen, Tobias Pfaff, Jay Zhangjie Wu, Runjian Chen, et al. Cosmos-drive-dreams: Scalable synthetic driving data generation with world foundation models. arXiv preprint arXiv:2506.09042, 2025. [57] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. [58] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. ECCV, 2024. [59] Aasheesh Singh, Aditya Kamireddypalli, Vineet Gandhi, and Madhava Krishna. Lidar guided small obstacle segmentation. In IROS, pages 85138520. IEEE, 2020. [60] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, pages 24462454, 2020. [61] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. [62] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [63] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Zhiyong Zhao, Yang Wang, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. Drivevlm: The convergence of autonomous driving and large vision-language models. CoRL, 2024. [64] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [65] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [66] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [67] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and José M. Álvarez. Omnidrive: holistic vision-language dataset for autonomous driving with counterfactual reasoning. CVPR, pages 2244222452, 2024. URL https://api.semanticscholar.org/CorpusID:269502377. [68] Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Yang Wen, Silei Wu, Hanming Deng, Zhiqi Li, Hao Tian, Lewei Lu, Xizhou Zhu, Xiaogang Wang, Yu Qiao, and Jifeng Dai. Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving. ArXiv, abs/2312.09245, 2023. URL https://api.semanticscholar.org/CorpusID:266210476. [69] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world-driven world models for autonomous driving. ECCV, 2024. [70] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jiagang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world-drive world models for autonomous driving. In ECCV, pages 5572. Springer, 2024. 14 [71] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [72] Yan Wang, Wenjie Luo, Junjie Bai, Yulong Cao, Tong Che, Ke Chen, Yuxiao Chen, Jenna Diamond, Yifan Ding, Wenhao Ding, et al. Alpamayo-r1: Bridging reasoning and action prediction for generalizable autonomous driving in the long tail. arXiv preprint arXiv:2511.00088, 2025. [73] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. CVPR, 2024. [74] Julong Wei, Shanshuai Yuan, Pengfei Li, Qingda Hu, Zhongxue Gan, and Wenchao Ding. Occllama: An occupancy-language-action generative world model for autonomous driving. arXiv preprint arXiv:2409.03272, 2024. [75] Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In CVPR, pages 69026912, 2024. [76] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. In CVPR, pages 1296612977, 2025. [77] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [78] Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models. arXiv preprint arXiv:2506.15564, 2025. [79] Runsheng Xu, Hubert Lin, Wonseok Jeon, Hao Feng, Yuliang Zou, Liting Sun, John Gorman, Kate Tolstaya, Sarah Tang, Brandyn White, et al. Wod-e2e: Waymo open dataset for end-to-end driving in challenging long-tail scenarios. arXiv preprint arXiv:2510.26125, 2025. [80] Tianshuo Xu, Hao Lu, Xu Yan, Yingjie Cai, Bingbing Liu, and Yingcong Chen. Occ-llm: Enhancing autonomous driving with occupancy-based large language models. ICRA, 2025. [81] Tianshuo Xu, Hao Lu, Xu Yan, Yingjie Cai, Bingbing Liu, and Yingcong Chen. Occ-llm: Enhancing autonomous driving with occupancy-based large language models. ICRA, 2025. [82] Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, and Sida Peng. Street gaussians: Modeling dynamic urban scenes with gaussian splatting. In ECCV, pages 156173. Springer, 2024. [83] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, Jun Zhang, Andreas Geiger, Yu Qiao, and Hongyang Li. Generalized predictive model for autonomous driving. In CVPR, 2024. [84] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, et al. Generalized predictive model for autonomous driving. In CVPR, pages 1466214672, 2024. [85] Zetong Yang, Li Chen, Yanan Sun, and Hongyang Li. Visual point cloud forecasting enables scalable autonomous driving. In CVPR, 2024. [86] Zhenjie Yang, Yilin Chai, Xiaosong Jia, Qifeng Li, Yuqian Shao, Xuekai Zhu, Haisheng Su, and Junchi Yan. Drivemoe: Mixture-of-experts for vision-language-action model in end-to-end autonomous driving, 2025. [87] Shuang Zeng, Xinyuan Chang, Xinran Liu, Zheng Pan, and Xing Wei. Driving with prior maps: Unified vector prior encoding for autonomous vehicle mapping. arXiv preprint arXiv:2409.05352, 2024. [88] Shuang Zeng, Xinyuan Chang, Mengwei Xie, Xinran Liu, Yifan Bai, Zheng Pan, Mu Xu, and Xing Wei. Futuresightdrive: Thinking visually with spatio-temporal cot for autonomous driving. NeurIPS, 2025. [89] Andy Zhai, Brae Liu, Bruno Fang, Chalse Cai, Ellie Ma, Ethan Yin, Hao Wang, Hugo Zhou, James Wang, Lights Shi, Lucy Liang, Make Wang, Qian Wang, Roy Gan, Ryan Yu, Shalfun Li, Starrick Liu, Sylas Chen, 15 Vincent Chen, and Z. Xu. Igniting vlms toward the embodied space. ArXiv, abs/2509.11766, 2025. URL https://api.semanticscholar.org/CorpusID:281315304. [90] Jiawei Zhang, Chejian Xu, and Bo Li. Chatscene: Knowledge-enabled safety-critical scenario generation for autonomous vehicles. In CVPR, 2024. [91] Kaiwen Zhang, Zhenyu Tang, Xiaotao Hu, Xingang Pan, Xiaoyang Guo, Yuan Liu, Jingwei Huang, Li Yuan, Qian Zhang, Xiaoxiao Long, Xun Cao, and Wei Yin. Epona: Autoregressive diffusion world model for autonomous driving. ArXiv, abs/2506.24113, 2025. URL https://api.semanticscholar.org/CorpusID:280010626. [92] Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, Wenjun Mei, and Xingang Wang. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. In CVPR, 2025. [93] Guosheng Zhao, Chaojun Ni, Xiaofeng Wang, Zheng Zhu, Xueyang Zhang, Yida Wang, Guan Huang, Xinze Chen, Boyuan Wang, Youyi Zhang, et al. Drivedreamer4d: World models are effective data machines for 4d driving scene representation. In CVPR, pages 1201512026, 2025. [94] Peiru Zheng, Yun Zhao, Zhan Gong, Hong Zhu, and Shaohua Wu. Simplellm4ad: An end-to-end vision-language model with graph visual question answering for autonomous driving. ArXiv preprint arXiv:2407.21293, 2024. [95] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu. Occworld: Learning 3d occupancy world model for autonomous driving. ECCV, 2024. [96] Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming Zhang, and Long Chen. Genad: Generative end-to-end autonomous driving. ECCV, 2024. [97] Wenzhao Zheng, Zetian Xia, Yuanhui Huang, Sicheng Zuo, Jie Zhou, and Jiwen Lu. Doe-1: Closed-loop autonomous driving with large world model. ArXiv, abs/2412.09627, 2024. URL https://api.semanticscholar. org/CorpusID:274656022. [98] Wenzhao Zheng, Zetian Xia, Yuanhui Huang, Sicheng Zuo, Jie Zhou, and Jiwen Lu. Doe-1: Closed-loop autonomous driving with large world model. arXiv preprint arXiv: 2412.09627, 2024. [99] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [100] Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes. In CVPR, pages 2163421643, 2024. [101] Xin Zhou, Dingkang Liang, Sifan Tu, Xiwu Chen, Yikang Ding, Dingyuan Zhang, Feiyang Tan, Hengshuang Zhao, and Xiang Bai. Hermes: unified self-driving world model for simultaneous 3d scene understanding and generation. ArXiv, abs/2501.14729, 2025. URL https://api.semanticscholar.org/CorpusID:275906961. [102] Zewei Zhou, Tianhui Cai, Seth Z. Zhao, Yun Zhang, Zhiyu Huang, Bolei Zhou, and Jiaqi Ma. Autovla: vision-language-action model for end-to-end autonomous driving with adaptive reasoning and reinforcement fine-tuning. ArXiv, abs/2506.13757, 2025. URL https://api.semanticscholar.org/CorpusID:279410595. [103] Sicheng Zuo, Wenzhao Zheng, Yuanhui Huang, Jie Zhou, and Jiwen Lu. Gaussianworld: Gaussian world model for streaming 3d occupancy prediction. CVPR, 2025."
        },
        {
            "title": "A Dataset",
            "content": "To comprehensively evaluate and enhance the multimodal world modeling capability required for end-to-end autonomous driving, we reconstruct heterogeneous open-source driving datasets into unified framework aligned with four essential cognitive competencies. Existing benchmarks mainly emphasize structured urban scenes or closed-loop simulation metrics, which fail to capture models robustness in open-world, long-tail, and low-probability-but-high-risk scenarios. In contrast, our objective is to measure how well model can understand, reason, and act under diverse real-world conditions. Figure 5 Long-tail perception and understanding of questions and answers. To this end, we integrate datasets covering road abnormalities and traffic accident anticipation (e.g., Lost and Found, StreetHazards, DADA-2000, and Anticipating Accidents in Dashcam Videos) together with the Waymo dataset, which contains densely interactive and ambiguous safety-critical events. We reorganize these data according to four key task dimensions: (1) Perception and Understanding, assessing visual semantics and contextual risk awareness; (2) Causal CoT Reasoning, explaining the underlying causes behind motion intentions; (3) Planning and Decision-Making, supervising physically feasible future trajectories; and (4) Instruction Following, evaluating whether control actions align with high-level navigation commands. This integrated categorization supports unified training and interpretable evaluation of cognitive, generative, and control capabilities. 17 A.1 Perception and Understanding: We convert scene annotations from anomaly and hazard datasets into true/false and multiple-choice QA items that evaluate not only basic semantic comprehension, but also driving commonsense, corner-case interpretation, and the recognition of previously unseen or rare object categories. This formulation ensures that the model can identify what is present in the scene, understand why it is safety-relevant, and generalize to atypical or long-tail conditions critical for real-world driving. Specific examples are shown in Fig 5. Small long-tailed object. We collected multiple datasets that contained small elongated objects. We determine whether there are small tail objects based on the segmentation map labels provided by the dataset, and thereby construct judgment question. We designed many random questions to enhance the generalization ability of the models question answering. The questioning format is as shown in List 1: Listing 1 Small long-tailed object long i o c n d i i ? \" , ## Prompt = s n + \" a e w True or s . \" # s n : \"Any l \" Are r n long i i s \" Does d i i have l \" Small long i o c r n \" Are r a \"Any l \" Are r a \" s a b t with g l ? \" , \"Any y long i t g e t ? \" , h i g p ? \" , long i t g ? \" , h i g t ? \" , h i g e ? \" , e with g l ? \" , long i o c ? \" , long i o c Long-tailed accident prediction. We collected videos of abnormal traffic accidents. The dataset classifies whether the video is abnormal based on whether it is abnormal as indicated by the dataset and the specific timestamp. We have designed various questions to enhance generalization. The questioning method is as shown in List 2: Listing 2 Long-tailed accident prediction ## Prompt = s n + \" a e w True or s . \" # s n : \"Has an abnormal i t u d e ? \" , \" Could r be any f h r e ? \" , \" Could a c i t u r ? \" , \" Are r any f \"Was r an s \" Might a c i t \" n e r i t e t now? \" , e p i i h t f c e i happen e ? \" e c t d i i t ? \" , k hidden a h t c d d c e ? \" , Long-tailed accident relationship. We collected video footage of abnormal traffic accidents, along with annotations indicating the abnormal entities involved. Based on these annotations, we designed multiplechoice questions. The questioning method is as shown in the list 3: Listing 3 Long-tailed accident relationship INPUT : r _ w OUTPUT: s n ( ) , i _ t ( ) , d t _ l ( t [ ] ) ( t [ str , ] ) , r _ e ( ) , r _ w ( ) # p 1 : t i a r d t i a r _ d t = [ opt ( l c e answer ) opt in d t _ l opt = r _ w ] # p 2 : Randomly e u i y (2 i / 4 i ) option_count = RANDOM_CHOICE( [ 2 , 4 ] ) s c _ d = option_count 1 # p 3 : e i a r len ( t t _ d t ) t t _ d : ( p e f u c t ) l e _ t t = RANDOM_SAMPLE( t t _ d t , t t _ d ) e : e d _ t t = t t _ d t + RANDOM_CHOICES( t t _ d t , k=d r o _ d len ( t t _ d t ) ) # p 4 : Combine and f o o l _ i = [ r _ w ] + e d _ t t SHUFFLE( _ i ) # p 5 : Map p n e p n _ t = [ , i _ t = dict ( zip ( i _ t , _ i ) ) ] [ : option_count ] (A/B/C/D) , , # p 6 : a o c r t _ e = next ( t a l r t , opt in i _ t . m ( ) opt == r _ w ) # p 7 : Generate s n h c e m t e o = \"Which h l i e i t u n s a n ? \" + , . n ( [ \" { t } . { opt } \" l e , opt in i _ t . m ( ) ] ) RETURN s n , i _ t , r _ e , r _ w Figure 6 Future Trajectories-Based CoT Reasoning. A.2 Causal CoT Reasoning: For sequences where the future driving outcome is observable, we construct QA pairs in which the answer is structured multi-step chain of thought that explains how the final driving decision is formed. During dataset construction, we utilize both the future image sequence and the ground-truth ego trajectory to ensure that the reasoning process is strictly aligned with the actual physical outcome. The reasoning is required to describe the scene context, identify key interactive agents, infer their potential intentions, and justify the final driving action that leads to the observed future behavior. This design encourages the model to learn reasoning that is predictive and causally grounded in realistic driving dynamics, instead of producing descriptive or speculative explanations after the fact. As result, the model is encouraged to understand why particular action is necessary for safety rather than merely recognizing what action is taken. Specific examples are shown in Fig 6. CoT reasoning based on future trajectories is presented in List 4: ## The prompts g r n CoT : Listing 4 Future Trajectories-Based CoT Reasoning You an autonomous v p n e r . Your k o l a v f t camera views , t t waypoints , and i n motion c u g o t p i from images waypoints , t o y , t e a n , e a n , must s through s e based on u and p l data and produce r u d s output . f t your y on f r can y make d i about t c h d not s n . Your s n t d s s based on c e e n i r t , but your j o as b s a i r s r t o y , am i your u e i t help you j o . r your r t ego i . You image and Inp ut : Past p in : %%<past_waypoints>%% Past o y : %%<p _ o y>%% Past e a n : %%<p _ e a n>%% Future po s : %%<fu tur e_ ay poi s>%% r a and Temporal System l t : > motion and t data e e d in ego a > +x = w d c n > +y = t e o > The g ( 0 , 0 ) c e i c d h ego i e o n system : > t : > > > Waypoints : e e i : e / o c e i : e / o > Sampling q c : 4Hz ( r 0 . 2 5 ) ( past_waypoints , t _ o y , t _ e a n ) each 16 > Past a time p : Format : Indexed = 0 [ > > > > [ x0 , y0 ] , [ x1 , y1 ] , 1 5 , with . . . , [ x15 , y15 ] ] = 0 > = 3.75 , = 1 5 > = 0 . 0 ( r frame ) 20 Format : Indexed = 0 > Future_waypoints 20 time p : > . . . , > > > = 0 > = 0 . 2 5 , = 1 9 > = 5 . 0 [ x1 , y1 ] , 1 9 , with [ x0 , y0 ] , [ [ x19 , y19 ] ] Your k : Based on p i v a and motion data , p e f o g r a d Chaino Thought f t your y on f r can y make d i about t c t e r . p . am i your s n c e , but your j o as b s your t d s s s n o s help you u not s n . Your s n t d s s based on c e image and Step 1 : Scene l s c e o a t f c , l n a c h , road geometry , e markings , u r . n , s l , s c n a , and any temporary road Step 2 : Key bje ct n y up 3 o n or e e I t c o t r v t r n g , and o v an e e f i impact on v , such as : f c g b k , l d ) , h ( i l ) : c e r t p t , and meaning ego i s e ( . . , green , red , l , Dynamic n : c e v o t b s each agent , v c Rare e i l , and s i a in p n o e i e ( i l ) : . . , i s , emergency p e , c , curbs , l r , temporary o , s e y n r i a c o r , cones , n u o workers , i road r s , parked i y s c n machinery , s l l s , a n impact on e t . e o f n Step 3 : Based on input images p v c , and f r yp nt r g e object ( example , e d . And how do s j s e ( both r and u images ) , t t ego i , s g road , merging o l ) p i f r movement s f r i g ego i . ego i ( o y , e a n , ay po in ts ) , e e o t ego Step 4 : Ac n Reason i : u command u f h t e a t . For example , n t and w down in f r . Then s t down and n t . \" . This t s : Based e on c e h d be l in as \" a l h d be as e as s e . image and t c t e r , v a a d Chaino Thought t d s u g p bys r o g . a o h h \" s \" u not o any n a e l s . v a s b and e r o s t e i making s . am i your u e i t help you f t your s n c e , but your b s u e i based on c e e n p e f your s n . Your s n can y make d i about image and t c r c y . u not y on f r a t as Output Format : Your output format u be o object with f o g u r : n 21 { \" n _ l s \" : \" . . . \" , \" _ e \" : \" . . . \" , \" e o _ e c \" : \" . . . \" , \" i _ i n \" : { \" i \" : \" . . . \" , \" s \" : \" . . . \" } } A.3 Planning and Instruction Following For planning task, we directly provide the model with question that describes the current and historical driving context, and the model is required to predict the future trajectory of the ego vehicle for the next several seconds. The answer is represented as sequence of future trajectory points in the ego coordinate frame. The model may output this trajectory either directly based on its internal world understanding or after producing CoT reasoning sequence when reasoning is beneficial for disambiguating complex interactions. This formulation ensures that trajectory prediction is guided by both contextual scene understanding and physically grounded motion patterns, enabling safe, smooth, and interpretable future driving behavior. To enable instruction-conditioned trajectory planning, we derive high-level navigation commands such as going straight, turning left, or turning right from the geometric properties of the ground-truth future trajectory. These commands are then incorporated into the QA pairs so that the model must generate trajectory that is consistent with the given navigation intent. During training, the model learns to align the predicted motion sequence with the provided instruction, establishing direct correspondence between semantic driving goals and trajectory generation. During evaluation, this setup allows us to assess whether the model can correctly adjust its predicted future motion according to the specified high-level driving intent, ensuring that the resulting planning behavior remains both controllable and interpretable in downstream driving scenes."
        },
        {
            "title": "B More Cases",
            "content": "To better demonstrate the effectiveness of our method. We have provided even more examples. We compare the understanding and reasoning capabilities of the most advanced GPT4o in scenarios as shown in Fig. 7. Our approach can provide more specific suggestions for planning. We have further provided visualizations for trajectory and weather control generation as shown in Fig. 8 and Fig. 9. This proves the effectiveness of our methods generation capability."
        },
        {
            "title": "C Limitation and Future Directions",
            "content": "Despite the promising performance of UniUGP in unifying scene understanding, future video generation, and trajectory planning for autonomous driving, it still has several limitations that point to valuable future research directions. C.1 Limitations First, while UniUGP uses over 10 diverse AD datasets to cover common and long-tail scenarios, its generalization to extreme rare events (e.g., unprecedented weather, novel obstacles) is constrained by training data coveragecritical for safety-critical systems. Second, the hybrid expert architectures computational efficiency is problematic: the generation expert, though useful for visual causal validation, demands excessive resources and must be disabled on resource-constrained mobile platforms to ensure real-time performance. Third, linguistic reasoning and physical dynamics alignment, though improved via multi-term loss functions, is suboptimal; in complex interactive scenarios (e.g., ambiguous pedestrian-vehicle interaction), chain-of-thought (CoT) reasoning may not tightly couple with physically consistent trajectory generation, causing minor 22 interpretability-action inconsistencies. Fourth, the four-stage training strategy relies on fixed dataset proportions in the final fusion stage, failing to dynamically adapt to different datasets complementary strengths and limiting task synergy. C.2 Future Directions To address these limitations, we propose targeted directions: enhance generalization to extreme long-tail scenarios via high-fidelity synthetic data generation (e.g., world models + generative AI) and few-shot/zero-shot learning; optimize model efficiency through lightweight generation expert designs (e.g., knowledge distillation, sparse activation) and reduced multi-expert redundant computations. Deepen multimodal alignment with cross-modal contrastive learning and hierarchical fusion mechanisms that adjust expert weights by scene complexity; reduce labeled data dependence via self-supervised signals (e.g., unsupervised video causal reasoning) and enable incremental adaptation with continual learning to avoid catastrophic forgetting. Extend interaction capabilities to dynamic real-time feedback (e.g., mid-task voice commands) and multi-agent reasoning for complex traffic interactions; integrate UniUGP into closed-loop systems for real-world testing, establishing performance-refinement feedback loop to boost safety and robustness. These efforts will evolve UniUGP into more practical framework bridging laboratory performance and real-world deployment. 23 Figure 7 Comparison of CoT in our method versus GPT4o. Our approach provides more specific planning results, while the general large model does not offer sufficiently detailed planning outcomes. 24 Figure 8 Video generation visualization with controllable weather conditions. Our model can generate videos of different weather conditions, which proves the efficiency of our generation model. Please zoom in to the best view. Figure 9 Video generation visualization with controllable trajectory conditions. Our model can generate videos of different trajectory conditions, which proves the efficiency of our generation model. Please zoom in to the best view."
        }
    ],
    "affiliations": []
}