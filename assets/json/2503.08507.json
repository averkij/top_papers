{
    "paper_title": "Referring to Any Person",
    "authors": [
        "Qing Jiang",
        "Lin Wu",
        "Zhaoyang Zeng",
        "Tianhe Ren",
        "Yuda Xiong",
        "Yihao Chen",
        "Qin Liu",
        "Lei Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Humans are undoubtedly the most important participants in computer vision, and the ability to detect any individual given a natural language description, a task we define as referring to any person, holds substantial practical value. However, we find that existing models generally fail to achieve real-world usability, and current benchmarks are limited by their focus on one-to-one referring, that hinder progress in this area. In this work, we revisit this task from three critical perspectives: task definition, dataset design, and model architecture. We first identify five aspects of referable entities and three distinctive characteristics of this task. Next, we introduce HumanRef, a novel dataset designed to tackle these challenges and better reflect real-world applications. From a model design perspective, we integrate a multimodal large language model with an object detection framework, constructing a robust referring model named RexSeek. Experimental results reveal that state-of-the-art models, which perform well on commonly used benchmarks like RefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple individuals. In contrast, RexSeek not only excels in human referring but also generalizes effectively to common object referring, making it broadly applicable across various perception tasks. Code is available at https://github.com/IDEA-Research/RexSeek"
        },
        {
            "title": "Start",
            "content": "Qing Jiang1,2 , Lin Wu1,2 , Zhaoyang Zeng1 , Tianhe Ren1 , Yuda Xiong1 Yihao Chen1 , Liu Qin1 , Lei Zhang1,2 1International Digital Economy Academy (IDEA) 2South China University of Technology mountchicken@outlook.com , leizhang@idea.edu.cn https://deepdataspace.com/blog/dino-xseek 5 2 0 2 1 1 ] . [ 1 7 0 5 8 0 . 3 0 5 2 : r Figure 1. We introduce referring to any person, task that requires detecting all individuals in an image which match given natural language description, and new model RexSeek designed for this task with strong perception and understanding capabilities that effectively captures attributes, spatial relations, interactions, reasoning, celebrity recognition, etc."
        },
        {
            "title": "Abstract",
            "content": "Humans are undoubtedly the most important participants in computer vision, and the ability to detect any individual given natural language description, task we deThis work was done when Qing Jiang and Lin Wu were interns at IDEA. Corresponding author. fine as referring to any person, holds substantial practical value. However, we find that existing models generally fail to achieve real-world usability, and current benchmarks are limited by their focus on one-to-one referring, that hinder progress in this area. In this work, we revisit this task from three critical perspectives: task definition, dataset design, and model architecture. We first identify five aspects of referable entities and three distinctive characteristics of this task. Next, we introduce HumanRef, novel dataset designed to tackle these challenges and better reflect realworld applications. From model design perspective, we integrate multimodal large language model with an object detection framework, constructing robust referring model named RexSeek. Experimental results reveal that stateof-the-art models, which perform well on commonly used benchmarks like RefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple individuals. In contrast, RexSeek not only excels in human referring but also generalizes effectively to common object referring, making it broadly applicable across various perception tasks. Code is available at https://github.com/IDEAResearch/RexSeek 1. Introduction Humans are central to computer vision [4, 1012, 16, 21, 27, 28, 30, 45, 46, 63, 65, 84, 85], and the ability to identify and detect specific individuals based on natural language descriptions, task we define as referring to any person, is crucial for numerous applications, including human-robot interaction, industrial automation, healthcare, etc. However, we argue that progress in this area has been hindered by unclear task definitions and lack of highquality data. Our findings show that despite achieving stateof-the-art performance on referring benchmarks RefCOCO/+/g [50, 75], most models remain impractical for realworld applications, as illustrated in Figure 2. To address this challenge, we revisit this task from three perspectives: task definition, dataset construction, and model design. We begin by formally defining the task of referring to any person: given natural language description and an input image, the model needs to detect all individuals in the image who match the description. To comprehensively capture the scope of this task, we identify five key aspects that define how humans can be referred to: i) Attributes: Encompassing intrinsic characteristics such as gender, age, action, clothing, etc. ii) Position: Describing spatial relationships both among individuals and between individuals and their surroundings. iii) Interaction: Accounting for human-tohuman, human-to-object, and human-to-environment interactions. iv) Reasoning: Involving multi-step inference that considers multiple objects to resolve complex referrings. v) Celebrity Recognition: Identifying specific individuals, whether by their real names or characters names. Next, we identify three crucial characteristics that define this task: i) Multi-Instance Referring: referring expression can correspond to multiple individuals. While mainstream referring datasets RefCOCO/+/g [50, 75] typically assume that each expression refers to single object, this does not align with real-world scenarios. We find through experiments that most models experience significant performance degradation when tasked with identifying more than Figure 2. Visualization results of Qwen2.5-VL [3], InternVL2.5 [14], and DeepSeek-VL2 [70] on the human referring task. Despite achieving strong results on referring benchmarks RefCOCO/+/g [50, 75], state-of-the-art models struggle when tasked with identifying multiple individuals as they output an insufficient number of bounding boxes. ii) Multi-Instance Discrimination: The one individual. image should contain multiple individuals in addition to the target person. This setting ensures that the model fully comprehends the referring expression to identify the correct individual rather than simply detecting all people in the image. iii) Rejection of Non-existence: If the referred person is not present in the image, the model should refuse to generate result rather than produce hallucinated output. Based on the task definition, we manually constructed novel dataset for human referring, named HumanRef. Unlike the traditional ReferItGame [29] annotation approach, where one annotator describes an object and another finds it based on the description, we adopt different annotation methodology. Our process begins with annotators listing the key properties of individuals in an image according to the predefined referable entities. Next, for each person, they determine whether these properties apply and result in property dictionary. Finally, large language model [71] composes these properties into referring expressions. HumanRef comprises 103,028 referring statements, with each expression referring to an average of 2.2 instances. We also split benchmark from HumanRef with 6,000 referring expressions spanning six subsets, ensuring comprehensive coverage across all referable properties. From the model design perspective, we argue that robust referring model should possess two key characteristics: i) Robust Perception Ability: The model should be capable of detecting all individuals in an image. ii) Strong Language Comprehension: The model should effectively interpret complex language descriptions of people. To address these requirements, we introduce RexSeek, detectionoriented multimodal large language model specifically designed for this task. Inspired by ChatRex [25], we formulate referring as retrieval-based task. RexSeek lintegrates 2 person detector [60] as its box input, ensuring strong perception capabilities while incorporating Qwen2.5 [71] as the LLM to enhance language comprehension. We adopt multi-stage training approach that progressively refines both detection and comprehension skills, equipping RexSeek with strong referring capabilities. Experimental results indicate that most state-of-the-art models [3, 9, 14, 25, 49, 60, 70, 74] exhibit performance degradation on the HumanRef benchmark, despite achieving strong results on RefCOCO/+/g. The primary limitation is that these models typically detect only single instance, as they are trained on datasets that assume one-to-one referring. In contrast, RexSeek, trained on HumanRef, exhibits strong referring capabilities. Additionally, benefiting from the multi-stage training approach, RexSeek also emerges with the ability to refer to generalized objects, extending its applicability beyond human-centric tasks. To summarize, our contributions are threefold: We introduce referring to any person with clear definition by identifying five aspects of referable entities and three key characteristics that distinguish this task. We introduce HumanRef, novel referring dataset, and establish challenging benchmark to drive progress in human-centric referring expression research. We propose RexSeek, detection-oriented multimodal large language model trained through multi-stage process, demonstrating strong referring capabilities for both humans and general objects. 2. Related Work Referring Expression Comprehension Task. Referring Expression Comprehension (REC) [29, 36, 48, 50, 56, 72, 75, 76, 76, 81] involves interpreting natural language expression to localize specific objects within an image. Unlike open-vocabulary object detection [15, 26, 34, 42, 52, 60, 61, 69, 73, 78] or phrase grounding [18, 23, 31, 54, 68], which identify objects based on brief category names or short phrases, REC requires understanding complex, freeform descriptions. This task necessitates not only recognizing object attributes and relationships but also comprehending spatial configurations and interactions, making it inherently more challenging. In this work, we systematically analyze the referable entities and the critical characteristics that define this task."
        },
        {
            "title": "The first",
            "content": "REC Datasets and Benchmarks. largescale Referring Expression Comprehension (REC) dataset, ReferItGame [29], was created through two-player game in which one annotator describes an object, and another selects it. This was later followed by more sophisticated datasets [7, 13, 17, 19, 54, 55], such as RefCOCO [75], RefCOCO+ [75], and RefCOCOg [50], which leverage MSCOCO [37] images to provide more complex referring expressions. Beyond these general datasets, others address domain attribute gender, age, race, profession, posture, appearance, clothing and accessories, action sub-domains examples position inner position (human to human), outer position (human to environment) interaction inner interaction (human with human), outer interaction (human with environment) reasoning celebrity recognition rejection inner position reasoning, outer position reasoning, attribute reasoning actor, character, athlete, entrepreneur, scientist, politician, singer attribute, position, interaction, reasoning male, female, white man, the police officer, person with shocked expression, person wearing mask, person standing the second person from left to right, person at the right, person closest to the microphone, person sitting in the chair two people holding hands, people locked in each others gaze, the person holding gun, person holding the certificate in hand all the people to the right of the person closest to the glass, person wearing lab coat but not putting their hand on the board Brad Pitt, Bruce Wayne, Cristiano Ronaldo, Rihanna, Elon Musk, Albert Einstein, Donald Trump man in red hat, three women in circle Table 1. The primary annotation domains and their corresponding sub-domains within HumanRef. specific challenges. CLEVR-Ref+ [40] focuses on geometric object referring. RefCrowd [57] targets person detection in crowded scenes. Ref-L4 [8] handles longer and more detailed descriptions. GRES [77] introduces multitarget referring expression segmentation. However, existing datasets typically assume one-to-one correspondence between referring expression and single instance, which fails to reflect real-world scenarios. To address this gap, we refine the referring task and introduce HumanRef, dataset specifically designed to support multi-instance referring and advance research in this domain. MLLM-based REC Methods Multimodal Large Language Models (MLLMs) [13, 14, 20, 32, 33, 35, 44, 47, 53, 64, 66, 70, 82] have demonstrated strong capabilities in both text and image comprehension, motivating efforts to integrate referring expression understanding into these models. common approach involves outputting bounding box coordinates as tokens [3, 9, 14, 51, 67, 70, 74, 79, 80, 83]. Alternatively, methods like Groma [49] and ChatRex [25] frame detection as retrieval task, where proposal model generates bounding boxes, and the LLM selects the index of the relevant box based on the referring expression. While these MLLM-based methods achieve high performance on RefCOCO/+/g, our experiments reveal that they remain inadequate for practical applications due to low recall rate on multi-instance referrings. 3. HumanRef Dataset In this section, we present the design philosophy, data acquisition process, annotation pipeline, and dataset statistics of the proposed HumanRef dataset. 3.1. Data Design Philosophy We define five key aspects that determine how humans can be referred to using natural language, including attribute, position, interaction, reasoning, and celebrity recognition. These categories are further elaborated with definitions and examples in Table 1. key distinction between HumanRef and existing referring datasets is its focus on multiinstance referring rather than one-to-one object referring. Our dataset ensures that single referring expression can 3 Figure 3. Overview of the mannual annotation pipeline of the HumanRef dataset. correspond to multiple individuals, providing more realistic and practical reflection of real-world scenarios. 3.2. Data Acquisition The HumanRef dataset is designed to capture human presence across diverse contexts, including natural environments, industrial settings, healthcare, sports, films, animations, etc. To ensure dataset diversity, we sourced images containing humans from web image dataset [5]. To filter candidate images, we first retained those with resolution larger than 1000 1000 pixels to ensure high-quality content. Next, we use an open-set object detector DINO-X [60] to detect human instances. To align with the multi-instance discrimination requirement, we retain only images containing at least four individuals. To assist the annotator in writing properties, we prompt the QwenVL-2.5 [3] model to create structured property dictionary for each person in the image, capturing details such as gender, clothing, actions, etc. Ultimately, this phase produced image, person box, and person description triples used for further annotation. 3.3. Manual Annotation For attribute, position, interaction, and reasoning subsets, we adopt manual annotation. This annotation process consists of three main steps, including property listing, property assignment, and referring style rewriting. Given an image, along with the corresponding person boxes and pre-labeled property dictionary, the annotation system will randomly select one annotation type from attribute, location, interaction, and reasoning to assign to the annotator. The following annotation process is then carried out: Property Listing: The annotator examines all individuals in the image, considering both their visual appearance, action, position, interaction, and the pre-labeled property dictionary. Based on these observations, the annotator compiles list of properties. To enhance dataset richness, annotators are encouraged to label attributes shared by multiple individuals while avoiding those common to all. Additionally, we monitor the word frequency of labeled referring expressions and restrict the use of high-frequency words to improve data diversity. Property Assignment: Once the properties are listed, annotators systematically assign them to the corresponding individuals. This interactive process involves selecting property value and clicking on the associated bounding boxes to link it to correct persons. The final output is structured dictionary, where keys represent property names and values contain lists of bounding box indices corresponding to the individuals possessing each property. Referring Style Rewriting: In the final step, we prompt Qwen2.5 [71] to reformulate the structured attribute dictionary into short, natural language referring expressions. The final annotated data also undergoes thorough review process to ensure its quality. 3.4. Automatic Annotation For celebrity recognition and rejection referring, we employ two efficient and effective automatic annotation pipelines. Celebrity Recognition: We first categorize celebrities into seven distinct fields: actors, film characters, athletes, singers, entrepreneurs, scientists, and politicians. For each field, we identify the most well-known individuals, compiling final list of 636 names, which we then used as prompts to retrieve images via the Bing Search API. The collected images include both individual and group photos, necessitating method to accurately associate each celebrity name with the correct person in the image. To achieve this, we https://www.microsoft.com/en-us/bing/apis 4 Figure 4. Visualization of the six subsets in the HumanRef Benchmark. first use the DINO-X [60] model to detect all human faces and persons, linking each detected face to its corresponding person box based on overlap measurements. If an image contains only one person, we assume this individual is the target celebrity. For images featuring multiple individuals, we use Python face recognition library, leveraging single-person image as recognition template to match and identify the same person in such images. Rejection Referring: The objective of this sub-dataset is to ensure that when referring description targets person who does not exist in the input image, the model rejects the referring request instead of hallucinating and outputting an incorrect bounding box. To construct this dataset, we first extract referring expressions from the attribute, position, interaction, and reasoning subsets. We then prompt Qwen2.5 [71] to modify these descriptions, transforming them into similar but semantically altered versions. For instance, description such as the person wearing blue hat may be changed to the person wearing red hat. To validate the generated descriptions, we prompt Molmo [20] to detect the modified referring expression. If no matching object is found in the output, the data is retained. 3.5. HumanRef Benchmark To construct the HumanRef Benchmark, we sample 1,000 referring expressions from each of the four manually annotated subsets. Additionally, for the celebrity and rejection subsets, we conduct separate manual annotation process to create 1,000 new referring expressions for each category, ensuring high-quality and challenging evaluation data. To further support advancements in referring expression seghttps://github.com/ageitgey/facerecognition type images referrings avg. boxes/ref type images referrings avg. boxes/ref attribute 8,614 52,513 2. attribute 838 1,000 2.8 HumanRef Train interaction 1,632 2,911 3.1 reasoning 4,474 6,808 3.0 HumanRef Benchmark interaction 940 1,000 2.1 reasoning 982 1,000 2.7 position 7,577 22,496 1. position 972 1,000 2.1 celebrity 4,990 4,990 1.0 celebrity 1,000 1,000 1.1 rejection 7,519 13,310 0 rejection 1,000 1,000 0 total 34,806 103,028 2. total 5,732 6,000 2.2 Table 2. Main statistics of the HumanRef dataset, including the number of images, the number of referring expressions, the average word count per referring expression, and the average number of instances associated with each referring expression. Datasets images refs vocabs RefCOCO [75] RefCOCO+ [75] RefCOCOg [50] HumanRef 1,519 1,519 1,521 5,732 10,771 10,908 5,253 6,000 1,874 2,288 2,479 2,714 avg. size 593x484 592x484 585x480 1432x1074 avg. person/image 5.72 5.72 2.73 8. avg. words/ref 3.43 3.34 9.07 6.69 avg. boxes/ref 1 1 1 2.2 Table 3. Comparison of the HumanRef Benchmark with RefCOCO/+/g. For fair comparison, we present only the statistics related to human referring in RefCOCO/+/g. mentation, we utilize SAM2 [59] to generate masks for each ground truth bounding box. Figure 4 presents example cases from the HumanRef Benchmark, illustrating the diversity and complexity of the dataset. 3.6. Statistics We first present the basic statistics of the HumanRef dataset and its subsets in Table 2, and then illustrate the characteristics of multi-instance referring and multi-instance discrimination in HumanRef in Figure 5. Additionally, Table 3 compares the HumanRef Benchmark with widely used referring benchmarks, including RefCOCO, RefCOCO+, and RefCOCOg. key distinction of HumanRef is its higher 5 Figure 5. Distribution of the number of individuals per image and the number of individuals referenced by each referring expression. image resolution and larger number of individuals per image, requiring models to precisely identify all correct individuals among multiple people. Unlike traditional benchmarks, where each referring expression corresponds to single person, HumanRef supports multi-instance referring, offering more realistic and challenging evaluation setting for referring expression comprehension. 4. RexSeek Model 4.1. Model Design Philosophy From model design perspective, we argue that robust i) referring model should have two essential capabilities: robust perception ability, where the model can reliably detect all individuals in an image, and ii) strong language comprehension, where the model can accurately interpret complex natural language descriptions of people. For the first capability, modern object detection models [26, 41, 60, 61] are highly effective at identifying people within images. However, these models often lack the necessary language comprehension abilities to process intricate and nuanced referring expressions. On the other hand, while MLLMs are proficient in understanding natural language, they often struggle with fine-grained object detection tasks. Inspired by ChatRex [25], we propose hybrid framework, RexSeek, which integrates the strengths of both object detection models and LLMs. RexSeek combines high-performance detection model with multimodal LLM to achieve both accurate detection and effective language understanding. 4.2. Architecture Following ChatRex, we formulate the referring task as retrieval-based process [25, 49]. As illustrated in Figure 6, RexSeek consists of three main components: vision encoders, person detector, and large language model. Given an input image, we first pass it through dual vision encoder module used in ChatRex. This module consists of CLIP [58] to extract low-resolution image features Flow and ConvNeXt [43] to extract high-resolution image features Fhigh. We adjust the input resolutions for both vision encoders to ensure they generate the same number of tokens Figure 6. Overview of the RexSeek model. RexSeek is retrievalbased model built upon ChatRex [25]. By integrating person detection model, RexSeek transforms the referring task from predicting box coordinates to retrieving the index of input boxes. at the last scale. The final vision tokens is obtained by concatenating these features at the channel dimension: = Concat(Flow, Fhigh) Next, we prompt DINO-X [60] to get the bounding boxes of persons {Bi}K i=1 in the image. For each bounding box, we extract its RoI features Ci and add their positional embeddings to generate object tokens Oi, which capture both the content and spatial context of each detected person: Oi = Ci + PE(Bi) Specifically, the RoI feature is extracted from the highresolution vision features using multi-scale RoI Align operation [24]. The positional embedding is computed by encoding the bounding box coordinates (x, y, w, h) using sinusoidal encoding function and concatenating the encoded values along the channel dimension. Finally, the vision tokens F, object tokens O, and text tokens are projected using different MLPs and then fed into the LLM. By default, we use Qwen2.5 [71] as the LLM. The LLM decodes the input to produce the corresponding box indices I: = LLM(F, O, ) The output consists of object indices that correspond to the bounding boxes of the target persons corresponding to the referring. This sequence is structured as follows: <g>referring</g><o><objm>...<objn></o> Here, <objm> and <objn> refer to specific object index tokens that correspond to the detected persons. The special tokens <g>, </g>, <o>, and </o> are used to format the output, linking the referring expression with the relevant object indices. 6 Method Baseline DINOX [60] InternVL-2.5-8B [14] Ferret-7B [74] Groma-7B [49] ChatRex-7B [25] Qwen2.5-VL-7B [3] DeepSeek-VL2-small [70] Molmo-7B-D [20] Property R 100.0 59.5 23.5 27.9 67.5 44.3 49.1 52.3 82.7 37.2 28.8 39.0 44.4 47.8 78.0 71.3 78.0 86.4 DF1 24.2 20.9 27.1 30.4 38.6 51.8 54.4 57.7 76.3 Position R 100.0 78.8 23.0 30.2 63.2 48.0 50.2 56.4 78.0 28.5 28.1 28.0 36.2 43.1 66.7 61.7 66.1 80.6 DF1 15.9 17.6 24.3 29.8 37.2 52.5 52.8 58.1 72.4 Interaction R 100.0 67.3 27.8 30.8 66.6 49.6 48.2 55.4 69.9 32.5 28.5 40.1 41.8 48.1 74.8 66.3 75.7 77.7 DF1 19.4 18.9 31.3 31.2 40.6 56.5 53.2 60.7 66.1 Reasoning R 100.0 76.2 17.5 19.7 59.1 36.6 34.6 46.6 72.1 42.6 32.1 22.8 33.7 41.4 65.1 61.2 61.7 80.4 DF1 30.3 22.2 18.9 22.8 34.8 42.8 40.3 50.1 65.5 Celebrity R 100.0 94.1 57.4 63.2 73.2 73.7 80.3 85.9 85.9 14.4 48.0 59.3 60.0 63.3 76.5 81.9 74.3 87.5 DF1 4.9 37.0 58.0 57.5 59.1 74.2 80.1 70.7 82.9 Average R 100.0 75.2 29.8 34.4 65.9 50.4 52.5 59.3 77.7 31.0 33.1 37.8 43.2 48.7 72.2 68.5 71.2 82.5 DF1 18.9 23.3 31.9 34.3 42.1 55.6 56.2 59.5 72.6 RexSeek-7B 87.2 86.8 81.5 86.1 86.3 83. 84.8 84.6 80.7 87.8 84.7 81. 83.4 86.5 84.2 85.9 85.8 82. Rejection Score 0.0 36.0 54.9 2.0 0.0 0.0 7.1 3.1 68.6 54.1 Table 4. Benchmarking multimodal models on HumanRef Benchmark. R, P, and DF1 represent Recall, Precision, and DensityF1, respectively. simple baseline that uses the bounding boxes of all persons in the image as results, simulating person detection model that does not follow the referring description. Molmo-7B-D predicts point coordinates as output and use point-in-mask evaluation criteria. Stage Trainable Modules Task # Samples Datasets Stage1 MLPs Image Captioning 976K ALLAVA-4V-Caption [6] Stage2 Stage3 MLPs + LLM + Vision Encoders MLPs + LLM + Vision Encoders Grounding & Region Understanding General Knowledge & Grounding & Region Understanding 2.07M 2.15M COCO [37], LVIS [22], O365 [62], Rexverse-2M [25] LLAVA-665K [38] Rexverse-2M [25] Stage MLPs + LLM + Vision Encoders Referring 103K HumanRef Table 5. Data, task, and trainable modules for each stage. 4.3. Four Stage Training Similar to other VLMs, we adopt pretraining followed by supervised fine-tuning approach [39]. Our training process consists of four stages. In the first stage, we align the visual and textual modalities using image-captioning data. In the second stage, we focus on perception training with detection-oriented data, enabling the model to retrieve relevant objects from input bounding boxes. In the third stage, we incorporate multimodal data to enhance the models general understanding abilities. Finally, in the fourth stage, we fine-tune the model using the HumanRef dataset, resulting in the final RexSeek model. The data, task, and trainable modules for each stage are shown in Table 5. 5. Experiments In this section, we first introduce the evaluation metrics used in our study and assess the performance of multimodal models on HumanRef. We perform comprehensive analysis to explore the challenges faced by existing models in handling the referring task. Additionally, we perform ablation experiments on RexSeek for model design choices. 5.1. Metrics We evaluate the referring task using Precision, Recall, and DensityF1 Score. Given referring expression, the model predicts one or more bounding boxes, and prediction is considered correct if its IoU with any ground truth box exceeds predefined threshold. Following the evaluation protocol in COCO [37], we report the average performance across IoU thresholds from 0.5 to 0.95 in increments of 0.05. For models that only output points, such as Molmo [20], prediction is considered correct if the predicted point falls within the mask of the corresponding instance. However, this evaluation is less strict than the IoUbased metric, as point-in-mask criteria impose looser spatial constraints, making direct comparisons less fair. For the rejection subset, we calculate the number of referring expressions that the model does not predict any boxes and divide it by the number of total expressions. To penalize models that indiscriminately detect all persons in an image to achieve high F1 score through high recall, we introduce the DensityF1 Score, which modifies the standard F1 Score with density-aware penalty: DensityF1 ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 2 Precisioni Recalli Precisioni + Recalli Di (1) where Di is the density penalty factor, defined as: Di = min(1.0,"
        },
        {
            "title": "GT Counti\nPredicted Counti",
            "content": ") (2) Here, GT Count is the total number of persons in an image, and Predicted Count is the number of predicted boxes for given referring expression. This penalty discourages over-detection by reducing the score when the predicted box count significantly exceeds the ground truth count. 5.2. Benchmarking on HumanRef In Table 4, we evaluate the performance of various multimodal models on the HumanRef benchmark. While these models perform well on the widely used RefCOCO, RefCOCO+, and RefCOCOg benchmarks, their performance significantly degrades on HumanRef. Our analysis reveals two common issues among these models: Low Recall for Multi Instance: We observe common issue among most models: when referring expression corresponds to multiple instances, recall drops significantly, as shown in Figure 7. This suggests that when multiple objects need to be detected, most models tend to predict only 7 Loading Stage stage1 stage2 stage3 HumanRef Average DF1 73.9 77.0 77. 73.5 77.3 78.0 68.2 72.2 73.0 Table 7. Ablation experiments on multi-stage training by loading models from different training stages and fine-tuning them on the HumanRef dataset. We Qwen2.5-3B as the base LLM. Figure 7. Visualizing the trend of recall and precision variations across different models as the number of instances corresponding to each referring expression increases. Model With Rejection Data Rejction Score RexSeek-7B RexSeek-7B No Yes 0 541 Method Shikra-7B [9] InternVL2-8B [14] Grounding DINO-L [42] Qwen2.5-VL-7B [3] MM1.5-7B [82] ChatRex-7B [25] RexSeek-7B RefCOCOg test val 82.2 82.3 82.7 82.7 87.0 86.1 87.2 87.2 87.1 - 88.6 88.8 84.4 84. Table 6. Rejection score comparison under different model scales with and without rejection data during training. Table 8. Zero-shot evaluation of RexSeek on RefCOCO/+/g. We use the open-set detector DINOX to detect the subject object in the image and use the detected bounding box as input to RexSeek. few bounding boxes, limiting their applicability in realworld scenarios. key factor contributing to this behavior is the nature of the training data. Most multimodal models are trained on RefCOCO, RefCOCO+, and RefCOCOg, where referring expressions rarely correspond to multiple instances. As result, these models become biased toward single-instance predictions. In contrast, RexSeek has been trained on datasets that explicitly include multi-instance referring expressions, demonstrate significantly improved ability to handle these real-world cases. Hallucination Issue: On the rejection subset, we observe that most models perform poorly with low rejection score. This indicates that regardless of whether the referred object is actually present in the image, these models tend to predict bounding box, exhibiting severe hallucination issue. In real-world referring applications, such as referring in video streams, it is crucial for models to accurately determine whether the specified object exists in the image. Additionally, we find that the rejection capability can be significantly improved by incorporating appropriate training data. As shown in Table 6, when trained without the rejection data in HumanRef, RexSeek also demonstrates strong hallucination tendencies. This highlights the critical role of dataset design in the referring task, as inadequate dataset construction can lead to overconfident predictions. 5.3. Ablations on RexSeek Ablation of Multi-stage Training: We analyzed the impact of the four-stage training approach used in RexSeek. As shown in Table 7, we conducted supervised fine-tuning on the HumanRef dataset after each training stage. The results demonstrate that the model achieves its best performance after undergoing SFT with general multimodal data Figure 8. RexSeek can refer to arbitrary objects beyond person. (LLaVA-665K [38]). We attribute this improvement to the model acquiring richer general knowledge from multimodal data, which enhances its ability to accurately refer to persons in complex scenarios. Generalization to Any Object Referring: Although RexSeek is trained exclusively on human-related referring data, we find that it also demonstrates the ability to refer to arbitrary objects. We first evaluate the performance of RexSeek on RefCOCOg. Given referring expressions, we apply DINO-X to detect the main object in the image, using the detected bounding box as input to RexSeek. As shown in Table 8, RexSeek achieves competitive performance on RefCOCO/+/g, despite not being explicitly trained on general object referring. Additionally, Figure 8 presents visualizations illustrating that RexSeek can also detect multiple instances even for non-human objects. We attribute this generalization ability to our multi-stage training approach, where perception and multimodal understanding training develop object comprehension, and fine-tuning on HumanRef effectively extends it to arbitrary objects. 8 6. Conclusion they fail In this work, we identify the fundamental limitations of existing referring datasets and models, demonstrating that to meet real-world application demands, particularly in multi-instance referring. To address this, we introduce HumanRef, large-scale benchmark reflecting real-world complexity, and propose RexSeek, retrievalbased detection MLLM integrating person detection with language model. Our multi-stage training approach equips RexSeek with strong generalization capabilities, allowing it to excel in human-centric referring while extending effectively to arbitrary object referring. Extensive evaluations highlight the struggles of state-of-the-art models with multiinstance detection and hallucination, underscoring the importance of dataset design and training strategies for more reliable and generalizable referring expression models."
        },
        {
            "title": "References",
            "content": "[1] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. 3 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 35: 2371623736, 2022. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 3, 4, 7, 8 [4] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael Black. Keep it smpl: Automatic estimation of 3d human pose and shape from single image. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 561578. Springer, 2016. 2 [5] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: https : / / github . com / Image-text pair dataset. kakaobrain/coyo-dataset, 2022. 4 [6] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model. arXiv preprint arXiv:2402.11684, 2024. [7] Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. Touchdown: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1253812547, 2019. 3 [8] Jierun Chen, Fangyun Wei, Jinjing Zhao, Sizhe Song, Bohuai Wu, Zhuoxuan Peng, S-H Gary Chan, and Hongyang Zhang. Revisiting referring expression comprehension evaluation in the era of large multimodal models. arXiv preprint arXiv:2406.16866, 2024. 3 [9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Shikra: Unleashing multiarXiv preprint Feng Zhu, and Rui Zhao. modal llms referential dialogue magic. arXiv:2306.15195, 2023. 3, 8 [10] Ling-Hao Chen, Jiawei Zhang, Yewen Li, Yiren Pang, Xiaobo Xia, and Tongliang Liu. Humanmac: Masked motion completion for human motion prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 95449555, 2023. 2 [11] Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang. Beyond triplet loss: deep quadruplet network for In Proceedings of the IEEE conperson re-identification. ference on computer vision and pattern recognition, pages 403412, 2017. [12] Weihua Chen, Xianzhe Xu, Jian Jia, Hao Luo, Yaohua Wang, Fan Wang, Rong Jin, and Xiuyu Sun. Beyond appearance: semantic controllable self-supervised learning frameIn Proceedings of work for human-centric visual tasks. the IEEE/CVF conference on computer vision and pattern recognition, pages 1505015061, 2023. 2 [13] Zhenfang Chen, Peng Wang, Lin Ma, Kwan-Yee Wong, and Qi Wu. Cops-ref: new dataset and task on compositional referring expression comprehension. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1008610095, 2020. 3 [14] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 2, 3, 7, 8 [15] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time In Proceedings of the open-vocabulary object detection. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. 3 [16] Yuanzheng Ci, Yizhou Wang, Meilin Chen, Shixiang Tang, Lei Bai, Feng Zhu, Rui Zhao, Fengwei Yu, Donglian Qi, and Wanli Ouyang. Unihcp: unified model for human-centric In Proceedings of the IEEE/CVF conference perceptions. on computer vision and pattern recognition, pages 17840 17852, 2023. 2 [17] Volkan Cirik, Taylor Berg-Kirkpatrick, and Louis-Philippe Morency. Refer360: referring expression recognition In Proceedings of the 58th Annual dataset in 360 images. Meeting of the Association for Computational Linguistics, pages 71897202, 2020. [18] Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja, Devi Parikh, and Ajay Divakaran. Align2ground: Weakly supervised phrase grounding guided by image-caption alignment. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26012610, 2019. 3 [19] Harm De Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron Courville. Guess9 what?! visual object discovery through multi-modal dialogue. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 55035512, 2017. 3 [20] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 3, 5, 7 [21] Riza Alp Guler and Iasonas Kokkinos. Holopose: Holistic 3d human reconstruction in-the-wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1088410894, 2019. 2 [22] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. [23] Tanmay Gupta, Arash Vahdat, Gal Chechik, Xiaodong Yang, Jan Kautz, and Derek Hoiem. Contrastive learning for weakly supervised phrase grounding. In European Conference on Computer Vision, pages 752768. Springer, 2020. 3 [24] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. 6 [25] Qing Jiang, Yuqin Yang, Yuda Xiong, Yihao Chen, Zhaoyang Zeng, Tianhe Ren, Lei Zhang, et al. Chatrex: Taming multimodal llm for joint perception and understanding. arXiv preprint arXiv:2411.18363, 2024. 2, 3, 6, 7, 8 [26] Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, and Lei Zhang. T-rex2: Towards generic object detection via text-visual prompt synergy. In European Conference on Computer Vision, pages 3857. Springer, 2025. 3, 6 [27] Xuan Ju, Ailing Zeng, Jianan Wang, Qiang Xu, and Lei Zhang. Human-art: versatile human-centric dataset In Proceedings of bridging natural and artificial scenes. the IEEE/CVF conference on computer vision and pattern recognition, pages 618629, 2023. 2 [28] Xuan Ju, Ailing Zeng, Chenchen Zhao, Jianan Wang, Lei Zhang, and Qiang Xu. Humansd: native skeleton-guided diffusion model for human image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1598815998, 2023. 2 [29] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 2, [30] Rawal Khirodkar, Timur Bagautdinov, Julieta Martinez, Su Zhaoen, Austin James, Peter Selednik, Stuart Anderson, and Shunsuke Saito. Sapiens: Foundation for human vision modIn European Conference on Computer Vision, pages els. 206228. Springer, 2024. 2 [31] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123:3273, 2017. 3 [32] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-ofexperts model. arXiv preprint arXiv:2410.05993, 2024. 3 [33] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 3 [34] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded the language-image pre-training. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1096510975, 2022. 3 In Proceedings of [35] Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, et al. Eagle 2: Building posttraining data strategies from scratch for frontier visionlanguage models. arXiv preprint arXiv:2501.14818, 2025. 3 [36] Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen Qian, and Bo Li. real-time cross-modality correlation filtering method for referring expression comprehension. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1088010889, 2020. 3 [37] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, pages 740755, 2014. 3, 7 [38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv: 2310.03744, 2023. 7, 8 [39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 7 [40] Runtao Liu, Chenxi Liu, Yutong Bai, and Alan Yuille. Clevr-ref+: Diagnosing visual reasoning with referring expressions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 41854194, 2019. [41] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 6 [42] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. 3, 8 [43] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In CVPR, pages 1197611986, 2022. 6 [44] Zhijian Liu, Ligeng Zhu, Baifeng Shi, Zhuoyang Zhang, Yuming Lou, Shang Yang, Haocheng Xi, Shiyi Cao, Yuxian Gu, Dacheng Li, et al. Nvila: Efficient frontier visual language models. arXiv preprint arXiv:2412.04468, 2024. 3 10 [45] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multiperson linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851866. 2023. [46] Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, and Heung-Yeung Shum. Humantomato: Text-aligned whole-body motion generation. arXiv preprint arXiv:2310.12978, 2023. 2 [47] Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Han-Jia Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. 3 [48] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task collaborative network for joint referring expression comprehension In Proceedings of the IEEE/CVF Conand segmentation. ference on computer vision and pattern recognition, pages 1003410043, 2020. 3 [49] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for grounding multimodal large language models. arXiv preprint arXiv:2404.13013, 2024. 3, 6, 7 [50] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L. Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, pages 1120, 2016. 2, 3, 5 [51] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang. MM1: methods, analysis & insights from multimodal LLM pretraining. arXiv: 2403.09611, 2024. [52] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple open-vocabulary object detection. In European conference on computer vision, pages 728755. Springer, 2022. 3 [53] OpenAI. Gpt-4v(ision) system card. https://cdn. openai.com/papers/GPTV_System_Card.pdf, 2023. 3 [54] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correIn Prospondences for richer image-to-sentence models. ceedings of the IEEE international conference on computer vision, pages 26412649, 2015. 3 [55] Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, and Anton van den Hengel. Reverie: Remote embodied visual referring exIn Proceedings of pression in real indoor environments. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 99829991, 2020. 3 [56] Yanyuan Qiao, Chaorui Deng, and Qi Wu. Referring expression comprehension: survey of methods and datasets. IEEE Transactions on Multimedia, 23:44264440, 2020. 3 [57] Heqian Qiu, Hongliang Li, Taijin Zhao, Lanxiao Wang, Qingbo Wu, and Fanman Meng. Refcrowd: Grounding the target in crowd with referring expressions. In Proceedings of the 30th ACM International Conference on Multimedia, pages 44354444, 2022. [58] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 87488763, 2021. 6 [59] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Sam 2: Rädle, Chloe Rolland, Laura Gustafson, et al. arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 5 [60] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: unified vision model for openworld object detection and understanding. arXiv preprint arXiv:2411.14347, 2024. 3, 4, 5, 6, 7 [61] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance arXiv preprint the\" edge\" of open-set object detection. arXiv:2405.10300, 2024. 3, 6 [62] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 84308439, 2019. 7 [63] Shixiang Tang, Cheng Chen, Qingsong Xie, Meilin Chen, Yizhou Wang, Yuanzheng Ci, Lei Bai, Feng Zhu, Haiyang Yang, Li Yi, et al. Humanbench: Towards general humancentric perception with projector assisted pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2197021982, 2023. 2 [64] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [65] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 16531660, 2014. 2 [66] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3 [67] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan 11 locations at any granularity with large language models. In European Conference on Computer Vision, pages 405422. Springer, 2025. 3 [81] Chao Zhang, Weiming Li, Wanli Ouyang, Qiang Wang, Woo-Shik Kim, and Sunghoon Hong. Referring expression comprehension with semantic visual relationship and word In Proceedings of the 27th ACM International mapping. Conference on Multimedia, pages 12581266, 2019. 3 [82] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al. Mm1. 5: Methods, analysis & insights from multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566, 2024. 3, [83] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferretv2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. 3 [84] Liang Zheng, Hengheng Zhang, Shaoyan Sun, Manmohan Chandraker, Yi Yang, and Qi Tian. Person re-identification in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 13671376, 2017. 2 [85] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and Yebin Liu. Deephuman: 3d human reconstruction from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 77397749, 2019. 2 Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 3 [68] Chenyun Wu, Zhe Lin, Scott Cohen, Trung Bui, and Subhransu Maji. Phrasecut: Language-based image segmenIn Proceedings of the IEEE/CVF Contation in the wild. ference on Computer Vision and Pattern Recognition, pages 1021610225, 2020. 3 [69] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions for openthe vocabulary object detection. IEEE/CVF conference on computer vision and pattern recognition, pages 1525415264, 2023. In Proceedings of [70] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-ofexperts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. 2, 3, 7 [71] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 2, 3, 4, 5, 6 [72] Sibei Yang, Guanbin Li, and Yizhou Yu. Dynamic graph attention for referring expression comprehension. In Proceedings of the IEEE/CVF international conference on computer vision, pages 46444653, 2019. 3 [73] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pretraining for open-world detection. Advances in Neural Information Processing Systems, 35:91259138, 2022. 3 [74] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. 3, [75] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions. In ECCV, pages 6985, 2016. 2, 3, 5 [76] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 13071315, 2018. 3 [77] Zhihan Yu and Ruifan Li. Revisiting counterfactual problems in referring expression comprehension. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1343813448, 2024. 3 [78] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and ShihFu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1439314402, 2021. 3 [79] Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon v2: Advancing multimodal perception with high-resolution scaling and visual-language co-referring. arXiv preprint arXiv:2403.09333, 2024. 3 [80] Yufei Zhan, Yousong Zhu, Zhiyang Chen, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon: Spelling out all object"
        },
        {
            "title": "Supplementary Material",
            "content": "7. More Details of HumanRef Dataset 7.1. Detailed Definition for Each Subset Attribute Subset: The attribute subset in HumanRef encompasses diverse range of descriptive properties used for referring to individuals. These attributes are systematically categorized into finite-value attributes, which have predefined set of possible values, and open-ended attributes, which allow for more flexible and detailed descriptions. As illustrated in Fig. 9, we provide structured breakdown of these attribute categories along with representative examples. This visualization highlights the key properties that can be used to distinguish individuals, including intrinsic characteristics (e.g., gender, age, ethnicity), appearance features (e.g., hairstyle, facial expressions), clothing and accessories, poses, and actions. By incorporating both structured and descriptive attributes, HumanRef ensures comprehensive and versatile annotation framework that better aligns with real-world referring scenarios. Position Subset: The position subset in HumanRef captures the spatial relationships of individuals within an image. We categorize these into two main types: inner position and outer position, which represent different spatial referencing strategies. Inner position describes the relative spatial relationships between individuals, while outer position refers to absolute spatial relationships using environmental landmarks. We show some detailed examples in Table. 9. Inner Position (Relative to Others) Outer Position (Relative to Environment) The leftmost person in the image. The second person from the left. The person at the top of the group. The person standing in the corner of the room. The person sitting on the red chair. The person standing at the left edge of the bridge. The person leaning against the wall next to tree. The person standing near the window of the room. Table 9. Examples of Inner and Outer Position References. Interaction Subset: The interaction subset in HumanRef captures the ways individuals interact with other people and objects within scene. We classify interactions into two categories: inner interaction and outer interaction. Inner interaction focuses on actions between individuals, while outer interaction describes interactions between person and objects or the surrounding environment. We show some detailed examples in Table. 10. Inner Interaction (HumanHuman) Outer Interaction (HumanObject/Environment) Two players making physical contact. The bride and groom walking hand in hand in the middle. The person raising another persons hand with their right hand. Two people embracing each other. The person holding dog on leash. The person reaching out to grab the football. The person using their right hand to pull toilet paper. The person holding transparent box containing roll of toilet paper with their left hand. The person holding pizza with one hand. The person holding pen in their hand. Table 10. Examples of Inner and Outer Interaction References. Reasoning Subset: The reasoning subset in HumanRef requires models to perform multi-step inference by first identifying reference person or object before determining the target individual. It is divided into three categories: inner position reasoning, outer position reasoning, and attribute reasoning. Inner position reasoning describes one individual using another as an anchor point, establishing relative spatial relationship between them. This approach ensures that the model must first locate reference person before resolving the intended target. Outer position reasoning follows similar principle but instead uses an absolute spatial reference tied to the surrounding environment, requiring the model to integrate positional understanding of both individuals and external scene elements. Attribute reasoning involves logical filtering within group of candidates who share common attribute, followed by an exclusion step based on an additional attribute with negation rule. This forces the model to refine its selection beyond direct attribute matching, ensuring more precise understanding of distinguishing characteristics. These three types of reasoning introduce hierarchical complexity into the referring task, strengthening the models ability to process contextual, spatial, and attributebased relationships in structured manner. We show some 1 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { } \"Gender\": [\"Male\", \"Female\"], \"Age\": [\"Infant\", \"Child\", \"Adolescent\", \"Adult\", \"Elderly\"], \"Ethnicity\": [\"White\", \"African\", \"Asian\", \"...\"], \"Profession\": [\"Doctor\", \"Engineer\", \"Teacher\", \"...\"], \"Appearance\": { \"Hair\": { \"Type\": [\"Short\", \"Long\", \"Curly\", \"Bald\", \"...\"], \"Color\": [\"Black\", \"Brown\", \"Blonde\", \"Red\", \"...\"] }, \"Beard\": { \"Type\": [\"Full Beard\", \"Goatee\", \"Mustache\", \"...\"], \"Color\": [\"Black\", \"White\", \"Brown\", \"...\"] \"Detailed\": [\"long and black beard\", \"...\"] }, \"Facial Expression\": { \"Type\": [\"Smiling\", \"Frowning\", \"Surprised\", \"...\"], \"Detailed\": [\"Smiling with closed eyes\", \"...\"] } }, \"Clothing & Accessories\": { \"Upper Body\": {\"Type\": [\"T-shirt\", \"Shirt\", \"...\"], \"Color\": [\"Red\", \"Blue\", \"...\"]}, \"Lower Body\": {\"Type\": [\"Jeans\", \"Skirt\", \"...\"], \"Color\": [\"Black\", \"White\", \"...\"]}, \"Shoes\": {\"Type\": [\"Sneakers\", \"Boots\", \"...\"], \"Color\": [\"Red\", \"Blue\", \"...\"]}, \"Hat\": {\"Type\": [\"Baseball Cap\", \"Knit Cap\", \"...\"], \"Color\": [\"Black\", \"White\", \"...\"]} }, \"Pose\": {\"Type\": [\"Standing\", \"Sitting\", \"...\"], \"Details\": [\"Sitting on bench\", \"...\"]}, \"Actions\": {\"Type\": [\"Walking\", \"Running\", \"...\"], \"Details\": [\"Walking dog\", \"...\"]} Figure 9. Attribute taxonomy in HumanRef. Finite-value attributes have predefined values, while open-ended attributes allow flexible descriptions. 8. Details for RexSeek model 8.1. Model Architecture We utilize dual-encoder design for vision processing. The low-resolution visual encoder is based on the CLIP ViTLarge-14-336 model, while the high-resolution visual encoder leverages the LAION ConvNeXt-Large-320 model. The input resolution is set to 336336 for the low-resolution encoder and 768768 for the high-resolution encoder, allowing the model to capture both coarse and fine-grained visual details effectively. 8.2. Training Details During the pretraining stage (Stage-1), we use batch size of 32 per device, resulting in total batch size of 256 across all devices. The instruction-tuning stage (Stage-2, Stage3, Stage-4) employs reduced batch size of 16 per device, with total batch size of 128. The learning rate is initialized at 1e-3 for pretraining and adjusted to 2e-5 during instruction tuning to ensure stable fine-tuning and convergence. detailed examples in Table. 11. Celebrity Subset: The celebrity subset in HumanRef focuses on identifying well-known individuals based on their names or recognizable personas. To provide structured classification, we divide celebrities into six categories: Character, Singer, Actor, Athlete, Entrepreneur, and Politician. This categorization ensures that the dataset covers diverse range of public figures from different domains, each of whom may be referred to by their real name or an associated identity. In Tab. 12, we provide detailed list of representative individuals from each category, illustrating the range of celebrities included in the dataset. 7.2. Details for Structured Property Dictionary To facilitate the annotation process, we employ Qwen2.5VL-7B to generate predefined structured property dictionary for each person in the image. This dictionary serves as reference for annotators, allowing them to construct property lists based on pre-generated attribute descriptions. Figure 10 presents the prompt used to generate these structured descriptions. 2 Reasoning Type Example Expressions Inner Reasoning Position Outer Reasoning Position Attribute Reasoning blue-and-white The person to the left of the child wearing striped shirt. The person to the right of the man in suit. The girl to the right of the person wearing blue headphones. All groom. individuals to the right of the The closest person to the left of the person in the corridor. The person to the left of the individual directly below the letter D. The child to the right of the girl standing under the blue arched wooden door. The person to the right of the individual sitting inside the shopping cart. The person wearing hat but not sitting. The person eating ice cream but not wearing red top. The person wearing sandals but not sitting on the bed. The person on the airplane but not showing their head. Table 11. Examples of Reasoning-Based Referring Expressions. 3 Character Singer Actor Athelete Eleven from the Strange Things, Obi-Wan Kenobi, Captain America, Queen Maeve, Buzz Lightyear, Mary Poppins, John Rambo, Hermione Granger, Rick Grimes, Ferris Bueller, Sheldon Cooper, Sarah Connor, Negan Smith, Amy Farrah Fowler, Professor Severus Snape, Shrek, Ada Shelby, Palpatine, Jorah Mormont, Thomas Shelby, Cersei Lannister, Luke Skywalker, Maximus Decimus Meridius, Sansa Stark, John Wick, Atticus Finch, Harry Potter, Wolverine, Mr. Bean, Ronald Weasley, Melisandre, Ross Geller, Bilbo Baggins, Samwise Gamgee, Maggie Greene, Jon Snow, Wednesday Addams, Hans Gruber, Rocky Balboa, Michael Corleone, Leonard Hofstadter, Chandler Bing, Littlefinger, Barbossa, Rachel Green, Howard Wolowitz, Jaime Lannister, Legolas, Axel Foley, James T. Kirk, Kevin McCallister, Margaery Tyrell, Leia Organa, Forrest Gump, Marty McFly, Han Solo, Billy Butcher, Mr. Kesuke Miyagi, Professor Albus Dumbledore, Steve Harrington, Sirius Black, Gus Fring, Jack Sparrow, Inigo Montoya, Professor Minerva McGonagall, Homelander, Arthur Shelby, Superman, Indiana Jones, Beetlejuice, Polly Gray, Hughie Campbell, Peter Venkman, Sandor Clegane, Soldier Boy, Joey Tribbiani, Rajesh Koothrappali, Gollum, Vito Corleone, Daryl Dixon, Aragorn, Andy Dufresne, Jesse Pinkman, Penny Hofstadter, Jean-Luc Picard, Lord Voldemort, Oberyn Martell, Luna Lovegood, Carol Peletier, Monica Geller, Alfred Pennyworth, Bernadette Rostenkowski-Wolowitz, Darth Maul, Thor, Neo, John McClane, Phoebe Buffay, Spock, Dorothy Gale Michael Bublé, Demi Lovato, Aerosmith, Drake, ZAYN, Jennifer Lopez, Olivia Rodrigo, Kali Uchis, Flo Rida, Doja Cat, Skrillex, Chris Brown, Ellie Goulding, 50 Cent, Katy Perry, Gucci Mane, Charli XCX, Avril Lavigne, Shawn Mendes, Lil Wayne, J. Cole, Linkin Park, Migos, Taylor Swift, DJ Khaled, Red Hot Chili Peppers, Justin Bieber, Calvin Harris, 2 Chainz, Frank Ocean, Jason Mraz, Alicia Keys, Miley Cyrus, Childish Gambino, Meghan Trainor, Tyga, Usher, SZA, Bad Bunny, Labrinth, Diplo, Jack Johnson, Halsey, Young Thug, Bon Jovi, Post Malone, Christina Aguilera, The Kooks, John Mayer, The 1975, Akon, G-Eazy, Panic! At the Disco, Eminem, Ed Sheeran, Maroon 5, Ne-Yo, Zedd, Dr. Dre, Queen, Nelly Furtado, Steve Lacy, Imagine Dragons, Sia, Mac DeMarco, Big Sean, Martin Garrix, Camila Cabello, The Rolling Stones, Khalid, Harry Styles, Charlie Puth, Kanye West, The Weeknd, Kendrick Lamar, Travis Scott, Kesha, Nelly, Tyler, The Creator, Billie Eilish, Metro Boomin, Gwen Stefani, Sean Paul, Vampire Weekend, Jay-Z, Kelly Clarkson, Stevie Wonder, Adele, Arctic Monkeys, Lorde, Britney Spears, Selena Gomez, Daddy Yankee, 21 Savage, David Guetta, Balvin, The Cure, Bruno Mars, Dua Lipa, Bruce Springsteen, Snoop Dogg, B.o.B, OutKast, Lady Gaga, Hozier, Wiz Khalifa, Foo Fighters, Lana Del Rey, Beyoncé, Madonna, Shakira, John Legend, Mark Ronson, Sam Smith, Billy Joel, Jeremih, Paramore, Chance the Rapper, DJ Snake, Sabrina Carpenter, Kid Cudi, Trey Songz, Kings of Leon, Enrique Iglesias, Pharrell Williams, Arcade Fire, Jessie J, Lil Uzi Vert, Bob Dylan Tom Wilkinson, Al Pacino, Kevin Costner, Franco Nero, Philip Seymour Hoffman, Alan Rickman, Leonardo DiCaprio, Ben Affleck, William Hurt, Mark Wahlberg, Jonah Hill, Shia LaBeouf, Don Cheadle, Orlando Bloom, Jeff Goldblum, Denzel Washington, Alec Baldwin, Bradley Cooper, Ed Harris, Jason Clarke, Mahershala Ali, Viggo Mortensen, Owen Wilson, Alan Arkin, James Caan, Nicolas Cage, Samuel L, David Strathairn, Matt Damon, George Clooney, Giovanni Ribisi, Jared Leto, Kevin Spacey, Matthew McConaughey, Gary Sinise, Pete Postlethwaite, Keanu Reeves, Timothy Spall, Harry Dean Stanton, John Carroll Lynch, Chiwetel Ejiofor, Woody Harrelson, Ryan Gosling, Joaquin Phoenix, Donald Sutherland, Paul Dano, Chris Hemsworth, David Oyelowo, Tom Hardy, Barry Pepper, Kurt Russell, Christian Bale, Jeff Daniels, Ben Whishaw, Sterling Hayden, Edward Norton, Sam Shepard, Andy Garcia, Harvey Keitel, Benicio Del Toro, Gene Hackman, Bruce Willis, Guy Pearce, Jonathan Pryce, Michael Fassbender, James Stewart, Zach Galifianakis, Forest Whitaker, Vincent Cassel, Michael Sheen, Tom Berenger, Jim Carrey, Steve Buscemi, Joe Pesci, Christian Berkel, Rutger Hauer, Mel Gibson, Elliott Gould, Tim Robbins, Daniel Craig, Jeffrey Wright, Matthew Modine, Domhnall Gleeson, Brendan Gleeson, John Hurt, Michael Stuhlbarg, Hugo Weaving, John Goodman, Mark Hamill, Colin Farrell, Ken Watanabe, Clint Eastwood, Ralph Fiennes, Val Kilmer, John Hawkes, Ben Kingsley, Seth Rogen, Robert Duvall, Brad Pitt, Max von Sydow, Stanley Tucci, Tom Cruise, Christopher Lloyd, Tommy Lee Jones, Jason Statham, Michael Caine, Paul Giamatti, Josh Hutcherson, Adrien Brody, Michael J, Jeremy Renner, Liam Neeson, Mark Ruffalo, Terrence Howard, John Cleese, Harrison Ford, Clive Owen, Jake Gyllenhaal, Will Smith, Danny DeVito, Elijah Wood, Sean Connery, Tom Sizemore, Stellan Skarsgård, Robin Williams, Hugh Jackman, John Lithgow, Benedict Cumberbatch, Mykelti Williamson, John Malkovich, Gary Oldman, Johnny Depp, Jeff Bridges, Hugh Grant, Jean Reno, Aaron Eckhart, Michael Madsen, Jude Law, J.K, Jon Voight, Casey Affleck, Robert Pattinson, Daniel Brühl, Billy Bob Thornton, Russell Crowe, Ewan McGregor, Christopher Walken, Morgan Freeman, Josh Brolin, Richard Harris, Shea Whigham, Bill Murray, Christoph Waltz, Jamie Foxx, Christopher Plummer, Ethan Hawke, Albert Finney, Miles Teller, Don Johnson, Javier Bardem, Bill Paxton, Robert De Niro, Timothée Chalamet, Sam Rockwell, Kevin Bacon, Simon Pegg, Sean Penn, Ving Rhames, Tom Hanks, Anthony Hopkins, Heath Ledger, Tim Roth, Martin Sheen, Michael Keaton, Joseph Gordon-Levitt, Kyle Chandler, John Travolta, Bruce Dern, Steve Carell, Dustin Hoffman, Oscar Isaac Dirk Nowitzki, Mia Hamm, Diana Taurasi, Allyson Felix, Sheryl Swoopes, David Ortiz, James Harden, Mike Trout, Mookie Betts, Chris Paul, Aitana Bonmati, Roger Federer, Faker, Annika Sorenstam, Thierry Henry, Jimmie Johnson, Tom Brady, Randy Moss, Shohei Ohtani, Kevin Durant, Zinedine Zidane, Calvin Johnson, Novak Djokovic, LeBron James, Alexia Putellas, Albert Pujols, Max Scherzer, Mariano Rivera, Ichiro Suzuki, Patrick Mahomes, Aaron Donald, Steve Nash, Georges St-Pierre, Giannis Antetokounmpo, Stephen Curry, Floyd Mayweather, Clayton Kershaw, Manny Pacquiao, Andrés Iniesta, Barry Bonds, Tim Duncan, Lauren Jackson, Luka Modric, Shelly-Ann Fraser Pryce, Bryce Harper, Ray Lewis, Simone Biles, Rafael Nadal, Derek Jeter, Shaun White, Michael Schumacher, Peyton Manning, Candace Parker, Nikola Jokic, Serena Williams, Jason Kidd, Andy Murray, Mikaela Shiffrin, Lewis Hamilton, Lisa Leslie, Bernard Hopkins, Kobe Bryant, Justin Verlander, Tamika Catchings, Alex Rodriguez, Jon Jones, Tiger Woods, Dwyane Wade, Kohei Uchimura, Michael Phelps, Xavi Hernandez, Cristiano Ronaldo, Usain Bolt, Max Verstappen, Kawhi Leonard, Venus Williams, Katie Ledecky, Kylian Mbappé, Maya Moore, Alex Ovechkin, Sidney Crosby, Phil Mickelson, Adrian Beltré, Kevin Garnett, Miguel Cabrera, Lionel Messi Entrepreneur Brian Chesky, Garrett Camp, Kevin Systrom, Sam Walton, Larry Ellison, Ratan Tata, Ritesh Agarwal, Ted Turner, Steve Jobs, Jack Ma, Richard Branson, Jeff Bezos, Che Guevara, Mike Pence, Li Ka-shing, Woodrow Wilson, Dwight D. Eisenhower, Abdel Fattah el-Sisi, Franklin D. Roosevelt, Yasser Arafat, Haruhiko Kuroda, Donald Trump, Bill Clinton, Stephen Schwarzman, Narendra Modi, Gianni Infantino, Masayoshi Son, Bernard Arnault, Hui Ka Yan, Benjamin Netanyahu, Winston Churchill, Vladimir Putin, John F. Kennedy, Theodore Roosevelt, Nelson Mandela, Sergey Brin, Margaret Thatcher, Xi Jinping, Ronald Reagan, Golda Meir, Recep Tayyip Erdogan, Charles de Gaulle, Jim Yong Kim, Warren Buffett, Qamar Javed Bajwa, Jerome H. Powell, Wang Jianlin, Lech Wałesa, Michel Temer, Doug McMillon, Mohammed bin Salman Al Saud, Lloyd Blankfein, Lee Hsien Loong, Jawaharlal Nehru, Shinzo Abe, Michael Bloomberg, Tony Blair, Li Keqiang, Rodrigo Duterte, Justin Trudeau, Hu Jintao, Bob Iger, Mario Draghi, Khalifa bin Zayed Al-Nahyan, Bashar al-Assad, Ayatollah Khomeini, Ali Hoseini-Khamenei, Indira Gandhi, Deng Xiaoping, Kim Jong-un, Ma Huateng, Joseph Stalin, Mikhail Gorbachev, Moon Jae-in, Mary Barra, Mahatma Gandhi, Christine Lagarde, Jokowi Widodo, Mao Zedong, Ken Griffin, Mustafa Kemal Atatürk, Theresa May, Aliko Dangote, Darren Woods, Jiang Zemin, Rupert Murdoch, Fidel Castro, Jean-Claude Juncker, Robert Mueller, Enrique Pena Nieto, Carlos Slim Helu, Tim Cook, Robin Li, Antonio Guterres, Larry Fink lbert Einstein, Fei-Fei Li, Jennifer Doudna, Yann LeCun, Thomas Edison, Gregor Mendel, Geoffrey Hinton, John von Neumann, Max Planck, Sam Altman, Tim Berners-Lee, Andrew Ng, Rosalind Franklin, Andre Geim, Marie Curie, Ilya Sutskever, Kip Thorne, James Watson, Srinivasa Ramanujan, Nikola Tesla, Niels Bohr, Enrico Fermi, Rachel Carson, Yoshua Bengio, Alan Turing, Stephen Hawking, Francis Crick, Linus Pauling, Demis Hassabis, Werner Heisenberg, Barbara McClintock Politician Scientist Table 12. Names for each sub-domain of the celebrity recognition subset. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 { \"Instruction\": \"I will provide you with an image of person. Please list as many detailed attributes as possible based on the following categories. Below are some examples you can refer to:\", \"Gender\": [\"Male\", \"Female\", \"Unknown\"], \"Age\": [\"Infant\", \"Child\", \"Teenager\", \"Adult\", \"Elderly\", \"Unknown\"], \"Ethnicity\": [\"Caucasian\", \"African\", \"Asian\", \"...\", \"Unknown\"], \"Occupation\": [\"Doctor\", \"Engineer\", \"Teacher\", \"...\", \"Unknown\"], \"Appearance\": { \"Hair\": { \"Type\": [\"Short\", \"Long\", \"Curly\", \"Straight\", \"Bald\", \"Ponytail\", \"Unknown\"], \"Color\": [\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Black\", \"White\", \"...\", \"Unknown\"], \"Description\": [\"Short black hair with dotted pattern\", \"...\", \"Unknown\"] }, \"Beard\": { \"Type\": [\"Full Beard\", \"Goatee\", \"Mustache\", \"Unknown\"], \"Color\": [\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Black\", \"White\", \"...\", \"Unknown\"], \"Description\": [\"Black full beard\", \"...\", \"Unknown\"] }, \"Expression\": { \"Type\": [\"Smiling\", \"Frowning\", \"Surprised\", \"Angry\", \"Sleeping\", \"Crying\", \"Laughing\", \" ...\", \"Unknown\"], \"Description\": [\"Smiling with closed eyes\", \"...\", \"Unknown\"] } }, \"Clothing & Accessories\": { \"Clothing\": { \"Upper\": { \"Type\": [\"T-shirt\", \"Shirt\", \"Blouse\", \"Unknown\"], \"Color\": [\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Black\", \"White\", \"...\", \"Unknown\"], \"Description\": [\"Purple hooded puffer jacket\", \"...\", \"Unknown\"] }, \"Lower\": { \"Type\": [\"Jeans\", \"Skirt\", \"Shorts\", \"Unknown\"], \"Color\": [\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Black\", \"White\", \"...\", \"Unknown\"], \"Description\": [\"Black pants with dotted patterns\", \"...\", \"Unknown\"] }, \"Shoes\": { \"Type\": [\"Sneakers\", \"Boots\", \"Sandals\", \"Barefoot\", \"Unknown\"], \"Color\": [\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Black\", \"White\", \"...\", \"Unknown\"], \"Description\": [\"White sneakers with red stripes\", \"...\", \"Unknown\"] } }, \"Accessories\": { \"Hat\": { \"Type\": [\"Baseball Cap\", \"Beanie\", \"Fedora\", \"Unknown\"], \"Color\": [\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Black\", \"White\", \"...\", \"Unknown\"], \"Description\": [\"Black beanie with white stripes\", \"...\", \"Unknown\"] }, \"Glasses\": { \"Type\": [\"Sunglasses\", \"Prescription Glasses\", \"Goggles\", \"Unknown\"], \"Color\": [\"Red\", \"Blue\", \"Green\", \"Yellow\", \"Black\", \"White\", \"...\", \"Unknown\"], \"Description\": [\"Black sunglasses with red frame\", \"...\", \"Unknown\"] } } }, \"Posture\": { \"Type\": [\"Standing\", \"Sitting\", \"Lying\", \"Arms Crossed\", \"Arms Raised Overhead\", \"...\", \" Unknown\"], \"Description\": [\"Sitting on bench\", \"Standing on one leg\", \"...\", \"Unknown\"] }, \"Actions\": { \"Type\": [\"Walking\", \"Running\", \"Jumping\", \"Sitting\", \"Standing\", \"Sleeping\", \"...\", \"Unknown\"], \"Description\": [\"Walking dog\", \"Reading book with red cover\", \"...\", \"Unknown\"] } } Figure 10. Prompt used to generate structured property dictionary."
        }
    ],
    "affiliations": [
        "International Digital Economy Academy (IDEA)",
        "South China University of Technology"
    ]
}