{
    "paper_title": "AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models",
    "authors": [
        "Xinghui Li",
        "Qichao Sun",
        "Pengze Zhang",
        "Fulong Ye",
        "Zhichao Liao",
        "Wanquan Feng",
        "Songtao Zhao",
        "Qian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarios. In this paper, we focus on a new task, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Feature Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and a novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce a Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as a plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results."
        },
        {
            "title": "Start",
            "content": "AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models Xinghui Li1 Qichao Sun1 Pengze Zhang1 Fulong Ye1 Zhichao Liao2 Wanquan Feng1 1ByteDance Songtao Zhao1 Qian He1 2Tsinghua University https://crayon-shinchan.github.io/AnyDressing/ 4 2 0 2 5 ] . [ 1 6 4 1 4 0 . 2 1 4 2 : r Figure 1. Customizable virtual dressing results of our AnyDressing. Reliability: AnyDressing is well-suited for variety of scenes and complex garments. Compatibility: AnyDressing is compatible with LoRA [15] and plugins such as ControlNet [55] and FaceID [54]."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarCorresponding author. ios. In this paper, we focus on new task, i.e., MultiGarment Virtual Dressing, and we propose novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Fea1 ture Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results. 1. Introduction In recent years, the field of image generation has experienced transformative advancements [3, 9, 22], particularly with methods based on Latent Diffusion Models (LDMs) achieving remarkable success in text-to-image generation tasks [14, 37, 38, 42, 44]. Considering only textual information is inadequate for image customization, numerous approaches incorporate reference images with textual descriptions for image generation [25, 39, 49]. Specially, the Virtual Dressing (VD) task of generating garment-centric human images based on the reference garments has sparked considerable research interest [4, 40, 47], due to its significant potential for practical applications in e-commerce and creative design. VD is used to be regarded as subtask of traditional subject-driven image customization, prior approaches [10, 18, 25, 30, 39, 41, 45, 57] simply integrate the features of reference image into the text embeddings without fully exploiting the information from the reference image. Several subsequent works [32, 54] more comprehensively utilize the features of the reference image by training additional cross-attention layers to integrate reference image features into the diffusion model. However, these methods struggle to preserve the intricate textures of the garment. Recently, some methods [4, 40, 47] focus on garment-centric image generation. Most of them leverage full copy of diffusion U-Net as the garment encoder named ReferenceNet to maintain fine-grained garment information. Nevertheless, these methods are tailored exclusively to single items of clothing and lack support for multiple conditions, thus hindering the ability to freely dress in any combination of various garments. In this work, our focus is on new task Multi-Garment Virtual Dressing, personalizing character wearing any combination of target garments according to the customized text prompt or other controls. The task poses several challenges, including: 1) Garment fidelity: preventing confusion among multiple garments while preserving the intricate textures of each; 2) Text-Image consistency: minimizing the influence of multiple garments on irrelevant regions to ensure the faithfulness of the generated images to the text prompts; 3) Plugin compatibility: enabling seamless integration with community control plugins for LDMs. To address the aforementioned issues, we propose AnyDressing, novel approach that customizes characters conditioned on any combination of garments and any personalized text prompts. AnyDressing primarily comprises two primary networks named GarmentsNet and DressingNet. The GarmentsNet leverages core Garment-Specific Feature Extractor (GFE) module to extract multi-garment detailed features, which utilizes parallelized self-attention LoRA layers within shared U-Net architecture to individually encode garment textures. This design not only avoids clothing blending but also ensures network efficiency, allowing for easy scalability to any number of garments. The DressingNet employs Dressing-Attention (DA) mechanism to seamlessly integrate multi-garment features into the denoising process. To ensure that each garment instance focuses specifically on its corresponding region, we further introduce novel Instance-Level Garment Localization (IGL) learning strategy in DA. This avoids influencing other irrelevant regions in the synthetic image, thus improving fidelity to arbitrary customized text prompts. Additionally, to enhance texture details, we design Garment-Enhanced Texture Learning (GTL) strategy that strengthens the supervision of attire by imposing constraints from perceptual features and high-frequency information. Extensive experiments show that AnyDressing has certain advantages in the quantitative and qualitative results compared to state-of-the-art methods. Especially, AnyDressing can serve as plugin compatible with various finetuned LDMs, customized LoRAs [15], and other extensions such as ControlNet [55] and IP-Adapter [54], enhancing the diversity and controllability of synthetic images. In summary, our contributions are as follows: We propose novel GarmentsNet to efficiently capture multi-garment textures in parallel by employing core Garment-Specific Feature Extractor. We design novel DressingNet incorporating DressingAttention mechanism and an Instance-Level Garment Localization Learning strategy to accurately inject multigarment features into their corresponding regions. We introduce Garment-Enhanced Texture Learning strategy to effectively enhance the fine-grained texture details in synthetic images. Our framework can seamlessly integrate with any community control plugins for diffusion models. Both quan2 titative and qualitative experimental results demonstrate the superiority of our AnyDressing. 2. Related Work Latent Diffusion Models. Latent Diffusion Models (LDMs) [38] have become widely used in text-to-image generation tasks. Recent advancements have focused on making generated content more stable and controllable. For instance, ControlNet [55] and T2I Adapter [35] introduced additional conditioning modules injecting control into the denoising U-net via extra branches, such as edges and pose. Additionally, large model fine-tuning methods like LoRA [15] have significantly enhanced LDMs generative capabilities in specific scenarios. In this work, we can integrate with various fine-tuned LDMs and customized LoRAs to enhance the diversity of generated images. Subject-Driven Image Generation. Subject-driven generation aims to produce content that aligns with the visual features of reference image. Methods for this task can be categorized into Tuning-based methods [10, 12, 25, 39] and Tuning-free methods [17, 23, 27, 29, 32, 50, 51, 54, 56]. Tuning-based methods, such as DreamBooth [39] and Custom-Diffusion [25] require optimizing specific text tokens to represent target concepts using limited set of subject images. On the other hand, Tuning-free methods generally encode the reference image into feature embeddings. FastComposer [51] integrates image features into text embeddings, while IP-Adapter[54] and SSR-Encoder[56] integrate image features into the denoising U-net through decoupled cross-attention mechanism. However, these methods struggle to preserve the fine-grained texture. Virtual Try-On. Virtual Try-On (VTON) aims to synthesize an image of specific person wearing desired garment. Early methods [5, 13, 19, 26, 28, 33, 46, 52] utilize generative adversarial networks (GANs) with two-stage strategy, which rely on an explicit warping module and struggle to handle complex backgrounds. Recent studies [6, 11, 24, 34, 53] have used pre-trained LDMs as priors for VTON tasks. LADI-VTON [34] and DCI-VTON [11] explicitly deform the clothes and then use diffusion models to fuse and refine them. Rencent works [6, 24, 53] employ parallel U-Nets for clothing feature extraction and inject features into denoising U-Net. However, VTON is essentially localized image editing task and requires an existing model image, lacking flexibility in application scenarios. Virtual Dressing. Virtual Dressing (VD) [4, 40, 47] aims to generate freely editable human images with reference garments and optional conditions. StableGarment [47] and IMAGDressing [40] leverage garment U-Net for extracting fine-grained clothing features and denoising U-Net with hybrid attention module to incorporate garment features into denoising process. Magic Clothing [4] additionally proposes joint classifier-free guidance to balance the control of garment features and text prompts. However, existing approaches are limited to processing single items of clothing, and difficult to maintain fidelity to text prompts. In contrast, our method allows for freely dressing multiple garments and produces coherent and attractive images following customized text prompts. 3. Preliminaries Stable Diffusion. The Diffusion Model belongs to class of generative models that generate data through iterative denoising from random noise. In this paper, we specifically employ Stable Diffusion [38]. Stable Diffusion is latent diffusion model that operates in the latent space of an autoencoder D(E()), where and represent the encoder and decoder, respectively. For given image x0 with its corresponding latent feature z0 = E(x0), the diffusion forward process is defined as: zt = αtz0 + 1 αtϵ, (1) where αt = (cid:81)t pre-defined variance schedule at timestep s. s=1(1 βs), ϵ (0, 1), and βs is the In the diffusion backward process, U-Net ϵθ is trained to predict the noise. Given the textual condition C, the training objective LLDM is defined as follows: LLDM = Ez0,ϵ,C,tϵ ϵθ(zt, C, t)2. (2) 4. Methodology Given target garments, the proposed AnyDressing aims to generate new image xdr, showcasing customized character dressed in multiple target garments across variety of scenes, styles and actions based on the text prompt. As illustrated in Fig. 2, AnyDressing comprises two primary networks: GarmentsNet and DressingNet. The GarmentsNet leverages the Garment-Specific Feature Extractor module to extract detailed features from multiple garments (Sec. 4.1). Meanwhile, the DressingNet integrates these features for virtual dressing using DressingAttention module and an Instance-Level Garment Localization Learning mechanism (Sec. 4.2). Additionally, Garment-Enhanced Texture Learning strategy is designed to further enhance crucial texture details in the synthesis images (Sec. 4.3). Next, we will introduce the aforementioned modules, along with training and inference processes (Sec. 4.4), in detail. 4.1. GarmentsNet Previous methods [4, 40, 47] leverage full copy of diffusion U-Net [2, 16] as garment encoding network, ensuring precise preservation of clothing details. However, these methods are limited to handling single garment and face 3 Figure 2. Overview of AnyDressing. Given target garments, AnyDressing customizes character dressed in multiple target garments. The GarmentsNet leverages the Garment-Specific Feature Extractor (GFE) module to extract detailed features from multiple garments. The DressingNet integrates these features for virtual dressing using Dressing-Attention (DA) module and an Instance-Level Garment Localization Learning mechanism. Moreover, the Garment-Enhanced Texture Learning (GTL) strategy further enhances texture details. significant garment confusion issues when applied to multigarment virtual dressing, as shown in Fig. 3. straightforward approach to dress multiple garments is to simply duplicate several garment encoding networks to manage different conditions. However, this method would result in substantial increase in the number of parameters, making it computationally impractical. Drawing inspiration from the successful practice of the aforementioned reference mechanisms, we observe that self-attention layers are crucial for the implicit warping of features, enabling the effective matching of input garments to the appropriate body parts. Meanwhile, other layers are typically responsible for general feature extraction and can be shared across different garments without compromising the models performance. Building on this insight, we innovatively design simple yet effective architecture named GarmentsNet, which employs core Garment-Specific Feature Extractor (GFE) module to individually encode features for each garment utilizing LoRA [15] layers within shared U-Net framework. As result, this design significantly avoids garment blending while maintaining network efficiency. As illustrated in Fig. 2, the GFE module employs parallelized self-attention mechanism to extract detailed features of multiple garments. Specifically, for each garment condition Fi, we define the proprietary self-attention LoRA matrix ˆWi as follows: ˆWi = { ˆWi q, ˆWi k, ˆWi v}, (3) q, ˆWi and ˆWi where ˆWi represent LoRA layers for the query, key and value projections of self-attention layers. We then concatenate self-attention results of each garment condition to obtain the aggregated garment features Fnew: Fi new = Sof tmax( Qi(Ki) )Vi, Fnew = Concat(F1 new, F2 new, , FN new), (4) (5) where Qi = Fi( ˆWq + ˆWi Vi = Fi( ˆWv + ˆVi resents the number of reference garments. k), v), only ˆW is trainable and repq), Ki = Fi( ˆWk + ˆWi Thanks to the multi-garment parallel processing design of our GFE module, GarmentsNet can seamlessly scale to any number of garments. Notably, this expansion requires only some newly added ˆW, and significantly minimizes both training and inference time compared with duplicating the entire garment encoding network. 4.2. DressingNet To incorporate multi-garment features during the diffusion process, we meticulously design the DressingNet, which serves as the main denoising net and primarily includes an adaptive Dressing-Attention mechanism and an InstanceLevel Garment Localization Learning strategy. 4.2.1 Adaptive Dressing-Attention In the VD task, the main denoising network is typically kept frozen during training [4, 40] to preserve its original editing and generation capabilities as much as possible. However, this prevents the pre trained attention modules from effectively capturing the relationship between reference garment features and latent features. To address this issue, we design an adaptive Dressing-Attention (DA) mechanism to efficiently integrate multi-garment texture cues into synthetic images, inspired by [54]. As shown in Fig. 2, the Dressing-Attention module includes frozen self-attention module and learnable cross-attention module. Let {F1, F2, , FN } denote garment features output by the GarmentsNet at corresponding positions, we first concatenate these features along the spatial dimension to obtain the final garment features: Fall = Concat(F1, F2, , FN ). We then introduce two trainable linear projection layers to align garment features with latent feature Z. Formally, the output of Dressing-Attention Znew is: and Znew = Sof tmax( )V+λSof tmax( QK Q(K) )V (6) where λ is hyperparameter ensuring the flexibility of incorporating garment features, and = ZWq, = ZWk, = ZWv, = FallW v. Here, Wq, Wk and Wv are frozen self-attention layers. To accelerate the coverage, we initialize the k, = FallW with Wk, Wv. k, 4.2.2 Instance-Level Garment Localization Learning Although the above Dressing-Attention (DA) mechanism facilitates the integration of multi-garment features, it may result in text-image inconsistency. We argue that this results from the garments attention map covering the entire image in the DA module, thereby injecting garment cues into the other irrelevant regions incorrectly. To tackle this issue, we introduce an Instance-Level Garment Localization (IGL) learning strategy, ensuring that each garment instance focuses solely on its corresponding region. Specifically, for each garment feature, we obtain its attention map with the latent noise in each layer of the DA module: = Sof tmax(Q(K)/ d), = (cid:88) j=1 Pj, (7) (8) where denotes the length of corresponding garment features. Then, regularization term Lloc is applied to explicitly learn attention localization for each garment instance: 5 Lloc ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) k=1 Ak Mk2, (9) where is the number of garments in the reference image, and Mk represents the reference garments segmentation mask. It is worth noting that the proposed IGL learning strategy is applied exclusively during the training phase and does not introduce any additional cost during inference. 4.3. Garment-Enhanced Texture Learning Generally, diffusion models are merely optimized relying on the mean-squared loss defined in Eqn. 2, which treats all regions of the synthetic image equally, resulting in struggle to maintain garment consistency, especially in cases of small text and intricate patterns. To synthesize fine-grained textures, we design Garment-Enhanced Texture Learning (GTL) strategy to strengthen the supervision of attire details in image space, incorporating perceptual loss Lperc and high-frequency loss Lhighf req. Before introducing the proposed two losses, we define the generated image as: ˆI = D(ˆz0), where denotes the VAE decoder, and ˆz0 is estimated through single step of inference from the latent zt: zt ˆz0 = 1 αtϵθ αt . (10) Considering the one-step inference may produce noisy and flawed images, the proposed losses are only applied at less noisy timestep (t η). To sum up, GTL can be defined as: (cid:40) Ltexture = Lperc + Lhighf req, 0, η > η . (11) Perception Loss To simultaneously enhance structural consistency and pattern similarity with reference garments, we employ perceptual loss based on the Deep Image Structure and Texture Similarity (DISTS) metric [7]. Specifically, we use the reference garments segmentation mask to isolate the attire in both the generated and ground truth images, averaging their structural and textural inconsistencies within perceptual feature space, defined as: Lprec = 1 N (cid:88) k=1 DIST S( ˆI Mk, Mk), (12) where signifies element-wise multiplication. High-Frequency Loss As intricate details in the dressing garments typically appear as high-frequency components with rich edge information, we use edge detection to extract this high-frequency information, aiming to strengthen the constraints on detailed patterns. Specifically, we utilize Canny edge detection operator [8] to capture these Single Grament Multiple Graments Dressing-Pair Method Proprietary Dataset CLIP-T CLIP-I CLIP-AS CLIP-T CLIP-I CLIP-AS CLIP-T CLIP-I CLIP-AS VITON-HD [5] IP-Adapter [54] StableGarment [47] MagicClothing [4] IMAGDressing [40] Ours 0.268 0.285 0.288 0.202 0. 0.644 0.583 0.640 0.734 0.741 5.674 5.781 5.703 5.077 5.881 0.272 0.281 0.298 0.230 0.296 0.632 0.587 0.619 0.684 0.710 5.678 5.648 5.784 5.133 5.931 0.277 0.284 0.266 0.242 0. 0.523 0.556 0.583 0.614 0.734 5.795 5.735 5.540 5.291 5.874 Table 1. Quantitative comparisons with baseline methods for both single-garment and multi-garment evaluation. rich-texture regions, and define the high-frequency loss Lhighf req as: Lhighf req ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) k=1 ˆI k2, (13) Method Texture Consistency Align with Prompt Image Quality Comprehensive Evaluation IP-Adapter [54] StableGarment [47] MagicClothing [4] IMAGDressing [40] Ours 0.45% 6.65% 11.95% 2.20% 2.05% 1.60% 4.85% 2.65% 3.75% 2.05% 9.00% 9.70% 2.10% 2.50% 3.90% 1.70% 93.80% 77.00% 71.80% 90.30% where = Mk , is the extracted edge map of I. Table 2. User study with baseline methods. 4.4. Training and Inference In training, we average Lloc across all layers and define overall loss as follows: LLDM = Ez0,ϵ,Ct,Cg,tϵ ϵθ(zt, Ct, Cg, t)2, = LLDM + λ1 Lloc + λ2Ltexture, (14) (15) where Ct and Cg represent text condition and clothing condition respectively. In the inference stage, we apply classifier-free guidance during the denoising process: ˆϵθ(zt, Ct, Cg, t) = ωϵθ(zt, Ct, Cg, t) + (1 ω)ϵθ(zt, t). (16) 5. Experiments 5.1. Setup Dataset. Notably, dataset comprising image triplets that include model images paired with multiple laid-out garments is currently lacking. Therefore, we utilize HumanParsing model to extract clothing items from DressCode [33] and an additional proprietary dataset collected from the internet, forming triplet data pairs (upper garment, lower garment, person image). In these triplets, one garment is an original laid-out image, while the other is segmented image from the persons image. Finally, we construct 26,114 public triplets from Dresscode and 37,065 triplets from proprietary dataset to train AnyDressing. For model evaluation, we introduce two benchmarks to evaluate the model on single-garment and multi-garment dressing respectively. Specifically, for single-garment evaluation, we handpick 300 diverse garments from VITON-HD [5] encompassing various styles and colors, and additionally gather 300 reference garments with intricate textures and non-garment images from the internet. For multi-garment evaluation, we meticulously collect 25 lowers from the internet and pair each with 10 different uppers, resulting in total of 250 pairs, called Dressing-Pair. We generate images for each test garment with the provided 7 text prompts. Implementation Details. In our experiments, we initialize the weights of GarmentsNet and DressingNet with the weights of Stable Diffusion v1.5 [38] and we use CLIP ViTL/14 [36] as the text encoder. Our model is trained on paired images with resolution of 768 576. The model is trained on 8 NVIDIA A100 GPUs for 100k steps with batch size of 4. We adopt AdamW [31] optimizer with fixed learning rate of 5e-5. During inference, we use DDIM [43] sampler with 30 steps and set guidance scale ω to 6.0. For more detailed settings, please refer to the supplementary materials. Baselines. We compare our method against the following state-of-the-art image synthesis method: IP-Adapter [54], MagicClothing [4], StableGarment [47] and IMAGDressing [40]. We directly use their released pre-trained models for comparison. For fair comparison, all experiments are conducted with resolution of 768 576. Evaluation Metrics. Similar to previous methods, we quantitatively evaluate the generated results from three perspectives: CLIP-T for text consistency, CLIP-I for texture consistency, and Aesthetic Score (AS) for overall generation quality. Especially, to better evaluate multi-garment dressing, we introduce new metric CLIP-I to assess texture consistency by leveraging OpenPose [1] to obtain the matching partitions of the reference garments in the synthesized image and averaging their CLIP-I metrics. 5.2. Qualitative Analysis Since the compared methods lack multi-garment support, we obtain baseline results by concatenating multiple garments along the spatial dimension as input. Fig. 3 presents visual comparisons between our method and baseline ap6 Figure 3. Qualitative comparisons with state-of-the-art methods. Please zoom in for more details. proaches. AnyDressing maintains superior consistency in clothing style and texture, and exhibits better text fidelity, while other methods struggle to balance garment preservation and prompt faithfulness. In particular, baselines encounter significant background contamination and garment confusion in multi-garment dressing results, whereas our method demonstrates exceptional reliability, which is attributed to our designed GarmentsNet and DressingNet architectures. And Fig. 4 presents the results of AnyDressing as plug-in module combined with other extensions and customized LoRAs, demonstrating its powerful compatibility. Please refer to the supplementary for more results. 5.3. Quantitative Comparisons Metric Evaluation. Tab. 1 shows the quantitative results of our methods against baselines. For single-garment evaluation, extensive experiments conducted on VITON-HD [5] and proprietary dataset prove the superiority of AnyDressing compared with all baselines. And our method significantly surpasses all baselines across all metrics in multigarment virtual dressing results, fully demonstrating AnyDressings reliability in handling both single-garment and multi-garment virtual dressing tasks. User Study. We conduct user study to evaluate the gener7 Figure 5. Ablation results on GFE and IGL modules. Figure 4. Examples of plug-in results of AnyDressing. GFE IGL GTL CLIP-T CLIP-I CLIP-AS (cid:37) (cid:34) (cid:34) (cid:34) (cid:37) (cid:37) (cid:34) (cid:34) (cid:37) (cid:37) (cid:37) (cid:34) 0.260 0.265 0.289 0.296 0.625 0.718 0.722 0.734 5.572 5.627 5.790 5.874 Table 3. Ablation study of AnyDressing. ation quality of our model. We use all test garments and prompts in our dataset and randomly show the users 25 single-garment results and 25 multi-garment results from the baselines and our method. Each participant is asked to select the most preferred result under four criteria: texture consistency, alignment with the text prompt, image quality and comprehensive evaluation. In the end, we receive valid responses from 40 users. The collected preferences are reported in Tab. 2. In terms of four criteria, our method is preferred by most participants, with percentages reaching 93.80%, 77.00%, 71.80% and 90.30% respectively. 5.4. Ablation Studies GFE & IGL. To validate the effectiveness of our proposed architecture, we employ traditional ReferenceNet [16] to encode multiple garments concurrently and then incorporate them into the denoising U-Net similar to [40] as our base model. As illustrated in Fig. 5, Base+GFE significantly reduces garment confusion and improves garment consistency compared to Base, which is attributed to the multi-garment parallel processing design of the GFE module. Base+GFE+IGL shows better fidelity to the text Figure 6. Ablation results on GTL module. prompts and further mitigates background contamination, which demonstrates IGL mechanism effectively constrains garment features to attend to the correct regions. The quantitative comparison in Tab. 3 further proves the effectiveness of each module, with GFE primarily improving the CLIP-I and IGL enhancing both CLIP-T and CLIP-AS. GTL. Fig. 6 intuitively demonstrates the effectiveness of our proposed GTL strategy, encouraging the model to enhance detail preservation, particularly in small text and intricate patterns. And quantitative result in Tab. 3 also verifies that our designed GTL improves texture consistency. 6. Conclusion This paper presents AnyDressing comprising two core networks named GarmentsNet and DressingNet to focus on new task, i.e., Multi-Garment Virtual Dressing. The GarmentsNet employs Garment-Specific Feature"
        },
        {
            "title": "The DressingNet",
            "content": "Extractor module to efficiently encode multi-garment features in parallel. integrates these features for virtual dressing using Dressing-Attention module and an Instance-Level Garment Localization Learning mechanism. Additionally, we design GarmentEnhanced Texture Learning strategy to further enhance texture details. Our approach can seamlessly integrate with any community control plugins. Extensive experiments show that AnyDressing achieves state-of-the-art results."
        },
        {
            "title": "References",
            "content": "[1] Cao, Hidalgo, Simon, SE Wei, and Sheikh. Openpose: Realtime multi-person 2d pose estimation using part affinity fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(1):172186, 2020. 6 [2] Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Xiao Yang, and Mohammad Soleymani. Magicdance: Realistic human dance video generation with motions & facial expressions transfer. arXiv preprint arXiv:2311.12052, 2023. 3 [3] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. 2 [4] Weifeng Chen, Tao Gu, Yuhao Xu, and Chengcai Chen. Magic clothing: Controllable garment-driven image synthesis. arXiv preprint arXiv:2404.09512, 2024. 2, 3, 5, 6 [5] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. try-on via Viton-hd: High-resolution virtual In Proceedings of the misalignment-aware normalization. IEEE/CVF conference on computer vision and pattern recognition, pages 1413114140, 2021. 3, 6, 7, 1 [6] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for virtual try-on. arXiv preprint arXiv:2403.05139, 2024. 3 [7] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image quality assessment: Unifying structure and texture IEEE transactions on pattern analysis and masimilarity. chine intelligence, 44(5):25672581, 2020. 5 [8] Lijun Ding and Ardeshir Goshtasby. On the canny edge detector. Pattern recognition, 34(3):721725, 2001. 5 [9] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. 2 [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2, 3 [11] Junhong Gou, Siyu Sun, Jianfu Zhang, Jianlou Si, Chen Qian, and Liqing Zhang. Taming the power of diffusion models for high-quality virtual try-on with appearance flow. In Proceedings of the 31st ACM International Conference on Multimedia, pages 75997607, 2023. 3 [12] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized lowrank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. [13] Sen He, Yi-Zhe Song, and Tao Xiang. Style-based global In Proceedings of the appearance flow for virtual try-on. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 34703479, 2022. 3 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 1, 2, 3, 4, 12 [16] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 3, 8, 2 [17] Qihan Huang, Siming Fu, Jinlong Liu, Hao Jiang, Yipeng Yu, and Jie Song. Resolving multi-condition confusion arXiv for finetuning-free personalized image generation. preprint arXiv:2409.17920, 2024. 3 [18] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin CK Chan, and Ziwei Liu. Reversion: Diffusion-based relation inversion from images. arXiv preprint arXiv:2303.13495, 2023. 2 [19] Thibaut Issenhuth, Jeremie Mary, and Clement Calauzenes. Do not mask what you do not need to mask: parser-free virtual try-on. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XX 16, pages 619635. Springer, 2020. 3 [20] Zhenchao Jin. Sssegmenation: An open source supervised arXiv semantic segmentation toolbox based on pytorch. preprint arXiv:2305.17091, 2023. 1 [21] Zhenchao Jin, Xiaowei Hu, Lingting Zhu, Luchuan Song, Li Yuan, and Lequan Yu. Idrnet: Intervention-driven relation network for semantic segmentation. Advances in Neural Information Processing Systems, 36, 2024. 1 [22] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1012410134, 2023. 2 [23] Chanran Kim, Jeongin Lee, Shichang Joung, Bongmo Kim, Instantfamily: Masked attention arXiv preprint and Yeul-Min Baek. for zero-shot multi-id image generation. arXiv:2404.19427, 2024. 3 [24] Jeongho Kim, Guojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. Stableviton: Learning semantic correspondence with latent diffusion model for virtual try-on. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81768185, 2024. [25] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization 9 of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 2, 3 [26] Sangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan Choi, and Jaegul Choo. High-resolution virtual try-on with In Eumisalignment and occlusion-handled conditions. ropean Conference on Computer Vision, pages 204219. Springer, 2022. 3 [27] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024. 3 [28] Kedan Li, Min Jin Chong, Jeffrey Zhang, and Jingen Liu. Toward accurate and realistic outfits visualization with attention to details. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15546 15555, 2021. [29] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing realistic human photos via stacked id embedding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 86408650, 2024. 3 [30] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple subjects. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pages 5750057519, 2023. 2 [31] Loshchilov. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [32] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu. Subjectdiffusion: Open domain personalized text-to-image generation without test-time fine-tuning. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 2, 3 [33] Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. Dress code: Highresolution multi-category virtual try-on. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 22312235, 2022. 3, 6, [34] Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, and Rita Cucchiara. Ladi-vton: Latent diffusion textual-inversion enhanced virtual try-on. In Proceedings of the 31st ACM International Conference on Multimedia, pages 85808589, 2023. 3 [35] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 42964304, 2024. 3 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 2 [38] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3, [39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 2, 3 [40] Fei Shen, Xin Jiang, Xin He, Hu Ye, Cong Wang, Xiaoyu Du, Zechao Li, and Jinghui Tang. Imagdressing-v1: CustomizarXiv preprint arXiv:2407.12705, able virtual dressing. 2024. 2, 3, 5, 6, 8 [41] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85438552, 2024. 2 [42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. PMLR, 2015. 2 [43] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 6 and Stefano Ermon. arXiv preprint [44] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [45] Yael Vinker, Andrey Voynov, Daniel Cohen-Or, and Ariel Shamir. Concept decomposition for visual exploration and inspiration. ACM Transactions on Graphics (TOG), 42(6): 113, 2023. 2 [46] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang. Toward characteristicpreserving image-based virtual try-on network. In Proceedings of the European conference on computer vision (ECCV), pages 589604, 2018. 3 [47] Rui Wang, Hailong Guo, Jiaming Liu, Huaxia Li, Haibo Zhao, Xu Tang, Yao Hu, Hao Tang, and Peipei Li. Stablegarment: Garment-centric generation via stable diffusion. arXiv preprint arXiv:2403.10783, 2024. 2, 3, [48] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 1 [49] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594315953, 2023. 2 10 [50] Zhichao Wei, Qingkun Su, Long Qin, and Weizhi Wang. Mm-diff: High-fidelity image personalization arXiv preprint via multi-modal condition integration. arXiv:2403.15059, 2024. 3 [51] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multisubject image generation with localized attention. International Journal of Computer Vision, pages 120, 2024. 3 [52] Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye Dong, Xijin Zhang, Feida Zhu, and Xiaodan Liang. Gpvton: Towards general purpose virtual try-on via collaboraIn Proceedings of tive local-flow global-parsing learning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2355023559, 2023. [53] Xu, Gu, Chen, and Chen. Ootdiffusion: Outfitting fusion based latent diffusion for controllable virtual try-on. arxiv 2024. arXiv preprint arXiv:2403.01779, 2024. 3 [54] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 1, 2, 3, 5, 6, 11 [55] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. 1, 2, 3, 11 [56] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 80698078, 2024. 3 [57] Zhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris Metaxas, and Jian Ren. Sine: Single image editing with textto-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60276037, 2023. 2 11 AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models"
        },
        {
            "title": "Supplementary Material",
            "content": "In the supplementary material, the sections are organized as follows: We provide more details regarding parameters, datasets and user study in Sec. 7. We further prove the scalability of AnyDressing in Sec. 8. We provide more ablation results in Sec. 9. We provide more comparisons with baselines, more qualitative results in the wild and more applications in Sec. 10. 7. Implementation Details 7.1. Detailed Parameters In our experiments, we use SOTA large multi-modal model CogVLM [48] to caption the image. The hyper-parameters used in our experiments are set as follows: For the Dressing-Attention mechanism, we set the hyperparameter λ = 0.7 during inference to get customized results. For the noisy timestep threshold discussed in the Garment-Enhanced Texture Learning (GTL) strategy, we set η = 350. The other hyper-parameters used in the experiment are as follows: λ1 = 0.01, λ2 = 0.001. 7.2. Datasets To facilitate research on multi-garment virtual dressing, dataset consisting of image triplets is necessary, with each triplet containing an upper garment image, lower garment image, and model image wearing the corresponding garments. However, existing in-shop garment to model pairs [5, 33] only contain single reference garment. We leverage the public DressCode dataset along with proprietary dataset to construct triplets, as illustrated in Fig. 7. Assuming we begin with the upper garment data, where we already have an in-shop upper garment and model image wearing it, we employ human parsing techniques [20, 21] to roughly segment and extract the lower garment portion from the model image, using it as the corresponding lower garment image. At this stage, the triplet comprises an inshop upper garment image, cropped lower garment image, and model image. Similarly, triplets derived from the lower garment data consist of cropped upper garment image, an in-shop lower garment image, and model image. Finally, we constructed 26,114 public triplets from Dresscode and 37,065 triplets from the proprietary dataset to train AnyDressing. It is worth noting that our model has not encountered garment pairs in the form of (in-shop upper garment, in-shop lower garment) or (cropped upper garment, cropped lower garment) during training. Nevertheless, it exhibits strong robustness during inference, indicating that the model has effectively learned the proper way to combine upper and lower garments through training. 7.3. User Study To compare with the baseline methods, we conduct user study as part of the evaluation. The survey randomly presented 50 sets of generated results to each participant. screenshot of the survey for set of generated results is displayed in Fig. 8, which includes five images and four questions: 1. Which result appears to have the highest consistency with reference garments? 2. Which result best matches the prompt [prompt]? 3. Which result appears to have the highest image quality? 4. Which result matches your best choice based on comprehensive considerations? For each set of results displayed in the survey, we ensured that their order was randomly shuffled to prevent bias. Responses where all answers had the same selection and responses with completely identical answers were considered invalid. Finally, we obtained total of 40 valid surveys to evaluate the model. 8. Scalability of AnyDressing To further validate the scalability of our designed GarmentsNet structure, we introduce more combinations of clothing items (hat, upper garment and lower garment), as illustrated in Fig. 10. As shown in Fig. 9, to train the model, we construct datasets using the same idea as introduced in Sec. 7.2. Specifically, we select 18,059 pairs from the proprietary dataset that satisfies the model image containing the hat, and use the human parsing techniques to obtain the cropped hat image from the model image. Notably, each additional garment condition requires only some newly added LoRA matrix ˆW in the GarmentSpecific Feature Extractor (GFE) module. And it requires only single forward pass (timestep = 0) to encode the clothing before injecting features into the DressingNet, minimizing the additional computational time during both the training and inference process. This experiment effectively demonstrates that our GarmentsNet can be extended to accommodate any number of clothing items. Additionally, thanks to our proposed Instance-Level Garment Lotion capability of FaceID [54] to provide an authentic virtual dressing experience. The visual results, as shown in Fig. 17. Stylized Customization. Furthermore, by utilizing stylized base models or customized LoRAs [15], we can generate creative and stylized outputs while preserving the intricate details of the garments, as shown in Fig. 16 and Fig. 18. calization (IGL) learning mechanism, AnyDressing can further prevent garment blending and enhance fidelity to customized text prompts. 9. More Ablation Study In Fig. 11, we present additional visual results to validate the effectiveness of the Garment-Specific Feature Extractor (GFE) module and the Instance-Level Garment Localization (IGL) learning mechanism. We employ traditional ReferenceNet [16] to encode multiple garments concurrently and then incorporate them into the denoising U-Net similar to [4, 40] as our base model. As shown in Fig. 11, Base model encounters severe clothing confusion issues, resulting in the colors and patterns of multiple garments blending. In contrast, Base+GFE significantly reduces garment confusion and improves garment consistency, which is attributed to the multi-garment parallel processing design of our designed GFE module. Base+GFE+IGL shows better fidelity to the text prompts and further mitigates background contamination, which demonstrates IGL mechanism effectively constrains garment features to attend to the correct regions and avoid influencing other irrelevant regions in the synthetic images. 10. More Results 10.1. More Comparisons As shown in Fig. 12-13, We provide more visual comparisons between our method and state-of-the-art baselines [4, 40, 47, 54]. It is clear from these comparisons that our method maintains superior consistency in clothing style and texture, and exhibits better text fidelity. 10.2. More Visual Results As shown in Fig. 14-16, we provide more multi-garment virtual dressing results of AnyDressing in the wild. It can be observed that our method produces high-quality customized virtual dressing results for various types of garment combinations, while faithfully adhering to personalized text prompts. Experiments in complex scenarios demonstrate that AnyDressing significantly enhances the practical application of Virtual Dressing in e-commerce and creative design. 10.3. More Applications Combined with ControlNet. Leveraging the capabilities of ControlNet [55], our model can generate personalized models guided by specific conditions. We present the OpenPose-guided generation results in Fig. 17. Combined with IP-Adapter. Our model enables the generation of target individuals wearing specified garments integrated with the IP-Adapter. We utilize the ID preservaFigure 7. Examples of the training dataset I. Figure 8. Screenshot of user study. 3 Figure 9. Examples of the training dataset II. Figure 10. Qualitative results of more combinations of clothing items. Figure 11. More ablation results on GFE and IGL modules. 5 Figure 12. More qualitative comparisons I. 6 Figure 13. More qualitative comparisons II. Figure 14. More qualitative results I. 8 Figure 15. More qualitative results II. 9 Figure 16. More qualitative results III. Figure 17. More results of combining ControlNet [55] and FaceID [54]. 11 Figure 18. More results of combining LoRAs [15]."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Tsinghua University"
    ]
}