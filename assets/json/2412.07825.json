{
    "paper_title": "3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark",
    "authors": [
        "Wufei Ma",
        "Haoyu Chen",
        "Guofeng Zhang",
        "Celso M de Melo",
        "Alan Yuille",
        "Jieneng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D spatial reasoning is the ability to analyze and interpret the positions, orientations, and spatial relationships of objects within the 3D space. This allows models to develop a comprehensive understanding of the 3D scene, enabling their applicability to a broader range of areas, such as autonomous navigation, robotics, and AR/VR. While large multi-modal models (LMMs) have achieved remarkable progress in a wide range of image and video understanding tasks, their capabilities to perform 3D spatial reasoning on diverse natural images are less studied. In this work we present the first comprehensive 3D spatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual question-answer pairs across 12 question types. We conduct robust and thorough evaluation of 3D spatial reasoning capabilities by balancing the data distribution and adopting a novel FlipEval strategy. To further study the robustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench includes two subsets with 3D spatial reasoning questions on paired images with common and uncommon viewpoints. We benchmark a wide range of open-sourced and proprietary LMMs, uncovering their limitations in various aspects of 3D awareness, such as height, orientation, location, and multi-object reasoning, as well as their degraded performance on images with uncommon camera viewpoints. Our 3DSRBench provide valuable findings and insights about the future development of LMMs with strong 3D reasoning capabilities. Our project page and dataset is available https://3dsrbench.github.io."
        },
        {
            "title": "Start",
            "content": "3DSRBench: Comprehensive 3D Spatial Reasoning Benchmark Wufei Ma1, Haoyu Chen2, Guofeng Zhang1, Celso de Melo3, Alan Yuille1, Jieneng Chen1 1Johns Hopkins University 2Carnegie Mellon University 3DEVCOM Army Research Laboratory 4 2 0 2 0 ] . [ 1 5 2 8 7 0 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "3D spatial reasoning is the ability to analyze and interpret the positions, orientations, and spatial relationships of objects within the 3D space. This allows models to develop comprehensive understanding of the 3D scene, enabling their applicability to broader range of areas, such as autonomous navigation, robotics, and AR/VR. While large multi-modal models (LMMs) have achieved remarkable progress in wide range of image and video understanding tasks, their capabilities to perform 3D spatial reasoning on diverse natural images are less studied. In this work we present the first comprehensive 3D spatial reasoning benchmark, 3DSRBench, with 2,772 manually annotated visual question-answer pairs across 12 question types. We conduct robust and thorough evaluation of 3D spatial reasoning capabilities by balancing the data distribution and adopting novel FlipEval strategy. To further study the robustness of 3D spatial reasoning w.r.t. camera 3D viewpoints, our 3DSRBench includes two subsets with 3D spatial reasoning questions on paired images with common and uncommon viewpoints. We benchmark wide range of open-sourced and proprietary LMMs, uncovering their limitations in various aspects of 3D awareness, such as height, orientation, location, and multi-object reasoning, as well as their degraded performance on images with uncommon camera viewpoints. Our 3DSRBench provide valuable findings and insights about the future development of LMMs with strong 3D reasoning capabilities. Our project page and dataset is available here. 1. Introduction Recent large multi-modal models (LMMs) [1, 3, 43] have achieved significant improvements in wide range of image and video understanding tasks, such as image captioning [2, 28], visual question answering [10, 20, 24, 31, 44], visual grounding [49], decision-making [7, 26, 34], and action recognition [35, 48]. However, recent studies have shown that even state-of-the-art LMMs exhibited limited 3D awareness [17, 36] and understanding of spatial relationships [46, 47], which are crucial for LMMs to develop Figure 1. 3D spatial reasoning performance comparison across leading models and human evaluation on 3DSRBench. Leading models (GPT-4o, Gemini-Pro, LLaVA-NeXT and Cambrian) lag behind human performance by at least -56% on average across four types of spatial relations (height, location, orientation, and multi-object). All models struggle with questions involving 3D orientation (e.g., on the left) and complex spatial reasoning (e.g., viewpoint towards object). See Fig. 2a for examples. comprehensive grasp of the 3D scene, to enable highlevel reasoning and interaction, and eventually, to achieve human-level intelligence. To study the 3D spatial reasoning capabilities of LMMs, previous works exploited synthetic environments and generated images with 3D ground-truths [46, 47]. Visual question-answer pairs were automatically synthesized by applying pre-defined rules on the known 3D scene graphs and other object attributes. The synthetic images exhibit significant domain gap with natural images and lacked the diversity and richness of contents in real-world. More recent works [13] explored real datasets with 3D annotations, e.g., Omni3D [6]. However, images in these datasets are still limited to specific domains, such as indoor rooms and self-driving scenes. In general, visual question-answer pairs generated with rule-based methods from 3D annotations (i) limit the scope of theirs datasets to very small set of rigid objects, and (ii) cannot enable fine-grained and robust evaluation of 3D spatial relationships that can only be achieved with human annotated datasets (see Sec. 3.1). In this work we present the first comprehensive 3D spa1 (a) (b) Figure 2. Overview of the our 3DSRBench. (a) Example questions from the four main types of 3D spatial reasoning questions, i.e., height, location, orientation, and multi-object reasoning. (b) To enable robust evaluation of the 3D spatial reasoning capabilities, we collect complementary images that lead to opposite answers given the same question and adopt novel FlipEval strategy to remove left/right biases in 3D with paired VQAs (see Sec. 3.4). tial reasoning benchmark, 3DSRBench, that features wide variety of 3D spatial reasoning questions on diverse and open-vocabulary entities, including rigid objects, humans, animals, and implicit concepts, such as logo on car or arrow on billboard. We manually annotate 2,100 visual question-answer pairs on natural images from the MSCOCO dataset [29], covering 12 subtypes of questions from 4 main categories, i.e., height, location, orientation, and multi-object reasoning. Each category of questions focus on different combinations of 3D properties, such as object 3D location, 3D ground plane, camera extrinsic calibration, and/or object 3D poses. Examples from each question category are presented in Figure 2a. Another challenge of 3D spatial reasoning arises from the 6D viewpoint of the camera, i.e., the 3D location and 3D orientation from which we are viewing the 3D scene. As shown in Fig. 4, 3D spatial reasoning questions can be easier for common 6D viewpoints, e.g., ones positioned at the eye level with natural viewing angles, while being more challenging for other uncommon viewpoints. Although uncommon viewpoints are less populated in most image datasets, cameras in embodied AI and robotics are often positioned in these uncommon viewpoints. Hence it is of crucial importance for LMMs to retain good 3D spatial reasoning performance for both common and uncommon viewpoints. To fairly compare the 3D spatial reasoning capabilities of LMMs w.r.t. different camera viewpoints, we annotate another 672 visual question-answer pairs on multi-view synthetic images rendered from the HSSD dataset [25]. Besides benchmarking wide variety of open-sourced and proprietary LMMs, our 3DSRBench serves as an important diagnosis benchmark for developing 3D spatially intelligent LMMs. Inspired by previous studies on 3D awareness of visual foundation models [17, 36], our 3DSRBench takes one step further and evaluates LMMs on fundamental 3D spatial reasoning questions, which provide valuable insights regarding the 3D awareness of visual encoders [11, 21, 27, 38, 39] and the 3D reasoning capabilities of language models [14, 16, 45, 50]. Such results would shed light on downstream tasks that build on 3D spatial reasoning, such as automatic navigation and robotic manipulation. To enable comprehensive and robust evaluation of 3D spatial reasoning capabilities, our 3DSRBench adopts several key designs: (1) balanced data distribution in multiple aspects, such as roughly same number of yes/no answers and complementary images pairs that lead to opposite answers given the same question (see Fig. 2b); (2) avoiding questions with trivial answers; and (3) novel FlipEval strategy designed for robust evaluation of 3D spatial reasoning questions. In summary, our 3DSRBench significantly advances the evaluation of 3D spatial reasoning capabilities by manually annotating 2,100 visual question-answer pairs on natural images from MS-COCO [29] and 672 on multi-view synthetic images. We annotate diverse and open-vocabulary entities and adopt key designs to enable robust and thorough evaluation that cannot be done with existing datasets. We benchmark wide variety of open-sourced and proprietary LMMs and study their 3D spatial reasoning capabilities w.r.t. different types of 3D awareness. We further analyze the robustness of 3D spatial reasoning capabilities w.r.t. common and uncommon camera 6D viewpoints and study range of LMMs with different visual encoder designs. Experimental results on different splits of our 3DSRBench provide valuable findings and insights that will benefit future research on 3D spatially intelligent LMMs. 2. Related Works 3D awareness of visual foundation models. With the recent advancements in large multi-modal models [3032], 2 there has been rising interest in applying these LMMs to broader range of tasks, such as chatting about human poses [18], embodied question answering [37], and robotic manipulation [22, 23]. Notably, these tasks involve reasoning and interacting with the 3D scenes, which largely builds on the 3D awareness of vision encoders. Previous works studied the 3D awareness of visual foundation models by adopting proxy tasks, such as part correspondence [17] and pose estimation [36], and quantitatively evaluating the 3D awareness with linear probing. Our work can be considered as one step further studying the 3D recognition and reasoning capabilities of LMMs by benchmarking their performance on fundamental 3D spatial relationship questions. Future research on downstream tasks, such as automatic navigation and robotic manipulation, could refer to the findings in our 3DSRBench and adopt LMMs with better 3D spatial reasoning capabilities. 3D spatial reasoning. Previous studies on 3D spatial reasoning capabilities of vision-language models often adopt synthetic environments, e.g., Blender, with controllable simulation and 3D groundtruths for automatic questionanswer generation [12, 46, 47]. However, synthetic images in these datasets exhibit large domain gap with natural images and it remains unclear if insights and findings from these datasets would generalize to the real image domain. More recent works, such as SpatialRGPT [13] and Cambrian-1 [44], built on existing datasets with 3D annotations [5, 6, 8, 19, 41, 42] and generated visual questionanswer pairs with pre-defined rules. Despite the improved image qualities, they are essentially limited to small number of rigid object categories in Omni3D [6]. To enable comprehensive and valuable evaluation of the 3D spatial reasoning capabilities, we manually annotate visual question-answer pairs, with diverse and open-vocabulary entities, such as logos on car or arrows on the billboard, and questions about comprehensive list of 3D spatial relationships from four main categories, i.e., height, location, orientation, and multi-object reasoning. 3. 3DSRBench In this section we introduce the 3DSRBench for comprehensively analyzing the 3D spatial reasoning capabilities of large multi-modal models (LMMs). We start by introducing the design considerations of 3DSRBench in Sec. 3.1, specifically how these design choices lead to robust and valuable evaluation of 3D spatial reasoning capabilities. Then we present the four main question types in Sec. 3.2, as well as the challenges in each type of questions. Next we discuss the three splits of our 3DSRBench and their scopes in Sec. 3.3. In Sec. 3.4 we present our evaluation strategies, including CircularEval and FlipEval. Lastly in Sec. 3.5 we provide an overview of our data collection procedures and the summary statistics. 3.1. Design of 3DSRBench When developing 3DSRBench, we incorporate the following four key designs to enable robust and valuable evaluation of 3D spatial reasoning capabilities. First, our 3D spatial reasoning questions are based on open-vocabulary entities. Previous spatial reasoning benchmarks [9, 44] largely relied on existing datasets with 3D annotations [6], which limited their scope to small number of rigid object categories. In our 3DSRBench, we annotate 3D spatial reasoning questions across broad range of open-vocabulary entities (see Fig. 2), enabling thorough analysis of the 3D awareness and 3D reasoning capabilities of LMMs over diverse, commonly encountered real-world objects. Next, we avoid questions with trivial answers. For instance, when comparing which of the two objects has smaller 3D distance to third anchor object, we avoid the cases when there is significant gap between the two distances. We focus on samples where the two distances are close, but the correct answer can be derived with annotators consensus (see Sec. 3.5). Moreover, we implement balanced data distribution in various aspects, such as roughly same number of yes/no answers and complementary image pairs [20] that lead to opposite answers given the same 3D spatial reasoning question (see Fig. 2b). This effectively removes priors in the answer distribution, e.g., pedestrians are often located lower than street lights, or the fact that objects higher in 3D space are also higher in 2D image plane. This design ensures that models cannot exploit biases or shortcuts for higher benchmark performance. Lastly, we adopt special evaluation strategies for robust evaluation, including previous CircularEval [33] and our novel FlipEval (see Sec. 3.4). 3.2. Question Types We present the 4 types of 3D spatial reasoning questions in our 3DSRBench. We discuss why they are challenging for LMMs and what kinds of 3D awareness and 3D spatial reasoning are needed to succeed in each type of questions. We present an overview of the 4 question types in Tab. 1. Height questions. For height-related question, we study if models can determine which of the two given objects is positioned higher in the 3D world space. To correctly answer the questions, model must (i) calibrate camera extrinsics, such as roll and pitch rotations, and then (ii) detect 3D locations of the objects in the 3D world space. This task poses significant challenge for large multi-modal models as these fine-grained 3D knowledge are hard to derive from the weak language supervision in standard multi-modal pretraining. In Figure 3a we illustrate two examples of height 3 (a) Height questions with different camera pitch rotations. (b) Comparison between 2D and 3D spatial reasoning questions. Figure 3. Challenges of 3D spatial reasoning questions in our 3DSRBench. See Sec. 3.2. (a) Height questions requires 3D spatial reasoning over combination of camera extrinsics and object 3D locations. Notice how different camera pitch rotations play crucial role to determine the final answer. (b) Previous 2D spatial reasoning questions can be addressed by analyzing objects 2D locations and depths, while our orientation questions require complex 3D spatial reasoning on objects 3D orientations and 3D locations. questions. Notice how different pitch rotations of the camera, i.e., viewing from above in the left figure and viewing upward in the right figure, play crucial role to determine the final answer. In both examples, relying solely on the 2D locations within the image plane or the 3D locations in the camera coordinate system would lead to incorrect answers. Location questions. There are three subtypes of locationrelated questions, i.e., determining (i) if two objects are next to or far from each other, (ii) which of the two objects is closer to the camera, and (iii) if an object is directly above or underneath another object. Models must not only ground the 2D locations of the objects, but also understand the depth of field presented in the image. Consider the location question in Fig. 2a. Although the 2D locations of the man and the hydrant are close, they are in fact far away from each other in the 3D space. Humans can determine the answer by estimating rough depths of the two objects, or from other visual cues, such as how the pedestrian walk leads towards the vanishing point. Other examples include the top two questions in Fig. 2b, which also require an understanding of the depth field. Orientation questions. Orientation-related questions study the 3D spatial reasoning that involves estimating the 3D orientation of an object. These questions are divided into three subtypes: determining which side of an object faces the camera, whether an object is in front of or behind another, and if an object is positioned on the left or right side of another. Unlike previous 2D spatial reasoning questions [9] that focus on spatial relationships w.r.t. the viewers perspective, our orientation-related questions emphasize spatial relationships from the objects perspective. As demonstrated in Fig. 3b, 2D spatial reasoning questions can be addressed by analyzing objects 2D locations and depths. Meanwhile, our orientation questions require estimating objects 3D orientation and perform 3D spatial reasoning across various dimensions of 3D information. Type # Subtypes Camera Loc. Orient. Reasoning Height Location Orientation Multi-Object 1 3 3 ++ ++ ++ ++ Table 1. Overview of the 4 types of 3D spatial reasoning questions. Each type of 3D spatial reasoning questions require different combinations of 3D awareness, i.e., camera calibration, 3D location, 3D orientation, and multi-object reasoning. Multi-object reasoning questions. Multi-object reasoning questions consider the 3D spatial relationships between multiple objects, such as asking which side of an object is facing another object, or with three objects, asking which of the given objects is facing towards or closer to the third object. In general, these questions require more advanced 3D awareness than simpler 3D concepts such as closer (to the camera) or higher, and require more complex reasoning over multiple objects 3D locations and orientations. 3.3. Benchmark Splits Our 3DSRBench is composed of three splits, real split with 2,100 3D spatial reasoning questions on MS-COCO images [29] and two synthetic splits with 672 questions on synthetic images rendered with 3D scenes in HSSD [25]. We evaluate the standard 3D spatial reasoning capabilities of LMMs on visual question-pairs from the real split, and with the synthetic split, we study the robustness of 3D spatial capabilities w.r.t. common and uncommon camera 6D viewpoints by analyzing the gap between the synthetic-common and synthetic-uncommon splits. With the HSSD 3D scenes and controllable photorealistic rendering, we obtain multi-view images of the same 3D scene, each rendered with common and an uncommon viewpoint. We ask the same 3D spatial reasoning question regarding the two images and study if models can obtain the correct answers on common and uncommon camera 6D viewpoints. We define common viewpoints as 6D camera poses with zero roll rotation, small pitch rotation, and taken 4 (a) Orientation questions on multi-view images from common (left) and uncommon (right) camera 6D viewpoints. (b) Multi-object reasoning questions on multi-view images from common (left) and uncommon (right) camera 6D viewpoints. Figure 4. Examples of the paired visual question-answer data in our 3DSRBench-synthetic. (a) Example questions from the four main types of 3D spatial reasoning questions. (b) To enable robust evaluation of the 3D spatial reasoning capabilities, we collect complementary images that lead to opposite answers given the same question and adopt novel FlipEval strategy (see Sec. 3.4). from the height of human, simulating the typical perspective when people take pictures. Conversely, uncommon viewpoints include 6D poses with noticeable roll rotation, large pitch rotation, or perspectives taken close to the ground or from high location. The two synthetic splits are denoted by synthetic-common and synthetic-uncommon and examples from the two splits are demonstrated in Fig. 4. Notice how the answers by GPT-4o are correct when shown the image from common camera 6D viewpoint and wrong when prompted from an uncommon viewpoint, despite both images present clear view of the 3D scene and humans can derive the correct answers without any difficulty. 3.4. Evaluation Since all 3D spatial reasoning questions in 3DSRBench have two or four answer choices, we formulate these questions as multiple choice questions with two or four options. To accommodate the free-form answers predicted by pretrained LMMs, we follow [33] and adopt LLM-involved choice extraction to obtain the predicted label. To enable robust evaluation of various 3D spatial reasoning capabilities, we adopt the following two designs during testing: CircularEval [33]. To avoid the bias of choice ordering and the influence of random guessing for multiple choice questions, we adopt CircularEval [33] for more robust benchmark performance. Specifically we feed each question into the LMM two or four times, each with different ordering of the answer choices. The LMM is considered successful in answering this question only if the predicted answer is correct for all passes. FlipEval. We further propose FlipEval to remove left- /right biases in 3D with paired visual question-answer pairs. By applying horizontal flip to the image, we obtain new visual question. The answer would generally remain the same, such as for location and height questions, but when it involves 3D spatial relationships such as left and right, the answer would change. We illustrate this idea in Fig. 2b, where the elephant logo is the on the left of the truck but changes to the right side after horizontal flipping. FlipEval effectively removes left/right biases in 3D spatial relationship, such as driver often sitting on the left side of the car or most people holding tools in their right hands. Lastly FlipEval also helps to avoid the influence of random guessing and enriches the image distribution in our 3DSRBench. 3.5. Data Collection We employ three annotators to annotate total of 2,772 unique visual question-answer pairs across 12 question types. We follow the annotation principles as discussed in Sec. 3.1 and adopt an two-stage pipeline to ensure various criteria are met. Specifically after annotations are collected in the first stage, we review the quality of the collected data and reject samples with low quality or ones that lead to imbalanced data distribution. Further new annotations are collected if necessary. Furthermore, we collect human responses for all visual question-answer pairs and disregard samples that dont reach consensus by human annotators. 4. Experiments In this section, we start by introducing our experimental settings in Sec. 4.1. Then we benchmark the 3D spatial reasoning capabilities of various open-sourced and proprietary LMMs on our 3DSRBench in Sec. 4.2. Next we con5 Model Baselines Random Random++ Human Open-sourced LLaVA-v1.5-7B [30] Cambrian-1-8B [44] LLaVA-NeXT-8B [32] Proprietary Claude-Sonnet [43] Gemini-1.5-Flash [43] Gemini-1.5-Pro [40] GPT-4o-mini [1] GPT-4o [1] 3DSRBench-real Overall Height Loc. Orient. Multi. 20.9 45.8 95.7 36.8 44.1 49.6 46.9 39.2 49.1 39.1 45.3 25.0 50.0 92.9 38.5 25.6 50.6 49.6 39.8 50.8 42.1 49. 25.0 50.0 96.4 46.4 57.0 62.7 60.0 59.9 62.9 51.8 62.3 16.8 41.7 97.7 27.7 36.5 36.8 32.8 13.2 37.5 23.4 23. 20.1 45.0 94.9 31.8 43.1 43.6 41.2 33.6 41.3 34.6 40.1 Table 2. Experimental comparison of state-of-the-art large multi-modal models on our 3DSRBench. Results show that state-of-the-art LMMs exhibit limited 3D spatial reasoning capabilities. Please refer to Sec. 4.2 for detailed analyses. sider variants of LMMs with different visual encoders and study how the 3D awareness of visual encoders contribute to the final 3D spatial reasoning capabilities in Sec. 4.3. In Sec. 4.4 we evaluate various LMMs on our 3DSRBenchsynthetic and analyze the robustness of LMMs w.r.t. uncommon camera viewpoints. Lastly we present some failure cases of GPT-4o in Sec. 4.5, revealing some interesting findings about the 3D spatial reasoning of LMMs. 4.1. Experimental Settings Testing data augmentation. We develop rule-based methods to augment the unique annotated visual questionanswer pairs and obtain larger number of testing data with balanced and rich set of 3D spatial relationships. For instance, given question asking which object is higher in the 3D world space, we generate new question asking which object is lower in the 3D world space. We end up with 2,625 questions on MS-COCO images [29], i.e., 3DSRBench-real, and 846 questions on synthetic images, i.e., 3DSRBench-synthetic. the data annotation process. We report the full results in Tab. 2. We make the following observations: (i) State-of-theart LMMs have limited 3D spatial reasoning capabilities, as found by low performance achieved by state-of-theart open-sourced and proprietary LMMs, falling far behind (ii) Scaling laws for LMMs human-level performance. are not effective for 3D spatial reasoning. Results show that despite significant more training data and computation spent on the proprietary LMMs, they do not show clear advantages over open-sourced counterparts, featuring highquality data with efficient training setups. Standard scaling laws demonstrate diminishing returns for 3D spatial reasoning capabilities and we believe more effective approaches, e.g., 3D-aware data, architecture, and training designs, would be necessary to significantly advance 3D spatial reasoning. 4.3. 3D Awareness of Vision Encoders We further study the 3D awareness of vision encoders by benchmarking variants of LLaVA-v1.5-7B [31] on our 3DSRBench with various vision encoders. Specifically, we consider range of pretrained visual foundation models, i.e., CLIP [39], MAE [21], DINOv2 [38], and SAM [27], and enhance the baseline model with mixed encoders. This can retain the performance on standard benchmarks with the semantic features and potentially involve useful features for 3D reasoning. Next we consider the spatial vision aggregator (SVA) [44] to further fuse the features from multiple vision encoders. We quantitatively evaluate these LMM variants on our 3DSRBench and report the results in Tab. 3. Results show that with mixed encoders, DINOv2 can improve the overall 3D spatial reasoning abilities of LMM. However, we notice significant improvements for height questions when adopting MAE and SAM as vision encoder, suggesting that having richer visual features could help localize objects better. With SVA [44], we can further improve the LMM with mixed encoder from 37.2% to 37.8%, demonstrating that fusing the semantic features with 3D-aware features from DINOv2 would benefit subsequent reasoning. 4.2. Results on 3D Spatial Reasoning Capabilities 4.4. Robustness to Uncommon Camera Viewpoints We benchmark wide range of open-sourced and proprietary LMMs on our 3DSRBench real and analyze their 3D spatial reasoning capabilities on different categories of questions. We consider two baseline results: (i) random: simple baseline that predicts random answers for all visual questions. (ii) random++: stronger random baseline that predicts consistent answers given different choice orders of same visual question in CircularEval. We report the full results in Tab. 2. (iii) human: human-level performance established by human evaluators that did not participate in We study the robustness of 3D spatial reasoning capabilities w.r.t. common and uncommon viewpoints. We evaluate variety of open-sourced and proprietary LMMs on our 3DSRBench-synthetic-common and 3DSRBenchsynthetic-uncommon splits. As demonstrated by the results in Tab. 4, we can see that all LMMs exhibit significantly degraded performance when generalizing from common viewpoints to uncommon viewpoints, e.g., 6.9% drop in accuracy for GPT-4o, 10.4% drop for Gemini-Pro, and 8.7% drop for LLaVA-NeXT-8B [32]. We visualize two 6 3DSRBench LLM Vision Encoder Connector Mean Height Loc. Orient. Multi. Baseline Vicuna-v1.5-7B [30, 50] CLIP-L14-336 [39] 2xMLP 36.8 38.5 46.4 27.7 31. Mixed Encoders Vicuna-v1.5-7B [30, 50] CLIP-L14-336 [39] + DINOv2-L14-224 [38] Vicuna-v1.5-7B [30, 50] CLIP-L14-336 [39] + MAE-H14 [21] Vicuna-v1.5-7B [30, 50] CLIP-L14-336 [39] + SAM-L [27] 2xMLP 2xMLP 2xMLP Connectors Vicuna-v1.5-7B [30, 50] CLIP-L14-336 [39] + DINOv2-L14-224 [38] Vicuna-v1.5-7B [30, 50] CLIP-L14-336 [39] + MAE-H14 [21] SVA [44] SVA [44] 37.2 33.1 27.9 37.8 34. 45.9 42.7 44.6 46.0 45.3 42.2 39.2 34.4 43.1 38.6 28.7 26.1 16.5 26.5 25. 33.6 27.5 21.5 35.9 30.2 Table 3. Experimental results on LMMs with various vision encoder setups. We use LLaVA-v1.5-7B as the baseline model and studies how vision encoders with different features contribute to the final 3D spatial reasoning abilities of LMMs. Model Random Random++ Open-sourced LLaVA-v1.5-7B [31] Cambrian-1-8B [44] LLaVA-NeXT-8B [32] Proprietary Qwen-VL-Plus [4] Qwen-VL-Max [4] Claude-Sonnet [3] Gemini-1.5-Flash [40] Gemini-1.5-Pro [40] GPT-4o-mini [1] GPT-4o [1] 3DSRBench-synthetic-common 3DSRBench-synthetic-uncommon Overall Height Loc. Orient. Multi. Overall Height Loc. Orient. Multi. 20.9 45.8 42.0 48.1 45.5 30.7 55.2 47.4 44.6 59.9 46.5 51.2 25.0 50.0 40.0 37.5 65. 35.0 62.5 47.5 57.5 65.0 47.5 70.0 25.0 50.0 50.6 56.1 57.9 37.8 69.5 58.5 59.8 69.5 53.7 70.1 16.8 41.7 20.8 39.6 10. 30.2 31.2 26.0 13.5 50.0 36.5 17.7 20.1 45.0 47.6 47.6 50.0 20.2 52.4 49.2 44.4 53.2 44.4 46.0 20.9 45.8 38.0 39.9 36. 21.0 48.6 39.4 37.7 49.5 40.3 44.3 25.0 50.0 41.0 35.0 47.5 15.0 52.5 60.0 42.5 42.5 42.5 60.0 25.0 50.0 43.6 45.7 44. 25.0 59.8 48.2 45.7 52.4 43.9 58.5 16.8 41.7 17.9 29.2 7.3 22.9 24.0 16.7 11.5 40.6 33.3 15.6 20.1 45.0 45.2 41.9 46. 16.1 51.6 38.7 46.0 54.8 40.3 42.7 Table 4. Experimental results on our 3DSRBench-synthetic-common and 3DSRBench-synthetic-uncommon. We study the robustness of 3D spatial reasoning capabilities of LMMs by analyzing the performance gap between the two splits with images from the same 3D scene but from common and uncommon viewpoints. We find that LMMs does not generalize well to images with 6D camera viewpoints less represented in their training set. See Sec. 4.4 for detailed discussions. failure examples of GPT-4o in Fig. 4, showing how GPT-4o is capable of predicting the correct answer when prompting with an image from common viewpoint but fails when being asked about the same question but the image is rendered from an uncommon viewpoint of the exact same scene. We attribute such degraded performance on uncommon viewpoints to two factors: (i) shift in camera 6D viewpoint distribution between training data and images in our 3DSRBench-synthetic-uncommon, and (ii) state-of-the-art LMMs adopt an implicit representation of 3D scenes. Current LMMs are heavily built on the scaling law of datadriven approaches. By leveraging large-scale data for pretraining, alignment, and instruction tuning, models develop an implicit 3D awareness and enable subsequent 3D spatial reasoning capabilities. Despite their success on wide range of academic and empirical benchmarks, our results demonstrate that state-of-the-art LMMs, open-sourced or proprietary, face severe challenges generalizing to less represented data, which in our case, are images from uncommon camera 6D viewpoints. Notably, although images from these uncommon viewpoints are scarce in current datasets, they are in fact very common for specific downstream tasks, such as autonomous navigation and robotic manipulation, where the cameras are often placed on robots rather than held in hand by humans. Degraded performance of current LMMs would largely limit their potential in these areas. Figure 5. Failure cases of GPT-4o on our 3DSRBench. We find that GPT-4o cannot perform rigorous 3D spatial reasoning and resort to visual cues for reasoning. The bottom two examples show that GPT-4o cannot perform certain complicated 3D spatial reasoning and would produce short and succinct guess instead. 4.5. Failure Cases 5. Conclusions We present some failure cases of GPT-4o in Fig. 5. We make the following two observations: (1) GPT-4o cannot perform rigorous 3D spatial reasoning and resort to various visual cues for reasoning. For the top figure, GPT-4o fail to estimate rough distance of the two objects, while relying on empirical facts that objects closer to the camera demonstrate more detailed appearances. For the second figure GPT-4o represent orientations with facing toward approaching traffic, which is vague and ineffective orientation representation, leading to the false predicted answer. (2) GPT-4o cannot perform certain complicated 3D spatial reasoning. When being asked about complicated 3D spatial reasoning questions, e.g., ones that require 3D spatial reasoning over 3D locations and orientations, GPT-4o would give short and succinct answer without complex reasoning. This can further serve as criteria to study the decision making of GPT-4o, e.g., when GPT-4o skip reasoning and predict answer directly, and analyze its weakness. In this work we study the 3D spatial reasoning capabilities of LMMs. We present new benchmark, 3DSRBench, by manually annotating 2,100 visual question-answer pairs on MS-COCO images, featuring diverse and open-vocabulary entities, such as logos on car or arrows on billboard, and balanced data distribution for robust evaluation. To study the robustness of 3D spatial reasoning capabilities w.r.t. camera 6D viewpoints, we further annotate 672 visual question-answer pairs on synthetic multi-view images, each with common and an uncommon camera viewpoint. Our benchmark further serves as an important diagnosis benchmark for developing 3D spatially intelligent LMMs, shedding light on downstream tasks that require 3D spatial reasoning. Experimental results on different splits of our 3DSRBench provide valuable findings and insights that will benefit future research on 3D spatially intelligent LMMs."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1, 6, 7 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 2022. 1 [3] Anthropic. https : / / www . Claude 3.5 Sonnet. anthropic.com/news/claude-3-5-sonnet. 1, 7 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 7 [5] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, et al. Arkitscenes: diverse real-world dataset for 3d indoor scene understanding using mobile rgb-d data. arXiv preprint arXiv:2111.08897, 2021. 3 [6] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. Omni3D: large benchmark and model for 3D object detection in the wild. In CVPR, 2023. 1, [7] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In CoRL, 2023. 1 [8] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In CVPR, 2020. 3 [9] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In CVPR, 2024. 3, 4 [10] Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, and Alan Yuille. Efficient large multi-modal models via visual context compression. In NeurIPS, 2024. 1 [11] Jieneng Chen, Qihang Yu, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Vitamin: Designing scalable vision models in the vision-language era. In CVPR, 2024. 2 [12] Zhenfang Chen, Kexin Yi, Yunzhu Li, Mingyu Ding, Antonio Torralba, Joshua Tenenbaum, and Chuang Gan. Comphy: Compositional physical reasoning of objects and events from videos. arXiv preprint arXiv:2205.01089, 2022. 3 [13] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision-language models. In NeurIPS, 2024. 1, 3 [14] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2023. 2 [15] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An opensource toolkit for evaluating large multi-modality models, 2024. 1 [16] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 2 [17] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In CVPR, 2024. 1, 2, 3 [18] Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, and Michael J. Black. Chatpose: Chatting about 3d human pose. In CVPR, 2024. 3 [19] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, 2012. [20] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In CVPR, 2017. 1, 3 [21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. 2, 6, 7, 1 [22] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023. 3 [23] Wenlong Huang, Chen Wang, Yunzhu Li, Ruohan Zhang, and Li Fei-Fei. Rekep: Spatio-temporal reasoning of relational keypoint constraints for robotic manipulation. arXiv preprint arXiv:2409.01652, 2024. 3 [24] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019. 1 [25] Mukul Khanna, Yongsen Mao, Hanxiao Jiang, Sanjay Haresh, Brennan Shacklett, Dhruv Batra, Alexander Clegg, Eric Undersander, Angel Chang, and Manolis Savva. Habitat synthetic scenes dataset (hssd-200): An analysis of 3d scene scale and realism tradeoffs for objectgoal navigation. In CVPR, 2024. 2, [26] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An opensource vision-language-action model. In CoRL, 2024. 1 [27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 2, 6, 7, 1 [28] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with [41] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua Susskind. Hypersim: photorealistic synthetic dataset for holistic indoor scene understanding. In ICCV, 2021. 3 [42] Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. Sun rgb-d: rgb-d scene understanding benchmark suite. In CVPR, 2015. 3 [43] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 1, 6 [44] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian1: fully open, vision-centric exploration of multimodal llms. In NeurIPS, 2024. 1, 3, 6, [45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2 [46] Xingrui Wang, Wufei Ma, Zhuowan Li, Adam Kortylewski, and Alan Yuille. 3d-aware visual question answering about parts, poses and occlusions. NeurIPS, 2024. 1, 3 [47] Xingrui Wang, Wufei Ma, Angtian Wang, Shuo Chen, Adam Kortylewski, and Alan Yuille. Compositional 4d dynamic scenes understanding with physics priors for video question answering. arXiv preprint arXiv:2406.00622, 2024. 1, 3 [48] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. 1 [49] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: strong arXiv preprint arXiv:2306.03514, image tagging model. 2023. 1 [50] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. NeurIPS, 2023. 2, 7 In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. [29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence In Zitnick. Microsoft coco: Common objects in context. ECCV, 2014. 2, 4, 6, 1 [30] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023. 2, 6, 7, 1 [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. 1, 6, 7 [32] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 2, 6, 7 [33] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? In ECCV, 2025. 3, [34] Taiming Lu, Tianmin Shu, Alan Yuille, Daniel Khashabi, and Jieneng Chen. Generative world explorer. arXiv preprint arXiv:2411.11844, 2024. 1 [35] Wufei Ma, Kai Li, Zhongshi Jiang, Moustafa Meshry, Qihao Liu, Huiyu Wang, Christian Häne, and Alan Yuille. Rethinking video-text understanding: Retrieval from counterfactually augmented data. In ECCV, 2024. 1 [36] Wufei Ma, Guanning Zeng, Guofeng Zhang, Qihao Liu, Letian Zhang, Adam Kortylewski, Yaoyao Liu, and Alan Yuille. Imagenet3d: Towards general-purpose object-level 3d understanding. arXiv preprint arXiv:2406.09613, 2024. 1, 2, 3 [37] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, Karmesh Yadav, Qiyang Li, Ben Newman, Mohit Sharma, Vincent Berges, Shiqi Zhang, Pulkit Agrawal, Yonatan Bisk, Dhruv Batra, Mrinal Kalakrishnan, Franziska Meier, Chris Paxton, Sasha Sax, and Aravind Rajeswaran. Openeqa: Embodied question answering in the era of foundation models. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [38] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 2, 6, 7, 1 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 6, [40] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 6, 7 10 3DSRBench: Comprehensive 3D Spatial Reasoning Benchmark"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 6. Distribution of closed-form answers in our 3DSRBench. swers rather than choosing from two given open-vocabulary entities. C. Example Questions in 3DSRBench We present two example questions for each of the 12 question types in Fig. 7 (height and location questions), Fig. 8 (orientation questions), and Fig. 9 (multi-object reasoning questions). A. Baseline LMMs Proprietary LMMs. To analyze the 3D spatial reasoning capabilities of state-of-the-art LMMs, we explore variety of proprietary LMMs, e.g., Qwen-VL, Claude, Gemini, and GPT. We follow Duan et al. [15] and formulate all questions as multiple choice questions with circular evaluation enabled. LLaVA-v1.5 [30]. LLaVA-v1.5-7B is strong opensourced LMM baseline built on Vicuna-v1.5 LLM and CLIP-ViT-L vision encoder. It extends the visual instruction tuning framework [31] with an MLP connector and scaled up image resolution. Cambrian-1 [44]. Cambrian-1 is strong LMM with It features an advanced connector vision-centric designs. design, spatial vision aggregator (SVA), and high-quality visual-instruction tuning data. LMMs with various vision encoder designs. We further experiment on family of LMMs, extending the LLaVAv1.5 baseline with various visual encoders design. We study the impact of the 3D awareness of visual encoders on the final 3D spatial reasoning capabilities. We adopt the same LLM and training strategy, exploring: (i) different mixed encoders: involving second visual encoder besides CLIP, e.g., DINOv2 [38], MAE [21], and SAM [27]; and (ii) different visual connectors: standard MLP connector and spatial vision aggregator (SVA) [44]. B. Data Statistics As discussed in Sec. 3, our 3DSRBench consists of three splits, real split with 2,100 visual question-answer pairs manually annotated on diverse and open-vocabulary objects from the COCO dataset [29], and two synthetic splits synthetic-common and synthetic-uncommon with 336 visual question-answer pairs in each split, featuring same questions on 3D scene rendered from common and an uncommon viewpoint. Our 3DSRBench consists of 12 question types from four main categories, as demonstrated in Figure 1. All question types are equally distributed, e.g., 175 questions for each question type in 3DSRBench-real. To show that our 3DSRBench is balanced in answers, we visualize the distribution of closed-form answers in Fig. 6, i.e., one of the four directions (front, left, etc.) or yes/no an1 Figure 7. Two example questions for each of the 12 question types (part I): height and location questions. Figure 8. Two example questions for each of the 12 question types (part II): orientation questions. 2 Figure 9. Two example questions for each of the 12 question types (part III): multi-object reasoning questions."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "DEVCOM Army Research Laboratory",
        "Johns Hopkins University"
    ]
}