{
    "paper_title": "Ultra-Sparse Memory Network",
    "authors": [
        "Zihao Huang",
        "Qiyang Min",
        "Hongzhi Huang",
        "Defa Zhu",
        "Yutao Zeng",
        "Ran Guo",
        "Xun Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "It is widely acknowledged that the performance of Transformer models is exponentially related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms traditional models. In our experiments, we train networks with up to 20 million memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within a given computational budget."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 1 ] . [ 1 4 6 3 2 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "ULTRA-SPARSE MEMORY NETWORK Zihao Huang, Qiyang Min, Hongzhi Huang, Defa Zhu, Yutao Zeng, Ran Guo, Xun Zhou Seed-Foundation-Model Team, ByteDance {huangzihao.notabot,minqiyang,huanghongzhi.51,zhudefa, yutao.zeng,guoran.94,zhouxun}@bytedance.com"
        },
        {
            "title": "ABSTRACT",
            "content": "It is widely acknowledged that the performance of Transformer models is exponentially related to their number of parameters and computational complexity. While approaches like Mixture of Experts (MoE) decouple parameter count from computational complexity, they still face challenges in inference due to high memory access costs. This work introduces UltraMem, incorporating large-scale, ultra-sparse memory layer to address these limitations. Our approach significantly reduces inference latency while maintaining model performance. We also investigate the scaling laws of this new architecture, demonstrating that it not only exhibits favorable scaling properties but outperforms traditional models. In our experiments, we train networks with up to 20 million memory slots. The results show that our method achieves state-of-the-art inference speed and model performance within given computational budget."
        },
        {
            "title": "INTRODUCTION",
            "content": "(a) Validation loss (b) Inference time (c) Memory access Figure 1: We ensured that three models have the same computation, and MoE and UltraMem have the same parameters. The x-axis is plotted on logarithmic scale. In (b) and (c), the sequence length is 1 because during decoding time, we can only predict one token at time, and the key/value cache length is 2048. The experiments in (b) and (c) are conducted on the A100-SXM-80GB. Recent advancements in natural language processing (NLP), driven by Large Language Models (LLMs) (Radford et al., 2019; Brown, 2020), require exponentially more computational resources as they scale, posing challenges in resource-limited environments like real-time applications. To address computational issues, the Mixture of Experts (MoE)(Fedus et al., 2022; Jiang et al., 2024) and Product Key Memory (PKM)(Lample et al., 2019) have been introduced. MoE selectively activates parameters, boosting training efficiency but impairing inference time due to increased memory access. PKM maintains consistent memory access with fewer value embeddings but its performance is significantly worse than MoE. As shown in Figure 1(b), an MoE model, despite having the same computational cost and twelve times more parameters than dense model, runs 2 to 6 times slower in inference, varying by batch size. These authors contributed equally to this work."
        },
        {
            "title": "Preprint",
            "content": "This slowdown, as depicted in Figure 1(c), stems from high memory access demands, highlighting its inefficiency in inference scenarios. The primary challenge is how to match or even surpass the effectiveness of the MoE model while maintaining memory access levels comparable to those of dense models. In this paper, we introduce UltraMem, an architecture that builds upon and extends the concepts from PKM. UltraMem incorporates large-scale, ultra-sparse memory layers that significantly enhance computational efficiency and reduce inference latency while maintaining or even improving model performance across various benchmarks. This architecture not only supports the deployment of highly effective language models in resource-constrained environments but also opens up new avenues for constructing even larger models without the previously associated prohibitive costs. In summary, we make the following contributions: 1. UltraMem is greatly enhanced compared to PKM, and outperforms MoE at same scale. Compared to PKM, UltraMem truly possesses the prerequisites for training large-scale models on extensive computational resources and has undergone comprehensive experimental validation. 2. UltraMem has significantly lower memory access cost during inference compared to MoE. Under common inference batch sizes, it can be up to 6 times faster than MoE with the same parameters and calculations. The inference speed of UltraMem is almost identical to that of dense model with equivalent computational resources. 3. We have verified the scaling ability of UltraMem. Similar to MoE, UltraMem has strong scaling ability, and we have observed stronger scaling ability than MoE."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Mixture of Expert. Shazeer et al. (2017) proposed MoE and Fedus et al. (2022) introduced the MoE in large language models, where each token selects one expert for inference each time, thereby increasing model parameters without increasing computation. Rajbhandari et al. (2022) introduced the concept of shared experts, where each token utilizes some fixed experts along with some unique experts. Subsequent research has focused on improving the gating functions of MoE, including token choice (Chi et al., 2022), non-trainable token choice (Roller et al., 2021) and expert choice (Zhou et al., 2022), primarily to address the issue of expert imbalance. Liu et al. (2024); Dai et al. (2024) opted to slice the experts into smaller segments while activating more experts per token, achieving significant performance improvements. Concurrent study (Krajewski et al., 2024) meticulously explored the benefits of granularity and increasing the number of experts, alongside investigating the scaling laws associated with MoE. In this paper, we use fine-grained MoE as our baseline, wherein the granularity of the MoE is set to 2. This means that each expert is half the size of the original MultiLayer Perceptron (MLP) , with two experts activated per token. Large Memory Layer. Lample et al. (2019) first introduced the concept of large memory layer, called PKM, which can be seen as slicing the MoE experts to the smallest possible configuration. Kim & Jung (2020) introduced concept similar to shared experts in MoE, allowing PKM and MLP to operate in parallel. Csordas et al. (2023) made slight modification to PKM by removing the Softmax operation. PEER (He, 2024) improved the activation of values in PKM to activate small expert with an inner dimension of 1, achieving significant performance gains. However, current research on PKM is limited to smaller models, and even the latest improved versions of PKM only outperform MoE in certain scenarios. Additionally, current PKM do not possess characteristics suitable for large-scale training. We address these issues in this paper. Tensor decomposition breaks down tensor into series of small matrices or tensors. In deep learning research, such methods are commonly used to approximate large tensor during training, aiming to save on computation and parameters. Product quantization (Jegou et al., 2010) breaks vector into multiple sub-vectors, allowing us to reconstruct the original vector using smaller number of sub-vectors, thereby reducing the model parameters. Bershatsky et al. (2024) initializes several matrices and core tensor, trains these parameters during the fine-tuning phase, and reconstructs the original large tensor in manner of Tucker Decomposition at the end of training to reduce training costs. We borrow this insight to improve PKMs key retrieval."
        },
        {
            "title": "Preprint",
            "content": "(a) Multilayer perceptron (b) Large memory layer Figure 2: An overview of multilayer perceptron (MLP) and large memory layer (LML). For the sake of brevity, we omit the third top-m operation from memory layer. An MLP typically consists of two linear layers and GeLU activation. We consider the weights of the first linear layer as keys, and those of the second linear layer as values. LML uses row and column keys to determine the 2-D logical address to index memory values, whereas MLP uses 1-D logical address. fetch value refers to retrieving values based on the indices with higher scores."
        },
        {
            "title": "3 ULTRAMEM",
            "content": "3.1 PRELIMINARY Here we firstly introduce the origin large memory layer (LML) based on product keys, which serves as the foundation for our proposed approach. The concept of product key-based memory layer (PKM) was first explored in prior work (Lample et al., 2019). In their approach, the authors incorporated an external memory module into language models, with the goal of expanding the models parameters while maintaining similar level of computational complexity. The overall structural diagram is depicted in Figure 2(b). memory layer generally consists of two parts: keys RN Dk and values RN Dv . To retrieve information from memory values, query vector RDk finds most relevant values by multiplying keys to obtain scores. The higher the scores are, the better impact should the values have. Consequently, this process can be formulated as: = σ(Kq) = Vs, (1) where is the scores, σ is non-linear activation, is the output. Attention layers, who memorize context contents, and MLP layers, who memorize world knowledge, also follow the above formulation with σ being SoftMax in attention layers and GeLU in MLP layers (Geva et al., 2020) (see Figure 2(a)). Product-key memory layers scale up the memory size with > 106, while activating only few values with top-m scores. Here, is hyper-parameter controlling sparsity. Though values are sparsely accessed, the keys, which are as large as values, must be fully computed to obtain scores before top-m activation following equation 1. To alleviate the computation complexity for keys, product keys are proposed. It utilizes 2-D logical address (see Figure 2(b)), typically grid where = , for memory value retrieval. Specifically, 2-D logical address (i, j) is used to index memory value at physical address + j. With such strategy, logical scores are then represented as matrix, which is further decomposed as an addition of row and column scores: srow = σTopM(Krowqrow(x)), Sgrid = σTopM(srow + scol = σTopM(Kcolqcol(x)), col), = SoftMax(vec(Sgrid)), (2) (3) where Krow, Kcol RnDk , qrow, qcol : RDi RDk convert input hidden RDi to row and column query, σTopM() preserves top-m largest elements in the input and set the rest to negative infinity, and the matrix addition with unmatched matrix shape is implemented by element broadcasting. It should be noted that removing σTopM from equation 2 does not make any difference. The only"
        },
        {
            "title": "Preprint",
            "content": "reason for applying top-m to the row and column scores is to reduce the computation for the last top-m operation on Sgrid. As srow, scol have only activated scores, Sgrid has only m2 candidates for top-m operation rather than , i.e., top-m complexity reduces from O(N log m) to O(( + m2) log m). Note that the Sgrid undergoes SoftMax operation akin to the one employed in the self-attention mechanism. Moreover, PKM adopts the multi-head mechanism from the self-attention module, wherein it utilizes multiple key sets to retrieve the shared values, we denote as the number of PKM heads."
        },
        {
            "title": "3.2 STRUCTURE IMPROVEMENTS",
            "content": "Improve PKM with bag of tricks. We first studied the structure of PKM and found that series of minor adjustments can steadily improve the models performance: 1) We remove the operation Softmax in equation 3, which is well-established in the studies (Shen et al., 2023; Csordas et al., 2023). 2) We conduct Layer Normalization (LN) (Ba et al., 2016) on query and keys for stability of training. 3) PKM suggests using constant learning rate of 0.001 to learn the values, which is much higher than the learning rate for other parameters. We found that gradually decaying the value learning rate provides further benefits. 4) PKM uses linear layer to generate query, we add causal depthwise convolutional layer (Howard, 2017) before this linear layer to enhance query. 5) Similar to Group Query Attention (Ainslie et al., 2023), we share query in two key sets. This can reduce the computational cost of generating the query by half, with little performance impact. 6) By halving Dv, we double the number of values. Under the condition of keeping the activation value parameter unchanged, we increased the diversity of activated values, and the model effect is further improved. In order to make the output consistent with hidden dimension, we add linear layer on the aggregated output. UltraMem Overall structure. We then take deeper investigation into the model structure and propose UltraMem. Figure 3 shows the PKM and our improved UltraMem structure, based on Pre-LayerNorm Transformer architecture. PKM replaces MLP or operates in parallel (Kim & Jung, 2020) with MLP in the one of deeper layers with memory layer. We notice three drawbacks to PKM: 1. As value size significantly increases, queries can harder find correct values. 2. Product key decomposition introduces bias on retrieval topology. For example, let (i, j) be the logical address for the top1 score, then top-2 score must be located on row or column j, which significantly limits the diversity of top-m selection. 3. There are issues with unbalanced multiGPU computation and communication during large-scale parameter training, as the full model parameters cannot be placed on single GPU. To alleviate problems 1 and 3, we decompose this large memory layer into multiple smaller memory layers distributed at fixed intervals across the transformer layers. Additionally, this skip-layer structure allows us to overlap the execution of the memory layer and the transformer layers, as the memory layer is predominantly memory-bound during training. 4 Figure 3: Overall of PKM and UltraMem."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Flow of Tucker Decomposed Query-Key Retrieval, here = 2. fetch refers to retrieving score based on given index. Tucker Decomposed Query-Key Retrieval (TDQKR). We explore more complex multiplicative approach to alleviate problem 1 and 2, where tucker decomposition (Malik & Becker, 2018) is adopted in place of product quantization. The whole process of TDQKR is illustrated in Figure 4. Specifically, tucker decomposition estimates grid scores with rank-r matrix multiplication: Scol = Kcolqcol(x), (4) Srow = Krowqrow(x), Sgrid = σTopM(S row Scol), (5) where Srow, Scol Rrn and Rrr is the tucker core, which is learnable parameter with random initialization. To produce shaped row and column score, the dimensions of the query and key are reshaped, resulting in Krow, Kcol Rrn(Dk/r) and qrow, qcol Rr(Dk/r), corresponding to Figure 4 step 1. However, equation 5 is inefficient to be directly applied in practice, as the top-m operation cannot be simplified with an equivalent two-phase top-m technique like product quantization can. As consequence, we propose an approximated top-m algorithm to tackle this problem. The key is to do rank-1 approximation for the tucker core, so that the overall top-m can be approximated by: σTopM(S ut, row Scol) σTopM((uSrow) (tScol)) (6) where u, Rr1. Note that (uSrow), (tScol) R1n are row vectors, then the two-phase topm technique pertains to the approximated objective σTopM((uSrow) (tScol)), corresponding to Figure 4 step 3. Overall, we conduct approximated top-m on row and column scores, filtering out non-top elements, then we use the concrete objective in the final top-m operated on Sgrid, keeping index scores precise: ut Srow =ITopM(uSrow) Srow Scol =ITopM(tScol) Scol Sgrid =σTopM(S row Scol), (7) (8) (9) (10) where ITopM() is binary value function, which converts top-m elements to 1 and otherwise to 0. Equation 8&9 corresponding to Figure 4 step 4&5,and Equation 10 corresponding to Figure 4 step 6&7. As for the rank-1 approximation, we leverage Singular Value Decomposition (SVD) (Abdi, 2007) to factorize the tucker core with u, be the left and right singular vectors corresponding to the leading singular value, corresponding to Figure 4 step 2. Last but not the least, the approximation error should be concerned when non-maximum singular values are as large as the maximum one. To mitigate this, an auxiliary loss that manages approximation"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Flow of Implicit Value Expansion, here = 4, = 16. error is introduced during training by constraining non-maximum eigenvalues: = UΛT, (by SVD) Laux = α 1 (cid:88) i=2 (max (0, λi τ ))2 , (11) (12) where, Λ denotes the singular values for in descending order, with τ serving as margin to prevent from degenerating into rank-1 matrix, and α is the coefficient for the loss. Implicit Value Expansion (IVE). Though sparsely used, maintaining large memory table is still costly during training due to the large amount of memory access. To reduce memory access as well as scale up the memory size, we propose virtual memory as an implicit value expansion. Given virtual expansion rate > 1, virtual memory expands the memory table into times size. We design virtual memories as multiple reparameterizations of the original memory value V, which we denote as physical memory. Then, linear projectors {Wpp [1, E], Wp RDvD } are utilized, and virtual memory block Vp corresponding to the p-th reparameterization can be defined as: Vp = VWp. (13) ]. is not necessarily consistent with the dimension of physical Then the overall virtual memory is concatenation of the virtual blocks = [ Note the dimension of virtual values values Dv. To apply the virtual memory is intuitive, where memory table can be replaced from to V. And times. Moreover, we suggest random to fit virtual memory size, the key size is expanded by shuffle for virtual memory to eliminate some unnecessary index topology prior introduced by row and column scoring. Concretely, if the virtual memory tables are unioned by concatenation, each memory value and its expansions would be located in the same column in logical address, and thus can be potentially more frequently chosen simultaneously. 1 , . . . , 0 , A naive reparameterization for virtual memory still introduces lots of computations, which is Dv v, and times GPU memory access. better idea is to compute reparameterization on demand. That is, we expand the logical address to triplets (i, j, p) where (i, j) is the original logical address and is index for the virtual memory block, and then simultaneously conduct sum pooling and compute virtual memory value. Consequently, equation 3 is rewritten as: = ˆs = ˆs =Shuffle(vec(Sgrid)), p ˆsp = (cid:88) (cid:88) (cid:0)V ˆsp (cid:1) (14) (15) p where ˆsp represents the scores corresponding to p-th virtual memory block. With equation 15, we can firstly lookup and pool values according to the virtual block index and then transform the reduced physical values directly into reduced virtual values. This trick reduces extra computation from Dv v, where is the number of tokens in batch, and has nearly no extra GPU memory access except for the linear projectors. Figure 5 shows the flow of IVE. to Dv Multi-Core Scoring (MCS). PKM shares single score across dimension Dv for each value. Empirically, assigning multiple scores to single value has shown to enhance performance. Thus,"
        },
        {
            "title": "Preprint",
            "content": "we rewrite the tucker core as series of component cores = (cid:80)h {C(i)}h i=1 to generate individual score maps S(i) rowC(i)Scol. Obviously, C(i). This allows employing Stucker = row( (cid:88) C(i))Scol = rowC(i)Scol = i (cid:88) S(i) tucker. (16) tucker = (cid:88) We keep top-m conducted on aggregated score Stucker, while applying individual scores S(i) vertically split value table = [V(1), . . . , V(h), ] with V(i) RN (Dv/h), i.e., tucker on = [ˆs(1)V(1), . . . , ˆs(h)V(h)]. (17) When this technique incorporates with IVE, we split physical memory values instead of virtual memory values to keep the equivalence in equation 15. Improved initialization. PKM initializes values with Gaussian distribution (0, 1 ). Since Dv PKM applies Softmax to the scores, the variance of the pooled outputs is 1/Dv. We argue that LML should be considered as component similar to an MLP and, therefore, should use an initialization method akin to that of MLPs. Before training, the output of an MLP typically follows Gaussian distribution (0, 1 2L ) (Brown, 2020), where is the total number of layers. We initialize value with (0, 2mHL ), where is the activated value number, is the head number, is the value expansion times. To ensure that the output distribution of UltraMem is (0, 1 2L ), We need to confirm that the mean of top-m score is 1, details see Appendix A."
        },
        {
            "title": "4 QUANTITATIVE ANALYSIS WHY ULTRAMEM INSTEAD OF MOE",
            "content": "The most effective method for enhancing model capacity without significantly raising computational costs is MoE. This strategy employs set of specialized sub-models, known as experts, which work together to tackle complex problems. However, the MoE model poses challenges for inference processes. Consider the Transformer hidden dimension as D, the inner dimension of MLP is 4D, given the inference batch size as B. Using the MoE with 2inNmoe (choose 2 in Nmoe experts per token) as an example, where the inner dimension of expert is 2D. Assuming the expert chosen is fully balanced, we can get the memory access of single MoE layer as min(2B, Nmoe) 2D2. For the UltraMem, assuming value dimension is D/2, and each token activates the top-m values, then its memory access is min(Bm, ) D/2. As the batch size increases, the memory access of MoE grows rapidly until it reaches an upper limit where all expert parameters need to be accessed. In contrast, the memory access of UltraMem increases very slowly, only reaching parity with MoE when the batch size is in the tens of thousands. However, in inference scenarios, the batch size is typically not very large. Figure 1 shows the inference time and memory access of 1.6 billion parameter Transformer with 2in34 MoE and 121 UltraMem. For larger batch sizes, see Figure 7 in Appendix. Compared to MoE, UltraMem achieves the maximum acceleration of 6 at batch size of 64, and also shows significant acceleration at other batch sizes."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we demonstrate the scaling capabilities of UltraMem, showing that it outperforms MoE. We additionally show how the performance of UltraMem varies with different top-m values and the number of parameters, and perform an ablation study to measure the impact of each part of UltraMem. 5.1 SETUP Datasets. Training data comes from RedPajama (Computer, 2023), containing 1 trillion tokens. RedPajama represents clean-room, fully open-source version of the LLaMa (Touvron et al., 2023) 1The number of parameters in UltraMem is 12 times the number of parameters in the dense layer. In this case, the total parameters and total computation of UltraMem are the same as the 2in34 MoE."
        },
        {
            "title": "Preprint",
            "content": "dataset. Validation data includes the C4 validation set (Raffel et al., 2020), derived from the Common Crawl web corpus. The C4 training set is also incorporated within the RedPajama training data. Tokenizer is based on the GPT-NeoX (Black et al., 2022) tokenizer, which uses the Byte-Pair Encoding (BPE) (Sennrich et al., 2015) algorithm and has vocabulary size of 50,432. Evaluation. We conducted comprehensive evaluation of all models across ten benchmark datasets. These datasets included MMLU, Trivia-QA, GPQA, and ARC for assessing the models knowledge capabilities; BBH, BoolQ, HellaSwag, and WinoGrande for evaluating reasoning skills; DROP for testing reading comprehension abilities; and AGIeval for measuring overall model performance. The decoding hyperparameters are aligned with those of LLaMA3 (Dubey et al., 2024). Detrails see Appendix E. Training details. We used standard pre-norm transformer (Xiong et al., 2020) with rotary embeddings (Su et al., 2024). Our dense models are built with 151M2, 680M, 1.6B, and 6.5B parameters. For sparse models, including UltraMem and MoE, we expand the sparse parameters twelve fold from the 151M, 680M, and 1.6B dense models. In MoE models, two experts are activated per token (Jiang et al., 2024), and we use standard balance loss (Fedus et al., 2022) with weight of 0.01 to ensure an even selection of all experts, thereby establishing MoE as strong baseline. We slightly increase the width of MoEs experts to align the computational and parameter costs with those of the UltraMem. In UltraMem models, auxiliary loss weight α = 0.001, margin τ = 0.15, the learning rate for values is ten times that of other parameters and linearly decays to equal the other parameters by the end of training. For details on model structure and hyperparameters, see Appendix E. For details on the optimizations made for large-scale training, please see the Appendix C,D. 5.2 EVALUATION ON LANGUAGE MODELING DATASETS We evaluate models of various sizes, the results are shown in Table 13, where FLOPs is the computation cost of single token, the curves showing changes over the course of training are provided in the Figure 11 in Appendix. We observe that as the model capacity increases, UltraMem can outperform MoE with the same parameter and computation. On the 1.6B dense model, an UltraMem model with 12x the parameters can match the performance of 6.5B dense model. Table 1: Performance metrics of various models Model Param FLOPs Val. GPQA TriviaQA BBH Hella Wino DROP Avg Dense-151M MoE-151M-2in32 UltraMem-151M-x12 Dense-680M MoE-680M-2in33 UltraMem-680M-x12 (B) 0.15 2.04 2.03 0.68 8.95 8.93 Dense-1.6B MoE-1.6B-2in34 UltraMem-1.6B-x12 1.61 21.36 21.41 (G) loss cot Swag Grande 0.30 0.35 0.35 1.36 1.50 1.49 3.21 3.52 3.50 2.96 2.63 2.67 2.64 2.39 2. 2.49 2.30 2.24 19.98 17.30 19.42 21.09 20.54 21.99 21.76 21.32 24.66 12.67 33.27 28.97 27.16 34.19 55. 39.65 59.56 66.38 22.57 35.07 23.24 48.44 22.65 43.96 24.65 48.83 26.63 62.71 26.62 64.15 26.41 58.6 29.46 67.34 30.63 71.52 52.49 55.96 50.83 54.93 59.98 60. 61.72 63.93 66.38 13.60 26.06 18.57 33.20 14.08 29.99 22.97 33.27 26.54 38.43 25.14 42.27 22.63 38.46 28.81 45.07 29.99 48.26 Dense-6.5B 6. 12.88 2.30 19.98 57.28 31.14 69.73 65. 33.12 46.19 5.3 VALUE NUMBER AND TOP-m In most sparse LLMs, such as MoE and UltraMem, there is clear positive correlation between sparsity and model performance. Therefore, in this section, we conduct series of scaling experiments 2Parameter counts in this paper exclude tokenizer vocabulary embedding and prediction head parameters. 3This table only includes evaluation results where the metrics have steadily increased with training. For all results, see the Table 7 in Appendix."
        },
        {
            "title": "Preprint",
            "content": "(a) Scaling validation loss (b) Loss across varying sparsity (c) Speed across varying sparsity Figure 6: (a). C4 validation loss of different models at different scale. (b). Scaling curves at different sparsity with 151M activated parameters. Each line represents the same model sparsity; e.g., 20K indicates that approximately one out of every 20,000 values will be activated. The loss decreases linearly as the sparse parameters increase exponentially. (c). Inference time for UltraMem and MoE with 1.6B activated parameters. The batch size is 512, sequence length is 1, and key/value cache length is 2048. With fixed activation parameters, UltraMems inference time remains nearly constant as sparse parameters increase, while MoEs inference time increases significantly. by varying selected top-m and value number, i.e., the parameters of the sparse modules, to verify the changes in model performance with respect to sparsity. The result is shown in Figure 6(b). It is evident that, at the same level of sparsity, the validation loss decreases as the number of parameters increases and can maintain certain degree of decline. Additionally, the smaller the sparsity, i.e., the larger the proportion of activated parameters, the better the model performance. However, this also results in higher memory access overhead. Thus, there is trade-off between memory access volume and scaling efficiency. In our final experiments, we selected sparsity ratio of 80K as the default model configuration. As sparse parameters increase, we observed differences in inference speeds between UltraMem and MoE, as depicted in Figure 6(c). For UltraMem, the inference time remains largely stable even as sparse parameters exponentially grow, provided that the activation parameters (top-m) are constant. In contrast, MoEs inference time escalates significantly under analogous conditions. Referring to Figure 1(b), with smaller batch sizes, the inference speed of MoE deteriorates even further compared to UltraMem. 5.4 ABLATION We conduct comprehensive ablation studies based on the 151M dense model. In the baseline, the PKM is version that operates in parallel with the MLP, making it stronger baseline. For this group of experiments, the learning rate (LR) is set to 1.2e-4, with training on 500B tokens and evaluating the cross entropy loss on the training and C4 validation sets. We ensure that the parameter count and computational cost of the final version of the model were essentially at the same level. Table 2 shows the ablation results. We identify 6 changes that significantly improved performance: 1. Doubling the number of values while halving their dimension, and simultaneously double the top-m selections to keep the active parameters consistent. 2. Splitting single UltraMem into multiple smaller units evenly across the transformer layers, with outputs skipping several blocks. This arrangement keeps the total parameter count, computational cost, and sparse parameter activation at or below pre-split levels. 3. Tucker Decomposition Query-Key Retrieval introduces negligible additional parameters while reducing computation, here = 2."
        },
        {
            "title": "Preprint",
            "content": "Table 2: Ablation study of model improvements Training Loss Validation Loss Dense Params(M) Sparse Params(G) FLOPs (M) PKM-151M-x10 +rm softmax +half vdim+proj +share query +split big mem&skip +query/key LN +IVE +TDQKR +MCS +improved init +value lr decay +query conv 2.604 2.570 -0.034 2.556 -0.014 2.560 +0.004 2.554 -0.006 2.553 -0.001 2.544 -0.009 2.538 -0.006 2.521 -0.017 2.518 -0.003 2.494 -0.024 2.493 -0.001 2.828 2.822 -0.006 2.800 -0.022 2.803 +0.003 2.788 -0.015 2.789 +0.001 2.772 -0.017 2.764 -0.008 2.761 -0.003 2.758 -0.003 2.736 -0.022 2.736 -0.000 Total Diff -0.111 -0. 173.01 173.01 178.47 173.46 161.64 161.64 172.37 172.37 172.37 172.37 172.37 172.38 -0.64 1.534 1.534 1.529 1.529 1.536 1.536 1.536 1.536 1.536 1.536 1.536 1.536 +0.002 346.06 346.06 356.98 346.96 323.32 323.54 344.98 344.98 344.98 344.98 344.98 345.02 -1. 4. Multi-Core Scoring significantly reduces training loss, and slightly reduces validation loss, here = 2. 5. Implicit Value Expansion slightly increases both the parameter count and computational cost, but the improvement is significant, here = 4. 6. The LR for the value parameters starts at ten times that of the other parameters and linearly decays to match them by the end of training. Among other changes, sharing the query helps cut computational costs with minor trade-off in performance. Normalizing the query/key greatly reduces spikes in training perplexity and enhances training stability, as shown in Figure 10(a). Improved initialization prevents score and output variance explosions in the early to middle training stages, detailed in Figure 10(b) and (c). Additionally, employing convolution further limits the variance divergence in UltraMem outputs(Figure 10.(c)). Beside, We conduct another ablation studies on IVE, TDQKR, and MCS with different configurations, which are documented in Table 3. For IVE, as increases, there is consistent improvement in model performance alongside notable increase in computational cost. However, the marginal gains decrease as rises, leading us to recommend = 4. For TDQKR and MCS, increasing and does not significantly change the computational load, but the effectiveness no longer shows marked improvement, hence we suggest using = 2 and = 2. Table 3: Ablation of different config on IVE, TDQKR, and MCS IVE TDQKR MCS Training loss Validation loss FLOPs(G) Baseline E=4 E=9 -0.009 -0.016 -0.017 -0.025 h=8 -0.012 2.553 2.789 +0.006 323.54 +6.6% +14.9% +26.4% 344.98 +0.001% +0.002% +0.003% 344.98 +0.001% +0.003% +0.007% E=16 Baseline -0.019 -0.027 Baseline 2.538 2. r=3 -0.0065 -0.0084 r=4 -0.0063 -0.0082 h=4 -0.017 +0.001 r=2 -0.006 -0.008 h=2 -0.017 -0.003 2.544 2."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we introduce UltraMem, which, compared to MoE, has minimal memory access and therefore achieves up to sixfold speed advantage. Concurrently, in terms of performance, UltraMem surpasses MoE with the same parameters and computation as model capacity increases, indicating its superior scaling capability. This work presents promising direction for developing more efficient and scalable language models."
        },
        {
            "title": "REFERENCES",
            "content": "Herve Abdi. Singular value decomposition (svd) and generalized singular value decomposition. Encyclopedia of measurement and statistics, 907(912):44, 2007. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Daniel Bershatsky, Daria Cherniuk, Talgat Daulbaev, Aleksandr Mikhalev, and Ivan Oseledets. Lotr: Low tensor rank weight adaptation. arXiv preprint arXiv:2402.01376, 2024. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022. Tom Brown. Language models are few-shot learners. arXiv preprint ArXiv:2005.14165, 2020. Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, et al. On the representation collapse of sparse mixture of experts. Advances in Neural Information Processing Systems, 35:3460034613, 2022. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Together Computer. Redpajama: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Robert Csordas, Kazuki Irie, and Jurgen Schmidhuber. Approximating two-layer feedforward networks for efficient transformers. arXiv preprint arXiv:2310.10837, 2023. Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixtureof-experts language models. arXiv preprint arXiv:2401.06066, 2024. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint arXiv:1903.00161, 2019. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020. Xu Owen He. Mixture of million experts. arXiv preprint arXiv:2407.04153, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020."
        },
        {
            "title": "Preprint",
            "content": "AG Howard. Mobilenets: Efficient convolu-tional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861, 2017. Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence, 33(1):117128, 2010. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. Gyuwan Kim and Tae-Hwan Jung. Large product key memory for pretrained language models. arXiv preprint arXiv:2010.03881, 2020. Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pioro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Krol, Tomasz Odrzygozdz, Piotr Sankowski, et al. Scaling laws for fine-grained mixture of experts. arXiv preprint arXiv:2402.07871, 2024. Guillaume Lample, Alexandre Sablayrolles, MarcAurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. Advances in Neural Information Processing Systems, 32, 2019. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-ofexperts language model. arXiv preprint arXiv:2405.04434, 2024. Osman Asif Malik and Stephen Becker. Low-rank tucker decomposition of large tensors using tensorsketch. Advances in neural information processing systems, 31, 2018. Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 115, 2021. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale. In International conference on machine learning, pp. 1833218346. PMLR, 2022. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. Advances in Neural Information Processing Systems, 34:1755517566, 2021. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015."
        },
        {
            "title": "Preprint",
            "content": "Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Kai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui Wang, and Jiang Bian. study on relu and softmax in transformer. arXiv preprint arXiv:2302.06461, 2023. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pp. 1052410533. PMLR, 2020. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023. Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems, 35:71037114, 2022."
        },
        {
            "title": "A ULTRAMEM INITIALIZATION",
            "content": "We initialize value with (0, is the value expansion times. To ensure that the output distribution of UltraMem is (0, 1 need to confirm that the mean of top-m score is 1. 2kHL ), where is the activated value number, is the head number, 2L ), We Assuming the candidate score follows (0, 1), and K. We can simplify the problem as follows: Given standard Gaussian distributed random variables X1, ..., Xn, and the random variable = mean(topm(X1, ..., Xn)), find the expected value E(Y ). It is difficult to obtain an analytical solution for E(Y), so we approximate E(Y) by sampling times points from Gaussian distribution and calculating the mean of the top-m values. Then we initialize the query layer norm weight as 1/(cid:112)E(Y ), the keys layer norm weight as 1/ to ensure the expected of candidate score is 1. Dk"
        },
        {
            "title": "B INFERENCE TIME AND MEMORY ACCESS",
            "content": "Figure 7 shows that UltraMem has much slower growth in memory access compared to MoE, only aligning with MoE in terms of memory access when the batch size reaches 131,072, and it continues to have an advantage in inference speed. (a) Inference time (b) Memory access Figure 7: Inference time and memory access of Transformer, MoE and UltraMem. We ensured that three models have the same computation, and MoE and UltraMem have the same parameters. The x-axis and y-axis are both plotted on logarithmic scale. The sequence length is 1 because during inference, we can only predict one token at time, and the key/value cache length is 2048. The modes run on the A100-SXM."
        },
        {
            "title": "C MEGATRON SUPPORT FOR TRAINING EFFICIENCY",
            "content": "As memory table scales towards billions even trillions of parameters, model parallelism becomes essential to distribute model parameters and optimizer states across multiple devices to ensure they fit into device memory and are trainable within reasonable time frame. We leverages Megatrons (Shoeybi et al., 2019; Narayanan et al., 2021) 3D parallelism (pipeline parallelism, data parallelism, and tensor parallelism) for training. However, several parallelism modifications are required to support the memory table effectively. Because pipeline parallelism cannot address scenarios where single layers parameters exceed the memory capacity of single device, and tensor parallelism is typically limited to relatively small group of GPUs, making it insufficient to meet the memory tables memory requirements. Consequently, we propose sharding the memory table across combination of data parallel and tensor parallel groups or its subgroups, to ensure efficient distribution and scalability. The memory table can be partitioned either number-wise or dimension-wise. The entire process of number-wise and dimension-wise partitioning, along with their communication volume analysis and guidance on how to choose the appropriate partitioning method, is detailed in Appendix D. In our structural improvements, halving dim can simultaneously reduce the communication overhead for both number-wise and dimension-wise partitioning. However, increasing top-m will proportionally"
        },
        {
            "title": "Preprint",
            "content": "(a) Number-wise partitioning (b) Dimension-wise partitioning Figure 8: Process of Number-wise partitioning and Dimension-wise-partitioning. The weighted sum pooling step is omitted in the diagram. increase the communication overhead. Additionally, Implicit Value Expansion, due to the increase in the size of values after weighted sum pooling, will further impact the communication volume for dimension-wise partitioning. To further augment performance, several key modifications have been implemented: Fused Lookup-Reduce Operator: This newly introduced operator accelerates computations and reduces memory usage by combining the lookup and weighted sum pooling operations into single, more efficient step. Asynchronous Execution Strategy: Recognizing the benefits of cross-layer utilization of the memory layer, we have adopted an asynchronous execution strategy. This strategic choice allows for the concurrent processing of memory calculations alongside dense network operations, substantially enhancing the overall system performance. These enhancements demonstrate the efficacy of our parallelism strategy within the Megatron framework, paving the way for more efficient training of large-scale models. NUMBER-WISE AND DIMENSION-WISE PARTITION DETAILS Figure 8 shows the process of number-wise and dimension-wise partition. For number-wise partitioning, we first perform an all-to-all on indices to distribute them to the corresponding devices. After the lookup operation, the results are sent back to the original devices, then do weighted sum pooling. For dimension-wise partitioning, we need to perform an all-gather operation on indices to obtain all indices across devices. The lookup operation is then performed, dimension-wise partitioning allows the results to be sent back to each device after completing the weighted sum pooling."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Relationship between and dim for communication volume of number-wise / dimensionwise equals 1, the shaded area is number-wise / dimension-wise greater than 1 Assuming the memory table is distributed across processors, the communication volume can be described as follows: Number-wise Partitioning Communication Volume (not considering indices deduplication): All-to-all transmission of indices: sizeof (int) bs topm (P 1)/P All-to-all transmission of embeddings after lookup: sizeof (bf loat16) bs topm dim (P 1)/P Dimension-wise Partitioning Communication Volume: AllGather indices: sizeof (int) bs topm (P 1) AllGather scores: sizeof (bf loat16) bs topm (P 1) All-to-all transmission of embeddings post-lookup reduction: sizeof (bf loat16) bs dim (P 1)/P Here dim is the value dimension, bs is the batch size times sequence length. Figure 9 shows the relationship between and dim for communication volume of these two partitioning methods, helping us choose the appropriate partitioning method under fixed configuration."
        },
        {
            "title": "E EXPERIMENT SETTING",
            "content": "Table 4 displays common hyper-parameter settings for all experiments. LR stands for Learning Rate, corresponding to the values 6e-4, 2.5e-4, 2e-4, and 1.2e-4 for dense models with sizes 151M, 680M, 1.6B, and 6.5B, respectively (Brown, 2020). Regarding the insertion of UltraMem, for UltraMem151M, its 3:5/6:8/9:11, where 3:5 indicates that UltraMem input is taken from layer 3 and inserted back into the output of layer 5, and so on. For UltraMem-680M, its 3:7/8:12/13:17/18:22. For UltraMem-1.6B, its 3:7/8:12/13:17/18:22/23:27/28:32. The settings for UltraMem and MoE models align with their dense counterparts based on dense parameter size. Table 6 shows the model parameter setting used in scaling experiments. Whats more, the common setting for UltraMem is shown in Table 5. Configuration Key Value Weight decay β1 β2 LR LR end ratio LR schedule LR warmup ratio Dropout Batch size Sequence length Training step 0.1 0.9 0.95 6e-4/2.5e-4/2e-4/1.2e-4 0.1 cosine 0.01 0.1 2048 2048 238418 Table 4: Training hyper-parameters Configuration Key Tucker rank Multi-core scoring Virtual memory expansion Aux loss weight α Aux loss margin τ Value 2 2 4 0.001 0.15 Table 5: Common UltraMem configuration Evaludation datasets. We use 10 benchmarks to evaluate all kind of models. 1. Knowledge: Massive Multitask Language Understanding (MMLU) (Hendrycks et al., 2020), TriviaQA (Joshi et al., 2017), Graduate-Level Google-Proof Q&A Benchmark (GPQA) (Rein et al., 2023), AI2 Reasoning Challenge (ARC) (Clark et al., 2018). 2. Reasoning: BIG-Bench Hard (BBH) (Suzgun et al., 2022), Boolean Questions (BoolQ) (Clark et al., 2019), HellaSwag (Hella) (Zellers et al., 2019), WinoGrande (Wino) (Sakaguchi et al., 2021). 3. Reading comprehension: Discrete Reasoning Over Paragraphs (DROP) (Dua et al., 2019). 4. Comprehensive ability: AGIEval (Zhong et al., 2023) Model Hidden Dim Inner Dim Attn Head Layer Dense-151M Dense-680M Dense-1.6B Dense-6.5B MoE-151M-2in32 MoE-680M-2in33 MoE-1.6B-2in34 UltraMem-151M-x10 UltraMem-151M-x12 UltraMem-680M-x12 UltraMem-1.6B-x12 1024 1536 2048 4096 1024 1536 2048 2048 1024 1536 2048 4096 6144 8192 16384 2528 3584 4672 8192 4096 6144 16 16 16 32 16 16 16 16 16 16 16 12 24 32 32 12 24 32 32 12 24 32 Topm - - - - 2 2 2 42x2 16x2 35x2 42x2 Expert Kdim Knum Ultra Mem Layer Param (B) FLOPs (G) - - - - 32 33 34 - - - - - - - - - - - 256 256 384 448 - - - - - - - 1024 1100 1632 1792 - - - - - - - 3 3 4 0.15 0.68 1.61 6.44 2.04 8.95 21.36 1.71 2.03 8.93 21.41 0.30 1.36 3.21 12.88 0.35 1.50 3.52 0.35 0.35 1.49 3.50 Table 6: Model parameter setting. Top-m means chosen expert number in MoE, means chosen value number times head number in UltraMem. Kdim means the key dimension in UltraMem. Knum means the number of keys, Knum2 is the number of values."
        },
        {
            "title": "F MORE EXPERIMENT RESULTS",
            "content": ""
        },
        {
            "title": "Preprint",
            "content": "(a) Training perplexity (b) Top1 score (c) UltraMem output standard deviation Figure 10: Model training state details. Top1 Score refers to the highest score among the retrieved keys. UltraMem Output Std represents the standard deviation of the outputs from the last layer of UltraMem. Table 7: All performance metrics of various models Param FLOPs ARC-C GPQA Trivia MMLU BBH BoolQ Hella Wino AGI DROP Avg Model Model (B) (G) Dense-151M MoE-151M-2in32 UltraMem-151M-x12 Dense-680M MoE-680M-2in33 UltraMem-680M-x 0.15 2.04 2.03 0.68 8.95 8.93 Dense-1.6B MoE-1.6B-2in34 UltraMem-1.6B-x12 1.61 21.36 21.41 0.30 0.35 0.35 1.36 1.50 1. 3.21 3.52 3.50 25.60 26.96 25.68 24.06 25.17 23.72 26.30 25.43 25.94 QA 12.67 33.27 28. 27.16 34.19 55.17 39.65 59.56 66.38 19.98 17.30 19.42 21.09 20.54 21.99 21.76 21.32 24.66 cot 22.57 23.24 22.65 24.65 26.63 26.62 26.41 29.46 30.63 26.50 26.58 25.62 24.64 24.38 24.97 26.19 26.18 24. Swag Grande Eval 50.15 55.96 47.74 46.42 43.70 48.20 51.50 42.78 59.8 35.07 48.44 43.96 48.83 62.71 64. 58.6 67.34 71.52 52.49 55.96 50.83 54.93 59.98 60.54 61.72 63.93 66.38 9.03 9.34 10.00 9.44 7.39 8. 9.22 6.63 8.77 13.60 18.57 14.08 22.97 26.54 25.14 22.63 28.81 29.99 26.77 31.56 28.89 30.42 33.13 35. 34.81 37.14 40.88 Dense-6.5B 6.44 12.88 28.16 19. 57.28 27.68 31.14 68.2 69.73 65. 9.23 33.12 41."
        },
        {
            "title": "Preprint",
            "content": "(a) Average accuracy (b) BBH-cot-3shot accuracy (c) DROP accuracy (d) Hellaswag accuracy (e) Winogrande 5shot accuracy (f) TriviaQA 5shot accuracy Figure 11: The changes in accuracy for all observable evaluation throughout the training."
        }
    ],
    "affiliations": [
        "Seed-Foundation-Model Team, ByteDance"
    ]
}